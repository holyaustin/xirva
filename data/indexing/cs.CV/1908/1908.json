[{"id": "1908.00024", "submitter": "Chiho Choi", "authors": "Chiho Choi, Srikanth Malla, Abhishek Patil, Joon Hee Choi", "title": "DROGON: A Trajectory Prediction Model based on Intention-Conditioned\n  Behavior Reasoning", "comments": "Conference on Robot Learning (CoRL) 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We propose a Deep RObust Goal-Oriented trajectory prediction Network (DROGON)\nfor accurate vehicle trajectory prediction by considering behavioral intentions\nof vehicles in traffic scenes. Our main insight is that the behavior (i.e.,\nmotion) of drivers can be reasoned from their high level possible goals (i.e.,\nintention) on the road. To succeed in such behavior reasoning, we build a\nconditional prediction model to forecast goal-oriented trajectories with the\nfollowing stages: (i) relational inference where we encode relational\ninteractions of vehicles using the perceptual context; (ii) intention\nestimation to compute the probability distributions of intentional goals based\non the inferred relations; and (iii) behavior reasoning where we reason about\nthe behaviors of vehicles as trajectories conditioned on the intentions. To\nthis end, we extend the proposed framework to the pedestrian trajectory\nprediction task, showing the potential applicability toward general trajectory\nprediction.\n", "versions": [{"version": "v1", "created": "Wed, 31 Jul 2019 18:04:28 GMT"}, {"version": "v2", "created": "Wed, 18 Sep 2019 16:51:48 GMT"}, {"version": "v3", "created": "Fri, 6 Nov 2020 09:59:58 GMT"}], "update_date": "2020-11-09", "authors_parsed": [["Choi", "Chiho", ""], ["Malla", "Srikanth", ""], ["Patil", "Abhishek", ""], ["Choi", "Joon Hee", ""]]}, {"id": "1908.00047", "submitter": "Berkan Demirel", "authors": "Berkan Demirel, Ramazan Gokberk Cinbis, Nazli Ikizler-Cinbis", "title": "Image Captioning with Unseen Objects", "comments": "To appear in British Machine Vision Conference (BMVC) 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image caption generation is a long standing and challenging problem at the\nintersection of computer vision and natural language processing. A number of\nrecently proposed approaches utilize a fully supervised object recognition\nmodel within the captioning approach. Such models, however, tend to generate\nsentences which only consist of objects predicted by the recognition models,\nexcluding instances of the classes without labelled training examples. In this\npaper, we propose a new challenging scenario that targets the image captioning\nproblem in a fully zero-shot learning setting, where the goal is to be able to\ngenerate captions of test images containing objects that are not seen during\ntraining. The proposed approach jointly uses a novel zero-shot object detection\nmodel and a template-based sentence generator. Our experiments show promising\nresults on the COCO dataset.\n", "versions": [{"version": "v1", "created": "Wed, 31 Jul 2019 18:48:52 GMT"}], "update_date": "2019-08-02", "authors_parsed": [["Demirel", "Berkan", ""], ["Cinbis", "Ramazan Gokberk", ""], ["Ikizler-Cinbis", "Nazli", ""]]}, {"id": "1908.00052", "submitter": "Chen Kong", "authors": "Chen Kong and Simon Lucey", "title": "Deep Non-Rigid Structure from Motion", "comments": "Oral Paper in ICCV 2019. arXiv admin note: substantial text overlap\n  with arXiv:1902.10840, arXiv:1907.13123", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current non-rigid structure from motion (NRSfM) algorithms are mainly limited\nwith respect to: (i) the number of images, and (ii) the type of shape\nvariability they can handle. This has hampered the practical utility of NRSfM\nfor many applications within vision. In this paper we propose a novel deep\nneural network to recover camera poses and 3D points solely from an ensemble of\n2D image coordinates. The proposed neural network is mathematically\ninterpretable as a multi-layer block sparse dictionary learning problem, and\ncan handle problems of unprecedented scale and shape complexity. Extensive\nexperiments demonstrate the impressive performance of our approach where we\nexhibit superior precision and robustness against all available\nstate-of-the-art works in the order of magnitude. We further propose a quality\nmeasure (based on the network weights) which circumvents the need for 3D\nground-truth to ascertain the confidence we have in the reconstruction.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jul 2019 00:26:07 GMT"}, {"version": "v2", "created": "Sun, 11 Aug 2019 19:07:59 GMT"}], "update_date": "2019-08-13", "authors_parsed": [["Kong", "Chen", ""], ["Lucey", "Simon", ""]]}, {"id": "1908.00061", "submitter": "Vikram Voleti", "authors": "Vincent Michalski, Vikram Voleti, Samira Ebrahimi Kahou, Anthony\n  Ortiz, Pascal Vincent, Chris Pal, Doina Precup", "title": "An Empirical Study of Batch Normalization and Group Normalization in\n  Conditional Computation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Batch normalization has been widely used to improve optimization in deep\nneural networks. While the uncertainty in batch statistics can act as a\nregularizer, using these dataset statistics specific to the training set\nimpairs generalization in certain tasks. Recently, alternative methods for\nnormalizing feature activations in neural networks have been proposed. Among\nthem, group normalization has been shown to yield similar, in some domains even\nsuperior performance to batch normalization. All these methods utilize a\nlearned affine transformation after the normalization operation to increase\nrepresentational power. Methods used in conditional computation define the\nparameters of these transformations as learnable functions of conditioning\ninformation. In this work, we study whether and where the conditional\nformulation of group normalization can improve generalization compared to\nconditional batch normalization. We evaluate performances on the tasks of\nvisual question answering, few-shot learning, and conditional image generation.\n", "versions": [{"version": "v1", "created": "Wed, 31 Jul 2019 19:37:16 GMT"}], "update_date": "2019-08-02", "authors_parsed": [["Michalski", "Vincent", ""], ["Voleti", "Vikram", ""], ["Kahou", "Samira Ebrahimi", ""], ["Ortiz", "Anthony", ""], ["Vincent", "Pascal", ""], ["Pal", "Chris", ""], ["Precup", "Doina", ""]]}, {"id": "1908.00069", "submitter": "Rayson Laroca", "authors": "Diego R. Lucio, Rayson Laroca, Luiz A. Zanlorensi, Gladston Moreira,\n  David Menotti", "title": "Simultaneous Iris and Periocular Region Detection Using Coarse\n  Annotations", "comments": "Accepted for presentation at the Conference on Graphics, Patterns and\n  Images (SIBGRAPI) 2019", "journal-ref": null, "doi": "10.1109/SIBGRAPI.2019.00032", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we propose to detect the iris and periocular regions\nsimultaneously using coarse annotations and two well-known object detectors:\nYOLOv2 and Faster R-CNN. We believe coarse annotations can be used in\nrecognition systems based on the iris and periocular regions, given the much\nsmaller engineering effort required to manually annotate the training images.\nWe manually made coarse annotations of the iris and periocular regions (122K\nimages from the visible (VIS) spectrum and 38K images from the near-infrared\n(NIR) spectrum). The iris annotations in the NIR databases were generated\nsemi-automatically by first applying an iris segmentation CNN and then\nperforming a manual inspection. These annotations were made for 11 well-known\npublic databases (3 NIR and 8 VIS) designed for the iris-based recognition\nproblem and are publicly available to the research community. Experimenting our\nproposal on these databases, we highlight two results. First, the Faster R-CNN\n+ Feature Pyramid Network (FPN) model reported an Intersection over Union (IoU)\nhigher than YOLOv2 (91.86% vs 85.30%). Second, the detection of the iris and\nperiocular regions being performed simultaneously is as accurate as performed\nseparately, but with a lower computational cost, i.e., two tasks were carried\nout at the cost of one.\n", "versions": [{"version": "v1", "created": "Wed, 31 Jul 2019 20:09:48 GMT"}], "update_date": "2021-05-14", "authors_parsed": [["Lucio", "Diego R.", ""], ["Laroca", "Rayson", ""], ["Zanlorensi", "Luiz A.", ""], ["Moreira", "Gladston", ""], ["Menotti", "David", ""]]}, {"id": "1908.00080", "submitter": "Faraz Hussain", "authors": "M.G. Sarwar Murshed, Christopher Murphy, Daqing Hou, Nazar Khan,\n  Ganesh Ananthanarayanan, Faraz Hussain", "title": "Machine Learning at the Network Edge: A Survey", "comments": "35 pages, 4 figures; restructured text to combine ML/DL into a single\n  section; updated tables/figures; added a new table summarizing major ML edge\n  applications, fixed typos", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Resource-constrained IoT devices, such as sensors and actuators, have become\nubiquitous in recent years. This has led to the generation of large quantities\nof data in real-time, which is an appealing target for AI systems. However,\ndeploying machine learning models on such end-devices is nearly impossible. A\ntypical solution involves offloading data to external computing systems (such\nas cloud servers) for further processing but this worsens latency, leads to\nincreased communication costs, and adds to privacy concerns. To address this\nissue, efforts have been made to place additional computing devices at the edge\nof the network, i.e close to the IoT devices where the data is generated.\nDeploying machine learning systems on such edge computing devices alleviates\nthe above issues by allowing computations to be performed close to the data\nsources. This survey describes major research efforts where machine learning\nsystems have been deployed at the edge of computer networks, focusing on the\noperational aspects including compression techniques, tools, frameworks, and\nhardware used in successful applications of intelligent edge systems.\n", "versions": [{"version": "v1", "created": "Wed, 31 Jul 2019 20:23:00 GMT"}, {"version": "v2", "created": "Wed, 29 Jan 2020 18:55:40 GMT"}, {"version": "v3", "created": "Mon, 21 Sep 2020 23:00:32 GMT"}, {"version": "v4", "created": "Sun, 23 May 2021 19:52:16 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Murshed", "M. G. Sarwar", ""], ["Murphy", "Christopher", ""], ["Hou", "Daqing", ""], ["Khan", "Nazar", ""], ["Ananthanarayanan", "Ganesh", ""], ["Hussain", "Faraz", ""]]}, {"id": "1908.00102", "submitter": "Tarang Chugh", "authors": "Tarang Chugh and Anil K. Jain", "title": "OCT Fingerprints: Resilience to Presentation Attacks", "comments": "Fingerprint presentation attack detection, OCT scanner; 9 pages, 8\n  figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Optical coherent tomography (OCT) fingerprint technology provides rich depth\ninformation, including internal fingerprint (papillary junction) and sweat\n(eccrine) glands, in addition to imaging any fake layers (presentation attacks)\nplaced over finger skin. Unlike 2D surface fingerprint scans, additional depth\ninformation provided by the cross-sectional OCT depth profile scans are\npurported to thwart fingerprint presentation attacks. We develop and evaluate a\npresentation attack detector (PAD) based on deep convolutional neural network\n(CNN). Input data to CNN are local patches extracted from the cross-sectional\nOCT depth profile scans captured using THORLabs Telesto series spectral-domain\nfingerprint reader. The proposed approach achieves a TDR of 99.73% @ FDR of\n0.2% on a database of 3,413 bonafide and 357 PA OCT scans, fabricated using 8\ndifferent PA materials. By employing a visualization technique, known as\nCNN-Fixations, we are able to identify the regions in the OCT scan patches that\nare crucial for fingerprint PAD detection.\n", "versions": [{"version": "v1", "created": "Wed, 31 Jul 2019 21:18:34 GMT"}], "update_date": "2019-08-02", "authors_parsed": [["Chugh", "Tarang", ""], ["Jain", "Anil K.", ""]]}, {"id": "1908.00111", "submitter": "Vasileios Belagiannis", "authors": "Leslie Casas, Attila Klimmek, Gustavo Carneiro, Nassir Navab,\n  Vasileios Belagiannis", "title": "Few-Shot Meta-Denoising", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of few-shot learning-based denoising where the training\nset contains just a handful of clean and noisy samples. A solution to mitigate\nthe small training set issue is to pre-train a denoising model with small\ntraining sets containing pairs of clean and synthesized noisy signals, produced\nfrom empirical noise priors, and fine-tune on the available small training set.\nWhile such transfer learning seems effective, it may not generalize well\nbecause of the limited amount of training data. In this work, we propose a new\nmeta-learning training approach for few-shot learning-based denoising problems.\nOur model is meta-trained using known synthetic noise models, and then\nfine-tuned with the small training set, with the real noise, as a few-shot\nlearning task. Meta-learning from small training sets of synthetically\ngenerated data during meta-training enables us to not only generate an infinite\nnumber of training tasks, but also train a model to learn with small training\nsets -- both advantages have the potential to improve the generalisation of the\ndenoising model. Our approach is empirically shown to produce more accurate\ndenoising results than supervised learning and transfer learning in three\ndenoising evaluations for images and 1-D signals. Interestingly, our study\nprovides strong indications that meta-learning has the potential to become the\nmain learning algorithm for denoising.\n", "versions": [{"version": "v1", "created": "Wed, 31 Jul 2019 21:40:44 GMT"}, {"version": "v2", "created": "Mon, 25 Nov 2019 21:19:46 GMT"}], "update_date": "2019-11-27", "authors_parsed": [["Casas", "Leslie", ""], ["Klimmek", "Attila", ""], ["Carneiro", "Gustavo", ""], ["Navab", "Nassir", ""], ["Belagiannis", "Vasileios", ""]]}, {"id": "1908.00114", "submitter": "Yi Xu", "authors": "Yi Xu, Shanglin Yang, Wei Sun, Li Tan, Kefeng Li, Hui Zhou", "title": "3D Virtual Garment Modeling from RGB Images", "comments": "9 pages; 9 figures; accepted to IEEE International Symposium on Mixed\n  and Augmented Reality 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel approach that constructs 3D virtual garment models from\nphotos. Unlike previous methods that require photos of a garment on a human\nmodel or a mannequin, our approach can work with various states of the garment:\non a model, on a mannequin, or on a flat surface. To construct a complete 3D\nvirtual model, our approach only requires two images as input, one front view\nand one back view. We first apply a multi-task learning network called JFNet\nthat jointly predicts fashion landmarks and parses a garment image into\nsemantic parts. The predicted landmarks are used for estimating sizing\ninformation of the garment. Then, a template garment mesh is deformed based on\nthe sizing information to generate the final 3D model. The semantic parts are\nutilized for extracting color textures from input images. The results of our\napproach can be used in various Virtual Reality and Mixed Reality applications.\n", "versions": [{"version": "v1", "created": "Wed, 31 Jul 2019 21:47:52 GMT"}], "update_date": "2019-08-02", "authors_parsed": [["Xu", "Yi", ""], ["Yang", "Shanglin", ""], ["Sun", "Wei", ""], ["Tan", "Li", ""], ["Li", "Kefeng", ""], ["Zhou", "Hui", ""]]}, {"id": "1908.00120", "submitter": "Zhizhong Han", "authors": "Zhizhong Han, Chao Chen, Yu-Shen Liu, Matthias Zwicker", "title": "ShapeCaptioner: Generative Caption Network for 3D Shapes by Learning a\n  Mapping from Parts Detected in Multiple Views to Sentences", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  3D shape captioning is a challenging application in 3D shape understanding.\nCaptions from recent multi-view based methods reveal that they cannot capture\npart-level characteristics of 3D shapes. This leads to a lack of detailed\npart-level description in captions, which human tend to focus on. To resolve\nthis issue, we propose ShapeCaptioner, a generative caption network, to perform\n3D shape captioning from semantic parts detected in multiple views. Our novelty\nlies in learning the knowledge of part detection in multiple views from 3D\nshape segmentations and transferring this knowledge to facilitate learning the\nmapping from 3D shapes to sentences. Specifically, ShapeCaptioner aggregates\nthe parts detected in multiple colored views using our novel part class\nspecific aggregation to represent a 3D shape, and then, employs a sequence to\nsequence model to generate the caption. Our outperforming results show that\nShapeCaptioner can learn 3D shape features with more detailed part\ncharacteristics to facilitate better 3D shape captioning than previous work.\n", "versions": [{"version": "v1", "created": "Wed, 31 Jul 2019 22:17:31 GMT"}], "update_date": "2019-08-02", "authors_parsed": [["Han", "Zhizhong", ""], ["Chen", "Chao", ""], ["Liu", "Yu-Shen", ""], ["Zwicker", "Matthias", ""]]}, {"id": "1908.00140", "submitter": "Max Reuter", "authors": "Max Reuter, Gheorghe-Teodor Bercea", "title": "Sublinear Subwindow Search", "comments": "8 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an efficient approximation algorithm for subwindow search that\nruns in sublinear time and memory. Applied to object localization, this\nalgorithm significantly reduces running time and memory usage while maintaining\ncompetitive accuracy scores compared to the state-of-the-art. The algorithm's\naccuracy also scales with both the size and the spatial coherence\n(nearby-element similarity) of the matrix. It is thus well-suited for real-time\napplications and against many matrices in general.\n", "versions": [{"version": "v1", "created": "Wed, 31 Jul 2019 23:21:52 GMT"}], "update_date": "2019-08-02", "authors_parsed": [["Reuter", "Max", ""], ["Bercea", "Gheorghe-Teodor", ""]]}, {"id": "1908.00151", "submitter": "Martin Sundermeyer", "authors": "Martin Sundermeyer, Maximilian Durner, En Yen Puang, Zoltan-Csaba\n  Marton, Narunas Vaskevicius, Kai O. Arras, Rudolph Triebel", "title": "Multi-path Learning for Object Pose Estimation Across Domains", "comments": "To appear at CVPR 2020; Code will be available here:\n  https://github.com/DLR-RM/AugmentedAutoencoder/tree/multipath", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a scalable approach for object pose estimation trained on\nsimulated RGB views of multiple 3D models together. We learn an encoding of\nobject views that does not only describe an implicit orientation of all objects\nseen during training, but can also relate views of untrained objects. Our\nsingle-encoder-multi-decoder network is trained using a technique we denote\n\"multi-path learning\": While the encoder is shared by all objects, each decoder\nonly reconstructs views of a single object. Consequently, views of different\ninstances do not have to be separated in the latent space and can share common\nfeatures. The resulting encoder generalizes well from synthetic to real data\nand across various instances, categories, model types and datasets. We\nsystematically investigate the learned encodings, their generalization, and\niterative refinement strategies on the ModelNet40 and T-LESS dataset. Despite\ntraining jointly on multiple objects, our 6D Object Detection pipeline achieves\nstate-of-the-art results on T-LESS at much lower runtimes than competing\napproaches.\n", "versions": [{"version": "v1", "created": "Thu, 1 Aug 2019 00:01:14 GMT"}, {"version": "v2", "created": "Fri, 3 Apr 2020 07:00:33 GMT"}], "update_date": "2020-04-06", "authors_parsed": [["Sundermeyer", "Martin", ""], ["Durner", "Maximilian", ""], ["Puang", "En Yen", ""], ["Marton", "Zoltan-Csaba", ""], ["Vaskevicius", "Narunas", ""], ["Arras", "Kai O.", ""], ["Triebel", "Rudolph", ""]]}, {"id": "1908.00169", "submitter": "Yadan Luo", "authors": "Yadan Luo, Zi Huang, Zheng Zhang, Ziwei Wang, Jingjing Li, Yang Yang", "title": "Curiosity-driven Reinforcement Learning for Diverse Visual Paragraph\n  Generation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Visual paragraph generation aims to automatically describe a given image from\ndifferent perspectives and organize sentences in a coherent way. In this paper,\nwe address three critical challenges for this task in a reinforcement learning\nsetting: the mode collapse, the delayed feedback, and the time-consuming\nwarm-up for policy networks. Generally, we propose a novel Curiosity-driven\nReinforcement Learning (CRL) framework to jointly enhance the diversity and\naccuracy of the generated paragraphs. First, by modeling the paragraph\ncaptioning as a long-term decision-making process and measuring the prediction\nuncertainty of state transitions as intrinsic rewards, the model is\nincentivized to memorize precise but rarely spotted descriptions to context,\nrather than being biased towards frequent fragments and generic patterns.\nSecond, since the extrinsic reward from evaluation is only available until the\ncomplete paragraph is generated, we estimate its expected value at each time\nstep with temporal-difference learning, by considering the correlations between\nsuccessive actions. Then the estimated extrinsic rewards are complemented by\ndense intrinsic rewards produced from the derived curiosity module, in order to\nencourage the policy to fully explore action space and find a global optimum.\nThird, discounted imitation learning is integrated for learning from human\ndemonstrations, without separately performing the time-consuming warm-up in\nadvance. Extensive experiments conducted on the Standford image-paragraph\ndataset demonstrate the effectiveness and efficiency of the proposed method,\nimproving the performance by 38.4% compared with state-of-the-art.\n", "versions": [{"version": "v1", "created": "Thu, 1 Aug 2019 01:33:28 GMT"}, {"version": "v2", "created": "Thu, 29 Aug 2019 09:57:16 GMT"}], "update_date": "2019-08-30", "authors_parsed": [["Luo", "Yadan", ""], ["Huang", "Zi", ""], ["Zhang", "Zheng", ""], ["Wang", "Ziwei", ""], ["Li", "Jingjing", ""], ["Yang", "Yang", ""]]}, {"id": "1908.00173", "submitter": "Jianlei Yang", "authors": "Xucheng Ye, Pengcheng Dai, Junyu Luo, Xin Guo, Yingjie Qi, Jianlei\n  Yang, Yiran Chen", "title": "Accelerating CNN Training by Pruning Activation Gradients", "comments": "accepted by ECCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Sparsification is an efficient approach to accelerate CNN inference, but it\nis challenging to take advantage of sparsity in training procedure because the\ninvolved gradients are dynamically changed. Actually, an important observation\nshows that most of the activation gradients in back-propagation are very close\nto zero and only have a tiny impact on weight-updating. Hence, we consider\npruning these very small gradients randomly to accelerate CNN training\naccording to the statistical distribution of activation gradients. Meanwhile,\nwe theoretically analyze the impact of pruning algorithm on the convergence.\nThe proposed approach is evaluated on AlexNet and ResNet-\\{18, 34, 50, 101,\n152\\} with CIFAR-\\{10, 100\\} and ImageNet datasets. Experimental results show\nthat our training approach could substantially achieve up to $5.92 \\times$\nspeedups at back-propagation stage with negligible accuracy loss.\n", "versions": [{"version": "v1", "created": "Thu, 1 Aug 2019 01:48:11 GMT"}, {"version": "v2", "created": "Thu, 26 Mar 2020 12:18:17 GMT"}, {"version": "v3", "created": "Mon, 20 Jul 2020 11:13:05 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Ye", "Xucheng", ""], ["Dai", "Pengcheng", ""], ["Luo", "Junyu", ""], ["Guo", "Xin", ""], ["Qi", "Yingjie", ""], ["Yang", "Jianlei", ""], ["Chen", "Yiran", ""]]}, {"id": "1908.00178", "submitter": "Dzung Doan Anh", "authors": "Anh-Dzung Doan, Yasir Latif, Tat-Jun Chin, Yu Liu, Thanh-Toan Do, Ian\n  Reid", "title": "Scalable Place Recognition Under Appearance Change for Autonomous\n  Driving", "comments": null, "journal-ref": "International Conference on Computer Vision (ICCV), 2019 (Oral)", "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A major challenge in place recognition for autonomous driving is to be robust\nagainst appearance changes due to short-term (e.g., weather, lighting) and\nlong-term (seasons, vegetation growth, etc.) environmental variations. A\npromising solution is to continuously accumulate images to maintain an adequate\nsample of the conditions and incorporate new changes into the place recognition\ndecision. However, this demands a place recognition technique that is scalable\non an ever growing dataset. To this end, we propose a novel place recognition\ntechnique that can be efficiently retrained and compressed, such that the\nrecognition of new queries can exploit all available data (including recent\nchanges) without suffering from visible growth in computational cost.\nUnderpinning our method is a novel temporal image matching technique based on\nHidden Markov Models. Our experiments show that, compared to state-of-the-art\ntechniques, our method has much greater potential for large-scale place\nrecognition for autonomous driving.\n", "versions": [{"version": "v1", "created": "Thu, 1 Aug 2019 02:04:27 GMT"}], "update_date": "2019-08-02", "authors_parsed": [["Doan", "Anh-Dzung", ""], ["Latif", "Yasir", ""], ["Chin", "Tat-Jun", ""], ["Liu", "Yu", ""], ["Do", "Thanh-Toan", ""], ["Reid", "Ian", ""]]}, {"id": "1908.00186", "submitter": "Chihiro Go", "authors": "Chihiro Go, Yuma Kinoshita, Sayaka Shiota and Hitoshi Kiya", "title": "Single-Shot High Dynamic Range Imaging with Spatially Varying Exposures\n  Considering Hue Distortion", "comments": "To appear in 2019 IEEE 8th Global Conference on Consumer Electronics", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We proposes a novel single-shot high dynamic range imaging scheme with\nspatially varying exposures (SVE) considering hue distortion. Single-shot\nimaging with SVE enables us to capture multi-exposure images from a single-shot\nimage, so high dynamic range images can be produced without ghost artifacts.\nHowever, SVE images have some pixels at which a range supported by camera\nsensors is exceeded. Therefore, generated images have some color distortion, so\nthat conventional imaging with SVE has never considered the influence of this\nrange limitation. To overcome this issue, we consider estimating the correct\nhue of a scene from raw images, and propose a method with the estimated hue\ninformation for correcting the hue of SVE images on the constant hue plain in\nthe RGB color space.\n", "versions": [{"version": "v1", "created": "Thu, 1 Aug 2019 02:33:55 GMT"}], "update_date": "2019-08-02", "authors_parsed": [["Go", "Chihiro", ""], ["Kinoshita", "Yuma", ""], ["Shiota", "Sayaka", ""], ["Kiya", "Hitoshi", ""]]}, {"id": "1908.00191", "submitter": "Anwesan Pal", "authors": "Anwesan Pal, Carlos Nieto-Granda and Henrik I. Christensen", "title": "DEDUCE: Diverse scEne Detection methods in Unseen Challenging\n  Environments", "comments": "Paper accepted at IROS'19", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, there has been a rapid increase in the number of service\nrobots deployed for aiding people in their daily activities. Unfortunately,\nmost of these robots require human input for training in order to do tasks in\nindoor environments. Successful domestic navigation often requires access to\nsemantic information about the environment, which can be learned without human\nguidance. In this paper, we propose a set of DEDUCE - Diverse scEne Detection\nmethods in Unseen Challenging Environments algorithms which incorporate deep\nfusion models derived from scene recognition systems and object detectors. The\nfive methods described here have been evaluated on several popular recent image\ndatasets, as well as real-world videos acquired through multiple mobile\nplatforms. The final results show an improvement over the existing\nstate-of-the-art visual place recognition systems.\n", "versions": [{"version": "v1", "created": "Thu, 1 Aug 2019 03:07:46 GMT"}], "update_date": "2019-08-02", "authors_parsed": [["Pal", "Anwesan", ""], ["Nieto-Granda", "Carlos", ""], ["Christensen", "Henrik I.", ""]]}, {"id": "1908.00211", "submitter": "Ang Li", "authors": "Ang Li, Jianzhong Qi, Rui Zhang, Xingjun Ma, Kotagiri Ramamohanarao", "title": "Generative Image Inpainting with Submanifold Alignment", "comments": "accepted by IJCAI 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image inpainting aims at restoring missing regions of corrupted images, which\nhas many applications such as image restoration and object removal. However,\ncurrent GAN-based generative inpainting models do not explicitly exploit the\nstructural or textural consistency between restored contents and their\nsurrounding contexts.To address this limitation, we propose to enforce the\nalignment (or closeness) between the local data submanifolds (or subspaces)\naround restored images and those around the original (uncorrupted) images\nduring the learning process of GAN-based inpainting models. We exploit Local\nIntrinsic Dimensionality (LID) to measure, in deep feature space, the alignment\nbetween data submanifolds learned by a GAN model and those of the original\ndata, from a perspective of both images (denoted as iLID) and local patches\n(denoted as pLID) of images. We then apply iLID and pLID as regularizations for\nGAN-based inpainting models to encourage two levels of submanifold alignment:\n1) an image-level alignment for improving structural consistency, and 2) a\npatch-level alignment for improving textural details. Experimental results on\nfour benchmark datasets show that our proposed model can generate more accurate\nresults than state-of-the-art models.\n", "versions": [{"version": "v1", "created": "Thu, 1 Aug 2019 05:01:17 GMT"}, {"version": "v2", "created": "Sat, 12 Sep 2020 04:44:03 GMT"}], "update_date": "2020-09-15", "authors_parsed": [["Li", "Ang", ""], ["Qi", "Jianzhong", ""], ["Zhang", "Rui", ""], ["Ma", "Xingjun", ""], ["Ramamohanarao", "Kotagiri", ""]]}, {"id": "1908.00213", "submitter": "Shunta Saito", "authors": "Seiya Tokui, Ryosuke Okuta, Takuya Akiba, Yusuke Niitani, Toru Ogawa,\n  Shunta Saito, Shuji Suzuki, Kota Uenishi, Brian Vogel, Hiroyuki Yamazaki\n  Vincent", "title": "Chainer: A Deep Learning Framework for Accelerating the Research Cycle", "comments": "Accepted for Applied Data Science Track in KDD'19", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.DC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Software frameworks for neural networks play a key role in the development\nand application of deep learning methods. In this paper, we introduce the\nChainer framework, which intends to provide a flexible, intuitive, and high\nperformance means of implementing the full range of deep learning models needed\nby researchers and practitioners. Chainer provides acceleration using Graphics\nProcessing Units with a familiar NumPy-like API through CuPy, supports general\nand dynamic models in Python through Define-by-Run, and also provides add-on\npackages for state-of-the-art computer vision models as well as distributed\ntraining.\n", "versions": [{"version": "v1", "created": "Thu, 1 Aug 2019 05:07:00 GMT"}], "update_date": "2019-08-02", "authors_parsed": [["Tokui", "Seiya", ""], ["Okuta", "Ryosuke", ""], ["Akiba", "Takuya", ""], ["Niitani", "Yusuke", ""], ["Ogawa", "Toru", ""], ["Saito", "Shunta", ""], ["Suzuki", "Shuji", ""], ["Uenishi", "Kota", ""], ["Vogel", "Brian", ""], ["Vincent", "Hiroyuki Yamazaki", ""]]}, {"id": "1908.00219", "submitter": "Nemanja Djuric", "authors": "Henggang Cui, Thi Nguyen, Fang-Chieh Chou, Tsung-Han Lin, Jeff\n  Schneider, David Bradley, Nemanja Djuric", "title": "Deep Kinematic Models for Kinematically Feasible Vehicle Trajectory\n  Predictions", "comments": "Accepted for publication at IEEE International Conference on Robotics\n  and Automation (ICRA) 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Self-driving vehicles (SDVs) hold great potential for improving traffic\nsafety and are poised to positively affect the quality of life of millions of\npeople. To unlock this potential one of the critical aspects of the autonomous\ntechnology is understanding and predicting future movement of vehicles\nsurrounding the SDV. This work presents a deep-learning-based method for\nkinematically feasible motion prediction of such traffic actors. Previous work\ndid not explicitly encode vehicle kinematics and instead relied on the models\nto learn the constraints directly from the data, potentially resulting in\nkinematically infeasible, suboptimal trajectory predictions. To address this\nissue we propose a method that seamlessly combines ideas from the AI with\nphysically grounded vehicle motion models. In this way we employ best of the\nboth worlds, coupling powerful learning models with strong feasibility\nguarantees for their outputs. The proposed approach is general, being\napplicable to any type of learning method. Extensive experiments using deep\nconvnets on real-world data strongly indicate its benefits, outperforming the\nexisting state-of-the-art.\n", "versions": [{"version": "v1", "created": "Thu, 1 Aug 2019 05:44:56 GMT"}, {"version": "v2", "created": "Tue, 3 Mar 2020 22:39:56 GMT"}, {"version": "v3", "created": "Sat, 24 Oct 2020 20:30:48 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Cui", "Henggang", ""], ["Nguyen", "Thi", ""], ["Chou", "Fang-Chieh", ""], ["Lin", "Tsung-Han", ""], ["Schneider", "Jeff", ""], ["Bradley", "David", ""], ["Djuric", "Nemanja", ""]]}, {"id": "1908.00222", "submitter": "Jia Zheng", "authors": "Jia Zheng and Junfei Zhang and Jing Li and Rui Tang and Shenghua Gao\n  and Zihan Zhou", "title": "Structured3D: A Large Photo-realistic Dataset for Structured 3D Modeling", "comments": "Accepted to ECCV 2020. Project website:\n  https://structured3d-dataset.org", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, there has been growing interest in developing learning-based\nmethods to detect and utilize salient semi-global or global structures, such as\njunctions, lines, planes, cuboids, smooth surfaces, and all types of\nsymmetries, for 3D scene modeling and understanding. However, the ground truth\nannotations are often obtained via human labor, which is particularly\nchallenging and inefficient for such tasks due to the large number of 3D\nstructure instances (e.g., line segments) and other factors such as viewpoints\nand occlusions. In this paper, we present a new synthetic dataset,\nStructured3D, with the aim of providing large-scale photo-realistic images with\nrich 3D structure annotations for a wide spectrum of structured 3D modeling\ntasks. We take advantage of the availability of professional interior designs\nand automatically extract 3D structures from them. We generate high-quality\nimages with an industry-leading rendering engine. We use our synthetic dataset\nin combination with real images to train deep networks for room layout\nestimation and demonstrate improved performance on benchmark datasets.\n", "versions": [{"version": "v1", "created": "Thu, 1 Aug 2019 06:01:19 GMT"}, {"version": "v2", "created": "Sun, 15 Mar 2020 12:10:52 GMT"}, {"version": "v3", "created": "Fri, 17 Jul 2020 04:27:08 GMT"}], "update_date": "2020-07-20", "authors_parsed": [["Zheng", "Jia", ""], ["Zhang", "Junfei", ""], ["Li", "Jing", ""], ["Tang", "Rui", ""], ["Gao", "Shenghua", ""], ["Zhou", "Zihan", ""]]}, {"id": "1908.00249", "submitter": "Ting Yao", "authors": "Jing Wang and Yingwei Pan and Ting Yao and Jinhui Tang and Tao Mei", "title": "Convolutional Auto-encoding of Sentence Topics for Image Paragraph\n  Generation", "comments": "IJCAI 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image paragraph generation is the task of producing a coherent story (usually\na paragraph) that describes the visual content of an image. The problem\nnevertheless is not trivial especially when there are multiple descriptive and\ndiverse gists to be considered for paragraph generation, which often happens in\nreal images. A valid question is how to encapsulate such gists/topics that are\nworthy of mention from an image, and then describe the image from one topic to\nanother but holistically with a coherent structure. In this paper, we present a\nnew design --- Convolutional Auto-Encoding (CAE) that purely employs\nconvolutional and deconvolutional auto-encoding framework for topic modeling on\nthe region-level features of an image. Furthermore, we propose an architecture,\nnamely CAE plus Long Short-Term Memory (dubbed as CAE-LSTM), that novelly\nintegrates the learnt topics in support of paragraph generation. Technically,\nCAE-LSTM capitalizes on a two-level LSTM-based paragraph generation framework\nwith attention mechanism. The paragraph-level LSTM captures the inter-sentence\ndependency in a paragraph, while sentence-level LSTM is to generate one\nsentence which is conditioned on each learnt topic. Extensive experiments are\nconducted on Stanford image paragraph dataset, and superior results are\nreported when comparing to state-of-the-art approaches. More remarkably,\nCAE-LSTM increases CIDEr performance from 20.93% to 25.15%.\n", "versions": [{"version": "v1", "created": "Thu, 1 Aug 2019 07:58:50 GMT"}], "update_date": "2019-08-02", "authors_parsed": [["Wang", "Jing", ""], ["Pan", "Yingwei", ""], ["Yao", "Ting", ""], ["Tang", "Jinhui", ""], ["Mei", "Tao", ""]]}, {"id": "1908.00258", "submitter": "Bruno Ferrarini", "authors": "Bruno Ferrarini, Maria Waheed, Sania Waheed, Shoaib Ehsan, Michael\n  Milford, Klaus D. McDonald-Maier", "title": "Visual Place Recognition for Aerial Robotics: Exploring\n  Accuracy-Computation Trade-off for Local Image Descriptors", "comments": null, "journal-ref": "NASA/ESA Conference on Adaptive Hardware and Systems (AHS 2019)", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual Place Recognition (VPR) is a fundamental yet challenging task for\nsmall Unmanned Aerial Vehicle (UAV). The core reasons are the extreme viewpoint\nchanges, and limited computational power onboard a UAV which restricts the\napplicability of robust but computation intensive state-of-the-art VPR methods.\nIn this context, a viable approach is to use local image descriptors for\nperforming VPR as these can be computed relatively efficiently without the need\nof any special hardware, such as a GPU. However, the choice of a local feature\ndescriptor is not trivial and calls for a detailed investigation as there is a\ntrade-off between VPR accuracy and the required computational effort. To fill\nthis research gap, this paper examines the performance of several\nstate-of-the-art local feature descriptors, both from accuracy and\ncomputational perspectives, specifically for VPR application utilizing standard\naerial datasets. The presented results confirm that a trade-off between\naccuracy and computational effort is inevitable while executing VPR on\nresource-constrained hardware.\n", "versions": [{"version": "v1", "created": "Thu, 1 Aug 2019 08:14:00 GMT"}], "update_date": "2019-08-02", "authors_parsed": [["Ferrarini", "Bruno", ""], ["Waheed", "Maria", ""], ["Waheed", "Sania", ""], ["Ehsan", "Shoaib", ""], ["Milford", "Michael", ""], ["McDonald-Maier", "Klaus D.", ""]]}, {"id": "1908.00262", "submitter": "Jaehoon Choi", "authors": "Jaehoon Choi, Minki Jeong, Taekyung Kim, Changick Kim", "title": "Pseudo-Labeling Curriculum for Unsupervised Domain Adaptation", "comments": "Accepted to British Machine Vision Conference (BMVC) 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  To learn target discriminative representations, using pseudo-labels is a\nsimple yet effective approach for unsupervised domain adaptation. However, the\nexistence of false pseudo-labels, which may have a detrimental influence on\nlearning target representations, remains a major challenge. To overcome this\nissue, we propose a pseudo-labeling curriculum based on a density-based\nclustering algorithm. Since samples with high density values are more likely to\nhave correct pseudo-labels, we leverage these subsets to train our target\nnetwork at the early stage, and utilize data subsets with low density values at\nthe later stage. We can progressively improve the capability of our network to\ngenerate pseudo-labels, and thus these target samples with pseudo-labels are\neffective for training our model. Moreover, we present a clustering constraint\nto enhance the discriminative power of the learned target features. Our\napproach achieves state-of-the-art performance on three benchmarks: Office-31,\nimageCLEF-DA, and Office-Home.\n", "versions": [{"version": "v1", "created": "Thu, 1 Aug 2019 08:23:04 GMT"}], "update_date": "2019-08-02", "authors_parsed": [["Choi", "Jaehoon", ""], ["Jeong", "Minki", ""], ["Kim", "Taekyung", ""], ["Kim", "Changick", ""]]}, {"id": "1908.00265", "submitter": "David B. Blumenthal", "authors": "David B. Blumenthal", "title": "New Techniques for Graph Edit Distance Computation", "comments": "Ph.D. Thesis, Free University of Bozen-Bolzano", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CV", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Due to their capacity to encode rich structural information, labeled graphs\nare often used for modeling various kinds of objects such as images, molecules,\nand chemical compounds. If pattern recognition problems such as clustering and\nclassification are to be solved on these domains, a (dis-)similarity measure\nfor labeled graphs has to be defined. A widely used measure is the graph edit\ndistance (GED), which, intuitively, is defined as the minimum amount of\ndistortion that has to be applied to a source graph in order to transform it\ninto a target graph. The main advantage of GED is its flexibility and\nsensitivity to small differences between the input graphs. Its main drawback is\nthat it is hard to compute.\n  In this thesis, new results and techniques for several aspects of computing\nGED are presented. Firstly, theoretical aspects are discussed: competing\ndefinitions of GED are harmonized, the problem of computing GED is\ncharacterized in terms of complexity, and several reductions from GED to the\nquadratic assignment problem (QAP) are presented. Secondly, solvers for the\nlinear sum assignment problem with error-correction (LSAPE) are discussed.\nLSAPE is a generalization of the well-known linear sum assignment problem\n(LSAP), and has to be solved as a subproblem by many GED algorithms. In\nparticular, a new solver is presented that efficiently reduces LSAPE to LSAP.\nThirdly, exact algorithms for computing GED are presented in a systematic way,\nand improvements of existing algorithms as well as a new mixed integer\nprogramming (MIP) based approach are introduced. Fourthly, a detailed overview\nof heuristic algorithms that approximate GED via upper and lower bounds is\nprovided, and eight new heuristics are described. Finally, a new easily\nextensible C++ library for exactly or approximately computing GED is presented.\n", "versions": [{"version": "v1", "created": "Thu, 1 Aug 2019 08:33:48 GMT"}], "update_date": "2019-08-02", "authors_parsed": [["Blumenthal", "David B.", ""]]}, {"id": "1908.00273", "submitter": "Yiyun Zhao", "authors": "Yiyun Zhao, Zhuqing Jiang, Aidong Men, Guodong Ju", "title": "Pyramid Real Image Denoising Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While deep Convolutional Neural Networks (CNNs) have shown extraordinary\ncapability of modelling specific noise and denoising, they still perform poorly\non real-world noisy images. The main reason is that the real-world noise is\nmore sophisticated and diverse. To tackle the issue of blind denoising, in this\npaper, we propose a novel pyramid real image denoising network (PRIDNet), which\ncontains three stages. First, the noise estimation stage uses channel attention\nmechanism to recalibrate the channel importance of input noise. Second, at the\nmulti-scale denoising stage, pyramid pooling is utilized to extract multi-scale\nfeatures. Third, the stage of feature fusion adopts a kernel selecting\noperation to adaptively fuse multi-scale features. Experiments on two datasets\nof real noisy photographs demonstrate that our approach can achieve competitive\nperformance in comparison with state-of-the-art denoisers in terms of both\nquantitative measure and visual perception quality. Code is available at\nhttps://github.com/491506870/PRIDNet.\n", "versions": [{"version": "v1", "created": "Thu, 1 Aug 2019 08:51:10 GMT"}, {"version": "v2", "created": "Tue, 22 Oct 2019 03:27:36 GMT"}], "update_date": "2019-10-23", "authors_parsed": [["Zhao", "Yiyun", ""], ["Jiang", "Zhuqing", ""], ["Men", "Aidong", ""], ["Ju", "Guodong", ""]]}, {"id": "1908.00274", "submitter": "M. Saquib Sarfraz", "authors": "M. Saquib Sarfraz, Constantin Seibold, Haroon Khalid, Rainer\n  Stiefelhagen", "title": "Content and Colour Distillation for Learning Image Translations with the\n  Spatial Profile Loss", "comments": "BMVC 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative adversarial networks has emerged as a defacto standard for image\ntranslation problems. To successfully drive such models, one has to rely on\nadditional networks e.g., discriminators and/or perceptual networks. Training\nthese networks with pixel based losses alone are generally not sufficient to\nlearn the target distribution. In this paper, we propose a novel method of\ncomputing the loss directly between the source and target images that enable\nproper distillation of shape/content and colour/style. We show that this is\nuseful in typical image-to-image translations allowing us to successfully drive\nthe generator without relying on additional networks. We demonstrate this on\nmany difficult image translation problems such as image-to-image domain\nmapping, single image super-resolution and photo realistic makeup transfer. Our\nextensive evaluation shows the effectiveness of the proposed formulation and\nits ability to synthesize realistic images. [Code release:\nhttps://github.com/ssarfraz/SPL]\n", "versions": [{"version": "v1", "created": "Thu, 1 Aug 2019 08:53:06 GMT"}], "update_date": "2019-08-02", "authors_parsed": [["Sarfraz", "M. Saquib", ""], ["Seibold", "Constantin", ""], ["Khalid", "Haroon", ""], ["Stiefelhagen", "Rainer", ""]]}, {"id": "1908.00275", "submitter": "Minjie Hua", "authors": "Minjie Hua, Yibing Nan and Shiguo Lian", "title": "Falls Prediction Based on Body Keypoints and Seq2Seq Architecture", "comments": "Accepted by HBU 2019 (ICCV Workshop)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a novel approach for predicting the falls of people in\nadvance from monocular video. First, all persons in the observed frames are\ndetected and tracked with the coordinates of their body keypoints being\nextracted meanwhile. A keypoints vectorization method is exploited to eliminate\nirrelevant information in the initial coordinate representation. Then, the\nobserved keypoint sequence of each person is input to the pose prediction\nmodule adapted from sequence-to-sequence(seq2seq) architecture to predict the\nfuture keypoint sequence. Finally, the predicted pose is analyzed by the falls\nclassifier to judge whether the person will fall down in the future. The pose\nprediction module and falls classifier are trained separately and tuned jointly\nusing Le2i dataset, which contains 191 videos of various normal daily\nactivities as well as falls performed by several actors. The contrast\nexperiments with mainstream raw RGB-based models show the accuracy improvement\nof utilizing body keypoints in falls classification. Moreover, the precognition\nof falls is proved effective by comparisons between models that with and\nwithout the pose prediction module.\n", "versions": [{"version": "v1", "created": "Thu, 1 Aug 2019 08:54:56 GMT"}, {"version": "v2", "created": "Fri, 30 Aug 2019 11:44:18 GMT"}], "update_date": "2019-09-02", "authors_parsed": [["Hua", "Minjie", ""], ["Nan", "Yibing", ""], ["Lian", "Shiguo", ""]]}, {"id": "1908.00309", "submitter": "Pedro Miraldo", "authors": "Romulo T. Rodrigues, Pedro Miraldo, Dimos V. Dimarogonas, A. Pedro\n  Aguiar", "title": "A Framework for Depth Estimation and Relative Localization of Ground\n  Robots using Computer Vision", "comments": "6 pages, 7 figures, conference", "journal-ref": "IEEE/RSJ International Conference on Intelligent Robots and\n  Systems (IROS), 2019", "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The 3D depth estimation and relative pose estimation problem within a\ndecentralized architecture is a challenging problem that arises in missions\nthat require coordination among multiple vision-controlled robots. The depth\nestimation problem aims at recovering the 3D information of the environment.\nThe relative localization problem consists of estimating the relative pose\nbetween two robots, by sensing each other's pose or sharing information about\nthe perceived environment. Most solutions for these problems use a set of\ndiscrete data without taking into account the chronological order of the\nevents. This paper builds on recent results on continuous estimation to propose\na framework that estimates the depth and relative pose between two\nnon-holonomic vehicles. The basic idea consists in estimating the depth of the\npoints by explicitly considering the dynamics of the camera mounted on a ground\nrobot, and feeding the estimates of 3D points observed by both cameras in a\nfilter that computes the relative pose between the robots. We evaluate the\nconvergence for a set of simulated scenarios and show experimental results\nvalidating the proposed framework.\n", "versions": [{"version": "v1", "created": "Thu, 1 Aug 2019 10:28:23 GMT"}], "update_date": "2019-08-02", "authors_parsed": [["Rodrigues", "Romulo T.", ""], ["Miraldo", "Pedro", ""], ["Dimarogonas", "Dimos V.", ""], ["Aguiar", "A. Pedro", ""]]}, {"id": "1908.00328", "submitter": "Jin Hyeok Yoo", "authors": "Jin Hyeok Yoo, Dongsuk Kum and Jun Won Choi", "title": "ScarfNet: Multi-scale Features with Deeply Fused and Redistributed\n  Semantics for Enhanced Object Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional neural network (CNN) has led to significant progress in object\ndetection. In order to detect the objects in various sizes, the object\ndetectors often exploit the hierarchy of the multi-scale feature maps called\nfeature pyramid, which is readily obtained by the CNN architecture. However,\nthe performance of these object detectors is limited since the bottom-level\nfeature maps, which experience fewer convolutional layers, lack the semantic\ninformation needed to capture the characteristics of the small objects. In\norder to address such problem, various methods have been proposed to increase\nthe depth for the bottom-level features used for object detection. While most\napproaches are based on the generation of additional features through the\ntop-down pathway with lateral connections, our approach directly fuses\nmulti-scale feature maps using bidirectional long short term memory (biLSTM) in\neffort to generate deeply fused semantics. Then, the resulting semantic\ninformation is redistributed to the individual pyramidal feature at each scale\nthrough the channel-wise attention model. We integrate our semantic combining\nand attentive redistribution feature network (ScarfNet) with baseline object\ndetectors, i.e., Faster R-CNN, single-shot multibox detector (SSD) and\nRetinaNet. Our experiments show that our method outperforms the existing\nfeature pyramid methods as well as the baseline detectors and achieve the state\nof the art performances in the PASCAL VOC and COCO detection benchmarks.\n", "versions": [{"version": "v1", "created": "Thu, 1 Aug 2019 11:07:17 GMT"}, {"version": "v2", "created": "Sat, 18 Jan 2020 08:47:55 GMT"}], "update_date": "2020-01-22", "authors_parsed": [["Yoo", "Jin Hyeok", ""], ["Kum", "Dongsuk", ""], ["Choi", "Jun Won", ""]]}, {"id": "1908.00329", "submitter": "Nao Mishima", "authors": "Nao Mishima, Tatsuo Kozakaya, Akihisa Moriya, Ryuzo Okada, Shinsaku\n  Hiura", "title": "Physical Cue based Depth-Sensing by Color Coding with Deaberration\n  Network", "comments": "To appear in BMVC2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Color-coded aperture (CCA) methods can physically measure the depth of a\nscene given by physical cues from a single-shot image of a monocular camera.\nHowever, they are vulnerable to actual lens aberrations in real scenes because\nthey assume an ideal lens for simplifying algorithms. In this paper, we propose\nphysical cue-based deep learning for CCA photography. To address actual lens\naberrations, we developed a deep deaberration network (DDN) that is\nadditionally equipped with a self-attention mechanism of position and color\nchannels to efficiently learn the lens aberration. Furthermore, a new Bayes L1\nloss function based on Bayesian deep learning enables to handle the uncertainty\nof depth estimation more accurately. Quantitative and qualitative comparisons\ndemonstrate that our method is superior to conventional methods including real\noutdoor scenes. Furthermore, compared to a long-baseline stereo camera, the\nproposed method provides an error-free depth map at close range, as there is no\nblind spot between the left and right cameras.\n", "versions": [{"version": "v1", "created": "Thu, 1 Aug 2019 11:08:10 GMT"}], "update_date": "2019-08-02", "authors_parsed": [["Mishima", "Nao", ""], ["Kozakaya", "Tatsuo", ""], ["Moriya", "Akihisa", ""], ["Okada", "Ryuzo", ""], ["Hiura", "Shinsaku", ""]]}, {"id": "1908.00340", "submitter": "Daniel Omeiza A", "authors": "Daniel Omeiza", "title": "Efficient Machine Learning for Large-Scale Urban Land-Use Forecasting in\n  Sub-Saharan Africa", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Urbanization is a common phenomenon in developing countries and it poses\nserious challenges when not managed effectively. Lack of proper planning and\nmanagement may cause the encroachment of urban fabrics into reserved or special\nregions which in turn can lead to an unsustainable increase in population.\nIneffective management and planning generally leads to depreciated standard of\nliving, where physical hazards like traffic accidents and disease vector\nbreeding become prevalent. In order to support urban planners and policy makers\nin effective planning and accurate decision making, we investigate urban\nland-use in sub-Saharan Africa. Land-use dynamics serves as a crucial parameter\nin current strategies and policies for natural resource management and\nmonitoring. Focusing on Nairobi, we use an efficient deep learning approach\nwith patch-based prediction to classify regions based on land-use from 2004 to\n2018 on a quarterly basis. We estimate changes in land-use within this period,\nand using the Autoregressive Integrated Moving Average (ARIMA) model, our\nresults forecast land-use for a given future date. Furthermore, we provide\nlabelled land-use maps which will be helpful to urban planners.\n", "versions": [{"version": "v1", "created": "Thu, 1 Aug 2019 11:43:38 GMT"}], "update_date": "2019-08-02", "authors_parsed": [["Omeiza", "Daniel", ""]]}, {"id": "1908.00347", "submitter": "Li Yuan", "authors": "Li Yuan, Tao Wang, Xiaopeng Zhang, Francis EH Tay, Zequn Jie, Wei Liu,\n  Jiashi Feng", "title": "Central Similarity Quantization for Efficient Image and Video Retrieval", "comments": "CVPR2020, Codes:\n  https://github.com/yuanli2333/Hadamard-Matrix-for-hashing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing data-dependent hashing methods usually learn hash functions from\npairwise or triplet data relationships, which only capture the data similarity\nlocally, and often suffer from low learning efficiency and low collision rate.\nIn this work, we propose a new \\emph{global} similarity metric, termed as\n\\emph{central similarity}, with which the hash codes of similar data pairs are\nencouraged to approach a common center and those for dissimilar pairs to\nconverge to different centers, to improve hash learning efficiency and\nretrieval accuracy. We principally formulate the computation of the proposed\ncentral similarity metric by introducing a new concept, i.e., \\emph{hash\ncenter} that refers to a set of data points scattered in the Hamming space with\na sufficient mutual distance between each other. We then provide an efficient\nmethod to construct well separated hash centers by leveraging the Hadamard\nmatrix and Bernoulli distributions. Finally, we propose the Central Similarity\nQuantization (CSQ) that optimizes the central similarity between data points\nw.r.t.\\ their hash centers instead of optimizing the local similarity. CSQ is\ngeneric and applicable to both image and video hashing scenarios. Extensive\nexperiments on large-scale image and video retrieval tasks demonstrate that CSQ\ncan generate cohesive hash codes for similar data pairs and dispersed hash\ncodes for dissimilar pairs, achieving a noticeable boost in retrieval\nperformance, i.e. 3\\%-20\\% in mAP over the previous state-of-the-arts. The code\nis at: \\url{https://github.com/yuanli2333/Hadamard-Matrix-for-hashing}\n", "versions": [{"version": "v1", "created": "Thu, 1 Aug 2019 12:07:57 GMT"}, {"version": "v2", "created": "Mon, 18 Nov 2019 00:37:04 GMT"}, {"version": "v3", "created": "Fri, 27 Mar 2020 01:48:48 GMT"}, {"version": "v4", "created": "Mon, 30 Mar 2020 00:49:19 GMT"}, {"version": "v5", "created": "Tue, 31 Mar 2020 00:55:29 GMT"}], "update_date": "2020-04-01", "authors_parsed": [["Yuan", "Li", ""], ["Wang", "Tao", ""], ["Zhang", "Xiaopeng", ""], ["Tay", "Francis EH", ""], ["Jie", "Zequn", ""], ["Liu", "Wei", ""], ["Feng", "Jiashi", ""]]}, {"id": "1908.00375", "submitter": "Dingquan Li", "authors": "Dingquan Li, Tingting Jiang and Ming Jiang", "title": "Quality Assessment of In-the-Wild Videos", "comments": "9 pages, 7 figures, 4 tables. ACM Multimedia 2019 camera ready. ->\n  Update alignment formatting of Table 1", "journal-ref": null, "doi": "10.1145/3343031.3351028", "report-no": null, "categories": "cs.MM cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quality assessment of in-the-wild videos is a challenging problem because of\nthe absence of reference videos and shooting distortions. Knowledge of the\nhuman visual system can help establish methods for objective quality assessment\nof in-the-wild videos. In this work, we show two eminent effects of the human\nvisual system, namely, content-dependency and temporal-memory effects, could be\nused for this purpose. We propose an objective no-reference video quality\nassessment method by integrating both effects into a deep neural network. For\ncontent-dependency, we extract features from a pre-trained image classification\nneural network for its inherent content-aware property. For temporal-memory\neffects, long-term dependencies, especially the temporal hysteresis, are\nintegrated into the network with a gated recurrent unit and a\nsubjectively-inspired temporal pooling layer. To validate the performance of\nour method, experiments are conducted on three publicly available in-the-wild\nvideo quality assessment databases: KoNViD-1k, CVD2014, and LIVE-Qualcomm,\nrespectively. Experimental results demonstrate that our proposed method\noutperforms five state-of-the-art methods by a large margin, specifically,\n12.39%, 15.71%, 15.45%, and 18.09% overall performance improvements over the\nsecond-best method VBLIINDS, in terms of SROCC, KROCC, PLCC and RMSE,\nrespectively. Moreover, the ablation study verifies the crucial role of both\nthe content-aware features and the modeling of temporal-memory effects. The\nPyTorch implementation of our method is released at\nhttps://github.com/lidq92/VSFA.\n", "versions": [{"version": "v1", "created": "Thu, 1 Aug 2019 13:08:04 GMT"}, {"version": "v2", "created": "Fri, 2 Aug 2019 07:16:53 GMT"}, {"version": "v3", "created": "Sat, 5 Oct 2019 14:31:25 GMT"}], "update_date": "2019-10-08", "authors_parsed": [["Li", "Dingquan", ""], ["Jiang", "Tingting", ""], ["Jiang", "Ming", ""]]}, {"id": "1908.00382", "submitter": "Pingping Zhang Dr", "authors": "Pingping Zhang and Wei Liu and Yinjie Lei and Huchuan Lu and Xiaoyun\n  Yang", "title": "Cascaded Context Pyramid for Full-Resolution 3D Semantic Scene\n  Completion", "comments": "This work has been accepted as an Oral presentation at ICCV2019,\n  including 10 pages, 6 figures and 6 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semantic Scene Completion (SSC) aims to simultaneously predict the volumetric\noccupancy and semantic category of a 3D scene. It helps intelligent devices to\nunderstand and interact with the surrounding scenes. Due to the high-memory\nrequirement, current methods only produce low-resolution completion\npredictions, and generally lose the object details. Furthermore, they also\nignore the multi-scale spatial contexts, which play a vital role for the 3D\ninference. To address these issues, in this work we propose a novel deep\nlearning framework, named Cascaded Context Pyramid Network (CCPNet), to jointly\ninfer the occupancy and semantic labels of a volumetric 3D scene from a single\ndepth image. The proposed CCPNet improves the labeling coherence with a\ncascaded context pyramid. Meanwhile, based on the low-level features, it\nprogressively restores the fine-structures of objects with Guided Residual\nRefinement (GRR) modules. Our proposed framework has three outstanding\nadvantages: (1) it explicitly models the 3D spatial context for performance\nimprovement; (2) full-resolution 3D volumes are produced with\nstructure-preserving details; (3) light-weight models with low-memory\nrequirements are captured with a good extensibility. Extensive experiments\ndemonstrate that in spite of taking a single-view depth map, our proposed\nframework can generate high-quality SSC results, and outperforms\nstate-of-the-art approaches on both the synthetic SUNCG and real NYU datasets.\n", "versions": [{"version": "v1", "created": "Thu, 1 Aug 2019 13:27:41 GMT"}], "update_date": "2019-08-02", "authors_parsed": [["Zhang", "Pingping", ""], ["Liu", "Wei", ""], ["Lei", "Yinjie", ""], ["Lu", "Huchuan", ""], ["Yang", "Xiaoyun", ""]]}, {"id": "1908.00398", "submitter": "Minkesh Asati", "authors": "Asati Minkesh, Kraisittipong Worranitta, Miyachi Taizo", "title": "Extract and Merge: Merging extracted humans from different images\n  utilizing Mask R-CNN", "comments": "12 pages, 13 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Selecting human objects out of the various type of objects in images and\nmerging them with other scenes is manual and day-to-day work for photo editors.\nAlthough recently Adobe photoshop released \"select subject\" tool which\nautomatically selects the foreground object in an image, but still requires\nfine manual tweaking separately. In this work, we proposed an application\nutilizing Mask R-CNN (for object detection and mask segmentation) that can\nextract human instances from multiple images and merge them with a new\nbackground. This application does not add any overhead to Mask R-CNN, running\nat 5 frames per second. It can extract human instances from any number of\nimages or videos from merging them together. We also structured the code to\naccept videos of different lengths as input and length of the output-video will\nbe equal to the longest input-video. We wanted to create a simple yet effective\napplication that can serve as a base for photo editing and do most\ntime-consuming work automatically, so, editors can focus more on the design\npart. Other application could be to group people together in a single picture\nwith a new background from different images which could not be physically\ntogether. We are showing single-person and multi-person extraction and\nplacement in two different backgrounds. Also, we are showing a video example\nwith single-person extraction.\n", "versions": [{"version": "v1", "created": "Thu, 1 Aug 2019 13:50:00 GMT"}], "update_date": "2019-08-02", "authors_parsed": [["Minkesh", "Asati", ""], ["Worranitta", "Kraisittipong", ""], ["Taizo", "Miyachi", ""]]}, {"id": "1908.00407", "submitter": "Wenbin He", "authors": "Wenbin He, Junpeng Wang, Hanqi Guo, Ko-Chih Wang, Han-Wei Shen, Mukund\n  Raj, Youssef S. G. Nashed, Tom Peterka", "title": "InSituNet: Deep Image Synthesis for Parameter Space Exploration of\n  Ensemble Simulations", "comments": null, "journal-ref": null, "doi": "10.1109/TVCG.2019.2934312", "report-no": null, "categories": "eess.IV cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose InSituNet, a deep learning based surrogate model to support\nparameter space exploration for ensemble simulations that are visualized in\nsitu. In situ visualization, generating visualizations at simulation time, is\nbecoming prevalent in handling large-scale simulations because of the I/O and\nstorage constraints. However, in situ visualization approaches limit the\nflexibility of post-hoc exploration because the raw simulation data are no\nlonger available. Although multiple image-based approaches have been proposed\nto mitigate this limitation, those approaches lack the ability to explore the\nsimulation parameters. Our approach allows flexible exploration of parameter\nspace for large-scale ensemble simulations by taking advantage of the recent\nadvances in deep learning. Specifically, we design InSituNet as a convolutional\nregression model to learn the mapping from the simulation and visualization\nparameters to the visualization results. With the trained model, users can\ngenerate new images for different simulation parameters under various\nvisualization settings, which enables in-depth analysis of the underlying\nensemble simulations. We demonstrate the effectiveness of InSituNet in\ncombustion, cosmology, and ocean simulations through quantitative and\nqualitative evaluations.\n", "versions": [{"version": "v1", "created": "Thu, 1 Aug 2019 14:07:12 GMT"}, {"version": "v2", "created": "Sun, 18 Aug 2019 20:38:34 GMT"}, {"version": "v3", "created": "Wed, 16 Oct 2019 20:04:57 GMT"}], "update_date": "2019-10-18", "authors_parsed": [["He", "Wenbin", ""], ["Wang", "Junpeng", ""], ["Guo", "Hanqi", ""], ["Wang", "Ko-Chih", ""], ["Shen", "Han-Wei", ""], ["Raj", "Mukund", ""], ["Nashed", "Youssef S. G.", ""], ["Peterka", "Tom", ""]]}, {"id": "1908.00433", "submitter": "Tatiana Malygina", "authors": "Tatiana Malygina, Elena Ericheva, Ivan Drokin", "title": "GANs 'N Lungs: improving pneumonia prediction", "comments": "Accepted as an extended abstract for MIDL 2019 [arXiv:1907.08612]", "journal-ref": null, "doi": null, "report-no": "MIDL/2019/ExtendedAbstract/rkexLAH0FE", "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  We propose a novel method to improve deep learning model performance on\nhighly-imbalanced tasks. The proposed method is based on CycleGAN to achieve\nbalanced dataset. We show that data augmentation with GAN helps to improve\naccuracy of pneumonia binary classification task even if the generative network\nwas trained on the same training dataset.\n", "versions": [{"version": "v1", "created": "Thu, 1 Aug 2019 14:29:57 GMT"}], "update_date": "2019-08-02", "authors_parsed": [["Malygina", "Tatiana", ""], ["Ericheva", "Elena", ""], ["Drokin", "Ivan", ""]]}, {"id": "1908.00439", "submitter": "Valentin Gabeur", "authors": "Valentin Gabeur, Jean-Sebastien Franco, Xavier Martin, Cordelia\n  Schmid, Gregory Rogez", "title": "Moulding Humans: Non-parametric 3D Human Shape Estimation from Single\n  Images", "comments": "Accepted at ICCV 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we tackle the problem of 3D human shape estimation from single\nRGB images. While the recent progress in convolutional neural networks has\nallowed impressive results for 3D human pose estimation, estimating the full 3D\nshape of a person is still an open issue. Model-based approaches can output\nprecise meshes of naked under-cloth human bodies but fail to estimate details\nand un-modelled elements such as hair or clothing. On the other hand,\nnon-parametric volumetric approaches can potentially estimate complete shapes\nbut, in practice, they are limited by the resolution of the output grid and\ncannot produce detailed estimates. In this work, we propose a non-parametric\napproach that employs a double depth map to represent the 3D shape of a person:\na visible depth map and a \"hidden\" depth map are estimated and combined, to\nreconstruct the human 3D shape as done with a \"mould\". This representation\nthrough 2D depth maps allows a higher resolution output with a much lower\ndimension than voxel-based volumetric representations. Additionally, our fully\nderivable depth-based model allows us to efficiently incorporate a\ndiscriminator in an adversarial fashion to improve the accuracy and \"humanness\"\nof the 3D output. We train and quantitatively validate our approach on SURREAL\nand on 3D-HUMANS, a new photorealistic dataset made of semi-synthetic in-house\nvideos annotated with 3D ground truth surfaces.\n", "versions": [{"version": "v1", "created": "Thu, 1 Aug 2019 14:41:28 GMT"}], "update_date": "2019-08-02", "authors_parsed": [["Gabeur", "Valentin", ""], ["Franco", "Jean-Sebastien", ""], ["Martin", "Xavier", ""], ["Schmid", "Cordelia", ""], ["Rogez", "Gregory", ""]]}, {"id": "1908.00448", "submitter": "Nicolas Paul Marchal", "authors": "Nicolas Marchal, Charlotte Moraldo, Roland Siegwart, Hermann Blum,\n  Cesar Cadena, Abel Gawel", "title": "Learning Densities in Feature Space for Reliable Segmentation of Indoor\n  Scenes", "comments": "Preprint version after acceptance of publication in the IEEE robotics\n  and automation letters", "journal-ref": "IEEE Robotics and Automation Letters, vol. 5, no. 2, pp.\n  1032-1038, April 2020", "doi": "10.1109/LRA.2020.2967313", "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning has enabled remarkable advances in scene understanding,\nparticularly in semantic segmentation tasks. Yet, current state of the art\napproaches are limited to a closed set of classes, and fail when facing novel\nelements, also known as out of distribution (OoD) data. This is a problem as\nautonomous agents will inevitably come across a wide range of objects, all of\nwhich cannot be included during training. We propose a novel method to\ndistinguish any object (foreground) from empty building structure (background)\nin indoor environments. We use normalizing flow to estimate the probability\ndistribution of high-dimensional background descriptors. Foreground objects are\ntherefore detected as areas in an image for which the descriptors are unlikely\ngiven the background distribution. As our method does not explicitly learn the\nrepresentation of individual objects, its performance generalizes well outside\nof the training examples. Our model results in an innovative solution to\nreliably segment foreground from background in indoor scenes, which opens the\nway to a safer deployment of robots in human environments.\n", "versions": [{"version": "v1", "created": "Thu, 1 Aug 2019 15:03:05 GMT"}, {"version": "v2", "created": "Mon, 23 Sep 2019 23:12:30 GMT"}, {"version": "v3", "created": "Tue, 7 Jan 2020 09:14:35 GMT"}, {"version": "v4", "created": "Mon, 13 Jan 2020 21:46:06 GMT"}], "update_date": "2020-06-02", "authors_parsed": [["Marchal", "Nicolas", ""], ["Moraldo", "Charlotte", ""], ["Siegwart", "Roland", ""], ["Blum", "Hermann", ""], ["Cadena", "Cesar", ""], ["Gawel", "Abel", ""]]}, {"id": "1908.00463", "submitter": "Igor Vasiljevic", "authors": "Igor Vasiljevic, Nick Kolkin, Shanyi Zhang, Ruotian Luo, Haochen Wang,\n  Falcon Z. Dai, Andrea F. Daniele, Mohammadreza Mostajabi, Steven Basart,\n  Matthew R. Walter, Gregory Shakhnarovich", "title": "DIODE: A Dense Indoor and Outdoor DEpth Dataset", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce DIODE, a dataset that contains thousands of diverse high\nresolution color images with accurate, dense, long-range depth measurements.\nDIODE (Dense Indoor/Outdoor DEpth) is the first public dataset to include RGBD\nimages of indoor and outdoor scenes obtained with one sensor suite. This is in\ncontrast to existing datasets that focus on just one domain/scene type and\nemploy different sensors, making generalization across domains difficult. The\ndataset is available for download at http://diode-dataset.org\n", "versions": [{"version": "v1", "created": "Thu, 1 Aug 2019 15:39:54 GMT"}, {"version": "v2", "created": "Thu, 29 Aug 2019 04:17:49 GMT"}], "update_date": "2019-08-30", "authors_parsed": [["Vasiljevic", "Igor", ""], ["Kolkin", "Nick", ""], ["Zhang", "Shanyi", ""], ["Luo", "Ruotian", ""], ["Wang", "Haochen", ""], ["Dai", "Falcon Z.", ""], ["Daniele", "Andrea F.", ""], ["Mostajabi", "Mohammadreza", ""], ["Basart", "Steven", ""], ["Walter", "Matthew R.", ""], ["Shakhnarovich", "Gregory", ""]]}, {"id": "1908.00473", "submitter": "Pengyi Zhang", "authors": "Pengyi Zhang, Yunxin Zhong, Yulin Deng, Xiaoying Tang, Xiaoqiong Li", "title": "A Survey on Deep Learning of Small Sample in Biomedical Image Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The success of deep learning has been witnessed as a promising technique for\ncomputer-aided biomedical image analysis, due to end-to-end learning framework\nand availability of large-scale labelled samples. However, in many cases of\nbiomedical image analysis, deep learning techniques suffer from the small\nsample learning (SSL) dilemma caused mainly by lack of annotations. To be more\npractical for biomedical image analysis, in this paper we survey the key SSL\ntechniques that help relieve the suffering of deep learning by combining with\nthe development of related techniques in computer vision applications. In order\nto accelerate the clinical usage of biomedical image analysis based on deep\nlearning techniques, we intentionally expand this survey to include the\nexplanation methods for deep models that are important to clinical decision\nmaking. We survey the key SSL techniques by dividing them into five categories:\n(1) explanation techniques, (2) weakly supervised learning techniques, (3)\ntransfer learning techniques, (4) active learning techniques, and (5)\nmiscellaneous techniques involving data augmentation, domain knowledge,\ntraditional shallow methods and attention mechanism. These key techniques are\nexpected to effectively support the application of deep learning in clinical\nbiomedical image analysis, and furtherly improve the analysis performance,\nespecially when large-scale annotated samples are not available. We bulid demos\nat https://github.com/PengyiZhang/MIADeepSSL.\n", "versions": [{"version": "v1", "created": "Thu, 1 Aug 2019 16:01:31 GMT"}], "update_date": "2019-08-02", "authors_parsed": [["Zhang", "Pengyi", ""], ["Zhong", "Yunxin", ""], ["Deng", "Yulin", ""], ["Tang", "Xiaoying", ""], ["Li", "Xiaoqiong", ""]]}, {"id": "1908.00478", "submitter": "Hungyueh Chiang", "authors": "Hung-Yueh Chiang, Yen-Liang Lin, Yueh-Cheng Liu, Winston H. Hsu", "title": "A Unified Point-Based Framework for 3D Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  3D point cloud segmentation remains challenging for structureless and\ntextureless regions. We present a new unified point-based framework for 3D\npoint cloud segmentation that effectively optimizes pixel-level features,\ngeometrical structures and global context priors of an entire scene. By\nback-projecting 2D image features into 3D coordinates, our network learns 2D\ntextural appearance and 3D structural features in a unified framework. In\naddition, we investigate a global context prior to obtain a better prediction.\nWe evaluate our framework on ScanNet online benchmark and show that our method\noutperforms several state-of-the-art approaches. We explore synthesizing camera\nposes in 3D reconstructed scenes for achieving higher performance. In-depth\nanalysis on feature combinations and synthetic camera pose verify that features\nfrom different modalities benefit each other and dense camera pose sampling\nfurther improves the segmentation results.\n", "versions": [{"version": "v1", "created": "Thu, 1 Aug 2019 16:09:59 GMT"}, {"version": "v2", "created": "Sat, 3 Aug 2019 14:42:33 GMT"}, {"version": "v3", "created": "Wed, 7 Aug 2019 10:11:52 GMT"}, {"version": "v4", "created": "Sun, 18 Aug 2019 06:13:37 GMT"}], "update_date": "2019-08-20", "authors_parsed": [["Chiang", "Hung-Yueh", ""], ["Lin", "Yen-Liang", ""], ["Liu", "Yueh-Cheng", ""], ["Hsu", "Winston H.", ""]]}, {"id": "1908.00485", "submitter": "Zhun Zhong", "authors": "Zhun Zhong, Liang Zheng, Zhiming Luo, Shaozi Li and Yi Yang", "title": "Learning to Adapt Invariance in Memory for Person Re-identification", "comments": "Extension of conference version: arXiv:1904.01990", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work considers the problem of unsupervised domain adaptation in person\nre-identification (re-ID), which aims to transfer knowledge from the source\ndomain to the target domain. Existing methods are primary to reduce the\ninter-domain shift between the domains, which however usually overlook the\nrelations among target samples. This paper investigates into the intra-domain\nvariations of the target domain and proposes a novel adaptation framework\nw.r.t. three types of underlying invariance, i.e., Exemplar-Invariance,\nCamera-Invariance, and Neighborhood-Invariance. Specifically, an exemplar\nmemory is introduced to store features of samples, which can effectively and\nefficiently enforce the invariance constraints over the global dataset. We\nfurther present the Graph-based Positive Prediction (GPP) method to explore\nreliable neighbors for the target domain, which is built upon the memory and is\ntrained on the source samples. Experiments demonstrate that 1) the three\ninvariance properties are indispensable for effective domain adaptation, 2) the\nmemory plays a key role in implementing invariance learning and improves the\nperformance with limited extra computation cost, 3) GPP could facilitate the\ninvariance learning and thus significantly improves the results, and 4) our\napproach produces new state-of-the-art adaptation accuracy on three re-ID\nlarge-scale benchmarks.\n", "versions": [{"version": "v1", "created": "Thu, 1 Aug 2019 16:20:16 GMT"}], "update_date": "2019-08-02", "authors_parsed": [["Zhong", "Zhun", ""], ["Zheng", "Liang", ""], ["Luo", "Zhiming", ""], ["Li", "Shaozi", ""], ["Yang", "Yi", ""]]}, {"id": "1908.00497", "submitter": "Yadong Mu", "authors": "Lu Chi and Guiyu Tian and Yadong Mu and Qi Tian", "title": "Two-Stream Video Classification with Cross-Modality Attention", "comments": "12 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fusing multi-modality information is known to be able to effectively bring\nsignificant improvement in video classification. However, the most popular\nmethod up to now is still simply fusing each stream's prediction scores at the\nlast stage. A valid question is whether there exists a more effective method to\nfuse information cross modality. With the development of attention mechanism in\nnatural language processing, there emerge many successful applications of\nattention in the field of computer vision. In this paper, we propose a\ncross-modality attention operation, which can obtain information from other\nmodality in a more effective way than two-stream. Correspondingly we implement\na compatible block named CMA block, which is a wrapper of our proposed\nattention operation. CMA can be plugged into many existing architectures. In\nthe experiments, we comprehensively compare our method with two-stream and\nnon-local models widely used in video classification. All experiments clearly\ndemonstrate strong performance superiority by our proposed method. We also\nanalyze the advantages of the CMA block by visualizing the attention map, which\nintuitively shows how the block helps the final prediction.\n", "versions": [{"version": "v1", "created": "Thu, 1 Aug 2019 16:46:42 GMT"}], "update_date": "2019-08-02", "authors_parsed": [["Chi", "Lu", ""], ["Tian", "Guiyu", ""], ["Mu", "Yadong", ""], ["Tian", "Qi", ""]]}, {"id": "1908.00524", "submitter": "Michelle Valente", "authors": "Michelle Valente, Cyril Joly and Arnaud de La Fortelle", "title": "Deep Sensor Fusion for Real-Time Odometry Estimation", "comments": "arXiv admin note: substantial text overlap with arXiv:1902.08536", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cameras and 2D laser scanners, in combination, are able to provide low-cost,\nlight-weight and accurate solutions, which make their fusion well-suited for\nmany robot navigation tasks. However, correct data fusion depends on precise\ncalibration of the rigid body transform between the sensors. In this paper we\npresent the first framework that makes use of Convolutional Neural Networks\n(CNNs) for odometry estimation fusing 2D laser scanners and mono-cameras. The\nuse of CNNs provides the tools to not only extract the features from the two\nsensors, but also to fuse and match them without needing a calibration between\nthe sensors. We transform the odometry estimation into an ordinal\nclassification problem in order to find accurate rotation and translation\nvalues between consecutive frames. Results on a real road dataset show that the\nfusion network runs in real-time and is able to improve the odometry estimation\nof a single sensor alone by learning how to fuse two different types of data\ninformation.\n", "versions": [{"version": "v1", "created": "Wed, 31 Jul 2019 15:29:15 GMT"}], "update_date": "2019-08-02", "authors_parsed": [["Valente", "Michelle", ""], ["Joly", "Cyril", ""], ["de La Fortelle", "Arnaud", ""]]}, {"id": "1908.00575", "submitter": "Kaichun Mo", "authors": "Kaichun Mo, Paul Guerrero, Li Yi, Hao Su, Peter Wonka, Niloy Mitra,\n  Leonidas J. Guibas", "title": "StructureNet: Hierarchical Graph Networks for 3D Shape Generation", "comments": "Conditionally Accepted to Siggraph Asia 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.CG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ability to generate novel, diverse, and realistic 3D shapes along with\nassociated part semantics and structure is central to many applications\nrequiring high-quality 3D assets or large volumes of realistic training data. A\nkey challenge towards this goal is how to accommodate diverse shape variations,\nincluding both continuous deformations of parts as well as structural or\ndiscrete alterations which add to, remove from, or modify the shape\nconstituents and compositional structure. Such object structure can typically\nbe organized into a hierarchy of constituent object parts and relationships,\nrepresented as a hierarchy of n-ary graphs. We introduce StructureNet, a\nhierarchical graph network which (i) can directly encode shapes represented as\nsuch n-ary graphs; (ii) can be robustly trained on large and complex shape\nfamilies; and (iii) can be used to generate a great diversity of realistic\nstructured shape geometries. Technically, we accomplish this by drawing\ninspiration from recent advances in graph neural networks to propose an\norder-invariant encoding of n-ary graphs, considering jointly both part\ngeometry and inter-part relations during network training. We extensively\nevaluate the quality of the learned latent spaces for various shape families\nand show significant advantages over baseline and competing methods. The\nlearned latent spaces enable several structure-aware geometry processing\napplications, including shape generation and interpolation, shape editing, or\nshape structure discovery directly from un-annotated images, point clouds, or\npartial scans.\n", "versions": [{"version": "v1", "created": "Thu, 1 Aug 2019 18:43:24 GMT"}], "update_date": "2019-08-05", "authors_parsed": [["Mo", "Kaichun", ""], ["Guerrero", "Paul", ""], ["Yi", "Li", ""], ["Su", "Hao", ""], ["Wonka", "Peter", ""], ["Mitra", "Niloy", ""], ["Guibas", "Leonidas J.", ""]]}, {"id": "1908.00615", "submitter": "Thibault F\\'evry", "authors": "Thibault F\\'evry, Jason Phang, Nan Wu, S. Gene Kim, Linda Moy,\n  Kyunghyun Cho, Krzysztof J. Geras", "title": "Improving localization-based approaches for breast cancer screening exam\n  classification", "comments": "MIDL 2019 [arXiv:1907.08612]", "journal-ref": null, "doi": null, "report-no": "MIDL/2019/ExtendedAbstract/HyxoAR_AK4", "categories": "eess.IV cs.CV stat.ML", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  We trained and evaluated a localization-based deep CNN for breast cancer\nscreening exam classification on over 200,000 exams (over 1,000,000 images).\nOur model achieves an AUC of 0.919 in predicting malignancy in patients\nundergoing breast cancer screening, reducing the error rate of the baseline (Wu\net al., 2019a) by 23%. In addition, the models generates bounding boxes for\nbenign and malignant findings, providing interpretable predictions.\n", "versions": [{"version": "v1", "created": "Thu, 1 Aug 2019 20:34:23 GMT"}], "update_date": "2019-08-05", "authors_parsed": [["F\u00e9vry", "Thibault", ""], ["Phang", "Jason", ""], ["Wu", "Nan", ""], ["Kim", "S. Gene", ""], ["Moy", "Linda", ""], ["Cho", "Kyunghyun", ""], ["Geras", "Krzysztof J.", ""]]}, {"id": "1908.00620", "submitter": "Christopher Metzler", "authors": "Christopher A. Metzler, Hayato Ikoma, Yifan Peng, Gordon Wetzstein", "title": "Deep Optics for Single-shot High-dynamic-range Imaging", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High-dynamic-range (HDR) imaging is crucial for many computer graphics and\nvision applications. Yet, acquiring HDR images with a single shot remains a\nchallenging problem. Whereas modern deep learning approaches are successful at\nhallucinating plausible HDR content from a single low-dynamic-range (LDR)\nimage, saturated scene details often cannot be faithfully recovered. Inspired\nby recent deep optical imaging approaches, we interpret this problem as jointly\ntraining an optical encoder and electronic decoder where the encoder is\nparameterized by the point spread function (PSF) of the lens, the bottleneck is\nthe sensor with a limited dynamic range, and the decoder is a convolutional\nneural network (CNN). The lens surface is then jointly optimized with the CNN\nin a training phase; we fabricate this optimized optical element and attach it\nas a hardware add-on to a conventional camera during inference. In extensive\nsimulations and with a physical prototype, we demonstrate that this end-to-end\ndeep optical imaging approach to single-shot HDR imaging outperforms both\npurely CNN-based approaches and other PSF engineering approaches.\n", "versions": [{"version": "v1", "created": "Thu, 1 Aug 2019 20:57:05 GMT"}], "update_date": "2019-08-05", "authors_parsed": [["Metzler", "Christopher A.", ""], ["Ikoma", "Hayato", ""], ["Peng", "Yifan", ""], ["Wetzstein", "Gordon", ""]]}, {"id": "1908.00634", "submitter": "Hanen Akouaydi", "authors": "Hanen Akouaydi, Sourour Njah, Wael Ouarda, Anis Samet, Thameur Dhieb,\n  Mourad Zaied and Adel M. Alimi", "title": "Neural Architecture based on Fuzzy Perceptual Representation For Online\n  Multilingual Handwriting Recognition", "comments": "15 pages; 17 figures", "journal-ref": "IEEE Transactions on Pattern Analysis and Machine Intelligence,\n  2019", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to the omnipresence of mobile devices, online handwritten scripts have\nbecome the most important feeding input to smartphones and tablet devices. To\nincrease online handwriting recognition performance, deeper neural networks\nhave extensively been used. In this context, our paper handles the problem of\nonline handwritten script recognition based on extraction features system and\ndeep approach system for sequences classification. Many solutions have appeared\nin order to facilitate the recognition of handwriting. Accordingly, we used an\nexistent method and combined with new classifiers in order to get a flexible\nsystem. Good results are achieved compared to online characters and words\nrecognition system on Latin and Arabic scripts. The performance of our two\nproposed systems is assessed by using five databases. Indeed, the recognition\nrate exceeds 98%.\n", "versions": [{"version": "v1", "created": "Thu, 1 Aug 2019 21:19:01 GMT"}], "update_date": "2019-08-05", "authors_parsed": [["Akouaydi", "Hanen", ""], ["Njah", "Sourour", ""], ["Ouarda", "Wael", ""], ["Samet", "Anis", ""], ["Dhieb", "Thameur", ""], ["Zaied", "Mourad", ""], ["Alimi", "Adel M.", ""]]}, {"id": "1908.00646", "submitter": "Estelle Massart", "authors": "Benjamin Szczapa, Mohamed Daoudi, Stefano Berretti, Alberto Del Bimbo,\n  Pietro Pala and Estelle Massart", "title": "Fitting, Comparison, and Alignment of Trajectories on Positive\n  Semi-Definite Matrices with Application to Action Recognition", "comments": "Updated version of the paper published in the workshop HBU2019. The\n  differences with the published version are a few small corrections, mainly\n  misleading notations for the distance function on p. 4, and missing square\n  root in the expression for \"d\", in the Thm. on p. 4. Noticeable changes w. r.\n  t. v1 and v2 on arxiv, please use this version instead", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we tackle the problem of action recognition using body\nskeletons extracted from video sequences. Our approach lies in the continuity\nof recent works representing video frames by Gramian matrices that describe a\ntrajectory on the Riemannian manifold of positive-semidefinite matrices of\nfixed rank. In comparison with previous works, the manifold of fixed-rank\npositive-semidefinite matrices is here endowed with a different metric, and we\nresort to different algorithms for the curve fitting and temporal alignment\nsteps. We evaluated our approach on three publicly available datasets\n(UTKinect-Action3D, KTH-Action and UAV-Gesture). The results of the proposed\napproach are competitive with respect to state-of-the-art methods, while only\ninvolving body skeletons.\n", "versions": [{"version": "v1", "created": "Thu, 1 Aug 2019 22:25:24 GMT"}, {"version": "v2", "created": "Mon, 5 Aug 2019 07:58:45 GMT"}, {"version": "v3", "created": "Mon, 9 Sep 2019 15:39:59 GMT"}], "update_date": "2019-09-10", "authors_parsed": [["Szczapa", "Benjamin", ""], ["Daoudi", "Mohamed", ""], ["Berretti", "Stefano", ""], ["Del Bimbo", "Alberto", ""], ["Pala", "Pietro", ""], ["Massart", "Estelle", ""]]}, {"id": "1908.00656", "submitter": "Alan McMillan", "authors": "Zheng Liu, Jinnian Zhang, Varun Jog, Po-Ling Loh, Alan B McMillan", "title": "Robustifying deep networks for image segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Purpose: The purpose of this study is to investigate the robustness of a\ncommonly-used convolutional neural network for image segmentation with respect\nto visually-subtle adversarial perturbations, and suggest new methods to make\nthese networks more robust to such perturbations. Materials and Methods: In\nthis retrospective study, the accuracy of brain tumor segmentation was studied\nin subjects with low- and high-grade gliomas. A three-dimensional UNet model\nwas implemented to segment four different MR series (T1-weighted, post-contrast\nT1-weighted, T2- weighted, and T2-weighted FLAIR) into four pixelwise labels\n(Gd-enhancing tumor, peritumoral edema, necrotic and non-enhancing tumor, and\nbackground). We developed attack strategies based on the Fast Gradient Sign\nMethod (FGSM), iterative FGSM (i-FGSM), and targeted iterative FGSM (ti-FGSM)\nto produce effective attacks. Additionally, we explored the effectiveness of\ndistillation and adversarial training via data augmentation to counteract\nadversarial attacks. Robustness was measured by comparing the Dice coefficient\nfor each attack method using Wilcoxon signed-rank tests. Results: Attacks based\non FGSM, i-FGSM, and ti-FGSM were effective in significantly reducing the\nquality of image segmentation with reductions in Dice coefficient by up to 65%.\nFor attack defenses, distillation performed significantly better than\nadversarial training approaches. However, all defense approaches performed\nworse compared to unperturbed test images. Conclusion: Segmentation networks\ncan be adversely affected by targeted attacks that introduce visually minor\n(and potentially undetectable) modifications to existing images. With an\nincreasing interest in applying deep learning techniques to medical imaging\ndata, it is important to quantify the ramifications of adversarial inputs\n(either intentional or unintentional).\n", "versions": [{"version": "v1", "created": "Thu, 1 Aug 2019 23:05:19 GMT"}], "update_date": "2019-08-05", "authors_parsed": [["Liu", "Zheng", ""], ["Zhang", "Jinnian", ""], ["Jog", "Varun", ""], ["Loh", "Po-Ling", ""], ["McMillan", "Alan B", ""]]}, {"id": "1908.00669", "submitter": "Zhangwei Yang", "authors": "Alex Yang, Charlie T. Veal, Derek T. Anderson, Grant J. Scott", "title": "Recognizing Image Objects by Relational Analysis Using Heterogeneous\n  Superpixels and Deep Convolutional Features", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Superpixel-based methodologies have become increasingly popular in computer\nvision, especially when the computation is too expensive in time or memory to\nperform with a large number of pixels or features. However, rarely is\nsuperpixel segmentation examined within the context of deep convolutional\nneural network architectures. This paper presents a novel neural architecture\nthat exploits the superpixel feature space. The visual feature space is\norganized using superpixels to provide the neural network with a substructure\nof the images. As the superpixels associate the visual feature space with parts\nof the objects in an image, the visual feature space is transformed into a\nstructured vector representation per superpixel. It is shown that it is\nfeasible to learn superpixel features using capsules and it is potentially\nbeneficial to perform image analysis in such a structured manner. This novel\ndeep learning architecture is examined in the context of an image\nclassification task, highlighting explicit interpretability (explainability) of\nthe network's decision making. The results are compared against a baseline deep\nneural model, as well as among superpixel capsule networks with a variety of\nhyperparameter settings.\n", "versions": [{"version": "v1", "created": "Fri, 2 Aug 2019 00:40:27 GMT"}], "update_date": "2019-08-05", "authors_parsed": [["Yang", "Alex", ""], ["Veal", "Charlie T.", ""], ["Anderson", "Derek T.", ""], ["Scott", "Grant J.", ""]]}, {"id": "1908.00672", "submitter": "Chunhua Shen", "authors": "Hao Lu, Yutong Dai, Chunhua Shen, Songcen Xu", "title": "Indices Matter: Learning to Index for Deep Image Matting", "comments": "Accepted to Proc. Int. Conf. Computer Vision 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We show that existing upsampling operators can be unified with the notion of\nthe index function. This notion is inspired by an observation in the decoding\nprocess of deep image matting where indices-guided unpooling can recover\nboundary details much better than other upsampling operators such as bilinear\ninterpolation. By looking at the indices as a function of the feature map, we\nintroduce the concept of learning to index, and present a novel index-guided\nencoder-decoder framework where indices are self-learned adaptively from data\nand are used to guide the pooling and upsampling operators, without the need of\nsupervision. At the core of this framework is a flexible network module, termed\nIndexNet, which dynamically predicts indices given an input. Due to its\nflexibility, IndexNet can be used as a plug-in applying to any off-the-shelf\nconvolutional networks that have coupled downsampling and upsampling stages.\n  We demonstrate the effectiveness of IndexNet on the task of natural image\nmatting where the quality of learned indices can be visually observed from\npredicted alpha mattes. Results on the Composition-1k matting dataset show that\nour model built on MobileNetv2 exhibits at least $16.1\\%$ improvement over the\nseminal VGG-16 based deep matting baseline, with less training data and lower\nmodel capacity. Code and models has been made available at:\nhttps://tinyurl.com/IndexNetV1\n", "versions": [{"version": "v1", "created": "Fri, 2 Aug 2019 01:10:42 GMT"}], "update_date": "2019-08-05", "authors_parsed": [["Lu", "Hao", ""], ["Dai", "Yutong", ""], ["Shen", "Chunhua", ""], ["Xu", "Songcen", ""]]}, {"id": "1908.00682", "submitter": "Feifan Lv", "authors": "Feifan Lv, Yu Li and Feng Lu", "title": "Attention Guided Low-light Image Enhancement with a Large Scale\n  Low-light Simulation Dataset", "comments": "18 pages, 16 figures, 5 tables, supplementary materials and project\n  page: http://phi-ai.org/project/AgLLNet/default.htm", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Low-light image enhancement is challenging in that it needs to consider not\nonly brightness recovery but also complex issues like color distortion and\nnoise, which usually hide in the dark. Simply adjusting the brightness of a\nlow-light image will inevitably amplify those artifacts. To address this\ndifficult problem, this paper proposes a novel end-to-end attention-guided\nmethod based on multi-branch convolutional neural network. To this end, we\nfirst construct a synthetic dataset with carefully designed low-light\nsimulation strategies. The dataset is much larger and more diverse than\nexisting ones. With the new dataset for training, our method learns two\nattention maps to guide the brightness enhancement and denoising tasks\nrespectively. The first attention map distinguishes underexposed regions from\nwell lit regions, and the second attention map distinguishes noises from real\ntextures. With their guidance, the proposed multi-branch\ndecomposition-and-fusion enhancement network works in an input adaptive way.\nMoreover, a reinforcement-net further enhances color and contrast of the output\nimage. Extensive experiments on multiple datasets demonstrate that our method\ncan produce high fidelity enhancement results for low-light images and\noutperforms the current state-of-the-art methods by a large margin both\nquantitatively and visually.\n", "versions": [{"version": "v1", "created": "Fri, 2 Aug 2019 02:28:00 GMT"}, {"version": "v2", "created": "Tue, 19 Nov 2019 05:44:30 GMT"}, {"version": "v3", "created": "Sun, 15 Mar 2020 03:09:03 GMT"}], "update_date": "2020-03-17", "authors_parsed": [["Lv", "Feifan", ""], ["Li", "Yu", ""], ["Lu", "Feng", ""]]}, {"id": "1908.00692", "submitter": "Tao Hu", "authors": "Tao Hu, Lichao Huang, Xianming Liu, Han Shen", "title": "Real Time Visual Tracking using Spatial-Aware Temporal Aggregation\n  Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  More powerful feature representations derived from deep neural networks\nbenefit visual tracking algorithms widely. However, the lack of exploitation on\ntemporal information prevents tracking algorithms from adapting to appearances\nchanging or resisting to drift. This paper proposes a correlation filter based\ntracking method which aggregates historical features in a spatial-aligned and\nscale-aware paradigm. The features of historical frames are sampled and\naggregated to search frame according to a pixel-level alignment module based on\ndeformable convolutions. In addition, we also use a feature pyramid structure\nto handle motion estimation at different scales, and address the different\ndemands on feature granularity between tracking losses and deformation offset\nlearning. By this design, the tracker, named as Spatial-Aware Temporal\nAggregation network (SATA), is able to assemble appearances and motion contexts\nof various scales in a time period, resulting in better performance compared to\na single static image. Our tracker achieves leading performance in OTB2013,\nOTB2015, VOT2015, VOT2016 and LaSOT, and operates at a real-time speed of 26\nFPS, which indicates our method is effective and practical. Our code will be\nmade publicly available at\n\\href{https://github.com/ecart18/SATA}{https://github.com/ecart18/SATA}.\n", "versions": [{"version": "v1", "created": "Fri, 2 Aug 2019 03:22:58 GMT"}], "update_date": "2019-08-05", "authors_parsed": [["Hu", "Tao", ""], ["Huang", "Lichao", ""], ["Liu", "Xianming", ""], ["Shen", "Han", ""]]}, {"id": "1908.00704", "submitter": "Alireza Naghizadeh", "authors": "Alireza Naghizadeh and Mohammadsajad Abavisani and Dimitris N. Metaxas", "title": "Greedy AutoAugment", "comments": "Pattern Recognition Letters (2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A major problem in data augmentation is to ensure that the generated new\nsamples cover the search space. This is a challenging problem and requires\nexploration for data augmentation policies to ensure their effectiveness in\ncovering the search space. In this paper, we propose Greedy AutoAugment as a\nhighly efficient search algorithm to find the best augmentation policies. We\nuse a greedy approach to reduce the exponential growth of the number of\npossible trials to linear growth. The Greedy Search also helps us to lead the\nsearch towards the sub-policies with better results, which eventually helps to\nincrease the accuracy. The proposed method can be used as a reliable addition\nto the current artifitial neural networks. Our experiments on four datasets\n(Tiny ImageNet, CIFAR-10, CIFAR-100, and SVHN) show that Greedy AutoAugment\nprovides better accuracy, while using 360 times fewer computational resources.\n", "versions": [{"version": "v1", "created": "Fri, 2 Aug 2019 05:28:03 GMT"}, {"version": "v2", "created": "Wed, 7 Oct 2020 01:34:48 GMT"}], "update_date": "2020-10-08", "authors_parsed": [["Naghizadeh", "Alireza", ""], ["Abavisani", "Mohammadsajad", ""], ["Metaxas", "Dimitris N.", ""]]}, {"id": "1908.00706", "submitter": "Puneet Mangla", "authors": "Puneet Mangla, Surgan Jandial, Sakshi Varshney, Vineeth N\n  Balasubramanian", "title": "AdvGAN++ : Harnessing latent layers for adversary generation", "comments": "Accepted at Neural Architects Workshop, ICCV 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adversarial examples are fabricated examples, indistinguishable from the\noriginal image that mislead neural networks and drastically lower their\nperformance. Recently proposed AdvGAN, a GAN based approach, takes input image\nas a prior for generating adversaries to target a model. In this work, we show\nhow latent features can serve as better priors than input images for adversary\ngeneration by proposing AdvGAN++, a version of AdvGAN that achieves higher\nattack rates than AdvGAN and at the same time generates perceptually realistic\nimages on MNIST and CIFAR-10 datasets.\n", "versions": [{"version": "v1", "created": "Fri, 2 Aug 2019 05:37:03 GMT"}, {"version": "v2", "created": "Mon, 23 Dec 2019 19:31:19 GMT"}], "update_date": "2019-12-25", "authors_parsed": [["Mangla", "Puneet", ""], ["Jandial", "Surgan", ""], ["Varshney", "Sakshi", ""], ["Balasubramanian", "Vineeth N", ""]]}, {"id": "1908.00707", "submitter": "Yadong Mu", "authors": "Guoqiang Gong and Liangfeng Zheng and Kun Bai and Yadong Mu", "title": "Scale Matters: Temporal Scale Aggregation Network for Precise Action\n  Localization in Untrimmed Videos", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Temporal action localization is a recently-emerging task, aiming to localize\nvideo segments from untrimmed videos that contain specific actions. Despite the\nremarkable recent progress, most two-stage action localization methods still\nsuffer from imprecise temporal boundaries of action proposals. This work\nproposes a novel integrated temporal scale aggregation network (TSA-Net). Our\nmain insight is that ensembling convolution filters with different dilation\nrates can effectively enlarge the receptive field with low computational cost,\nwhich inspires us to devise multi-dilation temporal convolution (MDC) block.\nFurthermore, to tackle video action instances with different durations, TSA-Net\nconsists of multiple branches of sub-networks. Each of them adopts stacked MDC\nblocks with different dilation parameters, accomplishing a temporal receptive\nfield specially optimized for specific-duration actions. We follow the\nformulation of boundary point detection, novelly detecting three kinds of\ncritical points (ie, starting / mid-point / ending) and pairing them for\nproposal generation. Comprehensive evaluations are conducted on two challenging\nvideo benchmarks, THUMOS14 and ActivityNet-1.3. Our proposed TSA-Net\ndemonstrates clear and consistent better performances and re-calibrates new\nstate-of-the-art on both benchmarks. For example, our new record on THUMOS14 is\n46.9% while the previous best is 42.8% under mAP@0.5.\n", "versions": [{"version": "v1", "created": "Fri, 2 Aug 2019 05:49:37 GMT"}], "update_date": "2019-08-05", "authors_parsed": [["Gong", "Guoqiang", ""], ["Zheng", "Liangfeng", ""], ["Bai", "Kun", ""], ["Mu", "Yadong", ""]]}, {"id": "1908.00709", "submitter": "Xin He", "authors": "Xin He, Kaiyong Zhao, Xiaowen Chu", "title": "AutoML: A Survey of the State-of-the-Art", "comments": "automated machine learning (AutoML), published in journal of\n  Knowledge-Based Systems", "journal-ref": "Knowledge-Based Systems, Volume 212, 5 January 2021, 106622", "doi": "10.1016/j.knosys.2020.106622", "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Deep learning (DL) techniques have penetrated all aspects of our lives and\nbrought us great convenience. However, building a high-quality DL system for a\nspecific task highly relies on human expertise, hindering the applications of\nDL to more areas. Automated machine learning (AutoML) becomes a promising\nsolution to build a DL system without human assistance, and a growing number of\nresearchers focus on AutoML. In this paper, we provide a comprehensive and\nup-to-date review of the state-of-the-art (SOTA) in AutoML. First, we introduce\nAutoML methods according to the pipeline, covering data preparation, feature\nengineering, hyperparameter optimization, and neural architecture search (NAS).\nWe focus more on NAS, as it is currently very hot sub-topic of AutoML. We\nsummarize the performance of the representative NAS algorithms on the CIFAR-10\nand ImageNet datasets and further discuss several worthy studying directions of\nNAS methods: one/two-stage NAS, one-shot NAS, and joint hyperparameter and\narchitecture optimization. Finally, we discuss some open problems of the\nexisting AutoML methods for future research.\n", "versions": [{"version": "v1", "created": "Fri, 2 Aug 2019 05:56:13 GMT"}, {"version": "v2", "created": "Wed, 14 Aug 2019 16:11:15 GMT"}, {"version": "v3", "created": "Mon, 10 Feb 2020 13:05:19 GMT"}, {"version": "v4", "created": "Thu, 27 Feb 2020 15:00:06 GMT"}, {"version": "v5", "created": "Wed, 8 Jul 2020 11:43:36 GMT"}, {"version": "v6", "created": "Fri, 16 Apr 2021 03:38:23 GMT"}], "update_date": "2021-04-19", "authors_parsed": [["He", "Xin", ""], ["Zhao", "Kaiyong", ""], ["Chu", "Xiaowen", ""]]}, {"id": "1908.00716", "submitter": "Vinay Kumar Venkataramana", "authors": "Vinay Kumar V and P Nagabhushan", "title": "Monitoring of people entering and exiting private areas using Computer\n  Vision", "comments": "4 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Entry-Exit surveillance is a novel research problem that addresses security\nconcerns when people attain absolute privacy in camera forbidden areas such as\ntoilets and changing rooms that are basic amenities to the humans in public\nplaces such as Shopping malls, Airports, Bus and Rail stations. The objective\nis, if not inside these camera forbidden areas, from outside, the individuals\nare to be monitored to analyze the time spent by them inside and also the\nsuspecting transformations in their appearances if any. In this paper, firstly,\na pseudo-annotated dataset of a laboratory observation of people entering and\nexiting the camera forbidden area captured using two cameras in contrast to the\nstate-of-the-art single-camera based EnEx dataset is presented. Conventionally\nthe proposed dataset is named \\textbf{\\textit{EnEx2}}. Next, a spatial\ntransition based event detection to determine the entry or exit of individuals\nis presented with standard results by evaluating the proposed model using the\nproposed dataset and the publicly available standard video surveillance\ndatasets that are hypothesized to Entry-Exit surveillance scenarios. The\nproposed dataset is expected to enkindle active research in Entry-Exit\nSurveillance domain.\n", "versions": [{"version": "v1", "created": "Fri, 2 Aug 2019 06:33:06 GMT"}, {"version": "v2", "created": "Wed, 28 Aug 2019 08:12:44 GMT"}], "update_date": "2019-08-29", "authors_parsed": [["Kumar", "Vinay", "V"], ["Nagabhushan", "P", ""]]}, {"id": "1908.00720", "submitter": "Xinhai Liu", "authors": "Xinhai Liu, Zhizhong Han, Xin Wen, Yu-Shen Liu, Matthias Zwicker", "title": "L2G Auto-encoder: Understanding Point Clouds by Local-to-Global\n  Reconstruction with Hierarchical Self-Attention", "comments": null, "journal-ref": null, "doi": "10.1145/3343031.3350960", "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Auto-encoder is an important architecture to understand point clouds in an\nencoding and decoding procedure of self reconstruction. Current auto-encoder\nmainly focuses on the learning of global structure by global shape\nreconstruction, while ignoring the learning of local structures. To resolve\nthis issue, we propose Local-to-Global auto-encoder (L2G-AE) to simultaneously\nlearn the local and global structure of point clouds by local to global\nreconstruction. Specifically, L2G-AE employs an encoder to encode the geometry\ninformation of multiple scales in a local region at the same time. In addition,\nwe introduce a novel hierarchical self-attention mechanism to highlight the\nimportant points, scales and regions at different levels in the information\naggregation of the encoder. Simultaneously, L2G-AE employs a recurrent neural\nnetwork (RNN) as decoder to reconstruct a sequence of scales in a local region,\nbased on which the global point cloud is incrementally reconstructed. Our\noutperforming results in shape classification, retrieval and upsampling show\nthat L2G-AE can understand point clouds better than state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Fri, 2 Aug 2019 06:50:59 GMT"}], "update_date": "2019-08-05", "authors_parsed": [["Liu", "Xinhai", ""], ["Han", "Zhizhong", ""], ["Wen", "Xin", ""], ["Liu", "Yu-Shen", ""], ["Zwicker", "Matthias", ""]]}, {"id": "1908.00722", "submitter": "Robin Strudel", "authors": "Robin Strudel, Alexander Pashevich, Igor Kalevatykh, Ivan Laptev,\n  Josef Sivic, Cordelia Schmid", "title": "Learning to combine primitive skills: A step towards versatile robotic\n  manipulation", "comments": "ICRA 2020. See the project webpage at\n  https://www.di.ens.fr/willow/research/rlbc/", "journal-ref": "IEEE ROBOTICS AND AUTOMATION LETTERS, JULY 2020. 4637-4643", "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Manipulation tasks such as preparing a meal or assembling furniture remain\nhighly challenging for robotics and vision. Traditional task and motion\nplanning (TAMP) methods can solve complex tasks but require full state\nobservability and are not adapted to dynamic scene changes. Recent learning\nmethods can operate directly on visual inputs but typically require many\ndemonstrations and/or task-specific reward engineering. In this work we aim to\novercome previous limitations and propose a reinforcement learning (RL)\napproach to task planning that learns to combine primitive skills. First,\ncompared to previous learning methods, our approach requires neither\nintermediate rewards nor complete task demonstrations during training. Second,\nwe demonstrate the versatility of our vision-based task planning in challenging\nsettings with temporary occlusions and dynamic scene changes. Third, we propose\nan efficient training of basic skills from few synthetic demonstrations by\nexploring recent CNN architectures and data augmentation. Notably, while all of\nour policies are learned on visual inputs in simulated environments, we\ndemonstrate the successful transfer and high success rates when applying such\npolicies to manipulation tasks on a real UR5 robotic arm.\n", "versions": [{"version": "v1", "created": "Fri, 2 Aug 2019 07:04:17 GMT"}, {"version": "v2", "created": "Thu, 26 Sep 2019 16:02:27 GMT"}, {"version": "v3", "created": "Sat, 20 Jun 2020 14:26:45 GMT"}], "update_date": "2020-06-23", "authors_parsed": [["Strudel", "Robin", ""], ["Pashevich", "Alexander", ""], ["Kalevatykh", "Igor", ""], ["Laptev", "Ivan", ""], ["Sivic", "Josef", ""], ["Schmid", "Cordelia", ""]]}, {"id": "1908.00732", "submitter": "Chundong Wang", "authors": "Jingxuan Jiang, Chundong Wang, Sudipta Chattopadhyay, and Wei Zhang", "title": "Road Context-aware Intrusion Detection System for Autonomous Cars", "comments": "This manuscript presents an intrusion detection system that makes use\n  of road context for autonomous cars", "journal-ref": null, "doi": "10.1007/978-3-030-41579-2_8", "report-no": null, "categories": "cs.CR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Security is of primary importance to vehicles. The viability of performing\nremote intrusions onto the in-vehicle network has been manifested. In regard to\nunmanned autonomous cars, limited work has been done to detect intrusions for\nthem while existing intrusion detection systems (IDSs) embrace limitations\nagainst strong adversaries. In this paper, we consider the very nature of\nautonomous car and leverage the road context to build a novel IDS, named Road\ncontext-aware IDS (RAIDS). When a computer-controlled car is driving through\ncontinuous roads, road contexts and genuine frames transmitted on the car's\nin-vehicle network should resemble a regular and intelligible pattern. RAIDS\nhence employs a lightweight machine learning model to extract road contexts\nfrom sensory information (e.g., camera images and distance sensor values) that\nare used to generate control signals for maneuvering the car. With such ongoing\nroad context, RAIDS validates corresponding frames observed on the in-vehicle\nnetwork. Anomalous frames that substantially deviate from road context will be\ndiscerned as intrusions. We have implemented a prototype of RAIDS with neural\nnetworks, and conducted experiments on a Raspberry Pi with extensive datasets\nand meaningful intrusion cases. Evaluations show that RAIDS significantly\noutperforms state-of-the-art IDS without using road context by up to 99.9%\naccuracy and short response time.\n", "versions": [{"version": "v1", "created": "Fri, 2 Aug 2019 07:48:31 GMT"}], "update_date": "2020-03-02", "authors_parsed": [["Jiang", "Jingxuan", ""], ["Wang", "Chundong", ""], ["Chattopadhyay", "Sudipta", ""], ["Zhang", "Wei", ""]]}, {"id": "1908.00733", "submitter": "Mohammad Sadegh Aliakbarian", "authors": "Mohammad Sadegh Aliakbarian, Fatemeh Sadat Saleh, Mathieu Salzmann,\n  Lars Petersson, Stephen Gould, Amirhossein Habibian", "title": "Learning Variations in Human Motion via Mix-and-Match Perturbation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human motion prediction is a stochastic process: Given an observed sequence\nof poses, multiple future motions are plausible. Existing approaches to\nmodeling this stochasticity typically combine a random noise vector with\ninformation about the previous poses. This combination, however, is done in a\ndeterministic manner, which gives the network the flexibility to learn to\nignore the random noise. In this paper, we introduce an approach to\nstochastically combine the root of variations with previous pose information,\nwhich forces the model to take the noise into account. We exploit this idea for\nmotion prediction by incorporating it into a recurrent encoder-decoder network\nwith a conditional variational autoencoder block that learns to exploit the\nperturbations. Our experiments demonstrate that our model yields high-quality\npose sequences that are much more diverse than those from state-of-the-art\nstochastic motion prediction techniques.\n", "versions": [{"version": "v1", "created": "Fri, 2 Aug 2019 07:48:48 GMT"}, {"version": "v2", "created": "Mon, 24 Feb 2020 22:03:12 GMT"}], "update_date": "2020-02-26", "authors_parsed": [["Aliakbarian", "Mohammad Sadegh", ""], ["Saleh", "Fatemeh Sadat", ""], ["Salzmann", "Mathieu", ""], ["Petersson", "Lars", ""], ["Gould", "Stephen", ""], ["Habibian", "Amirhossein", ""]]}, {"id": "1908.00748", "submitter": "Christian Payer", "authors": "Christian Payer, Darko \\v{S}tern, Horst Bischof, Martin Urschler", "title": "Integrating Spatial Configuration into Heatmap Regression Based CNNs for\n  Landmark Localization", "comments": "MIDL 2019 [arXiv:1907.08612]", "journal-ref": null, "doi": "10.1016/j.media.2019.03.007", "report-no": "MIDL/2019/ExtendedAbstract/r1xGxWJ0t4", "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  In many medical image analysis applications, often only a limited amount of\ntraining data is available, which makes training of convolutional neural\nnetworks (CNNs) challenging. In this work on anatomical landmark localization,\nwe propose a CNN architecture that learns to split the localization task into\ntwo simpler sub-problems, reducing the need for large training datasets. Our\nfully convolutional SpatialConfiguration-Net (SCN) dedicates one component to\nlocally accurate but ambiguous candidate predictions, while the other component\nimproves robustness to ambiguities by incorporating the spatial configuration\nof landmarks. In our experimental evaluation, we show that the proposed SCN\noutperforms related methods in terms of landmark localization error on\nsize-limited datasets.\n", "versions": [{"version": "v1", "created": "Fri, 2 Aug 2019 08:17:50 GMT"}], "update_date": "2019-08-05", "authors_parsed": [["Payer", "Christian", ""], ["\u0160tern", "Darko", ""], ["Bischof", "Horst", ""], ["Urschler", "Martin", ""]]}, {"id": "1908.00763", "submitter": "Ninnart Fuengfusin", "authors": "Ninnart Fuengfusin, Hakaru Tamukoh", "title": "Network with Sub-Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce network with sub-networks, a neural network which its weight\nlayers could be detached into sub-neural networks during inference. To develop\nweights and biases which could be inserted in both base and sub-neural\nnetworks, firstly, the parameters are copied from sub-model to base-model. Each\nmodel is forward-propagated separately. Gradients from a pair of networks are\naveraged and, used to update both networks. Our base model achieves the\ntest-accuracy which is comparable to the regularly trained models, while the\nmodel maintains the ability to detach weight layers.\n", "versions": [{"version": "v1", "created": "Fri, 2 Aug 2019 09:04:28 GMT"}, {"version": "v2", "created": "Tue, 3 Dec 2019 04:41:02 GMT"}], "update_date": "2019-12-04", "authors_parsed": [["Fuengfusin", "Ninnart", ""], ["Tamukoh", "Hakaru", ""]]}, {"id": "1908.00764", "submitter": "Jos\\'e Ignacio Orlando PhD", "authors": "Jos\\'e Ignacio Orlando, Anna Breger, Hrvoje Bogunovi\\'c, Sophie Riedl,\n  Bianca S. Gerendas, Martin Ehler, Ursula Schmidt-Erfurth", "title": "An amplified-target loss approach for photoreceptor layer segmentation\n  in pathological OCT scans", "comments": "Accepted for publication at MICCAI-OMIA 2019", "journal-ref": null, "doi": "10.1007/978-3-030-32956-3_4", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Segmenting anatomical structures such as the photoreceptor layer in retinal\noptical coherence tomography (OCT) scans is challenging in pathological\nscenarios. Supervised deep learning models trained with standard loss functions\nare usually able to characterize only the most common disease appeareance from\na training set, resulting in suboptimal performance and poor generalization\nwhen dealing with unseen lesions. In this paper we propose to overcome this\nlimitation by means of an augmented target loss function framework. We\nintroduce a novel amplified-target loss that explicitly penalizes errors within\nthe central area of the input images, based on the observation that most of the\nchallenging disease appeareance is usually located in this area. We\nexperimentally validated our approach using a data set with OCT scans of\npatients with macular diseases. We observe increased performance compared to\nthe models that use only the standard losses. Our proposed loss function\nstrongly supports the segmentation model to better distinguish photoreceptors\nin highly pathological scenarios.\n", "versions": [{"version": "v1", "created": "Fri, 2 Aug 2019 09:05:37 GMT"}, {"version": "v2", "created": "Mon, 21 Oct 2019 13:38:28 GMT"}], "update_date": "2019-10-22", "authors_parsed": [["Orlando", "Jos\u00e9 Ignacio", ""], ["Breger", "Anna", ""], ["Bogunovi\u0107", "Hrvoje", ""], ["Riedl", "Sophie", ""], ["Gerendas", "Bianca S.", ""], ["Ehler", "Martin", ""], ["Schmidt-Erfurth", "Ursula", ""]]}, {"id": "1908.00777", "submitter": "Zhenmei Shi", "authors": "Zhenmei Shi, Haoyang Fang, Yu-Wing Tai, Chi-Keung Tang", "title": "DAWN: Dual Augmented Memory Network for Unsupervised Video Object\n  Tracking", "comments": "Zhenmei and Haoyang have equal contribution", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Psychological studies have found that human visual tracking system involves\nlearning, memory, and planning. Despite recent successes, not many works have\nfocused on memory and planning in deep learning based tracking. We are thus\ninterested in memory augmented network, where an external memory remembers the\nevolving appearance of the target (foreground) object without backpropagation\nfor updating weights. Our Dual Augmented Memory Network (DAWN) is unique in\nremembering both target and background, and using an improved attention LSTM\nmemory to guide the focus on memorized features. DAWN is effective in\nunsupervised tracking in handling total occlusion, severe motion blur, abrupt\nchanges in target appearance, multiple object instances, and similar foreground\nand background features. We present extensive quantitative and qualitative\nexperimental comparison with state-of-the-art methods including top contenders\nin recent VOT challenges. Notably, despite the straightforward implementation,\nDAWN is ranked third in both VOT2016 and VOT2017 challenges with excellent\nsuccess rate among all VOT fast trackers running at fps > 10 in unsupervised\ntracking in both challenges. We propose DAWN-RPN, where we simply augment our\nmemory and attention LSTM modules to the state-of-the-art SiamRPN, and report\nimmediate performance gain, thus demonstrating DAWN can work well with and\ndirectly benefit other models to handle difficult cases as well.\n", "versions": [{"version": "v1", "created": "Fri, 2 Aug 2019 09:52:57 GMT"}, {"version": "v2", "created": "Thu, 8 Aug 2019 15:53:13 GMT"}], "update_date": "2019-08-09", "authors_parsed": [["Shi", "Zhenmei", ""], ["Fang", "Haoyang", ""], ["Tai", "Yu-Wing", ""], ["Tang", "Chi-Keung", ""]]}, {"id": "1908.00778", "submitter": "Mateus Riva", "authors": "Larissa de O. Penteado, Mateus Riva and Roberto M. Cesar Jr", "title": "A Structural Graph-Based Method for MRI Analysis", "comments": "Published in the Workshop of Works In Progress of the SIBGRAPI 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The importance of imaging exams, such as Magnetic Resonance Imaging (MRI),\nfor the diagnostic and follow-up of pediatric pathologies and the assessment of\nanatomical structures' development has been increasingly highlighted in recent\ntimes. Manual analysis of MRIs is time-consuming, subjective, and requires\nsignificant expertise. To mitigate this, automatic techniques are necessary.\nMost techniques focus on adult subjects, while pediatric MRI has specific\nchallenges such as the ongoing anatomical and histological changes related to\nnormal development of the organs, reduced signal-to-noise ratio due to the\nsmaller bodies, motion artifacts and cooperation issues, especially in long\nexams, which can in many cases preclude common analysis methods developed for\nuse in adults. Therefore, the development of a robust technique to aid in\npediatric MRI analysis is necessary. This paper presents the current\ndevelopment of a new method based on the learning and matching of structural\nrelational graphs (SRGs). The experiments were performed on liver MRI sequences\nof one patient from ICr-HC-FMUSP, and preliminary results showcased the\nviability of the project. Future experiments are expected to culminate with an\napplication for pediatric liver substructure and brain tumor segmentation.\n", "versions": [{"version": "v1", "created": "Fri, 2 Aug 2019 09:53:18 GMT"}], "update_date": "2019-08-05", "authors_parsed": [["Penteado", "Larissa de O.", ""], ["Riva", "Mateus", ""], ["Cesar", "Roberto M.", "Jr"]]}, {"id": "1908.00788", "submitter": "Max-Heinrich Laves", "authors": "Max-Heinrich Laves, Sontje Ihler, Tobias Ortmaier", "title": "Deformable Medical Image Registration Using a Randomly-Initialized CNN\n  as Regularization Prior", "comments": "Accepted at MIDL 2019 [arXiv:1907.08612]", "journal-ref": null, "doi": null, "report-no": "MIDL/2019/ExtendedAbstract/S1ehZFQ15E", "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  We present deformable unsupervised medical image registration using a\nrandomly-initialized deep convolutional neural network (CNN) as regularization\nprior. Conventional registration methods predict a transformation by minimizing\ndissimilarities between an image pair. The minimization is usually regularized\nwith manually engineered priors, which limits the potential of the\nregistration. By learning transformation priors from a large dataset, CNNs have\nachieved great success in deformable registration. However, learned methods are\nrestricted to domain-specific data and the required amounts of medical data are\ndifficult to obtain. Our approach uses the idea of deep image priors to combine\nconvolutional networks with conventional registration methods based on manually\nengineered priors. The proposed method is applied to brain MRI scans. We show\nthat our approach registers image pairs with state-of-the-art accuracy by\nproviding dense, pixel-wise correspondence maps. It does not rely on prior\ntraining and is therefore not limited to a specific image domain.\n", "versions": [{"version": "v1", "created": "Fri, 2 Aug 2019 10:19:44 GMT"}], "update_date": "2019-08-05", "authors_parsed": [["Laves", "Max-Heinrich", ""], ["Ihler", "Sontje", ""], ["Ortmaier", "Tobias", ""]]}, {"id": "1908.00792", "submitter": "Max-Heinrich Laves M. Sc.", "authors": "Max-Heinrich Laves, Sontje Ihler, Tobias Ortmaier", "title": "Uncertainty Quantification in Computer-Aided Diagnosis: Make Your Model\n  say \"I don't know\" for Ambiguous Cases", "comments": "Accepted at MIDL 2019 [arXiv:1907.08612]", "journal-ref": null, "doi": null, "report-no": "MIDL/2019/ExtendedAbstract/rJevPsX854", "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  We evaluate two different methods for the integration of prediction\nuncertainty into diagnostic image classifiers to increase patient safety in\ndeep learning. In the first method, Monte Carlo sampling is applied with\ndropout at test time to get a posterior distribution of the class labels\n(Bayesian ResNet). The second method extends ResNet to a probabilistic approach\nby predicting the parameters of the posterior distribution and sampling the\nfinal result from it (Variational ResNet).The variance of the posterior is used\nas metric for uncertainty.Both methods are trained on a data set of optical\ncoherence tomography scans showing four different retinal conditions. Our\nresults shown that cases in which the classifier predicts incorrectly correlate\nwith a higher uncertainty. Mean uncertainty of incorrectly diagnosed cases was\nbetween 4.6 and 8.1 times higher than mean uncertainty of correctly diagnosed\ncases. Modeling of the prediction uncertainty in computer-aided diagnosis with\ndeep learning yields more reliable results and is anticipated to increase\npatient safety.\n", "versions": [{"version": "v1", "created": "Fri, 2 Aug 2019 10:31:02 GMT"}], "update_date": "2019-08-05", "authors_parsed": [["Laves", "Max-Heinrich", ""], ["Ihler", "Sontje", ""], ["Ortmaier", "Tobias", ""]]}, {"id": "1908.00801", "submitter": "Luca Calatroni", "authors": "Luca Calatroni, Alessandro Lanza, Monica Pragliola, Fiorella Sgallari", "title": "Space-adaptive anisotropic bivariate Laplacian regularization for image\n  restoration", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.NA math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present a new regularization term for variational image\nrestoration which can be regarded as a space-variant anisotropic extension of\nthe classical isotropic Total Variation (TV) regularizer. The proposed\nregularizer comes from the statistical assumption that the gradients of the\ntarget image distribute locally according to space-variant bivariate Laplacian\ndistributions. The highly flexible variational structure of the corresponding\nregularizer encodes several free parameters which hold the potential for\nfaithfully modelling the local geometry in the image and describing local\norientation preferences. For an automatic estimation of such parameters, we\ndesign a robust maximum likelihood approach and report results on its\nreliability on synthetic data and natural images. A minimization algorithm\nbased on the Alternating Direction Method of Multipliers (ADMM) is presented\nfor the efficient numerical solution of the proposed variational model. Some\nexperimental results are reported which demonstrate the high-quality of\nrestorations achievable by the proposed model, in particular with respect to\nclassical Total Variation regularization.\n", "versions": [{"version": "v1", "created": "Fri, 2 Aug 2019 11:01:19 GMT"}], "update_date": "2019-08-05", "authors_parsed": [["Calatroni", "Luca", ""], ["Lanza", "Alessandro", ""], ["Pragliola", "Monica", ""], ["Sgallari", "Fiorella", ""]]}, {"id": "1908.00821", "submitter": "Yuenan Hou", "authors": "Yuenan Hou, Zheng Ma, Chunxiao Liu, and Chen Change Loy", "title": "Learning Lightweight Lane Detection CNNs by Self Attention Distillation", "comments": "9 pages, 8 figures; This paper is accepted by ICCV 2019; Our code is\n  available at https://github.com/cardwing/Codes-for-Lane-Detection", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Training deep models for lane detection is challenging due to the very subtle\nand sparse supervisory signals inherent in lane annotations. Without learning\nfrom much richer context, these models often fail in challenging scenarios,\ne.g., severe occlusion, ambiguous lanes, and poor lighting conditions. In this\npaper, we present a novel knowledge distillation approach, i.e., Self Attention\nDistillation (SAD), which allows a model to learn from itself and gains\nsubstantial improvement without any additional supervision or labels.\nSpecifically, we observe that attention maps extracted from a model trained to\na reasonable level would encode rich contextual information. The valuable\ncontextual information can be used as a form of 'free' supervision for further\nrepresentation learning through performing topdown and layer-wise attention\ndistillation within the network itself. SAD can be easily incorporated in any\nfeedforward convolutional neural networks (CNN) and does not increase the\ninference time. We validate SAD on three popular lane detection benchmarks\n(TuSimple, CULane and BDD100K) using lightweight models such as ENet, ResNet-18\nand ResNet-34. The lightest model, ENet-SAD, performs comparatively or even\nsurpasses existing algorithms. Notably, ENet-SAD has 20 x fewer parameters and\nruns 10 x faster compared to the state-of-the-art SCNN, while still achieving\ncompelling performance in all benchmarks. Our code is available at\nhttps://github.com/cardwing/Codes-for-Lane-Detection.\n", "versions": [{"version": "v1", "created": "Fri, 2 Aug 2019 12:13:34 GMT"}], "update_date": "2019-08-05", "authors_parsed": [["Hou", "Yuenan", ""], ["Ma", "Zheng", ""], ["Liu", "Chunxiao", ""], ["Loy", "Chen Change", ""]]}, {"id": "1908.00855", "submitter": "Lichao Zhang", "authors": "Lichao Zhang, Abel Gonzalez-Garcia, Joost van de Weijer, Martin\n  Danelljan, Fahad Shahbaz Khan", "title": "Learning the Model Update for Siamese Trackers", "comments": "Accepted at ICCV 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Siamese approaches address the visual tracking problem by extracting an\nappearance template from the current frame, which is used to localize the\ntarget in the next frame. In general, this template is linearly combined with\nthe accumulated template from the previous frame, resulting in an exponential\ndecay of information over time. While such an approach to updating has led to\nimproved results, its simplicity limits the potential gain likely to be\nobtained by learning to update. Therefore, we propose to replace the\nhandcrafted update function with a method which learns to update. We use a\nconvolutional neural network, called UpdateNet, which given the initial\ntemplate, the accumulated template and the template of the current frame aims\nto estimate the optimal template for the next frame. The UpdateNet is compact\nand can easily be integrated into existing Siamese trackers. We demonstrate the\ngenerality of the proposed approach by applying it to two Siamese trackers,\nSiamFC and DaSiamRPN. Extensive experiments on VOT2016, VOT2018, LaSOT, and\nTrackingNet datasets demonstrate that our UpdateNet effectively predicts the\nnew target template, outperforming the standard linear update. On the\nlarge-scale TrackingNet dataset, our UpdateNet improves the results of\nDaSiamRPN with an absolute gain of 3.9% in terms of success score.\n", "versions": [{"version": "v1", "created": "Fri, 2 Aug 2019 13:40:43 GMT"}, {"version": "v2", "created": "Fri, 6 Sep 2019 14:35:51 GMT"}], "update_date": "2019-09-09", "authors_parsed": [["Zhang", "Lichao", ""], ["Gonzalez-Garcia", "Abel", ""], ["van de Weijer", "Joost", ""], ["Danelljan", "Martin", ""], ["Khan", "Fahad Shahbaz", ""]]}, {"id": "1908.00858", "submitter": "Muhamad Risqi U. Saputra", "authors": "Muhamad Risqi U. Saputra, Pedro P. B. de Gusmao, Yasin Almalioglu,\n  Andrew Markham, Niki Trigoni", "title": "Distilling Knowledge From a Deep Pose Regressor Network", "comments": "Accepted to ICCV 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a novel method to distill knowledge from a deep pose\nregressor network for efficient Visual Odometry (VO). Standard distillation\nrelies on \"dark knowledge\" for successful knowledge transfer. As this knowledge\nis not available in pose regression and the teacher prediction is not always\naccurate, we propose to emphasize the knowledge transfer only when we trust the\nteacher. We achieve this by using teacher loss as a confidence score which\nplaces variable relative importance on the teacher prediction. We inject this\nconfidence score to the main training task via Attentive Imitation Loss (AIL)\nand when learning the intermediate representation of the teacher through\nAttentive Hint Training (AHT) approach. To the best of our knowledge, this is\nthe first work which successfully distill the knowledge from a deep pose\nregression network. Our evaluation on the KITTI and Malaga dataset shows that\nwe can keep the student prediction close to the teacher with up to 92.95%\nparameter reduction and 2.12x faster in computation time.\n", "versions": [{"version": "v1", "created": "Fri, 2 Aug 2019 13:48:31 GMT"}], "update_date": "2019-08-05", "authors_parsed": [["Saputra", "Muhamad Risqi U.", ""], ["de Gusmao", "Pedro P. B.", ""], ["Almalioglu", "Yasin", ""], ["Markham", "Andrew", ""], ["Trigoni", "Niki", ""]]}, {"id": "1908.00862", "submitter": "Lei Qi", "authors": "Lei Qi, Lei Wang, Jing Huo, Yinghuan Shi, Xin Geng and Yang Gao", "title": "Adversarial Camera Alignment Network for Unsupervised Cross-camera\n  Person Re-identification", "comments": "Accepted by IEEE Transactions on Circuits and Systems for Video\n  Technology (TCSVT)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In person re-identification (Re-ID), supervised methods usually need a large\namount of expensive label information, while unsupervised ones are still unable\nto deliver satisfactory identification performance. In this paper, we introduce\na novel person Re-ID task called unsupervised cross-camera person Re-ID, which\nonly needs the within-camera (intra-camera) label information but not\ncross-camera (inter-camera) labels which are more expensive to obtain. In\nreal-world applications, the intra-camera label information can be easily\ncaptured by tracking algorithms or few manual annotations. In this situation,\nthe main challenge becomes the distribution discrepancy across different camera\nviews, caused by the various body pose, occlusion, image resolution,\nillumination conditions, and background noises in different cameras. To address\nthis situation, we propose a novel Adversarial Camera Alignment Network (ACAN)\nfor unsupervised cross-camera person Re-ID. It consists of the camera-alignment\ntask and the supervised within-camera learning task. To achieve the camera\nalignment, we develop a Multi-Camera Adversarial Learning (MCAL) to map images\nof different cameras into a shared subspace. Particularly, we investigate two\ndifferent schemes, including the existing GRL (i.e., gradient reversal layer)\nscheme and the proposed scheme called \"other camera equiprobability\" (OCE), to\nconduct the multi-camera adversarial task. Based on this shared subspace, we\nthen leverage the within-camera labels to train the network. Extensive\nexperiments on five large-scale datasets demonstrate the superiority of ACAN\nover multiple state-of-the-art unsupervised methods that take advantage of\nlabeled source domains and generated images by GAN-based models. In particular,\nwe verify that the proposed multi-camera adversarial task does contribute to\nthe significant improvement.\n", "versions": [{"version": "v1", "created": "Fri, 2 Aug 2019 13:58:26 GMT"}, {"version": "v2", "created": "Fri, 9 Jul 2021 14:35:59 GMT"}], "update_date": "2021-07-12", "authors_parsed": [["Qi", "Lei", ""], ["Wang", "Lei", ""], ["Huo", "Jing", ""], ["Shi", "Yinghuan", ""], ["Geng", "Xin", ""], ["Gao", "Yang", ""]]}, {"id": "1908.00867", "submitter": "Will Price", "authors": "Will Price and Dima Damen", "title": "An Evaluation of Action Recognition Models on EPIC-Kitchens", "comments": "6 pages, 3 figures, 3 tables. Models released at\n  https://github.com/epic-kitchens/action-models", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We benchmark contemporary action recognition models (TSN, TRN, and TSM) on\nthe recently introduced EPIC-Kitchens dataset and release pretrained models on\nGitHub (https://github.com/epic-kitchens/action-models) for others to build\nupon. In contrast to popular action recognition datasets like Kinetics,\nSomething-Something, UCF101, and HMDB51, EPIC-Kitchens is shot from an\negocentric perspective and captures daily actions in-situ. In this report, we\naim to understand how well these models can tackle the challenges present in\nthis dataset, such as its long tail class distribution, unseen environment test\nset, and multiple tasks (verb, noun and, action classification). We discuss the\nmodels' shortcomings and avenues for future research.\n", "versions": [{"version": "v1", "created": "Fri, 2 Aug 2019 14:07:07 GMT"}], "update_date": "2019-08-05", "authors_parsed": [["Price", "Will", ""], ["Damen", "Dima", ""]]}, {"id": "1908.00887", "submitter": "Donsub Rim", "authors": "Donsub Rim", "title": "Exact and fast inversion of the approximate discrete Radon transform\n  from partial data", "comments": "4 pages, 1 figure", "journal-ref": "Appl. Math. Lett. 102 106159 (2020)", "doi": "10.1016/j.aml.2019.106159", "report-no": null, "categories": "math.NA cs.CC cs.CV cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give an exact inversion formula for the approximate discrete Radon\ntransform introduced in [Brady, SIAM J. Comput., 27(1), 107--119] that is of\ncost $O(N \\log N)$ for a square 2D image with $N$ pixels and requires only\npartial data.\n", "versions": [{"version": "v1", "created": "Fri, 2 Aug 2019 14:41:35 GMT"}, {"version": "v2", "created": "Sat, 23 Nov 2019 05:45:55 GMT"}, {"version": "v3", "created": "Mon, 18 May 2020 05:29:43 GMT"}], "update_date": "2020-05-19", "authors_parsed": [["Rim", "Donsub", ""]]}, {"id": "1908.00894", "submitter": "Rui Fan", "authors": "Rui Fan, Umar Ozgunalp, Brett Hosking, Ming Liu, Ioannis Pitas", "title": "Pothole Detection Based on Disparity Transformation and Road Surface\n  Modeling", "comments": "12 pages, 15 figures, IEEE Transactions on Image Processing", "journal-ref": null, "doi": "10.1109/TIP.2019.2933750", "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pothole detection is one of the most important tasks for road maintenance.\nComputer vision approaches are generally based on either 2D road image analysis\nor 3D road surface modeling. However, these two categories are always used\nindependently. Furthermore, the pothole detection accuracy is still far from\nsatisfactory. Therefore, in this paper, we present a robust pothole detection\nalgorithm that is both accurate and computationally efficient. A dense\ndisparity map is first transformed to better distinguish between damaged and\nundamaged road areas. To achieve greater disparity transformation efficiency,\ngolden section search and dynamic programming are utilized to estimate the\ntransformation parameters. Otsu's thresholding method is then used to extract\npotential undamaged road areas from the transformed disparity map. The\ndisparities in the extracted areas are modeled by a quadratic surface using\nleast squares fitting. To improve disparity map modeling robustness, the\nsurface normal is also integrated into the surface modeling process.\nFurthermore, random sample consensus is utilized to reduce the effects caused\nby outliers. By comparing the difference between the actual and modeled\ndisparity maps, the potholes can be detected accurately. Finally, the point\nclouds of the detected potholes are extracted from the reconstructed 3D road\nsurface. The experimental results show that the successful detection accuracy\nof the proposed system is around 98.7% and the overall pixel-level accuracy is\napproximately 99.6%.\n", "versions": [{"version": "v1", "created": "Fri, 2 Aug 2019 14:55:48 GMT"}, {"version": "v2", "created": "Fri, 22 Nov 2019 08:31:38 GMT"}], "update_date": "2019-11-25", "authors_parsed": [["Fan", "Rui", ""], ["Ozgunalp", "Umar", ""], ["Hosking", "Brett", ""], ["Liu", "Ming", ""], ["Pitas", "Ioannis", ""]]}, {"id": "1908.00902", "submitter": "Flip Phillips", "authors": "J. Farley Norman, James T. Todd, Flip Phillips", "title": "Effects of Illumination on the Categorization of Shiny Materials", "comments": "v2, 20 pages, 15 figures, 26 references", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The present research was designed to examine how patterns of illumination\ninfluence the perceptual categorization of metal, shiny black, and shiny white\nmaterials. The stimuli depicted three possible objects that were illuminated by\nfive possible HDRI light maps, which varied in their overall distributions of\nilluminant directions and intensities. The surfaces included a low roughness\nchrome material, a shiny black material, and a shiny white material with both\ndiffuse and specular components. Observers rated each stimulus by adjusting\nfour sliders to indicate their confidence that the depicted material was metal,\nshiny black, shiny white or something else, and these adjustments were\nconstrained so that the sum of all four settings was always 100%. The results\nrevealed that the metal and shiny black categories are easily confused. For\nexample, metal materials with low intensity light maps or a narrow range of\nilluminant directions are often judged as shiny black, whereas shiny black\nmaterials with high intensity light maps or a wide range of illuminant\ndirections are often judged as metal. A spherical harmonic analysis was\nperformed on the different light maps in an effort to quantitatively predict\nhow they would bias observers' judgments of metal and shiny black surfaces.\n", "versions": [{"version": "v1", "created": "Fri, 2 Aug 2019 15:19:54 GMT"}, {"version": "v2", "created": "Mon, 21 Oct 2019 16:22:27 GMT"}], "update_date": "2019-10-22", "authors_parsed": [["Norman", "J. Farley", ""], ["Todd", "James T.", ""], ["Phillips", "Flip", ""]]}, {"id": "1908.00907", "submitter": "Yeman Brhane Hagos", "authors": "Yeman Brhane Hagos, Priya Lakshmi Narayanan, Ayse U. Akarca, Teresa\n  Marafioti, and Yinyin Yuan", "title": "ConCORDe-Net: Cell Count Regularized Convolutional Neural Network for\n  Cell Detection in Multiplex Immunohistochemistry Images", "comments": "MICCAI2019 accepted, 3 figures,8.5 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In digital pathology, cell detection and classification are often\nprerequisites to quantify cell abundance and explore tissue spatial\nheterogeneity. However, these tasks are particularly challenging for multiplex\nimmunohistochemistry (mIHC) images due to high levels of variability in\nstaining, expression intensity, and inherent noise as a result of preprocessing\nartefacts. We proposed a deep learning method to detect and classify cells in\nmIHC whole-tumour slide images of breast cancer. Inspired by inception-v3, we\ndeveloped Cell COunt RegularizeD Convolutional neural Network (ConCORDe-Net)\nwhich integrates conventional dice overlap and a new cell count loss function\nfor optimizing cell detection, followed by a multi-stage convolutional neural\nnetwork for cell classification. In total, 20447 cells, belonging to five cell\nclasses were annotated by experts from 175 patches extracted from 6\nwhole-tumour mIHC images. These patches were randomly split into training,\nvalidation and testing sets. Using ConCORDe-Net, we obtained a cell detection\nF1 score of 0.873, which is the best score compared to three state of the art\nmethods. In particular, ConCORDe-Net excels at detecting closely located and\nweakly stained cells compared to other methods. Incorporating cell count loss\nin the objective function regularizes the network to learn weak gradient\nboundaries and separate weakly stained cells from background artefacts.\nMoreover, cell classification accuracy of 96.5% was achieved. These results\nsupport that incorporating problem-specific knowledge such as cell count into\ndeep learning-based cell detection architectures improve the robustness of the\nalgorithm.\n", "versions": [{"version": "v1", "created": "Thu, 1 Aug 2019 12:51:01 GMT"}], "update_date": "2019-08-05", "authors_parsed": [["Hagos", "Yeman Brhane", ""], ["Narayanan", "Priya Lakshmi", ""], ["Akarca", "Ayse U.", ""], ["Marafioti", "Teresa", ""], ["Yuan", "Yinyin", ""]]}, {"id": "1908.00936", "submitter": "Andreas Selmar Hauptmann", "authors": "Andreas Hauptmann, and Jonas Adler, and Simon Arridge, and Ozan\n  \\\"Oktem", "title": "Multi-Scale Learned Iterative Reconstruction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.NA cs.NE math.NA math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Model-based learned iterative reconstruction methods have recently been shown\nto outperform classical reconstruction algorithms. Applicability of these\nmethods to large scale inverse problems is however limited by the available\nmemory for training and extensive training times, the latter due to\ncomputationally expensive forward models. As a possible solution to these\nrestrictions we propose a multi-scale learned iterative reconstruction scheme\nthat computes iterates on discretisations of increasing resolution. This\nprocedure does not only reduce memory requirements, it also considerably speeds\nup reconstruction and training times, but most importantly is scalable to large\nscale inverse problems with non-trivial forward operators, such as those that\narise in many 3D tomographic applications. In particular, we propose a hybrid\nnetwork that combines the multi-scale iterative approach with a particularly\nexpressive network architecture which in combination exhibits excellent\nscalability in 3D.\n  Applicability of the algorithm is demonstrated for 3D cone beam computed\ntomography from real measurement data of an organic phantom. Additionally, we\nexamine scalability and reconstruction quality in comparison to established\nlearned reconstruction methods in two dimensions for low dose computed\ntomography on human phantoms.\n", "versions": [{"version": "v1", "created": "Thu, 1 Aug 2019 09:34:35 GMT"}, {"version": "v2", "created": "Wed, 11 Dec 2019 16:52:58 GMT"}, {"version": "v3", "created": "Mon, 20 Apr 2020 07:03:57 GMT"}], "update_date": "2020-04-21", "authors_parsed": [["Hauptmann", "Andreas", ""], ["Adler", "Jonas", ""], ["Arridge", "Simon", ""], ["\u00d6ktem", "Ozan", ""]]}, {"id": "1908.00943", "submitter": "Tahmida Mahmud", "authors": "Tahmida Mahmud, Mohammad Billah, Mahmudul Hasan, Amit K. Roy-Chowdhury", "title": "Prediction and Description of Near-Future Activities in Video", "comments": "15 pages, 4 figures, 14 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most of the existing works on human activity analysis focus on recognition or\nearly recognition of the activity labels from complete or partial observations.\nSimilarly, almost all of the existing video captioning approaches focus on the\nobserved events in videos. Predicting the labels and the captions of future\nactivities where no frames of the predicted activities have been observed is a\nchallenging problem, with important applications that require anticipatory\nresponse. In this work, we propose a system that can infer the labels and the\ncaptions of a sequence of future activities. Our proposed network for label\nprediction of a future activity sequence has three branches where the first\nbranch takes visual features from the objects present in the scene, the second\nbranch takes observed sequential activity features, and the third branch\ncaptures the last observed activity features. The predicted labels and the\nobserved scene context are then mapped to meaningful captions using a\nsequence-to-sequence learning-based method. Experiments on four challenging\nactivity analysis datasets and a video description dataset demonstrate that our\nlabel prediction approach achieves comparable performance with the\nstate-of-the-arts and our captioning framework outperform the\nstate-of-the-arts.\n", "versions": [{"version": "v1", "created": "Fri, 2 Aug 2019 16:25:59 GMT"}, {"version": "v2", "created": "Thu, 2 Apr 2020 05:18:56 GMT"}, {"version": "v3", "created": "Fri, 29 May 2020 17:01:52 GMT"}, {"version": "v4", "created": "Tue, 25 May 2021 03:48:42 GMT"}, {"version": "v5", "created": "Thu, 27 May 2021 02:46:23 GMT"}], "update_date": "2021-05-28", "authors_parsed": [["Mahmud", "Tahmida", ""], ["Billah", "Mohammad", ""], ["Hasan", "Mahmudul", ""], ["Roy-Chowdhury", "Amit K.", ""]]}, {"id": "1908.00967", "submitter": "David T. Hoffmann", "authors": "David T. Hoffmann, Dimitrios Tzionas, Micheal J. Black, Siyu Tang", "title": "Learning to Train with Synthetic Humans", "comments": "In German Conference on Pattern Recognition (GCPR)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural networks need big annotated datasets for training. However, manual\nannotation can be too expensive or even unfeasible for certain tasks, like\nmulti-person 2D pose estimation with severe occlusions. A remedy for this is\nsynthetic data with perfect ground truth. Here we explore two variations of\nsynthetic data for this challenging problem; a dataset with purely synthetic\nhumans and a real dataset augmented with synthetic humans. We then study which\napproach better generalizes to real data, as well as the influence of virtual\nhumans in the training loss. Using the augmented dataset, without considering\nsynthetic humans in the loss, leads to the best results. We observe that not\nall synthetic samples are equally informative for training, while the\ninformative samples are different for each training stage. To exploit this\nobservation, we employ an adversarial student-teacher framework; the teacher\nimproves the student by providing the hardest samples for its current state as\na challenge. Experiments show that the student-teacher framework outperforms\nnormal training on the purely synthetic dataset.\n", "versions": [{"version": "v1", "created": "Fri, 2 Aug 2019 17:49:30 GMT"}], "update_date": "2019-08-05", "authors_parsed": [["Hoffmann", "David T.", ""], ["Tzionas", "Dimitrios", ""], ["Black", "Micheal J.", ""], ["Tang", "Siyu", ""]]}, {"id": "1908.00975", "submitter": "Hengrong Lan", "authors": "Hengrong Lan, Daohuai Jiang, Changchun Yang, Fei Gao", "title": "Y-Net: A Hybrid Deep Learning Reconstruction Framework for Photoacoustic\n  Imaging in vivo", "comments": "submitted the journal version", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Photoacoustic imaging (PAI) is an emerging non-invasive imaging modality\ncombining the advantages of deep ultrasound penetration and high optical\ncontrast. Image reconstruction is an essential topic in PAI, which is\nunfortunately an ill-posed problem due to the complex and unknown\noptical/acoustic parameters in tissue. Conventional algorithms used in PAI\n(e.g., delay-and-sum) provide a fast solution while many artifacts remain,\nespecially for linear array probe with limited-view issue. Convolutional neural\nnetwork (CNN) has shown state-of-the-art results in computer vision, and more\nand more work based on CNN has been studied in medical image processing\nrecently. In this paper, we present a non-iterative scheme filling the gap\nbetween existing direct-processing and post-processing methods, and propose a\nnew framework Y-Net: a CNN architecture to reconstruct the PA image by\noptimizing both raw data and beamformed images once. The network connected two\nencoders with one decoder path, which optimally utilizes more information from\nraw data and beamformed image. The results of the test set showed good\nperformance compared with conventional reconstruction algorithms and other deep\nlearning methods. Our method is also validated with experiments both in-vitro\nand in vivo, which still performs better than other existing methods. The\nproposed Y-Net architecture also has high potential in medical image\nreconstruction for other imaging modalities beyond PAI.\n", "versions": [{"version": "v1", "created": "Fri, 2 Aug 2019 07:27:17 GMT"}], "update_date": "2019-08-06", "authors_parsed": [["Lan", "Hengrong", ""], ["Jiang", "Daohuai", ""], ["Yang", "Changchun", ""], ["Gao", "Fei", ""]]}, {"id": "1908.00999", "submitter": "Hao Tang", "authors": "Hao Tang, Dan Xu, Gaowen Liu, Wei Wang, Nicu Sebe, Yan Yan", "title": "Cycle In Cycle Generative Adversarial Networks for Keypoint-Guided Image\n  Generation", "comments": "9 pages, 8 figures, accepted to ACM MM 2019", "journal-ref": "ACM MM 2019", "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we propose a novel Cycle In Cycle Generative Adversarial\nNetwork (C$^2$GAN) for the task of keypoint-guided image generation. The\nproposed C$^2$GAN is a cross-modal framework exploring a joint exploitation of\nthe keypoint and the image data in an interactive manner. C$^2$GAN contains two\ndifferent types of generators, i.e., keypoint-oriented generator and\nimage-oriented generator. Both of them are mutually connected in an end-to-end\nlearnable fashion and explicitly form three cycled sub-networks, i.e., one\nimage generation cycle and two keypoint generation cycles. Each cycle not only\naims at reconstructing the input domain, and also produces useful output\ninvolving in the generation of another cycle. By so doing, the cycles constrain\neach other implicitly, which provides complementary information from the two\ndifferent modalities and brings extra supervision across cycles, thus\nfacilitating more robust optimization of the whole network. Extensive\nexperimental results on two publicly available datasets, i.e., Radboud Faces\nand Market-1501, demonstrate that our approach is effective to generate more\nphoto-realistic images compared with state-of-the-art models.\n", "versions": [{"version": "v1", "created": "Fri, 2 Aug 2019 18:21:28 GMT"}, {"version": "v2", "created": "Sat, 14 Sep 2019 17:49:23 GMT"}, {"version": "v3", "created": "Thu, 16 Apr 2020 00:53:39 GMT"}], "update_date": "2020-04-17", "authors_parsed": [["Tang", "Hao", ""], ["Xu", "Dan", ""], ["Liu", "Gaowen", ""], ["Wang", "Wei", ""], ["Sebe", "Nicu", ""], ["Yan", "Yan", ""]]}, {"id": "1908.01070", "submitter": "Vivek Singh", "authors": "Brian Teixeira, Birgi Tamersoy, Vivek Singh, Ankur Kapoor", "title": "Adaloss: Adaptive Loss Function for Landmark Localization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Landmark localization is a challenging problem in computer vision with a\nmultitude of applications. Recent deep learning based methods have shown\nimproved results by regressing likelihood maps instead of regressing the\ncoordinates directly. However, setting the precision of these regression\ntargets during the training is a cumbersome process since it creates a\ntrade-off between trainability vs localization accuracy. Using precise targets\nintroduces a significant sampling bias and hence makes the training more\ndifficult, whereas using imprecise targets results in inaccurate landmark\ndetectors. In this paper, we introduce \"Adaloss\", an objective function that\nadapts itself during the training by updating the target precision based on the\ntraining statistics. This approach does not require setting problem-specific\nparameters and shows improved stability in training and better localization\naccuracy during inference. We demonstrate the effectiveness of our proposed\nmethod in three different applications of landmark localization: 1) the\nchallenging task of precisely detecting catheter tips in medical X-ray images,\n2) localizing surgical instruments in endoscopic images, and 3) localizing\nfacial features on in-the-wild images where we show state-of-the-art results on\nthe 300-W benchmark dataset.\n", "versions": [{"version": "v1", "created": "Fri, 2 Aug 2019 21:18:50 GMT"}], "update_date": "2019-08-06", "authors_parsed": [["Teixeira", "Brian", ""], ["Tamersoy", "Birgi", ""], ["Singh", "Vivek", ""], ["Kapoor", "Ankur", ""]]}, {"id": "1908.01091", "submitter": "Cuong Nguyen", "authors": "Cuong V. Nguyen, Alessandro Achille, Michael Lam, Tal Hassner, Vijay\n  Mahadevan, Stefano Soatto", "title": "Toward Understanding Catastrophic Forgetting in Continual Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the relationship between catastrophic forgetting and properties of\ntask sequences. In particular, given a sequence of tasks, we would like to\nunderstand which properties of this sequence influence the error rates of\ncontinual learning algorithms trained on the sequence. To this end, we propose\na new procedure that makes use of recent developments in task space modeling as\nwell as correlation analysis to specify and analyze the properties we are\ninterested in. As an application, we apply our procedure to study two\nproperties of a task sequence: (1) total complexity and (2) sequential\nheterogeneity. We show that error rates are strongly and positively correlated\nto a task sequence's total complexity for some state-of-the-art algorithms. We\nalso show that, surprisingly, the error rates have no or even negative\ncorrelations in some cases to sequential heterogeneity. Our findings suggest\ndirections for improving continual learning benchmarks and methods.\n", "versions": [{"version": "v1", "created": "Fri, 2 Aug 2019 23:30:35 GMT"}], "update_date": "2019-08-06", "authors_parsed": [["Nguyen", "Cuong V.", ""], ["Achille", "Alessandro", ""], ["Lam", "Michael", ""], ["Hassner", "Tal", ""], ["Mahadevan", "Vijay", ""], ["Soatto", "Stefano", ""]]}, {"id": "1908.01098", "submitter": "Petra Bevandi\\'c", "authors": "Petra Bevandi\\'c, Ivan Kre\\v{s}o, Marin Or\\v{s}i\\'c, Sini\\v{s}a\n  \\v{S}egvi\\'c", "title": "Simultaneous Semantic Segmentation and Outlier Detection in Presence of\n  Domain Shift", "comments": "Accepted to German Conference on Pattern Recognition 2019. 25 pages,\n  10 figures, 9 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent success on realistic road driving datasets has increased interest in\nexploring robust performance in real-world applications. One of the major\nunsolved problems is to identify image content which can not be reliably\nrecognized with a given inference engine. We therefore study approaches to\nrecover a dense outlier map alongside the primary task with a single forward\npass, by relying on shared convolutional features. We consider semantic\nsegmentation as the primary task and perform extensive validation on WildDash\nval (inliers), LSUN val (outliers), and pasted objects from Pascal VOC 2007\n(outliers). We achieve the best validation performance by training to\ndiscriminate inliers from pasted ImageNet-1k content, even though ImageNet-1k\ncontains many road-driving pixels, and, at least nominally, fails to account\nfor the full diversity of the visual world. The proposed two-head model\nperforms comparably to the C-way multi-class model trained to predict uniform\ndistribution in outliers, while outperforming several other validated\napproaches. We evaluate our best two models on the WildDash test dataset and\nset a new state of the art on the WildDash benchmark.\n", "versions": [{"version": "v1", "created": "Sat, 3 Aug 2019 00:49:00 GMT"}], "update_date": "2019-08-06", "authors_parsed": [["Bevandi\u0107", "Petra", ""], ["Kre\u0161o", "Ivan", ""], ["Or\u0161i\u0107", "Marin", ""], ["\u0160egvi\u0107", "Sini\u0161a", ""]]}, {"id": "1908.01104", "submitter": "Haofu Liao", "authors": "Haofu Liao, Wei-An Lin, S. Kevin Zhou and Jiebo Luo", "title": "ADN: Artifact Disentanglement Network for Unsupervised Metal Artifact\n  Reduction", "comments": "This is the extended version of arXiv:1906.01806. This paper is\n  accepted to IEEE Transactions on Medical Imaging", "journal-ref": null, "doi": "10.1109/TMI.2019.2933425", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Current deep neural network based approaches to computed tomography (CT)\nmetal artifact reduction (MAR) are supervised methods that rely on synthesized\nmetal artifacts for training. However, as synthesized data may not accurately\nsimulate the underlying physical mechanisms of CT imaging, the supervised\nmethods often generalize poorly to clinical applications. To address this\nproblem, we propose, to the best of our knowledge, the first unsupervised\nlearning approach to MAR. Specifically, we introduce a novel artifact\ndisentanglement network that disentangles the metal artifacts from CT images in\nthe latent space. It supports different forms of generations (artifact\nreduction, artifact transfer, and self-reconstruction, etc.) with specialized\nloss functions to obviate the need for supervision with synthesized data.\nExtensive experiments show that when applied to a synthesized dataset, our\nmethod addresses metal artifacts significantly better than the existing\nunsupervised models designed for natural image-to-image translation problems,\nand achieves comparable performance to existing supervised models for MAR. When\napplied to clinical datasets, our method demonstrates better generalization\nability over the supervised models. The source code of this paper is publicly\navailable at https://github.com/liaohaofu/adn.\n", "versions": [{"version": "v1", "created": "Sat, 3 Aug 2019 01:54:46 GMT"}, {"version": "v2", "created": "Wed, 7 Aug 2019 03:10:52 GMT"}, {"version": "v3", "created": "Thu, 8 Aug 2019 23:05:40 GMT"}, {"version": "v4", "created": "Thu, 28 Nov 2019 01:27:58 GMT"}], "update_date": "2019-12-02", "authors_parsed": [["Liao", "Haofu", ""], ["Lin", "Wei-An", ""], ["Zhou", "S. Kevin", ""], ["Luo", "Jiebo", ""]]}, {"id": "1908.01114", "submitter": "Tianlong Chen", "authors": "Tianlong Chen, Shaojin Ding, Jingyi Xie, Ye Yuan, Wuyang Chen, Yang\n  Yang, Zhou Ren, Zhangyang Wang", "title": "ABD-Net: Attentive but Diverse Person Re-Identification", "comments": "ICCV2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Attention mechanism has been shown to be effective for person\nre-identification (Re-ID). However, the learned attentive feature embeddings\nwhich are often not naturally diverse nor uncorrelated, will compromise the\nretrieval performance based on the Euclidean distance. We advocate that\nenforcing diversity could greatly complement the power of attention. To this\nend, we propose an Attentive but Diverse Network (ABD-Net), which seamlessly\nintegrates attention modules and diversity regularization throughout the entire\nnetwork, to learn features that are representative, robust, and more\ndiscriminative. Specifically, we introduce a pair of complementary attention\nmodules, focusing on channel aggregation and position awareness, respectively.\nFurthermore, a new efficient form of orthogonality constraint is derived to\nenforce orthogonality on both hidden activations and weights. Through careful\nablation studies, we verify that the proposed attentive and diverse terms each\ncontributes to the performance gains of ABD-Net. On three popular benchmarks,\nABD-Net consistently outperforms existing state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Sat, 3 Aug 2019 03:12:11 GMT"}, {"version": "v2", "created": "Wed, 7 Aug 2019 15:54:15 GMT"}, {"version": "v3", "created": "Fri, 9 Aug 2019 15:00:31 GMT"}], "update_date": "2019-08-12", "authors_parsed": [["Chen", "Tianlong", ""], ["Ding", "Shaojin", ""], ["Xie", "Jingyi", ""], ["Yuan", "Ye", ""], ["Chen", "Wuyang", ""], ["Yang", "Yang", ""], ["Ren", "Zhou", ""], ["Wang", "Zhangyang", ""]]}, {"id": "1908.01166", "submitter": "Menglei Zhang", "authors": "Menglei Zhang, Zhou Liu, Lei Yu", "title": "CRNet: Image Super-Resolution Using A Convolutional Sparse Coding\n  Inspired Network", "comments": "10 pages, 12 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional Sparse Coding (CSC) has been attracting more and more attention\nin recent years, for making full use of image global correlation to improve\nperformance on various computer vision applications. However, very few studies\nfocus on solving CSC based image Super-Resolution (SR) problem. As a\nconsequence, there is no significant progress in this area over a period of\ntime. In this paper, we exploit the natural connection between CSC and\nConvolutional Neural Networks (CNN) to address CSC based image SR.\nSpecifically, Convolutional Iterative Soft Thresholding Algorithm (CISTA) is\nintroduced to solve CSC problem and it can be implemented using CNN\narchitectures. Then we develop a novel CSC based SR framework analogy to the\ntraditional SC based SR methods. Two models inspired by this framework are\nproposed for pre-/post-upsampling SR, respectively. Compared with recent\nstate-of-the-art SR methods, both of our proposed models show superior\nperformance in terms of both quantitative and qualitative measurements.\n", "versions": [{"version": "v1", "created": "Sat, 3 Aug 2019 13:10:03 GMT"}], "update_date": "2019-08-06", "authors_parsed": [["Zhang", "Menglei", ""], ["Liu", "Zhou", ""], ["Yu", "Lei", ""]]}, {"id": "1908.01174", "submitter": "Xiaofeng Liu", "authors": "Xiaofeng Liu, Zhenhua Guo, Site Li, Lingsheng Kong, Ping Jia, Jane\n  You, B.V.K. Kumar", "title": "Permutation-invariant Feature Restructuring for Correlation-aware Image\n  Set-based Recognition", "comments": "Accepted to ICCV 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of comparing the similarity of image sets with\nvariable-quantity, quality and un-ordered heterogeneous images. We use feature\nrestructuring to exploit the correlations of both inner$\\&$inter-set images.\nSpecifically, the residual self-attention can effectively restructure the\nfeatures using the other features within a set to emphasize the discriminative\nimages and eliminate the redundancy. Then, a sparse/collaborative\nlearning-based dependency-guided representation scheme reconstructs the probe\nfeatures conditional to the gallery features in order to adaptively align the\ntwo sets. This enables our framework to be compatible with both verification\nand open-set identification. We show that the parametric self-attention network\nand non-parametric dictionary learning can be trained end-to-end by a unified\nalternative optimization scheme, and that the full framework is\npermutation-invariant. In the numerical experiments we conducted, our method\nachieves top performance on competitive image set/video-based face recognition\nand person re-identification benchmarks.\n", "versions": [{"version": "v1", "created": "Sat, 3 Aug 2019 13:39:41 GMT"}], "update_date": "2019-08-06", "authors_parsed": [["Liu", "Xiaofeng", ""], ["Guo", "Zhenhua", ""], ["Li", "Site", ""], ["Kong", "Lingsheng", ""], ["Jia", "Ping", ""], ["You", "Jane", ""], ["Kumar", "B. V. K.", ""]]}, {"id": "1908.01180", "submitter": "Yafei Song", "authors": "Yafei Song, Di Zhu, Jia Li, Yonghong Tian, Mingyang Li", "title": "Learning Local Feature Descriptor with Motion Attribute for Vision-based\n  Localization", "comments": "This paper will be presented on IROS19", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, camera-based localization has been widely used for robotic\napplications, and most proposed algorithms rely on local features extracted\nfrom recorded images. For better performance, the features used for open-loop\nlocalization are required to be short-term globally static, and the ones used\nfor re-localization or loop closure detection need to be long-term static.\nTherefore, the motion attribute of a local feature point could be exploited to\nimprove localization performance, e.g., the feature points extracted from\nmoving persons or vehicles can be excluded from these systems due to their\nunsteadiness. In this paper, we design a fully convolutional network (FCN),\nnamed MD-Net, to perform motion attribute estimation and feature description\nsimultaneously. MD-Net has a shared backbone network to extract features from\nthe input image and two network branches to complete each sub-task. With\nMD-Net, we can obtain the motion attribute while avoiding increasing much more\ncomputation. Experimental results demonstrate that the proposed method can\nlearn distinct local feature descriptor along with motion attribute only using\nan FCN, by outperforming competing methods by a wide margin. We also show that\nthe proposed algorithm can be integrated into a vision-based localization\nalgorithm to improve estimation accuracy significantly.\n", "versions": [{"version": "v1", "created": "Sat, 3 Aug 2019 14:05:09 GMT"}, {"version": "v2", "created": "Wed, 7 Aug 2019 01:18:32 GMT"}], "update_date": "2019-08-08", "authors_parsed": [["Song", "Yafei", ""], ["Zhu", "Di", ""], ["Li", "Jia", ""], ["Tian", "Yonghong", ""], ["Li", "Mingyang", ""]]}, {"id": "1908.01189", "submitter": "Hazan Anayurt", "authors": "Hazan Anayurt, Sezai Artun Ozyegin, Ulfet Cetin, Utku Aktas, Sinan\n  Kalkan", "title": "Searching for Ambiguous Objects in Videos using Relational Referring\n  Expressions", "comments": "BMVC 2019 camera ready", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Humans frequently use referring (identifying) expressions to refer to\nobjects. Especially in ambiguous settings, humans prefer expressions (called\nrelational referring expressions) that describe an object with respect to a\ndistinguishing, unique object. Unlike studies on video object search using\nreferring expressions, in this paper, our focus is on (i) relational referring\nexpressions in highly ambiguous settings, and (ii) methods that can both\ngenerate and comprehend a referring expression. For this goal, we first\nintroduce a new dataset for video object search with referring expressions that\nincludes numerous copies of the objects, making it difficult to use\nnon-relational expressions. Moreover, we train two baseline deep networks on\nthis dataset, which show promising results. Finally, we propose a deep\nattention network that significantly outperforms the baselines on our dataset.\nThe dataset and the codes are available at\nhttps://github.com/hazananayurt/viref.\n", "versions": [{"version": "v1", "created": "Sat, 3 Aug 2019 15:06:14 GMT"}, {"version": "v2", "created": "Tue, 20 Aug 2019 07:18:54 GMT"}], "update_date": "2019-08-21", "authors_parsed": [["Anayurt", "Hazan", ""], ["Ozyegin", "Sezai Artun", ""], ["Cetin", "Ulfet", ""], ["Aktas", "Utku", ""], ["Kalkan", "Sinan", ""]]}, {"id": "1908.01210", "submitter": "Jun Gao", "authors": "Wenzheng Chen, Jun Gao, Huan Ling, Edward J. Smith, Jaakko Lehtinen,\n  Alec Jacobson, Sanja Fidler", "title": "Learning to Predict 3D Objects with an Interpolation-based\n  Differentiable Renderer", "comments": "Accepted to NeurIPS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Many machine learning models operate on images, but ignore the fact that\nimages are 2D projections formed by 3D geometry interacting with light, in a\nprocess called rendering. Enabling ML models to understand image formation\nmight be key for generalization. However, due to an essential rasterization\nstep involving discrete assignment operations, rendering pipelines are\nnon-differentiable and thus largely inaccessible to gradient-based ML\ntechniques. In this paper, we present {\\emph DIB-R}, a differentiable rendering\nframework which allows gradients to be analytically computed for all pixels in\nan image. Key to our approach is to view foreground rasterization as a weighted\ninterpolation of local properties and background rasterization as a\ndistance-based aggregation of global geometry. Our approach allows for accurate\noptimization over vertex positions, colors, normals, light directions and\ntexture coordinates through a variety of lighting models. We showcase our\napproach in two ML applications: single-image 3D object prediction, and 3D\ntextured object generation, both trained using exclusively using 2D\nsupervision. Our project website is: https://nv-tlabs.github.io/DIB-R/\n", "versions": [{"version": "v1", "created": "Sat, 3 Aug 2019 18:05:41 GMT"}, {"version": "v2", "created": "Thu, 21 Nov 2019 17:54:30 GMT"}], "update_date": "2019-11-22", "authors_parsed": [["Chen", "Wenzheng", ""], ["Gao", "Jun", ""], ["Ling", "Huan", ""], ["Smith", "Edward J.", ""], ["Lehtinen", "Jaakko", ""], ["Jacobson", "Alec", ""], ["Fidler", "Sanja", ""]]}, {"id": "1908.01224", "submitter": "Daniel Omeiza A", "authors": "Daniel Omeiza, Skyler Speakman, Celia Cintas, Komminist Weldermariam", "title": "Smooth Grad-CAM++: An Enhanced Inference Level Visualization Technique\n  for Deep Convolutional Neural Network Models", "comments": "Accepted in the Intelligent Systems Conference 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gaining insight into how deep convolutional neural network models perform\nimage classification and how to explain their outputs have been a concern to\ncomputer vision researchers and decision makers. These deep models are often\nreferred to as black box due to low comprehension of their internal workings.\nAs an effort to developing explainable deep learning models, several methods\nhave been proposed such as finding gradients of class output with respect to\ninput image (sensitivity maps), class activation map (CAM), and Gradient based\nClass Activation Maps (Grad-CAM). These methods under perform when localizing\nmultiple occurrences of the same class and do not work for all CNNs. In\naddition, Grad-CAM does not capture the entire object in completeness when used\non single object images, this affect performance on recognition tasks. With the\nintention to create an enhanced visual explanation in terms of visual\nsharpness, object localization and explaining multiple occurrences of objects\nin a single image, we present Smooth Grad-CAM++ \\footnote{Simple demo:\nhttp://35.238.22.135:5000/}, a technique that combines methods from two other\nrecent techniques---SMOOTHGRAD and Grad-CAM++. Our Smooth Grad-CAM++ technique\nprovides the capability of either visualizing a layer, subset of feature maps,\nor subset of neurons within a feature map at each instance at the inference\nlevel (model prediction process). After experimenting with few images, Smooth\nGrad-CAM++ produced more visually sharp maps with better localization of\nobjects in the given input images when compared with other methods.\n", "versions": [{"version": "v1", "created": "Sat, 3 Aug 2019 20:09:40 GMT"}], "update_date": "2019-08-06", "authors_parsed": [["Omeiza", "Daniel", ""], ["Speakman", "Skyler", ""], ["Cintas", "Celia", ""], ["Weldermariam", "Komminist", ""]]}, {"id": "1908.01238", "submitter": "Jie Tang", "authors": "Jie Tang, Fei-Peng Tian, Wei Feng, Jian Li and Ping Tan", "title": "Learning Guided Convolutional Network for Depth Completion", "comments": "Submitted to the IEEE Transactions on Image Processing (TIP)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dense depth perception is critical for autonomous driving and other robotics\napplications. However, modern LiDAR sensors only provide sparse depth\nmeasurement. It is thus necessary to complete the sparse LiDAR data, where a\nsynchronized guidance RGB image is often used to facilitate this completion.\nMany neural networks have been designed for this task. However, they often\nna\\\"{\\i}vely fuse the LiDAR data and RGB image information by performing\nfeature concatenation or element-wise addition. Inspired by the guided image\nfiltering, we design a novel guided network to predict kernel weights from the\nguidance image. These predicted kernels are then applied to extract the depth\nimage features. In this way, our network generates content-dependent and\nspatially-variant kernels for multi-modal feature fusion. Dynamically generated\nspatially-variant kernels could lead to prohibitive GPU memory consumption and\ncomputation overhead. We further design a convolution factorization to reduce\ncomputation and memory consumption. The GPU memory reduction makes it possible\nfor feature fusion to work in multi-stage scheme. We conduct comprehensive\nexperiments to verify our method on real-world outdoor, indoor and synthetic\ndatasets. Our method produces strong results. It outperforms state-of-the-art\nmethods on the NYUv2 dataset and ranks 1st on the KITTI depth completion\nbenchmark at the time of submission. It also presents strong generalization\ncapability under different 3D point densities, various lighting and weather\nconditions as well as cross-dataset evaluations. The code will be released for\nreproduction.\n", "versions": [{"version": "v1", "created": "Sat, 3 Aug 2019 22:06:34 GMT"}], "update_date": "2019-08-06", "authors_parsed": [["Tang", "Jie", ""], ["Tian", "Fei-Peng", ""], ["Feng", "Wei", ""], ["Li", "Jian", ""], ["Tan", "Ping", ""]]}, {"id": "1908.01242", "submitter": "Vinay Prabhu", "authors": "Vinay Uday Prabhu", "title": "Kannada-MNIST: A new handwritten digits dataset for the Kannada language", "comments": "The companion github repository for this paper is :\n  https://github.com/vinayprabhu/Kannada_MNIST", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we disseminate a new handwritten digits-dataset, termed\nKannada-MNIST, for the Kannada script, that can potentially serve as a direct\ndrop-in replacement for the original MNIST dataset. In addition to this\ndataset, we disseminate an additional real world handwritten dataset (with\n$10k$ images), which we term as the Dig-MNIST dataset that can serve as an\nout-of-domain test dataset. We also duly open source all the code as well as\nthe raw scanned images along with the scanner settings so that researchers who\nwant to try out different signal processing pipelines can perform end-to-end\ncomparisons. We provide high level morphological comparisons with the MNIST\ndataset and provide baselines accuracies for the dataset disseminated. The\ninitial baselines obtained using an oft-used CNN architecture ($96.8\\%$ for the\nmain test-set and $76.1\\%$ for the Dig-MNIST test-set) indicate that these\ndatasets do provide a sterner challenge with regards to generalizability than\nMNIST or the KMNIST datasets. We also hope this dissemination will spur the\ncreation of similar datasets for all the languages that use different symbols\nfor the numeral digits.\n", "versions": [{"version": "v1", "created": "Sat, 3 Aug 2019 22:33:52 GMT"}], "update_date": "2019-08-06", "authors_parsed": [["Prabhu", "Vinay Uday", ""]]}, {"id": "1908.01259", "submitter": "Tianfu Wu", "authors": "Xilai Li, Wei Sun and Tianfu Wu", "title": "Attentive Normalization", "comments": "ECCV2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In state-of-the-art deep neural networks, both feature normalization and\nfeature attention have become ubiquitous. % with significant performance\nimprovement shown in a vast amount of tasks. They are usually studied as\nseparate modules, however. In this paper, we propose a light-weight integration\nbetween the two schema and present Attentive Normalization (AN). Instead of\nlearning a single affine transformation, AN learns a mixture of affine\ntransformations and utilizes their weighted-sum as the final affine\ntransformation applied to re-calibrate features in an instance-specific way.\nThe weights are learned by leveraging channel-wise feature attention. In\nexperiments, we test the proposed AN using four representative neural\narchitectures in the ImageNet-1000 classification benchmark and the MS-COCO\n2017 object detection and instance segmentation benchmark. AN obtains\nconsistent performance improvement for different neural architectures in both\nbenchmarks with absolute increase of top-1 accuracy in ImageNet-1000 between\n0.5\\% and 2.7\\%, and absolute increase up to 1.8\\% and 2.2\\% for bounding box\nand mask AP in MS-COCO respectively. We observe that the proposed AN provides a\nstrong alternative to the widely used Squeeze-and-Excitation (SE) module. The\nsource codes are publicly available at https://github.com/iVMCL/AOGNet-v2 (the\nImageNet Classification Repo) and\nhttps://github.com/iVMCL/AttentiveNorm\\_Detection (the MS-COCO Detection and\nSegmentation Repo).\n", "versions": [{"version": "v1", "created": "Sun, 4 Aug 2019 02:17:34 GMT"}, {"version": "v2", "created": "Sat, 23 Nov 2019 03:43:35 GMT"}, {"version": "v3", "created": "Thu, 25 Mar 2021 17:16:13 GMT"}], "update_date": "2021-03-26", "authors_parsed": [["Li", "Xilai", ""], ["Sun", "Wei", ""], ["Wu", "Tianfu", ""]]}, {"id": "1908.01266", "submitter": "Zhao Zhang", "authors": "Zhao Zhang, Jiahuan Ren, Sheng Li, Richang Hong, Zhengjun Zha, Meng\n  Wang", "title": "Robust Subspace Discovery by Block-diagonal Adaptive\n  Locality-constrained Representation", "comments": "accepted by ACM Multimedia 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel and unsupervised representation learning model, i.e.,\nRobust Block-Diagonal Adaptive Locality-constrained Latent Representation\n(rBDLR). rBDLR is able to recover multi-subspace structures and extract the\nadaptive locality-preserving salient features jointly. Leveraging on the\nFrobenius-norm based latent low-rank representation model, rBDLR jointly learns\nthe coding coefficients and salient features, and improves the results by\nenhancing the robustness to outliers and errors in given data, preserving local\ninformation of salient features adaptively and ensuring the block-diagonal\nstructures of the coefficients. To improve the robustness, we perform the\nlatent representation and adaptive weighting in a recovered clean data space.\nTo force the coefficients to be block-diagonal, we perform auto-weighting by\nminimizing the reconstruction error based on salient features, constrained\nusing a block-diagonal regularizer. This ensures that a strict block-diagonal\nweight matrix can be obtained and salient features will possess the adaptive\nlocality preserving ability. By minimizing the difference between the\ncoefficient and weights matrices, we can obtain a block-diagonal coefficients\nmatrix and it can also propagate and exchange useful information between\nsalient features and coefficients. Extensive results demonstrate the\nsuperiority of rBDLR over other state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Sun, 4 Aug 2019 03:43:00 GMT"}], "update_date": "2019-08-06", "authors_parsed": [["Zhang", "Zhao", ""], ["Ren", "Jiahuan", ""], ["Li", "Sheng", ""], ["Hong", "Richang", ""], ["Zha", "Zhengjun", ""], ["Wang", "Meng", ""]]}, {"id": "1908.01279", "submitter": "Dmitry Konovalov", "authors": "Dina B. Efremova, Dmitry A. Konovalov, Thanongchai Siriapisith,\n  Worapan Kusakunniran, Peter Haddawy", "title": "Automatic segmentation of kidney and liver tumors in CT images", "comments": "Method description manuscript for our test predictions for the 2019\n  Kidney Tumor Segmentation Challenge, https://kits19.grand-challenge.org/home/", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic segmentation of hepatic lesions in computed tomography (CT) images\nis a challenging task to perform due to heterogeneous, diffusive shape of\ntumors and complex background. To address the problem more and more researchers\nrely on assistance of deep convolutional neural networks (CNN) with 2D or 3D\ntype architecture that have proven to be effective in a wide range of computer\nvision tasks, including medical image processing. In this technical report, we\ncarry out research focused on more careful approach to the process of learning\nrather than on complex architecture of the CNN. We have chosen MICCAI 2017 LiTS\ndataset for training process and the public 3DIRCADb dataset for validation of\nour method. The proposed algorithm reached DICE score 78.8% on the 3DIRCADb\ndataset. The described method was then applied to the 2019 Kidney Tumor\nSegmentation (KiTS-2019) challenge, where our single submission achieved 96.38%\nfor kidney and 67.38% for tumor Dice scores.\n", "versions": [{"version": "v1", "created": "Sun, 4 Aug 2019 06:31:02 GMT"}, {"version": "v2", "created": "Mon, 16 Sep 2019 23:46:24 GMT"}], "update_date": "2019-09-18", "authors_parsed": [["Efremova", "Dina B.", ""], ["Konovalov", "Dmitry A.", ""], ["Siriapisith", "Thanongchai", ""], ["Kusakunniran", "Worapan", ""], ["Haddawy", "Peter", ""]]}, {"id": "1908.01281", "submitter": "Zhongdao Wang", "authors": "Lanqing He, Zhongdao Wang, Yali Li, Shengjin Wang", "title": "Softmax Dissection: Towards Understanding Intra- and Inter-class\n  Objective for Embedding Learning", "comments": "Accepted to AAAI-2020, Oral presentation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The softmax loss and its variants are widely used as objectives for embedding\nlearning, especially in applications like face recognition. However, the intra-\nand inter-class objectives in the softmax loss are entangled, therefore a\nwell-optimized inter-class objective leads to relaxation on the intra-class\nobjective, and vice versa. In this paper, we propose to dissect the softmax\nloss into independent intra- and inter-class objective (D-Softmax). With\nD-Softmax as objective, we can have a clear understanding of both the intra-\nand inter-class objective, therefore it is straightforward to tune each part to\nthe best state. Furthermore, we find the computation of the inter-class\nobjective is redundant and propose two sampling-based variants of D-Softmax to\nreduce the computation cost. Training with regular-scale data, experiments in\nface verification show D-Softmax is favorably comparable to existing losses\nsuch as SphereFace and ArcFace. Training with massive-scale data, experiments\nshow the fast variants of D-Softmax significantly accelerates the training\nprocess (such as 64x) with only a minor sacrifice in performance, outperforming\nexisting acceleration methods of softmax in terms of both performance and\nefficiency.\n", "versions": [{"version": "v1", "created": "Sun, 4 Aug 2019 06:50:20 GMT"}, {"version": "v2", "created": "Wed, 12 Feb 2020 08:03:55 GMT"}], "update_date": "2020-02-13", "authors_parsed": [["He", "Lanqing", ""], ["Wang", "Zhongdao", ""], ["Li", "Yali", ""], ["Wang", "Shengjin", ""]]}, {"id": "1908.01287", "submitter": "Xuehang Zheng", "authors": "Il Yong Chun, Xuehang Zheng, Yong Long, Jeffrey A. Fessler", "title": "BCD-Net for Low-dose CT Reconstruction: Acceleration, Convergence, and\n  Generalization", "comments": "Accepted to MICCAI 2019, and the authors indicated by asterisks (*)\n  equally contributed to this work", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Obtaining accurate and reliable images from low-dose computed tomography (CT)\nis challenging. Regression convolutional neural network (CNN) models that are\nlearned from training data are increasingly gaining attention in low-dose CT\nreconstruction. This paper modifies the architecture of an iterative regression\nCNN, BCD-Net, for fast, stable, and accurate low-dose CT reconstruction, and\npresents the convergence property of the modified BCD-Net. Numerical results\nwith phantom data show that applying faster numerical solvers to model-based\nimage reconstruction (MBIR) modules of BCD-Net leads to faster and more\naccurate BCD-Net; BCD-Net significantly improves the reconstruction accuracy,\ncompared to the state-of-the-art MBIR method using learned transforms; BCD-Net\nachieves better image quality, compared to a state-of-the-art iterative NN\narchitecture, ADMM-Net. Numerical results with clinical data show that BCD-Net\ngeneralizes significantly better than a state-of-the-art deep (non-iterative)\nregression NN, FBPConvNet, that lacks MBIR modules.\n", "versions": [{"version": "v1", "created": "Sun, 4 Aug 2019 07:10:24 GMT"}], "update_date": "2019-08-06", "authors_parsed": [["Chun", "Il Yong", ""], ["Zheng", "Xuehang", ""], ["Long", "Yong", ""], ["Fessler", "Jeffrey A.", ""]]}, {"id": "1908.01293", "submitter": "Qunjie Zhou", "authors": "Qunjie Zhou, Torsten Sattler, Marc Pollefeys, Laura Leal-Taixe", "title": "To Learn or Not to Learn: Visual Localization from Essential Matrices", "comments": "Accepted to ICRA 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual localization is the problem of estimating a camera within a scene and\na key component in computer vision applications such as self-driving cars and\nMixed Reality. State-of-the-art approaches for accurate visual localization use\nscene-specific representations, resulting in the overhead of constructing these\nmodels when applying the techniques to new scenes. Recently, deep\nlearning-based approaches based on relative pose estimation have been proposed,\ncarrying the promise of easily adapting to new scenes. However, it has been\nshown such approaches are currently significantly less accurate than\nstate-of-the-art approaches. In this paper, we are interested in analyzing this\nbehavior. To this end, we propose a novel framework for visual localization\nfrom relative poses. Using a classical feature-based approach within this\nframework, we show state-of-the-art performance. Replacing the classical\napproach with learned alternatives at various levels, we then identify the\nreasons for why deep learned approaches do not perform well. Based on our\nanalysis, we make recommendations for future work.\n", "versions": [{"version": "v1", "created": "Sun, 4 Aug 2019 08:23:44 GMT"}, {"version": "v2", "created": "Mon, 9 Mar 2020 13:14:40 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Zhou", "Qunjie", ""], ["Sattler", "Torsten", ""], ["Pollefeys", "Marc", ""], ["Leal-Taixe", "Laura", ""]]}, {"id": "1908.01300", "submitter": "Sai Raam Venkatraman", "authors": "Sairaam Venkatraman, S. Balasubramanian, R. Raghunatha Sarma", "title": "Building Deep, Equivariant Capsule Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Capsule networks are constrained by the parameter-expensive nature of their\nlayers, and the general lack of provable equivariance guarantees. We present a\nvariation of capsule networks that aims to remedy this. We identify that\nlearning all pair-wise part-whole relationships between capsules of successive\nlayers is inefficient. Further, we also realise that the choice of prediction\nnetworks and the routing mechanism are both key to equivariance. Based on\nthese, we propose an alternative framework for capsule networks that learns to\nprojectively encode the manifold of pose-variations, termed the\nspace-of-variation (SOV), for every capsule-type of each layer. This is done\nusing a trainable, equivariant function defined over a grid of\ngroup-transformations. Thus, the prediction-phase of routing involves\nprojection into the SOV of a deeper capsule using the corresponding function.\nAs a specific instantiation of this idea, and also in order to reap the\nbenefits of increased parameter-sharing, we use type-homogeneous\ngroup-equivariant convolutions of shallower capsules in this phase. We also\nintroduce an equivariant routing mechanism based on degree-centrality. We show\nthat this particular instance of our general model is equivariant, and hence\npreserves the compositional representation of an input under transformations.\nWe conduct several experiments on standard object-classification datasets that\nshowcase the increased transformation-robustness, as well as general\nperformance, of our model to several capsule baselines.\n", "versions": [{"version": "v1", "created": "Sun, 4 Aug 2019 09:14:29 GMT"}, {"version": "v2", "created": "Wed, 25 Sep 2019 15:14:40 GMT"}, {"version": "v3", "created": "Thu, 26 Sep 2019 04:26:10 GMT"}], "update_date": "2019-09-27", "authors_parsed": [["Venkatraman", "Sairaam", ""], ["Balasubramanian", "S.", ""], ["Sarma", "R. Raghunatha", ""]]}, {"id": "1908.01301", "submitter": "Zhongdao Wang", "authors": "Yixuan Liu, Yuwang Wang, Shengjin Wang", "title": "Adversarial View-Consistent Learning for Monocular Depth Estimation", "comments": "BMVC 2019 Spotlight", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the problem of Monocular Depth Estimation (MDE).\nExisting approaches on MDE usually model it as a pixel-level regression\nproblem, ignoring the underlying geometry property. We empirically find this\nmay result in sub-optimal solution: while the predicted depth map presents\nsmall loss value in one specific view, it may exhibit large loss if viewed in\ndifferent directions. In this paper, inspired by multi-view stereo (MVS), we\npropose an Adversarial View-Consistent Learning (AVCL) framework to force the\nestimated depth map to be all reasonable viewed from multiple views. To this\nend, we first design a differentiable depth map warping operation, which is\nend-to-end trainable, and then propose a pose generator to generate novel views\nfor a given image in an adversarial manner. Collaborating with the\ndifferentiable depth map warping operation, the pose generator encourages the\ndepth estimation network to learn from hard views, hence produce\nview-consistent depth maps . We evaluate our method on NYU Depth V2 dataset and\nthe experimental results show promising performance gain upon state-of-the-art\nMDE approaches.\n", "versions": [{"version": "v1", "created": "Sun, 4 Aug 2019 09:37:24 GMT"}], "update_date": "2019-08-06", "authors_parsed": [["Liu", "Yixuan", ""], ["Wang", "Yuwang", ""], ["Wang", "Shengjin", ""]]}, {"id": "1908.01308", "submitter": "Gengyun Jia", "authors": "Gengyun Jia, Peipei Li, and Ran He", "title": "Theme Aware Aesthetic Distribution Prediction with Full Resolution\n  Photos", "comments": "11 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Aesthetic quality assessment (AQA) of photos is a challenging task due to the\nsubjective and diverse factors in human assessment process. Nowadays, it is\ncommon to tackle AQA with deep neural networks (DNNs) for their superior\nperformance on modeling such complex relations. However, traditional DNNs\nrequire fix-sized inputs, and resizing various inputs to a uniform size may\nsignificantly change their aesthetic features. Such transformations lead to the\nmismatches between photos and their aesthetic evaluations. Existing methods\nusually adopt two solutions for it. Some methods directly crop fix-sized\npatches from the inputs. The others alternately capture the aesthetic features\nfrom pre-defined multi-size inputs by inserting adaptive pooling or removing\nfully connected layers. However, the former destroys the global structures and\nlayout information, which are crucial in most situations. The latter has to\nresize images into several pre-defined sizes, which is not enough to reflect\nthe diversity of image sizes, and the aesthetic features are still destroyed.\nTo address this issue, we propose a simple and effective method that can handle\nthe arbitrary sizes of batch inputs to achieve AQA on the full resolution\nimages by combining image padding with ROI (region of interest) pooling.\nPadding keeps inputs of the same size, while ROI pooling cuts off the forward\npropagation of features on padding regions, thus eliminates the side effects of\npadding. Besides, we observe that the same image may receive different scores\nunder different themes, which we call the theme criterion bias. However,\nprevious works only focus on the aesthetic features of the images and ignore\nthe criterion bias brought by their themes. In this paper, we introduce the\ntheme information and propose a theme aware model. Extensive experiments prove\nthe effectiveness of the proposed method over the state-of-the-arts.\n", "versions": [{"version": "v1", "created": "Sun, 4 Aug 2019 10:03:38 GMT"}], "update_date": "2019-08-06", "authors_parsed": [["Jia", "Gengyun", ""], ["Li", "Peipei", ""], ["He", "Ran", ""]]}, {"id": "1908.01311", "submitter": "Chenyang Lei", "authors": "Chenyang Lei and Qifeng Chen", "title": "Fully Automatic Video Colorization with Self-Regularization and\n  Diversity", "comments": "Published at the Computer Vision and Pattern Recognition (CVPR), 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a fully automatic approach to video colorization with\nself-regularization and diversity. Our model contains a colorization network\nfor video frame colorization and a refinement network for spatiotemporal color\nrefinement. Without any labeled data, both networks can be trained with\nself-regularized losses defined in bilateral and temporal space. The bilateral\nloss enforces color consistency between neighboring pixels in a bilateral space\nand the temporal loss imposes constraints between corresponding pixels in two\nnearby frames. While video colorization is a multi-modal problem, our method\nuses a perceptual loss with diversity to differentiate various modes in the\nsolution space. Perceptual experiments demonstrate that our approach\noutperforms state-of-the-art approaches on fully automatic video colorization.\nThe results are shown in the supplementary video at\nhttps://youtu.be/Y15uv2jnK-4\n", "versions": [{"version": "v1", "created": "Sun, 4 Aug 2019 10:22:36 GMT"}], "update_date": "2019-08-06", "authors_parsed": [["Lei", "Chenyang", ""], ["Chen", "Qifeng", ""]]}, {"id": "1908.01313", "submitter": "Huaxi Huang", "authors": "Huaxi Huang, Junjie Zhang, Jian Zhang, Jingsong Xu, Qiang Wu", "title": "Low-Rank Pairwise Alignment Bilinear Network For Few-Shot Fine-Grained\n  Image Classification", "comments": null, "journal-ref": null, "doi": "10.1109/TMM.2020.3001510", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks have demonstrated advanced abilities on various visual\nclassification tasks, which heavily rely on the large-scale training samples\nwith annotated ground-truth. However, it is unrealistic always to require such\nannotation in real-world applications. Recently, Few-Shot learning (FS), as an\nattempt to address the shortage of training samples, has made significant\nprogress in generic classification tasks. Nonetheless, it is still challenging\nfor current FS models to distinguish the subtle differences between\nfine-grained categories given limited training data. To filling the\nclassification gap, in this paper, we address the Few-Shot Fine-Grained (FSFG)\nclassification problem, which focuses on tackling the fine-grained\nclassification under the challenging few-shot learning setting. A novel\nlow-rank pairwise bilinear pooling operation is proposed to capture the nuanced\ndifferences between the support and query images for learning an effective\ndistance metric. Moreover, a feature alignment layer is designed to match the\nsupport image features with query ones before the comparison. We name the\nproposed model Low-Rank Pairwise Alignment Bilinear Network (LRPABN), which is\ntrained in an end-to-end fashion. Comprehensive experimental results on four\nwidely used fine-grained classification datasets demonstrate that our LRPABN\nmodel achieves the superior performances compared to state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Sun, 4 Aug 2019 10:32:31 GMT"}, {"version": "v2", "created": "Mon, 20 Jan 2020 10:05:18 GMT"}, {"version": "v3", "created": "Thu, 28 May 2020 03:43:34 GMT"}], "update_date": "2021-03-11", "authors_parsed": [["Huang", "Huaxi", ""], ["Zhang", "Junjie", ""], ["Zhang", "Jian", ""], ["Xu", "Jingsong", ""], ["Wu", "Qiang", ""]]}, {"id": "1908.01314", "submitter": "Xiangxiang Chu", "authors": "Xiangxiang Chu, Bo Zhang, Ruijun Xu", "title": "MoGA: Searching Beyond MobileNetV3", "comments": "Accepted by ICASSP2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The evolution of MobileNets has laid a solid foundation for neural network\napplications on mobile end. With the latest MobileNetV3, neural architecture\nsearch again claimed its supremacy in network design. Unfortunately, till today\nall mobile methods mainly focus on CPU latencies instead of GPU, the latter,\nhowever, is much preferred in practice for it has faster speed, lower overhead\nand less interference. Bearing the target hardware in mind, we propose the\nfirst Mobile GPU-Aware (MoGA) neural architecture search in order to be\nprecisely tailored for real-world applications. Further, the ultimate objective\nto devise a mobile network lies in achieving better performance by maximizing\nthe utilization of bounded resources. Urging higher capability while\nrestraining time consumption is not reconcilable. We alleviate the tension by\nweighted evolution techniques. Moreover, we encourage increasing the number of\nparameters for higher representational power. With 200x fewer GPU days than\nMnasNet, we obtain a series of models that outperform MobileNetV3 under the\nsimilar latency constraints, i.e., MoGA-A achieves 75.9% top-1 accuracy on\nImageNet, MoGA-B meets 75.5% which costs only 0.5 ms more on mobile GPU. MoGA-C\nbest attests GPU-awareness by reaching 75.3% and being slower on CPU but faster\non GPU.The models and test code is made available here\nhttps://github.com/xiaomi-automl/MoGA.\n", "versions": [{"version": "v1", "created": "Sun, 4 Aug 2019 10:40:04 GMT"}, {"version": "v2", "created": "Fri, 13 Sep 2019 15:22:20 GMT"}, {"version": "v3", "created": "Mon, 14 Oct 2019 07:24:44 GMT"}, {"version": "v4", "created": "Tue, 3 Mar 2020 04:11:41 GMT"}], "update_date": "2020-03-04", "authors_parsed": [["Chu", "Xiangxiang", ""], ["Zhang", "Bo", ""], ["Xu", "Ruijun", ""]]}, {"id": "1908.01323", "submitter": "Chengjiang Long", "authors": "Bin Ding, Chengjiang Long, Ling Zhang, Chunxia Xiao", "title": "ARGAN: Attentive Recurrent Generative Adversarial Network for Shadow\n  Detection and Removal", "comments": "The paper was accepted to the IEEE / CVF International Conference on\n  Computer Vision (ICCV) 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose an attentive recurrent generative adversarial\nnetwork (ARGAN) to detect and remove shadows in an image. The generator\nconsists of multiple progressive steps. At each step a shadow attention\ndetector is firstly exploited to generate an attention map which specifies\nshadow regions in the input image.Given the attention map, a negative residual\nby a shadow remover encoder will recover a shadow-lighter or even a shadow-free\nimage. A discriminator is designed to classify whether the output image in the\nlast progressive step is real or fake. Moreover, ARGAN is suitable to be\ntrained with a semi-supervised strategy to make full use of sufficient\nunsupervised data. The experiments on four public datasets have demonstrated\nthat our ARGAN is robust to detect both simple and complex shadows and to\nproduce more realistic shadow removal results. It outperforms the\nstate-of-the-art methods, especially in detail of recovering shadow areas.\n", "versions": [{"version": "v1", "created": "Sun, 4 Aug 2019 12:18:12 GMT"}], "update_date": "2019-08-06", "authors_parsed": [["Ding", "Bin", ""], ["Long", "Chengjiang", ""], ["Zhang", "Ling", ""], ["Xiao", "Chunxia", ""]]}, {"id": "1908.01341", "submitter": "Zhenmei Shi", "authors": "Zhaoyang Yang, Zhenmei Shi, Xiaoyong Shen, Yu-Wing Tai", "title": "SF-Net: Structured Feature Network for Continuous Sign Language\n  Recognition", "comments": "12 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Continuous sign language recognition (SLR) aims to translate a signing\nsequence into a sentence. It is very challenging as sign language is rich in\nvocabulary, while many among them contain similar gestures and motions.\nMoreover, it is weakly supervised as the alignment of signing glosses is not\navailable. In this paper, we propose Structured Feature Network (SF-Net) to\naddress these challenges by effectively learn multiple levels of semantic\ninformation in the data. The proposed SF-Net extracts features in a structured\nmanner and gradually encodes information at the frame level, the gloss level\nand the sentence level into the feature representation. The proposed SF-Net can\nbe trained end-to-end without the help of other models or pre-training. We\ntested the proposed SF-Net on two large scale public SLR datasets collected\nfrom different continuous SLR scenarios. Results show that the proposed SF-Net\nclearly outperforms previous sequence level supervision based methods in terms\nof both accuracy and adaptability.\n", "versions": [{"version": "v1", "created": "Sun, 4 Aug 2019 13:34:41 GMT"}], "update_date": "2019-08-06", "authors_parsed": [["Yang", "Zhaoyang", ""], ["Shi", "Zhenmei", ""], ["Shen", "Xiaoyong", ""], ["Tai", "Yu-Wing", ""]]}, {"id": "1908.01351", "submitter": "Atri Mandal", "authors": "Atri Mandal, Shivali Agarwal, Nikhil Malhotra, Giriprasad Sridhara,\n  Anupama Ray, Daivik Swarup", "title": "Improving IT Support by Enhancing Incident Management Process with\n  Multi-modal Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  IT support services industry is going through a major transformation with AI\nbecoming commonplace. There has been a lot of effort in the direction of\nautomation at every human touchpoint in the IT support processes. Incident\nmanagement is one such process which has been a beacon process for AI based\nautomation. The vision is to automate the process from the time an\nincident/ticket arrives till it is resolved and closed. While text is the\nprimary mode of communicating the incidents, there has been a growing trend of\nusing alternate modalities like image to communicate the problem. A large\nfraction of IT support tickets today contain attached image data in the form of\nscreenshots, log messages, invoices and so on. These attachments help in better\nexplanation of the problem which aids in faster resolution. Anybody who aspires\nto provide AI based IT support, it is essential to build systems which can\nhandle multi-modal content. In this paper we present how incident management in\nIT support domain can be made much more effective using multi-modal analysis.\nThe information extracted from different modalities are correlated to enrich\nthe information in the ticket and used for better ticket routing and\nresolution. We evaluate our system using about 25000 real tickets containing\nattachments from selected problem areas. Our results demonstrate significant\nimprovements in both routing and resolution with the use of multi-modal ticket\nanalysis compared to only text based analysis.\n", "versions": [{"version": "v1", "created": "Sun, 4 Aug 2019 14:27:11 GMT"}], "update_date": "2019-08-06", "authors_parsed": [["Mandal", "Atri", ""], ["Agarwal", "Shivali", ""], ["Malhotra", "Nikhil", ""], ["Sridhara", "Giriprasad", ""], ["Ray", "Anupama", ""], ["Swarup", "Daivik", ""]]}, {"id": "1908.01367", "submitter": "Xiaochuan Yin", "authors": "Xiaochuan Yin and Chengju Liu", "title": "Unsupervised Learning of Depth and Deep Representation for Visual\n  Odometry from Monocular Videos in a Metric Space", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For ego-motion estimation, the feature representation of the scenes is\ncrucial. Previous methods indicate that both the low-level and semantic\nfeature-based methods can achieve promising results. Therefore, the\nincorporation of hierarchical feature representation may benefit from both\nmethods. From this perspective, we propose a novel direct feature odometry\nframework, named DFO, for depth estimation and hierarchical feature\nrepresentation learning from monocular videos. By exploiting the metric\ndistance, our framework is able to learn the hierarchical feature\nrepresentation without supervision. The pose is obtained with a coarse-to-fine\napproach from high-level to low-level features in enlarged feature maps. The\npixel-level attention mask can be self-learned to provide the prior\ninformation. In contrast to the previous methods, our proposed method\ncalculates the camera motion with a direct method rather than regressing the\nego-motion from the pose network. With this approach, the consistency of the\nscale factor of translation can be constrained. Additionally, the proposed\nmethod is thus compatible with the traditional SLAM pipeline. Experiments on\nthe KITTI dataset demonstrate the effectiveness of our method.\n", "versions": [{"version": "v1", "created": "Sun, 4 Aug 2019 15:48:31 GMT"}], "update_date": "2019-08-06", "authors_parsed": [["Yin", "Xiaochuan", ""], ["Liu", "Chengju", ""]]}, {"id": "1908.01373", "submitter": "Lior Golgher", "authors": "Shir Gur, Lior Wolf, Lior Golgher, Pablo Blinder", "title": "Unsupervised Microvascular Image Segmentation Using an Active Contours\n  Mimicking Neural Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The task of blood vessel segmentation in microscopy images is crucial for\nmany diagnostic and research applications. However, vessels can look vastly\ndifferent, depending on the transient imaging conditions, and collecting data\nfor supervised training is laborious. We present a novel deep learning method\nfor unsupervised segmentation of blood vessels. The method is inspired by the\nfield of active contours and we introduce a new loss term, which is based on\nthe morphological Active Contours Without Edges (ACWE) optimization method. The\nrole of the morphological operators is played by novel pooling layers that are\nincorporated to the network's architecture. We demonstrate the challenges that\nare faced by previous supervised learning solutions, when the imaging\nconditions shift. Our unsupervised method is able to outperform such previous\nmethods in both the labeled dataset, and when applied to similar but different\ndatasets. Our code, as well as efficient PyTorch reimplementations of the\nbaseline methods VesselNN and DeepVess is available on GitHub -\nhttps://github.com/shirgur/UMIS.\n", "versions": [{"version": "v1", "created": "Sun, 4 Aug 2019 16:36:58 GMT"}, {"version": "v2", "created": "Fri, 16 Aug 2019 14:41:48 GMT"}], "update_date": "2019-08-19", "authors_parsed": [["Gur", "Shir", ""], ["Wolf", "Lior", ""], ["Golgher", "Lior", ""], ["Blinder", "Pablo", ""]]}, {"id": "1908.01379", "submitter": "Adam Wolff", "authors": "Adam Wolff, Shachar Praisler, Ilya Tcenov and Guy Gilboa", "title": "Image-Guided Depth Sampling and Reconstruction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Depth acquisition, based on active illumination, is essential for autonomous\nand robotic navigation. LiDARs (Light Detection And Ranging) with mechanical,\nfixed, sampling templates are commonly used in today's autonomous vehicles. An\nemerging technology, based on solid-state depth sensors, with no mechanical\nparts, allows fast, adaptive, programmable scans.\n  In this paper, we investigate the topic of adaptive, image-driven, sampling\nand reconstruction strategies. First, we formulate a piece-wise linear depth\nmodel with several tolerance parameters and estimate its validity for indoor\nand outdoor scenes. Our model and experiments predict that, in the optimal\ncase, about 20-60 piece-wise linear structures can approximate well a depth\nmap. This translates to a depth-to-image sampling ratio of about 1/1200. We\npropose a simple, generic, sampling and reconstruction algorithm, based on\nsuper-pixels. We reach a sampling rate which is still far from the optimal\ncase. However, our sampling improves grid and random sampling, consistently,\nfor a wide variety of reconstruction methods. Moreover, our proposed\nreconstruction achieves state-of-the-art results, compared to image-guided\ndepth completion algorithms, reducing the required sampling rate by a factor of\n3-4. A single-pixel depth camera built in our lab illustrates the concept.\n", "versions": [{"version": "v1", "created": "Sun, 4 Aug 2019 17:55:47 GMT"}], "update_date": "2019-08-06", "authors_parsed": [["Wolff", "Adam", ""], ["Praisler", "Shachar", ""], ["Tcenov", "Ilya", ""], ["Gilboa", "Guy", ""]]}, {"id": "1908.01384", "submitter": "Yawei Zhao", "authors": "Yawei Zhao, En Zhu, Xinwang Liu, Chang Tang, Deke Guo, Jianping Yin", "title": "Simultaneous Clustering and Optimization for Evolving Datasets", "comments": "Accepted by IEEE Transactions on Knowledge and Data Engineering\n  (TKDE)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Simultaneous clustering and optimization (SCO) has recently drawn much\nattention due to its wide range of practical applications. Many methods have\nbeen previously proposed to solve this problem and obtain the optimal model.\nHowever, when a dataset evolves over time, those existing methods have to\nupdate the model frequently to guarantee accuracy; such updating is\ncomputationally infeasible. In this paper, we propose a new formulation of SCO\nto handle evolving datasets. Specifically, we propose a new variant of the\nalternating direction method of multipliers (ADMM) to solve this problem\nefficiently. The guarantee of model accuracy is analyzed theoretically for two\nspecific tasks: ridge regression and convex clustering. Extensive empirical\nstudies confirm the effectiveness of our method.\n", "versions": [{"version": "v1", "created": "Sun, 4 Aug 2019 18:45:42 GMT"}], "update_date": "2019-08-06", "authors_parsed": [["Zhao", "Yawei", ""], ["Zhu", "En", ""], ["Liu", "Xinwang", ""], ["Tang", "Chang", ""], ["Guo", "Deke", ""], ["Yin", "Jianping", ""]]}, {"id": "1908.01403", "submitter": "Yi Zheng", "authors": "Yi Zheng, Qitong Wang, Margrit Betke", "title": "Deep Neural Network for Semantic-based Text Recognition in Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  State-of-the-art text spotting systems typically aim to detect isolated words\nor word-by-word text in images of natural scenes and ignore the semantic\ncoherence within a region of text. However, when interpreted together,\nseemingly isolated words may be easier to recognize. On this basis, we propose\na novel \"semantic-based text recognition\" (STR) deep learning model that reads\ntext in images with the help of understanding context. STR consists of several\nmodules. We introduce the Text Grouping and Arranging (TGA) algorithm to\nconnect and order isolated text regions. A text-recognition network interprets\nisolated words. Benefiting from semantic information, a sequenceto-sequence\nnetwork model efficiently corrects inaccurate and uncertain phrases produced\nearlier in the STR pipeline. We present experiments on two new distinct\ndatasets that contain scanned catalog images of interior designs and\nphotographs of protesters with hand-written signs, respectively. Our results\nshow that our STR model outperforms a baseline method that uses\nstate-of-the-art single-wordrecognition techniques on both datasets. STR yields\na high accuracy rate of 90% on the catalog images and 71% on the more difficult\nprotest images, suggesting its generality in recognizing text.\n", "versions": [{"version": "v1", "created": "Sun, 4 Aug 2019 21:32:31 GMT"}, {"version": "v2", "created": "Thu, 15 Aug 2019 20:43:44 GMT"}, {"version": "v3", "created": "Mon, 9 Dec 2019 19:46:11 GMT"}], "update_date": "2019-12-11", "authors_parsed": [["Zheng", "Yi", ""], ["Wang", "Qitong", ""], ["Betke", "Margrit", ""]]}, {"id": "1908.01428", "submitter": "Stefan Maetschke", "authors": "Stefan Maetschke, Bhavna Antony, Hiroshi Ishikawa, Gadi Wollstein,\n  Joel Schuman, Rahil Garnavi", "title": "Inference of visual field test performance from OCT volumes using deep\n  learning", "comments": "12 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual field tests (VFT) are pivotal for glaucoma diagnosis and conducted\nregularly to monitor disease progression. Here we address the question to what\ndegree aggregate VFT measurements such as Visual Field Index (VFI) and Mean\nDeviation (MD) can be inferred from Optical Coherence Tomography (OCT) scans of\nthe Optic Nerve Head (ONH) or the macula. Accurate inference of VFT\nmeasurements from OCT could reduce examination time and cost. We propose a\nnovel 3D Convolutional Neural Network (CNN) for this task and compare its\naccuracy with classical machine learning (ML) algorithms trained on common,\nsegmentation-based OCT, features employed for glaucoma diagnostics. Peak\naccuracies were achieved on ONH scans when inferring VFI with a Pearson\nCorrelation (PC) of 0.88$\\pm$0.035 for the CNN and a significantly lower (p $<$\n0.01) PC of 0.74$\\pm$0.090 for the best performing, classical ML algorithm - a\nRandom Forest regressor. Estimation of MD was equally accurate with a PC of\n0.88$\\pm$0.023 on ONH scans for the CNN.\n", "versions": [{"version": "v1", "created": "Mon, 5 Aug 2019 00:35:18 GMT"}, {"version": "v2", "created": "Mon, 26 Aug 2019 03:23:32 GMT"}, {"version": "v3", "created": "Thu, 10 Oct 2019 04:43:27 GMT"}], "update_date": "2019-10-11", "authors_parsed": [["Maetschke", "Stefan", ""], ["Antony", "Bhavna", ""], ["Ishikawa", "Hiroshi", ""], ["Wollstein", "Gadi", ""], ["Schuman", "Joel", ""], ["Garnavi", "Rahil", ""]]}, {"id": "1908.01429", "submitter": "Yinghui Zhang", "authors": "Yinghui Zhang, Xiaojuan Deng, Jun Zhang, Hongwei Li", "title": "Restricted Linearized Augmented Lagrangian Method for Euler's Elastica\n  Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Euler's elastica model has been extensively studied and applied to image\nprocessing tasks. However, due to the high nonlinearity and nonconvexity of the\ninvolved curvature term, conventional algorithms suffer from slow convergence\nand high computational cost. Various fast algorithms have been proposed, among\nwhich, the augmented Lagrangian based ones are very popular in the community.\nHowever, parameter tuning might be very challenging for these methods. In this\npaper, a simple cutting-off strategy is introduced into the augmented\nLagrangian based algorithms for minimizing the Euler's elastica energy, which\nleads to easy parameter tuning and fast convergence. The cutting-off strategy\nis based on an observation of inconsistency inside the augmented Lagrangian\nbased algorithms. When the weighting parameter of the curvature term goes to\nzero, the energy functional boils down to the ROF model. So, a natural\nrequirement is that its augmented Lagrangian based algorithms should also\napproach the augmented Lagrangian based algorithms formulated directly for\nsolving the ROF model from the very beginning. Unfortunately, this is not the\ncase for certain existing augmented Lagrangian based algorithms. The proposed\ncutting-off strategy helps to decouple the tricky dependence between the\nauxiliary splitting variables, so as to remove the observed inconsistency.\nNumerical experiments suggest that the proposed algorithm enjoys easier\nparameter-tuning, faster convergence and even higher quality of image\nrestorations.\n", "versions": [{"version": "v1", "created": "Mon, 5 Aug 2019 00:40:41 GMT"}], "update_date": "2019-08-06", "authors_parsed": [["Zhang", "Yinghui", ""], ["Deng", "Xiaojuan", ""], ["Zhang", "Jun", ""], ["Li", "Hongwei", ""]]}, {"id": "1908.01442", "submitter": "Chenglong Li", "authors": "Chenglong Li, Yan Huang, Liang Wang, Jin Tang and Liang Lin", "title": "Learning Compact Target-Oriented Feature Representations for Visual\n  Tracking", "comments": "10 pages, 4 figures,6 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many state-of-the-art trackers usually resort to the pretrained convolutional\nneural network (CNN) model for correlation filtering, in which deep features\ncould usually be redundant, noisy and less discriminative for some certain\ninstances, and the tracking performance might thus be affected. To handle this\nproblem, we propose a novel approach, which takes both advantages of good\ngeneralization of generative models and excellent discrimination of\ndiscriminative models, for visual tracking. In particular, we learn compact,\ndiscriminative and target-oriented feature representations using the Laplacian\ncoding algorithm that exploits the dependence among the input local features in\na discriminative correlation filter framework. The feature representations and\nthe correlation filter are jointly learnt to enhance to each other via a fast\nsolver which only has very slight computational burden on the tracking speed.\nExtensive experiments on three benchmark datasets demonstrate that this\nproposed framework clearly outperforms baseline trackers with a modest impact\non the frame rate, and performs comparably against the state-of-the-art\nmethods.\n", "versions": [{"version": "v1", "created": "Mon, 5 Aug 2019 02:01:02 GMT"}], "update_date": "2019-08-06", "authors_parsed": [["Li", "Chenglong", ""], ["Huang", "Yan", ""], ["Wang", "Liang", ""], ["Tang", "Jin", ""], ["Lin", "Liang", ""]]}, {"id": "1908.01449", "submitter": "Andrew Kae", "authors": "Andrew Kae and Yale Song", "title": "Image to Video Domain Adaptation Using Web Supervision", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Training deep neural networks typically requires large amounts of labeled\ndata which may be scarce or expensive to obtain for a particular target domain.\nAs an alternative, we can leverage webly-supervised data (i.e. results from a\npublic search engine) which are relatively plentiful but may contain noisy\nresults. In this work, we propose a novel two-stage approach to learn a video\nclassifier using webly-supervised data. We argue that learning appearance\nfeatures and then temporal features sequentially, rather than simultaneously,\nis an easier optimization for this task. We show this by first learning an\nimage model from web images, which is used to initialize and train a video\nmodel. Our model applies domain adaptation to account for potential domain\nshift present between the source domain (webly-supervised data) and target\ndomain and also accounts for noise by adding a novel attention component. We\nreport results competitive with state-of-the-art for webly-supervised\napproaches on UCF-101 (while simplifying the training process) and also\nevaluate on Kinetics for comparison.\n", "versions": [{"version": "v1", "created": "Mon, 5 Aug 2019 02:50:59 GMT"}], "update_date": "2019-08-06", "authors_parsed": [["Kae", "Andrew", ""], ["Song", "Yale", ""]]}, {"id": "1908.01450", "submitter": "Yongtao Hu", "authors": "Guoxing Yu, Yongtao Hu, Jingwen Dai", "title": "TopoTag: A Robust and Scalable Topological Fiducial Marker System", "comments": "Accepted to TVCG", "journal-ref": null, "doi": "10.1109/TVCG.2020.2988466", "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fiducial markers have been playing an important role in augmented reality\n(AR), robot navigation, and general applications where the relative pose\nbetween a camera and an object is required. Here we introduce TopoTag, a robust\nand scalable topological fiducial marker system, which supports reliable and\naccurate pose estimation from a single image. TopoTag uses topological and\ngeometrical information in marker detection to achieve higher robustness.\nTopological information is extensively used for 2D marker detection, and\nfurther corresponding geometrical information for ID decoding. Robust 3D pose\nestimation is achieved by taking advantage of all TopoTag vertices. Without\nsacrificing bits for higher recall and precision like previous systems, TopoTag\ncan use full bits for ID encoding. TopoTag supports tens of thousands unique\nIDs and easily extends to millions of unique tags resulting in massive\nscalability. We collected a large test dataset including in total 169,713\nimages for evaluation, involving in-plane and out-of-plane rotation, image\nblur, different distances and various backgrounds, etc. Experiments on the\ndataset and real indoor and outdoor scene tests with a rolling shutter camera\nboth show that TopoTag significantly outperforms previous fiducial marker\nsystems in terms of various metrics, including detection accuracy, vertex\njitter, pose jitter and accuracy, etc. In addition, TopoTag supports occlusion\nas long as the main tag topological structure is maintained and allows for\nflexible shape design where users can customize internal and external marker\nshapes. Code for our marker design/generation, marker detection, and dataset\nare available at\nhttp://herohuyongtao.github.io/research/publications/topo-tag/.\n", "versions": [{"version": "v1", "created": "Mon, 5 Aug 2019 02:57:50 GMT"}, {"version": "v2", "created": "Sun, 12 Apr 2020 03:56:13 GMT"}, {"version": "v3", "created": "Wed, 15 Apr 2020 09:58:06 GMT"}], "update_date": "2020-04-16", "authors_parsed": [["Yu", "Guoxing", ""], ["Hu", "Yongtao", ""], ["Dai", "Jingwen", ""]]}, {"id": "1908.01469", "submitter": "Thang Dang Duy", "authors": "Dang Duy Thang and Toshihiro Matsui", "title": "Automated Detection System for Adversarial Examples with High-Frequency\n  Noises Sieve", "comments": "Appear to 11th International Symposium on Cyberspace Safety and\n  Security CSS 2019, Guangzhou, China", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks are being applied in many tasks with encouraging\nresults, and have often reached human-level performance. However, deep neural\nnetworks are vulnerable to well-designed input samples called adversarial\nexamples. In particular, neural networks tend to misclassify adversarial\nexamples that are imperceptible to humans. This paper introduces a new\ndetection system that automatically detects adversarial examples on deep neural\nnetworks. Our proposed system can mostly distinguish adversarial samples and\nbenign images in an end-to-end manner without human intervention. We exploit\nthe important role of the frequency domain in adversarial samples and propose a\nmethod that detects malicious samples in observations. When evaluated on two\nstandard benchmark datasets (MNIST and ImageNet), our method achieved an\nout-detection rate of 99.7 - 100% in many settings.\n", "versions": [{"version": "v1", "created": "Mon, 5 Aug 2019 05:05:29 GMT"}], "update_date": "2019-08-06", "authors_parsed": [["Thang", "Dang Duy", ""], ["Matsui", "Toshihiro", ""]]}, {"id": "1908.01477", "submitter": "Haibao Yu", "authors": "Haibao Yu, Tuopu Wen, Guangliang Cheng, Jiankai Sun, Qi Han, Jianping\n  Shi", "title": "GDRQ: Group-based Distribution Reshaping for Quantization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Low-bit quantization is challenging to maintain high performance with limited\nmodel capacity (e.g., 4-bit for both weights and activations). Naturally, the\ndistribution of both weights and activations in deep neural network are\nGaussian-like. Nevertheless, due to the limited bitwidth of low-bit model,\nuniform-like distributed weights and activations have been proved to be more\nfriendly to quantization while preserving accuracy~\\cite{Han2015Learning}.\nMotivated by this, we propose Scale-Clip, a Distribution Reshaping technique\nthat can reshape weights or activations into a uniform-like distribution in a\ndynamic manner. Furthermore, to increase the model capability for a low-bit\nmodel, a novel Group-based Quantization algorithm is proposed to split the\nfilters into several groups. Different groups can learn different quantization\nparameters, which can be elegantly merged in to batch normalization layer\nwithout extra computational cost in the inference stage. Finally, we integrate\nScale-Clip technique with Group-based Quantization algorithm and propose the\nGroup-based Distribution Reshaping Quantization (GDQR) framework to further\nimprove the quantization performance. Experiments on various networks (e.g.\nVGGNet and ResNet) and vision tasks (e.g. classification, detection and\nsegmentation) demonstrate that our framework achieves good performance.\n", "versions": [{"version": "v1", "created": "Mon, 5 Aug 2019 05:44:52 GMT"}], "update_date": "2019-08-06", "authors_parsed": [["Yu", "Haibao", ""], ["Wen", "Tuopu", ""], ["Cheng", "Guangliang", ""], ["Sun", "Jiankai", ""], ["Han", "Qi", ""], ["Shi", "Jianping", ""]]}, {"id": "1908.01481", "submitter": "Zhetong Liang", "authors": "Zhetong Liang, Jianrui Cai, Zisheng Cao, Lei Zhang", "title": "CameraNet: A Two-Stage Framework for Effective Camera ISP Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditional image signal processing (ISP) pipeline consists of a set of\nindividual image processing components onboard a camera to reconstruct a\nhigh-quality sRGB image from the sensor raw data. Due to the hand-crafted\nnature of the ISP components, traditional ISP pipeline has limited\nreconstruction quality under challenging scenes. Recently, the convolutional\nneural networks (CNNs) have demonstrated their competitiveness in solving many\nindividual image processing problems, such as image denoising, demosaicking,\nwhite balance and contrast enhancement. However, it remains a question whether\na CNN model can address the multiple tasks inside an ISP pipeline\nsimultaneously. We make a good attempt along this line and propose a novel\nframework, which we call CameraNet, for effective and general ISP pipeline\nlearning. The CameraNet is composed of two CNN modules to account for two sets\nof relatively uncorrelated subtasks in an ISP pipeline: restoration and\nenhancement. To train the two-stage CameraNet model, we specify two\ngroundtruths that can be easily created in the common workflow of photography.\nCameraNet is trained to progressively address the restoration and the\nenhancement subtasks with its two modules. Experiments show that the proposed\nCameraNet achieves consistently compelling reconstruction quality on three\nbenchmark datasets and outperforms traditional ISP pipelines.\n", "versions": [{"version": "v1", "created": "Mon, 5 Aug 2019 06:13:31 GMT"}, {"version": "v2", "created": "Thu, 8 Aug 2019 02:20:25 GMT"}], "update_date": "2019-08-09", "authors_parsed": [["Liang", "Zhetong", ""], ["Cai", "Jianrui", ""], ["Cao", "Zisheng", ""], ["Zhang", "Lei", ""]]}, {"id": "1908.01482", "submitter": "Juncheng Li", "authors": "Juncheng Li, Siliang Tang, Fei Wu, and Yueting Zhuang", "title": "Walking with MIND: Mental Imagery eNhanceD Embodied QA", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The EmbodiedQA is a task of training an embodied agent by intelligently\nnavigating in a simulated environment and gathering visual information to\nanswer questions. Existing approaches fail to explicitly model the mental\nimagery function of the agent, while the mental imagery is crucial to embodied\ncognition, and has a close relation to many high-level meta-skills such as\ngeneralization and interpretation. In this paper, we propose a novel Mental\nImagery eNhanceD (MIND) module for the embodied agent, as well as a relevant\ndeep reinforcement framework for training. The MIND module can not only model\nthe dynamics of the environment (e.g. 'what might happen if the agent passes\nthrough a door') but also help the agent to create a better understanding of\nthe environment (e.g. 'The refrigerator is usually in the kitchen'). Such\nknowledge makes the agent a faster and better learner in locating a feasible\npolicy with only a few trails. Furthermore, the MIND module can generate mental\nimages that are treated as short-term subgoals by our proposed deep\nreinforcement framework. These mental images facilitate policy learning since\nshort-term subgoals are easy to achieve and reusable. This yields better\nplanning efficiency than other algorithms that learn a policy directly from\nprimitive actions. Finally, the mental images visualize the agent's intentions\nin a way that human can understand, and this endows our agent's actions with\nmore interpretability. The experimental results and further analysis prove that\nthe agent with the MIND module is superior to its counterparts not only in EQA\nperformance but in many other aspects such as route planning, behavioral\ninterpretation, and the ability to generalize from a few examples.\n", "versions": [{"version": "v1", "created": "Mon, 5 Aug 2019 06:17:03 GMT"}], "update_date": "2019-08-06", "authors_parsed": [["Li", "Juncheng", ""], ["Tang", "Siliang", ""], ["Wu", "Fei", ""], ["Zhuang", "Yueting", ""]]}, {"id": "1908.01491", "submitter": "Chao Wen", "authors": "Chao Wen, Yinda Zhang, Zhuwen Li, Yanwei Fu", "title": "Pixel2Mesh++: Multi-View 3D Mesh Generation via Deformation", "comments": "Accepted by ICCV 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of shape generation in 3D mesh representation from a few\ncolor images with known camera poses. While many previous works learn to\nhallucinate the shape directly from priors, we resort to further improving the\nshape quality by leveraging cross-view information with a graph convolutional\nnetwork. Instead of building a direct mapping function from images to 3D shape,\nour model learns to predict series of deformations to improve a coarse shape\niteratively. Inspired by traditional multiple view geometry methods, our\nnetwork samples nearby area around the initial mesh's vertex locations and\nreasons an optimal deformation using perceptual feature statistics built from\nmultiple input images. Extensive experiments show that our model produces\naccurate 3D shape that are not only visually plausible from the input\nperspectives, but also well aligned to arbitrary viewpoints. With the help of\nphysically driven architecture, our model also exhibits generalization\ncapability across different semantic categories, number of input images, and\nquality of mesh initialization.\n", "versions": [{"version": "v1", "created": "Mon, 5 Aug 2019 07:00:11 GMT"}, {"version": "v2", "created": "Fri, 16 Aug 2019 16:24:37 GMT"}], "update_date": "2019-08-19", "authors_parsed": [["Wen", "Chao", ""], ["Zhang", "Yinda", ""], ["Li", "Zhuwen", ""], ["Fu", "Yanwei", ""]]}, {"id": "1908.01504", "submitter": "Weilin Wan", "authors": "Weilin Wan, Aaron Walsman, Dieter Fox", "title": "Part Segmentation for Highly Accurate Deformable Tracking in Occlusions\n  via Fully Convolutional Neural Networks", "comments": null, "journal-ref": "IEEE International Conference on Robotics and Automation 2019", "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Successfully tracking the human body is an important perceptual challenge for\nrobots that must work around people. Existing methods fall into two broad\ncategories: geometric tracking and direct pose estimation using machine\nlearning. While recent work has shown direct estimation techniques can be quite\npowerful, geometric tracking methods using point clouds can provide a very high\nlevel of 3D accuracy which is necessary for many robotic applications. However\nthese approaches can have difficulty in clutter when large portions of the\nsubject are occluded. To overcome this limitation, we propose a solution based\non fully convolutional neural networks (FCN). We develop an optimized Fast-FCN\nnetwork architecture for our application which allows us to filter observed\npoint clouds and improve tracking accuracy while maintaining interactive frame\nrates. We also show that this model can be trained with a limited number of\nexamples and almost no manual labelling by using an existing geometric tracker\nand data augmentation to automatically generate segmentation maps. We\ndemonstrate the accuracy of our full system by comparing it against an existing\ngeometric tracker, and show significant improvement in these challenging\nscenarios.\n", "versions": [{"version": "v1", "created": "Mon, 5 Aug 2019 08:08:56 GMT"}], "update_date": "2019-08-06", "authors_parsed": [["Wan", "Weilin", ""], ["Walsman", "Aaron", ""], ["Fox", "Dieter", ""]]}, {"id": "1908.01505", "submitter": "Hiroki Tanioka Dr", "authors": "Hiroki Tanioka", "title": "A Fast Content-Based Image Retrieval Method Using Deep Visual Features", "comments": "accepted in ICDAR-WML: The 2nd International Workshop on Machine\n  Learning 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fast and scalable Content-Based Image Retrieval using visual features is\nrequired for document analysis, Medical image analysis, etc. in the present\nage. Convolutional Neural Network (CNN) activations as features achieved their\noutstanding performance in this area. Deep Convolutional representations using\nthe softmax function in the output layer are also ones among visual features.\nHowever, almost all the image retrieval systems hold their index of visual\nfeatures on main memory in order to high responsiveness, limiting their\napplicability for big data applications. In this paper, we propose a fast\ncalculation method of cosine similarity with L2 norm indexed in advance on\nElasticsearch. We evaluate our approach with ImageNet Dataset and VGG-16\npre-trained model. The evaluation results show the effectiveness and efficiency\nof our proposed method.\n", "versions": [{"version": "v1", "created": "Mon, 5 Aug 2019 08:09:36 GMT"}], "update_date": "2019-08-06", "authors_parsed": [["Tanioka", "Hiroki", ""]]}, {"id": "1908.01517", "submitter": "Ben Usman", "authors": "Dina Bashkirova, Ben Usman, Kate Saenko", "title": "Adversarial Self-Defense for Cycle-Consistent GANs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The goal of unsupervised image-to-image translation is to map images from one\ndomain to another without the ground truth correspondence between the two\ndomains. State-of-art methods learn the correspondence using large numbers of\nunpaired examples from both domains and are based on generative adversarial\nnetworks. In order to preserve the semantics of the input image, the\nadversarial objective is usually combined with a cycle-consistency loss that\npenalizes incorrect reconstruction of the input image from the translated one.\nHowever, if the target mapping is many-to-one, e.g. aerial photos to maps, such\na restriction forces the generator to hide information in low-amplitude\nstructured noise that is undetectable by human eye or by the discriminator. In\nthis paper, we show how such self-attacking behavior of unsupervised\ntranslation methods affects their performance and provide two defense\ntechniques. We perform a quantitative evaluation of the proposed techniques and\nshow that making the translation model more robust to the self-adversarial\nattack increases its generation quality and reconstruction reliability and\nmakes the model less sensitive to low-amplitude perturbations.\n", "versions": [{"version": "v1", "created": "Mon, 5 Aug 2019 08:37:40 GMT"}], "update_date": "2019-08-06", "authors_parsed": [["Bashkirova", "Dina", ""], ["Usman", "Ben", ""], ["Saenko", "Kate", ""]]}, {"id": "1908.01523", "submitter": "Raoul de Charette", "authors": "Raoul de Charette, Sotiris Manitsaris", "title": "3D Reconstruction of Deformable Revolving Object under Heavy Hand\n  Interaction", "comments": "7 pages, 10 figures. Submitted to journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We reconstruct 3D deformable object through time, in the context of a live\npottery making process where the crafter molds the object. Because the object\nsuffers from heavy hand interaction, and is being deformed, classical\ntechniques cannot be applied. We use particle energy optimization to estimate\nthe object profile and benefit of the object radial symmetry to increase the\nrobustness of the reconstruction to both occlusion and noise. Our method works\nwith an unconstrained scalable setup with one or more depth sensors. We\nevaluate on our database (released upon publication) on a per-frame and\ntemporal basis and shows it significantly outperforms state-of-the-art\nachieving 7.60mm average object reconstruction error. Further ablation studies\ndemonstrate the effectiveness of our method.\n", "versions": [{"version": "v1", "created": "Mon, 5 Aug 2019 09:00:54 GMT"}], "update_date": "2019-08-06", "authors_parsed": [["de Charette", "Raoul", ""], ["Manitsaris", "Sotiris", ""]]}, {"id": "1908.01536", "submitter": "Liam Hiley BSc", "authors": "Liam Hiley and Alun Preece and Yulia Hicks and David Marshall and\n  Harrison Taylor", "title": "Discriminating Spatial and Temporal Relevance in Deep Taylor\n  Decompositions for Explainable Activity Recognition", "comments": "5 pages, 2 figures, published at IJCAI19 ExAI workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.HC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current techniques for explainable AI have been applied with some success to\nimage processing. The recent rise of research in video processing has called\nfor similar work n deconstructing and explaining spatio-temporal models. While\nmany techniques are designed for 2D convolutional models, others are inherently\napplicable to any input domain. One such body of work, deep Taylor\ndecomposition, propagates relevance from the model output distributively onto\nits input and thus is not restricted to image processing models. However, by\nexploiting a simple technique that removes motion information, we show that it\nis not the case that this technique is effective as-is for representing\nrelevance in non-image tasks. We instead propose a discriminative method that\nproduces a na\\\"ive representation of both the spatial and temporal relevance of\na frame as two separate objects. This new discriminative relevance model\nexposes relevance in the frame attributed to motion, that was previously\nambiguous in the original explanation. We observe the effectiveness of this\ntechnique on a range of samples from the UCF-101 action recognition dataset,\ntwo of which are demonstrated in this paper.\n", "versions": [{"version": "v1", "created": "Mon, 5 Aug 2019 09:42:25 GMT"}, {"version": "v2", "created": "Wed, 14 Aug 2019 14:36:13 GMT"}], "update_date": "2019-08-15", "authors_parsed": [["Hiley", "Liam", ""], ["Preece", "Alun", ""], ["Hicks", "Yulia", ""], ["Marshall", "David", ""], ["Taylor", "Harrison", ""]]}, {"id": "1908.01543", "submitter": "Chenglong Wang", "authors": "Chenglong Wang, Holger R. Roth, Takayuki Kitasaka, Masahiro Oda,\n  Yuichiro Hayashi, Yasushi Yoshino, Tokunori Yamamoto, Naoto Sassa, Momokazu\n  Goto, Kensaku Mori", "title": "Precise Estimation of Renal Vascular Dominant Regions Using Spatially\n  Aware Fully Convolutional Networks, Tensor-Cut and Voronoi Diagrams", "comments": null, "journal-ref": "Computerized Medical Imaging and Graphics 77 (2019): 101642", "doi": "10.1016/j.compmedimag.2019.101642", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a new approach for precisely estimating the renal\nvascular dominant region using a Voronoi diagram. To provide computer-assisted\ndiagnostics for the pre-surgical simulation of partial nephrectomy surgery, we\nmust obtain information on the renal arteries and the renal vascular dominant\nregions. We propose a fully automatic segmentation method that combines a\nneural network and tensor-based graph-cut methods to precisely extract the\nkidney and renal arteries. First, we use a convolutional neural network to\nlocalize the kidney regions and extract tiny renal arteries with a tensor-based\ngraph-cut method. Then we generate a Voronoi diagram to estimate the renal\nvascular dominant regions based on the segmented kidney and renal arteries. The\naccuracy of kidney segmentation in 27 cases with 8-fold cross validation\nreached a Dice score of 95%. The accuracy of renal artery segmentation in 8\ncases obtained a centerline overlap ratio of 80%. Each partition region\ncorresponds to a renal vascular dominant region. The final dominant-region\nestimation accuracy achieved a Dice coefficient of 80%. A clinical application\nshowed the potential of our proposed estimation approach in a real clinical\nsurgical environment. Further validation using large-scale database is our\nfuture work.\n", "versions": [{"version": "v1", "created": "Mon, 5 Aug 2019 10:11:41 GMT"}], "update_date": "2020-01-27", "authors_parsed": [["Wang", "Chenglong", ""], ["Roth", "Holger R.", ""], ["Kitasaka", "Takayuki", ""], ["Oda", "Masahiro", ""], ["Hayashi", "Yuichiro", ""], ["Yoshino", "Yasushi", ""], ["Yamamoto", "Tokunori", ""], ["Sassa", "Naoto", ""], ["Goto", "Momokazu", ""], ["Mori", "Kensaku", ""]]}, {"id": "1908.01570", "submitter": "Yuntao Chen", "authors": "Yuntao Chen, Chenxia Han, Naiyan Wang, Zhaoxiang Zhang", "title": "Revisiting Feature Alignment for One-stage Object Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, one-stage object detectors gain much attention due to their\nsimplicity in practice. Its fully convolutional nature greatly reduces the\ndifficulty of training and deployment compared with two-stage detectors which\nrequire NMS and sorting for the proposal stage. However, a fundamental issue\nlies in all one-stage detectors is the misalignment between anchor boxes and\nconvolutional features, which significantly hinders the performance of\none-stage detectors. In this work, we first reveal the deep connection between\nthe widely used im2col operator and the RoIAlign operator. Guided by this\nilluminating observation, we propose a RoIConv operator which aligns the\nfeatures and its corresponding anchors in one-stage detection in a principled\nway. We then design a fully convolutional AlignDet architecture which combines\nthe flexibility of learned anchors and the preciseness of aligned features.\nSpecifically, our AlignDet achieves a state-of-the-art mAP of 44.1 on the COCO\ntest-dev with ResNeXt-101 backbone.\n", "versions": [{"version": "v1", "created": "Mon, 5 Aug 2019 12:00:48 GMT"}], "update_date": "2019-08-06", "authors_parsed": [["Chen", "Yuntao", ""], ["Han", "Chenxia", ""], ["Wang", "Naiyan", ""], ["Zhang", "Zhaoxiang", ""]]}, {"id": "1908.01581", "submitter": "Quanshi Zhang", "authors": "Ruofan Liang, Tianlin Li, Longfei Li, Jing Wang, Quanshi Zhang", "title": "Knowledge Consistency between Neural Networks and Beyond", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper aims to analyze knowledge consistency between pre-trained deep\nneural networks. We propose a generic definition for knowledge consistency\nbetween neural networks at different fuzziness levels. A task-agnostic method\nis designed to disentangle feature components, which represent the consistent\nknowledge, from raw intermediate-layer features of each neural network. As a\ngeneric tool, our method can be broadly used for different applications. In\npreliminary experiments, we have used knowledge consistency as a tool to\ndiagnose representations of neural networks. Knowledge consistency provides new\ninsights to explain the success of existing deep-learning techniques, such as\nknowledge distillation and network compression. More crucially, knowledge\nconsistency can also be used to refine pre-trained networks and boost\nperformance.\n", "versions": [{"version": "v1", "created": "Mon, 5 Aug 2019 12:25:37 GMT"}, {"version": "v2", "created": "Tue, 14 Jan 2020 17:30:39 GMT"}], "update_date": "2020-01-15", "authors_parsed": [["Liang", "Ruofan", ""], ["Li", "Tianlin", ""], ["Li", "Longfei", ""], ["Wang", "Jing", ""], ["Zhang", "Quanshi", ""]]}, {"id": "1908.01593", "submitter": "Pratik Shah", "authors": "Aman Rana, Alarice Lowe, Marie Lithgow, Katharine Horback, Tyler\n  Janovitz, Annacarolina Da Silva, Harrison Tsai, Vignesh Shanmugam, Hyung-Jin\n  Yoon, Pratik Shah", "title": "High Accuracy Tumor Diagnoses and Benchmarking of Hematoxylin and Eosin\n  Stained Prostate Core Biopsy Images Generated by Explainable Deep Neural\n  Networks", "comments": null, "journal-ref": "JAMA Network. 2020;3(5):e205111", "doi": "10.1001/jamanetworkopen.2020.5111", "report-no": null, "categories": "q-bio.QM cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Histopathological diagnoses of tumors in tissue biopsy after Hematoxylin and\nEosin (H&E) staining is the gold standard for oncology care. H&E staining is\nslow and uses dyes, reagents and precious tissue samples that cannot be reused.\nThousands of native nonstained RGB Whole Slide Image (RWSI) patches of prostate\ncore tissue biopsies were registered with their H&E stained versions.\nConditional Generative Adversarial Neural Networks (cGANs) that automate\nconversion of native nonstained RWSI to computational H&E stained images were\nthen trained. High similarities between computational and H&E dye stained\nimages with Structural Similarity Index (SSIM) 0.902, Pearsons Correlation\nCoefficient (CC) 0.962 and Peak Signal to Noise Ratio (PSNR) 22.821 dB were\ncalculated. A second cGAN performed accurate computational destaining of H&E\ndye stained images back to their native nonstained form with SSIM 0.9, CC 0.963\nand PSNR 25.646 dB. A single-blind study computed more than 95% pixel-by-pixel\noverlap between prostate tumor annotations on computationally stained images,\nprovided by five-board certified MD pathologists, with those on H&E dye stained\ncounterparts. We report the first visualization and explanation of neural\nnetwork kernel activation maps during H&E staining and destaining of RGB images\nby cGANs. High similarities between kernel activation maps of computational and\nH&E stained images (Mean-Squared Errors <0.0005) provide additional\nmathematical and mechanistic validation of the staining system. Our neural\nnetwork framework thus is automated, explainable and performs high precision\nH&E staining and destaining of low cost native RGB images, and is computer\nvision and physician authenticated for rapid and accurate tumor diagnoses.\n", "versions": [{"version": "v1", "created": "Fri, 2 Aug 2019 13:25:07 GMT"}], "update_date": "2020-11-12", "authors_parsed": [["Rana", "Aman", ""], ["Lowe", "Alarice", ""], ["Lithgow", "Marie", ""], ["Horback", "Katharine", ""], ["Janovitz", "Tyler", ""], ["Da Silva", "Annacarolina", ""], ["Tsai", "Harrison", ""], ["Shanmugam", "Vignesh", ""], ["Yoon", "Hyung-Jin", ""], ["Shah", "Pratik", ""]]}, {"id": "1908.01594", "submitter": "Micha{\\l} Byra", "authors": "Michal Byra, Mei Wu, Xiaodong Zhang, Hyungseok Jang, Ya-Jun Ma, Eric Y\n  Chang, Sameer Shah, Jiang Du", "title": "Knee menisci segmentation and relaxometry of 3D ultrashort echo time\n  (UTE) cones MR imaging using attention U-Net with transfer learning", "comments": "30 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV physics.med-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The purpose of this work is to develop a deep learning-based method for knee\nmenisci segmentation in 3D ultrashort echo time (UTE) cones magnetic resonance\n(MR) imaging, and to automatically determine MR relaxation times, namely the\nT1, T1$\\rho$, and T2* parameters, which can be used to assess knee\nosteoarthritis (OA). Whole knee joint imaging was performed using 3D UTE cones\nsequences to collect data from 61 human subjects. Regions of interest (ROIs)\nwere outlined by two experienced radiologists based on subtracted\nT1$\\rho$-weighted MR images. Transfer learning was applied to develop 2D\nattention U-Net convolutional neural networks for the menisci segmentation\nbased on each radiologist's ROIs separately. Dice scores were calculated to\nassess segmentation performance. Next, the T1, T1$\\rho$, T2* relaxations, and\nROI areas were determined for the manual and automatic segmentations, then\ncompared.The models developed using ROIs provided by two radiologists achieved\nhigh Dice scores of 0.860 and 0.833, while the radiologists' manual\nsegmentations achieved a Dice score of 0.820. Linear correlation coefficients\nfor the T1, T1$\\rho$, and T2* relaxations calculated using the automatic and\nmanual segmentations ranged between 0.90 and 0.97, and there were no associated\ndifferences between the estimated average meniscal relaxation parameters. The\ndeep learning models achieved segmentation performance equivalent to the\ninter-observer variability of two radiologists. The proposed deep\nlearning-based approach can be used to efficiently generate automatic\nsegmentations and determine meniscal relaxations times. The method has the\npotential to help radiologists with the assessment of meniscal diseases, such\nas OA.\n", "versions": [{"version": "v1", "created": "Mon, 5 Aug 2019 12:56:11 GMT"}], "update_date": "2019-08-06", "authors_parsed": [["Byra", "Michal", ""], ["Wu", "Mei", ""], ["Zhang", "Xiaodong", ""], ["Jang", "Hyungseok", ""], ["Ma", "Ya-Jun", ""], ["Chang", "Eric Y", ""], ["Shah", "Sameer", ""], ["Du", "Jiang", ""]]}, {"id": "1908.01603", "submitter": "Deepak Gupta", "authors": "Efstratios Gavves, Ran Tao, Deepak K. Gupta and Arnold W. M. Smeulders", "title": "Model Decay in Long-Term Tracking", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Updating the tracker model with adverse bounding box predictions adds an\nunavoidable bias term to the learning. This bias term, which we refer to as\nmodel decay, offsets the learning and causes tracking drift. While its adverse\naffect might not be visible in short-term tracking, accumulation of this bias\nover a long-term can eventually lead to a permanent loss of the target. In this\npaper, we look at the problem of model bias from a mathematical perspective.\nFurther, we briefly examine the effect of various sources of tracking error on\nmodel decay, using a correlation filter (ECO) and a Siamese (SINT) tracker.\nBased on observations and insights, we propose simple additions that help to\nreduce model decay in long-term tracking. The proposed tracker is evaluated on\nfour long-term and one short term tracking benchmarks, demonstrating superior\naccuracy and robustness, even in 30 minute long videos.\n", "versions": [{"version": "v1", "created": "Mon, 5 Aug 2019 13:16:29 GMT"}], "update_date": "2019-08-06", "authors_parsed": [["Gavves", "Efstratios", ""], ["Tao", "Ran", ""], ["Gupta", "Deepak K.", ""], ["Smeulders", "Arnold W. M.", ""]]}, {"id": "1908.01642", "submitter": "Yoni Sher", "authors": "Yoni Sher", "title": "Review of Algorithms for Compressive Sensing of Images", "comments": "14 pages, 8 figures, all data available in appendix", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  We provide a comprehensive review of classical algorithms for compressive\nsensing of images, focused on Total variation methods, with a view to\napplication in LiDAR systems. Our primary focus is providing a full review for\nbeginners in the field, as well as simulating the kind of noise found in real\nLiDAR systems. To this end, we provide an overview of the theoretical\nbackground, a brief discussion of various considerations that come in to play\nin compressive sensing, and a standardized comparison of off-the-shelf methods,\nintended as a quick-start guide to choosing algorithms for compressive sensing\napplications.\n", "versions": [{"version": "v1", "created": "Mon, 5 Aug 2019 14:24:57 GMT"}], "update_date": "2019-08-06", "authors_parsed": [["Sher", "Yoni", ""]]}, {"id": "1908.01667", "submitter": "Chris Finlay", "authors": "Aram-Alexandre Pooladian, Chris Finlay, Tim Hoheisel, Adam Oberman", "title": "A principled approach for generating adversarial images under non-smooth\n  dissimilarity metrics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks perform well on real world data but are prone to\nadversarial perturbations: small changes in the input easily lead to\nmisclassification. In this work, we propose an attack methodology not only for\ncases where the perturbations are measured by $\\ell_p$ norms, but in fact any\nadversarial dissimilarity metric with a closed proximal form. This includes,\nbut is not limited to, $\\ell_1, \\ell_2$, and $\\ell_\\infty$ perturbations; the\n$\\ell_0$ counting \"norm\" (i.e. true sparseness); and the total variation\nseminorm, which is a (non-$\\ell_p$) convolutional dissimilarity measuring local\npixel changes. Our approach is a natural extension of a recent adversarial\nattack method, and eliminates the differentiability requirement of the metric.\nWe demonstrate our algorithm, ProxLogBarrier, on the MNIST, CIFAR10, and\nImageNet-1k datasets. We consider undefended and defended models, and show that\nour algorithm easily transfers to various datasets. We observe that\nProxLogBarrier outperforms a host of modern adversarial attacks specialized for\nthe $\\ell_0$ case. Moreover, by altering images in the total variation\nseminorm, we shed light on a new class of perturbations that exploit\nneighboring pixel information.\n", "versions": [{"version": "v1", "created": "Mon, 5 Aug 2019 14:57:01 GMT"}, {"version": "v2", "created": "Tue, 8 Oct 2019 17:21:21 GMT"}], "update_date": "2019-10-09", "authors_parsed": [["Pooladian", "Aram-Alexandre", ""], ["Finlay", "Chris", ""], ["Hoheisel", "Tim", ""], ["Oberman", "Adam", ""]]}, {"id": "1908.01683", "submitter": "Chih-Ting Liu", "authors": "Chih-Ting Liu, Chih-Wei Wu, Yu-Chiang Frank Wang, Shao-Yi Chien", "title": "Spatially and Temporally Efficient Non-local Attention Network for\n  Video-based Person Re-Identification", "comments": "This paper was accepted by 2019 British Machine Vision Conference\n  (BMVC)", "journal-ref": "BMVC2019", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video-based person re-identification (Re-ID) aims at matching video sequences\nof pedestrians across non-overlapping cameras. It is a practical yet\nchallenging task of how to embed spatial and temporal information of a video\ninto its feature representation. While most existing methods learn the video\ncharacteristics by aggregating image-wise features and designing attention\nmechanisms in Neural Networks, they only explore the correlation between frames\nat high-level features. In this work, we target at refining the intermediate\nfeatures as well as high-level features with non-local attention operations and\nmake two contributions. (i) We propose a Non-local Video Attention Network\n(NVAN) to incorporate video characteristics into the representation at multiple\nfeature levels. (ii) We further introduce a Spatially and Temporally Efficient\nNon-local Video Attention Network (STE-NVAN) to reduce the computation\ncomplexity by exploring spatial and temporal redundancy presented in pedestrian\nvideos. Extensive experiments show that our NVAN outperforms state-of-the-arts\nby 3.8% in rank-1 accuracy on MARS dataset and confirms our STE-NVAN displays a\nmuch superior computation footprint compared to existing methods.\n", "versions": [{"version": "v1", "created": "Mon, 5 Aug 2019 15:13:06 GMT"}], "update_date": "2019-08-06", "authors_parsed": [["Liu", "Chih-Ting", ""], ["Wu", "Chih-Wei", ""], ["Wang", "Yu-Chiang Frank", ""], ["Chien", "Shao-Yi", ""]]}, {"id": "1908.01703", "submitter": "Boyuan Ma", "authors": "Boyuan Ma, Xiaojuan Ban, Haiyou Huang, Yu Zhu", "title": "SESF-Fuse: An Unsupervised Deep Model for Multi-Focus Image Fusion", "comments": "technological report and fix some errors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we propose a novel unsupervised deep learning model to address\nmulti-focus image fusion problem. First, we train an encoder-decoder network in\nunsupervised manner to acquire deep feature of input images. And then we\nutilize these features and spatial frequency to measure activity level and\ndecision map. Finally, we apply some consistency verification methods to adjust\nthe decision map and draw out fused result. The key point behind of proposed\nmethod is that only the objects within the depth-of-field (DOF) have sharp\nappearance in the photograph while other objects are likely to be blurred. In\ncontrast to previous works, our method analyzes sharp appearance in deep\nfeature instead of original image. Experimental results demonstrate that the\nproposed method achieves the state-of-art fusion performance compared to\nexisting 16 fusion methods in objective and subjective assessment.\n", "versions": [{"version": "v1", "created": "Mon, 5 Aug 2019 15:54:09 GMT"}, {"version": "v2", "created": "Wed, 21 Aug 2019 17:03:46 GMT"}], "update_date": "2020-09-25", "authors_parsed": [["Ma", "Boyuan", ""], ["Ban", "Xiaojuan", ""], ["Huang", "Haiyou", ""], ["Zhu", "Yu", ""]]}, {"id": "1908.01707", "submitter": "Andrew Zhai", "authors": "Andrew Zhai, Hao-Yu Wu, Eric Tzeng, Dong Huk Park, Charles Rosenberg", "title": "Learning a Unified Embedding for Visual Search at Pinterest", "comments": "in Proceedings of the 25th ACM SIGKDD International Conference on\n  Knowledge and Discovery and Data Mining, 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  At Pinterest, we utilize image embeddings throughout our search and\nrecommendation systems to help our users navigate through visual content by\npowering experiences like browsing of related content and searching for exact\nproducts for shopping. In this work we describe a multi-task deep metric\nlearning system to learn a single unified image embedding which can be used to\npower our multiple visual search products. The solution we present not only\nallows us to train for multiple application objectives in a single deep neural\nnetwork architecture, but takes advantage of correlated information in the\ncombination of all training data from each application to generate a unified\nembedding that outperforms all specialized embeddings previously deployed for\neach product. We discuss the challenges of handling images from different\ndomains such as camera photos, high quality web images, and clean product\ncatalog images. We also detail how to jointly train for multiple product\nobjectives and how to leverage both engagement data and human labeled data. In\naddition, our trained embeddings can also be binarized for efficient storage\nand retrieval without compromising precision and recall. Through comprehensive\nevaluations on offline metrics, user studies, and online A/B experiments, we\ndemonstrate that our proposed unified embedding improves both relevance and\nengagement of our visual search products for both browsing and searching\npurposes when compared to existing specialized embeddings. Finally, the\ndeployment of the unified embedding at Pinterest has drastically reduced the\noperation and engineering cost of maintaining multiple embeddings while\nimproving quality.\n", "versions": [{"version": "v1", "created": "Mon, 5 Aug 2019 16:08:12 GMT"}], "update_date": "2019-08-06", "authors_parsed": [["Zhai", "Andrew", ""], ["Wu", "Hao-Yu", ""], ["Tzeng", "Eric", ""], ["Park", "Dong Huk", ""], ["Rosenberg", "Charles", ""]]}, {"id": "1908.01741", "submitter": "MinhDuc Vo", "authors": "Duc Minh Vo, Akihiro Sugimoto", "title": "Visual-Relation Conscious Image Generation from Structured-Text", "comments": "accepted at ECCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an end-to-end network for image generation from given\nstructured-text that consists of the visual-relation layout module and the\npyramid of GANs, namely stacking-GANs. Our visual-relation layout module uses\nrelations among entities in the structured-text in two ways: comprehensive\nusage and individual usage. We comprehensively use all available relations\ntogether to localize initial bounding-boxes of all the entities. We also use\nindividual relation separately to predict from the initial bounding-boxes\nrelation-units for all the relations in the input text. We then unify all the\nrelation-units to produce the visual-relation layout, i.e., bounding-boxes for\nall the entities so that each of them uniquely corresponds to each entity while\nkeeping its involved relations. Our visual-relation layout reflects the scene\nstructure given in the input text. The stacking-GANs is the stack of three GANs\nconditioned on the visual-relation layout and the output of previous GAN,\nconsistently capturing the scene structure. Our network realistically renders\nentities' details in high resolution while keeping the scene structure.\nExperimental results on two public datasets show outperformances of our method\nagainst state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Mon, 5 Aug 2019 17:33:00 GMT"}, {"version": "v2", "created": "Wed, 27 Nov 2019 03:21:10 GMT"}, {"version": "v3", "created": "Sat, 18 Jul 2020 05:26:53 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Vo", "Duc Minh", ""], ["Sugimoto", "Akihiro", ""]]}, {"id": "1908.01748", "submitter": "Albert Shaw", "authors": "Albert Shaw, Daniel Hunter, Forrest Iandola and Sammy Sidhu", "title": "SqueezeNAS: Fast neural architecture search for faster semantic\n  segmentation", "comments": "11 pages, 10 figures, 3 tables, 3 pages of appendix; Added found\n  networks to Appendix tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For real time applications utilizing Deep Neural Networks (DNNs), it is\ncritical that the models achieve high-accuracy on the target task and\nlow-latency inference on the target computing platform. While Neural\nArchitecture Search (NAS) has been effectively used to develop low-latency\nnetworks for image classification, there has been relatively little effort to\nuse NAS to optimize DNN architectures for other vision tasks. In this work, we\npresent what we believe to be the first proxyless hardware-aware search\ntargeted for dense semantic segmentation. With this approach, we advance the\nstate-of-the-art accuracy for latency-optimized networks on the Cityscapes\nsemantic segmentation dataset. Our latency-optimized small SqueezeNAS network\nachieves 68.02% validation class mIOU with less than 35 ms inference times on\nthe NVIDIA AGX Xavier. Our latency-optimized large SqueezeNAS network achieves\n73.62% class mIOU with less than 100 ms inference times. We demonstrate that\nsignificant performance gains are possible by utilizing NAS to find networks\noptimized for both the specific task and inference hardware. We also present\ndetailed analysis comparing our networks to recent state-of-the-art\narchitectures.\n", "versions": [{"version": "v1", "created": "Mon, 5 Aug 2019 17:46:36 GMT"}, {"version": "v2", "created": "Thu, 8 Aug 2019 06:46:25 GMT"}], "update_date": "2019-08-09", "authors_parsed": [["Shaw", "Albert", ""], ["Hunter", "Daniel", ""], ["Iandola", "Forrest", ""], ["Sidhu", "Sammy", ""]]}, {"id": "1908.01790", "submitter": "Veeru Talreja", "authors": "Veeru Talreja, Fariborz Taherkhani, Matthew C Valenti, Nasser M\n  Nasrabadi", "title": "Attribute-Guided Coupled GAN for Cross-Resolution Face Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a novel attribute-guided cross-resolution\n(low-resolution to high-resolution) face recognition framework that leverages a\ncoupled generative adversarial network (GAN) structure with adversarial\ntraining to find the hidden relationship between the low-resolution and\nhigh-resolution images in a latent common embedding subspace. The coupled GAN\nframework consists of two sub-networks, one dedicated to the low-resolution\ndomain and the other dedicated to the high-resolution domain. Each sub-network\naims to find a projection that maximizes the pair-wise correlation between the\ntwo feature domains in a common embedding subspace. In addition to projecting\nthe images into a common subspace, the coupled network also predicts facial\nattributes to improve the cross-resolution face recognition. Specifically, our\nproposed coupled framework exploits facial attributes to further maximize the\npair-wise correlation by implicitly matching facial attributes of the low and\nhigh-resolution images during the training, which leads to a more\ndiscriminative embedding subspace resulting in performance enhancement for\ncross-resolution face recognition. The efficacy of our approach compared with\nthe state-of-the-art is demonstrated using the LFWA, Celeb-A, SCFace and UCCS\ndatasets.\n", "versions": [{"version": "v1", "created": "Mon, 5 Aug 2019 18:10:55 GMT"}], "update_date": "2019-08-07", "authors_parsed": [["Talreja", "Veeru", ""], ["Taherkhani", "Fariborz", ""], ["Valenti", "Matthew C", ""], ["Nasrabadi", "Nasser M", ""]]}, {"id": "1908.01797", "submitter": "Xinyi Li", "authors": "Xinyi Li, Haibin Ling", "title": "Hybrid Camera Pose Estimation with Online Partitioning for SLAM", "comments": null, "journal-ref": "IEEE Robotics and Automation Letters ( Volume: 5, Issue: 2, April\n  2020)", "doi": "10.1109/LRA.2020.2967688", "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a hybrid real-time camera pose estimation framework with\na novel partitioning scheme and introduces motion averaging to monocular\nSimultaneous Localization and Mapping (SLAM) systems. Breaking through the\nlimitations of fixed-size temporal partitioning in many conventional SLAM\npipelines, our approach significantly improves the accuracy of local bundle\nadjustment by gathering spatially-strongly-connected cameras into each block.\nWith the dynamic initialization using intermediate computation values, \\XL{we\nimprove the Levenberg-Marquardt solver to further enhance the efficiency of the\nlocal optimization.} Moreover, the dense data association between blocks by our\nco-visibility-based partitioning enables us to explore and implement motion\naveraging to efficiently align the blocks globally, updating camera motion\nestimations on-the-fly. Experiments on benchmarks convincingly demonstrate the\npracticality and robustness of our proposed approach by significantly\noutperforming conventional approaches.\n", "versions": [{"version": "v1", "created": "Mon, 5 Aug 2019 18:26:03 GMT"}, {"version": "v2", "created": "Mon, 2 Nov 2020 19:40:08 GMT"}], "update_date": "2020-11-04", "authors_parsed": [["Li", "Xinyi", ""], ["Ling", "Haibin", ""]]}, {"id": "1908.01801", "submitter": "Kushal Kafle", "authors": "Kushal Kafle, Robik Shrestha, Brian Price, Scott Cohen, Christopher\n  Kanan", "title": "Answering Questions about Data Visualizations using Efficient Bimodal\n  Fusion", "comments": "Presented at WACV, 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Chart question answering (CQA) is a newly proposed visual question answering\n(VQA) task where an algorithm must answer questions about data visualizations,\ne.g. bar charts, pie charts, and line graphs. CQA requires capabilities that\nnatural-image VQA algorithms lack: fine-grained measurements, optical character\nrecognition, and handling out-of-vocabulary words in both questions and\nanswers. Without modifications, state-of-the-art VQA algorithms perform poorly\non this task. Here, we propose a novel CQA algorithm called parallel recurrent\nfusion of image and language (PReFIL). PReFIL first learns bimodal embeddings\nby fusing question and image features and then intelligently aggregates these\nlearned embeddings to answer the given question. Despite its simplicity, PReFIL\ngreatly surpasses state-of-the art systems and human baselines on both the\nFigureQA and DVQA datasets. Additionally, we demonstrate that PReFIL can be\nused to reconstruct tables by asking a series of questions about a chart.\n", "versions": [{"version": "v1", "created": "Mon, 5 Aug 2019 18:47:30 GMT"}, {"version": "v2", "created": "Wed, 22 Jul 2020 15:10:29 GMT"}], "update_date": "2020-07-23", "authors_parsed": [["Kafle", "Kushal", ""], ["Shrestha", "Robik", ""], ["Price", "Brian", ""], ["Cohen", "Scott", ""], ["Kanan", "Christopher", ""]]}, {"id": "1908.01862", "submitter": "Alessio Tonioni", "authors": "Daniele De Gregorio, Alessio Tonioni, Gianluca Palli and Luigi Di\n  Stefano", "title": "Semi-Automatic Labeling for Deep Learning in Robotics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose Augmented Reality Semi-automatic labeling (ARS), a\nsemi-automatic method which leverages on moving a 2D camera by means of a\nrobot, proving precise camera tracking, and an augmented reality pen to define\ninitial object bounding box, to create large labeled datasets with minimal\nhuman intervention. By removing the burden of generating annotated data from\nhumans, we make the Deep Learning technique applied to computer vision, that\ntypically requires very large datasets, truly automated and reliable. With the\nARS pipeline, we created effortlessly two novel datasets, one on\nelectromechanical components (industrial scenario) and one on fruits\n(daily-living scenario), and trained robustly two state-of-the-art object\ndetectors, based on convolutional neural networks, such as YOLO and SSD. With\nrespect to the conventional manual annotation of 1000 frames that takes us\nslightly more than 10 hours, the proposed approach based on ARS allows\nannotating 9 sequences of about 35000 frames in less than one hour, with a gain\nfactor of about 450. Moreover, both the precision and recall of object\ndetection is increased by about 15\\% with respect to manual labeling. All our\nsoftware is available as a ROS package in a public repository alongside the\nnovel annotated datasets.\n", "versions": [{"version": "v1", "created": "Mon, 5 Aug 2019 21:10:12 GMT"}], "update_date": "2019-08-07", "authors_parsed": [["De Gregorio", "Daniele", ""], ["Tonioni", "Alessio", ""], ["Palli", "Gianluca", ""], ["Di Stefano", "Luigi", ""]]}, {"id": "1908.01866", "submitter": "Peter He", "authors": "Peter He, Gerard Glowacki, Alexis Gkantiragas", "title": "Unsupervised Representations of Pollen in Bright-Field Microscopy", "comments": "Accepted at the Workshop on Computational Biology at the\n  International Conference on Machine Learning (ICML) in Long Beach, CA, USA on\n  June 14, 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the first unsupervised deep learning method for pollen analysis\nusing bright-field microscopy. Using a modest dataset of 650 images of pollen\ngrains collected from honey, we achieve family level identification of pollen.\nWe embed images of pollen grains into a low-dimensional latent space and\ncompare Euclidean and Riemannian metrics on these spaces for clustering. We\npropose this system for automated analysis of pollen and other microscopic\nbiological structures which have only small or unlabelled datasets available.\n", "versions": [{"version": "v1", "created": "Mon, 5 Aug 2019 21:29:51 GMT"}], "update_date": "2019-08-07", "authors_parsed": [["He", "Peter", ""], ["Glowacki", "Gerard", ""], ["Gkantiragas", "Alexis", ""]]}, {"id": "1908.01872", "submitter": "Xiaofeng Liu", "authors": "Xiaofeng Liu and Zhenhua Guo and Jane You and B.V.K Vijaya Kumar", "title": "Attention Control with Metric Learning Alignment for Image Set-based\n  Recognition", "comments": "Accepted to IEEE T-IFS (Extension of ECCV 2018 paper:\n  Dependency-aware Attention Control for Unconstrained Face Recognition with\n  Image Sets). arXiv admin note: substantial text overlap with\n  arXiv:1907.03030; text overlap with arXiv:1707.00130 by other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers the problem of image set-based face verification and\nidentification. Unlike traditional single sample (an image or a video) setting,\nthis situation assumes the availability of a set of heterogeneous collection of\norderless images and videos. The samples can be taken at different check\npoints, different identity documents $etc$. The importance of each image is\nusually considered either equal or based on a quality assessment of that image\nindependent of other images and/or videos in that image set. How to model the\nrelationship of orderless images within a set remains a challenge. We address\nthis problem by formulating it as a Markov Decision Process (MDP) in a latent\nspace. Specifically, we first propose a dependency-aware attention control\n(DAC) network, which uses actor-critic reinforcement learning for attention\ndecision of each image to exploit the correlations among the unordered images.\nAn off-policy experience replay is introduced to speed up the learning process.\nMoreover, the DAC is combined with a temporal model for videos using divide and\nconquer strategies. We also introduce a pose-guided representation (PGR) scheme\nthat can further boost the performance at extreme poses. We propose a\nparameter-free PGR without the need for training as well as a novel metric\nlearning-based PGR for pose alignment without the need for pose detection in\ntesting stage. Extensive evaluations on IJB-A/B/C, YTF, Celebrity-1000 datasets\ndemonstrate that our method outperforms many state-of-art approaches on the\nset-based as well as video-based face recognition databases.\n", "versions": [{"version": "v1", "created": "Mon, 5 Aug 2019 21:48:05 GMT"}], "update_date": "2019-08-07", "authors_parsed": [["Liu", "Xiaofeng", ""], ["Guo", "Zhenhua", ""], ["You", "Jane", ""], ["Kumar", "B. V. K Vijaya", ""]]}, {"id": "1908.01925", "submitter": "Qianyu Feng", "authors": "Qianyu Feng, Guoliang Kang, Hehe Fan, Yi Yang", "title": "Attract or Distract: Exploit the Margin of Open Set", "comments": "Presented at ICCV 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Open set domain adaptation aims to diminish the domain shift across domains,\nwith partially shared classes. There exist unknown target samples out of the\nknowledge of source domain. Compared to the close set setting, how to separate\nthe unknown (unshared) class from the known (shared) ones plays a key role.\nWhereas, previous methods did not emphasize the semantic structure of the open\nset data, which may introduce bias into the domain alignment and confuse the\nclassifier around the decision boundary. In this paper, we exploit the semantic\nstructure of open set data from two aspects: 1) Semantic Categorical Alignment,\nwhich aims to achieve good separability of target known classes by\ncategorically aligning the centroid of target with the source. 2)Semantic\nContrastive Mapping, which aims to push the unknown class away from the\ndecision boundary. Empirically, we demonstrate that our method performs\nfavourably against the state-of-the-art methods on representative benchmarks,\ne.g. Digit datasets and Office-31 datasets.\n", "versions": [{"version": "v1", "created": "Tue, 6 Aug 2019 01:36:01 GMT"}, {"version": "v2", "created": "Sat, 10 Aug 2019 05:27:13 GMT"}], "update_date": "2019-08-13", "authors_parsed": [["Feng", "Qianyu", ""], ["Kang", "Guoliang", ""], ["Fan", "Hehe", ""], ["Yang", "Yi", ""]]}, {"id": "1908.01931", "submitter": "Qian Guo", "authors": "Qian Guo, Yuhua Qian, Xinyan Liang, Yanhong She, Deyu Li, Jiye Liang", "title": "Logic could be learned from images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Logic reasoning is a significant ability of human intelligence and also an\nimportant task in artificial intelligence. The existing logic reasoning\nmethods, quite often, need to design some reasoning patterns beforehand. This\nhas led to an interesting question: can logic reasoning patterns be directly\nlearned from given data? The problem is termed as a data concept logic. In this\nstudy, a learning logic task from images, called a LiLi task, first is\nproposed. This task is to learn and reason the logic relation from images,\nwithout presetting any reasoning patterns. As a preliminary exploration, we\ndesign six LiLi data sets (Bitwise And, Bitwise Or, Bitwise Xor, Addition,\nSubtraction and Multiplication), in which each image is embedded with a n-digit\nnumber. It is worth noting that a learning model beforehand does not know the\nmeaning of the n-digit numbers embedded in images and the relation between the\ninput images and the output image. In order to tackle the task, in this work we\nuse many typical neural network models and produce fruitful results. However,\nthese models have the poor performances on the difficult logic task. For\nfurthermore addressing this task, a novel network framework called a divide and\nconquer model by adding some label information is designed, achieving a high\ntesting accuracy.\n", "versions": [{"version": "v1", "created": "Tue, 6 Aug 2019 02:24:31 GMT"}, {"version": "v2", "created": "Tue, 29 Jun 2021 15:22:57 GMT"}], "update_date": "2021-06-30", "authors_parsed": [["Guo", "Qian", ""], ["Qian", "Yuhua", ""], ["Liang", "Xinyan", ""], ["She", "Yanhong", ""], ["Li", "Deyu", ""], ["Liang", "Jiye", ""]]}, {"id": "1908.01940", "submitter": "Jerin Geo James", "authors": "Jerin Geo James, Pranay Agrawal, Ajit Rajwade", "title": "Restoration of Non-rigidly Distorted Underwater Images using a\n  Combination of Compressive Sensing and Local Polynomial Image Representations", "comments": "Accepted in ICCV 2019 for oral presentation", "journal-ref": "ICCV 2019", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Images of static scenes submerged beneath a wavy water surface exhibit severe\nnon-rigid distortions. The physics of water flow suggests that water surfaces\npossess spatio-temporal smoothness and temporal periodicity. Hence they possess\na sparse representation in the 3D discrete Fourier (DFT) basis. Motivated by\nthis, we pose the task of restoration of such video sequences as a compressed\nsensing (CS) problem. We begin by tracking a few salient feature points across\nthe frames of a video sequence of the submerged scene. Using these point\ntrajectories, we show that the motion fields at all other (non-tracked) points\ncan be effectively estimated using a typical CS solver. This by itself is a\nnovel contribution in the field of non-rigid motion estimation. We show that\nthis method outperforms state of the art algorithms for underwater image\nrestoration. We further consider a simple optical flow algorithm based on local\npolynomial expansion of the image frames (PEOF). Surprisingly, we demonstrate\nthat PEOF is more efficient and often outperforms all the state of the art\nmethods in terms of numerical measures. Finally, we demonstrate that a\ntwo-stage approach consisting of the CS step followed by PEOF much more\naccurately preserves the image structure and improves the (visual as well as\nnumerical) video quality as compared to just the PEOF stage.\n", "versions": [{"version": "v1", "created": "Tue, 6 Aug 2019 03:33:50 GMT"}], "update_date": "2019-08-07", "authors_parsed": [["James", "Jerin Geo", ""], ["Agrawal", "Pranay", ""], ["Rajwade", "Ajit", ""]]}, {"id": "1908.01950", "submitter": "Wang Rui", "authors": "Rui Wang, XiaoJun Wu and Josef Kittler", "title": "Multiple Riemannian Manifold-valued Descriptors based Image Set\n  Classification with Multi-Kernel Metric Learning", "comments": "15 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The importance of wild video based image set recognition is becoming\nmonotonically increasing. However, the contents of these collected videos are\noften complicated, and how to efficiently perform set modeling and feature\nextraction is a big challenge for set-based classification algorithms. In\nrecent years, some proposed image set classification methods have made a\nconsiderable advance by modeling the original image set with covariance matrix,\nlinear subspace, or Gaussian distribution. As a matter of fact, most of them\njust adopt a single geometric model to describe each given image set, which may\nlose some other useful information for classification. To tackle this problem,\nwe propose a novel algorithm to model each image set from a multi-geometric\nperspective. Specifically, the covariance matrix, linear subspace, and Gaussian\ndistribution are applied for set representation simultaneously. In order to\nfuse these multiple heterogeneous Riemannian manifoldvalued features, the\nwell-equipped Riemannian kernel functions are first utilized to map them into\nhigh dimensional Hilbert spaces. Then, a multi-kernel metric learning framework\nis devised to embed the learned hybrid kernels into a lower dimensional common\nsubspace for classification. We conduct experiments on four widely used\ndatasets corresponding to four different classification tasks: video-based face\nrecognition, set-based object categorization, video-based emotion recognition,\nand dynamic scene classification, to evaluate the classification performance of\nthe proposed algorithm. Extensive experimental results justify its superiority\nover the state-of-the-art.\n", "versions": [{"version": "v1", "created": "Tue, 6 Aug 2019 04:21:31 GMT"}], "update_date": "2019-08-07", "authors_parsed": [["Wang", "Rui", ""], ["Wu", "XiaoJun", ""], ["Kittler", "Josef", ""]]}, {"id": "1908.01957", "submitter": "Yang Mingkun", "authors": "MingKun Yang, Yushuo Guan, Minghui Liao, Xin He, Kaigui Bian, Song\n  Bai, Cong Yao and Xiang Bai", "title": "Symmetry-constrained Rectification Network for Scene Text Recognition", "comments": "The paper was accepted to ICCV2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reading text in the wild is a very challenging task due to the diversity of\ntext instances and the complexity of natural scenes. Recently, the community\nhas paid increasing attention to the problem of recognizing text instances with\nirregular shapes. One intuitive and effective way to handle this problem is to\nrectify irregular text to a canonical form before recognition. However, these\nmethods might struggle when dealing with highly curved or distorted text\ninstances. To tackle this issue, we propose in this paper a\nSymmetry-constrained Rectification Network (ScRN) based on local attributes of\ntext instances, such as center line, scale and orientation. Such constraints\nwith an accurate description of text shape enable ScRN to generate better\nrectification results than existing methods and thus lead to higher recognition\naccuracy. Our method achieves state-of-the-art performance on text with both\nregular and irregular shapes. Specifically, the system outperforms existing\nalgorithms by a large margin on datasets that contain quite a proportion of\nirregular text instances, e.g., ICDAR 2015, SVT-Perspective and CUTE80.\n", "versions": [{"version": "v1", "created": "Tue, 6 Aug 2019 05:00:27 GMT"}], "update_date": "2019-08-07", "authors_parsed": [["Yang", "MingKun", ""], ["Guan", "Yushuo", ""], ["Liao", "Minghui", ""], ["He", "Xin", ""], ["Bian", "Kaigui", ""], ["Bai", "Song", ""], ["Yao", "Cong", ""], ["Bai", "Xiang", ""]]}, {"id": "1908.01958", "submitter": "He Xinwei", "authors": "Xinwei He, Tengteng Huang, Song Bai, Xiang Bai", "title": "View N-gram Network for 3D Object Retrieval", "comments": "The paper was accepted to ICCV 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  How to aggregate multi-view representations of a 3D object into an\ninformative and discriminative one remains a key challenge for multi-view 3D\nobject retrieval. Existing methods either use view-wise pooling strategies\nwhich neglect the spatial information across different views or employ\nrecurrent neural networks which may face the efficiency problem. To address\nthese issues, we propose an effective and efficient framework called View\nN-gram Network (VNN). Inspired by n-gram models in natural language processing,\nVNN divides the view sequence into a set of visual n-grams, which involve\noverlapping consecutive view sub-sequences. By doing so, spatial information\nacross multiple views is captured, which helps to learn a discriminative global\nembedding for each 3D object. Experiments on 3D shape retrieval benchmarks,\nincluding ModelNet10, ModelNet40 and ShapeNetCore55 datasets, demonstrate the\nsuperiority of our proposed method.\n", "versions": [{"version": "v1", "created": "Tue, 6 Aug 2019 05:03:53 GMT"}, {"version": "v2", "created": "Thu, 15 Aug 2019 14:30:34 GMT"}], "update_date": "2019-08-16", "authors_parsed": [["He", "Xinwei", ""], ["Huang", "Tengteng", ""], ["Bai", "Song", ""], ["Bai", "Xiang", ""]]}, {"id": "1908.01961", "submitter": "Abhimitra Meka", "authors": "Abhimitra Meka, Mohammad Shafiei, Michael Zollhoefer, Christian\n  Richardt, Christian Theobalt", "title": "Real-Time Global Illumination Decomposition of Videos", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We propose the first approach for the decomposition of a monocular color\nvideo into direct and indirect illumination components in real time. We\nretrieve, in separate layers, the contribution made to the scene appearance by\nthe scene reflectance, the light sources and the reflections from various\ncoherent scene regions to one another. Existing techniques that invert global\nlight transport require image capture under multiplexed controlled lighting, or\nonly enable the decomposition of a single image at slow off-line frame rates.\nIn contrast, our approach works for regular videos and produces temporally\ncoherent decomposition layers at real-time frame rates. At the core of our\napproach are several sparsity priors that enable the estimation of the\nper-pixel direct and indirect illumination layers based on a small set of\njointly estimated base reflectance colors. The resulting variational\ndecomposition problem uses a new formulation based on sparse and dense sets of\nnon-linear equations that we solve efficiently using a novel alternating\ndata-parallel optimization strategy. We evaluate our approach qualitatively and\nquantitatively, and show improvements over the state of the art in this field,\nin both quality and runtime. In addition, we demonstrate various real-time\nappearance editing applications for videos with consistent illumination.\n", "versions": [{"version": "v1", "created": "Tue, 6 Aug 2019 05:23:45 GMT"}, {"version": "v2", "created": "Thu, 10 Jun 2021 21:28:52 GMT"}], "update_date": "2021-06-14", "authors_parsed": [["Meka", "Abhimitra", ""], ["Shafiei", "Mohammad", ""], ["Zollhoefer", "Michael", ""], ["Richardt", "Christian", ""], ["Theobalt", "Christian", ""]]}, {"id": "1908.01962", "submitter": "Zhanzhan Cheng", "authors": "Peng Zhang, Xinyu Zhu, Zhanzhan Cheng, Shuigeng Zhou and Yi Niu", "title": "REAPS: Towards Better Recognition of Fine-grained Images by Region\n  Attending and Part Sequencing", "comments": "PRCV 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Fine-grained image recognition has been a hot research topic in computer\nvision due to its various applications. The-state-of-the-art is the\npart/region-based approaches that first localize discriminative parts/regions,\nand then learn their fine-grained features. However, these approaches have some\ninherent drawbacks: 1) the discriminative feature representation of an object\nis prone to be disturbed by complicated background; 2) it is unreasonable and\ninflexible to fix the number of salient parts, because the intended parts may\nbe unavailable under certain circumstances due to occlusion or incompleteness,\nand 3) the spatial correlation among different salient parts has not been\nthoroughly exploited (if not completely neglected). To overcome these\ndrawbacks, in this paper we propose a new, simple yet robust method by building\npart sequence model on the attended object region. Concretely, we first try to\nalleviate the background effect by using a region attention mechanism to\ngenerate the attended region from the original image. Then, instead of\nlocalizing different salient parts and extracting their features separately, we\nlearn the part representation implicitly by applying a mapping function on the\nserialized features of the object. Finally, we combine the region attending\nnetwork and the part sequence learning network into a unified framework that\ncan be trained end-to-end with only image-level labels. Our extensive\nexperiments on three fine-grained benchmarks show that the proposed method\nachieves the state of the art performance.\n", "versions": [{"version": "v1", "created": "Tue, 6 Aug 2019 05:34:59 GMT"}], "update_date": "2019-08-07", "authors_parsed": [["Zhang", "Peng", ""], ["Zhu", "Xinyu", ""], ["Cheng", "Zhanzhan", ""], ["Zhou", "Shuigeng", ""], ["Niu", "Yi", ""]]}, {"id": "1908.01975", "submitter": "Huajun Zhou", "authors": "Zixuan Chen, Huajun Zhou, Xiaohua Xie, Jianhuang Lai", "title": "Contour Loss: Boundary-Aware Learning for Salient Object Segmentation", "comments": "12 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a learning model that makes full use of boundary information for\nsalient object segmentation. Specifically, we come up with a novel loss\nfunction, i.e., Contour Loss, which leverages object contours to guide models\nto perceive salient object boundaries. Such a boundary-aware network can learn\nboundary-wise distinctions between salient objects and background, hence\neffectively facilitating the saliency detection. Yet the Contour Loss\nemphasizes on the local saliency. We further propose the hierarchical global\nattention module (HGAM), which forces the model hierarchically attend to global\ncontexts, thus captures the global visual saliency. Comprehensive experiments\non six benchmark datasets show that our method achieves superior performance\nover state-of-the-art ones. Moreover, our model has a real-time speed of 26 fps\non a TITAN X GPU.\n", "versions": [{"version": "v1", "created": "Tue, 6 Aug 2019 06:23:50 GMT"}], "update_date": "2019-08-07", "authors_parsed": [["Chen", "Zixuan", ""], ["Zhou", "Huajun", ""], ["Xie", "Xiaohua", ""], ["Lai", "Jianhuang", ""]]}, {"id": "1908.01977", "submitter": "Chuan Wang", "authors": "Yi He, Jiayuan Shi, Chuan Wang, Haibin Huang, Jiaming Liu, Guanbin Li,\n  Risheng Liu, Jue Wang", "title": "Semi-supervised Skin Detection by Network with Mutual Guidance", "comments": "Accepted by ICCV 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present a new data-driven method for robust skin detection\nfrom a single human portrait image. Unlike previous methods, we incorporate\nhuman body as a weak semantic guidance into this task, considering acquiring\nlarge-scale of human labeled skin data is commonly expensive and\ntime-consuming. To be specific, we propose a dual-task neural network for joint\ndetection of skin and body via a semi-supervised learning strategy. The\ndual-task network contains a shared encoder but two decoders for skin and body\nseparately. For each decoder, its output also serves as a guidance for its\ncounterpart, making both decoders mutually guided. Extensive experiments were\nconducted to demonstrate the effectiveness of our network with mutual guidance,\nand experimental results show our network outperforms the state-of-the-art in\nskin detection.\n", "versions": [{"version": "v1", "created": "Tue, 6 Aug 2019 06:29:50 GMT"}], "update_date": "2019-08-07", "authors_parsed": [["He", "Yi", ""], ["Shi", "Jiayuan", ""], ["Wang", "Chuan", ""], ["Huang", "Haibin", ""], ["Liu", "Jiaming", ""], ["Li", "Guanbin", ""], ["Liu", "Risheng", ""], ["Wang", "Jue", ""]]}, {"id": "1908.01978", "submitter": "Binyuan Hui", "authors": "Pengfei Zhu, Binyuan Hui, Changqing Zhang, Dawei Du, Longyin Wen,\n  Qinghua Hu", "title": "Multi-view Deep Subspace Clustering Networks", "comments": "Submitted to the IEEE Transactions on Image Processing (TIP)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-view subspace clustering aims to discover the inherent structure by\nfusing multi-view complementary information. Most existing methods first\nextract multiple types of hand-crafted features and then learn a joint affinity\nmatrix for clustering. The disadvantage lies in two aspects: 1) Multi-view\nrelations are not embedded into feature learning. 2) The end-to-end learning\nmanner of deep learning is not well used in multi-view clustering. To address\nthe above issues, we propose a novel multi-view deep subspace clustering\nnetwork (MvDSCN) by learning a multi-view self-representation matrix in an\nend-to-end manner. MvDSCN consists of two sub-networks, i.e., diversity network\n(Dnet) and universality network (Unet). A latent space is built upon deep\nconvolutional auto-encoders and a self-representation matrix is learned in the\nlatent space using a fully connected layer. Dnet learns view-specific\nself-representation matrices while Unet learns a common self-representation\nmatrix for all views. To exploit the complementarity of multi-view\nrepresentations, Hilbert Schmidt Independence Criterion (HSIC) is introduced as\na diversity regularization, which can capture the non-linear and high-order\ninter-view relations. As different views share the same label space, the\nself-representation matrices of each view are aligned to the common one by a\nuniversality regularization. Experiments on both multi-feature and\nmulti-modality learning validate the superiority of the proposed multi-view\nsubspace clustering model.\n", "versions": [{"version": "v1", "created": "Tue, 6 Aug 2019 06:44:43 GMT"}], "update_date": "2019-08-07", "authors_parsed": [["Zhu", "Pengfei", ""], ["Hui", "Binyuan", ""], ["Zhang", "Changqing", ""], ["Du", "Dawei", ""], ["Wen", "Longyin", ""], ["Hu", "Qinghua", ""]]}, {"id": "1908.01997", "submitter": "Cheng Li", "authors": "Cheng Li, Hui Sun, Zaiyi Liu, Meiyun Wang, Hairong Zheng, Shanshan\n  Wang", "title": "Learning Cross-Modal Deep Representations for Multi-Modal MR Image\n  Segmentation", "comments": "9 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-modal magnetic resonance imaging (MRI) is essential in clinics for\ncomprehensive diagnosis and surgical planning. Nevertheless, the segmentation\nof multi-modal MR images tends to be time-consuming and challenging.\nConvolutional neural network (CNN)-based multi-modal MR image analysis commonly\nproceeds with multiple down-sampling streams fused at one or several layers.\nAlthough inspiring performance has been achieved, the feature fusion is usually\nconducted through simple summation or concatenation without optimization. In\nthis work, we propose a supervised image fusion method to selectively fuse the\nuseful information from different modalities and suppress the respective noise\nsignals. Specifically, an attention block is introduced as guidance for the\ninformation selection. From the different modalities, one modality that\ncontributes most to the results is selected as the master modality, which\nsupervises the information selection of the other assistant modalities. The\neffectiveness of the proposed method is confirmed through breast mass\nsegmentation in MR images of two modalities and better segmentation results are\nachieved compared to the state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Tue, 6 Aug 2019 07:42:44 GMT"}], "update_date": "2019-08-07", "authors_parsed": [["Li", "Cheng", ""], ["Sun", "Hui", ""], ["Liu", "Zaiyi", ""], ["Wang", "Meiyun", ""], ["Zheng", "Hairong", ""], ["Wang", "Shanshan", ""]]}, {"id": "1908.01998", "submitter": "Qi Fan", "authors": "Qi Fan, Wei Zhuo, Chi-Keung Tang, Yu-Wing Tai", "title": "Few-Shot Object Detection with Attention-RPN and Multi-Relation Detector", "comments": "CVPR2020 Camera Ready. (Fix Figure 3 and Table 5. More implementation\n  details in the supplementary material.)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conventional methods for object detection typically require a substantial\namount of training data and preparing such high-quality training data is very\nlabor-intensive. In this paper, we propose a novel few-shot object detection\nnetwork that aims at detecting objects of unseen categories with only a few\nannotated examples. Central to our method are our Attention-RPN, Multi-Relation\nDetector and Contrastive Training strategy, which exploit the similarity\nbetween the few shot support set and query set to detect novel objects while\nsuppressing false detection in the background. To train our network, we\ncontribute a new dataset that contains 1000 categories of various objects with\nhigh-quality annotations. To the best of our knowledge, this is one of the\nfirst datasets specifically designed for few-shot object detection. Once our\nfew-shot network is trained, it can detect objects of unseen categories without\nfurther training or fine-tuning. Our method is general and has a wide range of\npotential applications. We produce a new state-of-the-art performance on\ndifferent datasets in the few-shot setting. The dataset link is\nhttps://github.com/fanq15/Few-Shot-Object-Detection-Dataset.\n", "versions": [{"version": "v1", "created": "Tue, 6 Aug 2019 07:43:35 GMT"}, {"version": "v2", "created": "Mon, 23 Dec 2019 07:01:03 GMT"}, {"version": "v3", "created": "Tue, 31 Mar 2020 02:18:11 GMT"}, {"version": "v4", "created": "Sun, 10 May 2020 03:09:53 GMT"}], "update_date": "2020-05-12", "authors_parsed": [["Fan", "Qi", ""], ["Zhuo", "Wei", ""], ["Tang", "Chi-Keung", ""], ["Tai", "Yu-Wing", ""]]}, {"id": "1908.02013", "submitter": "Rafael Felix", "authors": "Rafael Felix and Ben Harwood and Michele Sasdelli and Gustavo Carneiro", "title": "Generalised Zero-Shot Learning with a Classifier Ensemble over\n  Multi-Modal Embedding Spaces", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generalised zero-shot learning (GZSL) methods aim to classify previously seen\nand unseen visual classes by leveraging the semantic information of those\nclasses. In the context of GZSL, semantic information is non-visual data such\nas a text description of both seen and unseen classes. Previous GZSL methods\nhave utilised transformations between visual and semantic embedding spaces, as\nwell as the learning of joint spaces that include both visual and semantic\ninformation. In either case, classification is then performed on a single\nlearned space. We argue that each embedding space contains complementary\ninformation for the GZSL problem. By using just a visual, semantic or joint\nspace some of this information will invariably be lost. In this paper, we\ndemonstrate the advantages of our new GZSL method that combines the\nclassification of visual, semantic and joint spaces. Most importantly, this\nensembling allows for more information from the source domains to be seen\nduring classification. An additional contribution of our work is the\napplication of a calibration procedure for each classifier in the ensemble.\nThis calibration mitigates the problem of model selection when combining the\nclassifiers. Lastly, our proposed method achieves state-of-the-art results on\nthe CUB, AWA1 and AWA2 benchmark data sets and provides competitive performance\non the SUN data set.\n", "versions": [{"version": "v1", "created": "Tue, 6 Aug 2019 08:35:13 GMT"}], "update_date": "2019-08-07", "authors_parsed": [["Felix", "Rafael", ""], ["Harwood", "Ben", ""], ["Sasdelli", "Michele", ""], ["Carneiro", "Gustavo", ""]]}, {"id": "1908.02021", "submitter": "Zhipeng Cai", "authors": "Zhipeng Cai and Tat-Jun Chin and Vladlen Koltun", "title": "Consensus Maximization Tree Search Revisited", "comments": "Accepted as oral presentation to ICCV'19", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consensus maximization is widely used for robust fitting in computer vision.\nHowever, solving it exactly, i.e., finding the globally optimal solution, is\nintractable. A* tree search, which has been shown to be fixed-parameter\ntractable, is one of the most efficient exact methods, though it is still\nlimited to small inputs. We make two key contributions towards improving A*\ntree search. First, we show that the consensus maximization tree structure used\npreviously actually contains paths that connect nodes at both adjacent and\nnon-adjacent levels. Crucially, paths connecting non-adjacent levels are\nredundant for tree search, but they were not avoided previously. We propose a\nnew acceleration strategy that avoids such redundant paths. In the second\ncontribution, we show that the existing branch pruning technique also\ndeteriorates quickly with the problem dimension. We then propose a new branch\npruning technique that is less dimension-sensitive to address this issue.\nExperiments show that both new techniques can significantly accelerate A* tree\nsearch, making it reasonably efficient on inputs that were previously out of\nreach.\n", "versions": [{"version": "v1", "created": "Tue, 6 Aug 2019 08:54:04 GMT"}, {"version": "v2", "created": "Thu, 8 Aug 2019 03:11:38 GMT"}, {"version": "v3", "created": "Sun, 25 Aug 2019 12:12:19 GMT"}], "update_date": "2019-08-27", "authors_parsed": [["Cai", "Zhipeng", ""], ["Chin", "Tat-Jun", ""], ["Koltun", "Vladlen", ""]]}, {"id": "1908.02023", "submitter": "Kai Han", "authors": "Kai Han, Yunhe Wang, Yixing Xu, Chunjing Xu, Dacheng Tao, Chang Xu", "title": "Full-Stack Filters to Build Minimum Viable CNNs", "comments": "Tech report", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep convolutional neural networks (CNNs) are usually over-parameterized,\nwhich cannot be easily deployed on edge devices such as mobile phones and smart\ncameras. Existing works used to decrease the number or size of requested\nconvolution filters for a minimum viable CNN on edge devices. In contrast, this\npaper introduces filters that are full-stack and can be used to generate many\nmore sub-filters. Weights of these sub-filters are inherited from full-stack\nfilters with the help of different binary masks. Orthogonal constraints are\napplied over binary masks to decrease their correlation and promote the\ndiversity of generated sub-filters. To preserve the same volume of output\nfeature maps, we can naturally reduce the number of established filters by only\nmaintaining a few full-stack filters and a set of binary masks. We also conduct\ntheoretical analysis on the memory cost and an efficient implementation is\nintroduced for the convolution of the proposed filters. Experiments on several\nbenchmark datasets and CNN models demonstrate that the proposed method is able\nto construct minimum viable convolution networks of comparable performance.\n", "versions": [{"version": "v1", "created": "Tue, 6 Aug 2019 08:55:47 GMT"}], "update_date": "2019-08-07", "authors_parsed": [["Han", "Kai", ""], ["Wang", "Yunhe", ""], ["Xu", "Yixing", ""], ["Xu", "Chunjing", ""], ["Tao", "Dacheng", ""], ["Xu", "Chang", ""]]}, {"id": "1908.02054", "submitter": "Shanshan Wang", "authors": "Yanxia Chen, Taohui Xiao, Cheng Li, Qiegen Liu and Shanshan Wang", "title": "Model-based Convolutional De-Aliasing Network Learning for Parallel MR\n  Imaging", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Parallel imaging has been an essential technique to accelerate MR imaging.\nNevertheless, the acceleration rate is still limited due to the ill-condition\nand challenges associated with the undersampled reconstruction. In this paper,\nwe propose a model-based convolutional de-aliasing network with adaptive\nparameter learning to achieve accurate reconstruction from multi-coil\nundersampled k-space data. Three main contributions have been made: a\nde-aliasing reconstruction model was proposed to accelerate parallel MR imaging\nwith deep learning exploring both spatial redundancy and multi-coil\ncorrelations; a split Bregman iteration algorithm was developed to solve the\nmodel efficiently; and unlike most existing parallel imaging methods which rely\non the accuracy of the estimated multi-coil sensitivity, the proposed method\ncan perform parallel reconstruction from undersampled data without explicit\nsensitivity calculation. Evaluations were conducted on \\emph{in vivo} brain\ndataset with a variety of undersampling patterns and different acceleration\nfactors. Our results demonstrated that this method could achieve superior\nperformance in both quantitative and qualitative analysis, compared to three\nstate-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Tue, 6 Aug 2019 10:20:56 GMT"}], "update_date": "2019-08-07", "authors_parsed": [["Chen", "Yanxia", ""], ["Xiao", "Taohui", ""], ["Li", "Cheng", ""], ["Liu", "Qiegen", ""], ["Wang", "Shanshan", ""]]}, {"id": "1908.02076", "submitter": "Yanlin Qian", "authors": "Yanlin Qian, Ke Chen and Huanglin Yu", "title": "Fast Fourier Color Constancy and Grayness Index for ISPA Illumination\n  Estimation Challenge", "comments": "The 3-page challenge report for the Illumination Estimation\n  Challenge, in the Int'l Workshop on Color Vision, affiliated to the 11th\n  Int'l Symposium on Image and Signal Processing and Analysis (ISPA2019,\n  Dubrovnik, Croatia). Second version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We briefly introduce two submissions to the Illumination Estimation\nChallenge, in the Int'l Workshop on Color Vision, affiliated to the 11th Int'l\nSymposium on Image and Signal Processing and Analysis. The\nFourier-transform-based submission is ranked 3rd, and the statistical\nGray-pixel-based one ranked 6th.\n", "versions": [{"version": "v1", "created": "Tue, 6 Aug 2019 11:03:41 GMT"}, {"version": "v2", "created": "Tue, 17 Sep 2019 11:53:40 GMT"}], "update_date": "2019-09-18", "authors_parsed": [["Qian", "Yanlin", ""], ["Chen", "Ke", ""], ["Yu", "Huanglin", ""]]}, {"id": "1908.02095", "submitter": "G\\\"ozde Nur G\\\"une\\c{s}li", "authors": "Gozde Nur Gunesli, Cenk Sokmensuer, and Cigdem Gunduz-Demir", "title": "AttentionBoost: Learning What to Attend by Boosting Fully Convolutional\n  Networks", "comments": "This work has been submitted to the IEEE for possible publication", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dense prediction models are widely used for image segmentation. One important\nchallenge is to sufficiently train these models to yield good generalizations\nfor hard-to-learn pixels. A typical group of such hard-to-learn pixels are\nboundaries between instances. Many studies have proposed to give specific\nattention to learning the boundary pixels. They include designing multi-task\nnetworks with an additional task of boundary prediction and increasing the\nweights of boundary pixels' predictions in the loss function. Such strategies\nrequire defining what to attend beforehand and incorporating this defined\nattention to the learning model. However, there may exist other groups of\nhard-to-learn pixels and manually defining and incorporating the appropriate\nattention for each group may not be feasible. In order to provide a more\nattainable and scalable solution, this paper proposes AttentionBoost, which is\na new multi-attention learning model based on adaptive boosting. AttentionBoost\ndesigns a multi-stage network and introduces a new loss adjustment mechanism\nfor a dense prediction model to adaptively learn what to attend at each stage\ndirectly on image data without necessitating any prior definition about what to\nattend. This mechanism modulates the attention of each stage to correct the\nmistakes of previous stages, by adjusting the loss weight of each pixel\nprediction separately with respect to how accurate the previous stages are on\nthis pixel. This mechanism enables AttentionBoost to learn different attentions\nfor different pixels at the same stage, according to difficulty of learning\nthese pixels, as well as multiple attentions for the same pixel at different\nstages, according to confidence of these stages on their predictions for this\npixel. Using gland segmentation as a showcase application, our experiments\ndemonstrate that AttentionBoost improves the results of its counterparts.\n", "versions": [{"version": "v1", "created": "Tue, 6 Aug 2019 12:06:12 GMT"}], "update_date": "2019-08-07", "authors_parsed": [["Gunesli", "Gozde Nur", ""], ["Sokmensuer", "Cenk", ""], ["Gunduz-Demir", "Cigdem", ""]]}, {"id": "1908.02116", "submitter": "Xuanyi Dong", "authors": "Xuanyi Dong, Yi Yang", "title": "Teacher Supervises Students How to Learn From Partially Labeled Images\n  for Facial Landmark Detection", "comments": "This paper was accepted to IEEE ICCV 2019. Model codes are publicly\n  available on GitHub: https://github.com/D-X-Y/landmark-detection", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Facial landmark detection aims to localize the anatomically defined points of\nhuman faces. In this paper, we study facial landmark detection from partially\nlabeled facial images. A typical approach is to (1) train a detector on the\nlabeled images; (2) generate new training samples using this detector's\nprediction as pseudo labels of unlabeled images; (3) retrain the detector on\nthe labeled samples and partial pseudo labeled samples. In this way, the\ndetector can learn from both labeled and unlabeled data to become robust. In\nthis paper, we propose an interaction mechanism between a teacher and two\nstudents to generate more reliable pseudo labels for unlabeled data, which are\nbeneficial to semi-supervised facial landmark detection. Specifically, the two\nstudents are instantiated as dual detectors. The teacher learns to judge the\nquality of the pseudo labels generated by the students and filter out\nunqualified samples before the retraining stage. In this way, the student\ndetectors get feedback from their teacher and are retrained by premium data\ngenerated by itself. Since the two students are trained by different samples, a\ncombination of their predictions will be more robust as the final prediction\ncompared to either prediction. Extensive experiments on 300-W and AFLW\nbenchmarks show that the interactions between teacher and students contribute\nto better utilization of the unlabeled data and achieves state-of-the-art\nperformance.\n", "versions": [{"version": "v1", "created": "Tue, 6 Aug 2019 12:57:24 GMT"}, {"version": "v2", "created": "Wed, 7 Aug 2019 07:56:07 GMT"}, {"version": "v3", "created": "Tue, 13 Aug 2019 17:09:26 GMT"}], "update_date": "2019-08-14", "authors_parsed": [["Dong", "Xuanyi", ""], ["Yang", "Yi", ""]]}, {"id": "1908.02123", "submitter": "Philipp Harzig", "authors": "Philipp Harzig, Yan-Ying Chen, Francine Chen, Rainer Lienhart", "title": "Addressing Data Bias Problems for Chest X-ray Image Report Generation", "comments": "Oral at BMVC 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic medical report generation from chest X-ray images is one\npossibility for assisting doctors to reduce their workload. However, the\ndifferent patterns and data distribution of normal and abnormal cases can bias\nmachine learning models. Previous attempts did not focus on isolating the\ngeneration of the abnormal and normal sentences in order to increase the\nvariability of generated paragraphs. To address this, we propose to separate\nabnormal and normal sentence generation by using two different word LSTMs in a\nhierarchical LSTM model. We conduct an analysis on the distinctiveness of\ngenerated sentences compared to the BLEU score, which increases when less\ndistinct reports are generated. We hope our findings will help to encourage the\ndevelopment of new metrics to better verify methods of automatic medical report\ngeneration.\n", "versions": [{"version": "v1", "created": "Tue, 6 Aug 2019 13:18:07 GMT"}], "update_date": "2019-08-07", "authors_parsed": [["Harzig", "Philipp", ""], ["Chen", "Yan-Ying", ""], ["Chen", "Francine", ""], ["Lienhart", "Rainer", ""]]}, {"id": "1908.02125", "submitter": "Wei-Ting Wang", "authors": "Wei-Ting Wang, Han-Lin Li, Wei-Shiang Lin, Cheng-Ming Chiang, Yi-Min\n  Tsai", "title": "Architecture-aware Network Pruning for Vision Quality Applications", "comments": "Accepted to be Published in the 26th IEEE International Conference on\n  Image Processing (ICIP 2019). Updated to contain the IEEE copyright notice", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional neural network (CNN) delivers impressive achievements in\ncomputer vision and machine learning field. However, CNN incurs high\ncomputational complexity, especially for vision quality applications because of\nlarge image resolution. In this paper, we propose an iterative\narchitecture-aware pruning algorithm with adaptive magnitude threshold while\ncooperating with quality-metric measurement simultaneously. We show the\nperformance improvement applied on vision quality applications and provide\ncomprehensive analysis with flexible pruning configuration. With the proposed\nmethod, the Multiply-Accumulate (MAC) of state-of-the-art low-light imaging\n(SID) and super-resolution (EDSR) are reduced by 58% and 37% without quality\ndrop, respectively. The memory bandwidth (BW) requirements of convolutional\nlayer can be also reduced by 20% to 40%.\n", "versions": [{"version": "v1", "created": "Mon, 5 Aug 2019 01:54:22 GMT"}], "update_date": "2019-08-07", "authors_parsed": [["Wang", "Wei-Ting", ""], ["Li", "Han-Lin", ""], ["Lin", "Wei-Shiang", ""], ["Chiang", "Cheng-Ming", ""], ["Tsai", "Yi-Min", ""]]}, {"id": "1908.02126", "submitter": "Ke Li", "authors": "Rongrong Ji, Ke Li, Yan Wang, Xiaoshuai Sun, Feng Guo, Xiaowei Guo,\n  Yongjian Wu, Feiyue Huang, and Jiebo Luo", "title": "Semi-Supervised Adversarial Monocular Depth Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we address the problem of monocular depth estimation when only\na limited number of training image-depth pairs are available. To achieve a high\nregression accuracy, the state-of-the-art estimation methods rely on CNNs\ntrained with a large number of image-depth pairs, which are prohibitively\ncostly or even infeasible to acquire. Aiming to break the curse of such\nexpensive data collections, we propose a semi-supervised adversarial learning\nframework that only utilizes a small number of image-depth pairs in conjunction\nwith a large number of easily-available monocular images to achieve high\nperformance. In particular, we use one generator to regress the depth and two\ndiscriminators to evaluate the predicted depth , i.e., one inspects the\nimage-depth pair while the other inspects the depth channel alone. These two\ndiscriminators provide their feedbacks to the generator as the loss to generate\nmore realistic and accurate depth predictions. Experiments show that the\nproposed approach can (1) improve most state-of-the-art models on the NYUD v2\ndataset by effectively leveraging additional unlabeled data sources; (2) reach\nstate-of-the-art accuracy when the training set is small, e.g., on the Make3D\ndataset; (3) adapt well to an unseen new dataset (Make3D in our case) after\ntraining on an annotated dataset (KITTI in our case).\n", "versions": [{"version": "v1", "created": "Tue, 6 Aug 2019 13:19:24 GMT"}], "update_date": "2019-08-07", "authors_parsed": [["Ji", "Rongrong", ""], ["Li", "Ke", ""], ["Wang", "Yan", ""], ["Sun", "Xiaoshuai", ""], ["Guo", "Feng", ""], ["Guo", "Xiaowei", ""], ["Wu", "Yongjian", ""], ["Huang", "Feiyue", ""], ["Luo", "Jiebo", ""]]}, {"id": "1908.02127", "submitter": "Longteng Guo", "authors": "Longteng Guo, Jing Liu, Jinhui Tang, Jiangwei Li, Wei Luo, Hanqing Lu", "title": "Aligning Linguistic Words and Visual Semantic Units for Image Captioning", "comments": "8 pages, 5 figures. Accepted by ACM MM 2019", "journal-ref": null, "doi": "10.1145/3343031.3350943", "report-no": null, "categories": "cs.CV cs.CL cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image captioning attempts to generate a sentence composed of several\nlinguistic words, which are used to describe objects, attributes, and\ninteractions in an image, denoted as visual semantic units in this paper. Based\non this view, we propose to explicitly model the object interactions in\nsemantics and geometry based on Graph Convolutional Networks (GCNs), and fully\nexploit the alignment between linguistic words and visual semantic units for\nimage captioning. Particularly, we construct a semantic graph and a geometry\ngraph, where each node corresponds to a visual semantic unit, i.e., an object,\nan attribute, or a semantic (geometrical) interaction between two objects.\nAccordingly, the semantic (geometrical) context-aware embeddings for each unit\nare obtained through the corresponding GCN learning processers. At each time\nstep, a context gated attention module takes as inputs the embeddings of the\nvisual semantic units and hierarchically align the current word with these\nunits by first deciding which type of visual semantic unit (object, attribute,\nor interaction) the current word is about, and then finding the most correlated\nvisual semantic units under this type. Extensive experiments are conducted on\nthe challenging MS-COCO image captioning dataset, and superior results are\nreported when comparing to state-of-the-art approaches.\n", "versions": [{"version": "v1", "created": "Tue, 6 Aug 2019 13:19:24 GMT"}], "update_date": "2019-08-07", "authors_parsed": [["Guo", "Longteng", ""], ["Liu", "Jing", ""], ["Tang", "Jinhui", ""], ["Li", "Jiangwei", ""], ["Luo", "Wei", ""], ["Lu", "Hanqing", ""]]}, {"id": "1908.02160", "submitter": "Jiangfan Han", "authors": "Jiangfan Han, Ping Luo, Xiaogang Wang", "title": "Deep Self-Learning From Noisy Labels", "comments": "Accepted by IEEE International Conference on Computer Vision (ICCV)\n  2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  ConvNets achieve good results when training from clean data, but learning\nfrom noisy labels significantly degrades performances and remains challenging.\nUnlike previous works constrained by many conditions, making them infeasible to\nreal noisy cases, this work presents a novel deep self-learning framework to\ntrain a robust network on the real noisy datasets without extra supervision.\nThe proposed approach has several appealing benefits. (1) Different from most\nexisting work, it does not rely on any assumption on the distribution of the\nnoisy labels, making it robust to real noises. (2) It does not need extra clean\nsupervision or accessorial network to help training. (3) A self-learning\nframework is proposed to train the network in an iterative end-to-end manner,\nwhich is effective and efficient. Extensive experiments in challenging\nbenchmarks such as Clothing1M and Food101-N show that our approach outperforms\nits counterparts in all empirical settings.\n", "versions": [{"version": "v1", "created": "Tue, 6 Aug 2019 13:43:58 GMT"}, {"version": "v2", "created": "Tue, 20 Aug 2019 08:37:35 GMT"}], "update_date": "2019-08-21", "authors_parsed": [["Han", "Jiangfan", ""], ["Luo", "Ping", ""], ["Wang", "Xiaogang", ""]]}, {"id": "1908.02166", "submitter": "Nir Billfeld", "authors": "Nir Billfeld, Moshe Kim", "title": "Semiparametric Wavelet-based JPEG IV Estimator for endogenously\n  truncated data", "comments": "18 pages", "journal-ref": "IEEE Access, 7, 99602-99621 (2019)", "doi": "10.1109/ACCESS.2019.2929571", "report-no": null, "categories": "stat.ME cs.CV cs.LG econ.EM stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A new and an enriched JPEG algorithm is provided for identifying redundancies\nin a sequence of irregular noisy data points which also accommodates a\nreference-free criterion function. Our main contribution is by formulating\nanalytically (instead of approximating) the inverse of the transpose of\nJPEGwavelet transform without involving matrices which are computationally\ncumbersome. The algorithm is suitable for the widely-spread situations where\nthe original data distribution is unobservable such as in cases where there is\ndeficient representation of the entire population in the training data (in\nmachine learning) and thus the covariate shift assumption is violated. The\nproposed estimator corrects for both biases, the one generated by endogenous\ntruncation and the one generated by endogenous covariates. Results from\nutilizing 2,000,000 different distribution functions verify the applicability\nand high accuracy of our procedure to cases in which the disturbances are\nneither jointly nor marginally normally distributed.\n", "versions": [{"version": "v1", "created": "Tue, 6 Aug 2019 13:54:52 GMT"}], "update_date": "2019-08-07", "authors_parsed": [["Billfeld", "Nir", ""], ["Kim", "Moshe", ""]]}, {"id": "1908.02182", "submitter": "Fabian Isensee", "authors": "Fabian Isensee and Klaus H. Maier-Hein", "title": "An attempt at beating the 3D U-Net", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The U-Net is arguably the most successful segmentation architecture in the\nmedical domain. Here we apply a 3D U-Net to the 2019 Kidney and Kidney Tumor\nSegmentation Challenge and attempt to improve upon it by augmenting it with\nresidual and pre-activation residual blocks. Cross-validation results on the\ntraining cases suggest only very minor, barely measurable improvements. Due to\nmarginally higher dice scores, the residual 3D U-Net is chosen for test set\nprediction. With a Composite Dice score of 91.23 on the test set, our method\noutperformed all 105 competing teams and won the KiTS2019 challenge by a small\nmargin.\n", "versions": [{"version": "v1", "created": "Tue, 6 Aug 2019 14:28:17 GMT"}, {"version": "v2", "created": "Fri, 4 Oct 2019 11:03:40 GMT"}], "update_date": "2019-10-07", "authors_parsed": [["Isensee", "Fabian", ""], ["Maier-Hein", "Klaus H.", ""]]}, {"id": "1908.02197", "submitter": "Dongwei Ren", "authors": "Dongwei Ren, Kai Zhang, Qilong Wang, Qinghua Hu, and Wangmeng Zuo", "title": "Neural Blind Deconvolution Using Deep Priors", "comments": "Accepted to CVPR 2020. The source code is available at\n  https://github.com/csdwren/SelfDeblur, and the supplementary file is at\n  https://csdwren.github.io/papers/SelfDeblur_supp.pdf", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Blind deconvolution is a classical yet challenging low-level vision problem\nwith many real-world applications. Traditional maximum a posterior (MAP) based\nmethods rely heavily on fixed and handcrafted priors that certainly are\ninsufficient in characterizing clean images and blur kernels, and usually adopt\nspecially designed alternating minimization to avoid trivial solution. In\ncontrast, existing deep motion deblurring networks learn from massive training\nimages the mapping to clean image or blur kernel, but are limited in handling\nvarious complex and large size blur kernels. To connect MAP and deep models, we\nin this paper present two generative networks for respectively modeling the\ndeep priors of clean image and blur kernel, and propose an unconstrained neural\noptimization solution to blind deconvolution. In particular, we adopt an\nasymmetric Autoencoder with skip connections for generating latent clean image,\nand a fully-connected network (FCN) for generating blur kernel. Moreover, the\nSoftMax nonlinearity is applied to the output layer of FCN to meet the\nnon-negative and equality constraints. The process of neural optimization can\nbe explained as a kind of \"zero-shot\" self-supervised learning of the\ngenerative networks, and thus our proposed method is dubbed SelfDeblur.\nExperimental results show that our SelfDeblur can achieve notable quantitative\ngains as well as more visually plausible deblurring results in comparison to\nstate-of-the-art blind deconvolution methods on benchmark datasets and\nreal-world blurry images. The source code is available at\nhttps://github.com/csdwren/SelfDeblur\n", "versions": [{"version": "v1", "created": "Tue, 6 Aug 2019 15:03:44 GMT"}, {"version": "v2", "created": "Wed, 18 Mar 2020 04:40:36 GMT"}], "update_date": "2020-03-19", "authors_parsed": [["Ren", "Dongwei", ""], ["Zhang", "Kai", ""], ["Wang", "Qilong", ""], ["Hu", "Qinghua", ""], ["Zuo", "Wangmeng", ""]]}, {"id": "1908.02199", "submitter": "Chen Ma", "authors": "Chen Ma, Chenxu Zhao, Hailin Shi, Li Chen, Junhai Yong and Dan Zeng", "title": "MetaAdvDet: Towards Robust Detection of Evolving Adversarial Attacks", "comments": "10 pages, 2 figures, accepted as the conference paper of Proceedings\n  of the 27th ACM International Conference on Multimedia (MM'19)", "journal-ref": null, "doi": "10.1145/3343031.3350887", "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks (DNNs) are vulnerable to adversarial attack which is\nmaliciously implemented by adding human-imperceptible perturbation to images\nand thus leads to incorrect prediction. Existing studies have proposed various\nmethods to detect the new adversarial attacks. However, new attack methods keep\nevolving constantly and yield new adversarial examples to bypass the existing\ndetectors. It needs to collect tens of thousands samples to train detectors,\nwhile the new attacks evolve much more frequently than the high-cost data\ncollection. Thus, this situation leads the newly evolved attack samples to\nremain in small scales. To solve such few-shot problem with the evolving\nattack, we propose a meta-learning based robust detection method to detect new\nadversarial attacks with limited examples. Specifically, the learning consists\nof a double-network framework: a task-dedicated network and a master network\nwhich alternatively learn the detection capability for either seen attack or a\nnew attack. To validate the effectiveness of our approach, we construct the\nbenchmarks with few-shot-fashion protocols based on three conventional\ndatasets, i.e. CIFAR-10, MNIST and Fashion-MNIST. Comprehensive experiments are\nconducted on them to verify the superiority of our approach with respect to the\ntraditional adversarial attack detection methods.\n", "versions": [{"version": "v1", "created": "Tue, 6 Aug 2019 15:06:21 GMT"}], "update_date": "2019-08-07", "authors_parsed": [["Ma", "Chen", ""], ["Zhao", "Chenxu", ""], ["Shi", "Hailin", ""], ["Chen", "Li", ""], ["Yong", "Junhai", ""], ["Zeng", "Dan", ""]]}, {"id": "1908.02231", "submitter": "Ziyuan Huang", "authors": "Ziyuan Huang, Changhong Fu, Yiming Li, Fuling Lin, Peng Lu", "title": "Learning Aberrance Repressed Correlation Filters for Real-Time UAV\n  Tracking", "comments": "iccv 2019 accepted, 10 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditional framework of discriminative correlation filters (DCF) is often\nsubject to undesired boundary effects. Several approaches to enlarge search\nregions have been already proposed in the past years to make up for this\nshortcoming. However, with excessive background information, more background\nnoises are also introduced and the discriminative filter is prone to learn from\nthe ambiance rather than the object. This situation, along with appearance\nchanges of objects caused by full/partial occlusion, illumination variation,\nand other reasons has made it more likely to have aberrances in the detection\nprocess, which could substantially degrade the credibility of its result.\nTherefore, in this work, a novel approach to repress the aberrances happening\nduring the detection process is proposed, i.e., aberrance repressed correlation\nfilter (ARCF). By enforcing restriction to the rate of alteration in response\nmaps generated in the detection phase, the ARCF tracker can evidently suppress\naberrances and is thus more robust and accurate to track objects. Considerable\nexperiments are conducted on different UAV datasets to perform object tracking\nfrom an aerial view, i.e., UAV123, UAVDT, and DTB70, with 243 challenging image\nsequences containing over 90K frames to verify the performance of the ARCF\ntracker and it has proven itself to have outperformed other 20 state-of-the-art\ntrackers based on DCF and deep-based frameworks with sufficient speed for\nreal-time applications.\n", "versions": [{"version": "v1", "created": "Tue, 6 Aug 2019 16:11:48 GMT"}, {"version": "v2", "created": "Wed, 7 Aug 2019 13:24:10 GMT"}], "update_date": "2019-08-08", "authors_parsed": [["Huang", "Ziyuan", ""], ["Fu", "Changhong", ""], ["Li", "Yiming", ""], ["Lin", "Fuling", ""], ["Lu", "Peng", ""]]}, {"id": "1908.02254", "submitter": "S M Nadim Uddin", "authors": "S. M. A. Sharif, Ghulam Mujtaba, S. M. Nadim Uddin", "title": "EdgeNet: A novel approach for Arabic numeral classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Despite the importance of handwritten numeral classification, a robust and\neffective method for a widely used language like Arabic is still due. This\nstudy focuses to overcome two major limitations of existing works: data\ndiversity and effective learning method. Hence, the existing Arabic numeral\ndatasets have been merged into a single dataset and augmented to introduce data\ndiversity. Moreover, a novel deep model has been proposed to exploit diverse\ndata samples of unified dataset. The proposed deep model utilizes the low-level\nedge features by propagating them through residual connection. To make a fair\ncomparison with the proposed model, the existing works have been studied under\nthe unified dataset. The comparison experiments illustrate that the unified\ndataset accelerates the performance of the existing works. Moreover, the\nproposed model outperforms the existing state-of-the-art Arabic handwritten\nnumeral classification methods and obtain an accuracy of 99.59% in the\nvalidation phase. Apart from that, different state-of-the-art classification\nmodels have studied with the same dataset to reveal their feasibility for the\nArabic numeral classification. Code available at\nhttp://github.com/sharif-apu/EdgeNet.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jul 2019 10:17:43 GMT"}], "update_date": "2019-08-07", "authors_parsed": [["Sharif", "S. M. A.", ""], ["Mujtaba", "Ghulam", ""], ["Uddin", "S. M. Nadim", ""]]}, {"id": "1908.02265", "submitter": "Jiasen Lu", "authors": "Jiasen Lu, Dhruv Batra, Devi Parikh, Stefan Lee", "title": "ViLBERT: Pretraining Task-Agnostic Visiolinguistic Representations for\n  Vision-and-Language Tasks", "comments": "11 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present ViLBERT (short for Vision-and-Language BERT), a model for learning\ntask-agnostic joint representations of image content and natural language. We\nextend the popular BERT architecture to a multi-modal two-stream model,\npro-cessing both visual and textual inputs in separate streams that interact\nthrough co-attentional transformer layers. We pretrain our model through two\nproxy tasks on the large, automatically collected Conceptual Captions dataset\nand then transfer it to multiple established vision-and-language tasks --\nvisual question answering, visual commonsense reasoning, referring expressions,\nand caption-based image retrieval -- by making only minor additions to the base\narchitecture. We observe significant improvements across tasks compared to\nexisting task-specific models -- achieving state-of-the-art on all four tasks.\nOur work represents a shift away from learning groundings between vision and\nlanguage only as part of task training and towards treating visual grounding as\na pretrainable and transferable capability.\n", "versions": [{"version": "v1", "created": "Tue, 6 Aug 2019 17:33:52 GMT"}], "update_date": "2019-08-07", "authors_parsed": [["Lu", "Jiasen", ""], ["Batra", "Dhruv", ""], ["Parikh", "Devi", ""], ["Lee", "Stefan", ""]]}, {"id": "1908.02288", "submitter": "Marc Combalia", "authors": "Marc Combalia, Noel C. F. Codella, Veronica Rotemberg, Brian Helba,\n  Veronica Vilaplana, Ofer Reiter, Cristina Carrera, Alicia Barreiro, Allan C.\n  Halpern, Susana Puig, Josep Malvehy", "title": "BCN20000: Dermoscopic Lesions in the Wild", "comments": "Abstract for BCN20000", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article summarizes the BCN20000 dataset, composed of 19424 dermoscopic\nimages of skin lesions captured from 2010 to 2016 in the facilities of the\nHospital Cl\\'inic in Barcelona. With this dataset, we aim to study the problem\nof unconstrained classification of dermoscopic images of skin cancer, including\nlesions found in hard-to-diagnose locations (nails and mucosa), large lesions\nwhich do not fit in the aperture of the dermoscopy device, and hypo-pigmented\nlesions. The BCN20000 will be provided to the participants of the ISIC\nChallenge 2019, where they will be asked to train algorithms to classify\ndermoscopic images of skin cancer automatically.\n", "versions": [{"version": "v1", "created": "Tue, 6 Aug 2019 11:16:07 GMT"}, {"version": "v2", "created": "Fri, 30 Aug 2019 09:42:42 GMT"}], "update_date": "2019-09-02", "authors_parsed": [["Combalia", "Marc", ""], ["Codella", "Noel C. F.", ""], ["Rotemberg", "Veronica", ""], ["Helba", "Brian", ""], ["Vilaplana", "Veronica", ""], ["Reiter", "Ofer", ""], ["Carrera", "Cristina", ""], ["Barreiro", "Alicia", ""], ["Halpern", "Allan C.", ""], ["Puig", "Susana", ""], ["Malvehy", "Josep", ""]]}, {"id": "1908.02300", "submitter": "Dogancan Temel", "authors": "Dogancan Temel and Melvin J. Mathew and Ghassan AlRegib and Yousuf M.\n  Khalifa", "title": "Relative Afferent Pupillary Defect Screening through Transfer Learning", "comments": "8 pages, 7 figures, 4 tables. IEEE Journal of Biomedical and Health\n  Informatics, 2019", "journal-ref": null, "doi": "10.1109/JBHI.2019.2933773", "report-no": null, "categories": "cs.CV eess.IV eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Abnormalities in pupillary light reflex can indicate optic nerve disorders\nthat may lead to permanent visual loss if not diagnosed in an early stage. In\nthis study, we focus on relative afferent pupillary defect (RAPD), which is\nbased on the difference between the reactions of the eyes when they are exposed\nto light stimuli. Incumbent RAPD assessment methods are based on subjective\npractices that can lead to unreliable measurements. To eliminate subjectivity\nand obtain reliable measurements, we introduced an automated framework to\ndetect RAPD. For validation, we conducted a clinical study with\nlab-on-a-headset, which can perform automated light reflex test. In addition to\nbenchmarking handcrafted algorithms, we proposed a transfer learning-based\napproach that transformed a deep learning-based generic object recognition\nalgorithm into a pupil detector. Based on the conducted experiments, proposed\nalgorithm RAPDNet can achieve a sensitivity and a specificity of 90.6% over 64\ntest cases in a balanced set, which corresponds to an AUC of 0.929 in ROC\nanalysis. According to our benchmark with three handcrafted algorithms and nine\nperformance metrics, RAPDNet outperforms all other algorithms in every\nperformance category.\n", "versions": [{"version": "v1", "created": "Tue, 6 Aug 2019 18:03:47 GMT"}], "update_date": "2019-08-08", "authors_parsed": [["Temel", "Dogancan", ""], ["Mathew", "Melvin J.", ""], ["AlRegib", "Ghassan", ""], ["Khalifa", "Yousuf M.", ""]]}, {"id": "1908.02353", "submitter": "Flavio de Barros Vidal", "authors": "Lucas F. Porto, Laise N. Correia Lima, Ademir Franco, Donald M.\n  Pianto, Carlos Eduardo Machado Palhares, Donald M. Pianto and Flavio de\n  Barros Vidal", "title": "Estimating sex and age for forensic applications using machine learning\n  based on facial measurements from frontal cephalometric landmarks", "comments": "17 pages, 17 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Facial analysis permits many investigations some of the most important of\nwhich are craniofacial identification, facial recognition, and age and sex\nestimation. In forensics, photo-anthropometry describes the study of facial\ngrowth and allows the identification of patterns in facial skull development by\nusing a group of cephalometric landmarks to estimate anthropological\ninformation. In several areas, automation of manual procedures has achieved\nadvantages over and similar measurement confidence as a forensic expert. This\nmanuscript presents an approach using photo-anthropometric indexes, generated\nfrom frontal faces cephalometric landmarks, to create an artificial neural\nnetwork classifier that allows the estimation of anthropological information,\nin this specific case age and sex. The work is focused on four tasks: i) sex\nestimation over ages from 5 to 22 years old, evaluating the interference of age\non sex estimation; ii) age estimation from photo-anthropometric indexes for\nfour age intervals (1 year, 2 years, 4 years and 5 years); iii) age group\nestimation for thresholds of over 14 and over 18 years old; and; iv) the\nprovision of a new data set, available for academic purposes only, with a large\nand complete set of facial photo-anthropometric points marked and checked by\nforensic experts, measured from over 18,000 faces of individuals from Brazil\nover the last 4 years. The proposed classifier obtained significant results,\nusing this new data set, for the sex estimation of individuals over 14 years\nold, achieving accuracy values greater than 0.85 by the F_1 measure. For age\nestimation, the accuracy results are 0.72 for measure with an age interval of 5\nyears. For the age group estimation, the measures of accuracy are greater than\n0.93 and 0.83 for thresholds of 14 and 18 years, respectively.\n", "versions": [{"version": "v1", "created": "Tue, 6 Aug 2019 20:33:11 GMT"}], "update_date": "2020-09-10", "authors_parsed": [["Porto", "Lucas F.", ""], ["Lima", "Laise N. Correia", ""], ["Franco", "Ademir", ""], ["Pianto", "Donald M.", ""], ["Palhares", "Carlos Eduardo Machado", ""], ["Pianto", "Donald M.", ""], ["Vidal", "Flavio de Barros", ""]]}, {"id": "1908.02391", "submitter": "Bojana Gajic", "authors": "Bojana Gajic, Ariel Amato, Ramon Baldrich, Carlo Gatta", "title": "Bag of Negatives for Siamese Architectures", "comments": "accepted for BMVC2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Training a Siamese architecture for re-identification with a large number of\nidentities is a challenging task due to the difficulty of finding relevant\nnegative samples efficiently. In this work we present Bag of Negatives (BoN), a\nmethod for accelerated and improved training of Siamese networks that scales\nwell on datasets with a very large number of identities. BoN is an efficient\nand loss-independent method, able to select a bag of high quality negatives,\nbased on a novel online hashing strategy.\n", "versions": [{"version": "v1", "created": "Tue, 6 Aug 2019 22:44:34 GMT"}], "update_date": "2019-08-08", "authors_parsed": [["Gajic", "Bojana", ""], ["Amato", "Ariel", ""], ["Baldrich", "Ramon", ""], ["Gatta", "Carlo", ""]]}, {"id": "1908.02422", "submitter": "Zhanzhan Cheng", "authors": "Chengwei Zhang, Yunlu Xu, Zhanzhan Cheng, Yi Niu, Shiliang Pu, Fei Wu\n  and Futai Zou", "title": "Adversarial Seeded Sequence Growing for Weakly-Supervised Temporal\n  Action Localization", "comments": "To be appeared in ACM MM2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Temporal action localization is an important yet challenging research topic\ndue to its various applications. Since the frame-level or segment-level\nannotations of untrimmed videos require amounts of labor expenditure, studies\non the weakly-supervised action detection have been springing up. However, most\nof existing frameworks rely on Class Activation Sequence (CAS) to localize\nactions by minimizing the video-level classification loss, which exploits the\nmost discriminative parts of actions but ignores the minor regions. In this\npaper, we propose a novel weakly-supervised framework by adversarial learning\nof two modules for eliminating such demerits. Specifically, the first module is\ndesigned as a well-designed Seeded Sequence Growing (SSG) Network for\nprogressively extending seed regions (namely the highly reliable regions\ninitialized by a CAS-based framework) to their expected boundaries. The second\nmodule is a specific classifier for mining trivial or incomplete action\nregions, which is trained on the shared features after erasing the seeded\nregions activated by SSG. In this way, a whole network composed of these two\nmodules can be trained in an adversarial manner. The goal of the adversary is\nto mine features that are difficult for the action classifier. That is, erasion\nfrom SSG will force the classifier to discover minor or even new action regions\non the input feature sequence, and the classifier will drive the seeds to grow,\nalternately. At last, we could obtain the action locations and categories from\nthe well-trained SSG and the classifier. Extensive experiments on two public\nbenchmarks THUMOS'14 and ActivityNet1.3 demonstrate the impressive performance\nof our proposed method compared with the state-of-the-arts.\n", "versions": [{"version": "v1", "created": "Wed, 7 Aug 2019 02:33:18 GMT"}], "update_date": "2019-08-08", "authors_parsed": [["Zhang", "Chengwei", ""], ["Xu", "Yunlu", ""], ["Cheng", "Zhanzhan", ""], ["Niu", "Yi", ""], ["Pu", "Shiliang", ""], ["Wu", "Fei", ""], ["Zou", "Futai", ""]]}, {"id": "1908.02426", "submitter": "Jing Cheng", "authors": "Jing Cheng, Haifeng Wang, Leslie Ying, Dong Liang", "title": "Model Learning: Primal Dual Networks for Fast MR imaging", "comments": "accepted in MICCAI2019. arXiv admin note: text overlap with\n  arXiv:1906.08143", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG physics.med-ph stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Magnetic resonance imaging (MRI) is known to be a slow imaging modality and\nundersampling in k-space has been used to increase the imaging speed. However,\nimage reconstruction from undersampled k-space data is an ill-posed inverse\nproblem. Iterative algorithms based on compressed sensing have been used to\naddress the issue. In this work, we unroll the iterations of the primal-dual\nhybrid gradient algorithm to a learnable deep network architecture, and\ngradually relax the constraints to reconstruct MR images from highly\nundersampled k-space data. The proposed method combines the theoretical\nconvergence guarantee of optimi-zation methods with the powerful learning\ncapability of deep networks. As the constraints are gradually relaxed, the\nreconstruction model is finally learned from the training data by updating in\nk-space and image domain alternatively. Experi-ments on in vivo MR data\ndemonstrate that the proposed method achieves supe-rior MR reconstructions from\nhighly undersampled k-space data over other state-of-the-art image\nreconstruction methods.\n", "versions": [{"version": "v1", "created": "Wed, 7 Aug 2019 02:59:08 GMT"}], "update_date": "2019-08-08", "authors_parsed": [["Cheng", "Jing", ""], ["Wang", "Haifeng", ""], ["Ying", "Leslie", ""], ["Liang", "Dong", ""]]}, {"id": "1908.02435", "submitter": "Andras Rozsa", "authors": "Andras Rozsa and Terrance E. Boult", "title": "Improved Adversarial Robustness by Reducing Open Space Risk via Tent\n  Activations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adversarial examples contain small perturbations that can remain\nimperceptible to human observers but alter the behavior of even the best\nperforming deep learning models and yield incorrect outputs. Since their\ndiscovery, adversarial examples have drawn significant attention in machine\nlearning: researchers try to reveal the reasons for their existence and improve\nthe robustness of machine learning models to adversarial perturbations. The\nstate-of-the-art defense is the computationally expensive and very time\nconsuming adversarial training via projected gradient descent (PGD). We\nhypothesize that adversarial attacks exploit the open space risk of classic\nmonotonic activation functions. This paper introduces the tent activation\nfunction with bounded open space risk and shows that tents make deep learning\nmodels more robust to adversarial attacks. We demonstrate on the MNIST dataset\nthat a classifier with tents yields an average accuracy of 91.8% against six\nwhite-box adversarial attacks, which is more than 15 percentage points above\nthe state of the art. On the CIFAR-10 dataset, our approach improves the\naverage accuracy against the six white-box adversarial attacks to 73.5% from\n41.8% achieved by adversarial training via PGD.\n", "versions": [{"version": "v1", "created": "Wed, 7 Aug 2019 04:11:01 GMT"}], "update_date": "2019-08-08", "authors_parsed": [["Rozsa", "Andras", ""], ["Boult", "Terrance E.", ""]]}, {"id": "1908.02436", "submitter": "Megha Nawhal", "authors": "Zhiwei Deng, Megha Nawhal, Lili Meng, Greg Mori", "title": "Continuous Graph Flow", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose Continuous Graph Flow, a generative continuous flow\nbased method that aims to model complex distributions of graph-structured data.\nOnce learned, the model can be applied to an arbitrary graph, defining a\nprobability density over the random variables represented by the graph. It is\nformulated as an ordinary differential equation system with shared and reusable\nfunctions that operate over the graphs. This leads to a new type of neural\ngraph message passing scheme that performs continuous message passing over\ntime. This class of models offers several advantages: a flexible representation\nthat can generalize to variable data dimensions; ability to model dependencies\nin complex data distributions; reversible and memory-efficient; and exact and\nefficient computation of the likelihood of the data. We demonstrate the\neffectiveness of our model on a diverse set of generation tasks across\ndifferent domains: graph generation, image puzzle generation, and layout\ngeneration from scene graphs. Our proposed model achieves significantly better\nperformance compared to state-of-the-art models.\n", "versions": [{"version": "v1", "created": "Wed, 7 Aug 2019 04:24:48 GMT"}, {"version": "v2", "created": "Sat, 28 Sep 2019 04:34:55 GMT"}], "update_date": "2019-10-01", "authors_parsed": [["Deng", "Zhiwei", ""], ["Nawhal", "Megha", ""], ["Meng", "Lili", ""], ["Mori", "Greg", ""]]}, {"id": "1908.02441", "submitter": "Jiwoong Park", "authors": "Jiwoong Park, Minsik Lee, Hyung Jin Chang, Kyuewang Lee, Jin Young\n  Choi", "title": "Symmetric Graph Convolutional Autoencoder for Unsupervised Graph\n  Representation Learning", "comments": "10 pages, 3 figures, ICCV 2019 accepted", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a symmetric graph convolutional autoencoder which produces a\nlow-dimensional latent representation from a graph. In contrast to the existing\ngraph autoencoders with asymmetric decoder parts, the proposed autoencoder has\na newly designed decoder which builds a completely symmetric autoencoder form.\nFor the reconstruction of node features, the decoder is designed based on\nLaplacian sharpening as the counterpart of Laplacian smoothing of the encoder,\nwhich allows utilizing the graph structure in the whole processes of the\nproposed autoencoder architecture. In order to prevent the numerical\ninstability of the network caused by the Laplacian sharpening introduction, we\nfurther propose a new numerically stable form of the Laplacian sharpening by\nincorporating the signed graphs. In addition, a new cost function which finds a\nlatent representation and a latent affinity matrix simultaneously is devised to\nboost the performance of image clustering tasks. The experimental results on\nclustering, link prediction and visualization tasks strongly support that the\nproposed model is stable and outperforms various state-of-the-art algorithms.\n", "versions": [{"version": "v1", "created": "Wed, 7 Aug 2019 05:08:15 GMT"}], "update_date": "2019-08-08", "authors_parsed": [["Park", "Jiwoong", ""], ["Lee", "Minsik", ""], ["Chang", "Hyung Jin", ""], ["Lee", "Kyuewang", ""], ["Choi", "Jin Young", ""]]}, {"id": "1908.02454", "submitter": "Akshay L Chandra", "authors": "Sai Vikas Desai, Akshay L Chandra, Wei Guo, Seishi Ninomiya, Vineeth N\n  Balasubramanian", "title": "An Adaptive Supervision Framework for Active Learning in Object\n  Detection", "comments": "Accepted in BMVC 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Active learning approaches in computer vision generally involve querying\nstrong labels for data. However, previous works have shown that weak\nsupervision can be effective in training models for vision tasks while greatly\nreducing annotation costs. Using this knowledge, we propose an adaptive\nsupervision framework for active learning and demonstrate its effectiveness on\nthe task of object detection. Instead of directly querying bounding box\nannotations (strong labels) for the most informative samples, we first query\nweak labels and optimize the model. Using a switching condition, the required\nsupervision level can be increased. Our framework requires little to no change\nin model architecture. Our extensive experiments show that the proposed\nframework can be used to train good generalizable models with much lesser\nannotation costs than the state of the art active learning approaches for\nobject detection.\n", "versions": [{"version": "v1", "created": "Wed, 7 Aug 2019 06:20:33 GMT"}, {"version": "v2", "created": "Thu, 10 Oct 2019 15:32:11 GMT"}, {"version": "v3", "created": "Tue, 15 Oct 2019 09:18:26 GMT"}], "update_date": "2019-10-16", "authors_parsed": [["Desai", "Sai Vikas", ""], ["Chandra", "Akshay L", ""], ["Guo", "Wei", ""], ["Ninomiya", "Seishi", ""], ["Balasubramanian", "Vineeth N", ""]]}, {"id": "1908.02460", "submitter": "Chenglong Li", "authors": "Zhengzheng Tu, Yan Ma, Chenglong Li, Jin Tang, Bin Luo", "title": "Edge-guided Non-local Fully Convolutional Network for Salient Object\n  Detection", "comments": "10 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fully Convolutional Neural Network (FCN) has been widely applied to salient\nobject detection recently by virtue of high-level semantic feature extraction,\nbut existing FCN based methods still suffer from continuous striding and\npooling operations leading to loss of spatial structure and blurred edges. To\nmaintain the clear edge structure of salient objects, we propose a novel\nEdge-guided Non-local FCN (ENFNet) to perform edge guided feature learning for\naccurate salient object detection. In a specific, we extract hierarchical\nglobal and local information in FCN to incorporate non-local features for\neffective feature representations. To preserve good boundaries of salient\nobjects, we propose a guidance block to embed edge prior knowledge into\nhierarchical feature maps. The guidance block not only performs feature-wise\nmanipulation but also spatial-wise transformation for effective edge\nembeddings. Our model is trained on the MSRA-B dataset and tested on five\npopular benchmark datasets. Comparing with the state-of-the-art methods, the\nproposed method achieves the best performance on all datasets.\n", "versions": [{"version": "v1", "created": "Wed, 7 Aug 2019 06:51:52 GMT"}, {"version": "v2", "created": "Sun, 11 Aug 2019 08:25:39 GMT"}], "update_date": "2019-08-13", "authors_parsed": [["Tu", "Zhengzheng", ""], ["Ma", "Yan", ""], ["Li", "Chenglong", ""], ["Tang", "Jin", ""], ["Luo", "Bin", ""]]}, {"id": "1908.02484", "submitter": "Eric Brachmann", "authors": "Eric Brachmann and Carsten Rother", "title": "Expert Sample Consensus Applied to Camera Re-Localization", "comments": "ICCV 2019. Supplementary materials included", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fitting model parameters to a set of noisy data points is a common problem in\ncomputer vision. In this work, we fit the 6D camera pose to a set of noisy\ncorrespondences between the 2D input image and a known 3D environment. We\nestimate these correspondences from the image using a neural network. Since the\ncorrespondences often contain outliers, we utilize a robust estimator such as\nRandom Sample Consensus (RANSAC) or Differentiable RANSAC (DSAC) to fit the\npose parameters. When the problem domain, e.g. the space of all 2D-3D\ncorrespondences, is large or ambiguous, a single network does not cover the\ndomain well. Mixture of Experts (MoE) is a popular strategy to divide a problem\ndomain among an ensemble of specialized networks, so called experts, where a\ngating network decides which expert is responsible for a given input. In this\nwork, we introduce Expert Sample Consensus (ESAC), which integrates DSAC in a\nMoE. Our main technical contribution is an efficient method to train ESAC\njointly and end-to-end. We demonstrate experimentally that ESAC handles two\nreal-world problems better than competing methods, i.e. scalability and\nambiguity. We apply ESAC to fitting simple geometric models to synthetic\nimages, and to camera re-localization for difficult, real datasets.\n", "versions": [{"version": "v1", "created": "Wed, 7 Aug 2019 08:23:03 GMT"}], "update_date": "2019-08-08", "authors_parsed": [["Brachmann", "Eric", ""], ["Rother", "Carsten", ""]]}, {"id": "1908.02486", "submitter": "Boyuan Jiang", "authors": "Boyuan Jiang, Mengmeng Wang, Weihao Gan, Wei Wu, Junjie Yan", "title": "STM: SpatioTemporal and Motion Encoding for Action Recognition", "comments": "Accepted by ICCV2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spatiotemporal and motion features are two complementary and crucial\ninformation for video action recognition. Recent state-of-the-art methods adopt\na 3D CNN stream to learn spatiotemporal features and another flow stream to\nlearn motion features. In this work, we aim to efficiently encode these two\nfeatures in a unified 2D framework. To this end, we first propose an STM block,\nwhich contains a Channel-wise SpatioTemporal Module (CSTM) to present the\nspatiotemporal features and a Channel-wise Motion Module (CMM) to efficiently\nencode motion features. We then replace original residual blocks in the ResNet\narchitecture with STM blcoks to form a simple yet effective STM network by\nintroducing very limited extra computation cost. Extensive experiments\ndemonstrate that the proposed STM network outperforms the state-of-the-art\nmethods on both temporal-related datasets (i.e., Something-Something v1 & v2\nand Jester) and scene-related datasets (i.e., Kinetics-400, UCF-101, and\nHMDB-51) with the help of encoding spatiotemporal and motion features together.\n", "versions": [{"version": "v1", "created": "Wed, 7 Aug 2019 08:28:38 GMT"}, {"version": "v2", "created": "Fri, 16 Aug 2019 16:12:24 GMT"}], "update_date": "2019-08-19", "authors_parsed": [["Jiang", "Boyuan", ""], ["Wang", "Mengmeng", ""], ["Gan", "Weihao", ""], ["Wu", "Wei", ""], ["Yan", "Junjie", ""]]}, {"id": "1908.02492", "submitter": "Zhengxu Yu", "authors": "Zhengxu Yu, Dong Shen, Zhongming Jin, Jianqiang Huang, Deng Cai,\n  Xian-Sheng Hua", "title": "Progressive Transfer Learning", "comments": "10 pages, 4 figures, journel verison of our published short paper on\n  IJCAI2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Model fine-tuning is a widely used transfer learning approach in person\nRe-identification (ReID) applications, which fine-tuning a pre-trained feature\nextraction model into the target scenario instead of training a model from\nscratch. It is challenging due to the significant variations inside the target\nscenario, e.g., different camera viewpoint, illumination changes, and\nocclusion. These variations result in a gap between the distribution of each\nmini-batch and the whole dataset's distribution when using mini-batch training.\nIn this paper, we study model fine-tuning from the perspective of the\naggregation and utilization of the global information of the dataset when using\nmini-batch training. Specifically, we introduce a novel network structure\ncalled Batch-related Convolutional Cell (BConv-Cell), which progressively\ncollects the global information of the dataset into a latent state and uses it\nto rectify the extracted feature. Based on BConv-Cells, we further proposed the\nProgressive Transfer Learning (PTL) method to facilitate the model fine-tuning\nprocess by jointly optimizing the BConv-Cells and the pre-trained ReID model.\nEmpirical experiments show that our proposal can improve the performance of the\nReID model greatly on MSMT17, Market-1501, CUHK03 and DukeMTMC-reID datasets.\nMoreover, we extend our proposal to the general image classification task. The\nexperiments in several image classification benchmark datasets demonstrate that\nour proposal can significantly improve the performance of baseline models. The\ncode has been released at \\url{https://github.com/ZJULearning/PTL}\n", "versions": [{"version": "v1", "created": "Wed, 7 Aug 2019 08:52:26 GMT"}, {"version": "v2", "created": "Thu, 8 Aug 2019 05:39:45 GMT"}, {"version": "v3", "created": "Fri, 6 Nov 2020 08:22:17 GMT"}], "update_date": "2020-11-09", "authors_parsed": [["Yu", "Zhengxu", ""], ["Shen", "Dong", ""], ["Jin", "Zhongming", ""], ["Huang", "Jianqiang", ""], ["Cai", "Deng", ""], ["Hua", "Xian-Sheng", ""]]}, {"id": "1908.02498", "submitter": "Gihyun Kwon", "authors": "Gihyun Kwon, Chihye Han, Dae-shik Kim", "title": "Generation of 3D Brain MRI Using Auto-Encoding Generative Adversarial\n  Networks", "comments": "8.5 pages, 4 figures, Accepted by the 22nd International Conference\n  on Medical Image Computing and Computer Assisted Intervention (MICCAI 2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As deep learning is showing unprecedented success in medical image analysis\ntasks, the lack of sufficient medical data is emerging as a critical problem.\nWhile recent attempts to solve the limited data problem using Generative\nAdversarial Networks (GAN) have been successful in generating realistic images\nwith diversity, most of them are based on image-to-image translation and thus\nrequire extensive datasets from different domains. Here, we propose a novel\nmodel that can successfully generate 3D brain MRI data from random vectors by\nlearning the data distribution. Our 3D GAN model solves both image blurriness\nand mode collapse problems by leveraging alpha-GAN that combines the advantages\nof Variational Auto-Encoder (VAE) and GAN with an additional code discriminator\nnetwork. We also use the Wasserstein GAN with Gradient Penalty (WGAN-GP) loss\nto lower the training instability. To demonstrate the effectiveness of our\nmodel, we generate new images of normal brain MRI and show that our model\noutperforms baseline models in both quantitative and qualitative measurements.\nWe also train the model to synthesize brain disorder MRI data to demonstrate\nthe wide applicability of our model. Our results suggest that the proposed\nmodel can successfully generate various types and modalities of 3D whole brain\nvolumes from a small set of training data.\n", "versions": [{"version": "v1", "created": "Wed, 7 Aug 2019 09:33:03 GMT"}], "update_date": "2019-08-08", "authors_parsed": [["Kwon", "Gihyun", ""], ["Han", "Chihye", ""], ["Kim", "Dae-shik", ""]]}, {"id": "1908.02507", "submitter": "Yujie Yuan", "authors": "Yu-Jie Yuan, Yu-Kun Lai, Jie Yang, Hongbo Fu, Lin Gao", "title": "Mesh Variational Autoencoders with Edge Contraction Pooling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  3D shape analysis is an important research topic in computer vision and\ngraphics. While existing methods have generalized image-based deep learning to\nmeshes using graph-based convolutions, the lack of an effective pooling\noperation restricts the learning capability of their networks. In this paper,\nwe propose a novel pooling operation for mesh datasets with the same\nconnectivity but different geometry, by building a mesh hierarchy using mesh\nsimplification. For this purpose, we develop a modified mesh simplification\nmethod to avoid generating highly irregularly sized triangles. Our pooling\noperation effectively encodes the correspondence between coarser and finer\nmeshes in the hierarchy. We then present a variational auto-encoder structure\nwith the edge contraction pooling and graph-based convolutions, to explore\nprobability latent spaces of 3D surfaces. Our network requires far fewer\nparameters than the original mesh VAE and thus can handle denser models thanks\nto our new pooling operation and convolutional kernels. Our evaluation also\nshows that our method has better generalization ability and is more reliable in\nvarious applications, including shape generation, shape interpolation and shape\nembedding.\n", "versions": [{"version": "v1", "created": "Wed, 7 Aug 2019 09:59:08 GMT"}], "update_date": "2019-08-08", "authors_parsed": [["Yuan", "Yu-Jie", ""], ["Lai", "Yu-Kun", ""], ["Yang", "Jie", ""], ["Fu", "Hongbo", ""], ["Gao", "Lin", ""]]}, {"id": "1908.02511", "submitter": "Dmitry Nikulin", "authors": "Dmitry Nikulin, Anastasia Ianina, Vladimir Aliev, Sergey Nikolenko", "title": "Free-Lunch Saliency via Attention in Atari Agents", "comments": "2019 ICCV Workshop on Interpreting and Explaining Visual Artificial\n  Intelligence Models. 15 pages, 14 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new approach to visualize saliency maps for deep neural network\nmodels and apply it to deep reinforcement learning agents trained on Atari\nenvironments. Our method adds an attention module that we call FLS (Free Lunch\nSaliency) to the feature extractor from an established baseline (Mnih et al.,\n2015). This addition results in a trainable model that can produce saliency\nmaps, i.e., visualizations of the importance of different parts of the input\nfor the agent's current decision making. We show experimentally that a network\nwith an FLS module exhibits performance similar to the baseline (i.e., it is\n\"free\", with no performance cost) and can be used as a drop-in replacement for\nreinforcement learning agents. We also design another feature extractor that\nscores slightly lower but provides higher-fidelity visualizations. In addition\nto attained scores, we report saliency metrics evaluated on the Atari-HEAD\ndataset of human gameplay.\n", "versions": [{"version": "v1", "created": "Wed, 7 Aug 2019 10:10:45 GMT"}, {"version": "v2", "created": "Wed, 30 Oct 2019 17:42:50 GMT"}], "update_date": "2019-11-01", "authors_parsed": [["Nikulin", "Dmitry", ""], ["Ianina", "Anastasia", ""], ["Aliev", "Vladimir", ""], ["Nikolenko", "Sergey", ""]]}, {"id": "1908.02564", "submitter": "Ghazal Ghazaei", "authors": "Ghazal Ghazaei, Federico Tombari, Nassir Navab, Kianoush Nazarpour", "title": "Grasp Type Estimation for Myoelectric Prostheses using Point Cloud\n  Feature Learning", "comments": null, "journal-ref": "Workshop on Human-aiding Robotics, International Conference on\n  Intelligent Robots and Systems (IROS) 2018", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Prosthetic hands can help people with limb difference to return to their life\nroutines. Commercial prostheses, however have several limitations in providing\nan acceptable dexterity. We approach these limitations by augmenting the\nprosthetic hands with an off-the-shelf depth sensor to enable the prosthesis to\nsee the object's depth, record a single view (2.5-D) snapshot, and estimate an\nappropriate grasp type; using a deep network architecture based on 3D point\nclouds called PointNet. The human can act as the supervisor throughout the\nprocedure by accepting or refusing the suggested grasp type. We achieved the\ngrasp classification accuracy of up to 88%. Contrary to the case of the RGB\ndata, the depth data provides all the necessary object shape information, which\nis required for grasp recognition. The PointNet not only enables using 3-D data\nin practice, but it also prevents excessive computations. Augmentation of the\nprosthetic hands with such a semi-autonomous system can lead to better\ndifferentiation of grasp types, less burden on user, and better performance.\n", "versions": [{"version": "v1", "created": "Wed, 7 Aug 2019 12:23:56 GMT"}], "update_date": "2019-08-08", "authors_parsed": [["Ghazaei", "Ghazal", ""], ["Tombari", "Federico", ""], ["Navab", "Nassir", ""], ["Nazarpour", "Kianoush", ""]]}, {"id": "1908.02582", "submitter": "Samuel Budd", "authors": "Samuel Budd, Matthew Sinclair, Bishesh Khanal, Jacqueline Matthew,\n  David Lloyd, Alberto Gomez, Nicolas Toussaint, Emma Robinson and Bernhard\n  Kainz", "title": "Confident Head Circumference Measurement from Ultrasound with Real-time\n  Feedback for Sonographers", "comments": "Accepted at MICCAI 2019; Demo video available on Twitter\n  (@sambuddinc)", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Manual estimation of fetal Head Circumference (HC) from Ultrasound (US) is a\nkey biometric for monitoring the healthy development of fetuses. Unfortunately,\nsuch measurements are subject to large inter-observer variability, resulting in\nlow early-detection rates of fetal abnormalities. To address this issue, we\npropose a novel probabilistic Deep Learning approach for real-time automated\nestimation of fetal HC. This system feeds back statistics on measurement\nrobustness to inform users how confident a deep neural network is in evaluating\nsuitable views acquired during free-hand ultrasound examination. In real-time\nscenarios, this approach may be exploited to guide operators to scan planes\nthat are as close as possible to the underlying distribution of training\nimages, for the purpose of improving inter-operator consistency. We train on\nfree-hand ultrasound data from over 2000 subjects (2848 training/540 test) and\nshow that our method is able to predict HC measurements within 1.81$\\pm$1.65mm\ndeviation from the ground truth, with 50% of the test images fully contained\nwithin the predicted confidence margins, and an average of 1.82$\\pm$1.78mm\ndeviation from the margin for the remaining cases that are not fully contained.\n", "versions": [{"version": "v1", "created": "Wed, 7 Aug 2019 12:35:54 GMT"}], "update_date": "2019-08-08", "authors_parsed": [["Budd", "Samuel", ""], ["Sinclair", "Matthew", ""], ["Khanal", "Bishesh", ""], ["Matthew", "Jacqueline", ""], ["Lloyd", "David", ""], ["Gomez", "Alberto", ""], ["Toussaint", "Nicolas", ""], ["Robinson", "Emma", ""], ["Kainz", "Bernhard", ""]]}, {"id": "1908.02620", "submitter": "Yunxiang Zhang", "authors": "Yunxiang Zhang, Chenglong Zhao, Bingbing Ni, Jian Zhang, Haoran Deng", "title": "Exploiting Channel Similarity for Accelerating Deep Convolutional Neural\n  Networks", "comments": "14 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To address the limitations of existing magnitude-based pruning algorithms in\ncases where model weights or activations are of large and similar magnitude, we\npropose a novel perspective to discover parameter redundancy among channels and\naccelerate deep CNNs via channel pruning. Precisely, we argue that channels\nrevealing similar feature information have functional overlap and that most\nchannels within each such similarity group can be removed without compromising\nmodel's representational power. After deriving an effective metric for\nevaluating channel similarity through probabilistic modeling, we introduce a\npruning algorithm via hierarchical clustering of channels. In particular, the\nproposed algorithm does not rely on sparsity training techniques or complex\ndata-driven optimization and can be directly applied to pre-trained models.\nExtensive experiments on benchmark datasets strongly demonstrate the superior\nacceleration performance of our approach over prior arts. On ImageNet, our\npruned ResNet-50 with 30% FLOPs reduced outperforms the baseline model.\n", "versions": [{"version": "v1", "created": "Tue, 6 Aug 2019 12:44:30 GMT"}], "update_date": "2019-08-08", "authors_parsed": [["Zhang", "Yunxiang", ""], ["Zhao", "Chenglong", ""], ["Ni", "Bingbing", ""], ["Zhang", "Jian", ""], ["Deng", "Haoran", ""]]}, {"id": "1908.02625", "submitter": "Jamie O'Reilly", "authors": "Jamie A. O'Reilly, Manas Sangworasil and Takenobu Matsuura", "title": "Kidney and Kidney Tumor Segmentation using a Logical Ensemble of U-nets\n  with Volumetric Validation", "comments": "9 pages, 4 figures, 1 table, competition submission manuscript", "journal-ref": null, "doi": "10.24926/548719.082", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automated medical image segmentation is a priority research area for\ncomputational methods. In particular, detection of cancerous tumors represents\na current challenge in this area with potential for real-world impact. This\npaper describes a method developed in response to the 2019 Kidney Tumor\nSegmentation Challenge (KiTS19). Axial computed tomography (CT) scans from 210\nkidney cancer patients were used to develop and evaluate this automatic\nsegmentation method based on a logical ensemble of fully-convolutional network\n(FCN) architectures, followed by volumetric validation. Data was pre-processed\nusing conventional computer vision techniques, thresholding, histogram\nequalization, morphological operations, centering, zooming and resizing. Three\nbinary FCN segmentation models were trained to classify kidney and tumor (2),\nand only tumor (1), respectively. Model output images were stacked and\nvolumetrically validated to produce the final segmentation for each patient\nscan. The average F1 score from kidney and tumor pixel classifications was\ncalculated as 0.6758 using preprocessed images and annotations; although\nrestoring to the original image format reduced this score. It remains to be\nseen how this compares to other solutions.\n", "versions": [{"version": "v1", "created": "Wed, 7 Aug 2019 13:28:26 GMT"}], "update_date": "2019-11-06", "authors_parsed": [["O'Reilly", "Jamie A.", ""], ["Sangworasil", "Manas", ""], ["Matsuura", "Takenobu", ""]]}, {"id": "1908.02626", "submitter": "Marco Rudolph", "authors": "Marco Rudolph, Bastian Wandt and Bodo Rosenhahn", "title": "Structuring Autoencoders", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose Structuring AutoEncoders (SAE). SAEs are neural\nnetworks which learn a low dimensional representation of data which are\nadditionally enriched with a desired structure in this low dimensional space.\nWhile traditional Autoencoders have proven to structure data naturally they\nfail to discover semantic structure that is hard to recognize in the raw data.\nThe SAE solves the problem by enhancing a traditional Autoencoder using weak\nsupervision to form a structured latent space. In the experiments we\ndemonstrate, that the structured latent space allows for a much more efficient\ndata representation for further tasks such as classification for sparsely\nlabeled data, an efficient choice of data to label, and morphing between\nclasses. To demonstrate the general applicability of our method, we show\nexperiments on the benchmark image datasets MNIST, Fashion-MNIST, DeepFashion2\nand on a dataset of 3D human shapes.\n", "versions": [{"version": "v1", "created": "Wed, 7 Aug 2019 13:29:11 GMT"}], "update_date": "2019-08-20", "authors_parsed": [["Rudolph", "Marco", ""], ["Wandt", "Bastian", ""], ["Rosenhahn", "Bodo", ""]]}, {"id": "1908.02632", "submitter": "Chen Shen", "authors": "Chen Shen, Rongrong Ji, Fuhai Chen, Xiaoshuai Sun, Xiangming Li", "title": "Scene-based Factored Attention for Image Captioning", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image captioning has attracted ever-increasing research attention in the\nmultimedia community. To this end, most cutting-edge works rely on an\nencoder-decoder framework with attention mechanisms, which have achieved\nremarkable progress. However, such a framework does not consider scene concepts\nto attend visual information, which leads to sentence bias in caption\ngeneration and defects the performance correspondingly. We argue that such\nscene concepts capture higher-level visual semantics and serve as an important\ncue in describing images. In this paper, we propose a novel scene-based\nfactored attention module for image captioning. Specifically, the proposed\nmodule first embeds the scene concepts into factored weights explicitly and\nattends the visual information extracted from the input image. Then, an\nadaptive LSTM is used to generate captions for specific scene types.\nExperimental results on Microsoft COCO benchmark show that the proposed\nscene-based attention module improves model performance a lot, which\noutperforms the state-of-the-art approaches under various evaluation metrics.\n", "versions": [{"version": "v1", "created": "Wed, 7 Aug 2019 13:43:25 GMT"}, {"version": "v2", "created": "Sun, 18 Aug 2019 03:37:53 GMT"}, {"version": "v3", "created": "Mon, 2 Sep 2019 16:11:16 GMT"}], "update_date": "2019-09-04", "authors_parsed": [["Shen", "Chen", ""], ["Ji", "Rongrong", ""], ["Chen", "Fuhai", ""], ["Sun", "Xiaoshuai", ""], ["Li", "Xiangming", ""]]}, {"id": "1908.02635", "submitter": "Fabian Brickwedde", "authors": "Fabian Brickwedde, Steffen Abraham, Rudolf Mester", "title": "Mono-Stixels: Monocular depth reconstruction of dynamic street scenes", "comments": "2018 IEEE International Conference on Robotics and Automation (ICRA\n  2018)", "journal-ref": null, "doi": "10.1109/ICRA.2018.8460490", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present mono-stixels, a compact environment representation\nspecially designed for dynamic street scenes. Mono-stixels are a novel approach\nto estimate stixels from a monocular camera sequence instead of the\ntraditionally used stereo depth measurements. Our approach jointly infers the\ndepth, motion and semantic information of the dynamic scene as a 1D energy\nminimization problem based on optical flow estimates, pixel-wise semantic\nsegmentation and camera motion. The optical flow of a stixel is described by a\nhomography. By applying the mono-stixel model the degrees of freedom of a\nstixel-homography are reduced to only up to two degrees of freedom.\nFurthermore, we exploit a scene model and semantic information to handle moving\nobjects. In our experiments we use the public available DeepFlow for optical\nflow estimation and FCN8s for the semantic information as inputs and show on\nthe KITTI 2015 dataset that mono-stixels provide a compact and reliable depth\nreconstruction of both the static and moving parts of the scene. Thereby,\nmono-stixels overcome the limitation to static scenes of previous\nstructure-from-motion approaches.\n", "versions": [{"version": "v1", "created": "Wed, 7 Aug 2019 13:54:51 GMT"}], "update_date": "2019-08-08", "authors_parsed": [["Brickwedde", "Fabian", ""], ["Abraham", "Steffen", ""], ["Mester", "Rudolf", ""]]}, {"id": "1908.02648", "submitter": "Seongmin Hwang", "authors": "Seongmin Hwang, Gwanghuyn Yu, Cheolkon Jung and Jinyoung Kim", "title": "Attention-Aware Linear Depthwise Convolution for Single Image\n  Super-Resolution", "comments": "9 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although deep convolutional neural networks (CNNs) have obtained outstanding\nperformance in image superresolution (SR), their computational cost increases\ngeometrically as CNN models get deeper and wider. Meanwhile, the features of\nintermediate layers are treated equally across the channel, thus hindering the\nrepresentational capability of CNNs. In this paper, we propose an\nattention-aware linear depthwise network to address the problems for single\nimage SR, named ALDNet. Specifically, linear depthwise convolution allows\nCNN-based SR models to preserve useful information for reconstructing a\nsuper-resolved image while reducing computational burden. Furthermore, we\ndesign an attention-aware branch that enhances the representation ability of\ndepthwise convolution layers by making full use of depthwise filter\ninterdependency. Experiments on publicly available benchmark datasets show that\nALDNet achieves superior performance to traditional depthwise separable\nconvolutions in terms of quantitative measurements and visual quality.\n", "versions": [{"version": "v1", "created": "Wed, 7 Aug 2019 14:09:46 GMT"}, {"version": "v2", "created": "Mon, 7 Oct 2019 10:22:59 GMT"}, {"version": "v3", "created": "Fri, 29 Nov 2019 06:17:16 GMT"}], "update_date": "2019-12-02", "authors_parsed": [["Hwang", "Seongmin", ""], ["Yu", "Gwanghuyn", ""], ["Jung", "Cheolkon", ""], ["Kim", "Jinyoung", ""]]}, {"id": "1908.02650", "submitter": "Antoine Pirovano", "authors": "Antoine Pirovano and Leandro G. Almeida and Said Ladjal", "title": "Regression Constraint for an Explainable Cervical Cancer Classifier", "comments": "5 pages, 9 figures, accepted at GRETSI 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article adresses the problem of automatic squamous cells classification\nfor cervical cancer screening using Deep Learning methods. We study different\narchitectures on a public dataset called Herlev dataset, which consists in\nclassifying cells, obtained by cervical pap smear, regarding the severity of\nthe abnormalities they represent. Furthermore, we use an attribution method to\nunderstand which cytomorphological features are actually learned as\ndiscriminative to classify severity of the abnormalities. Through this paper,\nwe show how we trained a performant classifier: 74.5\\% accuracy on severity\nclassification and 94\\% accuracy on normal/abnormal classification.\n", "versions": [{"version": "v1", "created": "Wed, 7 Aug 2019 14:12:04 GMT"}, {"version": "v2", "created": "Mon, 19 Aug 2019 07:52:40 GMT"}], "update_date": "2019-08-20", "authors_parsed": [["Pirovano", "Antoine", ""], ["Almeida", "Leandro G.", ""], ["Ladjal", "Said", ""]]}, {"id": "1908.02660", "submitter": "Kaiyu Yang", "authors": "Kaiyu Yang, Olga Russakovsky, Jia Deng", "title": "SpatialSense: An Adversarially Crowdsourced Benchmark for Spatial\n  Relation Recognition", "comments": "Accepted to ICCV 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding the spatial relations between objects in images is a\nsurprisingly challenging task. A chair may be \"behind\" a person even if it\nappears to the left of the person in the image (depending on which way the\nperson is facing). Two students that appear close to each other in the image\nmay not in fact be \"next to\" each other if there is a third student between\nthem.\n  We introduce SpatialSense, a dataset specializing in spatial relation\nrecognition which captures a broad spectrum of such challenges, allowing for\nproper benchmarking of computer vision techniques. SpatialSense is constructed\nthrough adversarial crowdsourcing, in which human annotators are tasked with\nfinding spatial relations that are difficult to predict using simple cues such\nas 2D spatial configuration or language priors. Adversarial crowdsourcing\nsignificantly reduces dataset bias and samples more interesting relations in\nthe long tail compared to existing datasets. On SpatialSense, state-of-the-art\nrecognition models perform comparably to simple baselines, suggesting that they\nrely on straightforward cues instead of fully reasoning about this complex\ntask. The SpatialSense benchmark provides a path forward to advancing the\nspatial reasoning capabilities of computer vision systems. The dataset and code\nare available at https://github.com/princeton-vl/SpatialSense.\n", "versions": [{"version": "v1", "created": "Wed, 7 Aug 2019 14:41:30 GMT"}, {"version": "v2", "created": "Thu, 29 Aug 2019 20:30:38 GMT"}], "update_date": "2019-09-02", "authors_parsed": [["Yang", "Kaiyu", ""], ["Russakovsky", "Olga", ""], ["Deng", "Jia", ""]]}, {"id": "1908.02664", "submitter": "Jon\\'a\\v{s} \\v{S}er\\'ych", "authors": "Jon\\'a\\v{s} \\v{S}er\\'ych and Ji\\v{r}\\'i Matas", "title": "Visual Coin-Tracking: Tracking of Planar Double-Sided Objects", "comments": "Accepted to GCPR 2019. Dataset available at\n  http://cmp.felk.cvut.cz/coin-tracking/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new video analysis problem -- tracking of rigid planar objects\nin sequences where both their sides are visible. Such coin-like objects often\nrotate fast with respect to an arbitrary axis producing unique challenges, such\nas fast incident light and aspect ratio change and rotational motion blur.\nDespite being common, neither tracking sequences containing coin-like objects\nnor suitable algorithm have been published. As a second contribution, we\npresent a novel coin-tracking benchmark containing 17 video sequences annotated\nwith object segmentation masks. Experiments show that the sequences differ\nsignificantly from the ones encountered in standard tracking datasets. We\npropose a baseline coin-tracking method based on convolutional neural network\nsegmentation and explicit pose modeling. Its performance confirms that\ncoin-tracking is an open and challenging problem.\n", "versions": [{"version": "v1", "created": "Wed, 7 Aug 2019 14:53:44 GMT"}], "update_date": "2019-08-08", "authors_parsed": [["\u0160er\u00fdch", "Jon\u00e1\u0161", ""], ["Matas", "Ji\u0159\u00ed", ""]]}, {"id": "1908.02669", "submitter": "Cameron Trotter", "authors": "Cameron Trotter, Georgia Atkinson, Matthew Sharpe, A. Stephen McGough,\n  Nick Wright, Per Berggren", "title": "The Northumberland Dolphin Dataset: A Multimedia Individual Cetacean\n  Dataset for Fine-Grained Categorisation", "comments": "4 pages, 4 figures, submitted to FGVC6 Workshop at CVPR2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Methods for cetacean research include photo-identification (photo-id) and\npassive acoustic monitoring (PAM) which generate thousands of images per\nexpedition that are currently hand categorised by researchers into the\nindividual dolphins sighted. With the vast amount of data obtained it is\ncrucially important to develop a system that is able to categorise this\nquickly. The Northumberland Dolphin Dataset (NDD) is an on-going novel dataset\nproject made up of above and below water images of, and spectrograms of\nwhistles from, white-beaked dolphins. These are produced by photo-id and PAM\ndata collection methods applied off the coast of Northumberland, UK. This\ndataset will aid in building cetacean identification models, reducing the\nnumber of human-hours required to categorise images. Example use cases and\nareas identified for speed up are examined.\n", "versions": [{"version": "v1", "created": "Wed, 7 Aug 2019 15:00:27 GMT"}], "update_date": "2019-08-08", "authors_parsed": [["Trotter", "Cameron", ""], ["Atkinson", "Georgia", ""], ["Sharpe", "Matthew", ""], ["McGough", "A. Stephen", ""], ["Wright", "Nick", ""], ["Berggren", "Per", ""]]}, {"id": "1908.02671", "submitter": "Bingzhang Hu", "authors": "Yuan Zhou and Bingzhang Hu and and Jun He and Yu Guan and Ling Shao", "title": "Dual-reference Age Synthesis", "comments": null, "journal-ref": "Neurocomputing(2020)", "doi": "10.1016/j.neucom.2020.06.023", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Age synthesis methods typically take a single image as input and use a\nspecific number to control the age of the generated image. In this paper, we\npropose a novel framework taking two images as inputs, named dual-reference age\nsynthesis (DRAS), which approaches the task differently; instead of using\n\"hard\" age information, i.e. a fixed number, our model determines the target\nage in a \"soft\" way, by employing a second reference image. Specifically, the\nproposed framework consists of an identity agent, an age agent and a generative\nadversarial network. It takes two images as input - an identity reference and\nan age reference - and outputs a new image that shares corresponding features\nwith each. Experimental results on two benchmark datasets (UTKFace and CACD)\ndemonstrate the appealing performance and flexibility of the proposed\nframework.\n", "versions": [{"version": "v1", "created": "Wed, 7 Aug 2019 15:06:08 GMT"}, {"version": "v2", "created": "Sat, 19 Oct 2019 13:13:01 GMT"}, {"version": "v3", "created": "Sun, 5 Apr 2020 03:09:26 GMT"}, {"version": "v4", "created": "Tue, 14 Apr 2020 06:05:30 GMT"}], "update_date": "2020-06-19", "authors_parsed": [["Zhou", "Yuan", ""], ["Hu", "Bingzhang", ""], ["He", "and Jun", ""], ["Guan", "Yu", ""], ["Shao", "Ling", ""]]}, {"id": "1908.02686", "submitter": "J\\\"org Wagner", "authors": "J\\\"org Wagner, Jan Mathias K\\\"ohler, Tobias Gindele, Leon Hetzel,\n  Jakob Thadd\\\"aus Wiedemer, Sven Behnke", "title": "Interpretable and Fine-Grained Visual Explanations for Convolutional\n  Neural Networks", "comments": "In Proceedings of IEEE Conference on Computer Vision and Pattern\n  Recognition (CVPR), Long Beach, CA, USA, June 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To verify and validate networks, it is essential to gain insight into their\ndecisions, limitations as well as possible shortcomings of training data. In\nthis work, we propose a post-hoc, optimization based visual explanation method,\nwhich highlights the evidence in the input image for a specific prediction. Our\napproach is based on a novel technique to defend against adversarial evidence\n(i.e. faulty evidence due to artefacts) by filtering gradients during\noptimization. The defense does not depend on human-tuned parameters. It enables\nexplanations which are both fine-grained and preserve the characteristics of\nimages, such as edges and colors. The explanations are interpretable, suited\nfor visualizing detailed evidence and can be tested as they are valid model\ninputs. We qualitatively and quantitatively evaluate our approach on a\nmultitude of models and datasets.\n", "versions": [{"version": "v1", "created": "Wed, 7 Aug 2019 15:39:55 GMT"}], "update_date": "2019-08-08", "authors_parsed": [["Wagner", "J\u00f6rg", ""], ["K\u00f6hler", "Jan Mathias", ""], ["Gindele", "Tobias", ""], ["Hetzel", "Leon", ""], ["Wiedemer", "Jakob Thadd\u00e4us", ""], ["Behnke", "Sven", ""]]}, {"id": "1908.02706", "submitter": "Veeru Talreja", "authors": "Veeru Talreja, Matthew C. Valenti, Nasser M. Nasrabadi", "title": "Zero-Shot Deep Hashing and Neural Network Based Error Correction for\n  Face Template Protection", "comments": "arXiv admin note: text overlap with arXiv:1902.04149", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a novel architecture that integrates a deep hashing\nframework with a neural network decoder (NND) for application to face template\nprotection. It improves upon existing face template protection techniques to\nprovide better matching performance with one-shot and multi-shot enrollment. A\nkey novelty of our proposed architecture is that the framework can also be used\nwith zero-shot enrollment. This implies that our architecture does not need to\nbe re-trained even if a new subject is to be enrolled into the system. The\nproposed architecture consists of two major components: a deep hashing (DH)\ncomponent, which is used for robust mapping of face images to their\ncorresponding intermediate binary codes, and a NND component, which corrects\nerrors in the intermediate binary codes that are caused by differences in the\nenrollment and probe biometrics due to factors such as variation in pose,\nillumination, and other factors. The final binary code generated by the NND is\nthen cryptographically hashed and stored as a secure face template in the\ndatabase. The efficacy of our approach with zero-shot, one-shot, and multi-shot\nenrollments is shown for CMU-PIE, Extended Yale B, WVU multimodal and Multi-PIE\nface databases. With zero-shot enrollment, the system achieves approximately\n85% genuine accept rates (GAR) at 0.01% false accept rate (FAR), and with\none-shot and multi-shot enrollments, it achieves approximately 99.95% GAR at\n0.01% FAR, while providing a high level of template security.\n", "versions": [{"version": "v1", "created": "Mon, 5 Aug 2019 19:39:25 GMT"}], "update_date": "2019-08-08", "authors_parsed": [["Talreja", "Veeru", ""], ["Valenti", "Matthew C.", ""], ["Nasrabadi", "Nasser M.", ""]]}, {"id": "1908.02711", "submitter": "Mohsen Ghafoorian", "authors": "Laurens Samson, Nanne van Noord, Olaf Booij, Michael Hofmann,\n  Efstratios Gavves, Mohsen Ghafoorian", "title": "I Bet You Are Wrong: Gambling Adversarial Networks for Structured\n  Semantic Segmentation", "comments": "13 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adversarial training has been recently employed for realizing structured\nsemantic segmentation, in which the aim is to preserve higher-level scene\nstructural consistencies in dense predictions. However, as we show, value-based\ndiscrimination between the predictions from the segmentation network and\nground-truth annotations can hinder the training process from learning to\nimprove structural qualities as well as disabling the network from properly\nexpressing uncertainties. In this paper, we rethink adversarial training for\nsemantic segmentation and propose to formulate the fake/real discrimination\nframework with a correct/incorrect training objective. More specifically, we\nreplace the discriminator with a \"gambler\" network that learns to spot and\ndistribute its budget in areas where the predictions are clearly wrong, while\nthe segmenter network tries to leave no clear clues for the gambler where to\nbet. Empirical evaluation on two road-scene semantic segmentation tasks shows\nthat not only does the proposed method re-enable expressing uncertainties, it\nalso improves pixel-wise and structure-based metrics.\n", "versions": [{"version": "v1", "created": "Wed, 7 Aug 2019 16:27:50 GMT"}], "update_date": "2019-08-08", "authors_parsed": [["Samson", "Laurens", ""], ["van Noord", "Nanne", ""], ["Booij", "Olaf", ""], ["Hofmann", "Michael", ""], ["Gavves", "Efstratios", ""], ["Ghafoorian", "Mohsen", ""]]}, {"id": "1908.02714", "submitter": "Yuki Endo", "authors": "Yoshihiro Kanamori, Yuki Endo", "title": "Relighting Humans: Occlusion-Aware Inverse Rendering for Full-Body Human\n  Images", "comments": "Published at SIGGRAPH Asia 2018 (ACM Transactions on Graphics).\n  Project page with codes, pretrained models, and human model lists is at\n  http://kanamori.cs.tsukuba.ac.jp/projects/relighting_human/", "journal-ref": null, "doi": "10.1145/3272127.3275104", "report-no": null, "categories": "cs.GR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Relighting of human images has various applications in image synthesis. For\nrelighting, we must infer albedo, shape, and illumination from a human\nportrait. Previous techniques rely on human faces for this inference, based on\nspherical harmonics (SH) lighting. However, because they often ignore light\nocclusion, inferred shapes are biased and relit images are unnaturally bright\nparticularly at hollowed regions such as armpits, crotches, or garment\nwrinkles. This paper introduces the first attempt to infer light occlusion in\nthe SH formulation directly. Based on supervised learning using convolutional\nneural networks (CNNs), we infer not only an albedo map, illumination but also\na light transport map that encodes occlusion as nine SH coefficients per pixel.\nThe main difficulty in this inference is the lack of training datasets compared\nto unlimited variations of human portraits. Surprisingly, geometric information\nincluding occlusion can be inferred plausibly even with a small dataset of\nsynthesized human figures, by carefully preparing the dataset so that the CNNs\ncan exploit the data coherency. Our method accomplishes more realistic\nrelighting than the occlusion-ignored formulation.\n", "versions": [{"version": "v1", "created": "Wed, 7 Aug 2019 16:35:27 GMT"}], "update_date": "2019-08-12", "authors_parsed": [["Kanamori", "Yoshihiro", ""], ["Endo", "Yuki", ""]]}, {"id": "1908.02723", "submitter": "Ian Fox", "authors": "Ian Fox and Jenna Wiens", "title": "Advocacy Learning: Learning through Competition and Class-Conditional\n  Representations", "comments": "Accepted IJCAI 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce advocacy learning, a novel supervised training scheme for\nattention-based classification problems. Advocacy learning relies on a\nframework consisting of two connected networks: 1) $N$ Advocates (one for each\nclass), each of which outputs an argument in the form of an attention map over\nthe input, and 2) a Judge, which predicts the class label based on these\narguments. Each Advocate produces a class-conditional representation with the\ngoal of convincing the Judge that the input example belongs to their class,\neven when the input belongs to a different class. Applied to several different\nclassification tasks, we show that advocacy learning can lead to small\nimprovements in classification accuracy over an identical supervised baseline.\nThough a series of follow-up experiments, we analyze when and how such\nclass-conditional representations improve discriminative performance. Though\nsomewhat counter-intuitive, a framework in which subnetworks are trained to\ncompetitively provide evidence in support of their class shows promise, in many\ncases performing on par with standard learning approaches. This provides a\nfoundation for further exploration into competition and class-conditional\nrepresentations in supervised learning.\n", "versions": [{"version": "v1", "created": "Wed, 7 Aug 2019 16:55:44 GMT"}], "update_date": "2019-08-08", "authors_parsed": [["Fox", "Ian", ""], ["Wiens", "Jenna", ""]]}, {"id": "1908.02726", "submitter": "Qianyu Feng", "authors": "Qianyu Feng, Yu Wu, Hehe Fan, Chenggang Yan, Yi Yang", "title": "Cascaded Revision Network for Novel Object Captioning", "comments": null, "journal-ref": null, "doi": "10.1109/TCSVT.2020.2965966", "report-no": null, "categories": "cs.MM cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image captioning, a challenging task where the machine automatically\ndescribes an image by sentences, has drawn significant attention in recent\nyears. Despite the remarkable improvements of recent approaches, however, these\nmethods are built upon a large set of training image-sentence pairs. The\nexpensive labor efforts hence limit the captioning model to describe the wider\nworld. In this paper, we present a novel network structure, Cascaded Revision\nNetwork, which aims at relieving the problem by equipping the model with\nout-of-domain knowledge. CRN first tries its best to describe an image using\nthe existing vocabulary from in-domain knowledge. Due to the lack of\nout-of-domain knowledge, the caption may be inaccurate or include ambiguous\nwords for the image with unknown (novel) objects. We propose to re-edit the\nprimary captioning sentence by a series of cascaded operations. We introduce a\nperplexity predictor to find out which words are most likely to be inaccurate\ngiven the input image. Thereafter, we utilize external knowledge from a\npre-trained object detection model and select more accurate words from\ndetection results by the visual matching module. In the last step, we design a\nsemantic matching module to ensure that the novel object is fit in the right\nposition. By this novel cascaded captioning-revising mechanism, CRN can\naccurately describe images with unseen objects. We validate the proposed method\nwith state-of-the-art performance on the held-out MSCOCO dataset as well as\nscale to ImageNet, demonstrating the effectiveness of this method.\n", "versions": [{"version": "v1", "created": "Tue, 6 Aug 2019 01:36:31 GMT"}], "update_date": "2020-01-14", "authors_parsed": [["Feng", "Qianyu", ""], ["Wu", "Yu", ""], ["Fan", "Hehe", ""], ["Yan", "Chenggang", ""], ["Yang", "Yi", ""]]}, {"id": "1908.02735", "submitter": "Pierre Jacob", "authors": "Pierre Jacob and David Picard and Aymeric Histace and Edouard Klein", "title": "Metric Learning With HORDE: High-Order Regularizer for Deep Embeddings", "comments": "Camera-ready for our ICCV 2019 paper (poster)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Learning an effective similarity measure between image representations is key\nto the success of recent advances in visual search tasks (e.g. verification or\nzero-shot learning). Although the metric learning part is well addressed, this\nmetric is usually computed over the average of the extracted deep features.\nThis representation is then trained to be discriminative. However, these deep\nfeatures tend to be scattered across the feature space. Consequently, the\nrepresentations are not robust to outliers, object occlusions, background\nvariations, etc. In this paper, we tackle this scattering problem with a\ndistribution-aware regularization named HORDE. This regularizer enforces\nvisually-close images to have deep features with the same distribution which\nare well localized in the feature space. We provide a theoretical analysis\nsupporting this regularization effect. We also show the effectiveness of our\napproach by obtaining state-of-the-art results on 4 well-known datasets\n(Cub-200-2011, Cars-196, Stanford Online Products and Inshop Clothes\nRetrieval).\n", "versions": [{"version": "v1", "created": "Wed, 7 Aug 2019 17:22:01 GMT"}], "update_date": "2019-08-08", "authors_parsed": [["Jacob", "Pierre", ""], ["Picard", "David", ""], ["Histace", "Aymeric", ""], ["Klein", "Edouard", ""]]}, {"id": "1908.02738", "submitter": "Adrian Dalca", "authors": "Adrian V. Dalca, Marianne Rakic, John Guttag, Mert R. Sabuncu", "title": "Learning Conditional Deformable Templates with Convolutional Networks", "comments": "NeurIPS 2019: Neural Information Processing Systems. Keywords:\n  deformable templates, conditional atlases, diffeomorphic image registration,\n  probabilistic models, neuroimaging", "journal-ref": "NeurIPS: Thirty-third Conference on Neural Information Processing\n  Systems, 2019", "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a learning framework for building deformable templates, which play\na fundamental role in many image analysis and computational anatomy tasks.\nConventional methods for template creation and image alignment to the template\nhave undergone decades of rich technical development. In these frameworks,\ntemplates are constructed using an iterative process of template estimation and\nalignment, which is often computationally very expensive. Due in part to this\nshortcoming, most methods compute a single template for the entire population\nof images, or a few templates for specific sub-groups of the data. In this\nwork, we present a probabilistic model and efficient learning strategy that\nyields either universal or conditional templates, jointly with a neural network\nthat provides efficient alignment of the images to these templates. We\ndemonstrate the usefulness of this method on a variety of domains, with a\nspecial focus on neuroimaging. This is particularly useful for clinical\napplications where a pre-existing template does not exist, or creating a new\none with traditional methods can be prohibitively expensive. Our code and\natlases are available online as part of the VoxelMorph library at\nhttp://voxelmorph.csail.mit.edu.\n", "versions": [{"version": "v1", "created": "Wed, 7 Aug 2019 17:29:36 GMT"}, {"version": "v2", "created": "Fri, 11 Oct 2019 16:04:22 GMT"}], "update_date": "2019-10-14", "authors_parsed": [["Dalca", "Adrian V.", ""], ["Rakic", "Marianne", ""], ["Guttag", "John", ""], ["Sabuncu", "Mert R.", ""]]}, {"id": "1908.02786", "submitter": "V\\'itor Louren\\c{c}o", "authors": "V\\'itor N. Louren\\c{c}o, Gabriela G. Silva, Leandro A. F. Fernandes", "title": "Hierarchy-of-Visual-Words: a Learning-based Approach for Trademark Image\n  Retrieval", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In this paper, we present the Hierarchy-of-Visual-Words (HoVW), a novel\ntrademark image retrieval (TIR) method that decomposes images into simpler\ngeometric shapes and defines a descriptor for binary trademark image\nrepresentation by encoding the hierarchical arrangement of component shapes.\nThe proposed hierarchical organization of visual data stores each component\nshape as a visual word. It is capable of representing the geometry of\nindividual elements and the topology of the trademark image, making the\ndescriptor robust against linear as well as to some level of nonlinear\ntransformation. Experiments show that HoVW outperforms previous TIR methods on\nthe MPEG-7 CE-1 and MPEG-7 CE-2 image databases.\n", "versions": [{"version": "v1", "created": "Wed, 7 Aug 2019 18:19:43 GMT"}], "update_date": "2019-08-09", "authors_parsed": [["Louren\u00e7o", "V\u00edtor N.", ""], ["Silva", "Gabriela G.", ""], ["Fernandes", "Leandro A. F.", ""]]}, {"id": "1908.02797", "submitter": "Yu Cheng", "authors": "Zhikang Zou, Yu Cheng, Xiaoye Qu, Shouling Ji, Xiaoxiao Guo, Pan Zhou", "title": "Attend To Count: Crowd Counting with Adaptive Capacity Multi-scale CNNs", "comments": "Accepted to Neurocomputing, code will be released soon", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Crowd counting is a challenging task due to the large variations in crowd\ndistributions. Previous methods tend to tackle the whole image with a single\nfixed structure, which is unable to handle diverse complicated scenes with\ndifferent crowd densities. Hence, we propose the Adaptive Capacity Multi-scale\nconvolutional neural networks (ACM-CNN), a novel crowd counting approach which\ncan assign different capacities to different portions of the input. The\nintuition is that the model should focus on important regions of the input\nimage and optimize its capacity allocation conditioning on the crowd intensive\ndegree. ACM-CNN consists of three types of modules: a coarse network, a fine\nnetwork, and a smooth network. The coarse network is used to explore the areas\nthat need to be focused via count attention mechanism, and generate a rough\nfeature map. Then the fine network processes the areas of interest into a fine\nfeature map. To alleviate the sense of division caused by fusion, the smooth\nnetwork is designed to combine two feature maps organically to produce\nhigh-quality density maps. Extensive experiments are conducted on five\nmainstream datasets. The results demonstrate the effectiveness of the proposed\nmodel for both density estimation and crowd counting tasks.\n", "versions": [{"version": "v1", "created": "Wed, 7 Aug 2019 18:57:35 GMT"}, {"version": "v2", "created": "Mon, 26 Aug 2019 05:37:24 GMT"}], "update_date": "2019-08-27", "authors_parsed": [["Zou", "Zhikang", ""], ["Cheng", "Yu", ""], ["Qu", "Xiaoye", ""], ["Ji", "Shouling", ""], ["Guo", "Xiaoxiao", ""], ["Zhou", "Pan", ""]]}, {"id": "1908.02809", "submitter": "Alexander Grabner", "authors": "Alexander Grabner, Peter M. Roth, Vincent Lepetit", "title": "GP2C: Geometric Projection Parameter Consensus for Joint 3D Pose and\n  Focal Length Estimation in the Wild", "comments": "Accepted to International Conference on Computer Vision (ICCV) 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a joint 3D pose and focal length estimation approach for object\ncategories in the wild. In contrast to previous methods that predict 3D poses\nindependently of the focal length or assume a constant focal length, we\nexplicitly estimate and integrate the focal length into the 3D pose estimation.\nFor this purpose, we combine deep learning techniques and geometric algorithms\nin a two-stage approach: First, we estimate an initial focal length and\nestablish 2D-3D correspondences from a single RGB image using a deep network.\nSecond, we recover 3D poses and refine the focal length by minimizing the\nreprojection error of the predicted correspondences. In this way, we exploit\nthe geometric prior given by the focal length for 3D pose estimation. This\nresults in two advantages: First, we achieve significantly improved 3D\ntranslation and 3D pose accuracy compared to existing methods. Second, our\napproach finds a geometric consensus between the individual projection\nparameters, which is required for precise 2D-3D alignment. We evaluate our\nproposed approach on three challenging real-world datasets (Pix3D, Comp, and\nStanford) with different object categories and significantly outperform the\nstate-of-the-art by up to 20% absolute in multiple different metrics.\n", "versions": [{"version": "v1", "created": "Wed, 7 Aug 2019 19:44:09 GMT"}], "update_date": "2019-08-09", "authors_parsed": [["Grabner", "Alexander", ""], ["Roth", "Peter M.", ""], ["Lepetit", "Vincent", ""]]}, {"id": "1908.02853", "submitter": "Alexander Grabner", "authors": "Alexander Grabner, Peter M. Roth, Vincent Lepetit", "title": "Location Field Descriptors: Single Image 3D Model Retrieval in the Wild", "comments": "Accepted to International Conference on 3D Vision (3DV) 2019 (Oral)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present Location Field Descriptors, a novel approach for single image 3D\nmodel retrieval in the wild. In contrast to previous methods that directly map\n3D models and RGB images to an embedding space, we establish a common low-level\nrepresentation in the form of location fields from which we compute pose\ninvariant 3D shape descriptors. Location fields encode correspondences between\n2D pixels and 3D surface coordinates and, thus, explicitly capture 3D shape and\n3D pose information without appearance variations which are irrelevant for the\ntask. This early fusion of 3D models and RGB images results in three main\nadvantages: First, the bottleneck location field prediction acts as a\nregularizer during training. Second, major parts of the system benefit from\ntraining on a virtually infinite amount of synthetic data. Finally, the\npredicted location fields are visually interpretable and unblackbox the system.\nWe evaluate our proposed approach on three challenging real-world datasets\n(Pix3D, Comp, and Stanford) with different object categories and significantly\noutperform the state-of-the-art by up to 20% absolute in multiple 3D retrieval\nmetrics.\n", "versions": [{"version": "v1", "created": "Wed, 7 Aug 2019 21:51:19 GMT"}], "update_date": "2019-08-09", "authors_parsed": [["Grabner", "Alexander", ""], ["Roth", "Peter M.", ""], ["Lepetit", "Vincent", ""]]}, {"id": "1908.02877", "submitter": "Aaron Reite", "authors": "Aaron Reite, Scott Kangas, Zackery Steck, Steven Goley, Jonathan Von\n  Stroh, and Steven Forsyth", "title": "Unsupervised Feature Learning in Remote Sensing", "comments": null, "journal-ref": null, "doi": "10.1117/12.2529791", "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The need for labeled data is among the most common and well-known practical\nobstacles to deploying deep learning algorithms to solve real-world problems.\nThe current generation of learning algorithms requires a large volume of data\nlabeled according to a static and pre-defined schema. Conversely, humans can\nquickly learn generalizations based on large quantities of unlabeled data, and\nturn these generalizations into classifications using spontaneous labels, often\nincluding labels not seen before. We apply a state-of-the-art unsupervised\nlearning algorithm to the noisy and extremely imbalanced xView data set to\ntrain a feature extractor that adapts to several tasks: visual similarity\nsearch that performs well on both common and rare classes; identifying outliers\nwithin a labeled data set; and learning a natural class hierarchy\nautomatically.\n", "versions": [{"version": "v1", "created": "Wed, 7 Aug 2019 23:48:49 GMT"}], "update_date": "2019-09-24", "authors_parsed": [["Reite", "Aaron", ""], ["Kangas", "Scott", ""], ["Steck", "Zackery", ""], ["Goley", "Steven", ""], ["Von Stroh", "Jonathan", ""], ["Forsyth", "Steven", ""]]}, {"id": "1908.02893", "submitter": "Aloisio Dourado", "authors": "Aloisio Dourado, Teofilo Emidio de Campos, Hansung Kim, Adrian Hilton", "title": "EdgeNet: Semantic Scene Completion from a Single RGB-D Image", "comments": "10 pages, 5 figures Accepted at ICPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semantic scene completion is the task of predicting a complete 3D\nrepresentation of volumetric occupancy with corresponding semantic labels for a\nscene from a single point of view. Previous works on Semantic Scene Completion\nfrom RGB-D data used either only depth or depth with colour by projecting the\n2D image into the 3D volume resulting in a sparse data representation. In this\nwork, we present a new strategy to encode colour information in 3D space using\nedge detection and flipped truncated signed distance. We also present EdgeNet,\na new end-to-end neural network architecture capable of handling features\ngenerated from the fusion of depth and edge information. Experimental results\nshow improvement of 6.9% over the state-of-the-art result on real data, for\nend-to-end approaches.\n", "versions": [{"version": "v1", "created": "Thu, 8 Aug 2019 01:00:11 GMT"}, {"version": "v2", "created": "Sun, 6 Sep 2020 20:17:18 GMT"}], "update_date": "2020-09-08", "authors_parsed": [["Dourado", "Aloisio", ""], ["de Campos", "Teofilo Emidio", ""], ["Kim", "Hansung", ""], ["Hilton", "Adrian", ""]]}, {"id": "1908.02900", "submitter": "Timnit Gebru", "authors": "Ernest Mwebaze, Timnit Gebru, Andrea Frome, Solomon Nsumba, Jeremy\n  Tusubira", "title": "iCassava 2019 Fine-Grained Visual Categorization Challenge", "comments": "Kaggle competition website:\n  https://www.kaggle.com/c/cassava-disease/overview, CVPR fine-grained visual\n  categorization website: https://sites.google.com/view/fgvc6", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Viral diseases are major sources of poor yields for cassava, the 2nd largest\nprovider of carbohydrates in Africa.At least 80% of small-holder farmer\nhouseholds in Sub-Saharan Africa grow cassava. Since many of these farmers have\nsmart phones, they can easily obtain photos of dis-eased and healthy cassava\nleaves in their farms, allowing the opportunity to use computer vision\ntechniques to monitor the disease type and severity and increase yields.\nHow-ever, annotating these images is extremely difficult as ex-perts who are\nable to distinguish between highly similar dis-eases need to be employed. We\nprovide a dataset of labeled and unlabeled cassava leaves and formulate a\nKaggle challenge to encourage participants to improve the performance of their\nalgorithms using semi-supervised approaches. This paper describes our dataset\nand challenge which is part of the Fine-Grained Visual Categorization workshop\nat CVPR2019.\n", "versions": [{"version": "v1", "created": "Thu, 8 Aug 2019 01:43:24 GMT"}, {"version": "v2", "created": "Tue, 24 Dec 2019 00:21:51 GMT"}], "update_date": "2019-12-25", "authors_parsed": [["Mwebaze", "Ernest", ""], ["Gebru", "Timnit", ""], ["Frome", "Andrea", ""], ["Nsumba", "Solomon", ""], ["Tusubira", "Jeremy", ""]]}, {"id": "1908.02923", "submitter": "Omid Mohamad Nezami", "authors": "Omid Mohamad Nezami, Mark Dras, Stephen Wan, Cecile Paris", "title": "Image Captioning using Facial Expression and Attention", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Benefiting from advances in machine vision and natural language processing\ntechniques, current image captioning systems are able to generate detailed\nvisual descriptions. For the most part, these descriptions represent an\nobjective characterisation of the image, although some models do incorporate\nsubjective aspects related to the observer's view of the image, such as\nsentiment; current models, however, usually do not consider the emotional\ncontent of images during the caption generation process. This paper addresses\nthis issue by proposing novel image captioning models which use facial\nexpression features to generate image captions. The models generate image\ncaptions using long short-term memory networks applying facial features in\naddition to other visual features at different time steps. We compare a\ncomprehensive collection of image captioning models with and without facial\nfeatures using all standard evaluation metrics. The evaluation metrics indicate\nthat applying facial features with an attention mechanism achieves the best\nperformance, showing more expressive and more correlated image captions, on an\nimage caption dataset extracted from the standard Flickr 30K dataset,\nconsisting of around 11K images containing faces. An analysis of the generated\ncaptions finds that, perhaps unexpectedly, the improvement in caption quality\nappears to come not from the addition of adjectives linked to emotional aspects\nof the images, but from more variety in the actions described in the captions.\n", "versions": [{"version": "v1", "created": "Thu, 8 Aug 2019 04:07:39 GMT"}, {"version": "v2", "created": "Thu, 9 Jan 2020 02:39:46 GMT"}, {"version": "v3", "created": "Wed, 15 Apr 2020 02:01:07 GMT"}], "update_date": "2020-04-16", "authors_parsed": [["Nezami", "Omid Mohamad", ""], ["Dras", "Mark", ""], ["Wan", "Stephen", ""], ["Paris", "Cecile", ""]]}, {"id": "1908.02924", "submitter": "Aleksei Tiulpin", "authors": "Roman Solovyev and Iaroslav Melekhov and Timo Lesonen and Elias\n  Vaattovaara and Osmo Tervonen and Aleksei Tiulpin", "title": "Bayesian Feature Pyramid Networks for Automatic Multi-Label Segmentation\n  of Chest X-rays and Assessment of Cardio-Thoratic Ratio", "comments": "Roman Solovyev and Iaroslav Melekhov contributed equally. Timo\n  Lesonen and Elias Vaattovaara contributed equally", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cardiothoratic ratio (CTR) estimated from chest radiographs is a marker\nindicative of cardiomegaly, the presence of which is in the criteria for heart\nfailure diagnosis. Existing methods for automatic assessment of CTR are driven\nby Deep Learning-based segmentation. However, these techniques produce only\npoint estimates of CTR but clinical decision making typically assumes the\nuncertainty. In this paper, we propose a novel method for chest X-ray\nsegmentation and CTR assessment in an automatic manner. In contrast to the\nprevious art, we, for the first time, propose to estimate CTR with uncertainty\nbounds. Our method is based on Deep Convolutional Neural Network with Feature\nPyramid Network (FPN) decoder. We propose two modifications of FPN: replace the\nbatch normalization with instance normalization and inject the dropout which\nallows to obtain the Monte-Carlo estimates of the segmentation maps at test\ntime. Finally, using the predicted segmentation mask samples, we estimate CTR\nwith uncertainty. In our experiments we demonstrate that the proposed method\ngeneralizes well to three different test sets. Finally, we make the annotations\nproduced by two radiologists for all our datasets publicly available.\n", "versions": [{"version": "v1", "created": "Thu, 8 Aug 2019 04:16:25 GMT"}], "update_date": "2019-08-09", "authors_parsed": [["Solovyev", "Roman", ""], ["Melekhov", "Iaroslav", ""], ["Lesonen", "Timo", ""], ["Vaattovaara", "Elias", ""], ["Tervonen", "Osmo", ""], ["Tiulpin", "Aleksei", ""]]}, {"id": "1908.02943", "submitter": "Omid Mohamad Nezami", "authors": "Omid Mohamad Nezami, Mark Dras, Stephen Wan, Cecile Paris, Len Hamey", "title": "Towards Generating Stylized Image Captions via Adversarial Training", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While most image captioning aims to generate objective descriptions of\nimages, the last few years have seen work on generating visually grounded image\ncaptions which have a specific style (e.g., incorporating positive or negative\nsentiment). However, because the stylistic component is typically the last part\nof training, current models usually pay more attention to the style at the\nexpense of accurate content description. In addition, there is a lack of\nvariability in terms of the stylistic aspects. To address these issues, we\npropose an image captioning model called ATTEND-GAN which has two core\ncomponents: first, an attention-based caption generator to strongly correlate\ndifferent parts of an image with different parts of a caption; and second, an\nadversarial training mechanism to assist the caption generator to add diverse\nstylistic components to the generated captions. Because of these components,\nATTEND-GAN can generate correlated captions as well as more human-like\nvariability of stylistic patterns. Our system outperforms the state-of-the-art\nas well as a collection of our baseline models. A linguistic analysis of the\ngenerated captions demonstrates that captions generated using ATTEND-GAN have a\nwider range of stylistic adjectives and adjective-noun pairs.\n", "versions": [{"version": "v1", "created": "Thu, 8 Aug 2019 06:25:38 GMT"}], "update_date": "2019-08-09", "authors_parsed": [["Nezami", "Omid Mohamad", ""], ["Dras", "Mark", ""], ["Wan", "Stephen", ""], ["Paris", "Cecile", ""], ["Hamey", "Len", ""]]}, {"id": "1908.02948", "submitter": "Guyue Hu", "authors": "Guyue Hu, Bo Cui, Yuan He, Shan Yu", "title": "Progressive Relation Learning for Group Activity Recognition", "comments": "8 pages; Accepted to CVPR2020; Supplementary Materials will appear on\n  site <CVF Open Access>;", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Group activities usually involve spatiotemporal dynamics among many\ninteractive individuals, while only a few participants at several key frames\nessentially define the activity. Therefore, effectively modeling the\ngroup-relevant and suppressing the irrelevant actions (and interactions) are\nvital for group activity recognition. In this paper, we propose a novel method\nbased on deep reinforcement learning to progressively refine the low-level\nfeatures and high-level relations of group activities. Firstly, we construct a\nsemantic relation graph (SRG) to explicitly model the relations among persons.\nThen, two agents adopting policy according to two Markov decision processes are\napplied to progressively refine the SRG. Specifically, one feature-distilling\n(FD) agent in the discrete action space refines the low-level spatio-temporal\nfeatures by distilling the most informative frames. Another relation-gating\n(RG) agent in continuous action space adjusts the high-level semantic graph to\npay more attention to group-relevant relations. The SRG, FD agent, and RG agent\nare optimized alternately to mutually boost the performance of each other.\nExtensive experiments on two widely used benchmarks demonstrate the\neffectiveness and superiority of the proposed approach.\n", "versions": [{"version": "v1", "created": "Thu, 8 Aug 2019 06:50:42 GMT"}, {"version": "v2", "created": "Tue, 3 Mar 2020 09:05:03 GMT"}], "update_date": "2020-03-04", "authors_parsed": [["Hu", "Guyue", ""], ["Cui", "Bo", ""], ["He", "Yuan", ""], ["Yu", "Shan", ""]]}, {"id": "1908.02950", "submitter": "Deepan Das", "authors": "Deepan Das, Noor Mohammed Ghouse, Shashank Verma, Yin Li", "title": "Semi Supervised Phrase Localization in a Bidirectional Caption-Image\n  Retrieval Framework", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a novel deep neural network architecture that links visual\nregions to corresponding textual segments including phrases and words. To\naccomplish this task, our architecture makes use of the rich semantic\ninformation available in a joint embedding space of multi-modal data. From this\njoint embedding space, we extract the associative localization maps that\ndevelop naturally, without explicitly providing supervision during training for\nthe localization task. The joint space is learned using a bidirectional ranking\nobjective that is optimized using a $N$-Pair loss formulation. This training\nmechanism demonstrates the idea that localization information is learned\ninherently while optimizing a Bidirectional Retrieval objective. The model's\nretrieval and localization performance is evaluated on MSCOCO and Flickr30K\nEntities datasets. This architecture outperforms the state of the art results\nin the semi-supervised phrase localization setting.\n", "versions": [{"version": "v1", "created": "Thu, 8 Aug 2019 07:01:26 GMT"}], "update_date": "2019-08-09", "authors_parsed": [["Das", "Deepan", ""], ["Ghouse", "Noor Mohammed", ""], ["Verma", "Shashank", ""], ["Li", "Yin", ""]]}, {"id": "1908.02962", "submitter": "Difei Gao", "authors": "Difei Gao, Ruiping Wang, Shiguang Shan, Xilin Chen", "title": "From Two Graphs to N Questions: A VQA Dataset for Compositional\n  Reasoning on Vision and Commonsense", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual Question Answering (VQA) is a challenging task for evaluating the\nability of comprehensive understanding of the world. Existing benchmarks\nusually focus on the reasoning abilities either only on the vision or mainly on\nthe knowledge with relatively simple abilities on vision. However, the ability\nof answering a question that requires alternatively inferring on the image\ncontent and the commonsense knowledge is crucial for an advanced VQA system. In\nthis paper, we introduce a VQA dataset that provides more challenging and\ngeneral questions about Compositional Reasoning on vIsion and Commonsense,\nwhich is named as CRIC. To create this dataset, we develop a powerful method to\nautomatically generate compositional questions and rich annotations from both\nthe scene graph of a given image and some external knowledge graph. Moreover,\nthis paper presents a new compositional model that is capable of implementing\nvarious types of reasoning functions on the image content and the knowledge\ngraph. Further, we analyze several baselines, state-of-the-art and our model on\nCRIC dataset. The experimental results show that the proposed task is\nchallenging, where state-of-the-art obtains 52.26% accuracy and our model\nobtains 58.38%.\n", "versions": [{"version": "v1", "created": "Thu, 8 Aug 2019 08:07:35 GMT"}, {"version": "v2", "created": "Fri, 23 Aug 2019 11:43:42 GMT"}], "update_date": "2019-08-26", "authors_parsed": [["Gao", "Difei", ""], ["Wang", "Ruiping", ""], ["Shan", "Shiguang", ""], ["Chen", "Xilin", ""]]}, {"id": "1908.02983", "submitter": "Eric Arazo", "authors": "Eric Arazo, Diego Ortego, Paul Albert, Noel E. O'Connor and Kevin\n  McGuinness", "title": "Pseudo-Labeling and Confirmation Bias in Deep Semi-Supervised Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semi-supervised learning, i.e. jointly learning from labeled and unlabeled\nsamples, is an active research topic due to its key role on relaxing human\nsupervision. In the context of image classification, recent advances to learn\nfrom unlabeled samples are mainly focused on consistency regularization methods\nthat encourage invariant predictions for different perturbations of unlabeled\nsamples. We, conversely, propose to learn from unlabeled data by generating\nsoft pseudo-labels using the network predictions. We show that a naive\npseudo-labeling overfits to incorrect pseudo-labels due to the so-called\nconfirmation bias and demonstrate that mixup augmentation and setting a minimum\nnumber of labeled samples per mini-batch are effective regularization\ntechniques for reducing it. The proposed approach achieves state-of-the-art\nresults in CIFAR-10/100, SVHN, and Mini-ImageNet despite being much simpler\nthan other methods. These results demonstrate that pseudo-labeling alone can\noutperform consistency regularization methods, while the opposite was supposed\nin previous work. Source code is available at https://git.io/fjQsC.\n", "versions": [{"version": "v1", "created": "Thu, 8 Aug 2019 09:17:54 GMT"}, {"version": "v2", "created": "Fri, 9 Aug 2019 12:35:42 GMT"}, {"version": "v3", "created": "Wed, 25 Sep 2019 14:06:11 GMT"}, {"version": "v4", "created": "Mon, 20 Jan 2020 14:54:03 GMT"}, {"version": "v5", "created": "Mon, 29 Jun 2020 08:18:31 GMT"}], "update_date": "2020-06-30", "authors_parsed": [["Arazo", "Eric", ""], ["Ortego", "Diego", ""], ["Albert", "Paul", ""], ["O'Connor", "Noel E.", ""], ["McGuinness", "Kevin", ""]]}, {"id": "1908.02990", "submitter": "Yilun Chen", "authors": "Yilun Chen, Shu Liu, Xiaoyong Shen, Jiaya Jia", "title": "Fast Point R-CNN", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a unified, efficient and effective framework for point-cloud based\n3D object detection. Our two-stage approach utilizes both voxel representation\nand raw point cloud data to exploit respective advantages. The first stage\nnetwork, with voxel representation as input, only consists of light\nconvolutional operations, producing a small number of high-quality initial\npredictions. Coordinate and indexed convolutional feature of each point in\ninitial prediction are effectively fused with the attention mechanism,\npreserving both accurate localization and context information. The second stage\nworks on interior points with their fused feature for further refining the\nprediction. Our method is evaluated on KITTI dataset, in terms of both 3D and\nBird's Eye View (BEV) detection, and achieves state-of-the-arts with a 15FPS\ndetection rate.\n", "versions": [{"version": "v1", "created": "Thu, 8 Aug 2019 09:52:09 GMT"}, {"version": "v2", "created": "Fri, 16 Aug 2019 03:05:50 GMT"}], "update_date": "2019-08-19", "authors_parsed": [["Chen", "Yilun", ""], ["Liu", "Shu", ""], ["Shen", "Xiaoyong", ""], ["Jia", "Jiaya", ""]]}, {"id": "1908.02995", "submitter": "Tatsuya Yokota", "authors": "Tatsuya Yokota, Hidekata Hontani, Qibin Zhao, Andrzej Cichocki", "title": "Manifold Modeling in Embedded Space: A Perspective for Interpreting Deep\n  Image Prior", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep image prior (DIP), which utilizes a deep convolutional network (ConvNet)\nstructure itself as an image prior, has attracted attentions in computer vision\nand machine learning communities. It empirically shows the effectiveness of\nConvNet structure for various image restoration applications. However, why the\nDIP works so well is still unknown, and why convolution operation is useful for\nimage reconstruction or enhancement is not very clear. In this study, we tackle\nthese questions. The proposed approach is dividing the convolution into\n``delay-embedding'' and ``transformation (\\ie encoder-decoder)'', and proposing\na simple, but essential, image/tensor modeling method which is closely related\nto dynamical systems and self-similarity. The proposed method named as manifold\nmodeling in embedded space (MMES) is implemented by using a novel\ndenoising-auto-encoder in combination with multi-way delay-embedding transform.\nIn spite of its simplicity, the image/tensor completion, super-resolution,\ndeconvolution, and denoising results of MMES are quite similar even competitive\nto DIP in our extensive experiments, and these results would help us for\nreinterpreting/characterizing the DIP from a perspective of ``low-dimensional\npatch-manifold prior''.\n", "versions": [{"version": "v1", "created": "Thu, 8 Aug 2019 10:05:09 GMT"}, {"version": "v2", "created": "Tue, 21 Jan 2020 08:14:35 GMT"}], "update_date": "2020-01-22", "authors_parsed": [["Yokota", "Tatsuya", ""], ["Hontani", "Hidekata", ""], ["Zhao", "Qibin", ""], ["Cichocki", "Andrzej", ""]]}, {"id": "1908.02996", "submitter": "Mathilde Bateson", "authors": "Mathilde Bateson, Jose Dolz, Hoel Kervadec, Herv\\'e Lombaert, Ismail\n  Ben Ayed", "title": "Constrained domain adaptation for Image segmentation", "comments": "Published in IEEE Transactions on Medical Imaging. First version in\n  MICCAI 2019", "journal-ref": "IEEE IEEE Transactions on Medical Imaging 2021", "doi": "10.1109/TMI.2021.3067688", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose to adapt segmentation networks with a constrained formulation,\nwhich embeds domain-invariant prior knowledge about the segmentation regions.\nSuch knowledge may take the form of simple anatomical information, e.g.,\nstructure size or shape, estimated from source samples or known a priori. Our\nmethod imposes domain-invariant inequality constraints on the network outputs\nof unlabeled target samples. It implicitly matches prediction statistics\nbetween target and source domains with permitted uncertainty of prior\nknowledge. We address our constrained problem with a differentiable penalty,\nfully suited for standard stochastic gradient descent approaches, removing the\nneed for computationally expensive Lagrangian optimization with dual\nprojections. Unlike current two-step adversarial training, our formulation is\nbased on a single loss in a single network, which simplifies adaptation by\navoiding extra adversarial steps, while improving convergence and quality of\ntraining.\n  The comparison of our approach with state-of-the-art adversarial methods\nreveals substantially better performance on the challenging task of adapting\nspine segmentation across different MRI modalities. Our results also show a\nrobustness to imprecision of size priors, approaching the accuracy of a fully\nsupervised model trained directly in a target domain.Our method can be readily\nused for various constraints and segmentation problems.\n", "versions": [{"version": "v1", "created": "Thu, 8 Aug 2019 10:07:39 GMT"}, {"version": "v2", "created": "Sun, 11 Apr 2021 20:37:07 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Bateson", "Mathilde", ""], ["Dolz", "Jose", ""], ["Kervadec", "Hoel", ""], ["Lombaert", "Herv\u00e9", ""], ["Ayed", "Ismail Ben", ""]]}, {"id": "1908.02999", "submitter": "Fabian Schilling", "authors": "Fabian Schilling and Julien Lecoeur and Fabrizio Schiano and Dario\n  Floreano", "title": "Learning Vision-based Flight in Drone Swarms by Imitation", "comments": "8 pages, 8 figures, accepted for publication in the IEEE Robotics and\n  Automation Letters (RA-L) on July 28, 2019. arXiv admin note: substantial\n  text overlap with arXiv:1809.00543", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV cs.LG cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Decentralized drone swarms deployed today either rely on sharing of positions\namong agents or detecting swarm members with the help of visual markers. This\nwork proposes an entirely visual approach to coordinate markerless drone swarms\nbased on imitation learning. Each agent is controlled by a small and efficient\nconvolutional neural network that takes raw omnidirectional images as inputs\nand predicts 3D velocity commands that match those computed by a flocking\nalgorithm. We start training in simulation and propose a simple yet effective\nunsupervised domain adaptation approach to transfer the learned controller to\nthe real world. We further train the controller with data collected in our\nmotion capture hall. We show that the convolutional neural network trained on\nthe visual inputs of the drone can learn not only robust inter-agent collision\navoidance but also cohesion of the swarm in a sample-efficient manner. The\nneural controller effectively learns to localize other agents in the visual\ninput, which we show by visualizing the regions with the most influence on the\nmotion of an agent. We remove the dependence on sharing positions among swarm\nmembers by taking only local visual information into account for control. Our\nwork can therefore be seen as the first step towards a fully decentralized,\nvision-based swarm without the need for communication or visual markers.\n", "versions": [{"version": "v1", "created": "Thu, 8 Aug 2019 10:19:48 GMT"}], "update_date": "2019-08-09", "authors_parsed": [["Schilling", "Fabian", ""], ["Lecoeur", "Julien", ""], ["Schiano", "Fabrizio", ""], ["Floreano", "Dario", ""]]}, {"id": "1908.03000", "submitter": "Marcell Wolnitza", "authors": "Marcell Wolnitza and Babette Dellen", "title": "Feature selection of neural networks is skewed towards the less abstract\n  cue", "comments": "8 pages, 5 figures, 8 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Artificial neural networks (ANNs) have become an important tool for image\nclassification with many applications in research and industry. However, it\nremains largely unknown how relevant image features are selected and how data\nproperties affect this process. In particular, we are interested whether the\nabstraction level of image cues correlating with class membership influences\nfeature selection. We perform experiments with binary images that contain a\ncombination of cues, representing two different levels of abstractions: one is\na pattern drawn from a random distribution where class membership correlates\nwith the statistics of the pattern, the other a combination of symbol-like\nentities, where the symbolic code correlates with class membership. When the\nnetwork is trained with data in which both cues are equally significant, we\nobserve that the cues at the lower abstraction level, i.e., the pattern, is\nlearned, while the symbolic information is largely ignored, even in networks\nwith many layers. Symbol-like entities are only learned if the importance of\nlow-level cues is reduced compared to the high-level ones. These findings raise\nimportant questions about the relevance of features that are learned by deep\nANNs and how learning could be shifted towards symbolic features.\n", "versions": [{"version": "v1", "created": "Thu, 8 Aug 2019 10:24:37 GMT"}], "update_date": "2019-08-09", "authors_parsed": [["Wolnitza", "Marcell", ""], ["Dellen", "Babette", ""]]}, {"id": "1908.03030", "submitter": "Andrew Gilbert", "authors": "Andrew Gilbert, Matthew Trumble, Adrian Hilton, John Collomosse", "title": "Semantic Estimation of 3D Body Shape and Pose using Minimal Cameras", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We aim to simultaneously estimate the 3D articulated pose and high fidelity\nvolumetric occupancy of human performance, from multiple viewpoint video (MVV)\nwith as few as two views. We use a multi-channel symmetric 3D convolutional\nencoder-decoder with a dual loss to enforce the learning of a latent embedding\nthat enables inference of skeletal joint positions and a volumetric\nreconstruction of the performance. The inference is regularised via a prior\nlearned over a dataset of view-ablated multi-view video footage of a wide range\nof subjects and actions, and show this to generalise well across unseen\nsubjects and actions. We demonstrate improved reconstruction accuracy and lower\npose estimation error relative to prior work on two MVV performance capture\ndatasets: Human 3.6M and TotalCapture.\n", "versions": [{"version": "v1", "created": "Thu, 8 Aug 2019 12:11:24 GMT"}, {"version": "v2", "created": "Mon, 7 Sep 2020 13:07:28 GMT"}], "update_date": "2020-09-08", "authors_parsed": [["Gilbert", "Andrew", ""], ["Trumble", "Matthew", ""], ["Hilton", "Adrian", ""], ["Collomosse", "John", ""]]}, {"id": "1908.03047", "submitter": "Bi Li", "authors": "Liang Wu, Chengquan Zhang, Jiaming Liu, Junyu Han, Jingtuo Liu, Errui\n  Ding, Xiang Bai", "title": "Editing Text in the Wild", "comments": "accepted by ACM MM 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we are interested in editing text in natural images, which\naims to replace or modify a word in the source image with another one while\nmaintaining its realistic look. This task is challenging, as the styles of both\nbackground and text need to be preserved so that the edited image is visually\nindistinguishable from the source image. Specifically, we propose an end-to-end\ntrainable style retention network (SRNet) that consists of three modules: text\nconversion module, background inpainting module and fusion module. The text\nconversion module changes the text content of the source image into the target\ntext while keeping the original text style. The background inpainting module\nerases the original text, and fills the text region with appropriate texture.\nThe fusion module combines the information from the two former modules, and\ngenerates the edited text images. To our knowledge, this work is the first\nattempt to edit text in natural images at the word level. Both visual effects\nand quantitative results on synthetic and real-world dataset (ICDAR 2013) fully\nconfirm the importance and necessity of modular decomposition. We also conduct\nextensive experiments to validate the usefulness of our method in various\nreal-world applications such as text image synthesis, augmented reality (AR)\ntranslation, information hiding, etc.\n", "versions": [{"version": "v1", "created": "Thu, 8 Aug 2019 12:59:18 GMT"}], "update_date": "2019-08-09", "authors_parsed": [["Wu", "Liang", ""], ["Zhang", "Chengquan", ""], ["Liu", "Jiaming", ""], ["Han", "Junyu", ""], ["Liu", "Jingtuo", ""], ["Ding", "Errui", ""], ["Bai", "Xiang", ""]]}, {"id": "1908.03055", "submitter": "Thomas Golda", "authors": "Thomas Golda, Nils Murzyn, Chengchao Qu, Kristian Kroschel", "title": "What goes around comes around: Cycle-Consistency-based Short-Term Motion\n  Prediction for Anomaly Detection using Generative Adversarial Networks", "comments": "Accepted for DLAM workshop at AVSS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Anomaly detection plays in many fields of research, along with the strongly\nrelated task of outlier detection, a very important role. Especially within the\ncontext of the automated analysis of video material recorded by surveillance\ncameras, abnormal situations can be of very different nature. For this purpose\nthis work investigates Generative-Adversarial-Network-based methods (GAN) for\nanomaly detection related to surveillance applications. The focus is on the\nusage of static camera setups, since this kind of camera is one of the most\noften used and belongs to the lower price segment. In order to address this\ntask, multiple subtasks are evaluated, including the influence of existing\noptical flow methods for the incorporation of short-term temporal information,\ndifferent forms of network setups and losses for GANs, and the use of\nmorphological operations for further performance improvement. With these\nextension we achieved up to 2.4% better results. Furthermore, the final method\nreduced the anomaly detection error for GAN-based methods by about 42.8%.\n", "versions": [{"version": "v1", "created": "Thu, 8 Aug 2019 13:20:31 GMT"}], "update_date": "2019-08-09", "authors_parsed": [["Golda", "Thomas", ""], ["Murzyn", "Nils", ""], ["Qu", "Chengchao", ""], ["Kroschel", "Kristian", ""]]}, {"id": "1908.03057", "submitter": "Roni Saputra Permana", "authors": "Roni Permana Saputra, Nemanja Rakicevic, Petar Kormushev", "title": "Sim-to-Real Learning for Casualty Detection from Ground Projected Point\n  Cloud Data", "comments": "10 pages, 10 figures, accepted to the IEEE/RSJ International\n  Conference on Intelligent Robots and Systems (IROS), 2019", "journal-ref": "2019 IEEE/RSJ International Conference on Intelligent Robots and\n  Systems (IROS), Macau, China, 2019, pp. 3918-3925", "doi": "10.1109/IROS40897.2019.8967642", "report-no": null, "categories": "cs.CV cs.RO", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  This paper addresses the problem of human body detection---particularly a\nhuman body lying on the ground (a.k.a. casualty)---using point cloud data. This\nability to detect a casualty is one of the most important features of mobile\nrescue robots, in order for them to be able to operate autonomously. We propose\na deep-learning-based casualty detection method using a deep convolutional\nneural network (CNN). This network is trained to be able to detect a casualty\nusing a point-cloud data input. In the method we propose, the point cloud input\nis pre-processed to generate a depth image-like ground-projected heightmap.\nThis heightmap is generated based on the projected distance of each point onto\nthe detected ground plane within the point cloud data. The generated heightmap\n-- in image form -- is then used as an input for the CNN to detect a human body\nlying on the ground. To train the neural network, we propose a novel\nsim-to-real approach, in which the network model is trained using synthetic\ndata obtained in simulation and then tested on real sensor data. To make the\nmodel transferable to real data implementations, during the training we adopt\nspecific data augmentation strategies with the synthetic training data. The\nexperimental results show that data augmentation introduced during the training\nprocess is essential for improving the performance of the trained model on real\ndata. More specifically, the results demonstrate that the data augmentations on\nraw point-cloud data have contributed to a considerable improvement of the\ntrained model performance.\n", "versions": [{"version": "v1", "created": "Thu, 8 Aug 2019 13:25:00 GMT"}, {"version": "v2", "created": "Fri, 9 Aug 2019 15:37:31 GMT"}], "update_date": "2020-02-19", "authors_parsed": [["Saputra", "Roni Permana", ""], ["Rakicevic", "Nemanja", ""], ["Kormushev", "Petar", ""]]}, {"id": "1908.03093", "submitter": "Hyojin Park", "authors": "Hyojin Park, Lars Lowe Sj\\\"osund, YoungJoon Yoo, Jihwan Bang and Nojun\n  Kwak", "title": "ExtremeC3Net: Extreme Lightweight Portrait Segmentation Networks using\n  Advanced C3-modules", "comments": "https://github.com/HYOJINPARK/ExtPortraitSeg", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Designing a lightweight and robust portrait segmentation algorithm is an\nimportant task for a wide range of face applications. However, the problem has\nbeen considered as a subset of the object segmentation problem. bviously,\nportrait segmentation has its unique requirements. First, because the portrait\nsegmentation is performed in the middle of a whole process of many realworld\napplications, it requires extremely lightweight models. Second, there has not\nbeen any public datasets in this domain that contain a sufficient number of\nimages with unbiased statistics. To solve the problems, we introduce a new\nextremely lightweight portrait segmentation model consisting of a two-branched\narchitecture based on the concentrated-comprehensive convolutions block. Our\nmethod reduces the number of parameters from 2.1M to 37.7K (around 98.2%\nreduction), while maintaining the accuracy within a 1% margin from the\nstate-of-the-art portrait segmentation method. In our qualitative and\nquantitative analysis on the EG1800 dataset, we show that our method\noutperforms various existing lightweight segmentation models. Second, we\npropose a simple method to create additional portrait segmentation data which\ncan improve accuracy on the EG1800 dataset. Also, we analyze the bias in public\ndatasets by additionally annotating race, gender, and age on our own. The\naugmented dataset, the additional annotations and code are available in\nhttps://github.com/HYOJINPARK/ExtPortraitSeg .\n", "versions": [{"version": "v1", "created": "Thu, 8 Aug 2019 14:31:18 GMT"}, {"version": "v2", "created": "Sun, 18 Aug 2019 07:18:48 GMT"}, {"version": "v3", "created": "Mon, 9 Dec 2019 16:26:26 GMT"}], "update_date": "2019-12-10", "authors_parsed": [["Park", "Hyojin", ""], ["Sj\u00f6sund", "Lars Lowe", ""], ["Yoo", "YoungJoon", ""], ["Bang", "Jihwan", ""], ["Kwak", "Nojun", ""]]}, {"id": "1908.03127", "submitter": "Matteo Poggi", "authors": "Lorenzo Andraghetti, Panteleimon Myriokefalitakis, Pier Luigi Dovesi,\n  Belen Luque, Matteo Poggi, Alessandro Pieropan, Stefano Mattoccia", "title": "Enhancing self-supervised monocular depth estimation with traditional\n  visual odometry", "comments": "Accepted to 3DV 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimating depth from a single image represents an attractive alternative to\nmore traditional approaches leveraging multiple cameras. In this field, deep\nlearning yielded outstanding results at the cost of needing large amounts of\ndata labeled with precise depth measurements for training. An issue softened by\nself-supervised approaches leveraging monocular sequences or stereo pairs in\nplace of expensive ground truth depth annotations. This paper enables to\nfurther improve monocular depth estimation by integrating into existing\nself-supervised networks a geometrical prior. Specifically, we propose a\nsparsity-invariant autoencoder able to process the output of conventional\nvisual odometry algorithms working in synergy with depth-from-mono networks.\nExperimental results on the KITTI dataset show that by exploiting the\ngeometrical prior, our proposal: i) outperforms existing approaches in the\nliterature and ii) couples well with both compact and complex depth-from-mono\narchitectures, allowing for its deployment on high-end GPUs as well as on\nembedded devices (e.g., NVIDIA Jetson TX2).\n", "versions": [{"version": "v1", "created": "Thu, 8 Aug 2019 15:39:34 GMT"}, {"version": "v2", "created": "Mon, 12 Aug 2019 19:28:18 GMT"}], "update_date": "2019-08-14", "authors_parsed": [["Andraghetti", "Lorenzo", ""], ["Myriokefalitakis", "Panteleimon", ""], ["Dovesi", "Pier Luigi", ""], ["Luque", "Belen", ""], ["Poggi", "Matteo", ""], ["Pieropan", "Alessandro", ""], ["Mattoccia", "Stefano", ""]]}, {"id": "1908.03176", "submitter": "Sobhan Soleymani", "authors": "Sobhan Soleymani, Ali Dabouei, Jeremy Dawson, Nasser M. Nasrabadi", "title": "Defending Against Adversarial Iris Examples Using Wavelet Decomposition", "comments": "The Tenth IEEE International Conference on Biometrics: Theory,\n  Applications, and Systems (BTAS 2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks have presented impressive performance in biometric\napplications. However, their performance is highly at risk when facing\ncarefully crafted input samples known as adversarial examples. In this paper,\nwe present three defense strategies to detect adversarial iris examples. These\ndefense strategies are based on wavelet domain denoising of the input examples\nby investigating each wavelet sub-band and removing the sub-bands that are most\naffected by the adversary. The first proposed defense strategy reconstructs\nmultiple denoised versions of the input example through manipulating the mid-\nand high-frequency components of the wavelet domain representation of the input\nexample and makes a decision upon the classification result of the majority of\nthe denoised examples. The second and third proposed defense strategies aim to\ndenoise each wavelet domain sub-band and determine the sub-bands that are most\nlikely affected by the adversary using the reconstruction error computed for\neach sub-band. We test the performance of the proposed defense strategies\nagainst several attack scenarios and compare the results with five state of the\nart defense strategies.\n", "versions": [{"version": "v1", "created": "Thu, 8 Aug 2019 17:08:25 GMT"}], "update_date": "2019-08-09", "authors_parsed": [["Soleymani", "Sobhan", ""], ["Dabouei", "Ali", ""], ["Dawson", "Jeremy", ""], ["Nasrabadi", "Nasser M.", ""]]}, {"id": "1908.03180", "submitter": "Paola Cascante-Bonilla", "authors": "Paola Cascante-Bonilla, Kalpathy Sitaraman, Mengjia Luo, Vicente\n  Ordonez", "title": "Moviescope: Large-scale Analysis of Movies using Multiple Modalities", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Film media is a rich form of artistic expression. Unlike photography, and\nshort videos, movies contain a storyline that is deliberately complex and\nintricate in order to engage its audience. In this paper we present a large\nscale study comparing the effectiveness of visual, audio, text, and\nmetadata-based features for predicting high-level information about movies such\nas their genre or estimated budget. We demonstrate the usefulness of\ncontent-based methods in this domain in contrast to human-based and\nmetadata-based predictions in the era of deep learning. Additionally, we\nprovide a comprehensive study of temporal feature aggregation methods for\nrepresenting video and text and find that simple pooling operations are\neffective in this domain. We also show to what extent different modalities are\ncomplementary to each other. To this end, we also introduce Moviescope, a new\nlarge-scale dataset of 5,000 movies with corresponding movie trailers (video +\naudio), movie posters (images), movie plots (text), and metadata.\n", "versions": [{"version": "v1", "created": "Thu, 8 Aug 2019 17:20:24 GMT"}], "update_date": "2019-08-09", "authors_parsed": [["Cascante-Bonilla", "Paola", ""], ["Sitaraman", "Kalpathy", ""], ["Luo", "Mengjia", ""], ["Ordonez", "Vicente", ""]]}, {"id": "1908.03182", "submitter": "Evan Shelhamer", "authors": "Dequan Wang, Evan Shelhamer, Bruno Olshausen, Trevor Darrell", "title": "Dynamic Scale Inference by Entropy Minimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given the variety of the visual world there is not one true scale for\nrecognition: objects may appear at drastically different sizes across the\nvisual field. Rather than enumerate variations across filter channels or\npyramid levels, dynamic models locally predict scale and adapt receptive fields\naccordingly. The degree of variation and diversity of inputs makes this a\ndifficult task. Existing methods either learn a feedforward predictor, which is\nnot itself totally immune to the scale variation it is meant to counter, or\nselect scales by a fixed algorithm, which cannot learn from the given task and\ndata. We extend dynamic scale inference from feedforward prediction to\niterative optimization for further adaptivity. We propose a novel entropy\nminimization objective for inference and optimize over task and structure\nparameters to tune the model to each input. Optimization during inference\nimproves semantic segmentation accuracy and generalizes better to extreme scale\nvariations that cause feedforward dynamic inference to falter.\n", "versions": [{"version": "v1", "created": "Thu, 8 Aug 2019 17:21:20 GMT"}], "update_date": "2019-08-09", "authors_parsed": [["Wang", "Dequan", ""], ["Shelhamer", "Evan", ""], ["Olshausen", "Bruno", ""], ["Darrell", "Trevor", ""]]}, {"id": "1908.03195", "submitter": "Ross Girshick", "authors": "Agrim Gupta, Piotr Doll\\'ar, Ross Girshick", "title": "LVIS: A Dataset for Large Vocabulary Instance Segmentation", "comments": "Extension of the CVPR'19 paper describing release v0.5, the LVIS\n  Challenge, and baseline results", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Progress on object detection is enabled by datasets that focus the research\ncommunity's attention on open challenges. This process led us from simple\nimages to complex scenes and from bounding boxes to segmentation masks. In this\nwork, we introduce LVIS (pronounced `el-vis'): a new dataset for Large\nVocabulary Instance Segmentation. We plan to collect ~2 million high-quality\ninstance segmentation masks for over 1000 entry-level object categories in 164k\nimages. Due to the Zipfian distribution of categories in natural images, LVIS\nnaturally has a long tail of categories with few training samples. Given that\nstate-of-the-art deep learning methods for object detection perform poorly in\nthe low-sample regime, we believe that our dataset poses an important and\nexciting new scientific challenge. LVIS is available at\nhttp://www.lvisdataset.org.\n", "versions": [{"version": "v1", "created": "Thu, 8 Aug 2019 17:57:01 GMT"}, {"version": "v2", "created": "Sun, 15 Sep 2019 15:10:45 GMT"}], "update_date": "2019-09-17", "authors_parsed": [["Gupta", "Agrim", ""], ["Doll\u00e1r", "Piotr", ""], ["Girshick", "Ross", ""]]}, {"id": "1908.03204", "submitter": "Wenshuai Zhao", "authors": "Wenshuai Zhao, Zengfeng Zeng", "title": "Multi Scale Supervised 3D U-Net for Kidney and Tumor Segmentation", "comments": "7 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  U-Net has achieved huge success in various medical image segmentation\nchallenges. Kinds of new architectures with bells and whistles might succeed in\ncertain dataset when employed with optimal hyper-parameter, but their\ngeneralization always can't be guaranteed. Here, we focused on the basic U-Net\narchitecture and proposed a multi scale supervised 3D U-Net for the\nsegmentation task in KiTS19 challenge. To enhance the performance, our work can\nbe summarized as three folds: first, we used multi scale supervision in the\ndecoder pathway, which could encourage the network to predict right results\nfrom the deep layers; second, with the aim to alleviate the bad effect from the\nsample imbalance of kidney and tumor, we adopted exponential logarithmic loss;\nthird, a connected-component based post processing method was designed to\nremove the obviously wrong voxels. In the published KiTS19 training dataset\n(totally 210 patients), we divided 42 patients to be test dataset and finally\nobtained DICE scores of 0.969 and 0.805 for the kidney and tumor respectively.\nIn the challenge, we finally achieved the 7th place among 106 teams with the\nComposite Dice of 0.8961, namely 0.9741 for kidney and 0.8181 for tumor.\n", "versions": [{"version": "v1", "created": "Fri, 9 Aug 2019 02:41:55 GMT"}, {"version": "v2", "created": "Tue, 13 Aug 2019 15:46:23 GMT"}], "update_date": "2019-08-14", "authors_parsed": [["Zhao", "Wenshuai", ""], ["Zeng", "Zengfeng", ""]]}, {"id": "1908.03231", "submitter": "Amor Ben Tanfous", "authors": "Amor Ben Tanfous, Hassen Drira, Boulbaba Ben Amor", "title": "Sparse Coding of Shape Trajectories for Facial Expression and Action\n  Recognition", "comments": "14 pages, 5 figures", "journal-ref": "IEEE Transactions on Pattern Analysis and Machine Intelligence\n  2019", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The detection and tracking of human landmarks in video streams has gained in\nreliability partly due to the availability of affordable RGB-D sensors. The\nanalysis of such time-varying geometric data is playing an important role in\nthe automatic human behavior understanding. However, suitable shape\nrepresentations as well as their temporal evolution, termed trajectories, often\nlie to nonlinear manifolds. This puts an additional constraint (i.e.,\nnonlinearity) in using conventional Machine Learning techniques. As a solution,\nthis paper accommodates the well-known Sparse Coding and Dictionary Learning\napproach to study time-varying shapes on the Kendall shape spaces of 2D and 3D\nlandmarks. We illustrate effective coding of 3D skeletal sequences for action\nrecognition and 2D facial landmark sequences for macro- and micro-expression\nrecognition. To overcome the inherent nonlinearity of the shape spaces,\nintrinsic and extrinsic solutions were explored. As main results, shape\ntrajectories give rise to more discriminative time-series with suitable\ncomputational properties, including sparsity and vector space structure.\nExtensive experiments conducted on commonly-used datasets demonstrate the\ncompetitiveness of the proposed approaches with respect to state-of-the-art.\n", "versions": [{"version": "v1", "created": "Thu, 8 Aug 2019 18:16:37 GMT"}], "update_date": "2019-08-12", "authors_parsed": [["Tanfous", "Amor Ben", ""], ["Drira", "Hassen", ""], ["Amor", "Boulbaba Ben", ""]]}, {"id": "1908.03237", "submitter": "Andong Cao", "authors": "Andong Cao, Ali Dhanaliwala, Jianbo Shi, Terence Gade, Brian Park", "title": "Image-based marker tracking and registration for intraoperative 3D\n  image-guided interventions using augmented reality", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Augmented reality has the potential to improve operating room workflow by\nallowing physicians to \"see\" inside a patient through the projection of imaging\ndirectly onto the surgical field. For this to be useful the acquired imaging\nmust be quickly and accurately registered with patient and the registration\nmust be maintained. Here we describe a method for projecting a CT scan with\nMicrosoft Hololens and then aligning that projection to a set of fiduciary\nmarkers. Radio-opaque stickers with unique QR-codes are placed on an object\nprior to acquiring a CT scan. The location of the markers in the CT scan are\nextracted and the CT scan is converted into a 3D surface object. The 3D object\nis then projected using the Hololens onto a table on which the same markers are\nplaced. We designed an algorithm that aligns the markers on the 3D object with\nthe markers on the table. To extract the markers and convert the CT into a 3D\nobject took less than 5 seconds. To align three markers, it took $0.9 \\pm 0.2$\nseconds to achieve an accuracy of $5 \\pm 2$ mm. These findings show that it is\nfeasible to use a combined radio-opaque optical marker, placed on a patient\nprior to a CT scan, to subsequently align the acquired CT scan with the\npatient.\n", "versions": [{"version": "v1", "created": "Thu, 8 Aug 2019 18:57:34 GMT"}], "update_date": "2019-08-12", "authors_parsed": [["Cao", "Andong", ""], ["Dhanaliwala", "Ali", ""], ["Shi", "Jianbo", ""], ["Gade", "Terence", ""], ["Park", "Brian", ""]]}, {"id": "1908.03238", "submitter": "Saeed Izadi", "authors": "Saeed Izadi, Zahra Mirikharaji, Mengliu Zhao, and Ghassan Hamarneh", "title": "WhiteNNer-Blind Image Denoising via Noise Whiteness Priors", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The accuracy of medical imaging-based diagnostics is directly impacted by the\nquality of the collected images. A passive approach to improve image quality is\none that lags behind improvements in imaging hardware, awaiting better sensor\ntechnology of acquisition devices. An alternative, active strategy is to\nutilize prior knowledge of the imaging system to directly post-process and\nimprove the acquired images. Traditionally, priors about the image properties\nare taken into account to restrict the solution space. However, few techniques\nexploit the prior about the noise properties. In this paper, we propose a\nneural network-based model for disentangling the signal and noise components of\nan input noisy image, without the need for any ground truth training data. We\ndesign a unified loss function that encodes priors about signal as well as\nnoise estimate in the form of regularization terms. Specifically, by using\ntotal variation and piecewise constancy priors along with noise whiteness\npriors such as auto-correlation and stationary losses, our network learns to\ndecouple an input noisy image into the underlying signal and noise components.\nWe compare our proposed method to Noise2Noise and Noise2Self, as well as\nnon-local mean and BM3D, on three public confocal laser endomicroscopy\ndatasets. Experimental results demonstrate the superiority of our network\ncompared to state-of-the-art in terms of PSNR and SSIM.\n", "versions": [{"version": "v1", "created": "Thu, 8 Aug 2019 19:00:17 GMT"}, {"version": "v2", "created": "Thu, 19 Sep 2019 19:02:22 GMT"}], "update_date": "2019-09-23", "authors_parsed": [["Izadi", "Saeed", ""], ["Mirikharaji", "Zahra", ""], ["Zhao", "Mengliu", ""], ["Hamarneh", "Ghassan", ""]]}, {"id": "1908.03245", "submitter": "Xiaohong Liu", "authors": "Xiaohong Liu, Yongrui Ma, Zhihao Shi, Jun Chen", "title": "GridDehazeNet: Attention-Based Multi-Scale Network for Image Dehazing", "comments": "10 pages, accepted in ICCV 2019, project page:\n  https://proteus1991.github.io/GridDehazeNet/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an end-to-end trainable Convolutional Neural Network (CNN), named\nGridDehazeNet, for single image dehazing. The GridDehazeNet consists of three\nmodules: pre-processing, backbone, and post-processing. The trainable\npre-processing module can generate learned inputs with better diversity and\nmore pertinent features as compared to those derived inputs produced by\nhand-selected pre-processing methods. The backbone module implements a novel\nattention-based multi-scale estimation on a grid network, which can effectively\nalleviate the bottleneck issue often encountered in the conventional\nmulti-scale approach. The post-processing module helps to reduce the artifacts\nin the final output. Experimental results indicate that the GridDehazeNet\noutperforms the state-of-the-arts on both synthetic and real-world images. The\nproposed hazing method does not rely on the atmosphere scattering model, and we\nprovide an explanation as to why it is not necessarily beneficial to take\nadvantage of the dimension reduction offered by the atmosphere scattering model\nfor image dehazing, even if only the dehazing results on synthetic images are\nconcerned.\n", "versions": [{"version": "v1", "created": "Thu, 8 Aug 2019 19:34:36 GMT"}], "update_date": "2019-08-12", "authors_parsed": [["Liu", "Xiaohong", ""], ["Ma", "Yongrui", ""], ["Shi", "Zhihao", ""], ["Chen", "Jun", ""]]}, {"id": "1908.03251", "submitter": "Siwei Zhang", "authors": "Yunxuan Zhang, Siwei Zhang, Yue He, Cheng Li, Chen Change Loy, Ziwei\n  Liu", "title": "One-shot Face Reenactment", "comments": "To appear in BMVC 2019 as a spotlight presentation. Code and models\n  are available at: https://github.com/bj80heyue/One_Shot_Face_Reenactment", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To enable realistic shape (e.g. pose and expression) transfer, existing face\nreenactment methods rely on a set of target faces for learning subject-specific\ntraits. However, in real-world scenario end-users often only have one target\nface at hand, rendering existing methods inapplicable. In this work, we bridge\nthis gap by proposing a novel one-shot face reenactment learning framework. Our\nkey insight is that the one-shot learner should be able to disentangle and\ncompose appearance and shape information for effective modeling. Specifically,\nthe target face appearance and the source face shape are first projected into\nlatent spaces with their corresponding encoders. Then these two latent spaces\nare associated by learning a shared decoder that aggregates multi-level\nfeatures to produce the final reenactment results. To further improve the\nsynthesizing quality on mustache and hair regions, we additionally propose\nFusionNet which combines the strengths of our learned decoder and the\ntraditional warping method. Extensive experiments show that our one-shot face\nreenactment system achieves superior transfer fidelity as well as identity\npreserving capability than alternatives. More remarkably, our approach trained\nwith only one target image per subject achieves competitive results to those\nusing a set of target images, demonstrating the practical merit of this work.\nCode, models and an additional set of reenacted faces have been publicly\nreleased at the project page.\n", "versions": [{"version": "v1", "created": "Mon, 5 Aug 2019 13:46:59 GMT"}], "update_date": "2019-08-12", "authors_parsed": [["Zhang", "Yunxuan", ""], ["Zhang", "Siwei", ""], ["He", "Yue", ""], ["Li", "Cheng", ""], ["Loy", "Chen Change", ""], ["Liu", "Ziwei", ""]]}, {"id": "1908.03266", "submitter": "Boyu Zhang", "authors": "Boyu Zhang and Azadeh Davoodi and Yu Hen Hu", "title": "Efficient Inference of CNNs via Channel Pruning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The deployment of Convolutional Neural Networks (CNNs) on resource\nconstrained platforms such as mobile devices and embedded systems has been\ngreatly hindered by their high implementation cost, and thus motivated a lot\nresearch interest in compressing and accelerating trained CNN models. Among\nvarious techniques proposed in literature, structured pruning, especially\nchannel pruning, has gain a lot focus due to 1) its superior performance in\nmemory, computation, and energy reduction; and 2) it is friendly to existing\nhardware and software libraries. In this paper, we investigate the intermediate\nresults of convolutional layers and present a novel pivoted QR factorization\nbased channel pruning technique that can prune any specified number of input\nchannels of any layer. We also explore more pruning opportunities in\nResNet-like architectures by applying two tweaks to our technique. Experiment\nresults on VGG-16 and ResNet-50 models with ImageNet ILSVRC 2012 dataset are\nvery impressive with 4.29X and 2.84X computation reduction while only\nsacrificing about 1.40\\% top-5 accuracy. Compared to many prior works, the\npruned models produced by our technique require up to 47.7\\% less computation\nwhile still achieve higher accuracies.\n", "versions": [{"version": "v1", "created": "Thu, 8 Aug 2019 20:57:27 GMT"}], "update_date": "2019-08-12", "authors_parsed": [["Zhang", "Boyu", ""], ["Davoodi", "Azadeh", ""], ["Hu", "Yu Hen", ""]]}, {"id": "1908.03272", "submitter": "Aman Shrivastava", "authors": "Aman Shrivastava, Karan Kant, Saurav Sengupta, Sung-Jun Kang, Marium\n  Khan, Asad Ali, Sean R. Moore, Beatrice C. Amadi, Paul Kelly, Donald E. Brown\n  and Sana Syed", "title": "Deep Learning for Visual Recognition of Environmental Enteropathy and\n  Celiac Disease", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Physicians use biopsies to distinguish between different but histologically\nsimilar enteropathies. The range of syndromes and pathologies that could cause\ndifferent gastrointestinal conditions makes this a difficult problem. Recently,\ndeep learning has been used successfully in helping diagnose cancerous tissues\nin histopathological images. These successes motivated the research presented\nin this paper, which describes a deep learning approach that distinguishes\nbetween Celiac Disease (CD) and Environmental Enteropathy (EE) and normal\ntissue from digitized duodenal biopsies. Experimental results show accuracies\nof over 90% for this approach. We also look into interpreting the neural\nnetwork model using Gradient-weighted Class Activation Mappings and filter\nactivations on input images to understand the visual explanations for the\ndecisions made by the model.\n", "versions": [{"version": "v1", "created": "Thu, 8 Aug 2019 21:44:30 GMT"}], "update_date": "2019-08-12", "authors_parsed": [["Shrivastava", "Aman", ""], ["Kant", "Karan", ""], ["Sengupta", "Saurav", ""], ["Kang", "Sung-Jun", ""], ["Khan", "Marium", ""], ["Ali", "Asad", ""], ["Moore", "Sean R.", ""], ["Amadi", "Beatrice C.", ""], ["Kelly", "Paul", ""], ["Brown", "Donald E.", ""], ["Syed", "Sana", ""]]}, {"id": "1908.03274", "submitter": "Ioan Andrei B\\^arsan", "authors": "Wei-Chiu Ma, Ignacio Tartavull, Ioan Andrei B\\^arsan, Shenlong Wang,\n  Min Bai, Gellert Mattyus, Namdar Homayounfar, Shrinidhi Kowshika\n  Lakshmikanth, Andrei Pokrovsky, Raquel Urtasun", "title": "Exploiting Sparse Semantic HD Maps for Self-Driving Vehicle Localization", "comments": "8 pages, 4 figures, 4 tables, 2019 IEEE/RSJ International Conference\n  on Intelligent Robots and Systems (IROS 2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose a novel semantic localization algorithm that\nexploits multiple sensors and has precision on the order of a few centimeters.\nOur approach does not require detailed knowledge about the appearance of the\nworld, and our maps require orders of magnitude less storage than maps utilized\nby traditional geometry- and LiDAR intensity-based localizers. This is\nimportant as self-driving cars need to operate in large environments. Towards\nthis goal, we formulate the problem in a Bayesian filtering framework, and\nexploit lanes, traffic signs, as well as vehicle dynamics to localize robustly\nwith respect to a sparse semantic map. We validate the effectiveness of our\nmethod on a new highway dataset consisting of 312km of roads. Our experiments\nshow that the proposed approach is able to achieve 0.05m lateral accuracy and\n1.12m longitudinal accuracy on average while taking up only 0.3% of the storage\nrequired by previous LiDAR intensity-based approaches.\n", "versions": [{"version": "v1", "created": "Thu, 8 Aug 2019 21:52:28 GMT"}], "update_date": "2019-08-12", "authors_parsed": [["Ma", "Wei-Chiu", ""], ["Tartavull", "Ignacio", ""], ["B\u00e2rsan", "Ioan Andrei", ""], ["Wang", "Shenlong", ""], ["Bai", "Min", ""], ["Mattyus", "Gellert", ""], ["Homayounfar", "Namdar", ""], ["Lakshmikanth", "Shrinidhi Kowshika", ""], ["Pokrovsky", "Andrei", ""], ["Urtasun", "Raquel", ""]]}, {"id": "1908.03289", "submitter": "Moshiur R Farazi", "authors": "Moshiur R Farazi, Salman H Khan, Nick Barnes", "title": "Question-Agnostic Attention for Visual Question Answering", "comments": "To appear in the proceedings of International Conference on Pattern\n  Recognition (ICPR) 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual Question Answering (VQA) models employ attention mechanisms to\ndiscover image locations that are most relevant for answering a specific\nquestion. For this purpose, several multimodal fusion strategies have been\nproposed, ranging from relatively simple operations (e.g., linear sum) to more\ncomplex ones (e.g., Block). The resulting multimodal representations define an\nintermediate feature space for capturing the interplay between visual and\nsemantic features, that is helpful in selectively focusing on image content. In\nthis paper, we propose a question-agnostic attention mechanism that is\ncomplementary to the existing question-dependent attention mechanisms. Our\nproposed model parses object instances to obtain an `object map' and applies\nthis map on the visual features to generate Question-Agnostic Attention (QAA)\nfeatures. In contrast to question-dependent attention approaches that are\nlearned end-to-end, the proposed QAA does not involve question-specific\ntraining, and can be easily included in almost any existing VQA model as a\ngeneric light-weight pre-processing step, thereby adding minimal computation\noverhead for training. Further, when used in complement with the\nquestion-dependent attention, the QAA allows the model to focus on the regions\ncontaining objects that might have been overlooked by the learned attention\nrepresentation. Through extensive evaluation on VQAv1, VQAv2 and TDIUC\ndatasets, we show that incorporating complementary QAA allows state-of-the-art\nVQA models to perform better, and provides significant boost to simplistic VQA\nmodels, enabling them to performance on par with highly sophisticated fusion\nstrategies.\n", "versions": [{"version": "v1", "created": "Fri, 9 Aug 2019 03:03:23 GMT"}, {"version": "v2", "created": "Sun, 6 Sep 2020 03:52:27 GMT"}], "update_date": "2020-09-08", "authors_parsed": [["Farazi", "Moshiur R", ""], ["Khan", "Salman H", ""], ["Barnes", "Nick", ""]]}, {"id": "1908.03295", "submitter": "Qiankun Tang", "authors": "Qiankun Tang, Shice Liu, Jie Li, Yu Hu", "title": "PosNeg-Balanced Anchors with Aligned Features for Single-Shot Object\n  Detection", "comments": "Submitted to a conference, under review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a novel single-shot object detector to ease the imbalance of\nforeground-background class by suppressing the easy negatives while increasing\nthe positives. To achieve this, we propose an Anchor Promotion Module (APM)\nwhich predicts the probability of each anchor as positive and adjusts their\ninitial locations and shapes to promote both the quality and quantity of\npositive anchors. In addition, we design an efficient Feature Alignment Module\n(FAM) to extract aligned features for fitting the promoted anchors with the\nhelp of both the location and shape transformation information from the APM. We\nassemble the two proposed modules to the backbone of VGG-16 and ResNet-101\nnetwork with an encoder-decoder architecture. Extensive experiments on MS COCO\nwell demonstrate our model performs competitively with alternative methods\n(40.0\\% mAP on \\textit{test-dev} set) and runs faster (28.6 \\textit{fps}).\n", "versions": [{"version": "v1", "created": "Fri, 9 Aug 2019 03:29:15 GMT"}], "update_date": "2019-08-12", "authors_parsed": [["Tang", "Qiankun", ""], ["Liu", "Shice", ""], ["Li", "Jie", ""], ["Hu", "Yu", ""]]}, {"id": "1908.03314", "submitter": "Zhuojun Chen", "authors": "Zhuojun Chen, Junhao Cheng, Yuchen Yuan, Dongping Liao, Yizhou Li,\n  Jiancheng Lv", "title": "Deep Density-aware Count Regressor", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We seek to improve crowd counting as we perceive limits of currently\nprevalent density map estimation approach on both prediction accuracy and time\nefficiency. We leverage multilevel pixelation of density map as it helps\nimprove SNR of training data and therefore, reduce prediction error. To achieve\na better model, we introduce multilayer gradient fusion for training a\ndensity-aware global count regressor. More specifically, on training stage, a\nbackbone network receives gradients from multiple branches to learn the density\ninformation, whereas those branches are to be detached to accelerate inference.\nBy taking advantages of such method, our model improves benchmark results on\npublic datasets and exhibits itself to be a new solution to crowd counting\nproblems in practice.\n", "versions": [{"version": "v1", "created": "Fri, 9 Aug 2019 04:44:13 GMT"}, {"version": "v2", "created": "Fri, 14 Feb 2020 08:13:03 GMT"}, {"version": "v3", "created": "Sun, 2 Aug 2020 02:02:05 GMT"}], "update_date": "2020-08-04", "authors_parsed": [["Chen", "Zhuojun", ""], ["Cheng", "Junhao", ""], ["Yuan", "Yuchen", ""], ["Liao", "Dongping", ""], ["Li", "Yizhou", ""], ["Lv", "Jiancheng", ""]]}, {"id": "1908.03323", "submitter": "Lingfeng Li", "authors": "Lingfeng Li, Shousheng Luo, Xue-Cheng Tai, Jiang Yang", "title": "Convex hull algorithms based on some variational models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Seeking the convex hull of an object is a very fundamental problem arising\nfrom various tasks. In this work, we propose two variational convex hull models\nusing level set representation for 2-dimensional data. The first one is an\nexact model, which can get the convex hull of one or multiple objects. In this\nmodel, the convex hull is characterized by the zero sublevel-set of a convex\nlevel set function, which is non-positive at every given point. By minimizing\nthe area of the zero sublevel-set, we can find the desired convex hull. The\nsecond one is intended to get convex hull of objects with outliers. Instead of\nrequiring all the given points are included, this model penalizes the distance\nfrom each given point to the zero sublevel-set. Literature methods are not able\nto handle outliers. For the solution of these models, we develop efficient\nnumerical schemes using alternating direction method of multipliers. Numerical\nexamples are given to demonstrate the advantages of the proposed methods.\n", "versions": [{"version": "v1", "created": "Fri, 9 Aug 2019 05:58:18 GMT"}], "update_date": "2019-08-12", "authors_parsed": [["Li", "Lingfeng", ""], ["Luo", "Shousheng", ""], ["Tai", "Xue-Cheng", ""], ["Yang", "Jiang", ""]]}, {"id": "1908.03335", "submitter": "Xiangyun Zhao", "authors": "Xiangyun Zhao, Yi Yang, Feng Zhou, Xiao Tan, Yuchen Yuan, Yingze Bao,\n  Ying Wu", "title": "Recognizing Part Attributes with Insufficient Data", "comments": "To appear in ICCV 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recognizing attributes of objects and their parts is important to many\ncomputer vision applications. Although great progress has been made to apply\nobject-level recognition, recognizing the attributes of parts remains less\napplicable since the training data for part attributes recognition is usually\nscarce especially for internet-scale applications. Furthermore, most existing\npart attribute recognition methods rely on the part annotation which is more\nexpensive to obtain. To solve the data insufficiency problem and get rid of\ndependence on the part annotation, we introduce a novel Concept Sharing Network\n(CSN) for part attribute recognition. A great advantage of CSN is its\ncapability of recognizing the part attribute (a combination of part location\nand appearance pattern) that has insufficient or zero training data, by\nlearning the part location and appearance pattern respectively from the\ntraining data that usually mix them in a single label. Extensive experiments on\nCUB-200-2011 [51], CelebA [35] and a newly proposed human attribute dataset\ndemonstrate the effectiveness of CSN and its advantages over other methods,\nespecially for the attributes with few training samples. Further experiments\nshow that CSN can also perform zero-shot part attribute recognition. The code\nwill be made available at\nhttps://github.com/Zhaoxiangyun/Concept-Sharing-Network.\n", "versions": [{"version": "v1", "created": "Fri, 9 Aug 2019 06:56:02 GMT"}, {"version": "v2", "created": "Tue, 13 Aug 2019 03:32:19 GMT"}], "update_date": "2019-08-14", "authors_parsed": [["Zhao", "Xiangyun", ""], ["Yang", "Yi", ""], ["Zhou", "Feng", ""], ["Tan", "Xiao", ""], ["Yuan", "Yuchen", ""], ["Bao", "Yingze", ""], ["Wu", "Ying", ""]]}, {"id": "1908.03339", "submitter": "Parisa Beham Mohamed Gani", "authors": "D.Sabarinathan, M.Parisa Beham and S.M.Md.Mansoor Roomi", "title": "Hyper Vision Net: Kidney Tumor Segmentation Using Coordinate\n  Convolutional Layer and Attention Unit", "comments": "9 pages, 3 figures, KiTs19 challenge", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  KiTs19 challenge paves the way to haste the improvement of solid kidney tumor\nsemantic segmentation methodologies. Accurate segmentation of kidney tumor in\ncomputer tomography (CT) images is a challenging task due to the non-uniform\nmotion, similar appearance and various shape. Inspired by this fact, in this\nmanuscript, we present a novel kidney tumor segmentation method using deep\nlearning network termed as Hyper vision Net model. All the existing U-net\nmodels are using a modified version of U-net to segment the kidney tumor\nregion. In the proposed architecture, we introduced supervision layers in the\ndecoder part, and it refines even minimal regions in the output. A dataset\nconsists of real arterial phase abdominal CT scans of 300 patients, including\n45964 images has been provided from KiTs19 for training and validation of the\nproposed model. Compared with the state-of-the-art segmentation methods, the\nresults demonstrate the superiority of our approach on training dice value\nscore of 0.9552 and 0.9633 in tumor region and kidney region, respectively.\n", "versions": [{"version": "v1", "created": "Fri, 9 Aug 2019 07:05:34 GMT"}], "update_date": "2019-08-12", "authors_parsed": [["Sabarinathan", "D.", ""], ["Beham", "M. Parisa", ""], ["Roomi", "S. M. Md. Mansoor", ""]]}, {"id": "1908.03361", "submitter": "Bj\\\"orn Barz", "authors": "Bj\\\"orn Barz, Kai Schr\\\"oter, Moritz M\\\"unch, Bin Yang, Andrea Unger,\n  Doris Dransch, Joachim Denzler", "title": "Enhancing Flood Impact Analysis using Interactive Retrieval of Social\n  Media Images", "comments": null, "journal-ref": "Archives of Data Science, Series A, 5.1, 2018", "doi": "10.5445/KSP/1000087327/06", "report-no": null, "categories": "cs.IR cs.CV cs.MM eess.IV", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  The analysis of natural disasters such as floods in a timely manner often\nsuffers from limited data due to a coarse distribution of sensors or sensor\nfailures. This limitation could be alleviated by leveraging information\ncontained in images of the event posted on social media platforms, so-called\n\"Volunteered Geographic Information (VGI)\". To save the analyst from the need\nto inspect all images posted online manually, we propose to use content-based\nimage retrieval with the possibility of relevance feedback for retrieving only\nrelevant images of the event to be analyzed. To evaluate this approach, we\nintroduce a new dataset of 3,710 flood images, annotated by domain experts\nregarding their relevance with respect to three tasks (determining the flooded\narea, inundation depth, water pollution). We compare several image features and\nrelevance feedback methods on that dataset, mixed with 97,085 distractor\nimages, and are able to improve the precision among the top 100 retrieval\nresults from 55% with the baseline retrieval to 87% after 5 rounds of feedback.\n", "versions": [{"version": "v1", "created": "Fri, 9 Aug 2019 08:29:57 GMT"}], "update_date": "2020-03-24", "authors_parsed": [["Barz", "Bj\u00f6rn", ""], ["Schr\u00f6ter", "Kai", ""], ["M\u00fcnch", "Moritz", ""], ["Yang", "Bin", ""], ["Unger", "Andrea", ""], ["Dransch", "Doris", ""], ["Denzler", "Joachim", ""]]}, {"id": "1908.03364", "submitter": "Yimin Lin", "authors": "Yimin Lin, Kai Wang, Wanxin Yi, Shiguo Lian", "title": "Deep Learning based Wearable Assistive System for Visually Impaired\n  People", "comments": "Accepted by ICCV/ACVR2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  In this paper, we propose a deep learning based assistive system to improve\nthe environment perception experience of visually impaired (VI). The system is\ncomposed of a wearable terminal equipped with an RGBD camera and an earphone, a\npowerful processor mainly for deep learning inferences and a smart phone for\ntouch-based interaction. A data-driven learning approach is proposed to predict\nsafe and reliable walkable instructions using RGBD data and the established\nsemantic map. This map is also used to help VI understand their 3D surrounding\nobjects and layout through well-designed touchscreen interactions. The\nquantitative and qualitative experimental results show that our learning based\nobstacle avoidance approach achieves excellent results in both indoor and\noutdoor datasets with low-lying obstacles. Meanwhile, user studies have also\nbeen carried out in various scenarios and showed the improvement of VI's\nenvironment perception experience with our system.\n", "versions": [{"version": "v1", "created": "Fri, 9 Aug 2019 08:41:06 GMT"}], "update_date": "2019-08-12", "authors_parsed": [["Lin", "Yimin", ""], ["Wang", "Kai", ""], ["Yi", "Wanxin", ""], ["Lian", "Shiguo", ""]]}, {"id": "1908.03381", "submitter": "Makarand Tapaswi", "authors": "Makarand Tapaswi and Marc T. Law and Sanja Fidler", "title": "Video Face Clustering with Unknown Number of Clusters", "comments": "Accepted to ICCV 2019, code and data at\n  https://github.com/makarandtapaswi/BallClustering_ICCV2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding videos such as TV series and movies requires analyzing who the\ncharacters are and what they are doing. We address the challenging problem of\nclustering face tracks based on their identity. Different from previous work in\nthis area, we choose to operate in a realistic and difficult setting where: (i)\nthe number of characters is not known a priori; and (ii) face tracks belonging\nto minor or background characters are not discarded.\n  To this end, we propose Ball Cluster Learning (BCL), a supervised approach to\ncarve the embedding space into balls of equal size, one for each cluster. The\nlearned ball radius is easily translated to a stopping criterion for iterative\nmerging algorithms. This gives BCL the ability to estimate the number of\nclusters as well as their assignment, achieving promising results on commonly\nused datasets. We also present a thorough discussion of how existing metric\nlearning literature can be adapted for this task.\n", "versions": [{"version": "v1", "created": "Fri, 9 Aug 2019 09:19:51 GMT"}, {"version": "v2", "created": "Tue, 20 Aug 2019 09:37:57 GMT"}], "update_date": "2019-08-21", "authors_parsed": [["Tapaswi", "Makarand", ""], ["Law", "Marc T.", ""], ["Fidler", "Sanja", ""]]}, {"id": "1908.03391", "submitter": "Qijun Zhao", "authors": "Qi He, Qijun Zhao, Ning Liu, Peng Chen, Zhihe Zhang, Rong Hou", "title": "Distinguishing Individual Red Pandas from Their Faces", "comments": "Accepted by the 2nd Chinese Conference on Pattern Recognition and\n  Computer Vision (PRCV 2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Individual identification is essential to animal behavior and ecology\nresearch and is of significant importance for protecting endangered species.\nRed pandas, among the world's rarest animals, are currently identified mainly\nby visual inspection and microelectronic chips, which are costly and\ninefficient. Motivated by recent advancement in computer-vision-based animal\nidentification, in this paper, we propose an automatic framework for\nidentifying individual red pandas based on their face images. We implement the\nframework by exploring well-established deep learning models with necessary\nadaptation for effectively dealing with red panda images. Based on a database\nof red panda images constructed by ourselves, we evaluate the effectiveness of\nthe proposed automatic individual red panda identification method. The\nevaluation results show the promising potential of automatically recognizing\nindividual red pandas from their faces. We are going to release our database\nand model in the public domain to promote the research on automatic animal\nidentification and particularly on the technique for protecting red pandas.\n", "versions": [{"version": "v1", "created": "Fri, 9 Aug 2019 10:01:28 GMT"}], "update_date": "2019-08-12", "authors_parsed": [["He", "Qi", ""], ["Zhao", "Qijun", ""], ["Liu", "Ning", ""], ["Chen", "Peng", ""], ["Zhang", "Zhihe", ""], ["Hou", "Rong", ""]]}, {"id": "1908.03409", "submitter": "Eugene Ie", "authors": "Haoshuo Huang, Vihan Jain, Harsh Mehta, Alexander Ku, Gabriel\n  Magalhaes, Jason Baldridge, Eugene Ie", "title": "Transferable Representation Learning in Vision-and-Language Navigation", "comments": "To appear in ICCV 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.LG cs.RO", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Vision-and-Language Navigation (VLN) tasks such as Room-to-Room (R2R) require\nmachine agents to interpret natural language instructions and learn to act in\nvisually realistic environments to achieve navigation goals. The overall task\nrequires competence in several perception problems: successful agents combine\nspatio-temporal, vision and language understanding to produce appropriate\naction sequences. Our approach adapts pre-trained vision and language\nrepresentations to relevant in-domain tasks making them more effective for VLN.\nSpecifically, the representations are adapted to solve both a cross-modal\nsequence alignment and sequence coherence task. In the sequence alignment task,\nthe model determines whether an instruction corresponds to a sequence of visual\nframes. In the sequence coherence task, the model determines whether the\nperceptual sequences are predictive sequentially in the instruction-conditioned\nlatent space. By transferring the domain-adapted representations, we improve\ncompetitive agents in R2R as measured by the success rate weighted by path\nlength (SPL) metric.\n", "versions": [{"version": "v1", "created": "Fri, 9 Aug 2019 10:58:01 GMT"}, {"version": "v2", "created": "Mon, 12 Aug 2019 22:00:55 GMT"}], "update_date": "2019-08-14", "authors_parsed": [["Huang", "Haoshuo", ""], ["Jain", "Vihan", ""], ["Mehta", "Harsh", ""], ["Ku", "Alexander", ""], ["Magalhaes", "Gabriel", ""], ["Baldridge", "Jason", ""], ["Ie", "Eugene", ""]]}, {"id": "1908.03438", "submitter": "Xuan Yang", "authors": "Xuan Yang, Zhengchao Chen, Baipeng Li, Dailiang Peng, Pan Chen, Bing\n  Zhang", "title": "A Fast and Precise Method for Large-Scale Land-Use Mapping Based on Deep\n  Learning", "comments": "Accepted at IEEE International Geoscience and Remote Sensing\n  Symposium (IGARSS) 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The land-use map is an important data that can reflect the use and\ntransformation of human land, and can provide valuable reference for land-use\nplanning. For the traditional image classification method, producing a high\nspatial resolution (HSR), land-use map in large-scale is a big project that\nrequires a lot of human labor, time, and financial expenditure. The rise of the\ndeep learning technique provides a new solution to the problems above. This\npaper proposes a fast and precise method that can achieve large-scale land-use\nclassification based on deep convolutional neural network (DCNN). In this\npaper, we optimize the data tiling method and the structure of DCNN for the\nmulti-channel data and the splicing edge effect, which are unique to remote\nsensing deep learning, and improve the accuracy of land-use classification. We\napply our improved methods in the Guangdong Province of China using GF-1\nimages, and achieve the land-use classification accuracy of 81.52%. It takes\nonly 13 hours to complete the work, which will take several months for human\nlabor.\n", "versions": [{"version": "v1", "created": "Fri, 9 Aug 2019 12:41:06 GMT"}], "update_date": "2019-08-12", "authors_parsed": [["Yang", "Xuan", ""], ["Chen", "Zhengchao", ""], ["Li", "Baipeng", ""], ["Peng", "Dailiang", ""], ["Chen", "Pan", ""], ["Zhang", "Bing", ""]]}, {"id": "1908.03448", "submitter": "Jialin Gao", "authors": "Jialin Gao, Zhixiang Shi, Jiani Li, Yufeng Yuan, Jiwei Li, Xi Zhou", "title": "Relation-Aware Pyramid Network (RapNet) for temporal action proposal", "comments": "Submission to temporal action proposal task in ActivityNet Challenge\n  2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this technical report, we describe our solution to temporal action\nproposal (task 1) in ActivityNet Challenge 2019. First, we fine-tune a\nResNet-50-C3D CNN on ActivityNet v1.3 based on Kinetics pretrained model to\nextract snippet-level video representations and then we design a Relation-Aware\nPyramid Network (RapNet) to generate temporal multiscale proposals with\nconfidence score. After that, we employ a two-stage snippet-level boundary\nadjustment scheme to re-rank the order of generated proposals. Ensemble methods\nare also been used to improve the performance of our solution, which helps us\nachieve 2nd place.\n", "versions": [{"version": "v1", "created": "Fri, 9 Aug 2019 13:20:32 GMT"}], "update_date": "2019-08-12", "authors_parsed": [["Gao", "Jialin", ""], ["Shi", "Zhixiang", ""], ["Li", "Jiani", ""], ["Yuan", "Yufeng", ""], ["Li", "Jiwei", ""], ["Zhou", "Xi", ""]]}, {"id": "1908.03454", "submitter": "Ayelet Heimowitz", "authors": "Ayelet Heimowitz, Joakim And\\'en and Amit Singer", "title": "Bias and variance reduction and denoising for CTF Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When using an electron microscope for imaging of particles embedded in\nvitreous ice, the objective lens will inevitably corrupt the projection images.\nThis corruption manifests as a band-pass filter on the micrograph. In addition,\nit causes the phase of several frequency bands to be flipped and distorts\nfrequency bands. As a precursor to compensating for this distortion, the\ncorrupting point spread function, which is termed the contrast transfer\nfunction (CTF) in reciprocal space, must be estimated. In this paper, we will\npresent a novel method for CTF estimation. Our method is based on the\nmulti-taper method for power spectral density estimation, which aims to reduce\nthe bias and variance of the estimator. Furthermore, we use known properties of\nthe CTF and of the background of the power spectrum to increase the accuracy of\nour estimation. We will show that the resulting estimates capture the\nzero-crossings of the CTF in the low-mid frequency range.\n", "versions": [{"version": "v1", "created": "Fri, 9 Aug 2019 13:32:41 GMT"}, {"version": "v2", "created": "Thu, 24 Oct 2019 13:00:41 GMT"}, {"version": "v3", "created": "Tue, 28 Jan 2020 16:36:04 GMT"}], "update_date": "2020-01-29", "authors_parsed": [["Heimowitz", "Ayelet", ""], ["And\u00e9n", "Joakim", ""], ["Singer", "Amit", ""]]}, {"id": "1908.03464", "submitter": "Zheng Wang", "authors": "Zheng Wang (1), Qiao Wang (2), Tingzhang Zhao (1), Xiaojun Ye (2) ((1)\n  Department of Computer Science, University of Science and Technology Beijing\n  (2) School of Software, Tsinghua University)", "title": "Zero-Shot Feature Selection via Transferring Supervised Knowledge", "comments": "Published in IJDWM21", "journal-ref": null, "doi": "10.4018/IJDWM.2021040101", "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Feature selection, an effective technique for dimensionality reduction, plays\nan important role in many machine learning systems. Supervised knowledge can\nsignificantly improve the performance. However, faced with the rapid growth of\nnewly emerging concepts, existing supervised methods might easily suffer from\nthe scarcity and validity of labeled data for training. In this paper, the\nauthors study the problem of zero-shot feature selection (i.e., building a\nfeature selection model that generalizes well to \"unseen\" concepts with limited\ntraining data of \"seen\" concepts). Specifically, they adopt class-semantic\ndescriptions (i.e., attributes) as supervision for feature selection, so as to\nutilize the supervised knowledge transferred from the seen concepts. For more\nreliable discriminative features, they further propose the\ncenter-characteristic loss which encourages the selected features to capture\nthe central characteristics of seen concepts. Extensive experiments conducted\non various real-world datasets demonstrate the effectiveness of the method.\n", "versions": [{"version": "v1", "created": "Fri, 9 Aug 2019 14:09:40 GMT"}, {"version": "v2", "created": "Fri, 28 Feb 2020 01:26:41 GMT"}, {"version": "v3", "created": "Wed, 14 Jul 2021 12:57:06 GMT"}], "update_date": "2021-07-15", "authors_parsed": [["Wang", "Zheng", ""], ["Wang", "Qiao", ""], ["Zhao", "Tingzhang", ""], ["Ye", "Xiaojun", ""]]}, {"id": "1908.03477", "submitter": "Michael Wray", "authors": "Michael Wray, Diane Larlus, Gabriela Csurka and Dima Damen", "title": "Fine-Grained Action Retrieval Through Multiple Parts-of-Speech\n  Embeddings", "comments": "Accepted for presentation at ICCV. Project Page:\n  https://mwray.github.io/FGAR", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of cross-modal fine-grained action retrieval between\ntext and video. Cross-modal retrieval is commonly achieved through learning a\nshared embedding space, that can indifferently embed modalities. In this paper,\nwe propose to enrich the embedding by disentangling parts-of-speech (PoS) in\nthe accompanying captions. We build a separate multi-modal embedding space for\neach PoS tag. The outputs of multiple PoS embeddings are then used as input to\nan integrated multi-modal space, where we perform action retrieval. All\nembeddings are trained jointly through a combination of PoS-aware and\nPoS-agnostic losses. Our proposal enables learning specialised embedding spaces\nthat offer multiple views of the same embedded entities.\n  We report the first retrieval results on fine-grained actions for the\nlarge-scale EPIC dataset, in a generalised zero-shot setting. Results show the\nadvantage of our approach for both video-to-text and text-to-video action\nretrieval. We also demonstrate the benefit of disentangling the PoS for the\ngeneric task of cross-modal video retrieval on the MSR-VTT dataset.\n", "versions": [{"version": "v1", "created": "Fri, 9 Aug 2019 14:41:06 GMT"}], "update_date": "2019-08-12", "authors_parsed": [["Wray", "Michael", ""], ["Larlus", "Diane", ""], ["Csurka", "Gabriela", ""], ["Damen", "Dima", ""]]}, {"id": "1908.03491", "submitter": "Jonathan Heek", "authors": "Jonathan Heek and Nal Kalchbrenner", "title": "Bayesian Inference for Large Scale Image Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian inference promises to ground and improve the performance of deep\nneural networks. It promises to be robust to overfitting, to simplify the\ntraining procedure and the space of hyperparameters, and to provide a\ncalibrated measure of uncertainty that can enhance decision making, agent\nexploration and prediction fairness. Markov Chain Monte Carlo (MCMC) methods\nenable Bayesian inference by generating samples from the posterior distribution\nover model parameters. Despite the theoretical advantages of Bayesian inference\nand the similarity between MCMC and optimization methods, the performance of\nsampling methods has so far lagged behind optimization methods for large scale\ndeep learning tasks. We aim to fill this gap and introduce ATMC, an adaptive\nnoise MCMC algorithm that estimates and is able to sample from the posterior of\na neural network. ATMC dynamically adjusts the amount of momentum and noise\napplied to each parameter update in order to compensate for the use of\nstochastic gradients. We use a ResNet architecture without batch normalization\nto test ATMC on the Cifar10 benchmark and the large scale ImageNet benchmark\nand show that, despite the absence of batch normalization, ATMC outperforms a\nstrong optimization baseline in terms of both classification accuracy and test\nlog-likelihood. We show that ATMC is intrinsically robust to overfitting on the\ntraining data and that ATMC provides a better calibrated measure of uncertainty\ncompared to the optimization baseline.\n", "versions": [{"version": "v1", "created": "Fri, 9 Aug 2019 15:15:56 GMT"}], "update_date": "2019-08-12", "authors_parsed": [["Heek", "Jonathan", ""], ["Kalchbrenner", "Nal", ""]]}, {"id": "1908.03557", "submitter": "Mark Yatskar", "authors": "Liunian Harold Li, Mark Yatskar, Da Yin, Cho-Jui Hsieh, Kai-Wei Chang", "title": "VisualBERT: A Simple and Performant Baseline for Vision and Language", "comments": "Work in Progress", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose VisualBERT, a simple and flexible framework for modeling a broad\nrange of vision-and-language tasks. VisualBERT consists of a stack of\nTransformer layers that implicitly align elements of an input text and regions\nin an associated input image with self-attention. We further propose two\nvisually-grounded language model objectives for pre-training VisualBERT on\nimage caption data. Experiments on four vision-and-language tasks including\nVQA, VCR, NLVR2, and Flickr30K show that VisualBERT outperforms or rivals with\nstate-of-the-art models while being significantly simpler. Further analysis\ndemonstrates that VisualBERT can ground elements of language to image regions\nwithout any explicit supervision and is even sensitive to syntactic\nrelationships, tracking, for example, associations between verbs and image\nregions corresponding to their arguments.\n", "versions": [{"version": "v1", "created": "Fri, 9 Aug 2019 17:57:13 GMT"}], "update_date": "2019-08-12", "authors_parsed": [["Li", "Liunian Harold", ""], ["Yatskar", "Mark", ""], ["Yin", "Da", ""], ["Hsieh", "Cho-Jui", ""], ["Chang", "Kai-Wei", ""]]}, {"id": "1908.03573", "submitter": "Rogier Wildeboer", "authors": "R. R. Wildeboer, R. J. G. van Sloun, C. K. Mannaerts, P. H. Moraes, G.\n  Salomon, M. C. Chammas, H. Wijkstra, M. Mischi", "title": "Synthetic Elastography using B-mode Ultrasound through a Deep\n  Fully-Convolutional Neural Network", "comments": "(c) 2020 IEEE. Personal use of this material is permitted. Permission\n  from IEEE must be obtained for all other uses, in any current or future\n  media, including reprinting/republishing this material for advertising or\n  promotional purposes, creating new collective works, for resale or\n  redistribution to servers or lists, or reuse of any copyrighted component of\n  this work in other works", "journal-ref": "IEEE Transactions on Ultrasonics, Ferroelectrics, and Frequency\n  Control, 2020", "doi": "10.1109/TUFFC.2020.2983099", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Shear-wave elastography (SWE) permits local estimation of tissue elasticity,\nan important imaging marker in biomedicine. This recently-developed, advanced\ntechnique assesses the speed of a laterally-travelling shear wave after an\nacoustic radiation force \"push\" to estimate local Young's moduli in an\noperator-independent fashion. In this work, we show how synthetic SWE (sSWE)\nimages can be generated based on conventional B-mode imaging through deep\nlearning. Using side-by-side-view B-mode/SWE images collected in 50 patients\nwith prostate cancer, we show that sSWE images with a pixel-wise mean absolute\nerror of 4.5+/-0.96 kPa with regard to the original SWE can be generated.\nVisualization of high-level feature levels through t-Distributed Stochastic\nNeighbor Embedding reveals substantial overlap between data from two different\nscanners. Qualitatively, we examined the use of the sSWE methodology for B-mode\nimages obtained with a scanner without SWE functionality. We also examined the\nuse of this type of network in elasticity imaging in the thyroid. Limitations\nof the technique reside in the fact that networks have to be retrained for\ndifferent organs, and that the method requires standardization of the imaging\nsettings and procedure. Future research will be aimed at development of sSWE as\nan elasticity-related tissue typing strategy that is solely based on B-mode\nultrasound acquisition, and the examination of its clinical utility.\n", "versions": [{"version": "v1", "created": "Fri, 9 Aug 2019 14:57:07 GMT"}, {"version": "v2", "created": "Sat, 4 Apr 2020 10:55:01 GMT"}], "update_date": "2020-04-07", "authors_parsed": [["Wildeboer", "R. R.", ""], ["van Sloun", "R. J. G.", ""], ["Mannaerts", "C. K.", ""], ["Moraes", "P. H.", ""], ["Salomon", "G.", ""], ["Chammas", "M. C.", ""], ["Wijkstra", "H.", ""], ["Mischi", "M.", ""]]}, {"id": "1908.03608", "submitter": "Jorge Calvo-Zaragoza", "authors": "Jorge Calvo-Zaragoza, Jan Haji\\v{c} Jr., Alexander Pacha", "title": "Understanding Optical Music Recognition", "comments": null, "journal-ref": "ACM Comput. Surv. 53, 4 (2020) Article 77", "doi": "10.1145/3397499", "report-no": null, "categories": "cs.CV cs.AI cs.IR cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For over 50 years, researchers have been trying to teach computers to read\nmusic notation, referred to as Optical Music Recognition (OMR). However, this\nfield is still difficult to access for new researchers, especially those\nwithout a significant musical background: few introductory materials are\navailable, and furthermore the field has struggled with defining itself and\nbuilding a shared terminology. In this tutorial, we address these shortcomings\nby (1) providing a robust definition of OMR and its relationship to related\nfields, (2) analyzing how OMR inverts the music encoding process to recover the\nmusical notation and the musical semantics from documents, (3) proposing a\ntaxonomy of OMR, with most notably a novel taxonomy of applications.\nAdditionally, we discuss how deep learning affects modern OMR research, as\nopposed to the traditional pipeline. Based on this work, the reader should be\nable to attain a basic understanding of OMR: its objectives, its inherent\nstructure, its relationship to other fields, the state of the art, and the\nresearch opportunities it affords.\n", "versions": [{"version": "v1", "created": "Wed, 7 Aug 2019 08:37:16 GMT"}, {"version": "v2", "created": "Wed, 14 Aug 2019 09:38:29 GMT"}, {"version": "v3", "created": "Wed, 29 Jul 2020 08:59:52 GMT"}], "update_date": "2020-07-30", "authors_parsed": [["Calvo-Zaragoza", "Jorge", ""], ["Haji\u010d", "Jan", "Jr."], ["Pacha", "Alexander", ""]]}, {"id": "1908.03621", "submitter": "Phil Ammirato", "authors": "Phil Ammirato, Alexander C. Berg", "title": "A Mask-RCNN Baseline for Probabilistic Object Detection", "comments": "2nd place in 1st PODC at CVPR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Probabilistic Object Detection Challenge evaluates object detection\nmethods using a new evaluation measure, Probability-based Detection Quality\n(PDQ), on a new synthetic image dataset. We present our submission to the\nchallenge, a fine-tuned version of Mask-RCNN with some additional\npost-processing. Our method, submitted under username pammirato, is currently\nsecond on the leaderboard with a score of 21.432, while also achieving the\nhighest spatial quality and average overall quality of detections. We hope this\nmethod can provide some insight into how detectors designed for mean average\nprecision (mAP) evaluation behave under PDQ, as well as a strong baseline for\nfuture work.\n", "versions": [{"version": "v1", "created": "Fri, 9 Aug 2019 20:45:46 GMT"}, {"version": "v2", "created": "Mon, 14 Oct 2019 15:45:38 GMT"}], "update_date": "2019-10-15", "authors_parsed": [["Ammirato", "Phil", ""], ["Berg", "Alexander C.", ""]]}, {"id": "1908.03630", "submitter": "Loris Nanni", "authors": "Alessandra Lumini, Loris Nanni, Alice Codogno, Filippo Berno", "title": "Learning morphological operators for skin detection", "comments": null, "journal-ref": "Journal of Artificial Intelligence and Systems (2019)", "doi": "10.33969/AIS.2019.11004", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we propose a novel post processing approach for skin detectors\nbased on trained morphological operators. The first step, consisting in skin\nsegmentation is performed according to an existing skin detection approach is\nperformed for skin segmentation, then a second step is carried out consisting\nin the application of a set of morphological operators to refine the resulting\nmask. Extensive experimental evaluation performed considering two different\ndetection approaches (one based on deep learning and a handcrafted one) carried\non 10 different datasets confirms the quality of the proposed method.\n", "versions": [{"version": "v1", "created": "Fri, 9 Aug 2019 21:05:11 GMT"}, {"version": "v2", "created": "Mon, 23 Sep 2019 22:01:10 GMT"}], "update_date": "2019-10-01", "authors_parsed": [["Lumini", "Alessandra", ""], ["Nanni", "Loris", ""], ["Codogno", "Alice", ""], ["Berno", "Filippo", ""]]}, {"id": "1908.03631", "submitter": "Jerry Liu", "authors": "Jerry Liu, Shenlong Wang, Raquel Urtasun", "title": "DSIC: Deep Stereo Image Compression", "comments": "Accepted at International Conference on Computer Vision 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we tackle the problem of stereo image compression, and leverage\nthe fact that the two images have overlapping fields of view to further\ncompress the representations. Our approach leverages state-of-the-art\nsingle-image compression autoencoders and enhances the compression with novel\nparametric skip functions to feed fully differentiable, disparity-warped\nfeatures at all levels to the encoder/decoder of the second image. Moreover, we\nmodel the probabilistic dependence between the image codes using a conditional\nentropy model. Our experiments show an impressive 30 - 50% reduction in the\nsecond image bitrate at low bitrates compared to deep single-image compression,\nand a 10 - 20% reduction at higher bitrates.\n", "versions": [{"version": "v1", "created": "Fri, 9 Aug 2019 21:10:23 GMT"}], "update_date": "2019-08-13", "authors_parsed": [["Liu", "Jerry", ""], ["Wang", "Shenlong", ""], ["Urtasun", "Raquel", ""]]}, {"id": "1908.03636", "submitter": "Uwe Schmidt", "authors": "Martin Weigert, Uwe Schmidt, Robert Haase, Ko Sugawara, Gene Myers", "title": "Star-convex Polyhedra for 3D Object Detection and Segmentation in\n  Microscopy", "comments": "Conference paper at WACV 2020", "journal-ref": null, "doi": "10.1109/WACV45572.2020.9093435", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurate detection and segmentation of cell nuclei in volumetric (3D)\nfluorescence microscopy datasets is an important step in many biomedical\nresearch projects. Although many automated methods for these tasks exist, they\noften struggle for images with low signal-to-noise ratios and/or dense packing\nof nuclei. It was recently shown for 2D microscopy images that these issues can\nbe alleviated by training a neural network to directly predict a suitable shape\nrepresentation (star-convex polygon) for cell nuclei. In this paper, we adopt\nand extend this approach to 3D volumes by using star-convex polyhedra to\nrepresent cell nuclei and similar shapes. To that end, we overcome the\nchallenges of 1) finding parameter-efficient star-convex polyhedra\nrepresentations that can faithfully describe cell nuclei shapes, 2) adapting to\nanisotropic voxel sizes often found in fluorescence microscopy datasets, and 3)\nefficiently computing intersections between pairs of star-convex polyhedra\n(required for non-maximum suppression). Although our approach is quite general,\nsince star-convex polyhedra include common shapes like bounding boxes and\nspheres as special cases, our focus is on accurate detection and segmentation\nof cell nuclei. Finally, we demonstrate on two challenging datasets that our\napproach (StarDist-3D) leads to superior results when compared to classical and\ndeep learning based methods.\n", "versions": [{"version": "v1", "created": "Fri, 9 Aug 2019 21:22:29 GMT"}, {"version": "v2", "created": "Mon, 15 Jun 2020 13:45:29 GMT"}], "update_date": "2020-06-16", "authors_parsed": [["Weigert", "Martin", ""], ["Schmidt", "Uwe", ""], ["Haase", "Robert", ""], ["Sugawara", "Ko", ""], ["Myers", "Gene", ""]]}, {"id": "1908.03651", "submitter": "Sema Berkiten", "authors": "Aurelia Guy and Sema Berkiten", "title": "A Distraction Score for Watermarks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In this work we propose a novel technique to quantify how distracting\nwatermarks are on an image. We begin with watermark detection using a two-tower\nCNN model composed of a binary classification task and a semantic segmentation\nprediction. With this model, we demonstrate significant improvement in image\nprecision while maintaining per-pixel accuracy, especially for our real-world\ndataset with sparse positive examples. We fit a nonlinear function to represent\ndetected watermarks by a single score correlated with human perception based on\ntheir size, location, and visual obstructiveness. Finally, we validate our\nmethod in an image ranking setup, which is the main application of our\nwatermark scoring algorithm.\n", "versions": [{"version": "v1", "created": "Fri, 9 Aug 2019 22:44:14 GMT"}], "update_date": "2019-08-13", "authors_parsed": [["Guy", "Aurelia", ""], ["Berkiten", "Sema", ""]]}, {"id": "1908.03671", "submitter": "Dohyun Kim Mr", "authors": "Dohyun Kim, Kyeorye Lee, Jiyeon Kim, Junseok Kwon, and Joongheon Kim", "title": "Deep ensemble network with explicit complementary model for\n  accuracy-balanced classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The average accuracy is one of major evaluation metrics for classification\nsystems, while the accuracy deviation is another important performance metric\nused to evaluate various deep neural networks. In this paper, we present a new\nensemble-like fast deep neural network, Harmony, that can reduce the accuracy\ndeviation among categories without degrading overall average accuracy. Harmony\nconsists of three sub-models, namely, Target model, Complementary model, and\nConductor model. In Harmony, an object is classified by using either Target\nmodel or Complementary model. Target model is a conventional classification\nnetwork for general categories, while Complementary model is a classification\nnetwork especially for weak categories that are inaccurately classified by\nTarget model. Conductor model is used to select one of two models. Experimental\nresults demonstrate that Harmony accurately classifies categories, while it\nreduces the accuracy deviation among the categories.\n", "versions": [{"version": "v1", "created": "Sat, 10 Aug 2019 02:34:42 GMT"}], "update_date": "2019-08-13", "authors_parsed": [["Kim", "Dohyun", ""], ["Lee", "Kyeorye", ""], ["Kim", "Jiyeon", ""], ["Kwon", "Junseok", ""], ["Kim", "Joongheon", ""]]}, {"id": "1908.03673", "submitter": "Xiongwei Wu", "authors": "Xiongwei Wu, Doyen Sahoo, Steven C.H. Hoi", "title": "Recent Advances in Deep Learning for Object Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Object detection is a fundamental visual recognition problem in computer\nvision and has been widely studied in the past decades. Visual object detection\naims to find objects of certain target classes with precise localization in a\ngiven image and assign each object instance a corresponding class label. Due to\nthe tremendous successes of deep learning based image classification, object\ndetection techniques using deep learning have been actively studied in recent\nyears. In this paper, we give a comprehensive survey of recent advances in\nvisual object detection with deep learning. By reviewing a large body of recent\nrelated work in literature, we systematically analyze the existing object\ndetection frameworks and organize the survey into three major parts: (i)\ndetection components, (ii) learning strategies, and (iii) applications &\nbenchmarks. In the survey, we cover a variety of factors affecting the\ndetection performance in detail, such as detector architectures, feature\nlearning, proposal generation, sampling strategies, etc. Finally, we discuss\nseveral future directions to facilitate and spur future research for visual\nobject detection with deep learning. Keywords: Object Detection, Deep Learning,\nDeep Convolutional Neural Networks\n", "versions": [{"version": "v1", "created": "Sat, 10 Aug 2019 02:54:17 GMT"}], "update_date": "2019-08-13", "authors_parsed": [["Wu", "Xiongwei", ""], ["Sahoo", "Doyen", ""], ["Hoi", "Steven C. H.", ""]]}, {"id": "1908.03675", "submitter": "Yinan Zhao", "authors": "Yinan Zhao, Brian Price, Scott Cohen, Danna Gurari", "title": "Unconstrained Foreground Object Search", "comments": "To appear in ICCV 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many people search for foreground objects to use when editing images. While\nexisting methods can retrieve candidates to aid in this, they are constrained\nto returning objects that belong to a pre-specified semantic class. We instead\npropose a novel problem of unconstrained foreground object (UFO) search and\nintroduce a solution that supports efficient search by encoding the background\nimage in the same latent space as the candidate foreground objects. A key\ncontribution of our work is a cost-free, scalable approach for creating a\nlarge-scale training dataset with a variety of foreground objects of differing\nsemantic categories per image location. Quantitative and human-perception\nexperiments with two diverse datasets demonstrate the advantage of our UFO\nsearch solution over related baselines.\n", "versions": [{"version": "v1", "created": "Sat, 10 Aug 2019 02:57:04 GMT"}], "update_date": "2019-08-13", "authors_parsed": [["Zhao", "Yinan", ""], ["Price", "Brian", ""], ["Cohen", "Scott", ""], ["Gurari", "Danna", ""]]}, {"id": "1908.03679", "submitter": "Francesco Caliva PhD", "authors": "Francesco Caliva, Claudia Iriondo, Alejandro Morales Martinez,\n  Sharmila Majumdar, Valentina Pedoia", "title": "Distance Map Loss Penalty Term for Semantic Segmentation", "comments": "Medical Imaging with Deep Learning (MIDL2019) Conference\n  [arXiv:1907.08612], Extended Abstract", "journal-ref": null, "doi": null, "report-no": "MIDL/2019/ExtendedAbstract/B1eIcvS45V", "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Convolutional neural networks for semantic segmentation suffer from low\nperformance at object boundaries. In medical imaging, accurate representation\nof tissue surfaces and volumes is important for tracking of disease biomarkers\nsuch as tissue morphology and shape features. In this work, we propose a novel\ndistance map derived loss penalty term for semantic segmentation. We propose to\nuse distance maps, derived from ground truth masks, to create a penalty term,\nguiding the network's focus towards hard-to-segment boundary regions. We\ninvestigate the effects of this penalizing factor against cross-entropy, Dice,\nand focal loss, among others, evaluating performance on a 3D MRI bone\nsegmentation task from the publicly available Osteoarthritis Initiative\ndataset. We observe a significant improvement in the quality of segmentation,\nwith better shape preservation at bone boundaries and areas affected by partial\nvolume. We ultimately aim to use our loss penalty term to improve the\nextraction of shape biomarkers and derive metrics to quantitatively evaluate\nthe preservation of shape.\n", "versions": [{"version": "v1", "created": "Sat, 10 Aug 2019 03:37:18 GMT"}], "update_date": "2019-08-13", "authors_parsed": [["Caliva", "Francesco", ""], ["Iriondo", "Claudia", ""], ["Martinez", "Alejandro Morales", ""], ["Majumdar", "Sharmila", ""], ["Pedoia", "Valentina", ""]]}, {"id": "1908.03682", "submitter": "Yang Liu", "authors": "Yang Liu, Jianpeng Zhang, Chao Gao, Jinghua Qu, Lixin Ji", "title": "Natural-Logarithm-Rectified Activation Function in Convolutional Neural\n  Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Activation functions play a key role in providing remarkable performance in\ndeep neural networks, and the rectified linear unit (ReLU) is one of the most\nwidely used activation functions. Various new activation functions and\nimprovements on ReLU have been proposed, but each carry performance drawbacks.\nIn this paper, we propose an improved activation function, which we name the\nnatural-logarithm-rectified linear unit (NLReLU). This activation function uses\nthe parametric natural logarithmic transform to improve ReLU and is simply\ndefined as. NLReLU not only retains the sparse activation characteristic of\nReLU, but it also alleviates the \"dying ReLU\" and vanishing gradient problems\nto some extent. It also reduces the bias shift effect and heteroscedasticity of\nneuron data distributions among network layers in order to accelerate the\nlearning process. The proposed method was verified across ten convolutional\nneural networks with different depths for two essential datasets. Experiments\nillustrate that convolutional neural networks with NLReLU exhibit higher\naccuracy than those with ReLU, and that NLReLU is comparable to other\nwell-known activation functions. NLReLU provides 0.16% and 2.04% higher\nclassification accuracy on average compared to ReLU when used in shallow\nconvolutional neural networks with the MNIST and CIFAR-10 datasets,\nrespectively. The average accuracy of deep convolutional neural networks with\nNLReLU is 1.35% higher on average with the CIFAR-10 dataset.\n", "versions": [{"version": "v1", "created": "Sat, 10 Aug 2019 03:51:36 GMT"}, {"version": "v2", "created": "Sun, 25 Aug 2019 02:24:49 GMT"}], "update_date": "2019-08-27", "authors_parsed": [["Liu", "Yang", ""], ["Zhang", "Jianpeng", ""], ["Gao", "Chao", ""], ["Qu", "Jinghua", ""], ["Ji", "Lixin", ""]]}, {"id": "1908.03684", "submitter": "Xing Wei", "authors": "Zhiheng Ma, Xing Wei, Xiaopeng Hong, Yihong Gong", "title": "Bayesian Loss for Crowd Count Estimation with Point Supervision", "comments": "Accepted by ICCV 2019 as an oral presentation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In crowd counting datasets, each person is annotated by a point, which is\nusually the center of the head. And the task is to estimate the total count in\na crowd scene. Most of the state-of-the-art methods are based on density map\nestimation, which convert the sparse point annotations into a \"ground truth\"\ndensity map through a Gaussian kernel, and then use it as the learning target\nto train a density map estimator. However, such a \"ground-truth\" density map is\nimperfect due to occlusions, perspective effects, variations in object shapes,\netc. On the contrary, we propose \\emph{Bayesian loss}, a novel loss function\nwhich constructs a density contribution probability model from the point\nannotations. Instead of constraining the value at every pixel in the density\nmap, the proposed training loss adopts a more reliable supervision on the count\nexpectation at each annotated point. Without bells and whistles, the loss\nfunction makes substantial improvements over the baseline loss on all tested\ndatasets. Moreover, our proposed loss function equipped with a standard\nbackbone network, without using any external detectors or multi-scale\narchitectures, plays favourably against the state of the arts. Our method\noutperforms previous best approaches by a large margin on the latest and\nlargest UCF-QNRF dataset. The source code is available at\n\\url{https://github.com/ZhihengCV/Baysian-Crowd-Counting}.\n", "versions": [{"version": "v1", "created": "Sat, 10 Aug 2019 04:01:36 GMT"}], "update_date": "2019-08-13", "authors_parsed": [["Ma", "Zhiheng", ""], ["Wei", "Xing", ""], ["Hong", "Xiaopeng", ""], ["Gong", "Yihong", ""]]}, {"id": "1908.03692", "submitter": "Guanghao Yin", "authors": "Guanghao Yin, Shouqian Sun, Hui Zhang, Dian Yu, Chao Li, Kejun Zhang,\n  Ning Zou", "title": "User independent Emotion Recognition with Residual Signal-Image Network", "comments": null, "journal-ref": "ICIP2019", "doi": "10.1109/ICIP.2019.8803627", "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  User independent emotion recognition with large scale physiological signals\nis a tough problem. There exist many advanced methods but they are conducted\nunder relatively small datasets with dozens of subjects. Here, we propose\nRes-SIN, a novel end-to-end framework using Electrodermal Activity(EDA) signal\nimages to classify human emotion. We first apply convex optimization-based EDA\n(cvxEDA) to decompose signals and mine the static and dynamic emotion changes.\nThen, we transform decomposed signals to images so that they can be effectively\nprocessed by CNN frameworks. The Res-SIN combines individual emotion features\nand external emotion benchmarks to accelerate convergence. We evaluate our\napproach on the PMEmo dataset, the currently largest emotional dataset\ncontaining music and EDA signals. To the best of author's knowledge, our method\nis the first attempt to classify large scale subject-independent emotion with\n7962 pieces of EDA signals from 457 subjects. Experimental results demonstrate\nthe reliability of our model and the binary classification accuracy of 73.65%\nand 73.43% on arousal and valence dimension can be used as a baseline.\n", "versions": [{"version": "v1", "created": "Sat, 10 Aug 2019 05:18:21 GMT"}, {"version": "v2", "created": "Mon, 3 Aug 2020 01:28:18 GMT"}], "update_date": "2020-08-04", "authors_parsed": [["Yin", "Guanghao", ""], ["Sun", "Shouqian", ""], ["Zhang", "Hui", ""], ["Yu", "Dian", ""], ["Li", "Chao", ""], ["Zhang", "Kejun", ""], ["Zou", "Ning", ""]]}, {"id": "1908.03693", "submitter": "Abdullah-Al-Zubaer Imran", "authors": "Abdullah-Al-Zubaer Imran, Demetri Terzopoulos", "title": "Semi-Supervised Multi-Task Learning With Chest X-Ray Images", "comments": "Accepted to Machine Learning in Medical Imaging (MLMI 2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Discriminative models that require full supervision are inefficacious in the\nmedical imaging domain when large labeled datasets are unavailable. By\ncontrast, generative modeling---i.e., learning data generation and\nclassification---facilitates semi-supervised training with limited labeled\ndata. Moreover, generative modeling can be advantageous in accomplishing\nmultiple objectives for better generalization. We propose a novel multi-task\nlearning model for jointly learning a classifier and a segmentor, from chest\nX-ray images, through semi-supervised learning. In addition, we propose a new\nloss function that combines absolute KL divergence with Tversky loss (KLTV) to\nyield faster convergence and better segmentation performance. Based on our\nexperimental results using a novel segmentation model, an Adversarial Pyramid\nProgressive Attention U-Net (APPAU-Net), we hypothesize that KLTV can be more\neffective for generalizing multi-tasking models while being competitive in\nsegmentation-only tasks.\n", "versions": [{"version": "v1", "created": "Sat, 10 Aug 2019 05:28:41 GMT"}, {"version": "v2", "created": "Mon, 26 Aug 2019 06:42:29 GMT"}], "update_date": "2019-08-27", "authors_parsed": [["Imran", "Abdullah-Al-Zubaer", ""], ["Terzopoulos", "Demetri", ""]]}, {"id": "1908.03701", "submitter": "Ziyuan Huang", "authors": "Changhong Fu, Ziyuan Huang, Yiming Li, Ran Duan, Peng Lu", "title": "Boundary Effect-Aware Visual Tracking for UAV with Online Enhanced\n  Background Learning and Multi-Frame Consensus Verification", "comments": "IROS 2019 accepted, 8 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to implicitly introduced periodic shifting of limited searching area,\nvisual object tracking using correlation filters often has to confront\nundesired boundary effect. As boundary effect severely degrade the quality of\nobject model, it has made it a challenging task for unmanned aerial vehicles\n(UAV) to perform robust and accurate object following. Traditional hand-crafted\nfeatures are also not precise and robust enough to describe the object in the\nviewing point of UAV. In this work, a novel tracker with online enhanced\nbackground learning is specifically proposed to tackle boundary effects. Real\nbackground samples are densely extracted to learn as well as update correlation\nfilters. Spatial penalization is introduced to offset the noise introduced by\nexceedingly more background information so that a more accurate appearance\nmodel can be established. Meanwhile, convolutional features are extracted to\nprovide a more comprehensive representation of the object. In order to mitigate\nchanges of objects' appearances, multi-frame technique is applied to learn an\nideal response map and verify the generated one in each frame. Exhaustive\nexperiments were conducted on 100 challenging UAV image sequences and the\nproposed tracker has achieved state-of-the-art performance.\n", "versions": [{"version": "v1", "created": "Sat, 10 Aug 2019 07:15:39 GMT"}], "update_date": "2019-08-13", "authors_parsed": [["Fu", "Changhong", ""], ["Huang", "Ziyuan", ""], ["Li", "Yiming", ""], ["Duan", "Ran", ""], ["Lu", "Peng", ""]]}, {"id": "1908.03706", "submitter": "Chunhua Shen", "authors": "Haokui Zhang, Chunhua Shen, Ying Li, Yuanzhouhan Cao, Yu Liu, Youliang\n  Yan", "title": "Exploiting temporal consistency for real-time video depth estimation", "comments": "Accepted to Proc. Int. Conf. Computer Vision 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Accuracy of depth estimation from static images has been significantly\nimproved recently, by exploiting hierarchical features from deep convolutional\nneural networks (CNNs). Compared with static images, vast information exists\namong video frames and can be exploited to improve the depth estimation\nperformance. In this work, we focus on exploring temporal information from\nmonocular videos for depth estimation. Specifically, we take the advantage of\nconvolutional long short-term memory (CLSTM) and propose a novel\nspatial-temporal CSLTM (ST-CLSTM) structure. Our ST-CLSTM structure can capture\nnot only the spatial features but also the temporal correlations/consistency\namong consecutive video frames with negligible increase in computational cost.\nAdditionally, in order to maintain the temporal consistency among the estimated\ndepth frames, we apply the generative adversarial learning scheme and design a\ntemporal consistency loss. The temporal consistency loss is combined with the\nspatial loss to update the model in an end-to-end fashion. By taking advantage\nof the temporal information, we build a video depth estimation framework that\nruns in real-time and generates visually pleasant results. Moreover, our\napproach is flexible and can be generalized to most existing depth estimation\nframeworks. Code is available at: https://tinyurl.com/STCLSTM\n", "versions": [{"version": "v1", "created": "Sat, 10 Aug 2019 07:47:36 GMT"}], "update_date": "2019-08-13", "authors_parsed": [["Zhang", "Haokui", ""], ["Shen", "Chunhua", ""], ["Li", "Ying", ""], ["Cao", "Yuanzhouhan", ""], ["Liu", "Yu", ""], ["Yan", "Youliang", ""]]}, {"id": "1908.03716", "submitter": "Junyu Gao", "authors": "Junyu Gao, Qi Wang, Yuan Yuan", "title": "SCAR: Spatial-/Channel-wise Attention Regression Networks for Crowd\n  Counting", "comments": "accepted by Neurocomputing", "journal-ref": null, "doi": "10.1016/j.neucom.2019.08.018", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, crowd counting is a hot topic in crowd analysis. Many CNN-based\ncounting algorithms attain good performance. However, these methods only focus\non the local appearance features of crowd scenes but ignore the large-range\npixel-wise contextual and crowd attention information. To remedy the above\nproblems, in this paper, we introduce the Spatial-/Channel-wise Attention\nModels into the traditional Regression CNN to estimate the density map, which\nis named as \"SCAR\". It consists of two modules, namely Spatial-wise Attention\nModel (SAM) and Channel-wise Attention Model (CAM). The former can encode the\npixel-wise context of the entire image to more accurately predict density maps\nat the pixel level. The latter attempts to extract more discriminative features\namong different channels, which aids model to pay attention to the head region,\nthe core of crowd scenes. Intuitively, CAM alleviates the mistaken estimation\nfor background regions. Finally, two types of attention information and\ntraditional CNN's feature maps are integrated by a concatenation operation.\nFurthermore, the extensive experiments are conducted on four popular datasets,\nShanghai Tech Part A/B, GCC, and UCF_CC_50 Dataset. The results show that the\nproposed method achieves state-of-the-art results.\n", "versions": [{"version": "v1", "created": "Sat, 10 Aug 2019 09:20:18 GMT"}], "update_date": "2019-08-13", "authors_parsed": [["Gao", "Junyu", ""], ["Wang", "Qi", ""], ["Yuan", "Yuan", ""]]}, {"id": "1908.03735", "submitter": "Bin Zhao", "authors": "Bin Zhao, Shuxue Ding, Hong Wu, Guohua Liu, Chen Cao, Song Jin,\n  Zhiyang Liu", "title": "Automatic acute ischemic stroke lesion segmentation using\n  semi-supervised learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ischemic stroke is a common disease in the elderly population, which can\ncause long-term disability and even death. However, the time window for\ntreatment of ischemic stroke in its acute stage is very short. To fast localize\nand quantitively evaluate the acute ischemic stroke (AIS) lesions, many\ndeep-learning-based lesion segmentation methods have been proposed in the\nliterature, where a deep convolutional neural network (CNN) was trained on\nhundreds of fully labeled subjects with accurate annotations of AIS lesions.\nDespite that high segmentation accuracy can be achieved, the accurate labels\nshould be annotated by experienced clinicians, and it is therefore very\ntime-consuming to obtain a large number of fully labeled subjects. In this\npaper, we propose a semi-supervised method to automatically segment AIS lesions\nin diffusion weighted images and apparent diffusion coefficient maps. By using\na large number of weakly labeled subjects and a small number of fully labeled\nsubjects, our proposed method is able to accurately detect and segment the AIS\nlesions. In particular, our proposed method consists of three parts: 1) a\ndouble-path classification net (DPC-Net) trained in a weakly-supervised way is\nused to detect the suspicious regions of AIS lesions; 2) a pixel-level K-Means\nclustering algorithm is used to identify the hyperintensive regions on the\nDWIs; and 3) a region-growing algorithm combines the outputs of the DPC-Net and\nthe K-Means to obtain the final precise lesion segmentation. In our experiment,\nwe use 460 weakly labeled subjects and 15 fully labeled subjects to train and\nfine-tune the proposed method. By evaluating on a clinical dataset with 150\nfully labeled subjects, our proposed method achieves a mean dice coefficient of\n0.642, and a lesion-wise F1 score of 0.822.\n", "versions": [{"version": "v1", "created": "Sat, 10 Aug 2019 11:48:02 GMT"}, {"version": "v2", "created": "Tue, 2 Jun 2020 02:53:45 GMT"}, {"version": "v3", "created": "Sun, 20 Sep 2020 13:20:00 GMT"}], "update_date": "2020-09-22", "authors_parsed": [["Zhao", "Bin", ""], ["Ding", "Shuxue", ""], ["Wu", "Hong", ""], ["Liu", "Guohua", ""], ["Cao", "Chen", ""], ["Jin", "Song", ""], ["Liu", "Zhiyang", ""]]}, {"id": "1908.03775", "submitter": "Mojtaba Sedigh Fazli", "authors": "Mojtaba S. Fazli, Rachel V. Stadler, BahaaEddin Alaila, Stephen A.\n  Vella, Silvia N. J. Moreno, Gary E. Ward, and Shannon Quinn", "title": "Lightweight and Scalable Particle Tracking and Motion Clustering of 3D\n  Cell Trajectories", "comments": "Accepted to 2019 IEEE International Conference on Data Science and\n  Advanced Analytics", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tracking cell particles in 3D microscopy videos is a challenging task but is\nof great significance for modeling the motion of cells. Proper characterization\nof the cell's shape, evolution, and their movement over time is crucial to\nunderstanding and modeling the mechanobiology of cell migration in many\ndiseases. One in particular, toxoplasmosis is the disease caused by the\nparasite Toxoplasma gondii. Roughly, one-third of the world's population tests\npositive for T. gondii. Its virulence is linked to its lytic cycle, predicated\non its motility and ability to enter and exit nucleated cells; therefore,\nstudies elucidating its motility patterns are critical to the eventual\ndevelopment of therapeutic strategies. Here, we present a computational\nframework for fast and scalable detection, tracking, and identification of T.\ngondii motion phenotypes in 3D videos, in a completely unsupervised fashion.\nOur pipeline consists of several different modules including preprocessing,\nsparsification, cell detection, cell tracking, trajectories extraction,\nparametrization of the trajectories; and finally, a clustering step.\nAdditionally, we identified the computational bottlenecks, and developed a\nlightweight and highly scalable pipeline through a combination of task\ndistribution and parallelism. Our results prove both the accuracy and\nperformance of our method.\n", "versions": [{"version": "v1", "created": "Sat, 10 Aug 2019 15:43:49 GMT"}, {"version": "v2", "created": "Wed, 14 Aug 2019 22:09:09 GMT"}, {"version": "v3", "created": "Tue, 12 Jan 2021 14:24:05 GMT"}], "update_date": "2021-01-13", "authors_parsed": [["Fazli", "Mojtaba S.", ""], ["Stadler", "Rachel V.", ""], ["Alaila", "BahaaEddin", ""], ["Vella", "Stephen A.", ""], ["Moreno", "Silvia N. J.", ""], ["Ward", "Gary E.", ""], ["Quinn", "Shannon", ""]]}, {"id": "1908.03792", "submitter": "Satoshi Kosugi", "authors": "Satoshi Kosugi, Toshihiko Yamasaki, Kiyoharu Aizawa", "title": "Object-Aware Instance Labeling for Weakly Supervised Object Detection", "comments": "Accepted to ICCV 2019 (oral)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Weakly supervised object detection (WSOD), where a detector is trained with\nonly image-level annotations, is attracting more and more attention. As a\nmethod to obtain a well-performing detector, the detector and the instance\nlabels are updated iteratively. In this study, for more efficient iterative\nupdating, we focus on the instance labeling problem, a problem of which label\nshould be annotated to each region based on the last localization result.\nInstead of simply labeling the top-scoring region and its highly overlapping\nregions as positive and others as negative, we propose more effective instance\nlabeling methods as follows. First, to solve the problem that regions covering\nonly some parts of the object tend to be labeled as positive, we find regions\ncovering the whole object focusing on the context classification loss. Second,\nconsidering the situation where the other objects contained in the image can be\nlabeled as negative, we impose a spatial restriction on regions labeled as\nnegative. Using these instance labeling methods, we train the detector on the\nPASCAL VOC 2007 and 2012 and obtain significantly improved results compared\nwith other state-of-the-art approaches.\n", "versions": [{"version": "v1", "created": "Sat, 10 Aug 2019 17:48:19 GMT"}], "update_date": "2019-08-13", "authors_parsed": [["Kosugi", "Satoshi", ""], ["Yamasaki", "Toshihiko", ""], ["Aizawa", "Kiyoharu", ""]]}, {"id": "1908.03809", "submitter": "Jonathan Howe", "authors": "Jonathan Howe, Kyle Pula, Aaron A. Reite", "title": "Conditional Generative Adversarial Networks for Data Augmentation and\n  Adaptation in Remotely Sensed Imagery", "comments": null, "journal-ref": null, "doi": "10.1117/12.2529586", "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The difficulty in obtaining labeled data relevant to a given task is among\nthe most common and well-known practical obstacles to applying deep learning\ntechniques to new or even slightly modified domains. The data volumes required\nby the current generation of supervised learning algorithms typically far\nexceed what a human needs to learn and complete a given task. We investigate\nways to expand a given labeled corpus of remote sensed imagery into a larger\ncorpus using Generative Adversarial Networks (GANs). We then measure how these\nadditional synthetic data affect supervised machine learning performance on an\nobject detection task.\n  Our data driven strategy is to train GANs to (1) generate synthetic\nsegmentation masks and (2) generate plausible synthetic remote sensing imagery\ncorresponding to these segmentation masks. Run sequentially, these GANs allow\nthe generation of synthetic remote sensing imagery complete with segmentation\nlabels. We apply this strategy to the data set from ISPRS' 2D Semantic Labeling\nContest - Potsdam, with a follow on vehicle detection task. We find that in\nscenarios with limited training data, augmenting the available data with such\nsynthetically generated data can improve detector performance.\n", "versions": [{"version": "v1", "created": "Sat, 10 Aug 2019 21:26:54 GMT"}], "update_date": "2019-09-24", "authors_parsed": [["Howe", "Jonathan", ""], ["Pula", "Kyle", ""], ["Reite", "Aaron A.", ""]]}, {"id": "1908.03812", "submitter": "Safa Alver", "authors": "Safa Alver, Ugur Halici", "title": "Attentive Deep Regression Networks for Real-Time Visual Face Tracking in\n  Video Surveillance", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual face tracking is one of the most important tasks in video surveillance\nsystems. However, due to the variations in pose, scale, expression, and\nillumination it is considered to be a difficult task. Recent studies show that\ndeep learning methods have a significant potential in object tracking tasks and\nadaptive feature selection methods can boost their performance. Motivated by\nthese, we propose an end-to-end attentive deep learning based tracker, that is\nbuild on top of the state-of-the-art GOTURN tracker, for the task of real-time\nvisual face tracking in video surveillance. Our method outperforms the\nstate-of-the-art GOTURN and IVT trackers by very large margins and it achieves\nspeeds that are very far beyond the requirements of real-time tracking.\nAdditionally, to overcome the scarce data problem in visual face tracking, we\nalso provide bounding box annotations for the G1 and G2 sets of ChokePoint\ndataset and make it suitable for further studies in face tracking under\nsurveillance conditions.\n", "versions": [{"version": "v1", "created": "Sat, 10 Aug 2019 21:40:25 GMT"}], "update_date": "2019-08-13", "authors_parsed": [["Alver", "Safa", ""], ["Halici", "Ugur", ""]]}, {"id": "1908.03826", "submitter": "Junru Wu", "authors": "Orest Kupyn, Tetiana Martyniuk, Junru Wu, Zhangyang Wang", "title": "DeblurGAN-v2: Deblurring (Orders-of-Magnitude) Faster and Better", "comments": "Accepted in ICCV 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new end-to-end generative adversarial network (GAN) for single\nimage motion deblurring, named DeblurGAN-v2, which considerably boosts\nstate-of-the-art deblurring efficiency, quality, and flexibility. DeblurGAN-v2\nis based on a relativistic conditional GAN with a double-scale discriminator.\nFor the first time, we introduce the Feature Pyramid Network into deblurring,\nas a core building block in the generator of DeblurGAN-v2. It can flexibly work\nwith a wide range of backbones, to navigate the balance between performance and\nefficiency. The plug-in of sophisticated backbones (e.g., Inception-ResNet-v2)\ncan lead to solid state-of-the-art deblurring. Meanwhile, with light-weight\nbackbones (e.g., MobileNet and its variants), DeblurGAN-v2 reaches 10-100 times\nfaster than the nearest competitors, while maintaining close to\nstate-of-the-art results, implying the option of real-time video deblurring. We\ndemonstrate that DeblurGAN-v2 obtains very competitive performance on several\npopular benchmarks, in terms of deblurring quality (both objective and\nsubjective), as well as efficiency. Besides, we show the architecture to be\neffective for general image restoration tasks too. Our codes, models and data\nare available at: https://github.com/KupynOrest/DeblurGANv2\n", "versions": [{"version": "v1", "created": "Sat, 10 Aug 2019 23:28:09 GMT"}], "update_date": "2019-08-13", "authors_parsed": [["Kupyn", "Orest", ""], ["Martyniuk", "Tetiana", ""], ["Wu", "Junru", ""], ["Wang", "Zhangyang", ""]]}, {"id": "1908.03835", "submitter": "Xinyu Gong", "authors": "Xinyu Gong, Shiyu Chang, Yifan Jiang, Zhangyang Wang", "title": "AutoGAN: Neural Architecture Search for Generative Adversarial Networks", "comments": "accepted by ICCV 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural architecture search (NAS) has witnessed prevailing success in image\nclassification and (very recently) segmentation tasks. In this paper, we\npresent the first preliminary study on introducing the NAS algorithm to\ngenerative adversarial networks (GANs), dubbed AutoGAN. The marriage of NAS and\nGANs faces its unique challenges. We define the search space for the generator\narchitectural variations and use an RNN controller to guide the search, with\nparameter sharing and dynamic-resetting to accelerate the process. Inception\nscore is adopted as the reward, and a multi-level search strategy is introduced\nto perform NAS in a progressive way. Experiments validate the effectiveness of\nAutoGAN on the task of unconditional image generation. Specifically, our\ndiscovered architectures achieve highly competitive performance compared to\ncurrent state-of-the-art hand-crafted GANs, e.g., setting new state-of-the-art\nFID scores of 12.42 on CIFAR-10, and 31.01 on STL-10, respectively. We also\nconclude with a discussion of the current limitations and future potential of\nAutoGAN. The code is available at https://github.com/TAMU-VITA/AutoGAN\n", "versions": [{"version": "v1", "created": "Sun, 11 Aug 2019 00:52:30 GMT"}], "update_date": "2019-08-13", "authors_parsed": [["Gong", "Xinyu", ""], ["Chang", "Shiyu", ""], ["Jiang", "Yifan", ""], ["Wang", "Zhangyang", ""]]}, {"id": "1908.03839", "submitter": "Yang Zhao", "authors": "Yang Zhao, Yifan Liu, Chunhua Shen, Yongsheng Gao, Shengwu Xiong", "title": "MobileFAN: Transferring Deep Hidden Representation for Face Alignment", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Facial landmark detection is a crucial prerequisite for many face analysis\napplications. Deep learning-based methods currently dominate the approach of\naddressing the facial landmark detection. However, such works generally\nintroduce a large number of parameters, resulting in high memory cost. In this\npaper, we aim for a lightweight as well as effective solution to facial\nlandmark detection. To this end, we propose an effective lightweight model,\nnamely Mobile Face Alignment Network (MobileFAN), using a simple backbone\nMobileNetV2 as the encoder and three deconvolutional layers as the decoder. The\nproposed MobileFAN, with only 8% of the model size and lower computational\ncost, achieves superior or equivalent performance compared with\nstate-of-the-art models. Moreover, by transferring the geometric structural\ninformation of a face graph from a large complex model to our proposed\nMobileFAN through feature-aligned distillation and feature-similarity\ndistillation, the performance of MobileFAN is further improved in effectiveness\nand efficiency for face alignment. Extensive experiment results on three\nchallenging facial landmark estimation benchmarks including COFW, 300W and WFLW\nshow the superiority of our proposed MobileFAN against state-of-the-art\nmethods.\n", "versions": [{"version": "v1", "created": "Sun, 11 Aug 2019 02:33:38 GMT"}, {"version": "v2", "created": "Mon, 30 Sep 2019 10:09:18 GMT"}, {"version": "v3", "created": "Wed, 20 Nov 2019 06:19:50 GMT"}], "update_date": "2019-11-21", "authors_parsed": [["Zhao", "Yang", ""], ["Liu", "Yifan", ""], ["Shen", "Chunhua", ""], ["Gao", "Yongsheng", ""], ["Xiong", "Shengwu", ""]]}, {"id": "1908.03846", "submitter": "Songyang Zhang", "authors": "Songyang Zhang, Jinsong Su, Jiebo Luo", "title": "Exploiting Temporal Relationships in Video Moment Localization with\n  Natural Language", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.CV cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of video moment localization with natural language,\ni.e. localizing a video segment described by a natural language sentence. While\nmost prior work focuses on grounding the query as a whole, temporal\ndependencies and reasoning between events within the text are not fully\nconsidered. In this paper, we propose a novel Temporal Compositional Modular\nNetwork (TCMN) where a tree attention network first automatically decomposes a\nsentence into three descriptions with respect to the main event, context event\nand temporal signal. Two modules are then utilized to measure the visual\nsimilarity and location similarity between each segment and the decomposed\ndescriptions. Moreover, since the main event and context event may rely on\ndifferent modalities (RGB or optical flow), we use late fusion to form an\nensemble of four models, where each model is independently trained by one\ncombination of the visual input. Experiments show that our model outperforms\nthe state-of-the-art methods on the TEMPO dataset.\n", "versions": [{"version": "v1", "created": "Sun, 11 Aug 2019 03:59:18 GMT"}], "update_date": "2019-08-13", "authors_parsed": [["Zhang", "Songyang", ""], ["Su", "Jinsong", ""], ["Luo", "Jiebo", ""]]}, {"id": "1908.03850", "submitter": "Jun Xu", "authors": "Haoqian Wang, Zhiwei Xu, Jun Xu, Wangpeng An, Lei Zhang, Qionghai Dai", "title": "Semi-Supervised Self-Growing Generative Adversarial Networks for Image\n  Recognition", "comments": "13 pages, 11 figures, 8 tables. arXiv admin note: text overlap with\n  arXiv:1606.03498 by other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image recognition is an important topic in computer vision and image\nprocessing, and has been mainly addressed by supervised deep learning methods,\nwhich need a large set of labeled images to achieve promising performance.\nHowever, in most cases, labeled data are expensive or even impossible to\nobtain, while unlabeled data are readily available from numerous free on-line\nresources and have been exploited to improve the performance of deep neural\nnetworks. To better exploit the power of unlabeled data for image recognition,\nin this paper, we propose a semi-supervised and generative approach, namely the\nsemi-supervised self-growing generative adversarial network (SGGAN). Label\ninference is a key step for the success of semi-supervised learning approaches.\nThere are two main problems in label inference: how to measure the confidence\nof the unlabeled data and how to generalize the classifier. We address these\ntwo problems via the generative framework and a novel\nconvolution-block-transformation technique, respectively. To stabilize and\nspeed up the training process of SGGAN, we employ the metric Maximum Mean\nDiscrepancy as the feature matching objective function and achieve larger gain\nthan the standard semi-supervised GANs (SSGANs), narrowing the gap to the\nsupervised methods. Experiments on several benchmark datasets show the\neffectiveness of the proposed SGGAN on image recognition and facial attribute\nrecognition tasks. By using the training data with only 4% labeled facial\nattributes, the SGGAN approach can achieve comparable accuracy with leading\nsupervised deep learning methods with all labeled facial attributes.\n", "versions": [{"version": "v1", "created": "Sun, 11 Aug 2019 04:10:33 GMT"}], "update_date": "2019-08-13", "authors_parsed": [["Wang", "Haoqian", ""], ["Xu", "Zhiwei", ""], ["Xu", "Jun", ""], ["An", "Wangpeng", ""], ["Zhang", "Lei", ""], ["Dai", "Qionghai", ""]]}, {"id": "1908.03851", "submitter": "Dingfu Zhou", "authors": "Dingfu Zhou, Jin Fang, Xibin Song, Chenye Guan, Junbo Yin, Yuchao Dai\n  and Ruigang Yang", "title": "IoU Loss for 2D/3D Object Detection", "comments": "Accepted by international conference on 3d vision 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In 2D/3D object detection task, Intersection-over-Union (IoU) has been widely\nemployed as an evaluation metric to evaluate the performance of different\ndetectors in the testing stage. However, during the training stage, the common\ndistance loss (\\eg, $L_1$ or $L_2$) is often adopted as the loss function to\nminimize the discrepancy between the predicted and ground truth Bounding Box\n(Bbox). To eliminate the performance gap between training and testing, the IoU\nloss has been introduced for 2D object detection in \\cite{yu2016unitbox} and\n\\cite{rezatofighi2019generalized}. Unfortunately, all these approaches only\nwork for axis-aligned 2D Bboxes, which cannot be applied for more general\nobject detection task with rotated Bboxes. To resolve this issue, we\ninvestigate the IoU computation for two rotated Bboxes first and then implement\na unified framework, IoU loss layer for both 2D and 3D object detection tasks.\nBy integrating the implemented IoU loss into several state-of-the-art 3D object\ndetectors, consistent improvements have been achieved for both bird-eye-view 2D\ndetection and point cloud 3D detection on the public KITTI benchmark.\n", "versions": [{"version": "v1", "created": "Sun, 11 Aug 2019 04:19:25 GMT"}], "update_date": "2019-08-13", "authors_parsed": [["Zhou", "Dingfu", ""], ["Fang", "Jin", ""], ["Song", "Xibin", ""], ["Guan", "Chenye", ""], ["Yin", "Junbo", ""], ["Dai", "Yuchao", ""], ["Yang", "Ruigang", ""]]}, {"id": "1908.03852", "submitter": "Yurui Ren", "authors": "Yurui Ren, Xiaoming Yu, Ruonan Zhang, Thomas H. Li, Shan Liu, Ge Li", "title": "StructureFlow: Image Inpainting via Structure-aware Appearance Flow", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image inpainting techniques have shown significant improvements by using deep\nneural networks recently. However, most of them may either fail to reconstruct\nreasonable structures or restore fine-grained textures. In order to solve this\nproblem, in this paper, we propose a two-stage model which splits the\ninpainting task into two parts: structure reconstruction and texture\ngeneration. In the first stage, edge-preserved smooth images are employed to\ntrain a structure reconstructor which completes the missing structures of the\ninputs. In the second stage, based on the reconstructed structures, a texture\ngenerator using appearance flow is designed to yield image details. Experiments\non multiple publicly available datasets show the superior performance of the\nproposed network.\n", "versions": [{"version": "v1", "created": "Sun, 11 Aug 2019 04:23:07 GMT"}], "update_date": "2019-08-13", "authors_parsed": [["Ren", "Yurui", ""], ["Yu", "Xiaoming", ""], ["Zhang", "Ruonan", ""], ["Li", "Thomas H.", ""], ["Liu", "Shan", ""], ["Li", "Ge", ""]]}, {"id": "1908.03856", "submitter": "Zhenyu Wu", "authors": "Zhenyu Wu, Karthik Suresh, Priya Narayanan, Hongyu Xu, Heesung Kwon\n  and Zhangyang Wang", "title": "Delving into Robust Object Detection from Unmanned Aerial Vehicles: A\n  Deep Nuisance Disentanglement Approach", "comments": "Accepted by International Conference on Computer Vision (ICCV) 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Object detection from images captured by Unmanned Aerial Vehicles (UAVs) is\nbecoming increasingly useful. Despite the great success of the generic object\ndetection methods trained on ground-to-ground images, a huge performance drop\nis observed when they are directly applied to images captured by UAVs. The\nunsatisfactory performance is owing to many UAV-specific nuisances, such as\nvarying flying altitudes, adverse weather conditions, dynamically changing\nviewing angles, etc. Those nuisances constitute a large number of fine-grained\ndomains, across which the detection model has to stay robust. Fortunately, UAVs\nwill record meta-data that depict those varying attributes, which are either\nfreely available along with the UAV images, or can be easily obtained. We\npropose to utilize those free meta-data in conjunction with associated UAV\nimages to learn domain-robust features via an adversarial training framework\ndubbed Nuisance Disentangled Feature Transform (NDFT), for the specific\nchallenging problem of object detection in UAV images, achieving a substantial\ngain in robustness to those nuisances. We demonstrate the effectiveness of our\nproposed algorithm, by showing state-of-the-art performance (single model) on\ntwo existing UAV-based object detection benchmarks. The code is available at\nhttps://github.com/TAMU-VITA/UAV-NDFT.\n", "versions": [{"version": "v1", "created": "Sun, 11 Aug 2019 05:24:25 GMT"}, {"version": "v2", "created": "Fri, 2 Oct 2020 18:27:09 GMT"}], "update_date": "2020-10-06", "authors_parsed": [["Wu", "Zhenyu", ""], ["Suresh", "Karthik", ""], ["Narayanan", "Priya", ""], ["Xu", "Hongyu", ""], ["Kwon", "Heesung", ""], ["Wang", "Zhangyang", ""]]}, {"id": "1908.03858", "submitter": "Wenzhong Zhou", "authors": "Wenzhong Zhou, Huiqian Du, Wenbo Mei and Liping Fang", "title": "Efficient Structurally-Strengthened Generative Adversarial Network for\n  MRI Reconstruction", "comments": "28 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Compressed sensing based magnetic resonance imaging (CS-MRI) provides an\nefficient way to reduce scanning time of MRI. Recently deep learning has been\nintroduced into CS-MRI to further improve the image quality and shorten\nreconstruction time. In this paper, we propose an efficient structurally\nstrengthened Generative Adversarial Network, termed ESSGAN, for reconstructing\nMR images from highly under-sampled k-space data. ESSGAN consists of a\nstructurally strengthened generator (SG) and a discriminator. In SG, we\nintroduce strengthened connections (SCs) to improve the utilization of the\nfeature maps between the proposed strengthened convolutional autoencoders\n(SCAEs), where each SCAE is a variant of a typical convolutional autoencoder.\nIn addition, we creatively introduce a residual in residual block (RIRB) to SG.\nRIRB increases the depth of SG, thus enhances feature expression ability of SG.\nMoreover, it can give the encoder blocks and the decoder blocks richer texture\nfeatures. To further reduce artifacts and preserve more image details, we\nintroduce an enhanced structural loss to SG. ESSGAN can provide higher image\nquality with less model parameters than the state-of-the-art deep\nlearning-based methods at different undersampling rates of different\nsubsampling masks, and reconstruct a 256*256 MR image in tens of milliseconds.\n", "versions": [{"version": "v1", "created": "Sun, 11 Aug 2019 06:13:00 GMT"}], "update_date": "2019-08-13", "authors_parsed": [["Zhou", "Wenzhong", ""], ["Du", "Huiqian", ""], ["Mei", "Wenbo", ""], ["Fang", "Liping", ""]]}, {"id": "1908.03864", "submitter": "Aurobrata Ghosh", "authors": "Aurobrata Ghosh, Zheng Zhong, Steve Cruz, Subbu Veeravasarapu,\n  Terrance E Boult, Maneesh Singh", "title": "To Beta or Not To Beta: Information Bottleneck for DigitaL Image\n  Forensics", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider an information theoretic approach to address the problem of\nidentifying fake digital images. We propose an innovative method to formulate\nthe issue of localizing manipulated regions in an image as a deep\nrepresentation learning problem using the Information Bottleneck (IB), which\nhas recently gained popularity as a framework for interpreting deep neural\nnetworks. Tampered images pose a serious predicament since digitized media is a\nubiquitous part of our lives. These are facilitated by the easy availability of\nimage editing software and aggravated by recent advances in deep generative\nmodels such as GANs. We propose InfoPrint, a computationally efficient solution\nto the IB formulation using approximate variational inference and compare it to\na numerical solution that is computationally expensive. Testing on a number of\nstandard datasets, we demonstrate that InfoPrint outperforms the\nstate-of-the-art and the numerical solution. Additionally, it also has the\nability to detect alterations made by inpainting GANs.\n", "versions": [{"version": "v1", "created": "Sun, 11 Aug 2019 07:28:35 GMT"}], "update_date": "2019-08-13", "authors_parsed": [["Ghosh", "Aurobrata", ""], ["Zhong", "Zheng", ""], ["Cruz", "Steve", ""], ["Veeravasarapu", "Subbu", ""], ["Boult", "Terrance E", ""], ["Singh", "Maneesh", ""]]}, {"id": "1908.03883", "submitter": "Stanislav Morozov", "authors": "Stanislav Morozov, Artem Babenko", "title": "Unsupervised Neural Quantization for Compressed-Domain Similarity Search", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We tackle the problem of unsupervised visual descriptors compression, which\nis a key ingredient of large-scale image retrieval systems. While the deep\nlearning machinery has benefited literally all computer vision pipelines, the\nexisting state-of-the-art compression methods employ shallow architectures, and\nwe aim to close this gap by our paper. In more detail, we introduce a DNN\narchitecture for the unsupervised compressed-domain retrieval, based on\nmulti-codebook quantization. The proposed architecture is designed to\nincorporate both fast data encoding and efficient distances computation via\nlookup tables. We demonstrate the exceptional advantage of our scheme over\nexisting quantization approaches on several datasets of visual descriptors via\noutperforming the previous state-of-the-art by a large margin.\n", "versions": [{"version": "v1", "created": "Sun, 11 Aug 2019 10:46:16 GMT"}], "update_date": "2019-08-13", "authors_parsed": [["Morozov", "Stanislav", ""], ["Babenko", "Artem", ""]]}, {"id": "1908.03884", "submitter": "Jogendra Nath Kundu", "authors": "Jogendra Nath Kundu, Nishank Lakkakula, R. Venkatesh Babu", "title": "UM-Adapt: Unsupervised Multi-Task Adaptation Using Adversarial\n  Cross-Task Distillation", "comments": "ICCV 2019 (Oral)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Aiming towards human-level generalization, there is a need to explore\nadaptable representation learning methods with greater transferability. Most\nexisting approaches independently address task-transferability and cross-domain\nadaptation, resulting in limited generalization. In this paper, we propose\nUM-Adapt - a unified framework to effectively perform unsupervised domain\nadaptation for spatially-structured prediction tasks, simultaneously\nmaintaining a balanced performance across individual tasks in a multi-task\nsetting. To realize this, we propose two novel regularization strategies; a)\nContour-based content regularization (CCR) and b) exploitation of inter-task\ncoherency using a cross-task distillation module. Furthermore, avoiding a\nconventional ad-hoc domain discriminator, we re-utilize the cross-task\ndistillation loss as output of an energy function to adversarially minimize the\ninput domain discrepancy. Through extensive experiments, we demonstrate\nsuperior generalizability of the learned representations simultaneously for\nmultiple tasks under domain-shifts from synthetic to natural environments.\nUM-Adapt yields state-of-the-art transfer learning results on ImageNet\nclassification and comparable performance on PASCAL VOC 2007 detection task,\neven with a smaller backbone-net. Moreover, the resulting semi-supervised\nframework outperforms the current fully-supervised multi-task learning\nstate-of-the-art on both NYUD and Cityscapes dataset.\n", "versions": [{"version": "v1", "created": "Sun, 11 Aug 2019 10:52:07 GMT"}, {"version": "v2", "created": "Mon, 19 Aug 2019 03:47:00 GMT"}, {"version": "v3", "created": "Mon, 16 Sep 2019 12:03:44 GMT"}], "update_date": "2019-09-17", "authors_parsed": [["Kundu", "Jogendra Nath", ""], ["Lakkakula", "Nishank", ""], ["Babu", "R. Venkatesh", ""]]}, {"id": "1908.03885", "submitter": "Gu Xinqian", "authors": "Xinqian Gu, Bingpeng Ma, Hong Chang, Shiguang Shan, Xilin Chen", "title": "Temporal Knowledge Propagation for Image-to-Video Person\n  Re-identification", "comments": "Accepted by ICCV 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many scenarios of Person Re-identification (Re-ID), the gallery set\nconsists of lots of surveillance videos and the query is just an image, thus\nRe-ID has to be conducted between image and videos. Compared with videos, still\nperson images lack temporal information. Besides, the information asymmetry\nbetween image and video features increases the difficulty in matching images\nand videos. To solve this problem, we propose a novel Temporal Knowledge\nPropagation (TKP) method which propagates the temporal knowledge learned by the\nvideo representation network to the image representation network. Specifically,\ngiven the input videos, we enforce the image representation network to fit the\noutputs of video representation network in a shared feature space. With back\npropagation, temporal knowledge can be transferred to enhance the image\nfeatures and the information asymmetry problem can be alleviated. With\nadditional classification and integrated triplet losses, our model can learn\nexpressive and discriminative image and video features for image-to-video\nre-identification. Extensive experiments demonstrate the effectiveness of our\nmethod and the overall results on two widely used datasets surpass the\nstate-of-the-art methods by a large margin. Code is available at:\nhttps://github.com/guxinqian/TKP\n", "versions": [{"version": "v1", "created": "Sun, 11 Aug 2019 11:22:27 GMT"}, {"version": "v2", "created": "Wed, 9 Oct 2019 14:22:18 GMT"}, {"version": "v3", "created": "Thu, 10 Oct 2019 02:10:27 GMT"}], "update_date": "2019-10-11", "authors_parsed": [["Gu", "Xinqian", ""], ["Ma", "Bingpeng", ""], ["Chang", "Hong", ""], ["Shan", "Shiguang", ""], ["Chen", "Xilin", ""]]}, {"id": "1908.03888", "submitter": "Anbang Yao", "authors": "Duo Li, Aojun Zhou, Anbang Yao", "title": "HBONet: Harmonious Bottleneck on Two Orthogonal Dimensions", "comments": "Accepted by ICCV 2019. Code and pretrained models are available at\n  https://github.com/d-li14/HBONet", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  MobileNets, a class of top-performing convolutional neural network\narchitectures in terms of accuracy and efficiency trade-off, are increasingly\nused in many resourceaware vision applications. In this paper, we present\nHarmonious Bottleneck on two Orthogonal dimensions (HBO), a novel architecture\nunit, specially tailored to boost the accuracy of extremely lightweight\nMobileNets at the level of less than 40 MFLOPs. Unlike existing bottleneck\ndesigns that mainly focus on exploring the interdependencies among the channels\nof either groupwise or depthwise convolutional features, our HBO improves\nbottleneck representation while maintaining similar complexity via jointly\nencoding the feature interdependencies across both spatial and channel\ndimensions. It has two reciprocal components, namely spatial\ncontraction-expansion and channel expansion-contraction, nested in a\nbilaterally symmetric structure. The combination of two interdependent\ntransformations performing on orthogonal dimensions of feature maps enhances\nthe representation and generalization ability of our proposed module,\nguaranteeing compelling performance with limited computational resource and\npower. By replacing the original bottlenecks in MobileNetV2 backbone with HBO\nmodules, we construct HBONets which are evaluated on ImageNet classification,\nPASCAL VOC object detection and Market-1501 person re-identification. Extensive\nexperiments show that with the severe constraint of computational budget our\nmodels outperform MobileNetV2 counterparts by remarkable margins of at most\n6.6%, 6.3% and 5.0% on the above benchmarks respectively. Code and pretrained\nmodels are available at https://github.com/d-li14/HBONet.\n", "versions": [{"version": "v1", "created": "Sun, 11 Aug 2019 11:37:39 GMT"}], "update_date": "2019-08-13", "authors_parsed": [["Li", "Duo", ""], ["Zhou", "Aojun", ""], ["Yao", "Anbang", ""]]}, {"id": "1908.03918", "submitter": "Changhao Chen", "authors": "Changhao Chen, Chris Xiaoxuan Lu, Bing Wang, Niki Trigoni, Andrew\n  Markham", "title": "DynaNet: Neural Kalman Dynamical Model for Motion Estimation and\n  Prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.RO stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Dynamical models estimate and predict the temporal evolution of physical\nsystems. State Space Models (SSMs) in particular represent the system dynamics\nwith many desirable properties, such as being able to model uncertainty in both\nthe model and measurements, and optimal (in the Bayesian sense) recursive\nformulations e.g. the Kalman Filter. However, they require significant domain\nknowledge to derive the parametric form and considerable hand-tuning to\ncorrectly set all the parameters. Data driven techniques e.g. Recurrent Neural\nNetworks have emerged as compelling alternatives to SSMs with wide success\nacross a number of challenging tasks, in part due to their ability to extract\nrelevant features from rich inputs. They however lack interpretability and\nrobustness to unseen conditions. In this work, we present DynaNet, a hybrid\ndeep learning and time-varying state-space model which can be trained\nend-to-end. Our neural Kalman dynamical model allows us to exploit the relative\nmerits of each approach. We demonstrate state-of-the-art estimation and\nprediction on a number of physically challenging tasks, including visual\nodometry, sensor fusion for visual-inertial navigation and pendulum control. In\naddition we show how DynaNet can indicate failures through investigation of\nproperties such as the rate of innovation (Kalman Gain).\n", "versions": [{"version": "v1", "created": "Sun, 11 Aug 2019 15:03:24 GMT"}, {"version": "v2", "created": "Fri, 14 Aug 2020 16:58:17 GMT"}], "update_date": "2020-08-17", "authors_parsed": [["Chen", "Changhao", ""], ["Lu", "Chris Xiaoxuan", ""], ["Wang", "Bing", ""], ["Trigoni", "Niki", ""], ["Markham", "Andrew", ""]]}, {"id": "1908.03919", "submitter": "Jogendra Nath Kundu", "authors": "Jogendra Nath Kundu, Maharshi Gor, Dakshit Agrawal, R. Venkatesh Babu", "title": "GAN-Tree: An Incrementally Learned Hierarchical Generative Framework for\n  Multi-Modal Data Distributions", "comments": "ICCV 2019 (code available at https://github.com/val-iisc/GANTree)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the remarkable success of generative adversarial networks, their\nperformance seems less impressive for diverse training sets, requiring learning\nof discontinuous mapping functions. Though multi-mode prior or multi-generator\nmodels have been proposed to alleviate this problem, such approaches may fail\ndepending on the empirically chosen initial mode components. In contrast to\nsuch bottom-up approaches, we present GAN-Tree, which follows a hierarchical\ndivisive strategy to address such discontinuous multi-modal data. Devoid of any\nassumption on the number of modes, GAN-Tree utilizes a novel mode-splitting\nalgorithm to effectively split the parent mode to semantically cohesive\nchildren modes, facilitating unsupervised clustering. Further, it also enables\nincremental addition of new data modes to an already trained GAN-Tree, by\nupdating only a single branch of the tree structure. As compared to prior\napproaches, the proposed framework offers a higher degree of flexibility in\nchoosing a large variety of mutually exclusive and exhaustive tree nodes called\nGAN-Set. Extensive experiments on synthetic and natural image datasets\nincluding ImageNet demonstrate the superiority of GAN-Tree against the prior\nstate-of-the-arts.\n", "versions": [{"version": "v1", "created": "Sun, 11 Aug 2019 15:15:49 GMT"}, {"version": "v2", "created": "Mon, 19 Aug 2019 03:42:52 GMT"}, {"version": "v3", "created": "Mon, 16 Sep 2019 11:56:58 GMT"}], "update_date": "2019-09-17", "authors_parsed": [["Kundu", "Jogendra Nath", ""], ["Gor", "Maharshi", ""], ["Agrawal", "Dakshit", ""], ["Babu", "R. Venkatesh", ""]]}, {"id": "1908.03930", "submitter": "Xiaohan Ding", "authors": "Xiaohan Ding, Yuchen Guo, Guiguang Ding, Jungong Han", "title": "ACNet: Strengthening the Kernel Skeletons for Powerful CNN via\n  Asymmetric Convolution Blocks", "comments": "Accepted to ICCV 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As designing appropriate Convolutional Neural Network (CNN) architecture in\nthe context of a given application usually involves heavy human works or\nnumerous GPU hours, the research community is soliciting the\narchitecture-neutral CNN structures, which can be easily plugged into multiple\nmature architectures to improve the performance on our real-world applications.\nWe propose Asymmetric Convolution Block (ACB), an architecture-neutral\nstructure as a CNN building block, which uses 1D asymmetric convolutions to\nstrengthen the square convolution kernels. For an off-the-shelf architecture,\nwe replace the standard square-kernel convolutional layers with ACBs to\nconstruct an Asymmetric Convolutional Network (ACNet), which can be trained to\nreach a higher level of accuracy. After training, we equivalently convert the\nACNet into the same original architecture, thus requiring no extra computations\nanymore. We have observed that ACNet can improve the performance of various\nmodels on CIFAR and ImageNet by a clear margin. Through further experiments, we\nattribute the effectiveness of ACB to its capability of enhancing the model's\nrobustness to rotational distortions and strengthening the central skeleton\nparts of square convolution kernels.\n", "versions": [{"version": "v1", "created": "Sun, 11 Aug 2019 16:06:58 GMT"}, {"version": "v2", "created": "Tue, 20 Aug 2019 09:24:11 GMT"}, {"version": "v3", "created": "Sat, 31 Aug 2019 12:50:35 GMT"}], "update_date": "2019-09-04", "authors_parsed": [["Ding", "Xiaohan", ""], ["Guo", "Yuchen", ""], ["Ding", "Guiguang", ""], ["Han", "Jungong", ""]]}, {"id": "1908.03935", "submitter": "Vanderson Martins do Rosario", "authors": "Vanderson M. do Rosario, Mauricio Breternitz Jr. and Edson Borin", "title": "Efficiency and Scalability of Multi-Lane Capsule Networks (MLCN)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Some Deep Neural Networks (DNN) have what we call lanes, or they can be\nreorganized as such. Lanes are paths in the network which are data-independent\nand typically learn different features or add resilience to the network. Given\ntheir data-independence, lanes are amenable for parallel processing. The\nMulti-lane CapsNet (MLCN) is a proposed reorganization of the Capsule Network\nwhich is shown to achieve better accuracy while bringing highly-parallel lanes.\nHowever, the efficiency and scalability of MLCN had not been systematically\nexamined. In this work, we study the MLCN network with multiple GPUs finding\nthat it is 2x more efficient than the original CapsNet when using\nmodel-parallelism. Further, we present the load balancing problem of\ndistributing heterogeneous lanes in homogeneous or heterogeneous accelerators\nand show that a simple greedy heuristic can be almost 50% faster than a naive\nrandom approach.\n", "versions": [{"version": "v1", "created": "Sun, 11 Aug 2019 17:04:14 GMT"}], "update_date": "2019-08-13", "authors_parsed": [["Rosario", "Vanderson M. do", ""], ["Breternitz", "Mauricio", "Jr."], ["Borin", "Edson", ""]]}, {"id": "1908.03945", "submitter": "Nathanael Lemessa Baisa", "authors": "Nathanael L. Baisa", "title": "Robust Online Multi-target Visual Tracking using a HISP Filter with\n  Discriminative Deep Appearance Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel online multi-target visual tracker based on the recently\ndeveloped Hypothesized and Independent Stochastic Population (HISP) filter. The\nHISP filter combines advantages of traditional tracking approaches like MHT and\npoint-process-based approaches like PHD filter, and it has linear complexity\nwhile maintaining track identities. We apply this filter for tracking multiple\ntargets in video sequences acquired under varying environmental conditions and\ntargets density using a tracking-by-detection approach. We also adopt deep CNN\nappearance representation by training a verification-identification network\n(VerIdNet) on large-scale person re-identification data sets. We construct an\naugmented likelihood in a principled manner using this deep CNN appearance\nfeatures and spatio-temporal information. Furthermore, we solve the problem of\ntwo or more targets having identical label considering the weight propagated\nwith each confirmed hypothesis. Extensive experiments on MOT16 and MOT17\nbenchmark data sets show that our tracker significantly outperforms several\nstate-of-the-art trackers in terms of tracking accuracy.\n", "versions": [{"version": "v1", "created": "Sun, 11 Aug 2019 18:15:57 GMT"}, {"version": "v2", "created": "Sun, 18 Aug 2019 14:12:46 GMT"}, {"version": "v3", "created": "Sat, 26 Oct 2019 12:30:32 GMT"}, {"version": "v4", "created": "Sun, 12 Jan 2020 14:37:25 GMT"}, {"version": "v5", "created": "Sat, 3 Oct 2020 19:48:38 GMT"}, {"version": "v6", "created": "Sun, 11 Oct 2020 11:54:32 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Baisa", "Nathanael L.", ""]]}, {"id": "1908.03958", "submitter": "Nishant Kumar", "authors": "Nishant Kumar, Nico Hoffmann, Martin Oelschl\\\"agel, Edmund Koch,\n  Matthias Kirsch, and Stefan Gumhold", "title": "Structural Similarity based Anatomical and Functional Brain Imaging\n  Fusion", "comments": "Accepted at MICCAI-MBIA 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multimodal medical image fusion helps in combining contrasting features from\ntwo or more input imaging modalities to represent fused information in a single\nimage. One of the pivotal clinical applications of medical image fusion is the\nmerging of anatomical and functional modalities for fast diagnosis of malignant\ntissues. In this paper, we present a novel end-to-end unsupervised\nlearning-based Convolutional Neural Network (CNN) for fusing the high and low\nfrequency components of MRI-PET grayscale image pairs, publicly available at\nADNI, by exploiting Structural Similarity Index (SSIM) as the loss function\nduring training. We then apply color coding for the visualization of the fused\nimage by quantifying the contribution of each input image in terms of the\npartial derivatives of the fused image. We find that our fusion and\nvisualization approach results in better visual perception of the fused image,\nwhile also comparing favorably to previous methods when applying various\nquantitative assessment metrics.\n", "versions": [{"version": "v1", "created": "Sun, 11 Aug 2019 20:44:52 GMT"}, {"version": "v2", "created": "Tue, 13 Aug 2019 14:33:49 GMT"}, {"version": "v3", "created": "Wed, 14 Aug 2019 07:45:31 GMT"}, {"version": "v4", "created": "Wed, 18 Sep 2019 20:20:35 GMT"}], "update_date": "2019-09-20", "authors_parsed": [["Kumar", "Nishant", ""], ["Hoffmann", "Nico", ""], ["Oelschl\u00e4gel", "Martin", ""], ["Koch", "Edmund", ""], ["Kirsch", "Matthias", ""], ["Gumhold", "Stefan", ""]]}, {"id": "1908.03978", "submitter": "Zhenwei Ma", "authors": "Gaoqi He, Zhenwei Ma, Binhao Huang, Bin Sheng, Yubo Yuan", "title": "Dynamic Region Division for Adaptive Learning Pedestrian Counting", "comments": "accepted by IEEE International Conference on Multimedia and Expo\n  (ICME) 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurate pedestrian counting algorithm is critical to eliminate insecurity in\nthe congested public scenes. However, counting pedestrians in crowded scenes\noften suffer from severe perspective distortion. In this paper, basing on the\nstraight-line double region pedestrian counting method, we propose a dynamic\nregion division algorithm to keep the completeness of counting objects.\nUtilizing the object bounding boxes obtained by YoloV3 and expectation division\nline of the scene, the boundary for nearby region and distant one is generated\nunder the premise of retaining whole head. Ulteriorly, appropriate learning\nmodels are applied to count pedestrians in each obtained region. In the distant\nregion, a novel inception dilated convolutional neural network is proposed to\nsolve the problem of choosing dilation rate. In the nearby region, YoloV3 is\nused for detecting the pedestrian in multi-scale. Accordingly, the total number\nof pedestrians in each frame is obtained by fusing the result in nearby and\ndistant regions. A typical subway pedestrian video dataset is chosen to conduct\nexperiment in this paper. The result demonstrate that proposed algorithm is\nsuperior to existing machine learning based methods in general performance.\n", "versions": [{"version": "v1", "created": "Mon, 12 Aug 2019 01:17:50 GMT"}], "update_date": "2019-08-13", "authors_parsed": [["He", "Gaoqi", ""], ["Ma", "Zhenwei", ""], ["Huang", "Binhao", ""], ["Sheng", "Bin", ""], ["Yuan", "Yubo", ""]]}, {"id": "1908.03983", "submitter": "Chuanxing Geng", "authors": "Chuanxing Geng, Lue Tao and Songcan Chen", "title": "Visual and Semantic Prototypes-Jointly Guided CNN for Generalized\n  Zero-shot Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the process of exploring the world, the curiosity constantly drives humans\nto cognize new things. Supposing you are a zoologist, for a presented animal\nimage, you can recognize it immediately if you know its class. Otherwise, you\nwould more likely attempt to cognize it by exploiting the side-information\n(e.g., semantic information, etc.) you have accumulated. Inspired by this, this\npaper decomposes the generalized zero-shot learning (G-ZSL) task into an open\nset recognition (OSR) task and a zero-shot learning (ZSL) task, where OSR\nrecognizes seen classes (if we have seen (or known) them) and rejects unseen\nclasses (if we have never seen (or known) them before), while ZSL identifies\nthe unseen classes rejected by the former. Simultaneously, without violating\nOSR's assumptions (only known class knowledge is available in training), we\nalso first attempt to explore a new generalized open set recognition (G-OSR) by\nintroducing the accumulated side-information from known classes to OSR. For\nG-ZSL, such a decomposition effectively solves the class overfitting problem\nwith easily misclassifying unseen classes as seen classes. The problem is\nubiquitous in most existing G-ZSL methods. On the other hand, for G-OSR,\nintroducing such semantic information of known classes not only improves the\nrecognition performance but also endows OSR with the cognitive ability of\nunknown classes. Specifically, a visual and semantic prototypes-jointly guided\nconvolutional neural network (VSG-CNN) is proposed to fulfill these two tasks\n(G-ZSL and G-OSR) in a unified end-to-end learning framework. Extensive\nexperiments on benchmark datasets demonstrate the advantages of our learning\nframework.\n", "versions": [{"version": "v1", "created": "Mon, 12 Aug 2019 02:29:16 GMT"}, {"version": "v2", "created": "Wed, 14 Aug 2019 12:58:23 GMT"}], "update_date": "2019-08-15", "authors_parsed": [["Geng", "Chuanxing", ""], ["Tao", "Lue", ""], ["Chen", "Songcan", ""]]}, {"id": "1908.04008", "submitter": "Senwei Liang", "authors": "Senwei Liang, Zhongzhan Huang, Mingfu Liang, Haizhao Yang", "title": "Instance Enhancement Batch Normalization: an Adaptive Regulator of Batch\n  Noise", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Batch Normalization (BN)(Ioffe and Szegedy 2015) normalizes the features of\nan input image via statistics of a batch of images and hence BN will bring the\nnoise to the gradient of the training loss. Previous works indicate that the\nnoise is important for the optimization and generalization of deep neural\nnetworks, but too much noise will harm the performance of networks. In our\npaper, we offer a new point of view that self-attention mechanism can help to\nregulate the noise by enhancing instance-specific information to obtain a\nbetter regularization effect. Therefore, we propose an attention-based BN\ncalled Instance Enhancement Batch Normalization (IEBN) that recalibrates the\ninformation of each channel by a simple linear transformation. IEBN has a good\ncapacity of regulating noise and stabilizing network training to improve\ngeneralization even in the presence of two kinds of noise attacks during\ntraining. Finally, IEBN outperforms BN with only a light parameter increment in\nimage classification tasks for different network structures and benchmark\ndatasets.\n", "versions": [{"version": "v1", "created": "Mon, 12 Aug 2019 05:42:09 GMT"}, {"version": "v2", "created": "Wed, 18 Sep 2019 02:52:32 GMT"}], "update_date": "2019-09-19", "authors_parsed": [["Liang", "Senwei", ""], ["Huang", "Zhongzhan", ""], ["Liang", "Mingfu", ""], ["Yang", "Haizhao", ""]]}, {"id": "1908.04011", "submitter": "Tan Wang", "authors": "Tan Wang, Xing Xu, Yang Yang, Alan Hanjalic, Heng Tao Shen, Jingkuan\n  Song", "title": "Matching Images and Text with Multi-modal Tensor Fusion and Re-ranking", "comments": "ACM Multimedia 2019 Oral", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A major challenge in matching images and text is that they have intrinsically\ndifferent data distributions and feature representations. Most existing\napproaches are based either on embedding or classification, the first one\nmapping image and text instances into a common embedding space for distance\nmeasuring, and the second one regarding image-text matching as a binary\nclassification problem. Neither of these approaches can, however, balance the\nmatching accuracy and model complexity well. We propose a novel framework that\nachieves remarkable matching performance with acceptable model complexity.\nSpecifically, in the training stage, we propose a novel Multi-modal Tensor\nFusion Network (MTFN) to explicitly learn an accurate image-text similarity\nfunction with rank-based tensor fusion rather than seeking a common embedding\nspace for each image-text instance. Then, during testing, we deploy a generic\nCross-modal Re-ranking (RR) scheme for refinement without requiring additional\ntraining procedure. Extensive experiments on two datasets demonstrate that our\nMTFN-RR consistently achieves the state-of-the-art matching performance with\nmuch less time complexity. The implementation code is available at\nhttps://github.com/Wangt-CN/MTFN-RR-PyTorch-Code.\n", "versions": [{"version": "v1", "created": "Mon, 12 Aug 2019 05:52:44 GMT"}, {"version": "v2", "created": "Wed, 29 Jul 2020 14:11:32 GMT"}], "update_date": "2020-07-30", "authors_parsed": [["Wang", "Tan", ""], ["Xu", "Xing", ""], ["Yang", "Yang", ""], ["Hanjalic", "Alan", ""], ["Shen", "Heng Tao", ""], ["Song", "Jingkuan", ""]]}, {"id": "1908.04013", "submitter": "Kun Cheng", "authors": "Kun Cheng, Hao-Zhi Huang, Chun Yuan, Lingyiqing Zhou, Wei Liu", "title": "Multi-Frame Content Integration with a Spatio-Temporal Attention\n  Mechanism for Person Video Motion Transfer", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing person video generation methods either lack the flexibility in\ncontrolling both the appearance and motion, or fail to preserve detailed\nappearance and temporal consistency. In this paper, we tackle the problem of\nmotion transfer for generating person videos, which provides controls on both\nthe appearance and the motion. Specifically, we transfer the motion of one\nperson in a target video to another person in a source video, while preserving\nthe appearance of the source person. Besides only relying on one source frame\nas the existing state-of-the-art methods, our proposed method integrates\ninformation from multiple source frames based on a spatio-temporal attention\nmechanism to preserve rich appearance details. In addition to a spatial\ndiscriminator employed for encouraging the frame-level fidelity, a multi-range\ntemporal discriminator is adopted to enforce the generated video to resemble\ntemporal dynamics of a real video in various time ranges. A challenging\nreal-world dataset, which contains about 500 dancing video clips with complex\nand unpredictable motions, is collected for the training and testing. Extensive\nexperiments show that the proposed method can produce more photo-realistic and\ntemporally consistent person videos than previous methods. As our method\ndecomposes the syntheses of the foreground and background into two branches, a\nflexible background substitution application can also be achieved.\n", "versions": [{"version": "v1", "created": "Mon, 12 Aug 2019 06:01:20 GMT"}], "update_date": "2019-08-13", "authors_parsed": [["Cheng", "Kun", ""], ["Huang", "Hao-Zhi", ""], ["Yuan", "Chun", ""], ["Zhou", "Lingyiqing", ""], ["Liu", "Wei", ""]]}, {"id": "1908.04014", "submitter": "Vincent Falconieri", "authors": "Vincent Falconieri", "title": "Douglas-Quaid -- Open Source Image Matching Library", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Security analysts need to classify, search and correlate numerous images.\nAutomatic classification tools improve the efficiency of such tasks. However,\nno open-source and turnkey library was found able to reach this goal. The\npresent paper introduces an Open-Source modular library for the specific cases\nof visual correlation and Image Matching named Douglas-Quaid. The design of the\nlibrary, chosen tradeoffs, encountered challenges, envisioned solutions as well\nas quality and speed results are presented in this paper. We also explore\nresearches directions and future potential developments of the library. Our\nclaim is that even partial automation of screenshots classification would\nreduce the burden on security teams and that Douglas-Quaid is a step forward in\nthis direction.\n", "versions": [{"version": "v1", "created": "Mon, 12 Aug 2019 06:02:31 GMT"}], "update_date": "2019-08-13", "authors_parsed": [["Falconieri", "Vincent", ""]]}, {"id": "1908.04015", "submitter": "YoungJoon Yoo", "authors": "YoungJoon Yoo, Sangdoo Yun, Hyung Jin Chang, Yiannis Demiris and Jin\n  Young Choi", "title": "Variational Autoencoded Regression: High Dimensional Regression of\n  Visual Data on Complex Manifold", "comments": "Published in CVPR 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a new high dimensional regression method by merging\nGaussian process regression into a variational autoencoder framework. In\ncontrast to other regression methods, the proposed method focuses on the case\nwhere output responses are on a complex high dimensional manifold, such as\nimages. Our contributions are summarized as follows: (i) A new regression\nmethod estimating high dimensional image responses, which is not handled by\nexisting regression algorithms, is proposed. (ii) The proposed regression\nmethod introduces a strategy to learn the latent space as well as the encoder\nand decoder so that the result of the regressed response in the latent space\ncoincide with the corresponding response in the data space. (iii) The proposed\nregression is embedded into a generative model, and the whole procedure is\ndeveloped by the variational autoencoder framework. We demonstrate the\nrobustness and effectiveness of our method through a number of experiments on\nvarious visual data regression problems.\n", "versions": [{"version": "v1", "created": "Mon, 12 Aug 2019 06:06:59 GMT"}], "update_date": "2019-08-13", "authors_parsed": [["Yoo", "YoungJoon", ""], ["Yun", "Sangdoo", ""], ["Chang", "Hyung Jin", ""], ["Demiris", "Yiannis", ""], ["Choi", "Jin Young", ""]]}, {"id": "1908.04018", "submitter": "Dawei Li", "authors": "Dawei Li, Yan Cao, Guoliang Shi, Xin Cai, Yang Chen, Sifan Wang, and\n  Siyuan Yan", "title": "An overlapping-free leaf segmentation method for plant point clouds", "comments": "24 Pages, 18 Figures, 7 Tables. Intends to submit to an open-access\n  journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic leaf segmentation, as well as identification and classification\nmethods that built upon it, are able to provide immediate monitoring for plant\ngrowth status to guarantee the output. Although 3D plant point clouds contain\nabundant phenotypic features, plant leaves are usually distributed in clusters\nand are sometimes seriously overlapped in the canopy. Therefore, it is still a\nbig challenge to automatically segment each individual leaf from a highly\ncrowded plant canopy in 3D for plant phenotyping purposes. In this work, we\npropose an overlapping-free individual leaf segmentation method for plant point\nclouds using the 3D filtering and facet region growing. In order to separate\nleaves with different overlapping situations, we develop a new 3D joint\nfiltering operator, which integrates a Radius-based Outlier Filter (RBOF) and a\nSurface Boundary Filter (SBF) to help to separate occluded leaves. By\nintroducing the facet over-segmentation and facet-based region growing, the\nnoise in segmentation is suppressed and labeled leaf centers can expand to\ntheir whole leaves, respectively. Our method can work on point clouds generated\nfrom three types of 3D imaging platforms, and also suitable for different kinds\nof plant species. In experiments, it obtains a point-level cover rate of 97%\nfor Epipremnum aureum, 99% for Monstera deliciosa, 99% for Calathea makoyana,\nand 87% for Hedera nepalensis sample plants. At the leaf level, our method\nreaches an average Recall at 100.00%, a Precision at 99.33%, and an average\nF-measure at 99.66%, respectively. The proposed method can also facilitate the\nautomatic traits estimation of each single leaf (such as the leaf area, length,\nand width), which has potential to become a highly effective tool for plant\nresearch and agricultural engineering.\n", "versions": [{"version": "v1", "created": "Mon, 12 Aug 2019 06:18:00 GMT"}], "update_date": "2019-08-13", "authors_parsed": [["Li", "Dawei", ""], ["Cao", "Yan", ""], ["Shi", "Guoliang", ""], ["Cai", "Xin", ""], ["Chen", "Yang", ""], ["Wang", "Sifan", ""], ["Yan", "Siyuan", ""]]}, {"id": "1908.04027", "submitter": "Oliver Mothes O.M.", "authors": "Oliver Mothes and Joachim Denzler", "title": "Self-supervised Data Bootstrapping for Deep Optical Character\n  Recognition of Identity Documents", "comments": "4 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The essential task of verifying person identities at airports and national\nborders is very time consuming. To accelerate it, optical character recognition\nfor identity documents (IDs) using dictionaries is not appropriate due to high\nvariability of the text content in IDs, e.g., individual street names or\nsurnames. Additionally, no properties of the used fonts in IDs are known.\nTherefore, we propose an iterative self-supervised bootstrapping approach using\na smart strategy to mine real character data from IDs. In combination with\nsynthetically generated character data, the real data is used to train\nefficient convolutional neural networks for character classification serving a\npractical runtime as well as a high accuracy. On a dataset with 74 character\nclasses, we achieve an average class-wise accuracy of 99.4 %. In contrast, if\nwe would apply a classifier trained only using synthetic data, the accuracy is\nreduced to 58.1 %. Finally, we show that our whole proposed pipeline\noutperforms an established open-source framework\n", "versions": [{"version": "v1", "created": "Mon, 12 Aug 2019 07:02:24 GMT"}], "update_date": "2019-08-13", "authors_parsed": [["Mothes", "Oliver", ""], ["Denzler", "Joachim", ""]]}, {"id": "1908.04034", "submitter": "Joakim Bruslund Haurum", "authors": "Joakim Bruslund Haurum and Chris H. Bahnsen and Thomas B. Moeslund", "title": "Is it Raining Outside? Detection of Rainfall using General-Purpose\n  Surveillance Cameras", "comments": "10 pages, 7 figures, CVPR2019 V4AS workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In integrated surveillance systems based on visual cameras, the mitigation of\nadverse weather conditions is an active research topic. Within this field, rain\nremoval algorithms have been developed that artificially remove rain streaks\nfrom images or video. In order to deploy such rain removal algorithms in a\nsurveillance setting, one must detect if rain is present in the scene. In this\npaper, we design a system for the detection of rainfall by the use of\nsurveillance cameras. We reimplement the former state-of-the-art method for\nrain detection and compare it against a modern CNN-based method by utilizing 3D\nconvolutions. The two methods are evaluated on our new AAU Visual Rain Dataset\n(VIRADA) that consists of 215 hours of general-purpose surveillance video from\ntwo traffic crossings. The results show that the proposed 3D CNN outperforms\nthe previous state-of-the-art method by a large margin on all metrics, for both\nof the traffic crossings. Finally, it is shown that the choice of\nregion-of-interest has a large influence on performance when trying to\ngeneralize the investigated methods. The AAU VIRADA dataset and our\nimplementation of the two rain detection algorithms are publicly available at\nhttps://bitbucket.org/aauvap/aau-virada.\n", "versions": [{"version": "v1", "created": "Mon, 12 Aug 2019 07:32:25 GMT"}], "update_date": "2019-08-13", "authors_parsed": [["Haurum", "Joakim Bruslund", ""], ["Bahnsen", "Chris H.", ""], ["Moeslund", "Thomas B.", ""]]}, {"id": "1908.04051", "submitter": "Pengxiang Yan", "authors": "Pengxiang Yan, Guanbin Li, Yuan Xie, Zhen Li, Chuan Wang, Tianshui\n  Chen, Liang Lin", "title": "Semi-Supervised Video Salient Object Detection Using Pseudo-Labels", "comments": "ICCV2019, code is available at\n  https://github.com/Kinpzz/RCRNet-Pytorch", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning-based video salient object detection has recently achieved\ngreat success with its performance significantly outperforming any other\nunsupervised methods. However, existing data-driven approaches heavily rely on\na large quantity of pixel-wise annotated video frames to deliver such promising\nresults. In this paper, we address the semi-supervised video salient object\ndetection task using pseudo-labels. Specifically, we present an effective video\nsaliency detector that consists of a spatial refinement network and a\nspatiotemporal module. Based on the same refinement network and motion\ninformation in terms of optical flow, we further propose a novel method for\ngenerating pixel-level pseudo-labels from sparsely annotated frames. By\nutilizing the generated pseudo-labels together with a part of manual\nannotations, our video saliency detector learns spatial and temporal cues for\nboth contrast inference and coherence enhancement, thus producing accurate\nsaliency maps. Experimental results demonstrate that our proposed\nsemi-supervised method even greatly outperforms all the state-of-the-art fully\nsupervised methods across three public benchmarks of VOS, DAVIS, and FBMS.\n", "versions": [{"version": "v1", "created": "Mon, 12 Aug 2019 08:32:48 GMT"}, {"version": "v2", "created": "Fri, 29 Nov 2019 02:03:15 GMT"}], "update_date": "2019-12-02", "authors_parsed": [["Yan", "Pengxiang", ""], ["Li", "Guanbin", ""], ["Xie", "Yuan", ""], ["Li", "Zhen", ""], ["Wang", "Chuan", ""], ["Chen", "Tianshui", ""], ["Lin", "Liang", ""]]}, {"id": "1908.04052", "submitter": "Yitian Yuan", "authors": "Yitian Yuan, Lin Ma, Wenwu Zhu", "title": "Sentence Specified Dynamic Video Thumbnail Generation", "comments": null, "journal-ref": "ACM Multimedia 2019", "doi": "10.1145/3343031.3350985", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the tremendous growth of videos over the Internet, video thumbnails,\nproviding video content previews, are becoming increasingly crucial to\ninfluencing users' online searching experiences. Conventional video thumbnails\nare generated once purely based on the visual characteristics of videos, and\nthen displayed as requested. Hence, such video thumbnails, without considering\nthe users' searching intentions, cannot provide a meaningful snapshot of the\nvideo contents that users concern. In this paper, we define a distinctively new\ntask, namely sentence specified dynamic video thumbnail generation, where the\ngenerated thumbnails not only provide a concise preview of the original video\ncontents but also dynamically relate to the users' searching intentions with\nsemantic correspondences to the users' query sentences. To tackle such a\nchallenging task, we propose a novel graph convolved video thumbnail pointer\n(GTP). Specifically, GTP leverages a sentence specified video graph\nconvolutional network to model both the sentence-video semantic interaction and\nthe internal video relationships incorporated with the sentence information,\nbased on which a temporal conditioned pointer network is then introduced to\nsequentially generate the sentence specified video thumbnails. Moreover, we\nannotate a new dataset based on ActivityNet Captions for the proposed new task,\nwhich consists of 10,000+ video-sentence pairs with each accompanied by an\nannotated sentence specified video thumbnail. We demonstrate that our proposed\nGTP outperforms several baseline methods on the created dataset, and thus\nbelieve that our initial results along with the release of the new dataset will\ninspire further research on sentence specified dynamic video thumbnail\ngeneration. Dataset and code are available at https://github.com/yytzsy/GTP.\n", "versions": [{"version": "v1", "created": "Mon, 12 Aug 2019 08:35:37 GMT"}, {"version": "v2", "created": "Wed, 16 Oct 2019 09:47:43 GMT"}], "update_date": "2019-10-17", "authors_parsed": [["Yuan", "Yitian", ""], ["Ma", "Lin", ""], ["Zhu", "Wenwu", ""]]}, {"id": "1908.04067", "submitter": "Wenqiang Xu", "authors": "Wenqiang Xu, Haiyang Wang, Fubo Qi, Cewu Lu", "title": "Explicit Shape Encoding for Real-Time Instance Segmentation", "comments": "to appear in ICCV2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a novel top-down instance segmentation framework\nbased on explicit shape encoding, named \\textbf{ESE-Seg}. It largely reduces\nthe computational consumption of the instance segmentation by explicitly\ndecoding the multiple object shapes with tensor operations, thus performs the\ninstance segmentation at almost the same speed as the object detection. ESE-Seg\nis based on a novel shape signature Inner-center Radius (IR), Chebyshev\npolynomial fitting and the strong modern object detectors. ESE-Seg with YOLOv3\noutperforms the Mask R-CNN on Pascal VOC 2012 at mAP$^r$@0.5 while 7 times\nfaster.\n", "versions": [{"version": "v1", "created": "Mon, 12 Aug 2019 09:47:03 GMT"}], "update_date": "2019-08-13", "authors_parsed": [["Xu", "Wenqiang", ""], ["Wang", "Haiyang", ""], ["Qi", "Fubo", ""], ["Lu", "Cewu", ""]]}, {"id": "1908.04107", "submitter": "Zhou Yu", "authors": "Zhou Yu, Yuhao Cui, Jun Yu, Dacheng Tao, Qi Tian", "title": "Multimodal Unified Attention Networks for Vision-and-Language\n  Interactions", "comments": "11 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning an effective attention mechanism for multimodal data is important in\nmany vision-and-language tasks that require a synergic understanding of both\nthe visual and textual contents. Existing state-of-the-art approaches use\nco-attention models to associate each visual object (e.g., image region) with\neach textual object (e.g., query word). Despite the success of these\nco-attention models, they only model inter-modal interactions while neglecting\nintra-modal interactions. Here we propose a general `unified attention' model\nthat simultaneously captures the intra- and inter-modal interactions of\nmultimodal features and outputs their corresponding attended representations.\nBy stacking such unified attention blocks in depth, we obtain the deep\nMultimodal Unified Attention Network (MUAN), which can seamlessly be applied to\nthe visual question answering (VQA) and visual grounding tasks. We evaluate our\nMUAN models on two VQA datasets and three visual grounding datasets, and the\nresults show that MUAN achieves top-level performance on both tasks without\nbells and whistles.\n", "versions": [{"version": "v1", "created": "Mon, 12 Aug 2019 12:12:17 GMT"}, {"version": "v2", "created": "Mon, 19 Aug 2019 06:14:06 GMT"}], "update_date": "2019-08-20", "authors_parsed": [["Yu", "Zhou", ""], ["Cui", "Yuhao", ""], ["Yu", "Jun", ""], ["Tao", "Dacheng", ""], ["Tian", "Qi", ""]]}, {"id": "1908.04121", "submitter": "Zhikang Zou", "authors": "Zhikang Zou, Huiliang Shao, Xiaoye Qu, Wei Wei, Pan Zhou", "title": "Enhanced 3D convolutional networks for crowd counting", "comments": "Accepted to BMVC 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, convolutional neural networks (CNNs) are the leading defacto method\nfor crowd counting. However, when dealing with video datasets, CNN-based\nmethods still process each video frame independently, thus ignoring the\npowerful temporal information between consecutive frames. In this work, we\npropose a novel architecture termed as \"temporal channel-aware\" (TCA) block,\nwhich achieves the capability of exploiting the temporal interdependencies\namong video sequences. Specifically, we incorporate 3D convolution kernels to\nencode local spatio-temporal features. Furthermore, the global contextual\ninformation is encoded into modulation weights which adaptively recalibrate\nchannel-aware feature responses. With the local and global context combined,\nthe proposed block enhances the discriminative ability of the feature\nrepresentations and contributes to more precise results in diverse scenes. By\nstacking TCA blocks together, we obtain the deep trainable architecture called\nenhanced 3D convolutional networks (E3D). The experiments on three benchmark\ndatasets show that the proposed method delivers state-of-the-art performance.\nTo verify the generality, an extended experiment is conducted on a vehicle\ndataset TRANCOS and our approach beats previous methods by large margins.\n", "versions": [{"version": "v1", "created": "Mon, 12 Aug 2019 12:39:28 GMT"}], "update_date": "2019-08-13", "authors_parsed": [["Zou", "Zhikang", ""], ["Shao", "Huiliang", ""], ["Qu", "Xiaoye", ""], ["Wei", "Wei", ""], ["Zhou", "Pan", ""]]}, {"id": "1908.04123", "submitter": "Kundan Kumar", "authors": "Kundan Kumar and Debashisa Samal and Suraj", "title": "Automated retinal vessel segmentation based on morphological\n  preprocessing and 2D-Gabor wavelets", "comments": "12 pages, 5 figures, conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Automated segmentation of vascular map in retinal images endeavors a\npotential benefit in diagnostic procedure of different ocular diseases. In this\npaper, we suggest a new unsupervised retinal blood vessel segmentation approach\nusing top-hat transformation, contrast-limited adaptive histogram equalization\n(CLAHE), and 2-D Gabor wavelet filters. Initially, retinal image is\npreprocessed using top-hat morphological transformation followed by CLAHE to\nenhance only the blood vessel pixels in the presence of exudates, optic disc,\nand fovea. Then, multiscale 2-D Gabor wavelet filters are applied on\npreprocessed image for better representation of thick and thin blood vessels\nlocated at different orientations. The efficacy of the presented algorithm is\nassessed on publicly available DRIVE database with manually labeled images. On\nDRIVE database, we achieve an average accuracy of 94.32% with a small standard\ndeviation of 0.004. In comparison with major algorithms, our algorithm produces\nbetter performance concerning the accuracy, sensitivity, and kappa agreement.\n", "versions": [{"version": "v1", "created": "Mon, 12 Aug 2019 12:44:05 GMT"}], "update_date": "2019-08-13", "authors_parsed": [["Kumar", "Kundan", ""], ["Samal", "Debashisa", ""], ["Suraj", "", ""]]}, {"id": "1908.04126", "submitter": "Egor Panfilov", "authors": "Egor Panfilov, Aleksei Tiulpin, Stefan Klein, Miika T. Nieminen and\n  Simo Saarakkala", "title": "Improving Robustness of Deep Learning Based Knee MRI Segmentation: Mixup\n  and Adversarial Domain Adaptation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Degeneration of articular cartilage (AC) is actively studied in knee\nosteoarthritis (OA) research via magnetic resonance imaging (MRI). Segmentation\nof AC tissues from MRI data is an essential step in quantification of their\ndamage. Deep learning (DL) based methods have shown potential in this realm and\nare the current state-of-the-art, however, their robustness to heterogeneity of\nMRI acquisition settings remains an open problem. In this study, we\ninvestigated two modern regularization techniques -- mixup and adversarial\nunsupervised domain adaptation (UDA) -- to improve the robustness of DL-based\nknee cartilage segmentation to new MRI acquisition settings. Our validation\nsetup included two datasets produced by different MRI scanners and using\ndistinct data acquisition protocols. We assessed the robustness of automatic\nsegmentation by comparing mixup and UDA approaches to a strong baseline method\nat different OA severity stages and, additionally, in relation to anatomical\nlocations. Our results showed that for moderate changes in knee MRI data\nacquisition settings both approaches may provide notable improvements in the\nrobustness, which are consistent for all stages of the disease and affect the\nclinically important areas of the knee joint. However, mixup may be considered\nas a recommended approach, since it is more computationally efficient and does\nnot require additional data from the target acquisition setup.\n", "versions": [{"version": "v1", "created": "Mon, 12 Aug 2019 12:50:59 GMT"}, {"version": "v2", "created": "Wed, 14 Aug 2019 09:23:16 GMT"}, {"version": "v3", "created": "Sun, 27 Oct 2019 16:20:46 GMT"}], "update_date": "2019-10-29", "authors_parsed": [["Panfilov", "Egor", ""], ["Tiulpin", "Aleksei", ""], ["Klein", "Stefan", ""], ["Nieminen", "Miika T.", ""], ["Saarakkala", "Simo", ""]]}, {"id": "1908.04130", "submitter": "Roberto Annunziata", "authors": "Roberto Annunziata, Christos Sagonas, Jacques Cali", "title": "Jointly Aligning Millions of Images with Deep Penalised Reconstruction\n  Congealing", "comments": "International Conference on Computer Vision 2019 (ICCV 2019), Seoul,\n  Korea", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Extrapolating fine-grained pixel-level correspondences in a fully\nunsupervised manner from a large set of misaligned images can benefit several\ncomputer vision and graphics problems, e.g. co-segmentation, super-resolution,\nimage edit propagation, structure-from-motion, and 3D reconstruction. Several\njoint image alignment and congealing techniques have been proposed to tackle\nthis problem, but robustness to initialisation, ability to scale to large\ndatasets, and alignment accuracy seem to hamper their wide applicability. To\novercome these limitations, we propose an unsupervised joint alignment method\nleveraging a densely fused spatial transformer network to estimate the warping\nparameters for each image and a low-capacity auto-encoder whose reconstruction\nerror is used as an auxiliary measure of joint alignment. Experimental results\non digits from multiple versions of MNIST (i.e., original, perturbed, affNIST\nand infiMNIST) and faces from LFW, show that our approach is capable of\naligning millions of images with high accuracy and robustness to different\nlevels and types of perturbation. Moreover, qualitative and quantitative\nresults suggest that the proposed method outperforms state-of-the-art\napproaches both in terms of alignment quality and robustness to initialisation.\n", "versions": [{"version": "v1", "created": "Mon, 12 Aug 2019 12:55:31 GMT"}, {"version": "v2", "created": "Mon, 14 Oct 2019 10:24:31 GMT"}], "update_date": "2019-10-15", "authors_parsed": [["Annunziata", "Roberto", ""], ["Sagonas", "Christos", ""], ["Cali", "Jacques", ""]]}, {"id": "1908.04156", "submitter": "Ziteng Gao", "authors": "Ziteng Gao, Limin Wang, Gangshan Wu", "title": "LIP: Local Importance-based Pooling", "comments": "ICCV 2019 camera ready version. Code: https://github.com/sebgao/LIP", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spatial downsampling layers are favored in convolutional neural networks\n(CNNs) to downscale feature maps for larger receptive fields and less memory\nconsumption. However, for discriminative tasks, there is a possibility that\nthese layers lose the discriminative details due to improper pooling\nstrategies, which could hinder the learning process and eventually result in\nsuboptimal models. In this paper, we present a unified framework over the\nexisting downsampling layers (e.g., average pooling, max pooling, and strided\nconvolution) from a local importance view. In this framework, we analyze the\nissues of these widely-used pooling layers and figure out the criteria for\ndesigning an effective downsampling layer. According to this analysis, we\npropose a conceptually simple, general, and effective pooling layer based on\nlocal importance modeling, termed as {\\em Local Importance-based Pooling}\n(LIP). LIP can automatically enhance discriminative features during the\ndownsampling procedure by learning adaptive importance weights based on inputs.\nExperiment results show that LIP consistently yields notable gains with\ndifferent depths and different architectures on ImageNet classification. In the\nchallenging MS COCO dataset, detectors with our LIP-ResNets as backbones obtain\na consistent improvement ($\\ge 1.4\\%$) over the vanilla ResNets, and especially\nachieve the current state-of-the-art performance in detecting small objects\nunder the single-scale testing scheme.\n", "versions": [{"version": "v1", "created": "Mon, 12 Aug 2019 14:02:53 GMT"}, {"version": "v2", "created": "Sun, 18 Aug 2019 08:01:45 GMT"}, {"version": "v3", "created": "Tue, 27 Aug 2019 08:09:44 GMT"}], "update_date": "2019-08-28", "authors_parsed": [["Gao", "Ziteng", ""], ["Wang", "Limin", ""], ["Wu", "Gangshan", ""]]}, {"id": "1908.04168", "submitter": "Natasha Westland", "authors": "Natasha Westland, Andr\\'e Seixas Dias, Marta Mrak", "title": "Decision Trees for Complexity Reduction in Video Compression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a method for complexity reduction in practical video\nencoders using multiple decision tree classifiers. The method is demonstrated\nfor the fast implementation of the 'High Efficiency Video Coding' (HEVC)\nstandard, chosen because of its high bit rate reduction capability but large\ncomplexity overhead. Optimal partitioning of each video frame into coding units\n(CUs) is the main source of complexity as a vast number of combinations are\ntested. The decision tree models were trained to identify when the CU testing\nprocess, a time-consuming Lagrangian optimisation, can be skipped i.e a high\nprobability that the CU can remain whole. A novel approach to finding the\nsimplest and most effective decision tree model called 'manual pruning' is\ndescribed. Implementing the skip criteria reduced the average encoding time by\n42.1% for a Bj{\\o}ntegaard Delta rate detriment of 0.7%, for 17 standard test\nsequences in a range of resolutions and quantisation parameters.\n", "versions": [{"version": "v1", "created": "Mon, 12 Aug 2019 14:27:16 GMT"}], "update_date": "2019-08-13", "authors_parsed": [["Westland", "Natasha", ""], ["Dias", "Andr\u00e9 Seixas", ""], ["Mrak", "Marta", ""]]}, {"id": "1908.04174", "submitter": "Shaobo Min", "authors": "Shaobo Min, Hantao Yao, Hongtao Xie, Zheng-Jun Zha, Yongdong Zhang", "title": "Domain-Specific Embedding Network for Zero-Shot Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Zero-Shot Learning (ZSL) seeks to recognize a sample from either seen or\nunseen domain by projecting the image data and semantic labels into a joint\nembedding space. However, most existing methods directly adapt a well-trained\nprojection from one domain to another, thereby ignoring the serious bias\nproblem caused by domain differences. To address this issue, we propose a novel\nDomain-Specific Embedding Network (DSEN) that can apply specific projections to\ndifferent domains for unbiased embedding, as well as several domain\nconstraints. In contrast to previous methods, the DSEN decomposes the\ndomain-shared projection function into one domain-invariant and two\ndomain-specific sub-functions to explore the similarities and differences\nbetween two domains. To prevent the two specific projections from breaking the\nsemantic relationship, a semantic reconstruction constraint is proposed by\napplying the same decoder function to them in a cycle consistency way.\nFurthermore, a domain division constraint is developed to directly penalize the\nmargin between real and pseudo image features in respective seen and unseen\ndomains, which can enlarge the inter-domain difference of visual features.\nExtensive experiments on four public benchmarks demonstrate the effectiveness\nof DSEN with an average of $9.2\\%$ improvement in terms of harmonic mean. The\ncode is available in \\url{https://github.com/mboboGO/DSEN-for-GZSL}.\n", "versions": [{"version": "v1", "created": "Mon, 12 Aug 2019 14:32:50 GMT"}], "update_date": "2019-08-13", "authors_parsed": [["Min", "Shaobo", ""], ["Yao", "Hantao", ""], ["Xie", "Hongtao", ""], ["Zha", "Zheng-Jun", ""], ["Zhang", "Yongdong", ""]]}, {"id": "1908.04181", "submitter": "Nils Gessert", "authors": "Nils Gessert and Alexander Schlaefer", "title": "Left Ventricle Quantification Using Direct Regression with Segmentation\n  Regularization and Ensembles of Pretrained 2D and 3D CNNs", "comments": "Accepted at the MICCAI Workshop STACOM 2019", "journal-ref": null, "doi": "10.1007/978-3-030-39074-7_39", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cardiac left ventricle (LV) quantification provides a tool for diagnosing\ncardiac diseases. Automatic calculation of all relevant LV indices from cardiac\nMR images is an intricate task due to large variations among patients and\ndeformation during the cardiac cycle. Typical methods are based on segmentation\nof the myocardium or direct regression from MR images. To consider cardiac\nmotion and deformation, recurrent neural networks and spatio-temporal\nconvolutional neural networks (CNNs) have been proposed. We study an approach\ncombining state-of-the-art models and emphasizing transfer learning to account\nfor the small dataset provided for the LVQuan19 challenge. We compare 2D\nspatial and 3D spatio-temporal CNNs for LV indices regression and cardiac phase\nclassification. To incorporate segmentation information, we propose an\narchitecture-independent segmentation-based regularization. To improve the\nrobustness further, we employ a search scheme that identifies the optimal\nensemble from a set of architecture variants. Evaluating on the LVQuan19\nChallenge training dataset with 5-fold cross-validation, we achieve mean\nabsolute errors of 111 +- 76mm^2, 1.84 +- 0.9mm and 1.22 +- 0.6mm for area,\ndimension and regional wall thickness regression, respectively. The error rate\nfor cardiac phase classification is 6.7%.\n", "versions": [{"version": "v1", "created": "Mon, 12 Aug 2019 14:46:00 GMT"}], "update_date": "2020-02-04", "authors_parsed": [["Gessert", "Nils", ""], ["Schlaefer", "Alexander", ""]]}, {"id": "1908.04186", "submitter": "Nils Gessert", "authors": "Nils Gessert, Martin Gromniak, Marcel Bengs, Lars Matth\\\"aus,\n  Alexander Schlaefer", "title": "Towards Deep Learning-Based EEG Electrode Detection Using Automatically\n  Generated Labels", "comments": "Accepted at the CURAC 2019 Conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Electroencephalography (EEG) allows for source measurement of electrical\nbrain activity. Particularly for inverse localization, the electrode positions\non the scalp need to be known. Often, systems such as optical digitizing\nscanners are used for accurate localization with a stylus. However, the\napproach is time-consuming as each electrode needs to be scanned manually and\nthe scanning systems are expensive. We propose using an RGBD camera to directly\ntrack electrodes in the images using deep learning methods. Studying and\nevaluating deep learning methods requires large amounts of labeled data. To\novercome the time-consuming data annotation, we generate a large number of\nground-truth labels using a robotic setup. We demonstrate that deep\nlearning-based electrode detection is feasible with a mean absolute error of\n5.69 +- 6.1mm and that our annotation scheme provides a useful environment for\nstudying deep learning methods for electrode detection.\n", "versions": [{"version": "v1", "created": "Mon, 12 Aug 2019 15:03:55 GMT"}], "update_date": "2019-08-13", "authors_parsed": [["Gessert", "Nils", ""], ["Gromniak", "Martin", ""], ["Bengs", "Marcel", ""], ["Matth\u00e4us", "Lars", ""], ["Schlaefer", "Alexander", ""]]}, {"id": "1908.04187", "submitter": "Yash Patel", "authors": "Yash Patel, Srikar Appalaraju, R. Manmatha", "title": "Human Perceptual Evaluations for Image Compression", "comments": "arXiv admin note: text overlap with arXiv:1907.08310", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, there has been much interest in deep learning techniques to do\nimage compression and there have been claims that several of these produce\nbetter results than engineered compression schemes (such as JPEG, JPEG2000 or\nBPG). A standard way of comparing image compression schemes today is to use\nperceptual similarity metrics such as PSNR or MS-SSIM (multi-scale structural\nsimilarity). This has led to some deep learning techniques which directly\noptimize for MS-SSIM by choosing it as a loss function. While this leads to a\nhigher MS-SSIM for such techniques, we demonstrate using user studies that the\nresulting improvement may be misleading. Deep learning techniques for image\ncompression with a higher MS-SSIM may actually be perceptually worse than\nengineered compression schemes with a lower MS-SSIM.\n", "versions": [{"version": "v1", "created": "Fri, 9 Aug 2019 16:53:43 GMT"}], "update_date": "2019-08-13", "authors_parsed": [["Patel", "Yash", ""], ["Appalaraju", "Srikar", ""], ["Manmatha", "R.", ""]]}, {"id": "1908.04197", "submitter": "Aakanksha Rana", "authors": "Aakanksha Rana, Praveer Singh, Giuseppe Valenzise, Frederic Dufaux,\n  Nikos Komodakis, Aljosa Smolic", "title": "Deep Tone Mapping Operator for High Dynamic Range Images", "comments": null, "journal-ref": null, "doi": "10.1109/TIP.2019.2936649", "report-no": null, "categories": "eess.IV cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A computationally fast tone mapping operator (TMO) that can quickly adapt to\na wide spectrum of high dynamic range (HDR) content is quintessential for\nvisualization on varied low dynamic range (LDR) output devices such as movie\nscreens or standard displays. Existing TMOs can successfully tone-map only a\nlimited number of HDR content and require an extensive parameter tuning to\nyield the best subjective-quality tone-mapped output. In this paper, we address\nthis problem by proposing a fast, parameter-free and scene-adaptable deep tone\nmapping operator (DeepTMO) that yields a high-resolution and high-subjective\nquality tone mapped output. Based on conditional generative adversarial network\n(cGAN), DeepTMO not only learns to adapt to vast scenic-content (e.g., outdoor,\nindoor, human, structures, etc.) but also tackles the HDR related\nscene-specific challenges such as contrast and brightness, while preserving the\nfine-grained details. We explore 4 possible combinations of\nGenerator-Discriminator architectural designs to specifically address some\nprominent issues in HDR related deep-learning frameworks like blurring, tiling\npatterns and saturation artifacts. By exploring different influences of scales,\nloss-functions and normalization layers under a cGAN setting, we conclude with\nadopting a multi-scale model for our task. To further leverage on the\nlarge-scale availability of unlabeled HDR data, we train our network by\ngenerating targets using an objective HDR quality metric, namely Tone Mapping\nImage Quality Index (TMQI). We demonstrate results both quantitatively and\nqualitatively, and showcase that our DeepTMO generates high-resolution,\nhigh-quality output images over a large spectrum of real-world scenes. Finally,\nwe evaluate the perceived quality of our results by conducting a pair-wise\nsubjective study which confirms the versatility of our method.\n", "versions": [{"version": "v1", "created": "Mon, 12 Aug 2019 15:30:55 GMT"}], "update_date": "2019-08-20", "authors_parsed": [["Rana", "Aakanksha", ""], ["Singh", "Praveer", ""], ["Valenzise", "Giuseppe", ""], ["Dufaux", "Frederic", ""], ["Komodakis", "Nikos", ""], ["Smolic", "Aljosa", ""]]}, {"id": "1908.04250", "submitter": "Indrajit Mazumdar", "authors": "Indrajit Mazumdar", "title": "Automated Brain Tumour Segmentation Using Deep Fully Residual\n  Convolutional Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automated brain tumour segmentation has the potential of making a massive\nimprovement in disease diagnosis, surgery, monitoring and surveillance.\nHowever, this task is extremely challenging. Here, we describe our automated\nsegmentation method using 2D CNNs that are based on U-Net. To deal with class\nimbalance effectively, we have used a weighted Dice loss function. We found\nthat increasing the depth of the 'U' shape beyond a certain level results in a\ndecrease in performance, so it is essential to choose an optimum depth. We also\nfound that 3D contextual information cannot be captured by a single 2D network\nthat is trained with patches extracted from multiple views whereas an ensemble\nof three 2D networks trained in multiple views can effectively capture the\ninformation and deliver much better performance. We obtained Dice scores of\n0.79 for enhancing tumour, 0.90 for whole tumour, and 0.82 for tumour core on\nthe BraTS 2018 validation set. Our method using 2D network consumes very less\ntime and memory, and is much simpler and easier to implement compared to the\nstate-of-the-art methods that used 3D networks; still, it manages to achieve\ncomparable performance to those methods.\n", "versions": [{"version": "v1", "created": "Mon, 12 Aug 2019 16:58:50 GMT"}, {"version": "v2", "created": "Mon, 23 Sep 2019 17:37:32 GMT"}, {"version": "v3", "created": "Tue, 12 May 2020 12:05:50 GMT"}], "update_date": "2020-05-13", "authors_parsed": [["Mazumdar", "Indrajit", ""]]}, {"id": "1908.04289", "submitter": "Gao Peng", "authors": "Peng Gao, Haoxuan You, Zhanpeng Zhang, Xiaogang Wang and Hongsheng Li", "title": "Multi-modality Latent Interaction Network for Visual Question Answering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.SD eess.AS eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Exploiting relationships between visual regions and question words have\nachieved great success in learning multi-modality features for Visual Question\nAnswering (VQA). However, we argue that existing methods mostly model relations\nbetween individual visual regions and words, which are not enough to correctly\nanswer the question. From humans' perspective, answering a visual question\nrequires understanding the summarizations of visual and language information.\nIn this paper, we proposed the Multi-modality Latent Interaction module (MLI)\nto tackle this problem. The proposed module learns the cross-modality\nrelationships between latent visual and language summarizations, which\nsummarize visual regions and question into a small number of latent\nrepresentations to avoid modeling uninformative individual region-word\nrelations. The cross-modality information between the latent summarizations are\npropagated to fuse valuable information from both modalities and are used to\nupdate the visual and word features. Such MLI modules can be stacked for\nseveral stages to model complex and latent relations between the two modalities\nand achieves highly competitive performance on public VQA benchmarks, VQA v2.0\nand TDIUC . In addition, we show that the performance of our methods could be\nsignificantly improved by combining with pre-trained language model BERT.\n", "versions": [{"version": "v1", "created": "Sat, 10 Aug 2019 05:57:01 GMT"}], "update_date": "2019-08-14", "authors_parsed": [["Gao", "Peng", ""], ["You", "Haoxuan", ""], ["Zhang", "Zhanpeng", ""], ["Wang", "Xiaogang", ""], ["Li", "Hongsheng", ""]]}, {"id": "1908.04293", "submitter": "Umit Rusen Aktas", "authors": "Umit Rusen Aktas, Chao Zhao, Marek Kopicki, Ales Leonardis, Jeremy L.\n  Wyatt", "title": "Deep Dexterous Grasping of Novel Objects from a Single View", "comments": "Submitted for IEEE Transactions on Robotics (T-RO). 14 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dexterous grasping of a novel object given a single view is an open problem.\nThis paper makes several contributions to its solution. First, we present a\nsimulator for generating and testing dexterous grasps. Second we present a data\nset, generated by this simulator, of 2.4 million simulated dexterous grasps of\nvariations of 294 base objects drawn from 20 categories. Third, we present a\nbasic architecture for generation and evaluation of dexterous grasps that may\nbe trained in a supervised manner. Fourth, we present three different\nevaluative architectures, employing ResNet-50 or VGG16 as their visual\nbackbone. Fifth, we train, and evaluate seventeen variants of\ngenerative-evaluative architectures on this simulated data set, showing\nimprovement from 69.53% grasp success rate to 90.49%. Finally, we present a\nreal robot implementation and evaluate the four most promising variants,\nexecuting 196 real robot grasps in total. We show that our best architectural\nvariant achieves a grasp success rate of 87.8% on real novel objects seen from\na single view, improving on a baseline of 57.1%.\n", "versions": [{"version": "v1", "created": "Sat, 10 Aug 2019 18:31:35 GMT"}], "update_date": "2019-08-14", "authors_parsed": [["Aktas", "Umit Rusen", ""], ["Zhao", "Chao", ""], ["Kopicki", "Marek", ""], ["Leonardis", "Ales", ""], ["Wyatt", "Jeremy L.", ""]]}, {"id": "1908.04297", "submitter": "Aakanksha Rana", "authors": "Cagri Ozcinar, Aakanksha Rana, and Aljosa Smolic", "title": "Super-resolution of Omnidirectional Images Using Adversarial Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.MM eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An omnidirectional image (ODI) enables viewers to look in every direction\nfrom a fixed point through a head-mounted display providing an immersive\nexperience compared to that of a standard image. Designing immersive virtual\nreality systems with ODIs is challenging as they require high resolution\ncontent. In this paper, we study super-resolution for ODIs and propose an\nimproved generative adversarial network based model which is optimized to\nhandle the artifacts obtained in the spherical observational space.\nSpecifically, we propose to use a fast PatchGAN discriminator, as it needs\nfewer parameters and improves the super-resolution at a fine scale. We also\nexplore the generative models with adversarial learning by introducing a\nspherical-content specific loss function, called 360-SS. To train and test the\nperformance of our proposed model we prepare a dataset of 4500 ODIs. Our\nresults demonstrate the efficacy of the proposed method and identify new\nchallenges in ODI super-resolution for future investigations.\n", "versions": [{"version": "v1", "created": "Mon, 12 Aug 2019 16:05:59 GMT"}], "update_date": "2019-08-14", "authors_parsed": [["Ozcinar", "Cagri", ""], ["Rana", "Aakanksha", ""], ["Smolic", "Aljosa", ""]]}, {"id": "1908.04321", "submitter": "Royston Rodrigues", "authors": "Royston Rodrigues, Neha Bhargava, Rajbabu Velmurugan, Subhasis\n  Chaudhuri", "title": "Multi-timescale Trajectory Prediction for Abnormal Human Activity\n  Detection", "comments": "Project Page :\n  https://rodrigues-royston.github.io/Multi-timescale_Trajectory_Prediction/ ,\n  This paper is under review in a conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A classical approach to abnormal activity detection is to learn a\nrepresentation for normal activities from the training data and then use this\nlearned representation to detect abnormal activities while testing. Typically,\nthe methods based on this approach operate at a fixed timescale - either a\nsingle time-instant (eg. frame-based) or a constant time duration (eg.\nvideo-clip based). But human abnormal activities can take place at different\ntimescales. For example, jumping is a short term anomaly and loitering is a\nlong term anomaly in a surveillance scenario. A single and pre-defined\ntimescale is not enough to capture the wide range of anomalies occurring with\ndifferent time duration. In this paper, we propose a multi-timescale model to\ncapture the temporal dynamics at different timescales. In particular, the\nproposed model makes future and past predictions at different timescales for a\ngiven input pose trajectory. The model is multi-layered where intermediate\nlayers are responsible to generate predictions corresponding to different\ntimescales. These predictions are combined to detect abnormal activities. In\naddition, we also introduce an abnormal activity data-set for research use that\ncontains 4,83,566 annotated frames. Data-set will be made available at\nhttps://rodrigues-royston.github.io/Multi-timescale_Trajectory_Prediction/ Our\nexperiments show that the proposed model can capture the anomalies of different\ntime duration and outperforms existing methods.\n", "versions": [{"version": "v1", "created": "Mon, 12 Aug 2019 18:12:20 GMT"}], "update_date": "2019-08-14", "authors_parsed": [["Rodrigues", "Royston", ""], ["Bhargava", "Neha", ""], ["Velmurugan", "Rajbabu", ""], ["Chaudhuri", "Subhasis", ""]]}, {"id": "1908.04339", "submitter": "Alejandro Newell", "authors": "Alejandro Newell, Lu Jiang, Chong Wang, Li-Jia Li, Jia Deng", "title": "Feature Partitioning for Efficient Multi-Task Architectures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-task learning holds the promise of less data, parameters, and time than\ntraining of separate models. We propose a method to automatically search over\nmulti-task architectures while taking resource constraints into consideration.\nWe propose a search space that compactly represents different parameter sharing\nstrategies. This provides more effective coverage and sampling of the space of\nmulti-task architectures. We also present a method for quick evaluation of\ndifferent architectures by using feature distillation. Together these\ncontributions allow us to quickly optimize for efficient multi-task models. We\nbenchmark on Visual Decathlon, demonstrating that we can automatically search\nfor and identify multi-task architectures that effectively make trade-offs\nbetween task resource requirements while achieving a high level of final\nperformance.\n", "versions": [{"version": "v1", "created": "Mon, 12 Aug 2019 19:06:32 GMT"}], "update_date": "2019-08-14", "authors_parsed": [["Newell", "Alejandro", ""], ["Jiang", "Lu", ""], ["Wang", "Chong", ""], ["Li", "Li-Jia", ""], ["Deng", "Jia", ""]]}, {"id": "1908.04342", "submitter": "Nilavra Bhattacharya", "authors": "Nilavra Bhattacharya, Qing Li, Danna Gurari", "title": "Why Does a Visual Question Have Different Answers?", "comments": null, "journal-ref": "The IEEE International Conference on Computer Vision (ICCV) 2019", "doi": null, "report-no": null, "categories": "cs.CV cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual question answering is the task of returning the answer to a question\nabout an image. A challenge is that different people often provide different\nanswers to the same visual question. To our knowledge, this is the first work\nthat aims to understand why. We propose a taxonomy of nine plausible reasons,\nand create two labelled datasets consisting of ~45,000 visual questions\nindicating which reasons led to answer differences. We then propose a novel\nproblem of predicting directly from a visual question which reasons will cause\nanswer differences as well as a novel algorithm for this purpose. Experiments\ndemonstrate the advantage of our approach over several related baselines on two\ndiverse datasets. We publicly share the datasets and code at\nhttps://vizwiz.org.\n", "versions": [{"version": "v1", "created": "Mon, 12 Aug 2019 19:19:48 GMT"}, {"version": "v2", "created": "Wed, 14 Aug 2019 18:55:01 GMT"}], "update_date": "2019-08-16", "authors_parsed": [["Bhattacharya", "Nilavra", ""], ["Li", "Qing", ""], ["Gurari", "Danna", ""]]}, {"id": "1908.04344", "submitter": "Sharmin Pathan", "authors": "Sharmin Pathan", "title": "Interior Object Detection and Color Harmonization", "comments": null, "journal-ref": "Frontiers in Artificial Intelligence and Machine Learning 2019", "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Confused about renovating your space? Choosing the perfect color for your\nwalls is always a challenging task. One does rounds of color consultation and\nseveral patch tests. This paper proposes an AI tool to pitch paint based on\nattributes of your room and other furniture, and visualize it on your walls. It\nmakes the color selection process easy. It takes in images of a room, detects\nfurniture objects using YOLO object detection. Once these objects have been\ndetected, the tool picks out color of the object. Later this object specific\ninformation gets appended to the room attributes (room_type, room_size,\npreferred_tone, etc) and a deep neural net is trained to make predictions for\ncolor/texture/wallpaper for the walls. Finally, these predictions are\nvisualized on the walls from the images provided. The idea is to take the\nknowledge of a color consultant and pitch colors that suit the walls and\nprovide a good contrast with the furniture and harmonize with different colors\nin the room. Transfer learning for YOLO object detection from the COCO dataset\nwas used as a starting point and the weights were later fine-tuned by training\non additional images. The model was trained on 1000 records listing the room\nand furniture attributes, to predict colors. Given the room image, this method\nfinds the best color scheme for the walls. These predictions are then\nvisualized on the walls in the image using image segmentation. The results are\nvisually appealing and automatically enhance the color look-and-feel.\n", "versions": [{"version": "v1", "created": "Wed, 31 Jul 2019 21:02:12 GMT"}, {"version": "v2", "created": "Thu, 18 Mar 2021 13:45:07 GMT"}], "update_date": "2021-03-19", "authors_parsed": [["Pathan", "Sharmin", ""]]}, {"id": "1908.04345", "submitter": "Guo-Hua Wang", "authors": "Guo-Hua Wang, Jianxin Wu", "title": "Repetitive Reprediction Deep Decipher for Semi-Supervised Learning", "comments": "Accepted by AAAI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most recent semi-supervised deep learning (deep SSL) methods used a similar\nparadigm: use network predictions to update pseudo-labels and use pseudo-labels\nto update network parameters iteratively. However, they lack theoretical\nsupport and cannot explain why predictions are good candidates for\npseudo-labels. In this paper, we propose a principled end-to-end framework\nnamed deep decipher (D2) for SSL. Within the D2 framework, we prove that\npseudo-labels are related to network predictions by an exponential link\nfunction, which gives a theoretical support for using predictions as\npseudo-labels. Furthermore, we demonstrate that updating pseudo-labels by\nnetwork predictions will make them uncertain. To mitigate this problem, we\npropose a training strategy called repetitive reprediction (R2). Finally, the\nproposed R2-D2 method is tested on the large-scale ImageNet dataset and\noutperforms state-of-the-art methods by 5 percentage points.\n", "versions": [{"version": "v1", "created": "Fri, 9 Aug 2019 11:57:16 GMT"}, {"version": "v2", "created": "Wed, 27 Nov 2019 01:59:50 GMT"}], "update_date": "2019-11-28", "authors_parsed": [["Wang", "Guo-Hua", ""], ["Wu", "Jianxin", ""]]}, {"id": "1908.04346", "submitter": "Huazhu Fu", "authors": "Tianyang Zhang, Huazhu Fu, Yitian Zhao, Jun Cheng, Mengjie Guo,\n  Zaiwang Gu, Bing Yang, Yuting Xiao, Shenghua Gao, Jiang Liu", "title": "SkrGAN: Sketching-rendering Unconditional Generative Adversarial\n  Networks for Medical Image Synthesis", "comments": "Accepted to MICCAI 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative Adversarial Networks (GANs) have the capability of synthesizing\nimages, which have been successfully applied to medical image synthesis tasks.\nHowever, most of existing methods merely consider the global contextual\ninformation and ignore the fine foreground structures, e.g., vessel, skeleton,\nwhich may contain diagnostic indicators for medical image analysis. Inspired by\nhuman painting procedure, which is composed of stroking and color rendering\nsteps, we propose a Sketching-rendering Unconditional Generative Adversarial\nNetwork (SkrGAN) to introduce a sketch prior constraint to guide the medical\nimage generation. In our SkrGAN, a sketch guidance module is utilized to\ngenerate a high quality structural sketch from random noise, then a color\nrender mapping is used to embed the sketch-based representations and resemble\nthe background appearances. Experimental results show that the proposed SkrGAN\nachieves the state-of-the-art results in synthesizing images for various image\nmodalities, including retinal color fundus, X-Ray, Computed Tomography (CT) and\nMagnetic Resonance Imaging (MRI). In addition, we also show that the\nperformances of medical image segmentation method have been improved by using\nour synthesized images as data augmentation.\n", "versions": [{"version": "v1", "created": "Tue, 6 Aug 2019 12:14:56 GMT"}], "update_date": "2019-08-14", "authors_parsed": [["Zhang", "Tianyang", ""], ["Fu", "Huazhu", ""], ["Zhao", "Yitian", ""], ["Cheng", "Jun", ""], ["Guo", "Mengjie", ""], ["Gu", "Zaiwang", ""], ["Yang", "Bing", ""], ["Xiao", "Yuting", ""], ["Gao", "Shenghua", ""], ["Liu", "Jiang", ""]]}, {"id": "1908.04347", "submitter": "Alexander Hepburn", "authors": "Alexander Hepburn, Valero Laparra, Ryan McConville, Raul\n  Santos-Rodriguez", "title": "Enforcing Perceptual Consistency on Generative Adversarial Networks by\n  Using the Normalised Laplacian Pyramid Distance", "comments": null, "journal-ref": "Proceedings of the Northern Lights Deep Learning Workshop. Vol. 1.\n  2020", "doi": "10.7557/18.5124", "report-no": null, "categories": "cs.CV cs.LG eess.IV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years there has been a growing interest in image generation through\ndeep learning. While an important part of the evaluation of the generated\nimages usually involves visual inspection, the inclusion of human perception as\na factor in the training process is often overlooked. In this paper we propose\nan alternative perceptual regulariser for image-to-image translation using\nconditional generative adversarial networks (cGANs). To do so automatically\n(avoiding visual inspection), we use the Normalised Laplacian Pyramid Distance\n(NLPD) to measure the perceptual similarity between the generated image and the\noriginal image. The NLPD is based on the principle of normalising the value of\ncoefficients with respect to a local estimate of mean energy at different\nscales and has already been successfully tested in different experiments\ninvolving human perception. We compare this regulariser with the originally\nproposed L1 distance and note that when using NLPD the generated images contain\nmore realistic values for both local and global contrast. We found that using\nNLPD as a regulariser improves image segmentation accuracy on generated images\nas well as improving two no-reference image quality metrics.\n", "versions": [{"version": "v1", "created": "Fri, 9 Aug 2019 08:33:51 GMT"}, {"version": "v2", "created": "Tue, 17 Nov 2020 10:48:29 GMT"}], "update_date": "2020-11-18", "authors_parsed": [["Hepburn", "Alexander", ""], ["Laparra", "Valero", ""], ["McConville", "Ryan", ""], ["Santos-Rodriguez", "Raul", ""]]}, {"id": "1908.04348", "submitter": "Francesco Ventura", "authors": "Francesco Ventura, Tania Cerquitelli", "title": "What's in the box? Explaining the black-box model through an evaluation\n  of its interpretable features", "comments": "5 pages, 5 images", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Algorithms are powerful and necessary tools behind a large part of the\ninformation we use every day. However, they may introduce new sources of bias,\ndiscrimination and other unfair practices that affect people who are unaware of\nit. Greater algorithm transparency is indispensable to provide more credible\nand reliable services. Moreover, requiring developers to design transparent\nalgorithm-driven applications allows them to keep the model accessible and\nhuman understandable, increasing the trust of end users. In this paper we\npresent EBAnO, a new engine able to produce prediction-local explanations for a\nblack-box model exploiting interpretable feature perturbations. EBAnO exploits\nthe hypercolumns representation together with the cluster analysis to identify\na set of interpretable features of images. Furthermore two indices have been\nproposed to measure the influence of input features on the final prediction\nmade by a CNN model. EBAnO has been preliminarily tested on a set of\nheterogeneous images. The results highlight the effectiveness of EBAnO in\nexplaining the CNN classification through the evaluation of interpretable\nfeatures influence.\n", "versions": [{"version": "v1", "created": "Wed, 31 Jul 2019 15:05:06 GMT"}], "update_date": "2019-08-14", "authors_parsed": [["Ventura", "Francesco", ""], ["Cerquitelli", "Tania", ""]]}, {"id": "1908.04349", "submitter": "Richard Cobos", "authors": "Richard Cobos, Jefferson Hernandez and Andres G. Abad", "title": "A fast multi-object tracking system using an object detector ensemble", "comments": "5 pages, 4 figures, 1 table, published in 2019 IEEE Colombian\n  Conference on Applications in Computational Intelligence (ColCACI)", "journal-ref": null, "doi": "10.1109/ColCACI.2019.8781972", "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multiple-Object Tracking (MOT) is of crucial importance for applications such\nas retail video analytics and video surveillance. Object detectors are often\nthe computational bottleneck of modern MOT systems, limiting their use for\nreal-time applications. In this paper, we address this issue by leveraging on\nan ensemble of detectors, each running every f frames. We measured the\nperformance of our system in the MOT16 benchmark. The proposed model surpassed\nother online entries of the MOT16 challenge in speed, while maintaining an\nacceptable accuracy.\n", "versions": [{"version": "v1", "created": "Tue, 6 Aug 2019 20:23:31 GMT"}], "update_date": "2019-08-14", "authors_parsed": [["Cobos", "Richard", ""], ["Hernandez", "Jefferson", ""], ["Abad", "Andres G.", ""]]}, {"id": "1908.04351", "submitter": "Brian Kenji Iwana", "authors": "Brian Kenji Iwana, Ryohei Kuroki, Seiichi Uchida", "title": "Explaining Convolutional Neural Networks using Softmax Gradient\n  Layer-wise Relevance Propagation", "comments": "Published at ICCV 2019 Workshops", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional Neural Networks (CNN) have become state-of-the-art in the field\nof image classification. However, not everything is understood about their\ninner representations. This paper tackles the interpretability and\nexplainability of the predictions of CNNs for multi-class classification\nproblems. Specifically, we propose a novel visualization method of pixel-wise\ninput attribution called Softmax-Gradient Layer-wise Relevance Propagation\n(SGLRP). The proposed model is a class discriminate extension to Deep Taylor\nDecomposition (DTD) using the gradient of softmax to back propagate the\nrelevance of the output probability to the input image. Through qualitative and\nquantitative analysis, we demonstrate that SGLRP can successfully localize and\nattribute the regions on input images which contribute to a target object's\nclassification. We show that the proposed method excels at discriminating the\ntarget objects class from the other possible objects in the images. We confirm\nthat SGLRP performs better than existing Layer-wise Relevance Propagation (LRP)\nbased methods and can help in the understanding of the decision process of\nCNNs.\n", "versions": [{"version": "v1", "created": "Tue, 6 Aug 2019 13:05:04 GMT"}, {"version": "v2", "created": "Sun, 3 Nov 2019 07:48:59 GMT"}, {"version": "v3", "created": "Thu, 7 Nov 2019 07:48:43 GMT"}], "update_date": "2019-11-11", "authors_parsed": [["Iwana", "Brian Kenji", ""], ["Kuroki", "Ryohei", ""], ["Uchida", "Seiichi", ""]]}, {"id": "1908.04353", "submitter": "Dong Cao", "authors": "Dong Cao, Lisha Xu, HaiBo Chen", "title": "Action Recognition in Untrimmed Videos with Composite Self-Attention\n  Two-Stream Framework", "comments": "Accepted to ACPR 2019", "journal-ref": null, "doi": "10.1007/978-3-030-41299-9_3", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the rapid development of deep learning algorithms, action recognition in\nvideo has achieved many important research results. One issue in action\nrecognition, Zero-Shot Action Recognition (ZSAR), has recently attracted\nconsiderable attention, which classify new categories without any positive\nexamples. Another difficulty in action recognition is that untrimmed data may\nseriously affect model performance. We propose a composite two-stream framework\nwith a pre-trained model. Our proposed framework includes a classifier branch\nand a composite feature branch. The graph network model is adopted in each of\nthe two branches, which effectively improves the feature extraction and\nreasoning ability of the framework. In the composite feature branch, a\n3-channel self-attention models are constructed to weight each frame in the\nvideo and give more attention to the key frames. Each self-attention models\nchannel outputs a set of attention weights to focus on a particular aspect of\nthe video, and a set of attention weights corresponds to a one-dimensional\nvector. The 3-channel self-attention models can evaluate key frames from\nmultiple aspects, and the output sets of attention weight vectors form an\nattention matrix, which effectively enhances the attention of key frames with\nstrong correlation of action. This model can implement action recognition under\nzero-shot conditions, and has good recognition performance for untrimmed video\ndata. Experimental results on relevant data sets confirm the validity of our\nmodel.\n", "versions": [{"version": "v1", "created": "Sun, 4 Aug 2019 02:44:37 GMT"}, {"version": "v2", "created": "Mon, 2 Sep 2019 05:36:11 GMT"}], "update_date": "2020-04-24", "authors_parsed": [["Cao", "Dong", ""], ["Xu", "Lisha", ""], ["Chen", "HaiBo", ""]]}, {"id": "1908.04355", "submitter": "Divyam Madaan", "authors": "Divyam Madaan, Jinwoo Shin, Sung Ju Hwang", "title": "Adversarial Neural Pruning with Latent Vulnerability Suppression", "comments": "Accepted to ICML 2020. Code available at\n  https://github.com/divyam3897/ANP_VS", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.CV cs.NE stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Despite the remarkable performance of deep neural networks on various\ncomputer vision tasks, they are known to be susceptible to adversarial\nperturbations, which makes it challenging to deploy them in real-world\nsafety-critical applications. In this paper, we conjecture that the leading\ncause of adversarial vulnerability is the distortion in the latent feature\nspace, and provide methods to suppress them effectively. Explicitly, we define\n\\emph{vulnerability} for each latent feature and then propose a new loss for\nadversarial learning, \\emph{Vulnerability Suppression (VS)} loss, that aims to\nminimize the feature-level vulnerability during training. We further propose a\nBayesian framework to prune features with high vulnerability to reduce both\nvulnerability and loss on adversarial samples. We validate our\n\\emph{Adversarial Neural Pruning with Vulnerability Suppression (ANP-VS)}\nmethod on multiple benchmark datasets, on which it not only obtains\nstate-of-the-art adversarial robustness but also improves the performance on\nclean examples, using only a fraction of the parameters used by the full\nnetwork. Further qualitative analysis suggests that the improvements come from\nthe suppression of feature-level vulnerability.\n", "versions": [{"version": "v1", "created": "Mon, 12 Aug 2019 19:33:58 GMT"}, {"version": "v2", "created": "Mon, 7 Oct 2019 08:48:00 GMT"}, {"version": "v3", "created": "Mon, 9 Dec 2019 07:14:39 GMT"}, {"version": "v4", "created": "Thu, 2 Jul 2020 13:47:36 GMT"}], "update_date": "2020-07-03", "authors_parsed": [["Madaan", "Divyam", ""], ["Shin", "Jinwoo", ""], ["Hwang", "Sung Ju", ""]]}, {"id": "1908.04373", "submitter": "Ke Yan", "authors": "Ke Yan, Youbao Tang, Yifan Peng, Veit Sandfort, Mohammadhadi Bagheri,\n  Zhiyong Lu, Ronald M. Summers", "title": "MULAN: Multitask Universal Lesion Analysis Network for Joint Lesion\n  Detection, Tagging, and Segmentation", "comments": "MICCAI 2019, including appendix. code:\n  https://github.com/rsummers11/CADLab/tree/master/MULAN_universal_lesion_analysis", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When reading medical images such as a computed tomography (CT) scan,\nradiologists generally search across the image to find lesions, characterize\nand measure them, and then describe them in the radiological report. To\nautomate this process, we propose a multitask universal lesion analysis network\n(MULAN) for joint detection, tagging, and segmentation of lesions in a variety\nof body parts, which greatly extends existing work of single-task lesion\nanalysis on specific body parts. MULAN is based on an improved Mask R-CNN\nframework with three head branches and a 3D feature fusion strategy. It\nachieves the state-of-the-art accuracy in the detection and tagging tasks on\nthe DeepLesion dataset, which contains 32K lesions in the whole body. We also\nanalyze the relationship between the three tasks and show that tag predictions\ncan improve detection accuracy via a score refinement layer.\n", "versions": [{"version": "v1", "created": "Mon, 12 Aug 2019 20:40:12 GMT"}], "update_date": "2019-08-14", "authors_parsed": [["Yan", "Ke", ""], ["Tang", "Youbao", ""], ["Peng", "Yifan", ""], ["Sandfort", "Veit", ""], ["Bagheri", "Mohammadhadi", ""], ["Lu", "Zhiyong", ""], ["Summers", "Ronald M.", ""]]}, {"id": "1908.04383", "submitter": "Dalton Lunga", "authors": "Dalton Lunga, Jonathan Gerrand, Hsiuhan Lexie Yang, Christopher Layton\n  and Robert Stewart", "title": "Apache Spark Accelerated Deep Learning Inference for Large Scale\n  Satellite Image Analytics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  The shear volumes of data generated from earth observation and remote sensing\ntechnologies continue to make major impact; leaping key geospatial applications\ninto the dual data and compute intensive era. As a consequence, this rapid\nadvancement poses new computational and data processing challenges. We\nimplement a novel remote sensing data flow (RESFlow) for advanced machine\nlearning and computing with massive amounts of remotely sensed imagery. The\ncore contribution is partitioning massive amount of data based on the spectral\nand semantic characteristics for distributed imagery analysis. RESFlow takes\nadvantage of both a unified analytics engine for large-scale data processing\nand the availability of modern computing hardware to harness the acceleration\nof deep learning inference on expansive remote sensing imagery. The framework\nincorporates a strategy to optimize resource utilization across multiple\nexecutors assigned to a single worker. We showcase its deployment across\ncomputationally and data-intensive on pixel-level labeling workloads. The\npipeline invokes deep learning inference at three stages; during deep feature\nextraction, deep metric mapping, and deep semantic segmentation. The tasks\nimpose compute intensive and GPU resource sharing challenges motivating for a\nparallelized pipeline for all execution steps. By taking advantage of Apache\nSpark, Nvidia DGX1, and DGX2 computing platforms, we demonstrate unprecedented\ncompute speed-ups for deep learning inference on pixel labeling workloads;\nprocessing 21,028~Terrabytes of imagery data and delivering an output maps at\narea rate of 5.245sq.km/sec, amounting to 453,168 sq.km/day - reducing a 28 day\nworkload to 21~hours.\n", "versions": [{"version": "v1", "created": "Thu, 8 Aug 2019 21:36:41 GMT"}], "update_date": "2019-08-14", "authors_parsed": [["Lunga", "Dalton", ""], ["Gerrand", "Jonathan", ""], ["Yang", "Hsiuhan Lexie", ""], ["Layton", "Christopher", ""], ["Stewart", "Robert", ""]]}, {"id": "1908.04384", "submitter": "Ameer Pasha Hosseinbor", "authors": "A. Pasha Hosseinbor, R. Zhdanov, and A. Ushveridze", "title": "An Unsupervised, Iterative N-Dimensional Point-Set Registration\n  Algorithm", "comments": "arXiv admin note: text overlap with arXiv:1702.01870", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An unsupervised, iterative point-set registration algorithm for an unlabeled\n(i.e. correspondence between points is unknown) N-dimensional Euclidean\npoint-cloud is proposed. It is based on linear least squares, and considers all\npossible point pairings and iteratively aligns the two sets until the number of\npoint pairs does not exceed the maximum number of allowable one-to-one\npairings.\n", "versions": [{"version": "v1", "created": "Tue, 6 Aug 2019 17:03:53 GMT"}], "update_date": "2019-08-14", "authors_parsed": [["Hosseinbor", "A. Pasha", ""], ["Zhdanov", "R.", ""], ["Ushveridze", "A.", ""]]}, {"id": "1908.04385", "submitter": "Zheng Liu", "authors": "Zheng Liu, Zidong Jiang, Wei Feng, Hui Feng", "title": "OD-GCN: Object Detection Boosted by Knowledge GCN", "comments": "6 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Classical CNN based object detection methods only extract the objects' image\nfeatures, but do not consider the high-level relationship among objects in\ncontext. In this article, the graph convolutional networks (GCN) is integrated\ninto the object detection framework to exploit the benefit of category\nrelationship among objects, which is able to provide extra confidence for any\npre-trained object detection model in our framework. In experiments, we test\nseveral popular base detection models on COCO dataset. The results show\npromising improvement on mAP by 1-5pp. In addition, visualized analysis reveals\nthe benchmark improvement is quite reasonable in human's opinion.\n", "versions": [{"version": "v1", "created": "Tue, 6 Aug 2019 02:23:29 GMT"}, {"version": "v2", "created": "Mon, 30 Sep 2019 03:17:40 GMT"}, {"version": "v3", "created": "Mon, 11 Nov 2019 03:27:23 GMT"}], "update_date": "2019-11-12", "authors_parsed": [["Liu", "Zheng", ""], ["Jiang", "Zidong", ""], ["Feng", "Wei", ""], ["Feng", "Hui", ""]]}, {"id": "1908.04386", "submitter": "Koba Natroshvili", "authors": "Koba Natroshvili", "title": "Detection of the Group of Traffic Signs with Central Slice Theorem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Our sensor system consists of a combination of Photonic Mixer Device - PMD\nand Mono optical cameras. Some traffic signs have stripes at 45{deg}. These\ntraffic signs cancel different restrictions on the road. We detect this class\nof signs with Radon transformation. Here the Radon transformation is calculated\nusing Central Slice Theorem. We approximate the slice of spectrum by the\nDiscrete Cosine Transformation (DCT).\n", "versions": [{"version": "v1", "created": "Sun, 4 Aug 2019 11:31:26 GMT"}], "update_date": "2019-08-14", "authors_parsed": [["Natroshvili", "Koba", ""]]}, {"id": "1908.04387", "submitter": "Muhammad Hamdan", "authors": "Muhammad K A Hamdan, Daine T. Rover, Matthew J. Darr, and John Just", "title": "Mass Estimation from Images using Deep Neural Network and Sparse Ground\n  Truth", "comments": "9 pages, 19 figures, pre-print NIPS2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Supervised learning is the workhorse for regression and classification tasks,\nbut the standard approach presumes ground truth for every measurement. In real\nworld applications, limitations due to expense or general in-feasibility due to\nthe specific application are common. In the context of agriculture\napplications, yield monitoring is one such example where simple-physics based\nmeasurements such as volume or force-impact have been used to quantify mass\nflow, which incur error due to sensor calibration. By utilizing semi-supervised\ndeep learning with gradient aggregation and a sequence of images, in this work\nwe can accurately estimate a physical quantity (mass) with complex data\nstructures and sparse ground truth. Using a vision system capturing images of a\nsugarcane elevator and running bamboo under controlled testing as a surrogate\nmaterial to harvesting sugarcane, mass is accurately predicted from images by\ntraining a DNN using only final load weights. The DNN succeeds in capturing the\ncomplex density physics of random stacking of slender rods internally as part\nof the mass prediction model, and surpasses older volumetric-based methods for\nmass prediction. Furthermore, by incorporating knowledge about the system\nphysics through the DNN architecture and penalty terms, improvements in\nprediction accuracy and stability, as well as faster learning are obtained. It\nis shown that the classic nonlinear regression optimization can be reformulated\nwith an aggregation term with some independence assumptions to achieve this\nfeat. Since the number of images for any given run are too large to fit on\ntypical GPU vRAM, an implementation is shown that compensates for the limited\nmemory but still achieve fast training times. The same approach presented\nherein could be applied to other applications like yield monitoring on grain\ncombines or other harvesters using vision or other instrumentation.\n", "versions": [{"version": "v1", "created": "Mon, 5 Aug 2019 02:59:18 GMT"}, {"version": "v2", "created": "Wed, 14 Aug 2019 21:08:43 GMT"}, {"version": "v3", "created": "Tue, 10 Sep 2019 17:46:09 GMT"}], "update_date": "2019-09-11", "authors_parsed": [["Hamdan", "Muhammad K A", ""], ["Rover", "Daine T.", ""], ["Darr", "Matthew J.", ""], ["Just", "John", ""]]}, {"id": "1908.04388", "submitter": "Faruk Ahmed", "authors": "Faruk Ahmed and Aaron Courville", "title": "Detecting semantic anomalies", "comments": "Preprint for AAAI '20 publication", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We critically appraise the recent interest in out-of-distribution (OOD)\ndetection and question the practical relevance of existing benchmarks. While\nthe currently prevalent trend is to consider different datasets as OOD, we\nargue that out-distributions of practical interest are ones where the\ndistinction is semantic in nature for a specified context, and that evaluative\ntasks should reflect this more closely. Assuming a context of object\nrecognition, we recommend a set of benchmarks, motivated by practical\napplications. We make progress on these benchmarks by exploring a multi-task\nlearning based approach, showing that auxiliary objectives for improved\nsemantic awareness result in improved semantic anomaly detection, with\naccompanying generalization benefits.\n", "versions": [{"version": "v1", "created": "Tue, 13 Aug 2019 14:56:40 GMT"}, {"version": "v2", "created": "Fri, 13 Sep 2019 16:35:16 GMT"}, {"version": "v3", "created": "Thu, 21 Nov 2019 20:48:53 GMT"}], "update_date": "2019-11-25", "authors_parsed": [["Ahmed", "Faruk", ""], ["Courville", "Aaron", ""]]}, {"id": "1908.04389", "submitter": "Moustafa Alzantot", "authors": "Moustafa Alzantot, Amy Widdicombe, Simon Julier, Mani Srivastava", "title": "NeuroMask: Explaining Predictions of Deep Neural Networks through Mask\n  Learning", "comments": null, "journal-ref": "Published in the DAIS 2019 - Workshop on Distributed Analytics\n  InfraStructure and Algorithms for Multi-Organization Federations", "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Neural Networks (DNNs) deliver state-of-the-art performance in many\nimage recognition and understanding applications. However, despite their\noutstanding performance, these models are black-boxes and it is hard to\nunderstand how they make their decisions. Over the past few years, researchers\nhave studied the problem of providing explanations of why DNNs predicted their\nresults. However, existing techniques are either obtrusive, requiring changes\nin model training, or suffer from low output quality. In this paper, we present\na novel method, NeuroMask, for generating an interpretable explanation of\nclassification model results. When applied to image classification models,\nNeuroMask identifies the image parts that are most important to classifier\nresults by applying a mask that hides/reveals different parts of the image,\nbefore feeding it back into the model. The mask values are tuned by minimizing\na properly designed cost function that preserves the classification result and\nencourages producing an interpretable mask. Experiments using state-of-the-art\nConvolutional Neural Networks for image recognition on different datasets\n(CIFAR-10 and ImageNet) show that NeuroMask successfully localizes the parts of\nthe input image which are most relevant to the DNN decision. By showing a\nvisual quality comparison between NeuroMask explanations and those of other\nmethods, we find NeuroMask to be both accurate and interpretable.\n", "versions": [{"version": "v1", "created": "Mon, 5 Aug 2019 07:33:30 GMT"}], "update_date": "2019-08-14", "authors_parsed": [["Alzantot", "Moustafa", ""], ["Widdicombe", "Amy", ""], ["Julier", "Simon", ""], ["Srivastava", "Mani", ""]]}, {"id": "1908.04390", "submitter": "Stefan Langer", "authors": "Stefan Langer, Robert M\\\"uller, Kyrill Schmid and Claudia\n  Linnhoff-Popien", "title": "Difficulty Classification of Mountainbike Downhill Trails utilizing Deep\n  Neural Networks", "comments": "11 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The difficulty of mountainbike downhill trails is a subjective perception.\nHowever, sports-associations and mountainbike park operators attempt to group\ntrails into different levels of difficulty with scales like the\nSingletrail-Skala (S0-S5) or colored scales (blue, red, black, ...) as proposed\nby The International Mountain Bicycling Association. Inconsistencies in\ndifficulty grading occur due to the various scales, different people grading\nthe trails, differences in topography, and more. We propose an end-to-end deep\nlearning approach to classify trails into three difficulties easy, medium, and\nhard by using sensor data. With mbientlab Meta Motion r0.2 sensor units, we\nrecord accelerometer- and gyroscope data of one rider on multiple trail\nsegments. A 2D convolutional neural network is trained with a stacked and\nconcatenated representation of the aforementioned data as its input. We run\nexperiments with five different sample- and five different kernel sizes and\nachieve a maximum Sparse Categorical Accuracy of 0.9097. To the best of our\nknowledge, this is the first work targeting computational difficulty\nclassification of mountainbike downhill trails.\n", "versions": [{"version": "v1", "created": "Mon, 5 Aug 2019 08:11:07 GMT"}], "update_date": "2019-11-12", "authors_parsed": [["Langer", "Stefan", ""], ["M\u00fcller", "Robert", ""], ["Schmid", "Kyrill", ""], ["Linnhoff-Popien", "Claudia", ""]]}, {"id": "1908.04391", "submitter": "Fei Xue", "authors": "Fei Xue, Xin Wang, Zike Yan, Qiuyuan Wang, Junqiu Wang, Hongbin Zha", "title": "Local Supports Global: Deep Camera Relocalization with Sequence\n  Enhancement", "comments": "Accept to ICCV 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose to leverage the local information in image sequences to support\nglobal camera relocalization. In contrast to previous methods that regress\nglobal poses from single images, we exploit the spatial-temporal consistency in\nsequential images to alleviate uncertainty due to visual ambiguities by\nincorporating a visual odometry (VO) component. Specifically, we introduce two\neffective steps called content-augmented pose estimation and motion-based\nrefinement. The content-augmentation step focuses on alleviating the\nuncertainty of pose estimation by augmenting the observation based on the\nco-visibility in local maps built by the VO stream. Besides, the motion-based\nrefinement is formulated as a pose graph, where the camera poses are further\noptimized by adopting relative poses provided by the VO component as additional\nmotion constraints. Thus, the global consistency can be guaranteed. Experiments\non the public indoor 7-Scenes and outdoor Oxford RobotCar benchmark datasets\ndemonstrate that benefited from local information inherent in the sequence, our\napproach outperforms state-of-the-art methods, especially in some challenging\ncases, e.g., insufficient texture, highly repetitive textures, similar\nappearances, and over-exposure.\n", "versions": [{"version": "v1", "created": "Tue, 6 Aug 2019 03:49:52 GMT"}], "update_date": "2019-08-14", "authors_parsed": [["Xue", "Fei", ""], ["Wang", "Xin", ""], ["Yan", "Zike", ""], ["Wang", "Qiuyuan", ""], ["Wang", "Junqiu", ""], ["Zha", "Hongbin", ""]]}, {"id": "1908.04392", "submitter": "Amir Mosavi Prof", "authors": "Husein Perez, Joseph H. M. Tah, Amir Mosavi", "title": "Deep Learning for Detecting Building Defects Using Convolutional Neural\n  Networks", "comments": "29 pages, 11 figures", "journal-ref": null, "doi": "10.20944/preprints201908.0068.v1", "report-no": null, "categories": "cs.CV cs.AI cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Clients are increasingly looking for fast and effective means to quickly and\nfrequently survey and communicate the condition of their buildings so that\nessential repairs and maintenance work can be done in a proactive and timely\nmanner before it becomes too dangerous and expensive. Traditional methods for\nthis type of work commonly comprise of engaging building surveyors to undertake\na condition assessment which involves a lengthy site inspection to produce a\nsystematic recording of the physical condition of the building elements,\nincluding cost estimates of immediate and projected long-term costs of renewal,\nrepair and maintenance of the building. Current asset condition assessment\nprocedures are extensively time consuming, laborious, and expensive and pose\nhealth and safety threats to surveyors, particularly at height and roof levels\nwhich are difficult to access. This paper aims at evaluating the application of\nconvolutional neural networks (CNN) towards an automated detection and\nlocalisation of key building defects, e.g., mould, deterioration, and stain,\nfrom images. The proposed model is based on pre-trained CNN classifier of\nVGG-16 (later compaired with ResNet-50, and Inception models), with class\nactivation mapping (CAM) for object localisation. The challenges and\nlimitations of the model in real-life applications have been identified. The\nproposed model has proven to be robust and able to accurately detect and\nlocalise building defects. The approach is being developed with the potential\nto scale-up and further advance to support automated detection of defects and\ndeterioration of buildings in real-time using mobile devices and drones.\n", "versions": [{"version": "v1", "created": "Tue, 6 Aug 2019 16:21:10 GMT"}], "update_date": "2019-08-14", "authors_parsed": [["Perez", "Husein", ""], ["Tah", "Joseph H. M.", ""], ["Mosavi", "Amir", ""]]}, {"id": "1908.04393", "submitter": "Umut \\\"Ozkaya", "authors": "Umut Ozkaya, Levent Seyfi", "title": "Fine-Tuning Models Comparisons on Garbage Classification for\n  Recyclability", "comments": "published in ISAS 2018-Winter", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this study, it is aimed to develop a deep learning application which\ndetects types of garbage into trash in order to provide recyclability with\nvision system. Training and testing will be performed with image data\nconsisting of several classes on different garbage types. The data set used\nduring training and testing will be generated from original frames taken from\ngarbage images. The data set used for deep learning structures has a total of\n2527 images with 6 different classes. Half of these images in the data set were\nused for training process and remaining part were used for testing procedure.\nAlso, transfer learning was used to obtain shorter training and test procedures\nwith and higher accuracy. As fine-tuned models, Alexnet, VGG16, Googlenet and\nResnet structures were carried. In order to test performance of classifiers,\ntwo different classifiers are used as Softmax and Support Vector Machines. 6\ndifferent type of trash images were correctly classified the highest accuracy\nwith GoogleNet+SVM as 97.86%.\n", "versions": [{"version": "v1", "created": "Wed, 7 Aug 2019 15:58:51 GMT"}], "update_date": "2019-08-14", "authors_parsed": [["Ozkaya", "Umut", ""], ["Seyfi", "Levent", ""]]}, {"id": "1908.04396", "submitter": "Xi Zhang", "authors": "Xi Zhang, Xiaolin Wu, Jun Du", "title": "Challenge of Spatial Cognition for Deep Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given the success of the deep convolutional neural networks (DCNNs) in\napplications of visual recognition and classification, it would be tantalizing\nto test if DCNNs can also learn spatial concepts, such as straightness,\nconvexity, left/right, front/back, relative size, aspect ratio, polygons, etc.,\nfrom varied visual examples of these concepts that are simple and yet vital for\nspatial reasoning. Much to our dismay, extensive experiments of the type of\ncognitive psychology demonstrate that the data-driven deep learning (DL) cannot\nsee through superficial variations in visual representations and grasp the\nspatial concept in abstraction. The root cause of failure turns out to be the\nlearning methodology, not the computational model of the neural network itself.\nBy incorporating task-specific convolutional kernels, we are able to construct\nDCNNs for spatial cognition tasks that can generalize to input images not drawn\nfrom the same distribution of the training set. This work raises a precaution\nthat without manually-incorporated priors or features DCCNs may fail spatial\ncognitive tasks at rudimentary level.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jul 2019 11:35:40 GMT"}, {"version": "v2", "created": "Tue, 12 May 2020 15:51:21 GMT"}], "update_date": "2020-05-13", "authors_parsed": [["Zhang", "Xi", ""], ["Wu", "Xiaolin", ""], ["Du", "Jun", ""]]}, {"id": "1908.04404", "submitter": "Andrew Shepley Mr", "authors": "Andrew Jason Shepley", "title": "Face Recognition in Unconstrained Conditions: A Systematic Review", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Face recognition is a biometric which is attracting significant research,\ncommercial and government interest, as it provides a discreet, non-intrusive\nway of detecting, and recognizing individuals, without need for the subject's\nknowledge or consent. This is due to reduced cost, and evolution in hardware\nand algorithms which have improved their ability to handle unconstrained\nconditions. Evidently affordable and efficient applications are required.\nHowever, there is much debate over which methods are most appropriate,\nparticularly in the context of the growing importance of deep neural\nnetwork-based face recognition systems. This systematic review attempts to\nprovide clarity on both issues by organizing the plethora of research and data\nin this field to clarify current research trends, state-of-the-art methods, and\nprovides an outline of their benefits and shortcomings. Overall, this research\ncovered 1,330 relevant studies, showing an increase of over 200% in research\ninterest in the field of face recognition over the past 6 years. Our results\nalso demonstrated that deep learning methods are the prime focus of modern\nresearch due to improvements in hardware databases and increasing understanding\nof neural networks. In contrast, traditional methods have lost favor amongst\nresearchers due to their inherent limitations in accuracy, and lack of\nefficiency when handling large amounts of data.\n", "versions": [{"version": "v1", "created": "Fri, 12 Jul 2019 23:54:50 GMT"}], "update_date": "2019-08-14", "authors_parsed": [["Shepley", "Andrew Jason", ""]]}, {"id": "1908.04413", "submitter": "Zaiwang Gu", "authors": "Hao Qiu, Zaiwang Gu, Lei Mou, Xiaoqian Mao, Liyang Fang, Yitian Zhao,\n  Jiang Liu, Jun Cheng", "title": "The Channel Attention based Context Encoder Network for Inner Limiting\n  Membrane Detection", "comments": "This paper has been accepted by the miccai workshop (OMIA-6)", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The optic disc segmentation is an important step for retinal image-based\ndisease diagnosis such as glaucoma. The inner limiting membrane (ILM) is the\nfirst boundary in the OCT, which can help to extract the retinal pigment\nepithelium (RPE) through gradient edge information to locate the boundary of\nthe optic disc. Thus, the ILM layer segmentation is of great importance for\noptic disc localization. In this paper, we build a new optic disc centered\ndataset from 20 volunteers and manually annotated the ILM boundary in each OCT\nscan as ground-truth. We also propose a channel attention based context encoder\nnetwork modified from the CE-Net to segment the optic disc. It mainly contains\nthree phases: the encoder module, the channel attention based context encoder\nmodule, and the decoder module. Finally, we demonstrate that our proposed\nmethod achieves state-of-the-art disc segmentation performance on our dataset\nmentioned above.\n", "versions": [{"version": "v1", "created": "Fri, 9 Aug 2019 13:48:50 GMT"}], "update_date": "2019-08-14", "authors_parsed": [["Qiu", "Hao", ""], ["Gu", "Zaiwang", ""], ["Mou", "Lei", ""], ["Mao", "Xiaoqian", ""], ["Fang", "Liyang", ""], ["Zhao", "Yitian", ""], ["Liu", "Jiang", ""], ["Cheng", "Jun", ""]]}, {"id": "1908.04422", "submitter": "Rui Chen", "authors": "Rui Chen, Songfang Han, Jing Xu, Hao Su", "title": "Point-Based Multi-View Stereo Network", "comments": "Accepted as ICCV 2019 oral presentation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce Point-MVSNet, a novel point-based deep framework for multi-view\nstereo (MVS). Distinct from existing cost volume approaches, our method\ndirectly processes the target scene as point clouds. More specifically, our\nmethod predicts the depth in a coarse-to-fine manner. We first generate a\ncoarse depth map, convert it into a point cloud and refine the point cloud\niteratively by estimating the residual between the depth of the current\niteration and that of the ground truth. Our network leverages 3D geometry\npriors and 2D texture information jointly and effectively by fusing them into a\nfeature-augmented point cloud, and processes the point cloud to estimate the 3D\nflow for each point. This point-based architecture allows higher accuracy, more\ncomputational efficiency and more flexibility than cost-volume-based\ncounterparts. Experimental results show that our approach achieves a\nsignificant improvement in reconstruction quality compared with\nstate-of-the-art methods on the DTU and the Tanks and Temples dataset. Our\nsource code and trained models are available at\nhttps://github.com/callmeray/PointMVSNet .\n", "versions": [{"version": "v1", "created": "Mon, 12 Aug 2019 22:21:52 GMT"}], "update_date": "2019-08-14", "authors_parsed": [["Chen", "Rui", ""], ["Han", "Songfang", ""], ["Xu", "Jing", ""], ["Su", "Hao", ""]]}, {"id": "1908.04441", "submitter": "Xiao Wang", "authors": "Rui Yang, Yabin Zhu, Xiao Wang, Chenglong Li, Jin Tang", "title": "Learning Target-oriented Dual Attention for Robust RGB-T Tracking", "comments": "Accepted by IEEE ICIP 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  RGB-Thermal object tracking attempt to locate target object using\ncomplementary visual and thermal infrared data. Existing RGB-T trackers fuse\ndifferent modalities by robust feature representation learning or adaptive\nmodal weighting. However, how to integrate dual attention mechanism for visual\ntracking is still a subject that has not been studied yet. In this paper, we\npropose two visual attention mechanisms for robust RGB-T object tracking.\nSpecifically, the local attention is implemented by exploiting the common\nvisual attention of RGB and thermal data to train deep classifiers. We also\nintroduce the global attention, which is a multi-modal target-driven attention\nestimation network. It can provide global proposals for the classifier together\nwith local proposals extracted from previous tracking result. Extensive\nexperiments on two RGB-T benchmark datasets validated the effectiveness of our\nproposed algorithm.\n", "versions": [{"version": "v1", "created": "Mon, 12 Aug 2019 23:54:35 GMT"}], "update_date": "2019-08-14", "authors_parsed": [["Yang", "Rui", ""], ["Zhu", "Yabin", ""], ["Wang", "Xiao", ""], ["Li", "Chenglong", ""], ["Tang", "Jin", ""]]}, {"id": "1908.04466", "submitter": "Hyeon Woo Lee", "authors": "Hyeon Woo Lee, Mert R. Sabuncu, Adrian V. Dalca", "title": "Few Labeled Atlases are Necessary for Deep-Learning-Based Segmentation", "comments": "Accepted as extended abstract to Machine Learning for Health (ML4H)\n  at NeurIPS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We tackle biomedical image segmentation in the scenario of only a few labeled\nbrain MR images. This is an important and challenging task in medical\napplications, where manual annotations are time-consuming. Current multi-atlas\nbased segmentation methods use image registration to warp segments from labeled\nimages onto a new scan. In a different paradigm, supervised learning-based\nsegmentation strategies have gained popularity. These method consistently use\nrelatively large sets of labeled training data, and their behavior in the\nregime of a few labeled biomedical images has not been thoroughly evaluated. In\nthis work, we provide two important results for segmentation in the scenario\nwhere few labeled images are available. First, we propose a straightforward\nimplementation of efficient semi-supervised learning-based registration method,\nwhich we showcase in a multi-atlas segmentation framework. Second, through an\nextensive empirical study, we evaluate the performance of a supervised\nsegmentation approach, where the training images are augmented via random\ndeformations. Surprisingly, we find that in both paradigms, accurate\nsegmentation is generally possible even in the context of few labeled images.\n", "versions": [{"version": "v1", "created": "Tue, 13 Aug 2019 02:39:52 GMT"}, {"version": "v2", "created": "Wed, 14 Aug 2019 05:14:25 GMT"}, {"version": "v3", "created": "Thu, 15 Aug 2019 14:57:22 GMT"}, {"version": "v4", "created": "Mon, 13 Jan 2020 05:38:22 GMT"}], "update_date": "2020-01-14", "authors_parsed": [["Lee", "Hyeon Woo", ""], ["Sabuncu", "Mert R.", ""], ["Dalca", "Adrian V.", ""]]}, {"id": "1908.04469", "submitter": "Chaowei Tan", "authors": "Chaowei Tan, Zhennan Yan, Shaoting Zhang, Kang Li, and Dimitris N.\n  Metaxas", "title": "Collaborative Multi-agent Learning for MR Knee Articular Cartilage\n  Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG cs.MA", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The 3D morphology and quantitative assessment of knee articular cartilages\n(i.e., femoral, tibial, and patellar cartilage) in magnetic resonance (MR)\nimaging is of great importance for knee radiographic osteoarthritis (OA)\ndiagnostic decision making. However, effective and efficient delineation of all\nthe knee articular cartilages in large-sized and high-resolution 3D MR knee\ndata is still an open challenge. In this paper, we propose a novel framework to\nsolve the MR knee cartilage segmentation task. The key contribution is the\nadversarial learning based collaborative multi-agent segmentation network. In\nthe proposed network, we use three parallel segmentation agents to label\ncartilages in their respective region of interest (ROI), and then fuse the\nthree cartilages by a novel ROI-fusion layer. The collaborative learning is\ndriven by an adversarial sub-network. The ROI-fusion layer not only fuses the\nindividual cartilages from multiple agents, but also backpropagates the\ntraining loss from the adversarial sub-network to each agent to enable joint\nlearning of shape and spatial constraints. Extensive evaluations are conducted\non a dataset including hundreds of MR knee volumes with diverse populations,\nand the proposed method shows superior performance.\n", "versions": [{"version": "v1", "created": "Tue, 13 Aug 2019 02:58:17 GMT"}], "update_date": "2019-08-14", "authors_parsed": [["Tan", "Chaowei", ""], ["Yan", "Zhennan", ""], ["Zhang", "Shaoting", ""], ["Li", "Kang", ""], ["Metaxas", "Dimitris N.", ""]]}, {"id": "1908.04501", "submitter": "Jungbeom Lee", "authors": "Jungbeom Lee, Eunji Kim, Sungmin Lee, Jangho Lee, Sungroh Yoon", "title": "Frame-to-Frame Aggregation of Active Regions in Web Videos for Weakly\n  Supervised Semantic Segmentation", "comments": "ICCV 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When a deep neural network is trained on data with only image-level labeling,\nthe regions activated in each image tend to identify only a small region of the\ntarget object. We propose a method of using videos automatically harvested from\nthe web to identify a larger region of the target object by using temporal\ninformation, which is not present in the static image. The temporal variations\nin a video allow different regions of the target object to be activated. We\nobtain an activated region in each frame of a video, and then aggregate the\nregions from successive frames into a single image, using a warping technique\nbased on optical flow. The resulting localization maps cover more of the target\nobject, and can then be used as proxy ground-truth to train a segmentation\nnetwork. This simple approach outperforms existing methods under the same level\nof supervision, and even approaches relying on extra annotations. Based on\nVGG-16 and ResNet 101 backbones, our method achieves the mIoU of 65.0 and 67.4,\nrespectively, on PASCAL VOC 2012 test images, which represents a new\nstate-of-the-art.\n", "versions": [{"version": "v1", "created": "Tue, 13 Aug 2019 06:04:25 GMT"}], "update_date": "2019-08-14", "authors_parsed": [["Lee", "Jungbeom", ""], ["Kim", "Eunji", ""], ["Lee", "Sungmin", ""], ["Lee", "Jangho", ""], ["Yoon", "Sungroh", ""]]}, {"id": "1908.04503", "submitter": "Ang Li", "authors": "Ang Li, Jianzhong Qi, Rui Zhang, Ramamohanarao Kotagiri", "title": "Boosted GAN with Semantically Interpretable Information for Image\n  Inpainting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image inpainting aims at restoring missing region of corrupted images, which\nhas many applications such as image restoration and object removal. However,\ncurrent GAN-based inpainting models fail to explicitly consider the semantic\nconsistency between restored images and original images. Forexample, given a\nmale image with image region of one eye missing, current models may restore it\nwith a female eye. This is due to the ambiguity of GAN-based inpainting models:\nthese models can generate many possible restorations given a missing region. To\naddress this limitation, our key insight is that semantically interpretable\ninformation (such as attribute and segmentation information) of input images\n(with missing regions) can provide essential guidance for the inpainting\nprocess. Based on this insight, we propose a boosted GAN with semantically\ninterpretable information for image inpainting that consists of an inpainting\nnetwork and a discriminative network. The inpainting network utilizes two\nauxiliary pretrained networks to discover the attribute and segmentation\ninformation of input images and incorporates them into the inpainting process\nto provide explicit semantic-level guidance. The discriminative network adopts\na multi-level design that can enforce regularizations not only on overall\nrealness but also on attribute and segmentation consistency with the original\nimages. Experimental results show that our proposed model can preserve\nconsistency on both attribute and segmentation level, and significantly\noutperforms the state-of-the-art models.\n", "versions": [{"version": "v1", "created": "Tue, 13 Aug 2019 06:05:24 GMT"}], "update_date": "2019-12-17", "authors_parsed": [["Li", "Ang", ""], ["Qi", "Jianzhong", ""], ["Zhang", "Rui", ""], ["Kotagiri", "Ramamohanarao", ""]]}, {"id": "1908.04512", "submitter": "Jiageng Mao", "authors": "Jiageng Mao, Xiaogang Wang and Hongsheng Li", "title": "Interpolated Convolutional Networks for 3D Point Cloud Understanding", "comments": "ICCV2019 oral. Code will be released soon", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Point cloud is an important type of 3D representation. However, directly\napplying convolutions on point clouds is challenging due to the sparse,\nirregular and unordered data structure. In this paper, we propose a novel\nInterpolated Convolution operation, InterpConv, to tackle the point cloud\nfeature learning and understanding problem. The key idea is to utilize a set of\ndiscrete kernel weights and interpolate point features to neighboring\nkernel-weight coordinates by an interpolation function for convolution. A\nnormalization term is introduced to handle neighborhoods of different sparsity\nlevels. Our InterpConv is shown to be permutation and sparsity invariant, and\ncan directly handle irregular inputs. We further design Interpolated\nConvolutional Neural Networks (InterpCNNs) based on InterpConv layers to handle\npoint cloud recognition tasks including shape classification, object part\nsegmentation and indoor scene semantic parsing. Experiments show that the\nnetworks can capture both fine-grained local structures and global shape\ncontext information effectively. The proposed approach achieves\nstate-of-the-art performance on public benchmarks including ModelNet40,\nShapeNet Parts and S3DIS.\n", "versions": [{"version": "v1", "created": "Tue, 13 Aug 2019 06:44:04 GMT"}], "update_date": "2019-08-14", "authors_parsed": [["Mao", "Jiageng", ""], ["Wang", "Xiaogang", ""], ["Li", "Hongsheng", ""]]}, {"id": "1908.04519", "submitter": "Jin Xia", "authors": "Jin Xia, Jiajun Tang, Cewu Lu", "title": "Three Branches: Detecting Actions With Richer Features", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present our three branch solutions for International Challenge on Activity\nRecognition at CVPR2019. This model seeks to fuse richer information of global\nvideo clip, short human attention and long-term human activity into a unified\nmodel. We have participated in two tasks: Task A, the Kinetics challenge and\nTask B, spatio-temporal action localization challenge. For Kinetics, we achieve\n21.59% error rate. For the AVA challenge, our final model obtains 32.49% mAP on\nthe test sets, which outperforms all submissions to the AVA challenge at CVPR\n2018 for more than 10% mAP. As the future work, we will introduce human\nactivity knowledge, which is a new dataset including key information of human\nactivity.\n", "versions": [{"version": "v1", "created": "Tue, 13 Aug 2019 07:23:44 GMT"}], "update_date": "2019-08-14", "authors_parsed": [["Xia", "Jin", ""], ["Tang", "Jiajun", ""], ["Lu", "Cewu", ""]]}, {"id": "1908.04520", "submitter": "Lin Gao", "authors": "Lin Gao, Jie Yang, Tong Wu, Yu-Jie Yuan, Hongbo Fu, Yu-Kun Lai, Hao\n  Zhang", "title": "SDM-NET: Deep Generative Network for Structured Deformable Mesh", "comments": "Conditionally Accepted to Siggraph Asia 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce SDM-NET, a deep generative neural network which produces\nstructured deformable meshes. Specifically, the network is trained to generate\na spatial arrangement of closed, deformable mesh parts, which respect the\nglobal part structure of a shape collection, e.g., chairs, airplanes, etc. Our\nkey observation is that while the overall structure of a 3D shape can be\ncomplex, the shape can usually be decomposed into a set of parts, each\nhomeomorphic to a box, and the finer-scale geometry of the part can be\nrecovered by deforming the box. The architecture of SDM-NET is that of a\ntwo-level variational autoencoder (VAE). At the part level, a PartVAE learns a\ndeformable model of part geometries. At the structural level, we train a\nStructured Parts VAE (SP-VAE), which jointly learns the part structure of a\nshape collection and the part geometries, ensuring a coherence between global\nshape structure and surface details. Through extensive experiments and\ncomparisons with the state-of-the-art deep generative models of shapes, we\ndemonstrate the superiority of SDM-NET in generating meshes with visual\nquality, flexible topology, and meaningful structures, which benefit shape\ninterpolation and other subsequently modeling tasks.\n", "versions": [{"version": "v1", "created": "Tue, 13 Aug 2019 07:26:15 GMT"}, {"version": "v2", "created": "Tue, 3 Sep 2019 09:08:28 GMT"}], "update_date": "2019-09-04", "authors_parsed": [["Gao", "Lin", ""], ["Yang", "Jie", ""], ["Wu", "Tong", ""], ["Yuan", "Yu-Jie", ""], ["Fu", "Hongbo", ""], ["Lai", "Yu-Kun", ""], ["Zhang", "Hao", ""]]}, {"id": "1908.04568", "submitter": "Mikhail Belyaev", "authors": "Maxim Pisov and Mikhail Goncharov and Nadezhda Kurochkina and Sergey\n  Morozov and Victor Gombolevskiy and Valeria Chernina and Anton Vladzymyrskyy\n  and Ksenia Zamyatina and Anna Chesnokova and Igor Pronin and Michael Shifrin\n  and Mikhail Belyaev", "title": "Incorporating Task-Specific Structural Knowledge into CNNs for Brain\n  Midline Shift Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Midline shift (MLS) is a well-established factor used for outcome prediction\nin traumatic brain injury, stroke and brain tumors. The importance of automatic\nestimation of MLS was recently highlighted by ACR Data Science Institute. In\nthis paper we introduce a novel deep learning based approach for the problem of\nMLS detection, which exploits task-specific structural knowledge. We evaluate\nour method on a large dataset containing heterogeneous images with significant\nMLS and show that its mean error approaches the inter-expert variability.\nFinally, we show the robustness of our approach by validating it on an external\ndataset, acquired during routine clinical practice.\n", "versions": [{"version": "v1", "created": "Tue, 13 Aug 2019 10:17:58 GMT"}, {"version": "v2", "created": "Thu, 21 Nov 2019 07:17:29 GMT"}, {"version": "v3", "created": "Sat, 14 Dec 2019 13:46:19 GMT"}], "update_date": "2019-12-17", "authors_parsed": [["Pisov", "Maxim", ""], ["Goncharov", "Mikhail", ""], ["Kurochkina", "Nadezhda", ""], ["Morozov", "Sergey", ""], ["Gombolevskiy", "Victor", ""], ["Chernina", "Valeria", ""], ["Vladzymyrskyy", "Anton", ""], ["Zamyatina", "Ksenia", ""], ["Chesnokova", "Anna", ""], ["Pronin", "Igor", ""], ["Shifrin", "Michael", ""], ["Belyaev", "Mikhail", ""]]}, {"id": "1908.04598", "submitter": "Hajime Taira", "authors": "Hajime Taira, Ignacio Rocco, Jiri Sedlar, Masatoshi Okutomi, Josef\n  Sivic, Tomas Pajdla, Torsten Sattler, Akihiko Torii", "title": "Is This The Right Place? Geometric-Semantic Pose Verification for Indoor\n  Visual Localization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual localization in large and complex indoor scenes, dominated by weakly\ntextured rooms and repeating geometric patterns, is a challenging problem with\nhigh practical relevance for applications such as Augmented Reality and\nrobotics. To handle the ambiguities arising in this scenario, a common strategy\nis, first, to generate multiple estimates for the camera pose from which a\ngiven query image was taken. The pose with the largest geometric consistency\nwith the query image, e.g., in the form of an inlier count, is then selected in\na second stage. While a significant amount of research has concentrated on the\nfirst stage, there is considerably less work on the second stage. In this\npaper, we thus focus on pose verification. We show that combining different\nmodalities, namely appearance, geometry, and semantics, considerably boosts\npose verification and consequently pose accuracy. We develop multiple\nhand-crafted as well as a trainable approach to join into the\ngeometric-semantic verification and show significant improvements over\nstate-of-the-art on a very challenging indoor dataset.\n", "versions": [{"version": "v1", "created": "Tue, 13 Aug 2019 12:12:05 GMT"}, {"version": "v2", "created": "Mon, 2 Sep 2019 09:45:42 GMT"}], "update_date": "2019-09-04", "authors_parsed": [["Taira", "Hajime", ""], ["Rocco", "Ignacio", ""], ["Sedlar", "Jiri", ""], ["Okutomi", "Masatoshi", ""], ["Sivic", "Josef", ""], ["Pajdla", "Tomas", ""], ["Sattler", "Torsten", ""], ["Torii", "Akihiko", ""]]}, {"id": "1908.04616", "submitter": "Mikaela Angelina Uy", "authors": "Mikaela Angelina Uy, Quang-Hieu Pham, Binh-Son Hua, Duc Thanh Nguyen,\n  Sai-Kit Yeung", "title": "Revisiting Point Cloud Classification: A New Benchmark Dataset and\n  Classification Model on Real-World Data", "comments": "ICCV 2019 Oral", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning techniques for point cloud data have demonstrated great\npotentials in solving classical problems in 3D computer vision such as 3D\nobject classification and segmentation. Several recent 3D object classification\nmethods have reported state-of-the-art performance on CAD model datasets such\nas ModelNet40 with high accuracy (~92%). Despite such impressive results, in\nthis paper, we argue that object classification is still a challenging task\nwhen objects are framed with real-world settings. To prove this, we introduce\nScanObjectNN, a new real-world point cloud object dataset based on scanned\nindoor scene data. From our comprehensive benchmark, we show that our dataset\nposes great challenges to existing point cloud classification techniques as\nobjects from real-world scans are often cluttered with background and/or are\npartial due to occlusions. We identify three key open problems for point cloud\nobject classification, and propose new point cloud classification neural\nnetworks that achieve state-of-the-art performance on classifying objects with\ncluttered background. Our dataset and code are publicly available in our\nproject page https://hkust-vgd.github.io/scanobjectnn/.\n", "versions": [{"version": "v1", "created": "Tue, 13 Aug 2019 12:56:03 GMT"}, {"version": "v2", "created": "Mon, 19 Aug 2019 13:18:02 GMT"}], "update_date": "2019-08-20", "authors_parsed": [["Uy", "Mikaela Angelina", ""], ["Pham", "Quang-Hieu", ""], ["Hua", "Binh-Son", ""], ["Nguyen", "Duc Thanh", ""], ["Yeung", "Sai-Kit", ""]]}, {"id": "1908.04634", "submitter": "Irina Nuidel", "authors": "A.A. Telnykh, I.V. Nuidel, Yu.R. Samorodova", "title": "Construction of efficient detectors for character information\n  recognition", "comments": "14 pages, 7 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We have developed and tested in numerical experiments a universal approach to\nsearching objects of a given type in captured video images (for example,\npeople's faces, vehicles, special characters, numbers and letters, etc.). The\nnovelty and versatility of this approach consists in a unique combination of\nthe well-known methods ranging from creating detectors to making decisions\nindependent of the type of recognition objects. The efficiencies of various\ntypes of basic features used for image coding, including the Haar features, the\nLBP features, and the modified Census transformation are compared. A\ncombination of the modified methods is used for constructing 11 types of\ndetectors of the number of railway carriages and for recognizing digits from\nzero to nine. The efficiency of the constructed detectors is studied.\n", "versions": [{"version": "v1", "created": "Tue, 13 Aug 2019 13:39:17 GMT"}], "update_date": "2019-08-14", "authors_parsed": [["Telnykh", "A. A.", ""], ["Nuidel", "I. V.", ""], ["Samorodova", "Yu. R.", ""]]}, {"id": "1908.04646", "submitter": "Abdullah Rashwan", "authors": "Abdullah Rashwan, Agastya Kalra, and Pascal Poupart", "title": "Matrix Nets: A New Deep Architecture for Object Detection", "comments": "Short paper, stay tuned for the full paper!", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present Matrix Nets (xNets), a new deep architecture for object detection.\nxNets map objects with different sizes and aspect ratios into layers where the\nsizes and the aspect ratios of the objects within their layers are nearly\nuniform. Hence, xNets provide a scale and aspect ratio aware architecture. We\nleverage xNets to enhance key-points based object detection. Our architecture\nachieves mAP of 47.8 on MS COCO, which is higher than any other single-shot\ndetector while using half the number of parameters and training 3x faster than\nthe next best architecture.\n", "versions": [{"version": "v1", "created": "Tue, 13 Aug 2019 13:54:31 GMT"}, {"version": "v2", "created": "Wed, 14 Aug 2019 04:11:13 GMT"}], "update_date": "2019-08-15", "authors_parsed": [["Rashwan", "Abdullah", ""], ["Kalra", "Agastya", ""], ["Poupart", "Pascal", ""]]}, {"id": "1908.04680", "submitter": "Chunhua Shen", "authors": "Bohan Zhuang, Jing Liu, Mingkui Tan, Lingqiao Liu, Ian Reid, Chunhua\n  Shen", "title": "Effective Training of Convolutional Neural Networks with Low-bitwidth\n  Weights and Activations", "comments": "Accepted to IEEE T. Pattern Analysis and Machine Intelligence\n  (TPAMI). Extended version of arXiv:1711.00205 (CVPR 2018)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  This paper tackles the problem of training a deep convolutional neural\nnetwork of both low-bitwidth weights and activations. Optimizing a\nlow-precision network is very challenging due to the non-differentiability of\nthe quantizer, which may result in substantial accuracy loss. To address this,\nwe propose three practical approaches, including (i) progressive quantization;\n(ii) stochastic precision; and (iii) joint knowledge distillation to improve\nthe network training. First, for progressive quantization, we propose two\nschemes to progressively find good local minima. Specifically, we propose to\nfirst optimize a net with quantized weights and subsequently quantize\nactivations. This is in contrast to the traditional methods which optimize them\nsimultaneously. Furthermore, we propose a second progressive quantization\nscheme which gradually decreases the bit-width from high-precision to\nlow-precision during training. Second, to alleviate the excessive training\nburden due to the multi-round training stages, we further propose a one-stage\nstochastic precision strategy to randomly sample and quantize sub-networks\nwhile keeping other parts in full-precision. Finally, we adopt a novel learning\nscheme to jointly train a full-precision model alongside the low-precision one.\nBy doing so, the full-precision model provides hints to guide the low-precision\nmodel training and significantly improves the performance of the low-precision\nnetwork. Extensive experiments on various datasets (e.g., CIFAR-100, ImageNet)\nshow the effectiveness of the proposed methods.\n", "versions": [{"version": "v1", "created": "Sat, 10 Aug 2019 11:48:55 GMT"}, {"version": "v2", "created": "Sun, 11 Oct 2020 06:12:21 GMT"}, {"version": "v3", "created": "Fri, 4 Jun 2021 00:26:53 GMT"}], "update_date": "2021-06-07", "authors_parsed": [["Zhuang", "Bohan", ""], ["Liu", "Jing", ""], ["Tan", "Mingkui", ""], ["Liu", "Lingqiao", ""], ["Reid", "Ian", ""], ["Shen", "Chunhua", ""]]}, {"id": "1908.04694", "submitter": "Shih-Chieh Su", "authors": "Shih-Chieh Su", "title": "Channel Decomposition into Painting Actions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work presents a method to decompose a convolutional layer of the deep\nneural network into painting actions. To behave like the human painter, these\nactions are driven by the cost simulating the hand movement, the paint color\nchange, the stroke shape and the stroking style. To help planning, the Mask\nR-CNN is applied to detect the object areas and decide the painting order. The\nproposed painting system introduces a variety of extensions in artistic styles,\nbased on the chosen parameters. Further experiments are performed to evaluate\nthe channel penetration and the channel sensitivity on the strokes.\n", "versions": [{"version": "v1", "created": "Sat, 10 Aug 2019 06:58:55 GMT"}, {"version": "v2", "created": "Sat, 26 Oct 2019 18:38:21 GMT"}, {"version": "v3", "created": "Tue, 29 Oct 2019 07:59:10 GMT"}, {"version": "v4", "created": "Tue, 12 Nov 2019 06:24:11 GMT"}], "update_date": "2019-11-13", "authors_parsed": [["Su", "Shih-Chieh", ""]]}, {"id": "1908.04702", "submitter": "Cam Bermudez", "authors": "Camilo Bermudez, Justin Blaber, Samuel W. Remedios, Jess E. Reynolds,\n  Catherine Lebel, Maureen McHugo, Stephan Heckers, Yuankai Huo, and Bennett A.\n  Landman", "title": "Generalizing Deep Whole Brain Segmentation for Pediatric and\n  Post-Contrast MRI with Augmented Transfer Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generalizability is an important problem in deep neural networks, especially\nin the context of the variability of data acquisition in clinical magnetic\nresonance imaging (MRI). Recently, the Spatially Localized Atlas Network Tiles\n(SLANT) approach has been shown to effectively segment whole brain non-contrast\nT1w MRI with 132 volumetric labels. Enhancing generalizability of SLANT would\nenable broader application of volumetric assessment in multi-site studies.\nTransfer learning (TL) is commonly used to update the neural network weights\nfor local factors; yet, it is commonly recognized to risk degradation of\nperformance on the original validation/test cohorts. Here, we explore TL by\ndata augmentation to address these concerns in the context of adapting SLANT to\nanatomical variation and scanning protocol. We consider two datasets: First, we\noptimize for age with 30 T1w MRI of young children with manually corrected\nvolumetric labels, and accuracy of automated segmentation defined relative to\nthe manually provided truth. Second, we optimize for acquisition with 36 paired\ndatasets of pre- and post-contrast clinically acquired T1w MRI, and accuracy of\nthe post-contrast segmentations assessed relative to the pre-contrast automated\nassessment. For both studies, we augment the original TL step of SLANT with\neither only the new data or with both original and new data. Over baseline\nSLANT, both approaches yielded significantly improved performance (signed rank\ntests; pediatric: 0.89 vs. 0.82 DSC, p<0.001; contrast: 0.80 vs 0.76, p<0.001).\nThe performance on the original test set decreased with the new-data only\ntransfer learning approach, so data augmentation was superior to strict\ntransfer learning.\n", "versions": [{"version": "v1", "created": "Tue, 13 Aug 2019 15:27:19 GMT"}], "update_date": "2019-08-14", "authors_parsed": [["Bermudez", "Camilo", ""], ["Blaber", "Justin", ""], ["Remedios", "Samuel W.", ""], ["Reynolds", "Jess E.", ""], ["Lebel", "Catherine", ""], ["McHugo", "Maureen", ""], ["Heckers", "Stephan", ""], ["Huo", "Yuankai", ""], ["Landman", "Bennett A.", ""]]}, {"id": "1908.04725", "submitter": "Theo Deprelle", "authors": "Theo Deprelle, Thibault Groueix, Matthew Fisher, Vladimir G. Kim,\n  Bryan C. Russell, Mathieu Aubry", "title": "Learning elementary structures for 3D shape generation and matching", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose to represent shapes as the deformation and combination of\nlearnable elementary 3D structures, which are primitives resulting from\ntraining over a collection of shape. We demonstrate that the learned elementary\n3D structures lead to clear improvements in 3D shape generation and matching.\nMore precisely, we present two complementary approaches for learning elementary\nstructures: (i) patch deformation learning and (ii) point translation learning.\nBoth approaches can be extended to abstract structures of higher dimensions for\nimproved results. We evaluate our method on two tasks: reconstructing ShapeNet\nobjects and estimating dense correspondences between human scans (FAUST inter\nchallenge). We show 16% improvement over surface deformation approaches for\nshape reconstruction and outperform FAUST inter challenge state of the art by\n6%.\n", "versions": [{"version": "v1", "created": "Tue, 13 Aug 2019 16:36:51 GMT"}, {"version": "v2", "created": "Wed, 14 Aug 2019 17:30:02 GMT"}], "update_date": "2019-09-10", "authors_parsed": [["Deprelle", "Theo", ""], ["Groueix", "Thibault", ""], ["Fisher", "Matthew", ""], ["Kim", "Vladimir G.", ""], ["Russell", "Bryan C.", ""], ["Aubry", "Mathieu", ""]]}, {"id": "1908.04767", "submitter": "Christian Marzahl", "authors": "Christian Marzahl, Marc Aubreville, Christof A. Bertram, Jason Stayt,\n  Anne-Katherine Jasensky, Florian Bartenschlager, Marco Fragoso-Garcia, Ann K.\n  Barton, Svenja Elsemann, Samir Jabari, Jens Krauth, Prathmesh Madhu, J\\\"orn\n  Voigt, Jenny Hill, Robert Klopfleisch and Andreas Maier", "title": "Deep Learning-Based Quantification of Pulmonary Hemosiderophages in\n  Cytology Slides", "comments": null, "journal-ref": "Sci Rep 10, 9795 (2020)", "doi": "10.1038/s41598-020-65958-2", "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Purpose: Exercise-induced pulmonary hemorrhage (EIPH) is a common syndrome in\nsport horses with negative impact on performance. Cytology of bronchoalveolar\nlavage fluid by use of a scoring system is considered the most sensitive\ndiagnostic method. Macrophages are classified depending on the degree of\ncytoplasmic hemosiderin content. The current gold standard is manual grading,\nwhich is however monotonous and time-consuming. Methods: We evaluated\nstate-of-the-art deep learning-based methods for single cell macrophage\nclassification and compared them against the performance of nine cytology\nexperts and evaluated inter- and intra-observer variability. Additionally, we\nevaluated object detection methods on a novel data set of 17 completely\nannotated cytology whole slide images (WSI) containing 78,047 hemosiderophages.\nResultsf: Our deep learning-based approach reached a concordance of 0.85,\npartially exceeding human expert concordance (0.68 to 0.86, $\\mu$=0.73,\n$\\sigma$ =0.04). Intra-observer variability was high (0.68 to 0.88) and\ninter-observer concordance was moderate (Fleiss kappa = 0.67). Our object\ndetection approach has a mean average precision of 0.66 over the five classes\nfrom the whole slide gigapixel image and a computation time of below two\nminutes. Conclusion: To mitigate the high inter- and intra-rater variability,\nwe propose our automated object detection pipeline, enabling accurate,\nreproducible and quick EIPH scoring in WSI.\n", "versions": [{"version": "v1", "created": "Mon, 12 Aug 2019 15:16:30 GMT"}], "update_date": "2021-03-01", "authors_parsed": [["Marzahl", "Christian", ""], ["Aubreville", "Marc", ""], ["Bertram", "Christof A.", ""], ["Stayt", "Jason", ""], ["Jasensky", "Anne-Katherine", ""], ["Bartenschlager", "Florian", ""], ["Fragoso-Garcia", "Marco", ""], ["Barton", "Ann K.", ""], ["Elsemann", "Svenja", ""], ["Jabari", "Samir", ""], ["Krauth", "Jens", ""], ["Madhu", "Prathmesh", ""], ["Voigt", "J\u00f6rn", ""], ["Hill", "Jenny", ""], ["Klopfleisch", "Robert", ""], ["Maier", "Andreas", ""]]}, {"id": "1908.04769", "submitter": "Xiaoxiao Li", "authors": "Xiaoxiao Li, Nicha C. Dvornek, Juntang Zhuang, Pamela Ventola, and\n  James Duncan", "title": "Graph Embedding Using Infomax for ASD Classification and Brain\n  Functional Difference Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Significant progress has been made using fMRI to characterize the brain\nchanges that occur in ASD, a complex neuro-developmental disorder. However, due\nto the high dimensionality and low signal-to-noise ratio of fMRI, embedding\ninformative and robust brain regional fMRI representations for both graph-level\nclassification and region-level functional difference detection tasks between\nASD and healthy control (HC) groups is difficult. Here, we model the whole\nbrain fMRI as a graph, which preserves geometrical and temporal information and\nuse a Graph Neural Network (GNN) to learn from the graph-structured fMRI data.\nWe investigate the potential of including mutual information (MI) loss\n(Infomax), which is an unsupervised term encouraging large MI of each nodal\nrepresentation and its corresponding graph-level summarized representation to\nlearn a better graph embedding. Specifically, this work developed a pipeline\nincluding a GNN encoder, a classifier and a discriminator, which forces the\nencoded nodal representations to both benefit classification and reveal the\ncommon nodal patterns in a graph. We simultaneously optimize graph-level\nclassification loss and Infomax. We demonstrated that Infomax graph embedding\nimproves classification performance as a regularization term. Furthermore, we\nfound separable nodal representations of ASD and HC groups in prefrontal\ncortex, cingulate cortex, visual regions, and other social, emotional and\nexecution related brain regions. In contrast with GNN with classification loss\nonly, the proposed pipeline can facilitate training more robust ASD\nclassification models. Moreover, the separable nodal representations can detect\nthe functional differences between the two groups and contribute to revealing\nnew ASD biomarkers.\n", "versions": [{"version": "v1", "created": "Fri, 9 Aug 2019 05:25:46 GMT"}, {"version": "v2", "created": "Wed, 14 Aug 2019 00:22:03 GMT"}], "update_date": "2019-08-15", "authors_parsed": [["Li", "Xiaoxiao", ""], ["Dvornek", "Nicha C.", ""], ["Zhuang", "Juntang", ""], ["Ventola", "Pamela", ""], ["Duncan", "James", ""]]}, {"id": "1908.04781", "submitter": "Jason Zhang", "authors": "Jason Y. Zhang, Panna Felsen, Angjoo Kanazawa, Jitendra Malik", "title": "Predicting 3D Human Dynamics from Video", "comments": "To Appear in ICCV 2019. (v2: Updated \"3D Pose from Video\" in Related\n  Work.)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a video of a person in action, we can easily guess the 3D future motion\nof the person. In this work, we present perhaps the first approach for\npredicting a future 3D mesh model sequence of a person from past video input.\nWe do this for periodic motions such as walking and also actions like bowling\nand squatting seen in sports or workout videos. While there has been a surge of\nfuture prediction problems in computer vision, most approaches predict 3D\nfuture from 3D past or 2D future from 2D past inputs. In this work, we focus on\nthe problem of predicting 3D future motion from past image sequences, which has\na plethora of practical applications in autonomous systems that must operate\nsafely around people from visual inputs. Inspired by the success of\nautoregressive models in language modeling tasks, we learn an intermediate\nlatent space on which we predict the future. This effectively facilitates\nautoregressive predictions when the input differs from the output domain. Our\napproach can be trained on video sequences obtained in-the-wild without 3D\nground truth labels. The project website with videos can be found at\nhttps://jasonyzhang.com/phd.\n", "versions": [{"version": "v1", "created": "Tue, 13 Aug 2019 17:58:36 GMT"}, {"version": "v2", "created": "Tue, 20 Aug 2019 17:58:41 GMT"}], "update_date": "2019-08-21", "authors_parsed": [["Zhang", "Jason Y.", ""], ["Felsen", "Panna", ""], ["Kanazawa", "Angjoo", ""], ["Malik", "Jitendra", ""]]}, {"id": "1908.04842", "submitter": "Geetika Arora", "authors": "Geetika Arora, Ranjeet Ranjan Jha, Akash Agrawal, Kamlesh Tiwari and\n  Aditya Nigam", "title": "SP-NET: One Shot Fingerprint Singular-Point Detector", "comments": "10 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Singular points of a fingerprint image are special locations having high\ncurvature properties. They can play a pivotal role in fingerprint normalization\nand reliable feature extraction. Accurate and efficient extraction of a\nsingular point plays a major role in successful fingerprint recognition and\nindexing. In this paper, a novel deep learning based architecture is proposed\nfor one shot (end-to-end) singular point detection from an input fingerprint\nimage. The model consists of a Macro-Localization Network and a\nMicro-Regression Network along with three stacked hourglass as a bottleneck.\nThe proposed model has been tested on three databases viz. FVC2002 DB1_A,\nFVC2002 DB2_A and FPL30K and has been found to achieve true detection rate of\n98.75%, 97.5% and 92.72% respectively, which is better than any other\nstate-of-the-art technique.\n", "versions": [{"version": "v1", "created": "Tue, 13 Aug 2019 19:51:58 GMT"}], "update_date": "2019-08-15", "authors_parsed": [["Arora", "Geetika", ""], ["Jha", "Ranjeet Ranjan", ""], ["Agrawal", "Akash", ""], ["Tiwari", "Kamlesh", ""], ["Nigam", "Aditya", ""]]}, {"id": "1908.04913", "submitter": "Jungseock Joo", "authors": "Kimmo K\\\"arkk\\\"ainen, Jungseock Joo", "title": "FairFace: Face Attribute Dataset for Balanced Race, Gender, and Age", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing public face datasets are strongly biased toward Caucasian faces, and\nother races (e.g., Latino) are significantly underrepresented. This can lead to\ninconsistent model accuracy, limit the applicability of face analytic systems\nto non-White race groups, and adversely affect research findings based on such\nskewed data. To mitigate the race bias in these datasets, we construct a novel\nface image dataset, containing 108,501 images, with an emphasis of balanced\nrace composition in the dataset. We define 7 race groups: White, Black, Indian,\nEast Asian, Southeast Asian, Middle East, and Latino. Images were collected\nfrom the YFCC-100M Flickr dataset and labeled with race, gender, and age\ngroups. Evaluations were performed on existing face attribute datasets as well\nas novel image datasets to measure generalization performance. We find that the\nmodel trained from our dataset is substantially more accurate on novel datasets\nand the accuracy is consistent between race and gender groups.\n", "versions": [{"version": "v1", "created": "Wed, 14 Aug 2019 01:42:41 GMT"}], "update_date": "2019-08-15", "authors_parsed": [["K\u00e4rkk\u00e4inen", "Kimmo", ""], ["Joo", "Jungseock", ""]]}, {"id": "1908.04915", "submitter": "Lin Xu", "authors": "Shiyang Yan, Jun Xu, Yuai Liu, and Lin Xu", "title": "HorNet: A Hierarchical Offshoot Recurrent Network for Improving Person\n  Re-ID via Image Captioning", "comments": "10 pages, 5 figures, published in IJCAI19", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Person re-identification (re-ID) aims to recognize a person-of-interest\nacross different cameras with notable appearance variance. Existing research\nworks focused on the capability and robustness of visual representation. In\nthis paper, instead, we propose a novel hierarchical offshoot recurrent network\n(HorNet) for improving person re-ID via image captioning. Image captions are\nsemantically richer and more consistent than visual attributes, which could\nsignificantly alleviate the variance. We use the similarity preserving\ngenerative adversarial network (SPGAN) and an image captioner to fulfill domain\ntransfer and language descriptions generation. Then the proposed HorNet can\nlearn the visual and language representation from both the images and captions\njointly, and thus enhance the performance of person re-ID. Extensive\nexperiments are conducted on several benchmark datasets with or without image\ncaptions, i.e., CUHK03, Market-1501, and Duke-MTMC, demonstrating the\nsuperiority of the proposed method. Our method can generate and extract\nmeaningful image captions while achieving state-of-the-art performance.\n", "versions": [{"version": "v1", "created": "Wed, 14 Aug 2019 01:44:50 GMT"}], "update_date": "2019-08-15", "authors_parsed": [["Yan", "Shiyang", ""], ["Xu", "Jun", ""], ["Liu", "Yuai", ""], ["Xu", "Lin", ""]]}, {"id": "1908.04917", "submitter": "Ya Zhao", "authors": "Ya Zhao, Rui Xu, Mingli Song", "title": "A Cascade Sequence-to-Sequence Model for Chinese Mandarin Lip Reading", "comments": "ACM MM Asia 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Lip reading aims at decoding texts from the movement of a speaker's mouth. In\nrecent years, lip reading methods have made great progress for English, at both\nword-level and sentence-level. Unlike English, however, Chinese Mandarin is a\ntone-based language and relies on pitches to distinguish lexical or grammatical\nmeaning, which significantly increases the ambiguity for the lip reading task.\nIn this paper, we propose a Cascade Sequence-to-Sequence Model for Chinese\nMandarin (CSSMCM) lip reading, which explicitly models tones when predicting\nsentence. Tones are modeled based on visual information and syntactic\nstructure, and are used to predict sentence along with visual information and\nsyntactic structure. In order to evaluate CSSMCM, a dataset called CMLR\n(Chinese Mandarin Lip Reading) is collected and released, consisting of over\n100,000 natural sentences from China Network Television website. When trained\non CMLR dataset, the proposed CSSMCM surpasses the performance of\nstate-of-the-art lip reading frameworks, which confirms the effectiveness of\nexplicit modeling of tones for Chinese Mandarin lip reading.\n", "versions": [{"version": "v1", "created": "Wed, 14 Aug 2019 01:49:32 GMT"}, {"version": "v2", "created": "Thu, 28 Nov 2019 01:31:38 GMT"}], "update_date": "2019-12-02", "authors_parsed": [["Zhao", "Ya", ""], ["Xu", "Rui", ""], ["Song", "Mingli", ""]]}, {"id": "1908.04919", "submitter": "Qingzhong Wang", "authors": "Qingzhong Wang and Antoni B. Chan", "title": "Towards Diverse and Accurate Image Captions via Reinforcing\n  Determinantal Point Process", "comments": "14 pages. Code is comming soon,please pay attention to my personal\n  page visal.cs.cityu.edu.hk/people/qingzhong-wang/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although significant progress has been made in the field of automatic image\ncaptioning, it is still a challenging task. Previous works normally pay much\nattention to improving the quality of the generated captions but ignore the\ndiversity of captions. In this paper, we combine determinantal point process\n(DPP) and reinforcement learning (RL) and propose a novel reinforcing DPP\n(R-DPP) approach to generate a set of captions with high quality and diversity\nfor an image. We show that R-DPP performs better on accuracy and diversity than\nusing noise as a control signal (GANs, VAEs). Moreover, R-DPP is able to\npreserve the modes of the learned distribution. Hence, beam search algorithm\ncan be applied to generate a single accurate caption, which performs better\nthan other RL-based models.\n", "versions": [{"version": "v1", "created": "Wed, 14 Aug 2019 01:51:49 GMT"}], "update_date": "2019-08-15", "authors_parsed": [["Wang", "Qingzhong", ""], ["Chan", "Antoni B.", ""]]}, {"id": "1908.04929", "submitter": "Uehwan Kim", "authors": "Ue-Hwan Kim, Jin-Man Park, Taek-Jin Song and Jong-Hwan Kim", "title": "3-D Scene Graph: A Sparse and Semantic Representation of Physical\n  Environments for Intelligent Agents", "comments": "Early Access", "journal-ref": null, "doi": "10.1109/TCYB.2019.2931042", "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Intelligent agents gather information and perceive semantics within the\nenvironments before taking on given tasks. The agents store the collected\ninformation in the form of environment models that compactly represent the\nsurrounding environments. The agents, however, can only conduct limited tasks\nwithout an efficient and effective environment model. Thus, such an environment\nmodel takes a crucial role for the autonomy systems of intelligent agents. We\nclaim the following characteristics for a versatile environment model:\naccuracy, applicability, usability, and scalability. Although a number of\nresearchers have attempted to develop such models that represent environments\nprecisely to a certain degree, they lack broad applicability, intuitive\nusability, and satisfactory scalability. To tackle these limitations, we\npropose 3-D scene graph as an environment model and the 3-D scene graph\nconstruction framework. The concise and widely used graph structure readily\nguarantees usability as well as scalability for 3-D scene graph. We demonstrate\nthe accuracy and applicability of the 3-D scene graph by exhibiting the\ndeployment of the 3-D scene graph in practical applications. Moreover, we\nverify the performance of the proposed 3-D scene graph and the framework by\nconducting a series of comprehensive experiments under various conditions.\n", "versions": [{"version": "v1", "created": "Wed, 14 Aug 2019 02:30:05 GMT"}], "update_date": "2019-08-15", "authors_parsed": [["Kim", "Ue-Hwan", ""], ["Park", "Jin-Man", ""], ["Song", "Taek-Jin", ""], ["Kim", "Jong-Hwan", ""]]}, {"id": "1908.04930", "submitter": "Rafael Felix", "authors": "Rafael Felix, Ben Harwood, Michele Sasdelli, Gustavo Carneiro", "title": "Generalised Zero-Shot Learning with Domain Classification in a Joint\n  Semantic and Visual Space", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generalised zero-shot learning (GZSL) is a classification problem where the\nlearning stage relies on a set of seen visual classes and the inference stage\naims to identify both the seen visual classes and a new set of unseen visual\nclasses. Critically, both the learning and inference stages can leverage a\nsemantic representation that is available for the seen and unseen classes. Most\nstate-of-the-art GZSL approaches rely on a mapping between latent visual and\nsemantic spaces without considering if a particular sample belongs to the set\nof seen or unseen classes. In this paper, we propose a novel GZSL method that\nlearns a joint latent representation that combines both visual and semantic\ninformation. This mitigates the need for learning a mapping between the two\nspaces. Our method also introduces a domain classification that estimates\nwhether a sample belongs to a seen or an unseen class. Our classifier then\ncombines a class discriminator with this domain classifier with the goal of\nreducing the natural bias that GZSL approaches have toward the seen classes.\nExperiments show that our method achieves state-of-the-art results in terms of\nharmonic mean, the area under the seen and unseen curve and unseen\nclassification accuracy on public GZSL benchmark data sets. Our code will be\navailable upon acceptance of this paper.\n", "versions": [{"version": "v1", "created": "Wed, 14 Aug 2019 02:30:51 GMT"}], "update_date": "2019-08-15", "authors_parsed": [["Felix", "Rafael", ""], ["Harwood", "Ben", ""], ["Sasdelli", "Michele", ""], ["Carneiro", "Gustavo", ""]]}, {"id": "1908.04950", "submitter": "C\\u{a}t\\u{a}lina Cangea", "authors": "C\\u{a}t\\u{a}lina Cangea, Eugene Belilovsky, Pietro Li\\`o, Aaron\n  Courville", "title": "VideoNavQA: Bridging the Gap between Visual and Embodied Question\n  Answering", "comments": "To appear at BMVC 2019. 15 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Embodied Question Answering (EQA) is a recently proposed task, where an agent\nis placed in a rich 3D environment and must act based solely on its egocentric\ninput to answer a given question. The desired outcome is that the agent learns\nto combine capabilities such as scene understanding, navigation and language\nunderstanding in order to perform complex reasoning in the visual world.\nHowever, initial advancements combining standard vision and language methods\nwith imitation and reinforcement learning algorithms have shown EQA might be\ntoo complex and challenging for these techniques. In order to investigate the\nfeasibility of EQA-type tasks, we build the VideoNavQA dataset that contains\npairs of questions and videos generated in the House3D environment. The goal of\nthis dataset is to assess question-answering performance from nearly-ideal\nnavigation paths, while considering a much more complete variety of questions\nthan current instantiations of the EQA task. We investigate several models,\nadapted from popular VQA methods, on this new benchmark. This establishes an\ninitial understanding of how well VQA-style methods can perform within this\nnovel EQA paradigm.\n", "versions": [{"version": "v1", "created": "Wed, 14 Aug 2019 04:44:26 GMT"}], "update_date": "2019-08-15", "authors_parsed": [["Cangea", "C\u0103t\u0103lina", ""], ["Belilovsky", "Eugene", ""], ["Li\u00f2", "Pietro", ""], ["Courville", "Aaron", ""]]}, {"id": "1908.04951", "submitter": "Qing Yu", "authors": "Qing Yu, Kiyoharu Aizawa", "title": "Unsupervised Out-of-Distribution Detection by Maximum Classifier\n  Discrepancy", "comments": null, "journal-ref": "ICCV2019", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Since deep learning models have been implemented in many commercial\napplications, it is important to detect out-of-distribution (OOD) inputs\ncorrectly to maintain the performance of the models, ensure the quality of the\ncollected data, and prevent the applications from being used for\nother-than-intended purposes. In this work, we propose a two-head deep\nconvolutional neural network (CNN) and maximize the discrepancy between the two\nclassifiers to detect OOD inputs. We train a two-head CNN consisting of one\ncommon feature extractor and two classifiers which have different decision\nboundaries but can classify in-distribution (ID) samples correctly. Unlike\nprevious methods, we also utilize unlabeled data for unsupervised training and\nwe use these unlabeled data to maximize the discrepancy between the decision\nboundaries of two classifiers to push OOD samples outside the manifold of the\nin-distribution (ID) samples, which enables us to detect OOD samples that are\nfar from the support of the ID samples. Overall, our approach significantly\noutperforms other state-of-the-art methods on several OOD detection benchmarks\nand two cases of real-world simulation.\n", "versions": [{"version": "v1", "created": "Wed, 14 Aug 2019 04:44:37 GMT"}], "update_date": "2019-09-10", "authors_parsed": [["Yu", "Qing", ""], ["Aizawa", "Kiyoharu", ""]]}, {"id": "1908.04955", "submitter": "Joseph Campbell", "authors": "Joseph Campbell, Simon Stepputtis, Heni Ben Amor", "title": "Probabilistic Multimodal Modeling for Human-Robot Interaction Tasks", "comments": "Project website:\n  http://interactive-robotics.engineering.asu.edu/interaction-primitives\n  Accompanying video: https://youtu.be/r5AqfxTDfLA", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human-robot interaction benefits greatly from multimodal sensor inputs as\nthey enable increased robustness and generalization accuracy. Despite this\nobservation, few HRI methods are capable of efficiently performing inference\nfor multimodal systems. In this work, we introduce a reformulation of\nInteraction Primitives which allows for learning from demonstration of\ninteraction tasks, while also gracefully handling nonlinearities inherent to\nmultimodal inference in such scenarios. We also empirically show that our\nmethod results in more accurate, more robust, and faster inference than\nstandard Interaction Primitives and other common methods in challenging HRI\nscenarios.\n", "versions": [{"version": "v1", "created": "Wed, 14 Aug 2019 04:58:20 GMT"}], "update_date": "2019-08-15", "authors_parsed": [["Campbell", "Joseph", ""], ["Stepputtis", "Simon", ""], ["Amor", "Heni Ben", ""]]}, {"id": "1908.04964", "submitter": "Anbang Yao", "authors": "Jiahui Zhang, Dawei Sun, Zixin Luo, Anbang Yao, Lei Zhou, Tianwei\n  Shen, Yurong Chen, Long Quan, Hongen Liao", "title": "Learning Two-View Correspondences and Geometry Using Order-Aware Network", "comments": "Accepted to ICCV 2019, and Winner solution to both tracks of CVPR IMW\n  2019 Challenge. Code will be available soon at\n  https://github.com/zjhthu/OANet.git", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CG cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Establishing correspondences between two images requires both local and\nglobal spatial context. Given putative correspondences of feature points in two\nviews, in this paper, we propose Order-Aware Network, which infers the\nprobabilities of correspondences being inliers and regresses the relative pose\nencoded by the essential matrix. Specifically, this proposed network is built\nhierarchically and comprises three novel operations. First, to capture the\nlocal context of sparse correspondences, the network clusters unordered input\ncorrespondences by learning a soft assignment matrix. These clusters are in a\ncanonical order and invariant to input permutations. Next, the clusters are\nspatially correlated to form the global context of correspondences. After that,\nthe context-encoded clusters are recovered back to the original size through a\nproposed upsampling operator. We intensively experiment on both outdoor and\nindoor datasets. The accuracy of the two-view geometry and correspondences are\nsignificantly improved over the state-of-the-arts. Code will be available at\nhttps://github.com/zjhthu/OANet.git.\n", "versions": [{"version": "v1", "created": "Wed, 14 Aug 2019 05:42:18 GMT"}], "update_date": "2019-08-15", "authors_parsed": [["Zhang", "Jiahui", ""], ["Sun", "Dawei", ""], ["Luo", "Zixin", ""], ["Yao", "Anbang", ""], ["Zhou", "Lei", ""], ["Shen", "Tianwei", ""], ["Chen", "Yurong", ""], ["Quan", "Long", ""], ["Liao", "Hongen", ""]]}, {"id": "1908.04968", "submitter": "Avisek Lahiri", "authors": "Avisek Lahiri, Arnav Kumar Jain, Divyasri Nadendla, Prabir Kumar\n  Biswas", "title": "Faster Unsupervised Semantic Inpainting: A GAN Based Approach", "comments": "Accepted as full paper at IEEE ICIP, 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose to improve the inference speed and visual quality\nof contemporary baseline of Generative Adversarial Networks (GAN) based\nunsupervised semantic inpainting. This is made possible with better\ninitialization of the core iterative optimization involved in the framework. To\nour best knowledge, this is also the first attempt of GAN based video\ninpainting with consideration to temporal cues. On single image inpainting, we\nachieve about 4.5-5$\\times$ speedup and 80$\\times$ on videos compared to\nbaseline. Simultaneously, our method has better spatial and temporal\nreconstruction qualities as found on three image and one video dataset.\n", "versions": [{"version": "v1", "created": "Wed, 14 Aug 2019 06:06:37 GMT"}], "update_date": "2019-08-15", "authors_parsed": [["Lahiri", "Avisek", ""], ["Jain", "Arnav Kumar", ""], ["Nadendla", "Divyasri", ""], ["Biswas", "Prabir Kumar", ""]]}, {"id": "1908.04979", "submitter": "Guoli Song", "authors": "Guoli Song, Shuhui Wang, Qingming Huang, Qi Tian", "title": "Harmonized Multimodal Learning with Gaussian Process Latent Variable\n  Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multimodal learning aims to discover the relationship between multiple\nmodalities. It has become an important research topic due to extensive\nmultimodal applications such as cross-modal retrieval. This paper attempts to\naddress the modality heterogeneity problem based on Gaussian process latent\nvariable models (GPLVMs) to represent multimodal data in a common space.\nPrevious multimodal GPLVM extensions generally adopt individual learning\nschemes on latent representations and kernel hyperparameters, which ignore\ntheir intrinsic relationship. To exploit strong complementarity among different\nmodalities and GPLVM components, we develop a novel learning scheme called\nHarmonization, where latent model parameters are jointly learned from each\nother. Beyond the correlation fitting or intra-modal structure preservation\nparadigms widely used in existing studies, the harmonization is derived in a\nmodel-driven manner to encourage the agreement between modality-specific GP\nkernels and the similarity of latent representations. We present a range of\nmultimodal learning models by incorporating the harmonization mechanism into\nseveral representative GPLVM-based approaches. Experimental results on four\nbenchmark datasets show that the proposed models outperform the strong\nbaselines for cross-modal retrieval tasks, and that the harmonized multimodal\nlearning method is superior in discovering semantically consistent latent\nrepresentation.\n", "versions": [{"version": "v1", "created": "Wed, 14 Aug 2019 06:40:28 GMT"}], "update_date": "2019-08-15", "authors_parsed": [["Song", "Guoli", ""], ["Wang", "Shuhui", ""], ["Huang", "Qingming", ""], ["Tian", "Qi", ""]]}, {"id": "1908.04992", "submitter": "Suichan Li", "authors": "Suichan Li, Dapeng Chen, Bin Liu, Nenghai Yu, Rui Zhao", "title": "Memory-Based Neighbourhood Embedding for Visual Recognition", "comments": "Accepted by ICCV2019 for oral presentation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning discriminative image feature embeddings is of great importance to\nvisual recognition. To achieve better feature embeddings, most current methods\nfocus on designing different network structures or loss functions, and the\nestimated feature embeddings are usually only related to the input images. In\nthis paper, we propose Memory-based Neighbourhood Embedding (MNE) to enhance a\ngeneral CNN feature by considering its neighbourhood. The method aims to solve\ntwo critical problems, i.e., how to acquire more relevant neighbours in the\nnetwork training and how to aggregate the neighbourhood information for a more\ndiscriminative embedding. We first augment an episodic memory module into the\nnetwork, which can provide more relevant neighbours for both training and\ntesting. Then the neighbours are organized in a tree graph with the target\ninstance as the root node. The neighbourhood information is gradually\naggregated to the root node in a bottom-up manner, and aggregation weights are\nsupervised by the class relationships between the nodes. We apply MNE on image\nsearch and few shot learning tasks. Extensive ablation studies demonstrate the\neffectiveness of each component, and our method significantly outperforms the\nstate-of-the-art approaches.\n", "versions": [{"version": "v1", "created": "Wed, 14 Aug 2019 07:19:12 GMT"}], "update_date": "2019-08-15", "authors_parsed": [["Li", "Suichan", ""], ["Chen", "Dapeng", ""], ["Liu", "Bin", ""], ["Yu", "Nenghai", ""], ["Zhao", "Rui", ""]]}, {"id": "1908.05005", "submitter": "Christoph Kamann", "authors": "Christoph Kamann, Carsten Rother", "title": "Benchmarking the Robustness of Semantic Segmentation Models", "comments": "CVPR 2020 camera ready", "journal-ref": null, "doi": "10.1109/CVPR42600.2020.00885", "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When designing a semantic segmentation module for a practical application,\nsuch as autonomous driving, it is crucial to understand the robustness of the\nmodule with respect to a wide range of image corruptions. While there are\nrecent robustness studies for full-image classification, we are the first to\npresent an exhaustive study for semantic segmentation, based on the\nstate-of-the-art model DeepLabv3+. To increase the realism of our study, we\nutilize almost 400,000 images generated from Cityscapes, PASCAL VOC 2012, and\nADE20K. Based on the benchmark study, we gain several new insights. Firstly,\ncontrary to full-image classification, model robustness increases with model\nperformance, in most cases. Secondly, some architecture properties affect\nrobustness significantly, such as a Dense Prediction Cell, which was designed\nto maximize performance on clean data only.\n", "versions": [{"version": "v1", "created": "Wed, 14 Aug 2019 07:51:56 GMT"}, {"version": "v2", "created": "Thu, 5 Mar 2020 10:12:40 GMT"}, {"version": "v3", "created": "Mon, 10 Aug 2020 13:56:01 GMT"}], "update_date": "2020-08-26", "authors_parsed": [["Kamann", "Christoph", ""], ["Rother", "Carsten", ""]]}, {"id": "1908.05006", "submitter": "Jake Lee", "authors": "Jake H. Lee, Kiri L. Wagstaff", "title": "Visualizing Image Content to Explain Novel Image Discovery", "comments": "Under Review", "journal-ref": null, "doi": "10.1007/s10618-020-00700-0", "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The initial analysis of any large data set can be divided into two phases:\n(1) the identification of common trends or patterns and (2) the identification\nof anomalies or outliers that deviate from those trends. We focus on the goal\nof detecting observations with novel content, which can alert us to artifacts\nin the data set or, potentially, the discovery of previously unknown phenomena.\nTo aid in interpreting and diagnosing the novel aspect of these selected\nobservations, we recommend the use of novelty detection methods that generate\nexplanations. In the context of large image data sets, these explanations\nshould highlight what aspect of a given image is new (color, shape, texture,\ncontent) in a human-comprehensible form. We propose DEMUD-VIS, the first method\nfor providing visual explanations of novel image content by employing a\nconvolutional neural network (CNN) to extract image features, a method that\nuses reconstruction error to detect novel content, and an up-convolutional\nnetwork to convert CNN feature representations back into image space. We\ndemonstrate this approach on diverse images from ImageNet, freshwater streams,\nand the surface of Mars.\n", "versions": [{"version": "v1", "created": "Wed, 14 Aug 2019 07:53:05 GMT"}], "update_date": "2020-10-14", "authors_parsed": [["Lee", "Jake H.", ""], ["Wagstaff", "Kiri L.", ""]]}, {"id": "1908.05008", "submitter": "Debayan Deb", "authors": "Debayan Deb, Jianbang Zhang, Anil K. Jain", "title": "AdvFaces: Adversarial Face Synthesis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Face recognition systems have been shown to be vulnerable to adversarial\nexamples resulting from adding small perturbations to probe images. Such\nadversarial images can lead state-of-the-art face recognition systems to\nfalsely reject a genuine subject (obfuscation attack) or falsely match to an\nimpostor (impersonation attack). Current approaches to crafting adversarial\nface images lack perceptual quality and take an unreasonable amount of time to\ngenerate them. We propose, AdvFaces, an automated adversarial face synthesis\nmethod that learns to generate minimal perturbations in the salient facial\nregions via Generative Adversarial Networks. Once AdvFaces is trained, it can\nautomatically generate imperceptible perturbations that can evade\nstate-of-the-art face matchers with attack success rates as high as 97.22% and\n24.30% for obfuscation and impersonation attacks, respectively.\n", "versions": [{"version": "v1", "created": "Wed, 14 Aug 2019 07:58:00 GMT"}], "update_date": "2019-08-15", "authors_parsed": [["Deb", "Debayan", ""], ["Zhang", "Jianbang", ""], ["Jain", "Anil K.", ""]]}, {"id": "1908.05020", "submitter": "Shrey Gadiya", "authors": "Shrey Gadiya, Deepak Anand, Amit Sethi", "title": "Histographs: Graphs in Histopathology", "comments": "5 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spatial arrangement of cells of various types, such as tumor infiltrating\nlymphocytes and the advancing edge of a tumor, are important features for\ndetecting and characterizing cancers. However, convolutional neural networks\n(CNNs) do not explicitly extract intricate features of the spatial arrangements\nof the cells from histopathology images. In this work, we propose to classify\ncancers using graph convolutional networks (GCNs) by modeling a tissue section\nas a multi-attributed spatial graph of its constituent cells. Cells are\ndetected using their nuclei in H&E stained tissue image, and each cell's\nappearance is captured as a multi-attributed high-dimensional vertex feature.\nThe spatial relations between neighboring cells are captured as edge features\nbased on their distances in a graph. We demonstrate the utility of this\napproach by obtaining classification accuracy that is competitive with CNNs,\nspecifically, Inception-v3, on two tasks-cancerous versus non-cancerous and in\nsitu versus invasive-on the BACH breast cancer dataset.\n", "versions": [{"version": "v1", "created": "Wed, 14 Aug 2019 08:56:59 GMT"}], "update_date": "2019-08-15", "authors_parsed": [["Gadiya", "Shrey", ""], ["Anand", "Deepak", ""], ["Sethi", "Amit", ""]]}, {"id": "1908.05024", "submitter": "Xing Wei", "authors": "Shizhou Zhang, Qi Zhang, Yifei Yang, Xing Wei, Peng Wang, Bingliang\n  Jiao, Yanning Zhang", "title": "Person Re-identification in Aerial Imagery", "comments": "IEEE Transactions on Multimedia", "journal-ref": null, "doi": "10.1109/TMM.2020.2977528", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nowadays, with the rapid development of consumer Unmanned Aerial Vehicles\n(UAVs), visual surveillance by utilizing the UAV platform has been very\nattractive. Most of the research works for UAV captured visual data are mainly\nfocused on the tasks of object detection and tracking. However, limited\nattention has been paid to the task of person Re-identification (ReID) which\nhas been widely studied in ordinary surveillance cameras with fixed\nemplacements. In this paper, to facilitate the research of person ReID in\naerial imagery, we collect a large scale airborne person ReID dataset named as\nPerson ReID for Aerial Imagery (PRAI-1581), which consists of 39,461 images of\n1581 person identities. The images of the dataset are shot by two DJI consumer\nUAVs flying at an altitude ranging from 20 to 60 meters above the ground, which\ncovers most of the real UAV surveillance scenarios. In addition, we propose to\nutilize subspace pooling of convolution feature maps to represent the input\nperson images. Our method can learn a discriminative and compact feature\nrepresentation for ReID in aerial imagery and can be trained in an end-to-end\nfashion efficiently. We conduct extensive experiments on the proposed dataset\nand the experimental results demonstrate that re-identify persons in aerial\nimagery is a challenging problem, where our method performs favorably against\nstate of the arts. Our dataset can be accessed via\n\\url{https://github.com/stormyoung/PRAI-1581}.\n", "versions": [{"version": "v1", "created": "Wed, 14 Aug 2019 09:04:39 GMT"}, {"version": "v2", "created": "Sun, 29 Dec 2019 03:07:05 GMT"}, {"version": "v3", "created": "Sun, 12 Apr 2020 14:00:22 GMT"}], "update_date": "2020-04-27", "authors_parsed": [["Zhang", "Shizhou", ""], ["Zhang", "Qi", ""], ["Yang", "Yifei", ""], ["Wei", "Xing", ""], ["Wang", "Peng", ""], ["Jiao", "Bingliang", ""], ["Zhang", "Yanning", ""]]}, {"id": "1908.05033", "submitter": "Ruihao Gong", "authors": "Ruihao Gong, Xianglong Liu, Shenghu Jiang, Tianxiang Li, Peng Hu,\n  Jiazhen Lin, Fengwei Yu, Junjie Yan", "title": "Differentiable Soft Quantization: Bridging Full-Precision and Low-Bit\n  Neural Networks", "comments": "IEEE ICCV 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hardware-friendly network quantization (e.g., binary/uniform quantization)\ncan efficiently accelerate the inference and meanwhile reduce memory\nconsumption of the deep neural networks, which is crucial for model deployment\non resource-limited devices like mobile phones. However, due to the\ndiscreteness of low-bit quantization, existing quantization methods often face\nthe unstable training process and severe performance degradation. To address\nthis problem, in this paper we propose Differentiable Soft Quantization (DSQ)\nto bridge the gap between the full-precision and low-bit networks. DSQ can\nautomatically evolve during training to gradually approximate the standard\nquantization. Owing to its differentiable property, DSQ can help pursue the\naccurate gradients in backward propagation, and reduce the quantization loss in\nforward process with an appropriate clipping range. Extensive experiments over\nseveral popular network structures show that training low-bit neural networks\nwith DSQ can consistently outperform state-of-the-art quantization methods.\nBesides, our first efficient implementation for deploying 2 to 4-bit DSQ on\ndevices with ARM architecture achieves up to 1.7$\\times$ speed up, compared\nwith the open-source 8-bit high-performance inference framework NCNN. [31]\n", "versions": [{"version": "v1", "created": "Wed, 14 Aug 2019 09:22:41 GMT"}], "update_date": "2019-08-15", "authors_parsed": [["Gong", "Ruihao", ""], ["Liu", "Xianglong", ""], ["Jiang", "Shenghu", ""], ["Li", "Tianxiang", ""], ["Hu", "Peng", ""], ["Lin", "Jiazhen", ""], ["Yu", "Fengwei", ""], ["Yan", "Junjie", ""]]}, {"id": "1908.05040", "submitter": "Vincent Christlein", "authors": "Vincent Christlein, Lukas Spranger, Mathias Seuret, Anguelos Nicolaou,\n  Pavel Kr\\'al, Andreas Maier", "title": "Deep Generalized Max Pooling", "comments": "ICDAR'19", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Global pooling layers are an essential part of Convolutional Neural Networks\n(CNN). They are used to aggregate activations of spatial locations to produce a\nfixed-size vector in several state-of-the-art CNNs. Global average pooling or\nglobal max pooling are commonly used for converting convolutional features of\nvariable size images to a fix-sized embedding. However, both pooling layer\ntypes are computed spatially independent: each individual activation map is\npooled and thus activations of different locations are pooled together. In\ncontrast, we propose Deep Generalized Max Pooling that balances the\ncontribution of all activations of a spatially coherent region by re-weighting\nall descriptors so that the impact of frequent and rare ones is equalized. We\nshow that this layer is superior to both average and max pooling on the\nclassification of Latin medieval manuscripts (CLAMM'16, CLAMM'17), as well as\nwriter identification (Historical-WI'17).\n", "versions": [{"version": "v1", "created": "Wed, 14 Aug 2019 09:33:41 GMT"}], "update_date": "2019-08-15", "authors_parsed": [["Christlein", "Vincent", ""], ["Spranger", "Lukas", ""], ["Seuret", "Mathias", ""], ["Nicolaou", "Anguelos", ""], ["Kr\u00e1l", "Pavel", ""], ["Maier", "Andreas", ""]]}, {"id": "1908.05054", "submitter": "Chris Alberti", "authors": "Chris Alberti, Jeffrey Ling, Michael Collins, David Reitter", "title": "Fusion of Detected Objects in Text for Visual Question Answering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To advance models of multimodal context, we introduce a simple yet powerful\nneural architecture for data that combines vision and natural language. The\n\"Bounding Boxes in Text Transformer\" (B2T2) also leverages referential\ninformation binding words to portions of the image in a single unified\narchitecture. B2T2 is highly effective on the Visual Commonsense Reasoning\nbenchmark (https://visualcommonsense.com), achieving a new state-of-the-art\nwith a 25% relative reduction in error rate compared to published baselines and\nobtaining the best performance to date on the public leaderboard (as of May 22,\n2019). A detailed ablation analysis shows that the early integration of the\nvisual features into the text analysis is key to the effectiveness of the new\narchitecture. A reference implementation of our models is provided\n(https://github.com/google-research/language/tree/master/language/question_answering/b2t2).\n", "versions": [{"version": "v1", "created": "Wed, 14 Aug 2019 10:03:12 GMT"}, {"version": "v2", "created": "Sun, 3 Nov 2019 05:04:09 GMT"}], "update_date": "2019-11-05", "authors_parsed": [["Alberti", "Chris", ""], ["Ling", "Jeffrey", ""], ["Collins", "Michael", ""], ["Reitter", "David", ""]]}, {"id": "1908.05062", "submitter": "Karsten Roth", "authors": "Karsten Roth, J\\\"urgen Hesser and Tomasz Konopczy\\'nski", "title": "Mask Mining for Improved Liver Lesion Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel procedure to improve liver and lesion segmentation from CT\nscans for U-Net based models. Our method extends standard segmentation\npipelines to focus on higher target recall or reduction of noisy false-positive\npredictions, boosting overall segmentation performance. To achieve this, we\ninclude segmentation errors into a new learning process appended to the main\ntraining setup, allowing the model to find features which explain away previous\nerrors. We evaluate this on semantically distinct architectures: cascaded two-\nand three-dimensional as well as combined learning setups for multitask\nsegmentation. Liver and lesion segmentation data are provided by the Liver\nTumor Segmentation challenge (LiTS), with an increase in dice score of up to 2\npoints.\n", "versions": [{"version": "v1", "created": "Wed, 14 Aug 2019 10:33:33 GMT"}, {"version": "v2", "created": "Tue, 3 Dec 2019 19:17:25 GMT"}, {"version": "v3", "created": "Wed, 26 Feb 2020 23:56:57 GMT"}, {"version": "v4", "created": "Wed, 11 Mar 2020 23:10:32 GMT"}], "update_date": "2020-03-13", "authors_parsed": [["Roth", "Karsten", ""], ["Hesser", "J\u00fcrgen", ""], ["Konopczy\u0144ski", "Tomasz", ""]]}, {"id": "1908.05067", "submitter": "Yi-Ting Yeh", "authors": "Yi-Ting Yeh, Tzu-Chuan Lin, Hsiao-Hua Cheng, Yu-Hsuan Deng, Shang-Yu\n  Su and Yun-Nung Chen", "title": "Reactive Multi-Stage Feature Fusion for Multimodal Dialogue Modeling", "comments": "Accepted for a poster session at the DSTC7 workshop at AAAI 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual question answering and visual dialogue tasks have been increasingly\nstudied in the multimodal field towards more practical real-world scenarios. A\nmore challenging task, audio visual scene-aware dialogue (AVSD), is proposed to\nfurther advance the technologies that connect audio, vision, and language,\nwhich introduces temporal video information and dialogue interactions between a\nquestioner and an answerer. This paper proposes an intuitive mechanism that\nfuses features and attention in multiple stages in order to well integrate\nmultimodal features, and the results demonstrate its capability in the\nexperiments. Also, we apply several state-of-the-art models in other tasks to\nthe AVSD task, and further analyze their generalization across different tasks.\n", "versions": [{"version": "v1", "created": "Wed, 14 Aug 2019 10:58:14 GMT"}], "update_date": "2019-08-15", "authors_parsed": [["Yeh", "Yi-Ting", ""], ["Lin", "Tzu-Chuan", ""], ["Cheng", "Hsiao-Hua", ""], ["Deng", "Yu-Hsuan", ""], ["Su", "Shang-Yu", ""], ["Chen", "Yun-Nung", ""]]}, {"id": "1908.05094", "submitter": "Wufeng Xue", "authors": "Xumin Tao and Hongrong Wei and Wufeng Xue and Dong Ni", "title": "Segmentation of Multimodal Myocardial Images Using Shape-Transfer GAN", "comments": "accepted by STACOM 21019", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Myocardium segmentation of late gadolinium enhancement (LGE) Cardiac MR\nimages is important for evaluation of infarction regions in clinical practice.\nThe pathological myocardium in LGE images presents distinctive brightness and\ntextures compared with the healthy tissues, making it much more challenging to\nbe segment. Instead, the balanced-Steady State Free Precession (bSSFP) cine\nimages show clearly boundaries and can be easily segmented. Given this fact, we\npropose a novel shape-transfer GAN for LGE images, which can 1) learn to\ngenerate realistic LGE images from bSSFP with the anatomical shape preserved,\nand 2) learn to segment the myocardium of LGE images from these generated\nimages. It's worth to note that no segmentation label of the LGE images is used\nduring this procedure. We test our model on dataset from the Multi-sequence\nCardiac MR Segmentation Challenge. The results show that the proposed\nShape-Transfer GAN can achieve accurate myocardium masks of LGE images.\n", "versions": [{"version": "v1", "created": "Wed, 14 Aug 2019 12:23:44 GMT"}], "update_date": "2019-08-15", "authors_parsed": [["Tao", "Xumin", ""], ["Wei", "Hongrong", ""], ["Xue", "Wufeng", ""], ["Ni", "Dong", ""]]}, {"id": "1908.05099", "submitter": "Fernando Navarro", "authors": "Fernando Navarro, Suprosanna Shit, Ivan Ezhov, Johannes Paetzold,\n  Andrei Gafita, Jan Peeken, Stephanie Combs and Bjoern Menze", "title": "Shape-Aware Complementary-Task Learning for Multi-Organ Segmentation", "comments": "Accepted in MLMI Workshop 2019 MICCAI", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-organ segmentation in whole-body computed tomography (CT) is a constant\npre-processing step which finds its application in organ-specific image\nretrieval, radiotherapy planning, and interventional image analysis. We address\nthis problem from an organ-specific shape-prior learning perspective. We\nintroduce the idea of complementary-task learning to enforce shape-prior\nleveraging the existing target labels. We propose two complementary-tasks\nnamely i) distance map regression and ii) contour map detection to explicitly\nencode the geometric properties of each organ. We evaluate the proposed\nsolution on the public VISCERAL dataset containing CT scans of multiple organs.\nWe report a significant improvement of overall dice score from 0.8849 to 0.9018\ndue to the incorporation of complementary-task learning.\n", "versions": [{"version": "v1", "created": "Wed, 14 Aug 2019 12:47:58 GMT"}], "update_date": "2019-08-15", "authors_parsed": [["Navarro", "Fernando", ""], ["Shit", "Suprosanna", ""], ["Ezhov", "Ivan", ""], ["Paetzold", "Johannes", ""], ["Gafita", "Andrei", ""], ["Peeken", "Jan", ""], ["Combs", "Stephanie", ""], ["Menze", "Bjoern", ""]]}, {"id": "1908.05104", "submitter": "Weijian Huang", "authors": "Yongjin Zhou, Weijian Huang, Pei Dong, Yong Xia, Shanshan Wang", "title": "D-UNet: a dimension-fusion U shape network for chronic stroke lesion\n  segmentation", "comments": null, "journal-ref": "IEEE/ACM Transactions on Computational Biology and Bioinformatics\n  (2019)", "doi": "10.1109/TCBB.2019.2939522.", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Assessing the location and extent of lesions caused by chronic stroke is\ncritical for medical diagnosis, surgical planning, and prognosis. In recent\nyears, with the rapid development of 2D and 3D convolutional neural networks\n(CNN), the encoder-decoder structure has shown great potential in the field of\nmedical image segmentation. However, the 2D CNN ignores the 3D information of\nmedical images, while the 3D CNN suffers from high computational resource\ndemands. This paper proposes a new architecture called dimension-fusion-UNet\n(D-UNet), which combines 2D and 3D convolution innovatively in the encoding\nstage. The proposed architecture achieves a better segmentation performance\nthan 2D networks, while requiring significantly less computation time in\ncomparison to 3D networks. Furthermore, to alleviate the data imbalance issue\nbetween positive and negative samples for the network training, we propose a\nnew loss function called Enhance Mixing Loss (EML). This function adds a\nweighted focal coefficient and combines two traditional loss functions. The\nproposed method has been tested on the ATLAS dataset and compared to three\nstate-of-the-art methods. The results demonstrate that the proposed method\nachieves the best quality performance in terms of DSC = 0.5349+0.2763 and\nprecision = 0.6331+0.295).\n", "versions": [{"version": "v1", "created": "Wed, 14 Aug 2019 12:54:04 GMT"}], "update_date": "2021-02-17", "authors_parsed": [["Zhou", "Yongjin", ""], ["Huang", "Weijian", ""], ["Dong", "Pei", ""], ["Xia", "Yong", ""], ["Wang", "Shanshan", ""]]}, {"id": "1908.05142", "submitter": "Lei Qi", "authors": "Lei Qi, Lei Wang, Jing Huo, Yinghuan Shi and Yang Gao", "title": "GreyReID: A Two-stream Deep Framework with RGB-grey Information for\n  Person Re-identification", "comments": "Accepted by ACM TOMM", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we observe that most false positive images (i.e., different\nidentities with query images) in the top ranking list usually have the similar\ncolor information with the query image in person re-identification (Re-ID).\nMeanwhile, when we use the greyscale images generated from RGB images to\nconduct the person Re-ID task, some hard query images can obtain better\nperformance compared with using RGB images. Therefore, RGB and greyscale images\nseem to be complementary to each other for person Re-ID. In this paper, we aim\nto utilize both RGB and greyscale images to improve the person Re-ID\nperformance. To this end, we propose a novel two-stream deep neural network\nwith RGB-grey information, which can effectively fuse RGB and greyscale feature\nrepresentations to enhance the generalization ability of Re-ID. Firstly, we\nconvert RGB images to greyscale images in each training batch. Based on these\nRGB and greyscale images, we train the RGB and greyscale branches,\nrespectively. Secondly, to build up connections between RGB and greyscale\nbranches, we merge the RGB and greyscale branches into a new joint branch.\nFinally, we concatenate the features of all three branches as the final feature\nrepresentation for Re-ID. Moreover, in the training process, we adopt the joint\nlearning scheme to simultaneously train each branch by the independent loss\nfunction, which can enhance the generalization ability of each branch. Besides,\na global loss function is utilized to further fine-tune the final concatenated\nfeature. The extensive experiments on multiple benchmark datasets fully show\nthat the proposed method can outperform the state-of-the-art person Re-ID\nmethods. Furthermore, using greyscale images can indeed improve the person\nRe-ID performance.\n", "versions": [{"version": "v1", "created": "Wed, 14 Aug 2019 14:24:34 GMT"}, {"version": "v2", "created": "Fri, 28 Aug 2020 07:42:17 GMT"}], "update_date": "2020-08-31", "authors_parsed": [["Qi", "Lei", ""], ["Wang", "Lei", ""], ["Huo", "Jing", ""], ["Shi", "Yinghuan", ""], ["Gao", "Yang", ""]]}, {"id": "1908.05146", "submitter": "Malte Splietker", "authors": "Malte Splietker and Sven Behnke", "title": "Directional TSDF: Modeling Surface Orientation for Coherent Meshes", "comments": "For supplementary material and videos, see this\n  http://www.ais.uni-bonn.de/videos/IROS_2019_Splietker/", "journal-ref": "IEEE/RSJ International Conference on Intelligent Robots and\n  Systems (IROS), Macau, China, November 2019", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Real-time 3D reconstruction from RGB-D sensor data plays an important role in\nmany robotic applications, such as object modeling and mapping. The popular\nmethod of fusing depth information into a truncated signed distance function\n(TSDF) and applying the marching cubes algorithm for mesh extraction has severe\nissues with thin structures: not only does it lead to loss of accuracy, but it\ncan generate completely wrong surfaces. To address this, we propose the\ndirectional TSDF - a novel representation that stores opposite surfaces\nseparate from each other. The marching cubes algorithm is modified accordingly\nto retrieve a coherent mesh representation. We further increase the accuracy by\nusing surface gradient-based ray casting for fusing new measurements. We show\nthat our method outperforms state-of-the-art TSDF reconstruction algorithms in\nmesh accuracy.\n", "versions": [{"version": "v1", "created": "Wed, 14 Aug 2019 14:27:52 GMT"}], "update_date": "2019-08-15", "authors_parsed": [["Splietker", "Malte", ""], ["Behnke", "Sven", ""]]}, {"id": "1908.05153", "submitter": "Bo Jiang", "authors": "Bo Jiang, Leiling Wang, Jin Tang, Bin Luo", "title": "Semi-supervised Learning with Adaptive Neighborhood Graph Propagation\n  Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph Convolutional Networks (GCNs) have been widely studied for compact data\nrepresentation and semi-supervised learning tasks. However, existing GCNs\nusually use a fixed neighborhood graph which is not guaranteed to be optimal\nfor semi-supervised learning tasks. In this paper, we first re-interpret graph\nconvolution operation in GCNs as a composition of feature propagation and\n(non-linear) transformation. Based on this observation, we then propose a\nunified adaptive neighborhood feature propagation model and derive a novel\nAdaptive Neighborhood Graph Propagation Network (ANGPN) for data representation\nand semi-supervised learning. The aim of ANGPN is to conduct both graph\nconstruction and graph convolution simultaneously and cooperatively in a\nunified formulation and thus can learn an optimal neighborhood graph that best\nserves graph convolution for data representation and semi-supervised learning.\nOne main benefit of ANGPN is that the learned (convolutional) representation\ncan provide useful weakly supervised information for constructing a better\nneighborhood graph which meanwhile facilitates data representation and\nlearning. Experimental results on four benchmark datasets demonstrate the\neffectiveness and benefit of the proposed ANGPN.\n", "versions": [{"version": "v1", "created": "Wed, 14 Aug 2019 14:48:53 GMT"}, {"version": "v2", "created": "Thu, 21 Nov 2019 07:51:14 GMT"}], "update_date": "2019-11-22", "authors_parsed": [["Jiang", "Bo", ""], ["Wang", "Leiling", ""], ["Tang", "Jin", ""], ["Luo", "Bin", ""]]}, {"id": "1908.05168", "submitter": "Pablo Navarrete Michelini", "authors": "Pablo Navarrete Michelini, Hanwen Liu, Yunhua Lu, Xingqun Jiang", "title": "A Tour of Convolutional Networks Guided by Linear Interpreters", "comments": "To appear in ICCV 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NA eess.IV math.NA", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Convolutional networks are large linear systems divided into layers and\nconnected by non-linear units. These units are the \"articulations\" that allow\nthe network to adapt to the input. To understand how a network manages to solve\na problem we must look at the articulated decisions in entirety. If we could\ncapture the actions of non-linear units for a particular input, we would be\nable to replay the whole system back and forth as if it was always linear. It\nwould also reveal the actions of non-linearities because the resulting linear\nsystem, a Linear Interpreter, depends on the input image. We introduce a\nhooking layer, called a LinearScope, which allows us to run the network and the\nlinear interpreter in parallel. Its implementation is simple, flexible and\nefficient. From here we can make many curious inquiries: how do these linear\nsystems look like? When the rows and columns of the transformation matrix are\nimages, how do they look like? What type of basis do these linear\ntransformations rely on? The answers depend on the problems presented, through\nwhich we take a tour to some popular architectures used for classification,\nsuper-resolution (SR) and image-to-image translation (I2I). For classification\nwe observe that popular networks use a pixel-wise vote per class strategy and\nheavily rely on bias parameters. For SR and I2I we find that CNNs use\nwavelet-type basis similar to the human visual system. For I2I we reveal\ncopy-move and template-creation strategies to generate outputs.\n", "versions": [{"version": "v1", "created": "Wed, 14 Aug 2019 15:18:45 GMT"}], "update_date": "2019-11-15", "authors_parsed": [["Michelini", "Pablo Navarrete", ""], ["Liu", "Hanwen", ""], ["Lu", "Yunhua", ""], ["Jiang", "Xingqun", ""]]}, {"id": "1908.05185", "submitter": "Jiangfan Han", "authors": "Jiangfan Han, Xiaoyi Dong, Ruimao Zhang, Dongdong Chen, Weiming Zhang,\n  Nenghai Yu, Ping Luo, Xiaogang Wang", "title": "Once a MAN: Towards Multi-Target Attack via Learning Multi-Target\n  Adversarial Network Once", "comments": "Accepted by ICCV 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern deep neural networks are often vulnerable to adversarial samples.\nBased on the first optimization-based attacking method, many following methods\nare proposed to improve the attacking performance and speed. Recently,\ngeneration-based methods have received much attention since they directly use\nfeed-forward networks to generate the adversarial samples, which avoid the\ntime-consuming iterative attacking procedure in optimization-based and\ngradient-based methods. However, current generation-based methods are only able\nto attack one specific target (category) within one model, thus making them not\napplicable to real classification systems that often have hundreds/thousands of\ncategories. In this paper, we propose the first Multi-target Adversarial\nNetwork (MAN), which can generate multi-target adversarial samples with a\nsingle model. By incorporating the specified category information into the\nintermediate features, it can attack any category of the target classification\nmodel during runtime. Experiments show that the proposed MAN can produce\nstronger attack results and also have better transferability than previous\nstate-of-the-art methods in both multi-target attack task and single-target\nattack task. We further use the adversarial samples generated by our MAN to\nimprove the robustness of the classification model. It can also achieve better\nclassification accuracy than other methods when attacked by various methods.\n", "versions": [{"version": "v1", "created": "Wed, 14 Aug 2019 15:59:21 GMT"}], "update_date": "2019-08-15", "authors_parsed": [["Han", "Jiangfan", ""], ["Dong", "Xiaoyi", ""], ["Zhang", "Ruimao", ""], ["Chen", "Dongdong", ""], ["Zhang", "Weiming", ""], ["Yu", "Nenghai", ""], ["Luo", "Ping", ""], ["Wang", "Xiaogang", ""]]}, {"id": "1908.05195", "submitter": "Seugnju Cho", "authors": "Seungju Cho, Tae Joon Jun, Byungsoo Oh, Daeyoung Kim", "title": "DAPAS : Denoising Autoencoder to Prevent Adversarial attack in Semantic\n  Segmentation", "comments": "Accepted to be published in: 2020 International Joint Conference on\n  Neural Networks (IJCNN), Glasgow, July 19--24, 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nowadays, Deep learning techniques show dramatic performance on computer\nvision area, and they even outperform human. But it is also vulnerable to some\nsmall perturbation called an adversarial attack. This is a problem combined\nwith the safety of artificial intelligence, which has recently been studied a\nlot. These attacks have shown that they can fool models of image\nclassification, semantic segmentation, and object detection. We point out this\nattack can be protected by denoise autoencoder, which is used for denoising the\nperturbation and restoring the original images. We experiment with various\nnoise distributions and verify the effect of denoise autoencoder against\nadversarial attack in semantic segmentation.\n", "versions": [{"version": "v1", "created": "Wed, 14 Aug 2019 16:13:00 GMT"}, {"version": "v2", "created": "Sun, 18 Aug 2019 12:15:53 GMT"}, {"version": "v3", "created": "Mon, 6 Apr 2020 08:06:32 GMT"}, {"version": "v4", "created": "Tue, 7 Apr 2020 07:01:28 GMT"}], "update_date": "2020-04-08", "authors_parsed": [["Cho", "Seungju", ""], ["Jun", "Tae Joon", ""], ["Oh", "Byungsoo", ""], ["Kim", "Daeyoung", ""]]}, {"id": "1908.05217", "submitter": "Hao Yang Dr", "authors": "Hao Yang, Hao Wu, Hao Chen", "title": "Detecting 11K Classes: Large Scale Object Detection without Fine-Grained\n  Bounding Boxes", "comments": "Accepted to ICCV 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in deep learning greatly boost the performance of object\ndetection. State-of-the-art methods such as Faster-RCNN, FPN and R-FCN have\nachieved high accuracy in challenging benchmark datasets. However, these\nmethods require fully annotated object bounding boxes for training, which are\nincredibly hard to scale up due to the high annotation cost. Weakly-supervised\nmethods, on the other hand, only require image-level labels for training, but\nthe performance is far below their fully-supervised counterparts. In this\npaper, we propose a semi-supervised large scale fine-grained detection method,\nwhich only needs bounding box annotations of a smaller number of coarse-grained\nclasses and image-level labels of large scale fine-grained classes, and can\ndetect all classes at nearly fully-supervised accuracy. We achieve this by\nutilizing the correlations between coarse-grained and fine-grained classes with\nshared backbone, soft-attention based proposal re-ranking, and a dual-level\nmemory module. Experiment results show that our methods can achieve close\naccuracy on object detection to state-of-the-art fully-supervised methods on\ntwo large scale datasets, ImageNet and OpenImages, with only a small fraction\nof fully annotated classes.\n", "versions": [{"version": "v1", "created": "Wed, 14 Aug 2019 16:42:15 GMT"}], "update_date": "2019-08-15", "authors_parsed": [["Yang", "Hao", ""], ["Wu", "Hao", ""], ["Chen", "Hao", ""]]}, {"id": "1908.05257", "submitter": "Liwei Wang", "authors": "Tiange Luo, Aoxue Li, Tao Xiang, Weiran Huang, Liwei Wang", "title": "Few-Shot Learning with Global Class Representations", "comments": "Accepted by ICCV2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose to tackle the challenging few-shot learning (FSL)\nproblem by learning global class representations using both base and novel\nclass training samples. In each training episode, an episodic class mean\ncomputed from a support set is registered with the global representation via a\nregistration module. This produces a registered global class representation for\ncomputing the classification loss using a query set. Though following a similar\nepisodic training pipeline as existing meta learning based approaches, our\nmethod differs significantly in that novel class training samples are involved\nin the training from the beginning. To compensate for the lack of novel class\ntraining samples, an effective sample synthesis strategy is developed to avoid\noverfitting. Importantly, by joint base-novel class training, our approach can\nbe easily extended to a more practical yet challenging FSL setting, i.e.,\ngeneralized FSL, where the label space of test data is extended to both base\nand novel classes. Extensive experiments show that our approach is effective\nfor both of the two FSL settings.\n", "versions": [{"version": "v1", "created": "Wed, 14 Aug 2019 17:36:37 GMT"}], "update_date": "2019-08-15", "authors_parsed": [["Luo", "Tiange", ""], ["Li", "Aoxue", ""], ["Xiang", "Tao", ""], ["Huang", "Weiran", ""], ["Wang", "Liwei", ""]]}, {"id": "1908.05263", "submitter": "Honglie Chen", "authors": "Honglie Chen and Weidi Xie and Andrea Vedaldi and Andrew Zisserman", "title": "AutoCorrect: Deep Inductive Alignment of Noisy Geometric Annotations", "comments": "BMVC 2019 (Spotlight)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose AutoCorrect, a method to automatically learn object-annotation\nalignments from a dataset with annotations affected by geometric noise. The\nmethod is based on a consistency loss that enables deep neural networks to be\ntrained, given only noisy annotations as input, to correct the annotations.\nWhen some noise-free annotations are available, we show that the consistency\nloss reduces to a stricter self-supervised loss. We also show that the method\ncan implicitly leverage object symmetries to reduce the ambiguity arising in\ncorrecting noisy annotations. When multiple object-annotation pairs are present\nin an image, we introduce a spatial memory map that allows the network to\ncorrect annotations sequentially, one at a time, while accounting for all other\nannotations in the image and corrections performed so far. Through ablation, we\nshow the benefit of these contributions, demonstrating excellent results on\ngeo-spatial imagery. Specifically, we show results using a new Railway tracks\ndataset as well as the public INRIA Buildings benchmarks, achieving new\nstate-of-the-art results for the latter.\n", "versions": [{"version": "v1", "created": "Wed, 14 Aug 2019 17:40:10 GMT"}], "update_date": "2019-08-15", "authors_parsed": [["Chen", "Honglie", ""], ["Xie", "Weidi", ""], ["Vedaldi", "Andrea", ""], ["Zisserman", "Andrew", ""]]}, {"id": "1908.05293", "submitter": "Rahul Mitra", "authors": "Rahul Mitra, Nitesh B. Gundavarapu, Abhishek Sharma, Arjun Jain", "title": "Multiview-Consistent Semi-Supervised Learning for 3D Human Pose\n  Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The best performing methods for 3D human pose estimation from monocular\nimages require large amounts of in-the-wild 2D and controlled 3D pose annotated\ndatasets which are costly and require sophisticated systems to acquire. To\nreduce this annotation dependency, we propose Multiview-Consistent Semi\nSupervised Learning (MCSS) framework that utilizes similarity in pose\ninformation from unannotated, uncalibrated but synchronized multi-view videos\nof human motions as additional weak supervision signal to guide 3D human pose\nregression. Our framework applies hard-negative mining based on temporal\nrelations in multi-view videos to arrive at a multi-view consistent pose\nembedding. When jointly trained with limited 3D pose annotations, our approach\nimproves the baseline by 25% and state-of-the-art by 8.7%, whilst using\nsubstantially smaller networks. Lastly, but importantly, we demonstrate the\nadvantages of the learned embedding and establish view-invariant pose retrieval\nbenchmarks on two popular, publicly available multi-view human pose datasets,\nHuman 3.6M and MPI-INF-3DHP, to facilitate future research.\n", "versions": [{"version": "v1", "created": "Wed, 14 Aug 2019 18:13:57 GMT"}, {"version": "v2", "created": "Sat, 30 Nov 2019 06:44:56 GMT"}, {"version": "v3", "created": "Tue, 25 Feb 2020 06:14:42 GMT"}], "update_date": "2020-02-26", "authors_parsed": [["Mitra", "Rahul", ""], ["Gundavarapu", "Nitesh B.", ""], ["Sharma", "Abhishek", ""], ["Jain", "Arjun", ""]]}, {"id": "1908.05311", "submitter": "Balamurali Murugesan", "authors": "Balamurali Murugesan, Kaushik Sarveswaran, Sharath M Shankaranarayana,\n  Keerthi Ram, Jayaraj Joseph, Mohanasankar Sivaprakasam", "title": "Conv-MCD: A Plug-and-Play Multi-task Module for Medical Image\n  Segmentation", "comments": "Accepted in MLMI 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For the task of medical image segmentation, fully convolutional network (FCN)\nbased architectures have been extensively used with various modifications. A\nrising trend in these architectures is to employ joint-learning of the target\nregion with an auxiliary task, a method commonly known as multi-task learning.\nThese approaches help impose smoothness and shape priors, which vanilla FCN\napproaches do not necessarily incorporate. In this paper, we propose a novel\nplug-and-play module, which we term as Conv-MCD, which exploits structural\ninformation in two ways - i) using the contour map and ii) using the distance\nmap, both of which can be obtained from ground truth segmentation maps with no\nadditional annotation costs. The key benefit of our module is the ease of its\naddition to any state-of-the-art architecture, resulting in a significant\nimprovement in performance with a minimal increase in parameters. To\nsubstantiate the above claim, we conduct extensive experiments using 4\nstate-of-the-art architectures across various evaluation metrics, and report a\nsignificant increase in performance in relation to the base networks. In\naddition to the aforementioned experiments, we also perform ablative studies\nand visualization of feature maps to further elucidate our approach.\n", "versions": [{"version": "v1", "created": "Wed, 14 Aug 2019 18:54:52 GMT"}], "update_date": "2019-08-16", "authors_parsed": [["Murugesan", "Balamurali", ""], ["Sarveswaran", "Kaushik", ""], ["Shankaranarayana", "Sharath M", ""], ["Ram", "Keerthi", ""], ["Joseph", "Jayaraj", ""], ["Sivaprakasam", "Mohanasankar", ""]]}, {"id": "1908.05317", "submitter": "Moi Hoon Yap", "authors": "Manu Goyal and Neil Reeves and Satyan Rajbhandari and Naseer Ahmad and\n  Chuan Wang and Moi Hoon Yap", "title": "Recognition of Ischaemia and Infection in Diabetic Foot Ulcers: Dataset\n  and Techniques", "comments": "25 pages, 13 figures and 3 tables", "journal-ref": "Computers in Biology and Medicine, Volume 117, February 2020,\n  103616", "doi": "10.1016/j.compbiomed.2020.103616", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recognition and analysis of Diabetic Foot Ulcers (DFU) using computerized\nmethods is an emerging research area with the evolution of image-based machine\nlearning algorithms. Existing research using visual computerized methods mainly\nfocuses on recognition, detection, and segmentation of the visual appearance of\nthe DFU as well as tissue classification. According to DFU medical\nclassification systems, the presence of infection (bacteria in the wound) and\nischaemia (inadequate blood supply) has important clinical implications for DFU\nassessment, which are used to predict the risk of amputation. In this work, we\npropose a new dataset and computer vision techniques to identify the presence\nof infection and ischaemia in DFU. This is the first time a DFU dataset with\nground truth labels of ischaemia and infection cases is introduced for research\npurposes. For the handcrafted machine learning approach, we propose a new\nfeature descriptor, namely the Superpixel Color Descriptor. Then we use the\nEnsemble Convolutional Neural Network (CNN) model for more effective\nrecognition of ischaemia and infection. We propose to use a natural\ndata-augmentation method, which identifies the region of interest on foot\nimages and focuses on finding the salient features existing in this area.\nFinally, we evaluate the performance of our proposed techniques on binary\nclassification, i.e. ischaemia versus non-ischaemia and infection versus\nnon-infection. Overall, our method performed better in the classification of\nischaemia than infection. We found that our proposed Ensemble CNN deep learning\nalgorithms performed better for both classification tasks as compared to\nhandcrafted machine learning algorithms, with 90% accuracy in ischaemia\nclassification and 73% in infection classification.\n", "versions": [{"version": "v1", "created": "Wed, 14 Aug 2019 19:11:36 GMT"}, {"version": "v2", "created": "Wed, 11 Sep 2019 16:30:22 GMT"}, {"version": "v3", "created": "Sun, 27 Oct 2019 09:53:40 GMT"}, {"version": "v4", "created": "Sat, 8 Feb 2020 20:44:57 GMT"}], "update_date": "2020-02-11", "authors_parsed": [["Goyal", "Manu", ""], ["Reeves", "Neil", ""], ["Rajbhandari", "Satyan", ""], ["Ahmad", "Naseer", ""], ["Wang", "Chuan", ""], ["Yap", "Moi Hoon", ""]]}, {"id": "1908.05324", "submitter": "Qicheng Lao", "authors": "Qicheng Lao, Mohammad Havaei, Ahmad Pesaranghader, Francis Dutil, Lisa\n  Di Jorio and Thomas Fevens", "title": "Dual Adversarial Inference for Text-to-Image Synthesis", "comments": "Accepted to ICCV 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Synthesizing images from a given text description involves engaging two types\nof information: the content, which includes information explicitly described in\nthe text (e.g., color, composition, etc.), and the style, which is usually not\nwell described in the text (e.g., location, quantity, size, etc.). However, in\nprevious works, it is typically treated as a process of generating images only\nfrom the content, i.e., without considering learning meaningful style\nrepresentations. In this paper, we aim to learn two variables that are\ndisentangled in the latent space, representing content and style respectively.\nWe achieve this by augmenting current text-to-image synthesis frameworks with a\ndual adversarial inference mechanism. Through extensive experiments, we show\nthat our model learns, in an unsupervised manner, style representations\ncorresponding to certain meaningful information present in the image that are\nnot well described in the text. The new framework also improves the quality of\nsynthesized images when evaluated on Oxford-102, CUB and COCO datasets.\n", "versions": [{"version": "v1", "created": "Wed, 14 Aug 2019 19:35:02 GMT"}], "update_date": "2019-08-16", "authors_parsed": [["Lao", "Qicheng", ""], ["Havaei", "Mohammad", ""], ["Pesaranghader", "Ahmad", ""], ["Dutil", "Francis", ""], ["Di Jorio", "Lisa", ""], ["Fevens", "Thomas", ""]]}, {"id": "1908.05338", "submitter": "Mostafa Mehdipour Ghazi", "authors": "Mostafa Mehdipour Ghazi, Mads Nielsen, Akshay Pai, Marc Modat, M.\n  Jorge Cardoso, S\\'ebastien Ourselin, Lauge S{\\o}rensen", "title": "Robust parametric modeling of Alzheimer's disease progression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quantitative characterization of disease progression using longitudinal data\ncan provide long-term predictions for the pathological stages of individuals.\nThis work studies the robust modeling of Alzheimer's disease progression using\nparametric methods. The proposed method linearly maps the individual's age to a\ndisease progression score (DPS) and jointly fits constrained generalized\nlogistic functions to the longitudinal dynamics of biomarkers as functions of\nthe DPS using M-estimation. Robustness of the estimates is quantified using\nbootstrapping via Monte Carlo resampling, and the estimated inflection points\nof the fitted functions are used to temporally order the modeled biomarkers in\nthe disease course. Kernel density estimation is applied to the obtained DPSs\nfor clinical status classification using a Bayesian classifier. Different\nM-estimators and logistic functions, including a novel type proposed in this\nstudy, called modified Stannard, are evaluated on the data from the Alzheimer's\nDisease Neuroimaging Initiative (ADNI) for robust modeling of volumetric MRI\nand PET biomarkers, CSF measurements, as well as cognitive tests. The results\nshow that the modified Stannard function fitted using the logistic loss\nachieves the best modeling performance with an average normalized MAE of 0.991\nacross all biomarkers and bootstraps. Applied to the ADNI test set, this model\nachieves a multiclass AUC of 0.934 in clinical status classification. The\nobtained results for the proposed model outperform almost all state-of-the-art\nresults in predicting biomarker values and classifying clinical status.\nFinally, the experiments show that the proposed model, trained using abundant\nADNI data, generalizes well to data from the National Alzheimer's Coordinating\nCenter (NACC) with an average normalized MAE of 1.182 and a multiclass AUC of\n0.929.\n", "versions": [{"version": "v1", "created": "Wed, 14 Aug 2019 20:26:21 GMT"}, {"version": "v2", "created": "Sat, 23 Nov 2019 16:58:15 GMT"}, {"version": "v3", "created": "Thu, 18 Jun 2020 14:38:05 GMT"}], "update_date": "2020-06-19", "authors_parsed": [["Ghazi", "Mostafa Mehdipour", ""], ["Nielsen", "Mads", ""], ["Pai", "Akshay", ""], ["Modat", "Marc", ""], ["Cardoso", "M. Jorge", ""], ["Ourselin", "S\u00e9bastien", ""], ["S\u00f8rensen", "Lauge", ""]]}, {"id": "1908.05343", "submitter": "Jelmer Wolterink", "authors": "Jelmer M. Wolterink and Tim Leiner and Ivana I\\v{s}gum", "title": "Graph Convolutional Networks for Coronary Artery Segmentation in Cardiac\n  CT Angiography", "comments": "MICCAI 2019 Workshop on Graph Learning in Medical Image (GLMI)", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Detection of coronary artery stenosis in coronary CT angiography (CCTA)\nrequires highly personalized surface meshes enclosing the coronary lumen. In\nthis work, we propose to use graph convolutional networks (GCNs) to predict the\nspatial location of vertices in a tubular surface mesh that segments the\ncoronary artery lumen. Predictions for individual vertex locations are based on\nlocal image features as well as on features of neighboring vertices in the mesh\ngraph. The method was trained and evaluated using the publicly available\nCoronary Artery Stenoses Detection and Quantification Evaluation Framework.\nSurface meshes enclosing the full coronary artery tree were automatically\nextracted. A quantitative evaluation on 78 coronary artery segments showed that\nthese meshes corresponded closely to reference annotations, with a Dice\nsimilarity coefficient of 0.75/0.73, a mean surface distance of 0.25/0.28 mm,\nand a Hausdorff distance of 1.53/1.86 mm in healthy/diseased vessel segments.\nThe results showed that inclusion of mesh information in a GCN improves\nsegmentation overlap and accuracy over a baseline model without interaction on\nthe mesh. The results indicate that GCNs allow efficient extraction of coronary\nartery surface meshes and that the use of GCNs leads to regular and more\naccurate meshes.\n", "versions": [{"version": "v1", "created": "Wed, 14 Aug 2019 20:48:27 GMT"}], "update_date": "2019-08-16", "authors_parsed": [["Wolterink", "Jelmer M.", ""], ["Leiner", "Tim", ""], ["I\u0161gum", "Ivana", ""]]}, {"id": "1908.05349", "submitter": "Wei Liu", "authors": "Wei Liu, Jie-Lin Qiu, Wei-Long Zheng, and Bao-Liang Lu", "title": "Multimodal Emotion Recognition Using Deep Canonical Correlation Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multimodal signals are more powerful than unimodal data for emotion\nrecognition since they can represent emotions more comprehensively. In this\npaper, we introduce deep canonical correlation analysis (DCCA) to multimodal\nemotion recognition. The basic idea behind DCCA is to transform each modality\nseparately and coordinate different modalities into a hyperspace by using\nspecified canonical correlation analysis constraints. We evaluate the\nperformance of DCCA on five multimodal datasets: the SEED, SEED-IV, SEED-V,\nDEAP, and DREAMER datasets. Our experimental results demonstrate that DCCA\nachieves state-of-the-art recognition accuracy rates on all five datasets:\n94.58% on the SEED dataset, 87.45% on the SEED-IV dataset, 84.33% and 85.62%\nfor two binary classification tasks and 88.51% for a four-category\nclassification task on the DEAP dataset, 83.08% on the SEED-V dataset, and\n88.99%, 90.57%, and 90.67% for three binary classification tasks on the DREAMER\ndataset. We also compare the noise robustness of DCCA with that of existing\nmethods when adding various amounts of noise to the SEED-V dataset. The\nexperimental results indicate that DCCA has greater robustness. By visualizing\nfeature distributions with t-SNE and calculating the mutual information between\ndifferent modalities before and after using DCCA, we find that the features\ntransformed by DCCA from different modalities are more homogeneous and\ndiscriminative across emotions.\n", "versions": [{"version": "v1", "created": "Tue, 13 Aug 2019 09:22:23 GMT"}], "update_date": "2019-08-16", "authors_parsed": [["Liu", "Wei", ""], ["Qiu", "Jie-Lin", ""], ["Zheng", "Wei-Long", ""], ["Lu", "Bao-Liang", ""]]}, {"id": "1908.05389", "submitter": "Junkun Jiang", "authors": "Junkun Jiang, Ruomei Wang, Shujin Lin, Fei Wang", "title": "SFSegNet: Parse Freehand Sketches using Deep Fully Convolutional\n  Networks", "comments": "Accepted for the 2019 International Joint Conference on Neural\n  Networks (IJCNN-19)", "journal-ref": null, "doi": "10.1109/IJCNN.2019.8851974", "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Parsing sketches via semantic segmentation is attractive but challenging,\nbecause (i) free-hand drawings are abstract with large variances in depicting\nobjects due to different drawing styles and skills; (ii) distorting lines drawn\non the touchpad make sketches more difficult to be recognized; (iii) the\nhigh-performance image segmentation via deep learning technologies needs\nenormous annotated sketch datasets during the training stage. In this paper, we\npropose a Sketch-target deep FCN Segmentation Network(SFSegNet) for automatic\nfree-hand sketch segmentation, labeling each sketch in a single object with\nmultiple parts. SFSegNet has an end-to-end network process between the input\nsketches and the segmentation results, composed of 2 parts: (i) a modified deep\nFully Convolutional Network(FCN) using a reweighting strategy to ignore\nbackground pixels and classify which part each pixel belongs to; (ii) affine\ntransform encoders that attempt to canonicalize the shaking strokes. We train\nour network with the dataset that consists of 10,000 annotated sketches, to\nfind an extensively applicable model to segment stokes semantically in one\nground truth. Extensive experiments are carried out and segmentation results\nshow that our method outperforms other state-of-the-art networks.\n", "versions": [{"version": "v1", "created": "Thu, 15 Aug 2019 01:35:36 GMT"}], "update_date": "2019-10-15", "authors_parsed": [["Jiang", "Junkun", ""], ["Wang", "Ruomei", ""], ["Lin", "Shujin", ""], ["Wang", "Fei", ""]]}, {"id": "1908.05407", "submitter": "Yuqing Song", "authors": "Yuqing Song, Shizhe Chen, Yida Zhao, Qin Jin", "title": "Unpaired Cross-lingual Image Caption Generation with Self-Supervised\n  Rewards", "comments": "Accepted by ACMMM 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generating image descriptions in different languages is essential to satisfy\nusers worldwide. However, it is prohibitively expensive to collect large-scale\npaired image-caption dataset for every target language which is critical for\ntraining descent image captioning models. Previous works tackle the unpaired\ncross-lingual image captioning problem through a pivot language, which is with\nthe help of paired image-caption data in the pivot language and pivot-to-target\nmachine translation models. However, such language-pivoted approach suffers\nfrom inaccuracy brought by the pivot-to-target translation, including\ndisfluency and visual irrelevancy errors. In this paper, we propose to generate\ncross-lingual image captions with self-supervised rewards in the reinforcement\nlearning framework to alleviate these two types of errors. We employ\nself-supervision from mono-lingual corpus in the target language to provide\nfluency reward, and propose a multi-level visual semantic matching model to\nprovide both sentence-level and concept-level visual relevancy rewards. We\nconduct extensive experiments for unpaired cross-lingual image captioning in\nboth English and Chinese respectively on two widely used image caption corpora.\nThe proposed approach achieves significant performance improvement over\nstate-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Thu, 15 Aug 2019 03:50:18 GMT"}], "update_date": "2019-08-16", "authors_parsed": [["Song", "Yuqing", ""], ["Chen", "Shizhe", ""], ["Zhao", "Yida", ""], ["Jin", "Qin", ""]]}, {"id": "1908.05418", "submitter": "Szu-Yeu Hu", "authors": "Szu-Yeu Hu, Wei-Hung Weng, Shao-Lun Lu, Yueh-Hung Cheng, Furen Xiao,\n  Feng-Ming Hsu, Jen-Tang Lu", "title": "Multimodal Volume-Aware Detection and Segmentation for Brain Metastases\n  Radiosurgery", "comments": "Accepted to 2019 MICCAI AIRT", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stereotactic radiosurgery (SRS), which delivers high doses of irradiation in\na single or few shots to small targets, has been a standard of care for brain\nmetastases. While very effective, SRS currently requires manually intensive\ndelineation of tumors. In this work, we present a deep learning approach for\nautomated detection and segmentation of brain metastases using multimodal\nimaging and ensemble neural networks. In order to address small and multiple\nbrain metastases, we further propose a volume-aware Dice loss which optimizes\nmodel performance using the information of lesion size. This work surpasses\ncurrent benchmark levels and demonstrates a reliable AI-assisted system for SRS\ntreatment planning for multiple brain metastases.\n", "versions": [{"version": "v1", "created": "Thu, 15 Aug 2019 04:48:53 GMT"}], "update_date": "2019-08-16", "authors_parsed": [["Hu", "Szu-Yeu", ""], ["Weng", "Wei-Hung", ""], ["Lu", "Shao-Lun", ""], ["Cheng", "Yueh-Hung", ""], ["Xiao", "Furen", ""], ["Hsu", "Feng-Ming", ""], ["Lu", "Jen-Tang", ""]]}, {"id": "1908.05425", "submitter": "Na Zhao", "authors": "Na Zhao and Tat-Seng Chua and Gim Hee Lee", "title": "PS^2-Net: A Locally and Globally Aware Network for Point-Based Semantic\n  Segmentation", "comments": null, "journal-ref": null, "doi": "10.1007/978-3-030-68787-8", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present the PS^2-Net -- a locally and globally aware deep\nlearning framework for semantic segmentation on 3D scene-level point clouds. In\norder to deeply incorporate local structures and global context to support 3D\nscene segmentation, our network is built on four repeatedly stacked encoders,\nwhere each encoder has two basic components: EdgeConv that captures local\nstructures and NetVLAD that models global context. Different from existing\nstart-of-the-art methods for point-based scene semantic segmentation that\neither violate or do not achieve permutation invariance, our PS^2-Net is\ndesigned to be permutation invariant which is an essential property of any deep\nnetwork used to process unordered point clouds. We further provide theoretical\nproof to guarantee the permutation invariance property of our network. We\nperform extensive experiments on two large-scale 3D indoor scene datasets and\ndemonstrate that our PS2-Net is able to achieve state-of-the-art performances\nas compared to existing approaches.\n", "versions": [{"version": "v1", "created": "Thu, 15 Aug 2019 05:35:27 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Zhao", "Na", ""], ["Chua", "Tat-Seng", ""], ["Lee", "Gim Hee", ""]]}, {"id": "1908.05436", "submitter": "Wei Mao", "authors": "Wei Mao, Miaomiao Liu, Mathieu Salzmann, Hongdong Li", "title": "Learning Trajectory Dependencies for Human Motion Prediction", "comments": "Accepted by ICCV2019(Oral)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human motion prediction, i.e., forecasting future body poses given observed\npose sequence, has typically been tackled with recurrent neural networks\n(RNNs). However, as evidenced by prior work, the resulted RNN models suffer\nfrom prediction errors accumulation, leading to undesired discontinuities in\nmotion prediction. In this paper, we propose a simple feed-forward deep network\nfor motion prediction, which takes into account both temporal smoothness and\nspatial dependencies among human body joints. In this context, we then propose\nto encode temporal information by working in trajectory space, instead of the\ntraditionally-used pose space. This alleviates us from manually defining the\nrange of temporal dependencies (or temporal convolutional filter size, as done\nin previous work). Moreover, spatial dependency of human pose is encoded by\ntreating a human pose as a generic graph (rather than a human skeletal\nkinematic tree) formed by links between every pair of body joints. Instead of\nusing a pre-defined graph structure, we design a new graph convolutional\nnetwork to learn graph connectivity automatically. This allows the network to\ncapture long range dependencies beyond that of human kinematic tree. We\nevaluate our approach on several standard benchmark datasets for motion\nprediction, including Human3.6M, the CMU motion capture dataset and 3DPW. Our\nexperiments clearly demonstrate that the proposed approach achieves state of\nthe art performance, and is applicable to both angle-based and position-based\npose representations. The code is available at\nhttps://github.com/wei-mao-2019/LearnTrajDep\n", "versions": [{"version": "v1", "created": "Thu, 15 Aug 2019 06:36:32 GMT"}, {"version": "v2", "created": "Fri, 16 Aug 2019 00:42:15 GMT"}, {"version": "v3", "created": "Tue, 7 Jul 2020 02:05:29 GMT"}], "update_date": "2020-07-08", "authors_parsed": [["Mao", "Wei", ""], ["Liu", "Miaomiao", ""], ["Salzmann", "Mathieu", ""], ["Li", "Hongdong", ""]]}, {"id": "1908.05460", "submitter": "Sree Harsha Nelaturu", "authors": "Ziheng Wang, Sree Harsha Nelaturu", "title": "Accelerated CNN Training Through Gradient Approximation", "comments": "An abridged version was presented at EMC^2 : Workshop On Energy\n  Efficient Machine Learning And Cognitive Computing For Embedded Applications\n  at ISCA 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Training deep convolutional neural networks such as VGG and ResNet by\ngradient descent is an expensive exercise requiring specialized hardware such\nas GPUs. Recent works have examined the possibility of approximating the\ngradient computation while maintaining the same convergence properties. While\npromising, the approximations only work on relatively small datasets such as\nMNIST. They also fail to achieve real wall-clock speedups due to lack of\nefficient GPU implementations of the proposed approximation methods. In this\nwork, we explore three alternative methods to approximate gradients, with an\nefficient GPU kernel implementation for one of them. We achieve wall-clock\nspeedup with ResNet-20 and VGG-19 on the CIFAR-10 dataset upwards of 7%, with a\nminimal loss in validation accuracy.\n", "versions": [{"version": "v1", "created": "Thu, 15 Aug 2019 09:11:31 GMT"}], "update_date": "2019-08-16", "authors_parsed": [["Wang", "Ziheng", ""], ["Nelaturu", "Sree Harsha", ""]]}, {"id": "1908.05467", "submitter": "Akos Dobay", "authors": "Samuel Gunz, Svenja Erne, Eric J. Rawdon, Garyfalia Ampanozi, Till\n  Sieberth, Raffael Affolter, Lars C. Ebert, Akos Dobay", "title": "Automated Rib Fracture Detection of Postmortem Computed Tomography\n  Images Using Machine Learning Techniques", "comments": "12 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Imaging techniques is widely used for medical diagnostics. This leads in some\ncases to a real bottleneck when there is a lack of medical practitioners and\nthe images have to be manually processed. In such a situation there is a need\nto reduce the amount of manual work by automating part of the analysis. In this\narticle, we investigate the potential of a machine learning algorithm for\nmedical image processing by computing a topological invariant classifier.\nFirst, we select retrospectively from our database of postmortem computed\ntomography images of rib fractures. The images are prepared by applying a rib\nunfolding tool that flattens the rib cage to form a two-dimensional projection.\nWe compare the results of our analysis with two independent convolutional\nneural network models. In the case of the neural network model, we obtain an\n$F_1$ Score of 0.73. To access the performance of our classifier, we compute\nthe relative proportion of images that were not shared between the two classes.\nWe obtain a precision of 0.60 for the images with rib fractures.\n", "versions": [{"version": "v1", "created": "Thu, 15 Aug 2019 09:37:42 GMT"}], "update_date": "2019-08-16", "authors_parsed": [["Gunz", "Samuel", ""], ["Erne", "Svenja", ""], ["Rawdon", "Eric J.", ""], ["Ampanozi", "Garyfalia", ""], ["Sieberth", "Till", ""], ["Affolter", "Raffael", ""], ["Ebert", "Lars C.", ""], ["Dobay", "Akos", ""]]}, {"id": "1908.05480", "submitter": "Evgeny Burnaev", "authors": "Anna Kuzina and Evgenii Egorov and Evgeny Burnaev", "title": "Bayesian Generative Models for Knowledge Transfer in MRI Semantic\n  Segmentation Problems", "comments": "24 page, 6 figures, 6 tabels", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic segmentation methods based on deep learning have recently\ndemonstrated state-of-the-art performance, outperforming the ordinary methods.\nNevertheless, these methods are inapplicable for small datasets, which are very\ncommon in medical problems. To this end, we propose a knowledge transfer method\nbetween diseases via the Generative Bayesian Prior network. Our approach is\ncompared to a pre-train approach and random initialization and obtains the best\nresults in terms of Dice Similarity Coefficient metric for the small subsets of\nthe Brain Tumor Segmentation 2018 database (BRATS2018).\n", "versions": [{"version": "v1", "created": "Thu, 15 Aug 2019 10:27:32 GMT"}], "update_date": "2019-08-16", "authors_parsed": [["Kuzina", "Anna", ""], ["Egorov", "Evgenii", ""], ["Burnaev", "Evgeny", ""]]}, {"id": "1908.05488", "submitter": "Qian Zhang Dr.", "authors": "Qian Zhang, Feifei Lee, Ya-Gang Wang, Qiu Chen", "title": "Improved Mix-up with KL-Entropy for Learning From Noisy Labels", "comments": "The research in this paper we think is not enough to publish, so we\n  will continue to research it. Due to we need to add more experiments and\n  change the whole structure of this paper, it will spend a lot of time. We\n  want to publish our research after all the change have been done, so we apply\n  to withdraw this version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the deep neural networks (DNN) has achieved excellent performance in\nimage classification researches, the training of DNNs needs a large of clean\ndata with accurate annotations. The collect of a dataset is easy, but it is\ndifficult to annotate the collecting data. On the websites, there exist a lot\nof image data which contains inaccurate annotations, but training on these\ndatasets may make networks easier to over-fit the noisy labels and cause\nperformance degradation. In this work, we propose an improved joint\noptimization framework, which mixed the mix-up entropy and Kullback-Leibler\n(KL) entropy as the loss function. The new loss function can give the better\nfine-tuning after the framework updates both the label annotations. We conduct\nexperiments on CIFAR-10 dataset and Clothing1M dataset. The result shows the\nadvantageous performance of our approach compared with other state-of-the-art\nmethods.\n", "versions": [{"version": "v1", "created": "Thu, 15 Aug 2019 10:55:37 GMT"}, {"version": "v2", "created": "Fri, 16 Aug 2019 14:36:56 GMT"}], "update_date": "2019-08-19", "authors_parsed": [["Zhang", "Qian", ""], ["Lee", "Feifei", ""], ["Wang", "Ya-Gang", ""], ["Chen", "Qiu", ""]]}, {"id": "1908.05489", "submitter": "Gianluca Maguolo", "authors": "Alessandra Lumini, Loris Nanni, Gianluca Maguolo", "title": "Deep learning for Plankton and Coral Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Oceans are the essential lifeblood of the Earth: they provide over 70% of the\noxygen and over 97% of the water. Plankton and corals are two of the most\nfundamental components of ocean ecosystems, the former due to their function at\nmany levels of the oceans food chain, the latter because they provide spawning\nand nursery grounds to many fish populations. Studying and monitoring plankton\ndistribution and coral reefs is vital for environment protection. In the last\nyears there has been a massive proliferation of digital imagery for the\nmonitoring of underwater ecosystems and much research is concentrated on the\nautomated recognition of plankton and corals. In this paper, we present a study\nabout an automated system for monitoring of underwater ecosystems. The system\nhere proposed is based on the fusion of different deep learning methods. We\nstudy how to create an ensemble based of different CNN models, fine tuned on\nseveral datasets with the aim of exploiting their diversity. The aim of our\nstudy is to experiment the possibility of fine-tuning pretrained CNN for\nunderwater imagery analysis, the opportunity of using different datasets for\npretraining models, the possibility to design an ensemble using the same\narchitecture with small variations in the training procedure. The experimental\nresults are very encouraging, our experiments performed on 5 well-knowns\ndatasets (3 plankton and 2 coral datasets) show that the fusion of such\ndifferent CNN models in a heterogeneous ensemble grants a substantial\nperformance improvement with respect to other state-of-the-art approaches in\nall the tested problems. One of the main contributions of this work is a wide\nexperimental evaluation of famous CNN architectures to report performance of\nboth single CNN and ensemble of CNNs in different problems. Moreover, we show\nhow to create an ensemble which improves the performance of the best single\nmodel.\n", "versions": [{"version": "v1", "created": "Thu, 15 Aug 2019 10:57:40 GMT"}, {"version": "v2", "created": "Tue, 10 Dec 2019 00:16:38 GMT"}], "update_date": "2019-12-11", "authors_parsed": [["Lumini", "Alessandra", ""], ["Nanni", "Loris", ""], ["Maguolo", "Gianluca", ""]]}, {"id": "1908.05498", "submitter": "Fei Qi", "authors": "Pengfei Wang, Chengquan Zhang, Fei Qi, Zuming Huang, Mengyi En, Junyu\n  Han, Jingtuo Liu, Errui Ding, Guangming Shi", "title": "A Single-Shot Arbitrarily-Shaped Text Detector based on Context Attended\n  Multi-Task Learning", "comments": "9 pages, 6 figures, 7 tables, To appear in ACM Multimedia 2019", "journal-ref": "In Proceedings of the 27th ACM International Conference on\n  Multimedia (MM '19), October 21-25, 2019, Nice, France", "doi": "10.1145/3343031.3350988", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Detecting scene text of arbitrary shapes has been a challenging task over the\npast years. In this paper, we propose a novel segmentation-based text detector,\nnamely SAST, which employs a context attended multi-task learning framework\nbased on a Fully Convolutional Network (FCN) to learn various geometric\nproperties for the reconstruction of polygonal representation of text regions.\nTaking sequential characteristics of text into consideration, a Context\nAttention Block is introduced to capture long-range dependencies of pixel\ninformation to obtain a more reliable segmentation. In post-processing, a\nPoint-to-Quad assignment method is proposed to cluster pixels into text\ninstances by integrating both high-level object knowledge and low-level pixel\ninformation in a single shot. Moreover, the polygonal representation of\narbitrarily-shaped text can be extracted with the proposed geometric properties\nmuch more effectively. Experiments on several benchmarks, including ICDAR2015,\nICDAR2017-MLT, SCUT-CTW1500, and Total-Text, demonstrate that SAST achieves\nbetter or comparable performance in terms of accuracy. Furthermore, the\nproposed algorithm runs at 27.63 FPS on SCUT-CTW1500 with a Hmean of 81.0% on a\nsingle NVIDIA Titan Xp graphics card, surpassing most of the existing\nsegmentation-based methods.\n", "versions": [{"version": "v1", "created": "Thu, 15 Aug 2019 11:36:52 GMT"}], "update_date": "2019-08-16", "authors_parsed": [["Wang", "Pengfei", ""], ["Zhang", "Chengquan", ""], ["Qi", "Fei", ""], ["Huang", "Zuming", ""], ["En", "Mengyi", ""], ["Han", "Junyu", ""], ["Liu", "Jingtuo", ""], ["Ding", "Errui", ""], ["Shi", "Guangming", ""]]}, {"id": "1908.05540", "submitter": "Amir Atapour Abarghouei", "authors": "Amir Atapour-Abarghouei and Toby P. Breckon", "title": "To complete or to estimate, that is the question: A Multi-Task Approach\n  to Depth Completion and Monocular Depth Estimation", "comments": "International Conference on 3D Vision (3DV) 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Robust three-dimensional scene understanding is now an ever-growing area of\nresearch highly relevant in many real-world applications such as autonomous\ndriving and robotic navigation. In this paper, we propose a multi-task\nlearning-based model capable of performing two tasks:- sparse depth completion\n(i.e. generating complete dense scene depth given a sparse depth image as the\ninput) and monocular depth estimation (i.e. predicting scene depth from a\nsingle RGB image) via two sub-networks jointly trained end to end using data\nrandomly sampled from a publicly available corpus of synthetic and real-world\nimages. The first sub-network generates a sparse depth image by learning lower\nlevel features from the scene and the second predicts a full dense depth image\nof the entire scene, leading to a better geometric and contextual understanding\nof the scene and, as a result, superior performance of the approach. The entire\nmodel can be used to infer complete scene depth from a single RGB image or the\nsecond network can be used alone to perform depth completion given a sparse\ndepth input. Using adversarial training, a robust objective function, a deep\narchitecture relying on skip connections and a blend of synthetic and\nreal-world training data, our approach is capable of producing superior high\nquality scene depth. Extensive experimental evaluation demonstrates the\nefficacy of our approach compared to contemporary state-of-the-art techniques\nacross both problem domains.\n", "versions": [{"version": "v1", "created": "Thu, 15 Aug 2019 13:50:50 GMT"}], "update_date": "2019-08-16", "authors_parsed": [["Atapour-Abarghouei", "Amir", ""], ["Breckon", "Toby P.", ""]]}, {"id": "1908.05547", "submitter": "Patrick Ebel", "authors": "Patrick Ebel, Anastasiia Mishchuk, Kwang Moo Yi, Pascal Fua and Eduard\n  Trulls", "title": "Beyond Cartesian Representations for Local Descriptors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The dominant approach for learning local patch descriptors relies on small\nimage regions whose scale must be properly estimated a priori by a keypoint\ndetector. In other words, if two patches are not in correspondence, their\ndescriptors will not match. A strategy often used to alleviate this problem is\nto \"pool\" the pixel-wise features over log-polar regions, rather than regularly\nspaced ones. By contrast, we propose to extract the \"support region\" directly\nwith a log-polar sampling scheme. We show that this provides us with a better\nrepresentation by simultaneously oversampling the immediate neighbourhood of\nthe point and undersampling regions far away from it. We demonstrate that this\nrepresentation is particularly amenable to learning descriptors with deep\nnetworks. Our models can match descriptors across a much wider range of scales\nthan was possible before, and also leverage much larger support regions without\nsuffering from occlusions. We report state-of-the-art results on three\ndifferent datasets.\n", "versions": [{"version": "v1", "created": "Thu, 15 Aug 2019 13:56:40 GMT"}], "update_date": "2019-08-16", "authors_parsed": [["Ebel", "Patrick", ""], ["Mishchuk", "Anastasiia", ""], ["Yi", "Kwang Moo", ""], ["Fua", "Pascal", ""], ["Trulls", "Eduard", ""]]}, {"id": "1908.05569", "submitter": "David Mac\\^edo", "authors": "David Mac\\^edo, Tsang Ing Ren, Cleber Zanchettin, Adriano L. I.\n  Oliveira, Teresa Ludermir", "title": "Entropic Out-of-Distribution Detection", "comments": "Accepted for publication in The International Joint Conference on\n  Neural Networks (IJCNN), 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Out-of-distribution (OOD) detection approaches usually present special\nrequirements (e.g., hyperparameter validation, collection of outlier data) and\nproduce side effects (e.g., classification accuracy drop, slower\nenergy-inefficient inferences). We argue that these issues are a consequence of\nthe SoftMax loss anisotropy and disagreement with the maximum entropy\nprinciple. Thus, we propose the IsoMax loss and the entropic score. The\nseamless drop-in replacement of the SoftMax loss by IsoMax loss requires\nneither additional data collection nor hyperparameter validation. The trained\nmodels do not exhibit classification accuracy drop and produce fast\nenergy-efficient inferences. Moreover, our experiments show that training\nneural networks with IsoMax loss significantly improves their OOD detection\nperformance. The IsoMax loss exhibits state-of-the-art performance under the\nmentioned conditions (fast energy-efficient inference, no classification\naccuracy drop, no collection of outlier data, and no hyperparameter\nvalidation), which we call the seamless OOD detection task. In future work,\ncurrent OOD detection methods may replace the SoftMax loss with the IsoMax loss\nto improve their performance on the commonly studied non-seamless OOD detection\nproblem.\n", "versions": [{"version": "v1", "created": "Thu, 15 Aug 2019 14:54:52 GMT"}, {"version": "v10", "created": "Thu, 26 Nov 2020 22:47:49 GMT"}, {"version": "v11", "created": "Mon, 30 Nov 2020 01:55:08 GMT"}, {"version": "v12", "created": "Sun, 11 Apr 2021 23:32:49 GMT"}, {"version": "v13", "created": "Mon, 24 May 2021 23:15:23 GMT"}, {"version": "v2", "created": "Sat, 17 Aug 2019 02:22:46 GMT"}, {"version": "v3", "created": "Tue, 20 Aug 2019 00:58:03 GMT"}, {"version": "v4", "created": "Wed, 5 Feb 2020 01:18:56 GMT"}, {"version": "v5", "created": "Tue, 18 Feb 2020 23:06:49 GMT"}, {"version": "v6", "created": "Sun, 7 Jun 2020 06:54:51 GMT"}, {"version": "v7", "created": "Thu, 11 Jun 2020 05:59:13 GMT"}, {"version": "v8", "created": "Thu, 23 Jul 2020 17:03:57 GMT"}, {"version": "v9", "created": "Mon, 3 Aug 2020 06:37:15 GMT"}], "update_date": "2021-05-26", "authors_parsed": [["Mac\u00eado", "David", ""], ["Ren", "Tsang Ing", ""], ["Zanchettin", "Cleber", ""], ["Oliveira", "Adriano L. I.", ""], ["Ludermir", "Teresa", ""]]}, {"id": "1908.05593", "submitter": "Jiabin Zhang", "authors": "Jiabin Zhang, Zheng Zhu, Wei Zou, Peng Li, Yanwei Li, Hu Su, Guan\n  Huang", "title": "FastPose: Towards Real-time Pose Estimation and Tracking via\n  Scale-normalized Multi-task Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Both accuracy and efficiency are significant for pose estimation and tracking\nin videos. State-of-the-art performance is dominated by two-stages top-down\nmethods. Despite the leading results, these methods are impractical for\nreal-world applications due to their separated architectures and complicated\ncalculation. This paper addresses the task of articulated multi-person pose\nestimation and tracking towards real-time speed. An end-to-end multi-task\nnetwork (MTN) is designed to perform human detection, pose estimation, and\nperson re-identification (Re-ID) tasks simultaneously. To alleviate the\nperformance bottleneck caused by scale variation problem, a paradigm which\nexploits scale-normalized image and feature pyramids (SIFP) is proposed to\nboost both performance and speed. Given the results of MTN, we adopt an\nocclusion-aware Re-ID feature strategy in the pose tracking module, where pose\ninformation is utilized to infer the occlusion state to make better use of\nRe-ID feature. In experiments, we demonstrate that the pose estimation and\ntracking performance improves steadily utilizing SIFP through different\nbackbones. Using ResNet-18 and ResNet-50 as backbones, the overall pose\ntracking framework achieves competitive performance with 29.4 FPS and 12.2 FPS,\nrespectively. Additionally, occlusion-aware Re-ID feature decreases the\nidentification switches by 37% in the pose tracking process.\n", "versions": [{"version": "v1", "created": "Thu, 15 Aug 2019 15:42:57 GMT"}], "update_date": "2019-08-16", "authors_parsed": [["Zhang", "Jiabin", ""], ["Zhu", "Zheng", ""], ["Zou", "Wei", ""], ["Li", "Peng", ""], ["Li", "Yanwei", ""], ["Su", "Hu", ""], ["Huang", "Guan", ""]]}, {"id": "1908.05599", "submitter": "Cheng Peng", "authors": "Cheng Peng, Wei-An Lin, Haofu Liao, Rama Chellappa, S. Kevin Zhou", "title": "Deep Slice Interpolation via Marginal Super-Resolution, Fusion and\n  Refinement", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a marginal super-resolution (MSR) approach based on 2D\nconvolutional neural networks (CNNs) for interpolating an anisotropic brain\nmagnetic resonance scan along the highly under-sampled direction, which is\nassumed to axial without loss of generality. Previous methods for slice\ninterpolation only consider data from pairs of adjacent 2D slices. The\npossibility of fusing information from the direction orthogonal to the 2D\nslices remains unexplored. Our approach performs MSR in both sagittal and\ncoronal directions, which provides an initial estimate for slice interpolation.\nThe interpolated slices are then fused and refined in the axial direction for\nimproved consistency. Since MSR consists of only 2D operations, it is more\nfeasible in terms of GPU memory consumption and requires fewer training samples\ncompared to 3D CNNs. Our experiments demonstrate that the proposed method\noutperforms traditional linear interpolation and baseline 2D/3D CNN-based\napproaches. We conclude by showcasing the method's practical utility in\nestimating brain volumes from under-sampled brain MR scans through semantic\nsegmentation.\n", "versions": [{"version": "v1", "created": "Thu, 15 Aug 2019 15:48:54 GMT"}], "update_date": "2019-08-16", "authors_parsed": [["Peng", "Cheng", ""], ["Lin", "Wei-An", ""], ["Liao", "Haofu", ""], ["Chellappa", "Rama", ""], ["Zhou", "S. Kevin", ""]]}, {"id": "1908.05602", "submitter": "Heikki Arponen Dr", "authors": "Heikki Arponen, Tom E Bishop", "title": "SHREWD: Semantic Hierarchy-based Relational Embeddings for\n  Weakly-supervised Deep Hashing", "comments": "4 pages, Published in ICLR LLD Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI cs.CV cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Using class labels to represent class similarity is a typical approach to\ntraining deep hashing systems for retrieval; samples from the same or different\nclasses take binary 1 or 0 similarity values. This similarity does not model\nthe full rich knowledge of semantic relations that may be present between data\npoints. In this work we build upon the idea of using semantic hierarchies to\nform distance metrics between all available sample labels; for example cat to\ndog has a smaller distance than cat to guitar. We combine this type of semantic\ndistance into a loss function to promote similar distances between the deep\nneural network embeddings. We also introduce an empirical Kullback-Leibler\ndivergence loss term to promote binarization and uniformity of the embeddings.\nWe test the resulting SHREWD method and demonstrate improvements in\nhierarchical retrieval scores using compact, binary hash codes instead of real\nvalued ones, and show that in a weakly supervised hashing setting we are able\nto learn competitively without explicitly relying on class labels, but instead\non similarities between labels.\n", "versions": [{"version": "v1", "created": "Mon, 12 Aug 2019 08:24:40 GMT"}], "update_date": "2019-08-16", "authors_parsed": [["Arponen", "Heikki", ""], ["Bishop", "Tom E", ""]]}, {"id": "1908.05612", "submitter": "Xue Yang", "authors": "Xue Yang, Junchi Yan, Ziming Feng, Tao He", "title": "R3Det: Refined Single-Stage Detector with Feature Refinement for\n  Rotating Object", "comments": "13 pages, 12 figures, 9 tables", "journal-ref": "Thirty-Five AAAI Conference on Artificial Intelligence (AAAI2021)", "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Rotation detection is a challenging task due to the difficulties of locating\nthe multi-angle objects and separating them effectively from the background.\nThough considerable progress has been made, for practical settings, there still\nexist challenges for rotating objects with large aspect ratio, dense\ndistribution and category extremely imbalance. In this paper, we propose an\nend-to-end refined single-stage rotation detector for fast and accurate object\ndetection by using a progressive regression approach from coarse to fine\ngranularity. Considering the shortcoming of feature misalignment in existing\nrefined single-stage detector, we design a feature refinement module to improve\ndetection performance by getting more accurate features. The key idea of\nfeature refinement module is to re-encode the position information of the\ncurrent refined bounding box to the corresponding feature points through\npixel-wise feature interpolation to realize feature reconstruction and\nalignment. For more accurate rotation estimation, an approximate SkewIoU loss\nis proposed to solve the problem that the calculation of SkewIoU is not\nderivable. Experiments on three popular remote sensing public datasets DOTA,\nHRSC2016, UCAS-AOD as well as one scene text dataset ICDAR2015 show the\neffectiveness of our approach. Tensorflow and Pytorch version codes are\navailable at https://github.com/Thinklab-SJTU/R3Det_Tensorflow and\nhttps://github.com/SJTU-Thinklab-Det/r3det-on-mmdetection, and R3Det is also\nintegrated in our open source rotation detection benchmark:\nhttps://github.com/yangxue0827/RotationDetection.\n", "versions": [{"version": "v1", "created": "Thu, 15 Aug 2019 15:56:37 GMT"}, {"version": "v2", "created": "Fri, 16 Aug 2019 10:13:30 GMT"}, {"version": "v3", "created": "Sun, 17 Nov 2019 07:00:27 GMT"}, {"version": "v4", "created": "Sat, 30 Nov 2019 14:55:45 GMT"}, {"version": "v5", "created": "Fri, 21 Feb 2020 12:57:03 GMT"}, {"version": "v6", "created": "Tue, 8 Dec 2020 05:52:52 GMT"}], "update_date": "2020-12-09", "authors_parsed": [["Yang", "Xue", ""], ["Yan", "Junchi", ""], ["Feng", "Ziming", ""], ["He", "Tao", ""]]}, {"id": "1908.05615", "submitter": "Cheng Peng", "authors": "Cheng Peng, Wei-An Lin, Rama Chellappa, S. Kevin Zhou", "title": "Towards multi-sequence MR image recovery from undersampled k-space data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Undersampled MR image recovery has been widely studied for accelerated MR\nacquisition. However, it has been mostly studied under a single sequence\nscenario, despite the fact that multi-sequence MR scan is common in practice.\nIn this paper, we aim to optimize multi-sequence MR image recovery from\nundersampled k-space data under an overall time constraint while considering\nthe difference in acquisition time for various sequences. We first formulate it\nas a constrained optimization problem and then show that finding the optimal\nsampling strategy for all sequences and the best recovery model at the same\ntime is combinatorial and hence computationally prohibitive. To solve this\nproblem, we propose a blind recovery model that simultaneously recovers\nmultiple sequences, and an efficient approach to find proper combination of\nsampling strategy and recovery model. Our experiments demonstrate that the\nproposed method outperforms sequence-wise recovery, and sheds light on how to\ndecide the undersampling strategy for sequences within an overall time budget.\n", "versions": [{"version": "v1", "created": "Thu, 15 Aug 2019 16:01:47 GMT"}, {"version": "v2", "created": "Fri, 16 Aug 2019 00:35:57 GMT"}], "update_date": "2019-08-19", "authors_parsed": [["Peng", "Cheng", ""], ["Lin", "Wei-An", ""], ["Chellappa", "Rama", ""], ["Zhou", "S. Kevin", ""]]}, {"id": "1908.05621", "submitter": "Bart Liefers", "authors": "Bart Liefers, Johanna M. Colijn, Cristina Gonz\\'alez-Gonzalo, Timo\n  Verzijden, Paul Mitchell, Carel B. Hoyng, Bram van Ginneken, Caroline C.W.\n  Klaver, Clara I. S\\'anchez", "title": "A deep learning model for segmentation of geographic atrophy to study\n  its long-term natural history", "comments": "22 pages, 3 tables, 4 figures, 1 supplemental figure", "journal-ref": "Ophthalmology, Published February 14, 2020", "doi": "10.1016/j.ophtha.2020.02.009", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Purpose: To develop and validate a deep learning model for automatic\nsegmentation of geographic atrophy (GA) in color fundus images (CFIs) and its\napplication to study growth rate of GA. Participants: 409 CFIs of 238 eyes with\nGA from the Rotterdam Study (RS) and the Blue Mountain Eye Study (BMES) for\nmodel development, and 5,379 CFIs of 625 eyes from the Age-Related Eye Disease\nStudy (AREDS) for analysis of GA growth rate. Methods: A deep learning model\nbased on an ensemble of encoder-decoder architectures was implemented and\noptimized for the segmentation of GA in CFIs. Four experienced graders\ndelineated GA in CFIs from RS and BMES. These manual delineations were used to\nevaluate the segmentation model using 5-fold cross-validation. The model was\nfurther applied to CFIs from the AREDS to study the growth rate of GA. Linear\nregression analysis was used to study associations between structural\nbiomarkers at baseline and GA growth rate. A general estimate of the\nprogression of GA area over time was made by combining growth rates of all eyes\nwith GA from the AREDS set. Results: The model obtained an average Dice\ncoefficient of 0.72 $\\pm$ 0.26 on the BMES and RS. An intraclass correlation\ncoefficient of 0.83 was reached between the automatically estimated GA area and\nthe graders' consensus measures. Eight automatically calculated structural\nbiomarkers (area, filled area, convex area, convex solidity, eccentricity,\nroundness, foveal involvement and perimeter) were significantly associated with\ngrowth rate. Combining all growth rates indicated that GA area grows\nquadratically up to an area of around 12 mm$^{2}$, after which growth rate\nstabilizes or decreases. Conclusion: The presented deep learning model allowed\nfor fully automatic and robust segmentation of GA in CFIs. These segmentations\ncan be used to extract structural characteristics of GA that predict its growth\nrate.\n", "versions": [{"version": "v1", "created": "Thu, 15 Aug 2019 16:12:52 GMT"}], "update_date": "2020-10-30", "authors_parsed": [["Liefers", "Bart", ""], ["Colijn", "Johanna M.", ""], ["Gonz\u00e1lez-Gonzalo", "Cristina", ""], ["Verzijden", "Timo", ""], ["Mitchell", "Paul", ""], ["Hoyng", "Carel B.", ""], ["van Ginneken", "Bram", ""], ["Klaver", "Caroline C. W.", ""], ["S\u00e1nchez", "Clara I.", ""]]}, {"id": "1908.05641", "submitter": "Shengkai Wu", "authors": "Shengkai Wu, Jinrong Yang, Xinggang Wang, Xiaoping Li", "title": "IoU-balanced Loss Functions for Single-stage Object Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Single-stage object detectors have been widely applied in computer vision\napplications due to their high efficiency. However, we find that the loss\nfunctions adopted by single-stage object detectors hurt the localization\naccuracy seriously. Firstly, the standard cross-entropy loss for classification\nis independent of the localization task and drives all the positive examples to\nlearn as high classification scores as possible regardless of localization\naccuracy during training. As a result, there will be many detections that have\nhigh classification scores but low IoU or detections that have low\nclassification scores but high IoU. Secondly, for the standard smooth L1 loss,\nthe gradient is dominated by the outliers that have poor localization accuracy\nduring training. The above two problems will decrease the localization accuracy\nof single-stage detectors. In this work, IoU-balanced loss functions that\nconsist of IoU-balanced classification loss and IoU-balanced localization loss\nare proposed to solve the above problems. The IoU-balanced classification loss\npays more attention to positive examples with high IoU and can enhance the\ncorrelation between classification and localization tasks. The IoU-balanced\nlocalization loss decreases the gradient of examples with low IoU and increases\nthe gradient of examples with high IoU, which can improve the localization\naccuracy of models. Extensive experiments on challenging public datasets such\nas MS COCO, PASCAL VOC and Cityscapes demonstrate that both IoU-balanced losses\ncan bring substantial improvement for the popular single-stage detectors,\nespecially for the localization accuracy. On COCO test-dev, the proposed\nmethods can substantially improve AP by $1.0\\%\\sim1.7\\%$ and AP75 by\n$1.0\\%\\sim2.4\\%$. On PASCAL VOC, it can also substantially improve AP by\n$1.3\\%\\sim1.5\\%$ and AP80, AP90 by $1.6\\%\\sim3.9\\%$.\n", "versions": [{"version": "v1", "created": "Thu, 15 Aug 2019 17:08:22 GMT"}, {"version": "v2", "created": "Mon, 14 Dec 2020 08:07:55 GMT"}], "update_date": "2020-12-15", "authors_parsed": [["Wu", "Shengkai", ""], ["Yang", "Jinrong", ""], ["Wang", "Xinggang", ""], ["Li", "Xiaoping", ""]]}, {"id": "1908.05649", "submitter": "Dongming Sun", "authors": "Dongming Sun, Xiao Huang, and Kailun Yang", "title": "A Multimodal Vision Sensor for Autonomous Driving", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes a multimodal vision sensor that integrates three types\nof cameras, including a stereo camera, a polarization camera and a panoramic\ncamera. Each sensor provides a specific dimension of information: the stereo\ncamera measures depth per pixel, the polarization obtains the degree of\npolarization, and the panoramic camera captures a 360-degree landscape. Data\nfusion and advanced environment perception could be built upon the combination\nof sensors. Designed especially for autonomous driving, this vision sensor is\nshipped with a robust semantic segmentation network. In addition, we\ndemonstrate how cross-modal enhancement could be achieved by registering the\ncolor image and the polarization image. An example of water hazard detection is\ngiven. To prove the multimodal vision sensor's compatibility with different\ndevices, a brief runtime performance analysis is carried out.\n", "versions": [{"version": "v1", "created": "Thu, 15 Aug 2019 17:45:17 GMT"}], "update_date": "2019-08-16", "authors_parsed": [["Sun", "Dongming", ""], ["Huang", "Xiao", ""], ["Yang", "Kailun", ""]]}, {"id": "1908.05663", "submitter": "Yigal Shenkman", "authors": "Yigal Shenkman, Bilal Qutteineh, Leo Joskowicz, Adi Szeskin, Yusef\n  Azraq, Arnaldo Mayer, Iris Eshed", "title": "Automatic detection and diagnosis of sacroiliitis in CT scans as\n  incidental findings", "comments": null, "journal-ref": null, "doi": "10.1016/j.media.2019.07.007", "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Early diagnosis of sacroiliitis may lead to preventive treatment which can\nsignificantly improve the patient's quality of life in the long run.\nOftentimes, a CT scan of the lower back or abdomen is acquired for suspected\nback pain. However, since the differences between a healthy and an inflamed\nsacroiliac joint in the early stages are subtle, the condition may be missed.\nWe have developed a new automatic algorithm for the diagnosis and grading of\nsacroiliitis CT scans as incidental findings, for patients who underwent CT\nscanning as part of their lower back pain workout. The method is based on\nsupervised machine and deep learning techniques. The input is a CT scan that\nincludes the patient's pelvis. The output is a diagnosis for each sacroiliac\njoint. The algorithm consists of four steps: 1) computation of an initial\nregion of interest (ROI) that includes the pelvic joints region using\nheuristics and a U-Net classifier; 2) refinement of the ROI to detect both\nsacroiliac joints using a four-tree random forest; 3) individual sacroiliitis\ngrading of each sacroiliac joint in each CT slice with a custom slice CNN\nclassifier, and; 4) sacroiliitis diagnosis and grading by combining the\nindividual slice grades using a random forest. Experimental results on 484\nsacroiliac joints yield a binary and a 3-class case classification accuracy of\n91.9% and 86%, a sensitivity of 95% and 82%, and an Area-Under-the-Curve of\n0.97 and 0.57, respectively. Automatic computer-based analysis of CT scans has\nthe potential of being a useful method for the diagnosis and grading of\nsacroiliitis as an incidental finding.\n", "versions": [{"version": "v1", "created": "Wed, 14 Aug 2019 18:51:34 GMT"}], "update_date": "2019-08-19", "authors_parsed": [["Shenkman", "Yigal", ""], ["Qutteineh", "Bilal", ""], ["Joskowicz", "Leo", ""], ["Szeskin", "Adi", ""], ["Azraq", "Yusef", ""], ["Mayer", "Arnaldo", ""], ["Eshed", "Iris", ""]]}, {"id": "1908.05667", "submitter": "Mutlu Demirer", "authors": "Barbaros S. Erdal, Mutlu Demirer, Chiemezie C. Amadi, Gehan F. M.\n  Ibrahim, Thomas P. O'Donnell, Rainer Grimmer, Andreas Wimmer, Kevin J.\n  Little, Vikash Gupta, Matthew T. Bigelow, Luciano M. Prevedello, Richard D.\n  White", "title": "Are Quantitative Features of Lung Nodules Reproducible at Different CT\n  Acquisition and Reconstruction Parameters?", "comments": null, "journal-ref": null, "doi": "10.1371/journal.pone.0240184", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consistency and duplicability in Computed Tomography (CT) output is essential\nto quantitative imaging for lung cancer detection and monitoring. This study of\nCT-detected lung nodules investigated the reproducibility of volume-, density-,\nand texture-based features (outcome variables) over routine ranges of\nradiation-dose, reconstruction kernel, and slice thickness. CT raw data of 23\nnodules were reconstructed using 320 acquisition/reconstruction conditions\n(combinations of 4 doses, 10 kernels, and 8 thicknesses). Scans at 12.5%, 25%,\nand 50% of protocol dose were simulated; reduced-dose and full-dose data were\nreconstructed using conventional filtered back-projection and\niterative-reconstruction kernels at a range of thicknesses (0.6-5.0 mm).\nFull-dose/B50f kernel reconstructions underwent expert segmentation for\nreference Region-Of-Interest (ROI) and nodule volume per thickness; each ROI\nwas applied to 40 corresponding images (combinations of 4 doses and 10\nkernels). Typical texture analysis metrics (including 5 histogram features, 13\nGray Level Co-occurrence Matrix, 5 Run Length Matrix, 2 Neighboring Gray-Level\nDependence Matrix, and 2 Neighborhood Gray-Tone Difference Matrix) were\ncomputed per ROI. Reconstruction conditions resulting in no significant change\nin volume, density, or texture metrics were identified as \"compatible pairs\"\nfor a given outcome variable. Our results indicate that as thickness increases,\nvolumetric reproducibility decreases, while reproducibility of histogram- and\ntexture-based features across different acquisition and reconstruction\nparameters improves. In order to achieve concomitant reproducibility of\nvolumetric and radiomic results across studies, balanced standardization of the\nimaging acquisition parameters is required.\n", "versions": [{"version": "v1", "created": "Wed, 14 Aug 2019 20:14:19 GMT"}], "update_date": "2021-01-27", "authors_parsed": [["Erdal", "Barbaros S.", ""], ["Demirer", "Mutlu", ""], ["Amadi", "Chiemezie C.", ""], ["Ibrahim", "Gehan F. M.", ""], ["O'Donnell", "Thomas P.", ""], ["Grimmer", "Rainer", ""], ["Wimmer", "Andreas", ""], ["Little", "Kevin J.", ""], ["Gupta", "Vikash", ""], ["Bigelow", "Matthew T.", ""], ["Prevedello", "Luciano M.", ""], ["White", "Richard D.", ""]]}, {"id": "1908.05669", "submitter": "Lei Qi", "authors": "Lei Qi, Lei Wang, Jing Huo, Yinghuan Shi and Yang Gao", "title": "Progressive Cross-camera Soft-label Learning for Semi-supervised Person\n  Re-identification", "comments": "Accepted by IEEE Transactions on Circuits and Systems for Video\n  Technology (TCSVT)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we focus on the semi-supervised person re-identification\n(Re-ID) case, which only has the intra-camera (within-camera) labels but not\ninter-camera (cross-camera) labels. In real-world applications, these\nintra-camera labels can be readily captured by tracking algorithms or few\nmanual annotations, when compared with cross-camera labels. In this case, it is\nvery difficult to explore the relationships between cross-camera persons in the\ntraining stage due to the lack of cross-camera label information. To deal with\nthis issue, we propose a novel Progressive Cross-camera Soft-label Learning\n(PCSL) framework for the semi-supervised person Re-ID task, which can generate\ncross-camera soft-labels and utilize them to optimize the network. Concretely,\nwe calculate an affinity matrix based on person-level features and adapt them\nto produce the similarities between cross-camera persons (i.e., cross-camera\nsoft-labels). To exploit these soft-labels to train the network, we investigate\nthe weighted cross-entropy loss and the weighted triplet loss from the\nclassification and discrimination perspectives, respectively. Particularly, the\nproposed framework alternately generates progressive cross-camera soft-labels\nand gradually improves feature representations in the whole learning course.\nExtensive experiments on five large-scale benchmark datasets show that PCSL\nsignificantly outperforms the state-of-the-art unsupervised methods that employ\nlabeled source domains or the images generated by the GAN-based models.\nFurthermore, the proposed method even has a competitive performance with\nrespect to deep supervised Re-ID methods.\n", "versions": [{"version": "v1", "created": "Thu, 15 Aug 2019 00:19:02 GMT"}, {"version": "v2", "created": "Tue, 24 Mar 2020 15:34:39 GMT"}], "update_date": "2020-03-25", "authors_parsed": [["Qi", "Lei", ""], ["Wang", "Lei", ""], ["Huo", "Jing", ""], ["Shi", "Yinghuan", ""], ["Gao", "Yang", ""]]}, {"id": "1908.05674", "submitter": "Dong Cao", "authors": "Dong Cao, Lisha Xu", "title": "Bypass Enhancement RGB Stream Model for Pedestrian Action Recognition of\n  Autonomous Vehicles", "comments": "Accepted to ACPR 2019 - Workshop on Computer Vision for Modern\n  Vehicles", "journal-ref": null, "doi": "10.1007/978-981-15-3651-9_2", "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pedestrian action recognition and intention prediction is one of the core\nissues in the field of autonomous driving. In this research field, action\nrecognition is one of the key technologies. A large number of scholars have\ndone a lot of work to im-prove the accuracy of the algorithm for the task.\nHowever, there are relatively few studies and improvements in the computational\ncomplexity of algorithms and sys-tem real-time. In the autonomous driving\napplication scenario, the real-time per-formance and ultra-low latency of the\nalgorithm are extremely important evalua-tion indicators, which are directly\nrelated to the availability and safety of the au-tonomous driving system. To\nthis end, we construct a bypass enhanced RGB flow model, which combines the\nprevious two-branch algorithm to extract RGB feature information and optical\nflow feature information respectively. In the train-ing phase, the two branches\nare merged by distillation method, and the bypass enhancement is combined in\nthe inference phase to ensure accuracy. The real-time behavior of the behavior\nrecognition algorithm is significantly improved on the premise that the\naccuracy does not decrease. Experiments confirm the superiority and\neffectiveness of our algorithm.\n", "versions": [{"version": "v1", "created": "Thu, 15 Aug 2019 07:37:14 GMT"}, {"version": "v2", "created": "Mon, 2 Sep 2019 07:02:31 GMT"}], "update_date": "2020-04-24", "authors_parsed": [["Cao", "Dong", ""], ["Xu", "Lisha", ""]]}, {"id": "1908.05724", "submitter": "Sudhanshu Mittal", "authors": "Sudhanshu Mittal, Maxim Tatarchenko, Thomas Brox", "title": "Semi-Supervised Semantic Segmentation with High- and Low-level\n  Consistency", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ability to understand visual information from limited labeled data is an\nimportant aspect of machine learning. While image-level classification has been\nextensively studied in a semi-supervised setting, dense pixel-level\nclassification with limited data has only drawn attention recently. In this\nwork, we propose an approach for semi-supervised semantic segmentation that\nlearns from limited pixel-wise annotated samples while exploiting additional\nannotation-free images. It uses two network branches that link semi-supervised\nclassification with semi-supervised segmentation including self-training. The\ndual-branch approach reduces both the low-level and the high-level artifacts\ntypical when training with few labels. The approach attains significant\nimprovement over existing methods, especially when trained with very few\nlabeled samples. On several standard benchmarks - PASCAL VOC 2012,\nPASCAL-Context, and Cityscapes - the approach achieves new state-of-the-art in\nsemi-supervised learning.\n", "versions": [{"version": "v1", "created": "Thu, 15 Aug 2019 19:32:49 GMT"}], "update_date": "2019-08-19", "authors_parsed": [["Mittal", "Sudhanshu", ""], ["Tatarchenko", "Maxim", ""], ["Brox", "Thomas", ""]]}, {"id": "1908.05730", "submitter": "Redha Ali", "authors": "Redha Ali, Russell C. Hardie, Manawaduge Supun De Silva, and Temesguen\n  Messay Kebede", "title": "Skin Lesion Segmentation and Classification for ISIC 2018 by Combining\n  Deep CNN and Handcrafted Features", "comments": "4 pages and 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This short report describes our submission to the ISIC 2018 Challenge in Skin\nLesion Analysis Towards Melanoma Detection for Task1 and Task 3. This work has\nbeen accomplished by a team of researchers at the University of Dayton Signal\nand Image Processing Lab. Our proposed approach is computationally efficient\nare combines information from both deep learning and handcrafted features. For\nTask3, we form a new type of image features, called hybrid features, which has\nstronger discrimination ability than single method features. These features are\nutilized as inputs to a decision-making model that is based on a multiclass\nSupport Vector Machine (SVM) classifier. The proposed technique is evaluated on\nonline validation databases. Our score was 0.841 with SVM classifier on the\nvalidation dataset.\n", "versions": [{"version": "v1", "created": "Wed, 14 Aug 2019 02:48:49 GMT"}], "update_date": "2019-08-19", "authors_parsed": [["Ali", "Redha", ""], ["Hardie", "Russell C.", ""], ["De Silva", "Manawaduge Supun", ""], ["Kebede", "Temesguen Messay", ""]]}, {"id": "1908.05750", "submitter": "Neeraj Battan", "authors": "Neeraj Battan, Abbhinav Venkat and Avinash Sharma", "title": "DeepHuMS: Deep Human Motion Signature for 3D Skeletal Sequences", "comments": "Under Review, Conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  3D Human Motion Indexing and Retrieval is an interesting problem due to the\nrise of several data-driven applications aimed at analyzing and/or re-utilizing\n3D human skeletal data, such as data-driven animation, analysis of sports\nbio-mechanics, human surveillance etc. Spatio-temporal articulations of humans,\nnoisy/missing data, different speeds of the same motion etc. make it\nchallenging and several of the existing state of the art methods use hand-craft\nfeatures along with optimization based or histogram based comparison in order\nto perform retrieval. Further, they demonstrate it only for very small datasets\nand few classes. We make a case for using a learned representation that should\nrecognize the motion as well as enforce a discriminative ranking. To that end,\nwe propose, a 3D human motion descriptor learned using a deep network. Our\nlearned embedding is generalizable and applicable to real-world data -\naddressing the aforementioned challenges and further enables sub-motion\nsearching in its embedding space using another network. Our model exploits the\ninter-class similarity using trajectory cues, and performs far superior in a\nself-supervised setting. State of the art results on all these fronts is shown\non two large scale 3D human motion datasets - NTU RGB+D and HDM05.\n", "versions": [{"version": "v1", "created": "Thu, 15 Aug 2019 20:34:22 GMT"}, {"version": "v2", "created": "Mon, 19 Aug 2019 20:08:12 GMT"}, {"version": "v3", "created": "Sun, 8 Dec 2019 04:18:14 GMT"}], "update_date": "2019-12-11", "authors_parsed": [["Battan", "Neeraj", ""], ["Venkat", "Abbhinav", ""], ["Sharma", "Avinash", ""]]}, {"id": "1908.05770", "submitter": "Jizong Peng", "authors": "Jizong Peng, Hoel Kervadec, Jose Dolz, Ismail Ben Ayed, Marco\n  Pedersoli, Christian Desrosiers", "title": "Discretely-constrained deep network for weakly supervised segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An efficient strategy for weakly-supervised segmentation is to impose\nconstraints or regularization priors on target regions. Recent efforts have\nfocused on incorporating such constraints in the training of convolutional\nneural networks (CNN), however this has so far been done within a continuous\noptimization framework. Yet, various segmentation constraints and\nregularization can be modeled and optimized more efficiently in a discrete\nformulation. This paper proposes a method, based on the alternating direction\nmethod of multipliers (ADMM) algorithm, to train a CNN with discrete\nconstraints and regularization priors. This method is applied to the\nsegmentation of medical images with weak annotations, where both size\nconstraints and boundary length regularization are enforced. Experiments on a\nbenchmark cardiac segmentation dataset show our method to yield a performance\nnear to full supervision.\n", "versions": [{"version": "v1", "created": "Thu, 15 Aug 2019 21:19:40 GMT"}], "update_date": "2019-08-19", "authors_parsed": [["Peng", "Jizong", ""], ["Kervadec", "Hoel", ""], ["Dolz", "Jose", ""], ["Ayed", "Ismail Ben", ""], ["Pedersoli", "Marco", ""], ["Desrosiers", "Christian", ""]]}, {"id": "1908.05782", "submitter": "Ouwen Huang", "authors": "Ouwen Huang, Will Long, Nick Bottenus, Gregg E. Trahey, Sina Farsiu,\n  Mark L. Palmeri", "title": "MimickNet, Matching Clinical Post-Processing Under Realistic Black-Box\n  Constraints", "comments": "This work has been submitted to the IEEE Transactions on Medical\n  Imaging on July 1st, 2019 for possible publication. Copyright may be\n  transferred without notice, after which this version may no longer be\n  accessible", "journal-ref": null, "doi": "10.1109/TMI.2020.2970867", "report-no": null, "categories": "eess.IV cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image post-processing is used in clinical-grade ultrasound scanners to\nimprove image quality (e.g., reduce speckle noise and enhance contrast). These\npost-processing techniques vary across manufacturers and are generally kept\nproprietary, which presents a challenge for researchers looking to match\ncurrent clinical-grade workflows. We introduce a deep learning framework,\nMimickNet, that transforms raw conventional delay-and-summed (DAS) beams into\nthe approximate post-processed images found on clinical-grade scanners.\nTraining MimickNet only requires post-processed image samples from a scanner of\ninterest without the need for explicit pairing to raw DAS data. This\nflexibility allows it to hypothetically approximate any manufacturer's\npost-processing without access to the pre-processed data. MimickNet generates\nimages with an average similarity index measurement (SSIM) of 0.930$\\pm$0.0892\non a 300 cineloop test set, and it generalizes to cardiac cineloops outside of\nour train-test distribution achieving an SSIM of 0.967$\\pm$0.002. We also\nexplore the theoretical SSIM achievable by evaluating MimickNet performance\nwhen trained under gray-box constraints (i.e., when both pre-processed and\npost-processed images are available). To our knowledge, this is the first work\nto establish deep learning models that closely approximate current\nclinical-grade ultrasound post-processing under realistic black-box constraints\nwhere before and after post-processing data is unavailable. MimickNet serves as\na clinical post-processing baseline for future works in ultrasound image\nformation to compare against. To this end, we have made the MimickNet software\nopen source.\n", "versions": [{"version": "v1", "created": "Thu, 15 Aug 2019 22:10:41 GMT"}], "update_date": "2020-03-10", "authors_parsed": [["Huang", "Ouwen", ""], ["Long", "Will", ""], ["Bottenus", "Nick", ""], ["Trahey", "Gregg E.", ""], ["Farsiu", "Sina", ""], ["Palmeri", "Mark L.", ""]]}, {"id": "1908.05786", "submitter": "Kyle Min", "authors": "Kyle Min, Jason J. Corso", "title": "TASED-Net: Temporally-Aggregating Spatial Encoder-Decoder Network for\n  Video Saliency Detection", "comments": "ICCV 2019 camera ready (Supplementary material: on CVF soon)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  TASED-Net is a 3D fully-convolutional network architecture for video saliency\ndetection. It consists of two building blocks: first, the encoder network\nextracts low-resolution spatiotemporal features from an input clip of several\nconsecutive frames, and then the following prediction network decodes the\nencoded features spatially while aggregating all the temporal information. As a\nresult, a single prediction map is produced from an input clip of multiple\nframes. Frame-wise saliency maps can be predicted by applying TASED-Net in a\nsliding-window fashion to a video. The proposed approach assumes that the\nsaliency map of any frame can be predicted by considering a limited number of\npast frames. The results of our extensive experiments on video saliency\ndetection validate this assumption and demonstrate that our fully-convolutional\nmodel with temporal aggregation method is effective. TASED-Net significantly\noutperforms previous state-of-the-art approaches on all three major large-scale\ndatasets of video saliency detection: DHF1K, Hollywood2, and UCFSports. After\nanalyzing the results qualitatively, we observe that our model is especially\nbetter at attending to salient moving objects.\n", "versions": [{"version": "v1", "created": "Thu, 15 Aug 2019 22:30:50 GMT"}], "update_date": "2019-08-19", "authors_parsed": [["Min", "Kyle", ""], ["Corso", "Jason J.", ""]]}, {"id": "1908.05794", "submitter": "Dan Xu", "authors": "Mihai Marian Puscas, Dan Xu, Andrea Pilzer, Nicu Sebe", "title": "Structured Coupled Generative Adversarial Networks for Unsupervised\n  Monocular Depth Estimation", "comments": "Accepted at 3DV 2019 as ORAL", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inspired by the success of adversarial learning, we propose a new end-to-end\nunsupervised deep learning framework for monocular depth estimation consisting\nof two Generative Adversarial Networks (GAN), deeply coupled with a structured\nConditional Random Field (CRF) model. The two GANs aim at generating distinct\nand complementary disparity maps and at improving the generation quality via\nexploiting the adversarial learning strategy. The deep CRF coupling model is\nproposed to fuse the generative and discriminative outputs from the dual GAN\nnets. As such, the model implicitly constructs mutual constraints on the two\nnetwork branches and between the generator and discriminator. This facilitates\nthe optimization of the whole network for better disparity generation.\nExtensive experiments on the KITTI, Cityscapes, and Make3D datasets clearly\ndemonstrate the effectiveness of the proposed approach and show superior\nperformance compared to state of the art methods. The code and models are\navailable at https://github.com/mihaipuscas/ 3dv---coupled-crf-disparity.\n", "versions": [{"version": "v1", "created": "Thu, 15 Aug 2019 23:26:59 GMT"}], "update_date": "2019-08-19", "authors_parsed": [["Puscas", "Mihai Marian", ""], ["Xu", "Dan", ""], ["Pilzer", "Andrea", ""], ["Sebe", "Nicu", ""]]}, {"id": "1908.05806", "submitter": "Jinkun Cao", "authors": "Jinkun Cao, Hongyang Tang, Hao-Shu Fang, Xiaoyong Shen, Cewu Lu and\n  Yu-Wing Tai", "title": "Cross-Domain Adaptation for Animal Pose Estimation", "comments": "accepted by ICCV'2019 for oral presentation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we are interested in pose estimation of animals. Animals\nusually exhibit a wide range of variations on poses and there is no available\nanimal pose dataset for training and testing. To address this problem, we build\nan animal pose dataset to facilitate training and evaluation. Considering the\nheavy labor needed to label dataset and it is impossible to label data for all\nconcerned animal species, we, therefore, proposed a novel cross-domain\nadaptation method to transform the animal pose knowledge from labeled animal\nclasses to unlabeled animal classes. We use the modest animal pose dataset to\nadapt learned knowledge to multiple animals species. Moreover, humans also\nshare skeleton similarities with some animals (especially four-footed mammals).\nTherefore, the easily available human pose dataset, which is of a much larger\nscale than our labeled animal dataset, provides important prior knowledge to\nboost up the performance on animal pose estimation. Experiments show that our\nproposed method leverages these pieces of prior knowledge well and achieves\nconvincing results on animal pose estimation.\n", "versions": [{"version": "v1", "created": "Fri, 16 Aug 2019 01:16:30 GMT"}, {"version": "v2", "created": "Mon, 19 Aug 2019 00:20:06 GMT"}], "update_date": "2019-08-20", "authors_parsed": [["Cao", "Jinkun", ""], ["Tang", "Hongyang", ""], ["Fang", "Hao-Shu", ""], ["Shen", "Xiaoyong", ""], ["Lu", "Cewu", ""], ["Tai", "Yu-Wing", ""]]}, {"id": "1908.05819", "submitter": "Binghui Chen", "authors": "Binghui Chen, Weihong Deng, Jiani Hu", "title": "Mixed High-Order Attention Network for Person Re-Identification", "comments": "ICCV 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Attention has become more attractive in person reidentification (ReID) as it\nis capable of biasing the allocation of available resources towards the most\ninformative parts of an input signal. However, state-of-the-art works\nconcentrate only on coarse or first-order attention design, e.g. spatial and\nchannels attention, while rarely exploring higher-order attention mechanism. We\ntake a step towards addressing this problem. In this paper, we first propose\nthe High-Order Attention (HOA) module to model and utilize the complex and\nhigh-order statistics information in attention mechanism, so as to capture the\nsubtle differences among pedestrians and to produce the discriminative\nattention proposals. Then, rethinking person ReID as a zero-shot learning\nproblem, we propose the Mixed High-Order Attention Network (MHN) to further\nenhance the discrimination and richness of attention knowledge in an explicit\nmanner. Extensive experiments have been conducted to validate the superiority\nof our MHN for person ReID over a wide variety of state-of-the-art methods on\nthree large-scale datasets, including Market-1501, DukeMTMC-ReID and CUHK03-NP.\nCode is available at http://www.bhchen.cn/.\n", "versions": [{"version": "v1", "created": "Fri, 16 Aug 2019 02:38:21 GMT"}], "update_date": "2019-08-19", "authors_parsed": [["Chen", "Binghui", ""], ["Deng", "Weihong", ""], ["Hu", "Jiani", ""]]}, {"id": "1908.05825", "submitter": "Riddhish Bhalodia", "authors": "Riddhish Bhalodia and Shireen Y. Elhabian and Ladislav Kavan and Ross\n  T. Whitaker", "title": "A Cooperative Autoencoder for Population-Based Regularization of CNN\n  Image Registration", "comments": "To appear in MICCAI 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spatial transformations are enablers in a variety of medical image analysis\napplications that entail aligning images to a common coordinate systems.\nPopulation analysis of such transformations is expected to capture the\nunderlying image and shape variations, and hence these transformations are\nrequired to produce anatomically feasible correspondences. This is usually\nenforced through some smoothness-based generic regularization on deformation\nfield. Alternatively, population-based regularization has been shown to produce\nanatomically accurate correspondences in cases where anatomically unaware\n(i.e., data independent) fail. Recently, deep networks have been for\nunsupervised image registration, these methods are computationally faster and\nmaintains the accuracy of state of the art methods. However, these networks use\nsmoothness penalty on deformation fields and ignores population-level\nstatistics of the transformations. We propose a novel neural network\narchitecture that simultaneously learns and uses the population-level\nstatistics of the spatial transformations to regularize the neural networks for\nunsupervised image registration. This regularization is in the form of a\nbottleneck autoencoder, which encodes the population level information of the\ndeformation fields in a low-dimensional manifold. The proposed architecture\nproduces deformation fields that describe the population-level features and\nassociated correspondences in an anatomically relevant manner and are\nstatistically compact relative to the state-of-the-art approaches while\nmaintaining computational efficiency. We demonstrate the efficacy of the\nproposed architecture on synthetic data sets, as well as 2D and 3D medical\ndata.\n", "versions": [{"version": "v1", "created": "Fri, 16 Aug 2019 03:06:19 GMT"}, {"version": "v2", "created": "Mon, 19 Aug 2019 14:44:47 GMT"}], "update_date": "2019-08-20", "authors_parsed": [["Bhalodia", "Riddhish", ""], ["Elhabian", "Shireen Y.", ""], ["Kavan", "Ladislav", ""], ["Whitaker", "Ross T.", ""]]}, {"id": "1908.05832", "submitter": "Huajie Jiang", "authors": "Huajie Jiang, Ruiping Wang, Shiguang Shan, Xilin Chen", "title": "Transferable Contrastive Network for Generalized Zero-Shot Learning", "comments": "To appear in ICCV 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Zero-shot learning (ZSL) is a challenging problem that aims to recognize the\ntarget categories without seen data, where semantic information is leveraged to\ntransfer knowledge from some source classes. Although ZSL has made great\nprogress in recent years, most existing approaches are easy to overfit the\nsources classes in generalized zero-shot learning (GZSL) task, which indicates\nthat they learn little knowledge about target classes. To tackle such problem,\nwe propose a novel Transferable Contrastive Network (TCN) that explicitly\ntransfers knowledge from the source classes to the target classes. It\nautomatically contrasts one image with different classes to judge whether they\nare consistent or not. By exploiting the class similarities to make knowledge\ntransfer from source images to similar target classes, our approach is more\nrobust to recognize the target images. Experiments on five benchmark datasets\nshow the superiority of our approach for GZSL.\n", "versions": [{"version": "v1", "created": "Fri, 16 Aug 2019 03:35:36 GMT"}], "update_date": "2019-08-19", "authors_parsed": [["Jiang", "Huajie", ""], ["Wang", "Ruiping", ""], ["Shan", "Shiguang", ""], ["Chen", "Xilin", ""]]}, {"id": "1908.05840", "submitter": "Ho Young Jhoo", "authors": "Hyunsu Kim, Ho Young Jhoo, Eunhyeok Park, and Sungjoo Yoo", "title": "Tag2Pix: Line Art Colorization Using Text Tag With SECat and Changing\n  Loss", "comments": "Accepted to ICCV 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Line art colorization is expensive and challenging to automate. A GAN\napproach is proposed, called Tag2Pix, of line art colorization which takes as\ninput a grayscale line art and color tag information and produces a quality\ncolored image. First, we present the Tag2Pix line art colorization dataset. A\ngenerator network is proposed which consists of convolutional layers to\ntransform the input line art, a pre-trained semantic extraction network, and an\nencoder for input color information. The discriminator is based on an auxiliary\nclassifier GAN to classify the tag information as well as genuineness. In\naddition, we propose a novel network structure called SECat, which makes the\ngenerator properly colorize even small features such as eyes, and also suggest\na novel two-step training method where the generator and discriminator first\nlearn the notion of object and shape and then, based on the learned notion,\nlearn colorization, such as where and how to place which color. We present both\nquantitative and qualitative evaluations which prove the effectiveness of the\nproposed method.\n", "versions": [{"version": "v1", "created": "Fri, 16 Aug 2019 04:24:38 GMT"}], "update_date": "2019-08-19", "authors_parsed": [["Kim", "Hyunsu", ""], ["Jhoo", "Ho Young", ""], ["Park", "Eunhyeok", ""], ["Yoo", "Sungjoo", ""]]}, {"id": "1908.05841", "submitter": "Tai-Long He", "authors": "Tai-Long He, Dylan B. A. Jones, Binxuan Huang, Yuyang Liu, Kazuyuki\n  Miyazaki, Zhe Jiang, E. Charlie White, Helen M. Worden, John R. Worden", "title": "Recurrent U-net: Deep learning to predict daily summertime ozone in the\n  United States", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.ao-ph cs.CV cs.LG physics.chem-ph physics.geo-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We use a hybrid deep learning model to predict June-July-August (JJA) daily\nmaximum 8-h average (MDA8) surface ozone concentrations in the US. A set of\nmeteorological fields from the ERA-Interim reanalysis as well as monthly mean\nNO$_x$ emissions from the Community Emissions Data System (CEDS) inventory are\nselected as predictors. Ozone measurements from the US Environmental Protection\nAgency (EPA) Air Quality System (AQS) from 1980 to 2009 are used to train the\nmodel, whereas data from 2010 to 2014 are used to evaluate the performance of\nthe model. The model captures well daily, seasonal and interannual variability\nin MDA8 ozone across the US. Feature maps show that the model captures\nteleconnections between MDA8 ozone and the meteorological fields, which are\nresponsible for driving the ozone dynamics. We used the model to evaluate\nrecent trends in NO$_x$ emissions in the US and found that the trend in the EPA\nemission inventory produced the largest negative bias in MDA8 ozone between\n2010-2016. The top-down emission trends from the Tropospheric Chemistry\nReanalysis (TCR-2), which is based on satellite observations, produced\npredictions in best agreement with observations. In urban regions, the trend in\nAQS NO$_2$ observations provided ozone predictions in agreement with\nobservations, whereas in rural regions the satellite-derived trends produced\nthe best agreement. In both rural and urban regions the EPA trend resulted in\nthe largest negative bias in predicted ozone. Our results suggest that the EPA\ninventory is overestimating the reductions in NO$_x$ emissions and that the\nsatellite-derived trend reflects the influence of reductions in NO$_x$\nemissions as well as changes in background NO$_x$. Our results demonstrate the\nsignificantly greater predictive capability that the deep learning model\nprovides over conventional atmospheric chemical transport models for air\nquality analyses.\n", "versions": [{"version": "v1", "created": "Fri, 16 Aug 2019 04:32:00 GMT"}], "update_date": "2019-08-19", "authors_parsed": [["He", "Tai-Long", ""], ["Jones", "Dylan B. A.", ""], ["Huang", "Binxuan", ""], ["Liu", "Yuyang", ""], ["Miyazaki", "Kazuyuki", ""], ["Jiang", "Zhe", ""], ["White", "E. Charlie", ""], ["Worden", "Helen M.", ""], ["Worden", "John R.", ""]]}, {"id": "1908.05858", "submitter": "Ting Yao", "authors": "Jianhao Zhang, Yingwei Pan, Ting Yao, He Zhao, Tao Mei", "title": "daBNN: A Super Fast Inference Framework for Binary Neural Networks on\n  ARM devices", "comments": "Accepted by 2019 ACMMM Open Source Software Competition. Source code:\n  https://github.com/JDAI-CV/dabnn", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is always well believed that Binary Neural Networks (BNNs) could\ndrastically accelerate the inference efficiency by replacing the arithmetic\noperations in float-valued Deep Neural Networks (DNNs) with bit-wise\noperations. Nevertheless, there has not been open-source implementation in\nsupport of this idea on low-end ARM devices (e.g., mobile phones and embedded\ndevices). In this work, we propose daBNN --- a super fast inference framework\nthat implements BNNs on ARM devices. Several speed-up and memory refinement\nstrategies for bit-packing, binarized convolution, and memory layout are\nuniquely devised to enhance inference efficiency. Compared to the recent\nopen-source BNN inference framework, BMXNet, our daBNN is\n$7\\times$$\\sim$$23\\times$ faster on a single binary convolution, and about\n$6\\times$ faster on Bi-Real Net 18 (a BNN variant of ResNet-18). The daBNN is a\nBSD-licensed inference framework, and its source code, sample projects and\npre-trained models are available on-line: https://github.com/JDAI-CV/dabnn.\n", "versions": [{"version": "v1", "created": "Fri, 16 Aug 2019 06:07:57 GMT"}], "update_date": "2019-08-19", "authors_parsed": [["Zhang", "Jianhao", ""], ["Pan", "Yingwei", ""], ["Yao", "Ting", ""], ["Zhao", "He", ""], ["Mei", "Tao", ""]]}, {"id": "1908.05860", "submitter": "Peng Chen", "authors": "Peng Chen, Tong Jia, Pengfei Wu, Jianjun Wu, Dongyue Chen", "title": "Learning Deep Representations by Mutual Information for Person\n  Re-identification", "comments": "8 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most existing person re-identification (ReID) methods have good feature\nrepresentations to distinguish pedestrians with deep convolutional neural\nnetwork (CNN) and metric learning methods. However, these works concentrate on\nthe similarity between encoder output and ground-truth, ignoring the\ncorrelation between input and encoder output, which affects the performance of\nidentifying different pedestrians. To address this limitation, We design a Deep\nInfoMax (DIM) network to maximize the mutual information (MI) between the input\nimage and encoder output, which doesn't need any auxiliary labels. To evaluate\nthe effectiveness of the DIM network, we propose end-to-end Global-DIM and\nLocal-DIM models. Additionally, the DIM network provides a new solution for\ncross-dataset unsupervised ReID issue as it needs no extra labels. The\nexperiments prove the superiority of MI theory on the ReID issue, which\nachieves the state-of-the-art results.\n", "versions": [{"version": "v1", "created": "Fri, 16 Aug 2019 06:15:30 GMT"}], "update_date": "2019-08-19", "authors_parsed": [["Chen", "Peng", ""], ["Jia", "Tong", ""], ["Wu", "Pengfei", ""], ["Wu", "Jianjun", ""], ["Chen", "Dongyue", ""]]}, {"id": "1908.05861", "submitter": "Avisek Lahiri", "authors": "Avisek Lahiri, Arnav Kumar Jain, Prabir Kumar Biswas", "title": "The Angel is in the Priors: Improving GAN based Image and Sequence\n  Inpainting with Better Noise and Structural Priors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Contemporary deep learning based inpainting algorithms are mainly based on a\nhybrid dual stage training policy of supervised reconstruction loss followed by\nan unsupervised adversarial critic loss. However, there is a dearth of\nliterature for a fully unsupervised GAN based inpainting framework. The primary\naversion towards the latter genre is due to its prohibitively slow iterative\noptimization requirement during inference to find a matching noise prior for a\nmasked image. In this paper, we show that priors matter in GAN: we learn a data\ndriven parametric network to predict a matching prior for a given image. This\nconverts an iterative paradigm to a single feed forward inference pipeline with\na massive 1500X speedup and simultaneous improvement in reconstruction quality.\nWe show that an additional structural prior imposed on GAN model results in\nhigher fidelity outputs. To extend our model for sequence inpainting, we\npropose a recurrent net based grouped noise prior learning. To our knowledge,\nthis is the first demonstration of an unsupervised GAN based sequence\ninpainting. A further improvement in sequence inpainting is achieved with an\nadditional subsequence consistency loss. These contributions improve the\nspatio-temporal characteristics of reconstructed sequences. Extensive\nexperiments conducted on SVHN, Standford Cars, CelebA and CelebA-HQ image\ndatasets, synthetic sequences and ViDTIMIT video datasets reveal that we\nconsistently improve upon previous unsupervised baseline and also achieve\ncomparable performances(sometimes also better) to hybrid benchmarks.\n", "versions": [{"version": "v1", "created": "Fri, 16 Aug 2019 06:30:40 GMT"}], "update_date": "2019-08-19", "authors_parsed": [["Lahiri", "Avisek", ""], ["Jain", "Arnav Kumar", ""], ["Biswas", "Prabir Kumar", ""]]}, {"id": "1908.05867", "submitter": "Zhaoyang Zhang", "authors": "Zhaoyang Zhang, Jingyu Li, Wenqi Shao, Zhanglin Peng, Ruimao Zhang,\n  Xiaogang Wang, Ping Luo", "title": "Differentiable Learning-to-Group Channels via Groupable Convolutional\n  Neural Networks", "comments": "accepted by ICCV 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Group convolution, which divides the channels of ConvNets into groups, has\nachieved impressive improvement over the regular convolution operation.\nHowever, existing models, eg. ResNeXt, still suffers from the sub-optimal\nperformance due to manually defining the number of groups as a constant over\nall of the layers. Toward addressing this issue, we present Groupable ConvNet\n(GroupNet) built by using a novel dynamic grouping convolution (DGConv)\noperation, which is able to learn the number of groups in an end-to-end manner.\nThe proposed approach has several appealing benefits. (1) DGConv provides a\nunified convolution representation and covers many existing convolution\noperations such as regular dense convolution, group convolution, and depthwise\nconvolution. (2) DGConv is a differentiable and flexible operation which learns\nto perform various convolutions from training data. (3) GroupNet trained with\nDGConv learns different number of groups for different convolution layers.\nExtensive experiments demonstrate that GroupNet outperforms its counterparts\nsuch as ResNet and ResNeXt in terms of accuracy and computational complexity.\nWe also present introspection and reproducibility study, for the first time,\nshowing the learning dynamics of training group numbers.\n", "versions": [{"version": "v1", "created": "Fri, 16 Aug 2019 06:50:33 GMT"}, {"version": "v2", "created": "Mon, 19 Aug 2019 08:06:53 GMT"}], "update_date": "2019-08-20", "authors_parsed": [["Zhang", "Zhaoyang", ""], ["Li", "Jingyu", ""], ["Shao", "Wenqi", ""], ["Peng", "Zhanglin", ""], ["Zhang", "Ruimao", ""], ["Wang", "Xiaogang", ""], ["Luo", "Ping", ""]]}, {"id": "1908.05868", "submitter": "Dongming Sun", "authors": "Lei Sun, Kaiwei Wang, Kailun Yang, and Kaite Xiang", "title": "See Clearer at Night: Towards Robust Nighttime Semantic Segmentation\n  through Day-Night Image Conversion", "comments": "13 pages, 7 figures, 2 tables, 2 equations. Artificial Intelligence\n  and Machine Learning in Defense Applications, SPIE Security + Defence 2019,\n  Strasbourg, France, September 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Currently, semantic segmentation shows remarkable efficiency and reliability\nin standard scenarios such as daytime scenes with favorable illumination\nconditions. However, in face of adverse conditions such as the nighttime,\nsemantic segmentation loses its accuracy significantly. One of the main causes\nof the problem is the lack of sufficient annotated segmentation datasets of\nnighttime scenes. In this paper, we propose a framework to alleviate the\naccuracy decline when semantic segmentation is taken to adverse conditions by\nusing Generative Adversarial Networks (GANs). To bridge the daytime and\nnighttime image domains, we made key observation that compared to datasets in\nadverse conditions, there are considerable amount of segmentation datasets in\nstandard conditions such as BDD and our collected ZJU datasets. Our GAN-based\nnighttime semantic segmentation framework includes two methods. In the first\nmethod, GANs were used to translate nighttime images to the daytime, thus\nsemantic segmentation can be performed using robust models already trained on\ndaytime datasets. In another method, we use GANs to translate different ratio\nof daytime images in the dataset to the nighttime but still with their labels.\nIn this sense, synthetic nighttime segmentation datasets can be generated to\nyield models prepared to operate at nighttime conditions robustly. In our\nexperiment, the later method significantly boosts the performance at the\nnighttime evidenced by quantitative results using Intersection over Union (IoU)\nand Pixel Accuracy (Acc). We show that the performance varies with respect to\nthe proportion of synthetic nighttime images in the dataset, where the sweet\nspot corresponds to most robust performance across the day and night.\n", "versions": [{"version": "v1", "created": "Fri, 16 Aug 2019 06:56:55 GMT"}], "update_date": "2019-08-19", "authors_parsed": [["Sun", "Lei", ""], ["Wang", "Kaiwei", ""], ["Yang", "Kailun", ""], ["Xiang", "Kaite", ""]]}, {"id": "1908.05874", "submitter": "Dan Nguyen", "authors": "Dan Nguyen, Rafe McBeth, Azar Sadeghnejad Barkousaraie, Gyanendra\n  Bohara, Chenyang Shen, Xun Jia, Steve Jiang", "title": "Incorporating human and learned domain knowledge into training deep\n  neural networks: A differentiable dose volume histogram and adversarial\n  inspired framework for generating Pareto optimal dose distributions in\n  radiation therapy", "comments": null, "journal-ref": null, "doi": "10.1002/mp.13955", "report-no": null, "categories": "physics.med-ph cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel domain specific loss, which is a differentiable loss\nfunction based on the dose volume histogram, and combine it with an adversarial\nloss for the training of deep neural networks to generate Pareto optimal dose\ndistributions. The mean squared error (MSE) loss, dose volume histogram (DVH)\nloss, and adversarial (ADV) loss were used to train 4 instances of the neural\nnetwork model: 1) MSE, 2) MSE+ADV, 3) MSE+DVH, and 4) MSE+DVH+ADV. 70 prostate\npatients were acquired, and the dose influence arrays were calculated for each\npatient. 1200 Pareto surface plans per patient were generated by\npseudo-randomizing the tradeoff weights (84,000 plans total). We divided the\ndata into 54 training, 6 validation, and 10 testing patients. Each model was\ntrained for 100,000 iterations, with a batch size of 2. The prediction time of\neach model is 0.052 seconds. Quantitatively, the MSE+DVH+ADV model had the\nlowest prediction error of 0.038 (conformation), 0.026 (homogeneity), 0.298\n(R50), 1.65% (D95), 2.14% (D98), 2.43% (D99). The MSE model had the worst\nprediction error of 0.134 (conformation), 0.041 (homogeneity), 0.520 (R50),\n3.91% (D95), 4.33% (D98), 4.60% (D99). For both the mean dose PTV error and the\nmax dose PTV, Body, Bladder and rectum error, the MSE+DVH+ADV outperformed all\nother models. All model's predictions have an average mean and max dose error\nless than 2.8% and 4.2%, respectively. Expert human domain specific knowledge\ncan be the largest driver in the performance improvement, and adversarial\nlearning can be used to further capture nuanced features. The real-time\nprediction capabilities allow for a physician to quickly navigate the tradeoff\nspace, and produce a dose distribution as a tangible endpoint for the\ndosimetrist to use for planning. This can considerably reduce the treatment\nplanning time, allowing for clinicians to focus their efforts on challenging\ncases.\n", "versions": [{"version": "v1", "created": "Fri, 16 Aug 2019 07:45:38 GMT"}, {"version": "v2", "created": "Sat, 7 Dec 2019 12:19:23 GMT"}], "update_date": "2020-07-01", "authors_parsed": [["Nguyen", "Dan", ""], ["McBeth", "Rafe", ""], ["Barkousaraie", "Azar Sadeghnejad", ""], ["Bohara", "Gyanendra", ""], ["Shen", "Chenyang", ""], ["Jia", "Xun", ""], ["Jiang", "Steve", ""]]}, {"id": "1908.05877", "submitter": "Xun Xu", "authors": "Xun Xu and Shaogang Gong and Timothy Hospedales", "title": "Zero-Shot Crowd Behavior Recognition", "comments": "Group and Crowd Behavior for Computer Vision 2017, Pages 341-369", "journal-ref": null, "doi": "10.1016/B978-0-12-809276-7.00018-7", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding crowd behavior in video is challenging for computer vision.\nThere have been increasing attempts on modeling crowded scenes by introducing\never larger property ontologies (attributes) and annotating ever larger\ntraining datasets. However, in contrast to still images, manually annotating\nvideo attributes needs to consider spatiotemporal evolution which is inherently\nmuch harder and more costly. Critically, the most interesting crowd behaviors\ncaptured in surveillance videos (e.g., street fighting, flash mobs) are either\nrare, thus have few examples for model training, or unseen previously. Existing\ncrowd analysis techniques are not readily scalable to recognize novel (unseen)\ncrowd behaviors. To address this problem, we investigate and develop methods\nfor recognizing visual crowd behavioral attributes without any training\nsamples, i.e., zero-shot learning crowd behavior recognition. To that end, we\nrelax the common assumption that each individual crowd video instance is only\nassociated with a single crowd attribute. Instead, our model learns to jointly\nrecognize multiple crowd behavioral attributes in each video instance by\nexploring multiattribute cooccurrence as contextual knowledge for optimizing\nindividual crowd attribute recognition. Joint multilabel attribute prediction\nin zero-shot learning is inherently nontrivial because cooccurrence statistics\ndoes not exist for unseen attributes. To solve this problem, we learn to\npredict cross-attribute cooccurrence from both online text corpus and\nmultilabel annotation of videos with known attributes. Our experiments show\nthat this approach to modeling multiattribute context not only improves\nzero-shot crowd behavior recognition on the WWW crowd video dataset, but also\ngeneralizes to novel behavior (violence) detection cross-domain in the Violence\nFlow video dataset.\n", "versions": [{"version": "v1", "created": "Fri, 16 Aug 2019 08:02:48 GMT"}], "update_date": "2019-08-19", "authors_parsed": [["Xu", "Xun", ""], ["Gong", "Shaogang", ""], ["Hospedales", "Timothy", ""]]}, {"id": "1908.05884", "submitter": "Jue Wang", "authors": "Jue Wang and Anoop Cherian", "title": "GODS: Generalized One-class Discriminative Subspaces for Anomaly\n  Detection", "comments": "Accepted by ICCV 2019, 8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  One-class learning is the classic problem of fitting a model to data for\nwhich annotations are available only for a single class. In this paper, we\npropose a novel objective for one-class learning. Our key idea is to use a pair\nof orthonormal frames -- as subspaces -- to \"sandwich\" the labeled data via\noptimizing for two objectives jointly: i) minimize the distance between the\norigins of the two subspaces, and ii) to maximize the margin between the\nhyperplanes and the data, either subspace demanding the data to be in its\npositive and negative orthant respectively. Our proposed objective however\nleads to a non-convex optimization problem, to which we resort to Riemannian\noptimization schemes and derive an efficient conjugate gradient scheme on the\nStiefel manifold. To study the effectiveness of our scheme, we propose a new\ndataset~\\emph{Dash-Cam-Pose}, consisting of clips with skeleton poses of humans\nseated in a car, the task being to classify the clips as normal or abnormal;\nthe latter is when any human pose is out-of-position with regard to say an\nairbag deployment. Our experiments on the proposed Dash-Cam-Pose dataset, as\nwell as several other standard anomaly/novelty detection benchmarks demonstrate\nthe benefits of our scheme, achieving state-of-the-art one-class accuracy.\n", "versions": [{"version": "v1", "created": "Fri, 16 Aug 2019 08:19:20 GMT"}], "update_date": "2019-08-19", "authors_parsed": [["Wang", "Jue", ""], ["Cherian", "Anoop", ""]]}, {"id": "1908.05887", "submitter": "Xiangyu Li", "authors": "Xiangyu Li, Gongning Luo and Kuanquan Wang", "title": "Multi-step Cascaded Networks for Brain Tumor Segmentation", "comments": "Paper for BraTS 2019 runs in conjunction with the MICCAI 2019\n  conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic brain tumor segmentation method plays an extremely important role\nin the whole process of brain tumor diagnosis and treatment. In this paper, we\npropose a multi-step cascaded network which takes the hierarchical topology of\nthe brain tumor substructures into consideration and segments the substructures\nfrom coarse to fine .During segmentation, the result of the former step is\nutilized as the prior information for the next step to guide the finer\nsegmentation process. The whole network is trained in an end-to-end fashion.\nBesides, to alleviate the gradient vanishing issue and reduce overfitting, we\nadded several auxiliary outputs as a kind of deep supervision for each step and\nintroduced several data augmentation strategies, respectively, which proved to\nbe quite efficient for brain tumor segmentation. Lastly, focal loss is utilized\nto solve the problem of remarkably imbalance of the tumor regions and\nbackground. Our model is tested on the BraTS 2019 validation dataset, the\npreliminary results of mean dice coefficients are 0.886, 0.813, 0.771 for the\nwhole tumor, tumor core and enhancing tumor respectively. Code is available at\nhttps://github.com/JohnleeHIT/Brats2019\n", "versions": [{"version": "v1", "created": "Fri, 16 Aug 2019 08:39:13 GMT"}, {"version": "v2", "created": "Tue, 24 Sep 2019 16:01:38 GMT"}, {"version": "v3", "created": "Wed, 25 Sep 2019 11:03:46 GMT"}], "update_date": "2019-09-26", "authors_parsed": [["Li", "Xiangyu", ""], ["Luo", "Gongning", ""], ["Wang", "Kuanquan", ""]]}, {"id": "1908.05898", "submitter": "Rui Lu", "authors": "Rui Lu, Feng Xue, Menghan Zhou, Anlong Ming and Yu Zhou", "title": "Occlusion-shared and Feature-separated Network for Occlusion\n  Relationship Reasoning", "comments": "Accepted by ICCV 2019. Code and pretrained model are available at\n  https://github.com/buptlr/OFNet", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Occlusion relationship reasoning demands closed contour to express the\nobject, and orientation of each contour pixel to describe the order\nrelationship between objects. Current CNN-based methods neglect two critical\nissues of the task: (1) simultaneous existence of the relevance and distinction\nfor the two elements, i.e, occlusion edge and occlusion orientation; and (2)\ninadequate exploration to the orientation features. For the reasons above, we\npropose the Occlusion-shared and Feature-separated Network (OFNet). On one\nhand, considering the relevance between edge and orientation, two sub-networks\nare designed to share the occlusion cue. On the other hand, the whole network\nis split into two paths to learn the high-level semantic features separately.\nMoreover, a contextual feature for orientation prediction is extracted, which\nrepresents the bilateral cue of the foreground and background areas. The\nbilateral cue is then fused with the occlusion cue to precisely locate the\nobject regions. Finally, a stripe convolution is designed to further aggregate\nfeatures from surrounding scenes of the occlusion edge. The proposed OFNet\nremarkably advances the state-of-the-art approaches on PIOD and BSDS ownership\ndataset. The source code is available at https://github.com/buptlr/OFNet.\n", "versions": [{"version": "v1", "created": "Fri, 16 Aug 2019 09:09:50 GMT"}], "update_date": "2019-08-19", "authors_parsed": [["Lu", "Rui", ""], ["Xue", "Feng", ""], ["Zhou", "Menghan", ""], ["Ming", "Anlong", ""], ["Zhou", "Yu", ""]]}, {"id": "1908.05900", "submitter": "Enze Xie", "authors": "Wenhai Wang, Enze Xie, Xiaoge Song, Yuhang Zang, Wenjia Wang, Tong Lu,\n  Gang Yu, Chunhua Shen", "title": "Efficient and Accurate Arbitrary-Shaped Text Detection with Pixel\n  Aggregation Network", "comments": "Accept by ICCV 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scene text detection, an important step of scene text reading systems, has\nwitnessed rapid development with convolutional neural networks. Nonetheless,\ntwo main challenges still exist and hamper its deployment to real-world\napplications. The first problem is the trade-off between speed and accuracy.\nThe second one is to model the arbitrary-shaped text instance. Recently, some\nmethods have been proposed to tackle arbitrary-shaped text detection, but they\nrarely take the speed of the entire pipeline into consideration, which may fall\nshort in practical applications.In this paper, we propose an efficient and\naccurate arbitrary-shaped text detector, termed Pixel Aggregation Network\n(PAN), which is equipped with a low computational-cost segmentation head and a\nlearnable post-processing. More specifically, the segmentation head is made up\nof Feature Pyramid Enhancement Module (FPEM) and Feature Fusion Module (FFM).\nFPEM is a cascadable U-shaped module, which can introduce multi-level\ninformation to guide the better segmentation. FFM can gather the features given\nby the FPEMs of different depths into a final feature for segmentation. The\nlearnable post-processing is implemented by Pixel Aggregation (PA), which can\nprecisely aggregate text pixels by predicted similarity vectors. Experiments on\nseveral standard benchmarks validate the superiority of the proposed PAN. It is\nworth noting that our method can achieve a competitive F-measure of 79.9% at\n84.2 FPS on CTW1500.\n", "versions": [{"version": "v1", "created": "Fri, 16 Aug 2019 09:14:09 GMT"}, {"version": "v2", "created": "Sun, 2 Aug 2020 03:38:04 GMT"}], "update_date": "2020-08-04", "authors_parsed": [["Wang", "Wenhai", ""], ["Xie", "Enze", ""], ["Song", "Xiaoge", ""], ["Zang", "Yuhang", ""], ["Wang", "Wenjia", ""], ["Lu", "Tong", ""], ["Yu", "Gang", ""], ["Shen", "Chunhua", ""]]}, {"id": "1908.05913", "submitter": "Jiyoung Lee", "authors": "Jiyoung Lee, Seungryong Kim, Sunok Kim, Jungin Park, Kwanghoon Sohn", "title": "Context-Aware Emotion Recognition Networks", "comments": "International Conference on Computer Vision (ICCV) 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditional techniques for emotion recognition have focused on the facial\nexpression analysis only, thus providing limited ability to encode context that\ncomprehensively represents the emotional responses. We present deep networks\nfor context-aware emotion recognition, called CAER-Net, that exploit not only\nhuman facial expression but also context information in a joint and boosting\nmanner. The key idea is to hide human faces in a visual scene and seek other\ncontexts based on an attention mechanism. Our networks consist of two\nsub-networks, including two-stream encoding networks to seperately extract the\nfeatures of face and context regions, and adaptive fusion networks to fuse such\nfeatures in an adaptive fashion. We also introduce a novel benchmark for\ncontext-aware emotion recognition, called CAER, that is more appropriate than\nexisting benchmarks both qualitatively and quantitatively. On several\nbenchmarks, CAER-Net proves the effect of context for emotion recognition. Our\ndataset is available at http://caer-dataset.github.io.\n", "versions": [{"version": "v1", "created": "Fri, 16 Aug 2019 09:59:15 GMT"}], "update_date": "2019-08-19", "authors_parsed": [["Lee", "Jiyoung", ""], ["Kim", "Seungryong", ""], ["Kim", "Sunok", ""], ["Park", "Jungin", ""], ["Sohn", "Kwanghoon", ""]]}, {"id": "1908.05926", "submitter": "Mikael Brudfors", "authors": "Mikael Brudfors, John Ashburner, Parashkev Nachev, Yael Balbastre", "title": "Empirical Bayesian Mixture Models for Medical Image Translation", "comments": "Accepted to the Simulation and Synthesis in Medical Imaging (SASHIMI)\n  workshop at MICCAI 2019", "journal-ref": null, "doi": "10.1007/978-3-030-32778-1_1", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatically generating one medical imaging modality from another is known\nas medical image translation, and has numerous interesting applications. This\npaper presents an interpretable generative modelling approach to medical image\ntranslation. By allowing a common model for group-wise normalisation and\nsegmentation of brain scans to handle missing data, the model allows for\npredicting entirely missing modalities from one, or a few, MR contrasts.\nFurthermore, the model can be trained on a fairly small number of subjects. The\nproposed model is validated on three clinically relevant scenarios. Results\nappear promising and show that a principled, probabilistic model of the\nrelationship between multi-channel signal intensities can be used to infer\nmissing modalities -- both MR contrasts and CT images.\n", "versions": [{"version": "v1", "created": "Fri, 16 Aug 2019 10:52:03 GMT"}], "update_date": "2020-05-08", "authors_parsed": [["Brudfors", "Mikael", ""], ["Ashburner", "John", ""], ["Nachev", "Parashkev", ""], ["Balbastre", "Yael", ""]]}, {"id": "1908.05932", "submitter": "Yuval Nirkin", "authors": "Yuval Nirkin, Yosi Keller and Tal Hassner", "title": "FSGAN: Subject Agnostic Face Swapping and Reenactment", "comments": "2019 IEEE/CVF International Conference on Computer Vision (ICCV)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present Face Swapping GAN (FSGAN) for face swapping and reenactment.\nUnlike previous work, FSGAN is subject agnostic and can be applied to pairs of\nfaces without requiring training on those faces. To this end, we describe a\nnumber of technical contributions. We derive a novel recurrent neural network\n(RNN)-based approach for face reenactment which adjusts for both pose and\nexpression variations and can be applied to a single image or a video sequence.\nFor video sequences, we introduce continuous interpolation of the face views\nbased on reenactment, Delaunay Triangulation, and barycentric coordinates.\nOccluded face regions are handled by a face completion network. Finally, we use\na face blending network for seamless blending of the two faces while preserving\ntarget skin color and lighting conditions. This network uses a novel Poisson\nblending loss which combines Poisson optimization with perceptual loss. We\ncompare our approach to existing state-of-the-art systems and show our results\nto be both qualitatively and quantitatively superior.\n", "versions": [{"version": "v1", "created": "Fri, 16 Aug 2019 11:16:22 GMT"}], "update_date": "2019-08-19", "authors_parsed": [["Nirkin", "Yuval", ""], ["Keller", "Yosi", ""], ["Hassner", "Tal", ""]]}, {"id": "1908.05959", "submitter": "Mauricio Orbes Arteaga", "authors": "Mauricio Orbes-Arteaga and Thomas Varsavsky and Carole H. Sudre and\n  Zach Eaton-Rosen and Lewis J. Haddow and Lauge S{\\o}rensen and Mads Nielsen\n  and Akshay Pai and S\\'ebastien Ourselin and Marc Modat and Parashkev Nachev\n  and M. Jorge Cardoso", "title": "Multi-Domain Adaptation in Brain MRI through Paired Consistency and\n  Adversarial Learning", "comments": "Accepted at 1st International Workshop on Domain Adaptation and\n  Representation Transfer held at MICCAI 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.AI cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Supervised learning algorithms trained on medical images will often fail to\ngeneralize across changes in acquisition parameters. Recent work in domain\nadaptation addresses this challenge and successfully leverages labeled data in\na source domain to perform well on an unlabeled target domain. Inspired by\nrecent work in semi-supervised learning we introduce a novel method to adapt\nfrom one source domain to $n$ target domains (as long as there is paired data\ncovering all domains). Our multi-domain adaptation method utilises a\nconsistency loss combined with adversarial learning. We provide results on\nwhite matter lesion hyperintensity segmentation from brain MRIs using the\nMICCAI 2017 challenge data as the source domain and two target domains. The\nproposed method significantly outperforms other domain adaptation baselines.\n", "versions": [{"version": "v1", "created": "Fri, 16 Aug 2019 13:06:18 GMT"}, {"version": "v2", "created": "Tue, 17 Sep 2019 09:31:53 GMT"}], "update_date": "2019-09-18", "authors_parsed": [["Orbes-Arteaga", "Mauricio", ""], ["Varsavsky", "Thomas", ""], ["Sudre", "Carole H.", ""], ["Eaton-Rosen", "Zach", ""], ["Haddow", "Lewis J.", ""], ["S\u00f8rensen", "Lauge", ""], ["Nielsen", "Mads", ""], ["Pai", "Akshay", ""], ["Ourselin", "S\u00e9bastien", ""], ["Modat", "Marc", ""], ["Nachev", "Parashkev", ""], ["Cardoso", "M. Jorge", ""]]}, {"id": "1908.05997", "submitter": "Yang Zhong", "authors": "Yang Zhong and Atsuto Maki", "title": "Regularizing CNN Transfer Learning with Randomised Regression", "comments": "Accepted to CVPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper is about regularizing deep convolutional networks (CNNs) based on\nan adaptive framework for transfer learning with limited training data in the\ntarget domain. Recent advances of CNN regularization in this context are\ncommonly due to the use of additional regularization objectives. They guide the\ntraining away from the target task using some forms of concrete tasks. Unlike\nthose related approaches, we suggest that an objective without a concrete goal\ncan still serve well as a regularized. In particular, we demonstrate\nPseudo-task Regularization (PtR) which dynamically regularizes a network by\nsimply attempting to regress image representations to pseudo-regression targets\nduring fine-tuning. That is, a CNN is efficiently regularized without\nadditional resources of data or prior domain expertise. In sum, the proposed\nPtR provides: a) an alternative for network regularization without dependence\non the design of concrete regularization objectives or extra annotations; b) a\ndynamically adjusted and maintained strength of regularization effect by\nbalancing the gradient norms between objectives on-line. Through numerous\nexperiments, surprisingly, the improvements on classification accuracy by PtR\nare shown greater or on a par to the recent state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Fri, 16 Aug 2019 14:57:12 GMT"}, {"version": "v2", "created": "Tue, 28 Apr 2020 09:50:27 GMT"}], "update_date": "2020-04-29", "authors_parsed": [["Zhong", "Yang", ""], ["Maki", "Atsuto", ""]]}, {"id": "1908.06022", "submitter": "Xiangxiang Chu", "authors": "Xiangxiang Chu, Bo Zhang, Jixiang Li, Qingyuan Li, Ruijun Xu", "title": "SCARLET-NAS: Bridging the gap between Stability and Scalability in\n  Weight-sharing Neural Architecture Search", "comments": "Make one shot nas scalable", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To discover powerful yet compact models is an important goal of neural\narchitecture search. Previous two-stage one-shot approaches are limited by\nsearch space with a fixed depth. It seems handy to include an additional skip\nconnection in the search space to make depths variable. However, it creates a\nlarge range of perturbation during supernet training and it has difficulty\ngiving a confident ranking for subnetworks. In this paper, we discover that\nskip connections bring about significant feature inconsistency compared with\nother operations, which potentially degrades the supernet performance. Based on\nthis observation, we tackle the problem by imposing an equivariant learnable\nstabilizer to homogenize such disparities. Experiments show that our proposed\nstabilizer helps to improve the supernet's convergence as well as ranking\nperformance. With an evolutionary search backend that incorporates the\nstabilized supernet as an evaluator, we derive a family of state-of-the-art\narchitectures, the SCARLET series of several depths, especially SCARLET-A\nobtains 76.9% top-1 accuracy on ImageNet. The models and evaluation code are\nreleased online https://github.com/xiaomi-automl/ScarletNAS.\n", "versions": [{"version": "v1", "created": "Fri, 16 Aug 2019 15:31:08 GMT"}, {"version": "v2", "created": "Mon, 19 Aug 2019 10:42:54 GMT"}, {"version": "v3", "created": "Fri, 13 Sep 2019 14:57:13 GMT"}, {"version": "v4", "created": "Thu, 28 Nov 2019 09:04:13 GMT"}, {"version": "v5", "created": "Thu, 2 Apr 2020 03:54:03 GMT"}], "update_date": "2020-04-03", "authors_parsed": [["Chu", "Xiangxiang", ""], ["Zhang", "Bo", ""], ["Li", "Jixiang", ""], ["Li", "Qingyuan", ""], ["Xu", "Ruijun", ""]]}, {"id": "1908.06037", "submitter": "Nick Pawlowski", "authors": "Nick Pawlowski, Suvrat Bhooshan, Nicolas Ballas, Francesco Ciompi, Ben\n  Glocker, Michal Drozdzal", "title": "Needles in Haystacks: On Classifying Tiny Objects in Large Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In some important computer vision domains, such as medical or hyperspectral\nimaging, we care about the classification of tiny objects in large images.\nHowever, most Convolutional Neural Networks (CNNs) for image classification\nwere developed using biased datasets that contain large objects, in mostly\ncentral image positions. To assess whether classical CNN architectures work\nwell for tiny object classification we build a comprehensive testbed containing\ntwo datasets: one derived from MNIST digits and one from histopathology images.\nThis testbed allows controlled experiments to stress-test CNN architectures\nwith a broad spectrum of signal-to-noise ratios. Our observations indicate\nthat: (1) There exists a limit to signal-to-noise below which CNNs fail to\ngeneralize and that this limit is affected by dataset size - more data leading\nto better performances; however, the amount of training data required for the\nmodel to generalize scales rapidly with the inverse of the object-to-image\nratio (2) in general, higher capacity models exhibit better generalization; (3)\nwhen knowing the approximate object sizes, adapting receptive field is\nbeneficial; and (4) for very small signal-to-noise ratio the choice of global\npooling operation affects optimization, whereas for relatively large\nsignal-to-noise values, all tested global pooling operations exhibit similar\nperformance.\n", "versions": [{"version": "v1", "created": "Fri, 16 Aug 2019 15:42:55 GMT"}, {"version": "v2", "created": "Mon, 6 Jan 2020 13:13:07 GMT"}], "update_date": "2020-01-07", "authors_parsed": [["Pawlowski", "Nick", ""], ["Bhooshan", "Suvrat", ""], ["Ballas", "Nicolas", ""], ["Ciompi", "Francesco", ""], ["Glocker", "Ben", ""], ["Drozdzal", "Michal", ""]]}, {"id": "1908.06047", "submitter": "Ahmadreza Baghaie", "authors": "Ahmadreza Baghaie", "title": "Robust Principal Component Analysis for Background Estimation of\n  Particle Image Velocimetry Data", "comments": "Presented in LISAT 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Particle Image Velocimetry (PIV) data processing procedures are adversely\naffected by light reflections and backgrounds as well as defects in the models\nand sticky particles that occlude the inner walls of the boundaries. In this\npaper, a novel approach is proposed for decomposition of the PIV data into\nbackground/foreground components, greatly reducing the effects of such\nartifacts. This is achieved by utilizing Robust Principal Component Analysis\n(RPCA) applied to the data matrix, generated by aggregating the vectorized PIV\nframes. It is assumed that the data matrix can be decomposed into two\nstatistically different components, a low-rank component depicting the still\nbackground and a sparse component representing the moving particles within the\nimaged geometry. Formulating the assumptions as an optimization problem,\nAugmented Lagrange Multiplier (ALM) method is used for decomposing the data\nmatrix into the low-rank and sparse components. Experiments and comparisons\nwith the state-of-the-art using several PIV image sequences reveal the\nsuperiority of the proposed approach for background removal of PIV data.\n", "versions": [{"version": "v1", "created": "Fri, 16 Aug 2019 16:28:20 GMT"}], "update_date": "2019-08-19", "authors_parsed": [["Baghaie", "Ahmadreza", ""]]}, {"id": "1908.06052", "submitter": "Yu-Jhe Li", "authors": "Yu-Jhe Li, Yun-Chun Chen, Yen-Yu Lin, Xiaofei Du, Yu-Chiang Frank Wang", "title": "Recover and Identify: A Generative Dual Model for Cross-Resolution\n  Person Re-Identification", "comments": "Accepted to ICCV 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Person re-identification (re-ID) aims at matching images of the same identity\nacross camera views. Due to varying distances between cameras and persons of\ninterest, resolution mismatch can be expected, which would degrade person re-ID\nperformance in real-world scenarios. To overcome this problem, we propose a\nnovel generative adversarial network to address cross-resolution person re-ID,\nallowing query images with varying resolutions. By advancing adversarial\nlearning techniques, our proposed model learns resolution-invariant image\nrepresentations while being able to recover the missing details in\nlow-resolution input images. The resulting features can be jointly applied for\nimproving person re-ID performance due to preserving resolution invariance and\nrecovering re-ID oriented discriminative details. Our experiments on five\nbenchmark datasets confirm the effectiveness of our approach and its\nsuperiority over the state-of-the-art methods, especially when the input\nresolutions are unseen during training.\n", "versions": [{"version": "v1", "created": "Fri, 16 Aug 2019 16:39:20 GMT"}], "update_date": "2019-08-19", "authors_parsed": [["Li", "Yu-Jhe", ""], ["Chen", "Yun-Chun", ""], ["Lin", "Yen-Yu", ""], ["Du", "Xiaofei", ""], ["Wang", "Yu-Chiang Frank", ""]]}, {"id": "1908.06062", "submitter": "Daniel Liu", "authors": "Daniel Liu, Ronald Yu, Hao Su", "title": "Adversarial shape perturbations on 3D point clouds", "comments": "18 pages, accepted to the 2020 ECCV workshop on Adversarial\n  Robustness in the Real World, source code available at this https url:\n  https://github.com/Daniel-Liu-c0deb0t/Adversarial-point-perturbations-on-3D-objects", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CR cs.LG eess.IV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The importance of training robust neural network grows as 3D data is\nincreasingly utilized in deep learning for vision tasks in robotics, drone\ncontrol, and autonomous driving. One commonly used 3D data type is 3D point\nclouds, which describe shape information. We examine the problem of creating\nrobust models from the perspective of the attacker, which is necessary in\nunderstanding how 3D neural networks can be exploited. We explore two\ncategories of attacks: distributional attacks that involve imperceptible\nperturbations to the distribution of points, and shape attacks that involve\ndeforming the shape represented by a point cloud. We explore three possible\nshape attacks for attacking 3D point cloud classification and show that some of\nthem are able to be effective even against preprocessing steps, like the\npreviously proposed point-removal defenses.\n", "versions": [{"version": "v1", "created": "Fri, 16 Aug 2019 17:19:34 GMT"}, {"version": "v2", "created": "Mon, 28 Sep 2020 00:04:59 GMT"}, {"version": "v3", "created": "Fri, 23 Oct 2020 04:55:16 GMT"}], "update_date": "2020-10-26", "authors_parsed": [["Liu", "Daniel", ""], ["Yu", "Ronald", ""], ["Su", "Hao", ""]]}, {"id": "1908.06066", "submitter": "Gen Li", "authors": "Gen Li, Nan Duan, Yuejian Fang, Ming Gong, Daxin Jiang, Ming Zhou", "title": "Unicoder-VL: A Universal Encoder for Vision and Language by Cross-modal\n  Pre-training", "comments": "accepted by AAAI-2020. arXiv admin note: text overlap with\n  arXiv:1909.11740 by other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose Unicoder-VL, a universal encoder that aims to learn joint\nrepresentations of vision and language in a pre-training manner. Borrow ideas\nfrom cross-lingual pre-trained models, such as XLM and Unicoder, both visual\nand linguistic contents are fed into a multi-layer Transformer for the\ncross-modal pre-training, where three pre-trained tasks are employed, including\nMasked Language Modeling (MLM), Masked Object Classification (MOC) and\nVisual-linguistic Matching (VLM). The first two tasks learn context-aware\nrepresentations for input tokens based on linguistic and visual contents\njointly. The last task tries to predict whether an image and a text describe\neach other. After pretraining on large-scale image-caption pairs, we transfer\nUnicoder-VL to caption-based image-text retrieval and visual commonsense\nreasoning, with just one additional output layer. We achieve state-of-the-art\nor comparable results on both two tasks and show the powerful ability of the\ncross-modal pre-training.\n", "versions": [{"version": "v1", "created": "Fri, 16 Aug 2019 17:26:56 GMT"}, {"version": "v2", "created": "Thu, 22 Aug 2019 12:00:21 GMT"}, {"version": "v3", "created": "Mon, 2 Dec 2019 10:15:38 GMT"}], "update_date": "2019-12-04", "authors_parsed": [["Li", "Gen", ""], ["Duan", "Nan", ""], ["Fang", "Yuejian", ""], ["Gong", "Ming", ""], ["Jiang", "Daxin", ""], ["Zhou", "Ming", ""]]}, {"id": "1908.06079", "submitter": "Zhizhong Li", "authors": "Zhizhong Li, Linjie Luo, Sergey Tulyakov, Qieyun Dai, Derek Hoiem", "title": "Task-Assisted Domain Adaptation with Anchor Tasks", "comments": "In WACV 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Some tasks, such as surface normals or single-view depth estimation, require\nper-pixel ground truth that is difficult to obtain on real images but easy to\nobtain on synthetic. However, models learned on synthetic images often do not\ngeneralize well to real images due to the domain shift. Our key idea to improve\ndomain adaptation is to introduce a separate anchor task (such as facial\nlandmarks) whose annotations can be obtained at no cost or are already\navailable on both synthetic and real datasets. To further leverage the implicit\nrelationship between the anchor and main tasks, we apply our \\freeze technique\nthat learns the cross-task guidance on the source domain with the final network\nlayers, and use it on the target domain. We evaluate our methods on surface\nnormal estimation on two pairs of datasets (indoor scenes and faces) with two\nkinds of anchor tasks (semantic segmentation and facial landmarks). We show\nthat blindly applying domain adaptation or training the auxiliary task on only\none domain may hurt performance, while using anchor tasks on both domains is\nbetter behaved. Our \\freeze technique outperforms competing approaches,\nreaching performance in facial images on par with a recently popular surface\nnormal estimation method using shape from shading domain knowledge.\n", "versions": [{"version": "v1", "created": "Fri, 16 Aug 2019 17:59:18 GMT"}, {"version": "v2", "created": "Mon, 6 Jan 2020 18:58:32 GMT"}, {"version": "v3", "created": "Tue, 10 Nov 2020 01:46:18 GMT"}], "update_date": "2020-11-11", "authors_parsed": [["Li", "Zhizhong", ""], ["Luo", "Linjie", ""], ["Tulyakov", "Sergey", ""], ["Dai", "Qieyun", ""], ["Hoiem", "Derek", ""]]}, {"id": "1908.06087", "submitter": "Xun Xu", "authors": "Xun Xu and Loong-Fah Cheong and Zhuwen Li", "title": "3D Rigid Motion Segmentation with Mixed and Unknown Number of Models", "comments": "IEEE Transactions on Pattern Analysis and Machine Intelligence\n  (TPAMI) 2019. arXiv admin note: substantial text overlap with\n  arXiv:1804.02142", "journal-ref": null, "doi": "10.1109/TPAMI.2019.2929146", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many real-world video sequences cannot be conveniently categorized as general\nor degenerate; in such cases, imposing a false dichotomy in using the\nfundamental matrix or homography model for motion segmentation on video\nsequences would lead to difficulty. Even when we are confronted with a general\nscene-motion, the fundamental matrix approach as a model for motion\nsegmentation still suffers from several defects, which we discuss in this\npaper. The full potential of the fundamental matrix approach could only be\nrealized if we judiciously harness information from the simpler homography\nmodel. From these considerations, we propose a multi-model spectral clustering\nframework that synergistically combines multiple models (homography and\nfundamental matrix) together. We show that the performance can be substantially\nimproved in this way. For general motion segmentation tasks, the number of\nindependently moving objects is often unknown a priori and needs to be\nestimated from the observations. This is referred to as model selection and it\nis essentially still an open research problem. In this work, we propose a set\nof model selection criteria balancing data fidelity and model complexity. We\nperform extensive testing on existing motion segmentation datasets with both\nsegmentation and model selection tasks, achieving state-of-the-art performance\non all of them; we also put forth a more realistic and challenging dataset\nadapted from the KITTI benchmark, containing real-world effects such as strong\nperspectives and strong forward translations not seen in the traditional\ndatasets.\n", "versions": [{"version": "v1", "created": "Fri, 16 Aug 2019 06:32:30 GMT"}], "update_date": "2019-08-20", "authors_parsed": [["Xu", "Xun", ""], ["Cheong", "Loong-Fah", ""], ["Li", "Zhuwen", ""]]}, {"id": "1908.06109", "submitter": "Johanna Wald", "authors": "Johanna Wald, Armen Avetisyan, Nassir Navab, Federico Tombari,\n  Matthias Nie{\\ss}ner", "title": "RIO: 3D Object Instance Re-Localization in Changing Indoor Environments", "comments": "ICCV 2019 (Oral) video https://youtu.be/367CeZtrEYM", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we introduce the task of 3D object instance re-localization\n(RIO): given one or multiple objects in an RGB-D scan, we want to estimate\ntheir corresponding 6DoF poses in another 3D scan of the same environment taken\nat a later point in time. We consider RIO a particularly important task in 3D\nvision since it enables a wide range of practical applications, including\nAI-assistants or robots that are asked to find a specific object in a 3D scene.\nTo address this problem, we first introduce 3RScan, a novel dataset and\nbenchmark, which features 1482 RGB-D scans of 478 environments across multiple\ntime steps. Each scene includes several objects whose positions change over\ntime, together with ground truth annotations of object instances and their\nrespective 6DoF mappings among re-scans. Automatically finding 6DoF object\nposes leads to a particular challenging feature matching task due to varying\npartial observations and changes in the surrounding context. To this end, we\nintroduce a new data-driven approach that efficiently finds matching features\nusing a fully-convolutional 3D correspondence network operating on multiple\nspatial scales. Combined with a 6DoF pose optimization, our method outperforms\nstate-of-the-art baselines on our newly-established benchmark, achieving an\naccuracy of 30.58%.\n", "versions": [{"version": "v1", "created": "Fri, 16 Aug 2019 18:00:41 GMT"}], "update_date": "2019-08-20", "authors_parsed": [["Wald", "Johanna", ""], ["Avetisyan", "Armen", ""], ["Navab", "Nassir", ""], ["Tombari", "Federico", ""], ["Nie\u00dfner", "Matthias", ""]]}, {"id": "1908.06112", "submitter": "Yisen Wang", "authors": "Yisen Wang, Xingjun Ma, Zaiyi Chen, Yuan Luo, Jinfeng Yi, James Bailey", "title": "Symmetric Cross Entropy for Robust Learning with Noisy Labels", "comments": "ICCV2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Training accurate deep neural networks (DNNs) in the presence of noisy labels\nis an important and challenging task. Though a number of approaches have been\nproposed for learning with noisy labels, many open issues remain. In this\npaper, we show that DNN learning with Cross Entropy (CE) exhibits overfitting\nto noisy labels on some classes (\"easy\" classes), but more surprisingly, it\nalso suffers from significant under learning on some other classes (\"hard\"\nclasses). Intuitively, CE requires an extra term to facilitate learning of hard\nclasses, and more importantly, this term should be noise tolerant, so as to\navoid overfitting to noisy labels. Inspired by the symmetric KL-divergence, we\npropose the approach of \\textbf{Symmetric cross entropy Learning} (SL),\nboosting CE symmetrically with a noise robust counterpart Reverse Cross Entropy\n(RCE). Our proposed SL approach simultaneously addresses both the under\nlearning and overfitting problem of CE in the presence of noisy labels. We\nprovide a theoretical analysis of SL and also empirically show, on a range of\nbenchmark and real-world datasets, that SL outperforms state-of-the-art\nmethods. We also show that SL can be easily incorporated into existing methods\nin order to further enhance their performance.\n", "versions": [{"version": "v1", "created": "Fri, 16 Aug 2019 18:01:32 GMT"}], "update_date": "2019-08-20", "authors_parsed": [["Wang", "Yisen", ""], ["Ma", "Xingjun", ""], ["Chen", "Zaiyi", ""], ["Luo", "Yuan", ""], ["Yi", "Jinfeng", ""], ["Bailey", "James", ""]]}, {"id": "1908.06126", "submitter": "Anna Smagina Mrs", "authors": "Anna Smagina, Egor Ershov, Anton Grigoryev", "title": "Multiple Light Source Dataset for Colour Research", "comments": null, "journal-ref": "Proceedings Volume 11433, Twelfth International Conference on\n  Machine Vision (ICMV 2019); 114332C", "doi": "10.1117/12.2559491", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We present a collection of 24 multiple object scenes each recorded under 18\nmultiple light source illumination scenarios. The illuminants are varying in\ndominant spectral colours, intensity and distance from the scene. We mainly\naddress the realistic scenarios for evaluation of computational colour\nconstancy algorithms, but also have aimed to make the data as general as\npossible for computational colour science and computer vision. Along with the\nimages of the scenes, we provide spectral characteristics of the camera, light\nsources and the objects and include pixel-by-pixel ground truth annotation of\nuniformly coloured object surfaces thus making this useful for benchmarking\ncolour-based image segmentation algorithms. The dataset is freely available at\nhttps://github.com/visillect/mls-dataset.\n", "versions": [{"version": "v1", "created": "Fri, 16 Aug 2019 18:49:32 GMT"}, {"version": "v2", "created": "Mon, 16 Sep 2019 10:53:09 GMT"}, {"version": "v3", "created": "Thu, 24 Oct 2019 07:06:11 GMT"}, {"version": "v4", "created": "Sun, 27 Oct 2019 10:42:04 GMT"}], "update_date": "2020-03-27", "authors_parsed": [["Smagina", "Anna", ""], ["Ershov", "Egor", ""], ["Grigoryev", "Anton", ""]]}, {"id": "1908.06141", "submitter": "Wentao Cheng", "authors": "Wentao Cheng, Weisi Lin, Kan Chen, Xinfeng Zhang", "title": "Cascaded Parallel Filtering for Memory-Efficient Image-Based\n  Localization", "comments": "Accepted at ICCV 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image-based localization (IBL) aims to estimate the 6DOF camera pose for a\ngiven query image. The camera pose can be computed from 2D-3D matches between a\nquery image and Structure-from-Motion (SfM) models. Despite recent advances in\nIBL, it remains difficult to simultaneously resolve the memory consumption and\nmatch ambiguity problems of large SfM models. In this work, we propose a\ncascaded parallel filtering method that leverages the feature, visibility and\ngeometry information to filter wrong matches under binary feature\nrepresentation. The core idea is that we divide the challenging filtering task\ninto two parallel tasks before deriving an auxiliary camera pose for final\nfiltering. One task focuses on preserving potentially correct matches, while\nanother focuses on obtaining high quality matches to facilitate subsequent more\npowerful filtering. Moreover, our proposed method improves the localization\naccuracy by introducing a quality-aware spatial reconfiguration method and a\nprincipal focal length enhanced pose estimation method. Experimental results on\nreal-world datasets demonstrate that our method achieves very competitive\nlocalization performances in a memory-efficient manner.\n", "versions": [{"version": "v1", "created": "Fri, 16 Aug 2019 19:39:17 GMT"}], "update_date": "2019-08-20", "authors_parsed": [["Cheng", "Wentao", ""], ["Lin", "Weisi", ""], ["Chen", "Kan", ""], ["Zhang", "Xinfeng", ""]]}, {"id": "1908.06163", "submitter": "Weiquan Mao", "authors": "Weiquan Mao, Beicheng Lou, Jiyao Yuan", "title": "TunaGAN: Interpretable GAN for Smart Editing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce a tunable generative adversary network (TunaGAN)\nthat uses an auxiliary network on top of existing generator networks\n(Style-GAN) to modify high-resolution face images according to user's\nhigh-level instructions, with good qualitative and quantitative performance. To\noptimize for feature disentanglement, we also investigate two different latent\nspace that could be traversed for modification. The problem of mode collapse is\ncharacterized in detail for model robustness. This work could be easily\nextended to content-aware image editor based on other GANs and provide insight\non mode collapse problems in more general settings.\n", "versions": [{"version": "v1", "created": "Fri, 16 Aug 2019 20:57:57 GMT"}], "update_date": "2019-08-20", "authors_parsed": [["Mao", "Weiquan", ""], ["Lou", "Beicheng", ""], ["Yuan", "Jiyao", ""]]}, {"id": "1908.06168", "submitter": "Meenakshi Khosla", "authors": "Meenakshi Khosla, Keith Jamison, Amy Kuceyeski and Mert R. Sabuncu", "title": "Detecting abnormalities in resting-state dynamics: An unsupervised\n  learning approach", "comments": "9 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV eess.IV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Resting-state functional MRI (rs-fMRI) is a rich imaging modality that\ncaptures spontaneous brain activity patterns, revealing clues about the\nconnectomic organization of the human brain. While many rs-fMRI studies have\nfocused on static measures of functional connectivity, there has been a recent\nsurge in examining the temporal patterns in these data. In this paper, we\nexplore two strategies for capturing the normal variability in resting-state\nactivity across a healthy population: (a) an autoencoder approach on the\nrs-fMRI sequence, and (b) a next frame prediction strategy. We show that both\napproaches can learn useful representations of rs-fMRI data and demonstrate\ntheir novel application for abnormality detection in the context of\ndiscriminating autism patients from healthy controls.\n", "versions": [{"version": "v1", "created": "Fri, 16 Aug 2019 21:03:08 GMT"}], "update_date": "2019-08-20", "authors_parsed": [["Khosla", "Meenakshi", ""], ["Jamison", "Keith", ""], ["Kuceyeski", "Amy", ""], ["Sabuncu", "Mert R.", ""]]}, {"id": "1908.06188", "submitter": "Trong Nguyen Nguyen", "authors": "Trong-Nguyen Nguyen, Jean Meunier", "title": "Applying Adversarial Auto-encoder for Estimating Human Walking Gait\n  Abnormality Index", "comments": null, "journal-ref": "Pattern Analysis and Applications (2019) 1-12", "doi": "10.1007/s10044-019-00790-7", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes an approach that estimates human walking gait quality\nindex using an adversarial auto-encoder (AAE), i.e. a combination of\nauto-encoder and generative adversarial network (GAN). Since most GAN-based\nmodels have been employed as data generators, our work introduces another\nperspective of their application. This method directly works on a sequence of\n3D point clouds representing the walking postures of a subject. By fitting a\ncylinder onto each point cloud and feeding obtained histograms to an\nappropriate AAE, our system is able to provide different measures that may be\nused as gait quality indices. The combinations of such quantities are also\ninvestigated to obtain improved indicators. The ability of our method is\ndemonstrated by experimenting on a large dataset of nearly 100 thousands point\nclouds and the results outperform related approaches that employ different\ninput data types.\n", "versions": [{"version": "v1", "created": "Fri, 16 Aug 2019 21:52:04 GMT"}], "update_date": "2019-08-20", "authors_parsed": [["Nguyen", "Trong-Nguyen", ""], ["Meunier", "Jean", ""]]}, {"id": "1908.06194", "submitter": "Sharib Ali Dr.", "authors": "Sharib Ali and Jens Rittscher", "title": "Conv2Warp: An unsupervised deformable image registration with continuous\n  convolution and warping", "comments": "8 pages (accepted at 10th International Workshop on Machine Learning\n  in Medical Imaging, in conjunction with MICCAI2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent successes in deep learning based deformable image registration (DIR)\nmethods have demonstrated that complex deformation can be learnt directly from\ndata while reducing computation time when compared to traditional methods.\nHowever, the reliance on fully linear convolutional layers imposes a uniform\nsampling of pixel/voxel locations which ultimately limits their performance. To\naddress this problem, we propose a novel approach of learning a continuous warp\nof the source image. Here, the required deformation vector fields are obtained\nfrom a concatenated linear and non-linear convolution layers and a learnable\nbicubic Catmull-Rom spline resampler. This allows to compute smooth deformation\nfield and more accurate alignment compared to using only linear convolutions\nand linear resampling. In addition, the continuous warping technique penalizes\ndisagreements that are due to topological changes. Our experiments demonstrate\nthat this approach manages to capture large non-linear deformations and\nminimizes the propagation of interpolation errors. While improving accuracy the\nmethod is computationally efficient. We present comparative results on a range\nof public 4D CT lung (POPI) and brain datasets (CUMC12, MGH10).\n", "versions": [{"version": "v1", "created": "Fri, 16 Aug 2019 22:21:07 GMT"}], "update_date": "2019-08-20", "authors_parsed": [["Ali", "Sharib", ""], ["Rittscher", "Jens", ""]]}, {"id": "1908.06209", "submitter": "Jonas Geiping", "authors": "Jonas Geiping and Michael Moeller", "title": "Parametric Majorization for Data-Driven Energy Minimization Methods", "comments": "16 pages, 5 figures, accepted for ICCV 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Energy minimization methods are a classical tool in a multitude of computer\nvision applications. While they are interpretable and well-studied, their\nregularity assumptions are difficult to design by hand. Deep learning\ntechniques on the other hand are purely data-driven, often provide excellent\nresults, but are very difficult to constrain to predefined physical or\nsafety-critical models. A possible combination between the two approaches is to\ndesign a parametric energy and train the free parameters in such a way that\nminimizers of the energy correspond to desired solution on a set of training\nexamples. Unfortunately, such formulations typically lead to bi-level\noptimization problems, on which common optimization algorithms are difficult to\nscale to modern requirements in data processing and efficiency. In this work,\nwe present a new strategy to optimize these bi-level problems. We investigate\nsurrogate single-level problems that majorize the target problems and can be\nimplemented with existing tools, leading to efficient algorithms without\ncollapse of the energy function. This framework of strategies enables new\navenues to the training of parameterized energy minimization models from large\ndata.\n", "versions": [{"version": "v1", "created": "Sat, 17 Aug 2019 00:10:41 GMT"}], "update_date": "2019-08-20", "authors_parsed": [["Geiping", "Jonas", ""], ["Moeller", "Michael", ""]]}, {"id": "1908.06213", "submitter": "Avinash Kori", "authors": "Avinash Kori, Ganapathi Krishnamurthi", "title": "Zero Shot Learning for Multi-Modal Real Time Image Registration", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In this report we present an unsupervised image registration framework, using\na pre-trained deep neural network as a feature extractor. We refer this to\nzero-shot learning, due to nonoverlap between training and testing dataset\n(none of the network modules in the processing pipeline were trained\nspecifically for the task of medical image registration). Highlights of our\ntechnique are: (a) No requirement of a training dataset (b) Keypoints\ni.e.locations of important features are automatically estimated (c) The number\nof key points in this model is fixed and can possibly be tuned as a\nhyperparameter. (d) Uncertaintycalculation of the proposed, transformation\nestimates (e) Real-time registration of images. Our technique was evaluated on\nBraTS, ALBERT, and collaborative hospital Brain MRI data. Results suggest that\nthe method proved to be robust for affine transformation models and the results\nare practically instantaneous, irrespective of the size of the input image\n", "versions": [{"version": "v1", "created": "Sat, 17 Aug 2019 00:37:25 GMT"}], "update_date": "2019-08-20", "authors_parsed": [["Kori", "Avinash", ""], ["Krishnamurthi", "Ganapathi", ""]]}, {"id": "1908.06217", "submitter": "Carlo Innamorati", "authors": "Carlo Innamorati and Bryan Russell and Danny M. Kaufman and and Niloy\n  J. Mitra", "title": "Neural Re-Simulation for Generating Bounces in Single Images", "comments": "Accepted to ICCV 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a method to generate videos of dynamic virtual objects plausibly\ninteracting via collisions with a still image's environment. Given a starting\ntrajectory, physically simulated with the estimated geometry of a single,\nstatic input image, we learn to 'correct' this trajectory to a visually\nplausible one via a neural network. The neural network can then be seen as\nlearning to 'correct' traditional simulation output, generated with incomplete\nand imprecise world information, to obtain context-specific, visually plausible\nre-simulated output, a process we call neural re-simulation. We train our\nsystem on a set of 50k synthetic scenes where a virtual moving object (ball)\nhas been physically simulated. We demonstrate our approach on both our\nsynthetic dataset and a collection of real-life images depicting everyday\nscenes, obtaining consistent improvement over baseline alternatives throughout.\n", "versions": [{"version": "v1", "created": "Sat, 17 Aug 2019 01:19:19 GMT"}, {"version": "v2", "created": "Tue, 20 Aug 2019 10:46:26 GMT"}, {"version": "v3", "created": "Sat, 24 Aug 2019 17:34:38 GMT"}], "update_date": "2019-08-27", "authors_parsed": [["Innamorati", "Carlo", ""], ["Russell", "Bryan", ""], ["Kaufman", "Danny M.", ""], ["Mitra", "and Niloy J.", ""]]}, {"id": "1908.06246", "submitter": "Bingyao Huang", "authors": "Bingyao Huang and Haibin Ling", "title": "CompenNet++: End-to-end Full Projector Compensation", "comments": "To appear in ICCV 2019. High-res supplementary material:\n  https://www3.cs.stonybrook.edu/~hling/publication/CompenNet++_sup-high-res.pdf.\n  Code: https://github.com/BingyaoHuang/CompenNet-plusplus", "journal-ref": null, "doi": "10.1109/ICCV.2019.00726", "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Full projector compensation aims to modify a projector input image such that\nit can compensate for both geometric and photometric disturbance of the\nprojection surface. Traditional methods usually solve the two parts separately,\nalthough they are known to correlate with each other. In this paper, we propose\nthe first end-to-end solution, named CompenNet++, to solve the two problems\njointly. Our work non-trivially extends CompenNet, which was recently proposed\nfor photometric compensation with promising performance. First, we propose a\nnovel geometric correction subnet, which is designed with a cascaded\ncoarse-to-fine structure to learn the sampling grid directly from photometric\nsampling images. Second, by concatenating the geometric correction subset with\nCompenNet, CompenNet++ accomplishes full projector compensation and is\nend-to-end trainable. Third, after training, we significantly simplify both\ngeometric and photometric compensation parts, and hence largely improves the\nrunning time efficiency. Moreover, we construct the first setup-independent\nfull compensation benchmark to facilitate the study on this topic. In our\nthorough experiments, our method shows clear advantages over previous arts with\npromising compensation quality and meanwhile being practically convenient.\n", "versions": [{"version": "v1", "created": "Sat, 17 Aug 2019 05:28:21 GMT"}], "update_date": "2020-03-04", "authors_parsed": [["Huang", "Bingyao", ""], ["Ling", "Haibin", ""]]}, {"id": "1908.06255", "submitter": "Bong-Nam Kang", "authors": "Bong-Nam Kang and Yonghyun Kim and Bongjin Jun and Daijin Kim", "title": "Attentional Feature-Pair Relation Networks for Accurate Face Recognition", "comments": "To appear in ICCV 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human face recognition is one of the most important research areas in\nbiometrics. However, the robust face recognition under a drastic change of the\nfacial pose, expression, and illumination is a big challenging problem for its\npractical application. Such variations make face recognition more difficult. In\nthis paper, we propose a novel face recognition method, called Attentional\nFeature-pair Relation Network (AFRN), which represents the face by the relevant\npairs of local appearance block features with their attention scores. The AFRN\nrepresents the face by all possible pairs of the 9x9 local appearance block\nfeatures, the importance of each pair is considered by the attention map that\nis obtained from the low-rank bilinear pooling, and each pair is weighted by\nits corresponding attention score. To increase the accuracy, we select top-K\npairs of local appearance block features as relevant facial information and\ndrop the remaining irrelevant. The weighted top-K pairs are propagated to\nextract the joint feature-pair relation by using bilinear attention network. In\nexperiments, we show the effectiveness of the proposed AFRN and achieve the\noutstanding performance in the 1:1 face verification and 1:N face\nidentification tasks compared to existing state-of-the-art methods on the\nchallenging LFW, YTF, CALFW, CPLFW, CFP, AgeDB, IJB-A, IJB-B, and IJB-C\ndatasets.\n", "versions": [{"version": "v1", "created": "Sat, 17 Aug 2019 07:32:49 GMT"}], "update_date": "2019-08-20", "authors_parsed": [["Kang", "Bong-Nam", ""], ["Kim", "Yonghyun", ""], ["Jun", "Bongjin", ""], ["Kim", "Daijin", ""]]}, {"id": "1908.06257", "submitter": "Changhee Won", "authors": "Changhee Won, Jongbin Ryu, Jongwoo Lim", "title": "OmniMVS: End-to-End Learning for Omnidirectional Stereo Matching", "comments": "Accepted by ICCV 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a novel end-to-end deep neural network model for\nomnidirectional depth estimation from a wide-baseline multi-view stereo setup.\nThe images captured with ultra wide field-of-view (FOV) cameras on an\nomnidirectional rig are processed by the feature extraction module, and then\nthe deep feature maps are warped onto the concentric spheres swept through all\ncandidate depths using the calibrated camera parameters. The 3D encoder-decoder\nblock takes the aligned feature volume to produce the omnidirectional depth\nestimate with regularization on uncertain regions utilizing the global context\ninformation. In addition, we present large-scale synthetic datasets for\ntraining and testing omnidirectional multi-view stereo algorithms. Our datasets\nconsist of 11K ground-truth depth maps and 45K fisheye images in four\northogonal directions with various objects and environments. Experimental\nresults show that the proposed method generates excellent results in both\nsynthetic and real-world environments, and it outperforms the prior art and the\nomnidirectional versions of the state-of-the-art conventional stereo\nalgorithms.\n", "versions": [{"version": "v1", "created": "Sat, 17 Aug 2019 07:57:03 GMT"}], "update_date": "2019-08-20", "authors_parsed": [["Won", "Changhee", ""], ["Ryu", "Jongbin", ""], ["Lim", "Jongwoo", ""]]}, {"id": "1908.06277", "submitter": "Lior Wolf", "authors": "Gidi Littwin and Lior Wolf", "title": "Deep Meta Functionals for Shape Representation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new method for 3D shape reconstruction from a single image, in\nwhich a deep neural network directly maps an image to a vector of network\nweights. The network \\textcolor{black}{parametrized by} these weights\nrepresents a 3D shape by classifying every point in the volume as either within\nor outside the shape. The new representation has virtually unlimited capacity\nand resolution, and can have an arbitrary topology. Our experiments show that\nit leads to more accurate shape inference from a 2D projection than the\nexisting methods, including voxel-, silhouette-, and mesh-based methods. The\ncode is available at: https://github.com/gidilittwin/Deep-Meta\n", "versions": [{"version": "v1", "created": "Sat, 17 Aug 2019 09:47:47 GMT"}], "update_date": "2019-08-20", "authors_parsed": [["Littwin", "Gidi", ""], ["Wolf", "Lior", ""]]}, {"id": "1908.06280", "submitter": "Wei Zhou", "authors": "Likun Shi, Wei Zhou, Zhibo Chen", "title": "No-Reference Light Field Image Quality Assessment Based on\n  Spatial-Angular Measurement", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CG cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Light field image quality assessment (LFI-QA) is a significant and\nchallenging research problem. It helps to better guide light field acquisition,\nprocessing and applications. However, only a few objective models have been\nproposed and none of them completely consider intrinsic factors affecting the\nLFI quality. In this paper, we propose a No-Reference Light Field image Quality\nAssessment (NR-LFQA) scheme, where the main idea is to quantify the LFI quality\ndegradation through evaluating the spatial quality and angular consistency. We\nfirst measure the spatial quality deterioration by capturing the naturalness\ndistribution of the light field cyclopean image array, which is formed when\nhuman observes the LFI. Then, as a transformed representation of LFI, the\nEpipolar Plane Image (EPI) contains the slopes of lines and involves the\nangular information. Therefore, EPI is utilized to extract the global and local\nfeatures from LFI to measure angular consistency degradation. Specifically, the\ndistribution of gradient direction map of EPI is proposed to measure the global\nangular consistency distortion in the LFI. We further propose the weighted\nlocal binary pattern to capture the characteristics of local angular\nconsistency degradation. Extensive experimental results on four publicly\navailable LFI quality datasets demonstrate that the proposed method outperforms\nstate-of-the-art 2D, 3D, multi-view, and LFI quality assessment algorithms.\n", "versions": [{"version": "v1", "created": "Sat, 17 Aug 2019 09:56:54 GMT"}], "update_date": "2019-08-20", "authors_parsed": [["Shi", "Likun", ""], ["Zhou", "Wei", ""], ["Chen", "Zhibo", ""]]}, {"id": "1908.06290", "submitter": "Zhifeng Li", "authors": "Lingxue Song, Dihong Gong, Zhifeng Li, Changsong Liu, and Wei Liu", "title": "Occlusion Robust Face Recognition Based on Mask Learning with\n  PairwiseDifferential Siamese Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Convolutional Neural Networks (CNNs) have been pushing the frontier of\nthe face recognition research in the past years. However, existing general CNN\nface models generalize poorly to the scenario of occlusions on variable facial\nareas. Inspired by the fact that a human visual system explicitly ignores\nocclusions and only focuses on non-occluded facial areas, we propose a mask\nlearning strategy to find and discard the corrupted feature elements for face\nrecognition. A mask dictionary is firstly established by exploiting the\ndifferences between the top convoluted features of occluded and occlusion-free\nface pairs using an innovatively designed Pairwise Differential Siamese Network\n(PDSN). Each item of this dictionary captures the correspondence between\noccluded facial areas and corrupted feature elements, which is named Feature\nDiscarding Mask (FDM). When dealing with a face image with random partial\nocclusions, we generate its FDM by combining relevant dictionary items and then\nmultiply it with the original features to eliminate those corrupted feature\nelements. Comprehensive experiments on both synthesized and realistic occluded\nface datasets show that the proposed approach significantly outperforms the\nstate-of-the-arts.\n", "versions": [{"version": "v1", "created": "Sat, 17 Aug 2019 10:49:50 GMT"}], "update_date": "2019-08-20", "authors_parsed": [["Song", "Lingxue", ""], ["Gong", "Dihong", ""], ["Li", "Zhifeng", ""], ["Liu", "Changsong", ""], ["Liu", "Wei", ""]]}, {"id": "1908.06294", "submitter": "Hong Zhang", "authors": "Hao Li, Hong Zhang, Xiaojuan Qi, Ruigang Yang and Gao Huang", "title": "Improved Techniques for Training Adaptive Deep Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adaptive inference is a promising technique to improve the computational\nefficiency of deep models at test time. In contrast to static models which use\nthe same computation graph for all instances, adaptive networks can dynamically\nadjust their structure conditioned on each input. While existing research on\nadaptive inference mainly focuses on designing more advanced architectures,\nthis paper investigates how to train such networks more effectively.\nSpecifically, we consider a typical adaptive deep network with multiple\nintermediate classifiers. We present three techniques to improve its training\nefficacy from two aspects: 1) a Gradient Equilibrium algorithm to resolve the\nconflict of learning of different classifiers; 2) an Inline Subnetwork\nCollaboration approach and a One-for-all Knowledge Distillation algorithm to\nenhance the collaboration among classifiers. On multiple datasets (CIFAR-10,\nCIFAR-100 and ImageNet), we show that the proposed approach consistently leads\nto further improved efficiency on top of state-of-the-art adaptive deep\nnetworks.\n", "versions": [{"version": "v1", "created": "Sat, 17 Aug 2019 10:53:53 GMT"}], "update_date": "2019-08-20", "authors_parsed": [["Li", "Hao", ""], ["Zhang", "Hong", ""], ["Qi", "Xiaojuan", ""], ["Yang", "Ruigang", ""], ["Huang", "Gao", ""]]}, {"id": "1908.06295", "submitter": "Binh-Son Hua", "authors": "Zhiyuan Zhang and Binh-Son Hua and Sai-Kit Yeung", "title": "ShellNet: Efficient Point Cloud Convolutional Neural Networks using\n  Concentric Shells Statistics", "comments": "International Conference on Computer Vision (ICCV) 2019 Oral", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning with 3D data has progressed significantly since the\nintroduction of convolutional neural networks that can handle point order\nambiguity in point cloud data. While being able to achieve good accuracies in\nvarious scene understanding tasks, previous methods often have low training\nspeed and complex network architecture. In this paper, we address these\nproblems by proposing an efficient end-to-end permutation invariant convolution\nfor point cloud deep learning. Our simple yet effective convolution operator\nnamed ShellConv uses statistics from concentric spherical shells to define\nrepresentative features and resolve the point order ambiguity, allowing\ntraditional convolution to perform on such features. Based on ShellConv we\nfurther build an efficient neural network named ShellNet to directly consume\nthe point clouds with larger receptive fields while maintaining less layers. We\ndemonstrate the efficacy of ShellNet by producing state-of-the-art results on\nobject classification, object part segmentation, and semantic scene\nsegmentation while keeping the network very fast to train.\n", "versions": [{"version": "v1", "created": "Sat, 17 Aug 2019 12:08:52 GMT"}], "update_date": "2019-08-20", "authors_parsed": [["Zhang", "Zhiyuan", ""], ["Hua", "Binh-Son", ""], ["Yeung", "Sai-Kit", ""]]}, {"id": "1908.06297", "submitter": "Binh-Son Hua", "authors": "Zhiyuan Zhang and Binh-Son Hua and David W. Rosen and Sai-Kit Yeung", "title": "Rotation Invariant Convolutions for 3D Point Clouds Deep Learning", "comments": "International Conference on 3D Vision (3DV) 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent progresses in 3D deep learning has shown that it is possible to design\nspecial convolution operators to consume point cloud data. However, a typical\ndrawback is that rotation invariance is often not guaranteed, resulting in\nnetworks being trained with data augmented with rotations. In this paper, we\nintroduce a novel convolution operator for point clouds that achieves rotation\ninvariance. Our core idea is to use low-level rotation invariant geometric\nfeatures such as distances and angles to design a convolution operator for\npoint cloud learning. The well-known point ordering problem is also addressed\nby a binning approach seamlessly built into the convolution. This convolution\noperator then serves as the basic building block of a neural network that is\nrobust to point clouds under 6DoF transformations such as translation and\nrotation. Our experiment shows that our method performs with high accuracy in\ncommon scene understanding tasks such as object classification and\nsegmentation. Compared to previous works, most importantly, our method is able\nto generalize and achieve consistent results across different scenarios in\nwhich training and testing can contain arbitrary rotations.\n", "versions": [{"version": "v1", "created": "Sat, 17 Aug 2019 12:31:57 GMT"}], "update_date": "2019-08-20", "authors_parsed": [["Zhang", "Zhiyuan", ""], ["Hua", "Binh-Son", ""], ["Rosen", "David W.", ""], ["Yeung", "Sai-Kit", ""]]}, {"id": "1908.06306", "submitter": "Badri Narayana Patro", "authors": "Badri N. Patro, Mayank Lunayach, Shivansh Patel and Vinay P.\n  Namboodiri", "title": "U-CAM: Visual Explanation using Uncertainty based Class Activation Maps", "comments": "ICCV 2019 (accepted)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.LG eess.IV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Understanding and explaining deep learning models is an imperative task.\nTowards this, we propose a method that obtains gradient-based certainty\nestimates that also provide visual attention maps. Particularly, we solve for\nvisual question answering task. We incorporate modern probabilistic deep\nlearning methods that we further improve by using the gradients for these\nestimates. These have two-fold benefits: a) improvement in obtaining the\ncertainty estimates that correlate better with misclassified samples and b)\nimproved attention maps that provide state-of-the-art results in terms of\ncorrelation with human attention regions. The improved attention maps result in\nconsistent improvement for various methods for visual question answering.\nTherefore, the proposed technique can be thought of as a recipe for obtaining\nimproved certainty estimates and explanation for deep learning models. We\nprovide detailed empirical analysis for the visual question answering task on\nall standard benchmarks and comparison with state of the art methods.\n", "versions": [{"version": "v1", "created": "Sat, 17 Aug 2019 14:39:36 GMT"}, {"version": "v2", "created": "Sun, 25 Aug 2019 19:07:15 GMT"}, {"version": "v3", "created": "Mon, 16 Sep 2019 15:04:57 GMT"}, {"version": "v4", "created": "Thu, 17 Oct 2019 07:20:32 GMT"}], "update_date": "2019-10-18", "authors_parsed": [["Patro", "Badri N.", ""], ["Lunayach", "Mayank", ""], ["Patel", "Shivansh", ""], ["Namboodiri", "Vinay P.", ""]]}, {"id": "1908.06307", "submitter": "Feihong Liu", "authors": "Feihong Liu, Jun Feng, Pew-Thian Yap, Dinggang Shen", "title": "Multi-Kernel Filtering for Nonstationary Noise: An Extension of\n  Bilateral Filtering Using Image Context", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bilateral filtering (BF) is one of the most classical denoising filters,\nhowever, the manually initialized filtering kernel hampers its adaptivity\nacross images with various characteristics. To deal with image variation (i.e.,\nnon-stationary noise), in this paper, we propose multi-kernel filter (MKF)\nwhich adapts filtering kernels to specific image characteristics automatically.\nThe design of MKF takes inspiration from adaptive mechanisms of human vision\nthat make full use of information in a visual context. More specifically, for\nsimulating the visual context and its adaptive function, we construct the image\ncontext based on which we simulate the contextual impact on filtering kernels.\nWe first design a hierarchically clustering algorithm to generate a hierarchy\nof large to small coherent image patches, organized as a cluster tree, so that\nobtain multi-scale image representation. The leaf cluster and corresponding\npredecessor clusters are used to generate one of multiple range kernels that\nare capable of catering to image variation. At first, we design a\nhierarchically clustering framework to generate a hierarchy of large to small\ncoherent image patches that organized as a cluster tree, so that obtain\nmulti-scale image representation, i.e., the image context. Next, a leaf cluster\nis used to generate one of the multiple kernels, and two corresponding\npredecessor clusters are used to fine-tune the adopted kernel. Ultimately, the\nsingle spatially-invariant kernel in BF becomes multiple spatially-varying\nones. We evaluate MKF on two public datasets, BSD300 and BrainWeb which are\nadded integrally-varying noise and spatially-varying noise, respectively.\nExtensive experiments show that MKF outperforms state-of-the-art filters w.r.t.\nboth mean absolute error and structural similarity.\n", "versions": [{"version": "v1", "created": "Sat, 17 Aug 2019 14:43:00 GMT"}, {"version": "v2", "created": "Mon, 26 Aug 2019 14:00:12 GMT"}, {"version": "v3", "created": "Tue, 27 Aug 2019 11:26:47 GMT"}, {"version": "v4", "created": "Mon, 23 Dec 2019 10:03:22 GMT"}], "update_date": "2019-12-24", "authors_parsed": [["Liu", "Feihong", ""], ["Feng", "Jun", ""], ["Yap", "Pew-Thian", ""], ["Shen", "Dinggang", ""]]}, {"id": "1908.06314", "submitter": "Jiaxin Gu", "authors": "Jiaxin Gu, Junhe Zhao, Xiaolong Jiang, Baochang Zhang, Jianzhuang Liu,\n  Guodong Guo, Rongrong Ji", "title": "Bayesian Optimized 1-Bit CNNs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep convolutional neural networks (DCNNs) have dominated the recent\ndevelopments in computer vision through making various record-breaking models.\nHowever, it is still a great challenge to achieve powerful DCNNs in\nresource-limited environments, such as on embedded devices and smart phones.\nResearchers have realized that 1-bit CNNs can be one feasible solution to\nresolve the issue; however, they are baffled by the inferior performance\ncompared to the full-precision DCNNs. In this paper, we propose a novel\napproach, called Bayesian optimized 1-bit CNNs (denoted as BONNs), taking the\nadvantage of Bayesian learning, a well-established strategy for hard problems,\nto significantly improve the performance of extreme 1-bit CNNs. We incorporate\nthe prior distributions of full-precision kernels and features into the\nBayesian framework to construct 1-bit CNNs in an end-to-end manner, which have\nnot been considered in any previous related methods. The Bayesian losses are\nachieved with a theoretical support to optimize the network simultaneously in\nboth continuous and discrete spaces, aggregating different losses jointly to\nimprove the model capacity. Extensive experiments on the ImageNet and CIFAR\ndatasets show that BONNs achieve the best classification performance compared\nto state-of-the-art 1-bit CNNs.\n", "versions": [{"version": "v1", "created": "Sat, 17 Aug 2019 15:35:51 GMT"}], "update_date": "2019-08-20", "authors_parsed": [["Gu", "Jiaxin", ""], ["Zhao", "Junhe", ""], ["Jiang", "Xiaolong", ""], ["Zhang", "Baochang", ""], ["Liu", "Jianzhuang", ""], ["Guo", "Guodong", ""], ["Ji", "Rongrong", ""]]}, {"id": "1908.06316", "submitter": "Fabian Brickwedde", "authors": "Fabian Brickwedde, Steffen Abraham, Rudolf Mester", "title": "Mono-SF: Multi-View Geometry Meets Single-View Depth for Monocular Scene\n  Flow Estimation of Dynamic Traffic Scenes", "comments": "accepted to IEEE International Conference on Computer Vision 2019\n  (ICCV 2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing 3D scene flow estimation methods provide the 3D geometry and 3D\nmotion of a scene and gain a lot of interest, for example in the context of\nautonomous driving. These methods are traditionally based on a temporal series\nof stereo images. In this paper, we propose a novel monocular 3D scene flow\nestimation method, called Mono-SF. Mono-SF jointly estimates the 3D structure\nand motion of the scene by combining multi-view geometry and single-view depth\ninformation. Mono-SF considers that the scene flow should be consistent in\nterms of warping the reference image in the consecutive image based on the\nprinciples of multi-view geometry. For integrating single-view depth in a\nstatistical manner, a convolutional neural network, called ProbDepthNet, is\nproposed. ProbDepthNet estimates pixel-wise depth distributions from a single\nimage rather than single depth values. Additionally, as part of ProbDepthNet, a\nnovel recalibration technique for regression problems is proposed to ensure\nwell-calibrated distributions. Our experiments show that Mono-SF outperforms\nstate-of-the-art monocular baselines and ablation studies support the Mono-SF\napproach and ProbDepthNet design.\n", "versions": [{"version": "v1", "created": "Sat, 17 Aug 2019 15:44:12 GMT"}], "update_date": "2019-08-20", "authors_parsed": [["Brickwedde", "Fabian", ""], ["Abraham", "Steffen", ""], ["Mester", "Rudolf", ""]]}, {"id": "1908.06326", "submitter": "Rahul Vashisht", "authors": "Rahul Vashisht, H.Viji, T.Sundararajan, D.Mohankumar, S.Sumitra", "title": "Structural Health Monitoring of Cantilever Beam, a Case Study -- Using\n  Bayesian Neural Network AND Deep Learning", "comments": "10 Pages", "journal-ref": null, "doi": "10.1007/978-981-13-8767-8_64", "report-no": "11", "categories": "cs.LG cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The advancement of machine learning algorithms has opened a wide scope for\nvibration-based SHM (Structural Health Monitoring). Vibration-based SHM is\nbased on the fact that damage will alter the dynamic properties viz.,\nstructural response, frequencies, mode shapes, etc of the structure. The\nresponses measured using sensors, which are high dimensional in nature, can be\nintelligently analyzed using machine learning techniques for damage assessment.\nNeural networks employing multilayer architectures are expressive models\ncapable of capturing complex relationships between input-output pairs but do\nnot account for uncertainty in network outputs. A BNN (Bayesian Neural Network)\nrefers to extending standard networks with posterior inference. It is a neural\nnetwork with a prior distribution on its weights. Deep learning architectures\nlike CNN (Convolutional neural network) and LSTM(Long Short Term Memory) are\ngood candidates for representation learning from high dimensional data. The\nadvantage of using CNN over multi-layer neural networks is that they are good\nfeature extractors as well as classifiers, which eliminates the need for\ngenerating hand-engineered features. LSTM networks are mainly used for sequence\nmodeling. This paper presents both a Bayesian multi-layer perceptron and deep\nlearning-based approach for damage detection and location identification in\nbeam-like structures. Raw frequency response data simulated using finite\nelement analysis is fed as the input of the network. As part of this, frequency\nresponse was generated for a series of simulations in the cantilever beam\ninvolving different damage scenarios. This case study shows the effectiveness\nof the above approaches to predict bending rigidity with an acceptable error\nrate.\n", "versions": [{"version": "v1", "created": "Sat, 17 Aug 2019 17:47:24 GMT"}], "update_date": "2019-08-20", "authors_parsed": [["Vashisht", "Rahul", ""], ["Viji", "H.", ""], ["Sundararajan", "T.", ""], ["Mohankumar", "D.", ""], ["Sumitra", "S.", ""]]}, {"id": "1908.06327", "submitter": "Andrea Burns", "authors": "Andrea Burns, Reuben Tan, Kate Saenko, Stan Sclaroff, Bryan A. Plummer", "title": "Language Features Matter: Effective Language Representations for\n  Vision-Language Tasks", "comments": "ICCV 2019 accepted paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Shouldn't language and vision features be treated equally in vision-language\n(VL) tasks? Many VL approaches treat the language component as an afterthought,\nusing simple language models that are either built upon fixed word embeddings\ntrained on text-only data or are learned from scratch. We believe that language\nfeatures deserve more attention, and conduct experiments which compare\ndifferent word embeddings, language models, and embedding augmentation steps on\nfive common VL tasks: image-sentence retrieval, image captioning, visual\nquestion answering, phrase grounding, and text-to-clip retrieval. Our\nexperiments provide some striking results; an average embedding language model\noutperforms an LSTM on retrieval-style tasks; state-of-the-art representations\nsuch as BERT perform relatively poorly on vision-language tasks. From this\ncomprehensive set of experiments we propose a set of best practices for\nincorporating the language component of VL tasks. To further elevate language\nfeatures, we also show that knowledge in vision-language problems can be\ntransferred across tasks to gain performance with multi-task training. This\nmulti-task training is applied to a new Graph Oriented Vision-Language\nEmbedding (GrOVLE), which we adapt from Word2Vec using WordNet and an original\nvisual-language graph built from Visual Genome, providing a ready-to-use\nvision-language embedding: http://ai.bu.edu/grovle.\n", "versions": [{"version": "v1", "created": "Sat, 17 Aug 2019 18:01:27 GMT"}], "update_date": "2019-08-20", "authors_parsed": [["Burns", "Andrea", ""], ["Tan", "Reuben", ""], ["Saenko", "Kate", ""], ["Sclaroff", "Stan", ""], ["Plummer", "Bryan A.", ""]]}, {"id": "1908.06336", "submitter": "Alexander Kuhnle", "authors": "Alexander Kuhnle, Ann Copestake", "title": "What is needed for simple spatial language capabilities in VQA?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual question answering (VQA) comprises a variety of language capabilities.\nThe diagnostic benchmark dataset CLEVR has fueled progress by helping to better\nassess and distinguish models in basic abilities like counting, comparing and\nspatial reasoning in vitro. Following this approach, we focus on spatial\nlanguage capabilities and investigate the question: what are the key\ningredients to handle simple visual-spatial relations? We look at the SAN,\nRelNet, FiLM and MC models and evaluate their learning behavior on diagnostic\ndata which is solely focused on spatial relations. Via comparative analysis and\ntargeted model modification we identify what really is required to\nsubstantially improve upon the CNN-LSTM baseline.\n", "versions": [{"version": "v1", "created": "Sat, 17 Aug 2019 20:12:39 GMT"}, {"version": "v2", "created": "Tue, 22 Oct 2019 19:03:21 GMT"}], "update_date": "2019-10-24", "authors_parsed": [["Kuhnle", "Alexander", ""], ["Copestake", "Ann", ""]]}, {"id": "1908.06337", "submitter": "Bilwaj Gaonkar", "authors": "Bilwaj Gaonkar, Joel Beckett, Mark Attiah, Christine Ahn, Matthew\n  Edwards, Bayard Wilson, Azim Laiwalla, Banafsheh Salehi, Bryan Yoo, Alex Bui,\n  Luke Macyszyn", "title": "EigenRank by Committee: A Data Subset Selection and Failure Prediction\n  paradigm for Robust Deep Learning based Medical Image Segmentation", "comments": null, "journal-ref": "Medical Image Analysis, Volume 67, 2021, Medical Image Analysis,\n  Volume 67,2021,101834,ISSN 1361-8415,", "doi": "10.1016/j.media.2020.101834", "report-no": null, "categories": "eess.IV cs.CV cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Translation of fully automated deep learning based medical image segmentation\ntechnologies to clinical workflows face two main algorithmic challenges. The\nfirst, is the collection and archival of large quantities of manually annotated\nground truth data for both training and validation. The second is the relative\ninability of the majority of deep learning based segmentation techniques to\nalert physicians to a likely segmentation failure. Here we propose a novel\nalgorithm, named `Eigenrank' which addresses both of these challenges.\nEigenrank can select for manual labeling, a subset of medical images from a\nlarge database, such that a U-Net trained on this subset is superior to one\ntrained on a randomly selected subset of the same size. Eigenrank can also be\nused to pick out, cases in a large database, where deep learning segmentation\nwill fail. We present our algorithm, followed by results and a discussion of\nhow Eigenrank exploits the Von Neumann information to perform both data subset\nselection and failure prediction for medical image segmentation using deep\nlearning.\n", "versions": [{"version": "v1", "created": "Sat, 17 Aug 2019 20:16:07 GMT"}, {"version": "v2", "created": "Mon, 18 Jan 2021 19:40:32 GMT"}], "update_date": "2021-01-20", "authors_parsed": [["Gaonkar", "Bilwaj", ""], ["Beckett", "Joel", ""], ["Attiah", "Mark", ""], ["Ahn", "Christine", ""], ["Edwards", "Matthew", ""], ["Wilson", "Bayard", ""], ["Laiwalla", "Azim", ""], ["Salehi", "Banafsheh", ""], ["Yoo", "Bryan", ""], ["Bui", "Alex", ""], ["Macyszyn", "Luke", ""]]}, {"id": "1908.06342", "submitter": "Trong Nguyen Nguyen", "authors": "Trong Nguyen Nguyen, Huu Hung Huynh, Jean Meunier", "title": "Matching-based Depth Camera and Mirrors for 3D Reconstruction", "comments": null, "journal-ref": "Proc. SPIE 10666, Three-Dimensional Imaging, Visualization, and\n  Display 2018, 1066610 (16 May 2018)", "doi": "10.1117/12.2304427", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Reconstructing 3D object models is playing an important role in many\napplications in the field of computer vision. Instead of employing a collection\nof cameras and/or sensors as in many studies, this paper proposes a simple way\nto build a cheaper system for 3D reconstruction using only one depth camera and\n2 or more mirrors. Each mirror is equivalently considered as a depth camera at\nanother viewpoint. Since all scene data are provided by only one depth sensor,\nour approach can be applied to moving objects and does not require any\nsynchronization protocol as with a set of cameras. Some experiments were\nperformed on easy-to-evaluate objects to confirm the reconstruction accuracy of\nour proposed system.\n", "versions": [{"version": "v1", "created": "Sat, 17 Aug 2019 22:32:45 GMT"}], "update_date": "2019-08-20", "authors_parsed": [["Nguyen", "Trong Nguyen", ""], ["Huynh", "Huu Hung", ""], ["Meunier", "Jean", ""]]}, {"id": "1908.06347", "submitter": "Trong Nguyen Nguyen", "authors": "Trong Nguyen Nguyen, Jean Meunier", "title": "Hybrid Deep Network for Anomaly Detection", "comments": "Paper accepted for BMVC 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a deep convolutional neural network (CNN) for\nanomaly detection in surveillance videos. The model is adapted from a typical\nauto-encoder working on video patches under the perspective of sparse\ncombination learning. Our CNN focuses on (unsupervisedly) learning common\ncharacteristics of normal events with the emphasis of their spatial locations\n(by supervised losses). To our knowledge, this is the first work that directly\nadapts the patch position as the target of a classification sub-network. The\nmodel is capable to provide a score of anomaly assessment for each video frame.\nOur experiments were performed on 4 benchmark datasets with various anomalous\nevents and the obtained results were competitive with state-of-the-art studies.\n", "versions": [{"version": "v1", "created": "Sat, 17 Aug 2019 23:08:30 GMT"}], "update_date": "2019-08-20", "authors_parsed": [["Nguyen", "Trong Nguyen", ""], ["Meunier", "Jean", ""]]}, {"id": "1908.06351", "submitter": "Trong Nguyen Nguyen", "authors": "Trong Nguyen Nguyen, Jean Meunier", "title": "Anomaly Detection in Video Sequence with Appearance-Motion\n  Correspondence", "comments": "Paper accepted for ICCV 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Anomaly detection in surveillance videos is currently a challenge because of\nthe diversity of possible events. We propose a deep convolutional neural\nnetwork (CNN) that addresses this problem by learning a correspondence between\ncommon object appearances (e.g. pedestrian, background, tree, etc.) and their\nassociated motions. Our model is designed as a combination of a reconstruction\nnetwork and an image translation model that share the same encoder. The former\nsub-network determines the most significant structures that appear in video\nframes and the latter one attempts to associate motion templates to such\nstructures. The training stage is performed using only videos of normal events\nand the model is then capable to estimate frame-level scores for an unknown\ninput. The experiments on 6 benchmark datasets demonstrate the competitive\nperformance of the proposed approach with respect to state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Sat, 17 Aug 2019 23:52:22 GMT"}], "update_date": "2019-08-20", "authors_parsed": [["Nguyen", "Trong Nguyen", ""], ["Meunier", "Jean", ""]]}, {"id": "1908.06354", "submitter": "Zhengyuan Yang", "authors": "Zhengyuan Yang, Boqing Gong, Liwei Wang, Wenbing Huang, Dong Yu, Jiebo\n  Luo", "title": "A Fast and Accurate One-Stage Approach to Visual Grounding", "comments": "ICCV 2019 Oral", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a simple, fast, and accurate one-stage approach to visual\ngrounding, inspired by the following insight. The performances of existing\npropose-and-rank two-stage methods are capped by the quality of the region\ncandidates they propose in the first stage --- if none of the candidates could\ncover the ground truth region, there is no hope in the second stage to rank the\nright region to the top. To avoid this caveat, we propose a one-stage model\nthat enables end-to-end joint optimization. The main idea is as straightforward\nas fusing a text query's embedding into the YOLOv3 object detector, augmented\nby spatial features so as to account for spatial mentions in the query. Despite\nbeing simple, this one-stage approach shows great potential in terms of both\naccuracy and speed for both phrase localization and referring expression\ncomprehension, according to our experiments. Given these results along with\ncareful investigations into some popular region proposals, we advocate for\nvisual grounding a paradigm shift from the conventional two-stage methods to\nthe one-stage framework.\n", "versions": [{"version": "v1", "created": "Sun, 18 Aug 2019 00:25:02 GMT"}], "update_date": "2019-08-20", "authors_parsed": [["Yang", "Zhengyuan", ""], ["Gong", "Boqing", ""], ["Wang", "Liwei", ""], ["Huang", "Wenbing", ""], ["Yu", "Dong", ""], ["Luo", "Jiebo", ""]]}, {"id": "1908.06368", "submitter": "Huizi Mao", "authors": "Huizi Mao, Xiaodong Yang, William J. Dally", "title": "A Delay Metric for Video Object Detection: What Average Precision Fails\n  to Tell", "comments": "ICCV 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Average precision (AP) is a widely used metric to evaluate detection accuracy\nof image and video object detectors. In this paper, we analyze object detection\nfrom videos and point out that AP alone is not sufficient to capture the\ntemporal nature of video object detection. To tackle this problem, we propose a\ncomprehensive metric, average delay (AD), to measure and compare detection\ndelay. To facilitate delay evaluation, we carefully select a subset of ImageNet\nVID, which we name as ImageNet VIDT with an emphasis on complex trajectories.\nBy extensively evaluating a wide range of detectors on VIDT, we show that most\nmethods drastically increase the detection delay but still preserve AP well. In\nother words, AP is not sensitive enough to reflect the temporal characteristics\nof a video object detector. Our results suggest that video object detection\nmethods should be additionally evaluated with a delay metric, particularly for\nlatency-critical applications such as autonomous vehicle perception.\n", "versions": [{"version": "v1", "created": "Sun, 18 Aug 2019 03:36:23 GMT"}, {"version": "v2", "created": "Wed, 6 Nov 2019 22:50:02 GMT"}], "update_date": "2019-11-11", "authors_parsed": [["Mao", "Huizi", ""], ["Yang", "Xiaodong", ""], ["Dally", "William J.", ""]]}, {"id": "1908.06372", "submitter": "Anant Gupta", "authors": "Anant Gupta, Atul Ingle, Mohit Gupta", "title": "Asynchronous Single-Photon 3D Imaging", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Single-photon avalanche diodes (SPADs) are becoming popular in time-of-flight\ndepth-ranging due to their unique ability to capture individual photons with\npicosecond timing resolution. However, ambient light (e.g., sunlight) incident\non a SPAD-based 3D camera leads to severe non-linear distortions (pileup) in\nthe measured waveform, resulting in large depth errors. We propose asynchronous\nsingle-photon 3D imaging, a family of acquisition schemes to mitigate pileup\nduring data acquisition itself. Asynchronous acquisition temporally misaligns\nSPAD measurement windows and the laser cycles through deterministically\npredefined or randomized offsets. Our key insight is that pileup distortions\ncan be \"averaged out\" by choosing a sequence of offsets that span the entire\ndepth range. We develop a generalized image formation model and perform\ntheoretical analysis to explore the space of asynchronous acquisition schemes\nand design high-performance schemes. Our simulations and experiments\ndemonstrate an improvement in depth accuracy of up to an order of magnitude as\ncompared to the state-of-the-art, across a wide range of imaging scenarios,\nincluding those with high ambient flux.\n", "versions": [{"version": "v1", "created": "Sun, 18 Aug 2019 04:03:36 GMT"}], "update_date": "2019-08-20", "authors_parsed": [["Gupta", "Anant", ""], ["Ingle", "Atul", ""], ["Gupta", "Mohit", ""]]}, {"id": "1908.06377", "submitter": "Chaoyang Wang", "authors": "Chaoyang Wang, Chen Kong, Simon Lucey", "title": "Distill Knowledge from NRSfM for Weakly Supervised 3D Pose Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose to learn a 3D pose estimator by distilling knowledge from\nNon-Rigid Structure from Motion (NRSfM). Our method uses solely 2D landmark\nannotations. No 3D data, multi-view/temporal footage, or object specific prior\nis required. This alleviates the data bottleneck, which is one of the major\nconcern for supervised methods. The challenge for using NRSfM as teacher is\nthat they often make poor depth reconstruction when the 2D projections have\nstrong ambiguity. Directly using those wrong depth as hard target would\nnegatively impact the student. Instead, we propose a novel loss that ties depth\nprediction to the cost function used in NRSfM. This gives the student pose\nestimator freedom to reduce depth error by associating with image features.\nValidated on H3.6M dataset, our learned 3D pose estimation network achieves\nmore accurate reconstruction compared to NRSfM methods. It also outperforms\nother weakly supervised methods, in spite of using significantly less\nsupervision.\n", "versions": [{"version": "v1", "created": "Sun, 18 Aug 2019 04:48:49 GMT"}], "update_date": "2019-08-20", "authors_parsed": [["Wang", "Chaoyang", ""], ["Kong", "Chen", ""], ["Lucey", "Simon", ""]]}, {"id": "1908.06381", "submitter": "Danylo Malyuta", "authors": "Danylo Malyuta, Christian Brommer, Daniel Hentzen, Thomas Stastny,\n  Roland Siegwart, Roland Brockers", "title": "Long-Duration Fully Autonomous Operation of Rotorcraft Unmanned Aerial\n  Systems for Remote-Sensing Data Acquisition", "comments": "38 pages, 28 figures", "journal-ref": "J Field Robotics (2019) 1-21", "doi": "10.1002/rob.21898", "report-no": null, "categories": "cs.RO cs.CV cs.SY eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent applications of unmanned aerial systems (UAS) to precision agriculture\nhave shown increased ease and efficiency in data collection at precise remote\nlocations. However, further enhancement of the field requires operation over\nlong periods of time, e.g. days or weeks. This has so far been impractical due\nto the limited flight times of such platforms and the requirement of humans in\nthe loop for operation. To overcome these limitations, we propose a fully\nautonomous rotorcraft UAS that is capable of performing repeated flights for\nlong-term observation missions without any human intervention. We address two\nkey technologies that are critical for such a system: full platform autonomy to\nenable mission execution independently from human operators and the ability of\nvision-based precision landing on a recharging station for automated energy\nreplenishment. High-level autonomous decision making is implemented as a\nhierarchy of master and slave state machines. Vision-based precision landing is\nenabled by estimating the landing pad's pose using a bundle of AprilTag\nfiducials configured for detection from a wide range of altitudes. We provide\nan extensive evaluation of the landing pad pose estimation accuracy as a\nfunction of the bundle's geometry. The functionality of the complete system is\ndemonstrated through two indoor experiments with a duration of 11 and 10.6\nhours, and one outdoor experiment with a duration of 4 hours. The UAS executed\n16, 48 and 22 flights respectively during these experiments. In the outdoor\nexperiment, the ratio between flying to collect data and charging was 1 to 10,\nwhich is similar to past work in this domain. All flights were fully autonomous\nwith no human in the loop. To our best knowledge this is the first research\npublication about the long-term outdoor operation of a quadrotor system with no\nhuman interaction.\n", "versions": [{"version": "v1", "created": "Sun, 18 Aug 2019 06:33:52 GMT"}], "update_date": "2019-08-20", "authors_parsed": [["Malyuta", "Danylo", ""], ["Brommer", "Christian", ""], ["Hentzen", "Daniel", ""], ["Stastny", "Thomas", ""], ["Siegwart", "Roland", ""], ["Brockers", "Roland", ""]]}, {"id": "1908.06382", "submitter": "Wenlong Zhang", "authors": "Wenlong Zhang, Yihao Liu, Chao Dong, Yu Qiao", "title": "RankSRGAN: Generative Adversarial Networks with Ranker for Image\n  Super-Resolution", "comments": "ICCV 2019 (Oral) camera-ready + supplementary; Project page:\n  https://wenlongzhang0724.github.io/Projects/RankSRGAN", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative Adversarial Networks (GAN) have demonstrated the potential to\nrecover realistic details for single image super-resolution (SISR). To further\nimprove the visual quality of super-resolved results, PIRM2018-SR Challenge\nemployed perceptual metrics to assess the perceptual quality, such as PI, NIQE,\nand Ma. However, existing methods cannot directly optimize these\nindifferentiable perceptual metrics, which are shown to be highly correlated\nwith human ratings. To address the problem, we propose Super-Resolution\nGenerative Adversarial Networks with Ranker (RankSRGAN) to optimize generator\nin the direction of perceptual metrics. Specifically, we first train a Ranker\nwhich can learn the behavior of perceptual metrics and then introduce a novel\nrank-content loss to optimize the perceptual quality. The most appealing part\nis that the proposed method can combine the strengths of different SR methods\nto generate better results. Extensive experiments show that RankSRGAN achieves\nvisually pleasing results and reaches state-of-the-art performance in\nperceptual metrics. Project page:\nhttps://wenlongzhang0724.github.io/Projects/RankSRGAN\n", "versions": [{"version": "v1", "created": "Sun, 18 Aug 2019 06:38:55 GMT"}, {"version": "v2", "created": "Mon, 26 Aug 2019 10:14:40 GMT"}], "update_date": "2019-08-27", "authors_parsed": [["Zhang", "Wenlong", ""], ["Liu", "Yihao", ""], ["Dong", "Chao", ""], ["Qiao", "Yu", ""]]}, {"id": "1908.06386", "submitter": "Tristan Aumentado-Armstrong", "authors": "Tristan Aumentado-Armstrong, Stavros Tsogkas, Allan Jepson, Sven\n  Dickinson", "title": "Geometric Disentanglement for Generative Latent Shape Models", "comments": "ICCV 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Representing 3D shape is a fundamental problem in artificial intelligence,\nwhich has numerous applications within computer vision and graphics. One avenue\nthat has recently begun to be explored is the use of latent representations of\ngenerative models. However, it remains an open problem to learn a generative\nmodel of shape that is interpretable and easily manipulated, particularly in\nthe absence of supervised labels. In this paper, we propose an unsupervised\napproach to partitioning the latent space of a variational autoencoder for 3D\npoint clouds in a natural way, using only geometric information. Our method\nmakes use of tools from spectral differential geometry to separate intrinsic\nand extrinsic shape information, and then considers several hierarchical\ndisentanglement penalties for dividing the latent space in this manner,\nincluding a novel one that penalizes the Jacobian of the latent representation\nof the decoded output with respect to the latent encoding. We show that the\nresulting representation exhibits intuitive and interpretable behavior,\nenabling tasks such as pose transfer and pose-aware shape retrieval that cannot\neasily be performed by models with an entangled representation.\n", "versions": [{"version": "v1", "created": "Sun, 18 Aug 2019 07:05:39 GMT"}], "update_date": "2019-08-20", "authors_parsed": [["Aumentado-Armstrong", "Tristan", ""], ["Tsogkas", "Stavros", ""], ["Jepson", "Allan", ""], ["Dickinson", "Sven", ""]]}, {"id": "1908.06387", "submitter": "M{\\aa}ns Larsson", "authors": "M{\\aa}ns Larsson and Erik Stenborg and Carl Toft and Lars Hammarstrand\n  and Torsten Sattler and Fredrik Kahl", "title": "Fine-Grained Segmentation Networks: Self-Supervised Segmentation for\n  Improved Long-Term Visual Localization", "comments": "Accepted to ICCV 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Long-term visual localization is the problem of estimating the camera pose of\na given query image in a scene whose appearance changes over time. It is an\nimportant problem in practice, for example, encountered in autonomous driving.\nIn order to gain robustness to such changes, long-term localization approaches\noften use segmantic segmentations as an invariant scene representation, as the\nsemantic meaning of each scene part should not be affected by seasonal and\nother changes. However, these representations are typically not very\ndiscriminative due to the limited number of available classes. In this paper,\nwe propose a new neural network, the Fine-Grained Segmentation Network (FGSN),\nthat can be used to provide image segmentations with a larger number of labels\nand can be trained in a self-supervised fashion. In addition, we show how FGSNs\ncan be trained to output consistent labels across seasonal changes. We\ndemonstrate through extensive experiments that integrating the fine-grained\nsegmentations produced by our FGSNs into existing localization algorithms leads\nto substantial improvements in localization performance.\n", "versions": [{"version": "v1", "created": "Sun, 18 Aug 2019 07:13:26 GMT"}], "update_date": "2019-08-20", "authors_parsed": [["Larsson", "M\u00e5ns", ""], ["Stenborg", "Erik", ""], ["Toft", "Carl", ""], ["Hammarstrand", "Lars", ""], ["Sattler", "Torsten", ""], ["Kahl", "Fredrik", ""]]}, {"id": "1908.06391", "submitter": "Kaixin Wang", "authors": "Kaixin Wang, Jun Hao Liew, Yingtian Zou, Daquan Zhou, Jiashi Feng", "title": "PANet: Few-Shot Image Semantic Segmentation with Prototype Alignment", "comments": "10 pages, 6 figures, ICCV 2019, code available at\n  https://github.com/kaixin96/PANet", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the great progress made by deep CNNs in image semantic segmentation,\nthey typically require a large number of densely-annotated images for training\nand are difficult to generalize to unseen object categories. Few-shot\nsegmentation has thus been developed to learn to perform segmentation from only\na few annotated examples. In this paper, we tackle the challenging few-shot\nsegmentation problem from a metric learning perspective and present PANet, a\nnovel prototype alignment network to better utilize the information of the\nsupport set. Our PANet learns class-specific prototype representations from a\nfew support images within an embedding space and then performs segmentation\nover the query images through matching each pixel to the learned prototypes.\nWith non-parametric metric learning, PANet offers high-quality prototypes that\nare representative for each semantic class and meanwhile discriminative for\ndifferent classes. Moreover, PANet introduces a prototype alignment\nregularization between support and query. With this, PANet fully exploits\nknowledge from the support and provides better generalization on few-shot\nsegmentation. Significantly, our model achieves the mIoU score of 48.1% and\n55.7% on PASCAL-5i for 1-shot and 5-shot settings respectively, surpassing the\nstate-of-the-art method by 1.8% and 8.6%.\n", "versions": [{"version": "v1", "created": "Sun, 18 Aug 2019 07:56:19 GMT"}, {"version": "v2", "created": "Fri, 7 Feb 2020 03:20:01 GMT"}], "update_date": "2020-02-10", "authors_parsed": [["Wang", "Kaixin", ""], ["Liew", "Jun Hao", ""], ["Zou", "Yingtian", ""], ["Zhou", "Daquan", ""], ["Feng", "Jiashi", ""]]}, {"id": "1908.06399", "submitter": "Thomas Rogers", "authors": "T W Rogers, J Gonzalez-Bueno, R Garcia Franco, E Lopez Star, D\n  M\\'endez Mar\\'in, J Vassallo, V C Lansingh, S Trikha, N Jaccard", "title": "Evaluation of an AI System for the Detection of Diabetic Retinopathy\n  from Images Captured with a Handheld Portable Fundus Camera: the MAILOR AI\n  study", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Objectives: To evaluate the performance of an Artificial Intelligence (AI)\nsystem (Pegasus, Visulytix Ltd., UK), at the detection of Diabetic Retinopathy\n(DR) from images captured by a handheld portable fundus camera.\n  Methods: A cohort of 6,404 patients (~80% with diabetes mellitus) was\nscreened for retinal diseases using a handheld portable fundus camera (Pictor\nPlus, Volk Optical Inc., USA) at the Mexican Advanced Imaging Laboratory for\nOcular Research. The images were graded for DR by specialists according to the\nScottish DR grading scheme. The performance of the AI system was evaluated,\nretrospectively, in assessing Referable DR (RDR) and Proliferative DR (PDR) and\ncompared to the performance on a publicly available desktop camera benchmark\ndataset.\n  Results: For RDR detection, Pegasus performed with an 89.4% (95% CI:\n88.0-90.7) Area Under the Receiver Operating Characteristic (AUROC) curve for\nthe MAILOR cohort, compared to an AUROC of 98.5% (95% CI: 97.8-99.2) on the\nbenchmark dataset. This difference was statistically significant. Moreover, no\nstatistically significant difference was found in performance for PDR detection\nwith Pegasus achieving an AUROC of 94.3% (95% CI: 91.0-96.9) on the MAILOR\ncohort and 92.2% (95% CI: 89.4-94.8) on the benchmark dataset.\n  Conclusions: Pegasus showed good transferability for the detection of PDR\nfrom a curated desktop fundus camera dataset to real-world clinical practice\nwith a handheld portable fundus camera. However, there was a substantial, and\nstatistically significant, decrease in the diagnostic performance for RDR when\nusing the handheld device.\n", "versions": [{"version": "v1", "created": "Sun, 18 Aug 2019 08:43:59 GMT"}], "update_date": "2019-08-20", "authors_parsed": [["Rogers", "T W", ""], ["Gonzalez-Bueno", "J", ""], ["Franco", "R Garcia", ""], ["Star", "E Lopez", ""], ["Mar\u00edn", "D M\u00e9ndez", ""], ["Vassallo", "J", ""], ["Lansingh", "V C", ""], ["Trikha", "S", ""], ["Jaccard", "N", ""]]}, {"id": "1908.06401", "submitter": "Naman Jain", "authors": "Sahil Shah, Naman Jain, Abhishek Sharma, Arjun Jain", "title": "On the Robustness of Human Pose Estimation", "comments": "Accepted in CVPR-W", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper provides a comprehensive and exhaustive study of adversarial\nattacks on human pose estimation models and the evaluation of their robustness.\nBesides highlighting the important differences between well-studied\nclassification and human pose-estimation systems w.r.t. adversarial attacks, we\nalso provide deep insights into the design choices of pose-estimation systems\nto shape future work. We benchmark the robustness of several 2D single person\npose-estimation architectures trained on multiple datasets, MPII and COCO. In\ndoing so, we also explore the problem of attacking non-classification networks\nincluding regression based networks, which has been virtually unexplored in the\npast.\n  \\par We find that compared to classification and semantic segmentation, human\npose estimation architectures are relatively robust to adversarial attacks with\nthe single-step attacks being surprisingly ineffective. Our study shows that\nthe heatmap-based pose-estimation models are notably robust than their direct\nregression-based systems and that the systems which explicitly model\nanthropomorphic semantics of human body fare better than their other\ncounterparts. Besides, targeted attacks are more difficult to obtain than\nun-targeted ones and some body-joints are easier to fool than the others. We\npresent visualizations of universal perturbations to facilitate unprecedented\ninsights into their workings on pose-estimation. Additionally, we show them to\ngeneralize well across different networks. Finally we perform a user study\nabout perceptibility of these examples.\n", "versions": [{"version": "v1", "created": "Sun, 18 Aug 2019 09:04:26 GMT"}, {"version": "v2", "created": "Thu, 10 Jun 2021 05:27:07 GMT"}], "update_date": "2021-06-11", "authors_parsed": [["Shah", "Sahil", ""], ["Jain", "Naman", ""], ["Sharma", "Abhishek", ""], ["Jain", "Arjun", ""]]}, {"id": "1908.06416", "submitter": "Rohan Ghosh", "authors": "Rohan Ghosh, Anupam K. Gupta, Mehul Motani", "title": "Investigating Convolutional Neural Networks using Spatial Orderness", "comments": "Presented at BMVC 2019: Workshop on Interpretable and Explainable\n  Machine Vision, Cardiff, UK", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional Neural Networks (CNN) have been pivotal to the success of many\nstate-of-the-art classification problems, in a wide variety of domains (for\ne.g. vision, speech, graphs and medical imaging). A commonality within those\ndomains is the presence of hierarchical, spatially agglomerative\nlocal-to-global interactions within the data. For two-dimensional images, such\ninteractions may induce an a priori relationship between the pixel data and the\nunderlying spatial ordering of the pixels. For instance in natural images,\nneighboring pixels are more likely contain similar values than non-neighboring\npixels which are further apart. To that end, we propose a statistical metric\ncalled spatial orderness, which quantifies the extent to which the input data\n(2D) obeys the underlying spatial ordering at various scales. In our\nexperiments, we mainly find that adding convolutional layers to a CNN could be\ncounterproductive for data bereft of spatial order at higher scales. We also\nobserve, quite counter-intuitively, that the spatial orderness of CNN feature\nmaps show a synchronized increase during the intial stages of training, and\nvalidation performance only improves after spatial orderness of feature maps\nstart decreasing. Lastly, we present a theoretical analysis (and empirical\nvalidation) of the spatial orderness of network weights, where we find that\nusing smaller kernel sizes leads to kernels of greater spatial orderness and\nvice-versa.\n", "versions": [{"version": "v1", "created": "Sun, 18 Aug 2019 10:05:24 GMT"}, {"version": "v2", "created": "Fri, 29 Nov 2019 16:35:10 GMT"}], "update_date": "2019-12-02", "authors_parsed": [["Ghosh", "Rohan", ""], ["Gupta", "Anupam K.", ""], ["Motani", "Mehul", ""]]}, {"id": "1908.06422", "submitter": "Bao Xin Chen", "authors": "Bao Xin Chen and Raghavender Sahdev and Dekun Wu and Xing Zhao and\n  Manos Papagelis and John K. Tsotsos", "title": "Scene Classification in Indoor Environments for Robots using Context\n  Based Word Embeddings", "comments": null, "journal-ref": "2018 IEEE International Conference of Robotics and Automation\n  (ICRA) Workshop", "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scene Classification has been addressed with numerous techniques in computer\nvision literature. However, with the increasing number of scene classes in\ndatasets in the field, it has become difficult to achieve high accuracy in the\ncontext of robotics. In this paper, we implement an approach which combines\ntraditional deep learning techniques with natural language processing methods\nto generate a word embedding based Scene Classification algorithm. We use the\nkey idea that context (objects in the scene) of an image should be\nrepresentative of the scene label meaning a group of objects could assist to\npredict the scene class. Objects present in the scene are represented by\nvectors and the images are re-classified based on the objects present in the\nscene to refine the initial classification by a Convolutional Neural Network\n(CNN). In our approach we address indoor Scene Classification task using a\nmodel trained with a reduced pre-processed version of the Places365 dataset and\nan empirical analysis is done on a real-world dataset that we built by\ncapturing image sequences using a GoPro camera. We also report results obtained\non a subset of the Places365 dataset using our approach and additionally show a\ndeployment of our approach on a robot operating in a real-world environment.\n", "versions": [{"version": "v1", "created": "Sun, 18 Aug 2019 11:17:19 GMT"}], "update_date": "2019-08-29", "authors_parsed": [["Chen", "Bao Xin", ""], ["Sahdev", "Raghavender", ""], ["Wu", "Dekun", ""], ["Zhao", "Xing", ""], ["Papagelis", "Manos", ""], ["Tsotsos", "John K.", ""]]}, {"id": "1908.06427", "submitter": "Samuel Albanie", "authors": "James Thewlis and Samuel Albanie and Hakan Bilen and Andrea Vedaldi", "title": "Unsupervised Learning of Landmarks by Descriptor Vector Exchange", "comments": "ICCV 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Equivariance to random image transformations is an effective method to learn\nlandmarks of object categories, such as the eyes and the nose in faces, without\nmanual supervision. However, this method does not explicitly guarantee that the\nlearned landmarks are consistent with changes between different instances of\nthe same object, such as different facial identities. In this paper, we develop\na new perspective on the equivariance approach by noting that dense landmark\ndetectors can be interpreted as local image descriptors equipped with\ninvariance to intra-category variations. We then propose a direct method to\nenforce such an invariance in the standard equivariant loss. We do so by\nexchanging descriptor vectors between images of different object instances\nprior to matching them geometrically. In this manner, the same vectors must\nwork regardless of the specific object identity considered. We use this\napproach to learn vectors that can simultaneously be interpreted as local\ndescriptors and dense landmarks, combining the advantages of both. Experiments\non standard benchmarks show that this approach can match, and in some cases\nsurpass state-of-the-art performance amongst existing methods that learn\nlandmarks without supervision. Code is available at\nwww.robots.ox.ac.uk/~vgg/research/DVE/.\n", "versions": [{"version": "v1", "created": "Sun, 18 Aug 2019 11:35:45 GMT"}], "update_date": "2019-08-20", "authors_parsed": [["Thewlis", "James", ""], ["Albanie", "Samuel", ""], ["Bilen", "Hakan", ""], ["Vedaldi", "Andrea", ""]]}, {"id": "1908.06440", "submitter": "Shengju Qian", "authors": "Shengju Qian, Keqiang Sun, Wayne Wu, Chen Qian, Jiaya Jia", "title": "Aggregation via Separation: Boosting Facial Landmark Detector with\n  Semi-Supervised Style Translation", "comments": "Accepted to ICCV 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Facial landmark detection, or face alignment, is a fundamental task that has\nbeen extensively studied. In this paper, we investigate a new perspective of\nfacial landmark detection and demonstrate it leads to further notable\nimprovement. Given that any face images can be factored into space of style\nthat captures lighting, texture and image environment, and a style-invariant\nstructure space, our key idea is to leverage disentangled style and shape space\nof each individual to augment existing structures via style translation. With\nthese augmented synthetic samples, our semi-supervised model surprisingly\noutperforms the fully-supervised one by a large margin. Extensive experiments\nverify the effectiveness of our idea with state-of-the-art results on WFLW,\n300W, COFW, and AFLW datasets. Our proposed structure is general and could be\nassembled into any face alignment frameworks. The code is made publicly\navailable at https://github.com/thesouthfrog/stylealign.\n", "versions": [{"version": "v1", "created": "Sun, 18 Aug 2019 13:08:39 GMT"}], "update_date": "2019-08-20", "authors_parsed": [["Qian", "Shengju", ""], ["Sun", "Keqiang", ""], ["Wu", "Wayne", ""], ["Qian", "Chen", ""], ["Jia", "Jiaya", ""]]}, {"id": "1908.06442", "submitter": "Yu Rong", "authors": "Yu Rong, Ziwei Liu, Cheng Li, Kaidi Cao, Chen Change Loy", "title": "Delving Deep Into Hybrid Annotations for 3D Human Recovery in the Wild", "comments": "To appear in ICCV 2019. Code and models are available at\n  https://penincillin.github.io/dct_iccv2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Though much progress has been achieved in single-image 3D human recovery,\nestimating 3D model for in-the-wild images remains a formidable challenge. The\nreason lies in the fact that obtaining high-quality 3D annotations for\nin-the-wild images is an extremely hard task that consumes enormous amount of\nresources and manpower. To tackle this problem, previous methods adopt a hybrid\ntraining strategy that exploits multiple heterogeneous types of annotations\nincluding 3D and 2D while leaving the efficacy of each annotation not\nthoroughly investigated. In this work, we aim to perform a comprehensive study\non cost and effectiveness trade-off between different annotations.\nSpecifically, we focus on the challenging task of in-the-wild 3D human recovery\nfrom single images when paired 3D annotations are not fully available. Through\nextensive experiments, we obtain several observations: 1) 3D annotations are\nefficient, whereas traditional 2D annotations such as 2D keypoints and body\npart segmentation are less competent in guiding 3D human recovery. 2) Dense\nCorrespondence such as DensePose is effective. When there are no paired\nin-the-wild 3D annotations available, the model exploiting dense correspondence\ncan achieve 92% of the performance compared to a model trained with paired 3D\ndata. We show that incorporating dense correspondence into in-the-wild 3D human\nrecovery is promising and competitive due to its high efficiency and relatively\nlow annotating cost. Our model trained with dense correspondence can serve as a\nstrong reference for future research.\n", "versions": [{"version": "v1", "created": "Sun, 18 Aug 2019 13:48:11 GMT"}, {"version": "v2", "created": "Tue, 20 Aug 2019 02:42:18 GMT"}], "update_date": "2019-08-21", "authors_parsed": [["Rong", "Yu", ""], ["Liu", "Ziwei", ""], ["Li", "Cheng", ""], ["Cao", "Kaidi", ""], ["Loy", "Chen Change", ""]]}, {"id": "1908.06444", "submitter": "Jinshan Pan", "authors": "Jinshan Pan, Yang Liu, Deqing Sun, Jimmy Ren, Ming-Ming Cheng, Jian\n  Yang, Jinhui Tang", "title": "Image Formation Model Guided Deep Image Super-Resolution", "comments": "AAAI 2020. The training code and models are available at\n  https://github.com/jspan/PHYSICS SR", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a simple and effective image super-resolution algorithm that\nimposes an image formation constraint on the deep neural networks via pixel\nsubstitution. The proposed algorithm first uses a deep neural network to\nestimate intermediate high-resolution images, blurs the intermediate images\nusing known blur kernels, and then substitutes values of the pixels at the\nun-decimated positions with those of the corresponding pixels from the\nlow-resolution images. The output of the pixel substitution process strictly\nsatisfies the image formation model and is further refined by the same deep\nneural network in a cascaded manner. The proposed framework is trained in an\nend-to-end fashion and can work with existing feed-forward deep neural networks\nfor super-resolution and converges fast in practice. Extensive experimental\nresults show that the proposed algorithm performs favorably against\nstate-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Sun, 18 Aug 2019 13:55:17 GMT"}, {"version": "v2", "created": "Sun, 25 Aug 2019 00:45:33 GMT"}, {"version": "v3", "created": "Sun, 29 Mar 2020 12:46:42 GMT"}], "update_date": "2020-03-31", "authors_parsed": [["Pan", "Jinshan", ""], ["Liu", "Yang", ""], ["Sun", "Deqing", ""], ["Ren", "Jimmy", ""], ["Cheng", "Ming-Ming", ""], ["Yang", "Jian", ""], ["Tang", "Jinhui", ""]]}, {"id": "1908.06452", "submitter": "S Deng", "authors": "Luming Liang, Sen Deng, Lionel Gueguen, Mingqiang Wei, Xinming Wu, and\n  Jing Qin", "title": "Convolutional Neural Network with Median Layers for Denoising\n  Salt-and-Pepper Contaminations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a deep fully convolutional neural network with a new type of\nlayer, named median layer, to restore images contaminated by the\nsalt-and-pepper (s&p) noise. A median layer simply performs median filtering on\nall feature channels. By adding this kind of layer into some widely used fully\nconvolutional deep neural networks, we develop an end-to-end network that\nremoves the extremely high-level s&p noise without performing any non-trivial\npreprocessing tasks, which is different from all the existing literature in s&p\nnoise removal. Experiments show that inserting median layers into a simple\nfully-convolutional network with the L2 loss significantly boosts the\nsignal-to-noise ratio. Quantitative comparisons testify that our network\noutperforms the state-of-the-art methods with a limited amount of training\ndata. The source code has been released for public evaluation and use\n(https://github.com/llmpass/medianDenoise).\n", "versions": [{"version": "v1", "created": "Sun, 18 Aug 2019 14:56:14 GMT"}], "update_date": "2019-08-20", "authors_parsed": [["Liang", "Luming", ""], ["Deng", "Sen", ""], ["Gueguen", "Lionel", ""], ["Wei", "Mingqiang", ""], ["Wu", "Xinming", ""], ["Qin", "Jing", ""]]}, {"id": "1908.06472", "submitter": "Andreas Kamilaris", "authors": "Andreas Kamilaris, Corjan van den Brink and Savvas Karatsiolis", "title": "Training Deep Learning Models via Synthetic Data: Application in\n  Unmanned Aerial Vehicles", "comments": "Workshop on Deep-learning based computer vision for UAV in\n  conjunction with CAIP 2019, Salerno, italy, September 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper describes preliminary work in the recent promising approach of\ngenerating synthetic training data for facilitating the learning procedure of\ndeep learning (DL) models, with a focus on aerial photos produced by unmanned\naerial vehicles (UAV). The general concept and methodology are described, and\npreliminary results are presented, based on a classification problem of fire\nidentification in forests as well as a counting problem of estimating number of\nhouses in urban areas. The proposed technique constitutes a new possibility for\nthe DL community, especially related to UAV-based imagery analysis, with much\npotential, promising results, and unexplored ground for further research.\n", "versions": [{"version": "v1", "created": "Sun, 18 Aug 2019 16:25:14 GMT"}], "update_date": "2019-08-20", "authors_parsed": [["Kamilaris", "Andreas", ""], ["Brink", "Corjan van den", ""], ["Karatsiolis", "Savvas", ""]]}, {"id": "1908.06473", "submitter": "Haipeng Xiong", "authors": "Haipeng Xiong, Hao Lu, Chengxin Liu, Liang Liu, Zhiguo Cao, Chunhua\n  Shen", "title": "From Open Set to Closed Set: Counting Objects by Spatial\n  Divide-and-Conquer", "comments": "Accepted by ICCV2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual counting, a task that predicts the number of objects from an\nimage/video, is an open-set problem by nature, i.e., the number of population\ncan vary in $[0,+\\infty)$ in theory. However, the collected images and labeled\ncount values are limited in reality, which means only a small closed set is\nobserved. Existing methods typically model this task in a regression manner,\nwhile they are likely to suffer from an unseen scene with counts out of the\nscope of the closed set. In fact, counting is decomposable. A dense region can\nalways be divided until sub-region counts are within the previously observed\nclosed set. Inspired by this idea, we propose a simple but effective approach,\nSpatial Divide-and- Conquer Network (S-DCNet). S-DCNet only learns from a\nclosed set but can generalize well to open-set scenarios via S-DC. S-DCNet is\nalso efficient. To avoid repeatedly computing sub-region convolutional\nfeatures, S-DC is executed on the feature map instead of on the input image.\nS-DCNet achieves the state-of-the-art performance on three crowd counting\ndatasets (ShanghaiTech, UCF_CC_50 and UCF-QNRF), a vehicle counting dataset\n(TRANCOS) and a plant counting dataset (MTC). Compared to the previous best\nmethods, S-DCNet brings a 20.2% relative improvement on the ShanghaiTech Part\nB, 20.9% on the UCF-QNRF, 22.5% on the TRANCOS and 15.1% on the MTC. Code has\nbeen made available at: https://github. com/xhp-hust-2018-2011/S-DCNet.\n", "versions": [{"version": "v1", "created": "Thu, 15 Aug 2019 03:19:16 GMT"}], "update_date": "2019-08-20", "authors_parsed": [["Xiong", "Haipeng", ""], ["Lu", "Hao", ""], ["Liu", "Chengxin", ""], ["Liu", "Liang", ""], ["Cao", "Zhiguo", ""], ["Shen", "Chunhua", ""]]}, {"id": "1908.06484", "submitter": "Rodolfo Migon Favaretto", "authors": "Rodolfo Migon Favaretto and Victor Araujo and Soraia Raupp Musse and\n  Felipe Vilanova and Angelo Brandelli Costa", "title": "A Software to Detect OCC Emotion, Big-Five Personality and Hofstede\n  Cultural Dimensions of Pedestrians from Video Sequences", "comments": "13 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a video analysis application to detect personality,\nemotion and cultural aspects from pedestrians in video sequences, along with a\nvisualizer of features. The proposed model considers a series of\ncharacteristics of the pedestrians and the crowd, such as number and size of\ngroups, distances, speeds, among others, and performs the mapping of these\ncharacteristics in personalities, emotions and cultural aspects, considering\nthe Cultural Dimensions of Hofstede (HCD), the Big-Five Personality Model\n(OCEAN) and the OCC Emotional Model. The main hypothesis is that there is a\nrelationship between so-called intrinsic human variables (such as emotion) and\nthe way people behave in space and time. The software was tested in a set of\nvideos from different countries and results seem promising in order to identify\nthese three different levels of psychological traits in the filmed sequences.\nIn addition, the data of the people present in the videos can be seen in a\ncrowd viewer.\n", "versions": [{"version": "v1", "created": "Sun, 18 Aug 2019 17:17:03 GMT"}], "update_date": "2019-08-20", "authors_parsed": [["Favaretto", "Rodolfo Migon", ""], ["Araujo", "Victor", ""], ["Musse", "Soraia Raupp", ""], ["Vilanova", "Felipe", ""], ["Costa", "Angelo Brandelli", ""]]}, {"id": "1908.06498", "submitter": "Aliasghar Mortazi", "authors": "Aliasghar Mortazi, Naji Khosravan, Drew A. Torigian, Sila Kurugol,\n  Ulas Bagci", "title": "Weakly Supervised Segmentation by A Deep Geodesic Prior", "comments": "Accepted to Machine Learning in Medical Imaging (MLMI 2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The performance of the state-of-the-art image segmentation methods heavily\nrelies on the high-quality annotations, which are not easily affordable,\nparticularly for medical data. To alleviate this limitation, in this study, we\npropose a weakly supervised image segmentation method based on a deep geodesic\nprior. We hypothesize that integration of this prior information can reduce the\nadverse effects of weak labels in segmentation accuracy. Our proposed algorithm\nis based on a prior information, extracted from an auto-encoder, trained to map\nobjects geodesic maps to their corresponding binary maps. The obtained\ninformation is then used as an extra term in the loss function of the\nsegmentor. In order to show efficacy of the proposed strategy, we have\nexperimented segmentation of cardiac substructures with clean and two levels of\nnoisy labels (L1, L2). Our experiments showed that the proposed algorithm\nboosted the performance of baseline deep learning-based segmentation for both\nclean and noisy labels by 4.4%, 4.6%(L1), and 6.3%(L2) in dice score,\nrespectively. We also showed that the proposed method was more robust in the\npresence of high-level noise due to the existence of shape priors.\n", "versions": [{"version": "v1", "created": "Sun, 18 Aug 2019 18:43:44 GMT"}], "update_date": "2019-08-20", "authors_parsed": [["Mortazi", "Aliasghar", ""], ["Khosravan", "Naji", ""], ["Torigian", "Drew A.", ""], ["Kurugol", "Sila", ""], ["Bagci", "Ulas", ""]]}, {"id": "1908.06537", "submitter": "Juhong Min", "authors": "Juhong Min, Jongmin Lee, Jean Ponce, Minsu Cho", "title": "Hyperpixel Flow: Semantic Correspondence with Multi-layer Neural\n  Features", "comments": "Accepted to ICCV 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Establishing visual correspondences under large intra-class variations\nrequires analyzing images at different levels, from features linked to\nsemantics and context to local patterns, while being invariant to\ninstance-specific details. To tackle these challenges, we represent images by\n\"hyperpixels\" that leverage a small number of relevant features selected among\nearly to late layers of a convolutional neural network. Taking advantage of the\ncondensed features of hyperpixels, we develop an effective real-time matching\nalgorithm based on Hough geometric voting. The proposed method, hyperpixel\nflow, sets a new state of the art on three standard benchmarks as well as a new\ndataset, SPair-71k, which contains a significantly larger number of image pairs\nthan existing datasets, with more accurate and richer annotations for in-depth\nanalysis.\n", "versions": [{"version": "v1", "created": "Sun, 18 Aug 2019 23:23:54 GMT"}], "update_date": "2019-08-20", "authors_parsed": [["Min", "Juhong", ""], ["Lee", "Jongmin", ""], ["Ponce", "Jean", ""], ["Cho", "Minsu", ""]]}, {"id": "1908.06539", "submitter": "Chih-Hui Ho", "authors": "Jen-Hui Chuang, Chih-Hui Ho, Ardian Umam, HsinYi Chen, Mu-Tien Lu,\n  Jenq-Neng Hwang, Tai-An Chen", "title": "A New Technique of Camera Calibration: A Geometric Approach Based on\n  Principal Lines", "comments": "Under review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Camera calibration is a crucial prerequisite in many applications of computer\nvision. In this paper, a new, geometry-based camera calibration technique is\nproposed, which resolves two main issues associated with the widely used\nZhang's method: (i) the lack of guidelines to avoid outliers in the computation\nand (ii) the assumption of fixed camera focal length. The proposed approach is\nbased on the closed-form solution of principal lines (PLs), with their\nintersection being the principal point while each PL can concisely represent\nrelative orientation/position (up to one degree of freedom for both) between a\nspecial pair of coordinate systems of image plane and calibration pattern. With\nsuch analytically tractable image features, computations associated with the\ncalibration are greatly simplified, while the guidelines in (i) can be\nestablished intuitively. Experimental results for synthetic and real data show\nthat the proposed approach does compare favorably with Zhang's method, in terms\nof correctness, robustness, and flexibility, and addresses issues (i) and (ii)\nsatisfactorily.\n", "versions": [{"version": "v1", "created": "Sun, 18 Aug 2019 23:28:05 GMT"}], "update_date": "2019-08-20", "authors_parsed": [["Chuang", "Jen-Hui", ""], ["Ho", "Chih-Hui", ""], ["Umam", "Ardian", ""], ["Chen", "HsinYi", ""], ["Lu", "Mu-Tien", ""], ["Hwang", "Jenq-Neng", ""], ["Chen", "Tai-An", ""]]}, {"id": "1908.06544", "submitter": "Yudhik Agrawal", "authors": "Abbhinav Venkat, Chaitanya Patel, Yudhik Agrawal, Avinash Sharma", "title": "HumanMeshNet: Polygonal Mesh Recovery of Humans", "comments": "to appear in ICCV-W, 2019. Project:\n  https://github.com/yudhik11/HumanMeshNet", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  3D Human Body Reconstruction from a monocular image is an important problem\nin computer vision with applications in virtual and augmented reality\nplatforms, animation industry, en-commerce domain, etc. While several of the\nexisting works formulate it as a volumetric or parametric learning with complex\nand indirect reliance on re-projections of the mesh, we would like to focus on\nimplicitly learning the mesh representation. To that end, we propose a novel\nmodel, HumanMeshNet, that regresses a template mesh's vertices, as well as\nreceives a regularization by the 3D skeletal locations in a multi-branch,\nmulti-task setup. The image to mesh vertex regression is further regularized by\nthe neighborhood constraint imposed by mesh topology ensuring smooth surface\nreconstruction. The proposed paradigm can theoretically learn local surface\ndeformations induced by body shape variations and can therefore learn\nhigh-resolution meshes going ahead. We show comparable performance with SoA (in\nterms of surface and joint error) with far lesser computational complexity,\nmodeling cost and therefore real-time reconstructions on three publicly\navailable datasets. We also show the generalizability of the proposed paradigm\nfor a similar task of predicting hand mesh models. Given these initial results,\nwe would like to exploit the mesh topology in an explicit manner going ahead.\n", "versions": [{"version": "v1", "created": "Mon, 19 Aug 2019 00:27:24 GMT"}], "update_date": "2019-08-20", "authors_parsed": [["Venkat", "Abbhinav", ""], ["Patel", "Chaitanya", ""], ["Agrawal", "Yudhik", ""], ["Sharma", "Avinash", ""]]}, {"id": "1908.06552", "submitter": "Phuc Nguyen X", "authors": "Phuc Xuan Nguyen, Deva Ramanan, Charless C. Fowlkes", "title": "Weakly-supervised Action Localization with Background Modeling", "comments": "To appear at ICCV 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe a latent approach that learns to detect actions in long sequences\ngiven training videos with only whole-video class labels. Our approach makes\nuse of two innovations to attention-modeling in weakly-supervised learning.\nFirst, and most notably, our framework uses an attention model to extract both\nforeground and background frames whose appearance is explicitly modeled. Most\nprior works ignore the background, but we show that modeling it allows our\nsystem to learn a richer notion of actions and their temporal extents. Second,\nwe combine bottom-up, class-agnostic attention modules with top-down,\nclass-specific activation maps, using the latter as form of self-supervision\nfor the former. Doing so allows our model to learn a more accurate model of\nattention without explicit temporal supervision. These modifications lead to\n10% AP@IoU=0.5 improvement over existing systems on THUMOS14. Our proposed\nweaklysupervised system outperforms recent state-of-the-arts by at least 4.3%\nAP@IoU=0.5. Finally, we demonstrate that weakly-supervised learning can be used\nto aggressively scale-up learning to in-the-wild, uncurated Instagram videos.\nThe addition of these videos significantly improves localization performance of\nour weakly-supervised model\n", "versions": [{"version": "v1", "created": "Mon, 19 Aug 2019 01:33:14 GMT"}], "update_date": "2019-08-20", "authors_parsed": [["Nguyen", "Phuc Xuan", ""], ["Ramanan", "Deva", ""], ["Fowlkes", "Charless C.", ""]]}, {"id": "1908.06566", "submitter": "Zhendong Zhang", "authors": "Zhendong Zhang, Cheolkon Jung and Xiaolong Liang", "title": "Adversarial Defense by Suppressing High-frequency Components", "comments": "3 pages. This paper is a technical report of the 5th place solution\n  in the IJCAI-2019 Alibaba Adversarial AI Challenge. This paper has been\n  accepted by the corresponding workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recent works show that deep neural networks trained on image classification\ndataset bias towards textures. Those models are easily fooled by applying small\nhigh-frequency perturbations to clean images. In this paper, we learn robust\nimage classification models by removing high-frequency components.\nSpecifically, we develop a differentiable high-frequency suppression module\nbased on discrete Fourier transform (DFT). Combining with adversarial training,\nwe won the 5th place in the IJCAI-2019 Alibaba Adversarial AI Challenge. Our\ncode is available online.\n", "versions": [{"version": "v1", "created": "Mon, 19 Aug 2019 02:43:35 GMT"}, {"version": "v2", "created": "Fri, 23 Aug 2019 15:49:51 GMT"}, {"version": "v3", "created": "Tue, 3 Sep 2019 09:00:46 GMT"}], "update_date": "2019-09-04", "authors_parsed": [["Zhang", "Zhendong", ""], ["Jung", "Cheolkon", ""], ["Liang", "Xiaolong", ""]]}, {"id": "1908.06576", "submitter": "Xiangyang He", "authors": "Xiangyang He, Yubo Tao, Qirui Wang, Hai Lin", "title": "A Co-analysis Framework for Exploring Multivariate Scientific Data", "comments": "31 pages, 7 figures", "journal-ref": "Visual Informatics Volume 2, Issue 4, December 2018, Pages 254-263", "doi": "10.1016/j.visinf.2018.12.005", "report-no": null, "categories": "cs.CV cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In complex multivariate data sets, different features usually include diverse\nassociations with different variables, and different variables are associated\nwithin different regions. Therefore, exploring the associations between\nvariables and voxels locally becomes necessary to better understand the\nunderlying phenomena. In this paper, we propose a co-analysis framework based\non biclusters, which are two subsets of variables and voxels with close\nscalar-value relationships, to guide the process of visually exploring\nmultivariate data. We first automatically extract all meaningful biclusters,\neach of which only contains voxels with a similar scalar-value pattern over a\nsubset of variables. These biclusters are organized according to their variable\nsets, and biclusters in each variable set are further grouped by a similarity\nmetric to reduce redundancy and support diversity during visual exploration.\nBiclusters are visually represented in coordinated views to facilitate\ninteractive exploration of multivariate data based on the similarity between\nbiclusters and the correlation of scalar values with different variables.\nExperiments on several representative multivariate scientific data sets\ndemonstrate the effectiveness of our framework in exploring local relationships\namong variables, biclusters and scalar values in the data.\n", "versions": [{"version": "v1", "created": "Mon, 19 Aug 2019 03:31:48 GMT"}], "update_date": "2019-08-20", "authors_parsed": [["He", "Xiangyang", ""], ["Tao", "Yubo", ""], ["Wang", "Qirui", ""], ["Lin", "Hai", ""]]}, {"id": "1908.06588", "submitter": "Mahdi Javanmardi", "authors": "Mahdi Javanmardi, Ehsan Javanmardi, and Shunsuke Kamijo", "title": "How far should self-driving cars see? Effect of observation range on\n  vehicle self-localization", "comments": "6 pages, 11 figures, IEEE International Conference on Intelligent\n  Transportation Systems 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accuracy and time efficiency are two essential requirements for the\nself-localization of autonomous vehicles. While the observation range\nconsidered for simultaneous localization and mapping (SLAM) has a significant\neffect on both accuracy and computation time, its effect is not well\ninvestigated in the literature. In this paper, we will answer the question: How\nfar should a driverless car observe during self-localization? We introduce a\nframework to dynamically define the observation range for localization to meet\nthe accuracy requirement for autonomous driving, while keeping the computation\ntime low. To model the effect of scanning range on the localization accuracy\nfor every point on the map, several map factors were employed. The capability\nof the proposed framework was verified using field data, demonstrating that it\nis able to improve the average matching time from 142.2 ms to 39.3 ms while\nkeeping the localization accuracy around 8.1 cm.\n", "versions": [{"version": "v1", "created": "Mon, 19 Aug 2019 04:32:44 GMT"}], "update_date": "2019-08-20", "authors_parsed": [["Javanmardi", "Mahdi", ""], ["Javanmardi", "Ehsan", ""], ["Kamijo", "Shunsuke", ""]]}, {"id": "1908.06592", "submitter": "Boren Li", "authors": "Boren Li, Boyu Zhuang, Mingyang Li, Jian Gu", "title": "Seq-SG2SL: Inferring Semantic Layout from Scene Graph Through Sequence\n  to Sequence Learning", "comments": "This paper will appear at ICCV 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generating semantic layout from scene graph is a crucial intermediate task\nconnecting text to image. We present a conceptually simple, flexible and\ngeneral framework using sequence to sequence (seq-to-seq) learning for this\ntask. The framework, called Seq-SG2SL, derives sequence proxies for the two\nmodality and a Transformer-based seq-to-seq model learns to transduce one into\nthe other. A scene graph is decomposed into a sequence of semantic fragments\n(SF), one for each relationship. A semantic layout is represented as the\nconsequence from a series of brick-action code segments (BACS), dictating the\nposition and scale of each object bounding box in the layout. Viewing the two\nbuilding blocks, SF and BACS, as corresponding terms in two different\nvocabularies, a seq-to-seq model is fittingly used to translate. A new metric,\nsemantic layout evaluation understudy (SLEU), is devised to evaluate the task\nof semantic layout prediction inspired by BLEU. SLEU defines relationships\nwithin a layout as unigrams and looks at the spatial distribution for n-grams.\nUnlike the binary precision of BLEU, SLEU allows for some tolerances spatially\nthrough thresholding the Jaccard Index and is consequently more adapted to the\ntask. Experimental results on the challenging Visual Genome dataset show\nimprovement over a non-sequential approach based on graph convolution.\n", "versions": [{"version": "v1", "created": "Mon, 19 Aug 2019 04:47:26 GMT"}], "update_date": "2019-08-20", "authors_parsed": [["Li", "Boren", ""], ["Zhuang", "Boyu", ""], ["Li", "Mingyang", ""], ["Gu", "Jian", ""]]}, {"id": "1908.06607", "submitter": "Jinqiang Bai", "authors": "Zhaoxiang Liu, Huan Hu, Zipeng Wang, Kai Wang, Jinqiang Bai, Shiguo\n  Lian", "title": "Video synthesis of human upper body with realistic face", "comments": "3 pages, 4 figures,Accepted by ISMAR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a generative adversarial learning-based human upper body\nvideo synthesis approach to generate an upper body video of target person that\nis consistent with the body motion, face expression, and pose of the person in\nsource video. We use upper body keypoints, facial action units and poses as\nintermediate representations between source video and target video. Instead of\ndirectly transferring the source video to the target video, we firstly map the\nsource person's facial action units and poses into the target person's facial\nlandmarks, then combine the normalized upper body keypoints and generated\nfacial landmarks with spatio-temporal smoothing to generate the corresponding\ntarget video's image. Experimental results demonstrated the effectiveness of\nour method.\n", "versions": [{"version": "v1", "created": "Mon, 19 Aug 2019 06:30:23 GMT"}, {"version": "v2", "created": "Tue, 20 Aug 2019 04:20:36 GMT"}, {"version": "v3", "created": "Thu, 12 Sep 2019 08:33:03 GMT"}], "update_date": "2019-09-13", "authors_parsed": [["Liu", "Zhaoxiang", ""], ["Hu", "Huan", ""], ["Wang", "Zipeng", ""], ["Wang", "Kai", ""], ["Bai", "Jinqiang", ""], ["Lian", "Shiguo", ""]]}, {"id": "1908.06616", "submitter": "Hajar Emami Gohari", "authors": "Hajar Emami, Majid Moradi Aliabadi, Ming Dong, Ratna Babu Chinnam", "title": "SPA-GAN: Spatial Attention GAN for Image-to-Image Translation", "comments": "IEEE Transactions on Multimedia, Digital Object Identifier:\n  10.1109/TMM.2020.2975961", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image-to-image translation is to learn a mapping between images from a source\ndomain and images from a target domain. In this paper, we introduce the\nattention mechanism directly to the generative adversarial network (GAN)\narchitecture and propose a novel spatial attention GAN model (SPA-GAN) for\nimage-to-image translation tasks. SPA-GAN computes the attention in its\ndiscriminator and use it to help the generator focus more on the most\ndiscriminative regions between the source and target domains, leading to more\nrealistic output images. We also find it helpful to introduce an additional\nfeature map loss in SPA-GAN training to preserve domain specific features\nduring translation. Compared with existing attention-guided GAN models, SPA-GAN\nis a lightweight model that does not need additional attention networks or\nsupervision. Qualitative and quantitative comparison against state-of-the-art\nmethods on benchmark datasets demonstrates the superior performance of SPA-GAN.\n", "versions": [{"version": "v1", "created": "Mon, 19 Aug 2019 07:09:05 GMT"}, {"version": "v2", "created": "Sun, 5 Apr 2020 08:09:07 GMT"}, {"version": "v3", "created": "Wed, 30 Dec 2020 22:46:05 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["Emami", "Hajar", ""], ["Aliabadi", "Majid Moradi", ""], ["Dong", "Ming", ""], ["Chinnam", "Ratna Babu", ""]]}, {"id": "1908.06623", "submitter": "Yanning Zhou", "authors": "Yanning Zhou, Hao Chen, Jiaqi Xu, Qi Dou, Pheng-Ann Heng", "title": "IRNet: Instance Relation Network for Overlapping Cervical Cell\n  Segmentation", "comments": "Accepted by the 22nd International Conference on Medical Image\n  Computing and Computer Assisted Intervention", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cell instance segmentation in Pap smear image remains challenging due to the\nwide existence of occlusion among translucent cytoplasm in cell clumps.\nConventional methods heavily rely on accurate nuclei detection results and are\neasily disturbed by miscellaneous objects. In this paper, we propose a novel\nInstance Relation Network (IRNet) for robust overlapping cell segmentation by\nexploring instance relation interaction. Specifically, we propose the Instance\nRelation Module to construct the cell association matrix for transferring\ninformation among individual cell-instance features. With the collaboration of\ndifferent instances, the augmented features gain benefits from contextual\ninformation and improve semantic consistency. Meanwhile, we proposed a sparsity\nconstrained Duplicate Removal Module to eliminate the misalignment between\nclassification and localization accuracy for candidates selection. The largest\ncervical Pap smear (CPS) dataset with more than 8000 cell annotations in Pap\nsmear image was constructed for comprehensive evaluation. Our method\noutperforms other methods by a large margin, demonstrating the effectiveness of\nexploring instance relation.\n", "versions": [{"version": "v1", "created": "Mon, 19 Aug 2019 07:34:02 GMT"}], "update_date": "2019-08-20", "authors_parsed": [["Zhou", "Yanning", ""], ["Chen", "Hao", ""], ["Xu", "Jiaqi", ""], ["Dou", "Qi", ""], ["Heng", "Pheng-Ann", ""]]}, {"id": "1908.06646", "submitter": "Hakan Ardo", "authors": "H{\\aa}kan Ard\\\"o, Mikael Nilsson", "title": "Multi Target Tracking by Learning from Generalized Graph Differences", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Formulating the multi object tracking problem as a network flow optimization\nproblem is a popular choice. In this paper an efficient way of learning the\nweights of such a network is presented. It separates the problem into one\nembedding of feasible solutions into a one dimensional feature space and one\noptimization problem. The embedding can be learned using standard SGD type\noptimization without relying on an additional optimizations within each step.\nTraining data is produced by performing small perturbations of ground truth\ntracks and representing them using generalized graph differences, which is an\nefficient way introduced to represent the difference between two graphs. The\nproposed method is evaluated on DukeMTMCT with competitive results.\n", "versions": [{"version": "v1", "created": "Mon, 19 Aug 2019 08:57:46 GMT"}], "update_date": "2019-08-20", "authors_parsed": [["Ard\u00f6", "H\u00e5kan", ""], ["Nilsson", "Mikael", ""]]}, {"id": "1908.06647", "submitter": "Jun Xu", "authors": "Ziqin Wang, Jun Xu, Li Liu, Fan Zhu, Ling Shao", "title": "RANet: Ranking Attention Network for Fast Video Object Segmentation", "comments": "Accepted by ICCV 2019. 10 pages, 7 figures, 6 tables. The\n  supplementary file can be found at\n  https://csjunxu.github.io/paper/2019ICCV/RANet_supp.pdf ; Code is available\n  at https://github.com/Storife/RANet", "journal-ref": null, "doi": "10.1109/ICCV.2019.00408", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite online learning (OL) techniques have boosted the performance of\nsemi-supervised video object segmentation (VOS) methods, the huge time costs of\nOL greatly restrict their practicality. Matching based and propagation based\nmethods run at a faster speed by avoiding OL techniques. However, they are\nlimited by sub-optimal accuracy, due to mismatching and drifting problems. In\nthis paper, we develop a real-time yet very accurate Ranking Attention Network\n(RANet) for VOS. Specifically, to integrate the insights of matching based and\npropagation based methods, we employ an encoder-decoder framework to learn\npixel-level similarity and segmentation in an end-to-end manner. To better\nutilize the similarity maps, we propose a novel ranking attention module, which\nautomatically ranks and selects these maps for fine-grained VOS performance.\nExperiments on DAVIS-16 and DAVIS-17 datasets show that our RANet achieves the\nbest speed-accuracy trade-off, e.g., with 33 milliseconds per frame and\nJ&F=85.5% on DAVIS-16. With OL, our RANet reaches J&F=87.1% on DAVIS-16,\nexceeding state-of-the-art VOS methods. The code can be found at\nhttps://github.com/Storife/RANet.\n", "versions": [{"version": "v1", "created": "Mon, 19 Aug 2019 08:58:49 GMT"}, {"version": "v2", "created": "Wed, 21 Aug 2019 03:45:25 GMT"}, {"version": "v3", "created": "Sun, 25 Aug 2019 15:30:36 GMT"}, {"version": "v4", "created": "Sun, 8 Sep 2019 04:01:27 GMT"}], "update_date": "2020-05-29", "authors_parsed": [["Wang", "Ziqin", ""], ["Xu", "Jun", ""], ["Liu", "Li", ""], ["Zhu", "Fan", ""], ["Shao", "Ling", ""]]}, {"id": "1908.06648", "submitter": "Yin Bi", "authors": "Yin Bi, Aaron Chadha, Alhabib Abbas, Eirina Bourtsoulatze and Yiannis\n  Andreopoulos", "title": "Graph-Based Object Classification for Neuromorphic Vision Sensing", "comments": "13 pages, 4 figures, ICCV 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neuromorphic vision sensing (NVS)\\ devices represent visual information as\nsequences of asynchronous discrete events (a.k.a., ``spikes'') in response to\nchanges in scene reflectance. Unlike conventional active pixel sensing (APS),\nNVS allows for significantly higher event sampling rates at substantially\nincreased energy efficiency and robustness to illumination changes. However,\nobject classification with NVS streams cannot leverage on state-of-the-art\nconvolutional neural networks (CNNs), since NVS does not produce frame\nrepresentations. To circumvent this mismatch between sensing and processing\nwith CNNs, we propose a compact graph representation for NVS. We couple this\nwith novel residual graph CNN architectures and show that, when trained on\nspatio-temporal NVS data for object classification, such residual graph CNNs\npreserve the spatial and temporal coherence of spike events, while requiring\nless computation and memory. Finally, to address the absence of large\nreal-world NVS datasets for complex recognition tasks, we present and make\navailable a 100k dataset of NVS recordings of the American sign language\nletters, acquired with an iniLabs DAVIS240c device under real-world conditions.\n", "versions": [{"version": "v1", "created": "Mon, 19 Aug 2019 08:59:21 GMT"}], "update_date": "2019-08-20", "authors_parsed": [["Bi", "Yin", ""], ["Chadha", "Aaron", ""], ["Abbas", "Alhabib", ""], ["Bourtsoulatze", "Eirina", ""], ["Andreopoulos", "Yiannis", ""]]}, {"id": "1908.06665", "submitter": "Yang Dongming", "authors": "Dongming Yang, YueXian Zou, Jian Zhang, Ge Li", "title": "C-RPNs: Promoting Object Detection in real world via a Cascade Structure\n  of Region Proposal Networks", "comments": "28 pages,10 figures", "journal-ref": null, "doi": "10.1016/j.neucom.2019.08.016", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, significant progresses have been made in object detection on common\nbenchmarks (i.e., Pascal VOC). However, object detection in real world is still\nchallenging due to the serious data imbalance. Images in real world are\ndominated by easy samples like the wide range of background and some easily\nrecognizable objects, for example. Although two-stage detectors like Faster\nR-CNN achieved big successes in object detection due to the strategy of\nextracting region proposals by region proposal network, they show their poor\nadaption in real-world object detection as a result of without considering\nmining hard samples during extracting region proposals. To address this issue,\nwe propose a Cascade framework of Region Proposal Networks, referred to as\nC-RPNs. The essence of C-RPNs is adopting multiple stages to mine hard samples\nwhile extracting region proposals and learn stronger classifiers. Meanwhile, a\nfeature chain and a score chain are proposed to help learning more\ndiscriminative representations for proposals. Moreover, a loss function of\ncascade stages is designed to train cascade classifiers through\nbackpropagation. Our proposed method has been evaluated on Pascal VOC and\nseveral challenging datasets like BSBDV 2017, CityPersons, etc. Our method\nachieves competitive results compared with the current state-of-the-arts and\nall-sided improvements in error analysis, validating its efficacy for detection\nin real world.\n", "versions": [{"version": "v1", "created": "Mon, 19 Aug 2019 09:37:08 GMT"}], "update_date": "2019-08-20", "authors_parsed": [["Yang", "Dongming", ""], ["Zou", "YueXian", ""], ["Zhang", "Jian", ""], ["Li", "Ge", ""]]}, {"id": "1908.06673", "submitter": "Congcong Wen", "authors": "Congcong Wen, Lina Yang, Ling Peng, Xiang Li, Tianhe Chi", "title": "Directionally Constrained Fully Convolutional Neural Network For\n  Airborne Lidar Point Cloud Classification", "comments": null, "journal-ref": "ISPRS Journal of Photogrammetry and Remote Sensing, 2020, 162:\n  50-62", "doi": "10.1016/j.isprsjprs.2020.02.004", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Point cloud classification plays an important role in a wide range of\nairborne light detection and ranging (LiDAR) applications, such as topographic\nmapping, forest monitoring, power line detection, and road detection. However,\ndue to the sensor noise, high redundancy, incompleteness, and complexity of\nairborne LiDAR systems, point cloud classification is challenging. In this\npaper, we proposed a directionally constrained fully convolutional neural\nnetwork (D-FCN) that can take the original 3D coordinates and LiDAR intensity\nas input; thus, it can directly apply to unstructured 3D point clouds for\nsemantic labeling. Specifically, we first introduce a novel directionally\nconstrained point convolution (D-Conv) module to extract locally representative\nfeatures of 3D point sets from the projected 2D receptive fields. To make full\nuse of the orientation information of neighborhood points, the proposed D-Conv\nmodule performs convolution in an orientation-aware manner by using a\ndirectionally constrained nearest neighborhood search. Then, we designed a\nmultiscale fully convolutional neural network with downsampling and upsampling\nblocks to enable multiscale point feature learning. The proposed D-FCN model\ncan therefore process input point cloud with arbitrary sizes and directly\npredict the semantic labels for all the input points in an end-to-end manner.\nWithout involving additional geometry features as input, the proposed method\nhas demonstrated superior performance on the International Society for\nPhotogrammetry and Remote Sensing (ISPRS) 3D labeling benchmark dataset. The\nresults show that our model has achieved a new state-of-the-art level of\nperformance with an average F1 score of 70.7%, and it has improved the\nperformance by a large margin on categories with a small number of points (such\nas powerline, car, and facade).\n", "versions": [{"version": "v1", "created": "Mon, 19 Aug 2019 09:57:24 GMT"}], "update_date": "2020-04-21", "authors_parsed": [["Wen", "Congcong", ""], ["Yang", "Lina", ""], ["Peng", "Ling", ""], ["Li", "Xiang", ""], ["Chi", "Tianhe", ""]]}, {"id": "1908.06683", "submitter": "Kenneth Lau", "authors": "Kenneth Lau, Jonas Adler, Jens Sj\\\"olund", "title": "A unified representation network for segmentation with missing\n  modalities", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Over the last few years machine learning has demonstrated groundbreaking\nresults in many areas of medical image analysis, including segmentation. A key\nassumption, however, is that the train- and test distributions match. We study\na realistic scenario where this assumption is clearly violated, namely\nsegmentation with missing input modalities. We describe two neural network\napproaches that can handle a variable number of input modalities. The first is\nmodality dropout: a simple but surprisingly effective modification of the\ntraining. The second is the unified representation network: a network\narchitecture that maps a variable number of input modalities into a unified\nrepresentation that can be used for downstream tasks such as segmentation. We\ndemonstrate that modality dropout makes a standard segmentation network\nreasonably robust to missing modalities, but that the same network works even\nbetter if trained on the unified representation.\n", "versions": [{"version": "v1", "created": "Mon, 19 Aug 2019 10:31:59 GMT"}], "update_date": "2019-08-20", "authors_parsed": [["Lau", "Kenneth", ""], ["Adler", "Jonas", ""], ["Sj\u00f6lund", "Jens", ""]]}, {"id": "1908.06692", "submitter": "Dzung Doan Anh", "authors": "Yu Liu, Yutong Dai, Anh-Dzung Doan, Lingqiao Liu, Ian Reid", "title": "In defense of OSVOS", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As a milestone for video object segmentation, one-shot video object\nsegmentation (OSVOS) has achieved a large margin compared to the conventional\noptical-flow based methods regarding to the segmentation accuracy. Its\nexcellent performance mainly benefit from the three-step training mechanism,\nthat are: (1) acquiring object features on the base dataset (i.e. ImageNet),\n(2) training the parent network on the training set of the target dataset (i.e.\nDAVIS-2016) to be capable of differentiating the object of interest from the\nbackground. (3) online fine-tuning the interested object on the first frame of\nthe target test set to overfit its appearance, then the model can be utilized\nto segment the same object in the rest frames of that video. In this paper, we\nargue that for the step (2), OSVOS has the limitation to 'overemphasize' the\ngeneric semantic object information while 'dilute' the instance cues of the\nobject(s), which largely block the whole training process. Through adding a\ncommon module, video loss, which we formulate with various forms of constraints\n(including weighted BCE loss, high-dimensional triplet loss, as well as a novel\nmixed instance-aware video loss), to train the parent network in the step (2),\nthe network is then better prepared for the step (3), i.e. online fine-tuning\non the target instance. Through extensive experiments using different network\nstructures as the backbone, we show that the proposed video loss module can\nimprove the segmentation performance significantly, compared to that of OSVOS.\nMeanwhile, since video loss is a common module, it can be generalized to other\nfine-tuning based methods and similar vision tasks such as depth estimation and\nsaliency detection.\n", "versions": [{"version": "v1", "created": "Mon, 19 Aug 2019 11:07:17 GMT"}, {"version": "v2", "created": "Tue, 20 Aug 2019 01:24:35 GMT"}], "update_date": "2019-08-21", "authors_parsed": [["Liu", "Yu", ""], ["Dai", "Yutong", ""], ["Doan", "Anh-Dzung", ""], ["Liu", "Lingqiao", ""], ["Reid", "Ian", ""]]}, {"id": "1908.06694", "submitter": "Adria Ruiz", "authors": "Adria Ruiz and Jakob Verbeek", "title": "Adaptative Inference Cost With Convolutional Neural Mixture Models", "comments": null, "journal-ref": "International Conference on Computer Vision (ICCV), 2019", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the outstanding performance of convolutional neural networks (CNNs)\nfor many vision tasks, the required computational cost during inference is\nproblematic when resources are limited. In this context, we propose\nConvolutional Neural Mixture Models (CNMMs), a probabilistic model embedding a\nlarge number of CNNs that can be jointly trained and evaluated in an efficient\nmanner. Within the proposed framework, we present different mechanisms to prune\nsubsets of CNNs from the mixture, allowing to easily adapt the computational\ncost required for inference. Image classification and semantic segmentation\nexperiments show that our method achieve excellent accuracy-compute trade-offs.\nMoreover, unlike most of previous approaches, a single CNMM provides a large\nrange of operating points along this trade-off, without any re-training.\n", "versions": [{"version": "v1", "created": "Mon, 19 Aug 2019 11:10:50 GMT"}], "update_date": "2019-08-20", "authors_parsed": [["Ruiz", "Adria", ""], ["Verbeek", "Jakob", ""]]}, {"id": "1908.06702", "submitter": "Jiacheng Chen", "authors": "Jiacheng Chen, Chen Liu, Jiaye Wu, Yasutaka Furukawa", "title": "Floor-SP: Inverse CAD for Floorplans by Sequential Room-wise Shortest\n  Path", "comments": "10 pages, 9 figures, accepted to ICCV 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a new approach for automated floorplan reconstruction\nfrom RGBD scans, a major milestone in indoor mapping research. The approach,\ndubbed Floor-SP, formulates a novel optimization problem, where room-wise\ncoordinate descent sequentially solves dynamic programming to optimize the\nfloorplan graph structure. The objective function consists of data terms guided\nby deep neural networks, consistency terms encouraging adjacent rooms to share\ncorners and walls, and the model complexity term. The approach does not require\ncorner/edge detection with thresholds, unlike most other methods. We have\nevaluated our system on production-quality RGBD scans of 527 apartments or\nhouses, including many units with non-Manhattan structures. Qualitative and\nquantitative evaluations demonstrate a significant performance boost over the\ncurrent state-of-the-art. Please refer to our project website\nhttp://jcchen.me/floor-sp/ for code and data.\n", "versions": [{"version": "v1", "created": "Mon, 19 Aug 2019 11:20:58 GMT"}], "update_date": "2019-08-20", "authors_parsed": [["Chen", "Jiacheng", ""], ["Liu", "Chen", ""], ["Wu", "Jiaye", ""], ["Furukawa", "Yasutaka", ""]]}, {"id": "1908.06726", "submitter": "Volker Willert", "authors": "Volker Willert and Martin Buczko", "title": "Some Aspects of Geometric Computer Vision for Analysing Dynamical Scenes\n  focusing Automotive Applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This draft summarizes some basics about geometric computer vision needed to\nimplement efficient computer vision algorithms for applications that use\nmeasurements from at least one digital camera mounted on a moving platform with\na special focus on automotive applications processing image streams taken from\ncameras mounted on a car. Our intention is twofold: On the one hand, we would\nlike to introduce well-known basic geometric relations in a compact way that\ncan also be found in lecture books about geometric computer vision like [1, 2].\nOn the other hand, we would like to share some experience about subtleties that\nshould be taken into account in order to set up quite simple but robust and\nfast vision algorithms that are able to run in real time. We added a\nconglomeration of literature, we found to be relevant when implementing basic\nalgorithms like optical flow, visual odometry and structure from motion. The\nreader should get some feeling about how the estimates of these algorithms are\ninterrelated, which parts of the algorithms are critical in terms of robustness\nand what kind of additional assumptions can be useful to constrain the solution\nspace of the underlying usually non-convex optimization problems.\n", "versions": [{"version": "v1", "created": "Mon, 19 Aug 2019 12:12:57 GMT"}], "update_date": "2019-08-20", "authors_parsed": [["Willert", "Volker", ""], ["Buczko", "Martin", ""]]}, {"id": "1908.06750", "submitter": "Amir Atapour Abarghouei", "authors": "Amir Atapour-Abarghouei and Stephen Bonner and Andrew Stephen McGough", "title": "A Kings Ransom for Encryption: Ransomware Classification using Augmented\n  One-Shot Learning and Bayesian Approximation", "comments": "Submitted to 2019 IEEE International Conference on Big Data", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Newly emerging variants of ransomware pose an ever-growing threat to computer\nsystems governing every aspect of modern life through the handling and analysis\nof big data. While various recent security-based approaches have focused on\ndetecting and classifying ransomware at the network or system level,\neasy-to-use post-infection ransomware classification for the lay user has not\nbeen attempted before. In this paper, we investigate the possibility of\nclassifying the ransomware a system is infected with simply based on a\nscreenshot of the splash screen or the ransom note captured using a consumer\ncamera commonly found in any modern mobile device. To train and evaluate our\nsystem, we create a sample dataset of the splash screens of 50 well-known\nransomware variants. In our dataset, only a single training image is available\nper ransomware. Instead of creating a large training dataset of ransomware\nscreenshots, we simulate screenshot capture conditions via carefully designed\ndata augmentation techniques, enabling simple and efficient one-shot learning.\nMoreover, using model uncertainty obtained via Bayesian approximation, we\nensure special input cases such as unrelated non-ransomware images and\npreviously-unseen ransomware variants are correctly identified for special\nhandling and not mis-classified. Extensive experimental evaluation demonstrates\nthe efficacy of our work, with accuracy levels of up to 93.6% for ransomware\nclassification.\n", "versions": [{"version": "v1", "created": "Mon, 19 Aug 2019 12:38:38 GMT"}], "update_date": "2019-08-20", "authors_parsed": [["Atapour-Abarghouei", "Amir", ""], ["Bonner", "Stephen", ""], ["McGough", "Andrew Stephen", ""]]}, {"id": "1908.06752", "submitter": "Aakanksha Rana", "authors": "Aakanksha Rana, Cagri Ozcinar, Aljoscha Smolic", "title": "Towards Generating Ambisonics Using Audio-Visual Cue for Virtual Reality", "comments": "ICASSP 2019 - 2019 IEEE International Conference on Acoustics, Speech\n  and Signal Processing (ICASSP)", "journal-ref": null, "doi": "10.1109/ICASSP.2019.8683318", "report-no": null, "categories": "cs.SD cs.CV cs.LG cs.MM eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ambisonics i.e., a full-sphere surround sound, is quintessential with\n360-degree visual content to provide a realistic virtual reality (VR)\nexperience. While 360-degree visual content capture gained a tremendous boost\nrecently, the estimation of corresponding spatial sound is still challenging\ndue to the required sound-field microphones or information about the\nsound-source locations. In this paper, we introduce a novel problem of\ngenerating Ambisonics in 360-degree videos using the audio-visual cue. With\nthis aim, firstly, a novel 360-degree audio-visual video dataset of 265 videos\nis introduced with annotated sound-source locations. Secondly, a pipeline is\ndesigned for an automatic Ambisonic estimation problem. Benefiting from the\ndeep learning-based audio-visual feature-embedding and prediction modules, our\npipeline estimates the 3D sound-source locations and further use such locations\nto encode to the B-format. To benchmark our dataset and pipeline, we\nadditionally propose evaluation criteria to investigate the performance using\ndifferent 360-degree input representations. Our results demonstrate the\nefficacy of the proposed pipeline and open up a new area of research in\n360-degree audio-visual analysis for future investigations.\n", "versions": [{"version": "v1", "created": "Fri, 16 Aug 2019 14:49:30 GMT"}], "update_date": "2019-08-20", "authors_parsed": [["Rana", "Aakanksha", ""], ["Ozcinar", "Cagri", ""], ["Smolic", "Aljoscha", ""]]}, {"id": "1908.06792", "submitter": "Yixing Huang", "authors": "Yixing Huang, Alexander Preuhs, Guenter Lauritsch, Michael Manhart,\n  Xiaolin Huang, and Andreas Maier", "title": "Data Consistent Artifact Reduction for Limited Angle Tomography with\n  Deep Learning Prior", "comments": "Accepted by MICCAI MLMIR workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Robustness of deep learning methods for limited angle tomography is\nchallenged by two major factors: a) due to insufficient training data the\nnetwork may not generalize well to unseen data; b) deep learning methods are\nsensitive to noise. Thus, generating reconstructed images directly from a\nneural network appears inadequate. We propose to constrain the reconstructed\nimages to be consistent with the measured projection data, while the unmeasured\ninformation is complemented by learning based methods. For this purpose, a data\nconsistent artifact reduction (DCAR) method is introduced: First, a prior image\nis generated from an initial limited angle reconstruction via deep learning as\na substitute for missing information. Afterwards, a conventional iterative\nreconstruction algorithm is applied, integrating the data consistency in the\nmeasured angular range and the prior information in the missing angular range.\nThis ensures data integrity in the measured area, while inaccuracies\nincorporated by the deep learning prior lie only in areas where no information\nis acquired. The proposed DCAR method achieves significant image quality\nimprovement: for 120-degree cone-beam limited angle tomography more than 10%\nRMSE reduction in noise-free case and more than 24% RMSE reduction in noisy\ncase compared with a state-of-the-art U-Net based method.\n", "versions": [{"version": "v1", "created": "Mon, 19 Aug 2019 13:28:35 GMT"}, {"version": "v2", "created": "Wed, 28 Aug 2019 08:47:31 GMT"}], "update_date": "2019-08-29", "authors_parsed": [["Huang", "Yixing", ""], ["Preuhs", "Alexander", ""], ["Lauritsch", "Guenter", ""], ["Manhart", "Michael", ""], ["Huang", "Xiaolin", ""], ["Maier", "Andreas", ""]]}, {"id": "1908.06812", "submitter": "Prune Truong", "authors": "Prune Truong, Stefanos Apostolopoulos, Agata Mosinska, Samuel Stucky,\n  Carlos Ciller, Sandro De Zanet", "title": "GLAMpoints: Greedily Learned Accurate Match points", "comments": null, "journal-ref": "In the IEEE International Conference on Computer Vision (ICCV),\n  2019, pp. 10732-10741", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a novel CNN-based feature point detector - GLAMpoints - learned\nin a semi-supervised manner. Our detector extracts repeatable, stable interest\npoints with a dense coverage, specifically designed to maximize the correct\nmatching in a specific domain, which is in contrast to conventional techniques\nthat optimize indirect metrics. In this paper, we apply our method on\nchallenging retinal slitlamp images, for which classical detectors yield\nunsatisfactory results due to low image quality and insufficient amount of\nlow-level features. We show that GLAMpoints significantly outperforms classical\ndetectors as well as state-of-the-art CNN-based methods in matching and\nregistration quality for retinal images. Our method can also be extended to\nother domains, such as natural images. Training code and model weights are\navailable at https://github.com/PruneTruong/GLAMpoints_pytorch.\n", "versions": [{"version": "v1", "created": "Mon, 19 Aug 2019 14:06:27 GMT"}, {"version": "v2", "created": "Thu, 19 Dec 2019 17:59:35 GMT"}, {"version": "v3", "created": "Mon, 15 Jun 2020 17:34:03 GMT"}], "update_date": "2020-06-16", "authors_parsed": [["Truong", "Prune", ""], ["Apostolopoulos", "Stefanos", ""], ["Mosinska", "Agata", ""], ["Stucky", "Samuel", ""], ["Ciller", "Carlos", ""], ["De Zanet", "Sandro", ""]]}, {"id": "1908.06837", "submitter": "Divyanshu Gupta", "authors": "Divyanshu Gupta, Shorya Jain, Utkarsh Tripathi, Pratik Chattopadhyay,\n  Lipo Wang", "title": "Fully Automated Image De-fencing using Conditional Generative\n  Adversarial Networks", "comments": "10 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Image de-fencing is one of the important aspects of recreational photography\nin which the objective is to remove the fence texture present in an image and\ngenerate an aesthetically pleasing version of the same image without the fence\ntexture. In this paper, we aim to develop an automated and effective technique\nfor fence removal and image reconstruction using conditional Generative\nAdversarial Networks (cGANs). These networks have been successfully applied in\nseveral domains of Computer Vision focusing on image generation and rendering.\nOur initial approach is based on a two-stage architecture involving two cGANs\nthat generate the fence mask and the inpainted image, respectively. Training of\nthese networks is carried out independently and, during evaluation, the input\nimage is passed through the two generators in succession to obtain the\nde-fenced image. The results obtained from this approach are satisfactory, but\nthe response time is long since the image has to pass through two sets of\nconvolution layers. To reduce the response time, we propose a second approach\ninvolving only a single cGAN architecture that is trained using the\nground-truth of fenced de-fenced image pairs along with the edge map of the\nfenced image produced by the Canny Filter. Incorporation of the edge map helps\nthe network to precisely detect the edges present in the input image, and also\nimparts it an ability to carry out high quality de-fencing in an efficient\nmanner, even in the presence of a fewer number of layers as compared to the\ntwo-stage network. Qualitative and quantitative experimental results reported\nin the manuscript reveal that the de-fenced images generated by the\nsingle-stage de-fencing network have similar visual quality to those produced\nby the two-stage network. Comparative performance analysis also emphasizes the\neffectiveness of our approach over state-of-the-art image de-fencing\ntechniques.\n", "versions": [{"version": "v1", "created": "Mon, 19 Aug 2019 14:44:24 GMT"}], "update_date": "2019-08-20", "authors_parsed": [["Gupta", "Divyanshu", ""], ["Jain", "Shorya", ""], ["Tripathi", "Utkarsh", ""], ["Chattopadhyay", "Pratik", ""], ["Wang", "Lipo", ""]]}, {"id": "1908.06881", "submitter": "Yaxing Wang", "authors": "Yaxing Wang, Abel Gonzalez-Garcia, Joost van de Weijer, Luis Herranz", "title": "SDIT: Scalable and Diverse Cross-domain Image Translation", "comments": "ACM-MM2019 camera ready", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, image-to-image translation research has witnessed remarkable\nprogress. Although current approaches successfully generate diverse outputs or\nperform scalable image transfer, these properties have not been combined into a\nsingle method. To address this limitation, we propose SDIT: Scalable and\nDiverse image-to-image translation. These properties are combined into a single\ngenerator. The diversity is determined by a latent variable which is randomly\nsampled from a normal distribution. The scalability is obtained by conditioning\nthe network on the domain attributes. Additionally, we also exploit an\nattention mechanism that permits the generator to focus on the domain-specific\nattribute. We empirically demonstrate the performance of the proposed method on\nface mapping and other datasets beyond faces.\n", "versions": [{"version": "v1", "created": "Mon, 19 Aug 2019 15:33:44 GMT"}], "update_date": "2019-08-20", "authors_parsed": [["Wang", "Yaxing", ""], ["Gonzalez-Garcia", "Abel", ""], ["van de Weijer", "Joost", ""], ["Herranz", "Luis", ""]]}, {"id": "1908.06896", "submitter": "Federico Magliani", "authors": "Federico Magliani, Laura Sani, Stefano Cagnoni, Andrea Prati", "title": "Genetic Algorithms for the Optimization of Diffusion Parameters in\n  Content-Based Image Retrieval", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Several computer vision and artificial intelligence projects are nowadays\nexploiting the manifold data distribution using, e.g., the diffusion process.\nThis approach has produced dramatic improvements on the final performance\nthanks to the application of such algorithms to the kNN graph. Unfortunately,\nthis recent technique needs a manual configuration of several parameters, thus\nit is not straightforward to find the best configuration for each dataset.\nMoreover, the brute-force approach is computationally very demanding when used\nto optimally set the parameters of the diffusion approach. We propose to use\ngenetic algorithms to find the optimal setting of all the diffusion parameters\nwith respect to retrieval performance for each different dataset. Our approach\nis faster than others used as references (brute-force, random-search and PSO).\nA comparison with these methods has been made on three public image datasets:\nOxford5k, Paris6k and Oxford105k.\n", "versions": [{"version": "v1", "created": "Mon, 19 Aug 2019 15:54:55 GMT"}], "update_date": "2019-08-20", "authors_parsed": [["Magliani", "Federico", ""], ["Sani", "Laura", ""], ["Cagnoni", "Stefano", ""], ["Prati", "Andrea", ""]]}, {"id": "1908.06903", "submitter": "Bharat Lal Bhatnagar", "authors": "Bharat Lal Bhatnagar, Garvita Tiwari, Christian Theobalt, Gerard\n  Pons-Moll", "title": "Multi-Garment Net: Learning to Dress 3D People from Images", "comments": "International Conference in Computer Vision (ICCV), 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present Multi-Garment Network (MGN), a method to predict body shape and\nclothing, layered on top of the SMPL model from a few frames (1-8) of a video.\nSeveral experiments demonstrate that this representation allows higher level of\ncontrol when compared to single mesh or voxel representations of shape. Our\nmodel allows to predict garment geometry, relate it to the body shape, and\ntransfer it to new body shapes and poses. To train MGN, we leverage a digital\nwardrobe containing 712 digital garments in correspondence, obtained with a\nnovel method to register a set of clothing templates to a dataset of real 3D\nscans of people in different clothing and poses. Garments from the digital\nwardrobe, or predicted by MGN, can be used to dress any body shape in arbitrary\nposes. We will make publicly available the digital wardrobe, the MGN model, and\ncode to dress SMPL with the garments.\n", "versions": [{"version": "v1", "created": "Mon, 19 Aug 2019 16:05:26 GMT"}, {"version": "v2", "created": "Tue, 20 Aug 2019 07:31:46 GMT"}], "update_date": "2019-08-21", "authors_parsed": [["Bhatnagar", "Bharat Lal", ""], ["Tiwari", "Garvita", ""], ["Theobalt", "Christian", ""], ["Pons-Moll", "Gerard", ""]]}, {"id": "1908.06911", "submitter": "Dietmar Saupe", "authors": "Markus Wagner and Hanhe Lin and Shujun Li and Dietmar Saupe", "title": "Algorithm Selection for Image Quality Assessment", "comments": "Presented at the Seventh Workshop on COnfiguration and SElection of\n  ALgorithms (COSEAL), Potsdam, Germany, August 26--27, 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Subjective perceptual image quality can be assessed in lab studies by human\nobservers. Objective image quality assessment (IQA) refers to algorithms for\nestimation of the mean subjective quality ratings. Many such methods have been\nproposed, both for blind IQA in which no original reference image is available\nas well as for the full-reference case. We compared 8 state-of-the-art\nalgorithms for blind IQA and showed that an oracle, able to predict the best\nperforming method for any given input image, yields a hybrid method that could\noutperform even the best single existing method by a large margin. In this\ncontribution we address the research question whether established methods to\nlearn such an oracle can improve blind IQA. We applied AutoFolio, a\nstate-of-the-art system that trains an algorithm selector to choose a\nwell-performing algorithm for a given instance. We also trained deep neural\nnetworks to predict the best method. Our results did not give a positive\nanswer, algorithm selection did not yield a significant improvement over the\nsingle best method. Looking into the results in depth, we observed that the\nnoise in images may have played a role in why our trained classifiers could not\npredict the oracle. This motivates the consideration of noisiness in IQA\nmethods, a property that has so far not been observed and that opens up several\ninteresting new research questions and applications.\n", "versions": [{"version": "v1", "created": "Mon, 19 Aug 2019 16:15:22 GMT"}], "update_date": "2019-08-20", "authors_parsed": [["Wagner", "Markus", ""], ["Lin", "Hanhe", ""], ["Li", "Shujun", ""], ["Saupe", "Dietmar", ""]]}, {"id": "1908.06912", "submitter": "Zongwei Zhou", "authors": "Zongwei Zhou, Vatsal Sodha, Md Mahfuzur Rahman Siddiquee, Ruibin Feng,\n  Nima Tajbakhsh, Michael B. Gotway, Jianming Liang", "title": "Models Genesis: Generic Autodidactic Models for 3D Medical Image\n  Analysis", "comments": "International Conference on Medical Image Computing and Computer\n  Assisted Intervention (MICCAI)", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transfer learning from natural image to medical image has established as one\nof the most practical paradigms in deep learning for medical image analysis.\nHowever, to fit this paradigm, 3D imaging tasks in the most prominent imaging\nmodalities (e.g., CT and MRI) have to be reformulated and solved in 2D, losing\nrich 3D anatomical information and inevitably compromising the performance. To\novercome this limitation, we have built a set of models, called Generic\nAutodidactic Models, nicknamed Models Genesis, because they are created ex\nnihilo (with no manual labeling), self-taught (learned by self-supervision),\nand generic (served as source models for generating application-specific target\nmodels). Our extensive experiments demonstrate that our Models Genesis\nsignificantly outperform learning from scratch in all five target 3D\napplications covering both segmentation and classification. More importantly,\nlearning a model from scratch simply in 3D may not necessarily yield\nperformance better than transfer learning from ImageNet in 2D, but our Models\nGenesis consistently top any 2D approaches including fine-tuning the models\npre-trained from ImageNet as well as fine-tuning the 2D versions of our Models\nGenesis, confirming the importance of 3D anatomical information and\nsignificance of our Models Genesis for 3D medical imaging. This performance is\nattributed to our unified self-supervised learning framework, built on a simple\nyet powerful observation: the sophisticated yet recurrent anatomy in medical\nimages can serve as strong supervision signals for deep models to learn common\nanatomical representation automatically via self-supervision. As open science,\nall pre-trained Models Genesis are available at\nhttps://github.com/MrGiovanni/ModelsGenesis.\n", "versions": [{"version": "v1", "created": "Mon, 19 Aug 2019 16:20:39 GMT"}], "update_date": "2019-08-20", "authors_parsed": [["Zhou", "Zongwei", ""], ["Sodha", "Vatsal", ""], ["Siddiquee", "Md Mahfuzur Rahman", ""], ["Feng", "Ruibin", ""], ["Tajbakhsh", "Nima", ""], ["Gotway", "Michael B.", ""], ["Liang", "Jianming", ""]]}, {"id": "1908.06925", "submitter": "Ricardo Borsoi", "authors": "Ricardo Augusto Borsoi, Tales Imbiriba, Jos\\'e Carlos Moreira\n  Bermudez, C\\'edric Richard", "title": "A Blind Multiscale Spatial Regularization Framework for Kernel-based\n  Spectral Unmixing", "comments": null, "journal-ref": null, "doi": "10.1109/TIP.2020.2978342", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Introducing spatial prior information in hyperspectral imaging (HSI) analysis\nhas led to an overall improvement of the performance of many HSI methods\napplied for denoising, classification, and unmixing. Extending such\nmethodologies to nonlinear settings is not always straightforward, specially\nfor unmixing problems where the consideration of spatial relationships between\nneighboring pixels might comprise intricate interactions between their\nfractional abundances and nonlinear contributions. In this paper, we consider a\nmultiscale regularization strategy for nonlinear spectral unmixing with\nkernels. The proposed methodology splits the unmixing problem into two\nsub-problems at two different spatial scales: a coarse scale containing\nlow-dimensional structures, and the original fine scale. The coarse spatial\ndomain is defined using superpixels that result from a multiscale\ntransformation. Spectral unmixing is then formulated as the solution of\nquadratically constrained optimization problems, which are solved efficiently\nby exploring their strong duality and a reformulation of their dual cost\nfunctions in the form of root-finding problems. Furthermore, we employ a\ntheory-based statistical framework to devise a consistent strategy to estimate\nall required parameters, including both the regularization parameters of the\nalgorithm and the number of superpixels of the transformation, resulting in a\ntruly blind (from the parameters setting perspective) unmixing method.\nExperimental results attest the superior performance of the proposed method\nwhen comparing with other, state-of-the-art, related strategies.\n", "versions": [{"version": "v1", "created": "Mon, 19 Aug 2019 16:52:14 GMT"}, {"version": "v2", "created": "Wed, 11 Dec 2019 21:31:00 GMT"}, {"version": "v3", "created": "Mon, 2 Mar 2020 19:31:46 GMT"}], "update_date": "2020-03-04", "authors_parsed": [["Borsoi", "Ricardo Augusto", ""], ["Imbiriba", "Tales", ""], ["Bermudez", "Jos\u00e9 Carlos Moreira", ""], ["Richard", "C\u00e9dric", ""]]}, {"id": "1908.06933", "submitter": "Ali Hatamizadeh", "authors": "Ali Hatamizadeh, Assaf Hoogi, Debleena Sengupta, Wuyue Lu, Brian\n  Wilcox, Daniel Rubin and Demetri Terzopoulos", "title": "Deep Active Lesion Segmentation", "comments": "Accepted to Machine Learning in Medical Imaging (MLMI 2019). Link to\n  source code added", "journal-ref": "MLMI 2019", "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Lesion segmentation is an important problem in computer-assisted diagnosis\nthat remains challenging due to the prevalence of low contrast, irregular\nboundaries that are unamenable to shape priors. We introduce Deep Active Lesion\nSegmentation (DALS), a fully automated segmentation framework for that\nleverages the powerful nonlinear feature extraction abilities of fully\nConvolutional Neural Networks (CNNs) and the precise boundary delineation\nabilities of Active Contour Models (ACMs). Our DALS framework benefits from an\nimproved level-set ACM formulation with a per-pixel-parameterized energy\nfunctional and a novel multiscale encoder-decoder CNN that learns an\ninitialization probability map along with parameter maps for the ACM. We\nevaluate our lesion segmentation model on a new Multiorgan Lesion Segmentation\n(MLS) dataset that contains images of various organs, including brain, liver,\nand lung, across different imaging modalities---MR and CT. Our results\ndemonstrate favorable performance compared to competing methods, especially for\nsmall training datasets. Source code :\n$\\text{https://github.com/ahatamiz/dals}$\n", "versions": [{"version": "v1", "created": "Mon, 19 Aug 2019 17:12:00 GMT"}, {"version": "v2", "created": "Wed, 21 Aug 2019 18:35:36 GMT"}, {"version": "v3", "created": "Tue, 8 Oct 2019 09:15:48 GMT"}, {"version": "v4", "created": "Sun, 30 Aug 2020 16:31:13 GMT"}], "update_date": "2020-09-01", "authors_parsed": [["Hatamizadeh", "Ali", ""], ["Hoogi", "Assaf", ""], ["Sengupta", "Debleena", ""], ["Lu", "Wuyue", ""], ["Wilcox", "Brian", ""], ["Rubin", "Daniel", ""], ["Terzopoulos", "Demetri", ""]]}, {"id": "1908.06943", "submitter": "Miriam H\\\"agele", "authors": "Miriam H\\\"agele, Philipp Seegerer, Sebastian Lapuschkin, Michael\n  Bockmayr, Wojciech Samek, Frederick Klauschen, Klaus-Robert M\\\"uller,\n  Alexander Binder", "title": "Resolving challenges in deep learning-based analyses of\n  histopathological images using explanation methods", "comments": null, "journal-ref": "Sci Rep 10, 6423 (2020)", "doi": "10.1038/s41598-020-62724-2", "report-no": null, "categories": "eess.IV cs.CV q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning has recently gained popularity in digital pathology due to its\nhigh prediction quality. However, the medical domain requires explanation and\ninsight for a better understanding beyond standard quantitative performance\nevaluation. Recently, explanation methods have emerged, which are so far still\nrarely used in medicine. This work shows their application to generate heatmaps\nthat allow to resolve common challenges encountered in deep learning-based\ndigital histopathology analyses. These challenges comprise biases typically\ninherent to histopathology data. We study binary classification tasks of tumor\ntissue discrimination in publicly available haematoxylin and eosin slides of\nvarious tumor entities and investigate three types of biases: (1) biases which\naffect the entire dataset, (2) biases which are by chance correlated with class\nlabels and (3) sampling biases. While standard analyses focus on patch-level\nevaluation, we advocate pixel-wise heatmaps, which offer a more precise and\nversatile diagnostic instrument and furthermore help to reveal biases in the\ndata. This insight is shown to not only detect but also to be helpful to remove\nthe effects of common hidden biases, which improves generalization within and\nacross datasets. For example, we could see a trend of improved area under the\nreceiver operating characteristic curve by 5% when reducing a labeling bias.\nExplanation techniques are thus demonstrated to be a helpful and highly\nrelevant tool for the development and the deployment phases within the life\ncycle of real-world applications in digital pathology.\n", "versions": [{"version": "v1", "created": "Thu, 15 Aug 2019 15:46:40 GMT"}, {"version": "v2", "created": "Fri, 24 Apr 2020 15:13:00 GMT"}], "update_date": "2020-04-28", "authors_parsed": [["H\u00e4gele", "Miriam", ""], ["Seegerer", "Philipp", ""], ["Lapuschkin", "Sebastian", ""], ["Bockmayr", "Michael", ""], ["Samek", "Wojciech", ""], ["Klauschen", "Frederick", ""], ["M\u00fcller", "Klaus-Robert", ""], ["Binder", "Alexander", ""]]}, {"id": "1908.06954", "submitter": "Lun Huang", "authors": "Lun Huang and Wenmin Wang and Jie Chen and Xiao-Yong Wei", "title": "Attention on Attention for Image Captioning", "comments": "Accepted to ICCV 2019 (Oral)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Attention mechanisms are widely used in current encoder/decoder frameworks of\nimage captioning, where a weighted average on encoded vectors is generated at\neach time step to guide the caption decoding process. However, the decoder has\nlittle idea of whether or how well the attended vector and the given attention\nquery are related, which could make the decoder give misled results. In this\npaper, we propose an Attention on Attention (AoA) module, which extends the\nconventional attention mechanisms to determine the relevance between attention\nresults and queries. AoA first generates an information vector and an attention\ngate using the attention result and the current context, then adds another\nattention by applying element-wise multiplication to them and finally obtains\nthe attended information, the expected useful knowledge. We apply AoA to both\nthe encoder and the decoder of our image captioning model, which we name as AoA\nNetwork (AoANet). Experiments show that AoANet outperforms all previously\npublished methods and achieves a new state-of-the-art performance of 129.8\nCIDEr-D score on MS COCO Karpathy offline test split and 129.6 CIDEr-D (C40)\nscore on the official online testing server. Code is available at\nhttps://github.com/husthuaan/AoANet.\n", "versions": [{"version": "v1", "created": "Mon, 19 Aug 2019 17:44:14 GMT"}, {"version": "v2", "created": "Wed, 21 Aug 2019 17:58:38 GMT"}], "update_date": "2019-08-22", "authors_parsed": [["Huang", "Lun", ""], ["Wang", "Wenmin", ""], ["Chen", "Jie", ""], ["Wei", "Xiao-Yong", ""]]}, {"id": "1908.06955", "submitter": "Li Zhang", "authors": "Li Zhang, Dan Xu, Anurag Arnab, Philip H.S. Torr", "title": "Dynamic Graph Message Passing Networks", "comments": "CVPR 2020 Oral", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modelling long-range dependencies is critical for complex scene understanding\ntasks such as semantic segmentation and object detection. Although CNNs have\nexcelled in many computer vision tasks, they are still limited in capturing\nlong-range structured relationships as they typically consist of layers of\nlocal kernels. A fully-connected graph is beneficial for such modelling,\nhowever, its computational overhead is prohibitive. We propose a dynamic graph\nmessage passing network, based on the message passing neural network framework,\nthat significantly reduces the computational complexity compared to related\nworks modelling a fully-connected graph. This is achieved by adaptively\nsampling nodes in the graph, conditioned on the input, for message passing.\nBased on the sampled nodes, we then dynamically predict node-dependent filter\nweights and the affinity matrix for propagating information between them. Using\nthis model, we show significant improvements with respect to strong,\nstate-of-the-art baselines on three different tasks and backbone architectures.\nOur approach also outperforms fully-connected graphs while using substantially\nfewer floating point operations and parameters.\n", "versions": [{"version": "v1", "created": "Mon, 19 Aug 2019 17:46:34 GMT"}, {"version": "v2", "created": "Wed, 1 Apr 2020 13:37:21 GMT"}, {"version": "v3", "created": "Sun, 14 Jun 2020 10:35:58 GMT"}], "update_date": "2020-06-16", "authors_parsed": [["Zhang", "Li", ""], ["Xu", "Dan", ""], ["Arnab", "Anurag", ""], ["Torr", "Philip H. S.", ""]]}, {"id": "1908.06963", "submitter": "Mohamed Hassan", "authors": "Mohamed Hassan, Vasileios Choutas, Dimitrios Tzionas and Michael J.\n  Black", "title": "Resolving 3D Human Pose Ambiguities with 3D Scene Constraints", "comments": "To appear in ICCV 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To understand and analyze human behavior, we need to capture humans moving\nin, and interacting with, the world. Most existing methods perform 3D human\npose estimation without explicitly considering the scene. We observe however\nthat the world constrains the body and vice-versa. To motivate this, we show\nthat current 3D human pose estimation methods produce results that are not\nconsistent with the 3D scene. Our key contribution is to exploit static 3D\nscene structure to better estimate human pose from monocular images. The method\nenforces Proximal Relationships with Object eXclusion and is called PROX. To\ntest this, we collect a new dataset composed of 12 different 3D scenes and RGB\nsequences of 20 subjects moving in and interacting with the scenes. We\nrepresent human pose using the 3D human body model SMPL-X and extend SMPLify-X\nto estimate body pose using scene constraints. We make use of the 3D scene\ninformation by formulating two main constraints. The inter-penetration\nconstraint penalizes intersection between the body model and the surrounding 3D\nscene. The contact constraint encourages specific parts of the body to be in\ncontact with scene surfaces if they are close enough in distance and\norientation. For quantitative evaluation we capture a separate dataset with 180\nRGB frames in which the ground-truth body pose is estimated using a motion\ncapture system. We show quantitatively that introducing scene constraints\nsignificantly reduces 3D joint error and vertex error. Our code and data are\navailable for research at https://prox.is.tue.mpg.de.\n", "versions": [{"version": "v1", "created": "Tue, 20 Aug 2019 17:23:00 GMT"}], "update_date": "2019-08-21", "authors_parsed": [["Hassan", "Mohamed", ""], ["Choutas", "Vasileios", ""], ["Tzionas", "Dimitrios", ""], ["Black", "Michael J.", ""]]}, {"id": "1908.06965", "submitter": "Md Mahfuzur Rahman Siddiquee", "authors": "Md Mahfuzur Rahman Siddiquee, Zongwei Zhou, Nima Tajbakhsh, Ruibin\n  Feng, Michael B. Gotway, Yoshua Bengio, and Jianming Liang", "title": "Learning Fixed Points in Generative Adversarial Networks: From\n  Image-to-Image Translation to Disease Detection and Localization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative adversarial networks (GANs) have ushered in a revolution in\nimage-to-image translation. The development and proliferation of GANs raises an\ninteresting question: can we train a GAN to remove an object, if present, from\nan image while otherwise preserving the image? Specifically, can a GAN\n\"virtually heal\" anyone by turning his medical image, with an unknown health\nstatus (diseased or healthy), into a healthy one, so that diseased regions\ncould be revealed by subtracting those two images? Such a task requires a GAN\nto identify a minimal subset of target pixels for domain translation, an\nability that we call fixed-point translation, which no GAN is equipped with\nyet. Therefore, we propose a new GAN, called Fixed-Point GAN, trained by (1)\nsupervising same-domain translation through a conditional identity loss, and\n(2) regularizing cross-domain translation through revised adversarial, domain\nclassification, and cycle consistency loss. Based on fixed-point translation,\nwe further derive a novel framework for disease detection and localization\nusing only image-level annotation. Qualitative and quantitative evaluations\ndemonstrate that the proposed method outperforms the state of the art in\nmulti-domain image-to-image translation and that it surpasses predominant\nweakly-supervised localization methods in both disease detection and\nlocalization. Implementation is available at\nhttps://github.com/jlianglab/Fixed-Point-GAN.\n", "versions": [{"version": "v1", "created": "Fri, 16 Aug 2019 19:59:01 GMT"}, {"version": "v2", "created": "Thu, 29 Aug 2019 16:24:54 GMT"}], "update_date": "2019-08-30", "authors_parsed": [["Siddiquee", "Md Mahfuzur Rahman", ""], ["Zhou", "Zongwei", ""], ["Tajbakhsh", "Nima", ""], ["Feng", "Ruibin", ""], ["Gotway", "Michael B.", ""], ["Bengio", "Yoshua", ""], ["Liang", "Jianming", ""]]}, {"id": "1908.06989", "submitter": "Manuel Dahnert", "authors": "Manuel Dahnert, Angela Dai, Leonidas Guibas, Matthias Nie{\\ss}ner", "title": "Joint Embedding of 3D Scan and CAD Objects", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  3D scan geometry and CAD models often contain complementary information\ntowards understanding environments, which could be leveraged through\nestablishing a mapping between the two domains. However, this is a challenging\ntask due to strong, lower-level differences between scan and CAD geometry. We\npropose a novel approach to learn a joint embedding space between scan and CAD\ngeometry, where semantically similar objects from both domains lie close\ntogether. To achieve this, we introduce a new 3D CNN-based approach to learn a\njoint embedding space representing object similarities across these domains. To\nlearn a shared space where scan objects and CAD models can interlace, we\npropose a stacked hourglass approach to separate foreground and background from\na scan object, and transform it to a complete, CAD-like representation to\nproduce a shared embedding space. This embedding space can then be used for CAD\nmodel retrieval; to further enable this task, we introduce a new dataset of\nranked scan-CAD similarity annotations, enabling new, fine-grained evaluation\nof CAD model retrieval to cluttered, noisy, partial scans. Our learned joint\nembedding outperforms current state of the art for CAD model retrieval by 12%\nin instance retrieval accuracy.\n", "versions": [{"version": "v1", "created": "Mon, 19 Aug 2019 18:00:03 GMT"}], "update_date": "2019-08-21", "authors_parsed": [["Dahnert", "Manuel", ""], ["Dai", "Angela", ""], ["Guibas", "Leonidas", ""], ["Nie\u00dfner", "Matthias", ""]]}, {"id": "1908.07007", "submitter": "Aaron Maschinot", "authors": "Piotr Teterwak, Aaron Sarna, Dilip Krishnan, Aaron Maschinot, David\n  Belanger, Ce Liu, William T. Freeman", "title": "Boundless: Generative Adversarial Networks for Image Extension", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Image extension models have broad applications in image editing,\ncomputational photography and computer graphics. While image inpainting has\nbeen extensively studied in the literature, it is challenging to directly apply\nthe state-of-the-art inpainting methods to image extension as they tend to\ngenerate blurry or repetitive pixels with inconsistent semantics. We introduce\nsemantic conditioning to the discriminator of a generative adversarial network\n(GAN), and achieve strong results on image extension with coherent semantics\nand visually pleasing colors and textures. We also show promising results in\nextreme extensions, such as panorama generation.\n", "versions": [{"version": "v1", "created": "Mon, 19 Aug 2019 18:15:35 GMT"}], "update_date": "2019-08-21", "authors_parsed": [["Teterwak", "Piotr", ""], ["Sarna", "Aaron", ""], ["Krishnan", "Dilip", ""], ["Maschinot", "Aaron", ""], ["Belanger", "David", ""], ["Liu", "Ce", ""], ["Freeman", "William T.", ""]]}, {"id": "1908.07070", "submitter": "Wenqi Xian", "authors": "Wenqi Xian, Zhengqi Li, Matthew Fisher, Jonathan Eisenmann, Eli\n  Shechtman, Noah Snavely", "title": "UprightNet: Geometry-Aware Camera Orientation Estimation from Single\n  Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce UprightNet, a learning-based approach for estimating 2DoF camera\norientation from a single RGB image of an indoor scene. Unlike recent methods\nthat leverage deep learning to perform black-box regression from image to\norientation parameters, we propose an end-to-end framework that incorporates\nexplicit geometric reasoning. In particular, we design a network that predicts\ntwo representations of scene geometry, in both the local camera and global\nreference coordinate systems, and solves for the camera orientation as the\nrotation that best aligns these two predictions via a differentiable least\nsquares module. This network can be trained end-to-end, and can be supervised\nwith both ground truth camera poses and intermediate representations of surface\ngeometry. We evaluate UprightNet on the single-image camera orientation task on\nsynthetic and real datasets, and show significant improvements over prior\nstate-of-the-art approaches.\n", "versions": [{"version": "v1", "created": "Mon, 19 Aug 2019 21:07:16 GMT"}], "update_date": "2019-08-21", "authors_parsed": [["Xian", "Wenqi", ""], ["Li", "Zhengqi", ""], ["Fisher", "Matthew", ""], ["Eisenmann", "Jonathan", ""], ["Shechtman", "Eli", ""], ["Snavely", "Noah", ""]]}, {"id": "1908.07085", "submitter": "Ehsan Nezhadarya", "authors": "Ehsan Nezhadarya, Yang Liu and Bingbing Liu", "title": "BoxNet: A Deep Learning Method for 2D Bounding Box Estimation from\n  Bird's-Eye View Point Cloud", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a learning-based method to estimate the object bounding box from\nits 2D bird's-eye view (BEV) LiDAR points. Our method, entitled BoxNet,\nexploits a simple deep neural network that can efficiently handle unordered\npoints. The method takes as input the 2D coordinates of all the points and the\noutput is a vector consisting of both the box pose (position and orientation in\nLiDAR coordinate system) and its size (width and length). In order to deal with\nthe angle discontinuity problem, we propose to estimate the double-angle\nsinusoidal values rather than the angle itself. We also predict the center\nrelative to the point cloud mean to boost the performance of estimating the\nlocation of the box. The proposed method does not rely on the ordering of\npoints as in many existing approaches, and can accurately predict the actual\nsize of the bounding box based on the prior information that is obtained from\nthe training data. BoxNet is validated using the KITTI 3D object dataset, with\nsignificant improvement compared with the state-of-the-art non-learning based\nmethods\n", "versions": [{"version": "v1", "created": "Mon, 19 Aug 2019 21:50:41 GMT"}], "update_date": "2019-08-21", "authors_parsed": [["Nezhadarya", "Ehsan", ""], ["Liu", "Yang", ""], ["Liu", "Bingbing", ""]]}, {"id": "1908.07086", "submitter": "Joshua Peterson", "authors": "Joshua C. Peterson, Ruairidh M. Battleday, Thomas L. Griffiths, Olga\n  Russakovsky", "title": "Human uncertainty makes classification more robust", "comments": "In Proceedings of the 2019 IEEE International Conference on Computer\n  Vision (ICCV)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The classification performance of deep neural networks has begun to asymptote\nat near-perfect levels. However, their ability to generalize outside the\ntraining set and their robustness to adversarial attacks have not. In this\npaper, we make progress on this problem by training with full label\ndistributions that reflect human perceptual uncertainty. We first present a new\nbenchmark dataset which we call CIFAR10H, containing a full distribution of\nhuman labels for each image of the CIFAR10 test set. We then show that, while\ncontemporary classifiers fail to exhibit human-like uncertainty on their own,\nexplicit training on our dataset closes this gap, supports improved\ngeneralization to increasingly out-of-training-distribution test datasets, and\nconfers robustness to adversarial attacks.\n", "versions": [{"version": "v1", "created": "Mon, 19 Aug 2019 22:08:22 GMT"}], "update_date": "2019-08-21", "authors_parsed": [["Peterson", "Joshua C.", ""], ["Battleday", "Ruairidh M.", ""], ["Griffiths", "Thomas L.", ""], ["Russakovsky", "Olga", ""]]}, {"id": "1908.07094", "submitter": "Yale Song", "authors": "Shuang Ma, Daniel McDuff, Yale Song", "title": "Unpaired Image-to-Speech Synthesis with Multimodal Information\n  Bottleneck", "comments": "ICCV 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep generative models have led to significant advances in cross-modal\ngeneration such as text-to-image synthesis. Training these models typically\nrequires paired data with direct correspondence between modalities. We\nintroduce the novel problem of translating instances from one modality to\nanother without paired data by leveraging an intermediate modality shared by\nthe two other modalities. To demonstrate this, we take the problem of\ntranslating images to speech. In this case, one could leverage disjoint\ndatasets with one shared modality, e.g., image-text pairs and text-speech\npairs, with text as the shared modality. We call this problem \"skip-modal\ngeneration\" because the shared modality is skipped during the generation\nprocess. We propose a multimodal information bottleneck approach that learns\nthe correspondence between modalities from unpaired data (image and speech) by\nleveraging the shared modality (text). We address fundamental challenges of\nskip-modal generation: 1) learning multimodal representations using a single\nmodel, 2) bridging the domain gap between two unrelated datasets, and 3)\nlearning the correspondence between modalities from unpaired data. We show\nqualitative results on image-to-speech synthesis; this is the first time such\nresults have been reported in the literature. We also show that our approach\nimproves performance on traditional cross-modal generation, suggesting that it\nimproves data efficiency in solving individual tasks.\n", "versions": [{"version": "v1", "created": "Mon, 19 Aug 2019 22:52:59 GMT"}], "update_date": "2019-08-21", "authors_parsed": [["Ma", "Shuang", ""], ["McDuff", "Daniel", ""], ["Song", "Yale", ""]]}, {"id": "1908.07101", "submitter": "Alexander Chang", "authors": "Alexander H. Chang (1), Shiyu Feng (1), Yipu Zhao (1), Justin S. Smith\n  (1) and Patricio A. Vela (1) ((1) Georgia Institute of Technology)", "title": "Autonomous, Monocular, Vision-Based Snake Robot Navigation and Traversal\n  of Cluttered Environments using Rectilinear Gait Motion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Rectilinear forms of snake-like robotic locomotion are anticipated to be an\nadvantage in obstacle-strewn scenarios characterizing urban disaster zones,\nsubterranean collapses, and other natural environments. The elongated,\nlaterally-narrow footprint associated with these motion strategies is\nwell-suited to traversal of confined spaces and narrow pathways. Navigation and\npath planning in the absence of global sensing, however, remains a pivotal\nchallenge to be addressed prior to practical deployment of these robotic\nmechanisms. Several challenges related to visual processing and localization\nneed to be resolved to to enable navigation. As a first pass in this direction,\nwe equip a wireless, monocular color camera to the head of a robotic snake.\nVisiual odometry and mapping from ORB-SLAM permits self-localization in planar,\nobstacle-strewn environments. Ground plane traversability segmentation in\nconjunction with perception-space collision detection permits path planning for\nnavigation. A previously presented dynamical reduction of rectilinear snake\nlocomotion to a non-holonomic kinematic vehicle informs both SLAM and planning.\nThe simplified motion model is then applied to track planned trajectories\nthrough an obstacle configuration. This navigational framework enables a\nsnake-like robotic platform to autonomously navigate and traverse unknown\nscenarios with only monocular vision.\n", "versions": [{"version": "v1", "created": "Mon, 19 Aug 2019 23:06:42 GMT"}], "update_date": "2019-08-21", "authors_parsed": [["Chang", "Alexander H.", "", "Georgia Institute of Technology"], ["Feng", "Shiyu", "", "Georgia Institute of Technology"], ["Zhao", "Yipu", "", "Georgia Institute of Technology"], ["Smith", "Justin S.", "", "Georgia Institute of Technology"], ["Vela", "Patricio A.", "", "Georgia Institute of Technology"]]}, {"id": "1908.07116", "submitter": "Xiao Wang", "authors": "Xiao Wang, Siyue Wang, Pin-Yu Chen, Yanzhi Wang, Brian Kulis, Xue Lin\n  and Peter Chin", "title": "Protecting Neural Networks with Hierarchical Random Switching: Towards\n  Better Robustness-Accuracy Trade-off for Stochastic Defenses", "comments": "Published as Conference Paper @ IJCAI 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite achieving remarkable success in various domains, recent studies have\nuncovered the vulnerability of deep neural networks to adversarial\nperturbations, creating concerns on model generalizability and new threats such\nas prediction-evasive misclassification or stealthy reprogramming. Among\ndifferent defense proposals, stochastic network defenses such as random neuron\nactivation pruning or random perturbation to layer inputs are shown to be\npromising for attack mitigation. However, one critical drawback of current\ndefenses is that the robustness enhancement is at the cost of noticeable\nperformance degradation on legitimate data, e.g., large drop in test accuracy.\nThis paper is motivated by pursuing for a better trade-off between adversarial\nrobustness and test accuracy for stochastic network defenses. We propose\nDefense Efficiency Score (DES), a comprehensive metric that measures the gain\nin unsuccessful attack attempts at the cost of drop in test accuracy of any\ndefense. To achieve a better DES, we propose hierarchical random switching\n(HRS), which protects neural networks through a novel randomization scheme. A\nHRS-protected model contains several blocks of randomly switching channels to\nprevent adversaries from exploiting fixed model structures and parameters for\ntheir malicious purposes. Extensive experiments show that HRS is superior in\ndefending against state-of-the-art white-box and adaptive adversarial\nmisclassification attacks. We also demonstrate the effectiveness of HRS in\ndefending adversarial reprogramming, which is the first defense against\nadversarial programs. Moreover, in most settings the average DES of HRS is at\nleast 5X higher than current stochastic network defenses, validating its\nsignificantly improved robustness-accuracy trade-off.\n", "versions": [{"version": "v1", "created": "Tue, 20 Aug 2019 00:29:23 GMT"}], "update_date": "2019-08-21", "authors_parsed": [["Wang", "Xiao", ""], ["Wang", "Siyue", ""], ["Chen", "Pin-Yu", ""], ["Wang", "Yanzhi", ""], ["Kulis", "Brian", ""], ["Lin", "Xue", ""], ["Chin", "Peter", ""]]}, {"id": "1908.07117", "submitter": "Verica Lazova", "authors": "Verica Lazova, Eldar Insafutdinov, Gerard Pons-Moll", "title": "360-Degree Textures of People in Clothing from a Single Image", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we predict a full 3D avatar of a person from a single image. We\ninfer texture and geometry in the UV-space of the SMPL model using an\nimage-to-image translation method. Given partial texture and segmentation\nlayout maps derived from the input view, our model predicts the complete\nsegmentation map, the complete texture map, and a displacement map. The\npredicted maps can be applied to the SMPL model in order to naturally\ngeneralize to novel poses, shapes, and even new clothing. In order to learn our\nmodel in a common UV-space, we non-rigidly register the SMPL model to thousands\nof 3D scans, effectively encoding textures and geometries as images in\ncorrespondence. This turns a difficult 3D inference task into a simpler\nimage-to-image translation one. Results on rendered scans of people and images\nfrom the DeepFashion dataset demonstrate that our method can reconstruct\nplausible 3D avatars from a single image. We further use our model to digitally\nchange pose, shape, swap garments between people and edit clothing. To\nencourage research in this direction we will make the source code available for\nresearch purpose.\n", "versions": [{"version": "v1", "created": "Tue, 20 Aug 2019 00:42:40 GMT"}], "update_date": "2019-08-21", "authors_parsed": [["Lazova", "Verica", ""], ["Insafutdinov", "Eldar", ""], ["Pons-Moll", "Gerard", ""]]}, {"id": "1908.07121", "submitter": "Chengchao Shen", "authors": "Chengchao Shen, Mengqi Xue, Xinchao Wang, Jie Song, Li Sun, Mingli\n  Song", "title": "Customizing Student Networks From Heterogeneous Teachers via Adaptive\n  Knowledge Amalgamation", "comments": "Accepted to ICCV 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A massive number of well-trained deep networks have been released by\ndevelopers online. These networks may focus on different tasks and in many\ncases are optimized for different datasets. In this paper, we study how to\nexploit such heterogeneous pre-trained networks, known as teachers, so as to\ntrain a customized student network that tackles a set of selective tasks\ndefined by the user. We assume no human annotations are available, and each\nteacher may be either single- or multi-task. To this end, we introduce a\ndual-step strategy that first extracts the task-specific knowledge from the\nheterogeneous teachers sharing the same sub-task, and then amalgamates the\nextracted knowledge to build the student network. To facilitate the training,\nwe employ a selective learning scheme where, for each unlabelled sample, the\nstudent learns adaptively from only the teacher with the least prediction\nambiguity. We evaluate the proposed approach on several datasets and\nexperimental results demonstrate that the student, learned by such adaptive\nknowledge amalgamation, achieves performances even better than those of the\nteachers.\n", "versions": [{"version": "v1", "created": "Tue, 20 Aug 2019 01:13:26 GMT"}], "update_date": "2019-08-21", "authors_parsed": [["Shen", "Chengchao", ""], ["Xue", "Mengqi", ""], ["Wang", "Xinchao", ""], ["Song", "Jie", ""], ["Sun", "Li", ""], ["Song", "Mingli", ""]]}, {"id": "1908.07129", "submitter": "Arka Sadhu", "authors": "Arka Sadhu, Kan Chen, Ram Nevatia", "title": "Zero-Shot Grounding of Objects from Natural Language Queries", "comments": "ICCV19 oral, camera-ready version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A phrase grounding system localizes a particular object in an image referred\nto by a natural language query. In previous work, the phrases were restricted\nto have nouns that were encountered in training, we extend the task to\nZero-Shot Grounding(ZSG) which can include novel, \"unseen\" nouns. Current\nphrase grounding systems use an explicit object detection network in a 2-stage\nframework where one stage generates sparse proposals and the other stage\nevaluates them. In the ZSG setting, generating appropriate proposals itself\nbecomes an obstacle as the proposal generator is trained on the entities common\nin the detection and grounding datasets. We propose a new single-stage model\ncalled ZSGNet which combines the detector network and the grounding system and\npredicts classification scores and regression parameters. Evaluation of ZSG\nsystem brings additional subtleties due to the influence of the relationship\nbetween the query and learned categories; we define four distinct conditions\nthat incorporate different levels of difficulty. We also introduce new\ndatasets, sub-sampled from Flickr30k Entities and Visual Genome, that enable\nevaluations for the four conditions. Our experiments show that ZSGNet achieves\nstate-of-the-art performance on Flickr30k and ReferIt under the usual \"seen\"\nsettings and performs significantly better than baseline in the zero-shot\nsetting.\n", "versions": [{"version": "v1", "created": "Tue, 20 Aug 2019 02:07:14 GMT"}], "update_date": "2019-08-21", "authors_parsed": [["Sadhu", "Arka", ""], ["Chen", "Kan", ""], ["Nevatia", "Ram", ""]]}, {"id": "1908.07135", "submitter": "Hongyuan Yu", "authors": "Hongyuan Yu, Chengquan Zhang, Xuan Li, Junyu Han, Errui Ding, Liang\n  Wang", "title": "An End-to-end Video Text Detector with Online Tracking", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video text detection is considered as one of the most difficult tasks in\ndocument analysis due to the following two challenges: 1) the difficulties\ncaused by video scenes, i.e., motion blur, illumination changes, and occlusion;\n2) the properties of text including variants of fonts, languages, orientations,\nand shapes. Most existing methods attempt to enhance the performance of video\ntext detection by cooperating with video text tracking, but treat these two\ntasks separately. In this work, we propose an end-to-end video text detection\nmodel with online tracking to address these two challenges. Specifically, in\nthe detection branch, we adopt ConvLSTM to capture spatial structure\ninformation and motion memory. In the tracking branch, we convert the tracking\nproblem to text instance association, and an appearance-geometry descriptor\nwith memory mechanism is proposed to generate robust representation of text\ninstances. By integrating these two branches into one trainable framework, they\ncan promote each other and the computational cost is significantly reduced.\nExperiments on existing video text benchmarks including ICDAR2013 Video,\nMinetto and YVT demonstrate that the proposed method significantly outperforms\nstate-of-the-art methods. Our method improves F-score by about 2 on all\ndatasets and it can run realtime with 24.36 fps on TITAN Xp.\n", "versions": [{"version": "v1", "created": "Tue, 20 Aug 2019 02:53:29 GMT"}], "update_date": "2019-08-21", "authors_parsed": [["Yu", "Hongyuan", ""], ["Zhang", "Chengquan", ""], ["Li", "Xuan", ""], ["Han", "Junyu", ""], ["Ding", "Errui", ""], ["Wang", "Liang", ""]]}, {"id": "1908.07144", "submitter": "Anhong Guo", "authors": "Anhong Guo, Junhan Kong, Michael Rivera, Frank F. Xu, Jeffrey P.\n  Bigham", "title": "StateLens: A Reverse Engineering Solution for Making Existing Dynamic\n  Touchscreens Accessible", "comments": "ACM UIST 2019", "journal-ref": null, "doi": "10.1145/3332165.3347873", "report-no": null, "categories": "cs.HC cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Blind people frequently encounter inaccessible dynamic touchscreens in their\neveryday lives that are difficult, frustrating, and often impossible to use\nindependently. Touchscreens are often the only way to control everything from\ncoffee machines and payment terminals, to subway ticket machines and in-flight\nentertainment systems. Interacting with dynamic touchscreens is difficult\nnon-visually because the visual user interfaces change, interactions often\noccur over multiple different screens, and it is easy to accidentally trigger\ninterface actions while exploring the screen. To solve these problems, we\nintroduce StateLens - a three-part reverse engineering solution that makes\nexisting dynamic touchscreens accessible. First, StateLens reverse engineers\nthe underlying state diagrams of existing interfaces using point-of-view videos\nfound online or taken by users using a hybrid crowd-computer vision pipeline.\nSecond, using the state diagrams, StateLens automatically generates\nconversational agents to guide blind users through specifying the tasks that\nthe interface can perform, allowing the StateLens iOS application to provide\ninteractive guidance and feedback so that blind users can access the interface.\nFinally, a set of 3D-printed accessories enable blind people to explore\ncapacitive touchscreens without the risk of triggering accidental touches on\nthe interface. Our technical evaluation shows that StateLens can accurately\nreconstruct interfaces from stationary, hand-held, and web videos; and, a user\nstudy of the complete system demonstrates that StateLens successfully enables\nblind users to access otherwise inaccessible dynamic touchscreens.\n", "versions": [{"version": "v1", "created": "Tue, 20 Aug 2019 03:23:35 GMT"}], "update_date": "2019-08-21", "authors_parsed": [["Guo", "Anhong", ""], ["Kong", "Junhan", ""], ["Rivera", "Michael", ""], ["Xu", "Frank F.", ""], ["Bigham", "Jeffrey P.", ""]]}, {"id": "1908.07170", "submitter": "Maayan Frid-Adar", "authors": "Maayan Frid-Adar, Rula Amer and Hayit Greenspan", "title": "Endotracheal Tube Detection and Segmentation in Chest Radiographs using\n  Synthetic Data", "comments": "Accepted to MICCAI 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Chest radiographs are frequently used to verify the correct intubation of\npatients in the emergency room. Fast and accurate identification and\nlocalization of the endotracheal (ET) tube is critical for the patient. In this\nstudy we propose a novel automated deep learning scheme for accurate detection\nand segmentation of the ET tubes. Development of automatic systems using deep\nlearning networks for classification and segmentation require large annotated\ndata which is not always available. Here we present an approach for\nsynthesizing ET tubes in real X-ray images. We suggest a method for training\nthe network, first with synthetic data and then with real X-ray images in a\nfine-tuning phase, which allows the network to train on thousands of cases\nwithout annotating any data. The proposed method was tested on 477 real chest\nradiographs from a public dataset and reached AUC of 0.99 in classifying the\npresence vs. absence of the ET tube, along with outputting high quality ET tube\nsegmentation maps.\n", "versions": [{"version": "v1", "created": "Tue, 20 Aug 2019 05:32:22 GMT"}], "update_date": "2019-08-21", "authors_parsed": [["Frid-Adar", "Maayan", ""], ["Amer", "Rula", ""], ["Greenspan", "Hayit", ""]]}, {"id": "1908.07172", "submitter": "Yu Sun", "authors": "Sun Yu, Ye Yun, Liu Wu, Gao Wenpeng, Fu YiLi, Mei Tao", "title": "Human Mesh Recovery from Monocular Images via a Skeleton-disentangled\n  Representation", "comments": "Accepted by ICCV2019. The code is released at\n  https://github.com/Arthur151/DSD-SATN", "journal-ref": "IEEE International Conference on Computer Vision, ICCV, 2019", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe an end-to-end method for recovering 3D human body mesh from\nsingle images and monocular videos. Different from the existing methods try to\nobtain all the complex 3D pose, shape, and camera parameters from one coupling\nfeature, we propose a skeleton-disentangling based framework, which divides\nthis task into multi-level spatial and temporal granularity in a decoupling\nmanner. In spatial, we propose an effective and pluggable \"disentangling the\nskeleton from the details\" (DSD) module. It reduces the complexity and\ndecouples the skeleton, which lays a good foundation for temporal modeling. In\ntemporal, the self-attention based temporal convolution network is proposed to\nefficiently exploit the short and long-term temporal cues. Furthermore, an\nunsupervised adversarial training strategy, temporal shuffles and order\nrecovery, is designed to promote the learning of motion dynamics. The proposed\nmethod outperforms the state-of-the-art 3D human mesh recovery methods by 15.4%\nMPJPE and 23.8% PA-MPJPE on Human3.6M. State-of-the-art results are also\nachieved on the 3D pose in the wild (3DPW) dataset without any fine-tuning.\nEspecially, ablation studies demonstrate that skeleton-disentangled\nrepresentation is crucial for better temporal modeling and generalization.\n", "versions": [{"version": "v1", "created": "Tue, 20 Aug 2019 05:35:58 GMT"}, {"version": "v2", "created": "Tue, 17 Sep 2019 10:35:56 GMT"}], "update_date": "2019-09-18", "authors_parsed": [["Yu", "Sun", ""], ["Yun", "Ye", ""], ["Wu", "Liu", ""], ["Wenpeng", "Gao", ""], ["YiLi", "Fu", ""], ["Tao", "Mei", ""]]}, {"id": "1908.07191", "submitter": "Shengju Qian", "authors": "Shengju Qian, Kwan-Yee Lin, Wayne Wu, Yangxiaokang Liu, Quan Wang,\n  Fumin Shen, Chen Qian, Ran He", "title": "Make a Face: Towards Arbitrary High Fidelity Face Manipulation", "comments": "Accepted to ICCV 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent studies have shown remarkable success in face manipulation task with\nthe advance of GANs and VAEs paradigms, but the outputs are sometimes limited\nto low-resolution and lack of diversity.\n  In this work, we propose Additive Focal Variational Auto-encoder (AF-VAE), a\nnovel approach that can arbitrarily manipulate high-resolution face images\nusing a simple yet effective model and only weak supervision of reconstruction\nand KL divergence losses. First, a novel additive Gaussian Mixture assumption\nis introduced with an unsupervised clustering mechanism in the structural\nlatent space, which endows better disentanglement and boosts multi-modal\nrepresentation with external memory. Second, to improve the perceptual quality\nof synthesized results, two simple strategies in architecture design are\nfurther tailored and discussed on the behavior of Human Visual System (HVS) for\nthe first time, allowing for fine control over the model complexity and sample\nquality. Human opinion studies and new state-of-the-art Inception Score (IS) /\nFrechet Inception Distance (FID) demonstrate the superiority of our approach\nover existing algorithms, advancing both the fidelity and extremity of face\nmanipulation task.\n", "versions": [{"version": "v1", "created": "Tue, 20 Aug 2019 06:53:55 GMT"}], "update_date": "2019-08-21", "authors_parsed": [["Qian", "Shengju", ""], ["Lin", "Kwan-Yee", ""], ["Wu", "Wayne", ""], ["Liu", "Yangxiaokang", ""], ["Wang", "Quan", ""], ["Shen", "Fumin", ""], ["Qian", "Chen", ""], ["He", "Ran", ""]]}, {"id": "1908.07201", "submitter": "Silvia Zuffi", "authors": "Silvia Zuffi, Angjoo Kanazawa, Tanya Berger-Wolf, Michael J. Black", "title": "Three-D Safari: Learning to Estimate Zebra Pose, Shape, and Texture from\n  Images \"In the Wild\"", "comments": "Accepted at ICCV 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the first method to perform automatic 3D pose, shape and texture\ncapture of animals from images acquired in-the-wild. In particular, we focus on\nthe problem of capturing 3D information about Grevy's zebras from a collection\nof images. The Grevy's zebra is one of the most endangered species in Africa,\nwith only a few thousand individuals left. Capturing the shape and pose of\nthese animals can provide biologists and conservationists with information\nabout animal health and behavior. In contrast to research on human pose, shape\nand texture estimation, training data for endangered species is limited, the\nanimals are in complex natural scenes with occlusion, they are naturally\ncamouflaged, travel in herds, and look similar to each other. To overcome these\nchallenges, we integrate the recent SMAL animal model into a network-based\nregression pipeline, which we train end-to-end on synthetically generated\nimages with pose, shape, and background variation. Going beyond\nstate-of-the-art methods for human shape and pose estimation, our method learns\na shape space for zebras during training. Learning such a shape space from\nimages using only a photometric loss is novel, and the approach can be used to\nlearn shape in other settings with limited 3D supervision. Moreover, we couple\n3D pose and shape prediction with the task of texture synthesis, obtaining a\nfull texture map of the animal from a single image. We show that the predicted\ntexture map allows a novel per-instance unsupervised optimization over the\nnetwork features. This method, SMALST (SMAL with learned Shape and Texture)\ngoes beyond previous work, which assumed manual keypoints and/or segmentation,\nto regress directly from pixels to 3D animal shape, pose and texture. Code and\ndata are available at https://github.com/silviazuffi/smalst.\n", "versions": [{"version": "v1", "created": "Tue, 20 Aug 2019 07:45:57 GMT"}, {"version": "v2", "created": "Tue, 17 Sep 2019 13:20:59 GMT"}], "update_date": "2019-09-18", "authors_parsed": [["Zuffi", "Silvia", ""], ["Kanazawa", "Angjoo", ""], ["Berger-Wolf", "Tanya", ""], ["Black", "Michael J.", ""]]}, {"id": "1908.07222", "submitter": "Mohammad Saeed Rad", "authors": "Mohammad Saeed Rad, Behzad Bozorgtabar, Urs-Viktor Marti, Max Basler,\n  Hazim Kemal Ekenel, Jean-Philippe Thiran", "title": "SROBB: Targeted Perceptual Loss for Single Image Super-Resolution", "comments": "ICCV 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  By benefiting from perceptual losses, recent studies have improved\nsignificantly the performance of the super-resolution task, where a\nhigh-resolution image is resolved from its low-resolution counterpart. Although\nsuch objective functions generate near-photorealistic results, their capability\nis limited, since they estimate the reconstruction error for an entire image in\nthe same way, without considering any semantic information. In this paper, we\npropose a novel method to benefit from perceptual loss in a more objective way.\nWe optimize a deep network-based decoder with a targeted objective function\nthat penalizes images at different semantic levels using the corresponding\nterms. In particular, the proposed method leverages our proposed OBB (Object,\nBackground and Boundary) labels, generated from segmentation labels, to\nestimate a suitable perceptual loss for boundaries, while considering texture\nsimilarity for backgrounds. We show that our proposed approach results in more\nrealistic textures and sharper edges, and outperforms other state-of-the-art\nalgorithms in terms of both qualitative results on standard benchmarks and\nresults of extensive user studies.\n", "versions": [{"version": "v1", "created": "Tue, 20 Aug 2019 08:39:48 GMT"}], "update_date": "2019-08-21", "authors_parsed": [["Rad", "Mohammad Saeed", ""], ["Bozorgtabar", "Behzad", ""], ["Marti", "Urs-Viktor", ""], ["Basler", "Max", ""], ["Ekenel", "Hazim Kemal", ""], ["Thiran", "Jean-Philippe", ""]]}, {"id": "1908.07236", "submitter": "Cristian Rodriguez", "authors": "Cristian Rodriguez-Opazo, Edison Marrese-Taylor, Fatemeh Sadat Saleh,\n  Hongdong Li, Stephen Gould", "title": "Proposal-free Temporal Moment Localization of a Natural-Language Query\n  in Video using Guided Attention", "comments": "Winter Conference on Applications of Computer Vision 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies the problem of temporal moment localization in a long\nuntrimmed video using natural language as the query. Given an untrimmed video\nand a sentence as the query, the goal is to determine the starting, and the\nending, of the relevant visual moment in the video, that corresponds to the\nquery sentence. While previous works have tackled this task by a\npropose-and-rank approach, we introduce a more efficient, end-to-end trainable,\nand {\\em proposal-free approach} that relies on three key components: a dynamic\nfilter to transfer language information to the visual domain, a new loss\nfunction to guide our model to attend the most relevant parts of the video, and\nsoft labels to model annotation uncertainty. We evaluate our method on two\nbenchmark datasets, Charades-STA and ActivityNet-Captions. Experimental results\nshow that our approach outperforms state-of-the-art methods on both datasets.\n", "versions": [{"version": "v1", "created": "Tue, 20 Aug 2019 09:22:54 GMT"}, {"version": "v2", "created": "Thu, 12 Mar 2020 10:02:31 GMT"}], "update_date": "2020-03-13", "authors_parsed": [["Rodriguez-Opazo", "Cristian", ""], ["Marrese-Taylor", "Edison", ""], ["Saleh", "Fatemeh Sadat", ""], ["Li", "Hongdong", ""], ["Gould", "Stephen", ""]]}, {"id": "1908.07253", "submitter": "Michel Moukari", "authors": "Michel Moukari, Lo\\\"ic Simon, Sylvaine Picard, Fr\\'ed\\'eric Jurie", "title": "n-MeRCI: A new Metric to Evaluate the Correlation Between Predictive\n  Uncertainty and True Error", "comments": null, "journal-ref": "IEEE/RJS International Conference on Intelligent Robots and\n  Systems (IROS), In press", "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As deep learning applications are becoming more and more pervasive in\nrobotics, the question of evaluating the reliability of inferences becomes a\ncentral question in the robotics community. This domain, known as predictive\nuncertainty, has come under the scrutiny of research groups developing Bayesian\napproaches adapted to deep learning such as Monte Carlo Dropout. Unfortunately,\nfor the time being, the real goal of predictive uncertainty has been swept\nunder the rug. Indeed, these approaches are solely evaluated in terms of raw\nperformance of the network prediction, while the quality of their estimated\nuncertainty is not assessed. Evaluating such uncertainty prediction quality is\nespecially important in robotics, as actions shall depend on the confidence in\nperceived information. In this context, the main contribution of this article\nis to propose a novel metric that is adapted to the evaluation of relative\nuncertainty assessment and directly applicable to regression with deep neural\nnetworks. To experimentally validate this metric, we evaluate it on a toy\ndataset and then apply it to the task of monocular depth estimation.\n", "versions": [{"version": "v1", "created": "Tue, 20 Aug 2019 09:51:08 GMT"}], "update_date": "2019-08-21", "authors_parsed": [["Moukari", "Michel", ""], ["Simon", "Lo\u00efc", ""], ["Picard", "Sylvaine", ""], ["Jurie", "Fr\u00e9d\u00e9ric", ""]]}, {"id": "1908.07262", "submitter": "Zhaoxiang Liu", "authors": "Zipeng Wang and Zhaoxiang Liu and Zezhou Chen and Huan Hu and Shiguo\n  Lian", "title": "A Neural Virtual Anchor Synthesizer based on Seq2Seq and GAN Models", "comments": "Accepted to ISMAR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a novel framework to generate realistic face video of an\nanchor, who is reading certain news. This task is also known as Virtual Anchor.\nGiven some paragraphs of words, we first utilize a pretrained Word2Vec model to\nembed each word into a vector; then we utilize a Seq2Seq-based model to\ntranslate these word embeddings into action units and head poses of the target\nanchor; these action units and head poses will be concatenated with facial\nlandmarks as well as the former $n$ synthesized frames, and the concatenation\nserves as input of a Pix2PixHD-based model to synthesize realistic facial\nimages for the virtual anchor. The experimental results demonstrate our\nframework is feasible for the synthesis of virtual anchor.\n", "versions": [{"version": "v1", "created": "Tue, 20 Aug 2019 10:27:31 GMT"}, {"version": "v2", "created": "Thu, 12 Sep 2019 04:38:58 GMT"}], "update_date": "2019-09-13", "authors_parsed": [["Wang", "Zipeng", ""], ["Liu", "Zhaoxiang", ""], ["Chen", "Zezhou", ""], ["Hu", "Huan", ""], ["Lian", "Shiguo", ""]]}, {"id": "1908.07269", "submitter": "Yu-Jing Lin", "authors": "Po-Wei Wu, Yu-Jing Lin, Che-Han Chang, Edward Y. Chang, Shih-Wei Liao", "title": "RelGAN: Multi-Domain Image-to-Image Translation via Relative Attributes", "comments": "Accepted to ICCV 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-domain image-to-image translation has gained increasing attention\nrecently. Previous methods take an image and some target attributes as inputs\nand generate an output image with the desired attributes. However, such methods\nhave two limitations. First, these methods assume binary-valued attributes and\nthus cannot yield satisfactory results for fine-grained control. Second, these\nmethods require specifying the entire set of target attributes, even if most of\nthe attributes would not be changed. To address these limitations, we propose\nRelGAN, a new method for multi-domain image-to-image translation. The key idea\nis to use relative attributes, which describes the desired change on selected\nattributes. Our method is capable of modifying images by changing particular\nattributes of interest in a continuous manner while preserving the other\nattributes. Experimental results demonstrate both the quantitative and\nqualitative effectiveness of our method on the tasks of facial attribute\ntransfer and interpolation.\n", "versions": [{"version": "v1", "created": "Tue, 20 Aug 2019 10:54:34 GMT"}], "update_date": "2019-08-21", "authors_parsed": [["Wu", "Po-Wei", ""], ["Lin", "Yu-Jing", ""], ["Chang", "Che-Han", ""], ["Chang", "Edward Y.", ""], ["Liao", "Shih-Wei", ""]]}, {"id": "1908.07274", "submitter": "Pingping Zhang Dr", "authors": "Yi Zeng and Pingping Zhang and Jianming Zhang and Zhe Lin and Huchuan\n  Lu", "title": "Towards High-Resolution Salient Object Detection", "comments": "Accepted by ICCV2019. The HRSOD dataset is available at\n  https://github.com/yi94code/HRSOD", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural network based methods have made a significant breakthrough in\nsalient object detection. However, they are typically limited to input images\nwith low resolutions ($400\\times400$ pixels or less). Little effort has been\nmade to train deep neural networks to directly handle salient object detection\nin very high-resolution images. This paper pushes forward high-resolution\nsaliency detection, and contributes a new dataset, named High-Resolution\nSalient Object Detection (HRSOD). To our best knowledge, HRSOD is the first\nhigh-resolution saliency detection dataset to date. As another contribution, we\nalso propose a novel approach, which incorporates both global semantic\ninformation and local high-resolution details, to address this challenging\ntask. More specifically, our approach consists of a Global Semantic Network\n(GSN), a Local Refinement Network (LRN) and a Global-Local Fusion Network\n(GLFN). GSN extracts the global semantic information based on down-sampled\nentire image. Guided by the results of GSN, LRN focuses on some local regions\nand progressively produces high-resolution predictions. GLFN is further\nproposed to enforce spatial consistency and boost performance. Experiments\nillustrate that our method outperforms existing state-of-the-art methods on\nhigh-resolution saliency datasets by a large margin, and achieves comparable or\neven better performance than them on widely-used saliency benchmarks. The HRSOD\ndataset is available at https://github.com/yi94code/HRSOD.\n", "versions": [{"version": "v1", "created": "Tue, 20 Aug 2019 11:24:02 GMT"}], "update_date": "2019-08-21", "authors_parsed": [["Zeng", "Yi", ""], ["Zhang", "Pingping", ""], ["Zhang", "Jianming", ""], ["Lin", "Zhe", ""], ["Lu", "Huchuan", ""]]}, {"id": "1908.07323", "submitter": "Zewen He", "authors": "Zewen He, He Huang, Yudong Wu, Guan Huang, Wensheng Zhang", "title": "Instance Scale Normalization for image understanding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scale variation remains a challenging problem for object detection. Common\nparadigms usually adopt multiscale training & testing (image pyramid) or FPN\n(feature pyramid network) to process objects in a wide scale range. However,\nmulti-scale methods aggravate more variations of scale that even deep\nconvolution neural networks with FPN cannot handle well. In this work, we\npropose an innovative paradigm called Instance Scale Normalization (ISN) to\nresolve the above problem. ISN compresses the scale space of objects into a\nconsistent range (ISN range), in both training and testing phases. This\nreassures the problem of scale variation fundamentally and reduces the\ndifficulty of network optimization. Experiments show that ISN surpasses\nmulti-scale counterpart significantly for object detection, instance\nsegmentation, and multi-task human pose estimation, on several architectures.\nOn COCO test-dev, our single model based on ISN achieves 46.5 mAP with a\nResNet-101 backbone, which is among the state-of-the-art (SOTA) candidates for\nobject detection.\n", "versions": [{"version": "v1", "created": "Tue, 20 Aug 2019 13:12:33 GMT"}, {"version": "v2", "created": "Wed, 10 Jun 2020 01:42:50 GMT"}], "update_date": "2020-06-11", "authors_parsed": [["He", "Zewen", ""], ["Huang", "He", ""], ["Wu", "Yudong", ""], ["Huang", "Guan", ""], ["Zhang", "Wensheng", ""]]}, {"id": "1908.07325", "submitter": "Hefeng Wu", "authors": "Tianshui Chen, Muxin Xu, Xiaolu Hui, Hefeng Wu, Liang Lin", "title": "Learning Semantic-Specific Graph Representation for Multi-Label Image\n  Recognition", "comments": "accepted by ICCV 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recognizing multiple labels of images is a practical and challenging task,\nand significant progress has been made by searching semantic-aware regions and\nmodeling label dependency. However, current methods cannot locate the semantic\nregions accurately due to the lack of part-level supervision or semantic\nguidance. Moreover, they cannot fully explore the mutual interactions among the\nsemantic regions and do not explicitly model the label co-occurrence. To\naddress these issues, we propose a Semantic-Specific Graph Representation\nLearning (SSGRL) framework that consists of two crucial modules: 1) a semantic\ndecoupling module that incorporates category semantics to guide learning\nsemantic-specific representations and 2) a semantic interaction module that\ncorrelates these representations with a graph built on the statistical label\nco-occurrence and explores their interactions via a graph propagation\nmechanism. Extensive experiments on public benchmarks show that our SSGRL\nframework outperforms current state-of-the-art methods by a sizable margin,\ne.g. with an mAP improvement of 2.5%, 2.6%, 6.7%, and 3.1% on the PASCAL VOC\n2007 & 2012, Microsoft-COCO and Visual Genome benchmarks, respectively. Our\ncodes and models are available at https://github.com/HCPLab-SYSU/SSGRL.\n", "versions": [{"version": "v1", "created": "Tue, 20 Aug 2019 13:19:28 GMT"}], "update_date": "2019-08-21", "authors_parsed": [["Chen", "Tianshui", ""], ["Xu", "Muxin", ""], ["Hui", "Xiaolu", ""], ["Wu", "Hefeng", ""], ["Lin", "Liang", ""]]}, {"id": "1908.07344", "submitter": "Chen Chen", "authors": "Chen Chen, Cheng Ouyang, Giacomo Tarroni, Jo Schlemper, Huaqi Qiu,\n  Wenjia Bai, Daniel Rueckert", "title": "Unsupervised Multi-modal Style Transfer for Cardiac MR Segmentation", "comments": "STACOM 2019 camera-ready. Winner of Multi-sequence Cardiac MR\n  Segmentation Challenge (MS-CMRSeg 2019) https://zmiclab.github.io/mscmrseg19/", "journal-ref": null, "doi": "10.1007/978-3-030-39074-7_22", "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this work, we present a fully automatic method to segment cardiac\nstructures from late-gadolinium enhanced (LGE) images without using labelled\nLGE data for training, but instead by transferring the anatomical knowledge and\nfeatures learned on annotated balanced steady-state free precession (bSSFP)\nimages, which are easier to acquire. Our framework mainly consists of two\nneural networks: a multi-modal image translation network for style transfer and\na cascaded segmentation network for image segmentation. The multi-modal image\ntranslation network generates realistic and diverse synthetic LGE images\nconditioned on a single annotated bSSFP image, forming a synthetic LGE training\nset. This set is then utilized to fine-tune the segmentation network\npre-trained on labelled bSSFP images, achieving the goal of unsupervised LGE\nimage segmentation. In particular, the proposed cascaded segmentation network\nis able to produce accurate segmentation by taking both shape prior and image\nappearance into account, achieving an average Dice score of 0.92 for the left\nventricle, 0.83 for the myocardium, and 0.88 for the right ventricle on the\ntest set.\n", "versions": [{"version": "v1", "created": "Tue, 20 Aug 2019 13:47:42 GMT"}, {"version": "v2", "created": "Wed, 21 Aug 2019 07:37:25 GMT"}, {"version": "v3", "created": "Sat, 9 Nov 2019 15:32:08 GMT"}], "update_date": "2020-02-06", "authors_parsed": [["Chen", "Chen", ""], ["Ouyang", "Cheng", ""], ["Tarroni", "Giacomo", ""], ["Schlemper", "Jo", ""], ["Qiu", "Huaqi", ""], ["Bai", "Wenjia", ""], ["Rueckert", "Daniel", ""]]}, {"id": "1908.07362", "submitter": "Chandra Churh Chatterjee", "authors": "Chandra Churh Chatterjee and Gopal Krishna", "title": "A Novel method for IDC Prediction in Breast Cancer Histopathology images\n  using Deep Residual Neural Networks", "comments": "Accepted at 2nd International Conference on Intelligent Communication\n  and Computational Techniques,2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Invasive ductal carcinoma (IDC), which is also sometimes known as the\ninfiltrating ductal carcinoma, is the most regular form of breast cancer. It\naccounts for about 80% of all breast cancers. According to the American Cancer\nSociety, more than 180,000 women in the United States are diagnosed with\ninvasive breast cancer each year. The survival rate associated with this form\nof cancer is about 77% to 93% depending on the stage at which they are being\ndiagnosed. The invasiveness and the frequency of the occurrence of these\ndisease makes it one of the difficult cancers to be diagnosed. Our proposed\nmethodology involves diagnosing the invasive ductal carcinoma with a deep\nresidual convolution network to classify the IDC affected histopathological\nimages from the normal images. The dataset for the purpose used is a benchmark\ndataset known as the Breast Histopathology Images. The microscopic RGB images\nare converted into a seven channel image matrix, which is then fed to the\nnetwork. The proposed model produces a 99.29% accurate approach towards the\nprediction of IDC in the histopathology images with an AUROC score of 0.9996.\nClassification ability of the model is tested using standard performance\nmetrics.\n", "versions": [{"version": "v1", "created": "Tue, 20 Aug 2019 14:01:14 GMT"}, {"version": "v2", "created": "Mon, 11 Nov 2019 07:48:32 GMT"}], "update_date": "2019-11-12", "authors_parsed": [["Chatterjee", "Chandra Churh", ""], ["Krishna", "Gopal", ""]]}, {"id": "1908.07388", "submitter": "Guoxian Yu", "authors": "Xuanwu Liu, Zhao Li, Jun Wang, Guoxian Yu, Carlotta Domeniconi,\n  Xiangliang Zhang", "title": "Cross-modal Zero-shot Hashing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hashing has been widely studied for big data retrieval due to its low storage\ncost and fast query speed. Zero-shot hashing (ZSH) aims to learn a hashing\nmodel that is trained using only samples from seen categories, but can\ngeneralize well to samples of unseen categories. ZSH generally uses category\nattributes to seek a semantic embedding space to transfer knowledge from seen\ncategories to unseen ones. As a result, it may perform poorly when labeled data\nare insufficient. ZSH methods are mainly designed for single-modality data,\nwhich prevents their application to the widely spread multi-modal data. On the\nother hand, existing cross-modal hashing solutions assume that all the\nmodalities share the same category labels, while in practice the labels of\ndifferent data modalities may be different. To address these issues, we propose\na general Cross-modal Zero-shot Hashing (CZHash) solution to effectively\nleverage unlabeled and labeled multi-modality data with different label spaces.\nCZHash first quantifies the composite similarity between instances using label\nand feature information. It then defines an objective function to achieve deep\nfeature learning compatible with the composite similarity preserving, category\nattribute space learning, and hashing coding function learning. CZHash further\nintroduces an alternative optimization procedure to jointly optimize these\nlearning objectives. Experiments on benchmark multi-modal datasets show that\nCZHash significantly outperforms related representative hashing approaches both\non effectiveness and adaptability.\n", "versions": [{"version": "v1", "created": "Mon, 19 Aug 2019 07:14:41 GMT"}], "update_date": "2019-08-21", "authors_parsed": [["Liu", "Xuanwu", ""], ["Li", "Zhao", ""], ["Wang", "Jun", ""], ["Yu", "Guoxian", ""], ["Domeniconi", "Carlotta", ""], ["Zhang", "Xiangliang", ""]]}, {"id": "1908.07404", "submitter": "Asim Muhammad", "authors": "Muhammad Asim, Fahad Shamshad, Ali Ahmed", "title": "Blind Image Deconvolution using Pretrained Generative Priors", "comments": "Accepted in BMVC 2019. Extended version of this paper can be found at\n  arXiv:1802.04073", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a novel approach to regularize the ill-posed blind image\ndeconvolution (blind image deblurring) problem using deep generative networks.\nWe employ two separate deep generative models - one trained to produce sharp\nimages while the other trained to generate blur kernels from lower dimensional\nparameters. To deblur, we propose an alternating gradient descent scheme\noperating in the latent lower-dimensional space of each of the pretrained\ngenerative models. Our experiments show excellent deblurring results even under\nlarge blurs and heavy noise. To improve the performance on rich image datasets\nnot well learned by the generative networks, we present a modification of the\nproposed scheme that governs the deblurring process under both generative and\nclassical priors.\n", "versions": [{"version": "v1", "created": "Tue, 20 Aug 2019 14:49:45 GMT"}], "update_date": "2019-08-21", "authors_parsed": [["Asim", "Muhammad", ""], ["Shamshad", "Fahad", ""], ["Ahmed", "Ali", ""]]}, {"id": "1908.07410", "submitter": "Giorgos Kordopatis-Zilos Mr.", "authors": "Giorgos Kordopatis-Zilos, Symeon Papadopoulos, Ioannis Patras, Ioannis\n  Kompatsiaris", "title": "ViSiL: Fine-grained Spatio-Temporal Video Similarity Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we introduce ViSiL, a Video Similarity Learning architecture\nthat considers fine-grained Spatio-Temporal relations between pairs of videos\n-- such relations are typically lost in previous video retrieval approaches\nthat embed the whole frame or even the whole video into a vector descriptor\nbefore the similarity estimation. By contrast, our Convolutional Neural Network\n(CNN)-based approach is trained to calculate video-to-video similarity from\nrefined frame-to-frame similarity matrices, so as to consider both intra- and\ninter-frame relations. In the proposed method, pairwise frame similarity is\nestimated by applying Tensor Dot (TD) followed by Chamfer Similarity (CS) on\nregional CNN frame features - this avoids feature aggregation before the\nsimilarity calculation between frames. Subsequently, the similarity matrix\nbetween all video frames is fed to a four-layer CNN, and then summarized using\nChamfer Similarity (CS) into a video-to-video similarity score -- this avoids\nfeature aggregation before the similarity calculation between videos and\ncaptures the temporal similarity patterns between matching frame sequences. We\ntrain the proposed network using a triplet loss scheme and evaluate it on five\npublic benchmark datasets on four different video retrieval problems where we\ndemonstrate large improvements in comparison to the state of the art. The\nimplementation of ViSiL is publicly available.\n", "versions": [{"version": "v1", "created": "Tue, 20 Aug 2019 15:06:24 GMT"}], "update_date": "2019-08-21", "authors_parsed": [["Kordopatis-Zilos", "Giorgos", ""], ["Papadopoulos", "Symeon", ""], ["Patras", "Ioannis", ""], ["Kompatsiaris", "Ioannis", ""]]}, {"id": "1908.07415", "submitter": "Trong Nguyen Nguyen", "authors": "Trong Nguyen Nguyen, Huu Hung Huynh, Jean Meunier", "title": "Estimating skeleton-based gait abnormality index by sparse deep\n  auto-encoder", "comments": "2018 IEEE Seventh International Conference on Communications and\n  Electronics (ICCE)", "journal-ref": null, "doi": "10.1109/CCE.2018.8465714", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes an approach estimating a gait abnormality index based on\nskeletal information provided by a depth camera. Differently from related works\nwhere the extraction of hand-crafted features is required to describe gait\ncharacteristics, our method automatically performs that stage with the support\nof a deep auto-encoder. In order to get visually interpretable features, we\nembedded a constraint of sparsity into the model. Similarly to most\ngait-related studies, the temporal factor is also considered as a\npost-processing in our system. This method provided promising results when\nexperimenting on a dataset containing nearly one hundred thousand skeleton\nsamples.\n", "versions": [{"version": "v1", "created": "Sat, 17 Aug 2019 21:42:40 GMT"}], "update_date": "2019-08-21", "authors_parsed": [["Nguyen", "Trong Nguyen", ""], ["Huynh", "Huu Hung", ""], ["Meunier", "Jean", ""]]}, {"id": "1908.07416", "submitter": "Trong Nguyen Nguyen", "authors": "Trong Nguyen Nguyen, Huu Hung Huynh, Jean Meunier", "title": "Skeleton-based Gait Index Estimation with LSTMs", "comments": "2018 IEEE/ACIS 17th International Conference on Computer and\n  Information Science (ICIS)", "journal-ref": null, "doi": "10.1109/ICIS.2018.8466522", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a method that estimates a gait index for a sequence\nof skeletons. Our system is a stack of an encoder and a decoder that are formed\nby Long Short-Term Memories (LSTMs). In the encoding stage, the characteristics\nof an input are automatically determined and are compressed into a latent\nspace. The decoding stage then attempts to reconstruct the input according to\nsuch intermediate representation. The reconstruction error is thus considered\nas a weak gait index. By combining such weak indices over a long-time movement,\nour system can provide a good estimation for the gait index. Our experiments on\na large dataset (nearly one hundred thousand skeletons) showed that the index\ngiven by the proposed method outperformed some recent works on gait analysis.\n", "versions": [{"version": "v1", "created": "Sat, 17 Aug 2019 21:22:55 GMT"}], "update_date": "2019-08-21", "authors_parsed": [["Nguyen", "Trong Nguyen", ""], ["Huynh", "Huu Hung", ""], ["Meunier", "Jean", ""]]}, {"id": "1908.07418", "submitter": "Trong Nguyen Nguyen", "authors": "Trong Nguyen Nguyen, Huu Hung Huynh, Jean Meunier", "title": "Assessment of gait normality using a depth camera and mirrors", "comments": "2018 IEEE EMBS International Conference on Biomedical & Health\n  Informatics (BHI)", "journal-ref": null, "doi": "10.1109/BHI.2018.8333364", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents an initial work on assessment of gait normality in which\nthe human body motion is represented by a sequence of enhanced depth maps. The\ninput data is provided by a system consisting of a Time-of-Flight (ToF) depth\ncamera and two mirrors. This approach proposes two feature types to describe\ncharacteristics of localized points of interest and the level of posture\nsymmetry. These two features are processed on a sequence of enhanced depth maps\nwith the support of a sliding window to provide two corresponding scores. The\ngait assessment is finally performed based on a weighted combination of these\ntwo scores. The evaluation is performed by experimenting on 6 simulated\nabnormal gaits.\n", "versions": [{"version": "v1", "created": "Sat, 17 Aug 2019 20:52:14 GMT"}], "update_date": "2019-08-21", "authors_parsed": [["Nguyen", "Trong Nguyen", ""], ["Huynh", "Huu Hung", ""], ["Meunier", "Jean", ""]]}, {"id": "1908.07422", "submitter": "Trong Nguyen Nguyen", "authors": "Trong-Nguyen Nguyen, Huu-Hung Huynh, Jean Meunier", "title": "Human Gait Symmetry Assessment using a Depth Camera and Mirrors", "comments": null, "journal-ref": "Computers in Biology and Medicine 101 (2018) 174-183", "doi": "10.1016/j.compbiomed.2018.08.021", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a reliable approach for human gait symmetry assessment\nusing a depth camera and two mirrors. The input of our system is a sequence of\n3D point clouds which are formed from a setup including a Time-of-Flight (ToF)\ndepth camera and two mirrors. A cylindrical histogram is estimated for\ndescribing the posture in each point cloud. The sequence of such histograms is\nthen separated into two sequences of sub-histograms representing two\nhalf-bodies. A cross-correlation technique is finally applied to provide values\ndescribing gait symmetry indices. The evaluation was performed on 9 different\ngait types to demonstrate the ability of our approach in assessing gait\nsymmetry. A comparison between our system and related methods, that employ\ndifferent input data types, is also provided.\n", "versions": [{"version": "v1", "created": "Fri, 16 Aug 2019 23:27:22 GMT"}], "update_date": "2019-08-21", "authors_parsed": [["Nguyen", "Trong-Nguyen", ""], ["Huynh", "Huu-Hung", ""], ["Meunier", "Jean", ""]]}, {"id": "1908.07423", "submitter": "Yu-Wei Chao", "authors": "Yu-Wei Chao, Jimei Yang, Weifeng Chen, Jia Deng", "title": "Learning to Sit: Synthesizing Human-Chair Interactions via Hierarchical\n  Control", "comments": "Accepted to AAAI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent progress on physics-based character animation has shown impressive\nbreakthroughs on human motion synthesis, through imitating motion capture data\nvia deep reinforcement learning. However, results have mostly been demonstrated\non imitating a single distinct motion pattern, and do not generalize to\ninteractive tasks that require flexible motion patterns due to varying\nhuman-object spatial configurations. To bridge this gap, we focus on one class\nof interactive tasks -- sitting onto a chair. We propose a hierarchical\nreinforcement learning framework which relies on a collection of subtask\ncontrollers trained to imitate simple, reusable mocap motions, and a meta\ncontroller trained to execute the subtasks properly to complete the main task.\nWe experimentally demonstrate the strength of our approach over different\nnon-hierarchical and hierarchical baselines. We also show that our approach can\nbe applied to motion prediction given an image input. A supplementary video can\nbe found at https://youtu.be/3CeN0OGz2cA.\n", "versions": [{"version": "v1", "created": "Tue, 20 Aug 2019 15:14:54 GMT"}, {"version": "v2", "created": "Wed, 16 Dec 2020 11:33:08 GMT"}], "update_date": "2020-12-17", "authors_parsed": [["Chao", "Yu-Wei", ""], ["Yang", "Jimei", ""], ["Chen", "Weifeng", ""], ["Deng", "Jia", ""]]}, {"id": "1908.07433", "submitter": "Kiru Park", "authors": "Kiru Park, Timothy Patten, Markus Vincze", "title": "Pix2Pose: Pixel-Wise Coordinate Regression of Objects for 6D Pose\n  Estimation", "comments": "Accepted at ICCV 2019 (Oral)", "journal-ref": null, "doi": "10.1109/ICCV.2019.00776", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimating the 6D pose of objects using only RGB images remains challenging\nbecause of problems such as occlusion and symmetries. It is also difficult to\nconstruct 3D models with precise texture without expert knowledge or\nspecialized scanning devices. To address these problems, we propose a novel\npose estimation method, Pix2Pose, that predicts the 3D coordinates of each\nobject pixel without textured models. An auto-encoder architecture is designed\nto estimate the 3D coordinates and expected errors per pixel. These pixel-wise\npredictions are then used in multiple stages to form 2D-3D correspondences to\ndirectly compute poses with the PnP algorithm with RANSAC iterations. Our\nmethod is robust to occlusion by leveraging recent achievements in generative\nadversarial training to precisely recover occluded parts. Furthermore, a novel\nloss function, the transformer loss, is proposed to handle symmetric objects by\nguiding predictions to the closest symmetric pose. Evaluations on three\ndifferent benchmark datasets containing symmetric and occluded objects show our\nmethod outperforms the state of the art using only RGB images.\n", "versions": [{"version": "v1", "created": "Tue, 20 Aug 2019 15:34:13 GMT"}], "update_date": "2020-03-24", "authors_parsed": [["Park", "Kiru", ""], ["Patten", "Timothy", ""], ["Vincze", "Markus", ""]]}, {"id": "1908.07446", "submitter": "Alex Gomez-Marin", "authors": "Regina Zaghi-Lara, Miguel \\'Angel Gea, Jordi Cam\\'i, Luis M.\n  Mart\\'inez, Alex Gomez-Marin", "title": "Playing magic tricks to deep neural networks untangles human deception", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.AI cs.CV", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Magic is the art of producing in the spectator an illusion of impossibility.\nAlthough the scientific study of magic is in its infancy, the advent of recent\ntracking algorithms based on deep learning allow now to quantify the skills of\nthe magician in naturalistic conditions at unprecedented resolution and\nrobustness. In this study, we deconstructed stage magic into purely motor\nmaneuvers and trained an artificial neural network (DeepLabCut) to follow coins\nas a professional magician made them appear and disappear in a series of\ntricks. Rather than using AI as a mere tracking tool, we conceived it as an\n\"artificial spectator\". When the coins were not visible, the algorithm was\ntrained to infer their location as a human spectator would (i.e. in the left\nfist). This created situations where the human was fooled while AI (as seen by\na human) was not, and vice versa. Magic from the perspective of the machine\nreveals our own cognitive biases.\n", "versions": [{"version": "v1", "created": "Tue, 20 Aug 2019 15:50:11 GMT"}], "update_date": "2019-08-21", "authors_parsed": [["Zaghi-Lara", "Regina", ""], ["Gea", "Miguel \u00c1ngel", ""], ["Cam\u00ed", "Jordi", ""], ["Mart\u00ednez", "Luis M.", ""], ["Gomez-Marin", "Alex", ""]]}, {"id": "1908.07475", "submitter": "Roman Klokov", "authors": "Roman Klokov, Jakob Verbeek, Edmond Boyer", "title": "Probabilistic Reconstruction Networks for 3D Shape Inference from a\n  Single Image", "comments": "Accepted as Oral at BMVC'19", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study end-to-end learning strategies for 3D shape inference from images,\nin particular from a single image. Several approaches in this direction have\nbeen investigated that explore different shape representations and suitable\nlearning architectures. We focus instead on the underlying probabilistic\nmechanisms involved and contribute a more principled probabilistic\ninference-based reconstruction framework, which we coin Probabilistic\nReconstruction Networks. This framework expresses image conditioned 3D shape\ninference through a family of latent variable models, and naturally decouples\nthe choice of shape representations from the inference itself. Moreover, it\nsuggests different options for the image conditioning and allows training in\ntwo regimes, using either Monte Carlo or variational approximation of the\nmarginal likelihood. Using our Probabilistic Reconstruction Networks we obtain\nsingle image 3D reconstruction results that set a new state of the art on the\nShapeNet dataset in terms of the intersection over union and earth mover's\ndistance evaluation metrics. Interestingly, we obtain these results using a\nbasic voxel grid representation, improving over recent work based on finer\npoint cloud or mesh based representations.\n", "versions": [{"version": "v1", "created": "Tue, 20 Aug 2019 16:28:10 GMT"}], "update_date": "2019-08-21", "authors_parsed": [["Klokov", "Roman", ""], ["Verbeek", "Jakob", ""], ["Boyer", "Edmond", ""]]}, {"id": "1908.07490", "submitter": "Hao Tan", "authors": "Hao Tan, Mohit Bansal", "title": "LXMERT: Learning Cross-Modality Encoder Representations from\n  Transformers", "comments": "EMNLP 2019 (14 pages; with new attention visualizations)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Vision-and-language reasoning requires an understanding of visual concepts,\nlanguage semantics, and, most importantly, the alignment and relationships\nbetween these two modalities. We thus propose the LXMERT (Learning\nCross-Modality Encoder Representations from Transformers) framework to learn\nthese vision-and-language connections. In LXMERT, we build a large-scale\nTransformer model that consists of three encoders: an object relationship\nencoder, a language encoder, and a cross-modality encoder. Next, to endow our\nmodel with the capability of connecting vision and language semantics, we\npre-train the model with large amounts of image-and-sentence pairs, via five\ndiverse representative pre-training tasks: masked language modeling, masked\nobject prediction (feature regression and label classification), cross-modality\nmatching, and image question answering. These tasks help in learning both\nintra-modality and cross-modality relationships. After fine-tuning from our\npre-trained parameters, our model achieves the state-of-the-art results on two\nvisual question answering datasets (i.e., VQA and GQA). We also show the\ngeneralizability of our pre-trained cross-modality model by adapting it to a\nchallenging visual-reasoning task, NLVR2, and improve the previous best result\nby 22% absolute (54% to 76%). Lastly, we demonstrate detailed ablation studies\nto prove that both our novel model components and pre-training strategies\nsignificantly contribute to our strong results; and also present several\nattention visualizations for the different encoders. Code and pre-trained\nmodels publicly available at: https://github.com/airsplay/lxmert\n", "versions": [{"version": "v1", "created": "Tue, 20 Aug 2019 17:05:18 GMT"}, {"version": "v2", "created": "Thu, 22 Aug 2019 17:54:29 GMT"}, {"version": "v3", "created": "Tue, 3 Dec 2019 19:30:19 GMT"}], "update_date": "2019-12-05", "authors_parsed": [["Tan", "Hao", ""], ["Bansal", "Mohit", ""]]}, {"id": "1908.07500", "submitter": "Wei Sun", "authors": "Wei Sun and Tianfu Wu", "title": "Image Synthesis From Reconfigurable Layout and Style", "comments": "Accepted to ICCV 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite remarkable recent progress on both unconditional and conditional\nimage synthesis, it remains a long-standing problem to learn generative models\nthat are capable of synthesizing realistic and sharp images from reconfigurable\nspatial layout (i.e., bounding boxes + class labels in an image lattice) and\nstyle (i.e., structural and appearance variations encoded by latent vectors),\nespecially at high resolution. By reconfigurable, it means that a model can\npreserve the intrinsic one-to-many mapping from a given layout to multiple\nplausible images with different styles, and is adaptive with respect to\nperturbations of a layout and style latent code. In this paper, we present a\nlayout- and style-based architecture for generative adversarial networks\n(termed LostGANs) that can be trained end-to-end to generate images from\nreconfigurable layout and style. Inspired by the vanilla StyleGAN, the proposed\nLostGAN consists of two new components: (i) learning fine-grained mask maps in\na weakly-supervised manner to bridge the gap between layouts and images, and\n(ii) learning object instance-specific layout-aware feature normalization\n(ISLA-Norm) in the generator to realize multi-object style generation. In\nexperiments, the proposed method is tested on the COCO-Stuff dataset and the\nVisual Genome dataset with state-of-the-art performance obtained. The code and\npretrained models are available at \\url{https://github.com/iVMCL/LostGANs}.\n", "versions": [{"version": "v1", "created": "Tue, 20 Aug 2019 17:22:31 GMT"}], "update_date": "2019-08-21", "authors_parsed": [["Sun", "Wei", ""], ["Wu", "Tianfu", ""]]}, {"id": "1908.07516", "submitter": "William Whiteley", "authors": "William Whiteley, Wing K. Luk, Jens Gregor", "title": "DirectPET: Full Size Neural Network PET Reconstruction from Sinogram\n  Data", "comments": "Submitted to the Journal of Medical Imaging", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV physics.med-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Purpose: Neural network image reconstruction directly from measurement data\nis a relatively new field of research, that until now has been limited to\nproducing small single-slice images (e.g., 1x128x128). This paper proposes a\nnovel and more efficient network design for Positron Emission Tomography called\nDirectPET which is capable of reconstructing multi-slice image volumes (i.e.,\n16x400x400) from sinograms.\n  Approach: Large-scale direct neural network reconstruction is accomplished by\naddressing the associated memory space challenge through the introduction of a\nspecially designed Radon inversion layer. Using patient data, we compare the\nproposed method to the benchmark Ordered Subsets Expectation Maximization\n(OSEM) algorithm using signal-to-noise ratio, bias, mean absolute error and\nstructural similarity measures. In addition, line profiles and full-width\nhalf-maximum measurements are provided for a sample of lesions.\n  Results: DirectPET is shown capable of producing images that are\nquantitatively and qualitatively similar to the OSEM target images in a\nfraction of the time. We also report on an experiment where DirectPET is\ntrained to map low count raw data to normal count target images demonstrating\nthe method's ability to maintain image quality under a low dose scenario.\n  Conclusion: The ability of DirectPET to quickly reconstruct high-quality,\nmulti-slice image volumes suggests potential clinical viability of the method.\nHowever, design parameters and performance boundaries need to be fully\nestablished before adoption can be considered.\n", "versions": [{"version": "v1", "created": "Mon, 19 Aug 2019 19:03:13 GMT"}, {"version": "v2", "created": "Mon, 2 Dec 2019 02:19:22 GMT"}, {"version": "v3", "created": "Tue, 3 Dec 2019 15:31:09 GMT"}, {"version": "v4", "created": "Tue, 11 Feb 2020 20:22:53 GMT"}], "update_date": "2020-02-13", "authors_parsed": [["Whiteley", "William", ""], ["Luk", "Wing K.", ""], ["Gregor", "Jens", ""]]}, {"id": "1908.07519", "submitter": "Wenjin Tao", "authors": "Wenjin Tao, Ming C. Leu, Zhaozheng Yin", "title": "Multi-Modal Recognition of Worker Activity for Human-Centered\n  Intelligent Manufacturing", "comments": "17 pages, 8 figures, 6 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC cs.LG eess.IV eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a human-centered intelligent manufacturing system, sensing and\nunderstanding of the worker's activity are the primary tasks. In this paper, we\npropose a novel multi-modal approach for worker activity recognition by\nleveraging information from different sensors and in different modalities.\nSpecifically, a smart armband and a visual camera are applied to capture\nInertial Measurement Unit (IMU) signals and videos, respectively. For the IMU\nsignals, we design two novel feature transform mechanisms, in both frequency\nand spatial domains, to assemble the captured IMU signals as images, which\nallow using convolutional neural networks to learn the most discriminative\nfeatures. Along with the above two modalities, we propose two other modalities\nfor the video data, at the video frame and video clip levels, respectively.\nEach of the four modalities returns a probability distribution on activity\nprediction. Then, these probability distributions are fused to output the\nworker activity classification result. A worker activity dataset of 6\nactivities is established, which at present contains 6 common activities in\nassembly tasks, i.e., grab a tool/part, hammer a nail, use a power-screwdriver,\nrest arms, turn a screwdriver, and use a wrench. The developed multi-modal\napproach is evaluated on this dataset and achieves recognition accuracies as\nhigh as 97% and 100% in the leave-one-out and half-half experiments,\nrespectively.\n", "versions": [{"version": "v1", "created": "Tue, 20 Aug 2019 15:46:07 GMT"}], "update_date": "2019-08-22", "authors_parsed": [["Tao", "Wenjin", ""], ["Leu", "Ming C.", ""], ["Yin", "Zhaozheng", ""]]}, {"id": "1908.07553", "submitter": "Josiah Wang", "authors": "Josiah Wang, Lucia Specia", "title": "Phrase Localization Without Paired Training Examples", "comments": "Accepted for oral presentation at the IEEE/CVF International\n  Conference on Computer Vision (ICCV) 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Localizing phrases in images is an important part of image understanding and\ncan be useful in many applications that require mappings between textual and\nvisual information. Existing work attempts to learn these mappings from\nexamples of phrase-image region correspondences (strong supervision) or from\nphrase-image pairs (weak supervision). We postulate that such paired\nannotations are unnecessary, and propose the first method for the phrase\nlocalization problem where neither training procedure nor paired, task-specific\ndata is required. Our method is simple but effective: we use off-the-shelf\napproaches to detect objects, scenes and colours in images, and explore\ndifferent approaches to measure semantic similarity between the categories of\ndetected visual elements and words in phrases. Experiments on two well-known\nphrase localization datasets show that this approach surpasses all weakly\nsupervised methods by a large margin and performs very competitively to\nstrongly supervised methods, and can thus be considered a strong baseline to\nthe task. The non-paired nature of our method makes it applicable to any domain\nand where no paired phrase localization annotation is available.\n", "versions": [{"version": "v1", "created": "Tue, 20 Aug 2019 18:07:37 GMT"}], "update_date": "2019-08-22", "authors_parsed": [["Wang", "Josiah", ""], ["Specia", "Lucia", ""]]}, {"id": "1908.07623", "submitter": "Chen Qin", "authors": "Chen Qin, Wenjia Bai, Jo Schlemper, Steffen E. Petersen, Stefan K.\n  Piechnik, Stefan Neubauer, and Daniel Rueckert", "title": "Joint Motion Estimation and Segmentation from Undersampled Cardiac MR\n  Image", "comments": "This work is published at MLMIR 2018: Machine Learning for Medical\n  Image Reconstruction", "journal-ref": null, "doi": "10.1007/978-3-030-00129-2_7", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accelerating the acquisition of magnetic resonance imaging (MRI) is a\nchallenging problem, and many works have been proposed to reconstruct images\nfrom undersampled k-space data. However, if the main purpose is to extract\ncertain quantitative measures from the images, perfect reconstructions may not\nalways be necessary as long as the images enable the means of extracting the\nclinically relevant measures. In this paper, we work on jointly predicting\ncardiac motion estimation and segmentation directly from undersampled data,\nwhich are two important steps in quantitatively assessing cardiac function and\ndiagnosing cardiovascular diseases. In particular, a unified model consisting\nof both motion estimation branch and segmentation branch is learned by\noptimising the two tasks simultaneously. Additional corresponding fully-sampled\nimages are incorporated into the network as a parallel sub-network to enhance\nand guide the learning during the training process. Experimental results using\ncardiac MR images from 220 subjects show that the proposed model is robust to\nundersampled data and is capable of predicting results that are close to that\nfrom fully-sampled ones, while bypassing the usual image reconstruction stage.\n", "versions": [{"version": "v1", "created": "Tue, 20 Aug 2019 21:54:01 GMT"}], "update_date": "2019-08-22", "authors_parsed": [["Qin", "Chen", ""], ["Bai", "Wenjia", ""], ["Schlemper", "Jo", ""], ["Petersen", "Steffen E.", ""], ["Piechnik", "Stefan K.", ""], ["Neubauer", "Stefan", ""], ["Rueckert", "Daniel", ""]]}, {"id": "1908.07625", "submitter": "Joseph Tighe", "authors": "Brais Martinez, Davide Modolo, Yuanjun Xiong, Joseph Tighe", "title": "Action recognition with spatial-temporal discriminative filter banks", "comments": "ICCV 2019 Accepted Paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Action recognition has seen a dramatic performance improvement in the last\nfew years. Most of the current state-of-the-art literature either aims at\nimproving performance through changes to the backbone CNN network, or they\nexplore different trade-offs between computational efficiency and performance,\nagain through altering the backbone network. However, almost all of these works\nmaintain the same last layers of the network, which simply consist of a global\naverage pooling followed by a fully connected layer. In this work we focus on\nhow to improve the representation capacity of the network, but rather than\naltering the backbone, we focus on improving the last layers of the network,\nwhere changes have low impact in terms of computational cost. In particular, we\nshow that current architectures have poor sensitivity to finer details and we\nexploit recent advances in the fine-grained recognition literature to improve\nour model in this aspect. With the proposed approach, we obtain\nstate-of-the-art performance on Kinetics-400 and Something-Something-V1, the\ntwo major large-scale action recognition benchmarks.\n", "versions": [{"version": "v1", "created": "Tue, 20 Aug 2019 21:57:32 GMT"}], "update_date": "2019-08-22", "authors_parsed": [["Martinez", "Brais", ""], ["Modolo", "Davide", ""], ["Xiong", "Yuanjun", ""], ["Tighe", "Joseph", ""]]}, {"id": "1908.07630", "submitter": "Parijat Dube", "authors": "Bishwaranjan Bhattacharjee, John R. Kender, Matthew Hill, Parijat\n  Dube, Siyu Huo, Michael R. Glass, Brian Belgodere, Sharath Pankanti, Noel\n  Codella, Patrick Watson", "title": "P2L: Predicting Transfer Learning for Images and Semantic Relations", "comments": "10 pages, 8 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transfer learning enhances learning across tasks, by leveraging previously\nlearned representations -- if they are properly chosen. We describe an\nefficient method to accurately estimate the appropriateness of a previously\ntrained model for use in a new learning task. We use this measure, which we\ncall \"Predict To Learn\" (\"P2L\"), in the two very different domains of images\nand semantic relations, where it predicts, from a set of \"source\" models, the\none model most likely to produce effective transfer for training a given\n\"target\" model. We validate our approach thoroughly, by assembling a collection\nof candidate source models, then fine-tuning each candidate to perform each of\na collection of target tasks, and finally measuring how well transfer has been\nenhanced. Across 95 tasks within multiple domains (images classification and\nsemantic relations), the P2L approach was able to select the best transfer\nlearning model on average, while the heuristic of choosing model trained with\nthe largest data set selected the best model in only 55 cases. These results\nsuggest that P2L captures important information in common between source and\ntarget tasks, and that this shared informational structure contributes to\nsuccessful transfer learning more than simple data size.\n", "versions": [{"version": "v1", "created": "Tue, 20 Aug 2019 22:09:40 GMT"}, {"version": "v2", "created": "Thu, 15 Oct 2020 20:08:59 GMT"}], "update_date": "2020-10-19", "authors_parsed": [["Bhattacharjee", "Bishwaranjan", ""], ["Kender", "John R.", ""], ["Hill", "Matthew", ""], ["Dube", "Parijat", ""], ["Huo", "Siyu", ""], ["Glass", "Michael R.", ""], ["Belgodere", "Brian", ""], ["Pankanti", "Sharath", ""], ["Codella", "Noel", ""], ["Watson", "Patrick", ""]]}, {"id": "1908.07640", "submitter": "Micha\\\"el Ramamonjisoa", "authors": "Giorgia Pitteri, Micha\\\"el Ramamonjisoa, Slobodan Ilic, Vincent\n  Lepetit", "title": "On Object Symmetries and 6D Pose Estimation from Images", "comments": "International Conference on 3D Vision", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Objects with symmetries are common in our daily life and in industrial\ncontexts, but are often ignored in the recent literature on 6D pose estimation\nfrom images. In this paper, we study in an analytical way the link between the\nsymmetries of a 3D object and its appearance in images. We explain why\nsymmetrical objects can be a challenge when training machine learning\nalgorithms that aim at estimating their 6D pose from images. We propose an\nefficient and simple solution that relies on the normalization of the pose\nrotation. Our approach is general and can be used with any 6D pose estimation\nalgorithm. Moreover, our method is also beneficial for objects that are 'almost\nsymmetrical', i.e. objects for which only a detail breaks the symmetry. We\nvalidate our approach within a Faster-RCNN framework on a synthetic dataset\nmade with objects from the T-Less dataset, which exhibit various types of\nsymmetries, as well as real sequences from T-Less.\n", "versions": [{"version": "v1", "created": "Tue, 20 Aug 2019 22:44:24 GMT"}], "update_date": "2019-08-22", "authors_parsed": [["Pitteri", "Giorgia", ""], ["Ramamonjisoa", "Micha\u00ebl", ""], ["Ilic", "Slobodan", ""], ["Lepetit", "Vincent", ""]]}, {"id": "1908.07644", "submitter": "Gamaleldin Elsayed", "authors": "Gamaleldin F. Elsayed and Simon Kornblith and Quoc V. Le", "title": "Saccader: Improving Accuracy of Hard Attention Models for Vision", "comments": "33rd Conference on Neural Information Processing Systems (NeurIPS\n  2019), Vancouver, Canada", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although deep convolutional neural networks achieve state-of-the-art\nperformance across nearly all image classification tasks, their decisions are\ndifficult to interpret. One approach that offers some level of interpretability\nby design is \\textit{hard attention}, which uses only relevant portions of the\nimage. However, training hard attention models with only class label\nsupervision is challenging, and hard attention has proved difficult to scale to\ncomplex datasets. Here, we propose a novel hard attention model, which we term\nSaccader. Key to Saccader is a pretraining step that requires only class labels\nand provides initial attention locations for policy gradient optimization. Our\nbest models narrow the gap to common ImageNet baselines, achieving $75\\%$ top-1\nand $91\\%$ top-5 while attending to less than one-third of the image.\n", "versions": [{"version": "v1", "created": "Tue, 20 Aug 2019 23:40:21 GMT"}, {"version": "v2", "created": "Wed, 11 Sep 2019 22:45:26 GMT"}, {"version": "v3", "created": "Sat, 7 Dec 2019 00:34:57 GMT"}], "update_date": "2019-12-10", "authors_parsed": [["Elsayed", "Gamaleldin F.", ""], ["Kornblith", "Simon", ""], ["Le", "Quoc V.", ""]]}, {"id": "1908.07646", "submitter": "Awais Mansoor", "authors": "Awais Mansoor and Marius George Linguraru", "title": "Communal Domain Learning for Registration in Drifted Image Spaces", "comments": "MLMI-2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Designing a registration framework for images that do not share the same\nprobability distribution is a major challenge in modern image analytics yet\ntrivial task for the human visual system (HVS). Discrepancies in probability\ndistributions, also known as \\emph{drifts}, can occur due to various reasons\nincluding, but not limited to differences in sequences and modalities (e.g.,\nMRI T1-T2 and MRI-CT registration), or acquisition settings (e.g., multisite,\ninter-subject, or intra-subject registrations). The popular assumption about\nthe working of HVS is that it exploits a communal feature subspace exists\nbetween the registering images or fields-of-view that encompasses key\ndrift-invariant features. Mimicking the approach that is potentially adopted by\nthe HVS, herein, we present a representation learning technique of this\ninvariant communal subspace that is shared by registering domains. The proposed\ncommunal domain learning (CDL) framework uses a set of hierarchical nonlinear\ntransforms to learn the communal subspace that minimizes the probability\ndifferences and maximizes the amount of shared information between the\nregistering domains. Similarity metric and parameter optimization calculations\nfor registration are subsequently performed in the drift-minimized learned\ncommunal subspace. This generic registration framework is applied to register\nmultisequence (MR: T1, T2) and multimodal (MR, CT) images. Results demonstrated\ngeneric applicability, consistent performance, and statistically significant\nimprovement for both multi-sequence and multi-modal data using the proposed\napproach ($p$-value$<0.001$; Wilcoxon rank sum test) over baseline methods.\n", "versions": [{"version": "v1", "created": "Tue, 20 Aug 2019 23:50:56 GMT"}], "update_date": "2019-08-22", "authors_parsed": [["Mansoor", "Awais", ""], ["Linguraru", "Marius George", ""]]}, {"id": "1908.07654", "submitter": "Fengze Liu", "authors": "Fengze Liu, Yuyin Zhou, Elliot Fishman, Alan Yuille", "title": "FusionNet: Incorporating Shape and Texture for Abnormality Detection in\n  3D Abdominal CT Scans", "comments": "Accepted to MICCAI 2019 Workshop(MLMI)(8 pages, 3 figures)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic abnormality detection in abdominal CT scans can help doctors\nimprove the accuracy and efficiency in diagnosis. In this paper we aim at\ndetecting pancreatic ductal adenocarcinoma (PDAC), the most common pancreatic\ncancer. Taking the fact that the existence of tumor can affect both the shape\nand the texture of pancreas, we design a system to extract the shape and\ntexture feature at the same time for detecting PDAC. In this paper we propose a\ntwo-stage method for this 3D classification task. First, we segment the\npancreas into a binary mask. Second, a FusionNet is proposed to take both the\nbinary mask and CT image as input and perform a binary classification. The\noptimal architecture of the FusionNet is obtained by searching a pre-defined\nfunctional space. We show that the classification results using either shape or\ntexture information are complementary, and by fusing them with the optimized\narchitecture, the performance improves by a large margin. Our method achieves a\nspecificity of 97% and a sensitivity of 92% on 200 normal scans and 136 scans\nwith PDAC.\n", "versions": [{"version": "v1", "created": "Wed, 21 Aug 2019 00:11:36 GMT"}, {"version": "v2", "created": "Tue, 27 Aug 2019 01:33:07 GMT"}], "update_date": "2019-08-28", "authors_parsed": [["Liu", "Fengze", ""], ["Zhou", "Yuyin", ""], ["Fishman", "Elliot", ""], ["Yuille", "Alan", ""]]}, {"id": "1908.07656", "submitter": "Alexander Glandon", "authors": "Mahbubul Alam, Manar D. Samad, Lasitha Vidyaratne, Alexander Glandon,\n  and Khan M. Iftekharuddin", "title": "Survey on Deep Neural Networks in Speech and Vision Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE cs.SD eess.AS eess.SP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This survey presents a review of state-of-the-art deep neural network\narchitectures, algorithms, and systems in vision and speech applications.\nRecent advances in deep artificial neural network algorithms and architectures\nhave spurred rapid innovation and development of intelligent vision and speech\nsystems. With availability of vast amounts of sensor data and cloud computing\nfor processing and training of deep neural networks, and with increased\nsophistication in mobile and embedded technology, the next-generation\nintelligent systems are poised to revolutionize personal and commercial\ncomputing. This survey begins by providing background and evolution of some of\nthe most successful deep learning models for intelligent vision and speech\nsystems to date. An overview of large-scale industrial research and development\nefforts is provided to emphasize future trends and prospects of intelligent\nvision and speech systems. Robust and efficient intelligent systems demand\nlow-latency and high fidelity in resource-constrained hardware platforms such\nas mobile devices, robots, and automobiles. Therefore, this survey also\nprovides a summary of key challenges and recent successes in running deep\nneural networks on hardware-restricted platforms, i.e. within limited memory,\nbattery life, and processing capabilities. Finally, emerging applications of\nvision and speech across disciplines such as affective computing, intelligent\ntransportation, and precision medicine are discussed. To our knowledge, this\npaper provides one of the most comprehensive surveys on the latest developments\nin intelligent vision and speech applications from the perspectives of both\nsoftware and hardware systems. Many of these emerging technologies using deep\nneural networks show tremendous promise to revolutionize research and\ndevelopment for future vision and speech systems.\n", "versions": [{"version": "v1", "created": "Fri, 16 Aug 2019 16:40:49 GMT"}, {"version": "v2", "created": "Sun, 1 Dec 2019 03:30:37 GMT"}], "update_date": "2019-12-03", "authors_parsed": [["Alam", "Mahbubul", ""], ["Samad", "Manar D.", ""], ["Vidyaratne", "Lasitha", ""], ["Glandon", "Alexander", ""], ["Iftekharuddin", "Khan M.", ""]]}, {"id": "1908.07669", "submitter": "Jiahua Dong", "authors": "Jiahua Dong, Yang Cong, Gan Sun, Dongdong Hou", "title": "Semantic-Transferable Weakly-Supervised Endoscopic Lesions Segmentation", "comments": "Accepted to 2019 IEEE/CVF International Conference on Computer Vision\n  (ICCV)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Weakly-supervised learning under image-level labels supervision has been\nwidely applied to semantic segmentation of medical lesions regions. However, 1)\nmost existing models rely on effective constraints to explore the internal\nrepresentation of lesions, which only produces inaccurate and coarse lesions\nregions; 2) they ignore the strong probabilistic dependencies between target\nlesions dataset (e.g., enteroscopy images) and well-to-annotated source\ndiseases dataset (e.g., gastroscope images). To better utilize these\ndependencies, we present a new semantic lesions representation transfer model\nfor weakly-supervised endoscopic lesions segmentation, which can exploit useful\nknowledge from relevant fully-labeled diseases segmentation task to enhance the\nperformance of target weakly-labeled lesions segmentation task. More\nspecifically, a pseudo label generator is proposed to leverage seed information\nto generate highly-confident pseudo pixel labels by incorporating class balance\nand super-pixel spatial prior. It can iteratively include more hard-to-transfer\nsamples from weakly-labeled target dataset into training set. Afterwards,\ndynamically searched feature centroids for same class among different datasets\nare aligned by accumulating previously-learned features. Meanwhile, adversarial\nlearning is also employed in this paper, to narrow the gap between the lesions\namong different datasets in output space. Finally, we build a new medical\nendoscopic dataset with 3659 images collected from more than 1100 volunteers.\nExtensive experiments on our collected dataset and several benchmark datasets\nvalidate the effectiveness of our model.\n", "versions": [{"version": "v1", "created": "Wed, 21 Aug 2019 01:50:44 GMT"}, {"version": "v2", "created": "Thu, 22 Aug 2019 01:28:26 GMT"}], "update_date": "2019-08-23", "authors_parsed": [["Dong", "Jiahua", ""], ["Cong", "Yang", ""], ["Sun", "Gan", ""], ["Hou", "Dongdong", ""]]}, {"id": "1908.07678", "submitter": "Zhen Zhu", "authors": "Zhen Zhu, Mengde Xu, Song Bai, Tengteng Huang, Xiang Bai", "title": "Asymmetric Non-local Neural Networks for Semantic Segmentation", "comments": "To appear in ICCV 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The non-local module works as a particularly useful technique for semantic\nsegmentation while criticized for its prohibitive computation and GPU memory\noccupation. In this paper, we present Asymmetric Non-local Neural Network to\nsemantic segmentation, which has two prominent components: Asymmetric Pyramid\nNon-local Block (APNB) and Asymmetric Fusion Non-local Block (AFNB). APNB\nleverages a pyramid sampling module into the non-local block to largely reduce\nthe computation and memory consumption without sacrificing the performance.\nAFNB is adapted from APNB to fuse the features of different levels under a\nsufficient consideration of long range dependencies and thus considerably\nimproves the performance. Extensive experiments on semantic segmentation\nbenchmarks demonstrate the effectiveness and efficiency of our work. In\nparticular, we report the state-of-the-art performance of 81.3 mIoU on the\nCityscapes test set. For a 256x128 input, APNB is around 6 times faster than a\nnon-local block on GPU while 28 times smaller in GPU running memory occupation.\nCode is available at: https://github.com/MendelXu/ANN.git.\n", "versions": [{"version": "v1", "created": "Wed, 21 Aug 2019 02:26:44 GMT"}, {"version": "v2", "created": "Thu, 22 Aug 2019 14:59:28 GMT"}, {"version": "v3", "created": "Sat, 24 Aug 2019 03:10:45 GMT"}, {"version": "v4", "created": "Wed, 28 Aug 2019 13:35:01 GMT"}, {"version": "v5", "created": "Thu, 29 Aug 2019 13:31:38 GMT"}], "update_date": "2019-08-30", "authors_parsed": [["Zhu", "Zhen", ""], ["Xu", "Mengde", ""], ["Bai", "Song", ""], ["Huang", "Tengteng", ""], ["Bai", "Xiang", ""]]}, {"id": "1908.07683", "submitter": "Kwanyong Park", "authors": "Kwanyong Park, Sanghyun Woo, Dahun Kim, Donghyeon Cho, In So Kweon", "title": "Preserving Semantic and Temporal Consistency for Unpaired Video-to-Video\n  Translation", "comments": "Accepted by ACM Multimedia(ACM MM) 2019", "journal-ref": null, "doi": "10.1145/3343031.3350864", "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we investigate the problem of unpaired video-to-video\ntranslation. Given a video in the source domain, we aim to learn the\nconditional distribution of the corresponding video in the target domain,\nwithout seeing any pairs of corresponding videos. While significant progress\nhas been made in the unpaired translation of images, directly applying these\nmethods to an input video leads to low visual quality due to the additional\ntime dimension. In particular, previous methods suffer from semantic\ninconsistency (i.e., semantic label flipping) and temporal flickering\nartifacts. To alleviate these issues, we propose a new framework that is\ncomposed of carefully-designed generators and discriminators, coupled with two\ncore objective functions: 1) content preserving loss and 2) temporal\nconsistency loss. Extensive qualitative and quantitative evaluations\ndemonstrate the superior performance of the proposed method against previous\napproaches. We further apply our framework to a domain adaptation task and\nachieve favorable results.\n", "versions": [{"version": "v1", "created": "Wed, 21 Aug 2019 02:54:21 GMT"}], "update_date": "2019-08-22", "authors_parsed": [["Park", "Kwanyong", ""], ["Woo", "Sanghyun", ""], ["Kim", "Dahun", ""], ["Cho", "Donghyeon", ""], ["Kweon", "In So", ""]]}, {"id": "1908.07704", "submitter": "Mizuho Nishio", "authors": "Mizuho Nishio, Koji Fujimoto, and Kaori Togashi", "title": "Lung segmentation on chest x-ray images in patients with severe abnormal\n  findings using deep learning", "comments": null, "journal-ref": null, "doi": "10.1002/ima.22528", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Rationale and objectives: Several studies have evaluated the usefulness of\ndeep learning for lung segmentation using chest x-ray (CXR) images with small-\nor medium-sized abnormal findings. Here, we built a database including both CXR\nimages with severe abnormalities and experts' lung segmentation results, and\naimed to evaluate our network's efficacy in lung segmentation from these\nimages. Materials and Methods: For lung segmentation, CXR images from the\nJapanese Society of Radiological Technology (JSRT, N = 247) and Montgomery\ndatabases (N = 138), were included, and 65 additional images depicting severe\nabnormalities from a public database were evaluated and annotated by a\nradiologist, thereby adding lung segmentation results to these images. Baseline\nU-net was used to segment the lungs in images from the three databases.\nSubsequently, the U-net network architecture was automatically optimized for\nlung segmentation from CXR images using Bayesian optimization. Dice similarity\ncoefficient (DSC) was calculated to confirm segmentation. Results: Our results\ndemonstrated that using baseline U-net yielded poorer lung segmentation results\nin our database than those in the JSRT and Montgomery databases, implying that\nrobust segmentation of lungs may be difficult because of severe abnormalities.\nThe DSC values with baseline U-net for the JSRT, Montgomery and our databases\nwere 0.979, 0.941, and 0.889, respectively, and with optimized U-net, 0.976,\n0.973, and 0.932, respectively. Conclusion: For robust lung segmentation, the\nU-net architecture was optimized via Bayesian optimization, and our results\ndemonstrate that the optimized U-net was more robust than baseline U-net in\nlung segmentation from CXR images with large-sized abnormalities.\n", "versions": [{"version": "v1", "created": "Wed, 21 Aug 2019 04:05:14 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Nishio", "Mizuho", ""], ["Fujimoto", "Koji", ""], ["Togashi", "Kaori", ""]]}, {"id": "1908.07709", "submitter": "Jie Luo", "authors": "Jie Luo, Sarah Frisken, Duo Wang, Alexandra Golby, Masashi Sugiyama,\n  William M. Wells III", "title": "Are Registration Uncertainty and Error Monotonically Associated", "comments": "Draft Ver.2", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In image-guided neurosurgery, current commercial systems usually provide only\nrigid registration, partly because it is harder to predict, validate and\nunderstand non-rigid registration error. For instance, when surgeons see a\ndiscrepancy in aligned image features, they may not be able to distinguish\nbetween registration error and actual tissue deformation caused by tumor\nresection. In this case, the spatial distribution of registration error could\nhelp them make more informed decisions, e.g., ignoring the registration where\nthe estimated error is high. However, error estimates are difficult to acquire.\nProbabilistic image registration (PIR) methods provide measures of registration\nuncertainty, which could be a surrogate for assessing the registration error.\nIt is intuitive and believed by many clinicians that high uncertainty indicates\na large error. However, the monotonic association between uncertainty and error\nhas not been examined in image registration literature. In this pilot study, we\nattempt to address this fundamental problem by looking at one PIR method, the\nGaussian process (GP) registration. We systematically investigate the relation\nbetween GP uncertainty and error based on clinical data and show empirically\nthat there is a weak-to-moderate positive monotonic correlation between\npoint-wise GP registration uncertainty and non-rigid registration error.\n", "versions": [{"version": "v1", "created": "Wed, 21 Aug 2019 04:57:25 GMT"}, {"version": "v2", "created": "Fri, 20 Mar 2020 19:01:37 GMT"}], "update_date": "2020-03-24", "authors_parsed": [["Luo", "Jie", ""], ["Frisken", "Sarah", ""], ["Wang", "Duo", ""], ["Golby", "Alexandra", ""], ["Sugiyama", "Masashi", ""], ["Wells", "William M.", "III"]]}, {"id": "1908.07726", "submitter": "Sulaiman Vesal", "authors": "Sulaiman Vesal, Nishant Ravikumar, Andreas Maier", "title": "Automated Multi-sequence Cardiac MRI Segmentation Using Supervised\n  Domain Adaptation", "comments": "Accepted at STACOM-MICCAI 2019", "journal-ref": null, "doi": "10.1007/978-3-030-39074-7_32", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Left ventricle segmentation and morphological assessment are essential for\nimproving diagnosis and our understanding of cardiomyopathy, which in turn is\nimperative for reducing risk of myocardial infarctions in patients.\nConvolutional neural network (CNN) based methods for cardiac magnetic resonance\n(CMR) image segmentation rely on supervision with pixel-level annotations, and\nmay not generalize well to images from a different domain. These methods are\ntypically sensitive to variations in imaging protocols and data acquisition.\nSince annotating multi-sequence CMR images is tedious and subject to inter- and\nintra-observer variations, developing methods that can automatically adapt from\none domain to the target domain is of great interest. In this paper, we propose\nan approach for domain adaptation in multi-sequence CMR segmentation task using\ntransfer learning that combines multi-source image information. We first train\nan encoder-decoder CNN on T2-weighted and balanced-Steady State Free Precession\n(bSSFP) MR images with pixel-level annotation and fine-tune the same network\nwith a limited number of Late Gadolinium Enhanced-MR (LGE-MR) subjects, to\nadapt the domain features. The domain-adapted network was trained with just\nfour LGE-MR training samples and obtained an average Dice score of $\\sim$85.0\\%\non the test set comprises of 40 LGE-MR subjects. The proposed method\nsignificantly outperformed a network without adaptation trained from scratch on\nthe same set of LGE-MR training data.\n", "versions": [{"version": "v1", "created": "Wed, 21 Aug 2019 07:16:07 GMT"}], "update_date": "2020-02-14", "authors_parsed": [["Vesal", "Sulaiman", ""], ["Ravikumar", "Nishant", ""], ["Maier", "Andreas", ""]]}, {"id": "1908.07732", "submitter": "Xuan Luo", "authors": "Xuan Luo, Yanmeng Kong, Jason Lawrence, Ricardo Martin-Brualla, and\n  Steve Seitz", "title": "KeystoneDepth: Visualizing History in 3D", "comments": "Project website: http://roxanneluo.github.io/KeystoneDepth.html ,\n  Video: https://youtu.be/5JrX-KKisC8 , More results:\n  http://roxanneluo.github.io/keystonedepth_supplementary/index.html", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces the largest and most diverse collection of rectified\nstereo image pairs to the research community, KeystoneDepth, consisting of tens\nof thousands of stereographs of historical people, events, objects, and scenes\nbetween 1860 and 1963. Leveraging the Keystone-Mast raw scans from the\nCalifornia Museum of Photography, we apply multiple processing steps to produce\nclean stereo image pairs, complete with calibration data, rectification\ntransforms, and depthmaps. A second contribution is a novel approach for view\nsynthesis that runs at real-time rates on a mobile device, simulating the\nexperience of looking through an open window into these historical scenes. We\nproduce results for thousands of antique stereographs, capturing many important\nhistorical moments.\n", "versions": [{"version": "v1", "created": "Wed, 21 Aug 2019 07:35:26 GMT"}, {"version": "v2", "created": "Thu, 19 Sep 2019 17:56:45 GMT"}], "update_date": "2019-09-20", "authors_parsed": [["Luo", "Xuan", ""], ["Kong", "Yanmeng", ""], ["Lawrence", "Jason", ""], ["Martin-Brualla", "Ricardo", ""], ["Seitz", "Steve", ""]]}, {"id": "1908.07736", "submitter": "Neslihan Bayramoglu", "authors": "Neslihan Bayramoglu, Aleksei Tiulpin, Jukka Hirvasniemi, Miika T.\n  Nieminen, Simo Saarakkala", "title": "Adaptive Segmentation of Knee Radiographs for Selecting the Optimal ROI\n  in Texture Analysis", "comments": null, "journal-ref": null, "doi": "10.1016/j.joca.2020.03.006", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The purposes of this study were to investigate: 1) the effect of placement of\nregion-of-interest (ROI) for texture analysis of subchondral bone in knee\nradiographs, and 2) the ability of several texture descriptors to distinguish\nbetween the knees with and without radiographic osteoarthritis (OA). Bilateral\nposterior-anterior knee radiographs were analyzed from the baseline of OAI and\nMOST datasets. A fully automatic method to locate the most informative region\nfrom subchondral bone using adaptive segmentation was developed. We used an\noversegmentation strategy for partitioning knee images into the compact regions\nthat follow natural texture boundaries. LBP, Fractal Dimension (FD), Haralick\nfeatures, Shannon entropy, and HOG methods were computed within the standard\nROI and within the proposed adaptive ROIs. Subsequently, we built logistic\nregression models to identify and compare the performances of each texture\ndescriptor and each ROI placement method using 5-fold cross validation setting.\nImportantly, we also investigated the generalizability of our approach by\ntraining the models on OAI and testing them on MOST dataset.We used area under\nthe receiver operating characteristic (ROC) curve (AUC) and average precision\n(AP) obtained from the precision-recall (PR) curve to compare the results. We\nfound that the adaptive ROI improves the classification performance (OA vs.\nnon-OA) over the commonly used standard ROI (up to 9% percent increase in AUC).\nWe also observed that, from all texture parameters, LBP yielded the best\nperformance in all settings with the best AUC of 0.840 [0.825, 0.852] and\nassociated AP of 0.804 [0.786, 0.820]. Compared to the current state-of-the-art\napproaches, our results suggest that the proposed adaptive ROI approach in\ntexture analysis of subchondral bone can increase the diagnostic performance\nfor detecting the presence of radiographic OA.\n", "versions": [{"version": "v1", "created": "Wed, 21 Aug 2019 07:48:02 GMT"}], "update_date": "2020-05-18", "authors_parsed": [["Bayramoglu", "Neslihan", ""], ["Tiulpin", "Aleksei", ""], ["Hirvasniemi", "Jukka", ""], ["Nieminen", "Miika T.", ""], ["Saarakkala", "Simo", ""]]}, {"id": "1908.07748", "submitter": "Chunlei Liu", "authors": "Chunlei Liu and Wenrui Ding and Xin Xia and Yuan Hu and Baochang Zhang\n  and Jianzhuang Liu and Bohan Zhuang and Guodong Guo", "title": "RBCN: Rectified Binary Convolutional Networks for Enhancing the\n  Performance of 1-bit DCNNs", "comments": "Published in IJCAI2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Binarized convolutional neural networks (BCNNs) are widely used to improve\nmemory and computation efficiency of deep convolutional neural networks (DCNNs)\nfor mobile and AI chips based applications. However, current BCNNs are not able\nto fully explore their corresponding full-precision models, causing a\nsignificant performance gap between them. In this paper, we propose rectified\nbinary convolutional networks (RBCNs), towards optimized BCNNs, by combining\nfull-precision kernels and feature maps to rectify the binarization process in\na unified framework. In particular, we use a GAN to train the 1-bit binary\nnetwork with the guidance of its corresponding full-precision model, which\nsignificantly improves the performance of BCNNs. The rectified convolutional\nlayers are generic and flexible, and can be easily incorporated into existing\nDCNNs such as WideResNets and ResNets. Extensive experiments demonstrate the\nsuperior performance of the proposed RBCNs over state-of-the-art BCNNs. In\nparticular, our method shows strong generalization on the object tracking task.\n", "versions": [{"version": "v1", "created": "Wed, 21 Aug 2019 08:28:44 GMT"}, {"version": "v2", "created": "Fri, 6 Sep 2019 07:46:17 GMT"}], "update_date": "2019-09-09", "authors_parsed": [["Liu", "Chunlei", ""], ["Ding", "Wenrui", ""], ["Xia", "Xin", ""], ["Hu", "Yuan", ""], ["Zhang", "Baochang", ""], ["Liu", "Jianzhuang", ""], ["Zhuang", "Bohan", ""], ["Guo", "Guodong", ""]]}, {"id": "1908.07750", "submitter": "Zhaoxiang Liu", "authors": "Zezhou Chen and Zhaoxiang Liu and Huan Hu and Jinqiang Bai and Shiguo\n  Lian and Fuyuan Shi and Kai Wang", "title": "A Realistic Face-to-Face Conversation System based on Deep Neural\n  Networks", "comments": "Accepted to ICCV 2019 workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.SD eess.AS eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To improve the experiences of face-to-face conversation with avatar, this\npaper presents a novel conversation system. It is composed of two\nsequence-to-sequence models respectively for listening and speaking and a\nGenerative Adversarial Network (GAN) based realistic avatar synthesizer. The\nmodels exploit the facial action and head pose to learn natural human\nreactions. Based on the models' output, the synthesizer uses the Pixel2Pixel\nmodel to generate realistic facial images. To show the improvement of our\nsystem, we use a 3D model based avatar driving scheme as a reference. We train\nand evaluate our neural networks with the data from ESPN shows. Experimental\nresults show that our conversation system can generate natural facial reactions\nand realistic facial images.\n", "versions": [{"version": "v1", "created": "Wed, 21 Aug 2019 08:34:50 GMT"}], "update_date": "2019-08-22", "authors_parsed": [["Chen", "Zezhou", ""], ["Liu", "Zhaoxiang", ""], ["Hu", "Huan", ""], ["Bai", "Jinqiang", ""], ["Lian", "Shiguo", ""], ["Shi", "Fuyuan", ""], ["Wang", "Kai", ""]]}, {"id": "1908.07765", "submitter": "Nahum Kiryati", "authors": "Yuval Landau and Nahum Kiryati", "title": "Dataset Growth in Medical Image Analysis Research", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Medical image analysis studies usually require medical image datasets for\ntraining, testing and validation of algorithms. The need is underscored by the\ndeep learning revolution and the dominance of machine learning in recent\nmedical image analysis research. Nevertheless, due to ethical and legal\nconstraints, commercial conflicts and the dependence on busy medical\nprofessionals, medical image analysis researchers have been described as \"data\nstarved\". Due to the lack of objective criteria for sufficiency of dataset\nsize, the research community implicitly sets ad-hoc standards by means of the\npeer review process. We hypothesize that peer review requires researchers to\nreport the use of ever-increasing datasets as one condition for acceptance of\ntheir work to reputable publication venues. To test this hypothesis, we scanned\nthe proceedings of the eminent MICCAI (Medical Image Computing and\nComputer-Assisted Intervention) conferences from 2011 to 2018. From a total of\n2136 articles, we focused on 907 papers involving human datasets of MRI\n(Magnetic Resonance Imaging), CT (Computed Tomography) and fMRI (functional\nMRI) images. For each modality, for each of the years 2011-2018 we calculated\nthe average, geometric mean and median number of human subjects used in that\nyear's MICCAI articles. The results corroborate the dataset growth hypothesis.\nSpecifically, the annual median dataset size in MICCAI articles has grown\nroughly 3-10 times from 2011 to 2018, depending on the imaging modality.\nStatistical analysis further supports the dataset growth hypothesis and reveals\nexponential growth of the geometric mean dataset size, with annual growth of\nabout 21% for MRI, 24% for CT and 31% for fMRI. In slight analogy to Moore's\nlaw, the results can provide guidance about trends in the expectations of the\nmedical image analysis community regarding dataset size.\n", "versions": [{"version": "v1", "created": "Wed, 21 Aug 2019 09:34:29 GMT"}], "update_date": "2019-08-22", "authors_parsed": [["Landau", "Yuval", ""], ["Kiryati", "Nahum", ""]]}, {"id": "1908.07772", "submitter": "Tobias Senst", "authors": "Maik Simon, Markus K\\\"uchhold, Tobias Senst, Erik Bochinski and Thomas\n  Sikora", "title": "Video-based Bottleneck Detection utilizing Lagrangian Dynamics in\n  Crowded Scenes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Avoiding bottleneck situations in crowds is critical for the safety and\ncomfort of people at large events or in public transportation. Based on the\nwork of Lagrangian motion analysis we propose a novel video-based\nbottleneckdetector by identifying characteristic stowage patterns in\ncrowd-movements captured by optical flow fields. The Lagrangian framework\nallows to assess complex timedependent crowd-motion dynamics at large temporal\nscales near the bottleneck by two dimensional Lagrangian fields. In particular\nwe propose long-term temporal filtered Finite Time Lyapunov Exponents (FTLE)\nfields that provide towards a more global segmentation of the crowd movements\nand allows to capture its deformations when a crowd is passing a bottleneck.\nFinally, these deformations are used for an automatic spatio-temporal detection\nof such situations. The performance of the proposed approach is shown in\nextensive evaluations on the existing J\\\"ulich and AGORASET datasets, that we\nhave updated with ground truth data for spatio-temporal bottleneck analysis.\n", "versions": [{"version": "v1", "created": "Wed, 21 Aug 2019 09:59:46 GMT"}], "update_date": "2019-08-22", "authors_parsed": [["Simon", "Maik", ""], ["K\u00fcchhold", "Markus", ""], ["Senst", "Tobias", ""], ["Bochinski", "Erik", ""], ["Sikora", "Thomas", ""]]}, {"id": "1908.07801", "submitter": "Haoshu Fang", "authors": "Hao-Shu Fang, Jianhua Sun, Runzhong Wang, Minghao Gou, Yong-Lu Li,\n  Cewu Lu", "title": "InstaBoost: Boosting Instance Segmentation via Probability Map Guided\n  Copy-Pasting", "comments": "ICCV 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Instance segmentation requires a large number of training samples to achieve\nsatisfactory performance and benefits from proper data augmentation. To enlarge\nthe training set and increase the diversity, previous methods have investigated\nusing data annotation from other domain (e.g. bbox, point) in a weakly\nsupervised mechanism. In this paper, we present a simple, efficient and\neffective method to augment the training set using the existing instance mask\nannotations. Exploiting the pixel redundancy of the background, we are able to\nimprove the performance of Mask R-CNN for 1.7 mAP on COCO dataset and 3.3 mAP\non Pascal VOC dataset by simply introducing random jittering to objects.\nFurthermore, we propose a location probability map based approach to explore\nthe feasible locations that objects can be placed based on local appearance\nsimilarity. With the guidance of such map, we boost the performance of\nR101-Mask R-CNN on instance segmentation from 35.7 mAP to 37.9 mAP without\nmodifying the backbone or network structure. Our method is simple to implement\nand does not increase the computational complexity. It can be integrated into\nthe training pipeline of any instance segmentation model without affecting the\ntraining and inference efficiency. Our code and models have been released at\nhttps://github.com/GothicAi/InstaBoost\n", "versions": [{"version": "v1", "created": "Wed, 21 Aug 2019 11:21:17 GMT"}], "update_date": "2019-08-22", "authors_parsed": [["Fang", "Hao-Shu", ""], ["Sun", "Jianhua", ""], ["Wang", "Runzhong", ""], ["Gou", "Minghao", ""], ["Li", "Yong-Lu", ""], ["Lu", "Cewu", ""]]}, {"id": "1908.07841", "submitter": "Julien Tierny", "authors": "Maxime Soler, Martin Petitfrere, Gilles Darche, Melanie Plainchault,\n  Bruno Conche and Julien Tierny", "title": "Ranking Viscous Finger Simulations to an Acquired Ground Truth with\n  Topology-aware Matchings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.geo-ph cs.CG cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This application paper presents a novel framework based on topological data\nanalysis for the automatic evaluation and ranking of viscous finger simulation\nruns in an ensemble with respect to a reference acquisition. Individual fingers\nin a given time-step are associated with critical point pairs in the distance\nfield to the injection point, forming persistence diagrams. Different metrics,\nbased on optimal transport, for comparing time-varying persistence diagrams in\nthis specific applicative case are introduced. We evaluate the relevance of the\nrankings obtained with these metrics, both qualitatively thanks to a\nlightweight web visual interface, and quantitatively by studying the deviation\nfrom a reference ranking suggested by experts. Extensive experiments show the\nquantitative superiority of our approach compared to traditional alternatives.\nOur web interface allows experts to conveniently explore the produced rankings.\nWe show a complete viscous fingering case study demonstrating the utility of\nour approach in the context of porous media fluid flow, where our framework can\nbe used to automatically discard physically-irrelevant simulation runs from the\nensemble and rank the most plausible ones. We document an in-situ\nimplementation to lighten I/O and performance constraints arising in the\ncontext of parametric studies.\n", "versions": [{"version": "v1", "created": "Tue, 20 Aug 2019 05:37:59 GMT"}], "update_date": "2019-08-22", "authors_parsed": [["Soler", "Maxime", ""], ["Petitfrere", "Martin", ""], ["Darche", "Gilles", ""], ["Plainchault", "Melanie", ""], ["Conche", "Bruno", ""], ["Tierny", "Julien", ""]]}, {"id": "1908.07846", "submitter": "Stephen Petrie Dr", "authors": "Stephen M. Petrie and T'Mir D. Julius", "title": "Representing text as abstract images enables image classifiers to also\n  simultaneously classify text", "comments": "Minor changes in order to submit paper to a different conference\n  (e.g. made minor changes to writing in several places and added extra data to\n  Table 3 in order to make it clearer)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a novel method for converting text data into abstract image\nrepresentations, which allows image-based processing techniques (e.g. image\nclassification networks) to be applied to text-based comparison problems. We\napply the technique to entity disambiguation of inventor names in US patents.\nThe method involves converting text from each pairwise comparison between two\ninventor name records into a 2D RGB (stacked) image representation. We then\ntrain an image classification neural network to discriminate between such\npairwise comparison images, and use the trained network to label each pair of\nrecords as either matched (same inventor) or non-matched (different inventors),\nobtaining highly accurate results. Our new text-to-image representation method\ncould also be used more broadly for other NLP comparison problems, such as\ndisambiguation of academic publications, or for problems that require\nsimultaneous classification of both text and image datasets.\n", "versions": [{"version": "v1", "created": "Mon, 19 Aug 2019 17:28:29 GMT"}, {"version": "v2", "created": "Fri, 27 Sep 2019 08:39:41 GMT"}, {"version": "v3", "created": "Thu, 6 Feb 2020 07:28:03 GMT"}], "update_date": "2020-02-07", "authors_parsed": [["Petrie", "Stephen M.", ""], ["Julius", "T'Mir D.", ""]]}, {"id": "1908.07860", "submitter": "Zhao Zhang", "authors": "Zhao Zhang, Lei Wang, Sheng Li, Yang Wang, Zheng Zhang, Zhengjun Zha\n  and Meng Wang", "title": "Adaptive Structure-constrained Robust Latent Low-Rank Coding for Image\n  Recovery", "comments": "Accepted by ICDM 2019 as a regular paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a robust representation learning model called\nAdaptive Structure-constrained Low-Rank Coding (AS-LRC) for the latent\nrepresentation of data. To recover the underlying subspaces more accurately,\nAS-LRC seamlessly integrates an adaptive weighting based block-diagonal\nstructure-constrained low-rank representation and the group sparse salient\nfeature extraction into a unified framework. Specifically, AS-LRC performs the\nlatent decomposition of given data into a low-rank reconstruction by a\nblock-diagonal codes matrix, a group sparse locality-adaptive salient feature\npart and a sparse error part. To enforce the block-diagonal structures adaptive\nto different real datasets for the low-rank recovery, AS-LRC clearly computes\nan auto-weighting matrix based on the locality-adaptive features and multiplies\nby the low-rank coefficients for direct minimization at the same time. This\nencourages the codes to be block-diagonal and can avoid the tricky issue of\nchoosing optimal neighborhood size or kernel width for the weight assignment,\nsuffered in most local geometrical structures-preserving low-rank coding\nmethods. In addition, our AS-LRC selects the L2,1-norm on the projection for\nextracting group sparse features rather than learning low-rank features by\nNuclear-norm regularization, which can make learnt features robust to noise and\noutliers in samples, and can also make the feature coding process efficient.\nExtensive visualizations and numerical results demonstrate the effectiveness of\nour AS-LRC for image representation and recovery.\n", "versions": [{"version": "v1", "created": "Wed, 21 Aug 2019 13:17:55 GMT"}, {"version": "v2", "created": "Thu, 22 Aug 2019 02:28:14 GMT"}], "update_date": "2019-08-23", "authors_parsed": [["Zhang", "Zhao", ""], ["Wang", "Lei", ""], ["Li", "Sheng", ""], ["Wang", "Yang", ""], ["Zhang", "Zheng", ""], ["Zha", "Zhengjun", ""], ["Wang", "Meng", ""]]}, {"id": "1908.07878", "submitter": "Zhao Zhang", "authors": "Zhao Zhang, Yulin Sun, Zheng Zhang, Yang Wang, Guangcan Liu and Meng\n  Wang", "title": "Learning Structured Twin-Incoherent Twin-Projective Latent Dictionary\n  Pairs for Classification", "comments": "Accepted by ICDM 2019 as a regular paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we extend the popular dictionary pair learning (DPL) into the\nscenario of twin-projective latent flexible DPL under a structured\ntwin-incoherence. Technically, a novel framework called Twin-Projective Latent\nFlexible DPL (TP-DPL) is proposed, which minimizes the twin-incoherence\nconstrained flexibly-relaxed reconstruction error to avoid the possible\nover-fitting issue and produce accurate reconstruction. In this setting, our\nTP-DPL integrates the twin-incoherence based latent flexible DPL and the joint\nembedding of codes as well as salient features by twin-projection into a\nunified model in an adaptive neighborhood-preserving manner. As a result,\nTP-DPL unifies the salient feature extraction, representation and\nclassification. The twin-incoherence constraint on codes and features can\nexplicitly ensure high intra-class compactness and inter-class separation over\nthem. TP-DPL also integrates the adaptive weighting to preserve the local\nneighborhood of the coefficients and salient features within each class\nexplicitly. For efficiency, TP-DPL uses Frobenius-norm and abandons the costly\nl0/l1-norm for group sparse representation. Another byproduct is that TP-DPL\ncan directly apply the class-specific twin-projective reconstruction residual\nto compute the label of data. Extensive results on public databases show that\nTP-DPL can deliver the state-of-the-art performance.\n", "versions": [{"version": "v1", "created": "Wed, 21 Aug 2019 13:59:00 GMT"}], "update_date": "2019-08-22", "authors_parsed": [["Zhang", "Zhao", ""], ["Sun", "Yulin", ""], ["Zhang", "Zheng", ""], ["Wang", "Yang", ""], ["Liu", "Guangcan", ""], ["Wang", "Meng", ""]]}, {"id": "1908.07904", "submitter": "Qing Guo", "authors": "Qing Guo, Wei Feng, Zhihao Chen, Ruijun Gao, Liang Wan, Song Wang", "title": "Effects of Blur and Deblurring to Visual Object Tracking", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Intuitively, motion blur may hurt the performance of visual object tracking.\nHowever, we lack quantitative evaluation of tracker robustness to different\nlevels of motion blur. Meanwhile, while image deblurring methods can produce\nvisually clearer videos for pleasing human eyes, it is unknown whether visual\nobject tracking can benefit from image deblurring or not. In this paper, we\naddress these two problems by constructing a Blurred Video Tracking benchmark,\nwhich contains a variety of videos with different levels of motion blurs, as\nwell as ground truth tracking results for evaluating trackers. We extensively\nevaluate 23 trackers on this benchmark and observe several new interesting\nresults. Specifically, we find that light blur may improve the performance of\nmany trackers, but heavy blur always hurts the tracking performance. We also\nfind that image deblurring may help to improve tracking performance on heavily\nblurred videos but hurt the performance on lightly blurred videos. According to\nthese observations, we propose a new GAN based scheme to improve the tracker\nrobustness to motion blurs. In this scheme, a finetuned discriminator is used\nas an adaptive assessor to selectively deblur frames during the tracking\nprocess. We use this scheme to successfully improve the accuracy and robustness\nof 6 trackers.\n", "versions": [{"version": "v1", "created": "Wed, 21 Aug 2019 15:02:14 GMT"}], "update_date": "2019-08-22", "authors_parsed": [["Guo", "Qing", ""], ["Feng", "Wei", ""], ["Chen", "Zhihao", ""], ["Gao", "Ruijun", ""], ["Wan", "Liang", ""], ["Wang", "Song", ""]]}, {"id": "1908.07905", "submitter": "Mohamed Abdelpakey", "authors": "Mohamed H. Abdelpakey and Mohamed S. Shehata", "title": "DomainSiam: Domain-Aware Siamese Network for Visual Object Tracking", "comments": "13 pages", "journal-ref": "14th International Symposium on Visual Computing (ISVC2019)", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual object tracking is a fundamental task in the field of computer vision.\nRecently, Siamese trackers have achieved state-of-the-art performance on recent\nbenchmarks. However, Siamese trackers do not fully utilize semantic and\nobjectness information from pre-trained networks that have been trained on the\nimage classification task. Furthermore, the pre-trained Siamese architecture is\nsparsely activated by the category label which leads to unnecessary\ncalculations and overfitting. In this paper, we propose to learn a\nDomain-Aware, that is fully utilizing semantic and objectness information while\nproducing a class-agnostic using a ridge regression network. Moreover, to\nreduce the sparsity problem, we solve the ridge regression problem with a\ndifferentiable weighted-dynamic loss function. Our tracker, dubbed DomainSiam,\nimproves the feature learning in the training phase and generalization\ncapability to other domains. Extensive experiments are performed on five\ntracking benchmarks including OTB2013 and OTB2015 for a validation set; as well\nas the VOT2017, VOT2018, LaSOT, TrackingNet, and GOT10k for a testing set.\nDomainSiam achieves state-of-the-art performance on these benchmarks while\nrunning at 53 FPS.\n", "versions": [{"version": "v1", "created": "Wed, 21 Aug 2019 15:04:24 GMT"}], "update_date": "2019-08-22", "authors_parsed": [["Abdelpakey", "Mohamed H.", ""], ["Shehata", "Mohamed S.", ""]]}, {"id": "1908.07906", "submitter": "Vinit Chunilal Sarode", "authors": "Vinit Sarode, Xueqian Li, Hunter Goforth, Yasuhiro Aoki, Rangaprasad\n  Arun Srivatsan, Simon Lucey, Howie Choset", "title": "PCRNet: Point Cloud Registration Network using PointNet Encoding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  PointNet has recently emerged as a popular representation for unstructured\npoint cloud data, allowing application of deep learning to tasks such as object\ndetection, segmentation and shape completion. However, recent works in\nliterature have shown the sensitivity of the PointNet representation to pose\nmisalignment. This paper presents a novel framework that uses the PointNet\nrepresentation to align point clouds and perform registration for applications\nsuch as tracking, 3D reconstruction and pose estimation. We develop a framework\nthat compares PointNet features of template and source point clouds to find the\ntransformation that aligns them accurately. Depending on the prior information\nabout the shape of the object formed by the point clouds, our framework can\nproduce approaches that are shape specific or general to unseen shapes. The\nshape specific approach uses a Siamese architecture with fully connected (FC)\nlayers and is robust to noise and initial misalignment in data. We perform\nextensive simulation and real-world experiments to validate the efficacy of our\napproach and compare the performance with state-of-art approaches.\n", "versions": [{"version": "v1", "created": "Wed, 21 Aug 2019 15:10:24 GMT"}, {"version": "v2", "created": "Mon, 4 Nov 2019 04:52:47 GMT"}], "update_date": "2019-11-05", "authors_parsed": [["Sarode", "Vinit", ""], ["Li", "Xueqian", ""], ["Goforth", "Hunter", ""], ["Aoki", "Yasuhiro", ""], ["Srivatsan", "Rangaprasad Arun", ""], ["Lucey", "Simon", ""], ["Choset", "Howie", ""]]}, {"id": "1908.07919", "submitter": "Jingdong Wang", "authors": "Jingdong Wang, Ke Sun, Tianheng Cheng, Borui Jiang, Chaorui Deng, Yang\n  Zhao, Dong Liu, Yadong Mu, Mingkui Tan, Xinggang Wang, Wenyu Liu, and Bin\n  Xiao", "title": "Deep High-Resolution Representation Learning for Visual Recognition", "comments": "To appear in TPAMI. State-of-the-art performance on human pose\n  estimation, semantic segmentation, object detection, instance segmentation,\n  and face alignment. Full version of arXiv:1904.04514. (arXiv admin note: text\n  overlap with arXiv:1904.04514)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High-resolution representations are essential for position-sensitive vision\nproblems, such as human pose estimation, semantic segmentation, and object\ndetection. Existing state-of-the-art frameworks first encode the input image as\na low-resolution representation through a subnetwork that is formed by\nconnecting high-to-low resolution convolutions \\emph{in series} (e.g., ResNet,\nVGGNet), and then recover the high-resolution representation from the encoded\nlow-resolution representation. Instead, our proposed network, named as\nHigh-Resolution Network (HRNet), maintains high-resolution representations\nthrough the whole process. There are two key characteristics: (i) Connect the\nhigh-to-low resolution convolution streams \\emph{in parallel}; (ii) Repeatedly\nexchange the information across resolutions. The benefit is that the resulting\nrepresentation is semantically richer and spatially more precise. We show the\nsuperiority of the proposed HRNet in a wide range of applications, including\nhuman pose estimation, semantic segmentation, and object detection, suggesting\nthat the HRNet is a stronger backbone for computer vision problems. All the\ncodes are available at~{\\url{https://github.com/HRNet}}.\n", "versions": [{"version": "v1", "created": "Tue, 20 Aug 2019 10:47:46 GMT"}, {"version": "v2", "created": "Fri, 13 Mar 2020 13:38:30 GMT"}], "update_date": "2020-03-16", "authors_parsed": [["Wang", "Jingdong", ""], ["Sun", "Ke", ""], ["Cheng", "Tianheng", ""], ["Jiang", "Borui", ""], ["Deng", "Chaorui", ""], ["Zhao", "Yang", ""], ["Liu", "Dong", ""], ["Mu", "Yadong", ""], ["Tan", "Mingkui", ""], ["Wang", "Xinggang", ""], ["Liu", "Wenyu", ""], ["Xiao", "Bin", ""]]}, {"id": "1908.07926", "submitter": "Yuxing Tang", "authors": "Yuxing Tang, Youbao Tang, Veit Sandfort, Jing Xiao, Ronald M. Summers", "title": "TUNA-Net: Task-oriented UNsupervised Adversarial Network for Disease\n  Recognition in Cross-Domain Chest X-rays", "comments": "MICCAI 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we exploit the unsupervised domain adaptation problem for\nradiology image interpretation across domains. Specifically, we study how to\nadapt the disease recognition model from a labeled source domain to an\nunlabeled target domain, so as to reduce the effort of labeling each new\ndataset. To address the shortcoming of cross-domain, unpaired image-to-image\ntranslation methods which typically ignore class-specific semantics, we propose\na task-driven, discriminatively trained, cycle-consistent generative\nadversarial network, termed TUNA-Net. It is able to preserve 1) low-level\ndetails, 2) high-level semantic information and 3) mid-level feature\nrepresentation during the image-to-image translation process, to favor the\ntarget disease recognition task. The TUNA-Net framework is general and can be\nreadily adapted to other learning tasks. We evaluate the proposed framework on\ntwo public chest X-ray datasets for pneumonia recognition. The TUNA-Net model\ncan adapt labeled adult chest X-rays in the source domain such that they appear\nas if they were drawn from pediatric X-rays in the unlabeled target domain,\nwhile preserving the disease semantics. Extensive experiments show the\nsuperiority of the proposed method as compared to state-of-the-art unsupervised\ndomain adaptation approaches. Notably, TUNA-Net achieves an AUC of 96.3% for\npediatric pneumonia classification, which is very close to that of the\nsupervised approach (98.1%), but without the need for labels on the target\ndomain.\n", "versions": [{"version": "v1", "created": "Wed, 21 Aug 2019 15:31:54 GMT"}], "update_date": "2019-08-22", "authors_parsed": [["Tang", "Yuxing", ""], ["Tang", "Youbao", ""], ["Sandfort", "Veit", ""], ["Xiao", "Jing", ""], ["Summers", "Ronald M.", ""]]}, {"id": "1908.07956", "submitter": "Jun Xu", "authors": "Jun Xu, Zhou Xu, Wangpeng An, Haoqian Wang, David Zhang", "title": "Non-negative Sparse and Collaborative Representation for Pattern\n  Classification", "comments": "26 pages, 11 tables, 3 figures. arXiv admin note: text overlap with\n  arXiv:1806.04329", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sparse representation (SR) and collaborative representation (CR) have been\nsuccessfully applied in many pattern classification tasks such as face\nrecognition. In this paper, we propose a novel Non-negative Sparse and\nCollaborative Representation (NSCR) for pattern classification. The NSCR\nrepresentation of each test sample is obtained by seeking a non-negative sparse\nand collaborative representation vector that represents the test sample as a\nlinear combination of training samples. We observe that the non-negativity can\nmake the SR and CR more discriminative and effective for pattern\nclassification. Based on the proposed NSCR, we propose a NSCR based classifier\nfor pattern classification. Extensive experiments on benchmark datasets\ndemonstrate that the proposed NSCR based classifier outperforms the previous SR\nor CR based approach, as well as state-of-the-art deep approaches, on diverse\nchallenging pattern classification tasks.\n", "versions": [{"version": "v1", "created": "Tue, 20 Aug 2019 12:54:24 GMT"}, {"version": "v2", "created": "Thu, 29 Aug 2019 06:15:56 GMT"}], "update_date": "2019-08-30", "authors_parsed": [["Xu", "Jun", ""], ["Xu", "Zhou", ""], ["An", "Wangpeng", ""], ["Wang", "Haoqian", ""], ["Zhang", "David", ""]]}, {"id": "1908.07962", "submitter": "Siavash Haghiri", "authors": "Siavash Haghiri, Felix Wichmann, Ulrike von Luxburg", "title": "Estimation of perceptual scales using ordinal embedding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we address the problem of measuring and analysing sensation,\nthe subjective magnitude of one's experience. We do this in the context of the\nmethod of triads: the sensation of the stimulus is evaluated via relative\njudgments of the form: \"Is stimulus S_i more similar to stimulus S_j or to\nstimulus S_k?\". We propose to use ordinal embedding methods from machine\nlearning to estimate the scaling function from the relative judgments. We\nreview two relevant and well-known methods in psychophysics which are partially\napplicable in our setting: non-metric multi-dimensional scaling (NMDS) and the\nmethod of maximum likelihood difference scaling (MLDS). We perform an extensive\nset of simulations, considering various scaling functions, to demonstrate the\nperformance of the ordinal embedding methods. We show that in contrast to\nexisting approaches our ordinal embedding approach allows, first, to obtain\nreasonable scaling function from comparatively few relative judgments, second,\nthe estimation of non-monotonous scaling functions, and, third,\nmulti-dimensional perceptual scales. In addition to the simulations, we analyse\ndata from two real psychophysics experiments using ordinal embedding methods.\nOur results show that in the one-dimensional, monotonically increasing\nperceptual scale our ordinal embedding approach works as well as MLDS, while in\nhigher dimensions, only our ordinal embedding methods can produce a desirable\nscaling function. To make our methods widely accessible, we provide an\nR-implementation and general rules of thumb on how to use ordinal embedding in\nthe context of psychophysics.\n", "versions": [{"version": "v1", "created": "Wed, 21 Aug 2019 16:12:27 GMT"}], "update_date": "2019-08-22", "authors_parsed": [["Haghiri", "Siavash", ""], ["Wichmann", "Felix", ""], ["von Luxburg", "Ulrike", ""]]}, {"id": "1908.07985", "submitter": "Royson Lee", "authors": "Royson Lee, Stylianos I. Venieris, {\\L}ukasz Dudziak, Sourav\n  Bhattacharya, Nicholas D. Lane", "title": "MobiSR: Efficient On-Device Super-Resolution through Heterogeneous\n  Mobile Processors", "comments": "Accepted at the 25th Annual International Conference on Mobile\n  Computing and Networking (MobiCom), 2019", "journal-ref": null, "doi": "10.1145/3300061.3345455", "report-no": null, "categories": "cs.CV cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, convolutional networks have demonstrated unprecedented\nperformance in the image restoration task of super-resolution (SR). SR entails\nthe upscaling of a single low-resolution image in order to meet\napplication-specific image quality demands and plays a key role in mobile\ndevices. To comply with privacy regulations and reduce the overhead of cloud\ncomputing, executing SR models locally on-device constitutes a key alternative\napproach. Nevertheless, the excessive compute and memory requirements of SR\nworkloads pose a challenge in mapping SR networks on resource-constrained\nmobile platforms. This work presents MobiSR, a novel framework for performing\nefficient super-resolution on-device. Given a target mobile platform, the\nproposed framework considers popular model compression techniques and traverses\nthe design space to reach the highest performing trade-off between image\nquality and processing speed. At run time, a novel scheduler dispatches\nincoming image patches to the appropriate model-engine pair based on the\npatch's estimated upscaling difficulty in order to meet the required image\nquality with minimum processing latency. Quantitative evaluation shows that the\nproposed framework yields on-device SR designs that achieve an average speedup\nof 2.13x over highly-optimized parallel difficulty-unaware mappings and 4.79x\nover highly-optimized single compute engine implementations.\n", "versions": [{"version": "v1", "created": "Wed, 21 Aug 2019 16:55:08 GMT"}], "update_date": "2019-08-22", "authors_parsed": [["Lee", "Royson", ""], ["Venieris", "Stylianos I.", ""], ["Dudziak", "\u0141ukasz", ""], ["Bhattacharya", "Sourav", ""], ["Lane", "Nicholas D.", ""]]}, {"id": "1908.08004", "submitter": "Deepak Anand", "authors": "Yaman Dang, Deepak Anand, Amit Sethi", "title": "Pixel-wise Segmentation of Right Ventricle of Heart", "comments": "Accepted at IEEE TENCON 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  One of the first steps in the diagnosis of most cardiac diseases, such as\npulmonary hypertension, coronary heart disease is the segmentation of\nventricles from cardiac magnetic resonance (MRI) images. Manual segmentation of\nthe right ventricle requires diligence and time, while its automated\nsegmentation is challenging due to shape variations and illdefined borders. We\npropose a deep learning based method for the accurate segmentation of right\nventricle, which does not require post-processing and yet it achieves the\nstate-of-the-art performance of 0.86 Dice coefficient and 6.73 mm Hausdorff\ndistance on RVSC-MICCAI 2012 dataset. We use a novel adaptive cost function to\ncounter extreme class-imbalance in the dataset. We present a comprehensive\ncomparative study of loss functions, architectures, and ensembling techniques\nto build a principled approach for biomedical segmentation tasks.\n", "versions": [{"version": "v1", "created": "Wed, 21 Aug 2019 17:29:29 GMT"}], "update_date": "2019-08-22", "authors_parsed": [["Dang", "Yaman", ""], ["Anand", "Deepak", ""], ["Sethi", "Amit", ""]]}, {"id": "1908.08016", "submitter": "Yi Sun", "authors": "Daniel Kang, Yi Sun, Dan Hendrycks, Tom Brown, Jacob Steinhardt", "title": "Testing Robustness Against Unforeseen Adversaries", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most existing adversarial defenses only measure robustness to L_p adversarial\nattacks. Not only are adversaries unlikely to exclusively create small L_p\nperturbations, adversaries are unlikely to remain fixed. Adversaries adapt and\nevolve their attacks; hence adversarial defenses must be robust to a broad\nrange of unforeseen attacks. We address this discrepancy between research and\nreality by proposing a new evaluation framework called ImageNet-UA. Our\nframework enables the research community to test ImageNet model robustness\nagainst attacks not encountered during training. To create ImageNet-UA's\ndiverse attack suite, we introduce a total of four novel adversarial attacks.\nWe also demonstrate that, in comparison to ImageNet-UA, prevailing L_inf\nrobustness assessments give a narrow account of model robustness. By evaluating\ncurrent defenses with ImageNet-UA, we find they provide little robustness to\nunforeseen attacks. We hope the greater variety and realism of ImageNet-UA\nenables development of more robust defenses which can generalize beyond attacks\nseen during training.\n", "versions": [{"version": "v1", "created": "Wed, 21 Aug 2019 17:36:48 GMT"}, {"version": "v2", "created": "Tue, 9 Jun 2020 05:17:48 GMT"}], "update_date": "2020-06-11", "authors_parsed": [["Kang", "Daniel", ""], ["Sun", "Yi", ""], ["Hendrycks", "Dan", ""], ["Brown", "Tom", ""], ["Steinhardt", "Jacob", ""]]}, {"id": "1908.08035", "submitter": "Yunguan Fu", "authors": "Yunguan Fu, Maria R. Robu, Bongjin Koo, Crispin Schneider, Stijn van\n  Laarhoven, Danail Stoyanov, Brian Davidson, Matthew J. Clarkson, Yipeng Hu", "title": "More unlabelled data or label more data? A study on semi-supervised\n  laparoscopic image segmentation", "comments": "Accepted to MICCAI MIL3ID 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Improving a semi-supervised image segmentation task has the option of adding\nmore unlabelled images, labelling the unlabelled images or combining both, as\nneither image acquisition nor expert labelling can be considered trivial in\nmost clinical applications. With a laparoscopic liver image segmentation\napplication, we investigate the performance impact by altering the quantities\nof labelled and unlabelled training data, using a semi-supervised segmentation\nalgorithm based on the mean teacher learning paradigm. We first report a\nsignificantly higher segmentation accuracy, compared with supervised learning.\nInterestingly, this comparison reveals that the training strategy adopted in\nthe semi-supervised algorithm is also responsible for this observed\nimprovement, in addition to the added unlabelled data. We then compare\ndifferent combinations of labelled and unlabelled data set sizes for training\nsemi-supervised segmentation networks, to provide a quantitative example of the\npractically useful trade-off between the two data planning strategies in this\nsurgical guidance application.\n", "versions": [{"version": "v1", "created": "Tue, 20 Aug 2019 20:54:58 GMT"}], "update_date": "2019-08-23", "authors_parsed": [["Fu", "Yunguan", ""], ["Robu", "Maria R.", ""], ["Koo", "Bongjin", ""], ["Schneider", "Crispin", ""], ["van Laarhoven", "Stijn", ""], ["Stoyanov", "Danail", ""], ["Davidson", "Brian", ""], ["Clarkson", "Matthew J.", ""], ["Hu", "Yipeng", ""]]}, {"id": "1908.08071", "submitter": "Ali Hatamizadeh", "authors": "Ali Hatamizadeh, Demetri Terzopoulos and Andriy Myronenko", "title": "End-to-End Boundary Aware Networks for Medical Image Segmentation", "comments": "Accepted to MICCAI Machine Learning in Medical Imaging (MLMI 2019)", "journal-ref": "MLMI 2019", "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fully convolutional neural networks (CNNs) have proven to be effective at\nrepresenting and classifying textural information, thus transforming image\nintensity into output class masks that achieve semantic image segmentation. In\nmedical image analysis, however, expert manual segmentation often relies on the\nboundaries of anatomical structures of interest. We propose boundary aware CNNs\nfor medical image segmentation. Our networks are designed to account for organ\nboundary information, both by providing a special network edge branch and\nedge-aware loss terms, and they are trainable end-to-end. We validate their\neffectiveness on the task of brain tumor segmentation using the BraTS 2018\ndataset. Our experiments reveal that our approach yields more accurate\nsegmentation results, which makes it promising for more extensive application\nto medical image segmentation.\n", "versions": [{"version": "v1", "created": "Wed, 21 Aug 2019 18:10:48 GMT"}, {"version": "v2", "created": "Tue, 10 Sep 2019 20:41:57 GMT"}], "update_date": "2019-09-12", "authors_parsed": [["Hatamizadeh", "Ali", ""], ["Terzopoulos", "Demetri", ""], ["Myronenko", "Andriy", ""]]}, {"id": "1908.08074", "submitter": "Haoliang Sun", "authors": "Haoliang Sun, Ronak Mehta, Hao H. Zhou, Zhichun Huang, Sterling C.\n  Johnson, Vivek Prabhakaran, and Vikas Singh", "title": "DUAL-GLOW: Conditional Flow-Based Generative Model for Modality Transfer", "comments": null, "journal-ref": "ICCV 2019", "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Positron emission tomography (PET) imaging is an imaging modality for\ndiagnosing a number of neurological diseases. In contrast to Magnetic Resonance\nImaging (MRI), PET is costly and involves injecting a radioactive substance\ninto the patient. Motivated by developments in modality transfer in vision, we\nstudy the generation of certain types of PET images from MRI data. We derive\nnew flow-based generative models which we show perform well in this small\nsample size regime (much smaller than dataset sizes available in standard\nvision tasks). Our formulation, DUAL-GLOW, is based on two invertible networks\nand a relation network that maps the latent spaces to each other. We discuss\nhow given the prior distribution, learning the conditional distribution of PET\ngiven the MRI image reduces to obtaining the conditional distribution between\nthe two latent codes w.r.t. the two image types. We also extend our framework\nto leverage 'side' information (or attributes) when available. By controlling\nthe PET generation through 'conditioning' on age, our model is also able to\ncapture brain FDG-PET (hypometabolism) changes, as a function of age. We\npresent experiments on the Alzheimers Disease Neuroimaging Initiative (ADNI)\ndataset with 826 subjects, and obtain good performance in PET image synthesis,\nqualitatively and quantitatively better than recent works.\n", "versions": [{"version": "v1", "created": "Wed, 21 Aug 2019 18:14:09 GMT"}], "update_date": "2019-08-23", "authors_parsed": [["Sun", "Haoliang", ""], ["Mehta", "Ronak", ""], ["Zhou", "Hao H.", ""], ["Huang", "Zhichun", ""], ["Johnson", "Sterling C.", ""], ["Prabhakaran", "Vivek", ""], ["Singh", "Vikas", ""]]}, {"id": "1908.08142", "submitter": "Cuong Nguyen", "authors": "Anh T. Tran, Cuong V. Nguyen, Tal Hassner", "title": "Transferability and Hardness of Supervised Classification Tasks", "comments": "This paper is published at the International Conference on Computer\n  Vision (ICCV) 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel approach for estimating the difficulty and transferability\nof supervised classification tasks. Unlike previous work, our approach is\nsolution agnostic and does not require or assume trained models. Instead, we\nestimate these values using an information theoretic approach: treating\ntraining labels as random variables and exploring their statistics. When\ntransferring from a source to a target task, we consider the conditional\nentropy between two such variables (i.e., label assignments of the two tasks).\nWe show analytically and empirically that this value is related to the loss of\nthe transferred model. We further show how to use this value to estimate task\nhardness. We test our claims extensively on three large scale data sets --\nCelebA (40 tasks), Animals with Attributes 2 (85 tasks), and Caltech-UCSD Birds\n200 (312 tasks) -- together representing 437 classification tasks. We provide\nresults showing that our hardness and transferability estimates are strongly\ncorrelated with empirical hardness and transferability. As a case study, we\ntransfer a learned face recognition model to CelebA attribute classification\ntasks, showing state of the art accuracy for tasks estimated to be highly\ntransferable.\n", "versions": [{"version": "v1", "created": "Wed, 21 Aug 2019 23:35:48 GMT"}], "update_date": "2019-08-23", "authors_parsed": [["Tran", "Anh T.", ""], ["Nguyen", "Cuong V.", ""], ["Hassner", "Tal", ""]]}, {"id": "1908.08156", "submitter": "Qi Bi", "authors": "Qi Bi, Kun Qin, Zhili Li, Han Zhang, Kai Xu", "title": "Multiple instance dense connected convolution neural network for aerial\n  image scene classification", "comments": "5 pages,3 figures, a conference paper accepted by IEEE ICIP 2019", "journal-ref": null, "doi": "10.1109/TIP.2020.2975718", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the development of deep learning, many state-of-the-art natural image\nscene classification methods have demonstrated impressive performance. While\nthe current convolution neural network tends to extract global features and\nglobal semantic information in a scene, the geo-spatial objects can be located\nat anywhere in an aerial image scene and their spatial arrangement tends to be\nmore complicated. One possible solution is to preserve more local semantic\ninformation and enhance feature propagation. In this paper, an end to end\nmultiple instance dense connected convolution neural network (MIDCCNN) is\nproposed for aerial image scene classification. First, a 23 layer dense\nconnected convolution neural network (DCCNN) is built and served as a backbone\nto extract convolution features. It is capable of preserving middle and low\nlevel convolution features. Then, an attention based multiple instance pooling\nis proposed to highlight the local semantics in an aerial image scene. Finally,\nwe minimize the loss between the bag-level predictions and the ground truth\nlabels so that the whole framework can be trained directly. Experiments on\nthree aerial image datasets demonstrate that our proposed methods can\noutperform current baselines by a large margin.\n", "versions": [{"version": "v1", "created": "Thu, 22 Aug 2019 00:59:47 GMT"}], "update_date": "2020-04-22", "authors_parsed": [["Bi", "Qi", ""], ["Qin", "Kun", ""], ["Li", "Zhili", ""], ["Zhang", "Han", ""], ["Xu", "Kai", ""]]}, {"id": "1908.08162", "submitter": "Tejas Zodage", "authors": "Rangaprasad Arun Srivatsan, Tejas Zodage, Howie Choset", "title": "Globally optimal registration of noisy point clouds", "comments": "9 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Registration of 3D point clouds is a fundamental task in several applications\nof robotics and computer vision. While registration methods such as iterative\nclosest point and variants are very popular, they are only locally optimal.\nThere has been some recent work on globally optimal registration, but they\nperform poorly in the presence of noise in the measurements. In this work we\ndevelop a mixed integer programming-based approach for globally optimal\nregistration that explicitly considers uncertainty in its optimization, and\nhence produces more accurate estimates. Furthermore, from a practical\nimplementation perspective we develop a multi-step optimization that combines\nfast local methods with our accurate global formulation. Through extensive\nsimulation and real world experiments we demonstrate improved performance over\nstate-of-the-art methods for various level of noise and outliers in the data as\nwell as for partial geometric overlap.\n", "versions": [{"version": "v1", "created": "Thu, 22 Aug 2019 01:24:09 GMT"}], "update_date": "2019-08-23", "authors_parsed": [["Srivatsan", "Rangaprasad Arun", ""], ["Zodage", "Tejas", ""], ["Choset", "Howie", ""]]}, {"id": "1908.08164", "submitter": "Qi Bi", "authors": "Qi Bi, Kun Qin, Han Zhang, Wenjun Han, Zhili Li, Kai Xu", "title": "Building change detection based on multi-scale filtering and grid\n  partition", "comments": "8 pages, 6 figures, conference paper", "journal-ref": "10th IAPR Workshop on Pattern Recognition in Remote Sensing\n  (PRRS),2018,1-6", "doi": "10.1109/PRRS.2018.8486194", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Building change detection is of great significance in high resolution remote\nsensing applications. Multi-index learning, one of the state-of-the-art\nbuilding change detection methods, still has drawbacks like incapability to\nfind change types directly and heavy computation consumption of MBI. In this\npaper, a two-stage building change detection method is proposed to address\nthese problems. In the first stage, a multi-scale filtering building index\n(MFBI) is calculated to detect building areas in each temporal with fast speed\nand moderate accuracy. In the second stage, images and the corresponding\nbuilding maps are partitioned into grids. In each grid, the ratio of building\nareas in time T2 and time T1 is calculated. Each grid is classified into one of\nthe three change patterns, i.e., significantly increase, significantly decrease\nand approximately unchanged. Exhaustive experiments indicate that the proposed\nmethod can detect building change types directly and outperform the current\nmulti-index learning method.\n", "versions": [{"version": "v1", "created": "Thu, 22 Aug 2019 01:38:47 GMT"}], "update_date": "2019-08-23", "authors_parsed": [["Bi", "Qi", ""], ["Qin", "Kun", ""], ["Zhang", "Han", ""], ["Han", "Wenjun", ""], ["Li", "Zhili", ""], ["Xu", "Kai", ""]]}, {"id": "1908.08178", "submitter": "Pengfei Zhang", "authors": "Pengfei Zhang, Yu Cao and Benyuan Liu", "title": "Multi-Stream Single Shot Spatial-Temporal Action Detection", "comments": null, "journal-ref": "26th IEEE International Conference on Image Processing (ICIP 2019)", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a 3D Convolutional Neural Networks (CNNs) based single shot\ndetector for spatial-temporal action detection tasks. Our model includes: (1)\ntwo short-term appearance and motion streams, with single RGB and optical flow\nimage input separately, in order to capture the spatial and temporal\ninformation for the current frame; (2) two long-term 3D ConvNet based stream,\nworking on sequences of continuous RGB and optical flow images to capture the\ncontext from past frames. Our model achieves strong performance for action\ndetection in video and can be easily integrated into any current two-stream\naction detection methods. We report a frame-mAP of 71.30% on the challenging\nUCF101-24 actions dataset, achieving the state-of-the-art result of the\none-stage methods. To the best of our knowledge, our work is the first system\nthat combined 3D CNN and SSD in action detection tasks.\n", "versions": [{"version": "v1", "created": "Thu, 22 Aug 2019 03:13:22 GMT"}], "update_date": "2019-08-23", "authors_parsed": [["Zhang", "Pengfei", ""], ["Cao", "Yu", ""], ["Liu", "Benyuan", ""]]}, {"id": "1908.08185", "submitter": "Chunyu Li", "authors": "Chunyu Li, Yusuke Monno, Hironori Hidaka and Masatoshi Okutomi", "title": "Pro-Cam SSfM: Projector-Camera System for Structure and Spectral\n  Reflectance from Motion", "comments": "Accepted by ICCV 2019. Project homepage:\n  http://www.ok.sc.e.titech.ac.jp/res/PCSSfM/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a novel projector-camera system for practical and\nlow-cost acquisition of a dense object 3D model with the spectral reflectance\nproperty. In our system, we use a standard RGB camera and leverage an\noff-the-shelf projector as active illumination for both the 3D reconstruction\nand the spectral reflectance estimation. We first reconstruct the 3D points\nwhile estimating the poses of the camera and the projector, which are\nalternately moved around the object, by combining multi-view structured light\nand structure-from-motion (SfM) techniques. We then exploit the projector for\nmultispectral imaging and estimate the spectral reflectance of each 3D point\nbased on a novel spectral reflectance estimation model considering the\ngeometric relationship between the reconstructed 3D points and the estimated\nprojector positions. Experimental results on several real objects demonstrate\nthat our system can precisely acquire a dense 3D model with the full spectral\nreflectance property using off-the-shelf devices.\n", "versions": [{"version": "v1", "created": "Thu, 22 Aug 2019 03:35:48 GMT"}], "update_date": "2019-08-23", "authors_parsed": [["Li", "Chunyu", ""], ["Monno", "Yusuke", ""], ["Hidaka", "Hironori", ""], ["Okutomi", "Masatoshi", ""]]}, {"id": "1908.08187", "submitter": "Daniel Sonntag", "authors": "Fabrizio Nunnari and Daniel Sonntag", "title": "A CNN toolbox for skin cancer classification", "comments": "DFKI Technical Report", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe a software toolbox for the configuration of deep neural networks\nin the domain of skin cancer classification. The implemented software\narchitecture allows developers to quickly set up new convolutional neural\nnetwork (CNN) architectures and hyper-parameter configurations. At the same\ntime, the user interface, manageable as a simple spreadsheet, allows\nnon-technical users to explore different configuration settings that need to be\nexplored when switching to different data sets. In future versions, meta\nleaning frameworks can be added, or AutoML systems that continuously improve\nover time. Preliminary results, conducted with two CNNs in the context melanoma\ndetection on dermoscopic images, quantify the impact of image augmentation,\nimage resolution, and rescaling filter on the overall detection performance and\ntraining time.\n", "versions": [{"version": "v1", "created": "Wed, 21 Aug 2019 13:27:58 GMT"}], "update_date": "2019-08-23", "authors_parsed": [["Nunnari", "Fabrizio", ""], ["Sonntag", "Daniel", ""]]}, {"id": "1908.08195", "submitter": "Chihiro Go", "authors": "Chihiro Go, Yuma Kinoshita, Sayaka Shiota, Hitoshi Kiya", "title": "An Image Fusion Scheme for Single-Shot High Dynamic Range Imaging with\n  Spatially Varying Exposures", "comments": null, "journal-ref": null, "doi": "10.1587/transfun.E102.A.1856", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a novel multi-exposure image fusion (MEF) scheme for\nsingle-shot high dynamic range imaging with spatially varying exposures (SVE).\nSingle-shot imaging with SVE enables us not only to produce images without\ncolor saturation regions from a single-shot image, but also to avoid ghost\nartifacts in the producing ones. However, the number of exposures is generally\nlimited to two, and moreover it is difficult to decide the optimum exposure\nvalues before the photographing. In the proposed scheme, a scene segmentation\nmethod is applied to input multi-exposure images, and then the luminance of the\ninput images is adjusted according to both of the number of scenes and the\nrelationship between exposure values and pixel values. The proposed method with\nthe luminance adjustment allows us to improve the above two issues. In this\npaper, we focus on dual-ISO imaging as one of single-shot imaging. In an\nexperiment, the proposed scheme is demonstrated to be effective for single-shot\nhigh dynamic range imaging with SVE, compared with conventional MEF schemes\nwith exposure compensation.\n", "versions": [{"version": "v1", "created": "Thu, 22 Aug 2019 04:21:10 GMT"}], "update_date": "2020-01-08", "authors_parsed": [["Go", "Chihiro", ""], ["Kinoshita", "Yuma", ""], ["Shiota", "Sayaka", ""], ["Kiya", "Hitoshi", ""]]}, {"id": "1908.08207", "submitter": "Minghui Liao", "authors": "Minghui Liao, Pengyuan Lyu, Minghang He, Cong Yao, Wenhao Wu, Xiang\n  Bai", "title": "Mask TextSpotter: An End-to-End Trainable Neural Network for Spotting\n  Text with Arbitrary Shapes", "comments": "Accepted by TPAMI. An extension of the conference version. arXiv\n  admin note: text overlap with arXiv:1807.02242", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unifying text detection and text recognition in an end-to-end training\nfashion has become a new trend for reading text in the wild, as these two tasks\nare highly relevant and complementary. In this paper, we investigate the\nproblem of scene text spotting, which aims at simultaneous text detection and\nrecognition in natural images. An end-to-end trainable neural network named as\nMask TextSpotter is presented. Different from the previous text spotters that\nfollow the pipeline consisting of a proposal generation network and a\nsequence-to-sequence recognition network, Mask TextSpotter enjoys a simple and\nsmooth end-to-end learning procedure, in which both detection and recognition\ncan be achieved directly from two-dimensional space via semantic segmentation.\nFurther, a spatial attention module is proposed to enhance the performance and\nuniversality. Benefiting from the proposed two-dimensional representation on\nboth detection and recognition, it easily handles text instances of irregular\nshapes, for instance, curved text. We evaluate it on four English datasets and\none multi-language dataset, achieving consistently superior performance over\nstate-of-the-art methods in both detection and end-to-end text recognition\ntasks. Moreover, we further investigate the recognition module of our method\nseparately, which significantly outperforms state-of-the-art methods on both\nregular and irregular text datasets for scene text recognition.\n", "versions": [{"version": "v1", "created": "Thu, 22 Aug 2019 05:31:53 GMT"}], "update_date": "2019-08-23", "authors_parsed": [["Liao", "Minghui", ""], ["Lyu", "Pengyuan", ""], ["He", "Minghang", ""], ["Yao", "Cong", ""], ["Wu", "Wenhao", ""], ["Bai", "Xiang", ""]]}, {"id": "1908.08209", "submitter": "Mohamed Dawod", "authors": "Mohamed Dawod, Sean Hanna", "title": "BIM-assisted object recognition for the on-site autonomous robotic\n  assembly of discrete structures", "comments": null, "journal-ref": "Construction Robotics, 2019", "doi": "10.1007/s41693-019-00021-9", "report-no": null, "categories": "cs.RO cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Robots-operating autonomous assembly applications in an unstructured\nenvironment require precise methods to locate the building components on site.\nHowever, the current available object detection systems are not well-optimised\nfor construction applications, due to the tedious setups incorporated for\nreferencing an object to a system and inability to cope with the elements\nimperfections. In this paper, we propose a flexible object pose estimation\nframework to enable robots to autonomously handle building components on-site\nwith an error tolerance to build a specific design target without the need to\nsort or label them. We implemented an object recognition approach that uses the\nvirtual representation model of all the objects found in a BIM model to\nautonomously search for the best-matched objects in a scene. The design layout\nis used to guide the robot to grasp and manipulate the found elements to build\nthe desired structure. We verify our proposed framework by testing it in an\nautomatic discrete wall assembly workflow. Although the precision is not as\nexpected, we analyse the possible reasons that might cause this imprecision,\nwhich paves the path for future improvements.\n", "versions": [{"version": "v1", "created": "Thu, 22 Aug 2019 05:44:08 GMT"}], "update_date": "2019-08-23", "authors_parsed": [["Dawod", "Mohamed", ""], ["Hanna", "Sean", ""]]}, {"id": "1908.08216", "submitter": "Sanath Narayan", "authors": "Sanath Narayan, Hisham Cholakkal, Fahad Shahbaz Khan, Ling Shao", "title": "3C-Net: Category Count and Center Loss for Weakly-Supervised Action\n  Localization", "comments": "To appear in ICCV 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Temporal action localization is a challenging computer vision problem with\nnumerous real-world applications. Most existing methods require laborious\nframe-level supervision to train action localization models. In this work, we\npropose a framework, called 3C-Net, which only requires video-level supervision\n(weak supervision) in the form of action category labels and the corresponding\ncount. We introduce a novel formulation to learn discriminative action features\nwith enhanced localization capabilities. Our joint formulation has three terms:\na classification term to ensure the separability of learned action features, an\nadapted multi-label center loss term to enhance the action feature\ndiscriminability and a counting loss term to delineate adjacent action\nsequences, leading to improved localization. Comprehensive experiments are\nperformed on two challenging benchmarks: THUMOS14 and ActivityNet 1.2. Our\napproach sets a new state-of-the-art for weakly-supervised temporal action\nlocalization on both datasets. On the THUMOS14 dataset, the proposed method\nachieves an absolute gain of 4.6% in terms of mean average precision (mAP),\ncompared to the state-of-the-art. Source code is available at\nhttps://github.com/naraysa/3c-net.\n", "versions": [{"version": "v1", "created": "Thu, 22 Aug 2019 06:20:38 GMT"}, {"version": "v2", "created": "Mon, 18 Nov 2019 12:28:41 GMT"}], "update_date": "2019-11-19", "authors_parsed": [["Narayan", "Sanath", ""], ["Cholakkal", "Hisham", ""], ["Khan", "Fahad Shahbaz", ""], ["Shao", "Ling", ""]]}, {"id": "1908.08223", "submitter": "Junghoon Seo", "authors": "Yooseung Wang, Junghoon Seo, and Taegyun Jeon", "title": "NL-LinkNet: Toward Lighter but More Accurate Road Extraction with\n  Non-Local Operations", "comments": "IEEE Geoscience and Remote Sensing Letters (2020, to appear)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Road extraction from very high resolution satellite (VHR) images is one of\nthe most important topics in the field of remote sensing. In this paper, we\npropose an efficient Non-Local LinkNet with non-local blocks that can grasp\nrelations between global features. This enables each spatial feature point to\nrefer to all other contextual information and results in more accurate road\nsegmentation. In detail, our single model without any post-processing like CRF\nrefinement, performed better than any other published state-of-the-art ensemble\nmodel in the official DeepGlobe Challenge. Moreover, our NL-LinkNet beat the\nD-LinkNet, the winner of the DeepGlobe challenge, with 43 \\% less parameters,\nless giga floating-point operations per seconds (GFLOPs) and shorter training\nconvergence time. We also present empirical analyses on the proper usages of\nnon-local blocks for the baseline model.\n", "versions": [{"version": "v1", "created": "Thu, 22 Aug 2019 06:56:57 GMT"}, {"version": "v2", "created": "Fri, 23 Oct 2020 06:29:12 GMT"}, {"version": "v3", "created": "Wed, 11 Nov 2020 10:25:37 GMT"}], "update_date": "2020-11-12", "authors_parsed": [["Wang", "Yooseung", ""], ["Seo", "Junghoon", ""], ["Jeon", "Taegyun", ""]]}, {"id": "1908.08239", "submitter": "Deokyun Kim", "authors": "Deokyun Kim, Minseon Kim, Gihyun Kwon, Dae-Shik Kim", "title": "Progressive Face Super-Resolution via Attention to Facial Landmark", "comments": "BMVC 2019 Accepted", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Face Super-Resolution (SR) is a subfield of the SR domain that specifically\ntargets the reconstruction of face images. The main challenge of face SR is to\nrestore essential facial features without distortion. We propose a novel face\nSR method that generates photo-realistic 8x super-resolved face images with\nfully retained facial details. To that end, we adopt a progressive training\nmethod, which allows stable training by splitting the network into successive\nsteps, each producing output with a progressively higher resolution. We also\npropose a novel facial attention loss and apply it at each step to focus on\nrestoring facial attributes in greater details by multiplying the pixel\ndifference and heatmap values. Lastly, we propose a compressed version of the\nstate-of-the-art face alignment network (FAN) for landmark heatmap extraction.\nWith the proposed FAN, we can extract the heatmaps suitable for face SR and\nalso reduce the overall training time. Experimental results verify that our\nmethod outperforms state-of-the-art methods in both qualitative and\nquantitative measurements, especially in perceptual quality.\n", "versions": [{"version": "v1", "created": "Thu, 22 Aug 2019 07:50:41 GMT"}], "update_date": "2019-08-23", "authors_parsed": [["Kim", "Deokyun", ""], ["Kim", "Minseon", ""], ["Kwon", "Gihyun", ""], ["Kim", "Dae-Shik", ""]]}, {"id": "1908.08242", "submitter": "Jiexiang Wang", "authors": "Jiexiang Wang, Cheng Bian, Meng Li, Xin Yang, Kai Ma, Wenao Ma, Jin\n  Yuan, Xinghao Ding, Yefeng Zheng", "title": "Uncertainty-Guided Domain Alignment for Layer Segmentation in OCT Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic and accurate segmentation for retinal and choroidal layers of\nOptical Coherence Tomography (OCT) is crucial for detection of various ocular\ndiseases. However, because of the variations in different equipments, OCT data\nobtained from different manufacturers might encounter appearance discrepancy,\nwhich could lead to performance fluctuation to a deep neural network. In this\npaper, we propose an uncertainty-guided domain alignment method to aim at\nalleviating this problem to transfer discriminative knowledge across distinct\ndomains. We disign a novel uncertainty-guided cross-entropy loss for boosting\nthe performance over areas with high uncertainty. An uncertainty-guided\ncurriculum transfer strategy is developed for the self-training (ST), which\nregards uncertainty as efficient and effective guidance to optimize the\nlearning process in target domain. Adversarial learning with feature\nrecalibration module (FRM) is applied to transfer informative knowledge from\nthe domain feature spaces adaptively. The experiments on two OCT datasets show\nthat the proposed methods can obtain significant segmentation improvements\ncompared with the baseline models.\n", "versions": [{"version": "v1", "created": "Thu, 22 Aug 2019 08:07:11 GMT"}, {"version": "v2", "created": "Fri, 30 Aug 2019 01:13:17 GMT"}], "update_date": "2019-09-02", "authors_parsed": [["Wang", "Jiexiang", ""], ["Bian", "Cheng", ""], ["Li", "Meng", ""], ["Yang", "Xin", ""], ["Ma", "Kai", ""], ["Ma", "Wenao", ""], ["Yuan", "Jin", ""], ["Ding", "Xinghao", ""], ["Zheng", "Yefeng", ""]]}, {"id": "1908.08244", "submitter": "Dheeraj Reddy Pailla", "authors": "Dheeraj Reddy Pailla, Varghese Kollerathu, Sai Saketh Chennamsetty", "title": "Object detection on aerial imagery using CenterNet", "comments": "6 pages, 6 tables, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Detection and classification of objects in aerial imagery have several\napplications like urban planning, crop surveillance, and traffic surveillance.\nHowever, due to the lower resolution of the objects and the effect of noise in\naerial images, extracting distinguishing features for the objects is a\nchallenge. We evaluate CenterNet, a state of the art method for real-time 2D\nobject detection, on the VisDrone2019 dataset. We evaluate the performance of\nthe model with different backbone networks in conjunction with varying\nresolutions during training and testing.\n", "versions": [{"version": "v1", "created": "Thu, 22 Aug 2019 08:10:23 GMT"}], "update_date": "2019-08-23", "authors_parsed": [["Pailla", "Dheeraj Reddy", ""], ["Kollerathu", "Varghese", ""], ["Chennamsetty", "Sai Saketh", ""]]}, {"id": "1908.08251", "submitter": "Mari\\\"elle Jansen", "authors": "Mari\\\"elle J.A. Jansen, Hugo J. Kuijf, Josien P.W. Pluim", "title": "Optimal input configuration of dynamic contrast enhanced MRI in\n  convolutional neural networks for liver segmentation", "comments": "Submitted to SPIE Medical Imaging 2019", "journal-ref": null, "doi": "10.1117/12.2506770", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Most MRI liver segmentation methods use a structural 3D scan as input, such\nas a T1 or T2 weighted scan. Segmentation performance may be improved by\nutilizing both structural and functional information, as contained in dynamic\ncontrast enhanced (DCE) MR series. Dynamic information can be incorporated in a\nsegmentation method based on convolutional neural networks in a number of ways.\nIn this study, the optimal input configuration of DCE MR images for\nconvolutional neural networks (CNNs) is studied. The performance of three\ndifferent input configurations for CNNs is studied for a liver segmentation\ntask. The three configurations are I) one phase image of the DCE-MR series as\ninput image; II) the separate phases of the DCE-MR as input images; and III)\nthe separate phases of the DCE-MR as channels of one input image. The three\ninput configurations are fed into a dilated fully convolutional network and\ninto a small U-net. The CNNs were trained using 19 annotated DCE-MR series and\ntested on another 19 annotated DCE-MR series. The performance of the three\ninput configurations for both networks is evaluated against manual annotations.\nThe results show that both neural networks perform better when the separate\nphases of the DCE-MR series are used as channels of an input image in\ncomparison to one phase as input image or the separate phases as input images.\nNo significant difference between the performances of the two network\narchitectures was found for the separate phases as channels of an input image.\n", "versions": [{"version": "v1", "created": "Thu, 22 Aug 2019 08:27:28 GMT"}], "update_date": "2019-08-23", "authors_parsed": [["Jansen", "Mari\u00eblle J. A.", ""], ["Kuijf", "Hugo J.", ""], ["Pluim", "Josien P. W.", ""]]}, {"id": "1908.08254", "submitter": "Mari\\\"elle Jansen", "authors": "Mari\\\"elle J.A. Jansen, Wouter B. Veldhuis, Maarten S. van Leeuwen,\n  Josien P.W. Pluim", "title": "Motion correction of dynamic contrast enhanced MRI of the liver", "comments": null, "journal-ref": null, "doi": "10.1117/12.2253842", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Motion correction of dynamic contrast enhanced magnetic resonance images\n(DCE-MRI) is a challenging task, due to changes in image appearance. In this\nstudy a groupwise registration, using a principle component analysis (PCA)\nbased metric,1 is evaluated for clinical DCE MRI of the liver. The groupwise\nregistration transforms the images to a common space, rather than to a\nreference volume as conventional pairwise methods do, and computes the\nsimilarity metric on all volumes simultaneously. This groupwise registration\nmethod is compared to a pairwise approach using a mutual information metric.\nClinical DCE MRI of the abdomen of eight patients were included. Per patient\none lesion in the liver was manually segmented in all temporal images (N=16).\nThe registered images were compared for accuracy, spatial and temporal\nsmoothness after transformation, and lesion volume change. Compared to a\npairwise method or no registration, groupwise registration provided better\nalignment. In our recently started clinical study groupwise registered clinical\nDCE MRI of the abdomen of nine patients were scored by three radiologists.\nGroupwise registration increased the assessed quality of alignment. The gain in\nreading time for the radiologist was estimated to vary from no difference to\nalmost a minute. A slight increase in reader confidence was also observed.\nRegistration had no added value for images with little motion. In conclusion,\nthe groupwise registration of DCE MR images results in better alignment than\nachieved by pairwise registration, which is beneficial for clinical assessment.\n", "versions": [{"version": "v1", "created": "Thu, 22 Aug 2019 08:39:51 GMT"}], "update_date": "2019-08-23", "authors_parsed": [["Jansen", "Mari\u00eblle J. A.", ""], ["Veldhuis", "Wouter B.", ""], ["van Leeuwen", "Maarten S.", ""], ["Pluim", "Josien P. W.", ""]]}, {"id": "1908.08279", "submitter": "Xiqi Yang", "authors": "Xiqi Yang, Qingfeng Zhang, Zhan Li", "title": "Contour Detection in Cassini ISS images based on Hierarchical Extreme\n  Learning Machine and Dense Conditional Random Field", "comments": null, "journal-ref": null, "doi": "10.1088/1674-4527/20/1/11", "report-no": null, "categories": "astro-ph.IM cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In Cassini ISS (Imaging Science Subsystem) images, contour detection is often\nperformed on disk-resolved object to accurately locate their center. Thus, the\ncontour detection is a key problem. Traditional edge detection methods, such as\nCanny and Roberts, often extract the contour with too much interior details and\nnoise. Although the deep convolutional neural network has been applied\nsuccessfully in many image tasks, such as classification and object detection,\nit needs more time and computer resources. In the paper, a contour detection\nalgorithm based on H-ELM (Hierarchical Extreme Learning Machine) and DenseCRF\n(Dense Conditional Random Field) is proposed for Cassini ISS images. The\nexperimental results show that this algorithm's performance is better than both\ntraditional machine learning methods such as SVM, ELM and even deep\nconvolutional neural network. And the extracted contour is closer to the actual\ncontour. Moreover, it can be trained and tested quickly on the general\nconfiguration of PC, so can be applied to contour detection for Cassini ISS\nimages.\n", "versions": [{"version": "v1", "created": "Thu, 22 Aug 2019 09:33:30 GMT"}], "update_date": "2020-02-05", "authors_parsed": [["Yang", "Xiqi", ""], ["Zhang", "Qingfeng", ""], ["Li", "Zhan", ""]]}, {"id": "1908.08289", "submitter": "Jiahao Lin", "authors": "Jiahao Lin, Gim Hee Lee", "title": "Trajectory Space Factorization for Deep Video-Based 3D Human Pose\n  Estimation", "comments": "13 pages, 5 figures. Accepted in BMVC 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing deep learning approaches on 3d human pose estimation for videos are\neither based on Recurrent or Convolutional Neural Networks (RNNs or CNNs).\nHowever, RNN-based frameworks can only tackle sequences with limited frames\nbecause sequential models are sensitive to bad frames and tend to drift over\nlong sequences. Although existing CNN-based temporal frameworks attempt to\naddress the sensitivity and drift problems by concurrently processing all input\nframes in the sequence, the existing state-of-the-art CNN-based framework is\nlimited to 3d pose estimation of a single frame from a sequential input. In\nthis paper, we propose a deep learning-based framework that utilizes matrix\nfactorization for sequential 3d human poses estimation. Our approach processes\nall input frames concurrently to avoid the sensitivity and drift problems, and\nyet outputs the 3d pose estimates for every frame in the input sequence. More\nspecifically, the 3d poses in all frames are represented as a motion matrix\nfactorized into a trajectory bases matrix and a trajectory coefficient matrix.\nThe trajectory bases matrix is precomputed from matrix factorization approaches\nsuch as Singular Value Decomposition (SVD) or Discrete Cosine Transform (DCT),\nand the problem of sequential 3d pose estimation is reduced to training a deep\nnetwork to regress the trajectory coefficient matrix. We demonstrate the\neffectiveness of our framework on long sequences by achieving state-of-the-art\nperformances on multiple benchmark datasets. Our source code is available at:\nhttps://github.com/jiahaoLjh/trajectory-pose-3d.\n", "versions": [{"version": "v1", "created": "Thu, 22 Aug 2019 10:03:30 GMT"}], "update_date": "2019-08-23", "authors_parsed": [["Lin", "Jiahao", ""], ["Lee", "Gim Hee", ""]]}, {"id": "1908.08297", "submitter": "Jiaxing Zhao", "authors": "Jia-Xing Zhao and Jiangjiang Liu and Den-Ping Fan and Yang Cao and\n  Jufeng Yang and Ming-Ming Cheng", "title": "EGNet:Edge Guidance Network for Salient Object Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Fully convolutional neural networks (FCNs) have shown their advantages in the\nsalient object detection task. However, most existing FCNs-based methods still\nsuffer from coarse object boundaries. In this paper, to solve this problem, we\nfocus on the complementarity between salient edge information and salient\nobject information. Accordingly, we present an edge guidance network (EGNet)\nfor salient object detection with three steps to simultaneously model these two\nkinds of complementary information in a single network. In the first step, we\nextract the salient object features by a progressive fusion way. In the second\nstep, we integrate the local edge information and global location information\nto obtain the salient edge features. Finally, to sufficiently leverage these\ncomplementary features, we couple the same salient edge features with salient\nobject features at various resolutions. Benefiting from the rich edge\ninformation and location information in salient edge features, the fused\nfeatures can help locate salient objects, especially their boundaries more\naccurately. Experimental results demonstrate that the proposed method performs\nfavorably against the state-of-the-art methods on six widely used datasets\nwithout any pre-processing and post-processing. The source code is available at\nhttp: //mmcheng.net/egnet/.\n", "versions": [{"version": "v1", "created": "Thu, 22 Aug 2019 10:36:36 GMT"}], "update_date": "2019-08-23", "authors_parsed": [["Zhao", "Jia-Xing", ""], ["Liu", "Jiangjiang", ""], ["Fan", "Den-Ping", ""], ["Cao", "Yang", ""], ["Yang", "Jufeng", ""], ["Cheng", "Ming-Ming", ""]]}, {"id": "1908.08307", "submitter": "G\\\"okhan \\\"Ozbulak", "authors": "G\\\"okhan \\\"Ozbulak", "title": "Image Colorization By Capsule Networks", "comments": "Accepted to New Trends in Image Restoration and Enhancement(NTIRE)\n  Workshop at CVPR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, a simple topology of Capsule Network (CapsNet) is investigated\nfor the problem of image colorization. The generative and segmentation\ncapabilities of the original CapsNet topology, which is proposed for image\nclassification problem, is leveraged for the colorization of the images by\nmodifying the network as follows:1) The original CapsNet model is adapted to\nmap the grayscale input to the output in the CIE Lab colorspace, 2) The feature\ndetector part of the model is updated by using deeper feature layers inherited\nfrom VGG-19 pre-trained model with weights in order to transfer low-level image\nrepresentation capability to this model, 3) The margin loss function is\nmodified as Mean Squared Error (MSE) loss to minimize the\nimage-to-imagemapping. The resulting CapsNet model is named as Colorizer\nCapsule Network (ColorCapsNet).The performance of the ColorCapsNet is evaluated\non the DIV2K dataset and promising results are obtained to investigate Capsule\nNetworks further for image colorization problem.\n", "versions": [{"version": "v1", "created": "Thu, 22 Aug 2019 11:03:14 GMT"}], "update_date": "2019-08-23", "authors_parsed": [["\u00d6zbulak", "G\u00f6khan", ""]]}, {"id": "1908.08331", "submitter": "Dominique Beaini", "authors": "Dominique Beaini, Sofiane Achiche, Alexandre Duperr\\'e, Maxime Raison", "title": "Deep Green Function Convolution for Improving Saliency in Convolutional\n  Neural Networks", "comments": "15 pages, 11 figures", "journal-ref": null, "doi": "10.1007/s00371-020-01795-8", "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Current saliency methods require to learn large scale regional features using\nsmall convolutional kernels, which is not possible with a simple feed-forward\nnetwork. Some methods solve this problem by using segmentation into superpixels\nwhile others downscale the image through the network and rescale it back to its\noriginal size. The objective of this paper is to show that saliency\nconvolutional neural networks (CNN) can be improved by using a Green's function\nconvolution (GFC) to extrapolate edges features into salient regions. The GFC\nacts as a gradient integrator, allowing to produce saliency features by filling\nthin edges directly inside the CNN. Hence, we propose the gradient integration\nand sum (GIS) layer that combines the edges features with the saliency\nfeatures. Using the HED and DSS architecture, we demonstrated that adding a GIS\nlayer near the network's output allows to reduce the sensitivity to the\nparameter initialization, to reduce the overfitting and to improve the\nrepeatability of the training. By simply adding a GIS layer to the\nstate-of-the-art DSS model, there is an absolute increase of 1.6% for the\nF-measure on the DUT-OMRON dataset, with only 10ms of additional computation\ntime. The GIS layer further allows the network to perform significantly better\nin the case of highly noisy images or low-brightness images. In fact, we\nobserved an F-measure improvement of 5.2% when noise was added to the dataset\nand 2.8% when the brightness was reduced. Since the GIS layer is model\nagnostic, it can be implemented into different fully convolutional networks. A\nmajor contribution of the current work is the first implementation of Green's\nfunction convolution inside a neural network, which allows the network to\noperate in the feature domain and in the gradient domain at the same time, thus\nimproving the regional representation via edge filling.\n", "versions": [{"version": "v1", "created": "Thu, 22 Aug 2019 12:14:26 GMT"}, {"version": "v2", "created": "Fri, 15 Nov 2019 04:07:46 GMT"}], "update_date": "2020-01-22", "authors_parsed": [["Beaini", "Dominique", ""], ["Achiche", "Sofiane", ""], ["Duperr\u00e9", "Alexandre", ""], ["Raison", "Maxime", ""]]}, {"id": "1908.08344", "submitter": "Tsung-Han Wu", "authors": "Yu-Kai Huang, Tsung-Han Wu, Yueh-Cheng Liu, Winston H. Hsu", "title": "Indoor Depth Completion with Boundary Consistency and Self-Attention", "comments": "Accepted by ICCVW (RLQ) 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Depth estimation features are helpful for 3D recognition. Commodity-grade\ndepth cameras are able to capture depth and color image in real-time. However,\nglossy, transparent or distant surface cannot be scanned properly by the\nsensor. As a result, enhancement and restoration from sensing depth is an\nimportant task. Depth completion aims at filling the holes that sensors fail to\ndetect, which is still a complex task for machine to learn. Traditional\nhand-tuned methods have reached their limits, while neural network based\nmethods tend to copy and interpolate the output from surrounding depth values.\nThis leads to blurred boundaries, and structures of the depth map are lost.\nConsequently, our main work is to design an end-to-end network improving\ncompletion depth maps while maintaining edge clarity. We utilize self-attention\nmechanism, previously used in image inpainting fields, to extract more useful\ninformation in each layer of convolution so that the complete depth map is\nenhanced. In addition, we propose boundary consistency concept to enhance the\ndepth map quality and structure. Experimental results validate the\neffectiveness of our self-attention and boundary consistency schema, which\noutperforms previous state-of-the-art depth completion work on Matterport3D\ndataset. Our code is publicly available at\nhttps://github.com/patrickwu2/Depth-Completion\n", "versions": [{"version": "v1", "created": "Thu, 22 Aug 2019 12:58:43 GMT"}, {"version": "v2", "created": "Wed, 23 Oct 2019 09:08:09 GMT"}], "update_date": "2019-10-24", "authors_parsed": [["Huang", "Yu-Kai", ""], ["Wu", "Tsung-Han", ""], ["Liu", "Yueh-Cheng", ""], ["Hsu", "Winston H.", ""]]}, {"id": "1908.08413", "submitter": "Jindong Gu", "authors": "Jindong Gu and Volker Tresp", "title": "Saliency Methods for Explaining Adversarial Attacks", "comments": null, "journal-ref": "Human-Centric Machine Learning, NeurIPS 2019 Workshop", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The classification decisions of neural networks can be misled by small\nimperceptible perturbations. This work aims to explain the misled\nclassifications using saliency methods. The idea behind saliency methods is to\nexplain the classification decisions of neural networks by creating so-called\nsaliency maps. Unfortunately, a number of recent publications have shown that\nmany of the proposed saliency methods do not provide insightful explanations. A\nprominent example is Guided Backpropagation (GuidedBP), which simply performs\n(partial) image recovery. However, our numerical analysis shows the saliency\nmaps created by GuidedBP do indeed contain class-discriminative information. We\npropose a simple and efficient way to enhance the saliency maps. The proposed\nenhanced GuidedBP shows the state-of-the-art performance to explain adversary\nclassifications.\n", "versions": [{"version": "v1", "created": "Thu, 22 Aug 2019 14:44:02 GMT"}, {"version": "v2", "created": "Wed, 28 Aug 2019 12:58:27 GMT"}, {"version": "v3", "created": "Wed, 2 Oct 2019 11:14:16 GMT"}, {"version": "v4", "created": "Mon, 21 Oct 2019 11:36:36 GMT"}], "update_date": "2019-10-22", "authors_parsed": [["Gu", "Jindong", ""], ["Tresp", "Volker", ""]]}, {"id": "1908.08431", "submitter": "Kerstin Kl\\\"aser", "authors": "Kerstin Kl\\\"aser, Thomas Varsavsky, Pawel Markiewicz, Tom Vercauteren,\n  David Atkinson, Kris Thielemans, Brian Hutton, M Jorge Cardoso, Sebastien\n  Ourselin", "title": "Improved MR to CT synthesis for PET/MR attenuation correction using\n  Imitation Learning", "comments": "Aceppted at SASHIMI2019", "journal-ref": null, "doi": "10.1007/978-3-030-32778-1_2", "report-no": null, "categories": "eess.IV cs.CV cs.LG physics.med-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ability to synthesise Computed Tomography images - commonly known as\npseudo CT, or pCT - from MRI input data is commonly assessed using an\nintensity-wise similarity, such as an L2-norm between the ground truth CT and\nthe pCT. However, given that the ultimate purpose is often to use the pCT as an\nattenuation map ($\\mu$-map) in Positron Emission Tomography Magnetic Resonance\nImaging (PET/MRI), minimising the error between pCT and CT is not necessarily\noptimal. The main objective should be to predict a pCT that, when used as\n$\\mu$-map, reconstructs a pseudo PET (pPET) which is as close as possible to\nthe gold standard PET. To this end, we propose a novel multi-hypothesis deep\nlearning framework that generates pCTs by minimising a combination of the\npixel-wise error between pCT and CT and a proposed metric-loss that itself is\nrepresented by a convolutional neural network (CNN) and aims to minimise\nsubsequent PET residuals. The model is trained on a database of 400 paired\nMR/CT/PET image slices. Quantitative results show that the network generates\npCTs that seem less accurate when evaluating the Mean Absolute Error on the pCT\n(69.68HU) compared to a baseline CNN (66.25HU), but lead to significant\nimprovement in the PET reconstruction - 115a.u. compared to baseline 140a.u.\n", "versions": [{"version": "v1", "created": "Wed, 21 Aug 2019 12:47:29 GMT"}, {"version": "v2", "created": "Tue, 27 Aug 2019 19:27:40 GMT"}], "update_date": "2020-04-15", "authors_parsed": [["Kl\u00e4ser", "Kerstin", ""], ["Varsavsky", "Thomas", ""], ["Markiewicz", "Pawel", ""], ["Vercauteren", "Tom", ""], ["Atkinson", "David", ""], ["Thielemans", "Kris", ""], ["Hutton", "Brian", ""], ["Cardoso", "M Jorge", ""], ["Ourselin", "Sebastien", ""]]}, {"id": "1908.08433", "submitter": "Deng-Ping Fan", "authors": "Deng-Ping Fan, ShengChuan Zhang, Yu-Huan Wu, Yun Liu, Ming-Ming Cheng,\n  Bo Ren, Paul L. Rosin, Rongrong Ji", "title": "Scoot: A Perceptual Metric for Facial Sketches", "comments": "Code & dataset:http://mmcheng.net/scoot/, 11 pages, ICCV 2019, First\n  one good evaluation metric for facial sketh that consistent with human\n  judgment. arXiv admin note: text overlap with arXiv:1804.02975", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human visual system has the strong ability to quick assess the perceptual\nsimilarity between two facial sketches. However, existing two widely-used\nfacial sketch metrics, e.g., FSIM and SSIM fail to address this perceptual\nsimilarity in this field. Recent study in facial modeling area has verified\nthat the inclusion of both structure and texture has a significant positive\nbenefit for face sketch synthesis (FSS). But which statistics are more\nimportant, and are helpful for their success? In this paper, we design a\nperceptual metric,called Structure Co-Occurrence Texture (Scoot), which\nsimultaneously considers the block-level spatial structure and co-occurrence\ntexture statistics. To test the quality of metrics, we propose three novel\nmeta-measures based on various reliable properties. Extensive experiments\ndemonstrate that our Scoot metric exceeds the performance of prior work.\nBesides, we built the first large scale (152k judgments) human-perception-based\nsketch database that can evaluate how well a metric is consistent with human\nperception. Our results suggest that \"spatial structure\" and \"co-occurrence\ntexture\" are two generally applicable perceptual features in face sketch\nsynthesis.\n", "versions": [{"version": "v1", "created": "Wed, 21 Aug 2019 11:55:47 GMT"}, {"version": "v2", "created": "Wed, 4 Sep 2019 06:06:31 GMT"}], "update_date": "2019-09-05", "authors_parsed": [["Fan", "Deng-Ping", ""], ["Zhang", "ShengChuan", ""], ["Wu", "Yu-Huan", ""], ["Liu", "Yun", ""], ["Cheng", "Ming-Ming", ""], ["Ren", "Bo", ""], ["Rosin", "Paul L.", ""], ["Ji", "Rongrong", ""]]}, {"id": "1908.08453", "submitter": "Abdelrahman Abdelhamed", "authors": "Abdelrahman Abdelhamed, Marcus A. Brubaker, and Michael S. Brown", "title": "Noise Flow: Noise Modeling with Conditional Normalizing Flows", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Modeling and synthesizing image noise is an important aspect in many computer\nvision applications. The long-standing additive white Gaussian and\nheteroscedastic (signal-dependent) noise models widely used in the literature\nprovide only a coarse approximation of real sensor noise. This paper introduces\nNoise Flow, a powerful and accurate noise model based on recent normalizing\nflow architectures. Noise Flow combines well-established basic parametric noise\nmodels (e.g., signal-dependent noise) with the flexibility and expressiveness\nof normalizing flow networks. The result is a single, comprehensive, compact\nnoise model containing fewer than 2500 parameters yet able to represent\nmultiple cameras and gain factors. Noise Flow dramatically outperforms existing\nnoise models, with 0.42 nats/pixel improvement over the camera-calibrated noise\nlevel functions, which translates to 52% improvement in the likelihood of\nsampled noise. Noise Flow represents the first serious attempt to go beyond\nsimple parametric models to one that leverages the power of deep learning and\ndata-driven noise distributions.\n", "versions": [{"version": "v1", "created": "Thu, 22 Aug 2019 15:30:32 GMT"}], "update_date": "2019-08-23", "authors_parsed": [["Abdelhamed", "Abdelrahman", ""], ["Brubaker", "Marcus A.", ""], ["Brown", "Michael S.", ""]]}, {"id": "1908.08466", "submitter": "Xiao-Yun Zhou", "authors": "Xiao-Yun Zhou, Peichao Li, Zhao-Yang Wang, Guang-Zhong Yang", "title": "U-Net Training with Instance-Layer Normalization", "comments": "8 pages, 3 figures, accepted by MICCAI-MMMI 2019 workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Normalization layers are essential in a Deep Convolutional Neural Network\n(DCNN). Various normalization methods have been proposed. The statistics used\nto normalize the feature maps can be computed at batch, channel, or instance\nlevel. However, in most of existing methods, the normalization for each layer\nis fixed. Batch-Instance Normalization (BIN) is one of the first proposed\nmethods that combines two different normalization methods and achieve diverse\nnormalization for different layers. However, two potential issues exist in BIN:\nfirst, the Clip function is not differentiable at input values of 0 and 1;\nsecond, the combined feature map is not with a normalized distribution which is\nharmful for signal propagation in DCNN. In this paper, an Instance-Layer\nNormalization (ILN) layer is proposed by using the Sigmoid function for the\nfeature map combination, and cascading group normalization. The performance of\nILN is validated on image segmentation of the Right Ventricle (RV) and Left\nVentricle (LV) using U-Net as the network architecture. The results show that\nthe proposed ILN outperforms previous traditional and popular normalization\nmethods with noticeable accuracy improvements for most validations, supporting\nthe effectiveness of the proposed ILN.\n", "versions": [{"version": "v1", "created": "Wed, 21 Aug 2019 11:24:25 GMT"}, {"version": "v2", "created": "Sun, 25 Aug 2019 14:18:15 GMT"}], "update_date": "2019-08-27", "authors_parsed": [["Zhou", "Xiao-Yun", ""], ["Li", "Peichao", ""], ["Wang", "Zhao-Yang", ""], ["Yang", "Guang-Zhong", ""]]}, {"id": "1908.08498", "submitter": "Evangelos Kazakos", "authors": "Evangelos Kazakos, Arsha Nagrani, Andrew Zisserman, Dima Damen", "title": "EPIC-Fusion: Audio-Visual Temporal Binding for Egocentric Action\n  Recognition", "comments": "Accepted for presentation at ICCV 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We focus on multi-modal fusion for egocentric action recognition, and propose\na novel architecture for multi-modal temporal-binding, i.e. the combination of\nmodalities within a range of temporal offsets. We train the architecture with\nthree modalities -- RGB, Flow and Audio -- and combine them with mid-level\nfusion alongside sparse temporal sampling of fused representations. In contrast\nwith previous works, modalities are fused before temporal aggregation, with\nshared modality and fusion weights over time. Our proposed architecture is\ntrained end-to-end, outperforming individual modalities as well as late-fusion\nof modalities.\n  We demonstrate the importance of audio in egocentric vision, on per-class\nbasis, for identifying actions as well as interacting objects. Our method\nachieves state of the art results on both the seen and unseen test sets of the\nlargest egocentric dataset: EPIC-Kitchens, on all metrics using the public\nleaderboard.\n", "versions": [{"version": "v1", "created": "Thu, 22 Aug 2019 17:07:04 GMT"}], "update_date": "2019-08-23", "authors_parsed": [["Kazakos", "Evangelos", ""], ["Nagrani", "Arsha", ""], ["Zisserman", "Andrew", ""], ["Damen", "Dima", ""]]}, {"id": "1908.08506", "submitter": "Zhan Xu", "authors": "Zhan Xu, Yang Zhou, Evangelos Kalogerakis, Karan Singh", "title": "Predicting Animation Skeletons for 3D Articulated Models via Volumetric\n  Nets", "comments": "3DV 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a learning method for predicting animation skeletons for input 3D\nmodels of articulated characters. In contrast to previous approaches that fit\npre-defined skeleton templates or predict fixed sets of joints, our method\nproduces an animation skeleton tailored for the structure and geometry of the\ninput 3D model. Our architecture is based on a stack of hourglass modules\ntrained on a large dataset of 3D rigged characters mined from the web. It\noperates on the volumetric representation of the input 3D shapes augmented with\ngeometric shape features that provide additional cues for joint and bone\nlocations. Our method also enables intuitive user control of the\nlevel-of-detail for the output skeleton. Our evaluation demonstrates that our\napproach predicts animation skeletons that are much more similar to the ones\ncreated by humans compared to several alternatives and baselines.\n", "versions": [{"version": "v1", "created": "Thu, 22 Aug 2019 17:26:46 GMT"}], "update_date": "2019-08-23", "authors_parsed": [["Xu", "Zhan", ""], ["Zhou", "Yang", ""], ["Kalogerakis", "Evangelos", ""], ["Singh", "Karan", ""]]}, {"id": "1908.08520", "submitter": "Zhiqiang Shen", "authors": "Zhiqiang Shen and Zhankui He and Wanyun Cui and Jiahui Yu and Yutong\n  Zheng and Chenchen Zhu and Marios Savvides", "title": "Adversarial-Based Knowledge Distillation for Multi-Model Ensemble and\n  Noisy Data Refinement", "comments": "This is an extended version of our previous conference paper\n  arXiv:1812.02425", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generic Image recognition is a fundamental and fairly important visual\nproblem in computer vision. One of the major challenges of this task lies in\nthe fact that single image usually has multiple objects inside while the labels\nare still one-hot, another one is noisy and sometimes missing labels when\nannotated by humans. In this paper, we focus on tackling these challenges\naccompanying with two different image recognition problems: multi-model\nensemble and noisy data recognition with a unified framework. As is well-known,\nusually the best performing deep neural models are ensembles of multiple\nbase-level networks, as it can mitigate the variation or noise containing in\nthe dataset. Unfortunately, the space required to store these many networks,\nand the time required to execute them at runtime, prohibit their use in\napplications where test sets are large (e.g., ImageNet). In this paper, we\npresent a method for compressing large, complex trained ensembles into a single\nnetwork, where the knowledge from a variety of trained deep neural networks\n(DNNs) is distilled and transferred to a single DNN. In order to distill\ndiverse knowledge from different trained (teacher) models, we propose to use\nadversarial-based learning strategy where we define a block-wise training loss\nto guide and optimize the predefined student network to recover the knowledge\nin teacher models, and to promote the discriminator network to distinguish\nteacher vs. student features simultaneously. Extensive experiments on\nCIFAR-10/100, SVHN, ImageNet and iMaterialist Challenge Dataset demonstrate the\neffectiveness of our MEAL method. On ImageNet, our ResNet-50 based MEAL\nachieves top-1/5 21.79%/5.99% val error, which outperforms the original model\nby 2.06%/1.14%. On iMaterialist Challenge Dataset, our MEAL obtains a\nremarkable improvement of top-3 1.15% (official evaluation metric) on a strong\nbaseline model of ResNet-101.\n", "versions": [{"version": "v1", "created": "Thu, 22 Aug 2019 17:51:16 GMT"}], "update_date": "2019-08-23", "authors_parsed": [["Shen", "Zhiqiang", ""], ["He", "Zhankui", ""], ["Cui", "Wanyun", ""], ["Yu", "Jiahui", ""], ["Zheng", "Yutong", ""], ["Zhu", "Chenchen", ""], ["Savvides", "Marios", ""]]}, {"id": "1908.08522", "submitter": "Yufei Ye", "authors": "Yufei Ye and Maneesh Singh and Abhinav Gupta and Shubham Tulsiani", "title": "Compositional Video Prediction", "comments": "accepted to ICCV19", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an approach for pixel-level future prediction given an input image\nof a scene. We observe that a scene is comprised of distinct entities that\nundergo motion and present an approach that operationalizes this insight. We\nimplicitly predict future states of independent entities while reasoning about\ntheir interactions, and compose future video frames using these predicted\nstates. We overcome the inherent multi-modality of the task using a global\ntrajectory-level latent random variable, and show that this allows us to sample\ndiverse and plausible futures. We empirically validate our approach against\nalternate representations and ways of incorporating multi-modality. We examine\ntwo datasets, one comprising of stacked objects that may fall, and the other\ncontaining videos of humans performing activities in a gym, and show that our\napproach allows realistic stochastic video prediction across these diverse\nsettings. See https://judyye.github.io/CVP/ for video predictions.\n", "versions": [{"version": "v1", "created": "Thu, 22 Aug 2019 17:55:58 GMT"}], "update_date": "2019-08-23", "authors_parsed": [["Ye", "Yufei", ""], ["Singh", "Maneesh", ""], ["Gupta", "Abhinav", ""], ["Tulsiani", "Shubham", ""]]}, {"id": "1908.08527", "submitter": "Tanmay Gupta", "authors": "Tanmay Gupta, Alexander Schwing and Derek Hoiem", "title": "ViCo: Word Embeddings from Visual Co-occurrences", "comments": "Accepted to ICCV 2019. Project Page: http://tanmaygupta.info/vico/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose to learn word embeddings from visual co-occurrences. Two words\nco-occur visually if both words apply to the same image or image region.\nSpecifically, we extract four types of visual co-occurrences between object and\nattribute words from large-scale, textually-annotated visual databases like\nVisualGenome and ImageNet. We then train a multi-task log-bilinear model that\ncompactly encodes word \"meanings\" represented by each co-occurrence type into a\nsingle visual word-vector. Through unsupervised clustering, supervised\npartitioning, and a zero-shot-like generalization analysis we show that our\nword embeddings complement text-only embeddings like GloVe by better\nrepresenting similarities and differences between visual concepts that are\ndifficult to obtain from text corpora alone. We further evaluate our embeddings\non five downstream applications, four of which are vision-language tasks.\nAugmenting GloVe with our embeddings yields gains on all tasks. We also find\nthat random embeddings perform comparably to learned embeddings on all\nsupervised vision-language tasks, contrary to conventional wisdom.\n", "versions": [{"version": "v1", "created": "Thu, 22 Aug 2019 17:58:52 GMT"}], "update_date": "2019-08-23", "authors_parsed": [["Gupta", "Tanmay", ""], ["Schwing", "Alexander", ""], ["Hoiem", "Derek", ""]]}, {"id": "1908.08529", "submitter": "Jyoti Aneja", "authors": "Jyoti Aneja, Harsh Agrawal, Dhruv Batra, Alexander Schwing", "title": "Sequential Latent Spaces for Modeling the Intention During Diverse Image\n  Captioning", "comments": "Accepted to ICCV 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Diverse and accurate vision+language modeling is an important goal to retain\ncreative freedom and maintain user engagement. However, adequately capturing\nthe intricacies of diversity in language models is challenging. Recent works\ncommonly resort to latent variable models augmented with more or less\nsupervision from object detectors or part-of-speech tags. Common to all those\nmethods is the fact that the latent variable either only initializes the\nsentence generation process or is identical across the steps of generation.\nBoth methods offer no fine-grained control. To address this concern, we propose\nSeq-CVAE which learns a latent space for every word position. We encourage this\ntemporal latent space to capture the 'intention' about how to complete the\nsentence by mimicking a representation which summarizes the future. We\nillustrate the efficacy of the proposed approach to anticipate the sentence\ncontinuation on the challenging MSCOCO dataset, significantly improving\ndiversity metrics compared to baselines while performing on par w.r.t sentence\nquality.\n", "versions": [{"version": "v1", "created": "Thu, 22 Aug 2019 17:59:08 GMT"}], "update_date": "2019-08-23", "authors_parsed": [["Aneja", "Jyoti", ""], ["Agrawal", "Harsh", ""], ["Batra", "Dhruv", ""], ["Schwing", "Alexander", ""]]}, {"id": "1908.08530", "submitter": "Yue Cao", "authors": "Weijie Su, Xizhou Zhu, Yue Cao, Bin Li, Lewei Lu, Furu Wei, Jifeng Dai", "title": "VL-BERT: Pre-training of Generic Visual-Linguistic Representations", "comments": "Accepted by ICLR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new pre-trainable generic representation for visual-linguistic\ntasks, called Visual-Linguistic BERT (VL-BERT for short). VL-BERT adopts the\nsimple yet powerful Transformer model as the backbone, and extends it to take\nboth visual and linguistic embedded features as input. In it, each element of\nthe input is either of a word from the input sentence, or a region-of-interest\n(RoI) from the input image. It is designed to fit for most of the\nvisual-linguistic downstream tasks. To better exploit the generic\nrepresentation, we pre-train VL-BERT on the massive-scale Conceptual Captions\ndataset, together with text-only corpus. Extensive empirical analysis\ndemonstrates that the pre-training procedure can better align the\nvisual-linguistic clues and benefit the downstream tasks, such as visual\ncommonsense reasoning, visual question answering and referring expression\ncomprehension. It is worth noting that VL-BERT achieved the first place of\nsingle model on the leaderboard of the VCR benchmark. Code is released at\n\\url{https://github.com/jackroos/VL-BERT}.\n", "versions": [{"version": "v1", "created": "Thu, 22 Aug 2019 17:59:30 GMT"}, {"version": "v2", "created": "Sat, 5 Oct 2019 11:18:38 GMT"}, {"version": "v3", "created": "Fri, 22 Nov 2019 09:42:53 GMT"}, {"version": "v4", "created": "Tue, 18 Feb 2020 02:59:17 GMT"}], "update_date": "2020-02-19", "authors_parsed": [["Su", "Weijie", ""], ["Zhu", "Xizhou", ""], ["Cao", "Yue", ""], ["Li", "Bin", ""], ["Lu", "Lewei", ""], ["Wei", "Furu", ""], ["Dai", "Jifeng", ""]]}, {"id": "1908.08584", "submitter": "Beinan Wang", "authors": "Beinan Wang, John Glossner, Daniel Iancu, Georgi N. Gaydadjiev", "title": "Feedbackward Decoding for Semantic Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL eess.IV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel approach for semantic segmentation that uses an encoder in\nthe reverse direction to decode. Many semantic segmentation networks adopt a\nfeedforward encoder-decoder architecture. Typically, an input is first\ndownsampled by the encoder to extract high-level semantic features and\ncontinues to be fed forward through the decoder module to recover low-level\nspatial clues. Our method works in an alternative direction that lets\ninformation flow backward from the last layer of the encoder towards the first.\nThe encoder performs encoding in the forward pass and the same network performs\ndecoding in the backward pass. Therefore, the encoder itself is also the\ndecoder. Compared to conventional encoder-decoder architectures, ours doesn't\nrequire additional layers for decoding and further reuses the encoder weights\nthereby reducing the total number of parameters required for processing. We\nshow by using only the 13 convolutional layers from VGG-16 plus one tiny\nclassification layer, our model significantly outperforms other frequently\ncited models that are also adapted from VGG-16. On the Cityscapes semantic\nsegmentation benchmark, our model uses 50.0% less parameters than SegNet and\nachieves an 18.1% higher \"IoU class\" score; it uses 28.3% less parameters than\nDeepLab LargeFOV and the achieved \"IoU class\" score is 3.9% higher; it uses\n89.1% fewer parameters than FCN-8s and the achieved \"IoU class\" score is 3.1%\nhigher. Our code will be publicly available on Github later.\n", "versions": [{"version": "v1", "created": "Thu, 22 Aug 2019 20:29:05 GMT"}], "update_date": "2019-08-28", "authors_parsed": [["Wang", "Beinan", ""], ["Glossner", "John", ""], ["Iancu", "Daniel", ""], ["Gaydadjiev", "Georgi N.", ""]]}, {"id": "1908.08588", "submitter": "Antonio Garcia-Uceda Juarez", "authors": "Antonio Garcia-Uceda Juarez, Raghavendra Selvan, Zaigham Saghir and\n  Marleen de Bruijne", "title": "A joint 3D UNet-Graph Neural Network-based method for Airway\n  Segmentation from chest CTs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an end-to-end deep learning segmentation method by combining a 3D\nUNet architecture with a graph neural network (GNN) model. In this approach,\nthe convolutional layers at the deepest level of the UNet are replaced by a\nGNN-based module with a series of graph convolutions. The dense feature maps at\nthis level are transformed into a graph input to the GNN module. The\nincorporation of graph convolutions in the UNet provides nodes in the graph\nwith information that is based on node connectivity, in addition to the local\nfeatures learnt through the downsampled paths. This information can help\nimprove segmentation decisions. By stacking several graph convolution layers,\nthe nodes can access higher order neighbourhood information without substantial\nincrease in computational expense. We propose two types of node connectivity in\nthe graph adjacency: i) one predefined and based on a regular node\nneighbourhood, and ii) one dynamically computed during training and using the\nnearest neighbour nodes in the feature space. We have applied this method to\nthe task of segmenting the airway tree from chest CT scans. Experiments have\nbeen performed on 32 CTs from the Danish Lung Cancer Screening Trial dataset.\nWe evaluate the performance of the UNet-GNN models with two types of graph\nadjacency and compare it with the baseline UNet.\n", "versions": [{"version": "v1", "created": "Thu, 22 Aug 2019 20:32:01 GMT"}], "update_date": "2019-08-26", "authors_parsed": [["Juarez", "Antonio Garcia-Uceda", ""], ["Selvan", "Raghavendra", ""], ["Saghir", "Zaigham", ""], ["de Bruijne", "Marleen", ""]]}, {"id": "1908.08589", "submitter": "Reuben Tan", "authors": "Reuben Tan, Mariya I. Vasileva, Kate Saenko, Bryan A. Plummer", "title": "Learning Similarity Conditions Without Explicit Supervision", "comments": "Accepted at ICCV 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many real-world tasks require models to compare images along multiple\nsimilarity conditions (e.g. similarity in color, category or shape). Existing\nmethods often reason about these complex similarity relationships by learning\ncondition-aware embeddings. While such embeddings aid models in learning\ndifferent notions of similarity, they also limit their capability to generalize\nto unseen categories since they require explicit labels at test time. To\naddress this deficiency, we propose an approach that jointly learns\nrepresentations for the different similarity conditions and their contributions\nas a latent variable without explicit supervision. Comprehensive experiments\nacross three datasets, Polyvore-Outfits, Maryland-Polyvore and UT-Zappos50k,\ndemonstrate the effectiveness of our approach: our model outperforms the\nstate-of-the-art methods, even those that are strongly supervised with\npre-defined similarity conditions, on fill-in-the-blank, outfit compatibility\nprediction and triplet prediction tasks. Finally, we show that our model learns\ndifferent visually-relevant semantic sub-spaces that allow it to generalize\nwell to unseen categories.\n", "versions": [{"version": "v1", "created": "Thu, 22 Aug 2019 20:35:04 GMT"}], "update_date": "2019-08-26", "authors_parsed": [["Tan", "Reuben", ""], ["Vasileva", "Mariya I.", ""], ["Saenko", "Kate", ""], ["Plummer", "Bryan A.", ""]]}, {"id": "1908.08597", "submitter": "Danielle Bragg", "authors": "Danielle Bragg, Oscar Koller, Mary Bellard, Larwan Berke, Patrick\n  Boudrealt, Annelies Braffort, Naomi Caselli, Matt Huenerfauth, Hernisa\n  Kacorri, Tessa Verhoef, Christian Vogler, Meredith Ringel Morris", "title": "Sign Language Recognition, Generation, and Translation: An\n  Interdisciplinary Perspective", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.CY cs.GR cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Developing successful sign language recognition, generation, and translation\nsystems requires expertise in a wide range of fields, including computer\nvision, computer graphics, natural language processing, human-computer\ninteraction, linguistics, and Deaf culture. Despite the need for deep\ninterdisciplinary knowledge, existing research occurs in separate disciplinary\nsilos, and tackles separate portions of the sign language processing pipeline.\nThis leads to three key questions: 1) What does an interdisciplinary view of\nthe current landscape reveal? 2) What are the biggest challenges facing the\nfield? and 3) What are the calls to action for people working in the field? To\nhelp answer these questions, we brought together a diverse group of experts for\na two-day workshop. This paper presents the results of that interdisciplinary\nworkshop, providing key background that is often overlooked by computer\nscientists, a review of the state-of-the-art, a set of pressing challenges, and\na call to action for the research community.\n", "versions": [{"version": "v1", "created": "Thu, 22 Aug 2019 21:05:17 GMT"}], "update_date": "2019-08-26", "authors_parsed": [["Bragg", "Danielle", ""], ["Koller", "Oscar", ""], ["Bellard", "Mary", ""], ["Berke", "Larwan", ""], ["Boudrealt", "Patrick", ""], ["Braffort", "Annelies", ""], ["Caselli", "Naomi", ""], ["Huenerfauth", "Matt", ""], ["Kacorri", "Hernisa", ""], ["Verhoef", "Tessa", ""], ["Vogler", "Christian", ""], ["Morris", "Meredith Ringel", ""]]}, {"id": "1908.08628", "submitter": "Hieu Le", "authors": "Hieu Le and Dimitris Samaras", "title": "Shadow Removal via Shadow Image Decomposition", "comments": "ICCV 19 Poster. Higher resolution version is available in the project\n  homepage: www3.cs.stonybrook.edu/~cvl/projects/SID/index.html", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel deep learning method for shadow removal. Inspired by\nphysical models of shadow formation, we use a linear illumination\ntransformation to model the shadow effects in the image that allows the shadow\nimage to be expressed as a combination of the shadow-free image, the shadow\nparameters, and a matte layer. We use two deep networks, namely SP-Net and\nM-Net, to predict the shadow parameters and the shadow matte respectively. This\nsystem allows us to remove the shadow effects on the images. We train and test\nour framework on the most challenging shadow removal dataset (ISTD). Compared\nto the state-of-the-art method, our model achieves a 40% error reduction in\nterms of root mean square error (RMSE) for the shadow area, reducing RMSE from\n13.3 to 7.9. Moreover, we create an augmented ISTD dataset based on an image\ndecomposition system by modifying the shadow parameters to generate new\nsynthetic shadow images. Training our model on this new augmented ISTD dataset\nfurther lowers the RMSE on the shadow area to 7.4.\n", "versions": [{"version": "v1", "created": "Fri, 23 Aug 2019 00:51:32 GMT"}], "update_date": "2019-08-26", "authors_parsed": [["Le", "Hieu", ""], ["Samaras", "Dimitris", ""]]}, {"id": "1908.08631", "submitter": "Daiki Matsunaga", "authors": "Li Honghan, Daiki Matsunaga, Tsubasa S. Matsui, Hiroki Aosaki, Shinji\n  Deguchi", "title": "Image based cellular contractile force evaluation with small-world\n  network inspired CNN: SW-UNet", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.bio-ph cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an image-based cellular contractile force evaluation method using\na machine learning technique. We use a special substrate that exhibits wrinkles\nwhen cells grab the substrate and contract, and the wrinkles can be used to\nvisualize the force magnitude and direction. In order to extract wrinkles from\nthe microscope images, we develop a new CNN (convolutional neural network)\narchitecture SW-UNet (small-world U-Net), which is a CNN that reflects the\nconcept of the small-world network. The SW-UNet shows better performance in\nwrinkle segmentation task compared to other methods: the error (Euclidean\ndistance) of SW-UNet is 4.9 times smaller than 2D-FFT (fast Fourier transform)\nbased segmentation approach, and is 2.9 times smaller than U-Net. As a\ndemonstration, we compare the contractile force of U2OS (human osteosarcoma)\ncells and show that cells with a mutation in the KRAS oncogne show larger force\ncompared to the wild-type cells. Our new machine learning based algorithm\nprovides us an efficient, automated and accurate method to evaluate the cell\ncontractile force.\n", "versions": [{"version": "v1", "created": "Fri, 23 Aug 2019 01:47:06 GMT"}], "update_date": "2019-08-26", "authors_parsed": [["Honghan", "Li", ""], ["Matsunaga", "Daiki", ""], ["Matsui", "Tsubasa S.", ""], ["Aosaki", "Hiroki", ""], ["Deguchi", "Shinji", ""]]}, {"id": "1908.08652", "submitter": "Suraj Tripathi", "authors": "Abhay Kumar, Nishant Jain, Suraj Tripathi, Chirag Singh, Kamal Krishna", "title": "MTCNET: Multi-task Learning Paradigm for Crowd Count Estimation", "comments": "5 pages, 3 figures, Accepted in IEEE AVSS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a Multi-Task Learning (MTL) paradigm based deep neural network\narchitecture, called MTCNet (Multi-Task Crowd Network) for crowd density and\ncount estimation. Crowd count estimation is challenging due to the non-uniform\nscale variations and the arbitrary perspective of an individual image. The\nproposed model has two related tasks, with Crowd Density Estimation as the main\ntask and Crowd-Count Group Classification as the auxiliary task. The auxiliary\ntask helps in capturing the relevant scale-related information to improve the\nperformance of the main task. The main task model comprises two blocks: VGG-16\nfront-end for feature extraction and a dilated Convolutional Neural Network for\ndensity map generation. The auxiliary task model shares the same front-end as\nthe main task, followed by a CNN classifier. Our proposed network achieves 5.8%\nand 14.9% lower Mean Absolute Error (MAE) than the state-of-the-art methods on\nShanghaiTech dataset without using any data augmentation. Our model also\noutperforms with 10.5% lower MAE on UCF_CC_50 dataset.\n", "versions": [{"version": "v1", "created": "Fri, 23 Aug 2019 03:30:53 GMT"}], "update_date": "2019-08-26", "authors_parsed": [["Kumar", "Abhay", ""], ["Jain", "Nishant", ""], ["Tripathi", "Suraj", ""], ["Singh", "Chirag", ""], ["Krishna", "Kamal", ""]]}, {"id": "1908.08674", "submitter": "Debabrata Paul", "authors": "Debabrata Paul and Bidyut Baran Chaudhuri", "title": "A BLSTM Network for Printed Bengali OCR System with High Accuracy", "comments": "6 pages, 6 figures, This OCR system is available online at\n  https://banglaocr.nltr.org", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper presents a printed Bengali and English text OCR system developed\nby us using a single hidden BLSTM-CTC architecture having 128 units. Here, we\ndid not use any peephole connection and dropout in the BLSTM, which helped us\nin getting better accuracy. This architecture was trained by 47,720 text lines\nthat include English words also. When tested over 20 different Bengali fonts,\nit has produced character level accuracy of 99.32% and word level accuracy of\n96.65%. A good Indic multi script OCR system is also developed by Google. It\nsometimes recognizes a character of Bengali into the same character of a\nnon-Bengali script, especially Assamese, which has no distinction from Bengali,\nexcept for a few characters. For example, Bengali character for 'RA' is\nsometimes recognized as that of Assamese, mainly in conjunct consonant forms.\nOur OCR is free from such errors. This OCR system is available online at\nhttps://banglaocr.nltr.org\n", "versions": [{"version": "v1", "created": "Fri, 23 Aug 2019 05:45:27 GMT"}], "update_date": "2019-08-26", "authors_parsed": [["Paul", "Debabrata", ""], ["Chaudhuri", "Bidyut Baran", ""]]}, {"id": "1908.08681", "submitter": "Diganta Misra", "authors": "Diganta Misra", "title": "Mish: A Self Regularized Non-Monotonic Activation Function", "comments": "Accepted to BMVC 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose $\\textit{Mish}$, a novel self-regularized non-monotonic activation\nfunction which can be mathematically defined as: $f(x)=x\\tanh(softplus(x))$. As\nactivation functions play a crucial role in the performance and training\ndynamics in neural networks, we validated experimentally on several well-known\nbenchmarks against the best combinations of architectures and activation\nfunctions. We also observe that data augmentation techniques have a favorable\neffect on benchmarks like ImageNet-1k and MS-COCO across multiple\narchitectures. For example, Mish outperformed Leaky ReLU on YOLOv4 with a\nCSP-DarkNet-53 backbone on average precision ($AP_{50}^{val}$) by 2.1$\\%$ in\nMS-COCO object detection and ReLU on ResNet-50 on ImageNet-1k in Top-1 accuracy\nby $\\approx$1$\\%$ while keeping all other network parameters and\nhyperparameters constant. Furthermore, we explore the mathematical formulation\nof Mish in relation with the Swish family of functions and propose an intuitive\nunderstanding on how the first derivative behavior may be acting as a\nregularizer helping the optimization of deep neural networks. Code is publicly\navailable at https://github.com/digantamisra98/Mish.\n", "versions": [{"version": "v1", "created": "Fri, 23 Aug 2019 06:22:06 GMT"}, {"version": "v2", "created": "Wed, 2 Oct 2019 16:59:14 GMT"}, {"version": "v3", "created": "Thu, 13 Aug 2020 05:42:12 GMT"}], "update_date": "2020-08-14", "authors_parsed": [["Misra", "Diganta", ""]]}, {"id": "1908.08692", "submitter": "Lingbo Liu", "authors": "Lingbo Liu, Zhilin Qiu, Guanbin Li, Shufan Liu, Wanli Ouyang, Liang\n  Lin", "title": "Crowd Counting with Deep Structured Scale Integration Network", "comments": "Accepted to ICCV 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic estimation of the number of people in unconstrained crowded scenes\nis a challenging task and one major difficulty stems from the huge scale\nvariation of people. In this paper, we propose a novel Deep Structured Scale\nIntegration Network (DSSINet) for crowd counting, which addresses the scale\nvariation of people by using structured feature representation learning and\nhierarchically structured loss function optimization. Unlike conventional\nmethods which directly fuse multiple features with weighted average or\nconcatenation, we first introduce a Structured Feature Enhancement Module based\non conditional random fields (CRFs) to refine multiscale features mutually with\na message passing mechanism. In this module, each scale-specific feature is\nconsidered as a continuous random variable and passes complementary information\nto refine the features at other scales. Second, we utilize a Dilated Multiscale\nStructural Similarity loss to enforce our DSSINet to learn the local\ncorrelation of people's scales within regions of various size, thus yielding\nhigh-quality density maps. Extensive experiments on four challenging benchmarks\nwell demonstrate the effectiveness of our method. Specifically, our DSSINet\nachieves improvements of 9.5% error reduction on Shanghaitech dataset and 24.9%\non UCF-QNRF dataset against the state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Fri, 23 Aug 2019 06:59:36 GMT"}], "update_date": "2019-08-26", "authors_parsed": [["Liu", "Lingbo", ""], ["Qiu", "Zhilin", ""], ["Li", "Guanbin", ""], ["Liu", "Shufan", ""], ["Ouyang", "Wanli", ""], ["Lin", "Liang", ""]]}, {"id": "1908.08704", "submitter": "Shunkai Li", "authors": "Shunkai Li, Fei Xue, Xin Wang, Zike Yan, Hongbin Zha", "title": "Sequential Adversarial Learning for Self-Supervised Deep Visual Odometry", "comments": "Accept to ICCV 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a self-supervised learning framework for visual odometry (VO) that\nincorporates correlation of consecutive frames and takes advantage of\nadversarial learning. Previous methods tackle self-supervised VO as a local\nstructure from motion (SfM) problem that recovers depth from single image and\nrelative poses from image pairs by minimizing photometric loss between warped\nand captured images. As single-view depth estimation is an ill-posed problem,\nand photometric loss is incapable of discriminating distortion artifacts of\nwarped images, the estimated depth is vague and pose is inaccurate. In contrast\nto previous methods, our framework learns a compact representation of\nframe-to-frame correlation, which is updated by incorporating sequential\ninformation. The updated representation is used for depth estimation. Besides,\nwe tackle VO as a self-supervised image generation task and take advantage of\nGenerative Adversarial Networks (GAN). The generator learns to estimate depth\nand pose to generate a warped target image. The discriminator evaluates the\nquality of generated image with high-level structural perception that overcomes\nthe problem of pixel-wise loss in previous methods. Experiments on KITTI and\nCityscapes datasets show that our method obtains more accurate depth with\ndetails preserved and predicted pose outperforms state-of-the-art\nself-supervised methods significantly.\n", "versions": [{"version": "v1", "created": "Fri, 23 Aug 2019 07:53:35 GMT"}], "update_date": "2019-08-26", "authors_parsed": [["Li", "Shunkai", ""], ["Xue", "Fei", ""], ["Wang", "Xin", ""], ["Yan", "Zike", ""], ["Zha", "Hongbin", ""]]}, {"id": "1908.08705", "submitter": "Aleksandr Petiushko", "authors": "Stepan Komkov, Aleksandr Petiushko", "title": "AdvHat: Real-world adversarial attack on ArcFace Face ID system", "comments": null, "journal-ref": "2020 25th International Conference on Pattern Recognition (ICPR)", "doi": "10.1109/ICPR48806.2021.9412236", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose a novel easily reproducible technique to attack the\nbest public Face ID system ArcFace in different shooting conditions. To create\nan attack, we print the rectangular paper sticker on a common color printer and\nput it on the hat. The adversarial sticker is prepared with a novel algorithm\nfor off-plane transformations of the image which imitates sticker location on\nthe hat. Such an approach confuses the state-of-the-art public Face ID model\nLResNet100E-IR, ArcFace@ms1m-refine-v2 and is transferable to other Face ID\nmodels.\n", "versions": [{"version": "v1", "created": "Fri, 23 Aug 2019 07:55:42 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Komkov", "Stepan", ""], ["Petiushko", "Aleksandr", ""]]}, {"id": "1908.08718", "submitter": "Seoung Wug Oh", "authors": "Seoung Wug Oh, Sungho Lee, Joon-Young Lee, Seon Joo Kim", "title": "Onion-Peel Networks for Deep Video Completion", "comments": "ICCV 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose the onion-peel networks for video completion. Given a set of\nreference images and a target image with holes, our network fills the hole by\nreferring the contents in the reference images. Our onion-peel network\nprogressively fills the hole from the hole boundary enabling it to exploit\nricher contextual information for the missing regions every step. Given a\nsufficient number of recurrences, even a large hole can be inpainted\nsuccessfully. To attend to the missing information visible in the reference\nimages, we propose an asymmetric attention block that computes similarities\nbetween the hole boundary pixels in the target and the non-hole pixels in the\nreferences in a non-local manner. With our attention block, our network can\nhave an unlimited spatial-temporal window size and fill the holes with globally\ncoherent contents. In addition, our framework is applicable to the image\ncompletion guided by the reference images without any modification, which is\ndifficult to do with the previous methods. We validate that our method produces\nvisually pleasing image and video inpainting results in realistic test cases.\n", "versions": [{"version": "v1", "created": "Fri, 23 Aug 2019 08:51:41 GMT"}], "update_date": "2019-08-26", "authors_parsed": [["Oh", "Seoung Wug", ""], ["Lee", "Sungho", ""], ["Lee", "Joon-Young", ""], ["Kim", "Seon Joo", ""]]}, {"id": "1908.08746", "submitter": "Juan Miguel Valverde", "authors": "Juan Miguel Valverde, Artem Shatillo, Riccardo de Feo, Olli Gr\\\"ohn,\n  Alejandra Sierra, Jussi Tohka", "title": "Automatic Rodent Brain MRI Lesion Segmentation with Fully Convolutional\n  Networks", "comments": "Accepted to Machine Learning in Medical Imaging (MLMI 2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Manual segmentation of rodent brain lesions from magnetic resonance images\n(MRIs) is an arduous, time-consuming and subjective task that is highly\nimportant in pre-clinical research. Several automatic methods have been\ndeveloped for different human brain MRI segmentation, but little research has\ntargeted automatic rodent lesion segmentation. The existing tools for\nperforming automatic lesion segmentation in rodents are constrained by strict\nassumptions about the data. Deep learning has been successfully used for\nmedical image segmentation. However, there has not been any deep learning\napproach specifically designed for tackling rodent brain lesion segmentation.\nIn this work, we propose a novel Fully Convolutional Network (FCN), RatLesNet,\nfor the aforementioned task. Our dataset consists of 131 T2-weighted rat brain\nscans from 4 different studies in which ischemic stroke was induced by\ntransient middle cerebral artery occlusion. We compare our method with two\nother 3D FCNs originally developed for anatomical segmentation (VoxResNet and\n3D-U-Net) with 5-fold cross-validation on a single study and a generalization\ntest, where the training was done on a single study and testing on three\nremaining studies. The labels generated by our method were quantitatively and\nqualitatively better than the predictions of the compared methods. The average\nDice coefficient achieved in the 5-fold cross-validation experiment with the\nproposed approach was 0.88, between 3.7% and 38% higher than the compared\narchitectures. The presented architecture also outperformed the other FCNs at\ngeneralizing on different studies, achieving the average Dice coefficient of\n0.79.\n", "versions": [{"version": "v1", "created": "Fri, 23 Aug 2019 10:15:37 GMT"}], "update_date": "2019-08-26", "authors_parsed": [["Valverde", "Juan Miguel", ""], ["Shatillo", "Artem", ""], ["de Feo", "Riccardo", ""], ["Gr\u00f6hn", "Olli", ""], ["Sierra", "Alejandra", ""], ["Tohka", "Jussi", ""]]}, {"id": "1908.08767", "submitter": "Yechong Huang", "authors": "Yechong Huang, Tao Song, Jiahang Xu, Yinan Chen, Xiahai Zhuang", "title": "KLDivNet: An unsupervised neural network for multi-modality image\n  registration", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-modality image registration is one of the most underlined processes in\nmedical image analysis. Recently, convolutional neural networks (CNNs) have\nshown significant potential in deformable registration. However, the lack of\nvoxel-wise ground truth challenges the training of CNNs for an accurate\nregistration. In this work, we propose a cross-modality similarity metric,\nbased on the KL-divergence of image variables, and implement an efficient\nestimation method using a CNN. This estimation network, referred to as\nKLDivNet, can be trained unsupervisedly. We then embed the KLDivNet into a\nregistration network to achieve the unsupervised deformable registration for\nmulti-modality images. We employed three datasets, i.e., AAL Brain, LiTS Liver\nand Hospital Liver, with both the intra- and inter-modality image registration\ntasks for validation. Results showed that our similarity metric was effective,\nand the proposed registration network delivered superior performance compared\nto the state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Fri, 23 Aug 2019 11:48:50 GMT"}, {"version": "v2", "created": "Wed, 5 Feb 2020 06:27:45 GMT"}], "update_date": "2020-02-06", "authors_parsed": [["Huang", "Yechong", ""], ["Song", "Tao", ""], ["Xu", "Jiahang", ""], ["Chen", "Yinan", ""], ["Zhuang", "Xiahai", ""]]}, {"id": "1908.08813", "submitter": "Georgios Karantaidis", "authors": "Georgios Karantaidis, Constantine Kotropoulos", "title": "Efficient Capon-Based Approach Exploiting Temporal Windowing For\n  Electric Network Frequency Estimation", "comments": "6 pages, 1 figure, IEEE International Workshop on Machine Learning\n  For Signal Processing (MLSP) 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Electric Network Frequency (ENF) fluctuations constitute a powerful tool in\nmultimedia forensics. An efficient approach for ENF estimation is introduced\nwith temporal windowing based on the filter-bank Capon spectral estimator. A\ntype of Gohberg-Semencul factorization of the model covariance matrix is used\ndue to the Toeplitz structure of the covariance matrix. Moreover, this approach\nuses, for the first time in the field of ENF, a temporal window, not\nnecessarily the rectangular one, at the stage preceding spectral estimation.\nKrylov matrices are employed for fast implementation of matrix inversions. The\nproposed approach outperforms the state-of-the-art methods in ENF estimation,\nwhen a short time window of $1$ second is employed in power recordings. In\nspeech recordings, the proposed approach yields highly accurate results with\nrespect to both time complexity and accuracy. Moreover, the impact of different\ntemporal windows is studied. The results show that even the most trivial\nmethods for ENF estimation, such as the Short-Time Fourier Transform, can\nprovide better results than the most recent state-of-the-art methods, when a\ntemporal window is employed. The correlation coefficient is used to measure the\nENF estimation accuracy.\n", "versions": [{"version": "v1", "created": "Thu, 22 Aug 2019 09:57:23 GMT"}, {"version": "v2", "created": "Wed, 16 Oct 2019 08:12:42 GMT"}], "update_date": "2019-10-17", "authors_parsed": [["Karantaidis", "Georgios", ""], ["Kotropoulos", "Constantine", ""]]}, {"id": "1908.08814", "submitter": "Weichen Dai", "authors": "Weichen Dai, Yu Zhang, Donglei Sun, Naira Hovakimyan, Ping Li", "title": "Multi-Spectral Visual Odometry without Explicit Stereo Matching", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-spectral sensors consisting of a standard (visible-light) camera and a\nlong-wave infrared camera can simultaneously provide both visible and thermal\nimages. Since thermal images are independent from environmental illumination,\nthey can help to overcome certain limitations of standard cameras under\ncomplicated illumination conditions. However, due to the difference in the\ninformation source of the two types of cameras, their images usually share very\nlow texture similarity. Hence, traditional texture-based feature matching\nmethods cannot be directly applied to obtain stereo correspondences. To tackle\nthis problem, a multi-spectral visual odometry method without explicit stereo\nmatching is proposed in this paper. Bundle adjustment of multi-view stereo is\nperformed on the visible and the thermal images using direct image alignment.\nScale drift can be avoided by additional temporal observations of map points\nwith the fixed-baseline stereo. Experimental results indicate that the proposed\nmethod can provide accurate visual odometry results with recovered metric\nscale. Moreover, the proposed method can also provide a metric 3D\nreconstruction in semi-dense density with multi-spectral information, which is\nnot available from existing multi-spectral methods.\n", "versions": [{"version": "v1", "created": "Fri, 23 Aug 2019 13:19:05 GMT"}], "update_date": "2019-08-26", "authors_parsed": [["Dai", "Weichen", ""], ["Zhang", "Yu", ""], ["Sun", "Donglei", ""], ["Hovakimyan", "Naira", ""], ["Li", "Ping", ""]]}, {"id": "1908.08815", "submitter": "\\'Angel F. Garc\\'ia-Fern\\'andez", "authors": "\\'Angel F. Garc\\'ia-Fern\\'andez, Lennart Svensson", "title": "Spooky effect in optimal OSPA estimation and how GOSPA solves it", "comments": "This paper received the third best paper award at the 22nd\n  International Conference on Information Fusion, Ottawa, Canada, 2019. Matlab\n  code of the GOSPA metric can be found in https://github.com/abusajana/GOSPA .\n  Additional information on MTT can be found in the online course\n  https://www.youtube.com/channel/UCa2-fpj6AV8T6JK1uTRuFpw", "journal-ref": "Proceedings of the 22nd International Conference on Information\n  Fusion, 2019", "doi": null, "report-no": null, "categories": "eess.SP cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we show the spooky effect at a distance that arises in optimal\nestimation of multiple targets with the optimal sub-pattern assignment (OSPA)\nmetric. This effect refers to the fact that if we have several independent\npotential targets at distant locations, a change in the probability of\nexistence of one of them can completely change the optimal estimation of the\nrest of the potential targets. As opposed to OSPA, the generalised OSPA (GOSPA)\nmetric ($\\alpha=2$) penalises localisation errors for properly detected\ntargets, false targets and missed targets. As a consequence, optimal GOSPA\nestimation aims to lower the number of false and missed targets, as well as the\nlocalisation error for properly detected targets, and avoids the spooky effect.\n", "versions": [{"version": "v1", "created": "Fri, 23 Aug 2019 13:21:42 GMT"}], "update_date": "2019-08-26", "authors_parsed": [["Garc\u00eda-Fern\u00e1ndez", "\u00c1ngel F.", ""], ["Svensson", "Lennart", ""]]}, {"id": "1908.08819", "submitter": "\\'Angel F. Garc\\'ia-Fern\\'andez", "authors": "\\'Angel F. Garc\\'ia-Fern\\'andez, Yuxuan Xia, Karl Granstr\\\"om, Lennart\n  Svensson, Jason L. Williams", "title": "Gaussian implementation of the multi-Bernoulli mixture filter", "comments": "Matlab code of the MBM and PMBM filters is provided in\n  https://github.com/Agarciafernandez/MTT . Additional information on MTT\n  including PMBM and MBM filters can be found in the online course\n  https://www.youtube.com/channel/UCa2-fpj6AV8T6JK1uTRuFpw", "journal-ref": "Proceedings of the 22nd International Conference on Information\n  Fusion, 2019", "doi": null, "report-no": null, "categories": "eess.SP cs.CV stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents the Gaussian implementation of the multi-Bernoulli\nmixture (MBM) filter. The MBM filter provides the filtering (multi-target)\ndensity for the standard dynamic and radar measurement models when the birth\nmodel is multi-Bernoulli or multi-Bernoulli mixture. Under linear/Gaussian\nmodels, the single target densities of the MBM mixture admit Gaussian\nclosed-form expressions. Murty's algorithm is used to select the global\nhypotheses with highest weights. The MBM filter is compared with other\nalgorithms in the literature via numerical simulations.\n", "versions": [{"version": "v1", "created": "Fri, 23 Aug 2019 13:31:49 GMT"}], "update_date": "2019-08-26", "authors_parsed": [["Garc\u00eda-Fern\u00e1ndez", "\u00c1ngel F.", ""], ["Xia", "Yuxuan", ""], ["Granstr\u00f6m", "Karl", ""], ["Svensson", "Lennart", ""], ["Williams", "Jason L.", ""]]}, {"id": "1908.08837", "submitter": "Haiyang Mei", "authors": "Xin Yang, Haiyang Mei, Jiqing Zhang, Ke Xu, Baocai Yin, Qiang Zhang,\n  Xiaopeng Wei", "title": "DRFN: Deep Recurrent Fusion Network for Single-Image Super-Resolution\n  with Large Factors", "comments": null, "journal-ref": "IEEE Transactions on Multimedia ( Volume: 21 , Issue: 2 , Feb.\n  2019 ) 328 - 337", "doi": "10.1109/TMM.2018.2863602", "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, single-image super-resolution has made great progress owing to the\ndevelopment of deep convolutional neural networks (CNNs). The vast majority of\nCNN-based models use a pre-defined upsampling operator, such as bicubic\ninterpolation, to upscale input low-resolution images to the desired size and\nlearn non-linear mapping between the interpolated image and ground truth\nhigh-resolution (HR) image. However, interpolation processing can lead to\nvisual artifacts as details are over-smoothed, particularly when the\nsuper-resolution factor is high. In this paper, we propose a Deep Recurrent\nFusion Network (DRFN), which utilizes transposed convolution instead of bicubic\ninterpolation for upsampling and integrates different-level features extracted\nfrom recurrent residual blocks to reconstruct the final HR images. We adopt a\ndeep recurrence learning strategy and thus have a larger receptive field, which\nis conducive to reconstructing an image more accurately. Furthermore, we show\nthat the multi-level fusion structure is suitable for dealing with image\nsuper-resolution problems. Extensive benchmark evaluations demonstrate that the\nproposed DRFN performs better than most current deep learning methods in terms\nof accuracy and visual effects, especially for large-scale images, while using\nfewer parameters.\n", "versions": [{"version": "v1", "created": "Fri, 23 Aug 2019 14:13:21 GMT"}], "update_date": "2019-08-28", "authors_parsed": [["Yang", "Xin", ""], ["Mei", "Haiyang", ""], ["Zhang", "Jiqing", ""], ["Xu", "Ke", ""], ["Yin", "Baocai", ""], ["Zhang", "Qiang", ""], ["Wei", "Xiaopeng", ""]]}, {"id": "1908.08840", "submitter": "Joseph Antony A", "authors": "Joseph Antony, Kevin McGuinness, Kieran Moran, and Noel E O' Connor", "title": "Feature Learning to Automatically Assess Radiographic Knee\n  Osteoarthritis Severity", "comments": "Book Chapter preprint :: Deep Learners and Deep Learner Descriptors\n  for Medical Applications", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  This chapter presents the investigations and the results of feature learning\nusing convolutional neural networks to automatically assess knee osteoarthritis\n(OA) severity and the associated clinical and diagnostic features of knee OA\nfrom X-ray images. Also, this chapter demonstrates that feature learning in a\nsupervised manner is more effective than using conventional handcrafted\nfeatures for automatic detection of knee joints and fine-grained knee OA image\nclassification. In the general machine learning approach to automatically\nassess knee OA severity, the first step is to localize the region of interest\nthat is to detect and extract the knee joint regions from the radiographs, and\nthe next step is to classify the localized knee joints based on a radiographic\nclassification scheme such as Kellgren and Lawrence grades. First, the existing\napproaches for detecting (or localizing) the knee joint regions based on\nhandcrafted features are reviewed and outlined. Next, three new approaches are\nintroduced: 1) to automatically detect the knee joint region using a fully\nconvolutional network, 2) to automatically assess the radiographic knee OA\nusing CNNs trained from scratch for classification and regression of knee joint\nimages to predict KL grades in ordinal and continuous scales, and 3) to\nquantify the knee OA severity optimizing a weighted ratio of two loss\nfunctions: categorical cross entropy and mean-squared error using\nmulti-objective convolutional learning and ordinal regression. Two public\ndatasets: the OAI and the MOST are used to evaluate the approaches with\npromising results that outperform existing approaches. In summary, this work\nprimarily contributes to the field of automated methods for localization\n(automatic detection) and quantification (image classification) of radiographic\nknee OA.\n", "versions": [{"version": "v1", "created": "Fri, 23 Aug 2019 14:23:35 GMT"}], "update_date": "2019-08-26", "authors_parsed": [["Antony", "Joseph", ""], ["McGuinness", "Kevin", ""], ["Moran", "Kieran", ""], ["Connor", "Noel E O'", ""]]}, {"id": "1908.08841", "submitter": "Runnan Chen Mr.", "authors": "Runnan Chen, Yuexin Ma, Nenglun Chen, Daniel Lee, and Wenping Wang", "title": "Cephalometric Landmark Detection by AttentiveFeature Pyramid Fusion and\n  Regression-Voting", "comments": "Early accepted by International Conference on Medical image computing\n  and computer-assisted intervention (MICCAI 2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Marking anatomical landmarks in cephalometric radiography is a critical\noperation in cephalometric analysis. Automatically and accurately locating\nthese landmarks is a challenging issue because different landmarks require\ndifferent levels of resolution and semantics. Based on this observation, we\npropose a novel attentive feature pyramid fusion module (AFPF) to explicitly\nshape high-resolution and semantically enhanced fusion features to achieve\nsignificantly higher accuracy than existing deep learning-based methods. We\nalso combine heat maps and offset maps to perform pixel-wise regression-voting\nto improve detection accuracy. By incorporating the AFPF and regression-voting,\nwe develop an end-to-end deep learning framework that improves detection\naccuracy by 7%~11% for all the evaluation metrics over the state-of-the-art\nmethod. We present ablation studies to give more insights into different\ncomponents of our method and demonstrate its generalization capability and\nstability for unseen data from diverse devices.\n", "versions": [{"version": "v1", "created": "Fri, 23 Aug 2019 14:34:44 GMT"}], "update_date": "2019-08-26", "authors_parsed": [["Chen", "Runnan", ""], ["Ma", "Yuexin", ""], ["Chen", "Nenglun", ""], ["Lee", "Daniel", ""], ["Wang", "Wenping", ""]]}, {"id": "1908.08847", "submitter": "G\\\"okhan Yildirim", "authors": "G\\\"okhan Yildirim, Nikolay Jetchev, Roland Vollgraf, Urs Bergmann", "title": "Generating High-Resolution Fashion Model Images Wearing Custom Outfits", "comments": "Accepted to the International Conference on Computer Vision, ICCV\n  2019, Workshop on Computer Vision for Fashion, Art and Design", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visualizing an outfit is an essential part of shopping for clothes. Due to\nthe combinatorial aspect of combining fashion articles, the available images\nare limited to a pre-determined set of outfits. In this paper, we broaden these\nvisualizations by generating high-resolution images of fashion models wearing a\ncustom outfit under an input body pose. We show that our approach can not only\ntransfer the style and the pose of one generated outfit to another, but also\ncreate realistic images of human bodies and garments.\n", "versions": [{"version": "v1", "created": "Fri, 23 Aug 2019 14:46:41 GMT"}], "update_date": "2019-08-26", "authors_parsed": [["Yildirim", "G\u00f6khan", ""], ["Jetchev", "Nikolay", ""], ["Vollgraf", "Roland", ""], ["Bergmann", "Urs", ""]]}, {"id": "1908.08854", "submitter": "Yuxing Xie", "authors": "Yuxing Xie, Jiaojiao Tian, Xiao Xiang Zhu", "title": "Linking Points With Labels in 3D: A Review of Point Cloud Semantic\n  Segmentation", "comments": "The title of published version was modified to \"Linking Points With\n  Labels in 3D: A Review of Point Cloud Semantic Segmentation\". To read its\n  final version please go to IEEE Geoscience and Remote Sensing Magazine on\n  IEEE XPlore: https://ieeexplore.ieee.org/document/9028090", "journal-ref": null, "doi": "10.1109/MGRS.2019.2937630", "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  3D Point Cloud Semantic Segmentation (PCSS) is attracting increasing\ninterest, due to its applicability in remote sensing, computer vision and\nrobotics, and due to the new possibilities offered by deep learning techniques.\nIn order to provide a needed up-to-date review of recent developments in PCSS,\nthis article summarizes existing studies on this topic. Firstly, we outline the\nacquisition and evolution of the 3D point cloud from the perspective of remote\nsensing and computer vision, as well as the published benchmarks for PCSS\nstudies. Then, traditional and advanced techniques used for Point Cloud\nSegmentation (PCS) and PCSS are reviewed and compared. Finally, important\nissues and open questions in PCSS studies are discussed.\n", "versions": [{"version": "v1", "created": "Fri, 23 Aug 2019 14:55:11 GMT"}, {"version": "v2", "created": "Tue, 3 Sep 2019 13:31:00 GMT"}, {"version": "v3", "created": "Sat, 27 Jun 2020 00:24:30 GMT"}], "update_date": "2020-06-30", "authors_parsed": [["Xie", "Yuxing", ""], ["Tian", "Jiaojiao", ""], ["Zhu", "Xiao Xiang", ""]]}, {"id": "1908.08856", "submitter": "Joseph Antony A", "authors": "Marc G\\'orriz, Joseph Antony, Kevin McGuinness, Xavier Gir\\'o-i-Nieto,\n  Noel E. O'Connor", "title": "Assessing Knee OA Severity with CNN attention-based end-to-end\n  architectures", "comments": "Proceedings of the 2nd International Conference on Medical Imaging\n  with Deep Learning", "journal-ref": "Proceedings of The 2nd International Conference on Medical Imaging\n  with Deep Learning, PMLR 102:197-214, 2019", "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  This work proposes a novel end-to-end convolutional neural network (CNN)\narchitecture to automatically quantify the severity of knee osteoarthritis (OA)\nusing X-Ray images, which incorporates trainable attention modules acting as\nunsupervised fine-grained detectors of the region of interest (ROI). The\nproposed attention modules can be applied at different levels and scales across\nany CNN pipeline helping the network to learn relevant attention patterns over\nthe most informative parts of the image at different resolutions. We test the\nproposed attention mechanism on existing state-of-the-art CNN architectures as\nour base models, achieving promising results on the benchmark knee OA datasets\nfrom the osteoarthritis initiative (OAI) and multicenter osteoarthritis study\n(MOST). All code from our experiments will be publicly available on the github\nrepository: https://github.com/marc-gorriz/KneeOA-CNNAttention\n", "versions": [{"version": "v1", "created": "Fri, 23 Aug 2019 14:59:52 GMT"}], "update_date": "2019-08-26", "authors_parsed": [["G\u00f3rriz", "Marc", ""], ["Antony", "Joseph", ""], ["McGuinness", "Kevin", ""], ["Gir\u00f3-i-Nieto", "Xavier", ""], ["O'Connor", "Noel E.", ""]]}, {"id": "1908.08870", "submitter": "Nick Byrne", "authors": "Nick Byrne, James R. Clough, Isra Valverde, Giovanni Montana, Andrew\n  P. King", "title": "Topology-preserving augmentation for CNN-based segmentation of\n  congenital heart defects from 3D paediatric CMR", "comments": "To be published at MICCAI PIPPI 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Patient-specific 3D printing of congenital heart anatomy demands an accurate\nsegmentation of the thin tissue interfaces which characterise these diagnoses.\nEven when a label set has a high spatial overlap with the ground truth,\ninaccurate delineation of these interfaces can result in topological errors.\nThese compromise the clinical utility of such models due to the anomalous\nappearance of defects. CNNs have achieved state-of-the-art performance in\nsegmentation tasks. Whilst data augmentation has often played an important\nrole, we show that conventional image resampling schemes used therein can\nintroduce topological changes in the ground truth labelling of augmented\nsamples. We present a novel pipeline to correct for these changes, using a\nfast-marching algorithm to enforce the topology of the ground truth labels\nwithin their augmented representations. In so doing, we invoke the idea of\ncardiac contiguous topology to describe an arbitrary combination of congenital\nheart defects and develop an associated, clinically meaningful metric to\nmeasure the topological correctness of segmentations. In a series of five-fold\ncross-validations, we demonstrate the performance gain produced by this\npipeline and the relevance of topological considerations to the segmentation of\ncongenital heart defects. We speculate as to the applicability of this approach\nto any segmentation task involving morphologically complex targets.\n", "versions": [{"version": "v1", "created": "Fri, 23 Aug 2019 15:34:09 GMT"}], "update_date": "2019-08-26", "authors_parsed": [["Byrne", "Nick", ""], ["Clough", "James R.", ""], ["Valverde", "Isra", ""], ["Montana", "Giovanni", ""], ["King", "Andrew P.", ""]]}, {"id": "1908.08873", "submitter": "Joseph Antony A", "authors": "Jaynal Abedin, Joseph Antony, Kevin McGuinness, Kieran Moran, Noel E\n  O'Connor, Dietrich Rebholz-Schuhmann, and John Newell", "title": "Predicting knee osteoarthritis severity: comparative modeling based on\n  patient's data and plain X-ray images", "comments": "Published in Nature Scientific Reports, 2019", "journal-ref": "Scientific reports 9, no. 1 (2019): 5761", "doi": "10.1038/s41598-019-42215-9", "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Knee osteoarthritis (KOA) is a disease that impairs knee function and causes\npain. A radiologist reviews knee X-ray images and grades the severity level of\nthe impairments according to the Kellgren and Lawrence grading scheme; a\nfive-point ordinal scale (0--4). In this study, we used Elastic Net (EN) and\nRandom Forests (RF) to build predictive models using patient assessment data\n(i.e. signs and symptoms of both knees and medication use) and a convolution\nneural network (CNN) trained using X-ray images only. Linear mixed effect\nmodels (LMM) were used to model the within subject correlation between the two\nknees. The root mean squared error for the CNN, EN, and RF models was 0.77,\n0.97, and 0.94 respectively. The LMM shows similar overall prediction accuracy\nas the EN regression but correctly accounted for the hierarchical structure of\nthe data resulting in more reliable inference. Useful explanatory variables\nwere identified that could be used for patient monitoring before X-ray imaging.\nOur analyses suggest that the models trained for predicting the KOA severity\nlevels achieve comparable results when modeling X-ray images and patient data.\nThe subjectivity in the KL grade is still a primary concern.\n", "versions": [{"version": "v1", "created": "Fri, 23 Aug 2019 15:38:59 GMT"}], "update_date": "2019-08-26", "authors_parsed": [["Abedin", "Jaynal", ""], ["Antony", "Joseph", ""], ["McGuinness", "Kevin", ""], ["Moran", "Kieran", ""], ["O'Connor", "Noel E", ""], ["Rebholz-Schuhmann", "Dietrich", ""], ["Newell", "John", ""]]}, {"id": "1908.08908", "submitter": "Huynh Manh", "authors": "Manh Huynh and Gita Alaghband", "title": "Trajectory Prediction by Coupling Scene-LSTM with Human Movement LSTM", "comments": "To appear in ISVC 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a novel human trajectory prediction system that incorporates the\nscene information (Scene-LSTM) as well as individual pedestrian movement\n(Pedestrian-LSTM) trained simultaneously within static crowded scenes. We\nsuperimpose a two-level grid structure (grid cells and subgrids) on the scene\nto encode spatial granularity plus common human movements. The Scene-LSTM\ncaptures the commonly traveled paths that can be used to significantly\ninfluence the accuracy of human trajectory prediction in local areas (i.e. grid\ncells). We further design scene data filters, consisting of a hard filter and a\nsoft filter, to select the relevant scene information in a local region when\nnecessary and combine it with Pedestrian-LSTM for forecasting a pedestrian's\nfuture locations. The experimental results on several publicly available\ndatasets demonstrate that our method outperforms related works and can produce\nmore accurate predicted trajectories in different scene contexts.\n", "versions": [{"version": "v1", "created": "Fri, 23 Aug 2019 17:31:59 GMT"}], "update_date": "2019-08-26", "authors_parsed": [["Huynh", "Manh", ""], ["Alaghband", "Gita", ""]]}, {"id": "1908.08914", "submitter": "Matthew Kowal", "authors": "Matthew Kowal, Gillian Sandison, Len Yabuki-Soh, Raner la Bastide", "title": "Region Tracking in an Image Sequence: Preventing Driver Inattention", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Driver inattention is a large problem on the roads around the world. The\nobjective of this project was to develop an eye tracking algorithm with\nsufficient computational efficiency and accuracy, to successfully realize when\nthe driver was looking away from the road for an extended period. The method of\ntracking involved the minimization of a functional, using the gradient descent\nand level set methods. The algorithm was then discretized and implemented using\nC and MATLAB. Multiple synthetic images, grey-scale and colour images were\ntested using the final design, with a desired region coverage of 82%. Further\nwork is needed to decrease the computation time, increase the robustness of the\nalgorithm, develop a small device capable of running the algorithm, as well as\nphysically implement this device into various vehicles.\n", "versions": [{"version": "v1", "created": "Fri, 23 Aug 2019 17:38:27 GMT"}], "update_date": "2019-08-26", "authors_parsed": [["Kowal", "Matthew", ""], ["Sandison", "Gillian", ""], ["Yabuki-Soh", "Len", ""], ["la Bastide", "Raner", ""]]}, {"id": "1908.08916", "submitter": "Dong Cao", "authors": "Dong Cao, Lisha Xu, and Dongdong Zhang", "title": "Cross-Enhancement Transform Two-Stream 3D ConvNets for Action\n  Recognition", "comments": "Accepted for publication in AIIPCC 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Action recognition is an important research topic in computer vision. It is\nthe basic work for visual understanding and has been applied in many fields.\nSince human actions can vary in different environments, it is difficult to\ninfer actions in completely different states with a same structural model. For\nthis case, we propose a Cross-Enhancement Transform Two-Stream 3D ConvNets\nalgorithm, which considers the action distribution characteristics on the\nspecific dataset. As a teaching model, stream with better performance in both\nstreams is expected to assist in training another stream. In this way, the\nenhanced-trained stream and teacher stream are combined to infer actions. We\nimplement experiments on the video datasets UCF-101, HMDB-51, and Kinetics-400,\nand the results confirm the effectiveness of our algorithm.\n", "versions": [{"version": "v1", "created": "Mon, 19 Aug 2019 15:48:35 GMT"}, {"version": "v2", "created": "Tue, 22 Oct 2019 08:17:33 GMT"}], "update_date": "2019-10-23", "authors_parsed": [["Cao", "Dong", ""], ["Xu", "Lisha", ""], ["Zhang", "Dongdong", ""]]}, {"id": "1908.08918", "submitter": "Jose Lamarca", "authors": "Jose Lamarca, Shaifali Parashar, Adrien Bartoli and J.M.M. Montiel", "title": "DefSLAM: Tracking and Mapping of Deforming Scenes from Monocular\n  Sequences", "comments": "Experiments results: https://www.youtube.com/watch?v=6mmhD2_t6Gs ;\n  More Results:\n  https://www.youtube.com/playlist?list=PLKBuKNhAV30SlKGJ9eaMlAExdWRypUy-K", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Monocular SLAM algorithms perform robustly when observing rigid scenes,\nhowever, they fail when the observed scene deforms, for example, in medical\nendoscopy applications. We present DefSLAM, the first monocular SLAM capable of\noperating in deforming scenes in real-time. Our approach intertwines\nShape-from-Template (SfT) and Non-Rigid Structure-from-Motion (NRSfM)\ntechniques to deal with the exploratory sequences typical of SLAM. A\ndeformation tracking thread recovers the pose of the camera and the deformation\nof the observed map, at frame rate, by means of SfT processing a template that\nmodels the scene shape-at-rest. A deformation mapping thread runs in parallel\nwith the tracking to update the template, at keyframe rate, by means of an\nisometric NRSfM processing a batch of full perspective keyframes. In our\nexperiments, DefSLAM processes close-up sequences of deforming scenes, both in\na laboratory controlled experiment and in medical endoscopy sequences,\nproducing accurate 3D models of the scene with respect to the moving camera.\n", "versions": [{"version": "v1", "created": "Tue, 20 Aug 2019 15:27:47 GMT"}, {"version": "v2", "created": "Tue, 25 Aug 2020 17:54:25 GMT"}], "update_date": "2020-08-26", "authors_parsed": [["Lamarca", "Jose", ""], ["Parashar", "Shaifali", ""], ["Bartoli", "Adrien", ""], ["Montiel", "J. M. M.", ""]]}, {"id": "1908.08919", "submitter": "Vandad Davoodnia", "authors": "Vandad Davoodnia, Saeed Ghorbani, Ali Etemad", "title": "In-bed Pressure-based Pose Estimation using Image Space Representation\n  Learning", "comments": "\\c{opyright}2021 IEEE. Personal use of this material is permitted.\n  Permission from IEEE must be obtained for all other uses, in any current or\n  future media, including reprinting/republishing this material for advertising\n  or promotional purposes, creating new collective works, for resale or\n  redistribution to servers or lists, or reuse of any copyrighted component of\n  this work in other works", "journal-ref": "2021 IEEE International Conference on Acoustics, Speech and Signal\n  Processing (ICASSP) (pp. 3965-3969). IEEE", "doi": "10.1109/ICASSP39728.2021.9413516", "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in deep pose estimation models have proven to be effective in\na wide range of applications such as health monitoring, sports, animations, and\nrobotics. However, pose estimation models fail to generalize when facing images\nacquired from in-bed pressure sensing systems. In this paper, we address this\nchallenge by presenting a novel end-to-end framework capable of accurately\nlocating body parts from vague pressure data. Our method exploits the idea of\nequipping an off-the-shelf pose estimator with a deep trainable neural network,\nwhich pre-processes and prepares the pressure data for subsequent pose\nestimation. Our model transforms the ambiguous pressure maps to images\ncontaining shapes and structures similar to the common input domain of the\npre-existing pose estimation methods. As a result, we show that our model is\nable to reconstruct unclear body parts, which in turn enables pose estimators\nto accurately and robustly estimate the pose. We train and test our method on a\nmanually annotated public pressure map dataset using a combination of loss\nfunctions. Results confirm the effectiveness of our method by the high visual\nquality in the generated images and the high pose estimation rates achieved.\n", "versions": [{"version": "v1", "created": "Wed, 21 Aug 2019 01:52:54 GMT"}, {"version": "v2", "created": "Wed, 30 Sep 2020 03:41:41 GMT"}, {"version": "v3", "created": "Tue, 18 May 2021 19:15:25 GMT"}], "update_date": "2021-05-20", "authors_parsed": [["Davoodnia", "Vandad", ""], ["Ghorbani", "Saeed", ""], ["Etemad", "Ali", ""]]}, {"id": "1908.08926", "submitter": "Bichen Wu", "authors": "Bichen Wu", "title": "Efficient Deep Neural Networks", "comments": "Ph.D. dissertation of Bichen Wu", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The success of deep neural networks (DNNs) is attributable to three factors:\nincreased compute capacity, more complex models, and more data. These factors,\nhowever, are not always present, especially for edge applications such as\nautonomous driving, augmented reality, and internet-of-things. Training DNNs\nrequires a large amount of data, which is difficult to obtain. Edge devices\nsuch as mobile phones have limited compute capacity, and therefore, require\nspecialized and efficient DNNs. However, due to the enormous design space and\nprohibitive training costs, designing efficient DNNs for different target\ndevices is challenging. So the question is, with limited data, compute\ncapacity, and model complexity, can we still successfully apply deep neural\nnetworks?\n  This dissertation focuses on the above problems and improving the efficiency\nof deep neural networks at four levels. Model efficiency: we designed neural\nnetworks for various computer vision tasks and achieved more than 10x faster\nspeed and lower energy. Data efficiency: we developed an advanced tool that\nenables 6.2x faster annotation of a LiDAR point cloud. We also leveraged domain\nadaptation to utilize simulated data, bypassing the need for real data.\nHardware efficiency: we co-designed neural networks and hardware accelerators\nand achieved 11.6x faster inference. Design efficiency: the process of finding\nthe optimal neural networks is time-consuming. Our automated neural\narchitecture search algorithms discovered, using 421x lower computational cost\nthan previous search methods, models with state-of-the-art accuracy and\nefficiency.\n", "versions": [{"version": "v1", "created": "Tue, 20 Aug 2019 23:26:04 GMT"}], "update_date": "2019-08-26", "authors_parsed": [["Wu", "Bichen", ""]]}, {"id": "1908.08927", "submitter": "Leonid  Bedratyuk", "authors": "Leonid Bedratyuk", "title": "2D moment invariants from the point of view of the classical invariant\n  theory", "comments": "20 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Invariants allow to classify images up to the action of a group of\ntransformations. In this paper we introduce notions of the algebras of\nsimultaneous polynomial and rational 2D moment invariants and prove that they\nare isomorphic to the algebras of joint polynomial and rational\n$SO(2)$-invariants of binary forms. Also, to simplify the calculating of\ninvariants we pass from an action of Lie group $SO(2)$ to an action of its Lie\nalgebra $\\mathfrak{so}_2$. This allow us to reduce the problem to standard\nproblems of the classical invariant theory.\n", "versions": [{"version": "v1", "created": "Tue, 20 Aug 2019 20:55:00 GMT"}, {"version": "v2", "created": "Sun, 1 Sep 2019 20:39:39 GMT"}], "update_date": "2019-09-04", "authors_parsed": [["Bedratyuk", "Leonid", ""]]}, {"id": "1908.08928", "submitter": "Frederico Belmonte Klein", "authors": "Frederico Belmonte Klein, Angelo Cangelosi", "title": "Human activity recognition from skeleton poses", "comments": "8 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human Action Recognition is an important task of Human Robot Interaction as\ncooperation between robots and humans requires that artificial agents recognise\ncomplex cues from the environment. A promising approach is using trained\nclassifiers to recognise human actions through sequences of skeleton poses\nextracted from images or RGB-D data from a sensor. However, with many different\ndata-sets focused on slightly different sets of actions and different\nalgorithms it is not clear which strategy produces highest accuracy for indoor\nactivities performed in a home environment. This work discussed, tested and\ncompared classic algorithms, namely, support vector machines and k-nearest\nneighbours, to 2 similar hierarchical neural gas approaches, the growing when\nrequired neural gas and the growing neural gas.\n", "versions": [{"version": "v1", "created": "Tue, 20 Aug 2019 15:12:19 GMT"}], "update_date": "2019-08-26", "authors_parsed": [["Klein", "Frederico Belmonte", ""], ["Cangelosi", "Angelo", ""]]}, {"id": "1908.08930", "submitter": "Shahin Mahdizadehaghdam", "authors": "Shahin Mahdizadehaghdam, Ashkan Panahi, Hamid Krim", "title": "Sparse Generative Adversarial Network", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new approach to Generative Adversarial Networks (GANs) to\nachieve an improved performance with additional robustness to its so-called and\nwell recognized mode collapse. We first proceed by mapping the desired data\nonto a frame-based space for a sparse representation to lift any limitation of\nsmall support features prior to learning the structure. To that end we start by\ndividing an image into multiple patches and modifying the role of the\ngenerative network from producing an entire image, at once, to creating a\nsparse representation vector for each image patch. We synthesize an entire\nimage by multiplying generated sparse representations to a pre-trained\ndictionary and assembling the resulting patches. This approach restricts the\noutput of the generator to a particular structure, obtained by imposing a Union\nof Subspaces (UoS) model to the original training data, leading to more\nrealistic images, while maintaining a desired diversity. To further regularize\nGANs in generating high-quality images and to avoid the notorious mode-collapse\nproblem, we introduce a third player in GANs, called reconstructor. This player\nutilizes an auto-encoding scheme to ensure that first, the input-output\nrelation in the generator is injective and second each real image corresponds\nto some input noise. We present a number of experiments, where the proposed\nalgorithm shows a remarkably higher inception score compared to the equivalent\nconventional GANs.\n", "versions": [{"version": "v1", "created": "Tue, 20 Aug 2019 23:35:41 GMT"}], "update_date": "2019-08-26", "authors_parsed": [["Mahdizadehaghdam", "Shahin", ""], ["Panahi", "Ashkan", ""], ["Krim", "Hamid", ""]]}, {"id": "1908.08932", "submitter": "Yawei Li", "authors": "Yawei Li, Shuhang Gu, Luc Van Gool, Radu Timofte", "title": "Learning Filter Basis for Convolutional Neural Network Compression", "comments": "Code is available at\n  https://github.com/ofsoundof/learning_filter_basis. ICCV 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional neural networks (CNNs) based solutions have achieved\nstate-of-the-art performances for many computer vision tasks, including\nclassification and super-resolution of images. Usually the success of these\nmethods comes with a cost of millions of parameters due to stacking deep\nconvolutional layers. Moreover, quite a large number of filters are also used\nfor a single convolutional layer, which exaggerates the parameter burden of\ncurrent methods. Thus, in this paper, we try to reduce the number of parameters\nof CNNs by learning a basis of the filters in convolutional layers. For the\nforward pass, the learned basis is used to approximate the original filters and\nthen used as parameters for the convolutional layers. We validate our proposed\nsolution for multiple CNN architectures on image classification and image\nsuper-resolution benchmarks and compare favorably to the existing\nstate-of-the-art in terms of reduction of parameters and preservation of\naccuracy.\n", "versions": [{"version": "v1", "created": "Fri, 23 Aug 2019 17:55:26 GMT"}, {"version": "v2", "created": "Mon, 23 Dec 2019 18:55:28 GMT"}], "update_date": "2019-12-24", "authors_parsed": [["Li", "Yawei", ""], ["Gu", "Shuhang", ""], ["Van Gool", "Luc", ""], ["Timofte", "Radu", ""]]}, {"id": "1908.08947", "submitter": "Sahar Yousefi", "authors": "Sahar Yousefi, Lydiane Hirschler, Merlijn van der Plas, Mohamed S.\n  Elmahdy, Hessam Sokooti, Matthias Van Osch, Marius Staring", "title": "Fast Dynamic Perfusion and Angiography Reconstruction using an\n  end-to-end 3D Convolutional Neural Network", "comments": "11 pages, 4 figures, 1 table, conference paper, accepted in MLMIR2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hadamard time-encoded pseudo-continuous arterial spin labeling (te-pCASL) is\na signal-to-noise ratio (SNR)-efficient MRI technique for acquiring dynamic\npCASL signals that encodes the temporal information into the labeling according\nto a Hadamard matrix. In the decoding step, the contribution of each sub-bolus\ncan be isolated resulting in dynamic perfusion scans. When acquiring te-ASL\nboth with and without flow-crushing, the ASL-signal in the arteries can be\nisolated resulting in 4D-angiographic information. However, obtaining\nmulti-timepoint perfusion and angiographic data requires two acquisitions. In\nthis study, we propose a 3D Dense-Unet convolutional neural network with a\nmulti-level loss function for reconstructing multi-timepoint perfusion and\nangiographic information from an interleaved $50\\%$-sampled crushed and\n$50\\%$-sampled non-crushed data, thereby negating the additional scan time. We\npresent a framework to generate dynamic pCASL training and validation data,\nbased on models of the intravascular and extravascular te-pCASL signals. The\nproposed network achieved SSIM values of $97.3 \\pm 1.1$ and $96.2 \\pm 11.1$\nrespectively for 4D perfusion and angiographic data reconstruction for 313 test\ndata-sets.\n", "versions": [{"version": "v1", "created": "Sat, 24 Aug 2019 15:51:20 GMT"}, {"version": "v2", "created": "Wed, 4 Sep 2019 08:16:39 GMT"}], "update_date": "2019-09-05", "authors_parsed": [["Yousefi", "Sahar", ""], ["Hirschler", "Lydiane", ""], ["van der Plas", "Merlijn", ""], ["Elmahdy", "Mohamed S.", ""], ["Sokooti", "Hessam", ""], ["Van Osch", "Matthias", ""], ["Staring", "Marius", ""]]}, {"id": "1908.08961", "submitter": "Max Tegmark", "authors": "Max Tegmark (MIT), Tailin Wu (MIT)", "title": "Pareto-optimal data compression for binary classification tasks", "comments": "Replaced to match version published in Entropy. 17 pages, 9 figs;\n  improved discussion, comparison with Blahut-Arimoto method", "journal-ref": "Entropy (2020), 22, 7", "doi": "10.3390/e22010007", "report-no": null, "categories": "cs.LG cs.CV cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The goal of lossy data compression is to reduce the storage cost of a data\nset $X$ while retaining as much information as possible about something ($Y$)\nthat you care about. For example, what aspects of an image $X$ contain the most\ninformation about whether it depicts a cat? Mathematically, this corresponds to\nfinding a mapping $X\\to Z\\equiv f(X)$ that maximizes the mutual information\n$I(Z,Y)$ while the entropy $H(Z)$ is kept below some fixed threshold. We\npresent a method for mapping out the Pareto frontier for classification tasks,\nreflecting the tradeoff between retained entropy and class information. We\nfirst show how a random variable $X$ (an image, say) drawn from a class\n$Y\\in\\{1,...,n\\}$ can be distilled into a vector $W=f(X)\\in \\mathbb{R}^{n-1}$\nlosslessly, so that $I(W,Y)=I(X,Y)$; for example, for a binary classification\ntask of cats and dogs, each image $X$ is mapped into a single real number $W$\nretaining all information that helps distinguish cats from dogs. For the $n=2$\ncase of binary classification, we then show how $W$ can be further compressed\ninto a discrete variable $Z=g_\\beta(W)\\in\\{1,...,m_\\beta\\}$ by binning $W$ into\n$m_\\beta$ bins, in such a way that varying the parameter $\\beta$ sweeps out the\nfull Pareto frontier, solving a generalization of the Discrete Information\nBottleneck (DIB) problem. We argue that the most interesting points on this\nfrontier are \"corners\" maximizing $I(Z,Y)$ for a fixed number of bins\n$m=2,3...$ which can be conveniently be found without multiobjective\noptimization. We apply this method to the CIFAR-10, MNIST and Fashion-MNIST\ndatasets, illustrating how it can be interpreted as an\ninformation-theoretically optimal image clustering algorithm.\n", "versions": [{"version": "v1", "created": "Fri, 23 Aug 2019 18:00:40 GMT"}, {"version": "v2", "created": "Wed, 15 Jan 2020 18:43:57 GMT"}], "update_date": "2020-01-16", "authors_parsed": [["Tegmark", "Max", "", "MIT"], ["Wu", "Tailin", "", "MIT"]]}, {"id": "1908.08984", "submitter": "Venkatesh Umaashankar Mr", "authors": "Venkatesh Umaashankar, Girish Shanmugam S and Aditi Prakash", "title": "Atlas: A Dataset and Benchmark for E-commerce Clothing Product\n  Categorization", "comments": "preprint", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In E-commerce, it is a common practice to organize the product catalog using\nproduct taxonomy. This enables the buyer to easily locate the item they are\nlooking for and also to explore various items available under a category.\nProduct taxonomy is a tree structure with 3 or more levels of depth and several\nleaf nodes. Product categorization is a large scale classification task that\nassigns a category path to a particular product. Research in this area is\nrestricted by the unavailability of good real-world datasets and the variations\nin taxonomy due to the absence of a standard across the different e-commerce\nstores. In this paper, we introduce a high-quality product taxonomy dataset\nfocusing on clothing products which contain 186,150 images under clothing\ncategory with 3 levels and 52 leaf nodes in the taxonomy. We explain the\nmethodology used to collect and label this dataset. Further, we establish the\nbenchmark by comparing image classification and Attention based Sequence models\nfor predicting the category path. Our benchmark model reaches a micro f-score\nof 0.92 on the test set. The dataset, code and pre-trained models are publicly\navailable at \\url{https://github.com/vumaasha/atlas}. We invite the community\nto improve upon these baselines.\n", "versions": [{"version": "v1", "created": "Mon, 12 Aug 2019 16:46:00 GMT"}], "update_date": "2019-08-27", "authors_parsed": [["Umaashankar", "Venkatesh", ""], ["S", "Girish Shanmugam", ""], ["Prakash", "Aditi", ""]]}, {"id": "1908.08985", "submitter": "Yunshan Ma", "authors": "Yunshan Ma, Xun Yang, Lizi Liao, Yixin Cao, Tat-Seng Chua", "title": "Who, Where, and What to Wear? Extracting Fashion Knowledge from Social\n  Media", "comments": "9 pages, 8 figures, ACMMM 2019", "journal-ref": null, "doi": "10.1145/3343031.3350889", "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fashion knowledge helps people to dress properly and addresses not only\nphysiological needs of users, but also the demands of social activities and\nconventions. It usually involves three mutually related aspects of: occasion,\nperson and clothing. However, there are few works focusing on extracting such\nknowledge, which will greatly benefit many downstream applications, such as\nfashion recommendation. In this paper, we propose a novel method to\nautomatically harvest fashion knowledge from social media. We unify three tasks\nof occasion, person and clothing discovery from multiple modalities of images,\ntexts and metadata. For person detection and analysis, we use the off-the-shelf\ntools due to their flexibility and satisfactory performance. For clothing\nrecognition and occasion prediction, we unify the two tasks by using a\ncontextualized fashion concept learning module, which captures the dependencies\nand correlations among different fashion concepts. To alleviate the heavy\nburden of human annotations, we introduce a weak label modeling module which\ncan effectively exploit machine-labeled data, a complementary of clean data. In\nexperiments, we contribute a benchmark dataset and conduct extensive\nexperiments from both quantitative and qualitative perspectives. The results\ndemonstrate the effectiveness of our model in fashion concept prediction, and\nthe usefulness of extracted knowledge with comprehensive analysis.\n", "versions": [{"version": "v1", "created": "Mon, 12 Aug 2019 08:05:59 GMT"}], "update_date": "2019-08-27", "authors_parsed": [["Ma", "Yunshan", ""], ["Yang", "Xun", ""], ["Liao", "Lizi", ""], ["Cao", "Yixin", ""], ["Chua", "Tat-Seng", ""]]}, {"id": "1908.08986", "submitter": "Elad Hoffer", "authors": "Elad Hoffer, Berry Weinstein, Itay Hubara, Tal Ben-Nun, Torsten\n  Hoefler, Daniel Soudry", "title": "Mix & Match: training convnets with mixed image sizes for improved\n  accuracy, speed and scale resiliency", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional neural networks (CNNs) are commonly trained using a fixed\nspatial image size predetermined for a given model. Although trained on images\nof aspecific size, it is well established that CNNs can be used to evaluate a\nwide range of image sizes at test time, by adjusting the size of intermediate\nfeature maps. In this work, we describe and evaluate a novel mixed-size\ntraining regime that mixes several image sizes at training time. We demonstrate\nthat models trained using our method are more resilient to image size changes\nand generalize well even on small images. This allows faster inference by using\nsmaller images attest time. For instance, we receive a 76.43% top-1 accuracy\nusing ResNet50 with an image size of 160, which matches the accuracy of the\nbaseline model with 2x fewer computations. Furthermore, for a given image size\nused at test time, we show this method can be exploited either to accelerate\ntraining or the final test accuracy. For example, we are able to reach a 79.27%\naccuracy with a model evaluated at a 288 spatial size for a relative\nimprovement of 14% over the baseline.\n", "versions": [{"version": "v1", "created": "Mon, 12 Aug 2019 08:27:49 GMT"}], "update_date": "2019-08-27", "authors_parsed": [["Hoffer", "Elad", ""], ["Weinstein", "Berry", ""], ["Hubara", "Itay", ""], ["Ben-Nun", "Tal", ""], ["Hoefler", "Torsten", ""], ["Soudry", "Daniel", ""]]}, {"id": "1908.08987", "submitter": "Qun Liu", "authors": "Qun Liu, Edward Collier, Supratik Mukhopadhyay", "title": "PCGAN-CHAR: Progressively Trained Classifier Generative Adversarial\n  Networks for Classification of Noisy Handwritten Bangla Characters", "comments": "Paper was accepted at the 21st International Conference on\n  Asia-Pacific Digital Libraries (ICADL 2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to the sparsity of features, noise has proven to be a great inhibitor in\nthe classification of handwritten characters. To combat this, most techniques\nperform denoising of the data before classification. In this paper, we\nconsolidate the approach by training an all-in-one model that is able to\nclassify even noisy characters. For classification, we progressively train a\nclassifier generative adversarial network on the characters from low to high\nresolution. We show that by learning the features at each resolution\nindependently a trained model is able to accurately classify characters even in\nthe presence of noise. We experimentally demonstrate the effectiveness of our\napproach by classifying noisy versions of MNIST, handwritten Bangla Numeral,\nand Basic Character datasets.\n", "versions": [{"version": "v1", "created": "Sun, 11 Aug 2019 08:01:58 GMT"}], "update_date": "2019-08-27", "authors_parsed": [["Liu", "Qun", ""], ["Collier", "Edward", ""], ["Mukhopadhyay", "Supratik", ""]]}, {"id": "1908.08988", "submitter": "Xiang Li", "authors": "Xiang Li, Shihao Ji", "title": "Neural Image Compression and Explanation", "comments": "Published as a journal paper at IEEE Access 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Explaining the prediction of deep neural networks (DNNs) and semantic image\ncompression are two active research areas of deep learning with a numerous of\napplications in decision-critical systems, such as surveillance cameras, drones\nand self-driving cars, where interpretable decision is critical and\nstorage/network bandwidth is limited. In this paper, we propose a novel\nend-to-end Neural Image Compression and Explanation (NICE) framework that\nlearns to (1) explain the predictions of convolutional neural networks (CNNs),\nand (2) subsequently compress the input images for efficient storage or\ntransmission. Specifically, NICE generates a sparse mask over an input image by\nattaching a stochastic binary gate to each pixel of the image, whose parameters\nare learned through the interaction with the CNN classifier to be explained.\nThe generated mask is able to capture the saliency of each pixel measured by\nits influence to the final prediction of CNN; it can also be used to produce a\nmixed-resolution image, where important pixels maintain their original high\nresolution and insignificant background pixels are subsampled to a low\nresolution. The produced images achieve a high compression rate (e.g., about\n0.6x of original image file size), while retaining a similar classification\naccuracy. Extensive experiments across multiple image classification benchmarks\ndemonstrate the superior performance of NICE compared to the state-of-the-art\nmethods in terms of explanation quality and semantic image compression rate.\nOur code is available at: https://github.com/lxuniverse/NICE.\n", "versions": [{"version": "v1", "created": "Fri, 9 Aug 2019 15:39:20 GMT"}, {"version": "v2", "created": "Tue, 8 Dec 2020 03:01:50 GMT"}], "update_date": "2020-12-09", "authors_parsed": [["Li", "Xiang", ""], ["Ji", "Shihao", ""]]}, {"id": "1908.08989", "submitter": "Maren Awiszus", "authors": "Maren Awiszus, Hanno Ackermann and Bodo Rosenhahn", "title": "Learning Disentangled Representations via Independent Subspaces", "comments": "Accepted at ICCV 2019 Workshop on Robust Subspace Learning and\n  Applications in Computer Vision", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image generating neural networks are mostly viewed as black boxes, where any\nchange in the input can have a number of globally effective changes on the\noutput. In this work, we propose a method for learning disentangled\nrepresentations to allow for localized image manipulations. We use face images\nas our example of choice. Depending on the image region, identity and other\nfacial attributes can be modified. The proposed network can transfer parts of a\nface such as shape and color of eyes, hair, mouth, etc.~directly between\npersons while all other parts of the face remain unchanged. The network allows\nto generate modified images which appear like realistic images. Our model\nlearns disentangled representations by weak supervision. We propose a localized\nresnet autoencoder optimized using several loss functions including a loss\nbased on the semantic segmentation, which we interpret as masks, and a loss\nwhich enforces disentanglement by decomposition of the latent space into\nstatistically independent subspaces. We evaluate the proposed solution w.r.t.\ndisentanglement and generated image quality. Convincing results are\ndemonstrated using the CelebA dataset.\n", "versions": [{"version": "v1", "created": "Mon, 26 Aug 2019 09:08:12 GMT"}], "update_date": "2019-08-27", "authors_parsed": [["Awiszus", "Maren", ""], ["Ackermann", "Hanno", ""], ["Rosenhahn", "Bodo", ""]]}, {"id": "1908.08990", "submitter": "Sebastian Agethen", "authors": "Sebastian Agethen, Winston H. Hsu", "title": "Deep Multi-Kernel Convolutional LSTM Networks and an Attention-Based\n  Mechanism for Videos", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Action recognition greatly benefits motion understanding in video analysis.\nRecurrent networks such as long short-term memory (LSTM) networks are a popular\nchoice for motion-aware sequence learning tasks. Recently, a convolutional\nextension of LSTM was proposed, in which input-to-hidden and hidden-to-hidden\ntransitions are modeled through convolution with a single kernel. This implies\nan unavoidable trade-off between effectiveness and efficiency. Herein, we\npropose a new enhancement to convolutional LSTM networks that supports\naccommodation of multiple convolutional kernels and layers. This resembles a\nNetwork-in-LSTM approach, which improves upon the aforementioned concern. In\naddition, we propose an attention-based mechanism that is specifically designed\nfor our multi-kernel extension. We evaluated our proposed extensions in a\nsupervised classification setting on the UCF-101 and Sports-1M datasets, with\nthe findings showing that our enhancements improve accuracy. We also undertook\nqualitative analysis to reveal the characteristics of our system and the\nconvolutional LSTM baseline.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jul 2019 05:51:20 GMT"}], "update_date": "2019-08-27", "authors_parsed": [["Agethen", "Sebastian", ""], ["Hsu", "Winston H.", ""]]}, {"id": "1908.08992", "submitter": "Anjana Wijekoon", "authors": "Anjana Wijekoon, Nirmalie Wiratunga, Kay Cooper", "title": "MEx: Multi-modal Exercises Dataset for Human Activity Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG eess.SP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  MEx: Multi-modal Exercises Dataset is a multi-sensor, multi-modal dataset,\nimplemented to benchmark Human Activity Recognition(HAR) and Multi-modal Fusion\nalgorithms. Collection of this dataset was inspired by the need for recognising\nand evaluating quality of exercise performance to support patients with\nMusculoskeletal Disorders(MSD). We select 7 exercises regularly recommended for\nMSD patients by physiotherapists and collected data with four sensors a\npressure mat, a depth camera and two accelerometers. The dataset contains three\ndata modalities; numerical time-series data, video data and pressure sensor\ndata posing interesting research challenges when reasoning for HAR and Exercise\nQuality Assessment. This paper presents our evaluation of the dataset on number\nof standard classification algorithms for the HAR task by comparing different\nfeature representation algorithms for each sensor. These results set a\nreference performance for each individual sensor that expose their strengths\nand weaknesses for the future tasks. In addition we visualise pressure mat data\nto explore the potential of the sensor to capture exercise performance quality.\nWith the recent advancement in multi-modal fusion, we also believe MEx is a\nsuitable dataset to benchmark not only HAR algorithms, but also fusion\nalgorithms of heterogeneous data types in multiple application domains.\n", "versions": [{"version": "v1", "created": "Tue, 13 Aug 2019 16:09:53 GMT"}], "update_date": "2019-08-27", "authors_parsed": [["Wijekoon", "Anjana", ""], ["Wiratunga", "Nirmalie", ""], ["Cooper", "Kay", ""]]}, {"id": "1908.08993", "submitter": "Dmitry Krotov", "authors": "Leopold Grinberg, John Hopfield, Dmitry Krotov", "title": "Local Unsupervised Learning for Image Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE q-bio.NC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Local Hebbian learning is believed to be inferior in performance to\nend-to-end training using a backpropagation algorithm. We question this popular\nbelief by designing a local algorithm that can learn convolutional filters at\nscale on large image datasets. These filters combined with patch normalization\nand very steep non-linearities result in a good classification accuracy for\nshallow networks trained locally, as opposed to end-to-end. The filters learned\nby our algorithm contain both orientation selective units and unoriented color\nunits, resembling the responses of pyramidal neurons located in the cytochrome\noxidase 'interblob' and 'blob' regions in the primary visual cortex of\nprimates. It is shown that convolutional networks with patch normalization\nsignificantly outperform standard convolutional networks on the task of\nrecovering the original classes when shadows are superimposed on top of\nstandard CIFAR-10 images. Patch normalization approximates the retinal\nadaptation to the mean light intensity, important for human vision. We also\ndemonstrate a successful transfer of learned representations between CIFAR-10\nand ImageNet 32x32 datasets. All these results taken together hint at the\npossibility that local unsupervised training might be a powerful tool for\nlearning general representations (without specifying the task) directly from\nunlabeled data.\n", "versions": [{"version": "v1", "created": "Wed, 14 Aug 2019 17:42:11 GMT"}], "update_date": "2019-08-27", "authors_parsed": [["Grinberg", "Leopold", ""], ["Hopfield", "John", ""], ["Krotov", "Dmitry", ""]]}, {"id": "1908.08994", "submitter": "Alexander Filonenko", "authors": "Alexander Filonenko, Konstantin Gudkov, Aleksei Lebedev, Nikita Orlov,\n  Ivan Zagaynov", "title": "FaSTExt: Fast and Small Text Extractor", "comments": "6 pages, 8th International Workshop on Camera-Based Document Analysis\n  & Recognition", "journal-ref": null, "doi": "10.1109/ICDARW.2019.30064", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Text detection in natural images is a challenging but necessary task for many\napplications. Existing approaches utilize large deep convolutional neural\nnetworks making it difficult to use them in real-world tasks. We propose a\nsmall yet relatively precise text extraction method. The basic component of it\nis a convolutional neural network which works in a fully-convolutional manner\nand produces results at multiple scales. Each scale output predicts whether a\npixel is a part of some word, its geometry, and its relation to neighbors at\nthe same scale and between scales. The key factor of reducing the complexity of\nthe model was the utilization of depthwise separable convolution, linear\nbottlenecks, and inverted residuals. Experiments on public datasets show that\nthe proposed network can effectively detect text while keeping the number of\nparameters in the range of 1.58 to 10.59 million in different configurations.\n", "versions": [{"version": "v1", "created": "Wed, 14 Aug 2019 15:25:24 GMT"}], "update_date": "2019-10-18", "authors_parsed": [["Filonenko", "Alexander", ""], ["Gudkov", "Konstantin", ""], ["Lebedev", "Aleksei", ""], ["Orlov", "Nikita", ""], ["Zagaynov", "Ivan", ""]]}, {"id": "1908.08996", "submitter": "Hongxin Lin", "authors": "Hongxin Lin, Zelin Xiao, Yang Tan, Hongyang Chao, Shengyong Ding", "title": "Justlookup: One Millisecond Deep Feature Extraction for Point Clouds By\n  Lookup Tables", "comments": "Accepted by ICME2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GT cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep models are capable of fitting complex high dimensional functions while\nusually yielding large computation load. There is no way to speed up the\ninference process by classical lookup tables due to the high-dimensional input\nand limited memory size. Recently, a novel architecture (PointNet) for point\nclouds has demonstrated that it is possible to obtain a complicated deep\nfunction from a set of 3-variable functions. In this paper, we exploit this\nproperty and apply a lookup table to encode these 3-variable functions. This\nmethod ensures that the inference time is only determined by the memory access\nno matter how complicated the deep function is. We conduct extensive\nexperiments on ModelNet and ShapeNet datasets and demonstrate that we can\ncomplete the inference process in 1.5 ms on an Intel i7-8700 CPU (single core\nmode), 32x speedup over the PointNet architecture without any performance\ndegradation.\n", "versions": [{"version": "v1", "created": "Wed, 14 Aug 2019 07:07:26 GMT"}], "update_date": "2019-08-27", "authors_parsed": [["Lin", "Hongxin", ""], ["Xiao", "Zelin", ""], ["Tan", "Yang", ""], ["Chao", "Hongyang", ""], ["Ding", "Shengyong", ""]]}, {"id": "1908.08997", "submitter": "Thomas Hartley", "authors": "Thomas Hartley, Kirill Sidorov, Christopher Willis and David Marshall", "title": "Gradient Weighted Superpixels for Interpretability in CNNs", "comments": "Presented at BMVC 2019: Workshop on Interpretable and Explainable\n  Machine Vision, Cardiff, UK", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As Convolutional Neural Networks embed themselves into our everyday lives,\nthe need for them to be interpretable increases. However, there is often a\ntrade-off between methods that are efficient to compute but produce an\nexplanation that is difficult to interpret, and those that are slow to compute\nbut provide a more interpretable result. This is particularly challenging in\nproblem spaces that require a large input volume, especially video which\ncombines both spatial and temporal dimensions. In this work we introduce the\nidea of scoring superpixels through the use of gradient based pixel scoring\ntechniques. We show qualitatively and quantitatively that this is able to\napproximate LIME, in a fraction of the time. We investigate our techniques\nusing both image classification, and action recognition networks on large scale\ndatasets (ImageNet and Kinetics-400 respectively).\n", "versions": [{"version": "v1", "created": "Fri, 16 Aug 2019 12:02:25 GMT"}], "update_date": "2019-08-27", "authors_parsed": [["Hartley", "Thomas", ""], ["Sidorov", "Kirill", ""], ["Willis", "Christopher", ""], ["Marshall", "David", ""]]}, {"id": "1908.08998", "submitter": "Wanling Gao", "authors": "Wanling Gao, Fei Tang, Lei Wang, Jianfeng Zhan, Chunxin Lan, Chunjie\n  Luo, Yunyou Huang, Chen Zheng, Jiahui Dai, Zheng Cao, Daoyi Zheng, Haoning\n  Tang, Kunlin Zhan, Biao Wang, Defei Kong, Tong Wu, Minghe Yu, Chongkang Tan,\n  Huan Li, Xinhui Tian, Yatao Li, Junchao Shao, Zhenyu Wang, Xiaoyu Wang, and\n  Hainan Ye", "title": "AIBench: An Industry Standard Internet Service AI Benchmark Suite", "comments": "24 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.IR cs.PF cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Today's Internet Services are undergoing fundamental changes and shifting to\nan intelligent computing era where AI is widely employed to augment services.\nIn this context, many innovative AI algorithms, systems, and architectures are\nproposed, and thus the importance of benchmarking and evaluating them rises.\nHowever, modern Internet services adopt a microservice-based architecture and\nconsist of various modules. The diversity of these modules and complexity of\nexecution paths, the massive scale and complex hierarchy of datacenter\ninfrastructure, the confidential issues of data sets and workloads pose great\nchallenges to benchmarking. In this paper, we present the first\nindustry-standard Internet service AI benchmark suite---AIBench with seventeen\nindustry partners, including several top Internet service providers. AIBench\nprovides a highly extensible, configurable, and flexible benchmark framework\nthat contains loosely coupled modules. We identify sixteen prominent AI problem\ndomains like learning to rank, each of which forms an AI component benchmark,\nfrom three most important Internet service domains: search engine, social\nnetwork, and e-commerce, which is by far the most comprehensive AI benchmarking\neffort. On the basis of the AIBench framework, abstracting the real-world data\nsets and workloads from one of the top e-commerce providers, we design and\nimplement the first end-to-end Internet service AI benchmark, which contains\nthe primary modules in the critical paths of an industry scale application and\nis scalable to deploy on different cluster scales. The specifications, source\ncode, and performance numbers are publicly available from the benchmark council\nweb site http://www.benchcouncil.org/AIBench/index.html.\n", "versions": [{"version": "v1", "created": "Tue, 13 Aug 2019 10:15:39 GMT"}, {"version": "v2", "created": "Wed, 23 Oct 2019 14:39:47 GMT"}], "update_date": "2019-10-24", "authors_parsed": [["Gao", "Wanling", ""], ["Tang", "Fei", ""], ["Wang", "Lei", ""], ["Zhan", "Jianfeng", ""], ["Lan", "Chunxin", ""], ["Luo", "Chunjie", ""], ["Huang", "Yunyou", ""], ["Zheng", "Chen", ""], ["Dai", "Jiahui", ""], ["Cao", "Zheng", ""], ["Zheng", "Daoyi", ""], ["Tang", "Haoning", ""], ["Zhan", "Kunlin", ""], ["Wang", "Biao", ""], ["Kong", "Defei", ""], ["Wu", "Tong", ""], ["Yu", "Minghe", ""], ["Tan", "Chongkang", ""], ["Li", "Huan", ""], ["Tian", "Xinhui", ""], ["Li", "Yatao", ""], ["Shao", "Junchao", ""], ["Wang", "Zhenyu", ""], ["Wang", "Xiaoyu", ""], ["Ye", "Hainan", ""]]}, {"id": "1908.08999", "submitter": "Tomas Jenicek", "authors": "Tomas Jenicek and Ond\\v{r}ej Chum", "title": "No Fear of the Dark: Image Retrieval under Varying Illumination\n  Conditions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image retrieval under varying illumination conditions, such as day and night\nimages, is addressed by image preprocessing, both hand-crafted and learned.\nPrior to extracting image descriptors by a convolutional neural network, images\nare photometrically normalised in order to reduce the descriptor sensitivity to\nillumination changes. We propose a learnable normalisation based on the U-Net\narchitecture, which is trained on a combination of single-camera multi-exposure\nimages and a newly constructed collection of similar views of landmarks during\nday and night. We experimentally show that both hand-crafted normalisation\nbased on local histogram equalisation and the learnable normalisation\noutperform standard approaches in varying illumination conditions, while\nstaying on par with the state-of-the-art methods on daylight illumination\nbenchmarks, such as Oxford or Paris datasets.\n", "versions": [{"version": "v1", "created": "Fri, 23 Aug 2019 19:30:37 GMT"}], "update_date": "2019-08-27", "authors_parsed": [["Jenicek", "Tomas", ""], ["Chum", "Ond\u0159ej", ""]]}, {"id": "1908.09000", "submitter": "Uziel Jaramillo Avila", "authors": "Uziel Jaramillo-Avila, Sean R. Anderson", "title": "Foveated image processing for faster object detection and recognition in\n  embedded systems using deep convolutional neural networks", "comments": null, "journal-ref": "Biomimetic and Biohybrid Systems (2019) 193--204", "doi": "10.1007/978-3-030-24741-6_17", "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Object detection and recognition algorithms using deep convolutional neural\nnetworks (CNNs) tend to be computationally intensive to implement. This\npresents a particular challenge for embedded systems, such as mobile robots,\nwhere the computational resources tend to be far less than for workstations. As\nan alternative to standard, uniformly sampled images, we propose the use of\nfoveated image sampling here to reduce the size of images, which are faster to\nprocess in a CNN due to the reduced number of convolution operations. We\nevaluate object detection and recognition on the Microsoft COCO database, using\nfoveated image sampling at different image sizes, ranging from 416x416 to 96x96\npixels, on an embedded GPU -- an NVIDIA Jetson TX2 with 256 CUDA cores. The\nresults show that it is possible to achieve a 4x speed-up in frame rates, from\n3.59 FPS to 15.24 FPS, using 416x416 and 128x128 pixel images respectively. For\nfoveated sampling, this image size reduction led to just a small decrease in\nrecall performance in the foveal region, to 92.0% of the baseline performance\nwith full-sized images, compared to a significant decrease to 50.1% of baseline\nrecall performance in uniformly sampled images, demonstrating the advantage of\nfoveated sampling.\n", "versions": [{"version": "v1", "created": "Thu, 15 Aug 2019 11:32:48 GMT"}], "update_date": "2019-08-27", "authors_parsed": [["Jaramillo-Avila", "Uziel", ""], ["Anderson", "Sean R.", ""]]}, {"id": "1908.09001", "submitter": "Eduard Ramon", "authors": "Eduard Ramon, Guillermo Ruiz, Thomas Batard, Xavier Gir\\'o-i-Nieto", "title": "Hyperparameter-Free Losses for Model-Based Monocular Reconstruction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work proposes novel hyperparameter-free losses for single view 3D\nreconstruction with morphable models (3DMM). We dispense with the\nhyperparameters used in other works by exploiting geometry, so that the shape\nof the object and the camera pose are jointly optimized in a sole term\nexpression. This simplification reduces the optimization time and its\ncomplexity. Moreover, we propose a novel implicit regularization technique\nbased on random virtual projections that does not require additional 2D or 3D\nannotations. Our experiments suggest that minimizing a shape reprojection error\ntogether with the proposed implicit regularization is especially suitable for\napplications that require precise alignment between geometry and image spaces,\nsuch as augmented reality. We evaluate our losses on a large scale dataset with\n3D ground truth and publish our implementations to facilitate reproducibility\nand public benchmarking in this field.\n", "versions": [{"version": "v1", "created": "Fri, 16 Aug 2019 14:32:19 GMT"}], "update_date": "2019-08-28", "authors_parsed": [["Ramon", "Eduard", ""], ["Ruiz", "Guillermo", ""], ["Batard", "Thomas", ""], ["Gir\u00f3-i-Nieto", "Xavier", ""]]}, {"id": "1908.09002", "submitter": "Chris Xiaoxuan Lu", "authors": "Chris Xiaoxuan Lu, Xuan Kan, Bowen Du, Changhao Chen, Hongkai Wen,\n  Andrew Markham, Niki Trigoni and John Stankovic", "title": "Autonomous Learning for Face Recognition in the Wild via Ambient\n  Wireless Cues", "comments": "11 pages, accepted in the Web Conference (WWW'2019)", "journal-ref": null, "doi": "10.1145/3308558.3313398", "report-no": null, "categories": "cs.CV cs.LG cs.NI stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Facial recognition is a key enabling component for emerging Internet of\nThings (IoT) services such as smart homes or responsive offices. Through the\nuse of deep neural networks, facial recognition has achieved excellent\nperformance. However, this is only possibly when trained with hundreds of\nimages of each user in different viewing and lighting conditions. Clearly, this\nlevel of effort in enrolment and labelling is impossible for wide-spread\ndeployment and adoption. Inspired by the fact that most people carry smart\nwireless devices with them, e.g. smartphones, we propose to use this wireless\nidentifier as a supervisory label. This allows us to curate a dataset of facial\nimages that are unique to a certain domain e.g. a set of people in a particular\noffice. This custom corpus can then be used to finetune existing pre-trained\nmodels e.g. FaceNet. However, due to the vagaries of wireless propagation in\nbuildings, the supervisory labels are noisy and weak.We propose a novel\ntechnique, AutoTune, which learns and refines the association between a face\nand wireless identifier over time, by increasing the inter-cluster separation\nand minimizing the intra-cluster distance. Through extensive experiments with\nmultiple users on two sites, we demonstrate the ability of AutoTune to design\nan environment-specific, continually evolving facial recognition system with\nentirely no user effort.\n", "versions": [{"version": "v1", "created": "Wed, 14 Aug 2019 18:39:09 GMT"}], "update_date": "2019-08-27", "authors_parsed": [["Lu", "Chris Xiaoxuan", ""], ["Kan", "Xuan", ""], ["Du", "Bowen", ""], ["Chen", "Changhao", ""], ["Wen", "Hongkai", ""], ["Markham", "Andrew", ""], ["Trigoni", "Niki", ""], ["Stankovic", "John", ""]]}, {"id": "1908.09003", "submitter": "Singamsetti Mohan Sai", "authors": "S. Mohan Sai, G. Gopichand, C. Vikas Reddy, K. Mona Teja", "title": "High Accurate Unhealthy Leaf Detection", "comments": "Page 4, 5 with 1 figure, and page 6 with 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  India is an agriculture-dependent country. As we all know that farming is the\nbackbone of our country it is our responsibility to preserve the crops.\nHowever, we cannot stop the destruction of crops by natural calamities at least\nwe have to try to protect our crops from diseases. To, detect a plant disease\nwe need a fast automatic way. So, this paper presents a model to identify the\nparticular disease of plant leaves at early stages so that we can prevent or\ntake a remedy to stop spreading of the disease. This proposed model is made\ninto five sessions. Image preprocessing includes the enhancement of the low\nlight image done using inception modules in CNN. Low-resolution image\nenhancement is done using an Adversarial Neural Network. This also includes\nConversion of RGB Image to YCrCb color space. Next, this paper presents a\nmethodology for image segmentation which is an important aspect for identifying\nthe disease symptoms. This segmentation is done using the genetic algorithm.\nDue to this process the segmentation of the leaf Image this helps in detection\nof the leaf mage automatically and classifying. Texture extraction is done\nusing the statistical model called GLCM and finally, the classification of the\ndiseases is done using the SVM using Different Kernels with the high accuracy.\n", "versions": [{"version": "v1", "created": "Wed, 14 Aug 2019 16:42:36 GMT"}], "update_date": "2019-08-27", "authors_parsed": [["Sai", "S. Mohan", ""], ["Gopichand", "G.", ""], ["Reddy", "C. Vikas", ""], ["Teja", "K. Mona", ""]]}, {"id": "1908.09005", "submitter": "Alexander Khoperskov V.", "authors": "Anna Klikunova and Alexander Khoperskov", "title": "Creation of digital elevation models for river floodplains", "comments": "10 pages, 9 figures, V International conference Information\n  Technology and Nanotechnology. Session Image Processing and Earth Remote\n  Sensing (IPERS-ITNT 2019), Samara, Russia, May 21-24, 2019", "journal-ref": "CEUR Workshop Proceedings, 2019, vol.2391, pp.275-284", "doi": null, "report-no": null, "categories": "cs.CV eess.IV physics.geo-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A procedure for constructing a digital elevation model (DEM) of the northern\npart of the Volga-Akhtuba interfluve is described. The basis of our DEM is the\nelevation matrix of Shuttle Radar Topography Mission (SRTM) for which we\ncarried out the refinement and updating of spatial data using satellite\nimagery, GPS data, depth measurements of the River Volga and River Akhtuba\nstream beds. The most important source of high-altitude data for the\nVolga-Akhtuba floodplain (VAF) can be the results of observations of the\ncoastlines dynamics of small reservoirs (lakes, eriks, small channels) arising\nin the process of spring flooding and disappearing during low-flow periods. A\nset of digitized coastlines at different times of flooding can significantly\nimprove the quality of the DEM. The method of constructing a digital elevation\nmodel includes an iterative procedure that uses the results of morphostructural\nanalysis of the DEM and the numerical hydrodynamic simulations of the VAF\nflooding based on the shallow water model.\n", "versions": [{"version": "v1", "created": "Sun, 18 Aug 2019 10:09:47 GMT"}], "update_date": "2019-08-27", "authors_parsed": [["Klikunova", "Anna", ""], ["Khoperskov", "Alexander", ""]]}, {"id": "1908.09006", "submitter": "Siddharth Samsi", "authors": "Jeffrey Liu, David Strohschein, Siddharth Samsi, Andrew Weinert", "title": "Large Scale Organization and Inference of an Imagery Dataset for Public\n  Safety", "comments": "Accepted for publication IEEE HPEC 2019", "journal-ref": null, "doi": "10.1109/HPEC.2019.8916437", "report-no": null, "categories": "cs.CV cs.LG eess.IV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video applications and analytics are routinely projected as a stressing and\nsignificant service of the Nationwide Public Safety Broadband Network. As part\nof a NIST PSCR funded effort, the New Jersey Office of Homeland Security and\nPreparedness and MIT Lincoln Laboratory have been developing a computer vision\ndataset of operational and representative public safety scenarios. The scale\nand scope of this dataset necessitates a hierarchical organization approach for\nefficient compute and storage. We overview architectural considerations using\nthe Lincoln Laboratory Supercomputing Cluster as a test architecture. We then\ndescribe how we intelligently organized the dataset across LLSC and evaluated\nit with large scale imagery inference across terabytes of data.\n", "versions": [{"version": "v1", "created": "Fri, 16 Aug 2019 18:20:01 GMT"}], "update_date": "2019-12-16", "authors_parsed": [["Liu", "Jeffrey", ""], ["Strohschein", "David", ""], ["Samsi", "Siddharth", ""], ["Weinert", "Andrew", ""]]}, {"id": "1908.09007", "submitter": "Maroua Mehri Ph.D.", "authors": "Walid Elhedda, Maroua Mehri and Mohamed Ali Mahjoub", "title": "A Comparative Study of Filtering Approaches Applied to Color Archival\n  Document Images", "comments": null, "journal-ref": null, "doi": null, "report-no": "W. Elhedda, M. Mehri and M. A. Mahjoub, A Comparative Study of\n  Filtering Approaches Applied to Color Archival Document Images. In\n  Proceedings of the 18th International Arab Conference on Information\n  Technology (ACIT), Hammamet, Tunisia, 2017", "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current systems used by the Tunisian national archives for the automatic\ntranscription of archival documents are hindered by many issues related to the\nperformance of the optical character recognition (OCR) tools. Indeed, using a\nclassical OCR system to transcribe and index ancient Arabic documents is not a\nstraightforward task due to the idiosyncrasies of this category of documents,\nsuch as noise and degradation. Thus, applying an enhancement method or a\ndenoising technique remains an essential prerequisite step to ease the archival\ndocument image analysis task. The state-of-the-art methods addressing the use\nof degraded document image enhancement and denoising are mainly based on\napplying filters. The most common filtering techniques applied to color images\nin the literature may be categorized into four approaches: scalar, marginal,\nvector and hybrid. To provide a set of comprehensive guidelines on the\nstrengths and weaknesses of these filtering approaches, a thorough comparative\nstudy is proposed in this article. Numerical experiments are carried out in\nthis study on color archival document images to show and quantify the\nperformance of each assessed filtering approach.\n", "versions": [{"version": "v1", "created": "Fri, 16 Aug 2019 12:05:14 GMT"}], "update_date": "2019-08-27", "authors_parsed": [["Elhedda", "Walid", ""], ["Mehri", "Maroua", ""], ["Mahjoub", "Mohamed Ali", ""]]}, {"id": "1908.09008", "submitter": "Apratim Bhattacharyya", "authors": "Apratim Bhattacharyya, Michael Hanselmann, Mario Fritz, Bernt Schiele,\n  Christoph-Nikolas Straehle", "title": "Conditional Flow Variational Autoencoders for Structured Sequence\n  Prediction", "comments": "To appear at Bayesian Deep Learning and Machine Learning for\n  Autonomous Driving @NeurIPS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Prediction of future states of the environment and interacting agents is a\nkey competence required for autonomous agents to operate successfully in the\nreal world. Prior work for structured sequence prediction based on latent\nvariable models imposes a uni-modal standard Gaussian prior on the latent\nvariables. This induces a strong model bias which makes it challenging to fully\ncapture the multi-modality of the distribution of the future states. In this\nwork, we introduce Conditional Flow Variational Autoencoders (CF-VAE) using our\nnovel conditional normalizing flow based prior to capture complex multi-modal\nconditional distributions for effective structured sequence prediction.\nMoreover, we propose two novel regularization schemes which stabilizes training\nand deals with posterior collapse for stable training and better fit to the\ntarget data distribution. Our experiments on three multi-modal structured\nsequence prediction datasets -- MNIST Sequences, Stanford Drone and HighD --\nshow that the proposed method obtains state of art results across different\nevaluation metrics.\n", "versions": [{"version": "v1", "created": "Sat, 24 Aug 2019 08:02:34 GMT"}, {"version": "v2", "created": "Tue, 8 Oct 2019 10:44:50 GMT"}, {"version": "v3", "created": "Tue, 18 Aug 2020 09:55:31 GMT"}], "update_date": "2020-08-19", "authors_parsed": [["Bhattacharyya", "Apratim", ""], ["Hanselmann", "Michael", ""], ["Fritz", "Mario", ""], ["Schiele", "Bernt", ""], ["Straehle", "Christoph-Nikolas", ""]]}, {"id": "1908.09009", "submitter": "Gideon Gbenga Oladipupo", "authors": "Gideon Gbenga Oladipupo", "title": "Development of a Robotic System for Automatic Wheel Removal and Fitting", "comments": "8 pages, 17 figures and 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO eess.IV", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  This paper discusses the image processing and computer vision algorithms for\nreal time detection and tracking of a sample wheel of a vehicle. During the\nmanual tyre changing process, spinal and other muscular injuries are common and\neven more serious injuries have been recorded when occasionally, tyres fail\n(burst) during this process. It, therefore, follows that the introduction of a\nrobotic system to take over this process would be a welcome development. This\nwork discusses various useful applicable algorithms, Circular Hough Transform\n(CHT) as well as Continuously adaptive mean shift (Camshift) and provides some\nof the software solutions which can be deployed with a robotic mechanical arm\nto make the task of tyre changing faster, safer and more efficient. Image\nacquisition and software to accurately detect and classify specific objects of\ninterest were implemented successfully, outcomes were discussed and areas for\nfurther studies suggested.\n", "versions": [{"version": "v1", "created": "Mon, 19 Aug 2019 09:24:59 GMT"}], "update_date": "2019-08-27", "authors_parsed": [["Oladipupo", "Gideon Gbenga", ""]]}, {"id": "1908.09031", "submitter": "Jiachen Li", "authors": "Jiachen Li and Wei Zhan and Yeping Hu and Masayoshi Tomizuka", "title": "Generic Tracking and Probabilistic Prediction Framework and Its\n  Application in Autonomous Driving", "comments": "IEEE Transactions on Intelligent Transportation Systems", "journal-ref": null, "doi": "10.1109/TITS.2019.2930310", "report-no": null, "categories": "cs.RO cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurately tracking and predicting behaviors of surrounding objects are key\nprerequisites for intelligent systems such as autonomous vehicles to achieve\nsafe and high-quality decision making and motion planning. However, there still\nremain challenges for multi-target tracking due to object number fluctuation\nand occlusion. To overcome these challenges, we propose a constrained mixture\nsequential Monte Carlo (CMSMC) method in which a mixture representation is\nincorporated in the estimated posterior distribution to maintain\nmulti-modality. Multiple targets can be tracked simultaneously within a unified\nframework without explicit data association between observations and tracking\ntargets. The framework can incorporate an arbitrary prediction model as the\nimplicit proposal distribution of the CMSMC method. An example in this paper is\na learning-based model for hierarchical time-series prediction, which consists\nof a behavior recognition module and a state evolution module. Both modules in\nthe proposed model are generic and flexible so as to be applied to a class of\ntime-series prediction problems where behaviors can be separated into different\nlevels. Finally, the proposed framework is applied to a numerical case study as\nwell as a task of on-road vehicle tracking, behavior recognition, and\nprediction in highway scenarios. Instead of only focusing on forecasting\ntrajectory of a single entity, we jointly predict continuous motions for\ninteractive entities simultaneously. The proposed approaches are evaluated from\nmultiple aspects, which demonstrate great potential for intelligent vehicular\nsystems and traffic surveillance systems.\n", "versions": [{"version": "v1", "created": "Fri, 23 Aug 2019 20:34:53 GMT"}], "update_date": "2020-03-31", "authors_parsed": [["Li", "Jiachen", ""], ["Zhan", "Wei", ""], ["Hu", "Yeping", ""], ["Tomizuka", "Masayoshi", ""]]}, {"id": "1908.09060", "submitter": "Zhengyang Wu", "authors": "Zhengyang Wu, Srivignesh Rajendran, Tarrence van As, Joelle\n  Zimmermann, Vijay Badrinarayanan, Andrew Rabinovich", "title": "EyeNet: A Multi-Task Network for Off-Axis Eye Gaze Estimation and User\n  Understanding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Eye gaze estimation and simultaneous semantic understanding of a user through\neye images is a crucial component in Virtual and Mixed Reality; enabling energy\nefficient rendering, multi-focal displays and effective interaction with 3D\ncontent. In head-mounted VR/MR devices the eyes are imaged off-axis to avoid\nblocking the user's gaze, this view-point makes drawing eye related inferences\nvery challenging. In this work, we present EyeNet, the first single deep neural\nnetwork which solves multiple heterogeneous tasks related to eye gaze\nestimation and semantic user understanding for an off-axis camera setting. The\ntasks include eye segmentation, blink detection, emotive expression\nclassification, IR LED glints detection, pupil and cornea center estimation. To\ntrain EyeNet end-to-end we employ both hand labelled supervision and model\nbased supervision. We benchmark all tasks on MagicEyes, a large and new dataset\nof 587 subjects with varying morphology, gender, skin-color, make-up and\nimaging conditions.\n", "versions": [{"version": "v1", "created": "Sat, 24 Aug 2019 00:47:39 GMT"}], "update_date": "2019-08-27", "authors_parsed": [["Wu", "Zhengyang", ""], ["Rajendran", "Srivignesh", ""], ["van As", "Tarrence", ""], ["Zimmermann", "Joelle", ""], ["Badrinarayanan", "Vijay", ""], ["Rabinovich", "Andrew", ""]]}, {"id": "1908.09066", "submitter": "Le Zhang Dr", "authors": "Le Zhang, Zenglin Shi, Ming-Ming Cheng, Yun Liu, Jia-Wang Bian, Joey\n  Tianyi Zhou, Guoyan Zheng and Zeng Zeng", "title": "Robust Regression via Deep Negative Correlation Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nonlinear regression has been extensively employed in many computer vision\nproblems (e.g., crowd counting, age estimation, affective computing). Under the\numbrella of deep learning, two common solutions exist i) transforming nonlinear\nregression to a robust loss function which is jointly optimizable with the deep\nconvolutional network, and ii) utilizing ensemble of deep networks. Although\nsome improved performance is achieved, the former may be lacking due to the\nintrinsic limitation of choosing a single hypothesis and the latter usually\nsuffers from much larger computational complexity. To cope with those issues,\nwe propose to regress via an efficient \"divide and conquer\" manner. The core of\nour approach is the generalization of negative correlation learning that has\nbeen shown, both theoretically and empirically, to work well for non-deep\nregression problems. Without extra parameters, the proposed method controls the\nbias-variance-covariance trade-off systematically and usually yields a deep\nregression ensemble where each base model is both \"accurate\" and \"diversified\".\nMoreover, we show that each sub-problem in the proposed method has less\nRademacher Complexity and thus is easier to optimize. Extensive experiments on\nseveral diverse and challenging tasks including crowd counting, personality\nanalysis, age estimation, and image super-resolution demonstrate the\nsuperiority over challenging baselines as well as the versatility of the\nproposed method.\n", "versions": [{"version": "v1", "created": "Sat, 24 Aug 2019 01:26:03 GMT"}], "update_date": "2019-08-27", "authors_parsed": [["Zhang", "Le", ""], ["Shi", "Zenglin", ""], ["Cheng", "Ming-Ming", ""], ["Liu", "Yun", ""], ["Bian", "Jia-Wang", ""], ["Zhou", "Joey Tianyi", ""], ["Zheng", "Guoyan", ""], ["Zeng", "Zeng", ""]]}, {"id": "1908.09067", "submitter": "Okyaz Eminaga", "authors": "Okyaz Eminaga, Mahmoud Abbas, Christian Kunder, Andreas M. Loening,\n  Jeanne Shen, James D. Brooks, Curtis P. Langlotz, and Daniel L. Rubin", "title": "Plexus Convolutional Neural Network (PlexusNet): A novel neural network\n  architecture for histologic image analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM cs.AI cs.CV eess.IV q-bio.TO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Different convolutional neural network (CNN) models have been tested for\ntheir application in histological image analyses. However, these models are\nprone to overfitting due to their large parameter capacity, requiring more data\nor valuable computational resources for model training. Given these\nlimitations, we introduced a novel architecture (termed PlexusNet). We utilized\n310 Hematoxylin and Eosin stained (H&E) annotated histological images of\nprostate cancer cases from TCGA-PRAD and Stanford University and 398 H&E whole\nslides images from the Camelyon 2016 challenge. PlexusNet-architecture -derived\nmodels were compared to models derived from several existing \"state of the art\"\narchitectures. We measured discrimination accuracy, calibration, and clinical\nutility. An ablation study was conducted to study the effect of each component\nof PlexusNet on model performance. A well-fitted PlexusNet-based model\ndelivered comparable classification performance (AUC: 0.963) in distinguishing\nprostate cancer from healthy tissues, although it was at least 23 times\nsmaller, had a better model calibration and clinical utility than the\ncomparison models. A separate smaller PlexusNet model accurately detected\nslides with breast cancer metastases (AUC: 0.978); it helped reduce the slide\nnumber to examine by 43.8% without consequences, although its parameter\ncapacity was 200 times smaller than ResNet18. We found that the partitioning of\nthe development set influences the model calibration for all models. However,\nwith PlexusNet architecture, we could achieve comparable well-calibrated models\ntrained on different partitions. In conclusion, PlexusNet represents a novel\nmodel architecture for histological image analysis that achieves classification\nperformance comparable to other models while providing orders-of-magnitude\nparameter reduction.\n", "versions": [{"version": "v1", "created": "Sat, 24 Aug 2019 01:29:34 GMT"}, {"version": "v2", "created": "Wed, 3 Jun 2020 04:43:21 GMT"}], "update_date": "2020-06-04", "authors_parsed": [["Eminaga", "Okyaz", ""], ["Abbas", "Mahmoud", ""], ["Kunder", "Christian", ""], ["Loening", "Andreas M.", ""], ["Shen", "Jeanne", ""], ["Brooks", "James D.", ""], ["Langlotz", "Curtis P.", ""], ["Rubin", "Daniel L.", ""]]}, {"id": "1908.09072", "submitter": "Zhaobing Kang", "authors": "Zhaobing Kang, Wei Zou and Zheng Zhu", "title": "Camera Pose Correction in SLAM Based on Bias Values of Map Points", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurate camera pose estimation result is essential for visual SLAM (VSLAM).\nThis paper presents a novel pose correction method to improve the accuracy of\nthe VSLAM system. Firstly, the relationship between the camera pose estimation\nerror and bias values of map points is derived based on the optimized function\nin VSLAM. Secondly, the bias value of the map point is calculated by a\nstatistical method. Finally, the camera pose estimation error is compensated\naccording to the first derived relationship. After the pose correction,\nprocedures of the original system, such as the bundle adjustment (BA)\noptimization, can be executed as before. Compared with existing methods, our\nalgorithm is compact and effective and can be easily generalized to different\nVSLAM systems. Additionally, the robustness to system noise of our method is\nbetter than feature selection methods, due to all original system information\nis preserved in our algorithm while only a subset is employed in the latter.\nExperimental results on benchmark datasets show that our approach leads to\nconsiderable improvements over state-of-the-art algorithms for absolute pose\nestimation.\n", "versions": [{"version": "v1", "created": "Sat, 24 Aug 2019 02:07:51 GMT"}], "update_date": "2019-08-27", "authors_parsed": [["Kang", "Zhaobing", ""], ["Zou", "Wei", ""], ["Zhu", "Zheng", ""]]}, {"id": "1908.09073", "submitter": "William Shen", "authors": "William B. Shen, Danfei Xu, Yuke Zhu, Leonidas J. Guibas, Li Fei-Fei,\n  Silvio Savarese", "title": "Situational Fusion of Visual Representation for Visual Navigation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A complex visual navigation task puts an agent in different situations which\ncall for a diverse range of visual perception abilities. For example, to \"go to\nthe nearest chair'', the agent might need to identify a chair in a living room\nusing semantics, follow along a hallway using vanishing point cues, and avoid\nobstacles using depth. Therefore, utilizing the appropriate visual perception\nabilities based on a situational understanding of the visual environment can\nempower these navigation models in unseen visual environments. We propose to\ntrain an agent to fuse a large set of visual representations that correspond to\ndiverse visual perception abilities. To fully utilize each representation, we\ndevelop an action-level representation fusion scheme, which predicts an action\ncandidate from each representation and adaptively consolidate these action\ncandidates into the final action. Furthermore, we employ a data-driven\ninter-task affinity regularization to reduce redundancies and improve\ngeneralization. Our approach leads to a significantly improved performance in\nnovel environments over ImageNet-pretrained baseline and other fusion methods.\n", "versions": [{"version": "v1", "created": "Sat, 24 Aug 2019 02:20:43 GMT"}], "update_date": "2019-08-27", "authors_parsed": [["Shen", "William B.", ""], ["Xu", "Danfei", ""], ["Zhu", "Yuke", ""], ["Guibas", "Leonidas J.", ""], ["Fei-Fei", "Li", ""], ["Savarese", "Silvio", ""]]}, {"id": "1908.09075", "submitter": "Chen Joya", "authors": "Joya Chen, Dong Liu, Bin Luo, Xuezheng Peng, Tong Xu, Enhong Chen", "title": "Residual Objectness for Imbalance Reduction", "comments": "Tech report", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For a long time, object detectors have suffered from extreme imbalance\nbetween foregrounds and backgrounds. While several sampling/reweighting schemes\nhave been explored to alleviate the imbalance, they are usually heuristic and\ndemand laborious hyper-parameters tuning, which is hard to achieve the\noptimality. In this paper, we first reveal that such the imbalance could be\naddressed in a learning-based manner. Guided by this illuminating observation,\nwe propose a novel Residual Objectness (ResObj) mechanism that addresses the\nimbalance by end-to-end optimization, while no further hand-crafted\nsampling/reweighting is required. Specifically, by applying multiple cascaded\nobjectness-related modules with residual connections, we formulate an elegant\nconsecutive refinement procedure for distinguishing the foregrounds from\nbackgrounds, thereby progressively addressing the imbalance. Extensive\nexperiments present the effectiveness of our method, as well as its\ncompatibility and adaptivity for both region-based and one-stage detectors,\nnamely, the RetinaNet-ResObj, YOLOv3-ResObj and FasterRCNN-ResObj achieve\nrelative 3.6%, 3.9%, 3.2% Average Precision (AP) improvements compared with\ntheir vanilla models on COCO, respectively.\n", "versions": [{"version": "v1", "created": "Sat, 24 Aug 2019 02:24:25 GMT"}], "update_date": "2019-08-27", "authors_parsed": [["Chen", "Joya", ""], ["Liu", "Dong", ""], ["Luo", "Bin", ""], ["Peng", "Xuezheng", ""], ["Xu", "Tong", ""], ["Chen", "Enhong", ""]]}, {"id": "1908.09086", "submitter": "Yan Huang", "authors": "Yan Huang, Qiang Wu, JingSong Xu, Yi Zhong", "title": "SBSGAN: Suppression of Inter-Domain Background Shift for Person\n  Re-Identification", "comments": "Accepted by ICCV2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cross-domain person re-identification (re-ID) is challenging due to the bias\nbetween training and testing domains. We observe that if backgrounds in the\ntraining and testing datasets are very different, it dramatically introduces\ndifficulties to extract robust pedestrian features, and thus compromises the\ncross-domain person re-ID performance. In this paper, we formulate such\nproblems as a background shift problem. A Suppression of Background Shift\nGenerative Adversarial Network (SBSGAN) is proposed to generate images with\nsuppressed backgrounds. Unlike simply removing backgrounds using binary masks,\nSBSGAN allows the generator to decide whether pixels should be preserved or\nsuppressed to reduce segmentation errors caused by noisy foreground masks.\nAdditionally, we take ID-related cues, such as vehicles and companions into\nconsideration. With high-quality generated images, a Densely Associated\n2-Stream (DA-2S) network is introduced with Inter Stream Densely Connection\n(ISDC) modules to strengthen the complementarity of the generated data and\nID-related cues. The experiments show that the proposed method achieves\ncompetitive performance on three re-ID datasets, ie., Market-1501,\nDukeMTMC-reID, and CUHK03, under the cross-domain person re-ID scenario.\n", "versions": [{"version": "v1", "created": "Sat, 24 Aug 2019 03:48:28 GMT"}], "update_date": "2019-08-27", "authors_parsed": [["Huang", "Yan", ""], ["Wu", "Qiang", ""], ["Xu", "JingSong", ""], ["Zhong", "Yi", ""]]}, {"id": "1908.09101", "submitter": "Haiyang Mei", "authors": "Xin Yang, Haiyang Mei, Ke Xu, Xiaopeng Wei, Baocai Yin, Rynson W.H.\n  Lau", "title": "Where Is My Mirror?", "comments": "Accepted by ICCV 2019. Project homepage:\n  https://mhaiyang.github.io/ICCV2019_MirrorNet/index.html", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mirrors are everywhere in our daily lives. Existing computer vision systems\ndo not consider mirrors, and hence may get confused by the reflected content\ninside a mirror, resulting in a severe performance degradation. However,\nseparating the real content outside a mirror from the reflected content inside\nit is non-trivial. The key challenge is that mirrors typically reflect contents\nsimilar to their surroundings, making it very difficult to differentiate the\ntwo. In this paper, we present a novel method to segment mirrors from an input\nimage. To the best of our knowledge, this is the first work to address the\nmirror segmentation problem with a computational approach. We make the\nfollowing contributions. First, we construct a large-scale mirror dataset that\ncontains mirror images with corresponding manually annotated masks. This\ndataset covers a variety of daily life scenes, and will be made publicly\navailable for future research. Second, we propose a novel network, called\nMirrorNet, for mirror segmentation, by modeling both semantical and low-level\ncolor/texture discontinuities between the contents inside and outside of the\nmirrors. Third, we conduct extensive experiments to evaluate the proposed\nmethod, and show that it outperforms the carefully chosen baselines from the\nstate-of-the-art detection and segmentation methods.\n", "versions": [{"version": "v1", "created": "Sat, 24 Aug 2019 06:57:04 GMT"}, {"version": "v2", "created": "Thu, 3 Oct 2019 10:44:28 GMT"}], "update_date": "2019-10-04", "authors_parsed": [["Yang", "Xin", ""], ["Mei", "Haiyang", ""], ["Xu", "Ke", ""], ["Wei", "Xiaopeng", ""], ["Yin", "Baocai", ""], ["Lau", "Rynson W. H.", ""]]}, {"id": "1908.09104", "submitter": "Yujie Lin", "authors": "Yujie Lin, Pengjie Ren, Zhumin Chen, Zhaochun Ren, Jun Ma, Maarten de\n  Rijke", "title": "Improving Outfit Recommendation with Co-supervision of Fashion\n  Generation", "comments": null, "journal-ref": null, "doi": "10.1145/3308558.3313614", "report-no": null, "categories": "cs.IR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The task of fashion recommendation includes two main challenges: visual\nunderstanding and visual matching. Visual understanding aims to extract\neffective visual features. Visual matching aims to model a human notion of\ncompatibility to compute a match between fashion items. Most previous studies\nrely on recommendation loss alone to guide visual understanding and matching.\nAlthough the features captured by these methods describe basic characteristics\n(e.g., color, texture, shape) of the input items, they are not directly related\nto the visual signals of the output items (to be recommended). This is\nproblematic because the aesthetic characteristics (e.g., style, design), based\non which we can directly infer the output items, are lacking. Features are\nlearned under the recommendation loss alone, where the supervision signal is\nsimply whether the given two items are matched or not. To address this problem,\nwe propose a neural co-supervision learning framework, called the FAshion\nRecommendation Machine (FARM). FARM improves visual understanding by\nincorporating the supervision of generation loss, which we hypothesize to be\nable to better encode aesthetic information. FARM enhances visual matching by\nintroducing a novel layer-to-layer matching mechanism to fuse aesthetic\ninformation more effectively, and meanwhile avoiding paying too much attention\nto the generation quality and ignoring the recommendation performance.\nExtensive experiments on two publicly available datasets show that FARM\noutperforms state-of-the-art models on outfit recommendation, in terms of AUC\nand MRR. Detailed analyses of generated and recommended items demonstrate that\nFARM can encode better features and generate high quality images as references\nto improve recommendation performance.\n", "versions": [{"version": "v1", "created": "Sat, 24 Aug 2019 07:37:57 GMT"}], "update_date": "2019-08-27", "authors_parsed": [["Lin", "Yujie", ""], ["Ren", "Pengjie", ""], ["Chen", "Zhumin", ""], ["Ren", "Zhaochun", ""], ["Ma", "Jun", ""], ["de Rijke", "Maarten", ""]]}, {"id": "1908.09108", "submitter": "Sagi Eppel", "authors": "Sagi Eppel, Alan Aspuru-Guzik", "title": "Generator evaluator-selector net for panoptic image segmentation and\n  splitting unfamiliar objects into parts", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In machine learning and other fields, suggesting a good solution to a problem\nis usually a harder task than evaluating the quality of such a solution. This\nasymmetry is the basis for a large number of selection oriented methods that\nuse a generator system to guess a set of solutions and an evaluator system to\nrank and select the best solutions. This work examines the use of this approach\nto the problem of panoptic image segmentation and class agnostic parts\nsegmentation. The generator/evaluator approach for this case consists of two\nindependent convolutional neural nets: a generator net that suggests variety\nsegments corresponding to objects, stuff and parts regions in the image, and an\nevaluator net that chooses the best segments to be merged into the segmentation\nmap. The result is a trial and error evolutionary approach in which a generator\nthat guesses segments with low average accuracy, but with wide variability, can\nstill produce good results when coupled with an accurate evaluator. The\ngenerator consists of a Pointer net that receives an image and a point in the\nimage, and predicts the region of the segment containing the point. Generating\nand evaluating each segment separately is essential in this case since it\ndemands exponentially fewer guesses compared to a system that guesses and\nevaluates the full segmentation map in each try. The classification of the\nselected segments is done by an independent region-specific classification net.\nThis allows the segmentation to be class agnostic and hence, capable of\nsegmenting unfamiliar categories that were not part of the training set. The\nmethod was examined on the COCO Panoptic segmentation benchmark and gave\nresults comparable to those of the basic semantic segmentation and Mask-RCNN\nmethods. In addition, the system was used for the task of splitting objects of\nunseen classes (that did not appear in the training set) into parts.\n", "versions": [{"version": "v1", "created": "Sat, 24 Aug 2019 09:01:27 GMT"}, {"version": "v2", "created": "Tue, 27 Aug 2019 02:02:07 GMT"}, {"version": "v3", "created": "Wed, 8 Apr 2020 19:28:03 GMT"}, {"version": "v4", "created": "Mon, 13 Apr 2020 08:55:36 GMT"}], "update_date": "2020-04-14", "authors_parsed": [["Eppel", "Sagi", ""], ["Aspuru-Guzik", "Alan", ""]]}, {"id": "1908.09124", "submitter": "Jintao Zhang", "authors": "Jintao Zhang", "title": "SeesawFaceNets: sparse and robust face verification model for mobile\n  platform", "comments": "8 pages, 2 figures. All source code and proposed models will be\n  released publicly later", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Convolutional Neural Network (DCNNs) come to be the most widely used\nsolution for most computer vision related tasks, and one of the most important\napplication scenes is face verification. Due to its high-accuracy performance,\ndeep face verification models of which the inference stage occurs on cloud\nplatform through internet plays the key role on most prectical scenes. However,\ntwo critical issues exist: First, individual privacy may not be well protected\nsince they have to upload their personal photo and other private information to\nthe online cloud backend. Secondly, either training or inference stage is\ntime-comsuming and the latency may affect customer experience, especially when\nthe internet link speed is not so stable or in remote areas where mobile\nreception is not so good, but also in cities where building and other\nconstruction may block mobile signals. Therefore, designing lightweight\nnetworks with low memory requirement and computational cost is one of the most\npractical solutions for face verification on mobile platform. In this paper, a\nnovel mobile network named SeesawFaceNets, a simple but effective model, is\nproposed for productively deploying face recognition for mobile devices. Dense\nexperimental results have shown that our proposed model SeesawFaceNets\noutperforms the baseline MobilefaceNets, with only {\\bf66\\%}(146M VS 221M\nMAdds) computational cost, smaller batch size and less training steps, and\nSeesawFaceNets achieve comparable performance with other SOTA model e.g.\nmobiface with only {\\bf54.2\\%}(1.3M VS 2.4M) parameters and {\\bf31.6\\%}(146M VS\n462M MAdds) computational cost, It is also eventually competitive against\nlarge-scale deep-networks face recognition on all 5 listed public validation\ndatasets, with {\\bf6.5\\%}(4.2M VS 65M) parameters and {\\bf4.35\\%}(526M VS 12G\nMAdds) computational cost.\n", "versions": [{"version": "v1", "created": "Sat, 24 Aug 2019 11:21:38 GMT"}, {"version": "v2", "created": "Tue, 27 Aug 2019 17:54:54 GMT"}, {"version": "v3", "created": "Sun, 1 Dec 2019 12:52:26 GMT"}], "update_date": "2019-12-03", "authors_parsed": [["Zhang", "Jintao", ""]]}, {"id": "1908.09140", "submitter": "Shanshan Wang", "authors": "Shanshan Wang, Yanxia Chen, Taohui Xiao, Ziwen Ke, Qiegen Liu, Hairong\n  Zheng", "title": "LANTERN: learn analysis transform network for dynamic magnetic resonance\n  imaging with small dataset", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes to learn analysis transform network for dynamic magnetic\nresonance imaging (LANTERN) with small dataset. Integrating the strength of\nCS-MRI and deep learning, the proposed framework is highlighted in three\ncomponents: (i) The spatial and temporal domains are sparsely constrained by\nusing adaptively trained CNN. (ii) We introduce an end-to-end framework to\nlearn the parameters in LANTERN to solve the difficulty of parameter selection\nin traditional methods. (iii) Compared to existing deep learning reconstruction\nmethods, our reconstruction accuracy is better when the amount of data is\nlimited. Our model is able to fully exploit the redundancy in spatial and\ntemporal of dynamic MR images. We performed quantitative and qualitative\nanalysis of cardiac datasets at different acceleration factors (2x-11x) and\ndifferent undersampling modes. In comparison with state-of-the-art methods,\nextensive experiments show that our method achieves consistent better\nreconstruction performance on the MRI reconstruction in terms of three\nquantitative metrics (PSNR, SSIM and HFEN) under different undersamling\npatterns and acceleration factors.\n", "versions": [{"version": "v1", "created": "Sat, 24 Aug 2019 14:04:58 GMT"}], "update_date": "2019-08-27", "authors_parsed": [["Wang", "Shanshan", ""], ["Chen", "Yanxia", ""], ["Xiao", "Taohui", ""], ["Ke", "Ziwen", ""], ["Liu", "Qiegen", ""], ["Zheng", "Hairong", ""]]}, {"id": "1908.09148", "submitter": "Szymon P{\\l}otka", "authors": "Tomasz W{\\l}odarczyk, Szymon P{\\l}otka, Tomasz Trzci\\'nski,\n  Przemys{\\l}aw Rokita, Nicole Sochacki-W\\'ojcicka, Micha{\\l} Lipa, Jakub\n  W\\'ojcicki", "title": "Estimation of preterm birth markers with U-Net segmentation network", "comments": "Accepted at MICCAI Workshop on Perinatal, Preterm and Paediatric\n  Image analysis (PIPPI) 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Preterm birth is the most common cause of neonatal death. Current diagnostic\nmethods that assess the risk of preterm birth involve the collection of\nmaternal characteristics and transvaginal ultrasound imaging conducted in the\nfirst and second trimester of pregnancy. Analysis of the ultrasound data is\nbased on visual inspection of images by gynaecologist, sometimes supported by\nhand-designed image features such as cervical length. Due to the complexity of\nthis process and its subjective component, approximately 30% of spontaneous\npreterm deliveries are not correctly predicted. Moreover, 10% of the predicted\npreterm deliveries are false-positives. In this paper, we address the problem\nof predicting spontaneous preterm delivery using machine learning. To achieve\nthis goal, we propose to first use a deep neural network architecture for\nsegmenting prenatal ultrasound images and then automatically extract two\nbiophysical ultrasound markers, cervical length (CL) and anterior cervical\nangle (ACA), from the resulting images. Our method allows to estimate\nultrasound markers without human oversight. Furthermore, we show that CL and\nACA markers, when combined, allow us to decrease false-negative ratio from 30%\nto 18%. Finally, contrary to the current approaches to diagnostics methods that\nrely only on gynaecologist's expertise, our method introduce objectively\nobtained results.\n", "versions": [{"version": "v1", "created": "Sat, 24 Aug 2019 15:14:11 GMT"}], "update_date": "2019-08-27", "authors_parsed": [["W\u0142odarczyk", "Tomasz", ""], ["P\u0142otka", "Szymon", ""], ["Trzci\u0144ski", "Tomasz", ""], ["Rokita", "Przemys\u0142aw", ""], ["Sochacki-W\u00f3jcicka", "Nicole", ""], ["Lipa", "Micha\u0142", ""], ["W\u00f3jcicki", "Jakub", ""]]}, {"id": "1908.09162", "submitter": "Thomas Spilsbury", "authors": "Thomas Spilsbury, Paavo Camps", "title": "Don't ignore Dropout in Fully Convolutional Networks", "comments": "14 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Data for Image segmentation models can be costly to obtain due to the\nprecision required by human annotators. We run a series of experiments showing\nthe effect of different kinds of Dropout training on the DeepLabv3+ Image\nsegmentation model when trained using a small dataset. We find that when\nappropriate forms of Dropout are applied in the right place in the model\narchitecture that non-insignificant improvement in Mean Intersection over Union\n(mIoU) score can be observed. In our best case, we find that applying Dropout\nscheduling in conjunction with SpatialDropout improves baseline mIoU from 0.49\nto 0.59. This result shows that even where a model architecture makes extensive\nuse of Batch Normalization, Dropout can still be an effective way of improving\nperformance in low data situations.\n", "versions": [{"version": "v1", "created": "Sat, 24 Aug 2019 16:28:40 GMT"}], "update_date": "2019-08-27", "authors_parsed": [["Spilsbury", "Thomas", ""], ["Camps", "Paavo", ""]]}, {"id": "1908.09163", "submitter": "Giorgos Tolias", "authors": "Giorgos Tolias and Filip Radenovic and Ond\\v{r}ej Chum", "title": "Targeted Mismatch Adversarial Attack: Query with a Flower to Retrieve\n  the Tower", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Access to online visual search engines implies sharing of private user\ncontent - the query images. We introduce the concept of targeted mismatch\nattack for deep learning based retrieval systems to generate an adversarial\nimage to conceal the query image. The generated image looks nothing like the\nuser intended query, but leads to identical or very similar retrieval results.\nTransferring attacks to fully unseen networks is challenging. We show\nsuccessful attacks to partially unknown systems, by designing various loss\nfunctions for the adversarial image construction. These include loss functions,\nfor example, for unknown global pooling operation or unknown input resolution\nby the retrieval system. We evaluate the attacks on standard retrieval\nbenchmarks and compare the results retrieved with the original and adversarial\nimage.\n", "versions": [{"version": "v1", "created": "Sat, 24 Aug 2019 16:30:44 GMT"}], "update_date": "2019-08-28", "authors_parsed": [["Tolias", "Giorgos", ""], ["Radenovic", "Filip", ""], ["Chum", "Ond\u0159ej", ""]]}, {"id": "1908.09186", "submitter": "Sergey Prokudin", "authors": "Sergey Prokudin, Christoph Lassner, Javier Romero", "title": "Efficient Learning on Point Clouds with Basis Point Sets", "comments": "ICCV 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the increased availability of 3D scanning technology, point clouds are\nmoving into the focus of computer vision as a rich representation of everyday\nscenes. However, they are hard to handle for machine learning algorithms due to\ntheir unordered structure. One common approach is to apply occupancy grid\nmapping, which dramatically increases the amount of data stored and at the same\ntime loses details through discretization. Recently, deep learning models were\nproposed to handle point clouds directly and achieve input permutation\ninvariance. However, these architectures often use an increased number of\nparameters and are computationally inefficient. In this work, we propose basis\npoint sets (BPS) as a highly efficient and fully general way to process point\nclouds with machine learning algorithms. The basis point set representation is\na residual representation that can be computed efficiently and can be used with\nstandard neural network architectures and other machine learning algorithms.\nUsing the proposed representation as the input to a simple fully connected\nnetwork allows us to match the performance of PointNet on a shape\nclassification task while using three orders of magnitude less floating-point\noperations. In a second experiment, we show how the proposed representation can\nbe used for registering high-resolution meshes to noisy 3D scans. Here, we\npresent the first method for single-pass high-resolution mesh registration,\navoiding time-consuming per-scan optimization and allowing real-time execution.\n", "versions": [{"version": "v1", "created": "Sat, 24 Aug 2019 18:53:52 GMT"}], "update_date": "2019-08-27", "authors_parsed": [["Prokudin", "Sergey", ""], ["Lassner", "Christoph", ""], ["Romero", "Javier", ""]]}, {"id": "1908.09191", "submitter": "Sivalogeswaran Ratnasingam", "authors": "Sivalogeswaran Ratnasingam", "title": "Deep Camera: A Fully Convolutional Neural Network for Image Signal\n  Processing", "comments": "11 pages, 6 figures, conference:ICCV 2019 workshop: Learning for\n  Computational Imaging (LCI)", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A conventional camera performs various signal processing steps sequentially\nto reconstruct an image from a raw Bayer image. When performing these\nprocessing in multiple stages the residual error from each stage accumulates in\nthe image and degrades the quality of the final reconstructed image. In this\npaper, we present a fully convolutional neural network (CNN) to perform defect\npixel correction, denoising, white balancing, exposure correction, demosaicing,\ncolor transform, and gamma encoding. To our knowledge, this is the first CNN\ntrained end-to-end to perform the entire image signal processing pipeline in a\ncamera. The neural network was trained using a large image database of raw\nBayer images. Through extensive experiments, we show that the proposed CNN\nbased image signal processing system performs better than the conventional\nsignal processing pipelines that perform the processing sequentially.\n", "versions": [{"version": "v1", "created": "Sat, 24 Aug 2019 19:31:15 GMT"}], "update_date": "2019-08-27", "authors_parsed": [["Ratnasingam", "Sivalogeswaran", ""]]}, {"id": "1908.09215", "submitter": "Pengfei Guo", "authors": "Pengfei Guo, Dawei Li, Xingde Li", "title": "Customized OCT images compression scheme with deep neural network", "comments": "One of author disagrees to release this paper at Arxiv", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We customize an end-to-end image compression framework for retina OCT images\nbased on deep convolutional neural networks (CNNs). The customized compression\nscheme consists of three parts: data Preprocessing, compression CNNs, and\nreconstruction CNNs. Data preprocessing module reduces the speckle noise of the\nOCT images and the segments out the region of interest. We added customized\nskip connections between the compression CNNs and the reconstruction CNNs to\nreserve the detail information and trained the two nets together with the\nsemantic segmented image patches from data preprocessing module. To train the\ntwo networks sensitive to both low frequency information and high frequency\ninformation, we adopted an objective function with two parts: A PatchGAN\ndiscriminator to judge the high frequency information and a differentiable\nMS-SSIM penalty to evaluate the low frequency information. The proposed\nframework was trained and evaluated on a publicly available OCT dataset. The\nevaluation showed above 99% similarity in terms of multi-scale structural\nsimilarity (MS-SSIM) when the compression ratio is as high as 40. Furthermore,\nthe reconstructed images of compression ratio 80 from the proposed framework\neven have better quality than that of compression ratio 20 from JPEG by visual\ncomparison. The testing result outperforms JPEG in term of both of MS-SSIM and\nvisualization, which is more obvious as the increase of compression ratio. Our\npreliminary result indicates the huge potential of deep neural networks on\ncustomized medical image compression.\n", "versions": [{"version": "v1", "created": "Sat, 24 Aug 2019 21:38:29 GMT"}, {"version": "v2", "created": "Tue, 27 Aug 2019 15:57:12 GMT"}], "update_date": "2019-08-28", "authors_parsed": [["Guo", "Pengfei", ""], ["Li", "Dawei", ""], ["Li", "Xingde", ""]]}, {"id": "1908.09216", "submitter": "Xuecheng Nie", "authors": "Xuecheng Nie and Yuncheng Li and Linjie Luo and Ning Zhang and Jiashi\n  Feng", "title": "Dynamic Kernel Distillation for Efficient Pose Estimation in Videos", "comments": "To appear in ICCV 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing video-based human pose estimation methods extensively apply large\nnetworks onto every frame in the video to localize body joints, which suffer\nhigh computational cost and hardly meet the low-latency requirement in\nrealistic applications. To address this issue, we propose a novel Dynamic\nKernel Distillation (DKD) model to facilitate small networks for estimating\nhuman poses in videos, thus significantly lifting the efficiency. In\nparticular, DKD introduces a light-weight distillator to online distill pose\nkernels via leveraging temporal cues from the previous frame in a one-shot\nfeed-forward manner. Then, DKD simplifies body joint localization into a\nmatching procedure between the pose kernels and the current frame, which can be\nefficiently computed via simple convolution. In this way, DKD fast transfers\npose knowledge from one frame to provide compact guidance for body joint\nlocalization in the following frame, which enables utilization of small\nnetworks in video-based pose estimation. To facilitate the training process,\nDKD exploits a temporally adversarial training strategy that introduces a\ntemporal discriminator to help generate temporally coherent pose kernels and\npose estimation results within a long range. Experiments on Penn Action and\nSub-JHMDB benchmarks demonstrate outperforming efficiency of DKD, specifically,\n10x flops reduction and 2x speedup over previous best model, and its\nstate-of-the-art accuracy.\n", "versions": [{"version": "v1", "created": "Sat, 24 Aug 2019 21:44:02 GMT"}], "update_date": "2019-08-27", "authors_parsed": [["Nie", "Xuecheng", ""], ["Li", "Yuncheng", ""], ["Luo", "Linjie", ""], ["Zhang", "Ning", ""], ["Feng", "Jiashi", ""]]}, {"id": "1908.09220", "submitter": "Xuecheng Nie", "authors": "Xuecheng Nie and Jianfeng Zhang and Shuicheng Yan and Jiashi Feng", "title": "Single-Stage Multi-Person Pose Machines", "comments": "To appear in ICCV 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-person pose estimation is a challenging problem. Existing methods are\nmostly two-stage based--one stage for proposal generation and the other for\nallocating poses to corresponding persons. However, such two-stage methods\ngenerally suffer low efficiency. In this work, we present the first\nsingle-stage model, Single-stage multi-person Pose Machine (SPM), to simplify\nthe pipeline and lift the efficiency for multi-person pose estimation. To\nachieve this, we propose a novel Structured Pose Representation (SPR) that\nunifies person instance and body joint position representations. Based on SPR,\nwe develop the SPM model that can directly predict structured poses for\nmultiple persons in a single stage, and thus offer a more compact pipeline and\nattractive efficiency advantage over two-stage methods. In particular, SPR\nintroduces the root joints to indicate different person instances and human\nbody joint positions are encoded into their displacements w.r.t. the roots. To\nbetter predict long-range displacements for some joints, SPR is further\nextended to hierarchical representations. Based on SPR, SPM can efficiently\nperform multi-person poses estimation by simultaneously predicting root joints\n(location of instances) and body joint displacements via CNNs. Moreover, to\ndemonstrate the generality of SPM, we also apply it to multi-person 3D pose\nestimation. Comprehensive experiments on benchmarks MPII, extended\nPASCAL-Person-Part, MSCOCO and CMU Panoptic clearly demonstrate the\nstate-of-the-art efficiency of SPM for multi-person 2D/3D pose estimation,\ntogether with outstanding accuracy.\n", "versions": [{"version": "v1", "created": "Sat, 24 Aug 2019 22:05:51 GMT"}], "update_date": "2019-08-27", "authors_parsed": [["Nie", "Xuecheng", ""], ["Zhang", "Jianfeng", ""], ["Yan", "Shuicheng", ""], ["Feng", "Jiashi", ""]]}, {"id": "1908.09231", "submitter": "Siyang Qin", "authors": "Siyang Qin, Alessandro Bissacco, Michalis Raptis, Yasuhisa Fujii, Ying\n  Xiao", "title": "Towards Unconstrained End-to-End Text Spotting", "comments": "Accepted to ICCV 2019 as oral presentation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We propose an end-to-end trainable network that can simultaneously detect and\nrecognize text of arbitrary shape, making substantial progress on the open\nproblem of reading scene text of irregular shape. We formulate arbitrary shape\ntext detection as an instance segmentation problem; an attention model is then\nused to decode the textual content of each irregularly shaped text region\nwithout rectification. To extract useful irregularly shaped text instance\nfeatures from image scale features, we propose a simple yet effective RoI\nmasking step. Additionally, we show that predictions from an existing\nmulti-step OCR engine can be leveraged as partially labeled training data,\nwhich leads to significant improvements in both the detection and recognition\naccuracy of our model. Our method surpasses the state-of-the-art for end-to-end\nrecognition tasks on the ICDAR15 (straight) benchmark by 4.6%, and on the\nTotal-Text (curved) benchmark by more than 16%.\n", "versions": [{"version": "v1", "created": "Sat, 24 Aug 2019 23:41:07 GMT"}], "update_date": "2019-08-27", "authors_parsed": [["Qin", "Siyang", ""], ["Bissacco", "Alessandro", ""], ["Raptis", "Michalis", ""], ["Fujii", "Yasuhisa", ""], ["Xiao", "Ying", ""]]}, {"id": "1908.09254", "submitter": "Md Sirajus Salekin", "authors": "Md Sirajus Salekin, Ghada Zamzmi, Dmitry Goldgof, Rangachar Kasturi,\n  Thao Ho, and Yu Sun", "title": "Multi-Channel Neural Network for Assessing Neonatal Pain from Videos", "comments": "Accepted to IEEE SMC 2019", "journal-ref": null, "doi": "10.1109/SMC.2019.8914537", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neonates do not have the ability to either articulate pain or communicate it\nnon-verbally by pointing. The current clinical standard for assessing neonatal\npain is intermittent and highly subjective. This discontinuity and subjectivity\ncan lead to inconsistent assessment, and therefore, inadequate treatment. In\nthis paper, we propose a multi-channel deep learning framework for assessing\nneonatal pain from videos. The proposed framework integrates information from\ntwo pain indicators or channels, namely facial expression and body movement,\nusing convolutional neural network (CNN). It also integrates temporal\ninformation using a recurrent neural network (LSTM). The experimental results\nprove the efficiency and superiority of the proposed temporal and multi-channel\nframework as compared to existing similar methods.\n", "versions": [{"version": "v1", "created": "Sun, 25 Aug 2019 05:51:54 GMT"}], "update_date": "2020-03-25", "authors_parsed": [["Salekin", "Md Sirajus", ""], ["Zamzmi", "Ghada", ""], ["Goldgof", "Dmitry", ""], ["Kasturi", "Rangachar", ""], ["Ho", "Thao", ""], ["Sun", "Yu", ""]]}, {"id": "1908.09262", "submitter": "Balamurali Murugesan", "authors": "Balamurali Murugesan, Vijaya Raghavan S, Kaushik Sarveswaran, Keerthi\n  Ram and Mohanasankar Sivaprakasam", "title": "Recon-GLGAN: A Global-Local context based Generative Adversarial Network\n  for MRI Reconstruction", "comments": "Accepted at MLMIR-MICCAIW 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Magnetic resonance imaging (MRI) is one of the best medical imaging\nmodalities as it offers excellent spatial resolution and soft-tissue contrast.\nBut, the usage of MRI is limited by its slow acquisition time, which makes it\nexpensive and causes patient discomfort. In order to accelerate the\nacquisition, multiple deep learning networks have been proposed. Recently,\nGenerative Adversarial Networks (GANs) have shown promising results in MRI\nreconstruction. The drawback with the proposed GAN based methods is it does not\nincorporate the prior information about the end goal which could help in better\nreconstruction. For instance, in the case of cardiac MRI, the physician would\nbe interested in the heart region which is of diagnostic relevance while\nexcluding the peripheral regions. In this work, we show that incorporating\nprior information about a region of interest in the model would offer better\nperformance. Thereby, we propose a novel GAN based architecture, Reconstruction\nGlobal-Local GAN (Recon-GLGAN) for MRI reconstruction. The proposed model\ncontains a generator and a context discriminator which incorporates global and\nlocal contextual information from images. Our model offers significant\nperformance improvement over the baseline models. Our experiments show that the\nconcept of a context discriminator can be extended to existing GAN based\nreconstruction models to offer better performance. We also demonstrate that the\nreconstructions from the proposed method give segmentation results similar to\nfully sampled images.\n", "versions": [{"version": "v1", "created": "Sun, 25 Aug 2019 07:07:35 GMT"}], "update_date": "2019-08-27", "authors_parsed": [["Murugesan", "Balamurali", ""], ["S", "Vijaya Raghavan", ""], ["Sarveswaran", "Kaushik", ""], ["Ram", "Keerthi", ""], ["Sivaprakasam", "Mohanasankar", ""]]}, {"id": "1908.09264", "submitter": "Samah Khawaled", "authors": "Samah Khawaled, Michael Zibulevsky and Yehoshua Y. Zeevi", "title": "Texture and Structure Two-view Classification of Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Textural and structural features can be regraded as \"two-view\" feature sets.\nInspired by the recent progress in multi-view learning, we propose a novel\ntwo-view classification method that models each feature set and optimizes the\nprocess of merging these views efficiently. Examples of implementation of this\napproach in classification of real-world data are presented, with special\nemphasis on medical images. We firstly decompose fully-textured images into two\nlayers of representation, corresponding to natural stochastic textures (NST)\nand structural layer, respectively. The structural, edge-and-curve-type,\ninformation is mostly represented by the local spatial phase, whereas, the pure\nNST has random phase and is characterized by Gaussianity and self-similarity.\nTherefore, the NST is modeled by the 2D self-similar process, fractional\nBrownian motion (fBm). The Hurst parameter, characteristic of fBm, specifies\nthe roughness or irregularity of the texture. This leads us to its estimation\nand implementation along other features extracted from the structure layer, to\nbuild the \"two-view\" features sets used in our classification scheme. A shallow\nneural net (NN) is exploited to execute the process of merging these feature\nsets, in a straightforward and efficient manner.\n", "versions": [{"version": "v1", "created": "Sun, 25 Aug 2019 07:13:25 GMT"}], "update_date": "2019-08-27", "authors_parsed": [["Khawaled", "Samah", ""], ["Zibulevsky", "Michael", ""], ["Zeevi", "Yehoshua Y.", ""]]}, {"id": "1908.09287", "submitter": "Benyamin Ghojogh", "authors": "Benyamin Ghojogh, Fakhri Karray, Mark Crowley", "title": "Principal Component Analysis Using Structural Similarity Index for\n  Images", "comments": "Paper for the methods named \"Image Structural Component Analysis\n  (ISCA)\" and \"Kernel Image Structural Component Analysis (Kernel ISCA)\"", "journal-ref": "International Conference on Image Analysis and Recognition,\n  Springer, pp. 77-88, 2019", "doi": "10.1007/978-3-030-27202-9_7", "report-no": null, "categories": "eess.IV cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the advances of deep learning in specific tasks using images, the\nprincipled assessment of image fidelity and similarity is still a critical\nability to develop. As it has been shown that Mean Squared Error (MSE) is\ninsufficient for this task, other measures have been developed with one of the\nmost effective being Structural Similarity Index (SSIM). Such measures can be\nused for subspace learning but existing methods in machine learning, such as\nPrincipal Component Analysis (PCA), are based on Euclidean distance or MSE and\nthus cannot properly capture the structural features of images. In this paper,\nwe define an image structure subspace which discriminates different types of\nimage distortions. We propose Image Structural Component Analysis (ISCA) and\nalso kernel ISCA by using SSIM, rather than Euclidean distance, in the\nformulation of PCA. This paper provides a bridge between image quality\nassessment and manifold learning opening a broad new area for future research.\n", "versions": [{"version": "v1", "created": "Sun, 25 Aug 2019 09:18:03 GMT"}], "update_date": "2019-08-27", "authors_parsed": [["Ghojogh", "Benyamin", ""], ["Karray", "Fakhri", ""], ["Crowley", "Mark", ""]]}, {"id": "1908.09288", "submitter": "Benyamin Ghojogh", "authors": "Benyamin Ghojogh, Fakhri Karray, Mark Crowley", "title": "Locally Linear Image Structural Embedding for Image Structure Manifold\n  Learning", "comments": "This is the paper for the methods named \"Locally Linear Image\n  Structural Embedding (LLISE)\" and \"Kernel Locally Linear Image Structural\n  Embedding (Kernel LLISE)\"", "journal-ref": "International Conference on Image Analysis and Recognition,\n  Springer, pp. 126-138, 2019", "doi": "10.1007/978-3-030-27202-9_11", "report-no": null, "categories": "stat.ML cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most of existing manifold learning methods rely on Mean Squared Error (MSE)\nor $\\ell_2$ norm. However, for the problem of image quality assessment, these\nare not promising measure. In this paper, we introduce the concept of an image\nstructure manifold which captures image structure features and discriminates\nimage distortions. We propose a new manifold learning method, Locally Linear\nImage Structural Embedding (LLISE), and kernel LLISE for learning this\nmanifold. The LLISE is inspired by Locally Linear Embedding (LLE) but uses SSIM\nrather than MSE. This paper builds a bridge between manifold learning and image\nfidelity assessment and it can open a new area for future investigations.\n", "versions": [{"version": "v1", "created": "Sun, 25 Aug 2019 09:32:45 GMT"}], "update_date": "2019-08-27", "authors_parsed": [["Ghojogh", "Benyamin", ""], ["Karray", "Fakhri", ""], ["Crowley", "Mark", ""]]}, {"id": "1908.09298", "submitter": "Jingkun Chen", "authors": "Jingkun Chen, Hongwei Li, Jianguo Zhang, Bjoern Menze", "title": "Adversarial Convolutional Networks with Weak Domain-Transfer for\n  Multi-Sequence Cardiac MR Images Segmentation", "comments": "9 pages, 4 figures, conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Analysis and modeling of the ventricles and myocardium are important in the\ndiagnostic and treatment of heart diseases. Manual delineation of those tissues\nin cardiac MR (CMR) scans is laborious and time-consuming. The ambiguity of the\nboundaries makes the segmentation task rather challenging. Furthermore, the\nannotations on some modalities such as Late Gadolinium Enhancement (LGE) MRI,\nare often not available. We propose an end-to-end segmentation framework based\non convolutional neural network (CNN) and adversarial learning. A dilated\nresidual U-shape network is used as a segmentor to generate the prediction\nmask; meanwhile, a CNN is utilized as a discriminator model to judge the\nsegmentation quality. To leverage the available annotations across modalities\nper patient, a new loss function named weak domain-transfer loss is introduced\nto the pipeline. The proposed model is evaluated on the public dataset released\nby the challenge organizer in MICCAI 2019, which consists of 45 sets of\nmulti-sequence CMR images. We demonstrate that the proposed adversarial\npipeline outperforms baseline deep-learning methods.\n", "versions": [{"version": "v1", "created": "Sun, 25 Aug 2019 10:43:55 GMT"}, {"version": "v2", "created": "Wed, 28 Aug 2019 14:37:23 GMT"}], "update_date": "2019-08-29", "authors_parsed": [["Chen", "Jingkun", ""], ["Li", "Hongwei", ""], ["Zhang", "Jianguo", ""], ["Menze", "Bjoern", ""]]}, {"id": "1908.09300", "submitter": "Theodoros Georgiou", "authors": "Umut \\\"Ozayd{\\i}n, Theodoros Georgiou, Michael Lew", "title": "A Comparison of CNN and Classic Features for Image Retrieval", "comments": "5 pages, 3 figures, 3 tables, CBMI 2019", "journal-ref": null, "doi": "10.1109/CBMI.2019.8877470", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Feature detectors and descriptors have been successfully used for various\ncomputer vision tasks, such as video object tracking and content-based image\nretrieval. Many methods use image gradients in different stages of the\ndetection-description pipeline to describe local image structures. Recently,\nsome, or all, of these stages have been replaced by convolutional neural\nnetworks (CNNs), in order to increase their performance. A detector is defined\nas a selection problem, which makes it more challenging to implement as a CNN.\nThey are therefore generally defined as regressors, converting input images to\nscore maps and keypoints can be selected with non-maximum suppression. This\npaper discusses and compares several recent methods that use CNNs for keypoint\ndetection. Experiments are performed both on the CNN based approaches, as well\nas a selection of conventional methods. In addition to qualitative measures\ndefined on keypoints and descriptors, the bag-of-words (BoW) model is used to\nimplement an image retrieval application, in order to determine how the methods\nperform in practice. The results show that each type of features are best in\ndifferent contexts.\n", "versions": [{"version": "v1", "created": "Sun, 25 Aug 2019 11:20:33 GMT"}], "update_date": "2021-03-18", "authors_parsed": [["\u00d6zayd\u0131n", "Umut", ""], ["Georgiou", "Theodoros", ""], ["Lew", "Michael", ""]]}, {"id": "1908.09317", "submitter": "Iro Laina", "authors": "Iro Laina, Christian Rupprecht, Nassir Navab", "title": "Towards Unsupervised Image Captioning with Shared Multimodal Embeddings", "comments": "ICCV 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding images without explicit supervision has become an important\nproblem in computer vision. In this paper, we address image captioning by\ngenerating language descriptions of scenes without learning from annotated\npairs of images and their captions. The core component of our approach is a\nshared latent space that is structured by visual concepts. In this space, the\ntwo modalities should be indistinguishable. A language model is first trained\nto encode sentences into semantically structured embeddings. Image features\nthat are translated into this embedding space can be decoded into descriptions\nthrough the same language model, similarly to sentence embeddings. This\ntranslation is learned from weakly paired images and text using a loss robust\nto noisy assignments and a conditional adversarial component. Our approach\nallows to exploit large text corpora outside the annotated distributions of\nimage/caption data. Our experiments show that the proposed domain alignment\nlearns a semantically meaningful representation which outperforms previous\nwork.\n", "versions": [{"version": "v1", "created": "Sun, 25 Aug 2019 12:56:41 GMT"}], "update_date": "2019-08-27", "authors_parsed": [["Laina", "Iro", ""], ["Rupprecht", "Christian", ""], ["Navab", "Nassir", ""]]}, {"id": "1908.09327", "submitter": "Siyan Zheng", "authors": "Zhibo Wang, Siyan Zheng, Mengkai Song, Qian Wang, Alireza Rahimpour,\n  Hairong Qi", "title": "advPattern: Physical-World Attacks on Deep Person Re-Identification via\n  Adversarially Transformable Patterns", "comments": "10 pages, 6 figures, Accepted by ICCV 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Person re-identification (re-ID) is the task of matching person images across\ncamera views, which plays an important role in surveillance and security\napplications. Inspired by great progress of deep learning, deep re-ID models\nbegan to be popular and gained state-of-the-art performance. However, recent\nworks found that deep neural networks (DNNs) are vulnerable to adversarial\nexamples, posing potential threats to DNNs based applications. This phenomenon\nthrows a serious question about whether deep re-ID based systems are vulnerable\nto adversarial attacks. In this paper, we take the first attempt to implement\nrobust physical-world attacks against deep re-ID. We propose a novel attack\nalgorithm, called advPattern, for generating adversarial patterns on clothes,\nwhich learns the variations of image pairs across cameras to pull closer the\nimage features from the same camera, while pushing features from different\ncameras farther. By wearing our crafted \"invisible cloak\", an adversary can\nevade person search, or impersonate a target person to fool deep re-ID models\nin physical world. We evaluate the effectiveness of our transformable patterns\non adversaries'clothes with Market1501 and our established PRCS dataset. The\nexperimental results show that the rank-1 accuracy of re-ID models for matching\nthe adversary decreases from 87.9% to 27.1% under Evading Attack. Furthermore,\nthe adversary can impersonate a target person with 47.1% rank-1 accuracy and\n67.9% mAP under Impersonation Attack. The results demonstrate that deep re-ID\nsystems are vulnerable to our physical attacks.\n", "versions": [{"version": "v1", "created": "Sun, 25 Aug 2019 13:33:35 GMT"}, {"version": "v2", "created": "Wed, 28 Aug 2019 02:58:45 GMT"}, {"version": "v3", "created": "Fri, 25 Oct 2019 13:52:10 GMT"}], "update_date": "2019-10-28", "authors_parsed": [["Wang", "Zhibo", ""], ["Zheng", "Siyan", ""], ["Song", "Mengkai", ""], ["Wang", "Qian", ""], ["Rahimpour", "Alireza", ""], ["Qi", "Hairong", ""]]}, {"id": "1908.09340", "submitter": "Jian Han", "authors": "Jian Han", "title": "Learning adaptively from the unknown for few-example video person re-ID", "comments": "This is a draft and there are many places to be revised", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper mainly studies one-example and few-example video person\nre-identification. A multi-branch network PAM that jointly learns local and\nglobal features is proposed. PAM has high accuracy, few parameters and\nconverges fast, which is suitable for few-example person re-identification. We\niteratively estimates labels for unlabeled samples, incorporates them into\ntraining sets, and trains a more robust network. We propose the static relative\ndistance sampling(SRD) strategy based on the relative distance between classes.\nFor the problem that SRD can not use all unlabeled samples, we propose adaptive\nrelative distance sampling (ARD) strategy. For one-example setting, We get\n89.78\\%, 56.13\\% rank-1 accuracy on PRID2011 and iLIDS-VID respectively, and\n85.16\\%, 45.36\\% mAP on DukeMTMC and MARS respectively, which exceeds the\nprevious methods by large margin.\n", "versions": [{"version": "v1", "created": "Sun, 25 Aug 2019 14:41:08 GMT"}], "update_date": "2019-08-27", "authors_parsed": [["Han", "Jian", ""]]}, {"id": "1908.09346", "submitter": "Weida Yang", "authors": "Weida Yang and Xindong Ai and Zuliu Yang and Yong Xu and Yong Zhao", "title": "Dedge-AGMNet:an effective stereo matching network optimized by depth\n  edge auxiliary task", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To improve the performance in ill-posed regions, this paper proposes an\natrous granular multi-scale network based on depth edge\nsubnetwork(Dedge-AGMNet). According to a general fact, the depth edge is the\nbinary semantic edge of instance-sensitive. This paper innovatively generates\nthe depth edge ground-truth by mining the semantic and instance dataset\nsimultaneously. To incorporate the depth edge cues efficiently, our network\nemploys the hard parameter sharing mechanism for the stereo matching branch and\ndepth edge branch. The network modifies SPP to Dedge-SPP, which fuses the depth\nedge features to the disparity estimation network. The granular convolution is\nextracted and extends to 3D architecture. Then we design the AGM module to\nbuild a more suitable structure. This module could capture the multi-scale\nreceptive field with fewer parameters. Integrating the ranks of different\nstereo datasets, our network outperforms other stereo matching networks and\nadvances state-of-the-art performances on the Sceneflow, KITTI 2012 and KITTI\n2015 benchmark datasets.\n", "versions": [{"version": "v1", "created": "Sun, 25 Aug 2019 15:39:33 GMT"}, {"version": "v2", "created": "Fri, 30 Aug 2019 01:32:40 GMT"}, {"version": "v3", "created": "Mon, 28 Oct 2019 08:43:47 GMT"}, {"version": "v4", "created": "Tue, 24 Mar 2020 04:38:07 GMT"}], "update_date": "2020-03-25", "authors_parsed": [["Yang", "Weida", ""], ["Ai", "Xindong", ""], ["Yang", "Zuliu", ""], ["Xu", "Yong", ""], ["Zhao", "Yong", ""]]}, {"id": "1908.09414", "submitter": "Jong Chul Ye", "authors": "Sungjun Lim, Hyoungjun Park, Sang-Eun Lee, Sunghoe Chang, and Jong\n  Chul Ye", "title": "CycleGAN with a Blur Kernel for Deconvolution Microscopy: Optimal\n  Transport Geometry", "comments": "This paper is accepted for IEEE Trans. Computational Imaging", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deconvolution microscopy has been extensively used to improve the resolution\nof the wide-field fluorescent microscopy, but the performance of classical\napproaches critically depends on the accuracy of a model and optimization\nalgorithms. Recently, the convolutional neural network (CNN) approaches have\nbeen studied as a fast and high performance alternative. Unfortunately, the CNN\napproaches usually require matched high resolution images for supervised\ntraining. In this paper, we present a novel unsupervised cycle-consistent\ngenerative adversarial network (cycleGAN) with a linear blur kernel, which can\nbe used for both blind- and non-blind image deconvolution. In contrast to the\nconventional cycleGAN approaches that require two deep generators, the proposed\ncycleGAN approach needs only a single deep generator and a linear blur kernel,\nwhich significantly improves the robustness and efficiency of network training.\nWe show that the proposed architecture is indeed a dual formulation of an\noptimal transport problem that uses a special form of the penalized least\nsquares cost as a transport cost. Experimental results using simulated and real\nexperimental data confirm the efficacy of the algorithm.\n", "versions": [{"version": "v1", "created": "Mon, 26 Aug 2019 00:34:16 GMT"}, {"version": "v2", "created": "Tue, 15 Oct 2019 07:25:41 GMT"}, {"version": "v3", "created": "Wed, 8 Jul 2020 05:05:59 GMT"}], "update_date": "2020-07-09", "authors_parsed": [["Lim", "Sungjun", ""], ["Park", "Hyoungjun", ""], ["Lee", "Sang-Eun", ""], ["Chang", "Sunghoe", ""], ["Ye", "Jong Chul", ""]]}, {"id": "1908.09419", "submitter": "Junghoon Seo", "authors": "Junghoon Seo, Jamyoung Koo, Taegyun Jeon", "title": "Deep Closed-Form Subspace Clustering", "comments": "Accepted at the 2019 ICCV Workshop on Robust Subspace Learning and\n  Applications in Computer Vision (RSL-CV 2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose Deep Closed-Form Subspace Clustering (DCFSC), a new embarrassingly\nsimple model for subspace clustering with learning non-linear mapping. Compared\nwith the previous deep subspace clustering (DSC) techniques, our DCFSC does not\nhave any parameters at all for the self-expressive layer. Instead, DCFSC\nutilizes the implicit data-driven self-expressive layer derived from\nclosed-form shallow auto-encoder. Moreover, DCFSC also has no complicated\noptimization scheme, unlike the other subspace clustering methods. With its\nextreme simplicity, DCFSC has significant memory-related benefits over the\nexisting DSC method, especially on the large dataset. Several experiments\nshowed that our DCFSC model had enough potential to be a new reference model\nfor subspace clustering on large-scale high-dimensional dataset.\n", "versions": [{"version": "v1", "created": "Mon, 26 Aug 2019 00:52:04 GMT"}], "update_date": "2019-08-27", "authors_parsed": [["Seo", "Junghoon", ""], ["Koo", "Jamyoung", ""], ["Jeon", "Taegyun", ""]]}, {"id": "1908.09428", "submitter": "Saeed Anwar", "authors": "Hafeez Anwar, Saeed Anwar, Sebastian Zambanini, Fatih Porikli", "title": "Deep Ancient Roman Republican Coin Classification via Feature Fusion and\n  Attention", "comments": "in Pattern Recognition", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We perform the classification of ancient Roman Republican coins via\nrecognizing their reverse motifs where various objects, faces, scenes, animals,\nand buildings are minted along with legends. Most of these coins are eroded due\nto their age and varying degrees of preservation, thereby affecting their\ninformative attributes for visual recognition. Changes in the positions of\nprincipal symbols on the reverse motifs also cause huge variations among the\ncoin types. Lastly, in-plane orientations, uneven illumination, and a moderate\nbackground clutter further make the classification task non-trivial and\nchallenging.\n  To this end, we present a novel network model, CoinNet, that employs compact\nbilinear pooling, residual groups, and feature attention layers. Furthermore,\nwe gathered the largest and most diverse image dataset of the Roman Republican\ncoins that contains more than 18,000 images belonging to 228 different reverse\nmotifs. On this dataset, our model achieves a classification accuracy of more\nthan \\textbf{98\\%} and outperforms the conventional bag-of-visual-words based\napproaches and more recent state-of-the-art deep learning methods. We also\nprovide a detailed ablation study of our network and its generalization\ncapability. Models and Datasets available at\nhttps://github.com/saeed-anwar/CoinNet\n", "versions": [{"version": "v1", "created": "Mon, 26 Aug 2019 01:28:57 GMT"}, {"version": "v2", "created": "Thu, 24 Dec 2020 06:18:16 GMT"}], "update_date": "2020-12-25", "authors_parsed": [["Anwar", "Hafeez", ""], ["Anwar", "Saeed", ""], ["Zambanini", "Sebastian", ""], ["Porikli", "Fatih", ""]]}, {"id": "1908.09442", "submitter": "Tianwei Lin", "authors": "Xin Li, Tianwei Lin, Xiao Liu, Chuang Gan, Wangmeng Zuo, Chao Li,\n  Xiang Long, Dongliang He, Fu Li, Shilei Wen", "title": "Deep Concept-wise Temporal Convolutional Networks for Action\n  Localization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing action localization approaches adopt shallow temporal convolutional\nnetworks (\\ie, TCN) on 1D feature map extracted from video frames. In this\npaper, we empirically find that stacking more conventional temporal convolution\nlayers actually deteriorates action classification performance, possibly\nascribing to that all channels of 1D feature map, which generally are highly\nabstract and can be regarded as latent concepts, are excessively recombined in\ntemporal convolution. To address this issue, we introduce a novel concept-wise\ntemporal convolution (CTC) layer as an alternative to conventional temporal\nconvolution layer for training deeper action localization networks. Instead of\nrecombining latent concepts, CTC layer deploys a number of temporal filters to\neach concept separately with shared filter parameters across concepts. Thus can\ncapture common temporal patterns of different concepts and significantly enrich\nrepresentation ability. Via stacking CTC layers, we proposed a deep\nconcept-wise temporal convolutional network (C-TCN), which boosts the\nstate-of-the-art action localization performance on THUMOS'14 from 42.8 to 52.1\nin terms of mAP(\\%), achieving a relative improvement of 21.7\\%. Favorable\nresult is also obtained on ActivityNet.\n", "versions": [{"version": "v1", "created": "Mon, 26 Aug 2019 02:56:07 GMT"}], "update_date": "2019-08-27", "authors_parsed": [["Li", "Xin", ""], ["Lin", "Tianwei", ""], ["Liu", "Xiao", ""], ["Gan", "Chuang", ""], ["Zuo", "Wangmeng", ""], ["Li", "Chao", ""], ["Long", "Xiang", ""], ["He", "Dongliang", ""], ["Li", "Fu", ""], ["Wen", "Shilei", ""]]}, {"id": "1908.09443", "submitter": "Ye Huang", "authors": "Ye Huang, Qingqing Wang, Wenjing Jia, Xiangjian He", "title": "See More Than Once -- Kernel-Sharing Atrous Convolution for Semantic\n  Segmentation", "comments": "Results updated. We have received a very large number of emails ask\n  for the code. Thanks for your interest. We will release the code once the\n  paper is offically published as we have promised", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The state-of-the-art semantic segmentation solutions usually leverage\ndifferent receptive fields via multiple parallel branches to handle objects\nwith different sizes. However, employing separate kernels for individual\nbranches degrades the generalization and representation abilities of the\nnetwork, and the number of parameters increases linearly in the number of\nbranches. To tackle this problem, we propose a novel network structure namely\nKernel-Sharing Atrous Convolution (KSAC), where branches of different receptive\nfields share the same kernel, i.e., let a single kernel see the input feature\nmaps more than once with different receptive fields, to facilitate\ncommunication among branches and perform feature augmentation inside the\nnetwork. Experiments conducted on the benchmark PASCAL VOC 2012 dataset show\nthat the proposed sharing strategy can not only boost a network s\ngeneralization and representation abilities but also reduce the model\ncomplexity significantly. Specifically, on the validation set, whe compared\nwith DeepLabV3+ equipped with MobileNetv2 backbone, 33% of parameters are\nreduced together with an mIOU improvement of 0.6%. When Xception is used as the\nbackbone, the mIOU is elevated from 83.34% to 85.96% with about 10M parameters\nsaved. In addition, different from the widely used ASPP structure, our proposed\nKSAC is able to further improve the mIOU by taking benefit of wider context\nwith larger atrous rates. Finally, our KSAC achieves mIOUs of 88.1% and 45.47%\non the PASCAL VOC 2012 test set and ADE20K dataset, respectively. Our full code\nwill be released on the Github.\n", "versions": [{"version": "v1", "created": "Mon, 26 Aug 2019 03:01:38 GMT"}, {"version": "v2", "created": "Tue, 27 Aug 2019 16:56:14 GMT"}, {"version": "v3", "created": "Mon, 9 Sep 2019 18:51:29 GMT"}, {"version": "v4", "created": "Sat, 16 Nov 2019 06:43:56 GMT"}], "update_date": "2019-11-19", "authors_parsed": [["Huang", "Ye", ""], ["Wang", "Qingqing", ""], ["Jia", "Wenjing", ""], ["He", "Xiangjian", ""]]}, {"id": "1908.09445", "submitter": "Zheng Zhu", "authors": "Zheng Zhu, Wei Zou, Guan Huang, Dalong Du, Chang Huang", "title": "High Performance Visual Object Tracking with Unified Convolutional\n  Networks", "comments": "Extended version of [arXiv:1711.04661] our UCT tracker in ICCV\n  VOT2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional neural networks (CNN) based tracking approaches have shown\nfavorable performance in recent benchmarks. Nonetheless, the chosen CNN\nfeatures are always pre-trained in different tasks and individual components in\ntracking systems are learned separately, thus the achieved tracking performance\nmay be suboptimal. Besides, most of these trackers are not designed towards\nreal-time applications because of their time-consuming feature extraction and\ncomplex optimization details. In this paper, we propose an end-to-end framework\nto learn the convolutional features and perform the tracking process\nsimultaneously, namely, a unified convolutional tracker (UCT). Specifically,\nthe UCT treats feature extractor and tracking process both as convolution\noperation and trains them jointly, which enables learned CNN features are\ntightly coupled with tracking process. During online tracking, an efficient\nmodel updating method is proposed by introducing peak-versus-noise ratio (PNR)\ncriterion, and scale changes are handled efficiently by incorporating a scale\nbranch into network. Experiments are performed on four challenging tracking\ndatasets: OTB2013, OTB2015, VOT2015 and VOT2016. Our method achieves leading\nperformance on these benchmarks while maintaining beyond real-time speed.\n", "versions": [{"version": "v1", "created": "Mon, 26 Aug 2019 03:09:03 GMT"}], "update_date": "2019-08-27", "authors_parsed": [["Zhu", "Zheng", ""], ["Zou", "Wei", ""], ["Huang", "Guan", ""], ["Du", "Dalong", ""], ["Huang", "Chang", ""]]}, {"id": "1908.09464", "submitter": "Junbang Liang", "authors": "Junbang Liang, Ming C. Lin", "title": "Shape-Aware Human Pose and Shape Reconstruction Using Multi-View Images", "comments": "To be published to ICCV 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a scalable neural network framework to reconstruct the 3D mesh of\na human body from multi-view images, in the subspace of the SMPL model. Use of\nmulti-view images can significantly reduce the projection ambiguity of the\nproblem, increasing the reconstruction accuracy of the 3D human body under\nclothing. Our experiments show that this method benefits from the synthetic\ndataset generated from our pipeline since it has good flexibility of variable\ncontrol and can provide ground-truth for validation. Our method outperforms\nexisting methods on real-world images, especially on shape estimations.\n", "versions": [{"version": "v1", "created": "Mon, 26 Aug 2019 04:35:15 GMT"}], "update_date": "2019-08-27", "authors_parsed": [["Liang", "Junbang", ""], ["Lin", "Ming C.", ""]]}, {"id": "1908.09474", "submitter": "Jiawang Bian", "authors": "Jia-Wang Bian, Yu-Huan Wu, Ji Zhao, Yun Liu, Le Zhang, Ming-Ming\n  Cheng, Ian Reid", "title": "An Evaluation of Feature Matchers for Fundamental Matrix Estimation", "comments": "Accepted to British Machine Vision Conference (BMVC) 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Matching two images while estimating their relative geometry is a key step in\nmany computer vision applications. For decades, a well-established pipeline,\nconsisting of SIFT, RANSAC, and 8-point algorithm, has been used for this task.\nRecently, many new approaches were proposed and shown to outperform previous\nalternatives on standard benchmarks, including the learned features,\ncorrespondence pruning algorithms, and robust estimators. However, whether it\nis beneficial to incorporate them into the classic pipeline is\nless-investigated. To this end, we are interested in i) evaluating the\nperformance of these recent algorithms in the context of image matching and\nepipolar geometry estimation, and ii) leveraging them to design more practical\nregistration systems. The experiments are conducted in four large-scale\ndatasets using strictly defined evaluation metrics, and the promising results\nprovide insight into which algorithms suit which scenarios. According to this,\nwe propose three high-quality matching systems and a Coarse-to-Fine RANSAC\nestimator. They show remarkable performances and have potentials to a large\npart of computer vision tasks. To facilitate future research, the full\nevaluation pipeline and the proposed methods are made publicly available.\n", "versions": [{"version": "v1", "created": "Mon, 26 Aug 2019 05:21:39 GMT"}, {"version": "v2", "created": "Wed, 11 Sep 2019 21:20:13 GMT"}], "update_date": "2019-09-13", "authors_parsed": [["Bian", "Jia-Wang", ""], ["Wu", "Yu-Huan", ""], ["Zhao", "Ji", ""], ["Liu", "Yun", ""], ["Zhang", "Le", ""], ["Cheng", "Ming-Ming", ""], ["Reid", "Ian", ""]]}, {"id": "1908.09475", "submitter": "Xiaoxue Chen", "authors": "Xiaoxue Chen, Tianwei Wang, Yuanzhi Zhu, Lianwen Jin, Canjie Luo", "title": "Adaptive Embedding Gate for Attention-Based Scene Text Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scene text recognition has attracted particular research interest because it\nis a very challenging problem and has various applications. The most\ncutting-edge methods are attentional encoder-decoder frameworks that learn the\nalignment between the input image and output sequences. In particular, the\ndecoder recurrently outputs predictions, using the prediction of the previous\nstep as a guidance for every time step. In this study, we point out that the\ninappropriate use of previous predictions in existing attention mechanisms\nrestricts the recognition performance and brings instability. To handle this\nproblem, we propose a novel module, namely adaptive embedding gate(AEG). The\nproposed AEG focuses on introducing high-order character language models to\nattention mechanism by controlling the information transmission between\nadjacent characters. AEG is a flexible module and can be easily integrated into\nthe state-of-the-art attentional methods. We evaluate its effectiveness as well\nas robustness on a number of standard benchmarks, including the IIIT$5$K, SVT,\nSVT-P, CUTE$80$, and ICDAR datasets. Experimental results demonstrate that AEG\ncan significantly boost recognition performance and bring better robustness.\n", "versions": [{"version": "v1", "created": "Mon, 26 Aug 2019 05:30:53 GMT"}], "update_date": "2019-08-27", "authors_parsed": [["Chen", "Xiaoxue", ""], ["Wang", "Tianwei", ""], ["Zhu", "Yuanzhi", ""], ["Jin", "Lianwen", ""], ["Luo", "Canjie", ""]]}, {"id": "1908.09492", "submitter": "Benjin Zhu", "authors": "Benjin Zhu, Zhengkai Jiang, Xiangxin Zhou, Zeming Li, and Gang Yu", "title": "Class-balanced Grouping and Sampling for Point Cloud 3D Object Detection", "comments": "technical report", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This report presents our method which wins the nuScenes3D Detection Challenge\n[17] held in Workshop on Autonomous Driving(WAD, CVPR 2019). Generally, we\nutilize sparse 3D convolution to extract rich semantic features, which are then\nfed into a class-balanced multi-head network to perform 3D object detection. To\nhandle the severe class imbalance problem inherent in the autonomous driving\nscenarios, we design a class-balanced sampling and augmentation strategy to\ngenerate a more balanced data distribution. Furthermore, we propose a balanced\ngroup-ing head to boost the performance for the categories withsimilar shapes.\nBased on the Challenge results, our methodoutperforms the PointPillars [14]\nbaseline by a large mar-gin across all metrics, achieving state-of-the-art\ndetection performance on the nuScenes dataset. Code will be released at CBGS.\n", "versions": [{"version": "v1", "created": "Mon, 26 Aug 2019 06:27:01 GMT"}], "update_date": "2019-08-27", "authors_parsed": [["Zhu", "Benjin", ""], ["Jiang", "Zhengkai", ""], ["Zhou", "Xiangxin", ""], ["Li", "Zeming", ""], ["Yu", "Gang", ""]]}, {"id": "1908.09493", "submitter": "Tobias Kuhn", "authors": "Tobias Kuhn, Steven Bourke, Levin Brinkmann, Tobias Buchwald, Conor\n  Digan, Hendrik Hache, Sebastian Jaeger, Patrick Lehmann, Oskar Maier, Stefan\n  Matting, Yura Okulovsky", "title": "Supporting stylists by recommending fashion style", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Outfittery is an online personalized styling service targeted at men. We have\nhundreds of stylists who create thousands of bespoke outfits for our customers\nevery day. A critical challenge faced by our stylists when creating these\noutfits is selecting an appropriate item of clothing that makes sense in the\ncontext of the outfit being created, otherwise known as style fit. Another\nsignificant challenge is knowing if the item is relevant to the customer based\non their tastes, physical attributes and price sensitivity. At Outfittery we\nleverage machine learning extensively and combine it with human domain\nexpertise to tackle these challenges. We do this by surfacing relevant items of\nclothing during the outfit building process based on what our stylist is doing\nand what the preferences of our customer are. In this paper we describe one way\nin which we help our stylists to tackle style fit for a particular item of\nclothing and its relevance to an outfit. A thorough qualitative and\nquantitative evaluation highlights the method's ability to recommend fashion\nitems by style fit.\n", "versions": [{"version": "v1", "created": "Mon, 26 Aug 2019 06:34:05 GMT"}], "update_date": "2019-08-27", "authors_parsed": [["Kuhn", "Tobias", ""], ["Bourke", "Steven", ""], ["Brinkmann", "Levin", ""], ["Buchwald", "Tobias", ""], ["Digan", "Conor", ""], ["Hache", "Hendrik", ""], ["Jaeger", "Sebastian", ""], ["Lehmann", "Patrick", ""], ["Maier", "Oskar", ""], ["Matting", "Stefan", ""], ["Okulovsky", "Yura", ""]]}, {"id": "1908.09511", "submitter": "Ting Yao", "authors": "Jiajun Deng and Yingwei Pan and Ting Yao and Wengang Zhou and Houqiang\n  Li and Tao Mei", "title": "Relation Distillation Networks for Video Object Detection", "comments": "ICCV 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It has been well recognized that modeling object-to-object relations would be\nhelpful for object detection. Nevertheless, the problem is not trivial\nespecially when exploring the interactions between objects to boost video\nobject detectors. The difficulty originates from the aspect that reliable\nobject relations in a video should depend on not only the objects in the\npresent frame but also all the supportive objects extracted over a long range\nspan of the video. In this paper, we introduce a new design to capture the\ninteractions across the objects in spatio-temporal context. Specifically, we\npresent Relation Distillation Networks (RDN) --- a new architecture that\nnovelly aggregates and propagates object relation to augment object features\nfor detection. Technically, object proposals are first generated via Region\nProposal Networks (RPN). RDN then, on one hand, models object relation via\nmulti-stage reasoning, and on the other, progressively distills relation\nthrough refining supportive object proposals with high objectness scores in a\ncascaded manner. The learnt relation verifies the efficacy on both improving\nobject detection in each frame and box linking across frames. Extensive\nexperiments are conducted on ImageNet VID dataset, and superior results are\nreported when comparing to state-of-the-art methods. More remarkably, our RDN\nachieves 81.8% and 83.2% mAP with ResNet-101 and ResNeXt-101, respectively.\nWhen further equipped with linking and rescoring, we obtain to-date the best\nreported mAP of 83.8% and 84.7%.\n", "versions": [{"version": "v1", "created": "Mon, 26 Aug 2019 07:45:43 GMT"}], "update_date": "2019-08-27", "authors_parsed": [["Deng", "Jiajun", ""], ["Pan", "Yingwei", ""], ["Yao", "Ting", ""], ["Zhou", "Wengang", ""], ["Li", "Houqiang", ""], ["Mei", "Tao", ""]]}, {"id": "1908.09514", "submitter": "Ting Yao", "authors": "Yang Chen and Yingwei Pan and Ting Yao and Xinmei Tian and Tao Mei", "title": "Mocycle-GAN: Unpaired Video-to-Video Translation", "comments": "Accepted as a full paper for ACMMM 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unsupervised image-to-image translation is the task of translating an image\nfrom one domain to another in the absence of any paired training examples and\ntends to be more applicable to practical applications. Nevertheless, the\nextension of such synthesis from image-to-image to video-to-video is not\ntrivial especially when capturing spatio-temporal structures in videos. The\ndifficulty originates from the aspect that not only the visual appearance in\neach frame but also motion between consecutive frames should be realistic and\nconsistent across transformation. This motivates us to explore both appearance\nstructure and temporal continuity in video synthesis. In this paper, we present\na new Motion-guided Cycle GAN, dubbed as Mocycle-GAN, that novelly integrates\nmotion estimation into unpaired video translator. Technically, Mocycle-GAN\ncapitalizes on three types of constrains: adversarial constraint discriminating\nbetween synthetic and real frame, cycle consistency encouraging an inverse\ntranslation on both frame and motion, and motion translation validating the\ntransfer of motion between consecutive frames. Extensive experiments are\nconducted on video-to-labels and labels-to-video translation, and superior\nresults are reported when comparing to state-of-the-art methods. More\nremarkably, we qualitatively demonstrate our Mocycle-GAN for both\nflower-to-flower and ambient condition transfer.\n", "versions": [{"version": "v1", "created": "Mon, 26 Aug 2019 07:51:17 GMT"}], "update_date": "2019-08-27", "authors_parsed": [["Chen", "Yang", ""], ["Pan", "Yingwei", ""], ["Yao", "Ting", ""], ["Tian", "Xinmei", ""], ["Mei", "Tao", ""]]}, {"id": "1908.09515", "submitter": "Olivier Verdier", "authors": "Ozan \\\"Oktem, Camille Pouchol, Olivier Verdier", "title": "Spatiotemporal PET reconstruction using ML-EM with learned diffeomorphic\n  deformation", "comments": null, "journal-ref": null, "doi": "10.1007/978-3-030-33843-5_14", "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Patient movement in emission tomography deteriorates reconstruction quality\nbecause of motion blur. Gating the data improves the situation somewhat: each\ngate contains a movement phase which is approximately stationary. A standard\nmethod is to use only the data from a few gates, with little movement between\nthem. However, the corresponding loss of data entails an increase of noise.\nMotion correction algorithms have been implemented to take into account all the\ngated data, but they do not scale well, especially not in 3D. We propose a\nnovel motion correction algorithm which addresses the scalability issue. Our\napproach is to combine an enhanced ML-EM algorithm with deep learning based\nmovement registration. The training is unsupervised, and with artificial data.\nWe expect this approach to scale very well to higher resolutions and to 3D, as\nthe overall cost of our algorithm is only marginally greater than that of a\nstandard ML-EM algorithm. We show that we can significantly decrease the noise\ncorresponding to a limited number of gates.\n", "versions": [{"version": "v1", "created": "Mon, 26 Aug 2019 08:04:49 GMT"}], "update_date": "2020-02-24", "authors_parsed": [["\u00d6ktem", "Ozan", ""], ["Pouchol", "Camille", ""], ["Verdier", "Olivier", ""]]}, {"id": "1908.09521", "submitter": "Helisa Dhamo", "authors": "Helisa Dhamo, Nassir Navab, Federico Tombari", "title": "Object-Driven Multi-Layer Scene Decomposition From a Single Image", "comments": "ICCV 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a method that tackles the challenge of predicting color and depth\nbehind the visible content of an image. Our approach aims at building up a\nLayered Depth Image (LDI) from a single RGB input, which is an efficient\nrepresentation that arranges the scene in layers, including originally occluded\nregions. Unlike previous work, we enable an adaptive scheme for the number of\nlayers and incorporate semantic encoding for better hallucination of partly\noccluded objects. Additionally, our approach is object-driven, which especially\nboosts the accuracy for the occluded intermediate objects. The framework\nconsists of two steps. First, we individually complete each object in terms of\ncolor and depth, while estimating the scene layout. Second, we rebuild the\nscene based on the regressed layers and enforce the recomposed image to\nresemble the structure of the original input. The learned representation\nenables various applications, such as 3D photography and diminished reality,\nall from a single RGB image.\n", "versions": [{"version": "v1", "created": "Mon, 26 Aug 2019 08:33:58 GMT"}], "update_date": "2019-08-27", "authors_parsed": [["Dhamo", "Helisa", ""], ["Navab", "Nassir", ""], ["Tombari", "Federico", ""]]}, {"id": "1908.09526", "submitter": "Rui Li", "authors": "Rui Li and Zhibin Pan and Yang Wang and Ping Wang", "title": "A Convolutional Neural Network with Mapping Layers for Hyperspectral\n  Image Classification", "comments": "UNDER REVIEW ON IEEE TRANS. GEOSCI. REMOTE SEN", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a convolutional neural network with mapping layers\n(MCNN) for hyperspectral image (HSI) classification. The proposed mapping\nlayers map the input patch into a low dimensional subspace by multilinear\nalgebra. We use our mapping layers to reduce the spectral and spatial\nredundancy and maintain most energy of the input. The feature extracted by our\nmapping layers can also reduce the number of following convolutional layers for\nfeature extraction. Our MCNN architecture avoids the declining accuracy with\nincreasing layers phenomenon of deep learning models for HSI classification and\nalso saves the training time for its effective mapping layers. Furthermore, we\nimpose the 3-D convolutional kernel on convolutional layer to extract the\nspectral-spatial features for HSI. We tested our MCNN on three datasets of\nIndian Pines, University of Pavia and Salinas, and we achieved the\nclassification accuracy of 98.3%, 99.5% and 99.3%, respectively. Experimental\nresults demonstrate that the proposed MCNN can significantly improve the\nclassification accuracy and save much time consumption.\n", "versions": [{"version": "v1", "created": "Mon, 26 Aug 2019 08:42:26 GMT"}], "update_date": "2019-08-27", "authors_parsed": [["Li", "Rui", ""], ["Pan", "Zhibin", ""], ["Wang", "Yang", ""], ["Wang", "Ping", ""]]}, {"id": "1908.09535", "submitter": "Wenjie Pei", "authors": "Canmiao Fu and Wenjie Pei and Qiong Cao and Chaopeng Zhang and Yong\n  Zhao and Xiaoyong Shen and Yu-Wing Tai", "title": "Non-local Recurrent Neural Memory for Supervised Sequence Modeling", "comments": "Accepted by ICCV 2019, Oral", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Typical methods for supervised sequence modeling are built upon the recurrent\nneural networks to capture temporal dependencies. One potential limitation of\nthese methods is that they only model explicitly information interactions\nbetween adjacent time steps in a sequence, hence the high-order interactions\nbetween nonadjacent time steps are not fully exploited. It greatly limits the\ncapability of modeling the long-range temporal dependencies since one-order\ninteractions cannot be maintained for a long term due to information dilution\nand gradient vanishing. To tackle this limitation, we propose the Non-local\nRecurrent Neural Memory (NRNM) for supervised sequence modeling, which performs\nnon-local operations to learn full-order interactions within a sliding temporal\nblock and models global interactions between blocks in a gated recurrent\nmanner. Consequently, our model is able to capture the long-range dependencies.\nBesides, the latent high-level features contained in high-order interactions\ncan be distilled by our model. We demonstrate the merits of our NRNM on two\ndifferent tasks: action recognition and sentiment analysis.\n", "versions": [{"version": "v1", "created": "Mon, 26 Aug 2019 09:01:57 GMT"}], "update_date": "2019-08-27", "authors_parsed": [["Fu", "Canmiao", ""], ["Pei", "Wenjie", ""], ["Cao", "Qiong", ""], ["Zhang", "Chaopeng", ""], ["Zhao", "Yong", ""], ["Shen", "Xiaoyong", ""], ["Tai", "Yu-Wing", ""]]}, {"id": "1908.09539", "submitter": "Junpeng Zhang", "authors": "Junpeng Zhang, Xiuping Jia and Jiankun Hu", "title": "Error Bounded Foreground and Background Modeling for Moving Object\n  Detection in Satellite Videos", "comments": null, "journal-ref": null, "doi": "10.1109/TGRS.2019.2953181", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Detecting moving objects from ground-based videos is commonly achieved by\nusing background subtraction techniques. Low-rank matrix decomposition inspires\na set of state-of-the-art approaches for this task. It is integrated with\nstructured sparsity regularization to achieve background subtraction in the\ndeveloped method of Low-rank and Structured Sparse Decomposition (LSD).\nHowever, when this method is applied to satellite videos where spatial\nresolution is poor and targets' contrast to the background is low, its\nperformance is limited as the data no longer fits adequately either the\nforeground structure or the background model. In this paper, we handle these\nunexplained data explicitly and address the moving target detection from space\nas one of the pioneer studies. We propose a technique by extending the\ndecomposition formulation with bounded errors, named Extended Low-rank and\nStructured Sparse Decomposition (E-LSD). This formulation integrates low-rank\nbackground, structured sparse foreground and their residuals in a matrix\ndecomposition problem. We provide an effective solution by introducing an\nalternative treatment and adopting the direct extension of Alternating\nDirection Method of Multipliers (ADMM). The proposed E-LSD was validated on two\nsatellite videos, and experimental results demonstrate the improvement in\nbackground modeling with boosted moving object detection precision over\nstate-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Mon, 26 Aug 2019 09:06:07 GMT"}], "update_date": "2020-04-22", "authors_parsed": [["Zhang", "Junpeng", ""], ["Jia", "Xiuping", ""], ["Hu", "Jiankun", ""]]}, {"id": "1908.09540", "submitter": "Yazan Abu Farha", "authors": "Yazan Abu Farha, Juergen Gall", "title": "Uncertainty-Aware Anticipation of Activities", "comments": "International Workshop on Human Behaviour Understanding, in\n  conjunction with ICCV 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Anticipating future activities in video is a task with many practical\napplications. While earlier approaches are limited to just a few seconds in the\nfuture, the prediction time horizon has just recently been extended to several\nminutes in the future. However, as increasing the predicted time horizon, the\nfuture becomes more uncertain and models that generate a single prediction fail\nat capturing the different possible future activities. In this paper, we\naddress the uncertainty modelling for predicting long-term future activities.\nBoth an action model and a length model are trained to model the probability\ndistribution of the future activities. At test time, we sample from the\npredicted distributions multiple samples that correspond to the different\npossible sequences of future activities. Our model is evaluated on two\nchallenging datasets and shows a good performance in capturing the multi-modal\nfuture activities without compromising the accuracy when predicting a single\nsequence of future activities.\n", "versions": [{"version": "v1", "created": "Mon, 26 Aug 2019 09:08:31 GMT"}, {"version": "v2", "created": "Thu, 29 Aug 2019 12:51:01 GMT"}], "update_date": "2019-08-30", "authors_parsed": [["Farha", "Yazan Abu", ""], ["Gall", "Juergen", ""]]}, {"id": "1908.09547", "submitter": "Qing Lian", "authors": "Qing Lian, Fengmao Lv, Lixin Duan, Boqing Gong", "title": "Constructing Self-motivated Pyramid Curriculums for Cross-Domain\n  Semantic Segmentation: A Non-Adversarial Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new approach, called self-motivated pyramid curriculum domain\nadaptation (PyCDA), to facilitate the adaptation of semantic segmentation\nneural networks from synthetic source domains to real target domains. Our\napproach draws on an insight connecting two existing works: curriculum domain\nadaptation and self-training. Inspired by the former, PyCDA constructs a\npyramid curriculum which contains various properties about the target domain.\nThose properties are mainly about the desired label distributions over the\ntarget domain images, image regions, and pixels. By enforcing the segmentation\nneural network to observe those properties, we can improve the network's\ngeneralization capability to the target domain. Motivated by the self-training,\nwe infer this pyramid of properties by resorting to the semantic segmentation\nnetwork itself. Unlike prior work, we do not need to maintain any additional\nmodels (e.g., logistic regression or discriminator networks) or to solve minmax\nproblems which are often difficult to optimize. We report state-of-the-art\nresults for the adaptation from both GTAV and SYNTHIA to Cityscapes, two\npopular settings in unsupervised domain adaptation for semantic segmentation.\n", "versions": [{"version": "v1", "created": "Mon, 26 Aug 2019 09:16:17 GMT"}], "update_date": "2019-08-27", "authors_parsed": [["Lian", "Qing", ""], ["Lv", "Fengmao", ""], ["Duan", "Lixin", ""], ["Gong", "Boqing", ""]]}, {"id": "1908.09550", "submitter": "Ting Yao", "authors": "Yiheng Zhang and Zhaofan Qiu and Jingen Liu and Ting Yao and Dong Liu\n  and Tao Mei", "title": "Customizable Architecture Search for Semantic Segmentation", "comments": "CVPR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a Customizable Architecture Search (CAS) approach\nto automatically generate a network architecture for semantic image\nsegmentation. The generated network consists of a sequence of stacked\ncomputation cells. A computation cell is represented as a directed acyclic\ngraph, in which each node is a hidden representation (i.e., feature map) and\neach edge is associated with an operation (e.g., convolution and pooling),\nwhich transforms data to a new layer. During the training, the CAS algorithm\nexplores the search space for an optimized computation cell to build a network.\nThe cells of the same type share one architecture but with different weights.\nIn real applications, however, an optimization may need to be conducted under\nsome constraints such as GPU time and model size. To this end, a cost\ncorresponding to the constraint will be assigned to each operation. When an\noperation is selected during the search, its associated cost will be added to\nthe objective. As a result, our CAS is able to search an optimized architecture\nwith customized constraints. The approach has been thoroughly evaluated on\nCityscapes and CamVid datasets, and demonstrates superior performance over\nseveral state-of-the-art techniques. More remarkably, our CAS achieves 72.3%\nmIoU on the Cityscapes dataset with speed of 108 FPS on an Nvidia TitanXp GPU.\n", "versions": [{"version": "v1", "created": "Mon, 26 Aug 2019 09:22:15 GMT"}], "update_date": "2019-08-27", "authors_parsed": [["Zhang", "Yiheng", ""], ["Qiu", "Zhaofan", ""], ["Liu", "Jingen", ""], ["Yao", "Ting", ""], ["Liu", "Dong", ""], ["Mei", "Tao", ""]]}, {"id": "1908.09560", "submitter": "Christoph Jud", "authors": "Christoph Jud, Damien Nguyen, Alina Giger, Robin Sandk\\\"uhler, Miriam\n  Krieger, Tony Lomax, Rares Salomir, Oliver Bieri, Philippe C. Cattin", "title": "Accelerated Motion-Aware MR Imaging via Motion Prediction from K-Space\n  Center", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motion has been a challenge for magnetic resonance (MR) imaging ever since\nthe MR has been invented. Especially in volumetric imaging of thoracic and\nabdominal organs, motion-awareness is essential for reducing motion artifacts\nin the final image. A recently proposed MR imaging approach copes with motion\nby observing the motion patterns during the acquisition. Repetitive scanning of\nthe k-space center region enables the extraction of the patient motion while\nacquiring the remaining part of the k-space. Due to highly redundant\nmeasurements of the center, the required scanning time of over 11 min and the\nreconstruction time of 2 h exceed clinical applicability though. We propose an\naccelerated motion-aware MR imaging method where the motion is inferred from\nsmall-sized k-space center patches and an initial training phase during which\nthe characteristic movements are modeled. Thereby, acquisition times are\nreduced by a factor of almost 2 and reconstruction times by two orders of\nmagnitude. Moreover, we improve the existing motion-aware approach with a\nsystematic temporal shift correction to achieve a sharper image reconstruction.\nWe tested our method on 12 volunteers and scanned their lungs and abdomen under\nfree breathing. We achieved equivalent to higher reconstruction quality using\nthe motion-prediction compared to the slower existing approach.\n", "versions": [{"version": "v1", "created": "Mon, 26 Aug 2019 09:35:34 GMT"}], "update_date": "2019-08-27", "authors_parsed": [["Jud", "Christoph", ""], ["Nguyen", "Damien", ""], ["Giger", "Alina", ""], ["Sandk\u00fchler", "Robin", ""], ["Krieger", "Miriam", ""], ["Lomax", "Tony", ""], ["Salomir", "Rares", ""], ["Bieri", "Oliver", ""], ["Cattin", "Philippe C.", ""]]}, {"id": "1908.09573", "submitter": "Yuming Shen", "authors": "Yuming Shen, Jie Qin, Jiaxin Chen, Li Liu and Fan Zhu", "title": "Embarrassingly Simple Binary Representation Learning", "comments": "ICCV 2019 CEFRL4 Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent binary representation learning models usually require sophisticated\nbinary optimization, similarity measure or even generative models as\nauxiliaries. However, one may wonder whether these non-trivial components are\nneeded to formulate practical and effective hashing models. In this paper, we\nanswer the above question by proposing an embarrassingly simple approach to\nbinary representation learning. With a simple classification objective, our\nmodel only incorporates two additional fully-connected layers onto the top of\nan arbitrary backbone network, whilst complying with the binary constraints\nduring training. The proposed model lower-bounds the Information Bottleneck\n(IB) between data samples and their semantics, and can be related to many\nrecent `learning to hash' paradigms. We show that, when properly designed, even\nsuch a simple network can generate effective binary codes, by fully exploring\ndata semantics without any held-out alternating updating steps or auxiliary\nmodels. Experiments are conducted on conventional large-scale benchmarks, i.e.,\nCIFAR-10, NUS-WIDE, and ImageNet, where the proposed simple model outperforms\nthe state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Mon, 26 Aug 2019 09:59:01 GMT"}], "update_date": "2019-08-27", "authors_parsed": [["Shen", "Yuming", ""], ["Qin", "Jie", ""], ["Chen", "Jiaxin", ""], ["Liu", "Li", ""], ["Zhu", "Fan", ""]]}, {"id": "1908.09584", "submitter": "Gundram Leifert", "authors": "Gundram Leifert and Roger Labahn and Tobias Gr\\\"uning and Svenja\n  Leifert", "title": "End-To-End Measure for Text Recognition", "comments": "to appear in proceeding at ICDAR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Measuring the performance of text recognition and text line detection engines\nis an important step to objectively compare systems and their configuration.\nThere exist well-established measures for both tasks separately. However, there\nis no sophisticated evaluation scheme to measure the quality of a combined text\nline detection and text recognition system. The F-measure on word level is a\nwell-known methodology, which is sometimes used in this context. Nevertheless,\nit does not take into account the alignment of hypothesis and ground truth text\nand can lead to deceptive results. Since users of automatic information\nretrieval pipelines in the context of text recognition are mainly interested in\nthe end-to-end performance of a given system, there is a strong need for such a\nmeasure. Hence, we present a measure to evaluate the quality of an end-to-end\ntext recognition system. The basis for this measure is the well established and\nwidely used character error rate, which is limited -- in its original form --\nto aligned hypothesis and ground truth texts. The proposed measure is flexible\nin a way that it can be configured to penalize different reading orders between\nthe hypothesis and ground truth and can take into account the geometric\nposition of the text lines. Additionally, it can ignore over- and under-\nsegmentation of text lines. With these parameters it is possible to get a\nmeasure fitting best to its own needs.\n", "versions": [{"version": "v1", "created": "Mon, 26 Aug 2019 10:35:31 GMT"}], "update_date": "2019-08-27", "authors_parsed": [["Leifert", "Gundram", ""], ["Labahn", "Roger", ""], ["Gr\u00fcning", "Tobias", ""], ["Leifert", "Svenja", ""]]}, {"id": "1908.09597", "submitter": "Felix Bragman", "authors": "Felix J.S. Bragman, Ryutaro Tanno, Sebastien Ourselin, Daniel C.\n  Alexander, M. Jorge Cardoso", "title": "Stochastic Filter Groups for Multi-Task CNNs: Learning Specialist and\n  Generalist Convolution Kernels", "comments": "Accepted for oral presentation at ICCV 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The performance of multi-task learning in Convolutional Neural Networks\n(CNNs) hinges on the design of feature sharing between tasks within the\narchitecture. The number of possible sharing patterns are combinatorial in the\ndepth of the network and the number of tasks, and thus hand-crafting an\narchitecture, purely based on the human intuitions of task relationships can be\ntime-consuming and suboptimal. In this paper, we present a probabilistic\napproach to learning task-specific and shared representations in CNNs for\nmulti-task learning. Specifically, we propose \"stochastic filter groups''\n(SFG), a mechanism to assign convolution kernels in each layer to \"specialist''\nor \"generalist'' groups, which are specific to or shared across different\ntasks, respectively. The SFG modules determine the connectivity between layers\nand the structures of task-specific and shared representations in the network.\nWe employ variational inference to learn the posterior distribution over the\npossible grouping of kernels and network parameters. Experiments demonstrate\nthat the proposed method generalises across multiple tasks and shows improved\nperformance over baseline methods.\n", "versions": [{"version": "v1", "created": "Mon, 26 Aug 2019 11:09:44 GMT"}], "update_date": "2019-08-27", "authors_parsed": [["Bragman", "Felix J. S.", ""], ["Tanno", "Ryutaro", ""], ["Ourselin", "Sebastien", ""], ["Alexander", "Daniel C.", ""], ["Cardoso", "M. Jorge", ""]]}, {"id": "1908.09625", "submitter": "Martin Mundt", "authors": "Martin Mundt, Iuliia Pliushch, Sagnik Majumder, Visvanathan Ramesh", "title": "Open Set Recognition Through Deep Neural Network Uncertainty: Does\n  Out-of-Distribution Detection Require Generative Classifiers?", "comments": "Accepted at the first workshop on Statistical Deep Learning for\n  Computer Vision (SDL-CV) at ICCV 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an analysis of predictive uncertainty based out-of-distribution\ndetection for different approaches to estimate various models' epistemic\nuncertainty and contrast it with extreme value theory based open set\nrecognition. While the former alone does not seem to be enough to overcome this\nchallenge, we demonstrate that uncertainty goes hand in hand with the latter\nmethod. This seems to be particularly reflected in a generative model approach,\nwhere we show that posterior based open set recognition outperforms\ndiscriminative models and predictive uncertainty based outlier rejection,\nraising the question of whether classifiers need to be generative in order to\nknow what they have not seen.\n", "versions": [{"version": "v1", "created": "Mon, 26 Aug 2019 12:19:53 GMT"}], "update_date": "2019-08-27", "authors_parsed": [["Mundt", "Martin", ""], ["Pliushch", "Iuliia", ""], ["Majumder", "Sagnik", ""], ["Ramesh", "Visvanathan", ""]]}, {"id": "1908.09630", "submitter": "Fariborz Taherkhani", "authors": "Fariborz Taherkhani, Jeremy Dawson, and Nasser M. Nasrabadi", "title": "Deep Sparse Band Selection for Hyperspectral Face Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Hyperspectral imaging systems collect and process information from specific\nwavelengths across the electromagnetic spectrum. The fusion of multi-spectral\nbands in the visible spectrum has been exploited to improve face recognition\nperformance over all the conventional broad band face images. In this book\nchapter, we propose a new Convolutional Neural Network (CNN) framework which\nadopts a structural sparsity learning technique to select the optimal spectral\nbands to obtain the best face recognition performance over all of the spectral\nbands. Specifically, in this method, images from all bands are fed to a CNN,\nand the convolutional filters in the first layer of the CNN are then\nregularized by employing a group Lasso algorithm to zero out the redundant\nbands during the training of the network. Contrary to other methods which\nusually select the useful bands manually or in a greedy fashion, our method\nselects the optimal spectral bands automatically to achieve the best face\nrecognition performance over all spectral bands. Moreover, experimental results\ndemonstrate that our method outperforms state of the art band selection methods\nfor face recognition on several publicly-available hyperspectral face image\ndatasets.\n", "versions": [{"version": "v1", "created": "Thu, 15 Aug 2019 23:51:33 GMT"}], "update_date": "2019-08-27", "authors_parsed": [["Taherkhani", "Fariborz", ""], ["Dawson", "Jeremy", ""], ["Nasrabadi", "Nasser M.", ""]]}, {"id": "1908.09638", "submitter": "Evangelos Ververas", "authors": "Evangelos Ververas, Stefanos Zafeiriou", "title": "SliderGAN: Synthesizing Expressive Face Images by Sliding 3D Blendshape\n  Parameters", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image-to-image (i2i) translation is the dense regression problem of learning\nhow to transform an input image into an output using aligned image pairs.\nRemarkable progress has been made in i2i translation with the advent of Deep\nConvolutional Neural Networks (DCNNs) and particular using the learning\nparadigm of Generative Adversarial Networks (GANs). In the absence of paired\nimages, i2i translation is tackled with one or multiple domain transformations\n(i.e., CycleGAN, StarGAN etc.). In this paper, we study a new problem, that of\nimage-to-image translation, under a set of continuous parameters that\ncorrespond to a model describing a physical process. In particular, we propose\nthe SliderGAN which transforms an input face image into a new one according to\nthe continuous values of a statistical blendshape model of facial motion. We\nshow that it is possible to edit a facial image according to expression and\nspeech blendshapes, using sliders that control the continuous values of the\nblendshape model. This provides much more flexibility in various tasks,\nincluding but not limited to face editing, expression transfer and face\nneutralisation, comparing to models based on discrete expressions or action\nunits.\n", "versions": [{"version": "v1", "created": "Mon, 26 Aug 2019 12:34:37 GMT"}], "update_date": "2019-08-27", "authors_parsed": [["Ververas", "Evangelos", ""], ["Zafeiriou", "Stefanos", ""]]}, {"id": "1908.09699", "submitter": "Chuanguang Yang", "authors": "Chuanguang Yang, Zhulin An, Hui Zhu, Xiaolong Hu, Kun Zhang, Kaiqiang\n  Xu, Chao Li, Yongjun Xu", "title": "Gated Convolutional Networks with Hybrid Connectivity for Image\n  Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": "Accepted by AAAI 2020", "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a simple yet effective method to reduce the redundancy of DenseNet\nby substantially decreasing the number of stacked modules by replacing the\noriginal bottleneck by our SMG module, which is augmented by local residual.\nFurthermore, SMG module is equipped with an efficient two-stage pipeline, which\naims to DenseNet-like architectures that need to integrate all previous\noutputs, i.e., squeezing the incoming informative but redundant features\ngradually by hierarchical convolutions as a hourglass shape and then exciting\nit by multi-kernel depthwise convolutions, the output of which would be compact\nand hold more informative multi-scale features. We further develop a forget and\nan update gate by introducing the popular attention modules to implement the\neffective fusion instead of a simple addition between reused and new features.\nDue to the Hybrid Connectivity (nested combination of global dense and local\nresidual) and Gated mechanisms, we called our network as the HCGNet.\nExperimental results on CIFAR and ImageNet datasets show that HCGNet is more\nprominently efficient than DenseNet, and can also significantly outperform\nstate-of-the-art networks with less complexity. Moreover, HCGNet also shows the\nremarkable interpretability and robustness by network dissection and\nadversarial defense, respectively. On MS-COCO, HCGNet can consistently learn\nbetter features than popular backbones.\n", "versions": [{"version": "v1", "created": "Mon, 26 Aug 2019 14:13:21 GMT"}, {"version": "v2", "created": "Sun, 8 Sep 2019 07:29:04 GMT"}, {"version": "v3", "created": "Thu, 28 Nov 2019 08:25:39 GMT"}], "update_date": "2019-12-02", "authors_parsed": [["Yang", "Chuanguang", ""], ["An", "Zhulin", ""], ["Zhu", "Hui", ""], ["Hu", "Xiaolong", ""], ["Zhang", "Kun", ""], ["Xu", "Kaiqiang", ""], ["Li", "Chao", ""], ["Xu", "Yongjun", ""]]}, {"id": "1908.09705", "submitter": "Ido Freeman", "authors": "Alessandro Cennamo, Ido Freeman, Anton Kummert", "title": "A Statistical Defense Approach for Detecting Adversarial Examples", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adversarial examples are maliciously modified inputs created to fool deep\nneural networks (DNN). The discovery of such inputs presents a major issue to\nthe expansion of DNN-based solutions. Many researchers have already contributed\nto the topic, providing both cutting edge-attack techniques and various\ndefensive strategies. In this work, we focus on the development of a system\ncapable of detecting adversarial samples by exploiting statistical information\nfrom the training-set. Our detector computes several distorted replicas of the\ntest input, then collects the classifier's prediction vectors to build a\nmeaningful signature for the detection task. Then, the signature is projected\nonto the class-specific statistic vector to infer the input's nature. The\nclassification output of the original input is used to select the\nclass-statistic vector. We show that our method reliably detects malicious\ninputs, outperforming state-of-the-art approaches in various settings, while\nbeing complementary to other defensive solutions.\n", "versions": [{"version": "v1", "created": "Mon, 26 Aug 2019 14:26:07 GMT"}], "update_date": "2019-08-27", "authors_parsed": [["Cennamo", "Alessandro", ""], ["Freeman", "Ido", ""], ["Kummert", "Anton", ""]]}, {"id": "1908.09715", "submitter": "Adam Van Etten", "authors": "Adam Van Etten", "title": "City-Scale Road Extraction from Satellite Imagery v2: Road Speeds and\n  Travel Times", "comments": "In Proceedings WACV 2020. 8 pages, 11 figures, 5 appendices. arXiv\n  admin note: text overlap with arXiv:1904.09901", "journal-ref": null, "doi": "10.1109/WACV45572.2020.9093593", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automated road network extraction from remote sensing imagery remains a\nsignificant challenge despite its importance in a broad array of applications.\nTo this end, we explore road network extraction at scale with inference of\nsemantic features of the graph, identifying speed limits and route travel times\nfor each roadway. We call this approach City-Scale Road Extraction from\nSatellite Imagery v2 (CRESIv2), Including estimates for travel time permits\ntrue optimal routing (rather than just the shortest geographic distance), which\nis not possible with existing remote sensing imagery based methods. We evaluate\nour method using two sources of labels (OpenStreetMap, and those from the\nSpaceNet dataset), and find that models both trained and tested on SpaceNet\nlabels outperform OpenStreetMap labels by greater than 60%. We quantify the\nperformance of our algorithm with the Average Path Length Similarity (APLS) and\nmap topology (TOPO) graph-theoretic metrics over a diverse test area covering\nfour cities in the SpaceNet dataset. For a traditional edge weight of geometric\ndistance, we find an aggregate of 5% improvement over existing methods for\nSpaceNet data. We also test our algorithm on Google satellite imagery with\nOpenStreetMap labels, and find a 23% improvement over previous work. Metric\nscores decrease by only 4% on large graphs when using travel time rather than\ngeometric distance for edge weights, indicating that optimizing routing for\ntravel time is feasible with this approach.\n", "versions": [{"version": "v1", "created": "Tue, 6 Aug 2019 12:59:28 GMT"}, {"version": "v2", "created": "Wed, 15 Jan 2020 05:28:25 GMT"}, {"version": "v3", "created": "Fri, 29 May 2020 20:32:08 GMT"}], "update_date": "2021-02-25", "authors_parsed": [["Van Etten", "Adam", ""]]}, {"id": "1908.09745", "submitter": "Xuejie Yu", "authors": "Zhong Ji, Xuejie Yu, Yunlong Yu, Yanwei Pang, and Zhongfei Zhang", "title": "A Semantics-Guided Class Imbalance Learning Model for Zero-Shot\n  Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Zero-Shot Classification (ZSC) equips the learned model with the ability to\nrecognize the visual instances from the novel classes via constructing the\ninteractions between the visual and the semantic modalities. In contrast to the\ntraditional image classification, ZSC is easily suffered from the\nclass-imbalance issue since it is more concerned with the class-level knowledge\ntransfer capability. In the real world, the class samples follow a long-tailed\ndistribution, and the discriminative information in the sample-scarce seen\nclasses is hard to be transferred to the related unseen classes in the\ntraditional batch-based training manner, which degrades the overall\ngeneralization ability a lot. Towards alleviating the class imbalance issue in\nZSC, we propose a sample-balanced training process to encourage all training\nclasses to contribute equally to the learned model. Specifically, we randomly\nselect the same number of images from each class across all training classes to\nform a training batch to ensure that the sample-scarce classes contribute\nequally as those classes with sufficient samples during each iteration.\nConsidering that the instances from the same class differ in class\nrepresentativeness, we further develop an efficient semantics-guided feature\nfusion model to obtain discriminative class visual prototype for the following\nvisual-semantic interaction process via distributing different weights to the\nselected samples based on their class representativeness. Extensive experiments\non three imbalanced ZSC benchmark datasets for both the Traditional ZSC (TZSC)\nand the Generalized ZSC (GZSC) tasks demonstrate our approach achieves\npromising results especially for the unseen categories those are closely\nrelated to the sample-scarce seen categories.\n", "versions": [{"version": "v1", "created": "Mon, 26 Aug 2019 15:38:33 GMT"}], "update_date": "2019-08-27", "authors_parsed": [["Ji", "Zhong", ""], ["Yu", "Xuejie", ""], ["Yu", "Yunlong", ""], ["Pang", "Yanwei", ""], ["Zhang", "Zhongfei", ""]]}, {"id": "1908.09747", "submitter": "Shiv Ram Dubey", "authors": "Yash Srivastava and Vaishnav Murali and Shiv Ram Dubey", "title": "Hard-Mining Loss based Convolutional Neural Network for Face Recognition", "comments": "Accepted in Fifth IAPR International Conference on Computer Vision\n  and Image Processing (CVIP), 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Face Recognition is one of the prominent problems in the computer vision\ndomain. Witnessing advances in deep learning, significant work has been\nobserved in face recognition, which touched upon various parts of the\nrecognition framework like Convolutional Neural Network (CNN), Layers, Loss\nfunctions, etc. Various loss functions such as Cross-Entropy, Angular-Softmax\nand ArcFace have been introduced to learn the weights of network for face\nrecognition. However, these loss functions do not give high priority to the\nhard samples as compared to the easy samples. Moreover, their learning process\nis biased due to a number of easy examples compared to hard examples. In this\npaper, we address this issue by considering hard examples with more priority.\nIn order to do so, We propose a Hard-Mining loss by increasing the loss for\nharder examples and decreasing the loss for easy examples. The proposed concept\nis generic and can be used with any existing loss function. We test the\nHard-Mining loss with different losses such as Cross-Entropy, Angular-Softmax\nand ArcFace. The proposed Hard-Mining loss is tested over widely used Labeled\nFaces in the Wild (LFW) and YouTube Faces (YTF) datasets. The training is\nperformed over CASIA-WebFace and MS-Celeb-1M datasets. We use the residual\nnetwork (i.e., ResNet18) for the experimental analysis. The experimental\nresults suggest that the performance of existing loss functions is boosted when\nused in the framework of the proposed Hard-Mining loss.\n", "versions": [{"version": "v1", "created": "Fri, 9 Aug 2019 06:55:45 GMT"}, {"version": "v2", "created": "Wed, 23 Dec 2020 00:49:00 GMT"}], "update_date": "2020-12-24", "authors_parsed": [["Srivastava", "Yash", ""], ["Murali", "Vaishnav", ""], ["Dubey", "Shiv Ram", ""]]}, {"id": "1908.09775", "submitter": "Dedimuni De Silva", "authors": "D.D.N. De Silva, H.W.M.K. Vithanage, K.S.D. Fernando, I.T.S.\n  Piyatilake", "title": "Multi-Path Learnable Wavelet Neural Network for Image Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the remarkable success of deep learning in pattern recognition, deep\nnetwork models face the problem of training a large number of parameters. In\nthis paper, we propose and evaluate a novel multi-path wavelet neural network\narchitecture for image classification with far less number of trainable\nparameters. The model architecture consists of a multi-path layout with several\nlevels of wavelet decompositions performed in parallel followed by fully\nconnected layers. These decomposition operations comprise wavelet neurons with\nlearnable parameters, which are updated during the training phase using the\nback-propagation algorithm. We evaluate the performance of the introduced\nnetwork using common image datasets without data augmentation except for SVHN\nand compare the results with influential deep learning models. Our findings\nsupport the possibility of reducing the number of parameters significantly in\ndeep neural networks without compromising its accuracy.\n", "versions": [{"version": "v1", "created": "Mon, 26 Aug 2019 16:21:56 GMT"}], "update_date": "2019-08-27", "authors_parsed": [["De Silva", "D. D. N.", ""], ["Vithanage", "H. W. M. K.", ""], ["Fernando", "K. S. D.", ""], ["Piyatilake", "I. T. S.", ""]]}, {"id": "1908.09791", "submitter": "Han Cai", "authors": "Han Cai, Chuang Gan, Tianzhe Wang, Zhekai Zhang, Song Han", "title": "Once-for-All: Train One Network and Specialize it for Efficient\n  Deployment", "comments": "ICLR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We address the challenging problem of efficient inference across many devices\nand resource constraints, especially on edge devices. Conventional approaches\neither manually design or use neural architecture search (NAS) to find a\nspecialized neural network and train it from scratch for each case, which is\ncomputationally prohibitive (causing $CO_2$ emission as much as 5 cars'\nlifetime) thus unscalable. In this work, we propose to train a once-for-all\n(OFA) network that supports diverse architectural settings by decoupling\ntraining and search, to reduce the cost. We can quickly get a specialized\nsub-network by selecting from the OFA network without additional training. To\nefficiently train OFA networks, we also propose a novel progressive shrinking\nalgorithm, a generalized pruning method that reduces the model size across many\nmore dimensions than pruning (depth, width, kernel size, and resolution). It\ncan obtain a surprisingly large number of sub-networks ($> 10^{19}$) that can\nfit different hardware platforms and latency constraints while maintaining the\nsame level of accuracy as training independently. On diverse edge devices, OFA\nconsistently outperforms state-of-the-art (SOTA) NAS methods (up to 4.0%\nImageNet top1 accuracy improvement over MobileNetV3, or same accuracy but 1.5x\nfaster than MobileNetV3, 2.6x faster than EfficientNet w.r.t measured latency)\nwhile reducing many orders of magnitude GPU hours and $CO_2$ emission. In\nparticular, OFA achieves a new SOTA 80.0% ImageNet top-1 accuracy under the\nmobile setting ($<$600M MACs). OFA is the winning solution for the 3rd Low\nPower Computer Vision Challenge (LPCVC), DSP classification track and the 4th\nLPCVC, both classification track and detection track. Code and 50 pre-trained\nmodels (for many devices & many latency constraints) are released at\nhttps://github.com/mit-han-lab/once-for-all.\n", "versions": [{"version": "v1", "created": "Mon, 26 Aug 2019 16:46:23 GMT"}, {"version": "v2", "created": "Sun, 5 Jan 2020 20:26:58 GMT"}, {"version": "v3", "created": "Sun, 8 Mar 2020 18:18:22 GMT"}, {"version": "v4", "created": "Sun, 26 Apr 2020 23:02:50 GMT"}, {"version": "v5", "created": "Wed, 29 Apr 2020 20:49:05 GMT"}], "update_date": "2020-05-01", "authors_parsed": [["Cai", "Han", ""], ["Gan", "Chuang", ""], ["Wang", "Tianzhe", ""], ["Zhang", "Zhekai", ""], ["Han", "Song", ""]]}, {"id": "1908.09798", "submitter": "Bowen Cheng", "authors": "Bowen Cheng and Liang-Chieh Chen and Yunchao Wei and Yukun Zhu and\n  Zilong Huang and Jinjun Xiong and Thomas Huang and Wen-Mei Hwu and Honghui\n  Shi", "title": "SPGNet: Semantic Prediction Guidance for Scene Parsing", "comments": "ICCV 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-scale context module and single-stage encoder-decoder structure are\ncommonly employed for semantic segmentation. The multi-scale context module\nrefers to the operations to aggregate feature responses from a large spatial\nextent, while the single-stage encoder-decoder structure encodes the high-level\nsemantic information in the encoder path and recovers the boundary information\nin the decoder path. In contrast, multi-stage encoder-decoder networks have\nbeen widely used in human pose estimation and show superior performance than\ntheir single-stage counterpart. However, few efforts have been attempted to\nbring this effective design to semantic segmentation. In this work, we propose\na Semantic Prediction Guidance (SPG) module which learns to re-weight the local\nfeatures through the guidance from pixel-wise semantic prediction. We find that\nby carefully re-weighting features across stages, a two-stage encoder-decoder\nnetwork coupled with our proposed SPG module can significantly outperform its\none-stage counterpart with similar parameters and computations. Finally, we\nreport experimental results on the semantic segmentation benchmark Cityscapes,\nin which our SPGNet attains 81.1% on the test set using only 'fine'\nannotations.\n", "versions": [{"version": "v1", "created": "Mon, 26 Aug 2019 16:58:12 GMT"}], "update_date": "2019-08-27", "authors_parsed": [["Cheng", "Bowen", ""], ["Chen", "Liang-Chieh", ""], ["Wei", "Yunchao", ""], ["Zhu", "Yukun", ""], ["Huang", "Zilong", ""], ["Xiong", "Jinjun", ""], ["Huang", "Thomas", ""], ["Hwu", "Wen-Mei", ""], ["Shi", "Honghui", ""]]}, {"id": "1908.09822", "submitter": "Zhiding Yu", "authors": "Yang Zou, Zhiding Yu, Xiaofeng Liu, B. V. K. Vijaya Kumar, Jinsong\n  Wang", "title": "Confidence Regularized Self-Training", "comments": "Accepted to ICCV 2019 (Oral)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.MM cs.RO", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Recent advances in domain adaptation show that deep self-training presents a\npowerful means for unsupervised domain adaptation. These methods often involve\nan iterative process of predicting on target domain and then taking the\nconfident predictions as pseudo-labels for retraining. However, since\npseudo-labels can be noisy, self-training can put overconfident label belief on\nwrong classes, leading to deviated solutions with propagated errors. To address\nthe problem, we propose a confidence regularized self-training (CRST)\nframework, formulated as regularized self-training. Our method treats\npseudo-labels as continuous latent variables jointly optimized via alternating\noptimization. We propose two types of confidence regularization: label\nregularization (LR) and model regularization (MR). CRST-LR generates soft\npseudo-labels while CRST-MR encourages the smoothness on network output.\nExtensive experiments on image classification and semantic segmentation show\nthat CRSTs outperform their non-regularized counterpart with state-of-the-art\nperformance. The code and models of this work are available at\nhttps://github.com/yzou2/CRST.\n", "versions": [{"version": "v1", "created": "Mon, 26 Aug 2019 17:56:13 GMT"}, {"version": "v2", "created": "Tue, 27 Aug 2019 05:26:12 GMT"}, {"version": "v3", "created": "Wed, 15 Jul 2020 10:57:38 GMT"}], "update_date": "2020-07-16", "authors_parsed": [["Zou", "Yang", ""], ["Yu", "Zhiding", ""], ["Liu", "Xiaofeng", ""], ["Kumar", "B. V. K. Vijaya", ""], ["Wang", "Jinsong", ""]]}, {"id": "1908.09825", "submitter": "Erlei Zhang", "authors": "Erlei Zhang, Zi Yang, Stephen Seiler, Mingli Chen, Weiguo Lu, Xuejun\n  Gu", "title": "Breast Ultrasound Computer-Aided Diagnosis Using Structure-Aware Triplet\n  Path Networks", "comments": "arXiv admin note: substantial text overlap with arXiv:1904.01076", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Breast ultrasound (US) is an effective imaging modality for breast cancer\ndetec-tion and diagnosis. The structural characteristics of breast lesion play\nan im-portant role in Computer-Aided Diagnosis (CAD). In this paper, a novel\nstruc-ture-aware triplet path networks (SATPN) was designed to integrate\nclassifica-tion and two image reconstruction tasks to achieve accurate\ndiagnosis on US im-ages with small training dataset. Specifically, we enhance\nclinically-approved breast lesion structure characteristics though converting\noriginal breast US imag-es to BIRADS-oriented feature maps (BFMs) with a\ndistance-transformation coupled Gaussian filter. Then, the converted BFMs were\nused as the inputs of SATPN, which performed lesion classification task and two\nunsupervised stacked convolutional Auto-Encoder (SCAE) networks for benign and\nmalignant image reconstruction tasks, independently. We trained the SATPN with\nan alter-native learning strategy by balancing image reconstruction error and\nclassification label prediction error. At the test stage, the lesion label was\ndetermined by the weighted voting with reconstruction error and label\nprediction error. We com-pared the performance of the SATPN with TPN using\noriginal image as input and our previous developed semi-supervised deep\nlearning methods using BFMs as inputs. Experimental results on two breast US\ndatasets showed that SATPN ranked the best among the three networks, with\nclassification accuracy around 93.5%. These findings indicated that SATPN is\npromising for effective breast US lesion CAD using small datasets.\n", "versions": [{"version": "v1", "created": "Fri, 9 Aug 2019 20:36:40 GMT"}, {"version": "v2", "created": "Sun, 14 Jun 2020 23:32:10 GMT"}], "update_date": "2020-06-16", "authors_parsed": [["Zhang", "Erlei", ""], ["Yang", "Zi", ""], ["Seiler", "Stephen", ""], ["Chen", "Mingli", ""], ["Lu", "Weiguo", ""], ["Gu", "Xuejun", ""]]}, {"id": "1908.09873", "submitter": "Marc G\\'orriz Blanch", "authors": "Marc G\\'orriz, Marta Mrak, Alan F. Smeaton, Noel E. O'Connor", "title": "End-to-End Conditional GAN-based Architectures for Image Colourisation", "comments": "IEEE 21st International Workshop on Multimedia Signal Processing,\n  27-29 Sept 2019, Kuala Lumpur, Malaysia", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In this work recent advances in conditional adversarial networks are\ninvestigated to develop an end-to-end architecture based on Convolutional\nNeural Networks (CNNs) to directly map realistic colours to an input greyscale\nimage. Observing that existing colourisation methods sometimes exhibit a lack\nof colourfulness, this paper proposes a method to improve colourisation\nresults. In particular, the method uses Generative Adversarial Neural Networks\n(GANs) and focuses on improvement of training stability to enable better\ngeneralisation in large multi-class image datasets. Additionally, the\nintegration of instance and batch normalisation layers in both generator and\ndiscriminator is introduced to the popular U-Net architecture, boosting the\nnetwork capabilities to generalise the style changes of the content. The method\nhas been tested using the ILSVRC 2012 dataset, achieving improved automatic\ncolourisation results compared to other methods based on GANs.\n", "versions": [{"version": "v1", "created": "Mon, 26 Aug 2019 18:29:22 GMT"}, {"version": "v2", "created": "Thu, 5 Sep 2019 12:05:53 GMT"}], "update_date": "2019-09-06", "authors_parsed": [["G\u00f3rriz", "Marc", ""], ["Mrak", "Marta", ""], ["Smeaton", "Alan F.", ""], ["O'Connor", "Noel E.", ""]]}, {"id": "1908.09884", "submitter": "Kai Han", "authors": "Kai Han and Andrea Vedaldi and Andrew Zisserman", "title": "Learning to Discover Novel Visual Categories via Deep Transfer\n  Clustering", "comments": "ICCV 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of discovering novel object categories in an image\ncollection. While these images are unlabelled, we also assume prior knowledge\nof related but different image classes. We use such prior knowledge to reduce\nthe ambiguity of clustering, and improve the quality of the newly discovered\nclasses. Our contributions are twofold. The first contribution is to extend\nDeep Embedded Clustering to a transfer learning setting; we also improve the\nalgorithm by introducing a representation bottleneck, temporal ensembling, and\nconsistency. The second contribution is a method to estimate the number of\nclasses in the unlabelled data. This also transfers knowledge from the known\nclasses, using them as probes to diagnose different choices for the number of\nclasses in the unlabelled subset. We thoroughly evaluate our method,\nsubstantially outperforming state-of-the-art techniques in a large number of\nbenchmarks, including ImageNet, OmniGlot, CIFAR-100, CIFAR-10, and SVHN.\n", "versions": [{"version": "v1", "created": "Mon, 26 Aug 2019 19:24:05 GMT"}], "update_date": "2019-08-28", "authors_parsed": [["Han", "Kai", ""], ["Vedaldi", "Andrea", ""], ["Zisserman", "Andrew", ""]]}, {"id": "1908.09891", "submitter": "Alexandre Cunha", "authors": "Fidel A. Guerrero-Pe\\~na and Pedro D. Marrero Fernandez and Tsang Ing\n  Ren and Alexandre Cunha", "title": "A Weakly Supervised Method for Instance Segmentation of Biological Cells", "comments": "Accepted at MICCAI Worshop 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We present a weakly supervised deep learning method to perform instance\nsegmentation of cells present in microscopy images. Annotation of biomedical\nimages in the lab can be scarce, incomplete, and inaccurate. This is of concern\nwhen supervised learning is used for image analysis as the discriminative power\nof a learning model might be compromised in these situations. To overcome the\ncurse of poor labeling, our method focuses on three aspects to improve\nlearning: i) we propose a loss function operating in three classes to\nfacilitate separating adjacent cells and to drive the optimizer to properly\nclassify underrepresented regions; ii) a contour-aware weight map model is\nintroduced to strengthen contour detection while improving the network\ngeneralization capacity; and iii) we augment data by carefully modulating local\nintensities on edges shared by adjoining regions and to account for possibly\nweak signals on these edges. Generated probability maps are segmented using\ndifferent methods, with the watershed based one generally offering the best\nsolutions, specially in those regions where the prevalence of a single class is\nnot clear. The combination of these contributions allows segmenting individual\ncells on challenging images. We demonstrate our methods in sparse and crowded\ncell images, showing improvements in the learning process for a fixed network\narchitecture.\n", "versions": [{"version": "v1", "created": "Mon, 26 Aug 2019 19:42:14 GMT"}], "update_date": "2019-08-28", "authors_parsed": [["Guerrero-Pe\u00f1a", "Fidel A.", ""], ["Fernandez", "Pedro D. Marrero", ""], ["Ren", "Tsang Ing", ""], ["Cunha", "Alexandre", ""]]}, {"id": "1908.09895", "submitter": "Chunhua Shen", "authors": "Hao Lu, Yutong Dai, Chunhua Shen, Songcen Xu", "title": "Index Network", "comments": "17 pages. Extended version of \"Indices Matter: Learning to Index for\n  Deep Image Matting\" at arXiv:1908.00672", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We show that existing upsampling operators can be unified using the notion of\nthe index function. This notion is inspired by an observation in the decoding\nprocess of deep image matting where indices-guided unpooling can often recover\nboundary details considerably better than other upsampling operators such as\nbilinear interpolation. By viewing the indices as a function of the feature\nmap, we introduce the concept of \"learning to index\", and present a novel\nindex-guided encoder-decoder framework where indices are self-learned\nadaptively from data and are used to guide the downsampling and upsampling\nstages, without extra training supervision. At the core of this framework is a\nnew learnable module, termed Index Network (IndexNet), which dynamically\ngenerates indices conditioned on the feature map itself. IndexNet can be used\nas a plug-in applying to almost all off-the-shelf convolutional networks that\nhave coupled downsampling and upsampling stages, giving the networks the\nability to dynamically capture variations of local patterns. In particular, we\ninstantiate and investigate five families of IndexNet and demonstrate their\neffectiveness on four dense prediction tasks, including image denoising, image\nmatting, semantic segmentation, and monocular depth estimation. Code and models\nhave been made available at: https://tinyurl.com/IndexNetV1\n", "versions": [{"version": "v1", "created": "Sun, 11 Aug 2019 10:52:47 GMT"}, {"version": "v2", "created": "Sun, 5 Apr 2020 14:12:48 GMT"}], "update_date": "2020-04-07", "authors_parsed": [["Lu", "Hao", ""], ["Dai", "Yutong", ""], ["Shen", "Chunhua", ""], ["Xu", "Songcen", ""]]}, {"id": "1908.09935", "submitter": "Yu Chen", "authors": "Alem Fitwi, Yu Chen, Sencun Zhu", "title": "No Peeking through My Windows: Conserving Privacy in Personal Drones", "comments": "To be presented at The Fifth IEEE Annual International Smart Cities\n  Conference (ISC2 2019), Casablanca, Morocco, October 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The drone technology has been increasingly used by many tech-savvy consumers,\na number of defense companies, hobbyists and enthusiasts during the last ten\nyears. Drones often come in various sizes and are designed for a multitude of\npurposes. Nowadays many people have small-sized personal drones for\nentertainment, filming, or transporting items from one place to another.\nHowever, personal drones lack a privacy-preserving mechanism. While in mission,\ndrones often trespass into the personal territories of other people and capture\nphotos or videos through windows without their knowledge and consent. They may\nalso capture video or pictures of people walking, sitting, or doing private\nthings within the drones' reach in clear form without their go permission. This\ncould potentially invade people's personal privacy. This paper, therefore,\nproposes a lightweight privacy-preserving-by-design method that prevents drones\nfrom peeking through windows of houses and capturing people doing private\nthings at home. It is a fast window object detection and scrambling technology\nbuilt based on image-enhancing, morphological transformation, segmentation and\ncontouring processes (MASP). Besides, a chaotic scrambling technique is\nincorporated into it for privacy purpose. Hence, this mechanism detects window\nobjects in every image or frame of a real-time video and masks them chaotically\nto protect the privacy of people. The experimental results validated that the\nproposed MASP method is lightweight and suitable to be employed in drones,\nconsidered as edge devices.\n", "versions": [{"version": "v1", "created": "Mon, 26 Aug 2019 22:06:55 GMT"}], "update_date": "2019-08-28", "authors_parsed": [["Fitwi", "Alem", ""], ["Chen", "Yu", ""], ["Zhu", "Sencun", ""]]}, {"id": "1908.09943", "submitter": "Furkan K{\\i}nl{\\i}", "authors": "Furkan K{\\i}nl{\\i} and Bar{\\i}\\c{s} \\\"Ozcan and Furkan K{\\i}ra\\c{c}", "title": "Fashion Image Retrieval with Capsule Networks", "comments": "Accepted to the International Conference on Computer Vision, ICCV\n  2019, Workshop on Computer Vision for Fashion, Art and Design", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this study, we investigate in-shop clothing retrieval performance of\ndensely-connected Capsule Networks with dynamic routing. To achieve this, we\npropose Triplet-based design of Capsule Network architecture with two different\nfeature extraction methods. In our design, Stacked-convolutional (SC) and\nResidual-connected (RC) blocks are used to form the input of capsule layers.\nExperimental results show that both of our designs outperform all variants of\nthe baseline study, namely FashionNet, without relying on the landmark\ninformation. Moreover, when compared to the SOTA architectures on clothing\nretrieval, our proposed Triplet Capsule Networks achieve comparable recall\nrates only with half of parameters used in the SOTA architectures.\n", "versions": [{"version": "v1", "created": "Mon, 26 Aug 2019 22:33:14 GMT"}], "update_date": "2019-08-28", "authors_parsed": [["K\u0131nl\u0131", "Furkan", ""], ["\u00d6zcan", "Bar\u0131\u015f", ""], ["K\u0131ra\u00e7", "Furkan", ""]]}, {"id": "1908.09948", "submitter": "Hossein Sadeghi Esfahani", "authors": "Hossein Sadeghi, Evgeny Andriyash, Walter Vinci, Lorenzo Buffoni,\n  Mohammad H. Amin", "title": "PixelVAE++: Improved PixelVAE with Discrete Prior", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Constructing powerful generative models for natural images is a challenging\ntask. PixelCNN models capture details and local information in images very well\nbut have limited receptive field. Variational autoencoders with a factorial\ndecoder can capture global information easily, but they often fail to\nreconstruct details faithfully. PixelVAE combines the best features of the two\nmodels and constructs a generative model that is able to learn local and global\nstructures. Here we introduce PixelVAE++, a VAE with three types of latent\nvariables and a PixelCNN++ for the decoder. We introduce a novel architecture\nthat reuses a part of the decoder as an encoder. We achieve the state of the\nart performance on binary data sets such as MNIST and Omniglot and achieve the\nstate of the art performance on CIFAR-10 among latent variable models while\nkeeping the latent variables informative.\n", "versions": [{"version": "v1", "created": "Mon, 26 Aug 2019 22:40:55 GMT"}], "update_date": "2019-08-28", "authors_parsed": [["Sadeghi", "Hossein", ""], ["Andriyash", "Evgeny", ""], ["Vinci", "Walter", ""], ["Buffoni", "Lorenzo", ""], ["Amin", "Mohammad H.", ""]]}, {"id": "1908.09990", "submitter": "Xugong Qin", "authors": "Xugong Qin, Yu Zhou, Dongbao Yang, Weiping Wang", "title": "Curved Text Detection in Natural Scene Images with Semi- and\n  Weakly-Supervised Learning", "comments": "Accepted by ICDAR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Detecting curved text in the wild is very challenging. Recently, most\nstate-of-the-art methods are segmentation based and require pixel-level\nannotations. We propose a novel scheme to train an accurate text detector using\nonly a small amount of pixel-level annotated data and a large amount of data\nannotated with rectangles or even unlabeled data. A baseline model is first\nobtained by training with the pixel-level annotated data and then used to\nannotate unlabeled or weakly labeled data. A novel strategy which utilizes\nground-truth bounding boxes to generate pseudo mask annotations is proposed in\nweakly-supervised learning. Experimental results on CTW1500 and Total-Text\ndemonstrate that our method can substantially reduce the requirement of\npixel-level annotated data. Our method can also generalize well across two\ndatasets. The performance of the proposed method is comparable with the\nstate-of-the-art methods with only 10% pixel-level annotated data and 90%\nrectangle-level weakly annotated data.\n", "versions": [{"version": "v1", "created": "Tue, 27 Aug 2019 02:21:49 GMT"}], "update_date": "2019-08-28", "authors_parsed": [["Qin", "Xugong", ""], ["Zhou", "Yu", ""], ["Yang", "Dongbao", ""], ["Wang", "Weiping", ""]]}, {"id": "1908.09993", "submitter": "Min Xu", "authors": "Ziqian Luo, Xiangrui Zeng, Zhipeng Bao, Min Xu", "title": "Deep Learning-Based Strategy for Macromolecules Classification with\n  Imbalanced Data from Cellular Electron Cryotomography", "comments": "13 pages. arXiv admin note: text overlap with arXiv:1710.09412,\n  arXiv:1710.05381, arXiv:1708.02002 by other authors", "journal-ref": "2019 International Joint Conference on Neural Networks (IJCNN)", "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning model trained by imbalanced data may not work satisfactorily\nsince it could be determined by major classes and thus may ignore the classes\nwith small amount of data. In this paper, we apply deep learning based\nimbalanced data classification for the first time to cellular macromolecular\ncomplexes captured by Cryo-electron tomography (Cryo-ET). We adopt a range of\nstrategies to cope with imbalanced data, including data sampling, bagging,\nboosting, Genetic Programming based method and. Particularly, inspired from\nInception 3D network, we propose a multi-path CNN model combining focal loss\nand mixup on the Cryo-ET dataset to expand the dataset, where each path had its\nbest performance corresponding to each type of data and let the network learn\nthe combinations of the paths to improve the classification performance. In\naddition, extensive experiments have been conducted to show our proposed method\nis flexible enough to cope with different number of classes by adjusting the\nnumber of paths in our multi-path model. To our knowledge, this work is the\nfirst application of deep learning methods of dealing with imbalanced data to\nthe internal tissue classification of cell macromolecular complexes, which\nopened up a new path for cell classification in the field of computational\nbiology.\n", "versions": [{"version": "v1", "created": "Tue, 27 Aug 2019 02:37:42 GMT"}], "update_date": "2019-08-28", "authors_parsed": [["Luo", "Ziqian", ""], ["Zeng", "Xiangrui", ""], ["Bao", "Zhipeng", ""], ["Xu", "Min", ""]]}, {"id": "1908.09995", "submitter": "Jr Zhang", "authors": "Jingran Zhang, Fumin Shen, Xing Xu, Heng Tao Shen", "title": "Temporal Reasoning Graph for Activity Recognition", "comments": "14pages, 8figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite great success has been achieved in activity analysis, it still has\nmany challenges. Most existing work in activity recognition pay more attention\nto design efficient architecture or video sampling strategy. However, due to\nthe property of fine-grained action and long term structure in video, activity\nrecognition is expected to reason temporal relation between video sequences. In\nthis paper, we propose an efficient temporal reasoning graph (TRG) to\nsimultaneously capture the appearance features and temporal relation between\nvideo sequences at multiple time scales. Specifically, we construct learnable\ntemporal relation graphs to explore temporal relation on the multi-scale range.\nAdditionally, to facilitate multi-scale temporal relation extraction, we design\na multi-head temporal adjacent matrix to represent multi-kinds of temporal\nrelations. Eventually, a multi-head temporal relation aggregator is proposed to\nextract the semantic meaning of those features convolving through the graphs.\nExtensive experiments are performed on widely-used large-scale datasets, such\nas Something-Something and Charades, and the results show that our model can\nachieve state-of-the-art performance. Further analysis shows that temporal\nrelation reasoning with our TRG can extract discriminative features for\nactivity recognition.\n", "versions": [{"version": "v1", "created": "Tue, 27 Aug 2019 02:39:52 GMT"}], "update_date": "2019-08-28", "authors_parsed": [["Zhang", "Jingran", ""], ["Shen", "Fumin", ""], ["Xu", "Xing", ""], ["Shen", "Heng Tao", ""]]}, {"id": "1908.09998", "submitter": "Gukyeong Kwon", "authors": "Gukyeong Kwon, Mohit Prabhushankar, Dogancan Temel, Ghassan AlRegib", "title": "Distorted Representation Space Characterization Through Backpropagated\n  Gradients", "comments": "5 pages, 5 figures, 2 tables, ICIP 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we utilize weight gradients from backpropagation to\ncharacterize the representation space learned by deep learning algorithms. We\ndemonstrate the utility of such gradients in applications including perceptual\nimage quality assessment and out-of-distribution classification. The\napplications are chosen to validate the effectiveness of gradients as features\nwhen the test image distribution is distorted from the train image\ndistribution. In both applications, the proposed gradient based features\noutperform activation features. In image quality assessment, the proposed\napproach is compared with other state of the art approaches and is generally\nthe top performing method on TID 2013 and MULTI-LIVE databases in terms of\naccuracy, consistency, linearity, and monotonic behavior. Finally, we analyze\nthe effect of regularization on gradients using CURE-TSR dataset for\nout-of-distribution classification.\n", "versions": [{"version": "v1", "created": "Tue, 27 Aug 2019 02:58:43 GMT"}], "update_date": "2019-08-28", "authors_parsed": [["Kwon", "Gukyeong", ""], ["Prabhushankar", "Mohit", ""], ["Temel", "Dogancan", ""], ["AlRegib", "Ghassan", ""]]}, {"id": "1908.09999", "submitter": "Fu Xiong", "authors": "Fu Xiong, Boshen Zhang, Yang Xiao, Zhiguo Cao, Taidong Yu, Joey Tianyi\n  Zhou, Junsong Yuan", "title": "A2J: Anchor-to-Joint Regression Network for 3D Articulated Pose\n  Estimation from a Single Depth Image", "comments": "Accepted by ICCV2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For 3D hand and body pose estimation task in depth image, a novel\nanchor-based approach termed Anchor-to-Joint regression network (A2J) with the\nend-to-end learning ability is proposed. Within A2J, anchor points able to\ncapture global-local spatial context information are densely set on depth image\nas local regressors for the joints. They contribute to predict the positions of\nthe joints in ensemble way to enhance generalization ability. The proposed 3D\narticulated pose estimation paradigm is different from the state-of-the-art\nencoder-decoder based FCN, 3D CNN and point-set based manners. To discover\ninformative anchor points towards certain joint, anchor proposal procedure is\nalso proposed for A2J. Meanwhile 2D CNN (i.e., ResNet-50) is used as backbone\nnetwork to drive A2J, without using time-consuming 3D convolutional or\ndeconvolutional layers. The experiments on 3 hand datasets and 2 body datasets\nverify A2J's superiority. Meanwhile, A2J is of high running speed around 100\nFPS on single NVIDIA 1080Ti GPU.\n", "versions": [{"version": "v1", "created": "Tue, 27 Aug 2019 02:58:47 GMT"}], "update_date": "2019-08-28", "authors_parsed": [["Xiong", "Fu", ""], ["Zhang", "Boshen", ""], ["Xiao", "Yang", ""], ["Cao", "Zhiguo", ""], ["Yu", "Taidong", ""], ["Zhou", "Joey Tianyi", ""], ["Yuan", "Junsong", ""]]}, {"id": "1908.10009", "submitter": "Peng Gao", "authors": "Peng Gao, Qiquan Zhang, Fei Wang, Liyi Xiao, Hamido Fujita, Yan Zhang", "title": "Learning Reinforced Attentional Representation for End-to-End Visual\n  Tracking", "comments": "Accepted by Information Sciences", "journal-ref": null, "doi": "10.1016/j.ins.2019.12.084", "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although numerous recent tracking approaches have made tremendous advances in\nthe last decade, achieving high-performance visual tracking remains a\nchallenge. In this paper, we propose an end-to-end network model to learn\nreinforced attentional representation for accurate target object discrimination\nand localization. We utilize a novel hierarchical attentional module with long\nshort-term memory and multi-layer perceptrons to leverage both inter- and\nintra-frame attention to effectively facilitate visual pattern emphasis.\nMoreover, we incorporate a contextual attentional correlation filter into the\nbackbone network to make our model trainable in an end-to-end fashion. Our\nproposed approach not only takes full advantage of informative geometries and\nsemantics but also updates correlation filters online without fine-tuning the\nbackbone network to enable the adaptation of variations in the target object's\nappearance. Extensive experiments conducted on several popular benchmark\ndatasets demonstrate that our proposed approach is effective and\ncomputationally efficient.\n", "versions": [{"version": "v1", "created": "Tue, 27 Aug 2019 03:55:17 GMT"}, {"version": "v2", "created": "Wed, 28 Aug 2019 00:39:16 GMT"}, {"version": "v3", "created": "Thu, 2 Jan 2020 01:07:09 GMT"}], "update_date": "2020-01-03", "authors_parsed": [["Gao", "Peng", ""], ["Zhang", "Qiquan", ""], ["Wang", "Fei", ""], ["Xiao", "Liyi", ""], ["Fujita", "Hamido", ""], ["Zhang", "Yan", ""]]}, {"id": "1908.10012", "submitter": "Yuanwei Wu", "authors": "Yuanwei Wu, Ziming Zhang and Guanghui Wang", "title": "Unsupervised Deep Feature Transfer for Low Resolution Image\n  Classification", "comments": "4 pages, accepted to ICCV19 Workshop and Challenge on Real-World\n  Recognition from Low-Quality Images and Videos", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a simple while effective unsupervised deep feature\ntransfer algorithm for low resolution image classification. No fine-tuning on\nconvenet filters is required in our method. We use pre-trained convenet to\nextract features for both high- and low-resolution images, and then feed them\ninto a two-layer feature transfer network for knowledge transfer. A SVM\nclassifier is learned directly using these transferred low resolution features.\nOur network can be embedded into the state-of-the-art deep neural networks as a\nplug-in feature enhancement module. It preserves data structures in feature\nspace for high resolution images, and transfers the distinguishing features\nfrom a well-structured source domain (high resolution features space) to a not\nwell-organized target domain (low resolution features space). Extensive\nexperiments on VOC2007 test set show that the proposed method achieves\nsignificant improvements over the baseline of using feature extraction.\n", "versions": [{"version": "v1", "created": "Tue, 27 Aug 2019 04:06:02 GMT"}, {"version": "v2", "created": "Sat, 19 Oct 2019 19:56:27 GMT"}], "update_date": "2019-10-22", "authors_parsed": [["Wu", "Yuanwei", ""], ["Zhang", "Ziming", ""], ["Wang", "Guanghui", ""]]}, {"id": "1908.10027", "submitter": "Maneet Singh", "authors": "Maneet Singh, Shruti Nagpal, Richa Singh, and Mayank Vatsa", "title": "Dual Directed Capsule Network for Very Low Resolution Image Recognition", "comments": "Accepted in the International Conference on Computer Vision (ICCV)\n  2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Very low resolution (VLR) image recognition corresponds to classifying images\nwith resolution 16x16 or less. Though it has widespread applicability when\nobjects are captured at a very large stand-off distance (e.g. surveillance\nscenario) or from wide angle mobile cameras, it has received limited attention.\nThis research presents a novel Dual Directed Capsule Network model, termed as\nDirectCapsNet, for addressing VLR digit and face recognition. The proposed\narchitecture utilizes a combination of capsule and convolutional layers for\nlearning an effective VLR recognition model. The architecture also incorporates\ntwo novel loss functions: (i) the proposed HR-anchor loss and (ii) the proposed\ntargeted reconstruction loss, in order to overcome the challenges of limited\ninformation content in VLR images. The proposed losses use high resolution\nimages as auxiliary data during training to \"direct\" discriminative feature\nlearning. Multiple experiments for VLR digit classification and VLR face\nrecognition are performed along with comparisons with state-of-the-art\nalgorithms. The proposed DirectCapsNet consistently showcases state-of-the-art\nresults; for example, on the UCCS face database, it shows over 95\\% face\nrecognition accuracy when 16x16 images are matched with 80x80 images.\n", "versions": [{"version": "v1", "created": "Tue, 27 Aug 2019 04:42:57 GMT"}], "update_date": "2019-08-28", "authors_parsed": [["Singh", "Maneet", ""], ["Nagpal", "Shruti", ""], ["Singh", "Richa", ""], ["Vatsa", "Mayank", ""]]}, {"id": "1908.10028", "submitter": "Hyunjung Shim Dr.", "authors": "Junsuk Choe and Hyunjung Shim", "title": "Attention-based Dropout Layer for Weakly Supervised Object Localization", "comments": "CVPR 2019 (Oral)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Weakly Supervised Object Localization (WSOL) techniques learn the object\nlocation only using image-level labels, without location annotations. A common\nlimitation for these techniques is that they cover only the most discriminative\npart of the object, not the entire object. To address this problem, we propose\nan Attention-based Dropout Layer (ADL), which utilizes the self-attention\nmechanism to process the feature maps of the model. The proposed method is\ncomposed of two key components: 1) hiding the most discriminative part from the\nmodel for capturing the integral extent of object, and 2) highlighting the\ninformative region for improving the recognition power of the model. Based on\nextensive experiments, we demonstrate that the proposed method is effective to\nimprove the accuracy of WSOL, achieving a new state-of-the-art localization\naccuracy in CUB-200-2011 dataset. We also show that the proposed method is much\nmore efficient in terms of both parameter and computation overheads than\nexisting techniques.\n", "versions": [{"version": "v1", "created": "Tue, 27 Aug 2019 04:48:16 GMT"}], "update_date": "2019-08-28", "authors_parsed": [["Choe", "Junsuk", ""], ["Shim", "Hyunjung", ""]]}, {"id": "1908.10049", "submitter": "Jianing Li", "authors": "Jianing Li, Jingdong Wang, Qi Tian, Wen Gao, Shiliang Zhang", "title": "Global-Local Temporal Representations For Video Person Re-Identification", "comments": null, "journal-ref": "ICCV2019", "doi": "10.1109/TIP.2020.2972108", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes the Global-Local Temporal Representation (GLTR) to\nexploit the multi-scale temporal cues in video sequences for video person\nRe-Identification (ReID). GLTR is constructed by first modeling the short-term\ntemporal cues among adjacent frames, then capturing the long-term relations\namong inconsecutive frames. Specifically, the short-term temporal cues are\nmodeled by parallel dilated convolutions with different temporal dilation rates\nto represent the motion and appearance of pedestrian. The long-term relations\nare captured by a temporal self-attention model to alleviate the occlusions and\nnoises in video sequences. The short and long-term temporal cues are aggregated\nas the final GLTR by a simple single-stream CNN. GLTR shows substantial\nsuperiority to existing features learned with body part cues or metric learning\non four widely-used video ReID datasets. For instance, it achieves Rank-1\nAccuracy of 87.02% on MARS dataset without re-ranking, better than current\nstate-of-the art.\n", "versions": [{"version": "v1", "created": "Tue, 27 Aug 2019 06:57:03 GMT"}], "update_date": "2020-04-22", "authors_parsed": [["Li", "Jianing", ""], ["Wang", "Jingdong", ""], ["Tian", "Qi", ""], ["Gao", "Wen", ""], ["Zhang", "Shiliang", ""]]}, {"id": "1908.10059", "submitter": "Mak Chihjun", "authors": "Zhijun Mai, Guosheng Hu, Dexiong Chen, Fumin Shen, Heng Tao Shen", "title": "MetaMixUp: Learning Adaptive Interpolation Policy of MixUp with\n  Meta-Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  MixUp is an effective data augmentation method to regularize deep neural\nnetworks via random linear interpolations between pairs of samples and their\nlabels. It plays an important role in model regularization, semi-supervised\nlearning and domain adaption. However, despite its empirical success, its\ndeficiency of randomly mixing samples has poorly been studied. Since deep\nnetworks are capable of memorizing the entire dataset, the corrupted samples\ngenerated by vanilla MixUp with a badly chosen interpolation policy will\ndegrade the performance of networks. To overcome the underfitting by corrupted\nsamples, inspired by Meta-learning (learning to learn), we propose a novel\ntechnique of learning to mixup in this work, namely, MetaMixUp. Unlike the\nvanilla MixUp that samples interpolation policy from a predefined distribution,\nthis paper introduces a meta-learning based online optimization approach to\ndynamically learn the interpolation policy in a data-adaptive way. The\nvalidation set performance via meta-learning captures the underfitting issue,\nwhich provides more information to refine interpolation policy. Furthermore, we\nadapt our method for pseudo-label based semisupervised learning (SSL) along\nwith a refined pseudo-labeling strategy. In our experiments, our method\nachieves better performance than vanilla MixUp and its variants under\nsupervised learning configuration. In particular, extensive experiments show\nthat our MetaMixUp adapted SSL greatly outperforms MixUp and many\nstate-of-the-art methods on CIFAR-10 and SVHN benchmarks under SSL\nconfiguration.\n", "versions": [{"version": "v1", "created": "Tue, 27 Aug 2019 07:26:35 GMT"}], "update_date": "2019-08-28", "authors_parsed": [["Mai", "Zhijun", ""], ["Hu", "Guosheng", ""], ["Chen", "Dexiong", ""], ["Shen", "Fumin", ""], ["Shen", "Heng Tao", ""]]}, {"id": "1908.10072", "submitter": "Bairui Wang", "authors": "Bairui Wang, Lin Ma, Wei Zhang, Wenhao Jiang, Jingwen Wang, Wei Liu", "title": "Controllable Video Captioning with POS Sequence Guidance Based on Gated\n  Fusion Network", "comments": "Accepted by ICCV 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose to guide the video caption generation with\nPart-of-Speech (POS) information, based on a gated fusion of multiple\nrepresentations of input videos. We construct a novel gated fusion network,\nwith one particularly designed cross-gating (CG) block, to effectively encode\nand fuse different types of representations, e.g., the motion and content\nfeatures of an input video. One POS sequence generator relies on this fused\nrepresentation to predict the global syntactic structure, which is thereafter\nleveraged to guide the video captioning generation and control the syntax of\nthe generated sentence. Specifically, a gating strategy is proposed to\ndynamically and adaptively incorporate the global syntactic POS information\ninto the decoder for generating each word. Experimental results on two\nbenchmark datasets, namely MSR-VTT and MSVD, demonstrate that the proposed\nmodel can well exploit complementary information from multiple representations,\nresulting in improved performances. Moreover, the generated global POS\ninformation can well capture the global syntactic structure of the sentence,\nand thus be exploited to control the syntactic structure of the description.\nSuch POS information not only boosts the video captioning performance but also\nimproves the diversity of the generated captions. Our code is at:\nhttps://github.com/vsislab/Controllable_XGating.\n", "versions": [{"version": "v1", "created": "Tue, 27 Aug 2019 08:22:54 GMT"}], "update_date": "2019-08-28", "authors_parsed": [["Wang", "Bairui", ""], ["Ma", "Lin", ""], ["Zhang", "Wei", ""], ["Jiang", "Wenhao", ""], ["Wang", "Jingwen", ""], ["Liu", "Wei", ""]]}, {"id": "1908.10098", "submitter": "Xin Wei", "authors": "Xin Wei, Ruixuan Yu, Jian Sun", "title": "HRGE-Net: Hierarchical Relational Graph Embedding Network for Multi-view\n  3D Shape Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  View-based approach that recognizes 3D shape through its projected 2D images\nachieved state-of-the-art performance for 3D shape recognition. One essential\nchallenge for view-based approach is how to aggregate the multi-view features\nextracted from 2D images to be a global 3D shape descriptor. In this work, we\npropose a novel feature aggregation network by fully investigating the\nrelations among views. We construct a relational graph with multi-view images\nas nodes, and design relational graph embedding by modeling pairwise and\nneighboring relations among views. By gradually coarsening the graph, we build\na hierarchical relational graph embedding network (HRGE-Net) to aggregate the\nmulti-view features to be a global shape descriptor. Extensive experiments show\nthat HRGE-Net achieves stateof-the-art performance for 3D shape classification\nand retrieval on benchmark datasets.\n", "versions": [{"version": "v1", "created": "Tue, 27 Aug 2019 09:23:16 GMT"}], "update_date": "2019-08-28", "authors_parsed": [["Wei", "Xin", ""], ["Yu", "Ruixuan", ""], ["Sun", "Jian", ""]]}, {"id": "1908.10109", "submitter": "Constantin Pape", "authors": "Artem Lukoyanov, Isabella Haberbosch, Constantin Pape, Alwin Kraemer,\n  Yannick Schwab, Anna Kreshuk", "title": "Synthetic patches, real images: screening for centrosome aberrations in\n  EM images of human cancer cells", "comments": "Accepted at MICCAI 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in high-throughput electron microscopy imaging enable\ndetailed study of centrosome aberrations in cancer cells. While the image\nacquisition in such pipelines is automated, manual detection of centrioles is\nstill necessary to select cells for re-imaging at higher magnification. In this\ncontribution we propose an algorithm which performs this step automatically and\nwith high accuracy. From the image labels produced by human experts and a 3D\nmodel of a centriole we construct an additional training set with patch-level\nlabels. A two-level DenseNet is trained on the hybrid training data with\nsynthetic patches and real images, achieving much better results on real\npatient data than training only at the image-level. The code can be found at\nhttps://github.com/kreshuklab/centriole_detection.\n", "versions": [{"version": "v1", "created": "Tue, 27 Aug 2019 09:48:02 GMT"}], "update_date": "2019-08-28", "authors_parsed": [["Lukoyanov", "Artem", ""], ["Haberbosch", "Isabella", ""], ["Pape", "Constantin", ""], ["Kraemer", "Alwin", ""], ["Schwab", "Yannick", ""], ["Kreshuk", "Anna", ""]]}, {"id": "1908.10136", "submitter": "Jr Zhang", "authors": "Jingran Zhang, Fumin Shen, Xing Xu, Heng Tao Shen", "title": "Cooperative Cross-Stream Network for Discriminative Action\n  Representation", "comments": "10 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spatial and temporal stream model has gained great success in video action\nrecognition. Most existing works pay more attention to designing effective\nfeatures fusion methods, which train the two-stream model in a separate way.\nHowever, it's hard to ensure discriminability and explore complementary\ninformation between different streams in existing works. In this work, we\npropose a novel cooperative cross-stream network that investigates the conjoint\ninformation in multiple different modalities. The jointly spatial and temporal\nstream networks feature extraction is accomplished by an end-to-end learning\nmanner. It extracts this complementary information of different modality from a\nconnection block, which aims at exploring correlations of different stream\nfeatures. Furthermore, different from the conventional ConvNet that learns the\ndeep separable features with only one cross-entropy loss, our proposed model\nenhances the discriminative power of the deeply learned features and reduces\nthe undesired modality discrepancy by jointly optimizing a modality ranking\nconstraint and a cross-entropy loss for both homogeneous and heterogeneous\nmodalities. The modality ranking constraint constitutes intra-modality\ndiscriminative embedding and inter-modality triplet constraint, and it reduces\nboth the intra-modality and cross-modality feature variations. Experiments on\nthree benchmark datasets demonstrate that by cooperating appearance and motion\nfeature extraction, our method can achieve state-of-the-art or competitive\nperformance compared with existing results.\n", "versions": [{"version": "v1", "created": "Tue, 27 Aug 2019 11:23:34 GMT"}], "update_date": "2019-08-28", "authors_parsed": [["Zhang", "Jingran", ""], ["Shen", "Fumin", ""], ["Xu", "Xing", ""], ["Shen", "Heng Tao", ""]]}, {"id": "1908.10139", "submitter": "Sreekanth Vempati", "authors": "Sreekanth Vempati, Korah T Malayil, Sruthi V, Sandeep R", "title": "Enabling Hyper-Personalisation: Automated Ad Creative Generation and\n  Ranking for Fashion e-Commerce", "comments": "Workshop on Recommender Systems in Fashion, 13th ACM Conference on\n  Recommender Systems, 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Homepage is the first touch point in the customer's journey and is one of the\nprominent channels of revenue for many e-commerce companies. A user's attention\nis mostly captured by homepage banner images (also called Ads/Creatives). The\nset of banners shown and their design, influence the customer's interest and\nplays a key role in optimizing the click through rates of the banners.\nPresently, massive and repetitive effort is put in, to manually create\naesthetically pleasing banner images. Due to the large amount of time and\neffort involved in this process, only a small set of banners are made live at\nany point. This reduces the number of banners created as well as the degree of\npersonalization that can be achieved. This paper thus presents a method to\ngenerate creatives automatically on a large scale in a short duration. The\navailability of diverse banners generated helps in improving personalization as\nthey can cater to the taste of larger audience. The focus of our paper is on\ngenerating wide variety of homepage banners that can be made as an input for\nuser level personalization engine. Following are the main contributions of this\npaper: 1) We introduce and explain the need for large scale banner generation\nfor e-commerce 2) We present on how we utilize existing deep learning based\ndetectors which can automatically annotate the required objects/tags from the\nimage. 3) We also propose a Genetic Algorithm based method to generate an\noptimal banner layout for the given image content, input components and other\ndesign constraints. 4) Further, to aid the process of picking the right set of\nbanners, we designed a ranking method and evaluated multiple models. All our\nexperiments have been performed on data from Myntra (http://www.myntra.com),\none of the top fashion e-commerce players in India.\n", "versions": [{"version": "v1", "created": "Tue, 27 Aug 2019 11:28:37 GMT"}], "update_date": "2019-08-28", "authors_parsed": [["Vempati", "Sreekanth", ""], ["Malayil", "Korah T", ""], ["V", "Sruthi", ""], ["R", "Sandeep", ""]]}, {"id": "1908.10155", "submitter": "Zhiwu Lu", "authors": "Yuqi Huo, Xiaoli Xu, Yao Lu, Yulei Niu, Zhiwu Lu, and Ji-Rong Wen", "title": "Mobile Video Action Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video action recognition, which is topical in computer vision and video\nanalysis, aims to allocate a short video clip to a pre-defined category such as\nbrushing hair or climbing stairs. Recent works focus on action recognition with\ndeep neural networks that achieve state-of-the-art results in need of\nhigh-performance platforms. Despite the fast development of mobile computing,\nvideo action recognition on mobile devices has not been fully discussed. In\nthis paper, we focus on the novel mobile video action recognition task, where\nonly the computational capabilities of mobile devices are accessible. Instead\nof raw videos with huge storage, we choose to extract multiple modalities\n(including I-frames, motion vectors, and residuals) directly from compressed\nvideos. By employing MobileNetV2 as backbone, we propose a novel Temporal\nTrilinear Pooling (TTP) module to fuse the multiple modalities for mobile video\naction recognition. In addition to motion vectors, we also provide a temporal\nfusion method to explicitly induce the temporal context. The efficiency test on\na mobile device indicates that our model can perform mobile video action\nrecognition at about 40FPS. The comparative results on two benchmarks show that\nour model outperforms existing action recognition methods in model size and\ntime consuming, but with competitive accuracy.\n", "versions": [{"version": "v1", "created": "Tue, 27 Aug 2019 12:16:55 GMT"}], "update_date": "2019-08-28", "authors_parsed": [["Huo", "Yuqi", ""], ["Xu", "Xiaoli", ""], ["Lu", "Yao", ""], ["Niu", "Yulei", ""], ["Lu", "Zhiwu", ""], ["Wen", "Ji-Rong", ""]]}, {"id": "1908.10163", "submitter": "Lazaro Janier Gonzalez-Soler Soler", "authors": "L\\'azaro J. Gonz\\'alez-Soler, Marta Gomez-Barrero, Leonardo Chang,\n  Airel P\\'erez-Su\\'arez, Christoph Busch", "title": "Fingerprint Presentation Attack Detection Based on Local Features\n  Encoding for Unknown Attacks", "comments": "14 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fingerprint-based biometric systems have experienced a large development in\nthe last years. Despite their many advantages, they are still vulnerable to\npresentation attacks (PAs). Therefore, the task of determining whether a sample\nstems from a live subject (i.e., bona fide) or from an artificial replica is a\nmandatory issue which has received a lot of attention recently. Nowadays, when\nthe materials for the fabrication of the Presentation Attack Instruments (PAIs)\nhave been used to train the PA Detection (PAD) methods, the PAIs can be\nsuccessfully identified. However, current PAD methods still face difficulties\ndetecting PAIs built from unknown materials or captured using other sensors.\nBased on that fact, we propose a new PAD technique based on three image\nrepresentation approaches combining local and global information of the\nfingerprint. By transforming these representations into a common feature space,\nwe can correctly discriminate bona fide from attack presentations in the\naforementioned scenarios. The experimental evaluation of our proposal over the\nLivDet 2011 to 2015 databases, yielded error rates outperforming the top\nstate-of-the-art results by up to 50\\% in the most challenging scenarios. In\naddition, the best configuration achieved the best results in the LivDet 2019\ncompetition (overall accuracy of 96.17\\%).\n", "versions": [{"version": "v1", "created": "Tue, 27 Aug 2019 12:44:37 GMT"}], "update_date": "2019-08-28", "authors_parsed": [["Gonz\u00e1lez-Soler", "L\u00e1zaro J.", ""], ["Gomez-Barrero", "Marta", ""], ["Chang", "Leonardo", ""], ["P\u00e9rez-Su\u00e1rez", "Airel", ""], ["Busch", "Christoph", ""]]}, {"id": "1908.10172", "submitter": "Mert B\\\"ulent Sar{\\i}y{\\i}ld{\\i}z", "authors": "Mert B\\\"ulent Sar{\\i}y{\\i}ld{\\i}z, Ramazan G\\\"okberk Cinbi\\c{s}, Erman\n  Ayday", "title": "Key Protected Classification for Collaborative Learning", "comments": "Accepted to Pattern Recognition", "journal-ref": null, "doi": "10.1016/j.patcog.2020.107327", "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large-scale datasets play a fundamental role in training deep learning\nmodels. However, dataset collection is difficult in domains that involve\nsensitive information. Collaborative learning techniques provide a\nprivacy-preserving solution, by enabling training over a number of private\ndatasets that are not shared by their owners. However, recently, it has been\nshown that the existing collaborative learning frameworks are vulnerable to an\nactive adversary that runs a generative adversarial network (GAN) attack. In\nthis work, we propose a novel classification model that is resilient against\nsuch attacks by design. More specifically, we introduce a key-based\nclassification model and a principled training scheme that protects class\nscores by using class-specific private keys, which effectively hide the\ninformation necessary for a GAN attack. We additionally show how to utilize\nhigh dimensional keys to improve the robustness against attacks without\nincreasing the model complexity. Our detailed experiments demonstrate the\neffectiveness of the proposed technique. Source code is available at\nhttps://github.com/mbsariyildiz/key-protected-classification.\n", "versions": [{"version": "v1", "created": "Tue, 27 Aug 2019 13:00:30 GMT"}, {"version": "v2", "created": "Wed, 22 Apr 2020 09:31:45 GMT"}], "update_date": "2020-04-23", "authors_parsed": [["Sar\u0131y\u0131ld\u0131z", "Mert B\u00fclent", ""], ["Cinbi\u015f", "Ramazan G\u00f6kberk", ""], ["Ayday", "Erman", ""]]}, {"id": "1908.10179", "submitter": "Dingyuan Zheng", "authors": "Dingyuan Zheng, Jimin Xiao, Kaizhu Huang, Yao Zhao", "title": "Segmentation Mask Guided End-to-End Person Search", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Person search aims to search for a target person among multiple images\nrecorded by multiple surveillance cameras, which faces various challenges from\nboth pedestrian detection and person re-identification. Besides the large\nintra-class variations owing to various illumination conditions, occlusions and\nvarying poses, background clutters in the detected pedestrian bounding boxes\nfurther deteriorate the extracted features for each person, making them less\ndiscriminative. To tackle these problems, we develop a novel approach which\nguides the network with segmentation masks so that discriminative features can\nbe learned invariant to the background clutters. We demonstrate that joint\noptimization of pedestrian detection, person re-identification and pedestrian\nsegmentation enables to produce more discriminative features for pedestrian,\nand consequently leads to better person search performance. Extensive\nexperiments on benchmark dataset CUHK-SYSU, show that our proposed model\nachieves the state-of-the-art performance with 86.3% mAP and 86.5 top-1\naccuracy respectively.\n", "versions": [{"version": "v1", "created": "Tue, 27 Aug 2019 13:16:00 GMT"}], "update_date": "2019-08-28", "authors_parsed": [["Zheng", "Dingyuan", ""], ["Xiao", "Jimin", ""], ["Huang", "Kaizhu", ""], ["Zhao", "Yao", ""]]}, {"id": "1908.10192", "submitter": "Andrei Boiarov", "authors": "Andrei Boiarov, Eduard Tyantov", "title": "Large Scale Landmark Recognition via Deep Metric Learning", "comments": "Accepted at CIKM 2019", "journal-ref": null, "doi": "10.1145/3357384.3357956", "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a novel approach for landmark recognition in images that\nwe've successfully deployed at Mail ru. This method enables us to recognize\nfamous places, buildings, monuments, and other landmarks in user photos. The\nmain challenge lies in the fact that it's very complicated to give a precise\ndefinition of what is and what is not a landmark. Some buildings, statues and\nnatural objects are landmarks; others are not. There's also no database with a\nfairly large number of landmarks to train a recognition model. A key feature of\nusing landmark recognition in a production environment is that the number of\nphotos containing landmarks is extremely small. This is why the model should\nhave a very low false positive rate as well as high recognition accuracy.\n  We propose a metric learning-based approach that successfully deals with\nexisting challenges and efficiently handles a large number of landmarks. Our\nmethod uses a deep neural network and requires a single pass inference that\nmakes it fast to use in production. We also describe an algorithm for cleaning\nlandmarks database which is essential for training a metric learning model. We\nprovide an in-depth description of basic components of our method like neural\nnetwork architecture, the learning strategy, and the features of our metric\nlearning approach. We show the results of proposed solutions in tests that\nemulate the distribution of photos with and without landmarks from a user\ncollection. We compare our method with others during these tests. The described\nsystem has been deployed as a part of a photo recognition solution at Cloud\nMail ru, which is the photo sharing and storage service at Mail ru Group.\n", "versions": [{"version": "v1", "created": "Tue, 27 Aug 2019 13:37:49 GMT"}, {"version": "v2", "created": "Wed, 28 Aug 2019 06:59:27 GMT"}, {"version": "v3", "created": "Thu, 29 Aug 2019 09:04:17 GMT"}], "update_date": "2019-08-30", "authors_parsed": [["Boiarov", "Andrei", ""], ["Tyantov", "Eduard", ""]]}, {"id": "1908.10219", "submitter": "Bo Li", "authors": "Bo Li, Marius de Groot, Meike Vernooij, Arfan Ikram, Wiro Niessen,\n  Esther Bron", "title": "Reproducible White Matter Tract Segmentation Using 3D U-Net on a\n  Large-scale DTI Dataset", "comments": "Machine Learning in Medical Imaging (MLMI), 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tract-specific diffusion measures, as derived from brain diffusion MRI, have\nbeen linked to white matter tract structural integrity and neurodegeneration.\nAs a consequence, there is a large interest in the automatic segmentation of\nwhite matter tract in diffusion tensor MRI data. Methods based on the\ntractography are popular for white matter tract segmentation. However, because\nof the limited consistency and long processing time, such methods may not be\nsuitable for clinical practice. We therefore developed a novel convolutional\nneural network based method to directly segment white matter tract trained on a\nlow-resolution dataset of 9149 DTI images. The method is optimized on input,\nloss function and network architecture selections. We evaluated both\nsegmentation accuracy and reproducibility, and reproducibility of determining\ntract-specific diffusion measures. The reproducibility of the method is higher\nthan that of the reference standard and the determined diffusion measures are\nconsistent. Therefore, we expect our method to be applicable in clinical\npractice and in longitudinal analysis of white matter microstructure.\n", "versions": [{"version": "v1", "created": "Mon, 26 Aug 2019 11:06:14 GMT"}], "update_date": "2019-08-28", "authors_parsed": [["Li", "Bo", ""], ["de Groot", "Marius", ""], ["Vernooij", "Meike", ""], ["Ikram", "Arfan", ""], ["Niessen", "Wiro", ""], ["Bron", "Esther", ""]]}, {"id": "1908.10221", "submitter": "Bo Li", "authors": "Bo Li, Wiro Niessen, Stefan Klein, Marius de Groot, Arfan Ikram, Meike\n  Vernooij, Esther Bron", "title": "A hybrid deep learning framework for integrated segmentation and\n  registration: evaluation on longitudinal white matter tract changes", "comments": "MICCAI 2019 (oral presentation)", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To accurately analyze changes of anatomical structures in longitudinal\nimaging studies, consistent segmentation across multiple time-points is\nrequired. Existing solutions often involve independent registration and\nsegmentation components. Registration between time-points is used either as a\nprior for segmentation in a subsequent time point or to perform segmentation in\na common space. In this work, we propose a novel hybrid convolutional neural\nnetwork (CNN) that integrates segmentation and registration into a single\nprocedure. We hypothesize that the joint optimization leads to increased\nperformance on both tasks. The hybrid CNN is trained by minimizing an\nintegrated loss function composed of four different terms, measuring\nsegmentation accuracy, similarity between registered images, deformation field\nsmoothness, and segmentation consistency. We applied this method to the\nsegmentation of white matter tracts, describing functionally grouped axonal\nfibers, using N=8045 longitudinal brain MRI data of 3249 individuals. The\nproposed method was compared with two multistage pipelines using two existing\nsegmentation methods combined with a conventional deformable registration\nalgorithm. In addition, we assessed the added value of the joint optimization\nfor segmentation and registration separately. The hybrid CNN yielded\nsignificantly higher accuracy, consistency and reproducibility of segmentation\nthan the multistage pipelines, and was orders of magnitude faster. Therefore,\nwe expect it can serve as a novel tool to support clinical and epidemiological\nanalyses on understanding microstructural brain changes over time.\n", "versions": [{"version": "v1", "created": "Mon, 26 Aug 2019 10:39:30 GMT"}], "update_date": "2019-08-28", "authors_parsed": [["Li", "Bo", ""], ["Niessen", "Wiro", ""], ["Klein", "Stefan", ""], ["de Groot", "Marius", ""], ["Ikram", "Arfan", ""], ["Vernooij", "Meike", ""], ["Bron", "Esther", ""]]}, {"id": "1908.10235", "submitter": "Hessam Sokooti", "authors": "Hessam Sokooti, Bob de Vos, Floris Berendsen, Mohsen Ghafoorian, Sahar\n  Yousefi, Boudewijn P.F. Lelieveldt, Ivana Isgum, and Marius Staring", "title": "3D Convolutional Neural Networks Image Registration Based on Efficient\n  Supervised Learning from Artificial Deformations", "comments": "TMI", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We propose a supervised nonrigid image registration method, trained using\nartificial displacement vector fields (DVF), for which we propose and compare\nthree network architectures. The artificial DVFs allow training in a fully\nsupervised and voxel-wise dense manner, but without the cost usually associated\nwith the creation of densely labeled data. We propose a scheme to artificially\ngenerate DVFs, and for chest CT registration augment these with simulated\nrespiratory motion. The proposed architectures are embedded in a multi-stage\napproach, to increase the capture range of the proposed networks in order to\nmore accurately predict larger displacements. The proposed method, RegNet, is\nevaluated on multiple databases of chest CT scans and achieved a target\nregistration error of 2.32 $\\pm$ 5.33 mm and 1.86 $\\pm$ 2.12 mm on SPREAD and\nDIR-Lab-4DCT studies, respectively. The average inference time of RegNet with\ntwo stages is about 2.2 s.\n", "versions": [{"version": "v1", "created": "Tue, 27 Aug 2019 14:34:45 GMT"}], "update_date": "2019-08-28", "authors_parsed": [["Sokooti", "Hessam", ""], ["de Vos", "Bob", ""], ["Berendsen", "Floris", ""], ["Ghafoorian", "Mohsen", ""], ["Yousefi", "Sahar", ""], ["Lelieveldt", "Boudewijn P. F.", ""], ["Isgum", "Ivana", ""], ["Staring", "Marius", ""]]}, {"id": "1908.10254", "submitter": "Xi Shen", "authors": "Xi Shen, Ilaria Pastrolin, Oumayma Bounou, Spyros Gidaris, Marc Smith,\n  Olivier Poncet, Mathieu Aubry", "title": "Large-Scale Historical Watermark Recognition: dataset and a new\n  consistency-based approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Historical watermark recognition is a highly practical, yet unsolved\nchallenge for archivists and historians. With a large number of well-defined\nclasses, cluttered and noisy samples, different types of representations, both\nsubtle differences between classes and high intra-class variation, historical\nwatermarks are also challenging for pattern recognition. In this paper,\novercoming the difficulty of data collection, we present a large public dataset\nwith more than 6k new photographs, allowing for the first time to tackle at\nscale the scenarios of practical interest for scholars: one-shot instance\nrecognition and cross-domain one-shot instance recognition amongst more than\n16k fine-grained classes. We demonstrate that this new dataset is large enough\nto train modern deep learning approaches, and show that standard methods can be\nimproved considerably by using mid-level deep features. More precisely, we\ndesign both a matching score and a feature fine-tuning strategy based on\nfiltering local matches using spatial consistency. This consistency-based\napproach provides important performance boost compared to strong baselines. Our\nmodel achieves 55% top-1 accuracy on our very challenging 16,753-class one-shot\ncross-domain recognition task, each class described by a single drawing from\nthe classic Briquet catalog. In addition to watermark classification, we show\nour approach provides promising results on fine-grained sketch-based image\nretrieval.\n", "versions": [{"version": "v1", "created": "Tue, 27 Aug 2019 15:00:32 GMT"}], "update_date": "2019-08-28", "authors_parsed": [["Shen", "Xi", ""], ["Pastrolin", "Ilaria", ""], ["Bounou", "Oumayma", ""], ["Gidaris", "Spyros", ""], ["Smith", "Marc", ""], ["Poncet", "Olivier", ""], ["Aubry", "Mathieu", ""]]}, {"id": "1908.10266", "submitter": "Santi Puch", "authors": "Santi Puch, Irina S\\'anchez, Matt Rowe", "title": "Few-shot Learning with Deep Triplet Networks for Brain Imaging Modality\n  Recognition", "comments": "Medical Image Learning with Less Labels and Imperfect Data, MICCAI\n  2019 workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Image modality recognition is essential for efficient imaging workflows in\ncurrent clinical environments, where multiple imaging modalities are used to\nbetter comprehend complex diseases. Emerging biomarkers from novel, rare\nmodalities are being developed to aid in such understanding, however the\navailability of these images is often limited. This scenario raises the\nnecessity of recognising new imaging modalities without them being collected\nand annotated in large amounts. In this work, we present a few-shot learning\nmodel for limited training examples based on Deep Triplet Networks. We show\nthat the proposed model is more accurate in distinguishing different modalities\nthan a traditional Convolutional Neural Network classifier when limited samples\nare available. Furthermore, we evaluate the performance of both classifiers\nwhen presented with noisy samples and provide an initial inspection of how the\nproposed model can incorporate measures of uncertainty to be more robust\nagainst out-of-sample examples.\n", "versions": [{"version": "v1", "created": "Tue, 27 Aug 2019 15:19:24 GMT"}], "update_date": "2019-08-28", "authors_parsed": [["Puch", "Santi", ""], ["S\u00e1nchez", "Irina", ""], ["Rowe", "Matt", ""]]}, {"id": "1908.10267", "submitter": "S Deng", "authors": "Sen Deng, Mingqiang Wei, Jun Wang, Luming Liang, Haoran Xie, and Meng\n  Wang", "title": "DRD-Net: Detail-recovery Image Deraining via Context Aggregation\n  Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image deraining is a fundamental, yet not well-solved problem in computer\nvision and graphics. The traditional image deraining approaches commonly behave\nineffectively in medium and heavy rain removal, while the learning-based ones\nlead to image degradations such as the loss of image details, halo artifacts\nand/or color distortion. Unlike existing image deraining approaches that lack\nthe detail-recovery mechanism, we propose an end-to-end detail-recovery image\nderaining network (termed a DRD-Net) for single images. We for the first time\nintroduce two sub-networks with a comprehensive loss function which synergize\nto derain and recover the lost details caused by deraining. We have three key\ncontributions. First, we present a rain residual network to remove rain streaks\nfrom the rainy images, which combines the squeeze-and-excitation (SE) operation\nwith residual blocks to make full advantage of spatial contextual information.\nSecond, we design a new connection style block, named structure detail context\naggregation block (SDCAB), which aggregates context feature information and has\na large reception field. Third, benefiting from the SDCAB, we construct a\ndetail repair network to encourage the lost details to return for eliminating\nimage degradations. We have validated our approach on four recognized datasets\n(three synthetic and one real-world). Both quantitative and qualitative\ncomparisons show that our approach outperforms the state-of-the-art deraining\nmethods in terms of the deraining robustness and detail accuracy. The source\ncode has been available for public evaluation and use on GitHub.\n", "versions": [{"version": "v1", "created": "Tue, 27 Aug 2019 15:22:09 GMT"}, {"version": "v2", "created": "Wed, 28 Aug 2019 12:01:53 GMT"}], "update_date": "2019-08-29", "authors_parsed": [["Deng", "Sen", ""], ["Wei", "Mingqiang", ""], ["Wang", "Jun", ""], ["Liang", "Luming", ""], ["Xie", "Haoran", ""], ["Wang", "Meng", ""]]}, {"id": "1908.10281", "submitter": "Santi Puch", "authors": "Santi Puch, Irina S\\'anchez, Aura Hern\\'andez, Gemma Piella, Vesna\n  Prchkovska", "title": "Global Planar Convolutions for improved context aggregation in Brain\n  Tumor Segmentation", "comments": "Brainlesion: Glioma, Multiple Sclerosis, Stroke and Traumatic Brain\n  Injuries. BrainLes 2018", "journal-ref": null, "doi": "10.1007/978-3-030-11726-9_35", "report-no": null, "categories": "eess.IV cs.CV stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In this work, we introduce the Global Planar Convolution module as a\nbuilding-block for fully-convolutional networks that aggregates global\ninformation and, therefore, enhances the context perception capabilities of\nsegmentation networks in the context of brain tumor segmentation. We implement\ntwo baseline architectures (3D UNet and a residual version of 3D UNet, ResUNet)\nand present a novel architecture based on these two architectures, ContextNet,\nthat includes the proposed Global Planar Convolution module. We show that the\naddition of such module eliminates the need of building networks with several\nrepresentation levels, which tend to be over-parametrized and to showcase slow\nrates of convergence. Furthermore, we provide a visual demonstration of the\nbehavior of GPC modules via visualization of intermediate representations. We\nfinally participate in the 2018 edition of the BraTS challenge with our best\nperforming models, that are based on ContextNet, and report the evaluation\nscores on the validation and the test sets of the challenge.\n", "versions": [{"version": "v1", "created": "Tue, 27 Aug 2019 15:38:50 GMT"}], "update_date": "2019-08-28", "authors_parsed": [["Puch", "Santi", ""], ["S\u00e1nchez", "Irina", ""], ["Hern\u00e1ndez", "Aura", ""], ["Piella", "Gemma", ""], ["Prchkovska", "Vesna", ""]]}, {"id": "1908.10285", "submitter": "Sandro Pezzelle", "authors": "Sandro Pezzelle and Raquel Fern\\'andez", "title": "Is the Red Square Big? MALeViC: Modeling Adjectives Leveraging Visual\n  Contexts", "comments": "Accepted at EMNLP-IJCNLP 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work aims at modeling how the meaning of gradable adjectives of size\n(`big', `small') can be learned from visually-grounded contexts. Inspired by\ncognitive and linguistic evidence showing that the use of these expressions\nrelies on setting a threshold that is dependent on a specific context, we\ninvestigate the ability of multi-modal models in assessing whether an object is\n`big' or `small' in a given visual scene. In contrast with the standard\ncomputational approach that simplistically treats gradable adjectives as\n`fixed' attributes, we pose the problem as relational: to be successful, a\nmodel has to consider the full visual context. By means of four main tasks, we\nshow that state-of-the-art models (but not a relatively strong baseline) can\nlearn the function subtending the meaning of size adjectives, though their\nperformance is found to decrease while moving from simple to more complex\ntasks. Crucially, models fail in developing abstract representations of\ngradable adjectives that can be used compositionally.\n", "versions": [{"version": "v1", "created": "Tue, 27 Aug 2019 15:44:17 GMT"}], "update_date": "2019-08-28", "authors_parsed": [["Pezzelle", "Sandro", ""], ["Fern\u00e1ndez", "Raquel", ""]]}, {"id": "1908.10307", "submitter": "Youngjun Cho", "authors": "Youngjun Cho, Nadia Bianchi-Berthouze", "title": "Physiological and Affective Computing through Thermal Imaging: A Survey", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Thermal imaging-based physiological and affective computing is an emerging\nresearch area enabling technologies to monitor our bodily functions and\nunderstand psychological and affective needs in a contactless manner. However,\nup to recently, research has been mainly carried out in very controlled lab\nsettings. As small size and even low-cost versions of thermal video cameras\nhave started to appear on the market, mobile thermal imaging is opening its\ndoor to ubiquitous and real-world applications. Here we review the literature\non the use of thermal imaging to track changes in physiological cues relevant\nto affective computing and the technological requirements set so far. In doing\nso, we aim to establish computational and methodological pipelines from thermal\nimages of the human skin to affective states and outline the research\nopportunities and challenges to be tackled to make ubiquitous real-life thermal\nimaging-based affect monitoring a possibility.\n", "versions": [{"version": "v1", "created": "Tue, 27 Aug 2019 16:30:51 GMT"}], "update_date": "2019-08-28", "authors_parsed": [["Cho", "Youngjun", ""], ["Bianchi-Berthouze", "Nadia", ""]]}, {"id": "1908.10335", "submitter": "Raoul de Charette", "authors": "Shirsendu Sukanta Halder, Jean-Fran\\c{c}ois Lalonde, Raoul de Charette", "title": "Physics-Based Rendering for Improving Robustness to Rain", "comments": "ICCV 2019. Supplementary pdf / videos available on project page", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To improve the robustness to rain, we present a physically-based rain\nrendering pipeline for realistically inserting rain into clear weather images.\nOur rendering relies on a physical particle simulator, an estimation of the\nscene lighting and an accurate rain photometric modeling to augment images with\narbitrary amount of realistic rain or fog. We validate our rendering with a\nuser study, proving our rain is judged 40% more realistic that\nstate-of-the-art. Using our generated weather augmented Kitti and Cityscapes\ndataset, we conduct a thorough evaluation of deep object detection and semantic\nsegmentation algorithms and show that their performance decreases in degraded\nweather, on the order of 15% for object detection and 60% for semantic\nsegmentation. Furthermore, we show refining existing networks with our\naugmented images improves the robustness of both object detection and semantic\nsegmentation algorithms. We experiment on nuScenes and measure an improvement\nof 15% for object detection and 35% for semantic segmentation compared to\noriginal rainy performance. Augmented databases and code are available on the\nproject page.\n", "versions": [{"version": "v1", "created": "Tue, 27 Aug 2019 17:13:46 GMT"}], "update_date": "2019-08-28", "authors_parsed": [["Halder", "Shirsendu Sukanta", ""], ["Lalonde", "Jean-Fran\u00e7ois", ""], ["de Charette", "Raoul", ""]]}, {"id": "1908.10344", "submitter": "Xiangping Zhu", "authors": "Xiangping Zhu and Xiatian Zhu and Minxian Li and Vittorio Murino and\n  Shaogang Gong", "title": "Intra-Camera Supervised Person Re-Identification: A New Benchmark", "comments": "9 pages, 3 figures, accepted by ICCV Workshop on Real-World\n  Recognition from Low-Quality Images and Videos, 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing person re-identification (re-id) methods rely mostly on a large set\nof inter-camera identity labelled training data, requiring a tedious data\ncollection and annotation process therefore leading to poor scalability in\npractical re-id applications. To overcome this fundamental limitation, we\nconsider person re-identification without inter-camera identity association but\nonly with identity labels independently annotated within each individual\ncamera-view. This eliminates the most time-consuming and tedious inter-camera\nidentity labelling process in order to significantly reduce the amount of human\nefforts required during annotation. It hence gives rise to a more scalable and\nmore feasible learning scenario, which we call Intra-Camera Supervised (ICS)\nperson re-id. Under this ICS setting with weaker label supervision, we\nformulate a Multi-Task Multi-Label (MTML) deep learning method. Given no\ninter-camera association, MTML is specially designed for self-discovering the\ninter-camera identity correspondence. This is achieved by inter-camera\nmulti-label learning under a joint multi-task inference framework. In addition,\nMTML can also efficiently learn the discriminative re-id feature\nrepresentations by fully using the available identity labels within each\ncamera-view. Extensive experiments demonstrate the performance superiority of\nour MTML model over the state-of-the-art alternative methods on three\nlarge-scale person re-id datasets in the proposed intra-camera supervised\nlearning setting.\n", "versions": [{"version": "v1", "created": "Tue, 27 Aug 2019 17:33:48 GMT"}], "update_date": "2019-08-28", "authors_parsed": [["Zhu", "Xiangping", ""], ["Zhu", "Xiatian", ""], ["Li", "Minxian", ""], ["Murino", "Vittorio", ""], ["Gong", "Shaogang", ""]]}, {"id": "1908.10349", "submitter": "Jiunn-Kai Huang", "authors": "Jiunn-Kai Huang, Shoutian Wang, Maani Ghaffari, and Jessy W. Grizzle", "title": "LiDARTag: A Real-Time Fiducial Tag System for Point Clouds", "comments": null, "journal-ref": "IEEE Robotics and Automation Letters, 31 March 2021", "doi": "10.1109/LRA.2021.3070302", "report-no": null, "categories": "cs.RO cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image-based fiducial markers are useful in problems such as object tracking\nin cluttered or textureless environments, camera (and multi-sensor) calibration\ntasks, and vision-based simultaneous localization and mapping (SLAM). The\nstate-of-the-art fiducial marker detection algorithms rely on the consistency\nof the ambient lighting. This paper introduces LiDARTag, a novel fiducial tag\ndesign and detection algorithm suitable for light detection and ranging (LiDAR)\npoint clouds. The proposed method runs in real-time and can process data at 100\nHz, which is faster than the currently available LiDAR sensor frequencies.\nBecause of the LiDAR sensors' nature, rapidly changing ambient lighting will\nnot affect the detection of a LiDARTag; hence, the proposed fiducial marker can\noperate in a completely dark environment. In addition, the LiDARTag nicely\ncomplements and is compatible with existing visual fiducial markers, such as\nAprilTags, allowing for efficient multi-sensor fusion and calibration tasks. We\nfurther propose a concept of minimizing a fitting error between a point cloud\nand the marker's template to estimate the marker's pose. The proposed method\nachieves millimeter error in translation and a few degrees in rotation. Due to\nLiDAR returns' sparsity, the point cloud is lifted to a continuous function in\na reproducing kernel Hilbert space where the inner product can be used to\ndetermine a marker's ID. The experimental results, verified by a motion capture\nsystem, confirm that the proposed method can reliably provide a tag's pose and\nunique ID code. The rejection of false positives is validated on the Google\nCartographer indoor dataset and the Honda H3D outdoor dataset. All\nimplementations are coded in C++ and are available at:\nhttps://github.com/UMich-BipedLab/LiDARTag.\n", "versions": [{"version": "v1", "created": "Fri, 23 Aug 2019 22:10:39 GMT"}, {"version": "v2", "created": "Tue, 3 Nov 2020 20:07:16 GMT"}, {"version": "v3", "created": "Sat, 13 Feb 2021 22:33:19 GMT"}], "update_date": "2021-04-05", "authors_parsed": [["Huang", "Jiunn-Kai", ""], ["Wang", "Shoutian", ""], ["Ghaffari", "Maani", ""], ["Grizzle", "Jessy W.", ""]]}, {"id": "1908.10357", "submitter": "Bowen Cheng", "authors": "Bowen Cheng, Bin Xiao, Jingdong Wang, Honghui Shi, Thomas S. Huang,\n  and Lei Zhang", "title": "HigherHRNet: Scale-Aware Representation Learning for Bottom-Up Human\n  Pose Estimation", "comments": "CVPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bottom-up human pose estimation methods have difficulties in predicting the\ncorrect pose for small persons due to challenges in scale variation. In this\npaper, we present HigherHRNet: a novel bottom-up human pose estimation method\nfor learning scale-aware representations using high-resolution feature\npyramids. Equipped with multi-resolution supervision for training and\nmulti-resolution aggregation for inference, the proposed approach is able to\nsolve the scale variation challenge in bottom-up multi-person pose estimation\nand localize keypoints more precisely, especially for small person. The feature\npyramid in HigherHRNet consists of feature map outputs from HRNet and upsampled\nhigher-resolution outputs through a transposed convolution. HigherHRNet\noutperforms the previous best bottom-up method by 2.5% AP for medium person on\nCOCO test-dev, showing its effectiveness in handling scale variation.\nFurthermore, HigherHRNet achieves new state-of-the-art result on COCO test-dev\n(70.5% AP) without using refinement or other post-processing techniques,\nsurpassing all existing bottom-up methods. HigherHRNet even surpasses all\ntop-down methods on CrowdPose test (67.6% AP), suggesting its robustness in\ncrowded scene. The code and models are available at\nhttps://github.com/HRNet/Higher-HRNet-Human-Pose-Estimation.\n", "versions": [{"version": "v1", "created": "Tue, 27 Aug 2019 17:54:08 GMT"}, {"version": "v2", "created": "Sun, 24 Nov 2019 05:51:01 GMT"}, {"version": "v3", "created": "Thu, 12 Mar 2020 16:13:53 GMT"}], "update_date": "2020-03-13", "authors_parsed": [["Cheng", "Bowen", ""], ["Xiao", "Bin", ""], ["Wang", "Jingdong", ""], ["Shi", "Honghui", ""], ["Huang", "Thomas S.", ""], ["Zhang", "Lei", ""]]}, {"id": "1908.10359", "submitter": "Xiangping Zhu", "authors": "Xiangping Zhu and Pietro Morerio and Vittorio Murino", "title": "Unsupervised Domain-Adaptive Person Re-identification Based on\n  Attributes", "comments": "5 pages, accepted by ICIP2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pedestrian attributes, e.g., hair length, clothes type and color, locally\ndescribe the semantic appearance of a person. Training person re-identification\n(ReID) algorithms under the supervision of such attributes have proven to be\neffective in extracting local features which are important for ReID. Unlike\nperson identity, attributes are consistent across different domains (or\ndatasets). However, most of ReID datasets lack attribute annotations. On the\nother hand, there are several datasets labeled with sufficient attributes for\nthe case of pedestrian attribute recognition. Exploiting such data for ReID\npurpose can be a way to alleviate the shortage of attribute annotations in ReID\ncase. In this work, an unsupervised domain adaptive ReID feature learning\nframework is proposed to make full use of attribute annotations. We propose to\ntransfer attribute-related features from their original domain to the ReID one:\nto this end, we introduce an adversarial discriminative domain adaptation\nmethod in order to learn domain invariant features for encoding semantic\nattributes. Experiments on three large-scale datasets validate the\neffectiveness of the proposed ReID framework.\n", "versions": [{"version": "v1", "created": "Tue, 27 Aug 2019 17:56:05 GMT"}], "update_date": "2019-08-28", "authors_parsed": [["Zhu", "Xiangping", ""], ["Morerio", "Pietro", ""], ["Murino", "Vittorio", ""]]}, {"id": "1908.10406", "submitter": "Ryan Visee", "authors": "Ryan J. Vis\\'ee, Jirapat Likitlersuang, and Jos\\'e Zariffa", "title": "An Effective and Efficient Method for Detecting Hands in Egocentric\n  Videos for Rehabilitation Applications", "comments": "7 pages, 3 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Objective: Individuals with spinal cord injury (SCI) report upper limb\nfunction as their top recovery priority. To accurately represent the true\nimpact of new interventions on patient function and independence, evaluation\nshould occur in a natural setting. Wearable cameras can be used to monitor hand\nfunction at home, using computer vision to automatically analyze the resulting\nvideos (egocentric video). A key step in this process, hand detection, is\ndifficult to do robustly and reliably, hindering deployment of a complete\nmonitoring system in the home and community. We propose an accurate and\nefficient hand detection method that uses a simple combination of existing\ndetection and tracking algorithms. Methods: Detection, tracking, and\ncombination methods were evaluated on a new hand detection dataset, consisting\nof 167,622 frames of egocentric videos collected on 17 individuals with SCI\nperforming activities of daily living in a home simulation laboratory. Results:\nThe F1-scores for the best detector and tracker alone (SSD and Median Flow)\nwere 0.90$\\pm$0.07 and 0.42$\\pm$0.18, respectively. The best combination\nmethod, in which a detector was used to initialize and reset a tracker,\nresulted in an F1-score of 0.87$\\pm$0.07 while being two times faster than the\nfastest detector alone. Conclusion: The combination of the fastest detector and\nbest tracker improved the accuracy over online trackers while improving the\nspeed of detectors. Significance: The method proposed here, in combination with\nwearable cameras, will help clinicians directly measure hand function in a\npatient's daily life at home, enabling independence after SCI.\n", "versions": [{"version": "v1", "created": "Tue, 27 Aug 2019 18:47:13 GMT"}], "update_date": "2019-08-29", "authors_parsed": [["Vis\u00e9e", "Ryan J.", ""], ["Likitlersuang", "Jirapat", ""], ["Zariffa", "Jos\u00e9", ""]]}, {"id": "1908.10410", "submitter": "Daniel Probst", "authors": "Daniel Probst and Jean-Louis Reymond", "title": "Visualization of Very Large High-Dimensional Data Sets as Minimum\n  Spanning Trees", "comments": "33 pages, 14 figures, 1 table, supplementary information included", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CV cs.DS cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The chemical sciences are producing an unprecedented amount of large,\nhigh-dimensional data sets containing chemical structures and associated\nproperties. However, there are currently no algorithms to visualize such data\nwhile preserving both global and local features with a sufficient level of\ndetail to allow for human inspection and interpretation. Here, we propose a\nsolution to this problem with a new data visualization method, TMAP, capable of\nrepresenting data sets of up to millions of data points and arbitrary high\ndimensionality as a two-dimensional tree (http://tmap.gdb.tools).\nVisualizations based on TMAP are better suited than t-SNE or UMAP for the\nexploration and interpretation of large data sets due to their tree-like\nnature, increased local and global neighborhood and structure preservation, and\nthe transparency of the methods the algorithm is based on. We apply TMAP to the\nmost used chemistry data sets including databases of molecules such as ChEMBL,\nFDB17, the Natural Products Atlas, DSSTox, as well as to the MoleculeNet\nbenchmark collection of data sets. We also show its broad applicability with\nfurther examples from biology, particle physics, and literature.\n", "versions": [{"version": "v1", "created": "Fri, 16 Aug 2019 15:14:19 GMT"}, {"version": "v2", "created": "Fri, 8 Nov 2019 10:43:40 GMT"}, {"version": "v3", "created": "Mon, 6 Jan 2020 14:32:02 GMT"}], "update_date": "2020-01-07", "authors_parsed": [["Probst", "Daniel", ""], ["Reymond", "Jean-Louis", ""]]}, {"id": "1908.10417", "submitter": "Corneliu Arsene Dr", "authors": "Corneliu Arsene", "title": "Complex Deep Learning Models for Denoising of Human Heart ECG signals", "comments": "51 pages, 23 figures", "journal-ref": "EUSIPCO.2019 (Pages 11- 18)", "doi": "10.5281/zenodo.3904247", "report-no": null, "categories": "cs.LG cs.CV eess.SP stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Effective and powerful methods for denoising real electrocardiogram (ECG)\nsignals are important for wearable sensors and devices. Deep Learning (DL)\nmodels have been used extensively in image processing and other domains with\ngreat success but only very recently have been used in processing ECG signals.\nThis paper presents several DL models namely Convolutional Neural Networks\n(CNNs), Long Short-Term Memory (LSTM), Restricted Boltzmann Machine (RBM)\ntogether with the more conventional filtering methods (low pass filtering, high\npass filtering, Notch filtering) and the standard wavelet-based technique for\ndenoising EEG signals. These methods are trained, tested and evaluated on\ndifferent synthetic and real ECG datasets taken from the MIT PhysioNet database\nand for different simulation conditions (i.e. various lengths of the ECG\nsignals, single or multiple records). The results show the CNN model is a\nperformant model that can be used for off-line denoising ECG applications where\nit is satisfactory to train on a clean part of an ECG signal from an ECG\nrecord, and then to test on the same ECG signal, which would have some high\nlevel of noise added to it. However, for real-time applications or near-real\ntime applications, this task becomes more cumbersome, as the clean part of an\nECG signal is very probable to be very limited in size. Therefore the solution\nput forth in this work is to train a CNN model on 1 second ECG noisy artificial\nmultiple heartbeat data (i.e. ECG at effort), which was generated in a first\ninstance based on few sequences of real signal heartbeat ECG data (i.e. ECG at\nrest). Afterwards it would be possible to use the trained CNN model in real\nlife situations to denoise the ECG signal.\n", "versions": [{"version": "v1", "created": "Tue, 27 Aug 2019 19:14:32 GMT"}, {"version": "v2", "created": "Thu, 12 Sep 2019 13:44:56 GMT"}, {"version": "v3", "created": "Tue, 23 Jun 2020 10:14:02 GMT"}], "update_date": "2020-06-24", "authors_parsed": [["Arsene", "Corneliu", ""]]}, {"id": "1908.10425", "submitter": "Steven Morad", "authors": "Steven D. Morad, Jeremy Nash, Shoya Higa, Russell Smith, Aaron\n  Parness, and Kobus Barnard", "title": "Improving Visual Feature Extraction in Glacial Environments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Glacial science could benefit tremendously from autonomous robots, but\nprevious glacial robots have had perception issues in these colorless and\nfeatureless environments, specifically with visual feature extraction. This\ntranslates to failures in visual odometry and visual navigation. Glaciologists\nuse near-infrared imagery to reveal the underlying heterogeneous spatial\nstructure of snow and ice, and we theorize that this hidden near-infrared\nstructure could produce more and higher quality features than available in\nvisible light. We took a custom camera rig to Igloo Cave at Mt. St. Helens to\ntest our theory. The camera rig contains two identical machine vision cameras,\none which was outfitted with multiple filters to see only near-infrared light.\nWe extracted features from short video clips taken inside Igloo Cave at Mt. St.\nHelens, using three popular feature extractors (FAST, SIFT, and SURF). We\nquantified the number of features and their quality for visual navigation by\ncomparing the resulting orientation estimates to ground truth. Our main\ncontribution is the use of NIR longpass filters to improve the quantity and\nquality of visual features in icy terrain, irrespective of the feature\nextractor used.\n", "versions": [{"version": "v1", "created": "Tue, 27 Aug 2019 19:29:34 GMT"}, {"version": "v2", "created": "Sat, 30 Nov 2019 03:18:30 GMT"}], "update_date": "2019-12-03", "authors_parsed": [["Morad", "Steven D.", ""], ["Nash", "Jeremy", ""], ["Higa", "Shoya", ""], ["Smith", "Russell", ""], ["Parness", "Aaron", ""], ["Barnard", "Kobus", ""]]}, {"id": "1908.10454", "submitter": "Nima Tajbakhsh", "authors": "Nima Tajbakhsh, Laura Jeyaseelan, Qian Li, Jeffrey Chiang, Zhihao Wu,\n  Xiaowei Ding", "title": "Embracing Imperfect Datasets: A Review of Deep Learning Solutions for\n  Medical Image Segmentation", "comments": "Accepted for publication in the journal of Medical Image Analysis", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The medical imaging literature has witnessed remarkable progress in\nhigh-performing segmentation models based on convolutional neural networks.\nDespite the new performance highs, the recent advanced segmentation models\nstill require large, representative, and high quality annotated datasets.\nHowever, rarely do we have a perfect training dataset, particularly in the\nfield of medical imaging, where data and annotations are both expensive to\nacquire. Recently, a large body of research has studied the problem of medical\nimage segmentation with imperfect datasets, tackling two major dataset\nlimitations: scarce annotations where only limited annotated data is available\nfor training, and weak annotations where the training data has only sparse\nannotations, noisy annotations, or image-level annotations. In this article, we\nprovide a detailed review of the solutions above, summarizing both the\ntechnical novelties and empirical results. We further compare the benefits and\nrequirements of the surveyed methodologies and provide our recommended\nsolutions. We hope this survey article increases the community awareness of the\ntechniques that are available to handle imperfect medical image segmentation\ndatasets.\n", "versions": [{"version": "v1", "created": "Tue, 27 Aug 2019 20:25:52 GMT"}, {"version": "v2", "created": "Wed, 12 Feb 2020 02:11:18 GMT"}], "update_date": "2020-02-13", "authors_parsed": [["Tajbakhsh", "Nima", ""], ["Jeyaseelan", "Laura", ""], ["Li", "Qian", ""], ["Chiang", "Jeffrey", ""], ["Wu", "Zhihao", ""], ["Ding", "Xiaowei", ""]]}, {"id": "1908.10455", "submitter": "Ehsan Adeli", "authors": "Mohammad Sabokrou, Mohammad Khalooei, Ehsan Adeli", "title": "Self-Supervised Representation Learning via Neighborhood-Relational\n  Encoding", "comments": "Accepted in International Conference on Computer Vision (ICCV) 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a novel self-supervised representation learning by\ntaking advantage of a neighborhood-relational encoding (NRE) among the training\ndata. Conventional unsupervised learning methods only focused on training deep\nnetworks to understand the primitive characteristics of the visual data, mainly\nto be able to reconstruct the data from a latent space. They often neglected\nthe relation among the samples, which can serve as an important metric for\nself-supervision. Different from the previous work, NRE aims at preserving the\nlocal neighborhood structure on the data manifold. Therefore, it is less\nsensitive to outliers. We integrate our NRE component with an encoder-decoder\nstructure for learning to represent samples considering their local\nneighborhood information. Such discriminative and unsupervised representation\nlearning scheme is adaptable to different computer vision tasks due to its\nindependence from intense annotation requirements. We evaluate our proposed\nmethod for different tasks, including classification, detection, and\nsegmentation based on the learned latent representations. In addition, we adopt\nthe auto-encoding capability of our proposed method for applications like\ndefense against adversarial example attacks and video anomaly detection.\nResults confirm the performance of our method is better or at least comparable\nwith the state-of-the-art for each specific application, but with a generic and\nself-supervised approach.\n", "versions": [{"version": "v1", "created": "Tue, 27 Aug 2019 20:26:01 GMT"}], "update_date": "2019-08-29", "authors_parsed": [["Sabokrou", "Mohammad", ""], ["Khalooei", "Mohammad", ""], ["Adeli", "Ehsan", ""]]}, {"id": "1908.10468", "submitter": "Ricardo Bigolin Lanfredi", "authors": "Ricardo Bigolin Lanfredi, Joyce D. Schroeder, Clement Vachet, Tolga\n  Tasdizen", "title": "Adversarial regression training for visualizing the progression of\n  chronic obstructive pulmonary disease with chest x-rays", "comments": "Accepted for MICCAI 2019", "journal-ref": "International Conference on Medical Image Computing and\n  Computer-Assisted Intervention. Springer, Cham, 2019. p. 685-693", "doi": "10.1007/978-3-030-32226-7_76", "report-no": null, "categories": "eess.IV cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Knowledge of what spatial elements of medical images deep learning methods\nuse as evidence is important for model interpretability, trustiness, and\nvalidation. There is a lack of such techniques for models in regression tasks.\nWe propose a method, called visualization for regression with a generative\nadversarial network (VR-GAN), for formulating adversarial training specifically\nfor datasets containing regression target values characterizing disease\nseverity. We use a conditional generative adversarial network where the\ngenerator attempts to learn to shift the output of a regressor through creating\ndisease effect maps that are added to the original images. Meanwhile, the\nregressor is trained to predict the original regression value for the modified\nimages. A model trained with this technique learns to provide visualization for\nhow the image would appear at different stages of the disease. We analyze our\nmethod in a dataset of chest x-rays associated with pulmonary function tests,\nused for diagnosing chronic obstructive pulmonary disease (COPD). For\nvalidation, we compute the difference of two registered x-rays of the same\npatient at different time points and correlate it to the generated disease\neffect map. The proposed method outperforms a technique based on classification\nand provides realistic-looking images, making modifications to images following\nwhat radiologists usually observe for this disease. Implementation code is\navailable at https://github.com/ricbl/vrgan.\n", "versions": [{"version": "v1", "created": "Tue, 27 Aug 2019 21:14:12 GMT"}], "update_date": "2020-07-27", "authors_parsed": [["Lanfredi", "Ricardo Bigolin", ""], ["Schroeder", "Joyce D.", ""], ["Vachet", "Clement", ""], ["Tasdizen", "Tolga", ""]]}, {"id": "1908.10486", "submitter": "Xueping Wang", "authors": "Xueping Wang, Rameswar Panda, Min Liu, Yaonan Wang and Amit K\n  Roy-Chowdhury", "title": "Exploiting Global Camera Network Constraints for Unsupervised Video\n  Person Re-identification", "comments": "This paper has been accepted to IEEE Transactions on Circuits and\n  Systems for Video Technology (T-CSVT)", "journal-ref": null, "doi": "10.1109/TCSVT.2020.3043444", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many unsupervised approaches have been proposed recently for the video-based\nre-identification problem since annotations of samples across cameras are\ntime-consuming. However, higher-order relationships across the entire camera\nnetwork are ignored by these methods, leading to contradictory outputs when\nmatching results from different camera pairs are combined. In this paper, we\naddress the problem of unsupervised video-based re-identification by proposing\na consistent cross-view matching (CCM) framework, in which global camera\nnetwork constraints are exploited to guarantee the matched pairs are with\nconsistency. Specifically, we first propose to utilize the first neighbor of\neach sample to discover relations among samples and find the groups in each\ncamera. Additionally, a cross-view matching strategy followed by global camera\nnetwork constraints is proposed to explore the matching relationships across\nthe entire camera network. Finally, we learn metric models for camera pairs\nprogressively by alternatively mining consistent cross-view matching pairs and\nupdating metric models using these obtained matches. Rigorous experiments on\ntwo widely-used benchmarks for video re-identification demonstrate the\nsuperiority of the proposed method over current state-of-the-art unsupervised\nmethods; for example, on the MARS dataset, our method achieves an improvement\nof 4.2\\% over unsupervised methods, and even 2.5\\% over one-shot\nsupervision-based methods for rank-1 accuracy.\n", "versions": [{"version": "v1", "created": "Tue, 27 Aug 2019 22:35:43 GMT"}, {"version": "v2", "created": "Fri, 31 Jul 2020 03:00:00 GMT"}, {"version": "v3", "created": "Sat, 12 Dec 2020 04:47:42 GMT"}], "update_date": "2020-12-15", "authors_parsed": [["Wang", "Xueping", ""], ["Panda", "Rameswar", ""], ["Liu", "Min", ""], ["Wang", "Yaonan", ""], ["Roy-Chowdhury", "Amit K", ""]]}, {"id": "1908.10489", "submitter": "Junlin Yang", "authors": "Junlin Yang, Nicha C. Dvornek, Fan Zhang, Juntang Zhuang, Julius\n  Chapiro, MingDe Lin, James S. Duncan", "title": "Domain-Agnostic Learning with Anatomy-Consistent Embedding for\n  Cross-Modality Liver Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Domain Adaptation (DA) has the potential to greatly help the generalization\nof deep learning models. However, the current literature usually assumes to\ntransfer the knowledge from the source domain to a specific known target\ndomain. Domain Agnostic Learning (DAL) proposes a new task of transferring\nknowledge from the source domain to data from multiple heterogeneous target\ndomains. In this work, we propose the Domain-Agnostic Learning framework with\nAnatomy-Consistent Embedding (DALACE) that works on both domain-transfer and\ntask-transfer to learn a disentangled representation, aiming to not only be\ninvariant to different modalities but also preserve anatomical structures for\nthe DA and DAL tasks in cross-modality liver segmentation. We validated and\ncompared our model with state-of-the-art methods, including CycleGAN, Task\nDriven Generative Adversarial Network (TD-GAN), and Domain Adaptation via\nDisentangled Representations (DADR). For the DA task, our DALACE model\noutperformed CycleGAN, TD-GAN ,and DADR with DSC of 0.847 compared to 0.721,\n0.793 and 0.806. For the DAL task, our model improved the performance with DSC\nof 0.794 from 0.522, 0.719 and 0.742 by CycleGAN, TD-GAN, and DADR. Further, we\nvisualized the success of disentanglement, which added human interpretability\nof the learned meaningful representations. Through ablation analysis, we\nspecifically showed the concrete benefits of disentanglement for downstream\ntasks and the role of supervision for better disentangled representation with\nsegmentation consistency to be invariant to domains with the proposed\nDomain-Agnostic Module (DAM) and to preserve anatomical information with the\nproposed Anatomy-Preserving Module (APM).\n", "versions": [{"version": "v1", "created": "Tue, 27 Aug 2019 22:44:41 GMT"}], "update_date": "2019-08-29", "authors_parsed": [["Yang", "Junlin", ""], ["Dvornek", "Nicha C.", ""], ["Zhang", "Fan", ""], ["Zhuang", "Juntang", ""], ["Chapiro", "Julius", ""], ["Lin", "MingDe", ""], ["Duncan", "James S.", ""]]}, {"id": "1908.10498", "submitter": "George Kesidis", "authors": "Zhen Xiang, David J. Miller, George Kesidis", "title": "Detection of Backdoors in Trained Classifiers Without Access to the\n  Training Set", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, a special type of data poisoning (DP) attack targeting Deep Neural\nNetwork (DNN) classifiers, known as a backdoor, was proposed. These attacks do\nnot seek to degrade classification accuracy, but rather to have the classifier\nlearn to classify to a target class whenever the backdoor pattern is present in\na test example. Launching backdoor attacks does not require knowledge of the\nclassifier or its training process - it only needs the ability to poison the\ntraining set with (a sufficient number of) exemplars containing a sufficiently\nstrong backdoor pattern (labeled with the target class). Here we address\npost-training detection of backdoor attacks in DNN image classifiers, seldom\nconsidered in existing works, wherein the defender does not have access to the\npoisoned training set, but only to the trained classifier itself, as well as to\nclean examples from the classification domain. This is an important scenario\nbecause a trained classifier may be the basis of e.g. a phone app that will be\nshared with many users. Detecting backdoors post-training may thus reveal a\nwidespread attack. We propose a purely unsupervised anomaly detection (AD)\ndefense against imperceptible backdoor attacks that: i) detects whether the\ntrained DNN has been backdoor-attacked; ii) infers the source and target\nclasses involved in a detected attack; iii) we even demonstrate it is possible\nto accurately estimate the backdoor pattern. We test our AD approach, in\ncomparison with alternative defenses, for several backdoor patterns, data sets,\nand attack settings and demonstrate its favorability. Our defense essentially\nrequires setting a single hyperparameter (the detection threshold), which can\ne.g. be chosen to fix the system's false positive rate.\n", "versions": [{"version": "v1", "created": "Tue, 27 Aug 2019 23:51:43 GMT"}, {"version": "v2", "created": "Wed, 29 Jan 2020 23:57:04 GMT"}, {"version": "v3", "created": "Wed, 19 Aug 2020 15:52:35 GMT"}], "update_date": "2020-08-20", "authors_parsed": [["Xiang", "Zhen", ""], ["Miller", "David J.", ""], ["Kesidis", "George", ""]]}, {"id": "1908.10508", "submitter": "Alex Gaudio", "authors": "Asim Smailagic, Pedro Costa, Alex Gaudio, Kartik Khandelwal, Mostafa\n  Mirshekari, Jonathon Fagert, Devesh Walawalkar, Susu Xu, Adrian Galdran, Pei\n  Zhang, Aur\\'elio Campilho, Hae Young Noh", "title": "O-MedAL: Online Active Deep Learning for Medical Image Analysis", "comments": "Code: https://github.com/adgaudio/o-medal ; Accepted and published by\n  Wiley Journal of Pattern Recognition and Knowledge Discovery ; Journal URL:\n  https://doi.org/10.1002/widm.1353", "journal-ref": "Wiley Interdisciplinary Reviews: Data Mining and Knowledge\n  Discovery 10.4 (2020): e1353", "doi": "10.1002/widm.1353", "report-no": null, "categories": "cs.LG cs.CV eess.IV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Active Learning methods create an optimized labeled training set from\nunlabeled data. We introduce a novel Online Active Deep Learning method for\nMedical Image Analysis. We extend our MedAL active learning framework to\npresent new results in this paper. Our novel sampling method queries the\nunlabeled examples that maximize the average distance to all training set\nexamples. Our online method enhances performance of its underlying baseline\ndeep network. These novelties contribute significant performance improvements,\nincluding improving the model's underlying deep network accuracy by 6.30%,\nusing only 25% of the labeled dataset to achieve baseline accuracy, reducing\nbackpropagated images during training by as much as 67%, and demonstrating\nrobustness to class imbalance in binary and multi-class tasks.\n", "versions": [{"version": "v1", "created": "Wed, 28 Aug 2019 00:48:12 GMT"}, {"version": "v2", "created": "Mon, 27 Jul 2020 20:53:28 GMT"}], "update_date": "2020-07-29", "authors_parsed": [["Smailagic", "Asim", ""], ["Costa", "Pedro", ""], ["Gaudio", "Alex", ""], ["Khandelwal", "Kartik", ""], ["Mirshekari", "Mostafa", ""], ["Fagert", "Jonathon", ""], ["Walawalkar", "Devesh", ""], ["Xu", "Susu", ""], ["Galdran", "Adrian", ""], ["Zhang", "Pei", ""], ["Campilho", "Aur\u00e9lio", ""], ["Noh", "Hae Young", ""]]}, {"id": "1908.10521", "submitter": "Zhao Zhang", "authors": "Yanyan Wei, Zhao Zhang, Haijun Zhang, Richang Hong and Meng Wang", "title": "A Coarse-to-Fine Multi-stream Hybrid Deraining Network for Single Image\n  Deraining", "comments": "Accepted by ICDM 2019 as a regular paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Single image deraining task is still a very challenging task due to its\nill-posed nature in reality. Recently, researchers have tried to fix this issue\nby training the CNN-based end-to-end models, but they still cannot extract the\nnegative rain streaks from rainy images precisely, which usually leads to an\nover de-rained or under de-rained result. To handle this issue, this paper\nproposes a new coarse-to-fine single image deraining framework termed\nMulti-stream Hybrid Deraining Network (shortly, MH-DerainNet). To obtain the\nnegative rain streaks during training process more accurately, we present a new\nmodule named dual path residual dense block, i.e., Residual path and Dense\npath. The Residual path is used to reuse com-mon features from the previous\nlayers while the Dense path can explore new features. In addition, to\nconcatenate different scaled features, we also apply the idea of multi-stream\nwith shortcuts between cascaded dual path residual dense block based streams.\nTo obtain more distinct derained images, we combine the SSIM loss and\nperceptual loss to preserve the per-pixel similarity as well as preserving the\nglobal structures so that the deraining result is more accurate. Extensive\nexperi-ments on both synthetic and real rainy images demonstrate that our\nMH-DerainNet can deliver significant improvements over several recent\nstate-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Wed, 28 Aug 2019 02:05:36 GMT"}], "update_date": "2019-08-29", "authors_parsed": [["Wei", "Yanyan", ""], ["Zhang", "Zhao", ""], ["Zhang", "Haijun", ""], ["Hong", "Richang", ""], ["Wang", "Meng", ""]]}, {"id": "1908.10526", "submitter": "Wenyan Cong", "authors": "Wenyan Cong, Jianfu Zhang, Li Niu, Liu Liu, Zhixin Ling, Weiyuan Li,\n  Liqing Zhang", "title": "Image Harmonization Dataset iHarmony4: HCOCO, HAdobe5k, HFlickr, and\n  Hday2night", "comments": "Our full paper arXiv:1911.13239 \"DoveNet: Deep Image Harmonization\n  via Domain Verification\" is accepted by CVPR2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image composition is an important operation in image processing, but the\ninconsistency between foreground and background significantly degrades the\nquality of composite image. Image harmonization, which aims to make the\nforeground compatible with the background, is a promising yet challenging task.\nHowever, the lack of high-quality public dataset for image harmonization, which\nsignificantly hinders the development of image harmonization techniques.\nTherefore, we contribute an image harmonization dataset iHarmony4 by generating\nsynthesized composite images based on existing COCO (resp., Adobe5k, day2night)\ndataset, leading to our HCOCO (resp., HAdobe5k, Hday2night) sub-dataset. To\nenrich the diversity of our dataset, we also generate synthesized composite\nimages based on our collected Flick images, leading to our HFlickr sub-dataset.\nThe image harmonization dataset iHarmony4 is released at\nhttps://github.com/bcmi/Image_Harmonization_Datasets.\n", "versions": [{"version": "v1", "created": "Wed, 28 Aug 2019 02:44:13 GMT"}, {"version": "v2", "created": "Sun, 24 Nov 2019 16:27:15 GMT"}, {"version": "v3", "created": "Wed, 27 Nov 2019 07:51:24 GMT"}, {"version": "v4", "created": "Fri, 20 Mar 2020 12:36:30 GMT"}], "update_date": "2020-03-23", "authors_parsed": [["Cong", "Wenyan", ""], ["Zhang", "Jianfu", ""], ["Niu", "Li", ""], ["Liu", "Liu", ""], ["Ling", "Zhixin", ""], ["Li", "Weiyuan", ""], ["Zhang", "Liqing", ""]]}, {"id": "1908.10534", "submitter": "Nikolaos Sarafianos", "authors": "Nikolaos Sarafianos, Xiang Xu, Ioannis A. Kakadiaris", "title": "Adversarial Representation Learning for Text-to-Image Matching", "comments": "To appear in ICCV 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For many computer vision applications such as image captioning, visual\nquestion answering, and person search, learning discriminative feature\nrepresentations at both image and text level is an essential yet challenging\nproblem. Its challenges originate from the large word variance in the text\ndomain as well as the difficulty of accurately measuring the distance between\nthe features of the two modalities. Most prior work focuses on the latter\nchallenge, by introducing loss functions that help the network learn better\nfeature representations but fail to account for the complexity of the textual\ninput. With that in mind, we introduce TIMAM: a Text-Image Modality Adversarial\nMatching approach that learns modality-invariant feature representations using\nadversarial and cross-modal matching objectives. In addition, we demonstrate\nthat BERT, a publicly-available language model that extracts word embeddings,\ncan successfully be applied in the text-to-image matching domain. The proposed\napproach achieves state-of-the-art cross-modal matching performance on four\nwidely-used publicly-available datasets resulting in absolute improvements\nranging from 2% to 5% in terms of rank-1 accuracy.\n", "versions": [{"version": "v1", "created": "Wed, 28 Aug 2019 03:25:13 GMT"}], "update_date": "2019-08-29", "authors_parsed": [["Sarafianos", "Nikolaos", ""], ["Xu", "Xiang", ""], ["Kakadiaris", "Ioannis A.", ""]]}, {"id": "1908.10535", "submitter": "Wenjie Pei", "authors": "Weinong Wang, Wenjie Pei, Qiong Cao, Shu Liu, Yu-Wing Tai", "title": "Push for Center Learning via Orthogonalization and Subspace Masking for\n  Person Re-Identification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Person re-identification aims to identify whether pairs of images belong to\nthe same person or not. This problem is challenging due to large differences in\ncamera views, lighting and background. One of the mainstream in learning CNN\nfeatures is to design loss functions which reinforce both the class separation\nand intra-class compactness. In this paper, we propose a novel Orthogonal\nCenter Learning method with Subspace Masking for person re-identification. We\nmake the following contributions: (i) we develop a center learning module to\nlearn the class centers by simultaneously reducing the intra-class differences\nand inter-class correlations by orthogonalization; (ii) we introduce a subspace\nmasking mechanism to enhance the generalization of the learned class centers;\nand (iii) we devise to integrate the average pooling and max pooling in a\nregularizing manner that fully exploits their powers. Extensive experiments\nshow that our proposed method consistently outperforms the state-of-the-art\nmethods on the large-scale ReID datasets including Market-1501, DukeMTMC-ReID,\nCUHK03 and MSMT17.\n", "versions": [{"version": "v1", "created": "Wed, 28 Aug 2019 03:32:57 GMT"}, {"version": "v2", "created": "Wed, 11 Dec 2019 05:56:37 GMT"}], "update_date": "2019-12-12", "authors_parsed": [["Wang", "Weinong", ""], ["Pei", "Wenjie", ""], ["Cao", "Qiong", ""], ["Liu", "Shu", ""], ["Tai", "Yu-Wing", ""]]}, {"id": "1908.10543", "submitter": "Juhong Min", "authors": "Juhong Min, Jongmin Lee, Jean Ponce, Minsu Cho", "title": "SPair-71k: A Large-scale Benchmark for Semantic Correspondence", "comments": "Extension of ICCV 2019 paper, Hyperpixel Flow: Semantic\n  Correspondence with Multi-layer Neural Features. arXiv admin note: text\n  overlap with arXiv:1908.06537", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Establishing visual correspondences under large intra-class variations, which\nis often referred to as semantic correspondence or semantic matching, remains a\nchallenging problem in computer vision. Despite its significance, however, most\nof the datasets for semantic correspondence are limited to a small amount of\nimage pairs with similar viewpoints and scales. In this paper, we present a new\nlarge-scale benchmark dataset of semantically paired images, SPair-71k, which\ncontains 70,958 image pairs with diverse variations in viewpoint and scale.\nCompared to previous datasets, it is significantly larger in number and\ncontains more accurate and richer annotations. We believe this dataset will\nprovide a reliable testbed to study the problem of semantic correspondence and\nwill help to advance research in this area. We provide the results of recent\nmethods on our new dataset as baselines for further research. Our benchmark is\navailable online at http://cvlab.postech.ac.kr/research/SPair-71k/.\n", "versions": [{"version": "v1", "created": "Wed, 28 Aug 2019 04:16:52 GMT"}], "update_date": "2019-08-29", "authors_parsed": [["Min", "Juhong", ""], ["Lee", "Jongmin", ""], ["Ponce", "Jean", ""], ["Cho", "Minsu", ""]]}, {"id": "1908.10546", "submitter": "Bowen Shi", "authors": "Bowen Shi, Aurora Martinez Del Rio, Jonathan Keane, Diane Brentari,\n  Greg Shakhnarovich, Karen Livescu", "title": "Fingerspelling recognition in the wild with iterative visual attention", "comments": "ICCV 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sign language recognition is a challenging gesture sequence recognition\nproblem, characterized by quick and highly coarticulated motion. In this paper\nwe focus on recognition of fingerspelling sequences in American Sign Language\n(ASL) videos collected in the wild, mainly from YouTube and Deaf social media.\nMost previous work on sign language recognition has focused on controlled\nsettings where the data is recorded in a studio environment and the number of\nsigners is limited. Our work aims to address the challenges of real-life data,\nreducing the need for detection or segmentation modules commonly used in this\ndomain. We propose an end-to-end model based on an iterative attention\nmechanism, without explicit hand detection or segmentation. Our approach\ndynamically focuses on increasingly high-resolution regions of interest. It\noutperforms prior work by a large margin. We also introduce a newly collected\ndata set of crowdsourced annotations of fingerspelling in the wild, and show\nthat performance can be further improved with this additional data set.\n", "versions": [{"version": "v1", "created": "Wed, 28 Aug 2019 04:52:32 GMT"}], "update_date": "2019-08-29", "authors_parsed": [["Shi", "Bowen", ""], ["Del Rio", "Aurora Martinez", ""], ["Keane", "Jonathan", ""], ["Brentari", "Diane", ""], ["Shakhnarovich", "Greg", ""], ["Livescu", "Karen", ""]]}, {"id": "1908.10548", "submitter": "Sumin Lee", "authors": "Sumin Lee (1), Sungchan Oh (2), Chanho Jung (3), and Changick Kim (1)\n  ((1) Korea Advanced Institute of Science and Technology (KAIST), (2)\n  Electronics and Telecommunications Research Institute (ETRI), (3) Hanbat\n  National University)", "title": "A Global-Local Emebdding Module for Fashion Landmark Detection", "comments": "Accepted to ICCV 2019 Workshop: Computer Vision for Fashion, Art and\n  Design", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Detecting fashion landmarks is a fundamental technique for visual clothing\nanalysis. Due to the large variation and non-rigid deformation of clothes,\nlocalizing fashion landmarks suffers from large spatial variances across poses,\nscales, and styles. Therefore, understanding contextual knowledge of clothes is\nrequired for accurate landmark detection. To that end, in this paper, we\npropose a fashion landmark detection network with a global-local embedding\nmodule. The global-local embedding module is based on a non-local operation for\ncapturing long-range dependencies and a subsequent convolution operation for\nadopting local neighborhood relations. With this processing, the network can\nconsider both global and local contextual knowledge for a clothing image. We\ndemonstrate that our proposed method has an excellent ability to learn advanced\ndeep feature representations for fashion landmark detection. Experimental\nresults on two benchmark datasets show that the proposed network outperforms\nthe state-of-the-art methods. Our code is available at\nhttps://github.com/shumming/GLE_FLD.\n", "versions": [{"version": "v1", "created": "Wed, 28 Aug 2019 04:57:12 GMT"}], "update_date": "2019-08-29", "authors_parsed": [["Lee", "Sumin", ""], ["Oh", "Sungchan", ""], ["Jung", "Chanho", ""], ["Kim", "Changick", ""]]}, {"id": "1908.10552", "submitter": "Yuan Yao", "authors": "Yuan Yao, Yu Zhang, Xutao Li, Yunming Ye", "title": "Heterogeneous Domain Adaptation via Soft Transfer Network", "comments": "Accepted by ACM Multimedia (ACM MM) 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Heterogeneous domain adaptation (HDA) aims to facilitate the learning task in\na target domain by borrowing knowledge from a heterogeneous source domain. In\nthis paper, we propose a Soft Transfer Network (STN), which jointly learns a\ndomain-shared classifier and a domain-invariant subspace in an end-to-end\nmanner, for addressing the HDA problem. The proposed STN not only aligns the\ndiscriminative directions of domains but also matches both the marginal and\nconditional distributions across domains. To circumvent negative transfer, STN\naligns the conditional distributions by using the soft-label strategy of\nunlabeled target data, which prevents the hard assignment of each unlabeled\ntarget data to only one category that may be incorrect. Further, STN introduces\nan adaptive coefficient to gradually increase the importance of the soft-labels\nsince they will become more and more accurate as the number of iterations\nincreases. We perform experiments on the transfer tasks of image-to-image,\ntext-to-image, and text-to-text. Experimental results testify that the STN\nsignificantly outperforms several state-of-the-art approaches.\n", "versions": [{"version": "v1", "created": "Wed, 28 Aug 2019 05:25:12 GMT"}], "update_date": "2019-08-29", "authors_parsed": [["Yao", "Yuan", ""], ["Zhang", "Yu", ""], ["Li", "Xutao", ""], ["Ye", "Yunming", ""]]}, {"id": "1908.10553", "submitter": "Jiawang Bian", "authors": "Jia-Wang Bian, Zhichao Li, Naiyan Wang, Huangying Zhan, Chunhua Shen,\n  Ming-Ming Cheng, Ian Reid", "title": "Unsupervised Scale-consistent Depth and Ego-motion Learning from\n  Monocular Video", "comments": "Accepted to NeurIPS 2019. Code is available at\n  https://github.com/JiawangBian/SC-SfMLearner-Release", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent work has shown that CNN-based depth and ego-motion estimators can be\nlearned using unlabelled monocular videos. However, the performance is limited\nby unidentified moving objects that violate the underlying static scene\nassumption in geometric image reconstruction. More significantly, due to lack\nof proper constraints, networks output scale-inconsistent results over\ndifferent samples, i.e., the ego-motion network cannot provide full camera\ntrajectories over a long video sequence because of the per-frame scale\nambiguity. This paper tackles these challenges by proposing a geometry\nconsistency loss for scale-consistent predictions and an induced\nself-discovered mask for handling moving objects and occlusions. Since we do\nnot leverage multi-task learning like recent works, our framework is much\nsimpler and more efficient. Comprehensive evaluation results demonstrate that\nour depth estimator achieves the state-of-the-art performance on the KITTI\ndataset. Moreover, we show that our ego-motion network is able to predict a\nglobally scale-consistent camera trajectory for long video sequences, and the\nresulting visual odometry accuracy is competitive with the recent model that is\ntrained using stereo videos. To the best of our knowledge, this is the first\nwork to show that deep networks trained using unlabelled monocular videos can\npredict globally scale-consistent camera trajectories over a long video\nsequence.\n", "versions": [{"version": "v1", "created": "Wed, 28 Aug 2019 05:25:46 GMT"}, {"version": "v2", "created": "Thu, 3 Oct 2019 00:34:14 GMT"}], "update_date": "2019-10-04", "authors_parsed": [["Bian", "Jia-Wang", ""], ["Li", "Zhichao", ""], ["Wang", "Naiyan", ""], ["Zhan", "Huangying", ""], ["Shen", "Chunhua", ""], ["Cheng", "Ming-Ming", ""], ["Reid", "Ian", ""]]}, {"id": "1908.10555", "submitter": "Shuhao Wang", "authors": "Gang Xu, Zhigang Song, Zhuo Sun, Calvin Ku, Zhe Yang, Cancheng Liu,\n  Shuhao Wang, Jianpeng Ma, Wei Xu", "title": "CAMEL: A Weakly Supervised Learning Framework for Histopathology Image\n  Segmentation", "comments": "10 pages, 9 figures, accepted by ICCV 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Histopathology image analysis plays a critical role in cancer diagnosis and\ntreatment. To automatically segment the cancerous regions, fully supervised\nsegmentation algorithms require labor-intensive and time-consuming labeling at\nthe pixel level. In this research, we propose CAMEL, a weakly supervised\nlearning framework for histopathology image segmentation using only image-level\nlabels. Using multiple instance learning (MIL)-based label enrichment, CAMEL\nsplits the image into latticed instances and automatically generates\ninstance-level labels. After label enrichment, the instance-level labels are\nfurther assigned to the corresponding pixels, producing the approximate\npixel-level labels and making fully supervised training of segmentation models\npossible. CAMEL achieves comparable performance with the fully supervised\napproaches in both instance-level classification and pixel-level segmentation\non CAMELYON16 and a colorectal adenoma dataset. Moreover, the generality of the\nautomatic labeling methodology may benefit future weakly supervised learning\nstudies for histopathology image analysis.\n", "versions": [{"version": "v1", "created": "Wed, 28 Aug 2019 05:32:07 GMT"}], "update_date": "2019-08-29", "authors_parsed": [["Xu", "Gang", ""], ["Song", "Zhigang", ""], ["Sun", "Zhuo", ""], ["Ku", "Calvin", ""], ["Yang", "Zhe", ""], ["Liu", "Cancheng", ""], ["Wang", "Shuhao", ""], ["Ma", "Jianpeng", ""], ["Xu", "Wei", ""]]}, {"id": "1908.10559", "submitter": "Saurabh Kumar", "authors": "Saurabh Kumar, Biplab Banerjee, Subhasis Chaudhuri", "title": "Online Sensor Hallucination via Knowledge Distillation for Multimodal\n  Image Classification", "comments": "Preprint: Manuscript under revision", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We deal with the problem of information fusion driven satellite image/scene\nclassification and propose a generic hallucination architecture considering\nthat all the available sensor information are present during training while\nsome of the image modalities may be absent while testing. It is well-known that\ndifferent sensors are capable of capturing complementary information for a\ngiven geographical area and a classification module incorporating information\nfrom all the sources are expected to produce an improved performance as\ncompared to considering only a subset of the modalities. However, the classical\nclassifier systems inherently require all the features used to train the module\nto be present for the test instances as well, which may not always be possible\nfor typical remote sensing applications (say, disaster management). As a\nremedy, we provide a robust solution in terms of a hallucination module that\ncan approximate the missing modalities from the available ones during the\ndecision-making stage. In order to ensure better knowledge transfer during\nmodality hallucination, we explicitly incorporate concepts of knowledge\ndistillation for the purpose of exploring the privileged (side) information in\nour framework and subsequently introduce an intuitive modular training\napproach. The proposed network is evaluated extensively on a large-scale corpus\nof PAN-MS image pairs (scene recognition) as well as on a benchmark\nhyperspectral image dataset (image classification) where we follow different\nexperimental scenarios and find that the proposed hallucination based module\nindeed is capable of capturing the multi-source information, albeit the\nexplicit absence of some of the sensor information, and aid in improved scene\ncharacterization.\n", "versions": [{"version": "v1", "created": "Wed, 28 Aug 2019 05:55:09 GMT"}], "update_date": "2019-08-29", "authors_parsed": [["Kumar", "Saurabh", ""], ["Banerjee", "Biplab", ""], ["Chaudhuri", "Subhasis", ""]]}, {"id": "1908.10568", "submitter": "Xuejing Liu", "authors": "Xuejing Liu, Liang Li, Shuhui Wang, Zheng-Jun Zha, Dechao Meng, and\n  Qingming Huang", "title": "Adaptive Reconstruction Network for Weakly Supervised Referring\n  Expression Grounding", "comments": "Accepted by ICCV 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Weakly supervised referring expression grounding aims at localizing the\nreferential object in an image according to the linguistic query, where the\nmapping between the referential object and query is unknown in the training\nstage. To address this problem, we propose a novel end-to-end adaptive\nreconstruction network (ARN). It builds the correspondence between image region\nproposal and query in an adaptive manner: adaptive grounding and collaborative\nreconstruction. Specifically, we first extract the subject, location and\ncontext features to represent the proposals and the query respectively. Then,\nwe design the adaptive grounding module to compute the matching score between\neach proposal and query by a hierarchical attention model. Finally, based on\nattention score and proposal features, we reconstruct the input query with a\ncollaborative loss of language reconstruction loss, adaptive reconstruction\nloss, and attribute classification loss. This adaptive mechanism helps our\nmodel to alleviate the variance of different referring expressions. Experiments\non four large-scale datasets show ARN outperforms existing state-of-the-art\nmethods by a large margin. Qualitative results demonstrate that the proposed\nARN can better handle the situation where multiple objects of a particular\ncategory situated together.\n", "versions": [{"version": "v1", "created": "Wed, 28 Aug 2019 06:49:54 GMT"}], "update_date": "2019-08-29", "authors_parsed": [["Liu", "Xuejing", ""], ["Li", "Liang", ""], ["Wang", "Shuhui", ""], ["Zha", "Zheng-Jun", ""], ["Meng", "Dechao", ""], ["Huang", "Qingming", ""]]}, {"id": "1908.10578", "submitter": "Sarah Alotaibi", "authors": "Sarah Alotaibi and William Smith", "title": "BioFaceNet: Deep Biophysical Face Image Interpretation", "comments": "Accepted to BMVC2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present BioFaceNet, a deep CNN that learns to decompose a\nsingle face image into biophysical parameters maps, diffuse and specular\nshading maps as well as estimating the spectral power distribution of the scene\nilluminant and the spectral sensitivity of the camera. The network comprises a\nfully convolutional encoder for estimating the spatial maps with a fully\nconnected branch for estimating the vector quantities. The network is trained\nusing a self-supervised appearance loss computed via a model-based decoder. The\ntask is highly underconstrained so we impose a number of model-based priors.\nSkin spectral reflectance is restricted to a biophysical model, we impose a\nstatistical prior on camera spectral sensitivities, a physical constraint on\nillumination spectra, a sparsity prior on specular reflections and direct\nsupervision on diffuse shading using a rough shape proxy. We show convincing\nqualitative results on in-the-wild data and introduce a benchmark for\nquantitative evaluation on this new task.\n", "versions": [{"version": "v1", "created": "Wed, 28 Aug 2019 07:28:48 GMT"}, {"version": "v2", "created": "Fri, 13 Sep 2019 16:32:12 GMT"}], "update_date": "2019-09-16", "authors_parsed": [["Alotaibi", "Sarah", ""], ["Smith", "William", ""]]}, {"id": "1908.10585", "submitter": "Katrien Laenen", "authors": "Katrien Laenen and Marie-Francine Moens", "title": "Attention-based Fusion for Outfit Recommendation", "comments": "6 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  This paper describes an attention-based fusion method for outfit\nrecommendation which fuses the information in the product image and description\nto capture the most important, fine-grained product features into the item\nrepresentation. We experiment with different kinds of attention mechanisms and\ndemonstrate that the attention-based fusion improves item understanding. We\noutperform state-of-the-art outfit recommendation results on three benchmark\ndatasets.\n", "versions": [{"version": "v1", "created": "Wed, 28 Aug 2019 07:41:53 GMT"}], "update_date": "2019-08-29", "authors_parsed": [["Laenen", "Katrien", ""], ["Moens", "Marie-Francine", ""]]}, {"id": "1908.10638", "submitter": "Aitor Alvarez-Gila", "authors": "Aitor Alvarez-Gila, Adrian Galdran, Estibaliz Garrote, Joost van de\n  Weijer", "title": "Self-supervised blur detection from synthetically blurred scenes", "comments": "Image and Vision Computing", "journal-ref": null, "doi": "10.1016/j.imavis.2019.08.008", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Blur detection aims at segmenting the blurred areas of a given image. Recent\ndeep learning-based methods approach this problem by learning an end-to-end\nmapping between the blurred input and a binary mask representing the\nlocalization of its blurred areas. Nevertheless, the effectiveness of such deep\nmodels is limited due to the scarcity of datasets annotated in terms of blur\nsegmentation, as blur annotation is labour intensive. In this work, we bypass\nthe need for such annotated datasets for end-to-end learning, and instead rely\non object proposals and a model for blur generation in order to produce a\ndataset of synthetically blurred images. This allows us to perform\nself-supervised learning over the generated image and ground truth blur mask\npairs using CNNs, defining a framework that can be employed in purely\nself-supervised, weakly supervised or semi-supervised configurations.\nInterestingly, experimental results of such setups over the largest blur\nsegmentation datasets available show that this approach achieves state of the\nart results in blur segmentation, even without ever observing any real blurred\nimage.\n", "versions": [{"version": "v1", "created": "Wed, 28 Aug 2019 10:58:55 GMT"}], "update_date": "2019-08-29", "authors_parsed": [["Alvarez-Gila", "Aitor", ""], ["Galdran", "Adrian", ""], ["Garrote", "Estibaliz", ""], ["van de Weijer", "Joost", ""]]}, {"id": "1908.10654", "submitter": "Shifeng Zhang", "authors": "Shifeng Zhang, Ajian Liu, Jun Wan, Yanyan Liang, Guogong Guo, Sergio\n  Escalera, Hugo Jair Escalante, Stan Z. Li", "title": "CASIA-SURF: A Large-scale Multi-modal Benchmark for Face Anti-spoofing", "comments": "Accepted by TBIOM; Journal extension of our previous conference\n  paper: arXiv:1812.00408", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Face anti-spoofing is essential to prevent face recognition systems from a\nsecurity breach. Much of the progresses have been made by the availability of\nface anti-spoofing benchmark datasets in recent years. However, existing face\nanti-spoofing benchmarks have limited number of subjects ($\\le\\negmedspace170$)\nand modalities ($\\leq\\negmedspace2$), which hinder the further development of\nthe academic community. To facilitate face anti-spoofing research, we introduce\na large-scale multi-modal dataset, namely CASIA-SURF, which is the largest\npublicly available dataset for face anti-spoofing in terms of both subjects and\nmodalities. Specifically, it consists of $1,000$ subjects with $21,000$ videos\nand each sample has $3$ modalities (i.e., RGB, Depth and IR). We also provide\ncomprehensive evaluation metrics, diverse evaluation protocols,\ntraining/validation/testing subsets and a measurement tool, developing a new\nbenchmark for face anti-spoofing. Moreover, we present a novel multi-modal\nmulti-scale fusion method as a strong baseline, which performs feature\nre-weighting to select the more informative channel features while suppressing\nthe less useful ones for each modality across different scales. Extensive\nexperiments have been conducted on the proposed dataset to verify its\nsignificance and generalization capability. The dataset is available at\nhttps://sites.google.com/qq.com/face-anti-spoofing/welcome/challengecvpr2019?authuser=0\n", "versions": [{"version": "v1", "created": "Wed, 28 Aug 2019 11:40:51 GMT"}, {"version": "v2", "created": "Tue, 4 Feb 2020 17:58:24 GMT"}], "update_date": "2020-02-05", "authors_parsed": [["Zhang", "Shifeng", ""], ["Liu", "Ajian", ""], ["Wan", "Jun", ""], ["Liang", "Yanyan", ""], ["Guo", "Guogong", ""], ["Escalera", "Sergio", ""], ["Escalante", "Hugo Jair", ""], ["Li", "Stan Z.", ""]]}, {"id": "1908.10661", "submitter": "Waleed Yousef", "authors": "Waleed A. Yousef, Ahmed A. Abouelkahire, Deyaaeldeen Almahallawi, Omar\n  S.Marzouk, Sameh K. Mohamed, Waleed A. Mustafa, Omar M. Osama, Ali A. Saleh,\n  Naglaa M. Abdelrazek", "title": "Method and System for Image Analysis to Detect Cancer", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Breast cancer is the most common cancer and is the leading cause of cancer\ndeath among women worldwide. Detection of breast cancer, while it is still\nsmall and confined to the breast, provides the best chance of effective\ntreatment. Computer Aided Detection (CAD) systems that detect cancer from\nmammograms will help in reducing the human errors that lead to missing breast\ncarcinoma. Literature is rich of scientific papers for methods of CAD design,\nyet with no complete system architecture to deploy those methods. On the other\nhand, commercial CADs are developed and deployed only to vendors' mammography\nmachines with no availability to public access. This paper presents a complete\nCAD; it is complete since it combines, on a hand, the rigor of algorithm design\nand assessment (method), and, on the other hand, the implementation and\ndeployment of a system architecture for public accessibility (system). (1) We\ndevelop a novel algorithm for image enhancement so that mammograms acquired\nfrom any digital mammography machine look qualitatively of the same clarity to\nradiologists' inspection; and is quantitatively standardized for the detection\nalgorithms. (2) We develop novel algorithms for masses and microcalcifications\ndetection with accuracy superior to both literature results and the majority of\napproved commercial systems. (3) We design, implement, and deploy a system\narchitecture that is computationally effective to allow for deploying these\nalgorithms to cloud for public access.\n", "versions": [{"version": "v1", "created": "Mon, 26 Aug 2019 22:28:47 GMT"}], "update_date": "2019-08-29", "authors_parsed": [["Yousef", "Waleed A.", ""], ["Abouelkahire", "Ahmed A.", ""], ["Almahallawi", "Deyaaeldeen", ""], ["Marzouk", "Omar S.", ""], ["Mohamed", "Sameh K.", ""], ["Mustafa", "Waleed A.", ""], ["Osama", "Omar M.", ""], ["Saleh", "Ali A.", ""], ["Abdelrazek", "Naglaa M.", ""]]}, {"id": "1908.10700", "submitter": "Tao Zhuo", "authors": "Tao Zhuo, Zhiyong Cheng, Peng Zhang, Yongkang Wong, Mohan Kankanhalli", "title": "Explainable Video Action Reasoning via Prior Knowledge and State\n  Transitions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human action analysis and understanding in videos is an important and\nchallenging task. Although substantial progress has been made in past years,\nthe explainability of existing methods is still limited. In this work, we\npropose a novel action reasoning framework that uses prior knowledge to explain\nsemantic-level observations of video state changes. Our method takes advantage\nof both classical reasoning and modern deep learning approaches. Specifically,\nprior knowledge is defined as the information of a target video domain,\nincluding a set of objects, attributes and relationships in the target video\ndomain, as well as relevant actions defined by the temporal attribute and\nrelationship changes (i.e. state transitions). Given a video sequence, we first\ngenerate a scene graph on each frame to represent concerned objects, attributes\nand relationships. Then those scene graphs are linked by tracking objects\nacross frames to form a spatio-temporal graph (also called video graph), which\nrepresents semantic-level video states. Finally, by sequentially examining each\nstate transition in the video graph, our method can detect and explain how\nthose actions are executed with prior knowledge, just like the logical manner\nof thinking by humans. Compared to previous works, the action reasoning results\nof our method can be explained by both logical rules and semantic-level\nobservations of video content changes. Besides, the proposed method can be used\nto detect multiple concurrent actions with detailed information, such as who\n(particular objects), when (time), where (object locations) and how (what kind\nof changes). Experiments on a re-annotated dataset CAD-120 show the\neffectiveness of our method.\n", "versions": [{"version": "v1", "created": "Wed, 28 Aug 2019 13:04:28 GMT"}], "update_date": "2019-08-29", "authors_parsed": [["Zhuo", "Tao", ""], ["Cheng", "Zhiyong", ""], ["Zhang", "Peng", ""], ["Wong", "Yongkang", ""], ["Kankanhalli", "Mohan", ""]]}, {"id": "1908.10701", "submitter": "Vijay Anand", "authors": "Vijay Anand and Vivek Kanhangad", "title": "Unsupervised Domain Adaptation for Cross-sensor Pore Detection in\n  High-resolution Fingerprint Images", "comments": "9 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the emergence of high-resolution fingerprint sensors, there has been a\nlot of focus on level-3 fingerprint features, especially the pores, for the\nnext generation automated fingerprint recognition systems (AFRS). Following the\nsuccess of deep learning in various computer vision tasks, researchers have\ndeveloped learning-based approaches for detection of pores in high-resolution\nfingerprint images. Generally, learning-based approaches provide better\nperformance than handcrafted feature-based approaches. However, domain\nadaptability of the existing learning-based pore detection methods has never\nbeen studied. In this paper, we study this aspect and propose an approach for\npore detection in cross-sensor scenarios. For this purpose, we have generated\nan in-house 1000 dpi fingerprint dataset with ground truth pore coordinates\n(referred to as IITI-HRFP-GT), and evaluated the performance of the existing\nlearning-based pore detection approaches. The core of the proposed approach for\ndetection of pores in cross-sensor scenarios is DeepDomainPore, which is a\nresidual learning-based convolutional neural network(CNN) trained for pore\ndetection. The domain adaptability in DeepDomainPore is achieved by embedding a\ngradient reversal layer between the CNN and a domain classifier network. The\nproposed approach achieves state-of-the-art performance in a cross-sensor\nscenario involving public high-resolution fingerprint datasets with 88.12% true\ndetection rate and 83.82% F-score.\n", "versions": [{"version": "v1", "created": "Wed, 28 Aug 2019 13:05:03 GMT"}, {"version": "v2", "created": "Tue, 28 Jan 2020 07:07:44 GMT"}], "update_date": "2020-01-29", "authors_parsed": [["Anand", "Vijay", ""], ["Kanhangad", "Vivek", ""]]}, {"id": "1908.10717", "submitter": "Tao Zhuo", "authors": "Tao Zhuo, Zhiyong Cheng, Mohan Kankanhalli", "title": "Fast Video Object Segmentation via Mask Transfer Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accuracy and processing speed are two important factors that affect the use\nof video object segmentation (VOS) in real applications. With the advanced\ntechniques of deep neural networks, the accuracy has been significantly\nimproved, however, the speed is still far below the real-time needs because of\nthe complicated network design, such as the requirement of the first frame\nfine-tuning step. To overcome this limitation, we propose a novel mask transfer\nnetwork (MTN), which can greatly boost the processing speed of VOS and also\nachieve a reasonable accuracy. The basic idea of MTN is to transfer the\nreference mask to the target frame via an efficient global pixel matching\nstrategy. The global pixel matching between the reference frame and the target\nframe is to ensure good matching results. To enhance the matching speed, we\nperform the matching on a downsampled feature map with 1/32 of the original\nframe size. At the same time, to preserve the detailed mask information in such\na small feature map, a mask network is designed to encode the annotated mask\ninformation with 512 channels. Finally, an efficient feature warping method is\nused to transfer the encoded reference mask to the target frame. Based on this\ndesign, our method avoids the fine-tuning step on the first frame and does not\nrely on the temporal cues and particular object categories. Therefore, it runs\nvery fast and can be conveniently trained only with images, as well as being\nrobust to unseen objects. Experiments on the DAVIS datasets demonstrate that\nMTN can achieve a speed of 37 fps, and also shows a competitive accuracy in\ncomparison to the state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Wed, 28 Aug 2019 13:31:34 GMT"}], "update_date": "2019-08-29", "authors_parsed": [["Zhuo", "Tao", ""], ["Cheng", "Zhiyong", ""], ["Kankanhalli", "Mohan", ""]]}, {"id": "1908.10737", "submitter": "Shichao Li", "authors": "Shichao Li, Kwang-Ting Cheng", "title": "Facial age estimation by deep residual decision making", "comments": "Following-up work for visualizing deep neural decision forest for\n  facial age estimation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Residual representation learning simplifies the optimization problem of\nlearning complex functions and has been widely used by traditional\nconvolutional neural networks. However, it has not been applied to deep neural\ndecision forest (NDF). In this paper we incorporate residual learning into NDF\nand the resulting model achieves state-of-the-art level accuracy on three\npublic age estimation benchmarks while requiring less memory and computation.\nWe further employ gradient-based technique to visualize the decision-making\nprocess of NDF and understand how it is influenced by facial image inputs. The\ncode and pre-trained models will be available at\nhttps://github.com/Nicholasli1995/VisualizingNDF.\n", "versions": [{"version": "v1", "created": "Wed, 28 Aug 2019 14:12:04 GMT"}], "update_date": "2019-08-29", "authors_parsed": [["Li", "Shichao", ""], ["Cheng", "Kwang-Ting", ""]]}, {"id": "1908.10769", "submitter": "Frank Zijlstra", "authors": "Frank Zijlstra, Max A. Viergever, and Peter R. Seevinck", "title": "SMART tracking: Simultaneous anatomical imaging and real-time passive\n  device tracking for MR-guided interventions", "comments": null, "journal-ref": "Physica Medica 64 (2019): 252-260", "doi": "10.1016/j.ejmp.2019.07.019", "report-no": null, "categories": "physics.med-ph cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Purpose: This study demonstrates a proof of concept of a method for\nsimultaneous anatomical imaging and real-time (SMART) passive device tracking\nfor MR-guided interventions.\n  Methods: Phase Correlation template matching was combined with a fast\nundersampled radial multi-echo acquisition using the white marker phenomenon\nafter the first echo. In this way, the first echo provides anatomical contrast,\nwhereas the other echoes provide white marker contrast to allow accurate device\nlocalization using fast simulations and template matching. This approach was\ntested on tracking of five 0.5 mm steel markers in an agarose phantom and on\ninsertion of an MRI-compatible 20 Gauge titanium needle in ex vivo porcine\ntissue. The locations of the steel markers were quantitatively compared to the\nmarker locations as found on a CT scan of the same phantom.\n  Results: The average pairwise error between the MRI and CT locations was 0.30\nmm for tracking of stationary steel spheres and 0.29 mm during motion.\nQualitative evaluation of the tracking of needle insertions showed that tracked\npositions were stable throughout needle insertion and retraction.\n  Conclusions: The proposed SMART tracking method provided accurate passive\ntracking of devices at high framerates, inclusion of real-time anatomical\nscanning, and the capability of automatic slice positioning. Furthermore, the\nmethod does not require specialized hardware and could therefore be applied to\ntrack any rigid metal device that causes appreciable magnetic field\ndistortions.\n", "versions": [{"version": "v1", "created": "Wed, 28 Aug 2019 15:14:59 GMT"}], "update_date": "2019-08-29", "authors_parsed": [["Zijlstra", "Frank", ""], ["Viergever", "Max A.", ""], ["Seevinck", "Peter R.", ""]]}, {"id": "1908.10797", "submitter": "Jia Huei Tan", "authors": "Jia Huei Tan, Chee Seng Chan, Joon Huang Chuah", "title": "Image Captioning with Sparse Recurrent Neural Network", "comments": "Corrected Eq 11, updated Table 5", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recurrent Neural Network (RNN) has been widely used to tackle a wide variety\nof language generation problems and are capable of attaining state-of-the-art\n(SOTA) performance. However despite its impressive results, the large number of\nparameters in the RNN model makes deployment to mobile and embedded devices\ninfeasible. Driven by this problem, many works have proposed a number of\npruning methods to reduce the sizes of the RNN model. In this work, we propose\nan end-to-end pruning method for image captioning models equipped with visual\nattention. Our proposed method is able to achieve sparsity levels up to 97.5%\nwithout significant performance loss relative to the baseline (~ 2% loss at 40x\ncompression after fine-tuning). Our method is also simple to use and tune,\nfacilitating faster development times for neural network practitioners. We\nperform extensive experiments on the popular MS-COCO dataset in order to\nempirically validate the efficacy of our proposed method.\n", "versions": [{"version": "v1", "created": "Wed, 28 Aug 2019 15:53:13 GMT"}, {"version": "v2", "created": "Mon, 28 Oct 2019 15:51:13 GMT"}], "update_date": "2019-10-29", "authors_parsed": [["Tan", "Jia Huei", ""], ["Chan", "Chee Seng", ""], ["Chuah", "Joon Huang", ""]]}, {"id": "1908.10842", "submitter": "Tong Zhang PhD", "authors": "Tong Zhang, Laurence H. Jackson, Alena Uus, James R. Clough, Lisa\n  Story, Mary A. Rutherford, Joseph V. Hajnal, Maria Deprez", "title": "Self-supervised Recurrent Neural Network for 4D Abdominal and In-utero\n  MR Imaging", "comments": "Accepted by MICCAI 2019 workshop on Machine Learning for Medical\n  Image Reconstruction", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurately estimating and correcting the motion artifacts are crucial for 3D\nimage reconstruction of the abdominal and in-utero magnetic resonance imaging\n(MRI). The state-of-art methods are based on slice-to-volume registration (SVR)\nwhere multiple 2D image stacks are acquired in three orthogonal orientations.\nIn this work, we present a novel reconstruction pipeline that only needs one\norientation of 2D MRI scans and can reconstruct the full high-resolution image\nwithout masking or registration steps. The framework consists of two main\nstages: the respiratory motion estimation using a self-supervised recurrent\nneural network, which learns the respiratory signals that are naturally\nembedded in the asymmetry relationship of the neighborhood slices and cluster\nthem according to a respiratory state. Then, we train a 3D deconvolutional\nnetwork for super-resolution (SR) reconstruction of the sparsely selected 2D\nimages using integrated reconstruction and total variation loss. We evaluate\nthe classification accuracy on 5 simulated images and compare our results with\nthe SVR method in adult abdominal and in-utero MRI scans. The results show that\nthe proposed pipeline can accurately estimate the respiratory state and\nreconstruct 4D SR volumes with better or similar performance to the 3D SVR\npipeline with less than 20\\% sparsely selected slices. The method has great\npotential to transform the 4D abdominal and in-utero MRI in clinical practice.\n", "versions": [{"version": "v1", "created": "Wed, 28 Aug 2019 17:24:26 GMT"}], "update_date": "2019-08-29", "authors_parsed": [["Zhang", "Tong", ""], ["Jackson", "Laurence H.", ""], ["Uus", "Alena", ""], ["Clough", "James R.", ""], ["Story", "Lisa", ""], ["Rutherford", "Mary A.", ""], ["Hajnal", "Joseph V.", ""], ["Deprez", "Maria", ""]]}, {"id": "1908.10851", "submitter": "Chengliang Dai", "authors": "Chengliang Dai, Yuanhan Mo, Elsa Angelini, Yike Guo, Wenjia Bai", "title": "Transfer Learning from Partial Annotations for Whole Brain Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Brain MR image segmentation is a key task in neuroimaging studies. It is\ncommonly conducted using standard computational tools, such as FSL, SPM,\nmulti-atlas segmentation etc, which are often registration-based and suffer\nfrom expensive computation cost. Recently, there is an increased interest using\ndeep neural networks for brain image segmentation, which have demonstrated\nadvantages in both speed and performance. However, neural networks-based\napproaches normally require a large amount of manual annotations for optimising\nthe massive amount of network parameters. For 3D networks used in volumetric\nimage segmentation, this has become a particular challenge, as a 3D network\nconsists of many more parameters compared to its 2D counterpart. Manual\nannotation of 3D brain images is extremely time-consuming and requires\nextensive involvement of trained experts. To address the challenge with limited\nmanual annotations, here we propose a novel multi-task learning framework for\nbrain image segmentation, which utilises a large amount of automatically\ngenerated partial annotations together with a small set of manually created\nfull annotations for network training. Our method yields a high performance\ncomparable to state-of-the-art methods for whole brain segmentation.\n", "versions": [{"version": "v1", "created": "Wed, 28 Aug 2019 17:46:56 GMT"}], "update_date": "2019-08-29", "authors_parsed": [["Dai", "Chengliang", ""], ["Mo", "Yuanhan", ""], ["Angelini", "Elsa", ""], ["Guo", "Yike", ""], ["Bai", "Wenjia", ""]]}, {"id": "1908.10899", "submitter": "Jeffrey Byrne", "authors": "Gregory Castanon and Nathan Shnidman and Tim Anderson and Jeffrey\n  Byrne", "title": "Out the Window: A Crowd-Sourced Dataset for Activity Classification in\n  Security Video", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Out the Window (OTW) dataset is a crowdsourced activity dataset\ncontaining 5,668 instances of 17 activities from the NIST Activities in\nExtended Video (ActEV) challenge. These videos are crowdsourced from workers on\nthe Amazon Mechanical Turk using a novel scenario acting strategy, which\ncollects multiple instances of natural activities per scenario. Turkers are\ninstructed to lean their mobile device against an upper story window\noverlooking an outdoor space, walk outside to perform a scenario involving\npeople, vehicles and objects, and finally upload the video to us for\nannotation. Performance evaluation for activity classification on VIRAT Ground\n2.0 shows that the OTW dataset provides an 8.3% improvement in mean\nclassification accuracy, and a 12.5% improvement on the most challenging\nactivities involving people with vehicles.\n", "versions": [{"version": "v1", "created": "Wed, 28 Aug 2019 18:28:12 GMT"}, {"version": "v2", "created": "Sun, 15 Sep 2019 19:21:43 GMT"}], "update_date": "2019-09-17", "authors_parsed": [["Castanon", "Gregory", ""], ["Shnidman", "Nathan", ""], ["Anderson", "Tim", ""], ["Byrne", "Jeffrey", ""]]}, {"id": "1908.10907", "submitter": "Qingsong Xu", "authors": "Qingsong Xu, Chaojun Ouyang, Tianhai Jiang, Xuanmei Fan, Duoxiang\n  Cheng", "title": "DFPENet-geology: A Deep Learning Framework for High Precision\n  Recognition and Segmentation of Co-seismic Landslides", "comments": "1. There are some problems in the method and results, and there is a\n  lot of room for improvement. Overall, the proposed DFPENet has a high\n  redundancy by combining the Attention Gate Mechanism and Gate Convolution\n  Networks, and we need to further improve and refine the results. 2. For our\n  own research, we need experts to provide comments on my work whether negative\n  or positive", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The following lists two main reasons for withdrawal for the public. 1. There\nare some problems in the method and results, and there is a lot of room for\nimprovement. In terms of method, \"Pre-trained Datasets (PD)\" represents\nselecting a small amount from the online test set, which easily causes the\nmodel to overfit the online test set and could not obtain robust performance.\nMore importantly, the proposed DFPENet has a high redundancy by combining the\nAttention Gate Mechanism and Gate Convolution Networks, and we need to revisit\nthe section of geological feature fusion, in terms of results, we need to\nfurther improve and refine. 2. arXiv is an open-access repository of electronic\npreprints without peer reviews. However, for our own research, we need experts\nto provide comments on my work whether negative or positive. I then would use\ntheir comments to significantly improve this manuscript. Therefore, we finally\ndecided to withdraw this manuscript in arXiv, and we will update to arXiv with\nthe final accepted manuscript to facilitate more researchers to use our\nproposed comprehensive and general scheme to recognize and segment seismic\nlandslides more efficiently.\n", "versions": [{"version": "v1", "created": "Wed, 28 Aug 2019 19:07:40 GMT"}, {"version": "v2", "created": "Thu, 13 Feb 2020 14:42:33 GMT"}], "update_date": "2020-02-14", "authors_parsed": [["Xu", "Qingsong", ""], ["Ouyang", "Chaojun", ""], ["Jiang", "Tianhai", ""], ["Fan", "Xuanmei", ""], ["Cheng", "Duoxiang", ""]]}, {"id": "1908.10933", "submitter": "Iuliia Kotseruba", "authors": "John K. Tsotsos and Iuliia Kotseruba and Alexander Andreopoulos and\n  Yulong Wu", "title": "A Possible Reason for why Data-Driven Beats Theory-Driven Computer\n  Vision", "comments": "8 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Why do some continue to wonder about the success and dominance of deep\nlearning methods in computer vision and AI? Is it not enough that these methods\nprovide practical solutions to many problems? Well no, it is not enough, at\nleast for those who feel there should be a science that underpins all of this\nand that we should have a clear understanding of how this success was achieved.\nHere, this paper proposes that the dominance we are witnessing would not have\nbeen possible by the methods of deep learning alone: the tacit change has been\nthe evolution of empirical practice in computer vision and AI over the past\ndecades. We demonstrate this by examining the distribution of sensor settings\nin vision datasets and performance of both classic and deep learning algorithms\nunder various camera settings. This reveals a strong mismatch between optimal\nperformance ranges of classical theory-driven algorithms and sensor setting\ndistributions in the common vision datasets, while data-driven models were\ntrained for those datasets. The head-to-head comparisons between data-driven\nand theory-driven models were therefore unknowingly biased against the\ntheory-driven models.\n", "versions": [{"version": "v1", "created": "Wed, 28 Aug 2019 20:19:41 GMT"}, {"version": "v2", "created": "Fri, 6 Sep 2019 22:08:39 GMT"}], "update_date": "2019-09-10", "authors_parsed": [["Tsotsos", "John K.", ""], ["Kotseruba", "Iuliia", ""], ["Andreopoulos", "Alexander", ""], ["Wu", "Yulong", ""]]}, {"id": "1908.10937", "submitter": "Vishwanath Sindagi", "authors": "Vishwanath A Sindagi, Vishal M. Patel", "title": "Multi-Level Bottom-Top and Top-Bottom Feature Fusion for Crowd Counting", "comments": "Accepted at ICCV 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Crowd counting presents enormous challenges in the form of large variation in\nscales within images and across the dataset. These issues are further\nexacerbated in highly congested scenes. Approaches based on straightforward\nfusion of multi-scale features from a deep network seem to be obvious solutions\nto this problem. However, these fusion approaches do not yield significant\nimprovements in the case of crowd counting in congested scenes. This is usually\ndue to their limited abilities in effectively combining the multi-scale\nfeatures for problems like crowd counting. To overcome this, we focus on how to\nefficiently leverage information present in different layers of the network.\nSpecifically, we present a network that involves: (i) a multi-level bottom-top\nand top-bottom fusion (MBTTBF) method to combine information from shallower to\ndeeper layers and vice versa at multiple levels, (ii) scale complementary\nfeature extraction blocks (SCFB) involving cross-scale residual functions to\nexplicitly enable flow of complementary features from adjacent conv layers\nalong the fusion paths. Furthermore, in order to increase the effectiveness of\nthe multi-scale fusion, we employ a principled way of generating scale-aware\nground-truth density maps for training. Experiments conducted on three datasets\nthat contain highly congested scenes (ShanghaiTech, UCF_CC_50, and UCF-QNRF)\ndemonstrate that the proposed method is able to outperform several recent\nmethods in all the datasets.\n", "versions": [{"version": "v1", "created": "Wed, 28 Aug 2019 20:45:25 GMT"}], "update_date": "2019-08-30", "authors_parsed": [["Sindagi", "Vishwanath A", ""], ["Patel", "Vishal M.", ""]]}, {"id": "1908.10945", "submitter": "Fidel Alejandro Guerrero Pe\\~na", "authors": "Fidel Alejandro Guerrero Pe\\~na, Pedro Diamel Marrero Fern\\'andez,\n  Tsang Ing Ren, Germano Crispim Vasconcelos, Alexandre Cunha", "title": "A Multiple Source Hourglass Deep Network for Multi-Focus Image Fusion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-Focus Image Fusion seeks to improve the quality of an acquired burst of\nimages with different focus planes. For solving the task, an activity level\nmeasurement and a fusion rule are typically established to select and fuse the\nmost relevant information from the sources. However, the design of this kind of\nmethod by hand is really hard and sometimes restricted to solution spaces where\nthe optimal all-in-focus images are not contained. Then, we propose here two\nfast and straightforward approaches for image fusion based on deep neural\nnetworks. Our solution uses a multiple source Hourglass architecture trained in\nan end-to-end fashion. Models are data-driven and can be easily generalized for\nother kinds of fusion problems. A segmentation approach is used for recognition\nof the focus map, while the weighted average rule is used for fusion. We\ndesigned a training loss function for our regression-based fusion function,\nwhich allows the network to learn both the activity level measurement and the\nfusion rule. Experimental results show our approach has comparable results to\nthe state-of-the-art methods with a 60X increase of computational efficiency\nfor 520X520 resolution images.\n", "versions": [{"version": "v1", "created": "Wed, 28 Aug 2019 21:01:24 GMT"}], "update_date": "2019-08-30", "authors_parsed": [["Pe\u00f1a", "Fidel Alejandro Guerrero", ""], ["Fern\u00e1ndez", "Pedro Diamel Marrero", ""], ["Ren", "Tsang Ing", ""], ["Vasconcelos", "Germano Crispim", ""], ["Cunha", "Alexandre", ""]]}, {"id": "1908.10995", "submitter": "Jong-Min Kim", "authors": "Jong-Min Kim, You-Jin Jeong, Han-Jae Chung, Chulhyun Lee, Chang-Hyun\n  Oh", "title": "Real-time interactive magnetic resonance (MR) temperature imaging in\n  both aqueous and adipose tissues using cascaded deep neural networks for\n  MR-guided focused ultrasound surgery (MRgFUS)", "comments": "40 pages, 11 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Purpose: To acquire the real-time interactive temperature map for aqueous and\nadipose tissue, the problems of long acquisition and processing time must be\naddressed. To overcome these major challenges, this paper proposes a cascaded\nconvolutional neural network (CNN) framework and multi-echo gradient echo\n(meGRE) with a single reference variable flip angle (srVFA).\n  Methods: To optimize the echo times for each method, MR images are acquired\nusing a meGRE sequence; meGRE images with two flip angles (FAs) and meGRE\nimages with a single FA are acquired during the pretreatment and treatment\nstages, respectively. These images are then processed and reconstructed by a\ncascaded CNN, which consists of two CNNs. The first CNN (called DeepACCnet)\nperforms HR complex MR image reconstruction from the LR MR image acquired\nduring the treatment stage, which is improved by the HR magnitude MR image\nacquired during the pretreatment stage. The second CNN (called DeepPROCnet)\ncopes with T1 mapping.\n  Results: Measurements of temperature and T1 changes obtained by meGRE\ncombined with srVFA and cascaded CNNs were achieved in an agarose gel phantom,\nex vivo porcine muscle, and ex vivo porcine muscle with fat layers (heating\ntests), and in vivo human prostate and brain (non-heating tests). In the\nheating test, the maximum differences between fiber-optic sensor and samples\nare less than 1 degree Celcius. In all cases, temperature mapping using the\ncascaded CNN achieved the best results in all cases. The acquisition and\nprocessing times for the proposed method are 0.8 s and 32 ms, respectively.\n  Conclusions: Real-time interactive HR MR temperature mapping for\nsimultaneously measuring aqueous and adipose tissue is feasible by combining a\ncascaded CNN with meGRE and srVFA.\n", "versions": [{"version": "v1", "created": "Thu, 29 Aug 2019 00:33:29 GMT"}], "update_date": "2019-08-30", "authors_parsed": [["Kim", "Jong-Min", ""], ["Jeong", "You-Jin", ""], ["Chung", "Han-Jae", ""], ["Lee", "Chulhyun", ""], ["Oh", "Chang-Hyun", ""]]}, {"id": "1908.10998", "submitter": "Yanxiang Gong", "authors": "Linjie Deng, Yanxiang Gong, Xinchen Lu, Xin Yi, Zheng Ma, Mei Xie", "title": "Focus-Enhanced Scene Text Recognition with Deformable Convolutions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, scene text recognition methods based on deep learning have sprung\nup in computer vision area. The existing methods achieved great performances,\nbut the recognition of irregular text is still challenging due to the various\nshapes and distorted patterns. Consider that at the time of reading words in\nthe real world, normally we will not rectify it in our mind but adjust our\nfocus and visual fields. Similarly, through utilizing deformable convolutional\nlayers whose geometric structures are adjustable, we present an enhanced\nrecognition network without the steps of rectification to deal with irregular\ntext in this work. A number of experiments have been applied, where the results\non public benchmarks demonstrate the effectiveness of our proposed components\nand shows that our method has reached satisfactory performances. The code will\nbe publicly available at https://github.com/Alpaca07/dtr soon.\n", "versions": [{"version": "v1", "created": "Thu, 29 Aug 2019 00:54:02 GMT"}, {"version": "v2", "created": "Mon, 23 Sep 2019 07:23:03 GMT"}], "update_date": "2019-09-24", "authors_parsed": [["Deng", "Linjie", ""], ["Gong", "Yanxiang", ""], ["Lu", "Xinchen", ""], ["Yi", "Xin", ""], ["Ma", "Zheng", ""], ["Xie", "Mei", ""]]}, {"id": "1908.11024", "submitter": "Byung Cheol Song", "authors": "Dae Ha Kim, Seung Hyun Lee, and Byung Cheol Song", "title": "Metric-based Regularization and Temporal Ensemble for Multi-task\n  Learning using Heterogeneous Unsupervised Tasks", "comments": "11 pages. To Appear in the IEEE International Conference on Computer\n  Vision Workshops (ICCVW) 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the ways to improve the performance of a target task is to learn the\ntransfer of abundant knowledge of a pre-trained network. However, learning of\nthe pre-trained network requires high computation capability and large-scale\nlabeled dataset. To mitigate the burden of large-scale labeling, learning in\nun/self-supervised manner can be a solution. In addition, using unsupervised\nmulti-task learning, a generalized feature representation can be learned.\nHowever, unsupervised multi-task learning can be biased to a specific task. To\novercome this problem, we propose the metric-based regularization term and\ntemporal task ensemble (TTE) for multi-task learning. Since these two\ntechniques prevent the entire network from learning in a state deviated to a\nspecific task, it is possible to learn a generalized feature representation\nthat appropriately reflects the characteristics of each task without biasing.\nExperimental results for three target tasks such as classification, object\ndetection and embedding clustering prove that the TTE-based multi-task\nframework is more effective than the state-of-the-art (SOTA) method in\nimproving the performance of a target task.\n", "versions": [{"version": "v1", "created": "Thu, 29 Aug 2019 02:39:27 GMT"}], "update_date": "2019-08-30", "authors_parsed": [["Kim", "Dae Ha", ""], ["Lee", "Seung Hyun", ""], ["Song", "Byung Cheol", ""]]}, {"id": "1908.11025", "submitter": "Zhiliang Zeng", "authors": "Zhiliang Zeng and Xianzhi Li and Ying Kin Yu and Chi-Wing Fu", "title": "Deep Floor Plan Recognition Using a Multi-Task Network with\n  Room-Boundary-Guided Attention", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a new approach to recognize elements in floor plan\nlayouts. Besides walls and rooms, we aim to recognize diverse floor plan\nelements, such as doors, windows and different types of rooms, in the floor\nlayouts. To this end, we model a hierarchy of floor plan elements and design a\ndeep multi-task neural network with two tasks: one to learn to predict\nroom-boundary elements, and the other to predict rooms with types. More\nimportantly, we formulate the room-boundary-guided attention mechanism in our\nspatial contextual module to carefully take room-boundary features into account\nto enhance the room-type predictions. Furthermore, we design a\ncross-and-within-task weighted loss to balance the multi-label tasks and\nprepare two new datasets for floor plan recognition. Experimental results\ndemonstrate the superiority and effectiveness of our network over the\nstate-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Thu, 29 Aug 2019 02:50:05 GMT"}], "update_date": "2019-08-30", "authors_parsed": [["Zeng", "Zhiliang", ""], ["Li", "Xianzhi", ""], ["Yu", "Ying Kin", ""], ["Fu", "Chi-Wing", ""]]}, {"id": "1908.11026", "submitter": "Xin Wen", "authors": "Xin Wen, Zhizhong Han, Xinhai Liu, Yu-Shen Liu", "title": "Point2SpatialCapsule: Aggregating Features and Spatial Relationships of\n  Local Regions on Point Clouds using Spatial-aware Capsules", "comments": null, "journal-ref": null, "doi": "10.1109/TIP.2020.3019925", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning discriminative shape representation directly on point clouds is\nstill challenging in 3D shape analysis and understanding. Recent studies\nusually involve three steps: first splitting a point cloud into some local\nregions, then extracting corresponding feature of each local region, and\nfinally aggregating all individual local region features into a global feature\nas shape representation using simple max pooling. However, such pooling-based\nfeature aggregation methods do not adequately take the spatial relationships\nbetween local regions into account, which greatly limits the ability to learn\ndiscriminative shape representation. To address this issue, we propose a novel\ndeep learning network, named Point2SpatialCapsule, for aggregating features and\nspatial relationships of local regions on point clouds, which aims to learn\nmore discriminative shape representation. Compared with traditional max-pooling\nbased feature aggregation networks, Point2SpatialCapsule can explicitly learn\nnot only geometric features of local regions but also spatial relationships\namong them. It consists of two modules. To resolve the disorder problem of\nlocal regions, the first module, named geometric feature aggregation, is\ndesigned to aggregate the local region features into the learnable cluster\ncenters, which explicitly encodes the spatial locations from the original 3D\nspace. The second module, named spatial relationship aggregation, is proposed\nfor further aggregating clustered features and the spatial relationships among\nthem in the feature space using the spatial-aware capsules developed in this\npaper. Compared to the previous capsule network based methods, the feature\nrouting on the spatial-aware capsules can learn more discriminative spatial\nrelationships among local regions for point clouds, which establishes a direct\nmapping between log priors and the spatial locations through feature clusters.\n", "versions": [{"version": "v1", "created": "Thu, 29 Aug 2019 02:54:31 GMT"}], "update_date": "2020-10-28", "authors_parsed": [["Wen", "Xin", ""], ["Han", "Zhizhong", ""], ["Liu", "Xinhai", ""], ["Liu", "Yu-Shen", ""]]}, {"id": "1908.11036", "submitter": "Yonghao Dang", "authors": "Yonghao Dang and Fuxing Yang and Jianqin Yin", "title": "DWnet: Deep-Wide Network for 3D Action Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose in this paper a deep-wide network (DWnet) which combines the deep\nstructure with the broad learning system (BLS) to recognize actions. Compared\nwith the deep structure, the novel model saves lots of testing time and almost\nachieves real-time testing. Furthermore, the DWnet can capture better features\nthan broad learning system can. In terms of methodology, we use pruned\nhierarchical co-occurrence network (PruHCN) to learn local and global\nspatial-temporal features. To obtain sufficient global information, BLS is used\nto expand features extracted by PruHCN. Experiments on two common skeletal\ndatasets demonstrate the advantage of the proposed model on testing time and\nthe effectiveness of the novel model to recognize the action.\n", "versions": [{"version": "v1", "created": "Thu, 29 Aug 2019 03:50:43 GMT"}], "update_date": "2019-08-30", "authors_parsed": [["Dang", "Yonghao", ""], ["Yang", "Fuxing", ""], ["Yin", "Jianqin", ""]]}, {"id": "1908.11039", "submitter": "Ngoc-Trung Tran", "authors": "Ngoc-Trung Tran, Fakhr-Eddine Ababsa, Maurice Charbit, Jacques\n  Feldmar, Dijana Petrovska-Delacr\\'etaz, G\\'erard Chollet", "title": "3D Face Pose and Animation Tracking via Eigen-Decomposition based\n  Bayesian Approach", "comments": null, "journal-ref": "Advances in Visual Computing - 9th International Symposium, ISVC\n  2013, Rethymnon, Crete, Greece, July 29-31, 2013. Proceedings, Part I, pages\n  562--571", "doi": "10.1007/978-3-642-41914-0\\_55", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a new method to track both the face pose and the face\nanimation with a monocular camera. The approach is based on the 3D face model\nCANDIDE and on the SIFT (Scale Invariant Feature Transform) descriptors,\nextracted around a few given landmarks (26 selected vertices of CANDIDE model)\nwith a Bayesian approach. The training phase is performed on a synthetic\ndatabase generated from the first video frame. At each current frame, the face\npose and animation parameters are estimated via a Bayesian approach, with a\nGaussian prior and a Gaussian likelihood function whose the mean and the\ncovariance matrix eigenvalues are updated from the previous frame using eigen\ndecomposition. Numerical results on pose estimation and landmark locations are\nreported using the Boston University Face Tracking (BUFT) database and Talking\nFace video. They show that our approach, compared to six other published\nalgorithms, provides a very good compromise and presents a promising\nperspective due to the good results in terms of landmark localization.\n", "versions": [{"version": "v1", "created": "Thu, 29 Aug 2019 03:59:51 GMT"}], "update_date": "2019-08-30", "authors_parsed": [["Tran", "Ngoc-Trung", ""], ["Ababsa", "Fakhr-Eddine", ""], ["Charbit", "Maurice", ""], ["Feldmar", "Jacques", ""], ["Petrovska-Delacr\u00e9taz", "Dijana", ""], ["Chollet", "G\u00e9rard", ""]]}, {"id": "1908.11044", "submitter": "Xiangyu Xu", "authors": "Xiangyu Xu, Enrique Dunn", "title": "Discrete Laplace Operator Estimation for Dynamic 3D Reconstruction", "comments": "Accepted for oral presentation at ICCV 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a general paradigm for dynamic 3D reconstruction from multiple\nindependent and uncontrolled image sources having arbitrary temporal sampling\ndensity and distribution. Our graph-theoretic formulation models the\nSpatio-temporal relationships among our observations in terms of the joint\nestimation of their 3D geometry and its discrete Laplace operator. Towards this\nend, we define a tri-convex optimization framework that leverages the geometric\nproperties and dependencies found among a Euclideanshape-space and the discrete\nLaplace operator describing its local and global topology. We present a\nreconstructability analysis, experiments on motion capture data and multi-view\nimage datasets, as well as explore applications to geometry-based event\nsegmentation and data association.\n", "versions": [{"version": "v1", "created": "Thu, 29 Aug 2019 04:17:23 GMT"}], "update_date": "2019-08-30", "authors_parsed": [["Xu", "Xiangyu", ""], ["Dunn", "Enrique", ""]]}, {"id": "1908.11060", "submitter": "Chankyu Choi", "authors": "Hong-Seok Lee, Youngmin Yoon, Pil-Hoon Jang, Chankyu Choi", "title": "PopEval: A Character-Level Approach to End-To-End Evaluation Compatible\n  with Word-Level Benchmark Dataset", "comments": "Accepted by ICDAR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The most prevalent scope of interest for OCR applications used to be scanned\ndocuments, but it has now shifted towards the natural scene. Despite the change\nof times, the existing evaluation methods are still based on the old criteria\nsuited better for the past interests. In this paper, we propose PopEval, a\nnovel evaluation approach for the recent OCR interests. The new and past\nevaluation algorithms were compared through the results on various datasets and\nOCR models. Compared to the other evaluation methods, the proposed evaluation\nalgorithm was closer to the human's qualitative evaluation than other existing\nmethods. Although the evaluation algorithm was devised as a character-level\napproach, the comparative experiment revealed that PopEval is also compatible\non existing benchmark datasets annotated at word-level. The proposed evaluation\nalgorithm is not only applicable to current end-to-end tasks, but also suggests\na new direction to redesign the evaluation concept for further OCR researches.\n", "versions": [{"version": "v1", "created": "Thu, 29 Aug 2019 05:38:37 GMT"}], "update_date": "2019-08-30", "authors_parsed": [["Lee", "Hong-Seok", ""], ["Yoon", "Youngmin", ""], ["Jang", "Pil-Hoon", ""], ["Choi", "Chankyu", ""]]}, {"id": "1908.11069", "submitter": "Vijay Vasudevan", "authors": "Jiquan Ngiam, Benjamin Caine, Wei Han, Brandon Yang, Yuning Chai, Pei\n  Sun, Yin Zhou, Xi Yi, Ouais Alsharif, Patrick Nguyen, Zhifeng Chen, Jonathon\n  Shlens, Vijay Vasudevan", "title": "StarNet: Targeted Computation for Object Detection in Point Clouds", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Detecting objects from LiDAR point clouds is an important component of\nself-driving car technology as LiDAR provides high resolution spatial\ninformation. Previous work on point-cloud 3D object detection has re-purposed\nconvolutional approaches from traditional camera imagery. In this work, we\npresent an object detection system called StarNet designed specifically to take\nadvantage of the sparse and 3D nature of point cloud data. StarNet is entirely\npoint-based, uses no global information, has data dependent anchors, and uses\nsampling instead of learned region proposals. We demonstrate how this design\nleads to competitive or superior performance on the large Waymo Open Dataset\nand the KITTI detection dataset, as compared to convolutional baselines. In\nparticular, we show how our detector can outperform a competitive baseline on\nPedestrian detection on the Waymo Open Dataset by more than 7 absolute mAP\nwhile being more computationally efficient. We show how our redesign---namely\nusing only local information and using sampling instead of learned\nproposals---leads to a significantly more flexible and adaptable system: we\ndemonstrate how we can vary the computational cost of a single trained StarNet\nwithout retraining, and how we can target proposals towards areas of interest\nwith priors and heuristics. Finally, we show how our design allows for\nincorporating temporal context by using detections from previous frames to\ntarget computation of the detector, which leads to further improvements in\nperformance without additional computational cost.\n", "versions": [{"version": "v1", "created": "Thu, 29 Aug 2019 06:54:46 GMT"}, {"version": "v2", "created": "Mon, 18 Nov 2019 21:56:35 GMT"}, {"version": "v3", "created": "Mon, 2 Dec 2019 22:15:26 GMT"}], "update_date": "2019-12-04", "authors_parsed": [["Ngiam", "Jiquan", ""], ["Caine", "Benjamin", ""], ["Han", "Wei", ""], ["Yang", "Brandon", ""], ["Chai", "Yuning", ""], ["Sun", "Pei", ""], ["Zhou", "Yin", ""], ["Yi", "Xi", ""], ["Alsharif", "Ouais", ""], ["Nguyen", "Patrick", ""], ["Chen", "Zhifeng", ""], ["Shlens", "Jonathon", ""], ["Vasudevan", "Vijay", ""]]}, {"id": "1908.11080", "submitter": "Rui Guo", "authors": "Rui Guo, Ronghua Liu, Na Li, Wei Liu", "title": "DV3+HED+: A DCNNs-based Framework to Monitor Temporary Works and ESAs in\n  Railway Construction Project Using VHR Satellite Images", "comments": "15 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current VHR(Very High Resolution) satellite images enable the detailed\nmonitoring of the earth and can capture the ongoing works of railway\nconstruction. In this paper, we present an integrated framework applied to\nmonitoring the railway construction in China, using QuickBird, GF-2 and Google\nEarth VHR satellite images. We also construct a novel DCNNs-based (Deep\nConvolutional Neural Networks) semantic segmentation network to label the\ntemporary works such as borrow & spoil area, camp, beam yard and\nESAs(Environmental Sensitive Areas) such as resident houses throughout the\nwhole railway construction project using VHR satellite images. In addition, we\nemploy HED edge detection sub-network to refine the boundary details and\nattention cross entropy loss function to fit the sample class disequilibrium\nproblem. Our semantic segmentation network is trained on 572 VHR true color\nimages, and tested on the 15 QuickBird true color images along\nRuichang-Jiujiang railway during 2015-2017. The experiment results show that\ncompared with the existing state-of-the-art approach, our approach has obvious\nimprovements with an overall accuracy of more than 80%.\n", "versions": [{"version": "v1", "created": "Thu, 29 Aug 2019 07:46:32 GMT"}], "update_date": "2019-08-30", "authors_parsed": [["Guo", "Rui", ""], ["Liu", "Ronghua", ""], ["Li", "Na", ""], ["Liu", "Wei", ""]]}, {"id": "1908.11092", "submitter": "Dong Lao", "authors": "Dong Lao and Ganesh Sundaramoorthi", "title": "Minimum Delay Object Detection From Video", "comments": "ICCV 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of detecting objects, as they come into view, from\nvideos in an online fashion. We provide the first real-time solution that is\nguaranteed to minimize the delay, i.e., the time between when the object comes\nin view and the declared detection time, subject to acceptable levels of\ndetection accuracy. The method leverages modern CNN-based object detectors that\noperate on a single frame, to aggregate detection results over frames to\nprovide reliable detection at a rate, specified by the user, in guaranteed\nminimal delay. To do this, we formulate the problem as a Quickest Detection\nproblem, which provides the aforementioned guarantees. We derive our algorithms\nfrom this theory. We show in experiments, that with an overhead of just 50 fps,\nwe can increase the number of correct detections and decrease the overall\ncomputational cost compared to running a modern single-frame detector.\n", "versions": [{"version": "v1", "created": "Thu, 29 Aug 2019 08:25:40 GMT"}], "update_date": "2019-08-30", "authors_parsed": [["Lao", "Dong", ""], ["Sundaramoorthi", "Ganesh", ""]]}, {"id": "1908.11111", "submitter": "Christian Joppi", "authors": "Christian Joppi, Marco Godi, Andrea Giachetti, Fabio Pellacini, Marco\n  Cristani", "title": "Texture Retrieval in the Wild through detection-based attributes", "comments": "ICIAP - International Conference on Image Analysis and Processing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Capturing the essence of a textile image in a robust way is important to\nretrieve it in a large repository, especially if it has been acquired in the\nwild (by taking a photo of the textile of interest). In this paper we show that\na texel-based representation fits well with this task. In particular, we refer\nto Texel-Att, a recent texel-based descriptor which has shown to capture fine\ngrained variations of a texture, for retrieval purposes. After a brief\nexplanation of Texel-Att, we will show in our experiments that this descriptor\nis robust to distortions resulting from acquisitions in the wild by setting up\nan experiment in which textures from the ElBa (an Element-Based texture\ndataset) are artificially distorted and then used to retrieve the original\nimage. We compare our approach with existing descriptors using a simple ranking\nframework based on distance functions. Results show that even under extreme\nconditions (such as a down-sampling with a factor of 10), we perform better\nthan alternative approaches.\n", "versions": [{"version": "v1", "created": "Thu, 29 Aug 2019 09:13:13 GMT"}, {"version": "v2", "created": "Fri, 30 Aug 2019 07:10:02 GMT"}, {"version": "v3", "created": "Wed, 4 Sep 2019 07:18:41 GMT"}, {"version": "v4", "created": "Fri, 4 Oct 2019 07:56:46 GMT"}], "update_date": "2019-10-07", "authors_parsed": [["Joppi", "Christian", ""], ["Godi", "Marco", ""], ["Giachetti", "Andrea", ""], ["Pellacini", "Fabio", ""], ["Cristani", "Marco", ""]]}, {"id": "1908.11112", "submitter": "Maarten Schellevis", "authors": "Maarten Schellevis", "title": "Improving Self-Supervised Single View Depth Estimation by Masking\n  Occlusion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Single view depth estimation models can be trained from video footage using a\nself-supervised end-to-end approach with view synthesis as the supervisory\nsignal. This is achieved with a framework that predicts depth and camera\nmotion, with a loss based on reconstructing a target video frame from\ntemporally adjacent frames. In this context, occlusion relates to parts of a\nscene that can be observed in the target frame but not in a frame used for\nimage reconstruction. Since the image reconstruction is based on sampling from\nthe adjacent frame, and occluded areas by definition cannot be sampled,\nreconstructed occluded areas corrupt to the supervisory signal. In previous\nwork arXiv:1806.01260 occlusion is handled based on reconstruction error; at\neach pixel location, only the reconstruction with the lowest error is included\nin the loss. The current study aims to determine whether performance\nimprovements of depth estimation models can be gained by during training only\nignoring those regions that are affected by occlusion.\n  In this work we introduce occlusion mask, a mask that during training can be\nused to specifically ignore regions that cannot be reconstructed due to\nocclusions. Occlusion mask is based entirely on predicted depth information. We\nintroduce two novel loss formulations which incorporate the occlusion mask. The\nmethod and implementation of arXiv:1806.01260 serves as the foundation for our\nmodifications as well as the baseline in our experiments. We demonstrate that\n(i) incorporating occlusion mask in the loss function improves the performance\nof single image depth prediction models on the KITTI benchmark. (ii) loss\nfunctions that select from reconstructions based on error are able to ignore\nsome of the reprojection error caused by object motion.\n", "versions": [{"version": "v1", "created": "Thu, 29 Aug 2019 09:13:29 GMT"}], "update_date": "2019-08-30", "authors_parsed": [["Schellevis", "Maarten", ""]]}, {"id": "1908.11127", "submitter": "Marco Godi", "authors": "Marco Godi, Christian Joppi, Andrea Giachetti, Fabio Pellacini, Marco\n  Cristani", "title": "Texel-Att: Representing and Classifying Element-based Textures by\n  Attributes", "comments": "Accepted as oral at BMVC 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Element-based textures are a kind of texture formed by nameable elements, the\ntexels [1], distributed according to specific statistical distributions; it is\nof primary importance in many sectors, namely textile, fashion and interior\ndesign industry. State-of-theart texture descriptors fail to properly\ncharacterize element-based texture, so we present Texel-Att to fill this gap.\nTexel-Att is the first fine-grained, attribute-based representation and\nclassification framework for element-based textures. It first individuates\ntexels, characterizing them with individual attributes; subsequently, texels\nare grouped and characterized through layout attributes, which give the\nTexel-Att representation. Texels are detected by a Mask-RCNN, trained on a\nbrand-new element-based texture dataset, ElBa, containing 30K texture images\nwith 3M fully-annotated texels. Examples of individual and layout attributes\nare exhibited to give a glimpse on the level of achievable graininess. In the\nexperiments, we present detection results to show that texels can be precisely\nindividuated, even on textures \"in the wild\"; to this sake, we individuate the\nelement-based classes of the Describable Texture Dataset (DTD), where almost\n900K texels have been manually annotated, leading to the Element-based DTD\n(E-DTD). Subsequently, classification and ranking results demonstrate the\nexpressivity of Texel-Att on ElBa and E-DTD, overcoming the alternative\nfeatures and relative attributes, doubling the best performance in some cases;\nfinally, we report interactive search results on ElBa and E-DTD: with Texel-Att\non the E-DTD dataset we are able to individuate within 10 iterations the\ndesired texture in the 90% of cases, against the 71% obtained with a\ncombination of the finest existing attributes so far. Dataset and code is\navailable at https://github.com/godimarcovr/Texel-Att\n", "versions": [{"version": "v1", "created": "Thu, 29 Aug 2019 09:50:44 GMT"}, {"version": "v2", "created": "Fri, 30 Aug 2019 07:14:19 GMT"}], "update_date": "2019-09-02", "authors_parsed": [["Godi", "Marco", ""], ["Joppi", "Christian", ""], ["Giachetti", "Andrea", ""], ["Pellacini", "Fabio", ""], ["Cristani", "Marco", ""]]}, {"id": "1908.11211", "submitter": "G\\\"ozde Nur G\\\"une\\c{s}li", "authors": "Can Fahrettin Koyuncu, Gozde Nur Gunesli, Rengul Cetin-Atalay, Cigdem\n  Gunduz-Demir", "title": "DeepDistance: A Multi-task Deep Regression Model for Cell Detection in\n  Inverted Microscopy Images", "comments": "Preprint submitted to Elsevier", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a new deep regression model, which we call DeepDistance,\nfor cell detection in images acquired with inverted microscopy. This model\nconsiders cell detection as a task of finding most probable locations that\nsuggest cell centers in an image. It represents this main task with a\nregression task of learning an inner distance metric. However, different than\nthe previously reported regression based methods, the DeepDistance model\nproposes to approach its learning as a multi-task regression problem where\nmultiple tasks are learned by using shared feature representations. To this\nend, it defines a secondary metric, normalized outer distance, to represent a\ndifferent aspect of the problem and proposes to define its learning as\ncomplementary to the main cell detection task. In order to learn these two\ncomplementary tasks more effectively, the DeepDistance model designs a fully\nconvolutional network (FCN) with a shared encoder path and end-to-end trains\nthis FCN to concurrently learn the tasks in parallel. DeepDistance uses the\ninner distances estimated by this FCN in a detection algorithm to locate\nindividual cells in a given image. For further performance improvement on the\nmain task, this paper also presents an extended version of the DeepDistance\nmodel. This extended model includes an auxiliary classification task and learns\nit in parallel to the two regression tasks by sharing feature representations\nwith them. Our experiments on three different human cell lines reveal that the\nproposed multi-task learning models, the DeepDistance model and its extended\nversion, successfully identify cell locations, even for the cell line that was\nnot used in training, and improve the results of the previous deep learning\nmethods.\n", "versions": [{"version": "v1", "created": "Thu, 29 Aug 2019 13:21:25 GMT"}], "update_date": "2019-08-30", "authors_parsed": [["Koyuncu", "Can Fahrettin", ""], ["Gunesli", "Gozde Nur", ""], ["Cetin-Atalay", "Rengul", ""], ["Gunduz-Demir", "Cigdem", ""]]}, {"id": "1908.11215", "submitter": "Jan-Nico Zaech", "authors": "Jan-Nico Zaech, Dengxin Dai, Martin Hahner, Luc Van Gool", "title": "Texture Underfitting for Domain Adaptation", "comments": "Accepted manuscript, IEEE Intelligent Transportation Systems\n  Conference, 6 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Comprehensive semantic segmentation is one of the key components for robust\nscene understanding and a requirement to enable autonomous driving. Driven by\nlarge scale datasets, convolutional neural networks show impressive results on\nthis task. However, a segmentation algorithm generalizing to various scenes and\nconditions would require an enormously diverse dataset, making the labour\nintensive data acquisition and labeling process prohibitively expensive. Under\nthe assumption of structural similarities between segmentation maps, domain\nadaptation promises to resolve this challenge by transferring knowledge from\nexisting, potentially simulated datasets to new environments where no\nsupervision exists. While the performance of this approach is contingent on the\nconcept that neural networks learn a high level understanding of scene\nstructure, recent work suggests that neural networks are biased towards\noverfitting to texture instead of learning structural and shape information.\nConsidering the ideas underlying semantic segmentation, we employ random image\nstylization to augment the training dataset and propose a training procedure\nthat facilitates texture underfitting to improve the performance of domain\nadaptation. In experiments with supervised as well as unsupervised methods for\nthe task of synthetic-to-real domain adaptation, we show that our approach\noutperforms conventional training methods.\n", "versions": [{"version": "v1", "created": "Thu, 29 Aug 2019 13:33:14 GMT"}], "update_date": "2019-08-30", "authors_parsed": [["Zaech", "Jan-Nico", ""], ["Dai", "Dengxin", ""], ["Hahner", "Martin", ""], ["Van Gool", "Luc", ""]]}, {"id": "1908.11240", "submitter": "Xinyu Yang", "authors": "Xinyu Yang, Majid Mirmehdi, Tilo Burghardt", "title": "Great Ape Detection in Challenging Jungle Camera Trap Footage via\n  Attention-Based Spatial and Temporal Feature Blending", "comments": "Accepted by ICCV workshop 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose the first multi-frame video object detection framework trained to\ndetect great apes. It is applicable to challenging camera trap footage in\ncomplex jungle environments and extends a traditional feature pyramid\narchitecture by adding self-attention driven feature blending in both the\nspatial as well as the temporal domain. We demonstrate that this extension can\ndetect distinctive species appearance and motion signatures despite significant\npartial occlusion. We evaluate the framework using 500 camera trap videos of\ngreat apes from the Pan African Programme containing 180K frames, which we\nmanually annotated with accurate per-frame animal bounding boxes. These clips\ncontain significant partial occlusions, challenging lighting, dynamic\nbackgrounds, and natural camouflage effects. We show that our approach performs\nhighly robustly and significantly outperforms frame-based detectors. We also\nperform detailed ablation studies and validation on the full ILSVRC 2015 VID\ndata corpus to demonstrate wider applicability at adequate performance levels.\nWe conclude that the framework is ready to assist human camera trap inspection\nefforts. We publish code, weights, and ground truth annotations with this\npaper.\n", "versions": [{"version": "v1", "created": "Thu, 29 Aug 2019 14:02:59 GMT"}], "update_date": "2019-08-30", "authors_parsed": [["Yang", "Xinyu", ""], ["Mirmehdi", "Majid", ""], ["Burghardt", "Tilo", ""]]}, {"id": "1908.11262", "submitter": "Dogancan Temel", "authors": "Dogancan Temel and Min-Hung Chen and Ghassan AlRegib", "title": "Traffic Sign Detection under Challenging Conditions: A Deeper Look Into\n  Performance Variations and Spectral Characteristics", "comments": "13 pages, 9 figures, 4 tables. IEEE Transactions on Intelligent\n  Transportation Systems, 2019", "journal-ref": null, "doi": "10.1109/TITS.2019.2931429", "report-no": null, "categories": "cs.CV cs.LG eess.IV eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traffic signs are critical for maintaining the safety and efficiency of our\nroads. Therefore, we need to carefully assess the capabilities and limitations\nof automated traffic sign detection systems. Existing traffic sign datasets are\nlimited in terms of type and severity of challenging conditions. Metadata\ncorresponding to these conditions are unavailable and it is not possible to\ninvestigate the effect of a single factor because of simultaneous changes in\nnumerous conditions. To overcome the shortcomings in existing datasets, we\nintroduced the CURE-TSD-Real dataset, which is based on simulated challenging\nconditions that correspond to adversaries that can occur in real-world\nenvironments and systems. We test the performance of two benchmark algorithms\nand show that severe conditions can result in an average performance\ndegradation of 29% in precision and 68% in recall. We investigate the effect of\nchallenging conditions through spectral analysis and show that challenging\nconditions can lead to distinct magnitude spectrum characteristics. Moreover,\nwe show that mean magnitude spectrum of changes in video sequences under\nchallenging conditions can be an indicator of detection performance.\nCURE-TSD-Real dataset is available online at\nhttps://github.com/olivesgatech/CURE-TSD.\n", "versions": [{"version": "v1", "created": "Thu, 29 Aug 2019 14:37:40 GMT"}], "update_date": "2019-08-30", "authors_parsed": [["Temel", "Dogancan", ""], ["Chen", "Min-Hung", ""], ["AlRegib", "Ghassan", ""]]}, {"id": "1908.11309", "submitter": "Radu Sibechi", "authors": "Radu Sibechi, Olaf Booij, Nora Baka, Peter Bloem", "title": "Exploiting Temporality for Semi-Supervised Video Segmentation", "comments": "Accepted as workshop paper at ICCV 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, there has been remarkable progress in supervised image\nsegmentation. Video segmentation is less explored, despite the temporal\ndimension being highly informative. Semantic labels, e.g. that cannot be\naccurately detected in the current frame, may be inferred by incorporating\ninformation from previous frames. However, video segmentation is challenging\ndue to the amount of data that needs to be processed and, more importantly, the\ncost involved in obtaining ground truth annotations for each frame. In this\npaper, we tackle the issue of label scarcity by using consecutive frames of a\nvideo, where only one frame is annotated. We propose a deep, end-to-end\ntrainable model which leverages temporal information in order to make use of\neasy to acquire unlabeled data. Our network architecture relies on a novel\ninterconnection of two components: a fully convolutional network to model\nspatial information and temporal units that are employed at intermediate levels\nof the convolutional network in order to propagate information through time.\nThe main contribution of this work is the guidance of the temporal signal\nthrough the network. We show that only placing a temporal module between the\nencoder and decoder is suboptimal (baseline). Our extensive experiments on the\nCityScapes dataset indicate that the resulting model can leverage unlabeled\ntemporal frames and significantly outperform both the frame-by-frame image\nsegmentation and the baseline approach.\n", "versions": [{"version": "v1", "created": "Thu, 29 Aug 2019 15:50:12 GMT"}], "update_date": "2019-08-30", "authors_parsed": [["Sibechi", "Radu", ""], ["Booij", "Olaf", ""], ["Baka", "Nora", ""], ["Bloem", "Peter", ""]]}, {"id": "1908.11310", "submitter": "Koustav Ghosal", "authors": "Koustav Ghosal, Aakanksha Rana, Aljosa Smolic", "title": "Aesthetic Image Captioning From Weakly-Labelled Photographs", "comments": "International Workshop on Cross-Modal Learning in Real World, ICCV\n  2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Aesthetic image captioning (AIC) refers to the multi-modal task of generating\ncritical textual feedbacks for photographs. While in natural image captioning\n(NIC), deep models are trained in an end-to-end manner using large curated\ndatasets such as MS-COCO, no such large-scale, clean dataset exists for AIC.\nTowards this goal, we propose an automatic cleaning strategy to create a\nbenchmarking AIC dataset, by exploiting the images and noisy comments easily\navailable from photography websites. We propose a probabilistic\ncaption-filtering method for cleaning the noisy web-data, and compile a\nlarge-scale, clean dataset \"AVA-Captions\", (230, 000 images with 5 captions per\nimage). Additionally, by exploiting the latent associations between aesthetic\nattributes, we propose a strategy for training the convolutional neural network\n(CNN) based visual feature extractor, the first component of the AIC framework.\nThe strategy is weakly supervised and can be effectively used to learn rich\naesthetic representations, without requiring expensive ground-truth\nannotations. We finally show-case a thorough analysis of the proposed\ncontributions using automatic metrics and subjective evaluations.\n", "versions": [{"version": "v1", "created": "Thu, 29 Aug 2019 15:50:28 GMT"}], "update_date": "2019-08-30", "authors_parsed": [["Ghosal", "Koustav", ""], ["Rana", "Aakanksha", ""], ["Smolic", "Aljosa", ""]]}, {"id": "1908.11312", "submitter": "Benjamin Hou", "authors": "Benjamin Hou, Athanasios Vlontzos, Amir Alansary, Daniel Rueckert and\n  Bernhard Kainz", "title": "Flexible Conditional Image Generation of Missing Data with Learned\n  Mental Maps", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Real-world settings often do not allow acquisition of high-resolution\nvolumetric images for accurate morphological assessment and diagnostic. In\nclinical practice it is frequently common to acquire only sparse data (e.g.\nindividual slices) for initial diagnostic decision making. Thereby, physicians\nrely on their prior knowledge (or mental maps) of the human anatomy to\nextrapolate the underlying 3D information. Accurate mental maps require years\nof anatomy training, which in the first instance relies on normative learning,\ni.e. excluding pathology. In this paper, we leverage Bayesian Deep Learning and\nenvironment mapping to generate full volumetric anatomy representations from\nnone to a small, sparse set of slices. We evaluate proof of concept\nimplementations based on Generative Query Networks (GQN) and Conditional BRUNO\nusing abdominal CT and brain MRI as well as in a clinical application involving\nsparse, motion-corrupted MR acquisition for fetal imaging. Our approach allows\nto reconstruct 3D volumes from 1 to 4 tomographic slices, with a SSIM of 0.7+\nand cross-correlation of 0.8+ compared to the 3D ground truth.\n", "versions": [{"version": "v1", "created": "Thu, 29 Aug 2019 15:53:40 GMT"}], "update_date": "2019-08-30", "authors_parsed": [["Hou", "Benjamin", ""], ["Vlontzos", "Athanasios", ""], ["Alansary", "Amir", ""], ["Rueckert", "Daniel", ""], ["Kainz", "Bernhard", ""]]}, {"id": "1908.11314", "submitter": "Zongsheng Yue", "authors": "Zongsheng Yue, Hongwei Yong, Qian Zhao, Lei Zhang and Deyu Meng", "title": "Variational Denoising Network: Toward Blind Noise Modeling and Removal", "comments": "11 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Blind image denoising is an important yet very challenging problem in\ncomputer vision due to the complicated acquisition process of real images. In\nthis work we propose a new variational inference method, which integrates both\nnoise estimation and image denoising into a unique Bayesian framework, for\nblind image denoising. Specifically, an approximate posterior, parameterized by\ndeep neural networks, is presented by taking the intrinsic clean image and\nnoise variances as latent variables conditioned on the input noisy image. This\nposterior provides explicit parametric forms for all its involved\nhyper-parameters, and thus can be easily implemented for blind image denoising\nwith automatic noise estimation for the test noisy image. On one hand, as other\ndata-driven deep learning methods, our method, namely variational denoising\nnetwork (VDN), can perform denoising efficiently due to its explicit form of\nposterior expression. On the other hand, VDN inherits the advantages of\ntraditional model-driven approaches, especially the good generalization\ncapability of generative models. VDN has good interpretability and can be\nflexibly utilized to estimate and remove complicated non-i.i.d. noise collected\nin real scenarios. Comprehensive experiments are performed to substantiate the\nsuperiority of our method in blind image denoising.\n", "versions": [{"version": "v1", "created": "Thu, 29 Aug 2019 15:54:06 GMT"}, {"version": "v2", "created": "Mon, 23 Sep 2019 14:31:04 GMT"}, {"version": "v3", "created": "Fri, 26 Jun 2020 13:08:28 GMT"}, {"version": "v4", "created": "Sat, 11 Jul 2020 08:48:57 GMT"}], "update_date": "2020-07-14", "authors_parsed": [["Yue", "Zongsheng", ""], ["Yong", "Hongwei", ""], ["Zhao", "Qian", ""], ["Zhang", "Lei", ""], ["Meng", "Deyu", ""]]}, {"id": "1908.11324", "submitter": "Ning Zhang", "authors": "Ning Zhang, Dechun Wang, Xinzi Sun, Pengfei Zhang, Chenxi Zhang, Yu\n  Cao, Benyuan Liu", "title": "3D Anchor-Free Lesion Detector on Computed Tomography Scans", "comments": null, "journal-ref": "TransAI 2019", "doi": "10.1109/TransAI.2019.00016", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Lesions are injuries and abnormal tissues in the human body. Detecting\nlesions in 3D Computed Tomography (CT) scans can be time-consuming even for\nvery experienced physicians and radiologists. In recent years, CNN based lesion\ndetectors have demonstrated huge potentials. Most of current state-of-the-art\nlesion detectors employ anchors to enumerate all possible bounding boxes with\nrespect to the dataset in process. This anchor mechanism greatly improves the\ndetection performance while also constraining the generalization ability of\ndetectors. In this paper, we propose an anchor-free lesion detector. The anchor\nmechanism is removed and lesions are formalized as single keypoints. By doing\nso, we witness a considerable performance gain in terms of both accuracy and\ninference speed compared with the anchor-based baseline\n", "versions": [{"version": "v1", "created": "Thu, 29 Aug 2019 16:15:45 GMT"}], "update_date": "2019-09-30", "authors_parsed": [["Zhang", "Ning", ""], ["Wang", "Dechun", ""], ["Sun", "Xinzi", ""], ["Zhang", "Pengfei", ""], ["Zhang", "Chenxi", ""], ["Cao", "Yu", ""], ["Liu", "Benyuan", ""]]}, {"id": "1908.11330", "submitter": "Gabriele Valvano", "authors": "Gabriele Valvano, Agisilaos Chartsias, Andrea Leo and Sotirios A.\n  Tsaftaris", "title": "Temporal Consistency Objectives Regularize the Learning of Disentangled\n  Representations", "comments": "9 pages, 4 figures (1 .gif), 1 table", "journal-ref": "Domain Adaptation and Representation Transfer and Medical Image\n  Learning with Less Labels and Imperfect Data. DART 2019, MIL3ID 2019. Lecture\n  Notes in Computer Science, vol 11795 pp 11-19. Springer, Cham", "doi": "10.1007/978-3-030-33391-1_2", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There has been an increasing focus in learning interpretable feature\nrepresentations, particularly in applications such as medical image analysis\nthat require explainability, whilst relying less on annotated data (since\nannotations can be tedious and costly). Here we build on recent innovations in\nstyle-content representations to learn anatomy, imaging characteristics\n(appearance) and temporal correlations. By introducing a self-supervised\nobjective of predicting future cardiac phases we improve disentanglement. We\npropose a temporal transformer architecture that given an image conditioned on\nphase difference, it predicts a future frame. This forces the anatomical\ndecomposition to be consistent with the temporal cardiac contraction in cine\nMRI and to have semantic meaning with less need for annotations. We demonstrate\nthat using this regularization, we achieve competitive results and improve\nsemi-supervised segmentation, especially when very few labelled data are\navailable. Specifically, we show Dice increase of up to 19\\% and 7\\% compared\nto supervised and semi-supervised approaches respectively on the ACDC dataset.\nCode is available at: https://github.com/gvalvano/sdtnet .\n", "versions": [{"version": "v1", "created": "Thu, 29 Aug 2019 16:23:50 GMT"}], "update_date": "2019-11-19", "authors_parsed": [["Valvano", "Gabriele", ""], ["Chartsias", "Agisilaos", ""], ["Leo", "Andrea", ""], ["Tsaftaris", "Sotirios A.", ""]]}, {"id": "1908.11331", "submitter": "Xin Zhong", "authors": "Xin Zhong and Frank Y. Shih", "title": "A Robust Image Watermarking System Based on Deep Neural Networks", "comments": null, "journal-ref": null, "doi": "10.1109/TMM.2020.3006415", "report-no": null, "categories": "cs.MM cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Digital image watermarking is the process of embedding and extracting\nwatermark covertly on a carrier image. Incorporating deep learning networks\nwith image watermarking has attracted increasing attention during recent years.\nHowever, existing deep learning-based watermarking systems cannot achieve\nrobustness, blindness, and automated embedding and extraction simultaneously.\nIn this paper, a fully automated image watermarking system based on deep neural\nnetworks is proposed to generalize the image watermarking processes. An\nunsupervised deep learning structure and a novel loss computation are proposed\nto achieve high capacity and high robustness without any prior knowledge of\npossible attacks. Furthermore, a challenging application of watermark\nextraction from camera-captured images is provided to validate the practicality\nas well as the robustness of the proposed system. Experimental results show the\nsuperiority performance of the proposed system as comparing against several\ncurrently available techniques.\n", "versions": [{"version": "v1", "created": "Thu, 29 Aug 2019 16:24:29 GMT"}], "update_date": "2020-07-07", "authors_parsed": [["Zhong", "Xin", ""], ["Shih", "Frank Y.", ""]]}, {"id": "1908.11332", "submitter": "Junde Wu", "authors": "Junde Wu", "title": "Generating adversarial examples in the harsh conditions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Neural Networks have been found vulnerable re-cently. A kind of\nwell-designed inputs, which called adver-sarial examples, can lead the networks\nto make incorrectpredictions. Depending on the different scenarios, goalsand\ncapabilities, the difficulties of the attacks are different.For example, a\ntargeted attack is more difficult than a non-targeted attack, a universal\nattack is more difficult than anon-universal attack, a transferable attack is\nmore difficultthan a nontransferable one. The question is: Is there existan\nattack that can meet all these requirements? In this pa-per, we answer this\nquestion by producing a kind of attacksunder these conditions. We learn a\nuniversal mapping tomap the sources to the adversarial examples. These\nexam-ples can fool classification networks to classify all of theminto one\ntargeted class, and also have strong transferability.Our code is released at:\nxxxxx.\n", "versions": [{"version": "v1", "created": "Thu, 29 Aug 2019 16:27:24 GMT"}, {"version": "v2", "created": "Fri, 13 Sep 2019 15:17:55 GMT"}, {"version": "v3", "created": "Wed, 16 Dec 2020 21:25:14 GMT"}], "update_date": "2020-12-18", "authors_parsed": [["Wu", "Junde", ""]]}, {"id": "1908.11344", "submitter": "Xiangyang He", "authors": "Xiangyang He, Yubo Tao, Qirui Wang, Hai Lin", "title": "Multivariate Spatial Data Visualization: A Survey", "comments": "16 pages, 5 figures. Corresponding author: Yubo Tao", "journal-ref": "Journal of Visualization, (2019), 1-16", "doi": "10.1007/s12650-019-00584-3", "report-no": null, "categories": "cs.HC cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multivariate spatial data plays an important role in computational science\nand engineering simulations. The potential features and hidden relationships in\nmultivariate data can assist scientists to gain an in-depth understanding of a\nscientific process, verify a hypothesis and further discover a new physical or\nchemical law. In this paper, we present a comprehensive survey of the\nstate-of-the-art techniques for multivariate spatial data visualization. We\nfirst introduce the basic concept and characteristics of multivariate spatial\ndata, and describe three main tasks in multivariate data visualization: feature\nclassification, fusion visualization, and correlation analysis. Finally, we\nprospect potential research topics for multivariate data visualization\naccording to the current research.\n", "versions": [{"version": "v1", "created": "Sun, 18 Aug 2019 06:07:17 GMT"}], "update_date": "2019-08-30", "authors_parsed": [["He", "Xiangyang", ""], ["Tao", "Yubo", ""], ["Wang", "Qirui", ""], ["Lin", "Hai", ""]]}, {"id": "1908.11412", "submitter": "Utkarsh Mall", "authors": "Utkarsh Mall, Kevin Matzen, Bharath Hariharan, Noah Snavely, Kavita\n  Bala", "title": "GeoStyle: Discovering Fashion Trends and Events", "comments": "Accepted in ICCV 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding fashion styles and trends is of great potential interest to\nretailers and consumers alike. The photos people upload to social media are a\nhistorical and public data source of how people dress across the world and at\ndifferent times. While we now have tools to automatically recognize the\nclothing and style attributes of what people are wearing in these photographs,\nwe lack the ability to analyze spatial and temporal trends in these attributes\nor make predictions about the future. In this paper, we address this need by\nproviding an automatic framework that analyzes large corpora of street imagery\nto (a) discover and forecast long-term trends of various fashion attributes as\nwell as automatically discovered styles, and (b) identify spatio-temporally\nlocalized events that affect what people wear. We show that our framework makes\nlong term trend forecasts that are >20% more accurate than the prior art, and\nidentifies hundreds of socially meaningful events that impact fashion across\nthe globe.\n", "versions": [{"version": "v1", "created": "Thu, 29 Aug 2019 18:26:33 GMT"}], "update_date": "2019-09-02", "authors_parsed": [["Mall", "Utkarsh", ""], ["Matzen", "Kevin", ""], ["Hariharan", "Bharath", ""], ["Snavely", "Noah", ""], ["Bala", "Kavita", ""]]}, {"id": "1908.11415", "submitter": "Zelun Wang", "authors": "Zelun Wang, Jyh-Charn Liu", "title": "Translating Math Formula Images to LaTeX Sequences Using Deep Neural\n  Networks with Sequence-level Training", "comments": "11 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose a deep neural network model with an encoder-decoder\narchitecture that translates images of math formulas into their LaTeX markup\nsequences. The encoder is a convolutional neural network (CNN) that transforms\nimages into a group of feature maps. To better capture the spatial\nrelationships of math symbols, the feature maps are augmented with 2D\npositional encoding before being unfolded into a vector. The decoder is a\nstacked bidirectional long short-term memory (LSTM) model integrated with the\nsoft attention mechanism, which works as a language model to translate the\nencoder output into a sequence of LaTeX tokens. The neural network is trained\nin two steps. The first step is token-level training using the\nMaximum-Likelihood Estimation (MLE) as the objective function. At completion of\nthe token-level training, the sequence-level training objective function is\nemployed to optimize the overall model based on the policy gradient algorithm\nfrom reinforcement learning. Our design also overcomes the exposure bias\nproblem by closing the feedback loop in the decoder during sequence-level\ntraining, i.e., feeding in the predicted token instead of the ground truth\ntoken at every time step. The model is trained and evaluated on the\nIM2LATEX-100K dataset and shows state-of-the-art performance on both\nsequence-based and image-based evaluation metrics.\n", "versions": [{"version": "v1", "created": "Thu, 29 Aug 2019 18:33:21 GMT"}, {"version": "v2", "created": "Mon, 9 Sep 2019 19:09:42 GMT"}], "update_date": "2019-09-11", "authors_parsed": [["Wang", "Zelun", ""], ["Liu", "Jyh-Charn", ""]]}, {"id": "1908.11457", "submitter": "Giorgia Pitteri", "authors": "Giorgia Pitteri, Slobodan Ilic, Vincent Lepetit", "title": "CorNet: Generic 3D Corners for 6D Pose Estimation of New Objects without\n  Retraining", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel approach to the detection and 3D pose estimation of\nobjects in color images. Its main contribution is that it does not require any\ntraining phases nor data for new objects, while state-of-the-art methods\ntypically require hours of training time and hundreds of training registered\nimages. Instead, our method relies only on the objects' geometries. Our method\nfocuses on objects with prominent corners, which covers a large number of\nindustrial objects. We first learn to detect object corners of various shapes\nin images and also to predict their 3D poses, by using training images of a\nsmall set of objects. To detect a new object in a given image, we first\nidentify its corners from its CAD model; we also detect the corners visible in\nthe image and predict their 3D poses. We then introduce a RANSAC-like algorithm\nthat robustly and efficiently detects and estimates the object's 3D pose by\nmatching its corners on the CAD model with their detected counterparts in the\nimage. Because we also estimate the 3D poses of the corners in the image,\ndetecting only 1 or 2 corners is sufficient to estimate the pose of the object,\nwhich makes the approach robust to occlusions. We finally rely on a final check\nthat exploits the full 3D geometry of the objects, in case multiple objects\nhave the same corner spatial arrangement. The advantages of our approach make\nit particularly attractive for industrial contexts, and we demonstrate our\napproach on the challenging T-LESS dataset.\n", "versions": [{"version": "v1", "created": "Thu, 29 Aug 2019 21:17:09 GMT"}], "update_date": "2019-09-02", "authors_parsed": [["Pitteri", "Giorgia", ""], ["Ilic", "Slobodan", ""], ["Lepetit", "Vincent", ""]]}, {"id": "1908.11462", "submitter": "Liu Yang", "authors": "Liu Yang and George Em Karniadakis", "title": "Potential Flow Generator with $L_2$ Optimal Transport Regularity for\n  Generative Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a potential flow generator with $L_2$ optimal transport\nregularity, which can be easily integrated into a wide range of generative\nmodels including different versions of GANs and flow-based models. We show the\ncorrectness and robustness of the potential flow generator in several 2D\nproblems, and illustrate the concept of \"proximity\" due to the $L_2$ optimal\ntransport regularity. Subsequently, we demonstrate the effectiveness of the\npotential flow generator in image translation tasks with unpaired training data\nfrom the MNIST dataset and the CelebA dataset.\n", "versions": [{"version": "v1", "created": "Thu, 29 Aug 2019 22:00:49 GMT"}], "update_date": "2019-09-02", "authors_parsed": [["Yang", "Liu", ""], ["Karniadakis", "George Em", ""]]}, {"id": "1908.11472", "submitter": "Jean Mercat", "authors": "Jean Mercat, Nicole El Zoghby, Guillaume Sandou, Dominique Beauvois,\n  and Guillermo Pita Gil", "title": "Kinematic Single Vehicle Trajectory Prediction Baselines and\n  Applications with the NGSIM Dataset", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the recent vehicle trajectory prediction literature, the most common\nbaselines are briefly introduced without the necessary information to reproduce\nit. In this article we produce reproducible vehicle prediction results from\nsimple models. For that purpose, the process is explicit, and the code is\navailable. Those baseline models are a constant velocity model and a\nsingle-vehicle prediction model. They are applied on the NGSIM US-101 and I-80\ndatasets using only relative positions. Thus, the process can be reproduced\nwith any database containing tracking of vehicle positions. The evaluation\nreports Root Mean Squared Error (RMSE), Final Displacement Error (FDE),\nNegative Log-Likelihood (NLL), and Miss Rate (MR). The NLL estimation needs a\ncareful definition because several formulations that differ from the\nmathematical definition are used in other works. This article is meant to be\nused along with the published code to establish baselines for further work. An\nextension is proposed to replace the constant velocity assumption with a\nlearned model using a recurrent neural network. This brings good improvements\nin accuracy and uncertainty estimation and opens possibilities for both complex\nand interpretable models.\n", "versions": [{"version": "v1", "created": "Thu, 29 Aug 2019 22:38:28 GMT"}, {"version": "v2", "created": "Fri, 31 Jan 2020 14:46:48 GMT"}, {"version": "v3", "created": "Mon, 1 Jun 2020 12:39:15 GMT"}, {"version": "v4", "created": "Wed, 28 Oct 2020 12:44:17 GMT"}], "update_date": "2020-10-29", "authors_parsed": [["Mercat", "Jean", ""], ["Zoghby", "Nicole El", ""], ["Sandou", "Guillaume", ""], ["Beauvois", "Dominique", ""], ["Gil", "Guillermo Pita", ""]]}, {"id": "1908.11502", "submitter": "Kristina Monakhova", "authors": "Kristina Monakhova, Joshua Yurtsever, Grace Kuo, Nick Antipa, Kyrollos\n  Yanny, and Laura Waller", "title": "Learned reconstructions for practical mask-based lensless imaging", "comments": null, "journal-ref": null, "doi": "10.1364/OE.27.028075", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mask-based lensless imagers are smaller and lighter than traditional lensed\ncameras. In these imagers, the sensor does not directly record an image of the\nscene; rather, a computational algorithm reconstructs it. Typically, mask-based\nlensless imagers use a model-based reconstruction approach that suffers from\nlong compute times and a heavy reliance on both system calibration and\nheuristically chosen denoisers. In this work, we address these limitations\nusing a bounded-compute, trainable neural network to reconstruct the image. We\nleverage our knowledge of the physical system by unrolling a traditional\nmodel-based optimization algorithm, whose parameters we optimize using\nexperimentally gathered ground-truth data. Optionally, images produced by the\nunrolled network are then fed into a jointly-trained denoiser. As compared to\ntraditional methods, our architecture achieves better perceptual image quality\nand runs 20x faster, enabling interactive previewing of the scene. We explore a\nspectrum between model-based and deep learning methods, showing the benefits of\nusing an intermediate approach. Finally, we test our network on images taken in\nthe wild with a prototype mask-based camera, demonstrating that our network\ngeneralizes to natural images.\n", "versions": [{"version": "v1", "created": "Fri, 30 Aug 2019 01:45:05 GMT"}], "update_date": "2019-09-27", "authors_parsed": [["Monakhova", "Kristina", ""], ["Yurtsever", "Joshua", ""], ["Kuo", "Grace", ""], ["Antipa", "Nick", ""], ["Yanny", "Kyrollos", ""], ["Waller", "Laura", ""]]}, {"id": "1908.11505", "submitter": "Lan Xu", "authors": "Lan Xu, Weipeng Xu, Vladislav Golyanik, Marc Habermann, Lu Fang and\n  Christian Theobalt", "title": "EventCap: Monocular 3D Capture of High-Speed Human Motions using an\n  Event Camera", "comments": "10 pages, 11 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The high frame rate is a critical requirement for capturing fast human\nmotions. In this setting, existing markerless image-based methods are\nconstrained by the lighting requirement, the high data bandwidth and the\nconsequent high computation overhead. In this paper, we propose EventCap ---\nthe first approach for 3D capturing of high-speed human motions using a single\nevent camera. Our method combines model-based optimization and CNN-based human\npose detection to capture high-frequency motion details and to reduce the\ndrifting in the tracking. As a result, we can capture fast motions at\nmillisecond resolution with significantly higher data efficiency than using\nhigh frame rate videos. Experiments on our new event-based fast human motion\ndataset demonstrate the effectiveness and accuracy of our method, as well as\nits robustness to challenging lighting conditions.\n", "versions": [{"version": "v1", "created": "Fri, 30 Aug 2019 01:59:41 GMT"}], "update_date": "2019-09-02", "authors_parsed": [["Xu", "Lan", ""], ["Xu", "Weipeng", ""], ["Golyanik", "Vladislav", ""], ["Habermann", "Marc", ""], ["Fang", "Lu", ""], ["Theobalt", "Christian", ""]]}, {"id": "1908.11506", "submitter": "Akira Kudo", "authors": "Akira Kudo, Yoshiro Kitamura, Yuanzhong Li, Satoshi Iizuka, Edgar\n  Simo-Serra", "title": "Virtual Thin Slice: 3D Conditional GAN-based Super-resolution for CT\n  Slice Interval", "comments": "10 pages, 6 figures, Accepted to Machine Learning for Medical Image\n  Reconstruction (MLMIR) at MICCAI 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many CT slice images are stored with large slice intervals to reduce storage\nsize in clinical practice. This leads to low resolution perpendicular to the\nslice images (i.e., z-axis), which is insufficient for 3D visualization or\nimage analysis. In this paper, we present a novel architecture based on\nconditional Generative Adversarial Networks (cGANs) with the goal of generating\nhigh resolution images of main body parts including head, chest, abdomen and\nlegs. However, GANs are known to have a difficulty with generating a diversity\nof patterns due to a phenomena known as mode collapse. To overcome the lack of\ngenerated pattern variety, we propose to condition the discriminator on the\ndifferent body parts. Furthermore, our generator networks are extended to be\nthree dimensional fully convolutional neural networks, allowing for the\ngeneration of high resolution images from arbitrary fields of view. In our\nverification tests, we show that the proposed method obtains the best scores by\nPSNR/SSIM metrics and Visual Turing Test, allowing for accurate reproduction of\nthe principle anatomy in high resolution. We expect that the proposed method\ncontribute to effective utilization of the existing vast amounts of thick CT\nimages stored in hospitals.\n", "versions": [{"version": "v1", "created": "Fri, 30 Aug 2019 02:01:04 GMT"}, {"version": "v2", "created": "Mon, 2 Sep 2019 01:37:08 GMT"}], "update_date": "2019-09-04", "authors_parsed": [["Kudo", "Akira", ""], ["Kitamura", "Yoshiro", ""], ["Li", "Yuanzhong", ""], ["Iizuka", "Satoshi", ""], ["Simo-Serra", "Edgar", ""]]}, {"id": "1908.11525", "submitter": "Issam Hadj Laradji", "authors": "Lironne Kurzman, David Vazquez, Issam Laradji", "title": "Class-Based Styling: Real-time Localized Style Transfer with Semantic\n  Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a Class-Based Styling method (CBS) that can map different styles\nfor different object classes in real-time. CBS achieves real-time performance\nby carrying out two steps simultaneously. While a semantic segmentation method\nis used to obtain the mask of each object class in a video frame, a styling\nmethod is used to style that frame globally. Then an object class can be styled\nby combining the segmentation mask and the styled image. The user can also\nselect multiple styles so that different object classes can have different\nstyles in a single frame. For semantic segmentation, we leverage DABNet that\nachieves high accuracy, yet only has 0.76 million parameters and runs at 104\nFPS. For the style transfer step, we use a popular real-time method proposed by\nJohnson et al. [7]. We evaluated CBS on a video of the CityScapes dataset and\nobserved high-quality localized style transfer results for different object\nclasses and real-time performance.\n", "versions": [{"version": "v1", "created": "Fri, 30 Aug 2019 04:08:15 GMT"}], "update_date": "2019-09-02", "authors_parsed": [["Kurzman", "Lironne", ""], ["Vazquez", "David", ""], ["Laradji", "Issam", ""]]}, {"id": "1908.11526", "submitter": "Yuchao Dai Dr.", "authors": "Yuchao Dai, Zhidong Zhu, Zhibo Rao, Bo Li", "title": "MVS^2: Deep Unsupervised Multi-view Stereo with Multi-View Symmetry", "comments": "Accepted by International Conference on 3D Vision (3DV 2019) as ORAL\n  presentation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The success of existing deep-learning based multi-view stereo (MVS)\napproaches greatly depends on the availability of large-scale supervision in\nthe form of dense depth maps. Such supervision, while not always possible,\ntends to hinder the generalization ability of the learned models in\nnever-seen-before scenarios. In this paper, we propose the first unsupervised\nlearning based MVS network, which learns the multi-view depth maps from the\ninput multi-view images and does not need ground-truth 3D training data. Our\nnetwork is symmetric in predicting depth maps for all views simultaneously,\nwhere we enforce cross-view consistency of multi-view depth maps during both\ntraining and testing stages. Thus, the learned multi-view depth maps naturally\ncomply with the underlying 3D scene geometry. Besides, our network also learns\nthe multi-view occlusion maps, which further improves the robustness of our\nnetwork in handling real-world occlusions. Experimental results on multiple\nbenchmarking datasets demonstrate the effectiveness of our network and the\nexcellent generalization ability.\n", "versions": [{"version": "v1", "created": "Fri, 30 Aug 2019 04:09:02 GMT"}], "update_date": "2019-09-02", "authors_parsed": [["Dai", "Yuchao", ""], ["Zhu", "Zhidong", ""], ["Rao", "Zhibo", ""], ["Li", "Bo", ""]]}, {"id": "1908.11528", "submitter": "Kyungyul Kim", "authors": "Byeongmoon Ji, Hyemin Jung, Jihyeun Yoon, Kyungyul Kim, Younghak Shin", "title": "Bin-wise Temperature Scaling (BTS): Improvement in Confidence\n  Calibration Performance through Simple Scaling Techniques", "comments": null, "journal-ref": "ICCV 2019 Workshop on Interpreting and Explaining Visual\n  Artificial Intelligence Models", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The prediction reliability of neural networks is important in many\napplications. Specifically, in safety-critical domains, such as cancer\nprediction or autonomous driving, a reliable confidence of model's prediction\nis critical for the interpretation of the results. Modern deep neural networks\nhave achieved a significant improvement in performance for many different image\nclassification tasks. However, these networks tend to be poorly calibrated in\nterms of output confidence. Temperature scaling is an efficient\npost-processing-based calibration scheme and obtains well calibrated results.\nIn this study, we leverage the concept of temperature scaling to build a\nsophisticated bin-wise scaling. Furthermore, we adopt augmentation of\nvalidation samples for elaborated scaling. The proposed methods consistently\nimprove calibration performance with various datasets and deep convolutional\nneural network models.\n", "versions": [{"version": "v1", "created": "Fri, 30 Aug 2019 04:18:27 GMT"}, {"version": "v2", "created": "Mon, 23 Sep 2019 07:30:26 GMT"}], "update_date": "2019-09-24", "authors_parsed": [["Ji", "Byeongmoon", ""], ["Jung", "Hyemin", ""], ["Yoon", "Jihyeun", ""], ["Kim", "Kyungyul", ""], ["Shin", "Younghak", ""]]}, {"id": "1908.11542", "submitter": "Bo Chen", "authors": "Bo Chen, Jiewei Cao, Alvaro Parra, Tat-Jun Chin", "title": "Satellite Pose Estimation with Deep Landmark Regression and Nonlinear\n  Pose Refinement", "comments": "Accepted by ICCVW 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an approach to estimate the 6DOF pose of a satellite, relative to\na canonical pose, from a single image. Such a problem is crucial in many space\nproximity operations, such as docking, debris removal, and inter-spacecraft\ncommunications. Our approach combines machine learning and geometric\noptimisation, by predicting the coordinates of a set of landmarks in the input\nimage, associating the landmarks to their corresponding 3D points on an a\npriori reconstructed 3D model, then solving for the object pose using\nnon-linear optimisation. Our approach is not only novel for this specific pose\nestimation task, which helps to further open up a relatively new domain for\nmachine learning and computer vision, but it also demonstrates superior\naccuracy and won the first place in the recent Kelvins Pose Estimation\nChallenge organised by the European Space Agency (ESA).\n", "versions": [{"version": "v1", "created": "Fri, 30 Aug 2019 05:49:45 GMT"}], "update_date": "2019-09-02", "authors_parsed": [["Chen", "Bo", ""], ["Cao", "Jiewei", ""], ["Parra", "Alvaro", ""], ["Chin", "Tat-Jun", ""]]}, {"id": "1908.11550", "submitter": "Junyi Zou", "authors": "Junyi Zou, Jinliang Zhang, Ludi Wang", "title": "Handwritten Chinese Character Recognition by Convolutional Neural\n  Network and Similarity Ranking", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Convolution Neural Networks (CNN) have recently achieved state-of-the art\nperformance on handwritten Chinese character recognition (HCCR). However, most\nof CNN models employ the SoftMax activation function and minimize cross entropy\nloss, which may cause loss of inter-class information. To cope with this\nproblem, we propose to combine cross entropy with similarity ranking function\nand use it as loss function. The experiments results show that the combination\nloss functions produce higher accuracy in HCCR. This report briefly reviews\ncross entropy loss function, a typical similarity ranking function: Euclidean\ndistance, and also propose a new similarity ranking function: Average variance\nsimilarity. Experiments are done to compare the performances of a CNN model\nwith three different loss functions. In the end, SoftMax cross entropy with\nAverage variance similarity produce the highest accuracy on handwritten Chinese\ncharacters recognition.\n", "versions": [{"version": "v1", "created": "Fri, 30 Aug 2019 06:21:52 GMT"}], "update_date": "2019-09-02", "authors_parsed": [["Zou", "Junyi", ""], ["Zhang", "Jinliang", ""], ["Wang", "Ludi", ""]]}, {"id": "1908.11569", "submitter": "Arnab Kumar Mondal", "authors": "Arnab Kumar Mondal, Aniket Agarwal, Jose Dolz and Christian Desrosiers", "title": "Revisiting CycleGAN for semi-supervised segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we study the problem of training deep networks for semantic\nimage segmentation using only a fraction of annotated images, which may\nsignificantly reduce human annotation efforts. Particularly, we propose a\nstrategy that exploits the unpaired image style transfer capabilities of\nCycleGAN in semi-supervised segmentation. Unlike recent works using adversarial\nlearning for semi-supervised segmentation, we enforce cycle consistency to\nlearn a bidirectional mapping between unpaired images and segmentation masks.\nThis adds an unsupervised regularization effect that boosts the segmentation\nperformance when annotated data is limited. Experiments on three different\npublic segmentation benchmarks (PASCAL VOC 2012, Cityscapes and ACDC)\ndemonstrate the effectiveness of the proposed method. The proposed model\nachieves 2-4% of improvement with respect to the baseline and outperforms\nrecent approaches for this task, particularly in low labeled data regime.\n", "versions": [{"version": "v1", "created": "Fri, 30 Aug 2019 07:14:43 GMT"}], "update_date": "2019-09-02", "authors_parsed": [["Mondal", "Arnab Kumar", ""], ["Agarwal", "Aniket", ""], ["Dolz", "Jose", ""], ["Desrosiers", "Christian", ""]]}, {"id": "1908.11585", "submitter": "Richard Elvira", "authors": "Richard Elvira, Juan D. Tard\\'os and J.M.M. Montiel", "title": "ORBSLAM-Atlas: a robust and accurate multi-map system", "comments": "2018 IEEE/RSJ International Conference on Intelligent Robots and\n  Systems (IROS)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose ORBSLAM-Atlas, a system able to handle an unlimited number of\ndisconnected sub-maps, that includes a robust map merging algorithm able to\ndetect sub-maps with common regions and seamlessly fuse them. The outstanding\nrobustness and accuracy of ORBSLAM are due to its ability to detect\nwide-baseline matches between keyframes, and to exploit them by means of\nnon-linear optimization, however it only can handle a single map. ORBSLAM-Atlas\nbrings the wide-baseline matching detection and exploitation to the multiple\nmap arena. The result is a SLAM system significantly more general and robust,\nable to perform multi-session mapping. If tracking is lost during exploration,\ninstead of freezing the map, a new sub-map is launched, and it can be fused\nwith the previous map when common parts are visited. Our criteria to declare\nthe camera lost contrast with previous approaches that simply count the number\nof tracked points, we propose to discard also inaccurately estimated camera\nposes due to bad geometrical conditioning. As a result, the map is split into\nmore accurate sub-maps, that are eventually merged in a more accurate global\nmap, thanks to the multi-mapping capabilities.\n  We provide extensive experimental validation in the EuRoC datasets, where\nORBSLAM-Atlas obtains accurate monocular and stereo results in the difficult\nsequences where ORBSLAM failed. We also build global maps after multiple\nsessions in the same room, obtaining the best results to date, between 2 and 3\ntimes more accurate than competing multi-map approaches. We also show the\nrobustness and capability of our system to deal with dynamic scenes,\nquantitatively in the EuRoC datasets and qualitatively in a densely populated\ncorridor where camera occlusions and tracking losses are frequent.\n", "versions": [{"version": "v1", "created": "Fri, 30 Aug 2019 08:09:46 GMT"}], "update_date": "2019-09-02", "authors_parsed": [["Elvira", "Richard", ""], ["Tard\u00f3s", "Juan D.", ""], ["Montiel", "J. M. M.", ""]]}, {"id": "1908.11587", "submitter": "Sungho Lee", "authors": "Sungho Lee, Seoung Wug Oh, DaeYeun Won, Seon Joo Kim", "title": "Copy-and-Paste Networks for Deep Video Inpainting", "comments": "ICCV 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel deep learning based algorithm for video inpainting. Video\ninpainting is a process of completing corrupted or missing regions in videos.\nVideo inpainting has additional challenges compared to image inpainting due to\nthe extra temporal information as well as the need for maintaining the temporal\ncoherency. We propose a novel DNN-based framework called the Copy-and-Paste\nNetworks for video inpainting that takes advantage of additional information in\nother frames of the video. The network is trained to copy corresponding\ncontents in reference frames and paste them to fill the holes in the target\nframe. Our network also includes an alignment network that computes affine\nmatrices between frames for the alignment, enabling the network to take\ninformation from more distant frames for robustness. Our method produces\nvisually pleasing and temporally coherent results while running faster than the\nstate-of-the-art optimization-based method. In addition, we extend our\nframework for enhancing over/under exposed frames in videos. Using this\nenhancement technique, we were able to significantly improve the lane detection\naccuracy on road videos.\n", "versions": [{"version": "v1", "created": "Fri, 30 Aug 2019 08:11:06 GMT"}], "update_date": "2019-09-02", "authors_parsed": [["Lee", "Sungho", ""], ["Oh", "Seoung Wug", ""], ["Won", "DaeYeun", ""], ["Kim", "Seon Joo", ""]]}, {"id": "1908.11602", "submitter": "Xudong Xu", "authors": "Xudong Xu, Bo Dai, Dahua Lin", "title": "Recursive Visual Sound Separation Using Minus-Plus Net", "comments": "accepted by ICCV2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sounds provide rich semantics, complementary to visual data, for many tasks.\nHowever, in practice, sounds from multiple sources are often mixed together. In\nthis paper we propose a novel framework, referred to as MinusPlus Network\n(MP-Net), for the task of visual sound separation. MP-Net separates sounds\nrecursively in the order of average energy, removing the separated sound from\nthe mixture at the end of each prediction, until the mixture becomes empty or\ncontains only noise. In this way, MP-Net could be applied to sound mixtures\nwith arbitrary numbers and types of sounds. Moreover, while MP-Net keeps\nremoving sounds with large energy from the mixture, sounds with small energy\ncould emerge and become clearer, so that the separation is more accurate.\nCompared to previous methods, MP-Net obtains state-of-the-art results on two\nlarge scale datasets, across mixtures with different types and numbers of\nsounds.\n", "versions": [{"version": "v1", "created": "Fri, 30 Aug 2019 09:05:26 GMT"}, {"version": "v2", "created": "Wed, 23 Oct 2019 07:32:46 GMT"}], "update_date": "2019-10-24", "authors_parsed": [["Xu", "Xudong", ""], ["Dai", "Bo", ""], ["Lin", "Dahua", ""]]}, {"id": "1908.11618", "submitter": "Chenhao Wang", "authors": "Chenhao Wang", "title": "Multi-Grained Spatio-temporal Modeling for Lip-reading", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Lip-reading aims to recognize speech content from videos via visual analysis\nof speakers' lip movements. This is a challenging task due to the existence of\nhomophemes-words which involve identical or highly similar lip movements, as\nwell as diverse lip appearances and motion patterns among the speakers. To\naddress these challenges, we propose a novel lip-reading model which captures\nnot only the nuance between words but also styles of different speakers, by a\nmulti-grained spatio-temporal modeling of the speaking process. Specifically,\nwe first extract both frame-level fine-grained features and short-term\nmedium-grained features by the visual front-end, which are then combined to\nobtain discriminative representations for words with similar phonemes. Next, a\nbidirectional ConvLSTM augmented with temporal attention aggregates\nspatio-temporal information in the entire input sequence, which is expected to\nbe able to capture the coarse-gained patterns of each word and robust to\nvarious conditions in speaker identity, lighting conditions, and so on. By\nmaking full use of the information from different levels in a unified\nframework, the model is not only able to distinguish words with similar\npronunciations, but also becomes robust to appearance changes. We evaluate our\nmethod on two challenging word-level lip-reading benchmarks and show the\neffectiveness of the proposed method, which also demonstrate the above claims.\n", "versions": [{"version": "v1", "created": "Fri, 30 Aug 2019 09:50:20 GMT"}, {"version": "v2", "created": "Mon, 2 Sep 2019 02:22:32 GMT"}], "update_date": "2019-09-04", "authors_parsed": [["Wang", "Chenhao", ""]]}, {"id": "1908.11624", "submitter": "Jeremy Tan", "authors": "Jeremy Tan, Anselm Au, Qingjie Meng, Bernhard Kainz", "title": "Semi-supervised Learning of Fetal Anatomy from Ultrasound", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semi-supervised learning methods have achieved excellent performance on\nstandard benchmark datasets using very few labelled images. Anatomy\nclassification in fetal 2D ultrasound is an ideal problem setting to test\nwhether these results translate to non-ideal data. Our results indicate that\ninclusion of a challenging background class can be detrimental and that\nsemi-supervised learning mostly benefits classes that are already distinct,\nsometimes at the expense of more similar classes.\n", "versions": [{"version": "v1", "created": "Fri, 30 Aug 2019 10:03:32 GMT"}], "update_date": "2019-09-02", "authors_parsed": [["Tan", "Jeremy", ""], ["Au", "Anselm", ""], ["Meng", "Qingjie", ""], ["Kainz", "Bernhard", ""]]}, {"id": "1908.11628", "submitter": "Sagie Benaim", "authors": "Sagie Benaim, Michael Khaitov, Tomer Galanti, Lior Wolf", "title": "Domain Intersection and Domain Difference", "comments": null, "journal-ref": "ICCV 2019", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a method for recovering the shared content between two visual\ndomains as well as the content that is unique to each domain. This allows us to\nmap from one domain to the other, in a way in which the content that is\nspecific for the first domain is removed and the content that is specific for\nthe second is imported from any image in the second domain. In addition, our\nmethod enables generation of images from the intersection of the two domains as\nwell as their union, despite having no such samples during training. The method\nis shown analytically to contain all the sufficient and necessary constraints.\nIt also outperforms the literature methods in an extensive set of experiments.\nOur code is available at\nhttps://github.com/sagiebenaim/DomainIntersectionDifference.\n", "versions": [{"version": "v1", "created": "Fri, 30 Aug 2019 10:08:43 GMT"}], "update_date": "2019-09-02", "authors_parsed": [["Benaim", "Sagie", ""], ["Khaitov", "Michael", ""], ["Galanti", "Tomer", ""], ["Wolf", "Lior", ""]]}, {"id": "1908.11645", "submitter": "Lukas Cavigelli", "authors": "Lukas Cavigelli, Georg Rutishauser, Luca Benini", "title": "EBPC: Extended Bit-Plane Compression for Deep Neural Network Inference\n  and Training Accelerators", "comments": "arXiv admin note: substantial text overlap with arXiv:1810.03979", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AR eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the wake of the success of convolutional neural networks in image\nclassification, object recognition, speech recognition, etc., the demand for\ndeploying these compute-intensive ML models on embedded and mobile systems with\ntight power and energy constraints at low cost, as well as for boosting\nthroughput in data centers, is growing rapidly. This has sparked a surge of\nresearch into specialized hardware accelerators. Their performance is typically\nlimited by I/O bandwidth, power consumption is dominated by I/O transfers to\noff-chip memory, and on-chip memories occupy a large part of the silicon area.\nWe introduce and evaluate a novel, hardware-friendly, and lossless compression\nscheme for the feature maps present within convolutional neural networks. We\npresent hardware architectures and synthesis results for the compressor and\ndecompressor in 65nm. With a throughput of one 8-bit word/cycle at 600MHz, they\nfit into 2.8kGE and 3.0kGE of silicon area, respectively - together the size of\nless than seven 8-bit multiply-add units at the same throughput. We show that\nan average compression ratio of 5.1x for AlexNet, 4x for VGG-16, 2.4x for\nResNet-34 and 2.2x for MobileNetV2 can be achieved - a gain of 45-70% over\nexisting methods. Our approach also works effectively for various number\nformats, has a low frame-to-frame variance on the compression ratio, and\nachieves compression factors for gradient map compression during training that\nare even better than for inference.\n", "versions": [{"version": "v1", "created": "Fri, 30 Aug 2019 10:47:31 GMT"}, {"version": "v2", "created": "Fri, 25 Oct 2019 14:47:32 GMT"}], "update_date": "2019-11-12", "authors_parsed": [["Cavigelli", "Lukas", ""], ["Rutishauser", "Georg", ""], ["Benini", "Luca", ""]]}, {"id": "1908.11656", "submitter": "Pierre Biasutti", "authors": "Pierre Biasutti, Vincent Lepetit, Jean-Fran\\c{c}ois Aujol, Mathieu\n  Br\\'edif and Aur\\'elie Bugeau", "title": "LU-Net: An Efficient Network for 3D LiDAR Point Cloud Semantic\n  Segmentation Based on End-to-End-Learned 3D Features and U-Net", "comments": "9 pages, 9 figures. arXiv admin note: substantial text overlap with\n  arXiv:1905.08748", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose LU-Net -- for LiDAR U-Net, a new method for the semantic\nsegmentation of a 3D LiDAR point cloud. Instead of applying some global 3D\nsegmentation method such as PointNet, we propose an end-to-end architecture for\nLiDAR point cloud semantic segmentation that efficiently solves the problem as\nan image processing problem. We first extract high-level 3D features for each\npoint given its 3D neighbors. Then, these features are projected into a 2D\nmultichannel range-image by considering the topology of the sensor. Thanks to\nthese learned features and this projection, we can finally perform the\nsegmentation using a simple U-Net segmentation network, which performs very\nwell while being very efficient. In this way, we can exploit both the 3D nature\nof the data and the specificity of the LiDAR sensor. This approach outperforms\nthe state-of-the-art by a large margin on the KITTI dataset, as our experiments\nshow. Moreover, this approach operates at 24fps on a single GPU. This is above\nthe acquisition rate of common LiDAR sensors which makes it suitable for\nreal-time applications.\n", "versions": [{"version": "v1", "created": "Fri, 30 Aug 2019 11:28:59 GMT"}], "update_date": "2019-09-02", "authors_parsed": [["Biasutti", "Pierre", ""], ["Lepetit", "Vincent", ""], ["Aujol", "Jean-Fran\u00e7ois", ""], ["Br\u00e9dif", "Mathieu", ""], ["Bugeau", "Aur\u00e9lie", ""]]}, {"id": "1908.11675", "submitter": "Minjie Hua", "authors": "Minjie Hua, Yibing Nan and Shiguo Lian", "title": "Small Obstacle Avoidance Based on RGB-D Semantic Segmentation", "comments": "Accepted by CVRSUAD 2019 (ICCV Workshop)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a novel obstacle avoidance system for road robots\nequipped with RGB-D sensor that captures scenes of its way forward. The purpose\nof the system is to have road robots move around autonomously and constantly\nwithout any collision even with small obstacles, which are often missed by\nexisting solutions. For each input RGB-D image, the system uses a new two-stage\nsemantic segmentation network followed by the morphological processing to\ngenerate the accurate semantic map containing road and obstacles. Based on the\nmap, the local path planning is applied to avoid possible collision.\nAdditionally, optical flow supervision and motion blurring augmented training\nscheme is applied to improve temporal consistency between adjacent frames and\novercome the disturbance caused by camera shake. Various experiments are\nconducted to show that the proposed architecture obtains high performance both\nin indoor and outdoor scenarios.\n", "versions": [{"version": "v1", "created": "Fri, 30 Aug 2019 12:10:15 GMT"}], "update_date": "2019-09-02", "authors_parsed": [["Hua", "Minjie", ""], ["Nan", "Yibing", ""], ["Lian", "Shiguo", ""]]}, {"id": "1908.11676", "submitter": "Roman Bachmann", "authors": "Roman Bachmann, J\\\"org Sp\\\"orri, Pascal Fua, Helge Rhodin", "title": "Motion Capture from Pan-Tilt Cameras with Unknown Orientation", "comments": "International Conference on 3D Vision 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In sports, such as alpine skiing, coaches would like to know the speed and\nvarious biomechanical variables of their athletes and competitors. Existing\nmethods use either body-worn sensors, which are cumbersome to setup, or manual\nimage annotation, which is time consuming. We propose a method for estimating\nan athlete's global 3D position and articulated pose using multiple cameras. By\ncontrast to classical markerless motion capture solutions, we allow cameras to\nrotate freely so that large capture volumes can be covered. In a first step,\ntight crops around the skier are predicted and fed to a 2D pose estimator\nnetwork. The 3D pose is then reconstructed using a bundle adjustment method.\nKey to our solution is the rotation estimation of Pan-Tilt cameras in a joint\noptimization with the athlete pose and conditioning on relative background\nmotion computed with feature tracking. Furthermore, we created a new alpine\nskiing dataset and annotated it with 2D pose labels, to overcome shortcomings\nof existing ones. Our method estimates accurate global 3D poses from images\nonly and provides coaches with an automatic and fast tool for measuring and\nimproving an athlete's performance.\n", "versions": [{"version": "v1", "created": "Fri, 30 Aug 2019 12:12:54 GMT"}], "update_date": "2019-09-02", "authors_parsed": [["Bachmann", "Roman", ""], ["Sp\u00f6rri", "J\u00f6rg", ""], ["Fua", "Pascal", ""], ["Rhodin", "Helge", ""]]}, {"id": "1908.11694", "submitter": "Adam Pantanowitz", "authors": "Adam Pantanowitz, Emmanuel Cohen, Philippe Gradidge, Nigel Crowther,\n  Vered Aharonson, Benjamin Rosman, David M Rubin", "title": "Estimation of Body Mass Index from Photographs using Deep Convolutional\n  Neural Networks", "comments": "7 pages, 4 figures, preprint journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Obesity is an important concern in public health, and Body Mass Index is one\nof the useful (and proliferant) measures. We use Convolutional Neural Networks\nto determine Body Mass Index from photographs in a study with 161 participants.\nLow data, a common problem in medicine, is addressed by reducing the\ninformation in the photographs by generating silhouette images. Results present\nwith high correlation when tested on unseen data.\n", "versions": [{"version": "v1", "created": "Thu, 29 Aug 2019 10:33:26 GMT"}], "update_date": "2019-09-02", "authors_parsed": [["Pantanowitz", "Adam", ""], ["Cohen", "Emmanuel", ""], ["Gradidge", "Philippe", ""], ["Crowther", "Nigel", ""], ["Aharonson", "Vered", ""], ["Rosman", "Benjamin", ""], ["Rubin", "David M", ""]]}, {"id": "1908.11714", "submitter": "Lichao Zhang", "authors": "Lichao Zhang, Martin Danelljan, Abel Gonzalez-Garcia, Joost van de\n  Weijer, Fahad Shahbaz Khan", "title": "Multi-Modal Fusion for End-to-End RGB-T Tracking", "comments": "Accepted at ICCVW (VOT) 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an end-to-end tracking framework for fusing the RGB and TIR\nmodalities in RGB-T tracking. Our baseline tracker is DiMP (Discriminative\nModel Prediction), which employs a carefully designed target prediction network\ntrained end-to-end using a discriminative loss. We analyze the effectiveness of\nmodality fusion in each of the main components in DiMP, i.e. feature extractor,\ntarget estimation network, and classifier. We consider several fusion\nmechanisms acting at different levels of the framework, including pixel-level,\nfeature-level and response-level. Our tracker is trained in an end-to-end\nmanner, enabling the components to learn how to fuse the information from both\nmodalities. As data to train our model, we generate a large-scale RGB-T dataset\nby considering an annotated RGB tracking dataset (GOT-10k) and synthesizing\npaired TIR images using an image-to-image translation approach. We perform\nextensive experiments on VOT-RGBT2019 dataset and RGBT210 dataset, evaluating\neach type of modality fusing on each model component. The results show that the\nproposed fusion mechanisms improve the performance of the single modality\ncounterparts. We obtain our best results when fusing at the feature-level on\nboth the IoU-Net and the model predictor, obtaining an EAO score of 0.391 on\nVOT-RGBT2019 dataset. With this fusion mechanism we achieve the\nstate-of-the-art performance on RGBT210 dataset.\n", "versions": [{"version": "v1", "created": "Fri, 30 Aug 2019 12:58:10 GMT"}], "update_date": "2019-09-02", "authors_parsed": [["Zhang", "Lichao", ""], ["Danelljan", "Martin", ""], ["Gonzalez-Garcia", "Abel", ""], ["van de Weijer", "Joost", ""], ["Khan", "Fahad Shahbaz", ""]]}, {"id": "1908.11722", "submitter": "Preslav Nakov", "authors": "Dimitrina Zlatkova, Preslav Nakov, Ivan Koychev", "title": "Fact-Checking Meets Fauxtography: Verifying Claims About Images", "comments": "Claims about Images; Fauxtography; Fact-Checking; Veracity; Fake News", "journal-ref": "EMNLP-2019", "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.CV cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The recent explosion of false claims in social media and on the Web in\ngeneral has given rise to a lot of manual fact-checking initiatives.\nUnfortunately, the number of claims that need to be fact-checked is several\norders of magnitude larger than what humans can handle manually. Thus, there\nhas been a lot of research aiming at automating the process. Interestingly,\nprevious work has largely ignored the growing number of claims about images.\nThis is despite the fact that visual imagery is more influential than text and\nnaturally appears alongside fake news. Here we aim at bridging this gap. In\nparticular, we create a new dataset for this problem, and we explore a variety\nof features modeling the claim, the image, and the relationship between the\nclaim and the image. The evaluation results show sizable improvements over the\nbaseline. We release our dataset, hoping to enable further research on\nfact-checking claims about images.\n", "versions": [{"version": "v1", "created": "Fri, 30 Aug 2019 13:12:21 GMT"}], "update_date": "2019-09-02", "authors_parsed": [["Zlatkova", "Dimitrina", ""], ["Nakov", "Preslav", ""], ["Koychev", "Ivan", ""]]}, {"id": "1908.11754", "submitter": "Zhanghui Kuang", "authors": "Zhanghui Kuang, Yiming Gao, Guanbin Li, Ping Luo, Yimin Chen, Liang\n  Lin and Wayne Zhang", "title": "Fashion Retrieval via Graph Reasoning Networks on a Similarity Pyramid", "comments": "ICCV 2019 (oral)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Matching clothing images from customers and online shopping stores has rich\napplications in E-commerce. Existing algorithms encoded an image as a global\nfeature vector and performed retrieval with the global representation. However,\ndiscriminative local information on clothes are submerged in this global\nrepresentation, resulting in sub-optimal performance. To address this issue, we\npropose a novel Graph Reasoning Network (GRNet) on a Similarity Pyramid, which\nlearns similarities between a query and a gallery cloth by using both global\nand local representations in multiple scales. The similarity pyramid is\nrepresented by a Graph of similarity, where nodes represent similarities\nbetween clothing components at different scales, and the final matching score\nis obtained by message passing along edges. In GRNet, graph reasoning is solved\nby training a graph convolutional network, enabling to align salient clothing\ncomponents to improve clothing retrieval. To facilitate future researches, we\nintroduce a new benchmark FindFashion, containing rich annotations of bounding\nboxes, views, occlusions, and cropping. Extensive experiments show that GRNet\nobtains new state-of-the-art results on two challenging benchmarks, e.g.,\npushing the top-1, top-20, and top-50 accuracies on DeepFashion to 26%, 64%,\nand 75% (i.e., 4%, 10%, and 10% absolute improvements), outperforming\ncompetitors with large margins. On FindFashion, GRNet achieves considerable\nimprovements on all empirical settings.\n", "versions": [{"version": "v1", "created": "Fri, 30 Aug 2019 14:19:24 GMT"}], "update_date": "2019-09-02", "authors_parsed": [["Kuang", "Zhanghui", ""], ["Gao", "Yiming", ""], ["Li", "Guanbin", ""], ["Luo", "Ping", ""], ["Chen", "Yimin", ""], ["Lin", "Liang", ""], ["Zhang", "Wayne", ""]]}, {"id": "1908.11757", "submitter": "Javad Zolfaghari Bengar", "authors": "Javad Zolfaghari Bengar, Abel Gonzalez-Garcia, Gabriel Villalonga,\n  Bogdan Raducanu, Hamed H. Aghdam, Mikhail Mozerov, Antonio M. Lopez, Joost\n  van de Weijer", "title": "Temporal Coherence for Active Learning in Videos", "comments": "Accepted at ICCVW 2019 (CVRSUAD-Road Scene Understanding and\n  Autonomous Driving)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Autonomous driving systems require huge amounts of data to train. Manual\nannotation of this data is time-consuming and prohibitively expensive since it\ninvolves human resources. Therefore, active learning emerged as an alternative\nto ease this effort and to make data annotation more manageable. In this paper,\nwe introduce a novel active learning approach for object detection in videos by\nexploiting temporal coherence. Our active learning criterion is based on the\nestimated number of errors in terms of false positives and false negatives. The\ndetections obtained by the object detector are used to define the nodes of a\ngraph and tracked forward and backward to temporally link the nodes. Minimizing\nan energy function defined on this graphical model provides estimates of both\nfalse positives and false negatives. Additionally, we introduce a synthetic\nvideo dataset, called SYNTHIA-AL, specially designed to evaluate active\nlearning for video object detection in road scenes. Finally, we show that our\napproach outperforms active learning baselines tested on two datasets.\n", "versions": [{"version": "v1", "created": "Fri, 30 Aug 2019 14:20:36 GMT"}], "update_date": "2019-09-02", "authors_parsed": [["Bengar", "Javad Zolfaghari", ""], ["Gonzalez-Garcia", "Abel", ""], ["Villalonga", "Gabriel", ""], ["Raducanu", "Bogdan", ""], ["Aghdam", "Hamed H.", ""], ["Mozerov", "Mikhail", ""], ["Lopez", "Antonio M.", ""], ["van de Weijer", "Joost", ""]]}, {"id": "1908.11789", "submitter": "Senthil Yogamani", "authors": "Marie Yahiaoui, Hazem Rashed, Letizia Mariotti, Ganesh Sistu, Ian\n  Clancy, Lucie Yahiaoui, Varun Ravi Kumar and Senthil Yogamani", "title": "FisheyeMODNet: Moving Object detection on Surround-view Cameras for\n  Autonomous Driving", "comments": "Accepted for ICCV 2019 Workshop on 360{\\deg} Perception and\n  Interaction. A shorter version was presented at IMVIP 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Moving Object Detection (MOD) is an important task for achieving robust\nautonomous driving. An autonomous vehicle has to estimate collision risk with\nother interacting objects in the environment and calculate an optional\ntrajectory. Collision risk is typically higher for moving objects than static\nones due to the need to estimate the future states and poses of the objects for\ndecision making. This is particularly important for near-range objects around\nthe vehicle which are typically detected by a fisheye surround-view system that\ncaptures a 360{\\deg} view of the scene. In this work, we propose a CNN\narchitecture for moving object detection using fisheye images that were\ncaptured in autonomous driving environment. As motion geometry is highly\nnon-linear and unique for fisheye cameras, we will make an improved version of\nthe current dataset public to encourage further research. To target embedded\ndeployment, we design a lightweight encoder sharing weights across sequential\nimages. The proposed network runs at 15 fps on a 1 teraflops automotive\nembedded system at accuracy of 40% IoU and 69.5% mIoU.\n", "versions": [{"version": "v1", "created": "Fri, 30 Aug 2019 15:29:46 GMT"}], "update_date": "2019-09-02", "authors_parsed": [["Yahiaoui", "Marie", ""], ["Rashed", "Hazem", ""], ["Mariotti", "Letizia", ""], ["Sistu", "Ganesh", ""], ["Clancy", "Ian", ""], ["Yahiaoui", "Lucie", ""], ["Kumar", "Varun Ravi", ""], ["Yogamani", "Senthil", ""]]}, {"id": "1908.11799", "submitter": "Qinghui Liu", "authors": "Qinghui Liu, Michael Kampffmeyer, Robert Jenssen, Arnt-B{\\o}rre\n  Salberg", "title": "Dense Dilated Convolutions Merging Network for Semantic Mapping of\n  Remote Sensing Images", "comments": "JURSE 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a network for semantic mapping called the Dense Dilated\nConvolutions Merging Network (DDCM-Net) to provide a deep learning approach\nthat can recognize multi-scale and complex shaped objects with similar color\nand textures, such as buildings, surfaces/roads, and trees in very high\nresolution remote sensing images. The proposed DDCM-Net consists of dense\ndilated convolutions merged with varying dilation rates. This can effectively\nenlarge the kernels' receptive fields, and, more importantly, obtain fused\nlocal and global context information to promote surrounding discriminative\ncapability. We demonstrate the effectiveness of the proposed DDCM-Net on the\npublicly available ISPRS Potsdam dataset and achieve a performance of 92.3%\nF1-score and 86.0% mean intersection over union accuracy by only using the RGB\nbands, without any post-processing. We also show results on the ISPRS Vaihingen\ndataset, where the DDCM-Net trained with IRRG bands, also obtained better\nmapping accuracy (89.8% F1-score) than previous state-of-the-art approaches.\n", "versions": [{"version": "v1", "created": "Fri, 30 Aug 2019 15:47:15 GMT"}], "update_date": "2019-09-02", "authors_parsed": [["Liu", "Qinghui", ""], ["Kampffmeyer", "Michael", ""], ["Jenssen", "Robert", ""], ["Salberg", "Arnt-B\u00f8rre", ""]]}, {"id": "1908.11820", "submitter": "Mohammadreza Mostajabi", "authors": "Mohammadreza Mostajabi", "title": "Learning Rich Representations For Structured Visual Prediction Tasks", "comments": "PhD Thesis", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe an approach to learning rich representations for images, that\nenables simple and effective predictors in a range of vision tasks involving\nspatially structured maps. Our key idea is to map small image elements to\nfeature representations extracted from a sequence of nested regions of\nincreasing spatial extent. These regions are obtained by \"zooming out\" from the\npixel/superpixel all the way to scene-level resolution, and hence we call these\nzoom-out features. Applied to semantic segmentation and other structured\nprediction tasks, our approach exploits statistical structure in the image and\nin the label space without setting up explicit structured prediction\nmechanisms, and thus avoids complex and expensive inference. Instead image\nelements are classified by a feedforward multilayer network with skip-layer\nconnections spanning the zoom-out levels. When used in conjunction with modern\nneural architectures such as ResNet, DenseNet and NASNet (to which it is\ncomplementary) our approach achieves competitive accuracy on segmentation\nbenchmarks.\n  In addition, we propose an approach for learning category-level semantic\nsegmentation purely from image-level classification tag. It exploits\nlocalization cues that emerge from training a modified zoom-out architecture\ntailored for classification tasks, to drive a weakly supervised process that\nautomatically labels a sparse, diverse training set of points likely to belong\nto classes of interest. Finally, we introduce data-driven regularization\nfunctions for the supervised training of CNNs. Our innovation takes the form of\na regularizer derived by learning an autoencoder over the set of annotations.\nThis approach leverages an improved representation of label space to inform\nextraction of features from images\n", "versions": [{"version": "v1", "created": "Fri, 30 Aug 2019 16:18:26 GMT"}], "update_date": "2019-09-02", "authors_parsed": [["Mostajabi", "Mohammadreza", ""]]}, {"id": "1908.11821", "submitter": "Lei Jiang", "authors": "Lei Jiang Xiao-Jun Wu Josef Kittler", "title": "Dual Attention MobDenseNet(DAMDNet) for Robust 3D Face Alignment", "comments": "10 pages", "journal-ref": "ICCV2019 workshop", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  3D face alignment of monocular images is a crucial process in the recognition\nof faces with disguise.3D face reconstruction facilitated by alignment can\nrestore the face structure which is helpful in detcting disguise\ninterference.This paper proposes a dual attention mechanism and an efficient\nend-to-end 3D face alignment framework.We build a stable network model through\nDepthwise Separable Convolution, Densely Connected Convolutional and\nLightweight Channel Attention Mechanism. In order to enhance the ability of the\nnetwork model to extract the spatial features of the face region, we adopt\nSpatial Group-wise Feature enhancement module to improve the representation\nability of the network. Different loss functions are applied jointly to\nconstrain the 3D parameters of a 3D Morphable Model (3DMM) and its 3D vertices.\nWe use a variety of data enhancement methods and generate large virtual pose\nface data sets to solve the data imbalance problem. The experiments on the\nchallenging AFLW,AFLW2000-3D datasets show that our algorithm significantly\nimproves the accuracy of 3D face alignment. Our experiments using the field DFW\ndataset show that DAMDNet exhibits excellent performance in the 3D alignment\nand reconstruction of challenging disguised faces.The model parameters and the\ncomplexity of the proposed method are also reduced significantly.The code is\npublicly available at https:// github.com/LeiJiangJNU/DAMDNet\n", "versions": [{"version": "v1", "created": "Fri, 30 Aug 2019 16:20:08 GMT"}], "update_date": "2019-09-02", "authors_parsed": [["Kittler", "Lei Jiang Xiao-Jun Wu Josef", ""]]}, {"id": "1908.11822", "submitter": "Ananya Gupta", "authors": "Ananya Gupta, Yao Peng, Simon Watson and Hujun Yin", "title": "Multi-Temporal Aerial Image Registration Using Semantic Features", "comments": "Accepted to 20th International Conference on Intelligent Data\n  Engineering and Automated Learning (IDEAL)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A semantic feature extraction method for multitemporal high resolution aerial\nimage registration is proposed in this paper. These features encode properties\nor information about temporally invariant objects such as roads and help deal\nwith issues such as changing foliage in image registration, which classical\nhandcrafted features are unable to address. These features are extracted from a\nsemantic segmentation network and have shown good robustness and accuracy in\nregistering aerial images across years and seasons in the experiments.\n", "versions": [{"version": "v1", "created": "Fri, 30 Aug 2019 16:20:28 GMT"}, {"version": "v2", "created": "Thu, 19 Sep 2019 13:23:35 GMT"}], "update_date": "2019-09-20", "authors_parsed": [["Gupta", "Ananya", ""], ["Peng", "Yao", ""], ["Watson", "Simon", ""], ["Yin", "Hujun", ""]]}, {"id": "1908.11824", "submitter": "Lei Ke", "authors": "Lei Ke, Wenjie Pei, Ruiyu Li, Xiaoyong Shen, Yu-Wing Tai", "title": "Reflective Decoding Network for Image Captioning", "comments": "ICCV 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  State-of-the-art image captioning methods mostly focus on improving visual\nfeatures, less attention has been paid to utilizing the inherent properties of\nlanguage to boost captioning performance. In this paper, we show that\nvocabulary coherence between words and syntactic paradigm of sentences are also\nimportant to generate high-quality image caption. Following the conventional\nencoder-decoder framework, we propose the Reflective Decoding Network (RDN) for\nimage captioning, which enhances both the long-sequence dependency and position\nperception of words in a caption decoder. Our model learns to collaboratively\nattend on both visual and textual features and meanwhile perceive each word's\nrelative position in the sentence to maximize the information delivered in the\ngenerated caption. We evaluate the effectiveness of our RDN on the COCO image\ncaptioning datasets and achieve superior performance over the previous methods.\nFurther experiments reveal that our approach is particularly advantageous for\nhard cases with complex scenes to describe by captions.\n", "versions": [{"version": "v1", "created": "Fri, 30 Aug 2019 16:25:55 GMT"}], "update_date": "2019-09-02", "authors_parsed": [["Ke", "Lei", ""], ["Pei", "Wenjie", ""], ["Li", "Ruiyu", ""], ["Shen", "Xiaoyong", ""], ["Tai", "Yu-Wing", ""]]}, {"id": "1908.11834", "submitter": "Shangbang Long", "authors": "Shangbang Long, Yushuo Guan, Bingxuan Wang, Kaigui Bian, Cong Yao", "title": "Rethinking Irregular Scene Text Recognition", "comments": "Technical report for participation in ICDAR2019-ArT recognition track", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reading text from natural images is challenging due to the great variety in\ntext font, color, size, complex background and etc.. The perspective distortion\nand non-linear spatial arrangement of characters make it further difficult.\nWhile rectification based method is intuitively grounded and has pushed the\nenvelope by far, its potential is far from being well exploited. In this paper,\nwe present a bag of tricks that prove to significantly improve the performance\nof rectification based method. On curved text dataset, our method achieves an\naccuracy of 89.6% on CUTE-80 and 76.3% on Total-Text, an improvement over\nprevious state-of-the-art by 6.3% and 14.7% respectively. Furthermore, our\ncombination of tricks helps us win the ICDAR 2019 Arbitrary-Shaped Text\nChallenge (Latin script), achieving an accuracy of 74.3% on the held-out test\nset. We release our code as well as data samples for further exploration at\nhttps://github.com/Jyouhou/ICDAR2019-ArT-Recognition-Alchemy\n", "versions": [{"version": "v1", "created": "Fri, 30 Aug 2019 16:47:08 GMT"}, {"version": "v2", "created": "Mon, 11 Nov 2019 17:25:43 GMT"}], "update_date": "2019-11-12", "authors_parsed": [["Long", "Shangbang", ""], ["Guan", "Yushuo", ""], ["Wang", "Bingxuan", ""], ["Bian", "Kaigui", ""], ["Yao", "Cong", ""]]}, {"id": "1908.11863", "submitter": "Rohan Akut Mr", "authors": "Rohan Akut, Sumukh Marathe, Rucha Apte, Ishan Joshi, Siddhivinayak\n  Kulkarni", "title": "Systematic Analysis of Image Generation using GANs", "comments": "Accepted in IEEE ICMLDS 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV eess.IV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative Adversarial Networks have been crucial in the developments made in\nunsupervised learning in recent times. Exemplars of image synthesis from text\nor other images, these networks have shown remarkable improvements over\nconventional methods in terms of performance. Trained on the adversarial\ntraining philosophy, these networks aim to estimate the potential distribution\nfrom the real data and then use this as input to generate the synthetic data.\nBased on this fundamental principle, several frameworks can be generated that\nare paragon implementations in several real-life applications such as art\nsynthesis, generation of high resolution outputs and synthesis of images from\nhuman drawn sketches, to name a few. While theoretically GANs present better\nresults and prove to be an improvement over conventional methods in many\nfactors, the implementation of these frameworks for dedicated applications\nremains a challenge. This study explores and presents a taxonomy of these\nframeworks and their use in various image to image synthesis and text to image\nsynthesis applications. The basic GANs, as well as a variety of different niche\nframeworks, are critically analyzed. The advantages of GANs for image\ngeneration over conventional methods as well their disadvantages amongst other\nframeworks are presented. The future applications of GANs in industries such as\nhealthcare, art and entertainment are also discussed.\n", "versions": [{"version": "v1", "created": "Fri, 30 Aug 2019 17:48:05 GMT"}], "update_date": "2019-09-02", "authors_parsed": [["Akut", "Rohan", ""], ["Marathe", "Sumukh", ""], ["Apte", "Rucha", ""], ["Joshi", "Ishan", ""], ["Kulkarni", "Siddhivinayak", ""]]}]