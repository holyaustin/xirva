[{"id": "1203.0076", "submitter": "Luis Quesada", "authors": "Luis Quesada", "title": "Using Barriers to Reduce the Sensitivity to Edge Miscalculations of\n  Casting-Based Object Projection Feature Estimation", "comments": "arXiv admin note: substantial text overlap with arXiv:1202.6586v1 and\n  arXiv:1111.3969", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  3D motion tracking is a critical task in many computer vision applications.\nUnsupervised markerless 3D motion tracking systems determine the most relevant\nobject in the screen and then track it by continuously estimating its\nprojection features (center and area) from the edge image and a point inside\nthe relevant object projection (namely, inner point), until the tracking fails.\nExisting reliable object projection feature estimation techniques are based on\nray-casting or grid-filling from the inner point. These techniques assume the\nedge image to be accurate. However, in real case scenarios, edge\nmiscalculations may arise from low contrast between the target object and its\nsurroundings or motion blur caused by low frame rates or fast moving target\nobjects. In this paper, we propose a barrier extension to casting-based\ntechniques that mitigates the effect of edge miscalculations.\n", "versions": [{"version": "v1", "created": "Thu, 1 Mar 2012 02:32:28 GMT"}], "update_date": "2012-03-19", "authors_parsed": [["Quesada", "Luis", ""]]}, {"id": "1203.0265", "submitter": "Olivia Saierli", "authors": "S. Chitra, J. B. Bhattacharjee and B. Thilakavathi", "title": "Image Fusion and Re-Modified SPIHT for Fused Image", "comments": "16 pages", "journal-ref": "Ann. Univ. Tibiscus Comp. Sci. Series VII/2 (2009), 143-158", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents the Discrete Wavelet based fusion techniques for\ncombining perceptually important image features. SPIHT (Set Partitioning in\nHierarchical Trees) algorithm is an efficient method for lossy and lossless\ncoding of fused image. This paper presents some modifications on the SPIHT\nalgorithm. It is based on the idea of insignificant correlation of wavelet\ncoefficient among the medium and high frequency sub bands. In RE-MSPIHT\nalgorithm, wavelet coefficients are scaled prior to SPIHT coding based on the\nsub band importance, with the goal of minimizing the MSE.\n", "versions": [{"version": "v1", "created": "Wed, 29 Feb 2012 17:57:12 GMT"}], "update_date": "2012-03-02", "authors_parsed": [["Chitra", "S.", ""], ["Bhattacharjee", "J. B.", ""], ["Thilakavathi", "B.", ""]]}, {"id": "1203.0488", "submitter": "Shu Kong", "authors": "Shu Kong and Donghui Wang", "title": "Multi-Level Feature Descriptor for Robust Texture Classification via\n  Locality-Constrained Collaborative Strategy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a simple but highly efficient ensemble for robust\ntexture classification, which can effectively deal with translation, scale and\nchanges of significant viewpoint problems. The proposed method first inherits\nthe spirit of spatial pyramid matching model (SPM), which is popular for\nencoding spatial distribution of local features, but in a flexible way,\npartitioning the original image into different levels and incorporating\ndifferent overlapping patterns of each level. This flexible setup helps capture\nthe informative features and produces sufficient local feature codes by some\nwell-chosen aggregation statistics or pooling operations within each\npartitioned region, even when only a few sample images are available for\ntraining. Then each texture image is represented by several orderless feature\ncodes and thereby all the training data form a reliable feature pond. Finally,\nto take full advantage of this feature pond, we develop a collaborative\nrepresentation-based strategy with locality constraint (LC-CRC) for the final\nclassification, and experimental results on three well-known public texture\ndatasets demonstrate the proposed approach is very competitive and even\noutperforms several state-of-the-art methods. Particularly, when only a few\nsamples of each category are available for training, our approach still\nachieves very high classification performance.\n", "versions": [{"version": "v1", "created": "Fri, 2 Mar 2012 15:15:50 GMT"}], "update_date": "2012-03-06", "authors_parsed": [["Kong", "Shu", ""], ["Wang", "Donghui", ""]]}, {"id": "1203.0744", "submitter": "Shu Kong", "authors": "Shu Kong and Donghui Wang", "title": "A Report on Multilinear PCA Plus Multilinear LDA to Deal with Tensorial\n  Data: Visual Classification as An Example", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In practical applications, we often have to deal with high order data, such\nas a grayscale image and a video sequence are intrinsically 2nd-order tensor\nand 3rd-order tensor, respectively. For doing clustering or classification of\nthese high order data, it is a conventional way to vectorize these data before\nhand, as PCA or FDA does, which often induce the curse of dimensionality\nproblem. For this reason, experts have developed many methods to deal with the\ntensorial data, such as multilinear PCA, multilinear LDA, and so on. In this\npaper, we still address the problem of high order data representation and\nrecognition, and propose to study the result of merging multilinear PCA and\nmultilinear LDA into one scenario, we name it \\textbf{GDA} for the abbreviation\nof Generalized Discriminant Analysis. To evaluate GDA, we perform a series of\nexperiments, and the experimental results demonstrate our GDA outperforms a\nselection of competing methods such (2D)$^2$PCA, (2D)$^2$LDA, and MDA.\n", "versions": [{"version": "v1", "created": "Sun, 4 Mar 2012 15:00:16 GMT"}], "update_date": "2012-03-06", "authors_parsed": [["Kong", "Shu", ""], ["Wang", "Donghui", ""]]}, {"id": "1203.0781", "submitter": "Takayuki Katsuki", "authors": "Takayuki Katsuki, Masato Inoue", "title": "Posterior Mean Super-Resolution with a Compound Gaussian Markov Random\n  Field Prior", "comments": "5 pages, 20 figures, 1 tables, accepted to ICASSP2012 (corrected\n  2012/3/23)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This manuscript proposes a posterior mean (PM) super-resolution (SR) method\nwith a compound Gaussian Markov random field (MRF) prior. SR is a technique to\nestimate a spatially high-resolution image from observed multiple\nlow-resolution images. A compound Gaussian MRF model provides a preferable\nprior for natural images that preserves edges. PM is the optimal estimator for\nthe objective function of peak signal-to-noise ratio (PSNR). This estimator is\nnumerically determined by using variational Bayes (VB). We then solve the\nconjugate prior problem on VB and the exponential-order calculation cost\nproblem of a compound Gaussian MRF prior with simple Taylor approximations. In\nexperiments, the proposed method roughly overcomes existing methods.\n", "versions": [{"version": "v1", "created": "Sun, 4 Mar 2012 22:12:54 GMT"}, {"version": "v2", "created": "Sat, 10 Mar 2012 04:11:08 GMT"}, {"version": "v3", "created": "Fri, 23 Mar 2012 02:52:46 GMT"}], "update_date": "2012-03-26", "authors_parsed": [["Katsuki", "Takayuki", ""], ["Inoue", "Masato", ""]]}, {"id": "1203.0856", "submitter": "Shu Kong", "authors": "Shu Kong and Donghui Wang", "title": "Online Discriminative Dictionary Learning for Image Classification Based\n  on Block-Coordinate Descent Method", "comments": "This paper was completed in Dec 2010, and submitted (unsuccessfully)\n  to ICCV2011", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Previous researches have demonstrated that the framework of dictionary\nlearning with sparse coding, in which signals are decomposed as linear\ncombinations of a few atoms of a learned dictionary, is well adept to\nreconstruction issues. This framework has also been used for discrimination\ntasks such as image classification. To achieve better performances of\nclassification, experts develop several methods to learn a discriminative\ndictionary in a supervised manner. However, another issue is that when the data\nbecome extremely large in scale, these methods will be no longer effective as\nthey are all batch-oriented approaches. For this reason, we propose a novel\nonline algorithm for discriminative dictionary learning, dubbed \\textbf{ODDL}\nin this paper. First, we introduce a linear classifier into the conventional\ndictionary learning formulation and derive a discriminative dictionary learning\nproblem. Then, we exploit an online algorithm to solve the derived problem.\nUnlike the most existing approaches which update dictionary and classifier\nalternately via iteratively solving sub-problems, our approach directly\nexplores them jointly. Meanwhile, it can largely shorten the runtime for\ntraining and is also particularly suitable for large-scale classification\nissues. To evaluate the performance of the proposed ODDL approach in image\nrecognition, we conduct some experiments on three well-known benchmarks, and\nthe experimental results demonstrate ODDL is fairly promising for image\nclassification tasks.\n", "versions": [{"version": "v1", "created": "Mon, 5 Mar 2012 10:43:15 GMT"}], "update_date": "2012-03-06", "authors_parsed": [["Kong", "Shu", ""], ["Wang", "Donghui", ""]]}, {"id": "1203.0876", "submitter": "Subhadip Basu", "authors": "Subhadip Basu, Nibaran Das, Ram Sarkar, Mahantapas Kundu, Mita\n  Nasipuri, Dipak Kumar Basu", "title": "An MLP based Approach for Recognition of Handwritten `Bangla' Numerals", "comments": null, "journal-ref": "Proc. 2nd Indian International Conference on Artificial\n  Intelligence, pp. 407-417, Dec. 2005, Pune", "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The work presented here involves the design of a Multi Layer Perceptron (MLP)\nbased pattern classifier for recognition of handwritten Bangla digits using a\n76 element feature vector. Bangla is the second most popular script and\nlanguage in the Indian subcontinent and the fifth most popular language in the\nworld. The feature set developed for representing handwritten Bangla numerals\nhere includes 24 shadow features, 16 centroid features and 36 longest-run\nfeatures. On experimentation with a database of 6000 samples, the technique\nyields an average recognition rate of 96.67% evaluated after three-fold cross\nvalidation of results. It is useful for applications related to OCR of\nhandwritten Bangla Digit and can also be extended to include OCR of handwritten\ncharacters of Bangla alphabet.\n", "versions": [{"version": "v1", "created": "Mon, 5 Mar 2012 12:06:54 GMT"}], "update_date": "2012-03-06", "authors_parsed": [["Basu", "Subhadip", ""], ["Das", "Nibaran", ""], ["Sarkar", "Ram", ""], ["Kundu", "Mahantapas", ""], ["Nasipuri", "Mita", ""], ["Basu", "Dipak Kumar", ""]]}, {"id": "1203.0882", "submitter": "Subhadip Basu", "authors": "Subhadip Basu, Nibaran Das, Ram Sarkar, Mahantapas Kundu, Mita\n  Nasipuri, Dipak Kumar Basu", "title": "Handwritten Bangla Alphabet Recognition using an MLP Based Classifier", "comments": null, "journal-ref": "Proc. of the 2nd National Conf. on Computer Processing of Bangla,\n  pp. 285-291, Feb-2005, Dhaka", "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The work presented here involves the design of a Multi Layer Perceptron (MLP)\nbased classifier for recognition of handwritten Bangla alphabet using a 76\nelement feature set Bangla is the second most popular script and language in\nthe Indian subcontinent and the fifth most popular language in the world. The\nfeature set developed for representing handwritten characters of Bangla\nalphabet includes 24 shadow features, 16 centroid features and 36 longest-run\nfeatures. Recognition performances of the MLP designed to work with this\nfeature set are experimentally observed as 86.46% and 75.05% on the samples of\nthe training and the test sets respectively. The work has useful application in\nthe development of a complete OCR system for handwritten Bangla text.\n", "versions": [{"version": "v1", "created": "Mon, 5 Mar 2012 12:22:23 GMT"}], "update_date": "2012-03-06", "authors_parsed": [["Basu", "Subhadip", ""], ["Das", "Nibaran", ""], ["Sarkar", "Ram", ""], ["Kundu", "Mahantapas", ""], ["Nasipuri", "Mita", ""], ["Basu", "Dipak Kumar", ""]]}, {"id": "1203.0905", "submitter": "Jos\\'e Ignacio Ronda", "authors": "Jos\\'e I. Ronda, Antonio Vald\\'es and Guillermo Gallego", "title": "Autocalibration with the Minimum Number of Cameras with Known Pixel\n  Shape", "comments": "19 pages, 14 figures, 7 tables, J. Math. Imaging Vis", "journal-ref": "Journal of Mathematical Imaging and Vision, Vol. 50, No. 3, pp.\n  179-198, Nov. 2014", "doi": "10.1007/s10851-014-0492-5", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In 3D reconstruction, the recovery of the calibration parameters of the\ncameras is paramount since it provides metric information about the observed\nscene, e.g., measures of angles and ratios of distances. Autocalibration\nenables the estimation of the camera parameters without using a calibration\ndevice, but by enforcing simple constraints on the camera parameters. In the\nabsence of information about the internal camera parameters such as the focal\nlength and the principal point, the knowledge of the camera pixel shape is\nusually the only available constraint. Given a projective reconstruction of a\nrigid scene, we address the problem of the autocalibration of a minimal set of\ncameras with known pixel shape and otherwise arbitrarily varying intrinsic and\nextrinsic parameters. We propose an algorithm that only requires 5 cameras (the\ntheoretical minimum), thus halving the number of cameras required by previous\nalgorithms based on the same constraint. To this purpose, we introduce as our\nbasic geometric tool the six-line conic variety (SLCV), consisting in the set\nof planes intersecting six given lines of 3D space in points of a conic. We\nshow that the set of solutions of the Euclidean upgrading problem for three\ncameras with known pixel shape can be parameterized in a computationally\nefficient way. This parameterization is then used to solve autocalibration from\nfive or more cameras, reducing the three-dimensional search space to a\ntwo-dimensional one. We provide experiments with real images showing the good\nperformance of the technique.\n", "versions": [{"version": "v1", "created": "Mon, 5 Mar 2012 13:18:44 GMT"}, {"version": "v2", "created": "Fri, 10 Jan 2014 23:55:00 GMT"}], "update_date": "2019-01-21", "authors_parsed": [["Ronda", "Jos\u00e9 I.", ""], ["Vald\u00e9s", "Antonio", ""], ["Gallego", "Guillermo", ""]]}, {"id": "1203.1005", "submitter": "Ehsan Elhamifar", "authors": "Ehsan Elhamifar and Rene Vidal", "title": "Sparse Subspace Clustering: Algorithm, Theory, and Applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR cs.IT cs.LG math.IT math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many real-world problems, we are dealing with collections of\nhigh-dimensional data, such as images, videos, text and web documents, DNA\nmicroarray data, and more. Often, high-dimensional data lie close to\nlow-dimensional structures corresponding to several classes or categories the\ndata belongs to. In this paper, we propose and study an algorithm, called\nSparse Subspace Clustering (SSC), to cluster data points that lie in a union of\nlow-dimensional subspaces. The key idea is that, among infinitely many possible\nrepresentations of a data point in terms of other points, a sparse\nrepresentation corresponds to selecting a few points from the same subspace.\nThis motivates solving a sparse optimization program whose solution is used in\na spectral clustering framework to infer the clustering of data into subspaces.\nSince solving the sparse optimization program is in general NP-hard, we\nconsider a convex relaxation and show that, under appropriate conditions on the\narrangement of subspaces and the distribution of data, the proposed\nminimization program succeeds in recovering the desired sparse representations.\nThe proposed algorithm can be solved efficiently and can handle data points\nnear the intersections of subspaces. Another key advantage of the proposed\nalgorithm with respect to the state of the art is that it can deal with data\nnuisances, such as noise, sparse outlying entries, and missing entries,\ndirectly by incorporating the model of the data into the sparse optimization\nprogram. We demonstrate the effectiveness of the proposed algorithm through\nexperiments on synthetic data as well as the two real-world problems of motion\nsegmentation and face clustering.\n", "versions": [{"version": "v1", "created": "Mon, 5 Mar 2012 18:58:32 GMT"}, {"version": "v2", "created": "Tue, 4 Sep 2012 17:29:45 GMT"}, {"version": "v3", "created": "Tue, 5 Feb 2013 03:22:00 GMT"}], "update_date": "2013-02-06", "authors_parsed": [["Elhamifar", "Ehsan", ""], ["Vidal", "Rene", ""]]}, {"id": "1203.1483", "submitter": "Eduard Gabriel B\\u{a}z\\u{a}van", "authors": "Eduard Gabriel B\\u{a}z\\u{a}van, Fuxin Li and Cristian Sminchisescu", "title": "Learning Random Kernel Approximations for Object Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Approximations based on random Fourier features have recently emerged as an\nefficient and formally consistent methodology to design large-scale kernel\nmachines. By expressing the kernel as a Fourier expansion, features are\ngenerated based on a finite set of random basis projections, sampled from the\nFourier transform of the kernel, with inner products that are Monte Carlo\napproximations of the original kernel. Based on the observation that different\nkernel-induced Fourier sampling distributions correspond to different kernel\nparameters, we show that an optimization process in the Fourier domain can be\nused to identify the different frequency bands that are useful for prediction\non training data. Moreover, the application of group Lasso to random feature\nvectors corresponding to a linear combination of multiple kernels, leads to\nefficient and scalable reformulations of the standard multiple kernel learning\nmodel \\cite{Varma09}. In this paper we develop the linear Fourier approximation\nmethodology for both single and multiple gradient-based kernel learning and\nshow that it produces fast and accurate predictors on a complex dataset such as\nthe Visual Object Challenge 2011 (VOC2011).\n", "versions": [{"version": "v1", "created": "Wed, 7 Mar 2012 14:33:26 GMT"}], "update_date": "2012-03-08", "authors_parsed": [["B\u0103z\u0103van", "Eduard Gabriel", ""], ["Li", "Fuxin", ""], ["Sminchisescu", "Cristian", ""]]}, {"id": "1203.1513", "submitter": "Joan Bruna", "authors": "Joan Bruna and St\\'ephane Mallat", "title": "Invariant Scattering Convolution Networks", "comments": "15 pages double column, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A wavelet scattering network computes a translation invariant image\nrepresentation, which is stable to deformations and preserves high frequency\ninformation for classification. It cascades wavelet transform convolutions with\nnon-linear modulus and averaging operators. The first network layer outputs\nSIFT-type descriptors whereas the next layers provide complementary invariant\ninformation which improves classification. The mathematical analysis of wavelet\nscattering networks explains important properties of deep convolution networks\nfor classification.\n  A scattering representation of stationary processes incorporates higher order\nmoments and can thus discriminate textures having the same Fourier power\nspectrum. State of the art classification results are obtained for handwritten\ndigits and texture discrimination, using a Gaussian kernel SVM and a generative\nPCA classifier.\n", "versions": [{"version": "v1", "created": "Mon, 5 Mar 2012 17:12:42 GMT"}, {"version": "v2", "created": "Thu, 8 Mar 2012 10:29:32 GMT"}], "update_date": "2012-03-09", "authors_parsed": [["Bruna", "Joan", ""], ["Mallat", "St\u00e9phane", ""]]}, {"id": "1203.1765", "submitter": "Kom Guillaume", "authors": "Guillaume Kom, Alain Tiedeu, Martin Kom, John Ngundam", "title": "A comparative evaluation of two algorithms of detection of masses on\n  mammograms", "comments": "9 pages, 5 figures, 1 table, Vol.3, No.1, February 2012,pp19-27;\n  Signal & Image Processing : An International Journal (SIPIJ),2012", "journal-ref": null, "doi": "10.5121/sipij.2012.3102", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we implement and carry out the comparison of two methods of\ncomputer-aided-detection of masses on mammograms. The two algorithms basically\nconsist of 3 steps each: segmentation, binarization and noise suppression using\ndifferent techniques for each step. A database of 60 images was used to compare\nthe performance of the two algorithms in terms of general detection efficiency,\nconservation of size and shape of detected masses.\n", "versions": [{"version": "v1", "created": "Thu, 8 Mar 2012 12:07:27 GMT"}], "update_date": "2012-03-09", "authors_parsed": [["Kom", "Guillaume", ""], ["Tiedeu", "Alain", ""], ["Kom", "Martin", ""], ["Ngundam", "John", ""]]}, {"id": "1203.1793", "submitter": "Riadh Bouslimi", "authors": "Riadh Bouslimi, Jalel Akaichi", "title": "Using Hausdorff Distance for New Medical Image Annotation", "comments": "7 pages, 3 figures, 2 tables; International Journal of Database\n  Management Systems (IJDMS) Vol.4, No.1, February 2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Medical images annotation is most of the time a repetitive hard task.\nCollecting old similar annotations and assigning them to new medical images may\nnot only enhance the annotation process, but also reduce ambiguity caused by\nrepetitive annotations. The goal of this work is to propose an approach based\non Hausdorff distance able to compute similarity between a new medical image\nand old stored images. User has to choose then one of the similar images and\nannotations related to the selected one are assigned to the new one.\n", "versions": [{"version": "v1", "created": "Thu, 8 Mar 2012 13:28:47 GMT"}], "update_date": "2012-03-09", "authors_parsed": [["Bouslimi", "Riadh", ""], ["Akaichi", "Jalel", ""]]}, {"id": "1203.1823", "submitter": "Chelsy Sapna  Josephus", "authors": "Chelsy Sapna Josephus, S. Remya", "title": "Enhancement Techniques for Local Content Preservation and Contrast\n  Improvement in Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There are several images that do not have uniform brightness which pose a\nchallenging problem for image enhancement systems. As histogram equalization\nhas been successfully used to correct for uniform brightness problems, a\nhistogram equalization method that utilizes human visual system based\nthresholding(human vision thresholding) as well as logarithmic processing\ntechniques were introduced later . But these methods are not good for\npreserving the local content of the image which is a major factor for various\nimages like medical and aerial images. Therefore new method is proposed here.\nThis method is referred as \"Human vision thresholding with enhancement\ntechnique for dark blurred images for local content preservation\". It uses\nhuman vision thresholding together with an existing enhancement method for dark\nblurred images. Furthermore a comparative study with another method for local\ncontent preservation is done which is further extended to make it suitable for\ncontrast improvement. Experimental results shows that the proposed methods\noutperforms the former existing methods in preserving the local content for\nstandard images, medical and aerial images.\n", "versions": [{"version": "v1", "created": "Thu, 8 Mar 2012 15:12:34 GMT"}], "update_date": "2012-03-09", "authors_parsed": [["Josephus", "Chelsy Sapna", ""], ["Remya", "S.", ""]]}, {"id": "1203.1985", "submitter": "Zhaowen Wang", "authors": "Zhaowen Wang, Jinjun Wang, Jing Xiao, Kai-Hsiang Lin, Thomas Huang", "title": "Substructure and Boundary Modeling for Continuous Action Recognition", "comments": "Detailed version of the CVPR 2012 paper. 15 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a probabilistic graphical model for continuous action\nrecognition with two novel components: substructure transition model and\ndiscriminative boundary model. The first component encodes the sparse and\nglobal temporal transition prior between action primitives in state-space model\nto handle the large spatial-temporal variations within an action class. The\nsecond component enforces the action duration constraint in a discriminative\nway to locate the transition boundaries between actions more accurately. The\ntwo components are integrated into a unified graphical structure to enable\neffective training and inference. Our comprehensive experimental results on\nboth public and in-house datasets show that, with the capability to incorporate\nadditional information that had not been explicitly or efficiently modeled by\nprevious methods, our proposed algorithm achieved significantly improved\nperformance for continuous action recognition.\n", "versions": [{"version": "v1", "created": "Fri, 9 Mar 2012 04:16:33 GMT"}], "update_date": "2012-03-12", "authors_parsed": [["Wang", "Zhaowen", ""], ["Wang", "Jinjun", ""], ["Xiao", "Jing", ""], ["Lin", "Kai-Hsiang", ""], ["Huang", "Thomas", ""]]}, {"id": "1203.2210", "submitter": "Risheng Liu", "authors": "Risheng Liu and Zhouchen Lin and Fernando De la Torre and Zhixun Su", "title": "Fixed-Rank Representation for Unsupervised Visual Learning", "comments": "accepted by CVPR 2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Subspace clustering and feature extraction are two of the most commonly used\nunsupervised learning techniques in computer vision and pattern recognition.\nState-of-the-art techniques for subspace clustering make use of recent advances\nin sparsity and rank minimization. However, existing techniques are\ncomputationally expensive and may result in degenerate solutions that degrade\nclustering performance in the case of insufficient data sampling. To partially\nsolve these problems, and inspired by existing work on matrix factorization,\nthis paper proposes fixed-rank representation (FRR) as a unified framework for\nunsupervised visual learning. FRR is able to reveal the structure of multiple\nsubspaces in closed-form when the data is noiseless. Furthermore, we prove that\nunder some suitable conditions, even with insufficient observations, FRR can\nstill reveal the true subspace memberships. To achieve robustness to outliers\nand noise, a sparse regularizer is introduced into the FRR framework. Beyond\nsubspace clustering, FRR can be used for unsupervised feature extraction. As a\nnon-trivial byproduct, a fast numerical solver is developed for FRR.\nExperimental results on both synthetic data and real applications validate our\ntheoretical analysis and demonstrate the benefits of FRR for unsupervised\nvisual learning.\n", "versions": [{"version": "v1", "created": "Fri, 9 Mar 2012 23:35:52 GMT"}, {"version": "v2", "created": "Tue, 17 Apr 2012 15:41:17 GMT"}], "update_date": "2012-04-18", "authors_parsed": [["Liu", "Risheng", ""], ["Lin", "Zhouchen", ""], ["De la Torre", "Fernando", ""], ["Su", "Zhixun", ""]]}, {"id": "1203.2386", "submitter": "Ashraf Qadir", "authors": "Ashraf Qadir, Jeremiah Neubert, and William Semke", "title": "On-Board Visual Tracking with Unmanned Aircraft System (UAS)", "comments": "Infotech@Aerospace 2011 Conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents the development of a real time tracking algorithm that\nruns on a 1.2 GHz PC/104 computer on-board a small UAV. The algorithm uses zero\nmean normalized cross correlation to detect and locate an object in the image.\nA kalman filter is used to make the tracking algorithm computationally\nefficient. Object position in an image frame is predicted using the motion\nmodel and a search window, centered at the predicted position is generated.\nObject position is updated with the measurement from object detection. The\ndetected position is sent to the motion controller to move the gimbal so that\nthe object stays at the center of the image frame. Detection and tracking is\nautonomously carried out on the payload computer and the system is able to work\nin two different methods. The first method starts detecting and tracking using\na stored image patch. The second method allows the operator on the ground to\nselect the interest object for the UAV to track. The system is capable of\nre-detecting an object, in the event of tracking failure. Performance of the\ntracking system was verified both in the lab and on the field by mounting the\npayload on a vehicle and simulating a flight. Tests show that the system can\ndetect and track a diverse set of objects in real time. Flight testing of the\nsystem will be conducted at the next available opportunity.\n", "versions": [{"version": "v1", "created": "Sun, 11 Mar 2012 23:57:36 GMT"}], "update_date": "2012-03-13", "authors_parsed": [["Qadir", "Ashraf", ""], ["Neubert", "Jeremiah", ""], ["Semke", "William", ""]]}, {"id": "1203.2404", "submitter": "Nobert Thomas Pallath", "authors": "Nobert Thomas Pallath and Tessamma Thomas", "title": "Video Object Tracking and Analysis for Computer Assisted Surgery", "comments": "11 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pedicle screw insertion technique has made revolution in the surgical\ntreatment of spinal fractures and spinal disorders. Although X- ray fluoroscopy\nbased navigation is popular, there is risk of prolonged exposure to X- ray\nradiation. Systems that have lower radiation risk are generally quite\nexpensive. The position and orientation of the drill is clinically very\nimportant in pedicle screw fixation. In this paper, the position and\norientation of the marker on the drill is determined using pattern recognition\nbased methods, using geometric features, obtained from the input video sequence\ntaken from CCD camera. A search is then performed on the video frames after\npreprocessing, to obtain the exact position and orientation of the drill.\nAnimated graphics, showing the instantaneous position and orientation of the\ndrill is then overlaid on the processed video for real time drill control and\nnavigation.\n", "versions": [{"version": "v1", "created": "Mon, 12 Mar 2012 05:39:34 GMT"}], "update_date": "2012-03-13", "authors_parsed": [["Pallath", "Nobert Thomas", ""], ["Thomas", "Tessamma", ""]]}, {"id": "1203.2514", "submitter": "Sreedhar Kallem", "authors": "K. Sreedhar and B. Panlal", "title": "Enhancement of Images using Morphological Transformation", "comments": "18 pages", "journal-ref": "International Journal of Computer Science & Information Technology\n  (IJCSIT) Vol 4, No 1, Feb 2012, 33-50", "doi": "10.5121/ijcsit.2012.4103", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper deals with enhancement of images with poor contrast and detection\nof background. Proposes a frame work which is used to detect the background in\nimages characterized by poor contrast. Image enhancement has been carried out\nby the two methods based on the Weber's law notion. The first method employs\ninformation from image background analysis by blocks, while the second\ntransformation method utilizes the opening operation, closing operation, which\nis employed to define the multi-background gray scale images. The complete\nimage processing is done using MATLAB simulation model. Finally, this paper is\norganized as follows as Morphological transformation and Weber's law. Image\nbackground approximation to the background by means of block analysis in\nconjunction with transformations that enhance images with poor lighting. The\nmultibackground notion is introduced by means of the opening by reconstruction\nshows a comparison among several techniques to improve contrast in images.\nFinally, conclusions are presented.\n", "versions": [{"version": "v1", "created": "Fri, 9 Mar 2012 13:22:25 GMT"}], "update_date": "2012-03-13", "authors_parsed": [["Sreedhar", "K.", ""], ["Panlal", "B.", ""]]}, {"id": "1203.2839", "submitter": "Jan Egger", "authors": "Jan Egger, Tina Kapur, Thomas Dukatz, Malgorzata Kolodziej, Dzenan\n  Zukic, Bernd Freisleben, Christopher Nimsky", "title": "Square-Cut: A Segmentation Algorithm on the Basis of a Rectangle Shape", "comments": "13 pages, 17 figures, 2 tables, 3 equations, 42 references", "journal-ref": "Egger J, Kapur T, Dukatz T, Kolodziej M, Zukic D, et al. (2012)\n  Square-Cut: A Segmentation Algorithm on the Basis of a Rectangle Shape. PLoS\n  ONE 7(2): e31064", "doi": "10.1371/journal.pone.0031064", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a rectangle-based segmentation algorithm that sets up a graph and\nperforms a graph cut to separate an object from the background. However,\ngraph-based algorithms distribute the graph's nodes uniformly and equidistantly\non the image. Then, a smoothness term is added to force the cut to prefer a\nparticular shape. This strategy does not allow the cut to prefer a certain\nstructure, especially when areas of the object are indistinguishable from the\nbackground. We solve this problem by referring to a rectangle shape of the\nobject when sampling the graph nodes, i.e., the nodes are distributed\nnonuniformly and non-equidistantly on the image. This strategy can be useful,\nwhen areas of the object are indistinguishable from the background. For\nevaluation, we focus on vertebrae images from Magnetic Resonance Imaging (MRI)\ndatasets to support the time consuming manual slice-by-slice segmentation\nperformed by physicians. The ground truth of the vertebrae boundaries were\nmanually extracted by two clinical experts (neurological surgeons) with several\nyears of experience in spine surgery and afterwards compared with the automatic\nsegmentation results of the proposed scheme yielding an average Dice Similarity\nCoefficient (DSC) of 90.97\\pm62.2%.\n", "versions": [{"version": "v1", "created": "Tue, 13 Mar 2012 15:41:14 GMT"}], "update_date": "2012-03-14", "authors_parsed": [["Egger", "Jan", ""], ["Kapur", "Tina", ""], ["Dukatz", "Thomas", ""], ["Kolodziej", "Malgorzata", ""], ["Zukic", "Dzenan", ""], ["Freisleben", "Bernd", ""], ["Nimsky", "Christopher", ""]]}, {"id": "1203.2992", "submitter": "Jason Williams", "authors": "Jason L. Williams", "title": "Hybrid Poisson and multi-Bernoulli filters", "comments": "Submitted to 15th International Conference on Information Fusion\n  (2012)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SY cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The probability hypothesis density (PHD) and multi-target multi-Bernoulli\n(MeMBer) filters are two leading algorithms that have emerged from random\nfinite sets (RFS). In this paper we study a method which combines these two\napproaches. Our work is motivated by a sister paper, which proves that the full\nBayes RFS filter naturally incorporates a Poisson component representing\ntargets that have never been detected, and a linear combination of\nmulti-Bernoulli components representing targets under track. Here we\ndemonstrate the benefit (in speed of track initiation) that maintenance of a\nPoisson component of undetected targets provides. Subsequently, we propose a\nmethod of recycling, which projects Bernoulli components with a low probability\nof existence onto the Poisson component (as opposed to deleting them). We show\nthat this allows us to achieve similar tracking performance using a fraction of\nthe number of Bernoulli components (i.e., tracks).\n", "versions": [{"version": "v1", "created": "Wed, 14 Mar 2012 02:53:32 GMT"}], "update_date": "2015-03-20", "authors_parsed": [["Williams", "Jason L.", ""]]}, {"id": "1203.2995", "submitter": "Jason Williams", "authors": "Jason L. Williams", "title": "Marginal multi-Bernoulli filters: RFS derivation of MHT, JIPDA and\n  association-based MeMBer", "comments": "Journal version at http://ieeexplore.ieee.org/document/7272821.\n  Matlab code of simple implementation included with ancillary files", "journal-ref": "IEEE Transactions on Aerospace and Electronic Systems, vol 51, no\n  3, pp 1664-1687, July 2015", "doi": "10.1109/TAES.2015.130550", "report-no": null, "categories": "cs.SY cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent developments in random finite sets (RFSs) have yielded a variety of\ntracking methods that avoid data association. This paper derives a form of the\nfull Bayes RFS filter and observes that data association is implicitly present,\nin a data structure similar to MHT. Subsequently, algorithms are obtained by\napproximating the distribution of associations. Two algorithms result: one\nnearly identical to JIPDA, and another related to the MeMBer filter. Both\nimprove performance in challenging environments.\n", "versions": [{"version": "v1", "created": "Wed, 14 Mar 2012 02:57:54 GMT"}, {"version": "v2", "created": "Tue, 15 Jan 2013 01:22:36 GMT"}, {"version": "v3", "created": "Fri, 21 Nov 2014 04:07:14 GMT"}, {"version": "v4", "created": "Mon, 1 Dec 2014 02:07:28 GMT"}, {"version": "v5", "created": "Thu, 30 Apr 2015 07:19:02 GMT"}, {"version": "v6", "created": "Wed, 24 Aug 2016 08:11:59 GMT"}], "update_date": "2016-08-25", "authors_parsed": [["Williams", "Jason L.", ""]]}, {"id": "1203.3114", "submitter": "Maria-Luisa Rosas-Cholula", "authors": "Maria-Luisa Sosas and Miguel-Octavio Arias", "title": "Integrated three-dimensional reconstruction using reflectance fields", "comments": "5 pages, 3 figures; Published in IJCSI Journal, Volume 9, Issue 1,\n  No. 3, January 2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A method to obtain three-dimensional data of real-world objects by\nintegrating their material properties is presented. The material properties are\ndefined by capturing the Reflectance Fields of the real-world objects. It is\nshown, unlike conventional reconstruction methods, the method is able to use\nthe reflectance information to recover surface depth for objects having a\nnon-Lambertian surface reflectance. It is, for recovering 3D data of objects\nexhibiting an anisotropic BRDF with an error less than 0.3%.\n", "versions": [{"version": "v1", "created": "Wed, 14 Mar 2012 15:31:16 GMT"}], "update_date": "2012-03-15", "authors_parsed": [["Sosas", "Maria-Luisa", ""], ["Arias", "Miguel-Octavio", ""]]}, {"id": "1203.3170", "submitter": "Shampa Sengupta", "authors": "Shampa Sengupta and Asit Kr. Das", "title": "Single Reduct Generation Based on Relative Indiscernibility of Rough Set\n  Theory", "comments": "13 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In real world everything is an object which represents particular classes.\nEvery object can be fully described by its attributes. Any real world dataset\ncontains large number of attributes and objects. Classifiers give poor\nperformance when these huge datasets are given as input to it for proper\nclassification. So from these huge dataset most useful attributes need to be\nextracted that contribute the maximum to the decision. In the paper, attribute\nset is reduced by generating reducts using the indiscernibility relation of\nRough Set Theory (RST). The method measures similarity among the attributes\nusing relative indiscernibility relation and computes attribute similarity set.\nThen the set is minimized and an attribute similarity table is constructed from\nwhich attribute similar to maximum number of attributes is selected so that the\nresultant minimum set of selected attributes (called reduct) cover all\nattributes of the attribute similarity table. The method has been applied on\nglass dataset collected from the UCI repository and the classification accuracy\nis calculated by various classifiers. The result shows the efficiency of the\nproposed method.\n", "versions": [{"version": "v1", "created": "Wed, 14 Mar 2012 18:34:05 GMT"}], "update_date": "2012-03-15", "authors_parsed": [["Sengupta", "Shampa", ""], ["Das", "Asit Kr.", ""]]}, {"id": "1203.3230", "submitter": "Andrea Masiero", "authors": "Andrea Masiero and Angelo Cenedese", "title": "Reconstruction error in a motion capture system", "comments": "3 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Marker-based motion capture (MoCap) systems can be composed by several dozens\nof cameras with the purpose of reconstructing the trajectories of hundreds of\ntargets. With a large amount of cameras it becomes interesting to determine the\noptimal reconstruction strategy. For such aim it is of fundamental importance\nto understand the information provided by different camera measurements and how\nthey are combined, i.e. how the reconstruction error changes by considering\ndifferent cameras. In this work, first, an approximation of the reconstruction\nerror variance is derived. The results obtained in some simulations suggest\nthat the proposed strategy allows to obtain a good approximation of the real\nerror variance with significant reduction of the computational time.\n", "versions": [{"version": "v1", "created": "Wed, 14 Mar 2012 22:46:29 GMT"}], "update_date": "2012-03-16", "authors_parsed": [["Masiero", "Andrea", ""], ["Cenedese", "Angelo", ""]]}, {"id": "1203.3270", "submitter": "Sushil Kumar Paul", "authors": "Sushil Kumar Paul, Mohammad Shorif Uddin and Saida Bouakaz", "title": "Extraction of Facial Feature Points Using Cumulative Histogram", "comments": "8 pages, 6 figures,3 equations, 2 tables, 20 references", "journal-ref": "IJCSI International Journal of Computer Science Issues, Vol. 9,\n  Issue 1, No 3, January 2012 ISSN (Online): 1694-0814 www.IJCSI.org", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a novel adaptive algorithm to extract facial feature\npoints automatically such as eyebrows corners, eyes corners, nostrils, nose\ntip, and mouth corners in frontal view faces, which is based on cumulative\nhistogram approach by varying different threshold values. At first, the method\nadopts the Viola-Jones face detector to detect the location of face and also\ncrops the face region in an image. From the concept of the human face\nstructure, the six relevant regions such as right eyebrow, left eyebrow, right\neye, left eye, nose, and mouth areas are cropped in a face image. Then the\nhistogram of each cropped relevant region is computed and its cumulative\nhistogram value is employed by varying different threshold values to create a\nnew filtering image in an adaptive way. The connected component of interested\narea for each relevant filtering image is indicated our respective feature\nregion. A simple linear search algorithm for eyebrows, eyes and mouth filtering\nimages and contour algorithm for nose filtering image are applied to extract\nour desired corner points automatically. The method was tested on a large BioID\nfrontal face database in different illuminations, expressions and lighting\nconditions and the experimental results have achieved average success rates of\n95.27%.\n", "versions": [{"version": "v1", "created": "Thu, 15 Mar 2012 05:20:27 GMT"}], "update_date": "2012-03-16", "authors_parsed": [["Paul", "Sushil Kumar", ""], ["Uddin", "Mohammad Shorif", ""], ["Bouakaz", "Saida", ""]]}, {"id": "1203.3512", "submitter": "Chris Russell", "authors": "Chris Russell, L'ubor Ladicky, Pushmeet Kohli, Philip H.S. Torr", "title": "Exact and Approximate Inference in Associative Hierarchical Networks\n  using Graph Cuts", "comments": "Appears in Proceedings of the Twenty-Sixth Conference on Uncertainty\n  in Artificial Intelligence (UAI2010)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2010-PG-501-508", "categories": "cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Markov Networks are widely used through out computer vision and machine\nlearning. An important subclass are the Associative Markov Networks which are\nused in a wide variety of applications. For these networks a good approximate\nminimum cost solution can be found efficiently using graph cut based move\nmaking algorithms such as alpha-expansion. Recently a related model has been\nproposed, the associative hierarchical network, which provides a natural\ngeneralisation of the Associative Markov Network for higher order cliques (i.e.\nclique size greater than two). This method provides a good model for object\nclass segmentation problem in computer vision. Within this paper we briefly\ndescribe the associative hierarchical network and provide a computationally\nefficient method for approximate inference based on graph cuts. Our method\nperforms well for networks containing hundreds of thousand of variables, and\nhigher order potentials are defined over cliques containing tens of thousands\nof variables. Due to the size of these problems standard linear programming\ntechniques are inapplicable. We show that our method has a bound of 4 for the\nsolution of general associative hierarchical network with arbitrary clique size\nnoting that few results on bounds exist for the solution of labelling of Markov\nNetworks with higher order cliques.\n", "versions": [{"version": "v1", "created": "Thu, 15 Mar 2012 11:17:56 GMT"}], "update_date": "2012-03-19", "authors_parsed": [["Russell", "Chris", ""], ["Ladicky", "L'ubor", ""], ["Kohli", "Pushmeet", ""], ["Torr", "Philip H. S.", ""]]}, {"id": "1203.3530", "submitter": "Shuang Hong Yang", "authors": "Shuang Hong Yang, Jiang Bian, Hongyuan Zha", "title": "Hybrid Generative/Discriminative Learning for Automatic Image Annotation", "comments": "Appears in Proceedings of the Twenty-Sixth Conference on Uncertainty\n  in Artificial Intelligence (UAI2010)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2010-PG-683-690", "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic image annotation (AIA) raises tremendous challenges to machine\nlearning as it requires modeling of data that are both ambiguous in input and\noutput, e.g., images containing multiple objects and labeled with multiple\nsemantic tags. Even more challenging is that the number of candidate tags is\nusually huge (as large as the vocabulary size) yet each image is only related\nto a few of them. This paper presents a hybrid generative-discriminative\nclassifier to simultaneously address the extreme data-ambiguity and\noverfitting-vulnerability issues in tasks such as AIA. Particularly: (1) an\nExponential-Multinomial Mixture (EMM) model is established to capture both the\ninput and output ambiguity and in the meanwhile to encourage prediction\nsparsity; and (2) the prediction ability of the EMM model is explicitly\nmaximized through discriminative learning that integrates variational inference\nof graphical models and the pairwise formulation of ordinal regression.\nExperiments show that our approach achieves both superior annotation\nperformance and better tag scalability.\n", "versions": [{"version": "v1", "created": "Thu, 15 Mar 2012 11:17:56 GMT"}], "update_date": "2012-03-19", "authors_parsed": [["Yang", "Shuang Hong", ""], ["Bian", "Jiang", ""], ["Zha", "Hongyuan", ""]]}, {"id": "1203.3537", "submitter": "Qian Zhu", "authors": "Qian Zhu, Branislav Kveton, Lily Mummert, Padmanabhan Pillai", "title": "Automatic Tuning of Interactive Perception Applications", "comments": "Appears in Proceedings of the Twenty-Sixth Conference on Uncertainty\n  in Artificial Intelligence (UAI2010)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2010-PG-743-751", "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Interactive applications incorporating high-data rate sensing and computer\nvision are becoming possible due to novel runtime systems and the use of\nparallel computation resources. To allow interactive use, such applications\nrequire careful tuning of multiple application parameters to meet required\nfidelity and latency bounds. This is a nontrivial task, often requiring expert\nknowledge, which becomes intractable as resources and application load\ncharacteristics change. This paper describes a method for automatic performance\ntuning that learns application characteristics and effects of tunable\nparameters online, and constructs models that are used to maximize fidelity for\na given latency constraint. The paper shows that accurate latency models can be\nlearned online, knowledge of application structure can be used to reduce the\ncomplexity of the learning task, and operating points can be found that achieve\n90% of the optimal fidelity by exploring the parameter space only 3% of the\ntime.\n", "versions": [{"version": "v1", "created": "Thu, 15 Mar 2012 11:17:56 GMT"}], "update_date": "2012-03-19", "authors_parsed": [["Zhu", "Qian", ""], ["Kveton", "Branislav", ""], ["Mummert", "Lily", ""], ["Pillai", "Padmanabhan", ""]]}, {"id": "1203.4009", "submitter": "Odemir Bruno PhD", "authors": "Ricardo Fabbri, Odemir Martinez Bruno and Luciano da Fontoura Costa", "title": "Scilab and SIP for Image Processing", "comments": "16 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MS cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper is an overview of Image Processing and Analysis using Scilab, a\nfree prototyping environment for numerical calculations similar to Matlab. We\ndemonstrate the capabilities of SIP -- the Scilab Image Processing Toolbox --\nwhich extends Scilab with many functions to read and write images in over 100\nmajor file formats, including PNG, JPEG, BMP, and TIFF. It also provides\nroutines for image filtering, edge detection, blurring, segmentation, shape\nanalysis, and image recognition. Basic directions to install Scilab and SIP are\ngiven, and also a mini-tutorial on Scilab. Three practical examples of image\nanalysis are presented, in increasing degrees of complexity, showing how\nadvanced image analysis techniques seems uncomplicated in this environment.\n", "versions": [{"version": "v1", "created": "Sun, 18 Mar 2012 23:51:19 GMT"}], "update_date": "2012-03-20", "authors_parsed": [["Fabbri", "Ricardo", ""], ["Bruno", "Odemir Martinez", ""], ["Costa", "Luciano da Fontoura", ""]]}, {"id": "1203.4176", "submitter": "Samaa Shohieb", "authors": "A.M. Riad, Hamdy K.Elmonier, Samaa. M. Shohieb, and A.S. Asem", "title": "SignsWorld; Deeping Into the Silence World and Hearing Its Signs (State\n  of the Art)", "comments": "20 pages, A state of art paper so it contains many references", "journal-ref": "International Journal of Computer Science & Information Technology\n  (IJCSIT) Vol 4, No 1, Feb 2012", "doi": null, "report-no": null, "categories": "cs.CL cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic speech processing systems are employed more and more often in real\nenvironments. Although the underlying speech technology is mostly language\nindependent, differences between languages with respect to their structure and\ngrammar have substantial effect on the recognition systems performance. In this\npaper, we present a review of the latest developments in the sign language\nrecognition research in general and in the Arabic sign language (ArSL) in\nspecific. This paper also presents a general framework for improving the deaf\ncommunity communication with the hearing people that is called SignsWorld. The\noverall goal of the SignsWorld project is to develop a vision-based technology\nfor recognizing and translating continuous Arabic sign language ArSL.\n", "versions": [{"version": "v1", "created": "Sat, 10 Mar 2012 10:16:10 GMT"}], "update_date": "2012-03-20", "authors_parsed": [["Riad", "A. M.", ""], ["Elmonier", "Hamdy K.", ""], ["Shohieb", "Samaa. M.", ""], ["Asem", "A. S.", ""]]}, {"id": "1203.4204", "submitter": "Amir Daneshgar", "authors": "Amir Daneshgar, Ramin Javadi, Basir Shariat Razavi", "title": "Clustering Using Isoperimetric Number of Trees", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose a graph-based data clustering algorithm which is\nbased on exact clustering of a minimum spanning tree in terms of a minimum\nisoperimetry criteria. We show that our basic clustering algorithm runs in $O(n\n\\log n)$ and with post-processing in $O(n^2)$ (worst case) time where $n$ is\nthe size of the data set. We also show that our generalized graph model which\nalso allows the use of potentials at vertices can be used to extract a more\ndetailed pack of information as the {\\it outlier profile} of the data set. In\nthis direction we show that our approach can be used to define the concept of\nan outlier-set in a precise way and we propose approximation algorithms for\nfinding such sets. We also provide a comparative performance analysis of our\nalgorithm with other related ones and we show that the new clustering algorithm\n(without the outlier extraction procedure) behaves quite effectively even on\nhard benchmarks and handmade examples.\n", "versions": [{"version": "v1", "created": "Mon, 19 Mar 2012 19:15:25 GMT"}], "update_date": "2012-03-20", "authors_parsed": [["Daneshgar", "Amir", ""], ["Javadi", "Ramin", ""], ["Razavi", "Basir Shariat", ""]]}, {"id": "1203.4280", "submitter": "Andreas Velten", "authors": "Otkrist Gupta, Andreas Velten, Thomas Willwacher, Ashok Veeraraghavan,\n  and Ramesh Raskar", "title": "Reconstruction of hidden 3D shapes using diffuse reflections", "comments": null, "journal-ref": null, "doi": "10.1364/OE.20.019096", "report-no": null, "categories": "physics.optics cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We analyze multi-bounce propagation of light in an unknown hidden volume and\ndemonstrate that the reflected light contains sufficient information to recover\nthe 3D structure of the hidden scene. We formulate the forward and inverse\ntheory of secondary and tertiary scattering reflection using ideas from energy\nfront propagation and tomography. We show that using careful choice of\napproximations, such as Fresnel approximation, greatly simplifies this problem\nand the inversion can be achieved via a backpropagation process. We provide a\ntheoretical analysis of the invertibility, uniqueness and choices of\nspace-time-angle dimensions using synthetic examples. We show that a 2D streak\ncamera can be used to discover and reconstruct hidden geometry. Using a 1D high\nspeed time of flight camera, we show that our method can be used recover 3D\nshapes of objects \"around the corner\".\n", "versions": [{"version": "v1", "created": "Mon, 19 Mar 2012 22:22:25 GMT"}], "update_date": "2015-06-04", "authors_parsed": [["Gupta", "Otkrist", ""], ["Velten", "Andreas", ""], ["Willwacher", "Thomas", ""], ["Veeraraghavan", "Ashok", ""], ["Raskar", "Ramesh", ""]]}, {"id": "1203.4355", "submitter": "Hyon Lim", "authors": "Hyon Lim and Sudipta Sinha and Michael Cohen and Matt Uyttendaele", "title": "Real-time Image-based 6-DOF Localization in Large-Scale Environments", "comments": "I would like to withdraw this paper due to copyright problem. Please\n  remove my article completely", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a real-time approach for image-based localization within large\nscenes that have been reconstructed offline using structure from motion (Sfm).\nFrom monocular video, our method continuously computes a precise 6-DOF camera\npose, by efficiently tracking natural features and matching them to 3D points\nin the Sfm point cloud. Our main contribution lies in efficiently interleaving\na fast keypoint tracker that uses inexpensive binary feature descriptors with a\nnew approach for direct 2D-to-3D matching. The 2D-to-3D matching avoids the\nneed for online extraction of scale-invariant features. Instead, offline we\nconstruct an indexed database containing multiple DAISY descriptors per 3D\npoint extracted at multiple scales. The key to the efficiency of our method\nlies in invoking DAISY descriptor extraction and matching sparingly during\nlocalization, and in distributing this computation over a window of successive\nframes. This enables the algorithm to run in real-time, without fluctuations in\nthe latency over long durations. We evaluate the method in large indoor and\noutdoor scenes. Our algorithm runs at over 30 Hz on a laptop and at 12 Hz on a\nlow-power, mobile computer suitable for onboard computation on a quadrotor\nmicro aerial vehicle.\n", "versions": [{"version": "v1", "created": "Tue, 20 Mar 2012 09:30:06 GMT"}, {"version": "v2", "created": "Tue, 27 Mar 2012 05:11:07 GMT"}], "update_date": "2015-03-20", "authors_parsed": [["Lim", "Hyon", ""], ["Sinha", "Sudipta", ""], ["Cohen", "Michael", ""], ["Uyttendaele", "Matt", ""]]}, {"id": "1203.4855", "submitter": "Shervan Fekri ershad", "authors": "Shervan Fekri Ershad", "title": "Texture Classification Approach Based on Combination of Edge &\n  Co-occurrence and Local Binary Pattern", "comments": "4 pages, 6 figures, 1 tables", "journal-ref": "Int'l Conf. IP, Comp. Vision, and Pattern Recognition, IPCV'11,\n  2011, pp. 626-629", "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  Texture classification is one of the problems which has been paid much\nattention on by computer scientists since late 90s. If texture classification\nis done correctly and accurately, it can be used in many cases such as Pattern\nrecognition, object tracking, and shape recognition. So far, there have been so\nmany methods offered to solve this problem. Near all these methods have tried\nto extract and define features to separate different labels of textures really\nwell. This article has offered an approach which has an overall process on the\nimages of textures based on Local binary pattern and Gray Level Co-occurrence\nmatrix and then by edge detection, and finally, extracting the statistical\nfeatures from the images would classify them. Although, this approach is a\ngeneral one and is could be used in different applications, the method has been\ntested on the stone texture and the results have been compared with some of the\nprevious approaches to prove the quality of proposed approach.\n", "versions": [{"version": "v1", "created": "Wed, 21 Mar 2012 23:33:30 GMT"}], "update_date": "2012-03-23", "authors_parsed": [["Ershad", "Shervan Fekri", ""]]}, {"id": "1203.4874", "submitter": "Feng Li", "authors": "Christopher Thorpe, Feng Li, Zijia Li, Zhan Yu, David Saunders, Jingyi\n  Yu", "title": "A Co-Prime Blur Scheme for Data Security in Video Surveillance", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a novel Coprime Blurred Pair (CBP) model for visual\ndata-hiding for security in camera surveillance. While most previous approaches\nhave focused on completely encrypting the video stream, we introduce a spatial\nencryption scheme by blurring the image/video contents to create a CBP. Our\ngoal is to obscure detail in public video streams by blurring while allowing\nbehavior to be recognized and to quickly deblur the stream so that details are\navailable if behavior is recognized as suspicious. We create a CBP by blurring\nthe same latent image with two unknown kernels. The two kernels are coprime\nwhen mapped to bivariate polynomials in the z domain. To deblur the CBP we\nfirst use the coprime constraint to approximate the kernels and sample the\nbivariate CBP polynomials in one dimension on the unit circle. At each sample\npoint, we factor the 1D polynomial pair and compose the results into a 2D\nkernel matrix. Finally, we compute the inverse Fast Fourier Transform (FFT) of\nthe kernel matrices to recover the coprime kernels and then the latent video\nstream. It is therefore only possible to deblur the video stream if a user has\naccess to both streams. To improve the practicability of our algorithm, we\nimplement our algorithm using a graphics processing unit (GPU) to decrypt the\nblurred video streams in real-time, and extensive experimental results\ndemonstrate that our new scheme can effectively protect sensitive identity\ninformation in surveillance videos and faithfully reconstruct the unblurred\nvideo stream when two blurred sequences are available.\n", "versions": [{"version": "v1", "created": "Thu, 22 Mar 2012 02:57:53 GMT"}], "update_date": "2012-03-23", "authors_parsed": [["Thorpe", "Christopher", ""], ["Li", "Feng", ""], ["Li", "Zijia", ""], ["Yu", "Zhan", ""], ["Saunders", "David", ""], ["Yu", "Jingyi", ""]]}, {"id": "1203.5078", "submitter": "Tranos Zuva", "authors": "Tranos Zuva, Oludayo O. Olugbara, Sunday O. Ojo and Seleman M. Ngwira", "title": "Kernel Density Feature Points Estimator for Content-Based Image\n  Retrieval", "comments": "ISSN 0975-5578 (Online) 0975-5934 (Print)", "journal-ref": "Signal & Image Processing: An International Journal (SIPIJ), Vol.4\n  No 1, February 2012, Pages: 103-111", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Research is taking place to find effective algorithms for content-based image\nrepresentation and description. There is a substantial amount of algorithms\navailable that use visual features (color, shape, texture). Shape feature has\nattracted much attention from researchers that there are many shape\nrepresentation and description algorithms in literature. These shape image\nrepresentation and description algorithms are usually not application\nindependent or robust, making them undesirable for generic shape description.\nThis paper presents an object shape representation using Kernel Density Feature\nPoints Estimator (KDFPE). In this method, the density of feature points within\ndefined rings around the centroid of the image is obtained. The KDFPE is then\napplied to the vector of the image. KDFPE is invariant to translation, scale\nand rotation. This method of image representation shows improved retrieval rate\nwhen compared to Density Histogram Feature Points (DHFP) method. Analytic\nanalysis is done to justify our method, which was compared with the DHFP to\nprove its robustness.\n", "versions": [{"version": "v1", "created": "Thu, 22 Mar 2012 18:47:57 GMT"}], "update_date": "2012-03-23", "authors_parsed": [["Zuva", "Tranos", ""], ["Olugbara", "Oludayo O.", ""], ["Ojo", "Sunday O.", ""], ["Ngwira", "Seleman M.", ""]]}, {"id": "1203.5128", "submitter": "Kunal Narayan Chaudhury", "authors": "Kunal N. Chaudhury", "title": "Acceleration of the shiftable O(1) algorithm for bilateral filtering and\n  non-local means", "comments": "10 figures, 6 tables", "journal-ref": "IEEE Transactions on Image Processing, vol. 22(4), pp. 1291- 1300,\n  2013", "doi": "10.1109/TIP.2012.2222903", "report-no": null, "categories": "cs.CV cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A direct implementation of the bilateral filter [1] requires O(\\sigma_s^2)\noperations per pixel, where \\sigma_s is the (effective) width of the spatial\nkernel. A fast implementation of the bilateral filter was recently proposed in\n[2] that required O(1) operations per pixel with respect to \\sigma_s. This was\ndone by using trigonometric functions for the range kernel of the bilateral\nfilter, and by exploiting their so-called shiftability property. In particular,\na fast implementation of the Gaussian bilateral filter was realized by\napproximating the Gaussian range kernel using raised cosines. Later, it was\ndemonstrated in [3] that this idea could be extended to a larger class of\nfilters, including the popular non-local means filter [4]. As already observed\nin [2], a flip side of this approach was that the run time depended on the\nwidth \\sigma_r of the range kernel. For an image with (local) intensity\nvariations in the range [0,T], the run time scaled as O(T^2/\\sigma^2_r) with\n\\sigma_r. This made it difficult to implement narrow range kernels,\nparticularly for images with large dynamic range. We discuss this problem in\nthis note, and propose some simple steps to accelerate the implementation in\ngeneral, and for small \\sigma_r in particular.\n  [1] C. Tomasi and R. Manduchi, \"Bilateral filtering for gray and color\nimages\", Proc. IEEE International Conference on Computer Vision, 1998.\n  [2] K.N. Chaudhury, Daniel Sage, and M. Unser, \"Fast O(1) bilateral filtering\nusing trigonometric range kernels\", IEEE Transactions on Image Processing,\n2011.\n  [3] K.N. Chaudhury, \"Constant-time filtering using shiftable kernels\", IEEE\nSignal Processing Letters, 2011.\n  [4] A. Buades, B. Coll, and J.M. Morel, \"A review of image denoising\nalgorithms, with a new one\", Multiscale Modeling and Simulation, 2005.\n", "versions": [{"version": "v1", "created": "Thu, 22 Mar 2012 21:09:37 GMT"}, {"version": "v2", "created": "Tue, 7 Aug 2012 18:57:45 GMT"}], "update_date": "2015-06-04", "authors_parsed": [["Chaudhury", "Kunal N.", ""]]}, {"id": "1203.5914", "submitter": "Martin Burger", "authors": "Michael Moeller and Martin Burger and Peter Dieterich and Albrecht\n  Schwab", "title": "A Framework for Automated Cell Tracking in Phase Contrast Microscopic\n  Videos based on Normal Velocities", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a novel framework for the automated tracking of cells,\nwith a particular focus on the challenging situation of phase contrast\nmicroscopic videos. Our framework is based on a topology preserving variational\nsegmentation approach applied to normal velocity components obtained from\noptical flow computations, which appears to yield robust tracking and automated\nextraction of cell trajectories. In order to obtain improved trackings of local\nshape features we discuss an additional correction step based on active\ncontours and the image Laplacian which we optimize for an example class of\ntransformed renal epithelial (MDCK-F) cells. We also test the framework for\nhuman melanoma cells and murine neutrophil granulocytes that were seeded on\ndifferent types of extracellular matrices. The results are validated with\nmanual tracking results.\n", "versions": [{"version": "v1", "created": "Tue, 27 Mar 2012 10:05:19 GMT"}], "update_date": "2012-03-28", "authors_parsed": [["Moeller", "Michael", ""], ["Burger", "Martin", ""], ["Dieterich", "Peter", ""], ["Schwab", "Albrecht", ""]]}, {"id": "1203.6329", "submitter": "Arnav Bhavsar", "authors": "Arnav Bhavsar", "title": "Analysis of Magnification in Depth from Defocus", "comments": "Typo fixed from the previous version: Changed 's' to 's^2' in the\n  formula for 'h_r' in the line below equation (11)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In depth from defocus (DFD), when images are captured with different camera\nparameters, a relative magnification is induced between them. Image warping is\na simpler solution to account for magnification than seemingly more accurate\noptical approaches. This work is an investigation into the effects of\nmagnification on the accuracy of DFD. We comment on issues regarding scaling\neffect on relative blur computation. We statistically analyze accountability of\nscale factor, commenting on the bias and efficiency of the estimator that does\nnot consider scale. We also discuss the effect of interpolation errors on blur\nestimation in a warping based solution to handle magnification and carry out\nexperimental analysis to comment on the blur estimation accuracy.\n", "versions": [{"version": "v1", "created": "Wed, 28 Mar 2012 18:16:46 GMT"}, {"version": "v2", "created": "Mon, 5 Nov 2012 15:10:37 GMT"}], "update_date": "2012-11-06", "authors_parsed": [["Bhavsar", "Arnav", ""]]}, {"id": "1203.6722", "submitter": "Vinay Bettadapura", "authors": "Vinay Bettadapura", "title": "Face Expression Recognition and Analysis: The State of the Art", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The automatic recognition of facial expressions has been an active research\ntopic since the early nineties. There have been several advances in the past\nfew years in terms of face detection and tracking, feature extraction\nmechanisms and the techniques used for expression classification. This paper\nsurveys some of the published work since 2001 till date. The paper presents a\ntime-line view of the advances made in this field, the applications of\nautomatic face expression recognizers, the characteristics of an ideal system,\nthe databases that have been used and the advances made in terms of their\nstandardization and a detailed summary of the state of the art. The paper also\ndiscusses facial parameterization using FACS Action Units (AUs) and MPEG-4\nFacial Animation Parameters (FAPs) and the recent advances in face detection,\ntracking and feature extraction methods. Notes have also been presented on\nemotions, expressions and facial features, discussion on the six prototypic\nexpressions and the recent studies on expression classifiers. The paper ends\nwith a note on the challenges and the future work. This paper has been written\nin a tutorial style with the intention of helping students and researchers who\nare new to this field.\n", "versions": [{"version": "v1", "created": "Fri, 30 Mar 2012 05:47:59 GMT"}], "update_date": "2012-04-02", "authors_parsed": [["Bettadapura", "Vinay", ""]]}]