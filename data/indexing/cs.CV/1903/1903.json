[{"id": "1903.00001", "submitter": "Heyi Li", "authors": "Heyi Li, Dongdong Chen, William H. Nailon, Mike E. Davies and Dave\n  Laurenson", "title": "A Deep DUAL-PATH Network for Improved Mammogram Image Processing", "comments": "To Appear in ICCASP 2019 May", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present, for the first time, a novel deep neural network architecture\ncalled \\dcn with a dual-path connection between the input image and output\nclass label for mammogram image processing. This architecture is built upon\nU-Net, which non-linearly maps the input data into a deep latent space. One\npath of the \\dcnn, the locality preserving learner, is devoted to\nhierarchically extracting and exploiting intrinsic features of the input, while\nthe other path, called the conditional graph learner, focuses on modeling the\ninput-mask correlations. The learned mask is further used to improve\nclassification results, and the two learning paths complement each other. By\nintegrating the two learners our new architecture provides a simple but\neffective way to jointly learn the segmentation and predict the class label.\nBenefiting from the powerful expressive capacity of deep neural networks a more\ndiscriminative representation can be learned, in which both the semantics and\nstructure are well preserved. Experimental results show that \\dcn achieves the\nbest mammography segmentation and classification simultaneously, outperforming\nrecent state-of-the-art models.\n", "versions": [{"version": "v1", "created": "Fri, 1 Mar 2019 11:51:47 GMT"}], "update_date": "2019-03-04", "authors_parsed": [["Li", "Heyi", ""], ["Chen", "Dongdong", ""], ["Nailon", "William H.", ""], ["Davies", "Mike E.", ""], ["Laurenson", "Dave", ""]]}, {"id": "1903.00035", "submitter": "Yizhe Zhang", "authors": "Yizhe Zhang, Lin Yang, Hao Zheng, Peixian Liang, Colleen Mangold,\n  Raquel G. Loreto, David P. Hughes, Danny Z. Chen", "title": "SPDA: Superpixel-based Data Augmentation for Biomedical Image\n  Segmentation", "comments": "To appear in MIDL2019 and PMLR", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Supervised training a deep neural network aims to \"teach\" the network to\nmimic human visual perception that is represented by image-and-label pairs in\nthe training data. Superpixelized (SP) images are visually perceivable to\nhumans, but a conventionally trained deep learning model often performs poorly\nwhen working on SP images. To better mimic human visual perception, we think it\nis desirable for the deep learning model to be able to perceive not only raw\nimages but also SP images. In this paper, we propose a new superpixel-based\ndata augmentation (SPDA) method for training deep learning models for\nbiomedical image segmentation. Our method applies a superpixel generation\nscheme to all the original training images to generate superpixelized images.\nThe SP images thus obtained are then jointly used with the original training\nimages to train a deep learning model. Our experiments of SPDA on four\nbiomedical image datasets show that SPDA is effective and can consistently\nimprove the performance of state-of-the-art fully convolutional networks for\nbiomedical image segmentation in 2D and 3D images. Additional studies also\ndemonstrate that SPDA can practically reduce the generalization gap.\n", "versions": [{"version": "v1", "created": "Thu, 28 Feb 2019 19:17:51 GMT"}], "update_date": "2019-03-04", "authors_parsed": [["Zhang", "Yizhe", ""], ["Yang", "Lin", ""], ["Zheng", "Hao", ""], ["Liang", "Peixian", ""], ["Mangold", "Colleen", ""], ["Loreto", "Raquel G.", ""], ["Hughes", "David P.", ""], ["Chen", "Danny Z.", ""]]}, {"id": "1903.00045", "submitter": "Tian Guo", "authors": "Shijian Li and Robert J. Walls and Lijie Xu and Tian Guo", "title": "Speeding up Deep Learning with Transient Servers", "comments": "Accepted to ICAC'19. 11 pages, 8 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF cs.CV cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distributed training frameworks, like TensorFlow, have been proposed as a\nmeans to reduce the training time of deep learning models by using a cluster of\nGPU servers. While such speedups are often desirable---e.g., for rapidly\nevaluating new model designs---they often come with significantly higher\nmonetary costs due to sublinear scalability. In this paper, we investigate the\nfeasibility of using training clusters composed of cheaper transient GPU\nservers to get the benefits of distributed training without the high costs.\n  We conduct the first large-scale empirical analysis, launching more than a\nthousand GPU servers of various capacities, aimed at understanding the\ncharacteristics of transient GPU servers and their impact on distributed\ntraining performance. Our study demonstrates the potential of transient servers\nwith a speedup of 7.7X with more than 62.9% monetary savings for some cluster\nconfigurations. We also identify a number of important challenges and\nopportunities for redesigning distributed training frameworks to be\ntransient-aware. For example, the dynamic cost and availability characteristics\nof transient servers suggest the need for frameworks to dynamically change\ncluster configurations to best take advantage of current conditions.\n", "versions": [{"version": "v1", "created": "Thu, 28 Feb 2019 19:47:59 GMT"}, {"version": "v2", "created": "Sun, 5 May 2019 07:20:37 GMT"}], "update_date": "2019-05-07", "authors_parsed": [["Li", "Shijian", ""], ["Walls", "Robert J.", ""], ["Xu", "Lijie", ""], ["Guo", "Tian", ""]]}, {"id": "1903.00068", "submitter": "Xinyun Zou", "authors": "Xinyun Zou, Soheil Kolouri, Praveen K. Pilly, Jeffrey L. Krichmar", "title": "Neuromodulated Goal-Driven Perception in Uncertain Domains", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In uncertain domains, the goals are often unknown and need to be predicted by\nthe organism or system. In this paper, contrastive excitation backprop (c-EB)\nwas used in a goal-driven perception task with pairs of noisy MNIST digits,\nwhere the system had to increase attention to one of the two digits\ncorresponding to a goal (i.e., even, odd, low value, or high value) and\ndecrease attention to the distractor digit or noisy background pixels. Because\nthe valid goal was unknown, an online learning model based on the cholinergic\nand noradrenergic neuromodulatory systems was used to predict a noisy goal\n(expected uncertainty) and re-adapt when the goal changed (unexpected\nuncertainty). This neurobiologically plausible model demonstrates how\nneuromodulatory systems can predict goals in uncertain domains and how\nattentional mechanisms can enhance the perception of that goal.\n", "versions": [{"version": "v1", "created": "Sat, 16 Feb 2019 19:46:09 GMT"}], "update_date": "2019-03-04", "authors_parsed": [["Zou", "Xinyun", ""], ["Kolouri", "Soheil", ""], ["Pilly", "Praveen K.", ""], ["Krichmar", "Jeffrey L.", ""]]}, {"id": "1903.00073", "submitter": "Yash Sharma", "authors": "Yash Sharma, Gavin Weiguang Ding, Marcus Brubaker", "title": "On the Effectiveness of Low Frequency Perturbations", "comments": "IJCAI 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Carefully crafted, often imperceptible, adversarial perturbations have been\nshown to cause state-of-the-art models to yield extremely inaccurate outputs,\nrendering them unsuitable for safety-critical application domains. In addition,\nrecent work has shown that constraining the attack space to a low frequency\nregime is particularly effective. Yet, it remains unclear whether this is due\nto generally constraining the attack search space or specifically removing high\nfrequency components from consideration. By systematically controlling the\nfrequency components of the perturbation, evaluating against the top-placing\ndefense submissions in the NeurIPS 2017 competition, we empirically show that\nperformance improvements in both the white-box and black-box transfer settings\nare yielded only when low frequency components are preserved. In fact, the\ndefended models based on adversarial training are roughly as vulnerable to low\nfrequency perturbations as undefended models, suggesting that the purported\nrobustness of state-of-the-art ImageNet defenses is reliant upon adversarial\nperturbations being high frequency in nature. We do find that under\n$\\ell_\\infty$ $\\epsilon=16/255$, the competition distortion bound, low\nfrequency perturbations are indeed perceptible. This questions the use of the\n$\\ell_\\infty$-norm, in particular, as a distortion metric, and, in turn,\nsuggests that explicitly considering the frequency space is promising for\nlearning robust models which better align with human perception.\n", "versions": [{"version": "v1", "created": "Thu, 28 Feb 2019 21:25:45 GMT"}, {"version": "v2", "created": "Sat, 1 Jun 2019 00:36:16 GMT"}], "update_date": "2019-06-04", "authors_parsed": [["Sharma", "Yash", ""], ["Ding", "Gavin Weiguang", ""], ["Brubaker", "Marcus", ""]]}, {"id": "1903.00087", "submitter": "Alakh Aggarwal", "authors": "Shailesh Shrivastava, Alakh Aggarwal, Pratik Chattopadhyay", "title": "Broad Neural Network for Change Detection in Aerial Images", "comments": "$\\textbf{Accepted at}$: IEMGraph (International Conference on\n  Emerging Technology in Modelling and Graphics) 2018 $$ $$ $\\textbf{Date of\n  Conference}$: 6-7 September, 2018 $$ $$ $\\textbf{Location of Conference}$:\n  Kolkatta, India", "journal-ref": "Advances in Intelligent Systems and Computing book series (AISC,\n  volume 937), Springer, Singapore, 2019", "doi": "10.1007/978-981-13-7403-6_31", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A change detection system takes as input two images of a region captured at\ntwo different times, and predicts which pixels in the region have undergone\nchange over the time period. Since pixel-based analysis can be erroneous due to\nnoise, illumination difference and other factors, contextual information is\nusually used to determine the class of a pixel (changed or not). This\ncontextual information is taken into account by considering a pixel of the\ndifference image along with its neighborhood. With the help of ground truth\ninformation, the labeled patterns are generated. Finally, Broad Learning\nclassifier is used to get prediction about the class of each pixel. Results\nshow that Broad Learning can classify the data set with a significantly higher\nF-Score than that of Multilayer Perceptron. Performance comparison has also\nbeen made with other popular classifiers, namely Multilayer Perceptron and\nRandom Forest.\n", "versions": [{"version": "v1", "created": "Thu, 28 Feb 2019 22:16:56 GMT"}, {"version": "v2", "created": "Sat, 20 Jul 2019 13:51:47 GMT"}], "update_date": "2019-07-23", "authors_parsed": [["Shrivastava", "Shailesh", ""], ["Aggarwal", "Alakh", ""], ["Chattopadhyay", "Pratik", ""]]}, {"id": "1903.00100", "submitter": "Shaojie Xu", "authors": "Shaojie Xu, Anvesha Amaravati, Justin Romberg, Arijit Raychowdhury", "title": "Appearance-based Gesture recognition in the compressed domain", "comments": "arXiv admin note: text overlap with arXiv:1605.08313", "journal-ref": "2017 IEEE International Conference on Acoustics, Speech and Signal\n  Processing (ICASSP), New Orleans, LA, 2017, pp. 1722-1726", "doi": "10.1109/ICASSP.2017.7952451", "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel appearance-based gesture recognition algorithm using\ncompressed domain signal processing techniques. Gesture features are extracted\ndirectly from the compressed measurements, which are the block averages and the\ncoded linear combinations of the image sensor's pixel values. We also improve\nboth the computational efficiency and the memory requirement of the previous\nDTW-based K-NN gesture classifiers. Both simulation testing and hardware\nimplementation strongly support the proposed algorithm.\n", "versions": [{"version": "v1", "created": "Tue, 19 Feb 2019 06:05:12 GMT"}], "update_date": "2019-03-04", "authors_parsed": [["Xu", "Shaojie", ""], ["Amaravati", "Anvesha", ""], ["Romberg", "Justin", ""], ["Raychowdhury", "Arijit", ""]]}, {"id": "1903.00102", "submitter": "Raffi Al-Qurran Raffi", "authors": "Ghadeer Al-Bdour, Raffi Al-Qurran, Mahmoud Al-Ayyoub, Ali Shatnawi", "title": "A detailed comparative study of open source deep learning frameworks", "comments": "26 pages, 25 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Learning (DL) is one of the hottest trends in machine learning as DL\napproaches produced results superior to the state-of-the-art in problematic\nareas such as image processing and natural language processing (NLP). To foster\nthe growth of DL, several open source frameworks appeared providing\nimplementations of the most common DL algorithms. These frameworks vary in the\nalgorithms they support and in the quality of their implementations. The\npurpose of this work is to provide a qualitative and quantitative comparison\namong three of the most popular and most comprehensive DL frameworks (namely\nGoogle's TensorFlow, University of Montreal's Theano and Microsoft's CNTK). The\nultimate goal of this work is to help end users make an informed decision about\nthe best DL framework that suits their needs and resources. To ensure that our\nstudy is as comprehensive as possible, we conduct several experiments using\nmultiple benchmark datasets from different fields (image processing, NLP, etc.)\nand measure the performance of the frameworks' implementations of different DL\nalgorithms. For most of our experiments, we find out that CNTK's\nimplementations are superior to the other ones under consideration.\n", "versions": [{"version": "v1", "created": "Mon, 25 Feb 2019 20:10:54 GMT"}, {"version": "v2", "created": "Wed, 6 May 2020 11:57:54 GMT"}], "update_date": "2020-05-07", "authors_parsed": [["Al-Bdour", "Ghadeer", ""], ["Al-Qurran", "Raffi", ""], ["Al-Ayyoub", "Mahmoud", ""], ["Shatnawi", "Ali", ""]]}, {"id": "1903.00107", "submitter": "Shuang Zhang", "authors": "Shuang Zhang, Ada Zhen, Robert L. Stevenson", "title": "GAN Based Image Deblurring Using Dark Channel Prior", "comments": "5 pages, 3 figures. Conference: Electronic Imaging", "journal-ref": null, "doi": "10.2352/ISSN.2470-1173.2019.13.COIMG-136", "report-no": null, "categories": "cs.CV eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A conditional general adversarial network (GAN) is proposed for image\ndeblurring problem. It is tailored for image deblurring instead of just\napplying GAN on the deblurring problem. Motivated by that, dark channel prior\nis carefully picked to be incorporated into the loss function for network\ntraining. To make it more compatible with neuron networks, its original\nindifferentiable form is discarded and L2 norm is adopted instead. On both\nsynthetic datasets and noisy natural images, the proposed network shows\nimproved deblurring performance and robustness to image noise qualitatively and\nquantitatively. Additionally, compared to the existing end-to-end deblurring\nnetworks, our network structure is light-weight, which ensures less training\nand testing time.\n", "versions": [{"version": "v1", "created": "Thu, 28 Feb 2019 23:21:30 GMT"}], "update_date": "2019-03-04", "authors_parsed": [["Zhang", "Shuang", ""], ["Zhen", "Ada", ""], ["Stevenson", "Robert L.", ""]]}, {"id": "1903.00110", "submitter": "Mohamed Elfeki", "authors": "Mohamed Elfeki and Ali Borji", "title": "Video Summarization via Actionness Ranking", "comments": null, "journal-ref": "Published in WACV-2019 as an Oral", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To automatically produce a brief yet expressive summary of a long video, an\nautomatic algorithm should start by resembling the human process of summary\ngeneration. Prior work proposed supervised and unsupervised algorithms to train\nmodels for learning the underlying behavior of humans by increasing modeling\ncomplexity or craft-designing better heuristics to simulate human summary\ngeneration process. In this work, we take a different approach by analyzing a\nmajor cue that humans exploit for the summary generation; the nature and\nintensity of actions.\n  We empirically observed that a frame is more likely to be included in\nhuman-generated summaries if it contains a substantial amount of deliberate\nmotion performed by an agent, which is referred to as actionness. Therefore, we\nhypothesize that learning to automatically generate summaries involves an\nimplicit knowledge of actionness estimation and ranking. We validate our\nhypothesis by running a user study that explores the correlation between\nhuman-generated summaries and actionness ranks. We also run a consensus and\nbehavioral analysis between human subjects to ensure reliable and consistent\nresults. The analysis exhibits a considerable degree of agreement among\nsubjects within obtained data and verifying our initial hypothesis.\n  Based on the study findings, we develop a method to incorporate actionness\ndata to explicitly regulate a learning algorithm that is trained for summary\ngeneration. We assess the performance of our approach to four summarization\nbenchmark datasets and demonstrate an evident advantage compared to\nstate-of-the-art summarization methods.\n", "versions": [{"version": "v1", "created": "Fri, 1 Mar 2019 00:03:14 GMT"}], "update_date": "2019-03-04", "authors_parsed": [["Elfeki", "Mohamed", ""], ["Borji", "Ali", ""]]}, {"id": "1903.00112", "submitter": "Huangying Zhan", "authors": "Huangying Zhan, Chamara Saroj Weerasekera, Ravi Garg, Ian Reid", "title": "Self-supervised Learning for Single View Depth and Surface Normal\n  Estimation", "comments": "6 pages, 3 figures, ICRA 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we present a self-supervised learning framework to\nsimultaneously train two Convolutional Neural Networks (CNNs) to predict depth\nand surface normals from a single image. In contrast to most existing\nframeworks which represent outdoor scenes as fronto-parallel planes at\npiece-wise smooth depth, we propose to predict depth with surface orientation\nwhile assuming that natural scenes have piece-wise smooth normals. We show that\na simple depth-normal consistency as a soft-constraint on the predictions is\nsufficient and effective for training both these networks simultaneously. The\ntrained normal network provides state-of-the-art predictions while the depth\nnetwork, relying on much realistic smooth normal assumption, outperforms the\ntraditional self-supervised depth prediction network by a large margin on the\nKITTI benchmark. Demo video: https://youtu.be/ZD-ZRsw7hdM\n", "versions": [{"version": "v1", "created": "Fri, 1 Mar 2019 00:07:12 GMT"}], "update_date": "2019-03-04", "authors_parsed": [["Zhan", "Huangying", ""], ["Weerasekera", "Chamara Saroj", ""], ["Garg", "Ravi", ""], ["Reid", "Ian", ""]]}, {"id": "1903.00117", "submitter": "Pingping Zhang Dr", "authors": "Yinjie Lei, Ziqin Zhou, Pingping Zhang, Yulan Guo, Zijun Ma, Lingqiao\n  Liu", "title": "A Sketch Based 3D Shape Retrieval Approach Based on Efficient Deep\n  Point-to-Subspace Metric Learning", "comments": "The first author wants to withdraw this paper. He has noticed several\n  setting errors in experiment parts", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A sketch based 3D shape retrieval\n", "versions": [{"version": "v1", "created": "Fri, 1 Mar 2019 01:13:30 GMT"}, {"version": "v2", "created": "Wed, 13 Mar 2019 14:07:45 GMT"}], "update_date": "2019-03-14", "authors_parsed": [["Lei", "Yinjie", ""], ["Zhou", "Ziqin", ""], ["Zhang", "Pingping", ""], ["Guo", "Yulan", ""], ["Ma", "Zijun", ""], ["Liu", "Lingqiao", ""]]}, {"id": "1903.00119", "submitter": "Matthew Cong", "authors": "Matthew Cong, Lana Lan, Ronald Fedkiw", "title": "Local Geometric Indexing of High Resolution Data for Facial\n  Reconstruction from Sparse Markers", "comments": "8 pages. Includes figures which were previously redacted. Added\n  acknowledgements section and minor changes to text", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When considering sparse motion capture marker data, one typically struggles\nto balance its overfitting via a high dimensional blendshape system versus\nunderfitting caused by smoothness constraints. With the current trend towards\nusing more and more data, our aim is not to fit the motion capture markers with\na parameterized (blendshape) model or to smoothly interpolate a surface through\nthe marker positions, but rather to find an instance in the high resolution\ndataset that contains local geometry to fit each marker. Just as is true for\ntypical machine learning applications, this approach benefits from a plethora\nof data, and thus we also consider augmenting the dataset via specially\ndesigned physical simulations that target the high resolution dataset such that\nthe simulation output lies on the same so-called manifold as the data targeted.\n", "versions": [{"version": "v1", "created": "Fri, 1 Mar 2019 01:37:46 GMT"}, {"version": "v2", "created": "Wed, 4 Sep 2019 23:38:29 GMT"}], "update_date": "2019-09-06", "authors_parsed": [["Cong", "Matthew", ""], ["Lan", "Lana", ""], ["Fedkiw", "Ronald", ""]]}, {"id": "1903.00133", "submitter": "Robert Pottorff", "authors": "Robert Pottorff, Jared Nielsen, David Wingate", "title": "Video Extrapolation with an Invertible Linear Embedding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We predict future video frames from complex dynamic scenes, using an\ninvertible neural network as the encoder of a nonlinear dynamic system with\nlatent linear state evolution. Our invertible linear embedding (ILE)\ndemonstrates successful learning, prediction and latent state inference. In\ncontrast to other approaches, ILE does not use any explicit reconstruction loss\nor simplistic pixel-space assumptions. Instead, it leverages invertibility to\noptimize the likelihood of image sequences exactly, albeit indirectly.\nComparison with a state-of-the-art method demonstrates the viability of our\napproach.\n", "versions": [{"version": "v1", "created": "Fri, 1 Mar 2019 02:52:51 GMT"}], "update_date": "2019-03-04", "authors_parsed": [["Pottorff", "Robert", ""], ["Nielsen", "Jared", ""], ["Wingate", "David", ""]]}, {"id": "1903.00142", "submitter": "Steven Spratley", "authors": "Steven Spratley, Daniel Beck, and Trevor Cohn", "title": "A Unified Neural Architecture for Instrumental Audio Tasks", "comments": "To appear in Proc. ICASSP 2019, May 12-17, Brighton, UK", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.CV cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Within Music Information Retrieval (MIR), prominent tasks -- including\npitch-tracking, source-separation, super-resolution, and synthesis -- typically\ncall for specialised methods, despite their similarities. Conditional\nGenerative Adversarial Networks (cGANs) have been shown to be highly versatile\nin learning general image-to-image translations, but have not yet been adapted\nacross MIR. In this work, we present an end-to-end supervisable architecture to\nperform all aforementioned audio tasks, consisting of a WaveNet synthesiser\nconditioned on the output of a jointly-trained cGAN spectrogram translator. In\ndoing so, we demonstrate the potential of such flexible techniques to unify MIR\ntasks, promote efficient transfer learning, and converge research to the\nimprovement of powerful, general methods. Finally, to the best of our\nknowledge, we present the first application of GANs to guided instrument\nsynthesis.\n", "versions": [{"version": "v1", "created": "Fri, 1 Mar 2019 03:28:54 GMT"}], "update_date": "2019-03-04", "authors_parsed": [["Spratley", "Steven", ""], ["Beck", "Daniel", ""], ["Cohn", "Trevor", ""]]}, {"id": "1903.00159", "submitter": "Sixing Hu", "authors": "Sixing Hu and Gim Hee Lee", "title": "Image-Based Geo-Localization Using Satellite Imagery", "comments": "IJCV preprint", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of localization on a geo-referenced satellite map given a query\nground view image is useful yet remains challenging due to the drastic change\nin viewpoint. To this end, in this paper we work on the extension of our\nearlier work on the Cross-View Matching Network (CVM-Net) for the\nground-to-aerial image matching task since the traditional image descriptors\nfail due to the drastic viewpoint change. In particular, we show more extensive\nexperimental results and analyses of the network architecture on our CVM-Net.\nFurthermore, we propose a Markov localization framework that enforces the\ntemporal consistency between image frames to enhance the geo-localization\nresults in the case where a video stream of ground view images is available.\nExperimental results show that our proposed Markov localization framework can\ncontinuously localize the vehicle within a small error on our Singapore\ndataset.\n", "versions": [{"version": "v1", "created": "Fri, 1 Mar 2019 05:20:10 GMT"}, {"version": "v2", "created": "Mon, 4 Mar 2019 03:26:54 GMT"}, {"version": "v3", "created": "Sun, 2 Jun 2019 02:54:20 GMT"}], "update_date": "2019-06-04", "authors_parsed": [["Hu", "Sixing", ""], ["Lee", "Gim Hee", ""]]}, {"id": "1903.00179", "submitter": "Ting Zhao", "authors": "Ting Zhao and Xiangqian Wu", "title": "Pyramid Feature Attention Network for Saliency detection", "comments": "Accepted by CVPR2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Saliency detection is one of the basic challenges in computer vision. How to\nextract effective features is a critical point for saliency detection. Recent\nmethods mainly adopt integrating multi-scale convolutional features\nindiscriminately. However, not all features are useful for saliency detection\nand some even cause interferences. To solve this problem, we propose Pyramid\nFeature Attention network to focus on effective high-level context features and\nlow-level spatial structural features. First, we design Context-aware Pyramid\nFeature Extraction (CPFE) module for multi-scale high-level feature maps to\ncapture rich context features. Second, we adopt channel-wise attention (CA)\nafter CPFE feature maps and spatial attention (SA) after low-level feature\nmaps, then fuse outputs of CA & SA together. Finally, we propose an edge\npreservation loss to guide network to learn more detailed information in\nboundary localization. Extensive evaluations on five benchmark datasets\ndemonstrate that the proposed method outperforms the state-of-the-art\napproaches under different evaluation metrics.\n", "versions": [{"version": "v1", "created": "Fri, 1 Mar 2019 06:58:09 GMT"}, {"version": "v2", "created": "Thu, 4 Apr 2019 13:25:55 GMT"}], "update_date": "2019-04-05", "authors_parsed": [["Zhao", "Ting", ""], ["Wu", "Xiangqian", ""]]}, {"id": "1903.00183", "submitter": "Guocai He", "authors": "Guocai He", "title": "Lung CT Imaging Sign Classification through Deep Learning on Small Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The annotated medical images are usually expensive to be collected. This\npaper proposes a deep learning method on small data to classify Common Imaging\nSigns of Lung diseases (CISL) in computed tomography (CT) images. We explore\nboth the real data and the data generated by Generative Adversarial Network\n(GAN) to improve the reliability and the generalization of learning. First, we\nuse GAN to generate a large number of CISLs from small annotated data, which\nare difficult to be distinguished from real counterparts. These generated\nsamples are used to pre-train a Convolutional Neural Network (CNN) for\nclassifying CISLs. Second, we fine-tune the CNN classification model with real\ndata. Experiments were conducted on the LISS database of CISLs. We successfully\nconvinced radiologists that our generated CISLs samples were real for 56.7% of\nour experiments. The pre-trained CNN model achieves 88.4% of mean accuracy of\nbinary classification, and after fine-tuning, the mean accuracy is\nsignificantly increased to 95.0%. For multi-classification of all types of\nCISLs and normal tissues, through the two stages of training, the mean\naccuracy, sensitivity and specificity are up to about 91.83%, 92.73% and 99.0%,\nrespectively. To our knowledge, this is the best result achieved on the LISS\ndatabase, which demonstrates that the proposed method is effective and\npromising for fulfilling deep learning on small data.\n", "versions": [{"version": "v1", "created": "Fri, 1 Mar 2019 07:34:35 GMT"}], "update_date": "2019-03-04", "authors_parsed": [["He", "Guocai", ""]]}, {"id": "1903.00231", "submitter": "Yuchao Dai Dr.", "authors": "Liyuan Pan, Yuchao Dai, Miaomiao Liu", "title": "Single Image Deblurring and Camera Motion Estimation with Depth Map", "comments": "Accepted by WACV 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Camera shake during exposure is a major problem in hand-held photography, as\nit causes image blur that destroys details in the captured images.~In the real\nworld, such blur is mainly caused by both the camera motion and the complex\nscene structure.~While considerable existing approaches have been proposed\nbased on various assumptions regarding the scene structure or the camera\nmotion, few existing methods could handle the real 6 DoF camera motion.~In this\npaper, we propose to jointly estimate the 6 DoF camera motion and remove the\nnon-uniform blur caused by camera motion by exploiting their underlying\ngeometric relationships, with a single blurry image and its depth map (either\ndirect depth measurements, or a learned depth map) as input.~We formulate our\njoint deblurring and 6 DoF camera motion estimation as an energy minimization\nproblem which is solved in an alternative manner. Our model enables the\nrecovery of the 6 DoF camera motion and the latent clean image, which could\nalso achieve the goal of generating a sharp sequence from a single blurry\nimage. Experiments on challenging real-world and synthetic datasets demonstrate\nthat image blur from camera shake can be well addressed within our proposed\nframework.\n", "versions": [{"version": "v1", "created": "Fri, 1 Mar 2019 10:08:08 GMT"}], "update_date": "2019-03-04", "authors_parsed": [["Pan", "Liyuan", ""], ["Dai", "Yuchao", ""], ["Liu", "Miaomiao", ""]]}, {"id": "1903.00241", "submitter": "Xinggang Wang", "authors": "Zhaojin Huang, Lichao Huang, Yongchao Gong, Chang Huang, Xinggang Wang", "title": "Mask Scoring R-CNN", "comments": "Accepted in CVPR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Letting a deep network be aware of the quality of its own predictions is an\ninteresting yet important problem. In the task of instance segmentation, the\nconfidence of instance classification is used as mask quality score in most\ninstance segmentation frameworks. However, the mask quality, quantified as the\nIoU between the instance mask and its ground truth, is usually not well\ncorrelated with classification score. In this paper, we study this problem and\npropose Mask Scoring R-CNN which contains a network block to learn the quality\nof the predicted instance masks. The proposed network block takes the instance\nfeature and the corresponding predicted mask together to regress the mask IoU.\nThe mask scoring strategy calibrates the misalignment between mask quality and\nmask score, and improves instance segmentation performance by prioritizing more\naccurate mask predictions during COCO AP evaluation. By extensive evaluations\non the COCO dataset, Mask Scoring R-CNN brings consistent and noticeable gain\nwith different models, and outperforms the state-of-the-art Mask R-CNN. We hope\nour simple and effective approach will provide a new direction for improving\ninstance segmentation. The source code of our method is available at\n\\url{https://github.com/zjhuang22/maskscoring_rcnn}.\n", "versions": [{"version": "v1", "created": "Fri, 1 Mar 2019 10:38:57 GMT"}], "update_date": "2019-03-04", "authors_parsed": [["Huang", "Zhaojin", ""], ["Huang", "Lichao", ""], ["Gong", "Yongchao", ""], ["Huang", "Chang", ""], ["Wang", "Xinggang", ""]]}, {"id": "1903.00252", "submitter": "Ji Liu", "authors": "Ji Liu and Lei Zhang", "title": "Optimal Projection Guided Transfer Hashing for Image Retrieval", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, learning to hash has been widely studied for image retrieval thanks\nto the computation and storage efficiency of binary codes. For most existing\nlearning to hash methods, sufficient training images are required and used to\nlearn precise hashing codes. However, in some real-world applications, there\nare not always sufficient training images in the domain of interest. In\naddition, some existing supervised approaches need a amount of labeled data,\nwhich is an expensive process in term of time, label and human expertise. To\nhandle such problems, inspired by transfer learning, we propose a simple yet\neffective unsupervised hashing method named Optimal Projection Guided Transfer\nHashing (GTH) where we borrow the images of other different but related domain\ni.e., source domain to help learn precise hashing codes for the domain of\ninterest i.e., target domain. Besides, we propose to seek for the maximum\nlikelihood estimation (MLE) solution of the hashing functions of target and\nsource domains due to the domain gap. Furthermore,an alternating optimization\nmethod is adopted to obtain the two projections of target and source domains\nsuch that the domain hashing disparity is reduced gradually. Extensive\nexperiments on various benchmark databases verify that our method outperforms\nmany state-of-the-art learning to hash methods. The implementation details are\navailable at https://github.com/liuji93/GTH.\n", "versions": [{"version": "v1", "created": "Fri, 1 Mar 2019 11:43:31 GMT"}], "update_date": "2019-03-04", "authors_parsed": [["Liu", "Ji", ""], ["Zhang", "Lei", ""]]}, {"id": "1903.00258", "submitter": "Ben Lonnqvist", "authors": "Ben Lonnqvist, Alasdair D. F. Clarke, Ramakrishna Chakravarthi", "title": "Crowding in humans is unlike that in convolutional neural networks", "comments": "34 pages, 30 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Object recognition is a primary function of the human visual system. It has\nrecently been claimed that the highly successful ability to recognise objects\nin a set of emergent computer vision systems---Deep Convolutional Neural\nNetworks (DCNNs)---can form a useful guide to recognition in humans. To test\nthis assertion, we systematically evaluated visual crowding, a dramatic\nbreakdown of recognition in clutter, in DCNNs and compared their performance to\nextant research in humans. We examined crowding in three architectures of DCNNs\nwith the same methodology as that used among humans. We manipulated multiple\nstimulus factors including inter-letter spacing, letter colour, size, and\nflanker location to assess the extent and shape of crowding in DCNNs. We found\nthat crowding followed a predictable pattern across architectures that was\ndifferent from that in humans. Some characteristic hallmarks of human crowding,\nsuch as invariance to size, the effect of target-flanker similarity, and\nconfusions between target and flanker identities, were completely missing,\nminimised or even reversed. These data show that DCNNs, while proficient in\nobject recognition, likely achieve this competence through a set of mechanisms\nthat are distinct from those in humans. They are not necessarily equivalent\nmodels of human or primate object recognition and caution must be exercised\nwhen inferring mechanisms derived from their operation.\n", "versions": [{"version": "v1", "created": "Fri, 1 Mar 2019 12:03:19 GMT"}, {"version": "v2", "created": "Mon, 25 Nov 2019 12:43:09 GMT"}], "update_date": "2019-11-26", "authors_parsed": [["Lonnqvist", "Ben", ""], ["Clarke", "Alasdair D. F.", ""], ["Chakravarthi", "Ramakrishna", ""]]}, {"id": "1903.00268", "submitter": "Margarita Grinvald", "authors": "Margarita Grinvald, Fadri Furrer, Tonci Novkovic, Jen Jen Chung, Cesar\n  Cadena, Roland Siegwart, Juan Nieto", "title": "Volumetric Instance-Aware Semantic Mapping and 3D Object Discovery", "comments": "8 pages, 4 figures. To be published in IEEE Robotics and Automation\n  Letters (RA-L) and 2019 IEEE/RSJ International Conference on Intelligent\n  Robots and Systems (IROS). Accompanying video material can be found at\n  http://youtu.be/Jvl42VJmYxg", "journal-ref": "IEEE Robotics and Automation Letters, vol. 4, no. 3, pp.\n  3037-3044, July 2019", "doi": "10.1109/LRA.2019.2923960", "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To autonomously navigate and plan interactions in real-world environments,\nrobots require the ability to robustly perceive and map complex, unstructured\nsurrounding scenes. Besides building an internal representation of the observed\nscene geometry, the key insight toward a truly functional understanding of the\nenvironment is the usage of higher-level entities during mapping, such as\nindividual object instances. We propose an approach to incrementally build\nvolumetric object-centric maps during online scanning with a localized RGB-D\ncamera. First, a per-frame segmentation scheme combines an unsupervised\ngeometric approach with instance-aware semantic object predictions. This allows\nus to detect and segment elements both from the set of known classes and from\nother, previously unseen categories. Next, a data association step tracks the\npredicted instances across the different frames. Finally, a map integration\nstrategy fuses information about their 3D shape, location, and, if available,\nsemantic class into a global volume. Evaluation on a publicly available dataset\nshows that the proposed approach for building instance-level semantic maps is\ncompetitive with state-of-the-art methods, while additionally able to discover\nobjects of unseen categories. The system is further evaluated within a\nreal-world robotic mapping setup, for which qualitative results highlight the\nonline nature of the method.\n", "versions": [{"version": "v1", "created": "Fri, 1 Mar 2019 12:32:46 GMT"}, {"version": "v2", "created": "Wed, 10 Jul 2019 14:39:33 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Grinvald", "Margarita", ""], ["Furrer", "Fadri", ""], ["Novkovic", "Tonci", ""], ["Chung", "Jen Jen", ""], ["Cadena", "Cesar", ""], ["Siegwart", "Roland", ""], ["Nieto", "Juan", ""]]}, {"id": "1903.00271", "submitter": "Hafez Farazi", "authors": "Hafez Farazi and Sven Behnke", "title": "Frequency Domain Transformer Networks for Video Prediction", "comments": "Accepted for European Symposium on Artificial Neural Networks,\n  Computational Intelligence and Machine Learning (ESANN), Bruges, Belgium, to\n  appear April 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The task of video prediction is forecasting the next frames given some\nprevious frames. Despite much recent progress, this task is still challenging\nmainly due to high nonlinearity in the spatial domain. To address this issue,\nwe propose a novel architecture, Frequency Domain Transformer Network (FDTN),\nwhich is an end-to-end learnable model that estimates and uses the\ntransformations of the signal in the frequency domain. Experimental evaluations\nshow that this approach can outperform some widely used video prediction\nmethods like Video Ladder Network (VLN) and Predictive Gated Pyramids (PGP).\n", "versions": [{"version": "v1", "created": "Fri, 1 Mar 2019 12:50:15 GMT"}], "update_date": "2019-03-04", "authors_parsed": [["Farazi", "Hafez", ""], ["Behnke", "Sven", ""]]}, {"id": "1903.00277", "submitter": "Bastien Moysset", "authors": "Eloi Alonso, Bastien Moysset, Ronaldo Messina", "title": "Adversarial Generation of Handwritten Text Images Conditioned on\n  Sequences", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  State-of-the-art offline handwriting text recognition systems tend to use\nneural networks and therefore require a large amount of annotated data to be\ntrained. In order to partially satisfy this requirement, we propose a system\nbased on Generative Adversarial Networks (GAN) to produce synthetic images of\nhandwritten words. We use bidirectional LSTM recurrent layers to get an\nembedding of the word to be rendered, and we feed it to the generator network.\nWe also modify the standard GAN by adding an auxiliary network for text\nrecognition. The system is then trained with a balanced combination of an\nadversarial loss and a CTC loss. Together, these extensions to GAN enable to\ncontrol the textual content of the generated word images. We obtain realistic\nimages on both French and Arabic datasets, and we show that integrating these\nsynthetic images into the existing training data of a text recognition system\ncan slightly enhance its performance.\n", "versions": [{"version": "v1", "created": "Fri, 1 Mar 2019 13:11:44 GMT"}], "update_date": "2020-11-12", "authors_parsed": [["Alonso", "Eloi", ""], ["Moysset", "Bastien", ""], ["Messina", "Ronaldo", ""]]}, {"id": "1903.00289", "submitter": "Tony Lindeberg", "authors": "Tony Lindeberg", "title": "Provably scale-covariant networks from oriented quasi quadrature\n  measures in cascade", "comments": "12 pages, 3 figures, 1 table", "journal-ref": "In: Proc. SSVM 2019: Scale Space and Variational Methods in\n  Computer Vision, Springer LNCS volume 11603, pages 328-240", "doi": "10.1007/978-3-030-22368-7_26", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article presents a continuous model for hierarchical networks based on a\ncombination of mathematically derived models of receptive fields and\nbiologically inspired computations. Based on a functional model of complex\ncells in terms of an oriented quasi quadrature combination of first- and\nsecond-order directional Gaussian derivatives, we couple such primitive\ncomputations in cascade over combinatorial expansions over image orientations.\nScale-space properties of the computational primitives are analysed and it is\nshown that the resulting representation allows for provable scale and rotation\ncovariance. A prototype application to texture analysis is developed and it is\ndemonstrated that a simplified mean-reduced representation of the resulting\nQuasiQuadNet leads to promising experimental results on three texture datasets.\n", "versions": [{"version": "v1", "created": "Fri, 1 Mar 2019 13:33:22 GMT"}, {"version": "v2", "created": "Thu, 27 Jun 2019 12:42:26 GMT"}], "update_date": "2019-06-28", "authors_parsed": [["Lindeberg", "Tony", ""]]}, {"id": "1903.00304", "submitter": "Bo Hu", "authors": "Bo Hu, Jianfei Cai, Tat-Jen Cham and Junsong Yuan", "title": "Progress Regression RNN for Online Spatial-Temporal Action Localization\n  in Unconstrained Videos", "comments": "11 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Previous spatial-temporal action localization methods commonly follow the\npipeline of object detection to estimate bounding boxes and labels of actions.\nHowever, the temporal relation of an action has not been fully explored. In\nthis paper, we propose an end-to-end Progress Regression Recurrent Neural\nNetwork (PR-RNN) for online spatial-temporal action localization, which learns\nto infer the action by temporal progress regression. Two new action attributes,\ncalled progression and progress rate, are introduced to describe the temporal\nengagement and relative temporal position of an action. In our method,\nframe-level features are first extracted by a Fully Convolutional Network\n(FCN). Subsequently, detection results and action progress attributes are\nregressed by the Convolutional Gated Recurrent Unit (ConvGRU) based on all the\nobserved frames instead of a single frame or a short clip. Finally, a novel\nonline linking method is designed to connect single-frame results to\nspatial-temporal tubes with the help of the estimated action progress\nattributes. Extensive experiments demonstrate that the progress attributes\nimprove the localization accuracy by providing more precise temporal position\nof an action in unconstrained videos. Our proposed PR-RNN achieves the\nstateof-the-art performance for most of the IoU thresholds on two benchmark\ndatasets.\n", "versions": [{"version": "v1", "created": "Fri, 1 Mar 2019 14:08:21 GMT"}], "update_date": "2019-03-04", "authors_parsed": [["Hu", "Bo", ""], ["Cai", "Jianfei", ""], ["Cham", "Tat-Jen", ""], ["Yuan", "Junsong", ""]]}, {"id": "1903.00317", "submitter": "Erwan Le Merrer", "authors": "Erwan Le Merrer and Gilles Tredan", "title": "TamperNN: Efficient Tampering Detection of Deployed Neural Nets", "comments": "In the 30th International Symposium on Software Reliability\n  Engineering (ISSRE 2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural networks are powering the deployment of embedded devices and Internet\nof Things. Applications range from personal assistants to critical ones such as\nself-driving cars. It has been shown recently that models obtained from neural\nnets can be trojaned ; an attacker can then trigger an arbitrary model behavior\nfacing crafted inputs. This has a critical impact on the security and\nreliability of those deployed devices. We introduce novel algorithms to detect\nthe tampering with deployed models, classifiers in particular. In the remote\ninteraction setup we consider, the proposed strategy is to identify markers of\nthe model input space that are likely to change class if the model is attacked,\nallowing a user to detect a possible tampering. This setup makes our proposal\ncompatible with a wide range of scenarios, such as embedded models, or models\nexposed through prediction APIs. We experiment those tampering detection\nalgorithms on the canonical MNIST dataset, over three different types of neural\nnets, and facing five different attacks (trojaning, quantization, fine-tuning,\ncompression and watermarking). We then validate over five large models (VGG16,\nVGG19, ResNet, MobileNet, DenseNet) with a state of the art dataset (VGGFace2),\nand report results demonstrating the possibility of an efficient detection of\nmodel tampering.\n", "versions": [{"version": "v1", "created": "Fri, 1 Mar 2019 14:32:45 GMT"}, {"version": "v2", "created": "Thu, 5 Sep 2019 11:42:19 GMT"}], "update_date": "2019-09-06", "authors_parsed": [["Merrer", "Erwan Le", ""], ["Tredan", "Gilles", ""]]}, {"id": "1903.00343", "submitter": "Huan Lei", "authors": "Huan Lei, Naveed Akhtar, Ajmal Mian", "title": "Octree guided CNN with Spherical Kernels for 3D Point Clouds", "comments": "Accepted in IEEE CVPR 2019. arXiv admin note: substantial text\n  overlap with arXiv:1805.07872", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an octree guided neural network architecture and spherical\nconvolutional kernel for machine learning from arbitrary 3D point clouds. The\nnetwork architecture capitalizes on the sparse nature of irregular point\nclouds, and hierarchically coarsens the data representation with space\npartitioning. At the same time, the proposed spherical kernels systematically\nquantize point neighborhoods to identify local geometric structures in the\ndata, while maintaining the properties of translation-invariance and asymmetry.\nWe specify spherical kernels with the help of network neurons that in turn are\nassociated with spatial locations. We exploit this association to avert dynamic\nkernel generation during network training that enables efficient learning with\nhigh resolution point clouds. The effectiveness of the proposed technique is\nestablished on the benchmark tasks of 3D object classification and\nsegmentation, achieving new state-of-the-art on ShapeNet and RueMonge2014\ndatasets.\n", "versions": [{"version": "v1", "created": "Thu, 28 Feb 2019 09:53:28 GMT"}], "update_date": "2019-03-04", "authors_parsed": [["Lei", "Huan", ""], ["Akhtar", "Naveed", ""], ["Mian", "Ajmal", ""]]}, {"id": "1903.00348", "submitter": "Xiaomeng Li", "authors": "Xiaomeng Li, Lequan Yu, Hao Chen, Chi-Wing Fu, Lei Xing, Pheng-Ann\n  Heng", "title": "Transformation Consistent Self-ensembling Model for Semi-supervised\n  Medical Image Segmentation", "comments": "Accept at IEEE Transactions on Neural Networks and Learning Systems", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep convolutional neural networks have achieved remarkable progress on a\nvariety of medical image computing tasks. A common problem when applying\nsupervised deep learning methods to medical images is the lack of labeled data,\nwhich is very expensive and time-consuming to be collected. In this paper, we\npresent a novel semi-supervised method for medical image segmentation, where\nthe network is optimized by the weighted combination of a common supervised\nloss for labeled inputs only and a regularization loss for both labeled and\nunlabeled data. To utilize the unlabeled data, our method encourages the\nconsistent predictions of the network-in-training for the same input under\ndifferent regularizations. Aiming for the semi-supervised segmentation problem,\nwe enhance the effect of regularization for pixel-level predictions by\nintroducing a transformation, including rotation and flipping, consistent\nscheme in our self-ensembling model. With the aim of semi-supervised\nsegmentation tasks, we introduce a transformation consistent strategy in our\nself-ensembling model to enhance the regularization effect for pixel-level\npredictions. We have extensively validated the proposed semi-supervised method\non three typical yet challenging medical image segmentation tasks: (i) skin\nlesion segmentation from dermoscopy images on International Skin Imaging\nCollaboration (ISIC) 2017 dataset, (ii) optic disc segmentation from fundus\nimages on Retinal Fundus Glaucoma Challenge (REFUGE) dataset, and (iii) liver\nsegmentation from volumetric CT scans on Liver Tumor Segmentation Challenge\n(LiTS) dataset. Compared to the state-of-the-arts, our proposed method shows\nsuperior segmentation performance on challenging 2D/3D medical images,\ndemonstrating the effectiveness of our semi-supervised method for medical image\nsegmentation.\n", "versions": [{"version": "v1", "created": "Thu, 28 Feb 2019 03:49:40 GMT"}, {"version": "v2", "created": "Mon, 4 Mar 2019 12:04:30 GMT"}, {"version": "v3", "created": "Fri, 8 May 2020 21:46:55 GMT"}], "update_date": "2020-05-12", "authors_parsed": [["Li", "Xiaomeng", ""], ["Yu", "Lequan", ""], ["Chen", "Hao", ""], ["Fu", "Chi-Wing", ""], ["Xing", "Lei", ""], ["Heng", "Pheng-Ann", ""]]}, {"id": "1903.00362", "submitter": "Aljo\\v{s}a O\\v{s}ep", "authors": "Aljosa Osep, Paul Voigtlaender, Jonathon Luiten, Stefan Breuers,\n  Bastian Leibe", "title": "Large-Scale Object Mining for Object Discovery from Unlabeled Video", "comments": "Updated version of ICRA'19 paper (additional qualitative results);\n  arXiv admin note: text overlap with arXiv:1712.08832", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the problem of object discovery from unlabeled driving\nvideos captured in a realistic automotive setting. Identifying recurring object\ncategories in such raw video streams is a very challenging problem. Not only do\nobject candidates first have to be localized in the input images, but many\ninteresting object categories occur relatively infrequently. Object discovery\nwill therefore have to deal with the difficulties of operating in the long tail\nof the object distribution. We demonstrate the feasibility of performing fully\nautomatic object discovery in such a setting by mining object tracks using a\ngeneric object tracker. In order to facilitate further research in object\ndiscovery, we release a collection of more than 360,000 automatically mined\nobject tracks from 10+ hours of video data (560,000 frames). We use this\ndataset to evaluate the suitability of different feature representations and\nclustering strategies for object discovery.\n", "versions": [{"version": "v1", "created": "Thu, 28 Feb 2019 16:53:48 GMT"}, {"version": "v2", "created": "Mon, 29 Apr 2019 13:46:27 GMT"}], "update_date": "2019-04-30", "authors_parsed": [["Osep", "Aljosa", ""], ["Voigtlaender", "Paul", ""], ["Luiten", "Jonathon", ""], ["Breuers", "Stefan", ""], ["Leibe", "Bastian", ""]]}, {"id": "1903.00366", "submitter": "Robik Shrestha", "authors": "Robik Shrestha, Kushal Kafle, Christopher Kanan", "title": "Answer Them All! Toward Universal Visual Question Answering Models", "comments": "8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual Question Answering (VQA) research is split into two camps: the first\nfocuses on VQA datasets that require natural image understanding and the second\nfocuses on synthetic datasets that test reasoning. A good VQA algorithm should\nbe capable of both, but only a few VQA algorithms are tested in this manner. We\ncompare five state-of-the-art VQA algorithms across eight VQA datasets covering\nboth domains. To make the comparison fair, all of the models are standardized\nas much as possible, e.g., they use the same visual features, answer\nvocabularies, etc. We find that methods do not generalize across the two\ndomains. To address this problem, we propose a new VQA algorithm that rivals or\nexceeds the state-of-the-art for both domains.\n", "versions": [{"version": "v1", "created": "Fri, 1 Mar 2019 15:25:24 GMT"}, {"version": "v2", "created": "Fri, 5 Apr 2019 17:25:36 GMT"}], "update_date": "2019-04-08", "authors_parsed": [["Shrestha", "Robik", ""], ["Kafle", "Kushal", ""], ["Kanan", "Christopher", ""]]}, {"id": "1903.00388", "submitter": "Shenghua He", "authors": "Shenghua He, Kyaw Thu Minn, Lilianna Solnica-Krezel, Hua Li and Mark\n  Anastasio", "title": "Automatic microscopic cell counting by use of unsupervised adversarial\n  domain adaptation and supervised density regression", "comments": "SPIE Medical imaging 2019 oral presentation", "journal-ref": null, "doi": "10.1117/12.2513058", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurate cell counting in microscopic images is important for medical\ndiagnoses and biological studies. However, manual cell counting is very\ntime-consuming, tedious, and prone to subjective errors. We propose a new\ndensity regression-based method for automatic cell counting that reduces the\nneed to manually annotate experimental images. A supervised learning-based\ndensity regression model (DRM) is trained with annotated synthetic images (the\nsource domain) and their corresponding ground truth density maps. A domain\nadaptation model (DAM) is built to map experimental images (the target domain)\nto the feature space of the source domain. By use of the unsupervised\nlearning-based DAM and supervised learning-based DRM, a cell density map of a\ngiven target image can be estimated, from which the number of cells can be\ncounted. Results from experimental immunofluorescent microscopic images of\nhuman embryonic stem cells demonstrate the promising performance of the\nproposed counting method.\n", "versions": [{"version": "v1", "created": "Fri, 1 Mar 2019 16:15:56 GMT"}, {"version": "v2", "created": "Fri, 22 Mar 2019 15:23:09 GMT"}], "update_date": "2019-03-25", "authors_parsed": [["He", "Shenghua", ""], ["Minn", "Kyaw Thu", ""], ["Solnica-Krezel", "Lilianna", ""], ["Li", "Hua", ""], ["Anastasio", "Mark", ""]]}, {"id": "1903.00389", "submitter": "Viktor Varkarakis", "authors": "Viktor Varkarakis, Shabab Bazrafkan, Peter Corcoran", "title": "Deep Neural Network and Data Augmentation Methodology for off-axis iris\n  segmentation in wearable headsets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A data augmentation methodology is presented and applied to generate a large\ndataset of off-axis iris regions and train a low-complexity deep neural\nnetwork. Although of low complexity the resulting network achieves a high level\nof accuracy in iris region segmentation for challenging off-axis eye-patches.\nInterestingly, this network is also shown to achieve high levels of performance\nfor regular, frontal, segmentation of iris regions, comparing favorably with\nstate-of-the-art techniques of significantly higher complexity. Due to its\nlower complexity, this network is well suited for deployment in embedded\napplications such as augmented and mixed reality headsets.\n", "versions": [{"version": "v1", "created": "Fri, 1 Mar 2019 16:17:00 GMT"}], "update_date": "2019-03-04", "authors_parsed": [["Varkarakis", "Viktor", ""], ["Bazrafkan", "Shabab", ""], ["Corcoran", "Peter", ""]]}, {"id": "1903.00395", "submitter": "Joshua Ebenezer", "authors": "Joshua Peter Ebenezer, Bijaylaxmi Das, Sudipta Mukhopadhyay", "title": "Single Image Haze Removal Using Conditional Wasserstein Generative\n  Adversarial Networks", "comments": "5 pages", "journal-ref": "2019 27th European Signal Processing Conference (EUSIPCO), 1-5", "doi": "10.23919/EUSIPCO.2019.8902992", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a method to restore a clear image from a haze-affected image using\na Wasserstein generative adversarial network. As the problem is\nill-conditioned, previous methods have required a prior on natural images or\nmultiple images of the same scene. We train a generative adversarial network to\nlearn the probability distribution of clear images conditioned on the\nhaze-affected images using the Wasserstein loss function, using a gradient\npenalty to enforce the Lipschitz constraint. The method is data-adaptive,\nend-to-end, and requires no further processing or tuning of parameters. We also\nincorporate the use of a texture-based loss metric and the L1 loss to improve\nresults, and show that our results are better than the current\nstate-of-the-art.\n", "versions": [{"version": "v1", "created": "Fri, 1 Mar 2019 16:32:05 GMT"}], "update_date": "2020-01-23", "authors_parsed": [["Ebenezer", "Joshua Peter", ""], ["Das", "Bijaylaxmi", ""], ["Mukhopadhyay", "Sudipta", ""]]}, {"id": "1903.00401", "submitter": "Karl Moritz Hermann", "authors": "Karl Moritz Hermann, Mateusz Malinowski, Piotr Mirowski, Andras\n  Banki-Horvath, Keith Anderson, Raia Hadsell", "title": "Learning To Follow Directions in Street View", "comments": null, "journal-ref": "AAAI 2020", "doi": null, "report-no": null, "categories": "cs.AI cs.CL cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Navigating and understanding the real world remains a key challenge in\nmachine learning and inspires a great variety of research in areas such as\nlanguage grounding, planning, navigation and computer vision. We propose an\ninstruction-following task that requires all of the above, and which combines\nthe practicality of simulated environments with the challenges of ambiguous,\nnoisy real world data. StreetNav is built on top of Google Street View and\nprovides visually accurate environments representing real places. Agents are\ngiven driving instructions which they must learn to interpret in order to\nsuccessfully navigate in this environment. Since humans equipped with driving\ninstructions can readily navigate in previously unseen cities, we set a high\nbar and test our trained agents for similar cognitive capabilities. Although\ndeep reinforcement learning (RL) methods are frequently evaluated only on data\nthat closely follow the training distribution, our dataset extends to multiple\ncities and has a clean train/test separation. This allows for thorough testing\nof generalisation ability. This paper presents the StreetNav environment and\ntasks, models that establish strong baselines, and extensive analysis of the\ntask and the trained agents.\n", "versions": [{"version": "v1", "created": "Fri, 1 Mar 2019 16:50:02 GMT"}, {"version": "v2", "created": "Thu, 21 Nov 2019 22:38:35 GMT"}], "update_date": "2019-11-25", "authors_parsed": [["Hermann", "Karl Moritz", ""], ["Malinowski", "Mateusz", ""], ["Mirowski", "Piotr", ""], ["Banki-Horvath", "Andras", ""], ["Anderson", "Keith", ""], ["Hadsell", "Raia", ""]]}, {"id": "1903.00405", "submitter": "Aritra Chowdhury", "authors": "Aritra Chowdhury, Malik Magdon-Ismail, Bulent Yener", "title": "Quantifying contribution and propagation of error from computational\n  steps, algorithms and hyperparameter choices in image classification\n  pipelines", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data science relies on pipelines that are organized in the form of\ninterdependent computational steps. Each step consists of various candidate\nalgorithms that maybe used for performing a particular function. Each algorithm\nconsists of several hyperparameters. Algorithms and hyperparameters must be\noptimized as a whole to produce the best performance. Typical machine learning\npipelines consist of complex algorithms in each of the steps. Not only is the\nselection process combinatorial, but it is also important to interpret and\nunderstand the pipelines. We propose a method to quantify the importance of\ndifferent components in the pipeline, by computing an error contribution\nrelative to an agnostic choice of computational steps, algorithms and\nhyperparameters. We also propose a methodology to quantify the propagation of\nerror from individual components of the pipeline with the help of a naive set\nof benchmark algorithms not involved in the pipeline. We demonstrate our\nmethodology on image classification pipelines. The agnostic and naive\nmethodologies quantify the error contribution and propagation respectively from\nthe computational steps, algorithms and hyperparameters in the image\nclassification pipeline. We show that algorithm selection and hyperparameter\noptimization methods like grid search, random search and Bayesian optimization\ncan be used to quantify the error contribution and propagation, and that random\nsearch is able to quantify them more accurately than Bayesian optimization.\nThis methodology can be used by domain experts to understand machine learning\nand data analysis pipelines in terms of their individual components, which can\nhelp in prioritizing different components of the pipeline.\n", "versions": [{"version": "v1", "created": "Thu, 21 Feb 2019 14:42:52 GMT"}], "update_date": "2019-03-04", "authors_parsed": [["Chowdhury", "Aritra", ""], ["Magdon-Ismail", "Malik", ""], ["Yener", "Bulent", ""]]}, {"id": "1903.00440", "submitter": "Michal Kawulok", "authors": "Michal Kawulok, Pawel Benecki, Szymon Piechaczek, Krzysztof\n  Hrynczenko, Daniel Kostrzewa, Jakub Nalepa", "title": "Deep Learning for Multiple-Image Super-Resolution", "comments": "Submitted to IEEE Geoscience and Remote Sensing Letters", "journal-ref": null, "doi": "10.1109/LGRS.2019.2940483", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Super-resolution reconstruction (SRR) is a process aimed at enhancing spatial\nresolution of images, either from a single observation, based on the learned\nrelation between low and high resolution, or from multiple images presenting\nthe same scene. SRR is particularly valuable, if it is infeasible to acquire\nimages at desired resolution, but many images of the same scene are available\nat lower resolution---this is inherent to a variety of remote sensing\nscenarios. Recently, we have witnessed substantial improvement in single-image\nSRR attributed to the use of deep neural networks for learning the relation\nbetween low and high resolution. Importantly, deep learning has not been\nexploited for multiple-image SRR, which benefits from information fusion and in\ngeneral allows for achieving higher reconstruction accuracy. In this letter, we\nintroduce a new method which combines the advantages of multiple-image fusion\nwith learning the low-to-high resolution mapping using deep networks. The\nreported experimental results indicate that our algorithm outperforms the\nstate-of-the-art SRR methods, including these that operate from a single image,\nas well as those that perform multiple-image fusion.\n", "versions": [{"version": "v1", "created": "Fri, 1 Mar 2019 17:59:59 GMT"}], "update_date": "2020-06-24", "authors_parsed": [["Kawulok", "Michal", ""], ["Benecki", "Pawel", ""], ["Piechaczek", "Szymon", ""], ["Hrynczenko", "Krzysztof", ""], ["Kostrzewa", "Daniel", ""], ["Nalepa", "Jakub", ""]]}, {"id": "1903.00445", "submitter": "Kevin Chen", "authors": "Kevin Chen, Juan Pablo de Vicente, Gabriel Sepulveda, Fei Xia, Alvaro\n  Soto, Marynel Vazquez, Silvio Savarese", "title": "A Behavioral Approach to Visual Navigation with Graph Localization\n  Networks", "comments": "Video: https://youtu.be/nN3B1F90CFM", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inspired by research in psychology, we introduce a behavioral approach for\nvisual navigation using topological maps. Our goal is to enable a robot to\nnavigate from one location to another, relying only on its visual input and the\ntopological map of the environment. We propose using graph neural networks for\nlocalizing the agent in the map, and decompose the action space into primitive\nbehaviors implemented as convolutional or recurrent neural networks. Using the\nGibson simulator, we verify that our approach outperforms relevant baselines\nand is able to navigate in both seen and unseen environments.\n", "versions": [{"version": "v1", "created": "Fri, 1 Mar 2019 18:16:03 GMT"}], "update_date": "2019-03-04", "authors_parsed": [["Chen", "Kevin", ""], ["de Vicente", "Juan Pablo", ""], ["Sepulveda", "Gabriel", ""], ["Xia", "Fei", ""], ["Soto", "Alvaro", ""], ["Vazquez", "Marynel", ""], ["Savarese", "Silvio", ""]]}, {"id": "1903.00450", "submitter": "Klaus Greff", "authors": "Klaus Greff, Rapha\\\"el Lopez Kaufman, Rishabh Kabra, Nick Watters,\n  Chris Burgess, Daniel Zoran, Loic Matthey, Matthew Botvinick, Alexander\n  Lerchner", "title": "Multi-Object Representation Learning with Iterative Variational\n  Inference", "comments": null, "journal-ref": "ICML 2019 (PMLR 97:2424-2433)", "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human perception is structured around objects which form the basis for our\nhigher-level cognition and impressive systematic generalization abilities. Yet\nmost work on representation learning focuses on feature learning without even\nconsidering multiple objects, or treats segmentation as an (often supervised)\npreprocessing step. Instead, we argue for the importance of learning to segment\nand represent objects jointly. We demonstrate that, starting from the simple\nassumption that a scene is composed of multiple entities, it is possible to\nlearn to segment images into interpretable objects with disentangled\nrepresentations. Our method learns -- without supervision -- to inpaint\noccluded parts, and extrapolates to scenes with more objects and to unseen\nobjects with novel feature combinations. We also show that, due to the use of\niterative variational inference, our system is able to learn multi-modal\nposteriors for ambiguous inputs and extends naturally to sequences.\n", "versions": [{"version": "v1", "created": "Fri, 1 Mar 2019 18:21:02 GMT"}, {"version": "v2", "created": "Wed, 15 May 2019 23:21:01 GMT"}, {"version": "v3", "created": "Mon, 27 Jul 2020 19:55:14 GMT"}], "update_date": "2020-07-29", "authors_parsed": [["Greff", "Klaus", ""], ["Kaufman", "Rapha\u00ebl Lopez", ""], ["Kabra", "Rishabh", ""], ["Watters", "Nick", ""], ["Burgess", "Chris", ""], ["Zoran", "Daniel", ""], ["Matthey", "Loic", ""], ["Botvinick", "Matthew", ""], ["Lerchner", "Alexander", ""]]}, {"id": "1903.00473", "submitter": "Liqun Lin", "authors": "Liqun Lin, Shiqi Yu, Tiesong Zhao, Member, IEEE and Zhou Wang, Fellow,\n  IEEE", "title": "PEA265: Perceptual Assessment of Video Compression Artifacts", "comments": "10 pages,15 figures,4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The most widely used video encoders share a common hybrid coding framework\nthat includes block-based motion estimation/compensation and block-based\ntransform coding. Despite their high coding efficiency, the encoded videos\noften exhibit visually annoying artifacts, denoted as Perceivable Encoding\nArtifacts (PEAs), which significantly degrade the visual Qualityof- Experience\n(QoE) of end users. To monitor and improve visual QoE, it is crucial to develop\nsubjective and objective measures that can identify and quantify various types\nof PEAs. In this work, we make the first attempt to build a large-scale\nsubjectlabelled database composed of H.265/HEVC compressed videos containing\nvarious PEAs. The database, namely the PEA265 database, includes 4 types of\nspatial PEAs (i.e. blurring, blocking, ringing and color bleeding) and 2 types\nof temporal PEAs (i.e. flickering and floating). Each containing at least\n60,000 image or video patches with positive and negative labels. To objectively\nidentify these PEAs, we train Convolutional Neural Networks (CNNs) using the\nPEA265 database. It appears that state-of-theart ResNeXt is capable of\nidentifying each type of PEAs with high accuracy. Furthermore, we define PEA\npattern and PEA intensity measures to quantify PEA levels of compressed video\nsequence. We believe that the PEA265 database and our findings will benefit the\nfuture development of video quality assessment methods and perceptually\nmotivated video encoders.\n", "versions": [{"version": "v1", "created": "Fri, 1 Mar 2019 15:25:35 GMT"}], "update_date": "2019-03-05", "authors_parsed": [["Lin", "Liqun", ""], ["Yu", "Shiqi", ""], ["Zhao", "Tiesong", ""], ["Member", "", ""], ["IEEE", "", ""], ["Wang", "Zhou", ""], ["Fellow", "", ""], ["IEEE", "", ""]]}, {"id": "1903.00502", "submitter": "Yizhe Zhu", "authors": "Yizhe Zhu and Jianwen Xie and Zhiqiang Tang and Xi Peng and Ahmed\n  Elgammal", "title": "Semantic-Guided Multi-Attention Localization for Zero-Shot Learning", "comments": "accepted to NeurIPS'19", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Zero-shot learning extends the conventional object classification to the\nunseen class recognition by introducing semantic representations of classes.\nExisting approaches predominantly focus on learning the proper mapping function\nfor visual-semantic embedding, while neglecting the effect of learning\ndiscriminative visual features. In this paper, we study the significance of the\ndiscriminative region localization. We propose a semantic-guided\nmulti-attention localization model, which automatically discovers the most\ndiscriminative parts of objects for zero-shot learning without any human\nannotations. Our model jointly learns cooperative global and local features\nfrom the whole object as well as the detected parts to categorize objects based\non semantic descriptions. Moreover, with the joint supervision of embedding\nsoftmax loss and class-center triplet loss, the model is encouraged to learn\nfeatures with high inter-class dispersion and intra-class compactness. Through\ncomprehensive experiments on three widely used zero-shot learning benchmarks,\nwe show the efficacy of the multi-attention localization and our proposed\napproach improves the state-of-the-art results by a considerable margin.\n", "versions": [{"version": "v1", "created": "Fri, 1 Mar 2019 19:25:24 GMT"}, {"version": "v2", "created": "Mon, 2 Dec 2019 03:20:41 GMT"}], "update_date": "2019-12-03", "authors_parsed": [["Zhu", "Yizhe", ""], ["Xie", "Jianwen", ""], ["Tang", "Zhiqiang", ""], ["Peng", "Xi", ""], ["Elgammal", "Ahmed", ""]]}, {"id": "1903.00535", "submitter": "Minxian Li", "authors": "Minxian Li, Xiatian Zhu, Shaogang Gong", "title": "Unsupervised Tracklet Person Re-Identification", "comments": "Accepted to appear in IEEE Transactions on Pattern Analysis and\n  Machine Intelligence. The new dataset is publicly available at\n  https://github.com/liminxian/DukeMTMC-SI-Tracklet", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most existing person re-identification (re-id) methods rely on supervised\nmodel learning on per-camera-pair manually labelled pairwise training data.\nThis leads to poor scalability in a practical re-id deployment, due to the lack\nof exhaustive identity labelling of positive and negative image pairs for every\ncamera-pair. In this work, we present an unsupervised re-id deep learning\napproach. It is capable of incrementally discovering and exploiting the\nunderlying re-id discriminative information from automatically generated person\ntracklet data end-to-end. We formulate an Unsupervised Tracklet Association\nLearning (UTAL) framework. This is by jointly learning within-camera tracklet\ndiscrimination and cross-camera tracklet association in order to maximise the\ndiscovery of tracklet identity matching both within and across camera views.\nExtensive experiments demonstrate the superiority of the proposed model over\nthe state-of-the-art unsupervised learning and domain adaptation person re-id\nmethods on eight benchmarking datasets.\n", "versions": [{"version": "v1", "created": "Fri, 1 Mar 2019 20:40:33 GMT"}], "update_date": "2019-03-05", "authors_parsed": [["Li", "Minxian", ""], ["Zhu", "Xiatian", ""], ["Gong", "Shaogang", ""]]}, {"id": "1903.00585", "submitter": "Uiwon Hwang", "authors": "Uiwon Hwang, Jaewoo Park, Hyemi Jang, Sungroh Yoon, Nam Ik Cho", "title": "PuVAE: A Variational Autoencoder to Purify Adversarial Examples", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks are widely used and exhibit excellent performance in\nmany areas. However, they are vulnerable to adversarial attacks that compromise\nthe network at the inference time by applying elaborately designed perturbation\nto input data. Although several defense methods have been proposed to address\nspecific attacks, other attack methods can circumvent these defense mechanisms.\nTherefore, we propose Purifying Variational Autoencoder (PuVAE), a method to\npurify adversarial examples. The proposed method eliminates an adversarial\nperturbation by projecting an adversarial example on the manifold of each\nclass, and determines the closest projection as a purified sample. We\nexperimentally illustrate the robustness of PuVAE against various attack\nmethods without any prior knowledge. In our experiments, the proposed method\nexhibits performances competitive with state-of-the-art defense methods, and\nthe inference time is approximately 130 times faster than that of Defense-GAN\nthat is the state-of-the art purifier model.\n", "versions": [{"version": "v1", "created": "Sat, 2 Mar 2019 00:38:38 GMT"}], "update_date": "2019-03-05", "authors_parsed": [["Hwang", "Uiwon", ""], ["Park", "Jaewoo", ""], ["Jang", "Hyemi", ""], ["Yoon", "Sungroh", ""], ["Cho", "Nam Ik", ""]]}, {"id": "1903.00586", "submitter": "Fausto Milletari", "authors": "Fausto Milletari, Vighnesh Birodkar, Michal Sofka", "title": "Straight to the point: reinforcement learning for user guidance in\n  ultrasound", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Point of care ultrasound (POCUS) consists in the use of ultrasound imaging in\ncritical or emergency situations to support clinical decisions by healthcare\nprofessionals and first responders. In this setting it is essential to be able\nto provide means to obtain diagnostic data to potentially inexperienced users\nwho did not receive an extensive medical training. Interpretation and\nacquisition of ultrasound images is not trivial. First, the user needs to find\na suitable sound window which can be used to get a clear image, and then he\nneeds to correctly interpret it to perform a diagnosis. Although many recent\napproaches focus on developing smart ultrasound devices that add interpretation\ncapabilities to existing systems, our goal in this paper is to present a\nreinforcement learning (RL) strategy which is capable to guide novice users to\nthe correct sonic window and enable them to obtain clinically relevant pictures\nof the anatomy of interest. We apply our approach to cardiac images acquired\nfrom the parasternal long axis (PLAx) view of the left ventricle of the heart.\n", "versions": [{"version": "v1", "created": "Sat, 2 Mar 2019 00:38:54 GMT"}], "update_date": "2019-03-05", "authors_parsed": [["Milletari", "Fausto", ""], ["Birodkar", "Vighnesh", ""], ["Sofka", "Michal", ""]]}, {"id": "1903.00618", "submitter": "Yu Yao", "authors": "Yu Yao, Mingze Xu, Yuchen Wang, David J. Crandall, Ella M. Atkins", "title": "Unsupervised Traffic Accident Detection in First-Person Videos", "comments": "Accepted to IROS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recognizing abnormal events such as traffic violations and accidents in\nnatural driving scenes is essential for successful autonomous driving and\nadvanced driver assistance systems. However, most work on video anomaly\ndetection suffers from two crucial drawbacks. First, they assume cameras are\nfixed and videos have static backgrounds, which is reasonable for surveillance\napplications but not for vehicle-mounted cameras. Second, they pose the problem\nas one-class classification, relying on arduously hand-labeled training\ndatasets that limit recognition to anomaly categories that have been explicitly\ntrained. This paper proposes an unsupervised approach for traffic accident\ndetection in first-person (dashboard-mounted camera) videos. Our major novelty\nis to detect anomalies by predicting the future locations of traffic\nparticipants and then monitoring the prediction accuracy and consistency\nmetrics with three different strategies. We evaluate our approach using a new\ndataset of diverse traffic accidents, AnAn Accident Detection (A3D), as well as\nanother publicly-available dataset. Experimental results show that our approach\noutperforms the state-of-the-art.\n", "versions": [{"version": "v1", "created": "Sat, 2 Mar 2019 03:24:21 GMT"}, {"version": "v2", "created": "Sun, 12 May 2019 13:05:02 GMT"}, {"version": "v3", "created": "Fri, 24 May 2019 19:05:06 GMT"}, {"version": "v4", "created": "Fri, 26 Jul 2019 03:05:24 GMT"}], "update_date": "2019-07-29", "authors_parsed": [["Yao", "Yu", ""], ["Xu", "Mingze", ""], ["Wang", "Yuchen", ""], ["Crandall", "David J.", ""], ["Atkins", "Ella M.", ""]]}, {"id": "1903.00620", "submitter": "Jie Li", "authors": "Jie Li, Yu Liu, Dong Gong, Qinfeng Shi, Xia Yuan, Chunxia Zhao, Ian\n  Reid", "title": "RGBD Based Dimensional Decomposition Residual Network for 3D Semantic\n  Scene Completion", "comments": "CVPR2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  RGB images differentiate from depth images as they carry more details about\nthe color and texture information, which can be utilized as a vital\ncomplementary to depth for boosting the performance of 3D semantic scene\ncompletion (SSC). SSC is composed of 3D shape completion (SC) and semantic\nscene labeling while most of the existing methods use depth as the sole input\nwhich causes the performance bottleneck. Moreover, the state-of-the-art methods\nemploy 3D CNNs which have cumbersome networks and tremendous parameters. We\nintroduce a light-weight Dimensional Decomposition Residual network (DDR) for\n3D dense prediction tasks. The novel factorized convolution layer is effective\nfor reducing the network parameters, and the proposed multi-scale fusion\nmechanism for depth and color image can improve the completion and segmentation\naccuracy simultaneously. Our method demonstrates excellent performance on two\npublic datasets. Compared with the latest method SSCNet, we achieve 5.9% gains\nin SC-IoU and 5.7% gains in SSC-IOU, albeit with only 21% network parameters\nand 16.6% FLOPs employed compared with that of SSCNet.\n", "versions": [{"version": "v1", "created": "Sat, 2 Mar 2019 04:14:31 GMT"}, {"version": "v2", "created": "Wed, 1 May 2019 03:54:34 GMT"}], "update_date": "2019-05-02", "authors_parsed": [["Li", "Jie", ""], ["Liu", "Yu", ""], ["Gong", "Dong", ""], ["Shi", "Qinfeng", ""], ["Yuan", "Xia", ""], ["Zhao", "Chunxia", ""], ["Reid", "Ian", ""]]}, {"id": "1903.00621", "submitter": "Chenchen Zhu", "authors": "Chenchen Zhu, Yihui He, Marios Savvides", "title": "Feature Selective Anchor-Free Module for Single-Shot Object Detection", "comments": "CVPR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We motivate and present feature selective anchor-free (FSAF) module, a simple\nand effective building block for single-shot object detectors. It can be\nplugged into single-shot detectors with feature pyramid structure. The FSAF\nmodule addresses two limitations brought up by the conventional anchor-based\ndetection: 1) heuristic-guided feature selection; 2) overlap-based anchor\nsampling. The general concept of the FSAF module is online feature selection\napplied to the training of multi-level anchor-free branches. Specifically, an\nanchor-free branch is attached to each level of the feature pyramid, allowing\nbox encoding and decoding in the anchor-free manner at an arbitrary level.\nDuring training, we dynamically assign each instance to the most suitable\nfeature level. At the time of inference, the FSAF module can work jointly with\nanchor-based branches by outputting predictions in parallel. We instantiate\nthis concept with simple implementations of anchor-free branches and online\nfeature selection strategy. Experimental results on the COCO detection track\nshow that our FSAF module performs better than anchor-based counterparts while\nbeing faster. When working jointly with anchor-based branches, the FSAF module\nrobustly improves the baseline RetinaNet by a large margin under various\nsettings, while introducing nearly free inference overhead. And the resulting\nbest model can achieve a state-of-the-art 44.6% mAP, outperforming all existing\nsingle-shot detectors on COCO.\n", "versions": [{"version": "v1", "created": "Sat, 2 Mar 2019 04:15:34 GMT"}], "update_date": "2019-03-05", "authors_parsed": [["Zhu", "Chenchen", ""], ["He", "Yihui", ""], ["Savvides", "Marios", ""]]}, {"id": "1903.00658", "submitter": "Xuanyu Zhu", "authors": "Xuanyu Zhu, Yi Xu, Hongteng Xu, Changjian Chen", "title": "Quaternion Convolutional Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural networks in the real domain have been studied for a long time and\nachieved promising results in many vision tasks for recent years. However, the\nextensions of the neural network models in other number fields and their\npotential applications are not fully-investigated yet. Focusing on color\nimages, which can be naturally represented as quaternion matrices, we propose a\nquaternion convolutional neural network (QCNN) model to obtain more\nrepresentative features. In particular, we redesign the basic modules like\nconvolution layer and fully-connected layer in the quaternion domain, which can\nbe used to establish fully-quaternion convolutional neural networks. Moreover,\nthese modules are compatible with almost all deep learning techniques and can\nbe plugged into traditional CNNs easily. We test our QCNN models in both color\nimage classification and denoising tasks. Experimental results show that they\noutperform the real-valued CNNs with same structures.\n", "versions": [{"version": "v1", "created": "Sat, 2 Mar 2019 08:57:35 GMT"}], "update_date": "2019-03-05", "authors_parsed": [["Zhu", "Xuanyu", ""], ["Xu", "Yi", ""], ["Xu", "Hongteng", ""], ["Chen", "Changjian", ""]]}, {"id": "1903.00676", "submitter": "Pedro Miraldo", "authors": "G. Dias Pais, Tiago J. Dias, Jacinto C. Nascimento, and Pedro Miraldo", "title": "OmniDRL: Robust Pedestrian Detection using Deep Reinforcement Learning\n  on Omnidirectional Cameras", "comments": "Accepted in 2019 IEEE Int'l Conf. Robotics and Automation (ICRA)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pedestrian detection is one of the most explored topics in computer vision\nand robotics. The use of deep learning methods allowed the development of new\nand highly competitive algorithms. Deep Reinforcement Learning has proved to be\nwithin the state-of-the-art in terms of both detection in perspective cameras\nand robotics applications. However, for detection in omnidirectional cameras,\nthe literature is still scarce, mostly because of their high levels of\ndistortion. This paper presents a novel and efficient technique for robust\npedestrian detection in omnidirectional images. The proposed method uses deep\nReinforcement Learning that takes advantage of the distortion in the image. By\nconsidering the 3D bounding boxes and their distorted projections into the\nimage, our method is able to provide the pedestrian's position in the world, in\ncontrast to the image positions provided by most state-of-the-art methods for\nperspective cameras. Our method avoids the need of pre-processing steps to\nremove the distortion, which is computationally expensive. Beyond the novel\nsolution, our method compares favorably with the state-of-the-art methodologies\nthat do not consider the underlying distortion for the detection task.\n", "versions": [{"version": "v1", "created": "Sat, 2 Mar 2019 10:16:38 GMT"}], "update_date": "2019-03-05", "authors_parsed": [["Pais", "G. Dias", ""], ["Dias", "Tiago J.", ""], ["Nascimento", "Jacinto C.", ""], ["Miraldo", "Pedro", ""]]}, {"id": "1903.00695", "submitter": "Noshaba Cheema", "authors": "Noshaba Cheema, Somayeh Hosseini, Janis Sprenger, Erik Herrmann, Han\n  Du, Klaus Fischer, Philipp Slusallek", "title": "Fine-Grained Semantic Segmentation of Motion Capture Data using Dilated\n  Temporal Fully-Convolutional Networks", "comments": "Eurographics 2019 - Short Papers", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human motion capture data has been widely used in data-driven character\nanimation. In order to generate realistic, natural-looking motions, most\ndata-driven approaches require considerable efforts of pre-processing,\nincluding motion segmentation and annotation. Existing (semi-) automatic\nsolutions either require hand-crafted features for motion segmentation or do\nnot produce the semantic annotations required for motion synthesis and building\nlarge-scale motion databases. In addition, human labeled annotation data\nsuffers from inter- and intra-labeler inconsistencies by design. We propose a\nsemi-automatic framework for semantic segmentation of motion capture data based\non supervised machine learning techniques. It first transforms a motion capture\nsequence into a ``motion image'' and applies a convolutional neural network for\nimage segmentation. Dilated temporal convolutions enable the extraction of\ntemporal information from a large receptive field. Our model outperforms two\nstate-of-the-art models for action segmentation, as well as a popular network\nfor sequence modeling. Most of all, our method is very robust under noisy and\ninaccurate training labels and thus can handle human errors during the labeling\nprocess.\n", "versions": [{"version": "v1", "created": "Sat, 2 Mar 2019 12:53:25 GMT"}], "update_date": "2019-03-05", "authors_parsed": [["Cheema", "Noshaba", ""], ["Hosseini", "Somayeh", ""], ["Sprenger", "Janis", ""], ["Herrmann", "Erik", ""], ["Du", "Han", ""], ["Fischer", "Klaus", ""], ["Slusallek", "Philipp", ""]]}, {"id": "1903.00705", "submitter": "Xuhao Jiang Mr", "authors": "Xuhao Jiang, Liquan Shen, Guorui Feng, Liangwei Yu, and Ping An", "title": "Deep Optimization model for Screen Content Image Quality Assessment\n  using Neural Networks", "comments": "12pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a novel quadratic optimized model based on the deep\nconvolutional neural network (QODCNN) for full-reference and no-reference\nscreen content image (SCI) quality assessment. Unlike traditional CNN methods\ntaking all image patches as training data and using average quality pooling,\nour model is optimized to obtain a more effective model including three steps.\nIn the first step, an end-to-end deep CNN is trained to preliminarily predict\nthe image visual quality, and batch normalized (BN) layers and l2\nregularization are employed to improve the speed and performance of network\nfitting. For second step, the pretrained model is fine-tuned to achieve better\nperformance under analysis of the raw training data. An adaptive weighting\nmethod is proposed in the third step to fuse local quality inspired by the\nperceptual property of the human visual system (HVS) that the HVS is sensitive\nto image patches containing texture and edge information. The novelty of our\nalgorithm can be concluded as follows: 1) with the consideration of correlation\nbetween local quality and subjective differential mean opinion score (DMOS),\nthe Euclidean distance is utilized to measure effectiveness of image patches,\nand the pretrained model is fine-tuned with more effective training data; 2) an\nadaptive pooling approach is employed to fuse patch quality of textual and\npictorial regions, whose feature only extracted from distorted images owns\nstrong noise robust and effects on both FR and NR IQA; 3) Considering the\ncharacteristics of SCIs, a deep and valid network architecture is designed for\nboth NR and FR visual quality evaluation of SCIs. Experimental results verify\nthat our model outperforms both current no-reference and full-reference image\nquality assessment methods on the benchmark screen content image quality\nassessment database (SIQAD).\n", "versions": [{"version": "v1", "created": "Sat, 2 Mar 2019 14:10:46 GMT"}], "update_date": "2019-03-05", "authors_parsed": [["Jiang", "Xuhao", ""], ["Shen", "Liquan", ""], ["Feng", "Guorui", ""], ["Yu", "Liangwei", ""], ["An", "Ping", ""]]}, {"id": "1903.00706", "submitter": "P. Christopher Staecker", "authors": "P. Christopher Staecker", "title": "Strong homotopy of digitally continuous functions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.GN cs.CV math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new type of homotopy relation for digitally continuous\nfunctions which we call ``strong homotopy.'' Both digital homotopy and strong\nhomotopy are natural digitizations of classical topological homotopy: the\ndifference between them is analogous to the difference between digital\n4-adjacency and 8-adjacency in the plane.\n  We explore basic properties of strong homotopy, and give some equivalent\ncharacterizations. In particular we show that strong homotopy is related to\n``punctuated homotopy,'' in which the function changes by only one point in\neach homotopy time step.\n  We also show that strongly homotopic maps always have the same induced\nhomomorphisms in the digital homology theory. This is not generally true for\ndigitally homotopic maps, though we do show that it is true for any homotopic\nselfmaps on the digital cycle $C_n$ with $n\\ge 4$.\n  We also define and consider strong homotopy equivalence of digital images.\nUsing some computer assistance, we produce a catalog of all small digital\nimages up to strong homotopy equivalence. We also briefly consider pointed\nstrong homotopy equivalence, and give an example of a pointed contractible\nimage which is not pointed strongly contractible.\n", "versions": [{"version": "v1", "created": "Sat, 2 Mar 2019 14:11:43 GMT"}], "update_date": "2019-03-05", "authors_parsed": [["Staecker", "P. Christopher", ""]]}, {"id": "1903.00709", "submitter": "Kai Xu", "authors": "Fenggen Yu, Kun Liu, Yan Zhang, Chenyang Zhu, Kai Xu", "title": "PartNet: A Recursive Part Decomposition Network for Fine-grained and\n  Hierarchical Shape Segmentation", "comments": "CVPR 2019; Corresponding author: Kai Xu (kevin.kai.xu@gmail.com);\n  Project page: www.kevinkaixu.net/projects/partnet.html", "journal-ref": "CVPR 2019", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning approaches to 3D shape segmentation are typically formulated as\na multi-class labeling problem. Existing models are trained for a fixed set of\nlabels, which greatly limits their flexibility and adaptivity. We opt for\ntop-down recursive decomposition and develop the first deep learning model for\nhierarchical segmentation of 3D shapes, based on recursive neural networks.\nStarting from a full shape represented as a point cloud, our model performs\nrecursive binary decomposition, where the decomposition network at all nodes in\nthe hierarchy share weights. At each node, a node classifier is trained to\ndetermine the type (adjacency or symmetry) and stopping criteria of its\ndecomposition. The features extracted in higher level nodes are recursively\npropagated to lower level ones. Thus, the meaningful decompositions in higher\nlevels provide strong contextual cues constraining the segmentations in lower\nlevels. Meanwhile, to increase the segmentation accuracy at each node, we\nenhance the recursive contextual feature with the shape feature extracted for\nthe corresponding part. Our method segments a 3D shape in point cloud into an\nunfixed number of parts, depending on the shape complexity, showing strong\ngenerality and flexibility. It achieves the state-of-the-art performance, both\nfor fine-grained and semantic segmentation, on the public benchmark and a new\nbenchmark of fine-grained segmentation proposed in this work. We also\ndemonstrate its application for fine-grained part refinements in image-to-shape\nreconstruction.\n", "versions": [{"version": "v1", "created": "Sat, 2 Mar 2019 14:36:49 GMT"}, {"version": "v2", "created": "Wed, 13 Mar 2019 15:46:47 GMT"}, {"version": "v3", "created": "Sun, 24 Mar 2019 14:12:11 GMT"}, {"version": "v4", "created": "Fri, 29 Mar 2019 04:14:16 GMT"}], "update_date": "2019-04-01", "authors_parsed": [["Yu", "Fenggen", ""], ["Liu", "Kun", ""], ["Zhang", "Yan", ""], ["Zhu", "Chenyang", ""], ["Xu", "Kai", ""]]}, {"id": "1903.00755", "submitter": "Ziming Zhang", "authors": "Ziming Zhang, Anil Kag, Alan Sullivan, Venkatesh Saligrama", "title": "Equilibrated Recurrent Neural Network: Neuronal Time-Delayed\n  Self-Feedback Improves Accuracy and Stability", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel {\\it Equilibrated Recurrent Neural Network} (ERNN) to\ncombat the issues of inaccuracy and instability in conventional RNNs. Drawing\nupon the concept of autapse in neuroscience, we propose augmenting an RNN with\na time-delayed self-feedback loop. Our sole purpose is to modify the dynamics\nof each internal RNN state and, at any time, enforce it to evolve close to the\nequilibrium point associated with the input signal at that time. We show that\nsuch self-feedback helps stabilize the hidden state transitions leading to fast\nconvergence during training while efficiently learning discriminative latent\nfeatures that result in state-of-the-art results on several benchmark datasets\nat test-time. We propose a novel inexact Newton method to solve fixed-point\nconditions given model parameters for generating the latent features at each\nhidden state. We prove that our inexact Newton method converges locally with\nlinear rate (under mild conditions). We leverage this result for efficient\ntraining of ERNNs based on backpropagation.\n", "versions": [{"version": "v1", "created": "Sat, 2 Mar 2019 20:01:44 GMT"}], "update_date": "2019-03-05", "authors_parsed": [["Zhang", "Ziming", ""], ["Kag", "Anil", ""], ["Sullivan", "Alan", ""], ["Saligrama", "Venkatesh", ""]]}, {"id": "1903.00760", "submitter": "Ziming Zhang", "authors": "Ziming Zhang, Wenju Xu, Alan Sullivan", "title": "Time-Delay Momentum: A Regularization Perspective on the Convergence and\n  Generalization of Stochastic Momentum for Deep Learning", "comments": "has errors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we study the problem of convergence and generalization error\nbound of stochastic momentum for deep learning from the perspective of\nregularization. To do so, we first interpret momentum as solving an\n$\\ell_2$-regularized minimization problem to learn the offsets between\narbitrary two successive model parameters. We call this {\\em time-delay\nmomentum} because the model parameter is updated after a few iterations towards\nfinding the minimizer. We then propose our learning algorithm, \\ie stochastic\ngradient descent (SGD) with time-delay momentum. We show that our algorithm can\nbe interpreted as solving a sequence of strongly convex optimization problems\nusing SGD. We prove that under mild conditions our algorithm can converge to a\nstationary point with rate of $O(\\frac{1}{\\sqrt{K}})$ and generalization error\nbound of $O(\\frac{1}{\\sqrt{n\\delta}})$ with probability at least $1-\\delta$,\nwhere $K,n$ are the numbers of model updates and training samples,\nrespectively. We demonstrate the empirical superiority of our algorithm in deep\nlearning in comparison with the state-of-the-art deep learning solvers.\n", "versions": [{"version": "v1", "created": "Sat, 2 Mar 2019 20:21:38 GMT"}, {"version": "v2", "created": "Sat, 1 Jun 2019 23:05:37 GMT"}], "update_date": "2019-06-04", "authors_parsed": [["Zhang", "Ziming", ""], ["Xu", "Wenju", ""], ["Sullivan", "Alan", ""]]}, {"id": "1903.00763", "submitter": "Jianrui Cai", "authors": "Jianrui Cai, Wangmeng Zuo, Lei Zhang", "title": "Extreme Channel Prior Embedded Network for Dynamic Scene Deblurring", "comments": "10 pages", "journal-ref": null, "doi": "10.1109/TIP.2020.2995048", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent years have witnessed the significant progress on convolutional neural\nnetworks (CNNs) in dynamic scene deblurring. While CNN models are generally\nlearned by the reconstruction loss defined on training data, incorporating\nsuitable image priors as well as regularization terms into the network\narchitecture could boost the deblurring performance. In this work, we propose\nan Extreme Channel Prior embedded Network (ECPeNet) to plug the extreme channel\npriors (i.e., priors on dark and bright channels) into a network architecture\nfor effective dynamic scene deblurring. A novel trainable extreme channel prior\nembedded layer (ECPeL) is developed to aggregate both extreme channel and\nblurry image representations, and sparse regularization is introduced to\nregularize the ECPeNet model learning. Furthermore, we present an effective\nmulti-scale network architecture that works in both coarse-to-fine and\nfine-to-coarse manners for better exploiting information flow across scales.\nExperimental results on GoPro and Kohler datasets show that our proposed\nECPeNet performs favorably against state-of-the-art deep image deblurring\nmethods in terms of both quantitative metrics and visual quality.\n", "versions": [{"version": "v1", "created": "Sat, 2 Mar 2019 20:49:00 GMT"}], "update_date": "2020-07-15", "authors_parsed": [["Cai", "Jianrui", ""], ["Zuo", "Wangmeng", ""], ["Zhang", "Lei", ""]]}, {"id": "1903.00774", "submitter": "Keiller Nogueira", "authors": "Keiller Nogueira, Jefersson A. dos Santos, Nathalia Menini, Thiago S.\n  F. Silva, Leonor Patricia C. Morellato, Ricardo da S. Torres", "title": "Spatio-Temporal Vegetation Pixel Classification By Using Convolutional\n  Networks", "comments": null, "journal-ref": null, "doi": "10.1109/LGRS.2019.2903194", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Plant phenology studies rely on long-term monitoring of life cycles of\nplants. High-resolution unmanned aerial vehicles (UAVs) and near-surface\ntechnologies have been used for plant monitoring, demanding the creation of\nmethods capable of locating and identifying plant species through time and\nspace. However, this is a challenging task given the high volume of data, the\nconstant data missing from temporal dataset, the heterogeneity of temporal\nprofiles, the variety of plant visual patterns, and the unclear definition of\nindividuals' boundaries in plant communities. In this letter, we propose a\nnovel method, suitable for phenological monitoring, based on Convolutional\nNetworks (ConvNets) to perform spatio-temporal vegetation pixel-classification\non high resolution images. We conducted a systematic evaluation using\nhigh-resolution vegetation image datasets associated with the Brazilian Cerrado\nbiome. Experimental results show that the proposed approach is effective,\novercoming other spatio-temporal pixel-classification strategies.\n", "versions": [{"version": "v1", "created": "Sat, 2 Mar 2019 22:07:25 GMT"}], "update_date": "2019-10-23", "authors_parsed": [["Nogueira", "Keiller", ""], ["Santos", "Jefersson A. dos", ""], ["Menini", "Nathalia", ""], ["Silva", "Thiago S. F.", ""], ["Morellato", "Leonor Patricia C.", ""], ["Torres", "Ricardo da S.", ""]]}, {"id": "1903.00782", "submitter": "Shehryar Khattak", "authors": "Shehryar Khattak, Christos Papachristos, Kostas Alexis", "title": "Marker based Thermal-Inertial Localization for Aerial Robots in\n  Obscurant Filled Environments", "comments": "10 pages, 5 figures, Published in International Symposium on Visual\n  Computing 2018", "journal-ref": null, "doi": "10.1007/978-3-030-03801-4_49", "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For robotic inspection tasks in known environments fiducial markers provide a\nreliable and low-cost solution for robot localization. However, detection of\nsuch markers relies on the quality of RGB camera data, which degrades\nsignificantly in the presence of visual obscurants such as fog and smoke. The\nability to navigate known environments in the presence of obscurants can be\ncritical for inspection tasks especially, in the aftermath of a disaster.\nAddressing such a scenario, this work proposes a method for the design of\nfiducial markers to be used with thermal cameras for the pose estimation of\naerial robots. Our low cost markers are designed to work in the long wave\ninfrared spectrum, which is not affected by the presence of obscurants, and can\nbe affixed to any object that has measurable temperature difference with\nrespect to its surroundings. Furthermore, the estimated pose from the fiducial\nmarkers is fused with inertial measurements in an extended Kalman filter to\nremove high frequency noise and error present in the fiducial pose estimates.\nThe proposed markers and the pose estimation method are experimentally\nevaluated in an obscurant filled environment using an aerial robot carrying a\nthermal camera.\n", "versions": [{"version": "v1", "created": "Sat, 2 Mar 2019 22:33:02 GMT"}], "update_date": "2019-03-05", "authors_parsed": [["Khattak", "Shehryar", ""], ["Papachristos", "Christos", ""], ["Alexis", "Kostas", ""]]}, {"id": "1903.00788", "submitter": "Ayush Jaiswal", "authors": "Ayush Jaiswal, Yue Wu, Wael AbdAlmageed, Iacopo Masi, Premkumar\n  Natarajan", "title": "AIRD: Adversarial Learning Framework for Image Repurposing Detection", "comments": "Camera-ready version for the IEEE Conference on Computer Vision and\n  Pattern Recognition (CVPR), 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image repurposing is a commonly used method for spreading misinformation on\nsocial media and online forums, which involves publishing untampered images\nwith modified metadata to create rumors and further propaganda. While manual\nverification is possible, given vast amounts of verified knowledge available on\nthe internet, the increasing prevalence and ease of this form of semantic\nmanipulation call for the development of robust automatic ways of assessing the\nsemantic integrity of multimedia data. In this paper, we present a novel method\nfor image repurposing detection that is based on the real-world adversarial\ninterplay between a bad actor who repurposes images with counterfeit metadata\nand a watchdog who verifies the semantic consistency between images and their\naccompanying metadata, where both players have access to a reference dataset of\nverified content, which they can use to achieve their goals. The proposed\nmethod exhibits state-of-the-art performance on location-identity,\nsubject-identity and painting-artist verification, showing its efficacy across\na diverse set of scenarios.\n", "versions": [{"version": "v1", "created": "Sat, 2 Mar 2019 23:14:58 GMT"}, {"version": "v2", "created": "Wed, 6 Mar 2019 16:10:51 GMT"}, {"version": "v3", "created": "Tue, 9 Apr 2019 21:17:49 GMT"}], "update_date": "2019-04-11", "authors_parsed": [["Jaiswal", "Ayush", ""], ["Wu", "Yue", ""], ["AbdAlmageed", "Wael", ""], ["Masi", "Iacopo", ""], ["Natarajan", "Premkumar", ""]]}, {"id": "1903.00793", "submitter": "Nam Vo", "authors": "Nam Vo, Lu Jiang, James Hays", "title": "Let's Transfer Transformations of Shared Semantic Representations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With a good image understanding capability, can we manipulate the images high\nlevel semantic representation? Such transformation operation can be used to\ngenerate or retrieve similar images but with a desired modification (for\nexample changing beach background to street background); similar ability has\nbeen demonstrated in zero shot learning, attribute composition and attribute\nmanipulation image search. In this work we show how one can learn\ntransformations with no training examples by learning them on another domain\nand then transfer to the target domain. This is feasible if: first,\ntransformation training data is more accessible in the other domain and second,\nboth domains share similar semantics such that one can learn transformations in\na shared embedding space. We demonstrate this on an image retrieval task where\nsearch query is an image, plus an additional transformation specification (for\nexample: search for images similar to this one but background is a street\ninstead of a beach). In one experiment, we transfer transformation from\nsynthesized 2D blobs image to 3D rendered image, and in the other, we transfer\nfrom text domain to natural image domain.\n", "versions": [{"version": "v1", "created": "Sat, 2 Mar 2019 23:53:45 GMT"}], "update_date": "2019-03-05", "authors_parsed": [["Vo", "Nam", ""], ["Jiang", "Lu", ""], ["Hays", "James", ""]]}, {"id": "1903.00798", "submitter": "Shehryar Khattak", "authors": "Shehryar Khattak, Christos Papachristos, Kostas Alexis", "title": "Keyframe-based Direct Thermal-Inertial Odometry", "comments": "7 pages, 8 figures, Accepted at International Conference on Robotics\n  and Automation (ICRA) 2019", "journal-ref": "2019 International Conference on Robotics and Automation (ICRA)", "doi": "10.1109/ICRA.2019.8793927", "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes an approach for fusing direct radiometric data from a\nthermal camera with inertial measurements to extend the robotic capabilities of\naerial robots for navigation in GPS-denied and visually degraded environments\nin the conditions of darkness and in the presence of airborne obscurants such\nas dust, fog and smoke. An optimization based approach is developed that\njointly minimizes the re-projection error of 3D landmarks and inertial\nmeasurement errors. The developed solution is extensively verified against both\nground-truth in an indoor laboratory setting, as well as inside an underground\nmine under severely visually degraded conditions.\n", "versions": [{"version": "v1", "created": "Sun, 3 Mar 2019 00:25:12 GMT"}], "update_date": "2019-11-20", "authors_parsed": [["Khattak", "Shehryar", ""], ["Papachristos", "Christos", ""], ["Alexis", "Kostas", ""]]}, {"id": "1903.00812", "submitter": "Liuhao Ge", "authors": "Liuhao Ge, Zhou Ren, Yuncheng Li, Zehao Xue, Yingying Wang, Jianfei\n  Cai, Junsong Yuan", "title": "3D Hand Shape and Pose Estimation from a Single RGB Image", "comments": "CVPR 2019 (Oral), project page:\n  https://sites.google.com/site/geliuhaontu/home/cvpr2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work addresses a novel and challenging problem of estimating the full 3D\nhand shape and pose from a single RGB image. Most current methods in 3D hand\nanalysis from monocular RGB images only focus on estimating the 3D locations of\nhand keypoints, which cannot fully express the 3D shape of hand. In contrast,\nwe propose a Graph Convolutional Neural Network (Graph CNN) based method to\nreconstruct a full 3D mesh of hand surface that contains richer information of\nboth 3D hand shape and pose. To train networks with full supervision, we create\na large-scale synthetic dataset containing both ground truth 3D meshes and 3D\nposes. When fine-tuning the networks on real-world datasets without 3D ground\ntruth, we propose a weakly-supervised approach by leveraging the depth map as a\nweak supervision in training. Through extensive evaluations on our proposed new\ndatasets and two public datasets, we show that our proposed method can produce\naccurate and reasonable 3D hand mesh, and can achieve superior 3D hand pose\nestimation accuracy when compared with state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Sun, 3 Mar 2019 02:27:13 GMT"}, {"version": "v2", "created": "Sat, 20 Apr 2019 07:46:49 GMT"}], "update_date": "2019-04-23", "authors_parsed": [["Ge", "Liuhao", ""], ["Ren", "Zhou", ""], ["Li", "Yuncheng", ""], ["Xue", "Zehao", ""], ["Wang", "Yingying", ""], ["Cai", "Jianfei", ""], ["Yuan", "Junsong", ""]]}, {"id": "1903.00821", "submitter": "Lei Tai", "authors": "Lei Tai, Peng Yun, Yuying Chen, Congcong Liu, Haoyang Ye, Ming Liu", "title": "Visual-based Autonomous Driving Deployment from a Stochastic and\n  Uncertainty-aware Perspective", "comments": "IROS 2019 camera-ready version, 7 pages, video:\n  https://youtu.be/ZYtsb9-1zXk", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  End-to-end visual-based imitation learning has been widely applied in\nautonomous driving. When deploying the trained visual-based driving policy, a\ndeterministic command is usually directly applied without considering the\nuncertainty of the input data. Such kind of policies may bring dramatical\ndamage when applied in the real world. In this paper, we follow the recent\nreal-to-sim pipeline by translating the testing world image back to the\ntraining domain when using the trained policy. In the translating process, a\nstochastic generator is used to generate various images stylized under the\ntraining domain randomly or directionally. Based on those translated images,\nthe trained uncertainty-aware imitation learning policy would output both the\npredicted action and the data uncertainty motivated by the aleatoric loss\nfunction. Through the uncertainty-aware imitation learning policy, we can\neasily choose the safest one with the lowest uncertainty among the generated\nimages. Experiments in the Carla navigation benchmark show that our strategy\noutperforms previous methods, especially in dynamic environments.\n", "versions": [{"version": "v1", "created": "Sun, 3 Mar 2019 03:59:41 GMT"}, {"version": "v2", "created": "Thu, 18 Jul 2019 06:27:18 GMT"}], "update_date": "2019-07-19", "authors_parsed": [["Tai", "Lei", ""], ["Yun", "Peng", ""], ["Chen", "Yuying", ""], ["Liu", "Congcong", ""], ["Ye", "Haoyang", ""], ["Liu", "Ming", ""]]}, {"id": "1903.00832", "submitter": "Hao Li", "authors": "Hao Li, Jun Li, Xiaozhu Lin, Xiaohua Qian", "title": "A Model-Driven Stack-Based Fully Convolutional Network for Pancreas\n  Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The irregular geometry and high inter-slice variability in computerized\ntomography (CT) scans of the human pancreas make an accurate segmentation of\nthis crucial organ a challenging task for existing data-driven deep learning\nmethods. To address this problem, we present a novel model-driven stack-based\nfully convolutional network with a sliding window fusion algorithm for pancreas\nsegmentation, termed MDS-Net. The MDS-Net's cost function includes a data\napproximation term and a prior knowledge regularization term combined with a\nstack scheme for capturing and fusing the two-dimensional (2D) and local\nthree-dimensional (3D) context information. Specifically, 3D CT scans are\ndivided into multiple stacks to capture the local spatial context feature. To\nhighlight the importance of single slices, the inter-slice relationships in the\nstack data are also incorporated in the MDS-Net framework. For implementing\nthis proposed model-driven method, we create a stack-based U-Net architecture\nand successfully derive its back-propagation procedure for end-to-end training.\nFurthermore, a sliding window fusion algorithm is utilized to improve the\nconsistency of adjacent CT slices and intra-stack. Finally, extensive\nquantitative assessments on the NIH Pancreas-CT dataset demonstrated higher\npancreatic segmentation accuracy and reliability of MDS-Net compared to other\nstate-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Sun, 3 Mar 2019 04:52:49 GMT"}, {"version": "v2", "created": "Wed, 11 Dec 2019 15:14:40 GMT"}, {"version": "v3", "created": "Sat, 17 Oct 2020 01:24:22 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Li", "Hao", ""], ["Li", "Jun", ""], ["Lin", "Xiaozhu", ""], ["Qian", "Xiaohua", ""]]}, {"id": "1903.00834", "submitter": "Zhifei Zhang", "authors": "Zhifei Zhang, Zhaowen Wang, Zhe Lin, Hairong Qi", "title": "Image Super-Resolution by Neural Texture Transfer", "comments": "Project Page:\n  http://web.eecs.utk.edu/~zzhang61/project_page/SRNTT/SRNTT.html. arXiv admin\n  note: text overlap with arXiv:1804.03360", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to the significant information loss in low-resolution (LR) images, it has\nbecome extremely challenging to further advance the state-of-the-art of single\nimage super-resolution (SISR). Reference-based super-resolution (RefSR), on the\nother hand, has proven to be promising in recovering high-resolution (HR)\ndetails when a reference (Ref) image with similar content as that of the LR\ninput is given. However, the quality of RefSR can degrade severely when Ref is\nless similar. This paper aims to unleash the potential of RefSR by leveraging\nmore texture details from Ref images with stronger robustness even when\nirrelevant Ref images are provided. Inspired by the recent work on image\nstylization, we formulate the RefSR problem as neural texture transfer. We\ndesign an end-to-end deep model which enriches HR details by adaptively\ntransferring the texture from Ref images according to their textural\nsimilarity. Instead of matching content in the raw pixel space as done by\nprevious methods, our key contribution is a multi-level matching conducted in\nthe neural space. This matching scheme facilitates multi-scale neural transfer\nthat allows the model to benefit more from those semantically related Ref\npatches, and gracefully degrade to SISR performance on the least relevant Ref\ninputs. We build a benchmark dataset for the general research of RefSR, which\ncontains Ref images paired with LR inputs with varying levels of similarity.\nBoth quantitative and qualitative evaluations demonstrate the superiority of\nour method over state-of-the-art.\n", "versions": [{"version": "v1", "created": "Sun, 3 Mar 2019 05:06:09 GMT"}, {"version": "v2", "created": "Wed, 6 Mar 2019 20:55:14 GMT"}], "update_date": "2019-03-11", "authors_parsed": [["Zhang", "Zhifei", ""], ["Wang", "Zhaowen", ""], ["Lin", "Zhe", ""], ["Qi", "Hairong", ""]]}, {"id": "1903.00839", "submitter": "Xihui Liu", "authors": "Xihui Liu, Zihao Wang, Jing Shao, Xiaogang Wang, Hongsheng Li", "title": "Improving Referring Expression Grounding with Cross-modal\n  Attention-guided Erasing", "comments": "Accepted by CVPR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Referring expression grounding aims at locating certain objects or persons in\nan image with a referring expression, where the key challenge is to comprehend\nand align various types of information from visual and textual domain, such as\nvisual attributes, location and interactions with surrounding regions. Although\nthe attention mechanism has been successfully applied for cross-modal\nalignments, previous attention models focus on only the most dominant features\nof both modalities, and neglect the fact that there could be multiple\ncomprehensive textual-visual correspondences between images and referring\nexpressions. To tackle this issue, we design a novel cross-modal\nattention-guided erasing approach, where we discard the most dominant\ninformation from either textual or visual domains to generate difficult\ntraining samples online, and to drive the model to discover complementary\ntextual-visual correspondences. Extensive experiments demonstrate the\neffectiveness of our proposed method, which achieves state-of-the-art\nperformance on three referring expression grounding datasets.\n", "versions": [{"version": "v1", "created": "Sun, 3 Mar 2019 05:55:15 GMT"}, {"version": "v2", "created": "Tue, 2 Apr 2019 07:33:37 GMT"}], "update_date": "2019-04-03", "authors_parsed": [["Liu", "Xihui", ""], ["Wang", "Zihao", ""], ["Shao", "Jing", ""], ["Wang", "Xiaogang", ""], ["Li", "Hongsheng", ""]]}, {"id": "1903.00853", "submitter": "Zehao Xiao", "authors": "Xiaolong Jiang, Zehao Xiao, Baochang Zhang, Xiantong Zhen, Xianbin\n  Cao, David Doermann, Ling Shao", "title": "Crowd Counting and Density Estimation by Trellis Encoder-Decoder Network", "comments": "CVPR 2019, Accepted", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Crowd counting has recently attracted increasing interest in computer vision\nbut remains a challenging problem. In this paper, we propose a trellis\nencoder-decoder network (TEDnet) for crowd counting, which focuses on\ngenerating high-quality density estimation maps. The major contributions are\nfour-fold. First, we develop a new trellis architecture that incorporates\nmultiple decoding paths to hierarchically aggregate features at different\nencoding stages, which can handle large variations of objects. Second, we\ndesign dense skip connections interleaved across paths to facilitate sufficient\nmulti-scale feature fusions and to absorb the supervision information. Third,\nwe propose a new combinatorial loss to enforce local coherence and spatial\ncorrelation in density maps. By distributedly imposing this combinatorial loss\non intermediate outputs, gradient vanishing can be largely alleviated for\nbetter back-propagation and faster convergence. Finally, our TEDnet achieves\nnew state-of-the art performance on four benchmarks, with an improvement up to\n14% in terms of MAE.\n", "versions": [{"version": "v1", "created": "Sun, 3 Mar 2019 08:04:26 GMT"}, {"version": "v2", "created": "Fri, 19 Apr 2019 06:27:59 GMT"}], "update_date": "2019-04-22", "authors_parsed": [["Jiang", "Xiaolong", ""], ["Xiao", "Zehao", ""], ["Zhang", "Baochang", ""], ["Zhen", "Xiantong", ""], ["Cao", "Xianbin", ""], ["Doermann", "David", ""], ["Shao", "Ling", ""]]}, {"id": "1903.00857", "submitter": "Gongjie Zhang", "authors": "Gongjie Zhang, Shijian Lu, Wei Zhang", "title": "CAD-Net: A Context-Aware Detection Network for Objects in Remote Sensing\n  Imagery", "comments": null, "journal-ref": null, "doi": "10.1109/TGRS.2019.2930982", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurate and robust detection of multi-class objects in optical remote\nsensing images is essential to many real-world applications such as urban\nplanning, traffic control, searching and rescuing, etc. However,\nstate-of-the-art object detection techniques designed for images captured using\nground-level sensors usually experience a sharp performance drop when directly\napplied to remote sensing images, largely due to the object appearance\ndifferences in remote sensing images in term of sparse texture, low contrast,\narbitrary orientations, large scale variations, etc. This paper presents a\nnovel object detection network (CAD-Net) that exploits attention-modulated\nfeatures as well as global and local contexts to address the new challenges in\ndetecting objects from remote sensing images. The proposed CAD-Net learns\nglobal and local contexts of objects by capturing their correlations with the\nglobal scene (at scene-level) and the local neighboring objects or features (at\nobject-level), respectively. In addition, it designs a spatial-and-scale-aware\nattention module that guides the network to focus on more informative regions\nand features as well as more appropriate feature scales. Experiments over two\npublicly available object detection datasets for remote sensing images\ndemonstrate that the proposed CAD-Net achieves superior detection performance.\nThe implementation codes will be made publicly available for facilitating\nfuture researches.\n", "versions": [{"version": "v1", "created": "Sun, 3 Mar 2019 08:16:11 GMT"}], "update_date": "2020-01-08", "authors_parsed": [["Zhang", "Gongjie", ""], ["Lu", "Shijian", ""], ["Zhang", "Wei", ""]]}, {"id": "1903.00858", "submitter": "Kiyoharu Aizawa Dr. Prof.", "authors": "Masashi Anzawa, Sosuke Amano, Yoko Yamakata, Keiko Motonaga, Akiko\n  Kamei, Kiyoharu Aizawa", "title": "Recognition of Multiple Food Items in a Single Photo for Use in a\n  Buffet-Style Restaurant", "comments": "5 pages, 7 figures", "journal-ref": "IEICE TRANSACTIONS on Information and Systems, 2019", "doi": "10.1587/transinf.2018EDL8183", "report-no": "Vol.E102-D No.2 pp.410-414", "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate image recognition of multiple food items in a single photo,\nfocusing on a buffet restaurant application, where menu changes at every meal,\nand only a few images per class are available. After detecting food areas, we\nperform hierarchical recognition. We evaluate our results, comparing to two\nbaseline methods.\n", "versions": [{"version": "v1", "created": "Sun, 3 Mar 2019 08:24:55 GMT"}], "update_date": "2019-04-16", "authors_parsed": [["Anzawa", "Masashi", ""], ["Amano", "Sosuke", ""], ["Yamakata", "Yoko", ""], ["Motonaga", "Keiko", ""], ["Kamei", "Akiko", ""], ["Aizawa", "Kiyoharu", ""]]}, {"id": "1903.00859", "submitter": "Bo Xiong", "authors": "Bo Xiong, Yannis Kalantidis, Deepti Ghadiyaram, Kristen Grauman", "title": "Less is More: Learning Highlight Detection from Video Duration", "comments": "To appear in CVPR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Highlight detection has the potential to significantly ease video browsing,\nbut existing methods often suffer from expensive supervision requirements,\nwhere human viewers must manually identify highlights in training videos. We\npropose a scalable unsupervised solution that exploits video duration as an\nimplicit supervision signal. Our key insight is that video segments from\nshorter user-generated videos are more likely to be highlights than those from\nlonger videos, since users tend to be more selective about the content when\ncapturing shorter videos. Leveraging this insight, we introduce a novel ranking\nframework that prefers segments from shorter videos, while properly accounting\nfor the inherent noise in the (unlabeled) training data. We use it to train a\nhighlight detector with 10M hashtagged Instagram videos. In experiments on two\nchallenging public video highlight detection benchmarks, our method\nsubstantially improves the state-of-the-art for unsupervised highlight\ndetection.\n", "versions": [{"version": "v1", "created": "Sun, 3 Mar 2019 08:34:16 GMT"}], "update_date": "2019-03-05", "authors_parsed": [["Xiong", "Bo", ""], ["Kalantidis", "Yannis", ""], ["Ghadiyaram", "Deepti", ""], ["Grauman", "Kristen", ""]]}, {"id": "1903.00865", "submitter": "Patrick Wan", "authors": "Renjie Wan, Boxin Shi, Haoliang Li, Ling-Yu Duan, and Alex C. Kot", "title": "Face Image Reflection Removal", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Face images captured through the glass are usually contaminated by\nreflections. The non-transmitted reflections make the reflection removal more\nchallenging than for general scenes, because important facial features are\ncompletely occluded. In this paper, we propose and solve the face image\nreflection removal problem. We remove non-transmitted reflections by\nincorporating inpainting ideas into a guided reflection removal framework and\nrecover facial features by considering various face-specific priors. We use a\nnewly collected face reflection image dataset to train our model and compare\nwith state-of-the-art methods. The proposed method shows advantages in\nestimating reflection-free face images for improving face recognition.\n", "versions": [{"version": "v1", "created": "Sun, 3 Mar 2019 09:11:17 GMT"}], "update_date": "2019-03-05", "authors_parsed": [["Wan", "Renjie", ""], ["Shi", "Boxin", ""], ["Li", "Haoliang", ""], ["Duan", "Ling-Yu", ""], ["Kot", "Alex C.", ""]]}, {"id": "1903.00875", "submitter": "Xuecai Hu", "authors": "Xuecai Hu, Haoyuan Mu, Xiangyu Zhang, Zilei Wang, Tieniu Tan, Jian Sun", "title": "Meta-SR: A Magnification-Arbitrary Network for Super-Resolution", "comments": "10 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent research on super-resolution has achieved great success due to the\ndevelopment of deep convolutional neural networks (DCNNs). However,\nsuper-resolution of arbitrary scale factor has been ignored for a long time.\nMost previous researchers regard super-resolution of different scale factors as\nindependent tasks. They train a specific model for each scale factor which is\ninefficient in computing, and prior work only take the super-resolution of\nseveral integer scale factors into consideration. In this work, we propose a\nnovel method called Meta-SR to firstly solve super-resolution of arbitrary\nscale factor (including non-integer scale factors) with a single model. In our\nMeta-SR, the Meta-Upscale Module is proposed to replace the traditional upscale\nmodule. For arbitrary scale factor, the Meta-Upscale Module dynamically\npredicts the weights of the upscale filters by taking the scale factor as input\nand use these weights to generate the HR image of arbitrary size. For any\nlow-resolution image, our Meta-SR can continuously zoom in it with arbitrary\nscale factor by only using a single model. We evaluated the proposed method\nthrough extensive experiments on widely used benchmark datasets on single image\nsuper-resolution. The experimental results show the superiority of our\nMeta-Upscale.\n", "versions": [{"version": "v1", "created": "Sun, 3 Mar 2019 10:17:45 GMT"}, {"version": "v2", "created": "Fri, 15 Mar 2019 07:52:32 GMT"}, {"version": "v3", "created": "Fri, 22 Mar 2019 02:23:10 GMT"}, {"version": "v4", "created": "Wed, 3 Apr 2019 15:10:13 GMT"}], "update_date": "2019-04-04", "authors_parsed": [["Hu", "Xuecai", ""], ["Mu", "Haoyuan", ""], ["Zhang", "Xiangyu", ""], ["Wang", "Zilei", ""], ["Tan", "Tieniu", ""], ["Sun", "Jian", ""]]}, {"id": "1903.00879", "submitter": "Karen L\\'opez-Linares Rom\\'an", "authors": "Karen L\\'opez-Linares, Inmaculada Garc\\'ia, Ainhoa Garc\\'ia-Familiar,\n  Iv\\'an Mac\\'ia, and Miguel A. Gonz\\'alez Ballester", "title": "3D convolutional neural network for abdominal aortic aneurysm\n  segmentation", "comments": "Submitted to Medical Image Analysis", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An abdominal aortic aneurysm (AAA) is a focal dilation of the aorta that, if\nnot treated, tends to grow and may rupture. A significant unmet need in the\nassessment of AAA disease, for the diagnosis, prognosis and follow-up, is the\ndetermination of rupture risk, which is currently based on the manual\nmeasurement of the aneurysm diameter in a selected Computed Tomography\nAngiography (CTA) scan. However, there is a lack of standardization determining\nthe degree and rate of disease progression, due to the lack of robust,\nautomated aneurysm segmentation tools that allow quantitatively analyzing the\nAAA. In this work, we aim at proposing the first 3D convolutional neural\nnetwork for the segmentation of aneurysms both from preoperative and\npostoperative CTA scans. We extensively validate its performance in terms of\ndiameter measurements, to test its applicability in the clinical practice, as\nwell as regarding the relative volume difference, and Dice and Jaccard scores.\nThe proposed method yields a mean diameter measurement error of 3.3 mm, a\nrelative volume difference of 8.58 %, and Dice and Jaccard scores of 87 % and\n77 %, respectively. At a clinical level, an aneurysm enlargement of 10 mm is\nconsidered relevant, thus, our method is suitable to automatically determine\nthe AAA diameter and opens up the opportunity for more complex aneurysm\nanalysis.\n", "versions": [{"version": "v1", "created": "Sun, 3 Mar 2019 10:55:48 GMT"}], "update_date": "2019-03-05", "authors_parsed": [["L\u00f3pez-Linares", "Karen", ""], ["Garc\u00eda", "Inmaculada", ""], ["Garc\u00eda-Familiar", "Ainhoa", ""], ["Mac\u00eda", "Iv\u00e1n", ""], ["Ballester", "Miguel A. Gonz\u00e1lez", ""]]}, {"id": "1903.00905", "submitter": "Anirudha Vishvakarma", "authors": "Anirudha Vishvakarma", "title": "MILDNet: A Lightweight Single Scaled Deep Ranking Architecture", "comments": "12 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-scale deep CNN architecture [1, 2, 3] successfully captures both fine\nand coarse level image descriptors for visual similarity task, but they come up\nwith expensive memory overhead and latency. In this paper, we propose a\ncompeting novel CNN architecture, called MILDNet, which merits by being vastly\ncompact (about 3 times). Inspired by the fact that successive CNN layers\nrepresent the image with increasing levels of abstraction, we compressed our\ndeep ranking model to a single CNN by coupling activations from multiple\nintermediate layers along with the last layer. Trained on the famous\nStreet2shop dataset [4], we demonstrate that our approach performs as good as\nthe current state-of-the-art models with only one third of the parameters,\nmodel size, training time and significant reduction in inference time. The\nsignificance of intermediate layers on image retrieval task has also been shown\nto be performing on popular datasets Holidays, Oxford, Paris [5]. So even\nthough our experiments are done on ecommerce domain, it is applicable to other\ndomains as well. We further did an ablation study to validate our hypothesis by\nchecking the impact on adding each intermediate layer. With this we also\npresent two more useful variants of MILDNet, a mobile model (12 times smaller)\nfor on-edge devices and a compactly featured model (512-d feature embeddings)\nfor systems with less RAMs and to reduce the ranking cost. Further we present\nan intuitive way to automatically create a tailored in-house triplet training\ndataset, which is very hard to create manually. This solution too can also be\ndeployed as an all-inclusive visual similarity solution. Finally, we present\nour entire production level architecture which currently powers visual\nsimilarity at Fynd.\n", "versions": [{"version": "v1", "created": "Sun, 3 Mar 2019 13:26:37 GMT"}, {"version": "v2", "created": "Thu, 14 Mar 2019 02:54:09 GMT"}], "update_date": "2019-03-15", "authors_parsed": [["Vishvakarma", "Anirudha", ""]]}, {"id": "1903.00912", "submitter": "Dingfu Zhou", "authors": "Dingfu Zhou, Yuchao Dai and Hongdong Li", "title": "Ground Plane based Absolute Scale Estimation for Monocular Visual\n  Odometry", "comments": "Accepted by IEEE Transactions on Intelligent Transportation Systems", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recovering the absolute metric scale from a monocular camera is a challenging\nbut highly desirable problem for monocular camera-based systems. By using\ndifferent kinds of cues, various approaches have been proposed for scale\nestimation, such as camera height, object size etc. In this paper, firstly, we\nsummarize different kinds of scale estimation approaches. Then, we propose a\nrobust divide and conquer the absolute scale estimation method based on the\nground plane and camera height by analyzing the advantages and disadvantages of\ndifferent approaches. By using the estimated scale, an effective scale\ncorrection strategy has been proposed to reduce the scale drift during the\nMonocular Visual Odometry (VO) estimation process. Finally, the effectiveness\nand robustness of the proposed method have been verified on both public and\nself-collected image sequences.\n", "versions": [{"version": "v1", "created": "Sun, 3 Mar 2019 13:45:50 GMT"}], "update_date": "2019-03-05", "authors_parsed": [["Zhou", "Dingfu", ""], ["Dai", "Yuchao", ""], ["Li", "Hongdong", ""]]}, {"id": "1903.00913", "submitter": "Lu Sheng", "authors": "Lu Sheng, Junting Pan, Jiaming Guo, Jing Shao, Xiaogang Wang, Chen\n  Change Loy", "title": "Unsupervised Bi-directional Flow-based Video Generation from one\n  Snapshot", "comments": "11 pages, 12 figures. Technical report for a project in progress", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Imagining multiple consecutive frames given one single snapshot is\nchallenging, since it is difficult to simultaneously predict diverse motions\nfrom a single image and faithfully generate novel frames without visual\ndistortions. In this work, we leverage an unsupervised variational model to\nlearn rich motion patterns in the form of long-term bi-directional flow fields,\nand apply the predicted flows to generate high-quality video sequences. In\ncontrast to the state-of-the-art approach, our method does not require external\nflow supervisions for learning. This is achieved through a novel module that\nperforms bi-directional flows prediction from a single image. In addition, with\nthe bi-directional flow consistency check, our method can handle occlusion and\nwarping artifacts in a principled manner. Our method can be trained end-to-end\nbased on arbitrarily sampled natural video clips, and it is able to capture\nmulti-modal motion uncertainty and synthesizes photo-realistic novel sequences.\nQuantitative and qualitative evaluations over synthetic and real-world datasets\ndemonstrate the effectiveness of the proposed approach over the\nstate-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Sun, 3 Mar 2019 13:48:32 GMT"}], "update_date": "2019-03-05", "authors_parsed": [["Sheng", "Lu", ""], ["Pan", "Junting", ""], ["Guo", "Jiaming", ""], ["Shao", "Jing", ""], ["Wang", "Xiaogang", ""], ["Loy", "Chen Change", ""]]}, {"id": "1903.00923", "submitter": "Jun Li", "authors": "Jun Li, Xiaozhu Lin, Hui Che, Hao Li, Xiaohua Qian", "title": "Pancreas segmentation with probabilistic map guided bi-directional\n  recurrent UNet", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pancreas segmentation in medical imaging data is of great significance for\nclinical pancreas diagnostics and treatment. However, the large population\nvariations in the pancreas shape and volume cause enormous segmentation\ndifficulties, even for state-of-the-art algorithms utilizing\nfully-convolutional neural networks (FCNs). Specifically, pancreas segmentation\nsuffers from the loss of spatial information in 2D methods, and the high\ncomputational cost of 3D methods. To alleviate these problems, we propose a\nprobabilistic-map-guided bi-directional recurrent UNet (PBR-UNet) architecture,\nwhich fuses intra-slice information and inter-slice probabilistic maps into a\nlocal 3D hybrid regularization scheme, which is followed by bi-directional\nrecurrent network optimization. The PBR-UNet method consists of an initial\nestimation module for efficiently extracting pixel-level probabilistic maps and\na primary segmentation module for propagating hybrid information through a 2.5D\nU-Net architecture. Specifically, local 3D information is inferred by combining\nan input image with the probabilistic maps of the adjacent slices into\nmultichannel hybrid data, and then hierarchically aggregating the hybrid\ninformation of the entire segmentation network. Besides, a bi-directional\nrecurrent optimization mechanism is developed to update the hybrid information\nin both the forward and the backward directions. This allows the proposed\nnetwork to make full and optimal use of the local context information.\nQuantitative and qualitative evaluation was performed on the NIH Pancreas-CT\ndataset, and our proposed PBR-UNet method achieved better segmentation results\nwith less computational cost compared to other state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Sun, 3 Mar 2019 15:13:33 GMT"}, {"version": "v2", "created": "Mon, 18 Mar 2019 16:00:54 GMT"}, {"version": "v3", "created": "Sun, 7 Apr 2019 15:21:28 GMT"}, {"version": "v4", "created": "Sun, 15 Dec 2019 15:16:04 GMT"}, {"version": "v5", "created": "Thu, 13 May 2021 12:31:52 GMT"}], "update_date": "2021-05-14", "authors_parsed": [["Li", "Jun", ""], ["Lin", "Xiaozhu", ""], ["Che", "Hui", ""], ["Li", "Hao", ""], ["Qian", "Xiaohua", ""]]}, {"id": "1903.00925", "submitter": "Jasmine Collins", "authors": "Jasmine Collins and Johannes Balle and Jonathon Shlens", "title": "Accelerating Training of Deep Neural Networks with a Standardization\n  Loss", "comments": "Technical report. Results presented at WiML 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A significant advance in accelerating neural network training has been the\ndevelopment of normalization methods, permitting the training of deep models\nboth faster and with better accuracy. These advances come with practical\nchallenges: for instance, batch normalization ties the prediction of individual\nexamples with other examples within a batch, resulting in a network that is\nheavily dependent on batch size. Layer normalization and group normalization\nare data-dependent and thus must be continually used, even at test-time. To\naddress the issues that arise from using explicit normalization techniques, we\npropose to replace existing normalization methods with a simple, secondary\nobjective loss that we term a standardization loss. This formulation is\nflexible and robust across different batch sizes and surprisingly, this\nsecondary objective accelerates learning on the primary training objective.\nBecause it is a training loss, it is simply removed at test-time, and no\nfurther effort is needed to maintain normalized activations. We find that a\nstandardization loss accelerates training on both small- and large-scale image\nclassification experiments, works with a variety of architectures, and is\nlargely robust to training across different batch sizes.\n", "versions": [{"version": "v1", "created": "Sun, 3 Mar 2019 15:17:06 GMT"}], "update_date": "2019-03-05", "authors_parsed": [["Collins", "Jasmine", ""], ["Balle", "Johannes", ""], ["Shlens", "Jonathon", ""]]}, {"id": "1903.00963", "submitter": "Cunjian Chen", "authors": "Cunjian Chen, Arun Ross", "title": "Matching Thermal to Visible Face Images Using a Semantic-Guided\n  Generative Adversarial Network", "comments": "Accepted for publication in 2019 14th IEEE International Conference\n  on Automatic Face & Gesture Recognition (FG 2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Designing face recognition systems that are capable of matching face images\nobtained in the thermal spectrum with those obtained in the visible spectrum is\na challenging problem. In this work, we propose the use of semantic-guided\ngenerative adversarial network (SG-GAN) to automatically synthesize visible\nface images from their thermal counterparts. Specifically, semantic labels,\nextracted by a face parsing network, are used to compute a semantic loss\nfunction to regularize the adversarial network during training. These semantic\ncues denote high-level facial component information associated with each pixel.\nFurther, an identity extraction network is leveraged to generate multi-scale\nfeatures to compute an identity loss function. To achieve photo-realistic\nresults, a perceptual loss function is introduced during network training to\nensure that the synthesized visible face is perceptually similar to the target\nvisible face image. We extensively evaluate the benefits of individual loss\nfunctions, and combine them effectively to learn the mapping from thermal to\nvisible face images. Experiments involving two multispectral face datasets show\nthat the proposed method achieves promising results in both face synthesis and\ncross-spectral face matching.\n", "versions": [{"version": "v1", "created": "Sun, 3 Mar 2019 19:00:05 GMT"}], "update_date": "2019-03-05", "authors_parsed": [["Chen", "Cunjian", ""], ["Ross", "Arun", ""]]}, {"id": "1903.00987", "submitter": "Andrea Nicastro", "authors": "Andrea Nicastro, Ronald Clark, Stefan Leutenegger", "title": "X-Section: Cross-Section Prediction for Enhanced RGBD Fusion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Detailed 3D reconstruction is an important challenge with application to\nrobotics, augmented and virtual reality, which has seen impressive progress\nthroughout the past years. Advancements were driven by the availability of\ndepth cameras (RGB-D), as well as increased compute power, e.g.\\ in the form of\nGPUs -- but also thanks to inclusion of machine learning in the process. Here,\nwe propose X-Section, an RGB-D 3D reconstruction approach that leverages deep\nlearning to make object-level predictions about thicknesses that can be readily\nintegrated into a volumetric multi-view fusion process, where we propose an\nextension to the popular KinectFusion approach. In essence, our method allows\nto complete shape in general indoor scenes behind what is sensed by the RGB-D\ncamera, which may be crucial e.g.\\ for robotic manipulation tasks or efficient\nscene exploration. Predicting object thicknesses rather than volumes allows us\nto work with comparably high spatial resolution without exploding memory and\ntraining data requirements on the employed Convolutional Neural Networks. In a\nseries of qualitative and quantitative evaluations, we demonstrate how we\naccurately predict object thickness and reconstruct general 3D scenes\ncontaining multiple objects.\n", "versions": [{"version": "v1", "created": "Sun, 3 Mar 2019 20:58:04 GMT"}, {"version": "v2", "created": "Mon, 8 Apr 2019 14:44:33 GMT"}, {"version": "v3", "created": "Mon, 12 Aug 2019 14:32:28 GMT"}], "update_date": "2019-08-13", "authors_parsed": [["Nicastro", "Andrea", ""], ["Clark", "Ronald", ""], ["Leutenegger", "Stefan", ""]]}, {"id": "1903.01000", "submitter": "Vivek Sharma", "authors": "Vivek Sharma, Makarand Tapaswi, M.Saquib Sarfraz, Rainer Stiefelhagen", "title": "Self-Supervised Learning of Face Representations for Video Face\n  Clustering", "comments": "To appear at International Conference on Automatic Face and Gesture\n  Recognition (2019) as an Oral. The datasets and code are available at\n  https://github.com/vivoutlaw/SSIAM", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Analyzing the story behind TV series and movies often requires understanding\nwho the characters are and what they are doing. With improving deep face\nmodels, this may seem like a solved problem. However, as face detectors get\nbetter, clustering/identification needs to be revisited to address increasing\ndiversity in facial appearance. In this paper, we address video face clustering\nusing unsupervised methods. Our emphasis is on distilling the essential\ninformation, identity, from the representations obtained using deep pre-trained\nface networks. We propose a self-supervised Siamese network that can be trained\nwithout the need for video/track based supervision, and thus can also be\napplied to image collections. We evaluate our proposed method on three video\nface clustering datasets. The experiments show that our methods outperform\ncurrent state-of-the-art methods on all datasets. Video face clustering is\nlacking a common benchmark as current works are often evaluated with different\nmetrics and/or different sets of face tracks.\n", "versions": [{"version": "v1", "created": "Sun, 3 Mar 2019 21:53:15 GMT"}], "update_date": "2019-03-05", "authors_parsed": [["Sharma", "Vivek", ""], ["Tapaswi", "Makarand", ""], ["Sarfraz", "M. Saquib", ""], ["Stiefelhagen", "Rainer", ""]]}, {"id": "1903.01013", "submitter": "Bardia Doosti", "authors": "Bardia Doosti", "title": "Hand Pose Estimation: A Survey", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The success of Deep Convolutional Neural Networks (CNNs) in recent years in\nalmost all the Computer Vision tasks on one hand, and the popularity of\nlow-cost consumer depth cameras on the other, has made Hand Pose Estimation a\nhot topic in computer vision field. In this report, we will first explain the\nhand pose estimation problem and will review major approaches solving this\nproblem, especially the two different problems of using depth maps or RGB\nimages. We will survey the most important papers in each field and will discuss\nthe strengths and weaknesses of each. Finally, we will explain the biggest\ndatasets in this field in detail and list 22 datasets with all their\nproperties. To the best of our knowledge this is the most complete list of all\nthe datasets in the hand pose estimation field.\n", "versions": [{"version": "v1", "created": "Sun, 3 Mar 2019 23:19:12 GMT"}, {"version": "v2", "created": "Sun, 2 Jun 2019 20:58:48 GMT"}], "update_date": "2019-06-04", "authors_parsed": [["Doosti", "Bardia", ""]]}, {"id": "1903.01015", "submitter": "Kumar Abhishek", "authors": "Saeid Asgari Taghanaki, Kumar Abhishek, Shekoofeh Azizi, Ghassan\n  Hamarneh", "title": "A Kernelized Manifold Mapping to Diminish the Effect of Adversarial\n  Perturbations", "comments": "Accepted to CVPR 2019. 10 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The linear and non-flexible nature of deep convolutional models makes them\nvulnerable to carefully crafted adversarial perturbations. To tackle this\nproblem, we propose a non-linear radial basis convolutional feature mapping by\nlearning a Mahalanobis-like distance function. Our method then maps the\nconvolutional features onto a linearly well-separated manifold, which prevents\nsmall adversarial perturbations from forcing a sample to cross the decision\nboundary. We test the proposed method on three publicly available image\nclassification and segmentation datasets namely, MNIST, ISBI ISIC 2017 skin\nlesion segmentation, and NIH Chest X-Ray-14. We evaluate the robustness of our\nmethod to different gradient (targeted and untargeted) and non-gradient based\nattacks and compare it to several non-gradient masking defense strategies. Our\nresults demonstrate that the proposed method can increase the resilience of\ndeep convolutional neural networks to adversarial perturbations without\naccuracy drop on clean data.\n", "versions": [{"version": "v1", "created": "Sun, 3 Mar 2019 23:21:23 GMT"}, {"version": "v2", "created": "Thu, 9 May 2019 00:37:01 GMT"}], "update_date": "2019-05-10", "authors_parsed": [["Taghanaki", "Saeid Asgari", ""], ["Abhishek", "Kumar", ""], ["Azizi", "Shekoofeh", ""], ["Hamarneh", "Ghassan", ""]]}, {"id": "1903.01031", "submitter": "Poojan Oza", "authors": "Poojan Oza and Vishal M. Patel", "title": "Active Authentication using an Autoencoder regularized CNN-based\n  One-Class Classifier", "comments": "Accepted and to appear at AFGR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Active authentication refers to the process in which users are unobtrusively\nmonitored and authenticated continuously throughout their interactions with\nmobile devices. Generally, an active authentication problem is modelled as a\none class classification problem due to the unavailability of data from the\nimpostor users. Normally, the enrolled user is considered as the target class\n(genuine) and the unauthorized users are considered as unknown classes\n(impostor). We propose a convolutional neural network (CNN) based approach for\none class classification in which a zero centered Gaussian noise and an\nautoencoder are used to model the pseudo-negative class and to regularize the\nnetwork to learn meaningful feature representations for one class data,\nrespectively. The overall network is trained using a combination of the\ncross-entropy and the reconstruction error losses. A key feature of the\nproposed approach is that any pre-trained CNN can be used as the base network\nfor one class classification. Effectiveness of the proposed framework is\ndemonstrated using three publically available face-based active authentication\ndatasets and it is shown that the proposed method achieves superior performance\ncompared to the traditional one class classification methods. The source code\nis available at: github.com/otkupjnoz/oc-acnn.\n", "versions": [{"version": "v1", "created": "Mon, 4 Mar 2019 01:20:15 GMT"}], "update_date": "2019-03-05", "authors_parsed": [["Oza", "Poojan", ""], ["Patel", "Vishal M.", ""]]}, {"id": "1903.01038", "submitter": "Yunbo Wang", "authors": "Yunbo Wang, Mingsheng Long, Jianmin Wang, Philip S. Yu", "title": "Spatiotemporal Pyramid Network for Video Action Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Two-stream convolutional networks have shown strong performance in video\naction recognition tasks. The key idea is to learn spatiotemporal features by\nfusing convolutional networks spatially and temporally. However, it remains\nunclear how to model the correlations between the spatial and temporal\nstructures at multiple abstraction levels. First, the spatial stream tends to\nfail if two videos share similar backgrounds. Second, the temporal stream may\nbe fooled if two actions resemble in short snippets, though appear to be\ndistinct in the long term. We propose a novel spatiotemporal pyramid network to\nfuse the spatial and temporal features in a pyramid structure such that they\ncan reinforce each other. From the architecture perspective, our network\nconstitutes hierarchical fusion strategies which can be trained as a whole\nusing a unified spatiotemporal loss. A series of ablation experiments support\nthe importance of each fusion strategy. From the technical perspective, we\nintroduce the spatiotemporal compact bilinear operator into video analysis\ntasks. This operator enables efficient training of bilinear fusion operations\nwhich can capture full interactions between the spatial and temporal features.\nOur final network achieves state-of-the-art results on standard video datasets.\n", "versions": [{"version": "v1", "created": "Mon, 4 Mar 2019 01:36:41 GMT"}], "update_date": "2019-03-05", "authors_parsed": [["Wang", "Yunbo", ""], ["Long", "Mingsheng", ""], ["Wang", "Jianmin", ""], ["Yu", "Philip S.", ""]]}, {"id": "1903.01067", "submitter": "Antoni Rosinol", "authors": "Antoni Rosinol, Torsten Sattler, Marc Pollefeys, Luca Carlone", "title": "Incremental Visual-Inertial 3D Mesh Generation with Structural\n  Regularities", "comments": "7 pages, 5 figures, ICRA accepted", "journal-ref": "IEEE Int. Conf. Robot. Autom. (ICRA), 2019", "doi": null, "report-no": null, "categories": "cs.CV cs.CG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual-Inertial Odometry (VIO) algorithms typically rely on a point cloud\nrepresentation of the scene that does not model the topology of the\nenvironment. A 3D mesh instead offers a richer, yet lightweight, model.\nNevertheless, building a 3D mesh out of the sparse and noisy 3D landmarks\ntriangulated by a VIO algorithm often results in a mesh that does not fit the\nreal scene. In order to regularize the mesh, previous approaches decouple state\nestimation from the 3D mesh regularization step, and either limit the 3D mesh\nto the current frame or let the mesh grow indefinitely. We propose instead to\ntightly couple mesh regularization and state estimation by detecting and\nenforcing structural regularities in a novel factor-graph formulation. We also\npropose to incrementally build the mesh by restricting its extent to the\ntime-horizon of the VIO optimization; the resulting 3D mesh covers a larger\nportion of the scene than a per-frame approach while its memory usage and\ncomputational complexity remain bounded. We show that our approach successfully\nregularizes the mesh, while improving localization accuracy, when structural\nregularities are present, and remains operational in scenes without\nregularities.\n", "versions": [{"version": "v1", "created": "Mon, 4 Mar 2019 04:24:50 GMT"}, {"version": "v2", "created": "Mon, 29 Jul 2019 16:36:41 GMT"}], "update_date": "2019-07-30", "authors_parsed": [["Rosinol", "Antoni", ""], ["Sattler", "Torsten", ""], ["Pollefeys", "Marc", ""], ["Carlone", "Luca", ""]]}, {"id": "1903.01072", "submitter": "Chee Seng Chan", "authors": "Jia Huei Tan, Chee Seng Chan, Joon Huang Chuah", "title": "COMIC: Towards A Compact Image Captioning Model with Attention", "comments": "Added source code link and new results in Table 3", "journal-ref": null, "doi": "10.1109/TMM.2019.2904878", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent works in image captioning have shown very promising raw performance.\nHowever, we realize that most of these encoder-decoder style networks with\nattention do not scale naturally to large vocabulary size, making them\ndifficult to be deployed on embedded system with limited hardware resources.\nThis is because the size of word and output embedding matrices grow\nproportionally with the size of vocabulary, adversely affecting the compactness\nof these networks. To address this limitation, this paper introduces a brand\nnew idea in the domain of image captioning. That is, we tackle the problem of\ncompactness of image captioning models which is hitherto unexplored. We showed\nthat, our proposed model, named COMIC for COMpact Image Captioning, achieves\ncomparable results in five common evaluation metrics with state-of-the-art\napproaches on both MS-COCO and InstaPIC-1.1M datasets despite having an\nembedding vocabulary size that is 39x - 99x smaller. The source code and models\nare available at:\nhttps://github.com/jiahuei/COMIC-Compact-Image-Captioning-with-Attention\n", "versions": [{"version": "v1", "created": "Mon, 4 Mar 2019 05:09:16 GMT"}, {"version": "v2", "created": "Sat, 16 Mar 2019 12:28:56 GMT"}, {"version": "v3", "created": "Tue, 11 Jun 2019 18:43:19 GMT"}], "update_date": "2019-06-13", "authors_parsed": [["Tan", "Jia Huei", ""], ["Chan", "Chee Seng", ""], ["Chuah", "Joon Huang", ""]]}, {"id": "1903.01078", "submitter": "Xiaoyang Guo", "authors": "Mingyang Liang, Xiaoyang Guo, Hongsheng Li, Xiaogang Wang, You Song", "title": "Unsupervised Cross-spectral Stereo Matching by Learning to Synthesize", "comments": "accepted by AAAI-19", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unsupervised cross-spectral stereo matching aims at recovering disparity\ngiven cross-spectral image pairs without any supervision in the form of ground\ntruth disparity or depth. The estimated depth provides additional information\ncomplementary to individual semantic features, which can be helpful for other\nvision tasks such as tracking, recognition and detection. However, there are\nlarge appearance variations between images from different spectral bands, which\nis a challenge for cross-spectral stereo matching. Existing deep unsupervised\nstereo matching methods are sensitive to the appearance variations and do not\nperform well on cross-spectral data. We propose a novel unsupervised\ncross-spectral stereo matching framework based on image-to-image translation.\nFirst, a style adaptation network transforms images across different spectral\nbands by cycle consistency and adversarial learning, during which appearance\nvariations are minimized. Then, a stereo matching network is trained with image\npairs from the same spectra using view reconstruction loss. At last, the\nestimated disparity is utilized to supervise the spectral-translation network\nin an end-to-end way. Moreover, a novel style adaptation network F-cycleGAN is\nproposed to improve the robustness of spectral translation. Our method can\ntackle appearance variations and enhance the robustness of unsupervised\ncross-spectral stereo matching. Experimental results show that our method\nachieves good performance without using depth supervision or explicit semantic\ninformation.\n", "versions": [{"version": "v1", "created": "Mon, 4 Mar 2019 05:29:00 GMT"}], "update_date": "2019-03-05", "authors_parsed": [["Liang", "Mingyang", ""], ["Guo", "Xiaoyang", ""], ["Li", "Hongsheng", ""], ["Wang", "Xiaogang", ""], ["Song", "You", ""]]}, {"id": "1903.01084", "submitter": "Shenghua He", "authors": "Shenghua He, Kyaw Thu Minn, Lilianna Solnica-Krezel, Mark Anastasio,\n  and Hua Li", "title": "Automatic microscopic cell counting by use of deeply-supervised density\n  regression model", "comments": "SPIE medical imaging 2019 oral presentation", "journal-ref": null, "doi": "10.1117/12.2513045", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurately counting cells in microscopic images is important for medical\ndiagnoses and biological studies, but manual cell counting is very tedious,\ntime-consuming, and prone to subjective errors, and automatic counting can be\nless accurate than desired. To improve the accuracy of automatic cell counting,\nwe propose here a novel method that employs deeply-supervised density\nregression. A fully convolutional neural network (FCNN) serves as the primary\nFCNN for density map regression. Innovatively, a set of auxiliary FCNNs are\nemployed to provide additional supervision for learning the intermediate layers\nof the primary CNN to improve network performance. In addition, the primary CNN\nis designed as a concatenating framework to integrate multi-scale features\nthrough shortcut connections in the network, which improves the granularity of\nthe features extracted from the intermediate CNN layers and further supports\nthe final density map estimation.\n", "versions": [{"version": "v1", "created": "Mon, 4 Mar 2019 05:57:43 GMT"}, {"version": "v2", "created": "Mon, 18 Mar 2019 21:47:52 GMT"}, {"version": "v3", "created": "Fri, 22 Mar 2019 15:20:56 GMT"}], "update_date": "2019-03-25", "authors_parsed": [["He", "Shenghua", ""], ["Minn", "Kyaw Thu", ""], ["Solnica-Krezel", "Lilianna", ""], ["Anastasio", "Mark", ""], ["Li", "Hua", ""]]}, {"id": "1903.01092", "submitter": "Arghya Pal", "authors": "Arghya Pal, Vineeth N Balasubramanian", "title": "Zero-Shot Task Transfer", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we present a novel meta-learning algorithm, i.e. TTNet, that\nregresses model parameters for novel tasks for which no ground truth is\navailable (zero-shot tasks). In order to adapt to novel zero-shot tasks, our\nmeta-learner learns from the model parameters of known tasks (with ground\ntruth) and the correlation of known tasks to zero-shot tasks. Such intuition\nfinds its foothold in cognitive science, where a subject (human baby) can adapt\nto a novel-concept (depth understanding) by correlating it with old concepts\n(hand movement or self-motion), without receiving explicit supervision. We\nevaluated our model on the Taskonomy dataset, with four tasks as zero-shot:\nsurface-normal, room layout, depth, and camera pose estimation. These tasks\nwere chosen based on the data acquisition complexity and the complexity\nassociated with the learning process using a deep network. Our proposed\nmethodology out-performs state-of-the-art models (which use ground truth)on\neach of our zero-shot tasks, showing promise on zero-shot task transfer. We\nalso conducted extensive experiments to study the various choices of our\nmethodology, as well as showed how the proposed method can also be used in\ntransfer learning. To the best of our knowledge, this is the firstsuch effort\non zero-shot learning in the task space.\n", "versions": [{"version": "v1", "created": "Mon, 4 Mar 2019 07:02:42 GMT"}], "update_date": "2019-03-05", "authors_parsed": [["Pal", "Arghya", ""], ["Balasubramanian", "Vineeth N", ""]]}, {"id": "1903.01177", "submitter": "Gaku Narita", "authors": "Gaku Narita, Takashi Seno, Tomoya Ishikawa and Yohsuke Kaji", "title": "PanopticFusion: Online Volumetric Semantic Mapping at the Level of Stuff\n  and Things", "comments": "8 pages, 6 figures, Accepted to 2019 IEEE/RSJ International\n  Conference on Intelligent Robots and Systems (IROS)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose PanopticFusion, a novel online volumetric semantic mapping system\nat the level of stuff and things. In contrast to previous semantic mapping\nsystems, PanopticFusion is able to densely predict class labels of a background\nregion (stuff) and individually segment arbitrary foreground objects (things).\nIn addition, our system has the capability to reconstruct a large-scale scene\nand extract a labeled mesh thanks to its use of a spatially hashed volumetric\nmap representation. Our system first predicts pixel-wise panoptic labels (class\nlabels for stuff regions and instance IDs for thing regions) for incoming RGB\nframes by fusing 2D semantic and instance segmentation outputs. The predicted\npanoptic labels are integrated into the volumetric map together with depth\nmeasurements while keeping the consistency of the instance IDs, which could\nvary frame to frame, by referring to the 3D map at that moment. In addition, we\nconstruct a fully connected conditional random field (CRF) model with respect\nto panoptic labels for map regularization. For online CRF inference, we propose\na novel unary potential approximation and a map division strategy.\n  We evaluated the performance of our system on the ScanNet (v2) dataset.\nPanopticFusion outperformed or compared with state-of-the-art offline 3D DNN\nmethods in both semantic and instance segmentation benchmarks. Also, we\ndemonstrate a promising augmented reality application using a 3D panoptic map\ngenerated by the proposed system.\n", "versions": [{"version": "v1", "created": "Mon, 4 Mar 2019 11:27:48 GMT"}, {"version": "v2", "created": "Mon, 9 Sep 2019 07:59:08 GMT"}], "update_date": "2019-09-10", "authors_parsed": [["Narita", "Gaku", ""], ["Seno", "Takashi", ""], ["Ishikawa", "Tomoya", ""], ["Kaji", "Yohsuke", ""]]}, {"id": "1903.01182", "submitter": "Hao-Yun Chen", "authors": "Hao-Yun Chen, Pei-Hsin Wang, Chun-Hao Liu, Shih-Chieh Chang, Jia-Yu\n  Pan, Yu-Ting Chen, Wei Wei, Da-Cheng Juan", "title": "Complement Objective Training", "comments": "ICLR'19 Camera Ready", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning with a primary objective, such as softmax cross entropy for\nclassification and sequence generation, has been the norm for training deep\nneural networks for years. Although being a widely-adopted approach, using\ncross entropy as the primary objective exploits mostly the information from the\nground-truth class for maximizing data likelihood, and largely ignores\ninformation from the complement (incorrect) classes. We argue that, in addition\nto the primary objective, training also using a complement objective that\nleverages information from the complement classes can be effective in improving\nmodel performance. This motivates us to study a new training paradigm that\nmaximizes the likelihood of the groundtruth class while neutralizing the\nprobabilities of the complement classes. We conduct extensive experiments on\nmultiple tasks ranging from computer vision to natural language understanding.\nThe experimental results confirm that, compared to the conventional training\nwith just one primary objective, training also with the complement objective\nfurther improves the performance of the state-of-the-art models across all\ntasks. In addition to the accuracy improvement, we also show that models\ntrained with both primary and complement objectives are more robust to\nsingle-step adversarial attacks.\n", "versions": [{"version": "v1", "created": "Mon, 4 Mar 2019 11:35:27 GMT"}, {"version": "v2", "created": "Thu, 21 Mar 2019 18:33:12 GMT"}], "update_date": "2019-03-25", "authors_parsed": [["Chen", "Hao-Yun", ""], ["Wang", "Pei-Hsin", ""], ["Liu", "Chun-Hao", ""], ["Chang", "Shih-Chieh", ""], ["Pan", "Jia-Yu", ""], ["Chen", "Yu-Ting", ""], ["Wei", "Wei", ""], ["Juan", "Da-Cheng", ""]]}, {"id": "1903.01192", "submitter": "Prasun Roy", "authors": "Prasun Roy, Saumik Bhattacharya, Subhankar Ghosh, Umapada Pal", "title": "STEFANN: Scene Text Editor using Font Adaptive Neural Network", "comments": "Accepted in The IEEE Conference on Computer Vision and Pattern\n  Recognition (CVPR) 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Textual information in a captured scene plays an important role in scene\ninterpretation and decision making. Though there exist methods that can\nsuccessfully detect and interpret complex text regions present in a scene, to\nthe best of our knowledge, there is no significant prior work that aims to\nmodify the textual information in an image. The ability to edit text directly\non images has several advantages including error correction, text restoration\nand image reusability. In this paper, we propose a method to modify text in an\nimage at character-level. We approach the problem in two stages. At first, the\nunobserved character (target) is generated from an observed character (source)\nbeing modified. We propose two different neural network architectures - (a)\nFANnet to achieve structural consistency with source font and (b) Colornet to\npreserve source color. Next, we replace the source character with the generated\ncharacter maintaining both geometric and visual consistency with neighboring\ncharacters. Our method works as a unified platform for modifying text in\nimages. We present the effectiveness of our method on COCO-Text and ICDAR\ndatasets both qualitatively and quantitatively.\n", "versions": [{"version": "v1", "created": "Mon, 4 Mar 2019 11:56:53 GMT"}, {"version": "v2", "created": "Sat, 25 Apr 2020 08:44:55 GMT"}], "update_date": "2020-04-28", "authors_parsed": [["Roy", "Prasun", ""], ["Bhattacharya", "Saumik", ""], ["Ghosh", "Subhankar", ""], ["Pal", "Umapada", ""]]}, {"id": "1903.01197", "submitter": "Di Xie", "authors": "Chao Li and Qiaoyong Zhong and Di Xie and Shiliang Pu", "title": "Collaborative Spatio-temporal Feature Learning for Video Action\n  Recognition", "comments": "CVPR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spatio-temporal feature learning is of central importance for action\nrecognition in videos. Existing deep neural network models either learn spatial\nand temporal features independently (C2D) or jointly with unconstrained\nparameters (C3D). In this paper, we propose a novel neural operation which\nencodes spatio-temporal features collaboratively by imposing a weight-sharing\nconstraint on the learnable parameters. In particular, we perform 2D\nconvolution along three orthogonal views of volumetric video data,which learns\nspatial appearance and temporal motion cues respectively. By sharing the\nconvolution kernels of different views, spatial and temporal features are\ncollaboratively learned and thus benefit from each other. The complementary\nfeatures are subsequently fused by a weighted summation whose coefficients are\nlearned end-to-end. Our approach achieves state-of-the-art performance on\nlarge-scale benchmarks and won the 1st place in the Moments in Time Challenge\n2018. Moreover, based on the learned coefficients of different views, we are\nable to quantify the contributions of spatial and temporal features. This\nanalysis sheds light on interpretability of the model and may also guide the\nfuture design of algorithm for video recognition.\n", "versions": [{"version": "v1", "created": "Mon, 4 Mar 2019 12:03:03 GMT"}], "update_date": "2019-03-05", "authors_parsed": [["Li", "Chao", ""], ["Zhong", "Qiaoyong", ""], ["Xie", "Di", ""], ["Pu", "Shiliang", ""]]}, {"id": "1903.01212", "submitter": "Jing Wang", "authors": "Jing Wang, Kuangen Zhang", "title": "Unsupervised Domain Adaptation Learning Algorithm for RGB-D Staircase\n  Recognition", "comments": "7 pages, 5 figures, 17 reference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Detection and recognition of staircase as upstairs, downstairs and negative\n(e.g., ladder) are the fundamental of assisting the visually impaired to travel\nindependently in unfamiliar environments. Previous researches have focused on\nusing massive amounts of RGB-D scene data to train traditional machine learning\n(ML) based models to detect and recognize the staircase. However, the\nperformance of traditional ML techniques is limited by the amount of labeled\nRGB-D staircase data. In this paper, we apply an unsupervised domain adaptation\napproach in deep architectures to transfer knowledge learned from the labeled\nRGB-D stationary staircase dataset to the unlabeled RGB-D escalator dataset. By\nutilizing the domain adaptation method, our feedforward convolutional neural\nnetworks (CNN) based feature extractor with 5 convolution layers can achieve\n100% classification accuracy on testing the labeled stationary staircase data\nand 80.6% classification accuracy on testing the unlabeled escalator data. We\ndemonstrate the success of the approach for classifying staircase on two\ndomains with a limited amount of data. To further demonstrate the effectiveness\nof the approach, we also validate the same CNN model without domain adaptation\nand compare its results with those of our proposed architecture.\n", "versions": [{"version": "v1", "created": "Mon, 4 Mar 2019 12:50:34 GMT"}, {"version": "v2", "created": "Tue, 5 Mar 2019 22:43:24 GMT"}, {"version": "v3", "created": "Mon, 18 Mar 2019 18:31:19 GMT"}, {"version": "v4", "created": "Wed, 20 Mar 2019 22:09:41 GMT"}], "update_date": "2019-03-22", "authors_parsed": [["Wang", "Jing", ""], ["Zhang", "Kuangen", ""]]}, {"id": "1903.01214", "submitter": "Wei-Wen Hsu", "authors": "Wei-Wen Hsu, Chung-Hao Chen, Chang Hoa, Yu-Ling Hou, Xiang Gao, Yun\n  Shao, Xueli Zhang, Jingjing Wang, Tao He, Yanghong Tai", "title": "Understanding the Mechanism of Deep Learning Framework for Lesion\n  Detection in Pathological Images with Breast Cancer", "comments": "v1", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The computer-aided detection (CADe) systems are developed to assist\npathologists in slide assessment, increasing diagnosis efficiency and reducing\nmissing inspections. Many studies have shown such a CADe system with deep\nlearning approaches outperforms the one using conventional methods that rely on\nhand-crafted features based on field-knowledge. However, most developers who\nadopted deep learning models directly focused on the efficacy of outcomes,\nwithout providing comprehensive explanations on why their proposed frameworks\ncan work effectively. In this study, we designed four experiments to verify the\nconsecutive concepts, showing that the deep features learned from pathological\npatches are interpretable by domain knowledge of pathology and enlightening for\nclinical diagnosis in the task of lesion detection. The experimental results\nshow the activation features work as morphological descriptors for specific\ncells or tissues, which agree with the clinical rules in classification. That\nis, the deep learning framework not only detects the distribution of tumor\ncells but also recognizes lymphocytes, collagen fibers, and some other non-cell\nstructural tissues. Most of the characteristics learned by the deep learning\nmodels have summarized the detection rules that can be recognized by the\nexperienced pathologists, whereas there are still some features may not be\nintuitive to domain experts but discriminative in classification for machines.\nThose features are worthy to be further studied in order to find out the\nreasonable correlations to pathological knowledge, from which pathological\nexperts may draw inspirations for exploring new characteristics in diagnosis.\n", "versions": [{"version": "v1", "created": "Mon, 4 Mar 2019 13:01:38 GMT"}], "update_date": "2019-03-05", "authors_parsed": [["Hsu", "Wei-Wen", ""], ["Chen", "Chung-Hao", ""], ["Hoa", "Chang", ""], ["Hou", "Yu-Ling", ""], ["Gao", "Xiang", ""], ["Shao", "Yun", ""], ["Zhang", "Xueli", ""], ["Wang", "Jingjing", ""], ["He", "Tao", ""], ["Tai", "Yanghong", ""]]}, {"id": "1903.01246", "submitter": "Oliver Scheel", "authors": "Oliver Scheel, Naveen Shankar Nagaraja, Loren Schwarz, Nassir Navab,\n  Federico Tombari", "title": "Attention-based Lane Change Prediction", "comments": "To Appear in IEEE International Conference on Robotics and Automation\n  (ICRA) 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Lane change prediction of surrounding vehicles is a key building block of\npath planning. The focus has been on increasing the accuracy of prediction by\nposing it purely as a function estimation problem at the cost of model\nunderstandability. However, the efficacy of any lane change prediction model\ncan be improved when both corner and failure cases are humanly understandable.\nWe propose an attention-based recurrent model to tackle both understandability\nand prediction quality. We also propose metrics which reflect the discomfort\nfelt by the driver. We show encouraging results on a publicly available dataset\nand proprietary fleet data.\n", "versions": [{"version": "v1", "created": "Mon, 4 Mar 2019 13:56:25 GMT"}, {"version": "v2", "created": "Thu, 7 Mar 2019 13:51:17 GMT"}], "update_date": "2019-03-08", "authors_parsed": [["Scheel", "Oliver", ""], ["Nagaraja", "Naveen Shankar", ""], ["Schwarz", "Loren", ""], ["Navab", "Nassir", ""], ["Tombari", "Federico", ""]]}, {"id": "1903.01248", "submitter": "Wenhui Cui", "authors": "Wenhui Cui, Yanlin Liu, Yuxing Li, Menghao Guo, Yiming Li, Xiuli Li,\n  Tianle Wang, Xiangzhu Zeng, Chuyang Ye", "title": "Semi-Supervised Brain Lesion Segmentation with an Adapted Mean Teacher\n  Model", "comments": "Accepted by Information Processing in Medical Imaging 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automated brain lesion segmentation provides valuable information for the\nanalysis and intervention of patients. In particular, methods based on\nconvolutional neural networks (CNNs) have achieved state-of-the-art\nsegmentation performance. However, CNNs usually require a decent amount of\nannotated data, which may be costly and time-consuming to obtain. Since\nunannotated data is generally abundant, it is desirable to use unannotated data\nto improve the segmentation performance for CNNs when limited annotated data is\navailable. In this work, we propose a semi-supervised learning (SSL) approach\nto brain lesion segmentation, where unannotated data is incorporated into the\ntraining of CNNs. We adapt the mean teacher model, which is originally\ndeveloped for SSL-based image classification, for brain lesion segmentation.\nAssuming that the network should produce consistent outputs for similar inputs,\na loss of segmentation consistency is designed and integrated into a\nself-ensembling framework. Specifically, we build a student model and a teacher\nmodel, which share the same CNN architecture for segmentation. The student and\nteacher models are updated alternately. At each step, the student model learns\nfrom the teacher model by minimizing the weighted sum of the segmentation loss\ncomputed from annotated data and the segmentation consistency loss between the\nteacher and student models computed from unannotated data. Then, the teacher\nmodel is updated by combining the updated student model with the historical\ninformation of teacher models using an exponential moving average strategy. For\ndemonstration, the proposed approach was evaluated on ischemic stroke lesion\nsegmentation, where it improves stroke lesion segmentation with the\nincorporation of unannotated data.\n", "versions": [{"version": "v1", "created": "Mon, 4 Mar 2019 14:05:02 GMT"}], "update_date": "2019-03-05", "authors_parsed": [["Cui", "Wenhui", ""], ["Liu", "Yanlin", ""], ["Li", "Yuxing", ""], ["Guo", "Menghao", ""], ["Li", "Yiming", ""], ["Li", "Xiuli", ""], ["Wang", "Tianle", ""], ["Zeng", "Xiangzhu", ""], ["Ye", "Chuyang", ""]]}, {"id": "1903.01292", "submitter": "Piotr Mirowski", "authors": "Piotr Mirowski, Andras Banki-Horvath, Keith Anderson, Denis\n  Teplyashin, Karl Moritz Hermann, Mateusz Malinowski, Matthew Koichi Grimes,\n  Karen Simonyan, Koray Kavukcuoglu, Andrew Zisserman, Raia Hadsell", "title": "The StreetLearn Environment and Dataset", "comments": "13 pages, 6 figures, 4 tables. arXiv admin note: text overlap with\n  arXiv:1804.00168", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Navigation is a rich and well-grounded problem domain that drives progress in\nmany different areas of research: perception, planning, memory, exploration,\nand optimisation in particular. Historically these challenges have been\nseparately considered and solutions built that rely on stationary datasets -\nfor example, recorded trajectories through an environment. These datasets\ncannot be used for decision-making and reinforcement learning, however, and in\ngeneral the perspective of navigation as an interactive learning task, where\nthe actions and behaviours of a learning agent are learned simultaneously with\nthe perception and planning, is relatively unsupported. Thus, existing\nnavigation benchmarks generally rely on static datasets (Geiger et al., 2013;\nKendall et al., 2015) or simulators (Beattie et al., 2016; Shah et al., 2018).\nTo support and validate research in end-to-end navigation, we present\nStreetLearn: an interactive, first-person, partially-observed visual\nenvironment that uses Google Street View for its photographic content and broad\ncoverage, and give performance baselines for a challenging goal-driven\nnavigation task. The environment code, baseline agent code, and the dataset are\navailable at http://streetlearn.cc\n", "versions": [{"version": "v1", "created": "Mon, 4 Mar 2019 16:21:22 GMT"}], "update_date": "2019-03-06", "authors_parsed": [["Mirowski", "Piotr", ""], ["Banki-Horvath", "Andras", ""], ["Anderson", "Keith", ""], ["Teplyashin", "Denis", ""], ["Hermann", "Karl Moritz", ""], ["Malinowski", "Mateusz", ""], ["Grimes", "Matthew Koichi", ""], ["Simonyan", "Karen", ""], ["Kavukcuoglu", "Koray", ""], ["Zisserman", "Andrew", ""], ["Hadsell", "Raia", ""]]}, {"id": "1903.01322", "submitter": "David Castro Pi\\~nol", "authors": "David Castro Pi\\~nol and Enrique Juan Mara\\~n\\'on Reyes", "title": "Automatic Handgun Detection in X-ray Images using Bag of Words Model\n  with Selective Search", "comments": "in Spanish", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Baggage inspection systems using X-ray screening are crucial for security.\nOnly 90% of threat objects are recognized from the X-ray system based in human\ninspection. Manual detection requires high concentration due to the images\ncomplexity and the challenges objects points of view. An algorithm based on Bag\nof Visual Word (BoVW) with Selective Search is proposed in this paper for\nhandguns detection in single energy X-ray images from the public GDXray\ndatabase. This approach is an adaptation of BoVW for X-ray baggage images\ncontext. In order to evaluate the proposed method the algorithm effectiveness\nrecognition was tested on all bounding boxes returned by selective search\nalgorithm in 200 images. The most relevant result is the precision and true\npositive rate (PPV = 80%, TPR= 92%). This approach achieves good performance\nfor handgun recognition. In addition, it is the first time the Selective Search\nlocalization algorithm was tested in baggage X-ray images and showed\npossibilities with Bag of Visual Words.\n", "versions": [{"version": "v1", "created": "Mon, 4 Mar 2019 16:09:17 GMT"}], "update_date": "2019-03-06", "authors_parsed": [["Pi\u00f1ol", "David Castro", ""], ["Reyes", "Enrique Juan Mara\u00f1\u00f3n", ""]]}, {"id": "1903.01330", "submitter": "Fantin Girard", "authors": "Fantin Girard, Conrad Kavalec, Farida Cheriet", "title": "Joint segmentation and classification of retinal arteries/veins from\n  fundus images", "comments": "Preprint accepted in Artificial Intelligence in Medicine", "journal-ref": "Artificial Intelligence in Medicine, Volume 94, 2019, Pages\n  96-109, ISSN 0933-3657", "doi": "10.1016/j.artmed.2019.02.004", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Objective Automatic artery/vein (A/V) segmentation from fundus images is\nrequired to track blood vessel changes occurring with many pathologies\nincluding retinopathy and cardiovascular pathologies. One of the clinical\nmeasures that quantifies vessel changes is the arterio-venous ratio (AVR) which\nrepresents the ratio between artery and vein diameters. This measure\nsignificantly depends on the accuracy of vessel segmentation and classification\ninto arteries and veins. This paper proposes a fast, novel method for semantic\nA/V segmentation combining deep learning and graph propagation.\n  Methods A convolutional neural network (CNN) is proposed to jointly segment\nand classify vessels into arteries and veins. The initial CNN labeling is\npropagated through a graph representation of the retinal vasculature, whose\nnodes are defined as the vessel branches and edges are weighted by the cost of\nlinking pairs of branches. To efficiently propagate the labels, the graph is\nsimplified into its minimum spanning tree.\n  Results The method achieves an accuracy of 94.8% for vessels segmentation.\nThe A/V classification achieves a specificity of 92.9% with a sensitivity of\n93.7% on the CT-DRIVE database compared to the state-of-the-art-specificity and\nsensitivity, both of 91.7%.\n  Conclusion The results show that our method outperforms the leading previous\nworks on a public dataset for A/V classification and is by far the fastest.\n  Significance The proposed global AVR calculated on the whole fundus image\nusing our automatic A/V segmentation method can better track vessel changes\nassociated to diabetic retinopathy than the standard local AVR calculated only\naround the optic disc.\n", "versions": [{"version": "v1", "created": "Mon, 4 Mar 2019 16:17:33 GMT"}], "update_date": "2019-03-05", "authors_parsed": [["Girard", "Fantin", ""], ["Kavalec", "Conrad", ""], ["Cheriet", "Farida", ""]]}, {"id": "1903.01347", "submitter": "Nikolay Sergievskiy", "authors": "Nikolay Sergievskiy, Alexander Ponamarev", "title": "Reduced Focal Loss: 1st Place Solution to xView object detection in\n  Satellite Imagery", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes our approach to the DIUx xView 2018 Detection Challenge\n[1]. This challenge focuses on a new satellite imagery dataset. The dataset\ncontains 60 object classes that are highly imbalanced. Due to the imbalanced\nnature of the dataset, the training process becomes significantly more\nchallenging. To address this problem, we introduce a novel Reduced Focal Loss\nfunction, which brought us 1st place in the DIUx xView 2018 Detection\nChallenge.\n", "versions": [{"version": "v1", "created": "Mon, 4 Mar 2019 16:36:13 GMT"}, {"version": "v2", "created": "Thu, 25 Apr 2019 20:44:17 GMT"}], "update_date": "2019-04-29", "authors_parsed": [["Sergievskiy", "Nikolay", ""], ["Ponamarev", "Alexander", ""]]}, {"id": "1903.01392", "submitter": "Sherif Abdulatif", "authors": "Karim Armanious, Sherif Abdulatif, Fady Aziz, Urs Schneider, Bin Yang", "title": "An Adversarial Super-Resolution Remedy for Radar Design Trade-offs", "comments": "Accepted in EUSIPCO 2019, 5 pages", "journal-ref": null, "doi": "10.23919/EUSIPCO.2019.8902510", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Radar is of vital importance in many fields, such as autonomous driving,\nsafety and surveillance applications. However, it suffers from stringent\nconstraints on its design parametrization leading to multiple trade-offs. For\nexample, the bandwidth in FMCW radars is inversely proportional with both the\nmaximum unambiguous range and range resolution. In this work, we introduce a\nnew method for circumventing radar design trade-offs. We propose the use of\nrecent advances in computer vision, more specifically generative adversarial\nnetworks (GANs), to enhance low-resolution radar acquisitions into higher\nresolution counterparts while maintaining the advantages of the low-resolution\nparametrization. The capability of the proposed method was evaluated on the\nvelocity resolution and range-azimuth trade-offs in micro-Doppler signatures\nand FMCW uniform linear array (ULA) radars, respectively.\n", "versions": [{"version": "v1", "created": "Mon, 4 Mar 2019 17:41:26 GMT"}, {"version": "v2", "created": "Thu, 20 Jun 2019 16:23:55 GMT"}], "update_date": "2019-11-26", "authors_parsed": [["Armanious", "Karim", ""], ["Abdulatif", "Sherif", ""], ["Aziz", "Fady", ""], ["Schneider", "Urs", ""], ["Yang", "Bin", ""]]}, {"id": "1903.01434", "submitter": "Manoj Kumar", "authors": "Manoj Kumar, Mohammad Babaeizadeh, Dumitru Erhan, Chelsea Finn, Sergey\n  Levine, Laurent Dinh, Durk Kingma", "title": "VideoFlow: A Conditional Flow-Based Model for Stochastic Video\n  Generation", "comments": "ICLR 2020 Camera-Ready. Previous title: VideoFlow: A Flow-Based\n  Generative Model for Video", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative models that can model and predict sequences of future events can,\nin principle, learn to capture complex real-world phenomena, such as physical\ninteractions. However, a central challenge in video prediction is that the\nfuture is highly uncertain: a sequence of past observations of events can imply\nmany possible futures. Although a number of recent works have studied\nprobabilistic models that can represent uncertain futures, such models are\neither extremely expensive computationally as in the case of pixel-level\nautoregressive models, or do not directly optimize the likelihood of the data.\nTo our knowledge, our work is the first to propose multi-frame video prediction\nwith normalizing flows, which allows for direct optimization of the data\nlikelihood, and produces high-quality stochastic predictions. We describe an\napproach for modeling the latent space dynamics, and demonstrate that\nflow-based generative models offer a viable and competitive approach to\ngenerative modelling of video.\n", "versions": [{"version": "v1", "created": "Mon, 4 Mar 2019 18:55:45 GMT"}, {"version": "v2", "created": "Mon, 10 Jun 2019 17:40:04 GMT"}, {"version": "v3", "created": "Wed, 12 Feb 2020 16:55:25 GMT"}], "update_date": "2020-02-13", "authors_parsed": [["Kumar", "Manoj", ""], ["Babaeizadeh", "Mohammad", ""], ["Erhan", "Dumitru", ""], ["Finn", "Chelsea", ""], ["Levine", "Sergey", ""], ["Dinh", "Laurent", ""], ["Kingma", "Durk", ""]]}, {"id": "1903.01489", "submitter": "Marcella Cornia", "authors": "Stefano Pini, Marcella Cornia, Federico Bolelli, Lorenzo Baraldi, Rita\n  Cucchiara", "title": "M-VAD Names: a Dataset for Video Captioning with Naming", "comments": "Source Code: https://github.com/aimagelab/mvad-names-dataset - Video\n  Demo: https://youtu.be/dOvtAXbOOH4", "journal-ref": "Multimedia Tools and Applications (2018)", "doi": "10.1007/s11042-018-7040-z", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current movie captioning architectures are not capable of mentioning\ncharacters with their proper name, replacing them with a generic \"someone\" tag.\nThe lack of movie description datasets with characters' visual annotations\nsurely plays a relevant role in this shortage. Recently, we proposed to extend\nthe M-VAD dataset by introducing such information. In this paper, we present an\nimproved version of the dataset, namely M-VAD Names, and its semi-automatic\nannotation procedure. The resulting dataset contains 63k visual tracks and 34k\ntextual mentions, all associated with character identities. To showcase the\nfeatures of the dataset and quantify the complexity of the naming task, we\ninvestigate multimodal architectures to replace the \"someone\" tags with proper\ncharacter names in existing video captions. The evaluation is further extended\nby testing this application on videos outside of the M-VAD Names dataset.\n", "versions": [{"version": "v1", "created": "Mon, 4 Mar 2019 19:05:27 GMT"}], "update_date": "2019-03-06", "authors_parsed": [["Pini", "Stefano", ""], ["Cornia", "Marcella", ""], ["Bolelli", "Federico", ""], ["Baraldi", "Lorenzo", ""], ["Cucchiara", "Rita", ""]]}, {"id": "1903.01505", "submitter": "Ke Yan", "authors": "Ke Yan, Yifan Peng, Zhiyong Lu, and Ronald M. Summers", "title": "Fine-grained lesion annotation in CT images with knowledge mined from\n  radiology reports", "comments": "4 pages, IEEE International Symposium on Biomedical Imaging (ISBI)\n  2019, oral presentation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In radiologists' routine work, one major task is to read a medical image,\ne.g., a CT scan, find significant lesions, and write sentences in the radiology\nreport to describe them. In this paper, we study the lesion description or\nannotation problem as an important step of computer-aided diagnosis (CAD).\nGiven a lesion image, our aim is to predict multiple relevant labels, such as\nthe lesion's body part, type, and attributes. To address this problem, we\ndefine a set of 145 labels based on RadLex to describe a large variety of\nlesions in the DeepLesion dataset. We directly mine training labels from the\nlesion's corresponding sentence in the radiology report, which requires minimal\nmanual effort and is easily generalizable to large data and label sets. A\nmulti-label convolutional neural network is then proposed for images with\nmulti-scale structure and a noise-robust loss. Quantitative and qualitative\nexperiments demonstrate the effectiveness of the framework. The average area\nunder ROC curve on 1,872 test lesions is 0.9083.\n", "versions": [{"version": "v1", "created": "Mon, 4 Mar 2019 19:42:50 GMT"}, {"version": "v2", "created": "Tue, 26 Mar 2019 19:33:49 GMT"}], "update_date": "2019-03-28", "authors_parsed": [["Yan", "Ke", ""], ["Peng", "Yifan", ""], ["Lu", "Zhiyong", ""], ["Summers", "Ronald M.", ""]]}, {"id": "1903.01522", "submitter": "Mohammad Farhadi Bajestani", "authors": "Mohammad Farhadi, Yezhou Yang", "title": "TKD: Temporal Knowledge Distillation for Active Perception", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks based methods have been proved to achieve outstanding\nperformance on object detection and classification tasks. Despite significant\nperformance improvement, due to the deep structures, they still require\nprohibitive runtime to process images and maintain the highest possible\nperformance for real-time applications. Observing the phenomenon that human\nvision system (HVS) relies heavily on the temporal dependencies among frames\nfrom the visual input to conduct recognition efficiently, we propose a novel\nframework dubbed as TKD: temporal knowledge distillation. This framework\ndistills the temporal knowledge from a heavy neural networks based model over\nselected video frames (the perception of the moments) to a light-weight model.\nTo enable the distillation, we put forward two novel procedures: 1) an\nLong-short Term Memory (LSTM) based key frame selection method; and 2) a novel\nteacher-bounded loss design. To validate, we conduct comprehensive empirical\nevaluations using different object detection methods over multiple datasets\nincluding Youtube-Objects and Hollywood scene dataset. Our results show\nconsistent improvement in accuracy-speed trad-offs for object detection over\nthe frames of the dynamic scene, compare to other modern object recognition\nmethods.\n", "versions": [{"version": "v1", "created": "Mon, 4 Mar 2019 20:15:56 GMT"}, {"version": "v2", "created": "Mon, 6 Jan 2020 22:34:42 GMT"}], "update_date": "2020-01-08", "authors_parsed": [["Farhadi", "Mohammad", ""], ["Yang", "Yezhou", ""]]}, {"id": "1903.01530", "submitter": "S\\'ebastien de Blois", "authors": "S\\'ebastien de Blois, Ihsen Hedhli, Christian Gagn\\'e", "title": "Learning of Image Dehazing Models for Segmentation Tasks", "comments": "Accepted in EUSIPCO 2019", "journal-ref": null, "doi": "10.23919/EUSIPCO.2019.8903046", "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To evaluate their performance, existing dehazing approaches generally rely on\ndistance measures between the generated image and its corresponding ground\ntruth. Despite its ability to produce visually good images, using pixel-based\nor even perceptual metrics do not guarantee, in general, that the produced\nimage is fit for being used as input for low-level computer vision tasks such\nas segmentation. To overcome this weakness, we are proposing a novel end-to-end\napproach for image dehazing, fit for being used as input to an image\nsegmentation procedure, while maintaining the visual quality of the generated\nimages. Inspired by the success of Generative Adversarial Networks (GAN), we\npropose to optimize the generator by introducing a discriminator network and a\nloss function that evaluates segmentation quality of dehazed images. In\naddition, we make use of a supplementary loss function that verifies that the\nvisual and the perceptual quality of the generated image are preserved in hazy\nconditions. Results obtained using the proposed technique are appealing, with a\nfavorable comparison to state-of-the-art approaches when considering the\nperformance of segmentation algorithms on the hazy images.\n", "versions": [{"version": "v1", "created": "Mon, 4 Mar 2019 20:41:05 GMT"}, {"version": "v2", "created": "Sat, 22 Jun 2019 15:03:46 GMT"}], "update_date": "2020-02-10", "authors_parsed": [["de Blois", "S\u00e9bastien", ""], ["Hedhli", "Ihsen", ""], ["Gagn\u00e9", "Christian", ""]]}, {"id": "1903.01534", "submitter": "Changhao Chen", "authors": "Changhao Chen, Stefano Rosa, Yishu Miao, Chris Xiaoxuan Lu, Wei Wu,\n  Andrew Markham, Niki Trigoni", "title": "Selective Sensor Fusion for Neural Visual-Inertial Odometry", "comments": "Accepted by CVPR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG cs.RO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Deep learning approaches for Visual-Inertial Odometry (VIO) have proven\nsuccessful, but they rarely focus on incorporating robust fusion strategies for\ndealing with imperfect input sensory data. We propose a novel end-to-end\nselective sensor fusion framework for monocular VIO, which fuses monocular\nimages and inertial measurements in order to estimate the trajectory whilst\nimproving robustness to real-life issues, such as missing and corrupted data or\nbad sensor synchronization. In particular, we propose two fusion modalities\nbased on different masking strategies: deterministic soft fusion and stochastic\nhard fusion, and we compare with previously proposed direct fusion baselines.\nDuring testing, the network is able to selectively process the features of the\navailable sensor modalities and produce a trajectory at scale. We present a\nthorough investigation on the performances on three public autonomous driving,\nMicro Aerial Vehicle (MAV) and hand-held VIO datasets. The results demonstrate\nthe effectiveness of the fusion strategies, which offer better performances\ncompared to direct fusion, particularly in presence of corrupted data. In\naddition, we study the interpretability of the fusion networks by visualising\nthe masking layers in different scenarios and with varying data corruption,\nrevealing interesting correlations between the fusion networks and imperfect\nsensory input data.\n", "versions": [{"version": "v1", "created": "Mon, 4 Mar 2019 20:51:37 GMT"}], "update_date": "2019-03-06", "authors_parsed": [["Chen", "Changhao", ""], ["Rosa", "Stefano", ""], ["Miao", "Yishu", ""], ["Lu", "Chris Xiaoxuan", ""], ["Wu", "Wei", ""], ["Markham", "Andrew", ""], ["Trigoni", "Niki", ""]]}, {"id": "1903.01545", "submitter": "Svebor Karaman", "authors": "Svebor Karaman, Xudong Lin, Xuefeng Hu, Shih-Fu Chang", "title": "Unsupervised Rank-Preserving Hashing for Large-Scale Image Retrieval", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an unsupervised hashing method which aims to produce binary codes\nthat preserve the ranking induced by a real-valued representation. Such compact\nhash codes enable the complete elimination of real-valued feature storage and\nallow for significant reduction of the computation complexity and storage cost\nof large-scale image retrieval applications. Specifically, we learn a neural\nnetwork-based model, which transforms the input representation into a binary\nrepresentation. We formalize the training objective of the network in an\nintuitive and effective way, considering each training sample as a query and\naiming to obtain the same retrieval results using the produced hash codes as\nthose obtained with the original features. This training formulation directly\noptimizes the hashing model for the target usage of the hash codes it produces.\nWe further explore the addition of a decoder trained to obtain an approximated\nreconstruction of the original features. At test time, we retrieved the most\npromising database samples with an efficient graph-based search procedure using\nonly our hash codes and perform re-ranking using the reconstructed features,\nthus without needing to access the original features at all. Experiments\nconducted on multiple publicly available large-scale datasets show that our\nmethod consistently outperforms all compared state-of-the-art unsupervised\nhashing methods and that the reconstruction procedure can effectively boost the\nsearch accuracy with a minimal constant additional cost.\n", "versions": [{"version": "v1", "created": "Mon, 4 Mar 2019 21:24:26 GMT"}], "update_date": "2019-03-06", "authors_parsed": [["Karaman", "Svebor", ""], ["Lin", "Xudong", ""], ["Hu", "Xuefeng", ""], ["Chang", "Shih-Fu", ""]]}, {"id": "1903.01568", "submitter": "Srikanth Malla", "authors": "Abhishek Patil, Srikanth Malla, Haiming Gang and Yi-Ting Chen", "title": "The H3D Dataset for Full-Surround 3D Multi-Object Detection and Tracking\n  in Crowded Urban Scenes", "comments": "The dataset is available at https://usa.honda-ri.com/H3D", "journal-ref": "IEEE International Conference on Robotics and Automation (ICRA),\n  2019", "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  3D multi-object detection and tracking are crucial for traffic scene\nunderstanding. However, the community pays less attention to these areas due to\nthe lack of a standardized benchmark dataset to advance the field. Moreover,\nexisting datasets (e.g., KITTI) do not provide sufficient data and labels to\ntackle challenging scenes where highly interactive and occluded traffic\nparticipants are present. To address the issues, we present the Honda Research\nInstitute 3D Dataset (H3D), a large-scale full-surround 3D multi-object\ndetection and tracking dataset collected using a 3D LiDAR scanner. H3D\ncomprises of 160 crowded and highly interactive traffic scenes with a total of\n1 million labeled instances in 27,721 frames. With unique dataset size, rich\nannotations, and complex scenes, H3D is gathered to stimulate research on\nfull-surround 3D multi-object detection and tracking. To effectively and\nefficiently annotate a large-scale 3D point cloud dataset, we propose a\nlabeling methodology to speed up the overall annotation cycle. A standardized\nbenchmark is created to evaluate full-surround 3D multi-object detection and\ntracking algorithms. 3D object detection and tracking algorithms are trained\nand tested on H3D. Finally, sources of errors are discussed for the development\nof future algorithms.\n", "versions": [{"version": "v1", "created": "Mon, 4 Mar 2019 22:17:46 GMT"}], "update_date": "2019-03-07", "authors_parsed": [["Patil", "Abhishek", ""], ["Malla", "Srikanth", ""], ["Gang", "Haiming", ""], ["Chen", "Yi-Ting", ""]]}, {"id": "1903.01581", "submitter": "Prithviraj Dhar", "authors": "Prithviraj Dhar, Carlos D. Castillo, Rama Chellappa", "title": "On measuring the iconicity of a face", "comments": "Accepted to WACV 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For a given identity in a face dataset, there are certain iconic images which\nare more representative of the subject than others. In this paper, we explore\nthe problem of computing the iconicity of a face. The premise of the proposed\napproach is as follows: For an identity containing a mixture of iconic and non\niconic images, if a given face cannot be successfully matched with any other\nface of the same identity, then the iconicity of the face image is low. Using\nthis information, we train a Siamese Multi-Layer Perceptron network, such that\neach of its twins predict iconicity scores of the image feature pair, fed in as\ninput. We observe the variation of the obtained scores with respect to\ncovariates such as blur, yaw, pitch, roll and occlusion to demonstrate that\nthey effectively predict the quality of the image and compare it with other\nexisting metrics. Furthermore, we use these scores to weight features for\ntemplate-based face verification and compare it with media averaging of\nfeatures.\n", "versions": [{"version": "v1", "created": "Mon, 4 Mar 2019 23:16:36 GMT"}], "update_date": "2019-03-06", "authors_parsed": [["Dhar", "Prithviraj", ""], ["Castillo", "Carlos D.", ""], ["Chellappa", "Rama", ""]]}, {"id": "1903.01594", "submitter": "Boyu Lu", "authors": "Boyu Lu, Jun-Cheng Chen and Rama Chellappa", "title": "Unsupervised Domain-Specific Deblurring via Disentangled Representations", "comments": "Accepted by CVPR 2019. Code is released at:\n  https://github.com/ustclby/Unsupervised-Domain-Specific-Deblurring/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image deblurring aims to restore the latent sharp images from the\ncorresponding blurred ones. In this paper, we present an unsupervised method\nfor domain-specific single-image deblurring based on disentangled\nrepresentations. The disentanglement is achieved by splitting the content and\nblur features in a blurred image using content encoders and blur encoders. We\nenforce a KL divergence loss to regularize the distribution range of extracted\nblur attributes such that little content information is contained. Meanwhile,\nto handle the unpaired training data, a blurring branch and the\ncycle-consistency loss are added to guarantee that the content structures of\nthe deblurred results match the original images. We also add an adversarial\nloss on deblurred results to generate visually realistic images and a\nperceptual loss to further mitigate the artifacts. We perform extensive\nexperiments on the tasks of face and text deblurring using both synthetic\ndatasets and real images, and achieve improved results compared to recent\nstate-of-the-art deblurring methods.\n", "versions": [{"version": "v1", "created": "Tue, 5 Mar 2019 00:00:27 GMT"}, {"version": "v2", "created": "Mon, 5 Aug 2019 08:13:18 GMT"}], "update_date": "2019-08-06", "authors_parsed": [["Lu", "Boyu", ""], ["Chen", "Jun-Cheng", ""], ["Chellappa", "Rama", ""]]}, {"id": "1903.01602", "submitter": "Chih-Yao Ma", "authors": "Chih-Yao Ma, Zuxuan Wu, Ghassan AlRegib, Caiming Xiong, Zsolt Kira", "title": "The Regretful Agent: Heuristic-Aided Navigation through Progress\n  Estimation", "comments": "CVPR 2019 (Oral), our code is available at\n  https://github.com/chihyaoma/regretful-agent", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As deep learning continues to make progress for challenging perception tasks,\nthere is increased interest in combining vision, language, and decision-making.\nSpecifically, the Vision and Language Navigation (VLN) task involves navigating\nto a goal purely from language instructions and visual information without\nexplicit knowledge of the goal. Recent successful approaches have made in-roads\nin achieving good success rates for this task but rely on beam search, which\nthoroughly explores a large number of trajectories and is unrealistic for\napplications such as robotics. In this paper, inspired by the intuition of\nviewing the problem as search on a navigation graph, we propose to use a\nprogress monitor developed in prior work as a learnable heuristic for search.\nWe then propose two modules incorporated into an end-to-end architecture: 1) A\nlearned mechanism to perform backtracking, which decides whether to continue\nmoving forward or roll back to a previous state (Regret Module) and 2) A\nmechanism to help the agent decide which direction to go next by showing\ndirections that are visited and their associated progress estimate (Progress\nMarker). Combined, the proposed approach significantly outperforms current\nstate-of-the-art methods using greedy action selection, with 5% absolute\nimprovement on the test server in success rates, and more importantly 8% on\nsuccess rates normalized by the path length. Our code is available at\nhttps://github.com/chihyaoma/regretful-agent .\n", "versions": [{"version": "v1", "created": "Tue, 5 Mar 2019 00:17:12 GMT"}], "update_date": "2019-03-06", "authors_parsed": [["Ma", "Chih-Yao", ""], ["Wu", "Zuxuan", ""], ["AlRegib", "Ghassan", ""], ["Xiong", "Caiming", ""], ["Kira", "Zsolt", ""]]}, {"id": "1903.01611", "submitter": "Jonathan Frankle", "authors": "Jonathan Frankle, Gintare Karolina Dziugaite, Daniel M. Roy, Michael\n  Carbin", "title": "Stabilizing the Lottery Ticket Hypothesis", "comments": "This article has been subsumed by \"Linear Mode Connectivity and the\n  Lottery Ticket Hypothesis\" (arXiv:1912.05671, ICML 2020). Please read/cite\n  that article instead", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pruning is a well-established technique for removing unnecessary structure\nfrom neural networks after training to improve the performance of inference.\nSeveral recent results have explored the possibility of pruning at\ninitialization time to provide similar benefits during training. In particular,\nthe \"lottery ticket hypothesis\" conjectures that typical neural networks\ncontain small subnetworks that can train to similar accuracy in a commensurate\nnumber of steps. The evidence for this claim is that a procedure based on\niterative magnitude pruning (IMP) reliably finds such subnetworks retroactively\non small vision tasks. However, IMP fails on deeper networks, and proposed\nmethods to prune before training or train pruned networks encounter similar\nscaling limitations. In this paper, we argue that these efforts have struggled\non deeper networks because they have focused on pruning precisely at\ninitialization. We modify IMP to search for subnetworks that could have been\nobtained by pruning early in training (0.1% to 7% through) rather than at\niteration 0. With this change, it finds small subnetworks of deeper networks\n(e.g., 80% sparsity on Resnet-50) that can complete the training process to\nmatch the accuracy of the original network on more challenging tasks (e.g.,\nImageNet). In situations where IMP fails at iteration 0, the accuracy benefits\nof delaying pruning accrue rapidly over the earliest iterations of training. To\nexplain these behaviors, we study subnetwork \"stability,\" finding that - as\naccuracy improves in this fashion - IMP subnetworks train to parameters closer\nto those of the full network and do so with improved consistency in the face of\ngradient noise. These results offer new insights into the opportunity to prune\nlarge-scale networks early in training and the behaviors underlying the lottery\nticket hypothesis\n", "versions": [{"version": "v1", "created": "Tue, 5 Mar 2019 00:52:12 GMT"}, {"version": "v2", "created": "Fri, 7 Jun 2019 23:40:16 GMT"}, {"version": "v3", "created": "Mon, 20 Jul 2020 16:50:33 GMT"}], "update_date": "2020-09-29", "authors_parsed": [["Frankle", "Jonathan", ""], ["Dziugaite", "Gintare Karolina", ""], ["Roy", "Daniel M.", ""], ["Carbin", "Michael", ""]]}, {"id": "1903.01612", "submitter": "Abhimanyu Dubey", "authors": "Abhimanyu Dubey, Laurens van der Maaten, Zeki Yalniz, Yixuan Li, Dhruv\n  Mahajan", "title": "Defense Against Adversarial Images using Web-Scale Nearest-Neighbor\n  Search", "comments": "CVPR 2019 Oral presentation; camera-ready with supplement (14 pages).\n  v1 updated from error in Table 2, row 10", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A plethora of recent work has shown that convolutional networks are not\nrobust to adversarial images: images that are created by perturbing a sample\nfrom the data distribution as to maximize the loss on the perturbed example. In\nthis work, we hypothesize that adversarial perturbations move the image away\nfrom the image manifold in the sense that there exists no physical process that\ncould have produced the adversarial image. This hypothesis suggests that a\nsuccessful defense mechanism against adversarial images should aim to project\nthe images back onto the image manifold. We study such defense mechanisms,\nwhich approximate the projection onto the unknown image manifold by a\nnearest-neighbor search against a web-scale image database containing tens of\nbillions of images. Empirical evaluations of this defense strategy on ImageNet\nsuggest that it is very effective in attack settings in which the adversary\ndoes not have access to the image database. We also propose two novel attack\nmethods to break nearest-neighbor defenses, and demonstrate conditions under\nwhich nearest-neighbor defense fails. We perform a series of ablation\nexperiments, which suggest that there is a trade-off between robustness and\naccuracy in our defenses, that a large image database (with hundreds of\nmillions of images) is crucial to get good performance, and that careful\nconstruction the image database is important to be robust against attacks\ntailored to circumvent our defenses.\n", "versions": [{"version": "v1", "created": "Tue, 5 Mar 2019 00:53:56 GMT"}, {"version": "v2", "created": "Sat, 4 May 2019 20:34:14 GMT"}], "update_date": "2019-05-07", "authors_parsed": [["Dubey", "Abhimanyu", ""], ["van der Maaten", "Laurens", ""], ["Yalniz", "Zeki", ""], ["Li", "Yixuan", ""], ["Mahajan", "Dhruv", ""]]}, {"id": "1903.01648", "submitter": "Tianyi Li", "authors": "Tianyi Li, Mai Xu, Ren Yang and Xiaoming Tao", "title": "A DenseNet Based Approach for Multi-Frame In-Loop Filter in HEVC", "comments": "10 pages, 4 figures. Accepted by Data Compression Conference 2019", "journal-ref": "Data Compression Conference 2019", "doi": "10.1109/TIP.2019.2921877", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High efficiency video coding (HEVC) has brought outperforming efficiency for\nvideo compression. To reduce the compression artifacts of HEVC, we propose a\nDenseNet based approach as the in-loop filter of HEVC, which leverages multiple\nadjacent frames to enhance the quality of each encoded frame. Specifically, the\nhigher-quality frames are found by a reference frame selector (RFS). Then, a\ndeep neural network for multi-frame in-loop filter (named MIF-Net) is developed\nto enhance the quality of each encoded frame by utilizing the spatial\ninformation of this frame and the temporal information of its neighboring\nhigher-quality frames. The MIF-Net is built on the recently developed DenseNet,\nbenefiting from the improved generalization capacity and computational\nefficiency. Finally, experimental results verify the effectiveness of our\nmulti-frame in-loop filter, outperforming the HM baseline and other\nstate-of-the-art approaches.\n", "versions": [{"version": "v1", "created": "Tue, 5 Mar 2019 03:43:10 GMT"}], "update_date": "2019-10-02", "authors_parsed": [["Li", "Tianyi", ""], ["Xu", "Mai", ""], ["Yang", "Ren", ""], ["Tao", "Xiaoming", ""]]}, {"id": "1903.01656", "submitter": "Shehryar Khattak", "authors": "Shehryar Khattak, Christos Papachristos, Kostas Alexis", "title": "Visual-Thermal Landmarks and Inertial Fusion for Navigation in Degraded\n  Visual Environments", "comments": "9 pages, 11 figures, Accepted at IEEE Aerospace Conference (AeroConf)\n  2019", "journal-ref": null, "doi": "10.1109/AERO.2019.8741787", "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With an ever-widening domain of aerial robotic applications, including many\nmission critical tasks such as disaster response operations, search and rescue\nmissions and infrastructure inspections taking place in GPS-denied\nenvironments, the need for reliable autonomous operation of aerial robots has\nbecome crucial. Operating in GPS-denied areas aerial robots rely on a multitude\nof sensors to localize and navigate. Visible spectrum cameras are the most\ncommonly used sensors due to their low cost and weight. However, in\nenvironments that are visually-degraded such as in conditions of poor\nillumination, low texture, or presence of obscurants including fog, smoke and\ndust, the reliability of visible light cameras deteriorates significantly.\nNevertheless, maintaining reliable robot navigation in such conditions is\nessential. In contrast to visible light cameras, thermal cameras offer\nvisibility in the infrared spectrum and can be used in a complementary manner\nwith visible spectrum cameras for robot localization and navigation tasks,\nwithout paying the significant weight and power penalty typically associated\nwith carrying other sensors. Exploiting this fact, in this work we present a\nmulti-sensor fusion algorithm for reliable odometry estimation in GPS-denied\nand degraded visual environments. The proposed method utilizes information from\nboth the visible and thermal spectra for landmark selection and prioritizes\nfeature extraction from informative image regions based on a metric over\nspatial entropy. Furthermore, inertial sensing cues are integrated to improve\nthe robustness of the odometry estimation process. To verify our solution, a\nset of challenging experiments were conducted inside a) an obscurant filed\nmachine shop-like industrial environment, as well as b) a dark subterranean\nmine in the presence of heavy airborne dust.\n", "versions": [{"version": "v1", "created": "Tue, 5 Mar 2019 04:08:14 GMT"}], "update_date": "2019-07-02", "authors_parsed": [["Khattak", "Shehryar", ""], ["Papachristos", "Christos", ""], ["Alexis", "Kostas", ""]]}, {"id": "1903.01659", "submitter": "Shehryar Khattak", "authors": "Shehryar Khattak, Christos Papachristos, Kostas Alexis", "title": "Vision-Depth Landmarks and Inertial Fusion for Navigation in Degraded\n  Visual Environments", "comments": "11 pages, 6 figures, Published in International Symposium on Visual\n  Computing (ISVC) 2018", "journal-ref": null, "doi": "10.1007/978-3-030-03801-4_46", "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a method for tight fusion of visual, depth and inertial\ndata in order to extend robotic capabilities for navigation in GPS-denied,\npoorly illuminated, and texture-less environments. Visual and depth information\nare fused at the feature detection and descriptor extraction levels to augment\none sensing modality with the other. These multimodal features are then further\nintegrated with inertial sensor cues using an extended Kalman filter to\nestimate the robot pose, sensor bias terms, and landmark positions\nsimultaneously as part of the filter state. As demonstrated through a set of\nhand-held and Micro Aerial Vehicle experiments, the proposed algorithm is shown\nto perform reliably in challenging visually-degraded environments using RGB-D\ninformation from a lightweight and low-cost sensor and data from an IMU.\n", "versions": [{"version": "v1", "created": "Tue, 5 Mar 2019 04:25:26 GMT"}], "update_date": "2019-03-06", "authors_parsed": [["Khattak", "Shehryar", ""], ["Papachristos", "Christos", ""], ["Alexis", "Kostas", ""]]}, {"id": "1903.01671", "submitter": "Hideki Tamura", "authors": "Hideki Tamura, Konrad E. Prokott, Roland W. Fleming", "title": "Distinguishing mirror from glass: A 'big data' approach to material\n  perception", "comments": "40 pages, 5 figures, 7 supplement figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visually identifying materials is crucial for many tasks, yet material\nperception remains poorly understood. Distinguishing mirror from glass is\nparticularly challenging as both materials derive their appearance from their\nsurroundings, yet we rarely experience difficulties telling them apart. Here we\ntook a 'big data' approach to uncovering the underlying visual cues and\nprocesses, leveraging recent advances in neural network models of vision. We\ntrained thousands of convolutional neural networks on >750,000 simulated mirror\nand glass objects, and compared their performance with human judgments, as well\nas alternative classifiers based on 'hand-engineered' image features. For\nrandomly chosen images, all classifiers and humans performed with high\naccuracy, and therefore correlated highly with one another. To tease the models\napart, we then painstakingly assembled a diagnostic image set for which humans\nmake highly systematic errors, allowing us to decouple accuracy from human-like\nperformance. A large-scale, systematic search through feedforward neural\narchitectures revealed that relatively shallow networks predicted human\njudgments better than any other models. However, surprisingly, no network\ncorrelated better than 0.6 with humans (below inter-human correlations). Thus,\nalthough the model sets new standards for simulating human vision in a\nchallenging material perception task, the results cast doubt on recent claims\nthat such architectures are generally good models of human vision.\n", "versions": [{"version": "v1", "created": "Tue, 5 Mar 2019 05:05:05 GMT"}], "update_date": "2019-03-06", "authors_parsed": [["Tamura", "Hideki", ""], ["Prokott", "Konrad E.", ""], ["Fleming", "Roland W.", ""]]}, {"id": "1903.01688", "submitter": "Rodolfo Migon Favaretto", "authors": "Rodolfo Migon Favaretto and Leandro Dihl and Soraia Raupp Musse and\n  Felipe Vilanova and Angelo Brandelli Costa", "title": "Using Big Five Personality Model to Detect Cultural Aspects in Crowds", "comments": null, "journal-ref": null, "doi": "10.1109/SIBGRAPI.2017.36", "report-no": null, "categories": "cs.CV eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The use of information technology in the study of human behavior is a subject\nof great scientific interest. Cultural and personality aspects are factors that\ninfluence how people interact with one another in a crowd. This paper presents\na methodology to detect cultural characteristics of crowds in video sequences.\nBased on filmed sequences, pedestrians are detected, tracked and characterized.\nSuch information is then used to find out cultural differences in those videos,\nbased on the Big-five personality model. Regarding cultural differences of each\ncountry, results indicate that this model generates coherent information when\ncompared to data provided in literature.\n", "versions": [{"version": "v1", "created": "Tue, 5 Mar 2019 06:04:11 GMT"}], "update_date": "2019-03-06", "authors_parsed": [["Favaretto", "Rodolfo Migon", ""], ["Dihl", "Leandro", ""], ["Musse", "Soraia Raupp", ""], ["Vilanova", "Felipe", ""], ["Costa", "Angelo Brandelli", ""]]}, {"id": "1903.01695", "submitter": "Hao Jiang", "authors": "Hao Jiang, Quanzeng You", "title": "Real-time Multiple People Hand Localization in 4D Point Clouds", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose novel real-time algorithm to localize hands and find their\nassociations with multiple people in the cluttered 4D volumetric data (dynamic\n3D volumes). Different from the traditional multiple view approaches, which\nfind key points in 2D and then triangulate to recover the 3D locations, our\nmethod directly processes the dynamic 3D data that involve both clutter and\ncrowd. The volumetric representation is more desirable than the partial\nobservations from different view points and enables more robust and accurate\nresults. However, due to the large amount of data in the volumetric\nrepresentation brute force 3D schemes are slow. In this paper, we propose novel\nreal-time methods to tackle the problem to achieve both higher accuracy and\nfaster speed than previous approaches. Our method detects the 3D bounding box\nof each subject and localizes the hands of each person. We develop new 2D\nfeatures for fast candidate proposals and optimize the trajectory linking using\na new max-covering bipartite matching formulation, which is critical for robust\nperformance. We propose a novel decomposition method to reduce the key point\nlocalization in each person 3D volume to a sequence of efficient 2D problems.\nOur experiments show that the proposed method is faster than different\ncompeting methods and it gives almost half the localization error.\n", "versions": [{"version": "v1", "created": "Tue, 5 Mar 2019 06:46:35 GMT"}], "update_date": "2019-03-06", "authors_parsed": [["Jiang", "Hao", ""], ["You", "Quanzeng", ""]]}, {"id": "1903.01700", "submitter": "Xiao Song", "authors": "Xiao Song, Xu Zhao, Liangji Fang, Hanwen Hu", "title": "EdgeStereo: An Effective Multi-Task Learning Network for Stereo Matching\n  and Edge Detection", "comments": "Accepted for publication in International Journal of Computer Vision\n  (IJCV)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, leveraging on the development of end-to-end convolutional neural\nnetworks (CNNs), deep stereo matching networks have achieved remarkable\nperformance far exceeding traditional approaches. However, state-of-the-art\nstereo frameworks still have difficulties at finding correct correspondences in\ntexture-less regions, detailed structures, small objects and near boundaries,\nwhich could be alleviated by geometric clues such as edge contours and\ncorresponding constraints. To improve the quality of disparity estimates in\nthese challenging areas, we propose an effective multi-task learning network,\nEdgeStereo, composed of a disparity estimation branch and an edge detection\nbranch, which enables end-to-end predictions of both disparity map and edge\nmap. To effectively incorporate edge cues, we propose the edge-aware smoothness\nloss and edge feature embedding for inter-task interactions. It is demonstrated\nthat based on our unified model, edge detection task and stereo matching task\ncan promote each other. In addition, we design a compact module called residual\npyramid to replace the commonly-used multi-stage cascaded structures or 3-D\nconvolution based regularization modules in current stereo matching networks.\nBy the time of the paper submission, EdgeStereo achieves state-of-art\nperformance on the FlyingThings3D dataset, KITTI 2012 and KITTI 2015 stereo\nbenchmarks, outperforming other published stereo matching methods by a\nnoteworthy margin. EdgeStereo also achieves comparable generalization\nperformance for disparity estimation because of the incorporation of edge cues.\n", "versions": [{"version": "v1", "created": "Tue, 5 Mar 2019 07:00:40 GMT"}, {"version": "v2", "created": "Wed, 11 Dec 2019 05:55:13 GMT"}], "update_date": "2019-12-12", "authors_parsed": [["Song", "Xiao", ""], ["Zhao", "Xu", ""], ["Fang", "Liangji", ""], ["Hu", "Hanwen", ""]]}, {"id": "1903.01712", "submitter": "Zhengwei Bai", "authors": "Zhengwei Bai, Baigen Cai, Wei Shangguan, Linguo Chai", "title": "Deep Learning Based Motion Planning For Autonomous Vehicle Using\n  Spatiotemporal LSTM Network", "comments": "5 pages, 8 figures, Accepted to 2018 Chinese Automation Congress\n  (CAC)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motion Planning, as a fundamental technology of automatic navigation for the\nautonomous vehicle, is still an open challenging issue in the real-life traffic\nsituation and is mostly applied by the model-based approaches. However, due to\nthe complexity of the traffic situations and the uncertainty of the edge cases,\nit is hard to devise a general motion planning system for the autonomous\nvehicle. In this paper, we proposed a motion planning model based on deep\nlearning (named as spatiotemporal LSTM network), which is able to generate a\nreal-time reflection based on spatiotemporal information extraction. To be\nspecific, the model based on spatiotemporal LSTM network has three main\nstructure. Firstly, the Convolutional Long-short Term Memory (Conv-LSTM) is\nused to extract hidden features through sequential image data. Then, the 3D\nConvolutional Neural Network(3D-CNN) is applied to extract the spatiotemporal\ninformation from the multi-frame feature information. Finally, the fully\nconnected neural networks are used to construct a control model for autonomous\nvehicle steering angle. The experiments demonstrated that the proposed method\ncan generate a robust and accurate visual motion planning results for the\nautonomous vehicle.\n", "versions": [{"version": "v1", "created": "Tue, 5 Mar 2019 07:46:47 GMT"}], "update_date": "2019-03-06", "authors_parsed": [["Bai", "Zhengwei", ""], ["Cai", "Baigen", ""], ["Shangguan", "Wei", ""], ["Chai", "Linguo", ""]]}, {"id": "1903.01716", "submitter": "Wei Jiang", "authors": "Wei Jiang and Na Ying", "title": "Improve Object Detection by Data Enhancement based on Generative\n  Adversarial Nets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The accuracy of the object detection model depends on whether the anchor\nboxes effectively trained. Because of the small number of GT boxes or object\ntarget is invariant in the training phase, cannot effectively train anchor\nboxes. Improving detection accuracy by extending the dataset is an effective\nway. We propose a data enhancement method based on the foreground-background\nseparation model. While this model uses a binary image of object target random\nperturb original dataset image. Perturbation methods include changing the color\nchannel of the object, adding salt noise to the object, and enhancing contrast.\nThe main contribution of this paper is to propose a data enhancement method\nbased on GAN and improve detection accuracy of DSSD. Results are shown on both\nPASCAL VOC2007 and PASCAL VOC2012 dataset. Our model with 321x321 input\nachieves 78.7% mAP on the VOC2007 test, 76.6% mAP on the VOC2012 test.\n", "versions": [{"version": "v1", "created": "Tue, 5 Mar 2019 08:05:29 GMT"}], "update_date": "2019-03-06", "authors_parsed": [["Jiang", "Wei", ""], ["Ying", "Na", ""]]}, {"id": "1903.01735", "submitter": "Quoc-Tin Phan", "authors": "Quoc-Tin Phan, Michele Vascotto, Giulia Boato", "title": "Hue Modification Localization By Pair Matching", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hue modification is the adjustment of hue property on color images.\nConducting hue modification on an image is trivial, and it can be abused to\nfalsify opinions of viewers. Since shapes, edges or textural information\nremains unchanged after hue modification, this type of manipulation is\nrelatively hard to be detected and localized. Since small patches inherit the\nsame Color Filter Array (CFA) configuration and demosaicing, any distortion\nmade by local hue modification can be detected by patch matching within the\nsame image. In this paper, we propose to localize hue modification by means of\na Siamese neural network specifically designed for matching two inputs. By\ncrafting the network outputs, we are able to form a heatmap which potentially\nhighlights malicious regions. Our proposed method deals well not only with\nuncompressed images but also with the presence of JPEG compression, an\noperation usually hindering the exploitation of CFA and demosaicing artifacts.\nExperimental evidences corroborate the effectiveness of the proposed method.\n", "versions": [{"version": "v1", "created": "Tue, 5 Mar 2019 09:08:21 GMT"}], "update_date": "2019-03-06", "authors_parsed": [["Phan", "Quoc-Tin", ""], ["Vascotto", "Michele", ""], ["Boato", "Giulia", ""]]}, {"id": "1903.01784", "submitter": "Silvio Giancola", "authors": "Silvio Giancola, Jesus Zarzar, Bernard Ghanem", "title": "Leveraging Shape Completion for 3D Siamese Tracking", "comments": "Accepted in CVPR19", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Point clouds are challenging to process due to their sparsity, therefore\nautonomous vehicles rely more on appearance attributes than pure geometric\nfeatures. However, 3D LIDAR perception can provide crucial information for\nurban navigation in challenging light or weather conditions. In this paper, we\ninvestigate the versatility of Shape Completion for 3D Object Tracking in LIDAR\npoint clouds. We design a Siamese tracker that encodes model and candidate\nshapes into a compact latent representation. We regularize the encoding by\nenforcing the latent representation to decode into an object model shape. We\nobserve that 3D object tracking and 3D shape completion complement each other.\nLearning a more meaningful latent representation shows better discriminatory\ncapabilities, leading to improved tracking performance. We test our method on\nthe KITTI Tracking set using car 3D bounding boxes. Our model reaches a 76.94%\nSuccess rate and 81.38% Precision for 3D Object Tracking, with the shape\ncompletion regularization leading to an improvement of 3% in both metrics.\n", "versions": [{"version": "v1", "created": "Tue, 5 Mar 2019 12:29:10 GMT"}, {"version": "v2", "created": "Thu, 28 Mar 2019 13:03:45 GMT"}], "update_date": "2019-03-29", "authors_parsed": [["Giancola", "Silvio", ""], ["Zarzar", "Jesus", ""], ["Ghanem", "Bernard", ""]]}, {"id": "1903.01804", "submitter": "Abhinav Valada", "authors": "Federico Boniardi, Abhinav Valada, Rohit Mohan, Tim Caselitz, Wolfram\n  Burgard", "title": "Robot Localization in Floor Plans Using a Room Layout Edge Extraction\n  Network", "comments": "Accepted for IROS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Indoor localization is one of the crucial enablers for deployment of service\nrobots. Although several successful techniques for indoor localization have\nbeen proposed, the majority of them relies on maps generated from data gathered\nwith the same sensor modality used for localization. Typically, tedious labor\nby experts is needed to acquire this data, thus limiting the readiness of the\nsystem as well as its ease of installation for inexperienced operators. In this\npaper, we propose a memory and computationally efficient monocular camera-based\nlocalization system that allows a robot to estimate its pose given an\narchitectural floor plan. Our method employs a convolutional neural network to\npredict room layout edges from a single camera image and estimates the robot\npose using a particle filter that matches the extracted edges to the given\nfloor plan. We evaluate our localization system using multiple real-world\nexperiments and demonstrate that it has the robustness and accuracy required\nfor reliable indoor navigation.\n", "versions": [{"version": "v1", "created": "Tue, 5 Mar 2019 13:09:18 GMT"}, {"version": "v2", "created": "Fri, 12 Jul 2019 10:59:11 GMT"}], "update_date": "2019-07-15", "authors_parsed": [["Boniardi", "Federico", ""], ["Valada", "Abhinav", ""], ["Mohan", "Rohit", ""], ["Caselitz", "Tim", ""], ["Burgard", "Wolfram", ""]]}, {"id": "1903.01811", "submitter": "Afzal Ahmad", "authors": "Afzal Ahmad and Muhammad Adeel Pasha", "title": "Towards Design Space Exploration and Optimization of Fast Algorithms for\n  Convolutional Neural Networks (CNNs) on FPGAs", "comments": "Preprint: Accepted in 22nd IEEE Design, Automation & Test in Europe\n  Conference and Exhibition (DATE'19), Florence, Italy, March 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional Neural Networks (CNNs) have gained widespread popularity in the\nfield of computer vision and image processing. Due to huge computational\nrequirements of CNNs, dedicated hardware-based implementations are being\nexplored to improve their performance. Hardware platforms such as Field\nProgrammable Gate Arrays (FPGAs) are widely being used to design parallel\narchitectures for this purpose. In this paper, we analyze Winograd minimal\nfiltering or fast convolution algorithms to reduce the arithmetic complexity of\nconvolutional layers of CNNs. We explore a complex design space to find the\nsets of parameters that result in improved throughput and power-efficiency. We\nalso design a pipelined and parallel Winograd convolution engine that improves\nthe throughput and power-efficiency while reducing the computational complexity\nof the overall system. Our proposed designs show up to 4.75$\\times$ and\n1.44$\\times$ improvements in throughput and power-efficiency, respectively, in\ncomparison to the state-of-the-art design while using approximately\n2.67$\\times$ more multipliers. Furthermore, we obtain savings of up to 53.6\\%\nin logic resources compared with the state-of-the-art implementation.\n", "versions": [{"version": "v1", "created": "Tue, 5 Mar 2019 13:28:07 GMT"}], "update_date": "2019-03-06", "authors_parsed": [["Ahmad", "Afzal", ""], ["Pasha", "Muhammad Adeel", ""]]}, {"id": "1903.01814", "submitter": "Tim Lukas Holch", "authors": "Constantin Steppa, Tim Lukas Holch", "title": "HexagDLy - Processing hexagonally sampled data with CNNs in PyTorch", "comments": null, "journal-ref": "SoftwareX, 9, 193-198, 2019", "doi": "10.1016/j.softx.2019.02.010", "report-no": null, "categories": "cs.CV astro-ph.IM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  HexagDLy is a Python-library extending the PyTorch deep learning framework\nwith convolution and pooling operations on hexagonal grids. It aims to ease the\naccess to convolutional neural networks for applications that rely on\nhexagonally sampled data as, for example, commonly found in ground-based\nastroparticle physics experiments.\n", "versions": [{"version": "v1", "created": "Tue, 5 Mar 2019 13:32:03 GMT"}], "update_date": "2019-04-03", "authors_parsed": [["Steppa", "Constantin", ""], ["Holch", "Tim Lukas", ""]]}, {"id": "1903.01828", "submitter": "Simon Lang", "authors": "Simon R Lang, Martin H Luerssen and David M Powers", "title": "Virtual Ground Truth, and Pre-selection of 3D Interest Points for\n  Improved Repeatability Evaluation of 2D Detectors", "comments": "Accepted for publication in CCVPR 2018 Conference Proceedings,\n  Wellington, New Zealand. 11 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In Computer Vision, finding simple features is performed using classifiers\ncalled interest point (IP) detectors, which are often utilised to track\nfeatures as the scene changes. For 2D based classifiers it has been intuitive\nto measure repeated point reliability using 2D metrics given the difficulty to\nestablish ground truth beyond 2D. The aim is to bridge the gap between 2D\nclassifiers and 3D environments, and improve performance analysis of 2D IP\nclassification on 3D objects. This paper builds on existing work with 3D\nscanned and artificial models to test conventional 2D feature detectors with\nthe assistance of virtualised 3D scenes. Virtual space depth is leveraged in\ntests to perform pre-selection of closest repeatable points in both 2D and 3D\ncontexts before repeatability is measured. This more reliable ground truth is\nused to analyse testing configurations with a singular and 12 model dataset\nacross affine transforms in x, y and z rotation, as well as x,y scaling with 9\nwell known IP detectors. The virtual scene's ground truth demonstrates that 3D\npre-selection eliminates a large portion of false positives that are normally\nconsidered repeated in 2D configurations. The results indicate that 3D virtual\nenvironments can provide assistance in comparing the performance of\nconventional detectors when extending their applications to 3D environments,\nand can result in better classification of features when testing prospective\nclassifiers' performance. A ROC based informedness measure also highlights\ntradeoffs in 2D/3D performance compared to conventional repeatability measures.\n", "versions": [{"version": "v1", "created": "Tue, 5 Mar 2019 14:03:13 GMT"}], "update_date": "2019-03-06", "authors_parsed": [["Lang", "Simon R", ""], ["Luerssen", "Martin H", ""], ["Powers", "David M", ""]]}, {"id": "1903.01864", "submitter": "Zhixin Wang", "authors": "Zhixin Wang and Kui Jia", "title": "Frustum ConvNet: Sliding Frustums to Aggregate Local Point-Wise Features\n  for Amodal 3D Object Detection", "comments": "IROS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we propose a novel method termed \\emph{Frustum ConvNet\n(F-ConvNet)} for amodal 3D object detection from point clouds. Given 2D region\nproposals in an RGB image, our method first generates a sequence of frustums\nfor each region proposal, and uses the obtained frustums to group local points.\nF-ConvNet aggregates point-wise features as frustum-level feature vectors, and\narrays these feature vectors as a feature map for use of its subsequent\ncomponent of fully convolutional network (FCN), which spatially fuses\nfrustum-level features and supports an end-to-end and continuous estimation of\noriented boxes in the 3D space. We also propose component variants of\nF-ConvNet, including an FCN variant that extracts multi-resolution frustum\nfeatures, and a refined use of F-ConvNet over a reduced 3D space. Careful\nablation studies verify the efficacy of these component variants. F-ConvNet\nassumes no prior knowledge of the working 3D environment and is thus\ndataset-agnostic. We present experiments on both the indoor SUN-RGBD and\noutdoor KITTI datasets. F-ConvNet outperforms all existing methods on SUN-RGBD,\nand at the time of submission it outperforms all published works on the KITTI\nbenchmark. Code has been made available at:\n{\\url{https://github.com/zhixinwang/frustum-convnet}.}\n", "versions": [{"version": "v1", "created": "Tue, 5 Mar 2019 14:46:58 GMT"}, {"version": "v2", "created": "Wed, 14 Aug 2019 12:38:26 GMT"}], "update_date": "2019-08-15", "authors_parsed": [["Wang", "Zhixin", ""], ["Jia", "Kui", ""]]}, {"id": "1903.01882", "submitter": "Reuben Feinman", "authors": "Reuben Feinman, Brenden M. Lake", "title": "Learning a smooth kernel regularizer for convolutional neural networks", "comments": "Submitted to CogSci 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern deep neural networks require a tremendous amount of data to train,\noften needing hundreds or thousands of labeled examples to learn an effective\nrepresentation. For these networks to work with less data, more structure must\nbe built into their architectures or learned from previous experience. The\nlearned weights of convolutional neural networks (CNNs) trained on large\ndatasets for object recognition contain a substantial amount of structure.\nThese representations have parallels to simple cells in the primary visual\ncortex, where receptive fields are smooth and contain many regularities.\nIncorporating smoothness constraints over the kernel weights of modern CNN\narchitectures is a promising way to improve their sample complexity. We propose\na smooth kernel regularizer that encourages spatial correlations in convolution\nkernel weights. The correlation parameters of this regularizer are learned from\nprevious experience, yielding a method with a hierarchical Bayesian\ninterpretation. We show that our correlated regularizer can help constrain\nmodels for visual recognition, improving over an L2 regularization baseline.\n", "versions": [{"version": "v1", "created": "Tue, 5 Mar 2019 15:07:29 GMT"}], "update_date": "2019-03-06", "authors_parsed": [["Feinman", "Reuben", ""], ["Lake", "Brenden M.", ""]]}, {"id": "1903.01905", "submitter": "Bernhard Kainz", "authors": "Daniel Grzech, Lo\\\"ic le Folgoc, Mattias P. Heinrich, Bishesh Khanal,\n  Jakub Moll, Julia A. Schnabel, Ben Glocker, Bernhard Kainz", "title": "FastReg: Fast Non-Rigid Registration via Accelerated Optimisation on the\n  Manifold of Diffeomorphisms", "comments": "There is an ongoing dispute about the presentation of this paper. It\n  will be withdrawn until the dispute is resoved", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an implementation of a new approach to diffeomorphic non-rigid\nregistration of medical images. The method is based on optical flow and warps\nimages via gradient flow with the standard $L^2$ inner product. To compute the\ntransformation, we rely on accelerated optimisation on the manifold of\ndiffeomorphisms. We achieve regularity properties of Sobolev gradient flows,\nwhich are expensive to compute, owing to a novel method of averaging the\ngradients in time rather than space. We successfully register brain MRI and\nchallenging abdominal CT scans at speeds orders of magnitude faster than\nprevious approaches. We make our code available in a public repository:\nhttps://github.com/dgrzech/fastreg\n", "versions": [{"version": "v1", "created": "Tue, 5 Mar 2019 15:41:47 GMT"}, {"version": "v2", "created": "Tue, 23 Apr 2019 15:37:43 GMT"}, {"version": "v3", "created": "Wed, 24 Apr 2019 10:02:27 GMT"}], "update_date": "2019-04-25", "authors_parsed": [["Grzech", "Daniel", ""], ["Folgoc", "Lo\u00efc le", ""], ["Heinrich", "Mattias P.", ""], ["Khanal", "Bishesh", ""], ["Moll", "Jakub", ""], ["Schnabel", "Julia A.", ""], ["Glocker", "Ben", ""], ["Kainz", "Bernhard", ""]]}, {"id": "1903.01931", "submitter": "Jianlin Su", "authors": "Jianlin Su", "title": "O-GAN: Extremely Concise Approach for Auto-Encoding Generative\n  Adversarial Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In this paper, we propose Orthogonal Generative Adversarial Networks\n(O-GANs). We decompose the network of discriminator orthogonally and add an\nextra loss into the objective of common GANs, which can enforce discriminator\nbecome an effective encoder. The same extra loss can be embedded into any kind\nof GANs and there is almost no increase in computation. Furthermore, we discuss\nthe principle of our method, which is relative to the fully-exploiting of the\nremaining degrees of freedom of discriminator. As we know, our solution is the\nsimplest approach to train a generative adversarial network with auto-encoding\nability.\n", "versions": [{"version": "v1", "created": "Tue, 5 Mar 2019 17:01:49 GMT"}], "update_date": "2019-03-06", "authors_parsed": [["Su", "Jianlin", ""]]}, {"id": "1903.01945", "submitter": "Yazan Abu Farha", "authors": "Yazan Abu Farha and Juergen Gall", "title": "MS-TCN: Multi-Stage Temporal Convolutional Network for Action\n  Segmentation", "comments": "CVPR 2019 Camera Ready", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Temporally locating and classifying action segments in long untrimmed videos\nis of particular interest to many applications like surveillance and robotics.\nWhile traditional approaches follow a two-step pipeline, by generating\nframe-wise probabilities and then feeding them to high-level temporal models,\nrecent approaches use temporal convolutions to directly classify the video\nframes. In this paper, we introduce a multi-stage architecture for the temporal\naction segmentation task. Each stage features a set of dilated temporal\nconvolutions to generate an initial prediction that is refined by the next one.\nThis architecture is trained using a combination of a classification loss and a\nproposed smoothing loss that penalizes over-segmentation errors. Extensive\nevaluation shows the effectiveness of the proposed model in capturing\nlong-range dependencies and recognizing action segments. Our model achieves\nstate-of-the-art results on three challenging datasets: 50Salads, Georgia Tech\nEgocentric Activities (GTEA), and the Breakfast dataset.\n", "versions": [{"version": "v1", "created": "Tue, 5 Mar 2019 17:29:37 GMT"}, {"version": "v2", "created": "Tue, 2 Apr 2019 15:35:40 GMT"}], "update_date": "2019-04-03", "authors_parsed": [["Farha", "Yazan Abu", ""], ["Gall", "Juergen", ""]]}, {"id": "1903.01949", "submitter": "Lei Cui", "authors": "Minghao Li, Lei Cui, Shaohan Huang, Furu Wei, Ming Zhou, Zhoujun Li", "title": "TableBank: A Benchmark Dataset for Table Detection and Recognition", "comments": "LREC 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present TableBank, a new image-based table detection and recognition\ndataset built with novel weak supervision from Word and Latex documents on the\ninternet. Existing research for image-based table detection and recognition\nusually fine-tunes pre-trained models on out-of-domain data with a few thousand\nhuman-labeled examples, which is difficult to generalize on real-world\napplications. With TableBank that contains 417K high quality labeled tables, we\nbuild several strong baselines using state-of-the-art models with deep neural\nnetworks. We make TableBank publicly available and hope it will empower more\ndeep learning approaches in the table detection and recognition task. The\ndataset and models are available at\n\\url{https://github.com/doc-analysis/TableBank}.\n", "versions": [{"version": "v1", "created": "Tue, 5 Mar 2019 17:34:21 GMT"}, {"version": "v2", "created": "Mon, 6 Jul 2020 09:19:56 GMT"}], "update_date": "2020-07-07", "authors_parsed": [["Li", "Minghao", ""], ["Cui", "Lei", ""], ["Huang", "Shaohan", ""], ["Wei", "Furu", ""], ["Zhou", "Ming", ""], ["Li", "Zhoujun", ""]]}, {"id": "1903.01980", "submitter": "Matthew Wicker", "authors": "Luca Cardelli, Marta Kwiatkowska, Luca Laurenti, Nicola Paoletti,\n  Andrea Patane, and Matthew Wicker", "title": "Statistical Guarantees for the Robustness of Bayesian Neural Networks", "comments": "9 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a probabilistic robustness measure for Bayesian Neural Networks\n(BNNs), defined as the probability that, given a test point, there exists a\npoint within a bounded set such that the BNN prediction differs between the\ntwo. Such a measure can be used, for instance, to quantify the probability of\nthe existence of adversarial examples. Building on statistical verification\ntechniques for probabilistic models, we develop a framework that allows us to\nestimate probabilistic robustness for a BNN with statistical guarantees, i.e.,\nwith a priori error and confidence bounds. We provide experimental comparison\nfor several approximate BNN inference techniques on image classification tasks\nassociated to MNIST and a two-class subset of the GTSRB dataset. Our results\nenable quantification of uncertainty of BNN predictions in adversarial\nsettings.\n", "versions": [{"version": "v1", "created": "Tue, 5 Mar 2019 18:49:40 GMT"}], "update_date": "2019-03-06", "authors_parsed": [["Cardelli", "Luca", ""], ["Kwiatkowska", "Marta", ""], ["Laurenti", "Luca", ""], ["Paoletti", "Nicola", ""], ["Patane", "Andrea", ""], ["Wicker", "Matthew", ""]]}, {"id": "1903.02025", "submitter": "Mohammad Asiful Hossain", "authors": "Mohammad Asiful Hossain, Mehrdad Hosseinzadeh, Omit Chanda, Yang Wang", "title": "Crowd Counting Using Scale-Aware Attention Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider the problem of crowd counting in images. Given an\nimage of a crowded scene, our goal is to estimate the density map of this\nimage, where each pixel value in the density map corresponds to the crowd\ndensity at the corresponding location in the image. Given the estimated density\nmap, the final crowd count can be obtained by summing over all values in the\ndensity map. One challenge of crowd counting is the scale variation in images.\nIn this work, we propose a novel scale-aware attention network to address this\nchallenge. Using the attention mechanism popular in recent deep learning\narchitectures, our model can automatically focus on certain global and local\nscales appropriate for the image. By combining these global and local scale\nattention, our model outperforms other state-of-the-art methods for crowd\ncounting on several benchmark datasets.\n", "versions": [{"version": "v1", "created": "Tue, 5 Mar 2019 19:36:21 GMT"}], "update_date": "2019-03-07", "authors_parsed": [["Hossain", "Mohammad Asiful", ""], ["Hosseinzadeh", "Mehrdad", ""], ["Chanda", "Omit", ""], ["Wang", "Yang", ""]]}, {"id": "1903.02026", "submitter": "Pingkun Yan", "authors": "Grant Haskins, Uwe Kruger, Pingkun Yan", "title": "Deep Learning in Medical Image Registration: A Survey", "comments": "Accepted for publication by Machine Vision and Applications on\n  January 8, 2020", "journal-ref": null, "doi": "10.1007/s00138-020-01060-x", "report-no": null, "categories": "q-bio.QM cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The establishment of image correspondence through robust image registration\nis critical to many clinical tasks such as image fusion, organ atlas creation,\nand tumor growth monitoring, and is a very challenging problem. Since the\nbeginning of the recent deep learning renaissance, the medical imaging research\ncommunity has developed deep learning based approaches and achieved the\nstate-of-the-art in many applications, including image registration. The rapid\nadoption of deep learning for image registration applications over the past few\nyears necessitates a comprehensive summary and outlook, which is the main scope\nof this survey. This requires placing a focus on the different research areas\nas well as highlighting challenges that practitioners face. This survey,\ntherefore, outlines the evolution of deep learning based medical image\nregistration in the context of both research challenges and relevant\ninnovations in the past few years. Further, this survey highlights future\nresearch directions to show how this field may be possibly moved forward to the\nnext level.\n", "versions": [{"version": "v1", "created": "Tue, 5 Mar 2019 19:37:51 GMT"}, {"version": "v2", "created": "Tue, 21 Jan 2020 14:58:06 GMT"}], "update_date": "2020-01-22", "authors_parsed": [["Haskins", "Grant", ""], ["Kruger", "Uwe", ""], ["Yan", "Pingkun", ""]]}, {"id": "1903.02040", "submitter": "Yuxing Tang", "authors": "Yuxing Tang and Youbao Tang and Mei Han and Jing Xiao and Ronald M.\n  Summers", "title": "Abnormal Chest X-ray Identification With Generative Adversarial\n  One-Class Classifier", "comments": "Accepted as an oral presentation in IEEE International Symposium on\n  Biomedical Imaging (ISBI) 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Being one of the most common diagnostic imaging tests, chest radiography\nrequires timely reporting of potential findings in the images. In this paper,\nwe propose an end-to-end architecture for abnormal chest X-ray identification\nusing generative adversarial one-class learning. Unlike previous approaches,\nour method takes only normal chest X-ray images as input. The architecture is\ncomposed of three deep neural networks, each of which learned by competing\nwhile collaborating among them to model the underlying content structure of the\nnormal chest X-rays. Given a chest X-ray image in the testing phase, if it is\nnormal, the learned architecture can well model and reconstruct the content; if\nit is abnormal, since the content is unseen in the training phase, the model\nwould perform poorly in its reconstruction. It thus enables distinguishing\nabnormal chest X-rays from normal ones. Quantitative and qualitative\nexperiments demonstrate the effectiveness and efficiency of our approach, where\nan AUC of 0.841 is achieved on the challenging NIH Chest X-ray dataset in a\none-class learning setting, with the potential in reducing the workload for\nradiologists.\n", "versions": [{"version": "v1", "created": "Tue, 5 Mar 2019 20:24:23 GMT"}], "update_date": "2019-03-07", "authors_parsed": [["Tang", "Yuxing", ""], ["Tang", "Youbao", ""], ["Han", "Mei", ""], ["Xiao", "Jing", ""], ["Summers", "Ronald M.", ""]]}, {"id": "1903.02056", "submitter": "Erdem Akag\\\"und\\\"uz", "authors": "Erdem Akagunduz, Adrian G. Bors and Karla K. Evans", "title": "Defining Image Memorability using the Visual Memory Schema", "comments": "Submitted to TPAMI on Aug 4, 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Memorability of an image is a characteristic determined by the human\nobservers' ability to remember images they have seen. Yet recent work on image\nmemorability defines it as an intrinsic property that can be obtained\nindependent of the observer. {The current study aims to enhance our\nunderstanding and prediction of image memorability, improving upon existing\napproaches by incorporating the properties of cumulative human annotations.} We\npropose a new concept called the Visual Memory Schema (VMS) referring to an\norganisation of image components human observers share when encoding and\nrecognising images. The concept of VMS is operationalised by asking human\nobservers to define memorable regions of images they were asked to remember\nduring an episodic memory test. We then statistically assess the consistency of\nVMSs across observers for either correctly or incorrectly recognised images.\nThe associations of the VMSs with eye fixations and saliency are analysed\nseparately as well. Lastly, we adapt various deep learning architectures for\nthe reconstruction and prediction of memorable regions in images and analyse\nthe results when using transfer learning at the outputs of different\nconvolutional network layers.\n", "versions": [{"version": "v1", "created": "Tue, 5 Mar 2019 21:12:27 GMT"}], "update_date": "2019-03-07", "authors_parsed": [["Akagunduz", "Erdem", ""], ["Bors", "Adrian G.", ""], ["Evans", "Karla K.", ""]]}, {"id": "1903.02074", "submitter": "Jonathon Sather", "authors": "Jonathon Sather and Xiaozheng Jane Zhang", "title": "Viewpoint Optimization for Autonomous Strawberry Harvesting with Deep\n  Reinforcement Learning", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Autonomous harvesting may provide a viable solution to mounting labor\npressures in the United States's strawberry industry. However, due to\nbottlenecks in machine perception and economic viability, a profitable and\ncommercially adopted strawberry harvesting system remains elusive. In this\nresearch, we explore the feasibility of using deep reinforcement learning to\novercome these bottlenecks and develop a practical algorithm to address the\nsub-objective of viewpoint optimization, or the development of a control policy\nto direct a camera to favorable vantage points for autonomous harvesting. We\nevaluate the algorithm's performance in a custom, open-source simulated\nenvironment and observe encouraging results. Our trained agent yields 8.7 times\nhigher returns than random actions and 8.8 percent faster exploration than our\nbest baseline policy, which uses visual servoing. Visual investigation shows\nthe agent is able to fixate on favorable viewpoints, despite having no explicit\nmeans to propagate information through time. Overall, we conclude that deep\nreinforcement learning is a promising area of research to advance the state of\nthe art in autonomous strawberry harvesting.\n", "versions": [{"version": "v1", "created": "Tue, 5 Mar 2019 21:54:46 GMT"}, {"version": "v2", "created": "Thu, 2 May 2019 05:01:24 GMT"}], "update_date": "2019-05-03", "authors_parsed": [["Sather", "Jonathon", ""], ["Zhang", "Xiaozheng Jane", ""]]}, {"id": "1903.02080", "submitter": "Senthil Yogamani", "authors": "Sambit Mohapatra, Heinrich Gotzig, Senthil Yogamani, Stefan Milz and\n  Raoul Zollner", "title": "Exploring Deep Spiking Neural Networks for Automated Driving\n  Applications", "comments": "Accepted for Oral Presentation at VISAPP 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural networks have become the standard model for various computer vision\ntasks in automated driving including semantic segmentation, moving object\ndetection, depth estimation, visual odometry, etc. The main flavors of neural\nnetworks which are used commonly are convolutional (CNN) and recurrent (RNN).\nIn spite of rapid progress in embedded processors, power consumption and cost\nis still a bottleneck. Spiking Neural Networks (SNNs) are gradually progressing\nto achieve low-power event-driven hardware architecture which has a potential\nfor high efficiency. In this paper, we explore the role of deep spiking neural\nnetworks (SNN) for automated driving applications. We provide an overview of\nprogress on SNN and argue how it can be a good fit for automated driving\napplications.\n", "versions": [{"version": "v1", "created": "Fri, 11 Jan 2019 18:40:42 GMT"}], "update_date": "2019-03-07", "authors_parsed": [["Mohapatra", "Sambit", ""], ["Gotzig", "Heinrich", ""], ["Yogamani", "Senthil", ""], ["Milz", "Stefan", ""], ["Zollner", "Raoul", ""]]}, {"id": "1903.02110", "submitter": "Behzad Hasani", "authors": "Behzad Hasani, Pooran Singh Negi, Mohammad H. Mahoor", "title": "Bounded Residual Gradient Networks (BReG-Net) for Facial Affect\n  Computing", "comments": "To appear in 14th IEEE International Conference on Automatic Face &\n  Gesture Recognition (FG 2019)", "journal-ref": "2019 14th IEEE International Conference on Automatic Face &\n  Gesture Recognition (FG 2019)", "doi": "10.1109/FG.2019.8756587", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Residual-based neural networks have shown remarkable results in various\nvisual recognition tasks including Facial Expression Recognition (FER). Despite\nthe tremendous efforts have been made to improve the performance of FER systems\nusing DNNs, existing methods are not generalizable enough for practical\napplications. This paper introduces Bounded Residual Gradient Networks\n(BReG-Net) for facial expression recognition, in which the shortcut connection\nbetween the input and the output of the ResNet module is replaced with a\ndifferentiable function with a bounded gradient. This configuration prevents\nthe network from facing the vanishing or exploding gradient problem. We show\nthat utilizing such non-linear units will result in shallower networks with\nbetter performance. Further, by using a weighted loss function which gives a\nhigher priority to less represented categories, we can achieve an overall\nbetter recognition rate. The results of our experiments show that BReG-Nets\noutperform state-of-the-art methods on three publicly available facial\ndatabases in the wild, on both the categorical and dimensional models of\naffect.\n", "versions": [{"version": "v1", "created": "Tue, 5 Mar 2019 23:31:20 GMT"}], "update_date": "2020-04-17", "authors_parsed": [["Hasani", "Behzad", ""], ["Negi", "Pooran Singh", ""], ["Mahoor", "Mohammad H.", ""]]}, {"id": "1903.02120", "submitter": "Chunhua Shen", "authors": "Zhi Tian, Tong He, Chunhua Shen, Youliang Yan", "title": "Decoders Matter for Semantic Segmentation: Data-Dependent Decoding\n  Enables Flexible Feature Aggregation", "comments": "Accepted to Proc. IEEE Conf. Computer Vision and Pattern Recognition\n  (CVPR), 2019. Content may change prior to final publication", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent semantic segmentation methods exploit encoder-decoder architectures to\nproduce the desired pixel-wise segmentation prediction. The last layer of the\ndecoders is typically a bilinear upsampling procedure to recover the final\npixel-wise prediction. We empirically show that this oversimple and\ndata-independent bilinear upsampling may lead to sub-optimal results.\n  In this work, we propose a data-dependent upsampling (DUpsampling) to replace\nbilinear, which takes advantages of the redundancy in the label space of\nsemantic segmentation and is able to recover the pixel-wise prediction from\nlow-resolution outputs of CNNs. The main advantage of the new upsampling layer\nlies in that with a relatively lower-resolution feature map such as\n$\\frac{1}{16}$ or $\\frac{1}{32}$ of the input size, we can achieve even better\nsegmentation accuracy, significantly reducing computation complexity. This is\nmade possible by 1) the new upsampling layer's much improved reconstruction\ncapability; and more importantly 2) the DUpsampling based decoder's flexibility\nin leveraging almost arbitrary combinations of the CNN encoders' features.\nExperiments demonstrate that our proposed decoder outperforms the\nstate-of-the-art decoder, with only $\\sim$20\\% of computation. Finally, without\nany post-processing, the framework equipped with our proposed decoder achieves\nnew state-of-the-art performance on two datasets: 88.1\\% mIOU on PASCAL VOC\nwith 30\\% computation of the previously best model; and 52.5\\% mIOU on PASCAL\nContext.\n", "versions": [{"version": "v1", "created": "Tue, 5 Mar 2019 23:59:41 GMT"}, {"version": "v2", "created": "Sat, 9 Mar 2019 23:30:45 GMT"}, {"version": "v3", "created": "Fri, 5 Apr 2019 01:27:46 GMT"}], "update_date": "2019-04-08", "authors_parsed": [["Tian", "Zhi", ""], ["He", "Tong", ""], ["Shen", "Chunhua", ""], ["Yan", "Youliang", ""]]}, {"id": "1903.02133", "submitter": "Yunfan Liu", "authors": "Qi Li, Yunfan Liu, Zhenan Sun", "title": "Age Progression and Regression with Spatial Attention Modules", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Age progression and regression refers to aesthetically render-ing a given\nface image to present effects of face aging and rejuvenation, respectively.\nAlthough numerous studies have been conducted in this topic, there are two\nmajor problems: 1) multiple models are usually trained to simulate different\nage mappings, and 2) the photo-realism of generated face images is heavily\ninfluenced by the variation of training images in terms of pose, illumination,\nand background. To address these issues, in this paper, we propose a framework\nbased on conditional Generative Adversarial Networks (cGANs) to achieve age\nprogression and regression simultaneously. Particularly, since face aging and\nrejuvenation are largely different in terms of image translation patterns, we\nmodel these two processes using two separate generators, each dedicated to one\nage changing process. In addition, we exploit spatial attention mechanisms to\nlimit image modifications to regions closely related to age changes, so that\nimages with high visual fidelity could be synthesized for in-the-wild cases.\nExperiments on multiple datasets demonstrate the ability of our model in\nsynthesizing lifelike face images at desired ages with personalized features\nwell preserved, and keeping age-irrelevant regions unchanged.\n", "versions": [{"version": "v1", "created": "Wed, 6 Mar 2019 01:31:30 GMT"}, {"version": "v2", "created": "Mon, 7 Oct 2019 01:51:48 GMT"}], "update_date": "2019-10-08", "authors_parsed": [["Li", "Qi", ""], ["Liu", "Yunfan", ""], ["Sun", "Zhenan", ""]]}, {"id": "1903.02137", "submitter": "Da Li", "authors": "Da Li and Zhang Zhang", "title": "Large-Scale Pedestrian Retrieval Competition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Large-Scale Pedestrian Retrieval Competition (LSPRC) mainly focuses on\nperson retrieval which is an important end application in intelligent vision\nsystem of surveillance. Person retrieval aims at searching the interested\ntarget with specific visual attributes or images. The low image quality,\nvarious camera viewpoints, large pose variations and occlusions in real scenes\nmake it a challenge problem. By providing large-scale surveillance data in real\nscene and standard evaluation methods that are closer to real application, the\ncompetition aims to improve the robust of related algorithms and further meet\nthe complicated situations in real application. LSPRC includes two kinds of\ntasks, i.e., Attribute based Pedestrian Retrieval (PR-A) and Re-IDentification\n(ReID) based Pedestrian Retrieval (PR-ID). The normal evaluation index, i.e.,\nmean Average Precision (mAP), is used to measure the performances of the two\ntasks under various scale, pose and occlusion. While the method of system\nevaluation is introduced to evaluate the person retrieval system in which the\nrelated algorithms of the two tasks are integrated into a large-scale video\nparsing platform (named ISEE) combing with algorithm of pedestrian detection.\n", "versions": [{"version": "v1", "created": "Wed, 6 Mar 2019 01:55:35 GMT"}], "update_date": "2019-03-07", "authors_parsed": [["Li", "Da", ""], ["Zhang", "Zhang", ""]]}, {"id": "1903.02152", "submitter": "Jiangchao Yao", "authors": "Jiangchao Yao, Ya Zhang, Ivor W. Tsang and Jun Sun", "title": "Safeguarded Dynamic Label Regression for Generalized Noisy Supervision", "comments": "Submitted to Transactions on Image Processing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning with noisy labels, which aims to reduce expensive labors on accurate\nannotations, has become imperative in the Big Data era. Previous noise\ntransition based method has achieved promising results and presented a\ntheoretical guarantee on performance in the case of class-conditional noise.\nHowever, this type of approaches critically depend on an accurate\npre-estimation of the noise transition, which is usually impractical.\nSubsequent improvement adapts the pre-estimation along with the training\nprogress via a Softmax layer. However, the parameters in the Softmax layer are\nhighly tweaked for the fragile performance due to the ill-posed stochastic\napproximation. To address these issues, we propose a Latent Class-Conditional\nNoise model (LCCN) that naturally embeds the noise transition under a Bayesian\nframework. By projecting the noise transition into a Dirichlet-distributed\nspace, the learning is constrained on a simplex based on the whole dataset,\ninstead of some ad-hoc parametric space. We then deduce a dynamic label\nregression method for LCCN to iteratively infer the latent labels, to\nstochastically train the classifier and to model the noise. Our approach\nsafeguards the bounded update of the noise transition, which avoids previous\narbitrarily tuning via a batch of samples. We further generalize LCCN for\nopen-set noisy labels and the semi-supervised setting. We perform extensive\nexperiments with the controllable noise data sets, CIFAR-10 and CIFAR-100, and\nthe agnostic noise data sets, Clothing1M and WebVision17. The experimental\nresults have demonstrated that the proposed model outperforms several\nstate-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Wed, 6 Mar 2019 03:20:09 GMT"}], "update_date": "2019-03-07", "authors_parsed": [["Yao", "Jiangchao", ""], ["Zhang", "Ya", ""], ["Tsang", "Ivor W.", ""], ["Sun", "Jun", ""]]}, {"id": "1903.02155", "submitter": "Xie De", "authors": "De Xie, Cheng Deng, Hao Wang, Chao Li, Dapeng Tao", "title": "Semantic Adversarial Network with Multi-scale Pyramid Attention for\n  Video Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Two-stream architecture have shown strong performance in video classification\ntask. The key idea is to learn spatio-temporal features by fusing convolutional\nnetworks spatially and temporally. However, there are some problems within such\narchitecture. First, it relies on optical flow to model temporal information,\nwhich are often expensive to compute and store. Second, it has limited ability\nto capture details and local context information for video data. Third, it\nlacks explicit semantic guidance that greatly decrease the classification\nperformance. In this paper, we proposed a new two-stream based deep framework\nfor video classification to discover spatial and temporal information only from\nRGB frames, moreover, the multi-scale pyramid attention (MPA) layer and the\nsemantic adversarial learning (SAL) module is introduced and integrated in our\nframework. The MPA enables the network capturing global and local feature to\ngenerate a comprehensive representation for video, and the SAL can make this\nrepresentation gradually approximate to the real video semantics in an\nadversarial manner. Experimental results on two public benchmarks demonstrate\nour proposed methods achieves state-of-the-art results on standard video\ndatasets.\n", "versions": [{"version": "v1", "created": "Wed, 6 Mar 2019 03:36:11 GMT"}], "update_date": "2019-03-07", "authors_parsed": [["Xie", "De", ""], ["Deng", "Cheng", ""], ["Wang", "Hao", ""], ["Li", "Chao", ""], ["Tao", "Dapeng", ""]]}, {"id": "1903.02165", "submitter": "Jon McCormack", "authors": "Dilpreet Singh, Nina Rajcic, Simon Colton and Jon McCormack", "title": "Camera Obscurer: Generative Art for Design Inspiration", "comments": "Accepted for EvoMUSART 2019: 8th International Conference on\n  Computational Intelligence in Music, Sound, Art and Design. April 2019,\n  Leipzig, Germany", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate using generated decorative art as a source of inspiration for\ndesign tasks. Using a visual similarity search for image retrieval, the\n\\emph{Camera Obscurer} app enables rapid searching of tens of thousands of\ngenerated abstract images of various types. The seed for a visual similarity\nsearch is a given image, and the retrieved generated images share some visual\nsimilarity with the seed. Implemented in a hand-held device, the app empowers\nusers to use photos of their surroundings to search through the archive of\ngenerated images and other image archives. Being abstract in nature, the\nretrieved images supplement the seed image rather than replace it, providing\ndifferent visual stimuli including shapes, colours, textures and\njuxtapositions, in addition to affording their own interpretations. This\napproach can therefore be used to provide inspiration for a design task, with\nthe abstract images suggesting new ideas that might give direction to a graphic\ndesign project. We describe a crowdsourcing experiment with the app to estimate\nuser confidence in retrieved images, and we describe a pilot study where Camera\nObscurer provided inspiration for a design task. These experiments have enabled\nus to describe future improvements, and to begin to understand sources of\nvisual inspiration for design tasks.\n", "versions": [{"version": "v1", "created": "Wed, 6 Mar 2019 04:05:47 GMT"}], "update_date": "2019-03-07", "authors_parsed": [["Singh", "Dilpreet", ""], ["Rajcic", "Nina", ""], ["Colton", "Simon", ""], ["McCormack", "Jon", ""]]}, {"id": "1903.02172", "submitter": "Marwan Mattar", "authors": "Marwan Mattar, Roozbeh Mottaghi, Julian Togelius, Danny Lange", "title": "AAAI-2019 Workshop on Games and Simulations for Artificial Intelligence", "comments": "AAAI-2019 Workshop on Games and Simulations for Artificial\n  Intelligence", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL cs.CV cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This volume represents the accepted submissions from the AAAI-2019 Workshop\non Games and Simulations for Artificial Intelligence held on January 29, 2019\nin Honolulu, Hawaii, USA. https://www.gamesim.ai\n", "versions": [{"version": "v1", "created": "Wed, 6 Mar 2019 04:49:07 GMT"}], "update_date": "2019-03-07", "authors_parsed": [["Mattar", "Marwan", ""], ["Mottaghi", "Roozbeh", ""], ["Togelius", "Julian", ""], ["Lange", "Danny", ""]]}, {"id": "1903.02193", "submitter": "Qin Zou", "authors": "Qin Zou, Hanwen Jiang, Qiyu Dai, Yuanhao Yue, Long Chen and Qian Wang", "title": "Robust Lane Detection from Continuous Driving Scenes Using Deep Neural\n  Networks", "comments": "IEEE Transactions on Vehicular Technology, 69(1), 2020", "journal-ref": null, "doi": "10.1109/TVT.2019.2949603", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Lane detection in driving scenes is an important module for autonomous\nvehicles and advanced driver assistance systems. In recent years, many\nsophisticated lane detection methods have been proposed. However, most methods\nfocus on detecting the lane from one single image, and often lead to\nunsatisfactory performance in handling some extremely-bad situations such as\nheavy shadow, severe mark degradation, serious vehicle occlusion, and so on. In\nfact, lanes are continuous line structures on the road. Consequently, the lane\nthat cannot be accurately detected in one current frame may potentially be\ninferred out by incorporating information of previous frames. To this end, we\ninvestigate lane detection by using multiple frames of a continuous driving\nscene, and propose a hybrid deep architecture by combining the convolutional\nneural network (CNN) and the recurrent neural network (RNN). Specifically,\ninformation of each frame is abstracted by a CNN block, and the CNN features of\nmultiple continuous frames, holding the property of time-series, are then fed\ninto the RNN block for feature learning and lane prediction. Extensive\nexperiments on two large-scale datasets demonstrate that, the proposed method\noutperforms the competing methods in lane detection, especially in handling\ndifficult situations.\n", "versions": [{"version": "v1", "created": "Wed, 6 Mar 2019 06:29:18 GMT"}, {"version": "v2", "created": "Wed, 29 Apr 2020 16:00:39 GMT"}], "update_date": "2020-04-30", "authors_parsed": [["Zou", "Qin", ""], ["Jiang", "Hanwen", ""], ["Dai", "Qiyu", ""], ["Yue", "Yuanhao", ""], ["Chen", "Long", ""], ["Wang", "Qian", ""]]}, {"id": "1903.02196", "submitter": "Pramuditha Perera", "authors": "Pramuditha Perera and Vishal M. Patel", "title": "Deep Transfer Learning for Multiple Class Novelty Detection", "comments": "CVPR 2019 Accepted Paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a transfer learning-based solution for the problem of multiple\nclass novelty detection. In particular, we propose an end-to-end deep-learning\nbased approach in which we investigate how the knowledge contained in an\nexternal, out-of-distributional dataset can be used to improve the performance\nof a deep network for visual novelty detection. Our solution differs from the\nstandard deep classification networks on two accounts. First, we use a novel\nloss function, membership loss, in addition to the classical cross-entropy loss\nfor training networks. Secondly, we use the knowledge from the external dataset\nmore effectively to learn globally negative filters, filters that respond to\ngeneric objects outside the known class set. We show that thresholding the\nmaximal activation of the proposed network can be used to identify novel\nobjects effectively. Extensive experiments on four publicly available novelty\ndetection datasets show that the proposed method achieves significant\nimprovements over the state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Wed, 6 Mar 2019 06:36:57 GMT"}], "update_date": "2019-03-07", "authors_parsed": [["Perera", "Pramuditha", ""], ["Patel", "Vishal M.", ""]]}, {"id": "1903.02204", "submitter": "Guangfeng Lin", "authors": "Guangfeng Lin and Wanjun Chen and Kaiyang Liao and Xiaobing Kang and\n  Caixia Fan", "title": "Transfer feature generating networks with semantic classes structure for\n  zero-shot learning", "comments": null, "journal-ref": "IEEE Access,2019", "doi": "10.1109/ACCESS.2019.2958052", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Feature generating networks face to the most important question, which is the\nfitting difference (inconsistence) of the distribution between the generated\nfeature and the real data. This inconsistence further influence the performance\nof the networks model, because training samples from seen classes is disjointed\nwith testing samples from unseen classes in zero-shot learning (ZSL). In\ngeneralization zero-shot learning (GZSL), testing samples come from not only\nseen classes but also unseen classes for closer to the practical situation.\nTherefore, most of feature generating networks difficultly obtain satisfactory\nperformance for the challenging GZSL by adversarial learning the distribution\nof semantic classes. To alleviate the negative influence of this inconsistence\nfor ZSL and GZSL, transfer feature generating networks with semantic classes\nstructure (TFGNSCS) is proposed to construct networks model for improving the\nperformance of ZSL and GZSL. TFGNSCS can not only consider the semantic\nstructure relationship between seen and unseen classes, but also learn the\ndifference of generating features by transferring classification model\ninformation from seen to unseen classes in networks. The proposed method can\nintegrate the transfer loss, the classification loss and the Wasserstein\ndistance loss to generate enough CNN features, on which softmax classifiers are\ntrained for ZSL and GZSL. Experiments demonstrate that the performance of\nTFGNSCS outperforms that of the state of the arts on four challenging datasets,\nwhich are CUB,FLO,SUN, AWA in GZSL.\n", "versions": [{"version": "v1", "created": "Wed, 6 Mar 2019 06:56:48 GMT"}, {"version": "v2", "created": "Sat, 20 Jul 2019 08:36:15 GMT"}], "update_date": "2019-12-11", "authors_parsed": [["Lin", "Guangfeng", ""], ["Chen", "Wanjun", ""], ["Liao", "Kaiyang", ""], ["Kang", "Xiaobing", ""], ["Fan", "Caixia", ""]]}, {"id": "1903.02225", "submitter": "Mkhuseli Ngxande", "authors": "Mkhuseli Ngxande, Jules-Raymond Tapamo, Michael Burke", "title": "DepthwiseGANs: Fast Training Generative Adversarial Networks for\n  Realistic Image Synthesis", "comments": "6 pages, 8 figures, To appear in the Proceedings of Southern African\n  Universities Power EngineeringConference/Robotics and Mechatronics/Pattern\n  Recognition Association of South Africa(SAUPEC/RobMech/PRASA), January 20-30\n  2019, Bloemfotein, South Africa", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recent work has shown significant progress in the direction of synthetic data\ngeneration using Generative Adversarial Networks (GANs). GANs have been applied\nin many fields of computer vision including text-to-image conversion, domain\ntransfer, super-resolution, and image-to-video applications. In computer\nvision, traditional GANs are based on deep convolutional neural networks.\nHowever, deep convolutional neural networks can require extensive computational\nresources because they are based on multiple operations performed by\nconvolutional layers, which can consist of millions of trainable parameters.\nTraining a GAN model can be difficult and it takes a significant amount of time\nto reach an equilibrium point. In this paper, we investigate the use of\ndepthwise separable convolutions to reduce training time while maintaining data\ngeneration performance. Our results show that a DepthwiseGAN architecture can\ngenerate realistic images in shorter training periods when compared to a\nStarGan architecture, but that model capacity still plays a significant role in\ngenerative modelling. In addition, we show that depthwise separable\nconvolutions perform best when only applied to the generator. For quality\nevaluation of generated images, we use the Fr\\'echet Inception Distance (FID),\nwhich compares the similarity between the generated image distribution and that\nof the training dataset.\n", "versions": [{"version": "v1", "created": "Wed, 6 Mar 2019 07:56:49 GMT"}], "update_date": "2019-03-07", "authors_parsed": [["Ngxande", "Mkhuseli", ""], ["Tapamo", "Jules-Raymond", ""], ["Burke", "Michael", ""]]}, {"id": "1903.02232", "submitter": "Xun Xu", "authors": "Kaimo Lin, Nianjuan Jiang, Loong Fah Cheong, Jiangbo Lu, Xun Xu", "title": "Robust Video Background Identification by Dominant Rigid Motion\n  Estimation", "comments": "Asian Conference on Computer Vision 2018 (ACCV2018)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ability to identify the static background in videos captured by a moving\ncamera is an important pre-requisite for many video applications (e.g. video\nstabilization, stitching, and segmentation). Existing methods usually face\ndifficulties when the foreground objects occupy a larger area than the\nbackground in the image. Many methods also cannot scale up to handle densely\nsampled feature trajectories. In this paper, we propose an efficient\nlocal-to-global method to identify background, based on the assumption that as\nlong as there is sufficient camera motion, the cumulative background features\nwill have the largest amount of trajectories. Our motion model at the two-frame\nlevel is based on the epipolar geometry so that there will be no\nover-segmentation problem, another issue that plagues the 2D motion\nsegmentation approach. Foreground objects erroneously labelled due to\nintermittent motions are also taken care of by checking their global\nconsistency with the final estimated background motion. Lastly, by virtue of\nits efficiency, our method can deal with densely sampled trajectories. It\noutperforms several state-of-the-art motion segmentation methods on public\ndatasets, both quantitatively and qualitatively.\n", "versions": [{"version": "v1", "created": "Wed, 6 Mar 2019 08:11:12 GMT"}], "update_date": "2019-03-07", "authors_parsed": [["Lin", "Kaimo", ""], ["Jiang", "Nianjuan", ""], ["Cheong", "Loong Fah", ""], ["Lu", "Jiangbo", ""], ["Xu", "Xun", ""]]}, {"id": "1903.02236", "submitter": "Eissa Alreshidi Dr.", "authors": "Eissa Jaber Alreshidi and Mohammad Bilal", "title": "Characterizing Human Behaviours Using Statistical Motion Descriptor", "comments": null, "journal-ref": "Signal & Image Processing: An International Journal (SIPIJ) 2019", "doi": "10.5121/sipij.2019.10102", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Identifying human behaviors is a challenging research problem due to the\ncomplexity and variation of appearances and postures, the variation of camera\nsettings, and view angles. In this paper, we try to address the problem of\nhuman behavior identification by introducing a novel motion descriptor based on\nstatistical features. The method first divide the video into N number of\ntemporal segments. Then for each segment, we compute dense optical flow, which\nprovides instantaneous velocity information for all the pixels. We then compute\nHistogram of Optical Flow (HOOF) weighted by the norm and quantized into 32\nbins. We then compute statistical features from the obtained HOOF forming a\ndescriptor vector of 192- dimensions. We then train a non-linear multi-class\nSVM that classify different human behaviors with the accuracy of 72.1%. We\nevaluate our method by using publicly available human action data set.\nExperimental results shows that our proposed method out performs state of the\nart methods.\n", "versions": [{"version": "v1", "created": "Wed, 6 Mar 2019 08:19:49 GMT"}], "update_date": "2019-03-08", "authors_parsed": [["Alreshidi", "Eissa Jaber", ""], ["Bilal", "Mohammad", ""]]}, {"id": "1903.02240", "submitter": "Namhyuk Ahn", "authors": "Namhyuk Ahn, Byungkon Kang, Kyung-Ah Sohn", "title": "Efficient Deep Neural Network for Photo-realistic Image Super-Resolution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent progress in the deep learning-based models has improved\nphoto-realistic (or perceptual) single-image super-resolution significantly.\nHowever, despite their powerful performance, many methods are difficult to\napply to real-world applications because of the heavy computational\nrequirements. To facilitate the use of a deep model under such demands, we\nfocus on keeping the network efficient while maintaining its performance. In\ndetail, we design an architecture that implements a cascading mechanism on a\nresidual network to boost the performance with limited resources via\nmulti-level feature fusion. In addition, our proposed model adopts group\nconvolution and recursive scheme in order to achieve extreme efficiency. We\nfurther improve the perceptual quality of the output by employing the\nadversarial learning paradigm and a multi-scale discriminator approach. The\nperformance of our method is investigated through extensive internal\nexperiments and benchmark using various datasets. Our results show that our\nmodels outperform the recent methods with similar complexity, for both\ntraditional pixel-based and perception-based tasks.\n", "versions": [{"version": "v1", "created": "Wed, 6 Mar 2019 08:33:04 GMT"}, {"version": "v2", "created": "Mon, 1 Jul 2019 07:19:29 GMT"}, {"version": "v3", "created": "Wed, 15 Jul 2020 18:05:29 GMT"}], "update_date": "2020-07-17", "authors_parsed": [["Ahn", "Namhyuk", ""], ["Kang", "Byungkon", ""], ["Sohn", "Kyung-Ah", ""]]}, {"id": "1903.02252", "submitter": "Arjun Akula", "authors": "Arjun R Akula, Song-Chun Zhu", "title": "Visual Discourse Parsing", "comments": null, "journal-ref": "CVPR 2019 Workshop on Language and Vision", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Text-level discourse parsing aims to unmask how two segments (or sentences)\nin the text are related to each other. We propose the task of Visual Discourse\nParsing, which requires understanding discourse relations among scenes in a\nvideo. Here we use the term scene to refer to a subset of video frames that can\nbetter summarize the video. In order to collect a dataset for learning\ndiscourse cues from videos, one needs to manually identify the scenes from a\nlarge pool of video frames and then annotate the discourse relations between\nthem. This is clearly a time consuming, expensive and tedious task. In this\nwork, we propose an approach to identify discourse cues from the videos without\nthe need to explicitly identify and annotate the scenes. We also present a\nnovel dataset containing 310 videos and the corresponding discourse cues to\nevaluate our approach. We believe that many of the multi-discipline Artificial\nIntelligence problems such as Visual Dialog and Visual Storytelling would\ngreatly benefit from the use of visual discourse cues.\n", "versions": [{"version": "v1", "created": "Wed, 6 Mar 2019 09:09:47 GMT"}, {"version": "v2", "created": "Wed, 13 Mar 2019 21:39:16 GMT"}], "update_date": "2019-05-17", "authors_parsed": [["Akula", "Arjun R", ""], ["Zhu", "Song-Chun", ""]]}, {"id": "1903.02271", "submitter": "Michael Tschannen", "authors": "Mario Lucic, Michael Tschannen, Marvin Ritter, Xiaohua Zhai, Olivier\n  Bachem, Sylvain Gelly", "title": "High-Fidelity Image Generation With Fewer Labels", "comments": "Mario Lucic, Michael Tschannen, and Marvin Ritter contributed equally\n  to this work. ICML 2019 camera-ready version. Code available at\n  https://github.com/google/compare_gan", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep generative models are becoming a cornerstone of modern machine learning.\nRecent work on conditional generative adversarial networks has shown that\nlearning complex, high-dimensional distributions over natural images is within\nreach. While the latest models are able to generate high-fidelity, diverse\nnatural images at high resolution, they rely on a vast quantity of labeled\ndata. In this work we demonstrate how one can benefit from recent work on self-\nand semi-supervised learning to outperform the state of the art on both\nunsupervised ImageNet synthesis, as well as in the conditional setting. In\nparticular, the proposed approach is able to match the sample quality (as\nmeasured by FID) of the current state-of-the-art conditional model BigGAN on\nImageNet using only 10% of the labels and outperform it using 20% of the\nlabels.\n", "versions": [{"version": "v1", "created": "Wed, 6 Mar 2019 09:52:49 GMT"}, {"version": "v2", "created": "Tue, 14 May 2019 15:27:42 GMT"}], "update_date": "2019-05-15", "authors_parsed": [["Lucic", "Mario", ""], ["Tschannen", "Michael", ""], ["Ritter", "Marvin", ""], ["Zhai", "Xiaohua", ""], ["Bachem", "Olivier", ""], ["Gelly", "Sylvain", ""]]}, {"id": "1903.02306", "submitter": "Isabel Funke", "authors": "Isabel Funke and S\\\"oren Torge Mees and J\\\"urgen Weitz and Stefanie\n  Speidel", "title": "Video-based surgical skill assessment using 3D convolutional neural\n  networks", "comments": "IPCAI 2019/ IJCARS", "journal-ref": "IJCARS 14.7 (2019) pp. 1217-1225", "doi": "10.1007/s11548-019-01995-1", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Purpose: A profound education of novice surgeons is crucial to ensure that\nsurgical interventions are effective and safe. One important aspect is the\nteaching of technical skills for minimally invasive or robot-assisted\nprocedures. This includes the objective and preferably automatic assessment of\nsurgical skill. Recent studies presented good results for automatic, objective\nskill evaluation by collecting and analyzing motion data such as trajectories\nof surgical instruments. However, obtaining the motion data generally requires\nadditional equipment for instrument tracking or the availability of a robotic\nsurgery system to capture kinematic data. In contrast, we investigate a method\nfor automatic, objective skill assessment that requires video data only. This\nhas the advantage that video can be collected effortlessly during minimally\ninvasive and robot-assisted training scenarios.\n  Methods: Our method builds on recent advances in deep learning-based video\nclassification. Specifically, we propose to use an inflated 3D ConvNet to\nclassify snippets, i.e., stacks of a few consecutive frames, extracted from\nsurgical video. The network is extended into a Temporal Segment Network during\ntraining.\n  Results: We evaluate the method on the publicly available JIGSAWS dataset,\nwhich consists of recordings of basic robot-assisted surgery tasks performed on\na dry lab bench-top model. Our approach achieves high skill classification\naccuracies ranging from 95.1% to 100.0%.\n  Conclusions: Our results demonstrate the feasibility of deep learning-based\nassessment of technical skill from surgical video. Notably, the 3D ConvNet is\nable to learn meaningful patterns directly from the data, alleviating the need\nfor manual feature engineering. Further evaluation will require more annotated\ndata for training and testing.\n", "versions": [{"version": "v1", "created": "Wed, 6 Mar 2019 10:54:09 GMT"}, {"version": "v2", "created": "Wed, 15 May 2019 15:17:51 GMT"}, {"version": "v3", "created": "Wed, 4 Sep 2019 12:49:50 GMT"}], "update_date": "2019-09-05", "authors_parsed": [["Funke", "Isabel", ""], ["Mees", "S\u00f6ren Torge", ""], ["Weitz", "J\u00fcrgen", ""], ["Speidel", "Stefanie", ""]]}, {"id": "1903.02330", "submitter": "Muhammed Kocabas", "authors": "Muhammed Kocabas and Salih Karagoz and Emre Akbas", "title": "Self-Supervised Learning of 3D Human Pose using Multi-view Geometry", "comments": "CVPR 2019 camera ready. Code is available at\n  https://github.com/mkocabas/EpipolarPose", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Training accurate 3D human pose estimators requires large amount of 3D\nground-truth data which is costly to collect. Various weakly or self supervised\npose estimation methods have been proposed due to lack of 3D data.\nNevertheless, these methods, in addition to 2D ground-truth poses, require\neither additional supervision in various forms (e.g. unpaired 3D ground truth\ndata, a small subset of labels) or the camera parameters in multiview settings.\nTo address these problems, we present EpipolarPose, a self-supervised learning\nmethod for 3D human pose estimation, which does not need any 3D ground-truth\ndata or camera extrinsics. During training, EpipolarPose estimates 2D poses\nfrom multi-view images, and then, utilizes epipolar geometry to obtain a 3D\npose and camera geometry which are subsequently used to train a 3D pose\nestimator. We demonstrate the effectiveness of our approach on standard\nbenchmark datasets i.e. Human3.6M and MPI-INF-3DHP where we set the new\nstate-of-the-art among weakly/self-supervised methods. Furthermore, we propose\na new performance measure Pose Structure Score (PSS) which is a scale\ninvariant, structure aware measure to evaluate the structural plausibility of a\npose with respect to its ground truth. Code and pretrained models are available\nat https://github.com/mkocabas/EpipolarPose\n", "versions": [{"version": "v1", "created": "Wed, 6 Mar 2019 11:41:32 GMT"}, {"version": "v2", "created": "Tue, 9 Apr 2019 06:40:57 GMT"}], "update_date": "2019-04-10", "authors_parsed": [["Kocabas", "Muhammed", ""], ["Karagoz", "Salih", ""], ["Akbas", "Emre", ""]]}, {"id": "1903.02351", "submitter": "Chi Zhang", "authors": "Chi Zhang, Guosheng Lin, Fayao Liu, Rui Yao, Chunhua Shen", "title": "CANet: Class-Agnostic Segmentation Networks with Iterative Refinement\n  and Attentive Few-Shot Learning", "comments": "Accepted to CVPR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent progress in semantic segmentation is driven by deep Convolutional\nNeural Networks and large-scale labeled image datasets. However, data labeling\nfor pixel-wise segmentation is tedious and costly. Moreover, a trained model\ncan only make predictions within a set of pre-defined classes. In this paper,\nwe present CANet, a class-agnostic segmentation network that performs few-shot\nsegmentation on new classes with only a few annotated images available. Our\nnetwork consists of a two-branch dense comparison module which performs\nmulti-level feature comparison between the support image and the query image,\nand an iterative optimization module which iteratively refines the predicted\nresults. Furthermore, we introduce an attention mechanism to effectively fuse\ninformation from multiple support examples under the setting of k-shot\nlearning. Experiments on PASCAL VOC 2012 show that our method achieves a mean\nIntersection-over-Union score of 55.4% for 1-shot segmentation and 57.1% for\n5-shot segmentation, outperforming state-of-the-art methods by a large margin\nof 14.6% and 13.2%, respectively.\n", "versions": [{"version": "v1", "created": "Wed, 6 Mar 2019 13:10:28 GMT"}], "update_date": "2019-03-07", "authors_parsed": [["Zhang", "Chi", ""], ["Lin", "Guosheng", ""], ["Liu", "Fayao", ""], ["Yao", "Rui", ""], ["Shen", "Chunhua", ""]]}, {"id": "1903.02358", "submitter": "Jiasong Wu", "authors": "Jiasong Wu, Hongshan Ren, Youyong Kong, Chunfeng Yang, Lotfi Senhadji,\n  Huazhong Shu", "title": "Compressing complex convolutional neural network based on an improved\n  deep compression algorithm", "comments": "5 pages, 2 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although convolutional neural network (CNN) has made great progress, large\nredundant parameters restrict its deployment on embedded devices, especially\nmobile devices. The recent compression works are focused on real-value\nconvolutional neural network (Real CNN), however, to our knowledge, there is no\nattempt for the compression of complex-value convolutional neural network\n(Complex CNN). Compared with the real-valued network, the complex-value neural\nnetwork is easier to optimize, generalize, and has better learning potential.\nThis paper extends the commonly used deep compression algorithm from real\ndomain to complex domain and proposes an improved deep compression algorithm\nfor the compression of Complex CNN. The proposed algorithm compresses the\nnetwork about 8 times on CIFAR-10 dataset with less than 3% accuracy loss. On\nthe ImageNet dataset, our method compresses the model about 16 times and the\naccuracy loss is about 2% without retraining.\n", "versions": [{"version": "v1", "created": "Wed, 6 Mar 2019 13:20:36 GMT"}], "update_date": "2019-03-07", "authors_parsed": [["Wu", "Jiasong", ""], ["Ren", "Hongshan", ""], ["Kong", "Youyong", ""], ["Yang", "Chunfeng", ""], ["Senhadji", "Lotfi", ""], ["Shu", "Huazhong", ""]]}, {"id": "1903.02489", "submitter": "Alexandre Gari\\'epy", "authors": "Alexandre Gari\\'epy, Jean-Christophe Ruel, Brahim Chaib-draa and\n  Philippe Gigu\\`ere", "title": "GQ-STN: Optimizing One-Shot Grasp Detection based on Robustness\n  Classifier", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Grasping is a fundamental robotic task needed for the deployment of household\nrobots or furthering warehouse automation. However, few approaches are able to\nperform grasp detection in real time (frame rate). To this effect, we present\nGrasp Quality Spatial Transformer Network (GQ-STN), a one-shot grasp detection\nnetwork. Being based on the Spatial Transformer Network (STN), it produces not\nonly a grasp configuration, but also directly outputs a depth image centered at\nthis configuration. By connecting our architecture to an externally-trained\ngrasp robustness evaluation network, we can train efficiently to satisfy a\nrobustness metric via the backpropagation of the gradient emanating from the\nevaluation network. This removes the difficulty of training detection networks\non sparsely annotated databases, a common issue in grasping. We further propose\nto use this robustness classifier to compare approaches, being more reliable\nthan the traditional rectangle metric. Our GQ-STN is able to detect robust\ngrasps on the depth images of the Dex-Net 2.0 dataset with 92.4 % accuracy in a\nsingle pass of the network. We finally demonstrate in a physical benchmark that\nour method can propose robust grasps more often than previous sampling-based\nmethods, while being more than 60 times faster.\n", "versions": [{"version": "v1", "created": "Wed, 6 Mar 2019 16:53:46 GMT"}, {"version": "v2", "created": "Thu, 1 Aug 2019 00:56:36 GMT"}], "update_date": "2019-08-02", "authors_parsed": [["Gari\u00e9py", "Alexandre", ""], ["Ruel", "Jean-Christophe", ""], ["Chaib-draa", "Brahim", ""], ["Gigu\u00e8re", "Philippe", ""]]}, {"id": "1903.02494", "submitter": "Guolei Sun", "authors": "Hisham Cholakkal, Guolei Sun, Fahad Shahbaz Khan, Ling Shao", "title": "Object Counting and Instance Segmentation with Image-level Supervision", "comments": "The first two authors have equal contribution. To appear in CVPR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Common object counting in a natural scene is a challenging problem in\ncomputer vision with numerous real-world applications. Existing image-level\nsupervised common object counting approaches only predict the global object\ncount and rely on additional instance-level supervision to also determine\nobject locations. We propose an image-level supervised approach that provides\nboth the global object count and the spatial distribution of object instances\nby constructing an object category density map. Motivated by psychological\nstudies, we further reduce image-level supervision using a limited object count\ninformation (up to four). To the best of our knowledge, we are the first to\npropose image-level supervised density map estimation for common object\ncounting and demonstrate its effectiveness in image-level supervised instance\nsegmentation. Comprehensive experiments are performed on the PASCAL VOC and\nCOCO datasets. Our approach outperforms existing methods, including those using\ninstance-level supervision, on both datasets for common object counting.\nMoreover, our approach improves state-of-the-art image-level supervised\ninstance segmentation with a relative gain of 17.8% in terms of average best\noverlap, on the PASCAL VOC 2012 dataset. Code link:\nhttps://github.com/GuoleiSun/CountSeg\n", "versions": [{"version": "v1", "created": "Wed, 6 Mar 2019 17:06:51 GMT"}, {"version": "v2", "created": "Mon, 13 May 2019 05:41:30 GMT"}], "update_date": "2019-05-14", "authors_parsed": [["Cholakkal", "Hisham", ""], ["Sun", "Guolei", ""], ["Khan", "Fahad Shahbaz", ""], ["Shao", "Ling", ""]]}, {"id": "1903.02495", "submitter": "Lakshmanan Nataraj", "authors": "Jawadul H. Bappy, Cody Simons, Lakshmanan Nataraj, B.S. Manjunath,\n  Amit K. Roy-Chowdhury", "title": "Hybrid LSTM and Encoder-Decoder Architecture for Detection of Image\n  Forgeries", "comments": null, "journal-ref": null, "doi": "10.1109/TIP.2019.2895466", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With advanced image journaling tools, one can easily alter the semantic\nmeaning of an image by exploiting certain manipulation techniques such as\ncopy-clone, object splicing, and removal, which mislead the viewers. In\ncontrast, the identification of these manipulations becomes a very challenging\ntask as manipulated regions are not visually apparent. This paper proposes a\nhigh-confidence manipulation localization architecture which utilizes\nresampling features, Long-Short Term Memory (LSTM) cells, and encoder-decoder\nnetwork to segment out manipulated regions from non-manipulated ones.\nResampling features are used to capture artifacts like JPEG quality loss,\nupsampling, downsampling, rotation, and shearing. The proposed network exploits\nlarger receptive fields (spatial maps) and frequency domain correlation to\nanalyze the discriminative characteristics between manipulated and\nnon-manipulated regions by incorporating encoder and LSTM network. Finally,\ndecoder network learns the mapping from low-resolution feature maps to\npixel-wise predictions for image tamper localization. With predicted mask\nprovided by final layer (softmax) of the proposed architecture, end-to-end\ntraining is performed to learn the network parameters through back-propagation\nusing ground-truth masks. Furthermore, a large image splicing dataset is\nintroduced to guide the training process. The proposed method is capable of\nlocalizing image manipulations at pixel level with high precision, which is\ndemonstrated through rigorous experimentation on three diverse datasets.\n", "versions": [{"version": "v1", "created": "Wed, 6 Mar 2019 17:09:46 GMT"}], "update_date": "2019-06-26", "authors_parsed": [["Bappy", "Jawadul H.", ""], ["Simons", "Cody", ""], ["Nataraj", "Lakshmanan", ""], ["Manjunath", "B. S.", ""], ["Roy-Chowdhury", "Amit K.", ""]]}, {"id": "1903.02499", "submitter": "Sen He", "authors": "Sen He, Hamed R. Tavakoli, Ali Borji, Nicolas Pugeault", "title": "Human Attention in Image Captioning: Dataset and Analysis", "comments": "To appear at ICCV 2019", "journal-ref": "IEEE International Conference on Computer Vision (ICCV 2019)", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this work, we present a novel dataset consisting of eye movements and\nverbal descriptions recorded synchronously over images. Using this data, we\nstudy the differences in human attention during free-viewing and image\ncaptioning tasks. We look into the relationship between human attention and\nlanguage constructs during perception and sentence articulation. We also\nanalyse attention deployment mechanisms in the top-down soft attention approach\nthat is argued to mimic human attention in captioning tasks, and investigate\nwhether visual saliency can help image captioning. Our study reveals that (1)\nhuman attention behaviour differs in free-viewing and image description tasks.\nHumans tend to fixate on a greater variety of regions under the latter task,\n(2) there is a strong relationship between described objects and attended\nobjects ($97\\%$ of the described objects are being attended), (3) a\nconvolutional neural network as feature encoder accounts for human-attended\nregions during image captioning to a great extent (around $78\\%$), (4)\nsoft-attention mechanism differs from human attention, both spatially and\ntemporally, and there is low correlation between caption scores and attention\nconsistency scores. These indicate a large gap between humans and machines in\nregards to top-down attention, and (5) by integrating the soft attention model\nwith image saliency, we can significantly improve the model's performance on\nFlickr30k and MSCOCO benchmarks. The dataset can be found at:\nhttps://github.com/SenHe/Human-Attention-in-Image-Captioning.\n", "versions": [{"version": "v1", "created": "Wed, 6 Mar 2019 17:15:49 GMT"}, {"version": "v2", "created": "Mon, 5 Aug 2019 13:02:05 GMT"}, {"version": "v3", "created": "Wed, 7 Aug 2019 08:44:21 GMT"}], "update_date": "2019-08-08", "authors_parsed": [["He", "Sen", ""], ["Tavakoli", "Hamed R.", ""], ["Borji", "Ali", ""], ["Pugeault", "Nicolas", ""]]}, {"id": "1903.02500", "submitter": "Huitong Pan", "authors": "Huitong Pan, Yushan Feng, Quan Chen, Craig Meyer and Xue Feng", "title": "Prostate Segmentation from 3D MRI Using a Two-Stage Model and\n  Variable-Input Based Uncertainty Measure", "comments": "IEEE International Symposium on Biomedical Imaging (ISBI) 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a two-stage segmentation model, variable-input based\nuncertainty measures and an uncertainty-guided post-processing method for\nprostate segmentation on 3D magnetic resonance images (MRI). The two-stage\nmodel was based on 3D dilated U-Nets with the first stage to localize the\nprostate and the second stage to obtain an accurate segmentation from cropped\nimages. For data augmentation, we proposed the variable-input method which\ncrops the region of interest with additional random variations. Similar to\nother deep learning models, the proposed model also faced the challenge of\nsuboptimal performance in certain testing cases due to varied training and\ntesting image characteristics. Therefore, it is valuable to evaluate the\nconfidence and performance of the network using uncertainty measures, which are\noften calculated from the probability maps or their standard deviations with\nmultiple model outputs for the same testing case. However, few studies have\nquantitatively compared different methods of uncertainty calculation.\nFurthermore, unlike the commonly used Bayesian dropout during testing, we\ndeveloped uncertainty measures based on the variable input images at the second\nstage and evaluated its performance by calculating the correlation with\nground-truth-based performance metrics, such as Dice score. For performance\nestimation, we predicted Dice scores and Hausdorff distance with the most\ncorrelated uncertainty measure. For post-processing, we performed Gaussian\nfilter on the underperformed slices to improve segmentation quality. Using\nPROMISE-12 data, we demonstrated the robustness of the two-stage model and\nshowed high correlation of the proposed variable-input based uncertainty\nmeasures with GT-based performance. The uncertainty-guided post-processing\nmethod significantly improved label smoothness.\n", "versions": [{"version": "v1", "created": "Wed, 6 Mar 2019 17:18:33 GMT"}], "update_date": "2019-03-07", "authors_parsed": [["Pan", "Huitong", ""], ["Feng", "Yushan", ""], ["Chen", "Quan", ""], ["Meyer", "Craig", ""], ["Feng", "Xue", ""]]}, {"id": "1903.02501", "submitter": "Sen He", "authors": "Sen He, Hamed R. Tavakoli, Ali Borji, Yang Mi, and Nicolas Pugeault", "title": "Understanding and Visualizing Deep Visual Saliency Models", "comments": "To appear in CVPR2019, camera ready version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recently, data-driven deep saliency models have achieved high performance and\nhave outperformed classical saliency models, as demonstrated by results on\ndatasets such as the MIT300 and SALICON. Yet, there remains a large gap between\nthe performance of these models and the inter-human baseline. Some outstanding\nquestions include what have these models learned, how and where they fail, and\nhow they can be improved. This article attempts to answer these questions by\nanalyzing the representations learned by individual neurons located at the\nintermediate layers of deep saliency models. To this end, we follow the steps\nof existing deep saliency models, that is borrowing a pre-trained model of\nobject recognition to encode the visual features and learning a decoder to\ninfer the saliency. We consider two cases when the encoder is used as a fixed\nfeature extractor and when it is fine-tuned, and compare the inner\nrepresentations of the network. To study how the learned representations depend\non the task, we fine-tune the same network using the same image set but for two\ndifferent tasks: saliency prediction versus scene classification. Our analyses\nreveal that: 1) some visual regions (e.g. head, text, symbol, vehicle) are\nalready encoded within various layers of the network pre-trained for object\nrecognition, 2) using modern datasets, we find that fine-tuning pre-trained\nmodels for saliency prediction makes them favor some categories (e.g. head)\nover some others (e.g. text), 3) although deep models of saliency outperform\nclassical models on natural images, the converse is true for synthetic stimuli\n(e.g. pop-out search arrays), an evidence of significant difference between\nhuman and data-driven saliency models, and 4) we confirm that, after-fine\ntuning, the change in inner-representations is mostly due to the task and not\nthe domain shift in the data.\n", "versions": [{"version": "v1", "created": "Wed, 6 Mar 2019 17:21:04 GMT"}, {"version": "v2", "created": "Wed, 13 Mar 2019 11:43:04 GMT"}, {"version": "v3", "created": "Wed, 3 Apr 2019 10:28:07 GMT"}], "update_date": "2019-04-04", "authors_parsed": [["He", "Sen", ""], ["Tavakoli", "Hamed R.", ""], ["Borji", "Ali", ""], ["Mi", "Yang", ""], ["Pugeault", "Nicolas", ""]]}, {"id": "1903.02507", "submitter": "Jiayun Li", "authors": "Jiayun Li, Mohammad K. Ebrahimpour, Azadeh Moghtaderi, Yen-Yun Yu", "title": "Image captioning with weakly-supervised attention penalty", "comments": "10 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stories are essential for genealogy research since they can help build\nemotional connections with people. A lot of family stories are reserved in\nhistorical photos and albums. Recent development on image captioning models\nmakes it feasible to \"tell stories\" for photos automatically. The attention\nmechanism has been widely adopted in many state-of-the-art encoder-decoder\nbased image captioning models, since it can bridge the gap between the visual\npart and the language part. Most existing captioning models implicitly trained\nattention modules with word-likelihood loss. Meanwhile, lots of studies have\ninvestigated intrinsic attentions for visual models using gradient-based\napproaches. Ideally, attention maps predicted by captioning models should be\nconsistent with intrinsic attentions from visual models for any given visual\nconcept. However, no work has been done to align implicitly learned attention\nmaps with intrinsic visual attentions. In this paper, we proposed a novel model\nthat measured consistency between captioning predicted attentions and intrinsic\nvisual attentions. This alignment loss allows explicit attention correction\nwithout using any expensive bounding box annotations. We developed and\nevaluated our model on COCO dataset as well as a genealogical dataset from\nAncestry.com Operations Inc., which contains billions of historical photos. The\nproposed model achieved better performances on all commonly used language\nevaluation metrics for both datasets.\n", "versions": [{"version": "v1", "created": "Wed, 6 Mar 2019 17:32:44 GMT"}], "update_date": "2019-03-07", "authors_parsed": [["Li", "Jiayun", ""], ["Ebrahimpour", "Mohammad K.", ""], ["Moghtaderi", "Azadeh", ""], ["Yu", "Yen-Yun", ""]]}, {"id": "1903.02511", "submitter": "Miguel Vasco", "authors": "Miguel Vasco, Francisco S. Melo, David Martins de Matos, Ana Paiva,\n  Tetsunari Inamura", "title": "Learning multimodal representations for sample-efficient recognition of\n  human actions", "comments": "7 pages, 6 figures, submitted to 2019 IEEE/RSJ International\n  Conference on Intelligent Robots and Systems (IROS)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Humans interact in rich and diverse ways with the environment. However, the\nrepresentation of such behavior by artificial agents is often limited. In this\nwork we present \\textit{motion concepts}, a novel multimodal representation of\nhuman actions in a household environment. A motion concept encompasses a\nprobabilistic description of the kinematics of the action along with its\ncontextual background, namely the location and the objects held during the\nperformance. Furthermore, we present Online Motion Concept Learning (OMCL), a\nnew algorithm which learns novel motion concepts from action demonstrations and\nrecognizes previously learned motion concepts. The algorithm is evaluated on a\nvirtual-reality household environment with the presence of a human avatar. OMCL\noutperforms standard motion recognition algorithms on an one-shot recognition\ntask, attesting to its potential for sample-efficient recognition of human\nactions.\n", "versions": [{"version": "v1", "created": "Wed, 6 Mar 2019 17:37:21 GMT"}], "update_date": "2019-03-07", "authors_parsed": [["Vasco", "Miguel", ""], ["Melo", "Francisco S.", ""], ["de Matos", "David Martins", ""], ["Paiva", "Ana", ""], ["Inamura", "Tetsunari", ""]]}, {"id": "1903.02521", "submitter": "Aritra Chowdhury", "authors": "Aritra Chowdhury, Malik Magdin-Ismail, Bulent Yener", "title": "Quantifying error contributions of computational steps, algorithms and\n  hyperparameter choices in image classification pipelines", "comments": "arXiv admin note: substantial text overlap with arXiv:1903.00405", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data science relies on pipelines that are organized in the form of\ninterdependent computational steps. Each step consists of various candidate\nalgorithms that maybe used for performing a particular function. Each algorithm\nconsists of several hyperparameters. Algorithms and hyperparameters must be\noptimized as a whole to produce the best performance. Typical machine learning\npipelines typically consist of complex algorithms in each of the steps. Not\nonly is the selection process combinatorial, but it is also important to\ninterpret and understand the pipelines. We propose a method to quantify the\nimportance of different layers in the pipeline, by computing an error\ncontribution relative to an agnostic choice of algorithms in that layer. We\ndemonstrate our methodology on image classification pipelines. The agnostic\nmethodology quantifies the error contributions from the computational steps,\nalgorithms and hyperparameters in the image classification pipeline. We show\nthat algorithm selection and hyper-parameter optimization methods can be used\nto quantify the error contribution and that random search is able to quantify\nthe contribution more accurately than Bayesian optimization. This methodology\ncan be used by domain experts to understand machine learning and data analysis\npipelines in terms of their individual components, which can help in\nprioritizing different components of the pipeline.\n", "versions": [{"version": "v1", "created": "Mon, 25 Feb 2019 19:16:58 GMT"}], "update_date": "2019-03-07", "authors_parsed": [["Chowdhury", "Aritra", ""], ["Magdin-Ismail", "Malik", ""], ["Yener", "Bulent", ""]]}, {"id": "1903.02531", "submitter": "Somil Bansal", "authors": "Somil Bansal, Varun Tolani, Saurabh Gupta, Jitendra Malik, Claire\n  Tomlin", "title": "Combining Optimal Control and Learning for Visual Navigation in Novel\n  Environments", "comments": "Project website: https://vtolani95.github.io/WayPtNav/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.CV cs.LG cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Model-based control is a popular paradigm for robot navigation because it can\nleverage a known dynamics model to efficiently plan robust robot trajectories.\nHowever, it is challenging to use model-based methods in settings where the\nenvironment is a priori unknown and can only be observed partially through\non-board sensors on the robot. In this work, we address this short-coming by\ncoupling model-based control with learning-based perception. The learning-based\nperception module produces a series of waypoints that guide the robot to the\ngoal via a collision-free path. These waypoints are used by a model-based\nplanner to generate a smooth and dynamically feasible trajectory that is\nexecuted on the physical system using feedback control. Our experiments in\nsimulated real-world cluttered environments and on an actual ground vehicle\ndemonstrate that the proposed approach can reach goal locations more reliably\nand efficiently in novel environments as compared to purely geometric\nmapping-based or end-to-end learning-based alternatives. Our approach does not\nrely on detailed explicit 3D maps of the environment, works well with low frame\nrates, and generalizes well from simulation to the real world. Videos\ndescribing our approach and experiments are available on the project website.\n", "versions": [{"version": "v1", "created": "Wed, 6 Mar 2019 18:11:32 GMT"}, {"version": "v2", "created": "Wed, 17 Jul 2019 22:32:51 GMT"}], "update_date": "2019-07-19", "authors_parsed": [["Bansal", "Somil", ""], ["Tolani", "Varun", ""], ["Gupta", "Saurabh", ""], ["Malik", "Jitendra", ""], ["Tomlin", "Claire", ""]]}, {"id": "1903.02547", "submitter": "Xiujun Li", "authors": "Liyiming Ke and Xiujun Li and Yonatan Bisk and Ari Holtzman and Zhe\n  Gan and Jingjing Liu and Jianfeng Gao and Yejin Choi and Siddhartha Srinivasa", "title": "Tactical Rewind: Self-Correction via Backtracking in Vision-and-Language\n  Navigation", "comments": "CVPR 2019 Oral, video demo: https://youtu.be/AD9TNohXoPA", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV cs.LG cs.NE cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the Frontier Aware Search with backTracking (FAST) Navigator, a\ngeneral framework for action decoding, that achieves state-of-the-art results\non the Room-to-Room (R2R) Vision-and-Language navigation challenge of Anderson\net. al. (2018). Given a natural language instruction and photo-realistic image\nviews of a previously unseen environment, the agent was tasked with navigating\nfrom source to target location as quickly as possible. While all current\napproaches make local action decisions or score entire trajectories using beam\nsearch, ours balances local and global signals when exploring an unobserved\nenvironment. Importantly, this lets us act greedily but use global signals to\nbacktrack when necessary. Applying FAST framework to existing state-of-the-art\nmodels achieved a 17% relative gain, an absolute 6% gain on Success rate\nweighted by Path Length (SPL).\n", "versions": [{"version": "v1", "created": "Wed, 6 Mar 2019 18:54:55 GMT"}, {"version": "v2", "created": "Tue, 2 Apr 2019 17:48:26 GMT"}], "update_date": "2019-04-03", "authors_parsed": [["Ke", "Liyiming", ""], ["Li", "Xiujun", ""], ["Bisk", "Yonatan", ""], ["Holtzman", "Ari", ""], ["Gan", "Zhe", ""], ["Liu", "Jingjing", ""], ["Gao", "Jianfeng", ""], ["Choi", "Yejin", ""], ["Srinivasa", "Siddhartha", ""]]}, {"id": "1903.02582", "submitter": "Tavi Halperin", "authors": "Tavi Halperin, Harel Cain, Ofir Bibi, Michael Werman", "title": "Clear Skies Ahead: Towards Real-Time Automatic Sky Replacement in Video", "comments": "Eurographics 2019. Supplementary video: https://youtu.be/1uZ46YzX-pI", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Digital videos such as those captured by a smartphone often exhibit exposure\ninconsistencies, a poorly exposed sky, or simply suffer from an uninteresting\nor plain looking sky. Professionals may edit these videos using advanced and\ntime-consuming tools unavailable to most users, to replace the sky with a more\nexpressive or imaginative sky. In this work, we propose an algorithm for\nautomatic replacement of the sky region in a video with a different sky,\nproviding nonprofessional users with a simple yet efficient tool to seamlessly\nreplace the sky. The method is fast, achieving close to real-time performance\non mobile devices and the user's involvement can remain as limited as simply\nselecting the replacement sky.\n", "versions": [{"version": "v1", "created": "Wed, 6 Mar 2019 19:03:47 GMT"}], "update_date": "2019-03-08", "authors_parsed": [["Halperin", "Tavi", ""], ["Cain", "Harel", ""], ["Bibi", "Ofir", ""], ["Werman", "Michael", ""]]}, {"id": "1903.02585", "submitter": "Guanxiong Liu", "authors": "Guanxiong Liu, Issa Khalil, Abdallah Khreishah", "title": "GanDef: A GAN based Adversarial Training Defense for Neural Network\n  Classifier", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning models, especially neural network (NN) classifiers, are\nwidely used in many applications including natural language processing,\ncomputer vision and cybersecurity. They provide high accuracy under the\nassumption of attack-free scenarios. However, this assumption has been defied\nby the introduction of adversarial examples -- carefully perturbed samples of\ninput that are usually misclassified. Many researchers have tried to develop a\ndefense against adversarial examples; however, we are still far from achieving\nthat goal. In this paper, we design a Generative Adversarial Net (GAN) based\nadversarial training defense, dubbed GanDef, which utilizes a competition game\nto regulate the feature selection during the training. We analytically show\nthat GanDef can train a classifier so it can defend against adversarial\nexamples. Through extensive evaluation on different white-box adversarial\nexamples, the classifier trained by GanDef shows the same level of test\naccuracy as those trained by state-of-the-art adversarial training defenses.\nMore importantly, GanDef-Comb, a variant of GanDef, could utilize the\ndiscriminator to achieve a dynamic trade-off between correctly classifying\noriginal and adversarial examples. As a result, it achieves the highest overall\ntest accuracy when the ratio of adversarial examples exceeds 41.7%.\n", "versions": [{"version": "v1", "created": "Wed, 6 Mar 2019 19:09:47 GMT"}], "update_date": "2019-03-08", "authors_parsed": [["Liu", "Guanxiong", ""], ["Khalil", "Issa", ""], ["Khreishah", "Abdallah", ""]]}, {"id": "1903.02639", "submitter": "Keegan Lensink", "authors": "Eldad Haber, Keegan Lensink, Eran Treister, Lars Ruthotto", "title": "IMEXnet: A Forward Stable Deep Neural Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep convolutional neural networks have revolutionized many machine learning\nand computer vision tasks, however, some remaining key challenges limit their\nwider use. These challenges include improving the network's robustness to\nperturbations of the input image and the limited ``field of view'' of\nconvolution operators. We introduce the IMEXnet that addresses these challenges\nby adapting semi-implicit methods for partial differential equations. Compared\nto similar explicit networks, such as residual networks, our network is more\nstable, which has recently shown to reduce the sensitivity to small changes in\nthe input features and improve generalization. The addition of an implicit step\nconnects all pixels in each channel of the image and therefore addresses the\nfield of view problem while still being comparable to standard convolutions in\nterms of the number of parameters and computational complexity. We also present\na new dataset for semantic segmentation and demonstrate the effectiveness of\nour architecture using the NYU Depth dataset.\n", "versions": [{"version": "v1", "created": "Wed, 6 Mar 2019 22:33:06 GMT"}, {"version": "v2", "created": "Fri, 17 May 2019 21:45:28 GMT"}], "update_date": "2019-05-21", "authors_parsed": [["Haber", "Eldad", ""], ["Lensink", "Keegan", ""], ["Treister", "Eran", ""], ["Ruthotto", "Lars", ""]]}, {"id": "1903.02665", "submitter": "Ognjen Arandjelovi\\'c PhD", "authors": "Jessica Cooper, Ognjen Arandjelovic", "title": "Understanding Ancient Coin Images", "comments": "2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, a range of problems within the broad umbrella of automatic,\ncomputer vision based analysis of ancient coins has been attracting an\nincreasing amount of attention. Notwithstanding this research effort, the\nresults achieved by the state of the art in the published literature remain\npoor and far from sufficiently well performing for any practical purpose. In\nthe present paper we present a series of contributions which we believe will\nbenefit the interested community. Firstly, we explain that the approach of\nvisual matching of coins, universally adopted in all existing published papers\non the topic, is not of practical interest because the number of ancient coin\ntypes exceeds by far the number of those types which have been imaged, be it in\ndigital form (e.g. online) or otherwise (traditional film, in print, etc.).\nRather, we argue that the focus should be on the understanding of the semantic\ncontent of coins. Hence, we describe a novel method which uses real-world\nmultimodal input to extract and associate semantic concepts with the correct\ncoin images and then using a novel convolutional neural network learn the\nappearance of these concepts. Empirical evidence on a real-world and by far the\nlargest data set of ancient coins, we demonstrate highly promising results.\n", "versions": [{"version": "v1", "created": "Thu, 7 Mar 2019 00:07:45 GMT"}, {"version": "v2", "created": "Wed, 13 Mar 2019 21:58:09 GMT"}], "update_date": "2019-03-15", "authors_parsed": [["Cooper", "Jessica", ""], ["Arandjelovic", "Ognjen", ""]]}, {"id": "1903.02678", "submitter": "Xi Shen", "authors": "Xi Shen, Alexei A. Efros and Mathieu Aubry", "title": "Discovering Visual Patterns in Art Collections with Spatially-consistent\n  Feature Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Our goal in this paper is to discover near duplicate patterns in large\ncollections of artworks. This is harder than standard instance mining due to\ndifferences in the artistic media (oil, pastel, drawing, etc), and\nimperfections inherent in the copying process. The key technical insight is to\nadapt a standard deep feature to this task by fine-tuning it on the specific\nart collection using self-supervised learning. More specifically, spatial\nconsistency between neighbouring feature matches is used as supervisory\nfine-tuning signal. The adapted feature leads to more accurate style-invariant\nmatching, and can be used with a standard discovery approach, based on\ngeometric verification, to identify duplicate patterns in the dataset. The\napproach is evaluated on several different datasets and shows surprisingly good\nqualitative discovery results. For quantitative evaluation of the method, we\nannotated 273 near duplicate details in a dataset of 1587 artworks attributed\nto Jan Brueghel and his workshop. Beyond artwork, we also demonstrate\nimprovement on localization on the Oxford5K photo dataset as well as on\nhistorical photograph localization on the Large Time Lags Location (LTLL)\ndataset.\n", "versions": [{"version": "v1", "created": "Thu, 7 Mar 2019 01:12:16 GMT"}, {"version": "v2", "created": "Fri, 8 Mar 2019 09:16:27 GMT"}], "update_date": "2019-03-11", "authors_parsed": [["Shen", "Xi", ""], ["Efros", "Alexei A.", ""], ["Aubry", "Mathieu", ""]]}, {"id": "1903.02679", "submitter": "Peyman Moghadam", "authors": "Shafeeq Elanattil, Peyman Moghadam", "title": "Synthetic Human Model Dataset for Skeleton Driven Non-rigid Motion\n  Tracking and 3D Reconstruction", "comments": "More information at\n  https://research.csiro.au/robotics/our-work/databases/synthetic-human-model-dataset/", "journal-ref": null, "doi": "10.25919/5c495488b0f4e", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a synthetic dataset for evaluating non-rigid 3D human\nreconstruction based on conventional RGB-D cameras. The dataset consist of\nseven motion sequences of a single human model. For each motion sequence\nper-frame ground truth geometry and ground truth skeleton are given. The\ndataset also contains skinning weights of the human model. More information\nabout the dataset can be found at:\nhttps://research.csiro.au/robotics/our-work/databases/synthetic-human-model-dataset/\n", "versions": [{"version": "v1", "created": "Thu, 7 Mar 2019 01:13:24 GMT"}], "update_date": "2019-03-08", "authors_parsed": [["Elanattil", "Shafeeq", ""], ["Moghadam", "Peyman", ""]]}, {"id": "1903.02688", "submitter": "Jie Chen", "authors": "Jie Chen, Lap-Pui Chau and Junhui Hou", "title": "Stratified Labeling for Surface Consistent Parallax Correction and\n  Occlusion Completion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The light field faithfully records the spatial and angular configurations of\nthe scene, which facilitates a wide range of imaging possibilities. In this\nwork, we propose an LF synthesis algorithm which renders high quality novel LF\nviews far outside the range of angular baselines of the given references. A\nstratified synthesis strategy is adopted which parses the scene content based\non stratified disparity layers and across a varying range of spatial\ngranularities. Such a stratified methodology proves to help preserve scene\nstructures over large perspective shifts, and it provides informative clues for\ninferring the textures of occluded regions. A generative-adversarial network\nmodel is further adopted for parallax correction and occlusion completion\nconditioned on the stratified synthesis features. Experiments show that our\nproposed model can provide more reliable novel view synthesis quality at large\nbaseline extension ratios. Over 3dB quality improvement has been achieved\nagainst state-of-the-art LF view synthesis algorithms.\n", "versions": [{"version": "v1", "created": "Thu, 7 Mar 2019 02:03:56 GMT"}, {"version": "v2", "created": "Sat, 23 Mar 2019 04:43:25 GMT"}], "update_date": "2019-03-26", "authors_parsed": [["Chen", "Jie", ""], ["Chau", "Lap-Pui", ""], ["Hou", "Junhui", ""]]}, {"id": "1903.02695", "submitter": "Chris Von Csefalvay", "authors": "Chris von Csefalvay", "title": "Novel quantitative indicators of digital ophthalmoscopy image quality", "comments": "12 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the advent of smartphone indirect ophthalmoscopy, teleophthalmology -\nthe use of specialist ophthalmology assets at a distance from the patient - has\nexperienced a breakthrough, promising enormous benefits especially for\nhealthcare in distant, inaccessible or opthalmologically underserved areas,\nwhere specialists are either unavailable or too few in number. However,\naccurate teleophthalmology requires high-quality ophthalmoscopic imagery. This\npaper considers three feature families - statistical metrics, gradient-based\nmetrics and wavelet transform coefficient derived indicators - as possible\nmetrics to identify unsharp or blurry images. By using standard machine\nlearning techniques, the suitability of these features for image quality\nassessment is confirmed, albeit on a rather small data set. With the increased\navailability and decreasing cost of digital ophthalmoscopy on one hand and the\nincreased prevalence of diabetic retinopathy worldwide on the other, creating\ntools that can determine whether an image is likely to be diagnostically\nsuitable can play a significant role in accelerating and streamlining the\nteleophthalmology process. This paper highlights the need for more research in\nthis area, including the compilation of a diverse database of ophthalmoscopic\nimagery, annotated with quality markers, to train the Point of Acquisition\nerror detection algorithms of the future.\n", "versions": [{"version": "v1", "created": "Thu, 7 Mar 2019 02:21:41 GMT"}], "update_date": "2019-03-08", "authors_parsed": [["von Csefalvay", "Chris", ""]]}, {"id": "1903.02702", "submitter": "Shihao Sun", "authors": "Yi Peng, Shihao Sun, Zheng Wang, Yining Pan, Ruirui Li", "title": "Robust Semantic Segmentation By Dense Fusion Network On Blurred VHR\n  Remote Sensing Images", "comments": "Submitted to BigDIA 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Robust semantic segmentation of VHR remote sensing images from UAV sensors is\ncritical for earth observation, land use, land cover or mapping applications.\nSeveral factors such as shadows, weather disruption and camera shakes making\nthis problem highly challenging, especially only using RGB images. In this\npaper, we propose the use of multi-modality data including NIR, RGB and DSM to\nincrease robustness of segmentation in blurred or partially damaged VHR remote\nsensing images. By proposing a cascaded dense encoder-decoder network and the\nSELayer based fusion and assembling techniques, the proposed RobustDenseNet\nachieves steady performance when the image quality is decreasing, compared with\nthe state-of-the-art semantic segmentation model.\n", "versions": [{"version": "v1", "created": "Thu, 7 Mar 2019 02:53:13 GMT"}, {"version": "v2", "created": "Tue, 1 Dec 2020 09:26:29 GMT"}], "update_date": "2020-12-02", "authors_parsed": [["Peng", "Yi", ""], ["Sun", "Shihao", ""], ["Wang", "Zheng", ""], ["Pan", "Yining", ""], ["Li", "Ruirui", ""]]}, {"id": "1903.02707", "submitter": "Rakib Hyder", "authors": "Rakib Hyder, Viraj Shah, Chinmay Hegde, and M. Salman Asif", "title": "Alternating Phase Projected Gradient Descent with Generative Priors for\n  Solving Compressive Phase Retrieval", "comments": "Published in ICASSP 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The classical problem of phase retrieval arises in various signal acquisition\nsystems. Due to the ill-posed nature of the problem, the solution requires\nassumptions on the structure of the signal. In the last several years, sparsity\nand support-based priors have been leveraged successfully to solve this\nproblem. In this work, we propose replacing the sparsity/support priors with\ngenerative priors and propose two algorithms to solve the phase retrieval\nproblem. Our proposed algorithms combine the ideas from AltMin approach for\nnon-convex sparse phase retrieval and projected gradient descent approach for\nsolving linear inverse problems using generative priors. We empirically show\nthat the performance of our method with projected gradient descent is superior\nto the existing approach for solving phase retrieval under generative priors.\nWe support our method with an analysis of sample complexity with Gaussian\nmeasurements.\n", "versions": [{"version": "v1", "created": "Thu, 7 Mar 2019 03:03:14 GMT"}], "update_date": "2019-03-08", "authors_parsed": [["Hyder", "Rakib", ""], ["Shah", "Viraj", ""], ["Hegde", "Chinmay", ""], ["Asif", "M. Salman", ""]]}, {"id": "1903.02728", "submitter": "Ji Zhang", "authors": "Ji Zhang, Kevin J. Shih, Ahmed Elgammal, Andrew Tao, Bryan Catanzaro", "title": "Graphical Contrastive Losses for Scene Graph Parsing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most scene graph parsers use a two-stage pipeline to detect visual\nrelationships: the first stage detects entities, and the second predicts the\npredicate for each entity pair using a softmax distribution. We find that such\npipelines, trained with only a cross entropy loss over predicate classes,\nsuffer from two common errors. The first, Entity Instance Confusion, occurs\nwhen the model confuses multiple instances of the same type of entity (e.g.\nmultiple cups). The second, Proximal Relationship Ambiguity, arises when\nmultiple subject-predicate-object triplets appear in close proximity with the\nsame predicate, and the model struggles to infer the correct subject-object\npairings (e.g. mis-pairing musicians and their instruments). We propose a set\nof contrastive loss formulations that specifically target these types of errors\nwithin the scene graph parsing problem, collectively termed the Graphical\nContrastive Losses. These losses explicitly force the model to disambiguate\nrelated and unrelated instances through margin constraints specific to each\ntype of confusion. We further construct a relationship detector, called RelDN,\nusing the aforementioned pipeline to demonstrate the efficacy of our proposed\nlosses. Our model outperforms the winning method of the OpenImages Relationship\nDetection Challenge by 4.7\\% (16.5\\% relative) on the test set. We also show\nimproved results over the best previous methods on the Visual Genome and Visual\nRelationship Detection datasets.\n", "versions": [{"version": "v1", "created": "Thu, 7 Mar 2019 05:07:43 GMT"}, {"version": "v2", "created": "Sat, 16 Mar 2019 01:01:20 GMT"}, {"version": "v3", "created": "Thu, 28 Mar 2019 21:40:45 GMT"}, {"version": "v4", "created": "Fri, 19 Apr 2019 05:30:22 GMT"}, {"version": "v5", "created": "Fri, 16 Aug 2019 21:30:29 GMT"}], "update_date": "2019-08-20", "authors_parsed": [["Zhang", "Ji", ""], ["Shih", "Kevin J.", ""], ["Elgammal", "Ahmed", ""], ["Tao", "Andrew", ""], ["Catanzaro", "Bryan", ""]]}, {"id": "1903.02731", "submitter": "Junde Wu", "authors": "Junde Wu and Xiaoguang Di and Jiehao Huang and Yu Zhang", "title": "Integrating neural networks into the blind deblurring framework to\n  compete with the end-to-end learning-based methods", "comments": null, "journal-ref": null, "doi": "10.1109/TIP.2020.2994413", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, end-to-end learning-based methods based on deep neural network\n(DNN) have been proven effective for blind deblurring. Without human-made\nassumptions and numerical algorithms, they are able to restore images with\nfewer artifacts and better perceptual quality. However, in practice, we also\nfind some of their drawbacks. Without the theoretical guidance, these methods\ncan not perform well when the motion is complex and sometimes generate\nunreasonable results. In this paper, for overcoming these drawbacks, we\nintegrate deep convolution neural networks into conventional deblurring\nframework. Specifically, we build Stacked Estimation Residual Net (SEN) to\nestimate the motion flow map and Recurrent Prior Generative and Adversarial Net\n(RP-GAN) to learn the implicit image prior in the optimization model. Comparing\nwith state-of-the-art end-to-end learning-based methods, our method restores\nreasonable details and shows better generalization ability.\n", "versions": [{"version": "v1", "created": "Thu, 7 Mar 2019 05:20:25 GMT"}, {"version": "v2", "created": "Sat, 22 Jun 2019 02:08:40 GMT"}], "update_date": "2020-07-15", "authors_parsed": [["Wu", "Junde", ""], ["Di", "Xiaoguang", ""], ["Huang", "Jiehao", ""], ["Zhang", "Yu", ""]]}, {"id": "1903.02740", "submitter": "Zaiwang Gu", "authors": "Zaiwang Gu, Jun Cheng, Huazhu Fu, Kang Zhou, Huaying Hao, Yitian Zhao,\n  Tianyang Zhang, Shenghua Gao and Jiang Liu", "title": "CE-Net: Context Encoder Network for 2D Medical Image Segmentation", "comments": "accepted by IEEE transcations on medical imaging, (TMI)", "journal-ref": null, "doi": "10.1109/TMI.2019.2903562", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Medical image segmentation is an important step in medical image analysis.\nWith the rapid development of convolutional neural network in image processing,\ndeep learning has been used for medical image segmentation, such as optic disc\nsegmentation, blood vessel detection, lung segmentation, cell segmentation,\netc. Previously, U-net based approaches have been proposed. However, the\nconsecutive pooling and strided convolutional operations lead to the loss of\nsome spatial information. In this paper, we propose a context encoder network\n(referred to as CE-Net) to capture more high-level information and preserve\nspatial information for 2D medical image segmentation. CE-Net mainly contains\nthree major components: a feature encoder module, a context extractor and a\nfeature decoder module. We use pretrained ResNet block as the fixed feature\nextractor. The context extractor module is formed by a newly proposed dense\natrous convolution (DAC) block and residual multi-kernel pooling (RMP) block.\nWe applied the proposed CE-Net to different 2D medical image segmentation\ntasks. Comprehensive results show that the proposed method outperforms the\noriginal U-Net method and other state-of-the-art methods for optic disc\nsegmentation, vessel detection, lung segmentation, cell contour segmentation\nand retinal optical coherence tomography layer segmentation.\n", "versions": [{"version": "v1", "created": "Thu, 7 Mar 2019 06:24:27 GMT"}], "update_date": "2019-03-08", "authors_parsed": [["Gu", "Zaiwang", ""], ["Cheng", "Jun", ""], ["Fu", "Huazhu", ""], ["Zhou", "Kang", ""], ["Hao", "Huaying", ""], ["Zhao", "Yitian", ""], ["Zhang", "Tianyang", ""], ["Gao", "Shenghua", ""], ["Liu", "Jiang", ""]]}, {"id": "1903.02741", "submitter": "Chi Zhang", "authors": "Chi Zhang, Feng Gao, Baoxiong Jia, Yixin Zhu, Song-Chun Zhu", "title": "RAVEN: A Dataset for Relational and Analogical Visual rEasoNing", "comments": "CVPR 2019 paper. Supplementary:\n  http://wellyzhang.github.io/attach/cvpr19zhang_supp.pdf Project:\n  http://wellyzhang.github.io/project/raven.html", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dramatic progress has been witnessed in basic vision tasks involving\nlow-level perception, such as object recognition, detection, and tracking.\nUnfortunately, there is still an enormous performance gap between artificial\nvision systems and human intelligence in terms of higher-level vision problems,\nespecially ones involving reasoning. Earlier attempts in equipping machines\nwith high-level reasoning have hovered around Visual Question Answering (VQA),\none typical task associating vision and language understanding. In this work,\nwe propose a new dataset, built in the context of Raven's Progressive Matrices\n(RPM) and aimed at lifting machine intelligence by associating vision with\nstructural, relational, and analogical reasoning in a hierarchical\nrepresentation. Unlike previous works in measuring abstract reasoning using\nRPM, we establish a semantic link between vision and reasoning by providing\nstructure representation. This addition enables a new type of abstract\nreasoning by jointly operating on the structure representation. Machine\nreasoning ability using modern computer vision is evaluated in this newly\nproposed dataset. Additionally, we also provide human performance as a\nreference. Finally, we show consistent improvement across all models by\nincorporating a simple neural module that combines visual understanding and\nstructure reasoning.\n", "versions": [{"version": "v1", "created": "Thu, 7 Mar 2019 06:28:44 GMT"}], "update_date": "2019-03-08", "authors_parsed": [["Zhang", "Chi", ""], ["Gao", "Feng", ""], ["Jia", "Baoxiong", ""], ["Zhu", "Yixin", ""], ["Zhu", "Song-Chun", ""]]}, {"id": "1903.02765", "submitter": "Jianhao Jiao", "authors": "Jianhao Jiao, Rui Fan, Han Ma, Ming Liu", "title": "Using DP Towards A Shortest Path Problem-Related Application", "comments": "8 pages, 8 figures, accepted by IEEE International Conference on\n  Robotics and Automation (ICRA) 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The detection of curved lanes is still challenging for autonomous driving\nsystems. Although current cutting-edge approaches have performed well in real\napplications, most of them are based on strict model assumptions. Similar to\nother visual recognition tasks, lane detection can be formulated as a\ntwo-dimensional graph searching problem, which can be solved by finding several\noptimal paths along with line segments and boundaries. In this paper, we\npresent a directed graph model, in which dynamic programming is used to deal\nwith a specific shortest path problem. This model is particularly suitable to\nrepresent objects with long continuous shape structure, e.g., lanes and roads.\nWe apply the designed model and proposed an algorithm for detecting lanes by\nformulating it as the shortest path problem. To evaluate the performance of our\nproposed algorithm, we tested five sequences (including 1573 frames) from the\nKITTI database. The results showed that our method achieves an average\nsuccessful detection precision of 97.5%.\n", "versions": [{"version": "v1", "created": "Thu, 7 Mar 2019 08:11:15 GMT"}], "update_date": "2019-03-08", "authors_parsed": [["Jiao", "Jianhao", ""], ["Fan", "Rui", ""], ["Ma", "Han", ""], ["Liu", "Ming", ""]]}, {"id": "1903.02775", "submitter": "Yuanxi Ma", "authors": "Yuanxi Ma, Cen Wang, Shiying Li and Jingyi Yu", "title": "Hair Segmentation on Time-of-Flight RGBD Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Robust segmentation of hair from portrait images remains challenging: hair\ndoes not conform to a uniform shape, style or even color; dark hair in\nparticular lacks features. We present a novel computational imaging solution\nthat tackles the problem from both input and processing fronts. We explore\nusing Time-of-Flight (ToF) RGBD sensors on recent mobile devices. We first\nconduct a comprehensive analysis to show that scattering and inter-reflection\ncause different noise patterns on hair vs. non-hair regions on ToF images, by\nchanging the light path and/or combining multiple paths. We then develop a deep\nnetwork based approach that employs both ToF depth map and the RGB gradient\nmaps to produce an initial hair segmentation with labeled hair components. We\nthen refine the result by imposing ToF noise prior under the conditional random\nfield. We collect the first ToF RGBD hair dataset with 20k+ head images\ncaptured on 30 human subjects with a variety of hairstyles at different view\nangles. Comprehensive experiments show that our approach outperforms the RGB\nbased techniques in accuracy and robustness and can handle traditionally\nchallenging cases such as dark hair, similar hair/background, similar\nhair/foreground, etc.\n", "versions": [{"version": "v1", "created": "Thu, 7 Mar 2019 08:55:35 GMT"}, {"version": "v2", "created": "Mon, 11 Mar 2019 07:47:57 GMT"}, {"version": "v3", "created": "Mon, 20 Apr 2020 14:01:42 GMT"}], "update_date": "2020-04-21", "authors_parsed": [["Ma", "Yuanxi", ""], ["Wang", "Cen", ""], ["Li", "Shiying", ""], ["Yu", "Jingyi", ""]]}, {"id": "1903.02793", "submitter": "Pu Zhang", "authors": "Pu Zhang, Wanli Ouyang, Pengfei Zhang, Jianru Xue, Nanning Zheng", "title": "SR-LSTM: State Refinement for LSTM towards Pedestrian Trajectory\n  Prediction", "comments": "Accepted by CVPR2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In crowd scenarios, reliable trajectory prediction of pedestrians requires\ninsightful understanding of their social behaviors. These behaviors have been\nwell investigated by plenty of studies, while it is hard to be fully expressed\nby hand-craft rules. Recent studies based on LSTM networks have shown great\nability to learn social behaviors. However, many of these methods rely on\nprevious neighboring hidden states but ignore the important current intention\nof the neighbors. In order to address this issue, we propose a data-driven\nstate refinement module for LSTM network (SR-LSTM), which activates the\nutilization of the current intention of neighbors, and jointly and iteratively\nrefines the current states of all participants in the crowd through a message\npassing mechanism. To effectively extract the social effect of neighbors, we\nfurther introduce a social-aware information selection mechanism consisting of\nan element-wise motion gate and a pedestrian-wise attention to select useful\nmessage from neighboring pedestrians. Experimental results on two public\ndatasets, i.e. ETH and UCY, demonstrate the effectiveness of our proposed\nSR-LSTM and we achieves state-of-the-art results.\n", "versions": [{"version": "v1", "created": "Thu, 7 Mar 2019 09:49:57 GMT"}], "update_date": "2019-03-08", "authors_parsed": [["Zhang", "Pu", ""], ["Ouyang", "Wanli", ""], ["Zhang", "Pengfei", ""], ["Xue", "Jianru", ""], ["Zheng", "Nanning", ""]]}, {"id": "1903.02827", "submitter": "Xiangru Lin", "authors": "Weifeng Ge, Xiangru Lin, Yizhou Yu", "title": "Weakly Supervised Complementary Parts Models for Fine-Grained Image\n  Classification from the Bottom Up", "comments": "Accepted to appear in CVPR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a training dataset composed of images and corresponding category\nlabels, deep convolutional neural networks show a strong ability in mining\ndiscriminative parts for image classification. However, deep convolutional\nneural networks trained with image level labels only tend to focus on the most\ndiscriminative parts while missing other object parts, which could provide\ncomplementary information. In this paper, we approach this problem from a\ndifferent perspective. We build complementary parts models in a weakly\nsupervised manner to retrieve information suppressed by dominant object parts\ndetected by convolutional neural networks. Given image level labels only, we\nfirst extract rough object instances by performing weakly supervised object\ndetection and instance segmentation using Mask R-CNN and CRF-based\nsegmentation. Then we estimate and search for the best parts model for each\nobject instance under the principle of preserving as much diversity as\npossible. In the last stage, we build a bi-directional long short-term memory\n(LSTM) network to fuze and encode the partial information of these\ncomplementary parts into a comprehensive feature for image classification.\nExperimental results indicate that the proposed method not only achieves\nsignificant improvement over our baseline models, but also outperforms\nstate-of-the-art algorithms by a large margin (6.7%, 2.8%, 5.2% respectively)\non Stanford Dogs 120, Caltech-UCSD Birds 2011-200 and Caltech 256.\n", "versions": [{"version": "v1", "created": "Thu, 7 Mar 2019 10:54:16 GMT"}], "update_date": "2019-03-08", "authors_parsed": [["Ge", "Weifeng", ""], ["Lin", "Xiangru", ""], ["Yu", "Yizhou", ""]]}, {"id": "1903.02832", "submitter": "Metin Sezgin", "authors": "Erelcan Yanik, Tevfik Metin Sezgin", "title": "Active Scene Learning", "comments": "To be submitted to the Pattern Recognition Journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sketch recognition allows natural and efficient interaction in pen-based\ninterfaces. A key obstacle to building accurate sketch recognizers has been the\ndifficulty of creating large amounts of annotated training data. Several\nauthors have attempted to address this issue by creating synthetic data, and by\nbuilding tools that support efficient annotation. Two prominent sets of\napproaches stand out from the rest of the crowd. They use interim classifiers\ntrained with a small set of labeled data to aid the labeling of the remainder\nof the data. The first set of approaches uses a classifier trained with a\npartially labeled dataset to automatically label unlabeled instances. The\nothers, based on active learning, save annotation effort by giving priority to\nlabeling informative data instances. The former is sub-optimal since it doesn't\nprioritize the order of labeling to favor informative instances, while the\nlatter makes the strong assumption that unlabeled data comes in an already\nsegmented form (i.e. the ink in the training data is already assembled into\ngroups forming isolated object instances). In this paper, we propose an active\nlearning framework that combines the strengths of these methods, while\naddressing their weaknesses. In particular, we propose two methods for deciding\nhow batches of unsegmented sketch scenes should be labeled. The first method,\nscene-wise selection, assesses the informativeness of each drawing (sketch\nscene) as a whole, and asks the user to annotate all objects in the drawing.\nThe latter, segment-wise selection, attempts more precise targeting to locate\ninformative fragments of drawings for user labeling. We show that both\nselection schemes outperform random selection. Furthermore, we demonstrate that\nprecise targeting yields superior performance. Overall, our approach allows\nreaching top accuracy figures with up to 30% savings in annotation cost.\n", "versions": [{"version": "v1", "created": "Thu, 7 Mar 2019 11:07:54 GMT"}], "update_date": "2019-03-08", "authors_parsed": [["Yanik", "Erelcan", ""], ["Sezgin", "Tevfik Metin", ""]]}, {"id": "1903.02871", "submitter": "Jan Egger", "authors": "Christina Gsaxner, Peter M. Roth, J\\\"urgen Wallner, Jan Egger", "title": "Exploit fully automatic low-level segmented PET data for training\n  high-level deep learning algorithms for the corresponding CT data", "comments": "20 pages", "journal-ref": "PLoS ONE 14(3): e0212550 (2019)", "doi": "10.1371/journal.pone.0212550", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an approach for fully automatic urinary bladder segmentation in CT\nimages with artificial neural networks in this study. Automatic medical image\nanalysis has become an invaluable tool in the different treatment stages of\ndiseases. Especially medical image segmentation plays a vital role, since\nsegmentation is often the initial step in an image analysis pipeline. Since\ndeep neural networks have made a large impact on the field of image processing\nin the past years, we use two different deep learning architectures to segment\nthe urinary bladder. Both of these architectures are based on pre-trained\nclassification networks that are adapted to perform semantic segmentation.\nSince deep neural networks require a large amount of training data,\nspecifically images and corresponding ground truth labels, we furthermore\npropose a method to generate such a suitable training data set from Positron\nEmission Tomography/Computed Tomography image data. This is done by applying\nthresholding to the Positron Emission Tomography data for obtaining a ground\ntruth and by utilizing data augmentation to enlarge the dataset. In this study,\nwe discuss the influence of data augmentation on the segmentation results, and\ncompare and evaluate the proposed architectures in terms of qualitative and\nquantitative segmentation performance. The results presented in this study\nallow concluding that deep neural networks can be considered a promising\napproach to segment the urinary bladder in CT images.\n", "versions": [{"version": "v1", "created": "Thu, 7 Mar 2019 12:26:03 GMT"}], "update_date": "2019-03-08", "authors_parsed": [["Gsaxner", "Christina", ""], ["Roth", "Peter M.", ""], ["Wallner", "J\u00fcrgen", ""], ["Egger", "Jan", ""]]}, {"id": "1903.02874", "submitter": "Yansong Tang", "authors": "Yansong Tang, Dajun Ding, Yongming Rao, Yu Zheng, Danyang Zhang, Lili\n  Zhao, Jiwen Lu, Jie Zhou", "title": "COIN: A Large-scale Dataset for Comprehensive Instructional Video\n  Analysis", "comments": "CVPR2019, project page: https://coin-dataset.github.io/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There are substantial instructional videos on the Internet, which enables us\nto acquire knowledge for completing various tasks. However, most existing\ndatasets for instructional video analysis have the limitations in diversity and\nscale,which makes them far from many real-world applications where more diverse\nactivities occur. Moreover, it still remains a great challenge to organize and\nharness such data. To address these problems, we introduce a large-scale\ndataset called \"COIN\" for COmprehensive INstructional video analysis. Organized\nwith a hierarchical structure, the COIN dataset contains 11,827 videos of 180\ntasks in 12 domains (e.g., vehicles, gadgets, etc.) related to our daily life.\nWith a new developed toolbox, all the videos are annotated effectively with a\nseries of step descriptions and the corresponding temporal boundaries.\nFurthermore, we propose a simple yet effective method to capture the\ndependencies among different steps, which can be easily plugged into\nconventional proposal-based action detection methods for localizing important\nsteps in instructional videos. In order to provide a benchmark for\ninstructional video analysis, we evaluate plenty of approaches on the COIN\ndataset under different evaluation criteria. We expect the introduction of the\nCOIN dataset will promote the future in-depth research on instructional video\nanalysis for the community.\n", "versions": [{"version": "v1", "created": "Thu, 7 Mar 2019 12:32:32 GMT"}], "update_date": "2019-03-08", "authors_parsed": [["Tang", "Yansong", ""], ["Ding", "Dajun", ""], ["Rao", "Yongming", ""], ["Zheng", "Yu", ""], ["Zhang", "Danyang", ""], ["Zhao", "Lili", ""], ["Lu", "Jiwen", ""], ["Zhou", "Jie", ""]]}, {"id": "1903.02959", "submitter": "Ruizhi Liao", "authors": "Ruizhi Liao, Esra A. Turk, Miaomiao Zhang, Jie Luo, Elfar\n  Adalsteinsson, P. Ellen Grant, Polina Golland", "title": "Temporal Registration in Application to In-utero MRI Time Series", "comments": "arXiv admin note: text overlap with arXiv:1608.03907", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a robust method to correct for motion in volumetric in-utero MRI\ntime series. Time-course analysis for in-utero volumetric MRI time series often\nsuffers from substantial and unpredictable fetal motion. Registration provides\nvoxel correspondences between images and is commonly employed for motion\ncorrection. Current registration methods often fail when aligning images that\nare substantially different from a template (reference image). To achieve\naccurate and robust alignment, we make a Markov assumption on the nature of\nmotion and take advantage of the temporal smoothness in the image data. Forward\nmessage passing in the corresponding hidden Markov model (HMM) yields an\nestimation algorithm that only has to account for relatively small motion\nbetween consecutive frames. We evaluate the utility of the temporal model in\nthe context of in-utero MRI time series alignment by examining the accuracy of\npropagated segmentation label maps. Our results suggest that the proposed model\ncaptures accurately the temporal dynamics of transformations in in-utero MRI\ntime series.\n", "versions": [{"version": "v1", "created": "Wed, 6 Mar 2019 18:07:32 GMT"}], "update_date": "2019-03-08", "authors_parsed": [["Liao", "Ruizhi", ""], ["Turk", "Esra A.", ""], ["Zhang", "Miaomiao", ""], ["Luo", "Jie", ""], ["Adalsteinsson", "Elfar", ""], ["Grant", "P. Ellen", ""], ["Golland", "Polina", ""]]}, {"id": "1903.02974", "submitter": "Richard Droste", "authors": "Richard Droste, Yifan Cai, Harshita Sharma, Pierre Chatelain, Lior\n  Drukker, Aris T. Papageorghiou, J. Alison Noble", "title": "Ultrasound Image Representation Learning by Modeling Sonographer Visual\n  Attention", "comments": "Accepted at the international conference on Information Processing in\n  Medical Imaging (IPMI) 2019", "journal-ref": null, "doi": "10.1007/978-3-030-20351-1_46", "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image representations are commonly learned from class labels, which are a\nsimplistic approximation of human image understanding. In this paper we\ndemonstrate that transferable representations of images can be learned without\nmanual annotations by modeling human visual attention. The basis of our\nanalyses is a unique gaze tracking dataset of sonographers performing routine\nclinical fetal anomaly screenings. Models of sonographer visual attention are\nlearned by training a convolutional neural network (CNN) to predict gaze on\nultrasound video frames through visual saliency prediction or gaze-point\nregression. We evaluate the transferability of the learned representations to\nthe task of ultrasound standard plane detection in two contexts. Firstly, we\nperform transfer learning by fine-tuning the CNN with a limited number of\nlabeled standard plane images. We find that fine-tuning the saliency predictor\nis superior to training from random initialization, with an average F1-score\nimprovement of 9.6% overall and 15.3% for the cardiac planes. Secondly, we\ntrain a simple softmax regression on the feature activations of each CNN layer\nin order to evaluate the representations independently of transfer learning\nhyper-parameters. We find that the attention models derive strong\nrepresentations, approaching the precision of a fully-supervised baseline model\nfor all but the last layer.\n", "versions": [{"version": "v1", "created": "Thu, 7 Mar 2019 15:05:31 GMT"}, {"version": "v2", "created": "Mon, 27 May 2019 11:55:31 GMT"}], "update_date": "2019-05-28", "authors_parsed": [["Droste", "Richard", ""], ["Cai", "Yifan", ""], ["Sharma", "Harshita", ""], ["Chatelain", "Pierre", ""], ["Drukker", "Lior", ""], ["Papageorghiou", "Aris T.", ""], ["Noble", "J. Alison", ""]]}, {"id": "1903.02982", "submitter": "Florian Strub", "authors": "Florian Strub and Marie-Agathe Charpagne and Tresa M. Pollock", "title": "Correction of Electron Back-scattered Diffraction datasets using an\n  evolutionary algorithm", "comments": "This short paper target an audience working in Machine Learning. A\n  long version of this paper exists towards people working in Materials (more\n  experiments, more experimental details and analysis), namely arXiv:1903.02988", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV physics.ins-det", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In materials science and particularly electron microscopy, Electron\nBack-scatter Diffraction (EBSD) is a common and powerful mapping technique for\ncollecting local crystallographic data at the sub-micron scale. The quality of\nthe reconstruction of the maps is critical to study the spatial distribution of\nphases and crystallographic orientation relationships between phases, a key\ninterest in materials science. However, EBSD data is known to suffer from\ndistortions that arise from several instrument and detector artifacts. In this\npaper, we present an unsupervised method that corrects those distortions, and\nenables or enhances phase differentiation in EBSD data. The method uses a\nsegmented electron image of the phases of interest (laths, precipitates, voids,\ninclusions) gathered using detectors that generate less distorted data, of the\nsame area than the EBSD map, and then searches for the best transformation to\ncorrect the distortions of the initial EBSD data. To do so, the Covariance\nMatrix Adaptation Evolution Strategy (CMA-ES) is implemented to distort the\nEBSD until it matches the reference electron image. Fast and versatile, this\nmethod does not require any human annotation and can be applied to large\ndatasets and wide areas, where the distortions are important. Besides, this\nmethod requires very little assumption concerning the shape of the distortion\nfunction. Some application examples in multiphase materials with feature sizes\ndown to 1 $\\mu$m are presented, including a Titanium alloy and a Nickel-base\nsuperalloy.\n", "versions": [{"version": "v1", "created": "Thu, 7 Mar 2019 15:21:44 GMT"}], "update_date": "2019-03-08", "authors_parsed": [["Strub", "Florian", ""], ["Charpagne", "Marie-Agathe", ""], ["Pollock", "Tresa M.", ""]]}, {"id": "1903.03029", "submitter": "Alptekin Temizel", "authors": "Bilgin Aksoy and Alptekin Temizel", "title": "Attack Type Agnostic Perceptual Enhancement of Adversarial Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adversarial images are samples that are intentionally modified to deceive\nmachine learning systems. They are widely used in applications such as CAPTHAs\nto help distinguish legitimate human users from bots. However, the noise\nintroduced during the adversarial image generation process degrades the\nperceptual quality and introduces artificial colours; making it also difficult\nfor humans to classify images and recognise objects. In this letter, we propose\na method to enhance the perceptual quality of these adversarial images. The\nproposed method is attack type agnostic and could be used in association with\nthe existing attacks in the literature. Our experiments show that the generated\nadversarial images have lower Euclidean distance values while maintaining the\nsame adversarial attack performance. Distances are reduced by 5.88% to 41.27%\nwith an average reduction of 22% over the different attack and network types.\n", "versions": [{"version": "v1", "created": "Thu, 7 Mar 2019 16:35:03 GMT"}, {"version": "v2", "created": "Sun, 10 Mar 2019 17:37:33 GMT"}, {"version": "v3", "created": "Fri, 10 May 2019 14:15:47 GMT"}], "update_date": "2019-05-13", "authors_parsed": [["Aksoy", "Bilgin", ""], ["Temizel", "Alptekin", ""]]}, {"id": "1903.03044", "submitter": "Olivier Debeir Pr", "authors": "Olivier Debeir, Justine Allard, Christine Decaestecker, Jean-Pierre\n  Hermand", "title": "Characterization of Posidonia Oceanica Seagrass Aerenchyma through Whole\n  Slide Imaging: A Pilot Study", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.TO cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Characterizing the tissue morphology and anatomy of seagrasses is essential\nto predicting their acoustic behavior. In this pilot study, we use histology\ntechniques and whole slide imaging (WSI) to describe the composition and\ntopology of the aerenchyma of an entire leaf blade in an automatic way\ncombining the advantages of X-ray microtomography and optical microscopy.\nParaffin blocks are prepared in such a way that microtome slices contain an\narbitrarily large number of cross sections distributed along the full length of\na blade. The sample organization in the paraffin block coupled with whole slide\nimage analysis allows high throughput data extraction and an exhaustive\ncharacterization along the whole blade length. The core of the work are image\nprocessing algorithms that can identify cells and air lacunae (or void) from\nfiber strand, epidermis, mesophyll and vascular system. A set of specific\nfeatures is developed to adequately describe the convexity of cells and voids\nwhere standard descriptors fail. The features scrutinize the local curvature of\nthe object borders to allow an accurate discrimination between void and cell\nthrough machine learning. The algorithm allows to reconstruct the cells and\ncell membrane features that are relevant to tissue density, compressibility and\nrigidity. Size distribution of the different cell types and gas spaces, total\nbiomass and total void volume fraction are then extracted from the high\nresolution slices to provide a complete characterization of the tissue along\nthe leave from its base to the apex.\n", "versions": [{"version": "v1", "created": "Thu, 7 Mar 2019 17:01:32 GMT"}, {"version": "v2", "created": "Mon, 11 Mar 2019 14:51:28 GMT"}], "update_date": "2019-03-12", "authors_parsed": [["Debeir", "Olivier", ""], ["Allard", "Justine", ""], ["Decaestecker", "Christine", ""], ["Hermand", "Jean-Pierre", ""]]}, {"id": "1903.03087", "submitter": "Shuai Shao", "authors": "Shuai Shao, Yan-Jiang Wang, Bao-Di Liu, Weifeng Liu, Rui Xu", "title": "Label Embedded Dictionary Learning for Image Classification", "comments": "25pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, label consistent k-svd (LC-KSVD) algorithm has been successfully\napplied in image classification. The objective function of LC-KSVD is consisted\nof reconstruction error, classification error and discriminative sparse codes\nerror with L0-norm sparse regularization term. The L0-norm, however, leads to\nNP-hard problem. Despite some methods such as orthogonal matching pursuit can\nhelp solve this problem to some extent, it is quite difficult to find the\noptimum sparse solution. To overcome this limitation, we propose a label\nembedded dictionary learning (LEDL) method to utilise the L1-norm as the sparse\nregularization term so that we can avoid the hard-to-optimize problem by\nsolving the convex optimization problem. Alternating direction method of\nmultipliers and blockwise coordinate descent algorithm are then exploited to\noptimize the corresponding objective function. Extensive experimental results\non six benchmark datasets illustrate that the proposed algorithm has achieved\nsuperior performance compared to some conventional classification algorithms.\n", "versions": [{"version": "v1", "created": "Thu, 7 Mar 2019 18:26:36 GMT"}, {"version": "v2", "created": "Wed, 17 Apr 2019 07:40:05 GMT"}], "update_date": "2019-04-18", "authors_parsed": [["Shao", "Shuai", ""], ["Wang", "Yan-Jiang", ""], ["Liu", "Bao-Di", ""], ["Liu", "Weifeng", ""], ["Xu", "Rui", ""]]}, {"id": "1903.03137", "submitter": "Fergal Cotter", "authors": "Fergal Cotter, Nick Kingsbury", "title": "A Learnable ScatterNet: Locally Invariant Convolutional Layers", "comments": "4 pages, 1 Figure, pre-print of paper submitted to ICIP 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we explore tying together the ideas from Scattering Transforms\nand Convolutional Neural Networks (CNN) for Image Analysis by proposing a\nlearnable ScatterNet. Previous attempts at tying them together in hybrid\nnetworks have tended to keep the two parts separate, with the ScatterNet\nforming a fixed front end and a CNN forming a learned backend. We instead look\nat adding learning between scattering orders, as well as adding learned layers\nbefore the ScatterNet. We do this by breaking down the scattering orders into\nsingle convolutional-like layers we call 'locally invariant' layers, and adding\na learned mixing term to this layer. Our experiments show that these locally\ninvariant layers can improve accuracy when added to either a CNN or a\nScatterNet. We also discover some surprising results in that the ScatterNet may\nbe best positioned after one or more layers of learning rather than at the\nfront of a neural network.\n", "versions": [{"version": "v1", "created": "Thu, 7 Mar 2019 19:30:19 GMT"}], "update_date": "2019-03-11", "authors_parsed": [["Cotter", "Fergal", ""], ["Kingsbury", "Nick", ""]]}, {"id": "1903.03148", "submitter": "Adrian Dalca", "authors": "Adrian V. Dalca, John Guttag, Mert R. Sabuncu", "title": "Anatomical Priors in Convolutional Networks for Unsupervised Biomedical\n  Segmentation", "comments": "Presented at CVPR 2018. IEEE CVPR proceedings pp. 9290-9299", "journal-ref": null, "doi": "10.1109/CVPR.2018.00968", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of segmenting a biomedical image into anatomical\nregions of interest. We specifically address the frequent scenario where we\nhave no paired training data that contains images and their manual\nsegmentations. Instead, we employ unpaired segmentation images to build an\nanatomical prior. Critically these segmentations can be derived from imaging\ndata from a different dataset and imaging modality than the current task. We\nintroduce a generative probabilistic model that employs the learned prior\nthrough a convolutional neural network to compute segmentations in an\nunsupervised setting. We conducted an empirical analysis of the proposed\napproach in the context of structural brain MRI segmentation, using a\nmulti-study dataset of more than 14,000 scans. Our results show that an\nanatomical prior can enable fast unsupervised segmentation which is typically\nnot possible using standard convolutional networks. The integration of\nanatomical priors can facilitate CNN-based anatomical segmentation in a range\nof novel clinical problems, where few or no annotations are available and thus\nstandard networks are not trainable. The code is freely available at\nhttp://github.com/adalca/neuron.\n", "versions": [{"version": "v1", "created": "Thu, 7 Mar 2019 19:51:03 GMT"}], "update_date": "2019-03-14", "authors_parsed": [["Dalca", "Adrian V.", ""], ["Guttag", "John", ""], ["Sabuncu", "Mert R.", ""]]}, {"id": "1903.03161", "submitter": "Poojan Oza", "authors": "Poojan Oza and Vishal M. Patel", "title": "Deep CNN-based Multi-task Learning for Open-Set Recognition", "comments": "Under Review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel deep convolutional neural network (CNN) based multi-task\nlearning approach for open-set visual recognition. We combine a classifier\nnetwork and a decoder network with a shared feature extractor network within a\nmulti-task learning framework. We show that this approach results in better\nopen-set recognition accuracy. In our approach, reconstruction errors from the\ndecoder network are utilized for open-set rejection. In addition, we model the\ntail of the reconstruction error distribution from the known classes using the\nstatistical Extreme Value Theory to improve the overall performance.\nExperiments on multiple image classification datasets are performed and it is\nshown that this method can perform significantly better than many competitive\nopen set recognition algorithms available in the literature. The code will be\nmade available at: github.com/otkupjnoz/mlosr.\n", "versions": [{"version": "v1", "created": "Thu, 7 Mar 2019 20:11:32 GMT"}], "update_date": "2019-03-11", "authors_parsed": [["Oza", "Poojan", ""], ["Patel", "Vishal M.", ""]]}, {"id": "1903.03166", "submitter": "Satwik Kottur", "authors": "Satwik Kottur, Jos\\'e M. F. Moura, Devi Parikh, Dhruv Batra, Marcus\n  Rohrbach", "title": "CLEVR-Dialog: A Diagnostic Dataset for Multi-Round Reasoning in Visual\n  Dialog", "comments": "13 pages, 11 figures, 3 tables, accepted as a short paper at NAACL\n  2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual Dialog is a multimodal task of answering a sequence of questions\ngrounded in an image, using the conversation history as context. It entails\nchallenges in vision, language, reasoning, and grounding. However, studying\nthese subtasks in isolation on large, real datasets is infeasible as it\nrequires prohibitively-expensive complete annotation of the 'state' of all\nimages and dialogs.\n  We develop CLEVR-Dialog, a large diagnostic dataset for studying multi-round\nreasoning in visual dialog. Specifically, we construct a dialog grammar that is\ngrounded in the scene graphs of the images from the CLEVR dataset. This\ncombination results in a dataset where all aspects of the visual dialog are\nfully annotated. In total, CLEVR-Dialog contains 5 instances of 10-round\ndialogs for about 85k CLEVR images, totaling to 4.25M question-answer pairs.\n  We use CLEVR-Dialog to benchmark performance of standard visual dialog\nmodels; in particular, on visual coreference resolution (as a function of the\ncoreference distance). This is the first analysis of its kind for visual dialog\nmodels that was not possible without this dataset. We hope the findings from\nCLEVR-Dialog will help inform the development of future models for visual\ndialog. Our dataset and code are publicly available.\n", "versions": [{"version": "v1", "created": "Thu, 7 Mar 2019 20:18:39 GMT"}, {"version": "v2", "created": "Wed, 18 Sep 2019 18:04:43 GMT"}], "update_date": "2019-09-20", "authors_parsed": [["Kottur", "Satwik", ""], ["Moura", "Jos\u00e9 M. F.", ""], ["Parikh", "Devi", ""], ["Batra", "Dhruv", ""], ["Rohrbach", "Marcus", ""]]}, {"id": "1903.03180", "submitter": "Chuning Zhu", "authors": "Zhu Chuning", "title": "Fast Video Retargeting Based on Seam Carving with Parental Labeling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Seam carving is a state-of-the-art content-aware image resizing technique\nthat effectively preserves the salient areas of an image. However, when applied\nto video retargeting, not only is it time intensive, but it also creates highly\nvisible frame-wise discontinuities. In this paper, we propose a novel video\nretargeting method based on seam carving. First, for a single frame, we locate\nand remove several seams instead of one seam at once. Second, we use a dynamic\nspatiotemporal buffer of energy maps and a standard deviation operator to carve\nout the same seams in a temporal cube of frames with low variation in energy.\nLast but not least, an improved energy function that considers motions detected\nthrough difference method is employed. During testing, these enhancements\nresult in a 93 percent reduction in processing time and a higher frame-wise\nconsistency, thus showing superior performance compared to existing video\nretargeting methods.\n", "versions": [{"version": "v1", "created": "Thu, 7 Mar 2019 20:52:17 GMT"}], "update_date": "2019-03-11", "authors_parsed": [["Chuning", "Zhu", ""]]}, {"id": "1903.03207", "submitter": "Guohao Yu", "authors": "Guohao Yu, Alina Zare, Hudanyun Sheng, Roser Matamala, Joel\n  Reyes-Cabrera, Felix B. Fritschi and Thomas E. Juenger", "title": "Root Identification in Minirhizotron Imagery with Multiple Instance\n  Learning", "comments": null, "journal-ref": null, "doi": "10.1007/s00138-020-01088-z", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, multiple instance learning (MIL) algorithms to automatically\nperform root detection and segmentation in minirhizotron imagery using only\nimage-level labels are proposed. Root and soil characteristics vary from\nlocation to location, thus, supervised machine learning approaches that are\ntrained with local data provide the best ability to identify and segment roots\nin minirhizotron imagery. However, labeling roots for training data (or\notherwise) is an extremely tedious and time-consuming task. This paper aims to\naddress this problem by labeling data at the image level (rather than the\nindividual root or root pixel level) and train algorithms to perform individual\nroot pixel level segmentation using MIL strategies. Three MIL methods (multiple\ninstance adaptive cosine coherence estimator, multiple instance support vector\nmachine, multiple instance learning with randomized trees) were applied to root\ndetection and compared to non-MIL approches. The results show that MIL methods\nimprove root segmentation in challenging minirhizotron imagery and reduce the\nlabeling burden. In our results, multiple instance support vector machine\noutperformed other methods. The multiple instance adaptive cosine coherence\nestimator algorithm was a close second with an added advantage that it learned\nan interpretable root signature which identified the traits used to distinguish\nroots from soil and did not require parameter selection.\n", "versions": [{"version": "v1", "created": "Thu, 7 Mar 2019 22:20:05 GMT"}, {"version": "v2", "created": "Mon, 11 Mar 2019 16:59:22 GMT"}, {"version": "v3", "created": "Mon, 18 May 2020 20:33:52 GMT"}], "update_date": "2020-05-26", "authors_parsed": [["Yu", "Guohao", ""], ["Zare", "Alina", ""], ["Sheng", "Hudanyun", ""], ["Matamala", "Roser", ""], ["Reyes-Cabrera", "Joel", ""], ["Fritschi", "Felix B.", ""], ["Juenger", "Thomas E.", ""]]}, {"id": "1903.03215", "submitter": "Subhankar Roy", "authors": "Subhankar Roy, Aliaksandr Siarohin, Enver Sangineto, Samuel Rota Bulo,\n  Nicu Sebe, Elisa Ricci", "title": "Unsupervised Domain Adaptation using Feature-Whitening and Consensus\n  Loss", "comments": "CVPR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A classifier trained on a dataset seldom works on other datasets obtained\nunder different conditions due to domain shift. This problem is commonly\naddressed by domain adaptation methods. In this work we introduce a novel deep\nlearning framework which unifies different paradigms in unsupervised domain\nadaptation. Specifically, we propose domain alignment layers which implement\nfeature whitening for the purpose of matching source and target feature\ndistributions. Additionally, we leverage the unlabeled target data by proposing\nthe Min-Entropy Consensus loss, which regularizes training while avoiding the\nadoption of many user-defined hyper-parameters. We report results on publicly\navailable datasets, considering both digit classification and object\nrecognition tasks. We show that, in most of our experiments, our approach\nimproves upon previous methods, setting new state-of-the-art performances.\n", "versions": [{"version": "v1", "created": "Thu, 7 Mar 2019 23:07:15 GMT"}, {"version": "v2", "created": "Sun, 16 Feb 2020 17:59:37 GMT"}], "update_date": "2020-02-18", "authors_parsed": [["Roy", "Subhankar", ""], ["Siarohin", "Aliaksandr", ""], ["Sangineto", "Enver", ""], ["Bulo", "Samuel Rota", ""], ["Sebe", "Nicu", ""], ["Ricci", "Elisa", ""]]}, {"id": "1903.03221", "submitter": "Fernando Dobarro", "authors": "Agust\\'in Mailing, Segundo A. Molina, Jos\\'e L. Hamkalo, Fernando R.\n  Dobarro, Juan M. Medina, Bruno Cernuschi-Fr\\'ias, Daniel A. Fern\\'andez and\n  \\'Erica Schlaps", "title": "Pattern Recognition in SAR Images using Fractional Random Fields and its\n  Possible Application to the Problem of the Detection of Oil Spills in Open\n  Sea", "comments": "Keywords: Fractional Processes, Wavelets. 3 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this note we deal with the detection of oil spills in open sea via self\nsimilar, long range dependence random fields and wavelet filters. We show some\npreliminary experimental results of our technique with Sentinel 1 SAR images.\n", "versions": [{"version": "v1", "created": "Thu, 7 Mar 2019 23:48:29 GMT"}], "update_date": "2019-03-11", "authors_parsed": [["Mailing", "Agust\u00edn", ""], ["Molina", "Segundo A.", ""], ["Hamkalo", "Jos\u00e9 L.", ""], ["Dobarro", "Fernando R.", ""], ["Medina", "Juan M.", ""], ["Cernuschi-Fr\u00edas", "Bruno", ""], ["Fern\u00e1ndez", "Daniel A.", ""], ["Schlaps", "\u00c9rica", ""]]}, {"id": "1903.03238", "submitter": "Xinshao Wang Dr", "authors": "Xinshao Wang, Yang Hua, Elyor Kodirov, Neil M. Robertson", "title": "Ranked List Loss for Deep Metric Learning", "comments": "Accepted to T-PAMI. Therefore, to read the offical version, please go\n  to IEEE Xplore. Fine-grained image retrieval task. Our source code is\n  available online: https://github.com/XinshaoAmosWang/Ranked-List-Loss-for-DML", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The objective of deep metric learning (DML) is to learn embeddings that can\ncapture semantic similarity and dissimilarity information among data points.\nExisting pairwise or tripletwise loss functions used in DML are known to suffer\nfrom slow convergence due to a large proportion of trivial pairs or triplets as\nthe model improves. To improve this, ranking-motivated structured losses are\nproposed recently to incorporate multiple examples and exploit the structured\ninformation among them. They converge faster and achieve state-of-the-art\nperformance. In this work, we unveil two limitations of existing\nranking-motivated structured losses and propose a novel ranked list loss to\nsolve both of them. First, given a query, only a fraction of data points is\nincorporated to build the similarity structure. Consequently, some useful\nexamples are ignored and the structure is less informative. To address this, we\npropose to build a set-based similarity structure by exploiting all instances\nin the gallery. The learning setting can be interpreted as few-shot retrieval:\ngiven a mini-batch, every example is iteratively used as a query, and the rest\nones compose the gallery to search, i.e., the support set in few-shot setting.\nThe rest examples are split into a positive set and a negative set. For every\nmini-batch, the learning objective of ranked list loss is to make the query\ncloser to the positive set than to the negative set by a margin. Second,\nprevious methods aim to pull positive pairs as close as possible in the\nembedding space. As a result, the intraclass data distribution tends to be\nextremely compressed. In contrast, we propose to learn a hypersphere for each\nclass in order to preserve useful similarity structure inside it, which\nfunctions as regularisation. Extensive experiments demonstrate the superiority\nof our proposal by comparing with the state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Fri, 8 Mar 2019 01:28:17 GMT"}, {"version": "v2", "created": "Wed, 3 Apr 2019 17:10:02 GMT"}, {"version": "v3", "created": "Tue, 16 Apr 2019 20:26:35 GMT"}, {"version": "v4", "created": "Wed, 1 May 2019 11:15:55 GMT"}, {"version": "v5", "created": "Sat, 3 Aug 2019 12:02:54 GMT"}, {"version": "v6", "created": "Thu, 7 May 2020 12:45:29 GMT"}, {"version": "v7", "created": "Mon, 22 Jun 2020 23:22:22 GMT"}, {"version": "v8", "created": "Fri, 19 Mar 2021 10:10:48 GMT"}], "update_date": "2021-03-22", "authors_parsed": [["Wang", "Xinshao", ""], ["Hua", "Yang", ""], ["Kodirov", "Elyor", ""], ["Robertson", "Neil M.", ""]]}, {"id": "1903.03273", "submitter": "Diana Wofk", "authors": "Diana Wofk, Fangchang Ma, Tien-Ju Yang, Sertac Karaman, Vivienne Sze", "title": "FastDepth: Fast Monocular Depth Estimation on Embedded Systems", "comments": "Accepted for presentation at ICRA 2019. 8 pages, 6 figures, 7 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Depth sensing is a critical function for robotic tasks such as localization,\nmapping and obstacle detection. There has been a significant and growing\ninterest in depth estimation from a single RGB image, due to the relatively low\ncost and size of monocular cameras. However, state-of-the-art single-view depth\nestimation algorithms are based on fairly complex deep neural networks that are\ntoo slow for real-time inference on an embedded platform, for instance, mounted\non a micro aerial vehicle. In this paper, we address the problem of fast depth\nestimation on embedded systems. We propose an efficient and lightweight\nencoder-decoder network architecture and apply network pruning to further\nreduce computational complexity and latency. In particular, we focus on the\ndesign of a low-latency decoder. Our methodology demonstrates that it is\npossible to achieve similar accuracy as prior work on depth estimation, but at\ninference speeds that are an order of magnitude faster. Our proposed network,\nFastDepth, runs at 178 fps on an NVIDIA Jetson TX2 GPU and at 27 fps when using\nonly the TX2 CPU, with active power consumption under 10 W. FastDepth achieves\nclose to state-of-the-art accuracy on the NYU Depth v2 dataset. To the best of\nthe authors' knowledge, this paper demonstrates real-time monocular depth\nestimation using a deep neural network with the lowest latency and highest\nthroughput on an embedded platform that can be carried by a micro aerial\nvehicle.\n", "versions": [{"version": "v1", "created": "Fri, 8 Mar 2019 04:07:01 GMT"}], "update_date": "2019-03-11", "authors_parsed": [["Wofk", "Diana", ""], ["Ma", "Fangchang", ""], ["Yang", "Tien-Ju", ""], ["Karaman", "Sertac", ""], ["Sze", "Vivienne", ""]]}, {"id": "1903.03295", "submitter": "Romero Morais", "authors": "Romero Morais, Vuong Le, Truyen Tran, Budhaditya Saha, Moussa Mansour,\n  Svetha Venkatesh", "title": "Learning Regularity in Skeleton Trajectories for Anomaly Detection in\n  Videos", "comments": "Accepted for publication in CVPR'19; Included link for source code", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Appearance features have been widely used in video anomaly detection even\nthough they contain complex entangled factors. We propose a new method to model\nthe normal patterns of human movements in surveillance video for anomaly\ndetection using dynamic skeleton features. We decompose the skeletal movements\ninto two sub-components: global body movement and local body posture. We model\nthe dynamics and interaction of the coupled features in our novel\nMessage-Passing Encoder-Decoder Recurrent Network. We observed that the\ndecoupled features collaboratively interact in our spatio-temporal model to\naccurately identify human-related irregular events from surveillance video\nsequences. Compared to traditional appearance-based models, our method achieves\nsuperior outlier detection performance. Our model also offers \"open-box\"\nexamination and decision explanation made possible by the semantically\nunderstandable features and a network architecture supporting interpretability.\n", "versions": [{"version": "v1", "created": "Fri, 8 Mar 2019 05:47:11 GMT"}, {"version": "v2", "created": "Thu, 18 Apr 2019 02:58:45 GMT"}], "update_date": "2019-04-19", "authors_parsed": [["Morais", "Romero", ""], ["Le", "Vuong", ""], ["Tran", "Truyen", ""], ["Saha", "Budhaditya", ""], ["Mansour", "Moussa", ""], ["Venkatesh", "Svetha", ""]]}, {"id": "1903.03299", "submitter": "Zhanzhan Cheng", "authors": "Zhanzhan Cheng, Jing Lu, Yi Niu, Shiliang Pu, Fei Wu and Shuigeng Zhou", "title": "You Only Recognize Once: Towards Fast Video Text Spotting", "comments": "Accepted by ACM Multimedia 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Video text spotting is still an important research topic due to its various\nreal-applications. Previous approaches usually fall into the four-staged\npipeline: text detection in individual images, framewisely recognizing\nlocalized text regions, tracking text streams and generating final results with\ncomplicated post-processing skills, which might suffer from the huge\ncomputational cost as well as the interferences of low-quality text. In this\npaper, we propose a fast and robust video text spotting framework by only\nrecognizing the localized text one-time instead of frame-wisely recognition.\nSpecifically, we first obtain text regions in videos with a well-designed\nspatial-temporal detector. Then we concentrate on developing a novel text\nrecommender for selecting the highest-quality text from text streams and only\nrecognizing the selected ones. Here, the recommender assembles text tracking,\nquality scoring and recognition into an end-to-end trainable module, which not\nonly avoids the interferences from low-quality text but also dramatically\nspeeds up the video text spotting process. In addition, we collect a larger\nscale video text dataset (LSVTD) for promoting the video text spotting\ncommunity, which contains 100 text videos from 22 different real-life\nscenarios. Extensive experiments on two public benchmarks show that our method\ngreatly speeds up the recognition process averagely by 71 times compared with\nthe frame-wise manner, and also achieves the remarkable state-of-the-art.\n", "versions": [{"version": "v1", "created": "Fri, 8 Mar 2019 06:21:10 GMT"}, {"version": "v2", "created": "Tue, 22 Oct 2019 01:33:38 GMT"}], "update_date": "2019-10-23", "authors_parsed": [["Cheng", "Zhanzhan", ""], ["Lu", "Jing", ""], ["Niu", "Yi", ""], ["Pu", "Shiliang", ""], ["Wu", "Fei", ""], ["Zhou", "Shuigeng", ""]]}, {"id": "1903.03303", "submitter": "Junyu Gao", "authors": "Qi Wang, Junyu Gao, Wei Lin and Yuan Yuan", "title": "Learning from Synthetic Data for Crowd Counting in the Wild", "comments": "Accepted by CVPR2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, counting the number of people for crowd scenes is a hot topic\nbecause of its widespread applications (e.g. video surveillance, public\nsecurity). It is a difficult task in the wild: changeable environment,\nlarge-range number of people cause the current methods can not work well. In\naddition, due to the scarce data, many methods suffer from over-fitting to a\ndifferent extent. To remedy the above two problems, firstly, we develop a data\ncollector and labeler, which can generate the synthetic crowd scenes and\nsimultaneously annotate them without any manpower. Based on it, we build a\nlarge-scale, diverse synthetic dataset. Secondly, we propose two schemes that\nexploit the synthetic data to boost the performance of crowd counting in the\nwild: 1) pretrain a crowd counter on the synthetic data, then finetune it using\nthe real data, which significantly prompts the model's performance on real\ndata; 2) propose a crowd counting method via domain adaptation, which can free\nhumans from heavy data annotations. Extensive experiments show that the first\nmethod achieves the state-of-the-art performance on four real datasets, and the\nsecond outperforms our baselines. The dataset and source code are available at\nhttps://gjy3035.github.io/GCC-CL/.\n", "versions": [{"version": "v1", "created": "Fri, 8 Mar 2019 06:40:50 GMT"}], "update_date": "2019-03-11", "authors_parsed": [["Wang", "Qi", ""], ["Gao", "Junyu", ""], ["Lin", "Wei", ""], ["Yuan", "Yuan", ""]]}, {"id": "1903.03313", "submitter": "Yong Xia", "authors": "Yutong Xie, Jianpeng Zhang, Yong Xia, and Chunhua Shen", "title": "A Mutual Bootstrapping Model for Automated Skin Lesion Segmentation and\n  Classification", "comments": "Accepted at IEEE Transactions on Medical Imaging, Early Access", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automated skin lesion segmentation and classification are two most essential\nand related tasks in the computer-aided diagnosis of skin cancer. Despite their\nprevalence, deep learning models are usually designed for only one task,\nignoring the potential benefits in jointly performing both tasks. In this\npaper, we propose the mutual bootstrapping deep convolutional neural networks\n(MB-DCNN) model for simultaneous skin lesion segmentation and classification.\nThis model consists of a coarse segmentation network (coarse-SN), a mask-guided\nclassification network (mask-CN), and an enhanced segmentation network\n(enhanced-SN). On one hand, the coarse-SN generates coarse lesion masks that\nprovide a prior bootstrapping for mask-CN to help it locate and classify skin\nlesions accurately. On the other hand, the lesion localization maps produced by\nmask-CN are then fed into enhanced-SN, aiming to transfer the localization\ninformation learned by mask-CN to enhanced-SN for accurate lesion segmentation.\nIn this way, both segmentation and classification networks mutually transfer\nknowledge between each other and facilitate each other in a bootstrapping way.\nMeanwhile, we also design a novel rank loss and jointly use it with the Dice\nloss in segmentation networks to address the issues caused by class imbalance\nand hard-easy pixel imbalance. We evaluate the proposed MB-DCNN model on the\nISIC-2017 and PH2 datasets, and achieve a Jaccard index of 80.4% and 89.4% in\nskin lesion segmentation and an average AUC of 93.8% and 97.7% in skin lesion\nclassification, which are superior to the performance of representative\nstate-of-the-art skin lesion segmentation and classification methods. Our\nresults suggest that it is possible to boost the performance of skin lesion\nsegmentation and classification simultaneously via training a unified model to\nperform both tasks in a mutual bootstrapping way.\n", "versions": [{"version": "v1", "created": "Fri, 8 Mar 2019 07:57:40 GMT"}, {"version": "v2", "created": "Tue, 12 Mar 2019 03:10:32 GMT"}, {"version": "v3", "created": "Tue, 17 Sep 2019 03:37:13 GMT"}, {"version": "v4", "created": "Wed, 12 Feb 2020 01:18:30 GMT"}], "update_date": "2020-02-13", "authors_parsed": [["Xie", "Yutong", ""], ["Zhang", "Jianpeng", ""], ["Xia", "Yong", ""], ["Shen", "Chunhua", ""]]}, {"id": "1903.03322", "submitter": "Weiyue Wang", "authors": "Weiyue Wang, Duygu Ceylan, Radomir Mech, Ulrich Neumann", "title": "3DN: 3D Deformation Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Applications in virtual and augmented reality create a demand for rapid\ncreation and easy access to large sets of 3D models. An effective way to\naddress this demand is to edit or deform existing 3D models based on a\nreference, e.g., a 2D image which is very easy to acquire. Given such a source\n3D model and a target which can be a 2D image, 3D model, or a point cloud\nacquired as a depth scan, we introduce 3DN, an end-to-end network that deforms\nthe source model to resemble the target. Our method infers per-vertex offset\ndisplacements while keeping the mesh connectivity of the source model fixed. We\npresent a training strategy which uses a novel differentiable operation, mesh\nsampling operator, to generalize our method across source and target models\nwith varying mesh densities. Mesh sampling operator can be seamlessly\nintegrated into the network to handle meshes with different topologies.\nQualitative and quantitative results show that our method generates higher\nquality results compared to the state-of-the art learning-based methods for 3D\nshape generation. Code is available at github.com/laughtervv/3DN.\n", "versions": [{"version": "v1", "created": "Fri, 8 Mar 2019 08:35:48 GMT"}], "update_date": "2019-03-11", "authors_parsed": [["Wang", "Weiyue", ""], ["Ceylan", "Duygu", ""], ["Mech", "Radomir", ""], ["Neumann", "Ulrich", ""]]}, {"id": "1903.03326", "submitter": "Tianshui Chen", "authors": "Tianshui Chen, Weihao Yu, Riquan Chen, Liang Lin", "title": "Knowledge-Embedded Routing Network for Scene Graph Generation", "comments": "Accepted by CVPR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To understand a scene in depth not only involves locating/recognizing\nindividual objects, but also requires to infer the relationships and\ninteractions among them. However, since the distribution of real-world\nrelationships is seriously unbalanced, existing methods perform quite poorly\nfor the less frequent relationships. In this work, we find that the statistical\ncorrelations between object pairs and their relationships can effectively\nregularize semantic space and make prediction less ambiguous, and thus well\naddress the unbalanced distribution issue. To achieve this, we incorporate\nthese statistical correlations into deep neural networks to facilitate scene\ngraph generation by developing a Knowledge-Embedded Routing Network. More\nspecifically, we show that the statistical correlations between objects\nappearing in images and their relationships, can be explicitly represented by a\nstructured knowledge graph, and a routing mechanism is learned to propagate\nmessages through the graph to explore their interactions. Extensive experiments\non the large-scale Visual Genome dataset demonstrate the superiority of the\nproposed method over current state-of-the-art competitors.\n", "versions": [{"version": "v1", "created": "Fri, 8 Mar 2019 08:53:02 GMT"}], "update_date": "2019-03-11", "authors_parsed": [["Chen", "Tianshui", ""], ["Yu", "Weihao", ""], ["Chen", "Riquan", ""], ["Lin", "Liang", ""]]}, {"id": "1903.03336", "submitter": "Nils Wandel", "authors": "Niloofar Azizi, Nils Wandel, Sven Behnke", "title": "Complex Valued Gated Auto-encoder for Video Frame Prediction", "comments": "To appear in: 27th European Symposium on Artificial Neural Networks,\n  Computational Intelligence and Machine Learning (ESANN), Bruges, Belgium,\n  2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, complex valued artificial neural networks have gained\nincreasing interest as they allow neural networks to learn richer\nrepresentations while potentially incorporating less parameters. Especially in\nthe domain of computer graphics, many traditional operations rely heavily on\ncomputations in the complex domain, thus complex valued neural networks apply\nnaturally. In this paper, we perform frame predictions in video sequences using\na complex valued gated auto-encoder. First, our method is motivated showing how\nthe Fourier transform can be seen as the basis for translational operations.\nThen, we present how a complex neural network can learn such transformations\nand compare its performance and parameter efficiency to a real-valued gated\nautoencoder. Furthermore, we show how extending both - the real and the complex\nvalued - neural networks by using convolutional units can significantly improve\nprediction performance and parameter efficiency. The networks are assessed on a\nmoving noise and a bouncing ball dataset.\n", "versions": [{"version": "v1", "created": "Fri, 8 Mar 2019 09:31:48 GMT"}], "update_date": "2019-03-11", "authors_parsed": [["Azizi", "Niloofar", ""], ["Wandel", "Nils", ""], ["Behnke", "Sven", ""]]}, {"id": "1903.03340", "submitter": "Mia Kokic", "authors": "Mia Kokic, Danica Kragic and Jeannette Bohg", "title": "Learning to Estimate Pose and Shape of Hand-Held Objects from RGB Images", "comments": null, "journal-ref": "IEEE/RSJ International Conference on Intelligent Robots and\n  Systems (IROS), 2019", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a system for modeling hand-object interactions in 3D from RGB\nimages that show a hand which is holding a novel object from a known category.\nWe design a Convolutional Neural Network (CNN) for Hand-held Object Pose and\nShape estimation called HOPS-Net and utilize prior work to estimate the hand\npose and configuration. We leverage the insight that information about the hand\nfacilitates object pose and shape estimation by incorporating the hand into\nboth training and inference of the object pose and shape as well as the\nrefinement of the estimated pose. The network is trained on a large synthetic\ndataset of objects in interaction with a human hand. To bridge the gap between\nreal and synthetic images, we employ an image-to-image translation model\n(Augmented CycleGAN) that generates realistically textured objects given a\nsynthetic rendering. This provides a scalable way of generating annotated data\nfor training HOPS-Net. Our quantitative experiments show that even noisy hand\nparameters significantly help object pose and shape estimation. The qualitative\nexperiments show results of pose and shape estimation of objects held by a hand\n\"in the wild\".\n", "versions": [{"version": "v1", "created": "Fri, 8 Mar 2019 09:40:51 GMT"}, {"version": "v2", "created": "Fri, 25 Oct 2019 12:56:27 GMT"}, {"version": "v3", "created": "Mon, 11 Nov 2019 13:37:32 GMT"}], "update_date": "2019-11-12", "authors_parsed": [["Kokic", "Mia", ""], ["Kragic", "Danica", ""], ["Bohg", "Jeannette", ""]]}, {"id": "1903.03341", "submitter": "Foteini Simistira Liwicki", "authors": "Rajkumar Saini, Derek Dobson, Jon Morrey, Marcus Liwicki, and Foteini\n  Simistira Liwicki", "title": "ICDAR 2019 Historical Document Reading Challenge on Large Structured\n  Chinese Family Records", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a Historical Document Reading Challenge on Large Chinese\nStructured Family Records, in short ICDAR2019 HDRC CHINESE. The objective of\nthe proposed competition is to recognize and analyze the layout, and finally\ndetect and recognize the textlines and characters of the large historical\ndocument collection containing more than 20 000 pages kindly provided by\nFamilySearch.\n", "versions": [{"version": "v1", "created": "Fri, 8 Mar 2019 09:48:32 GMT"}, {"version": "v2", "created": "Mon, 18 Mar 2019 10:54:15 GMT"}, {"version": "v3", "created": "Fri, 10 May 2019 06:49:26 GMT"}], "update_date": "2019-05-13", "authors_parsed": [["Saini", "Rajkumar", ""], ["Dobson", "Derek", ""], ["Morrey", "Jon", ""], ["Liwicki", "Marcus", ""], ["Liwicki", "Foteini Simistira", ""]]}, {"id": "1903.03349", "submitter": "Keelin Murphy", "authors": "Keelin Murphy, Shifa Salman Habib, Syed Mohammad Asad Zaidi, Saira\n  Khowaja, Aamir Khan, Jaime Melendez, Ernst T. Scholten, Farhan Amad, Steven\n  Schalekamp, Maurits Verhagen, Rick H. H. M. Philipsen, Annet Meijers, Bram\n  van Ginneken", "title": "Computer aided detection of tuberculosis on chest radiographs: An\n  evaluation of the CAD4TB v6 system", "comments": "Published in Scientific Reports", "journal-ref": "Scientific Reports 10, 5492 (2020)", "doi": "10.1038/s41598-020-62148-y", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  There is a growing interest in the automated analysis of chest X-Ray (CXR) as\na sensitive and inexpensive means of screening susceptible populations for\npulmonary tuberculosis. In this work we evaluate the latest version of CAD4TB,\na commercial software platform designed for this purpose. Version 6 of CAD4TB\nwas released in 2018 and is here tested on a fully independent dataset of 5565\nCXR images with GeneXpert (Xpert) sputum test results available (854 Xpert\npositive subjects). A subset of 500 subjects (50% Xpert positive) was reviewed\nand annotated by 5 expert observers independently to obtain a radiological\nreference standard. The latest version of CAD4TB is found to outperform all\nprevious versions in terms of area under receiver operating curve (ROC) with\nrespect to both Xpert and radiological reference standards. Improvements with\nrespect to Xpert are most apparent at high sensitivity levels with a\nspecificity of 76% obtained at a fixed 90% sensitivity. When compared with the\nradiological reference standard, CAD4TB v6 also outperformed previous versions\nby a considerable margin and achieved 98% specificity at the 90% sensitivity\nsetting. No substantial difference was found between the performance of CAD4TB\nv6 and any of the various expert observers against the Xpert reference\nstandard. A cost and efficiency analysis on this dataset demonstrates that in a\nstandard clinical situation, operating at 90% sensitivity, users of CAD4TB v6\ncan process 132 subjects per day at n average cost per screen of \\$5.95 per\nsubject, while users of version 3 process only 85 subjects per day at a cost of\n\\$8.38 per subject. At all tested operating points version 6 is shown to be\nmore efficient and cost effective than any other version.\n", "versions": [{"version": "v1", "created": "Fri, 8 Mar 2019 10:04:44 GMT"}, {"version": "v2", "created": "Thu, 2 Apr 2020 12:55:37 GMT"}], "update_date": "2020-04-03", "authors_parsed": [["Murphy", "Keelin", ""], ["Habib", "Shifa Salman", ""], ["Zaidi", "Syed Mohammad Asad", ""], ["Khowaja", "Saira", ""], ["Khan", "Aamir", ""], ["Melendez", "Jaime", ""], ["Scholten", "Ernst T.", ""], ["Amad", "Farhan", ""], ["Schalekamp", "Steven", ""], ["Verhagen", "Maurits", ""], ["Philipsen", "Rick H. H. M.", ""], ["Meijers", "Annet", ""], ["van Ginneken", "Bram", ""]]}, {"id": "1903.03372", "submitter": "Anjan Dutta", "authors": "Anjan Dutta and Zeynep Akata", "title": "Semantically Tied Paired Cycle Consistency for Zero-Shot Sketch-based\n  Image Retrieval", "comments": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\n  2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Zero-shot sketch-based image retrieval (SBIR) is an emerging task in computer\nvision, allowing to retrieve natural images relevant to sketch queries that\nmight not been seen in the training phase. Existing works either require\naligned sketch-image pairs or inefficient memory fusion layer for mapping the\nvisual information to a semantic space. In this work, we propose a semantically\naligned paired cycle-consistent generative (SEM-PCYC) model for zero-shot SBIR,\nwhere each branch maps the visual information to a common semantic space via an\nadversarial training. Each of these branches maintains a cycle consistency that\nonly requires supervision at category levels, and avoids the need of\nhighly-priced aligned sketch-image pairs. A classification criteria on the\ngenerators' outputs ensures the visual to semantic space mapping to be\ndiscriminating. Furthermore, we propose to combine textual and hierarchical\nside information via a feature selection auto-encoder that selects\ndiscriminating side information within a same end-to-end model. Our results\ndemonstrate a significant boost in zero-shot SBIR performance over the\nstate-of-the-art on the challenging Sketchy and TU-Berlin datasets.\n", "versions": [{"version": "v1", "created": "Fri, 8 Mar 2019 11:20:39 GMT"}], "update_date": "2019-03-11", "authors_parsed": [["Dutta", "Anjan", ""], ["Akata", "Zeynep", ""]]}, {"id": "1903.03374", "submitter": "Karim Armanious", "authors": "Karim Armanious, Chenming Jiang, Sherif Abdulatif, Thomas K\\\"ustner,\n  Sergios Gatidis, Bin Yang", "title": "Unsupervised Medical Image Translation Using Cycle-MedGAN", "comments": "Submitted to EUSIPCO 2019, 5 pages", "journal-ref": null, "doi": "10.23919/EUSIPCO.2019.8902799", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image-to-image translation is a new field in computer vision with multiple\npotential applications in the medical domain. However, for supervised image\ntranslation frameworks, co-registered datasets, paired in a pixel-wise sense,\nare required. This is often difficult to acquire in realistic medical\nscenarios. On the other hand, unsupervised translation frameworks often result\nin blurred translated images with unrealistic details. In this work, we propose\na new unsupervised translation framework which is titled Cycle-MedGAN. The\nproposed framework utilizes new non-adversarial cycle losses which direct the\nframework to minimize the textural and perceptual discrepancies in the\ntranslated images. Qualitative and quantitative comparisons against other\nunsupervised translation approaches demonstrate the performance of the proposed\nframework for PET-CT translation and MR motion correction.\n", "versions": [{"version": "v1", "created": "Fri, 8 Mar 2019 11:27:34 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["Armanious", "Karim", ""], ["Jiang", "Chenming", ""], ["Abdulatif", "Sherif", ""], ["K\u00fcstner", "Thomas", ""], ["Gatidis", "Sergios", ""], ["Yang", "Bin", ""]]}, {"id": "1903.03412", "submitter": "Dong Zhang", "authors": "Dong-dong Zhang, Lei Zhang, Vladimir Zaborovsky, Feng Xie, Yan-wen Wu,\n  Ting-ting Lu", "title": "Research on the pixel-based and object-oriented methods of urban feature\n  extraction with GF-2 remote-sensing images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.OH cs.CV cs.LG physics.data-an", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  During the rapid urbanization construction of China, acquisition of urban\ngeographic information and timely data updating are important and fundamental\ntasks for the refined management of cities. With the development of domestic\nremote sensing technology, the application of Gaofen-2 (GF-2) high-resolution\nremote sensing images can greatly improve the accuracy of information\nextraction. This paper introduces an approach using object-oriented\nclassification methods for urban feature extraction based on GF-2 satellite\ndata. A combination of spectral, spatial attributes and membership functions\nwas employed for mapping the urban features of Qinhuai District, Nanjing. The\ndata preprocessing is carried out by ENVI software, and the subsequent data is\nexported into the eCognition software for object-oriented classification and\nextraction of urban feature information. Finally, the obtained raster image\nclassification results are vectorized using the ARCGIS software, and the vector\ngraphics are stored in the library, which can be used for further analysis and\nmodeling. Accuracy assessment was performed using ground truth data acquired by\nvisual interpretation and from other reliable secondary data sources. Compared\nwith the result of pixel-based supervised (neural net) classification, the\ndeveloped object-oriented method can significantly improve extraction accuracy,\nand after manual interpretation, an overall accuracy of 95.44% can be achieved,\nwith a Kappa coefficient of 0.9405, which objectively confirmed the superiority\nof the object-oriented method and the feasibility of the utilization of GF-2\nsatellite data.\n", "versions": [{"version": "v1", "created": "Fri, 8 Mar 2019 15:19:36 GMT"}], "update_date": "2019-03-11", "authors_parsed": [["Zhang", "Dong-dong", ""], ["Zhang", "Lei", ""], ["Zaborovsky", "Vladimir", ""], ["Xie", "Feng", ""], ["Wu", "Yan-wen", ""], ["Lu", "Ting-ting", ""]]}, {"id": "1903.03445", "submitter": "Enzo Ferrante", "authors": "Nicolas Roulet and Diego Fernandez Slezak and Enzo Ferrante", "title": "Joint Learning of Brain Lesion and Anatomy Segmentation from\n  Heterogeneous Datasets", "comments": "Accepted for publication at MIDL 2019. Open reviews available at:\n  https://openreview.net/forum?id=Syest0rxlN", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Brain lesion and anatomy segmentation in magnetic resonance images are\nfundamental tasks in neuroimaging research and clinical practice. Given enough\ntraining data, convolutional neuronal networks (CNN) proved to outperform all\nexistent techniques in both tasks independently. However, to date, little work\nhas been done regarding simultaneous learning of brain lesion and anatomy\nsegmentation from disjoint datasets.\n  In this work we focus on training a single CNN model to predict brain tissue\nand lesion segmentations using heterogeneous datasets labeled independently,\naccording to only one of these tasks (a common scenario when using publicly\navailable datasets). We show that label contradiction issues can arise in this\ncase, and propose a novel adaptive cross entropy (ACE) loss function that makes\nsuch training possible. We provide quantitative evaluation in two different\nscenarios, benchmarking the proposed method in comparison with a multi-network\napproach. Our experiments suggest that ACE loss enables training of single\nmodels when standard cross entropy and Dice loss functions tend to fail.\nMoreover, we show that it is possible to achieve competitive results when\ncomparing with multiple networks trained for independent tasks.\n", "versions": [{"version": "v1", "created": "Fri, 8 Mar 2019 13:49:44 GMT"}, {"version": "v2", "created": "Mon, 15 Apr 2019 15:23:14 GMT"}], "update_date": "2019-04-16", "authors_parsed": [["Roulet", "Nicolas", ""], ["Slezak", "Diego Fernandez", ""], ["Ferrante", "Enzo", ""]]}, {"id": "1903.03462", "submitter": "Panagiotis Meletis", "authors": "Panagiotis Meletis and Gijs Dubbelman", "title": "On Boosting Semantic Street Scene Segmentation with Weak Supervision", "comments": "Oral presentation IEEE IV 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Training convolutional networks for semantic segmentation requires per-pixel\nground truth labels, which are very time consuming and hence costly to obtain.\nTherefore, in this work, we research and develop a hierarchical deep network\narchitecture and the corresponding loss for semantic segmentation that can be\ntrained from weak supervision, such as bounding boxes or image level labels, as\nwell as from strong per-pixel supervision. We demonstrate that the hierarchical\nstructure and the simultaneous training on strong (per-pixel) and weak\n(bounding boxes) labels, even from separate datasets, constantly increases the\nperformance against per-pixel only training. Moreover, we explore the more\nchallenging case of adding weak image-level labels. We collect street scene\nimages and weak labels from the immense Open Images dataset to generate the\nOpenScapes dataset, and we use this novel dataset to increase segmentation\nperformance on two established per-pixel labeled datasets, Cityscapes and\nVistas. We report performance gains up to +13.2% mIoU on crucial street scene\nclasses, and inference speed of 20 fps on a Titan V GPU for Cityscapes at 512 x\n1024 resolution. Our network and OpenScapes dataset are shared with the\nresearch community.\n", "versions": [{"version": "v1", "created": "Fri, 8 Mar 2019 14:16:11 GMT"}, {"version": "v2", "created": "Tue, 16 Jul 2019 14:07:16 GMT"}], "update_date": "2019-07-17", "authors_parsed": [["Meletis", "Panagiotis", ""], ["Dubbelman", "Gijs", ""]]}, {"id": "1903.03477", "submitter": "Pratik Kanani Mr.", "authors": "Vedant Singh, Manan Oza, Himanshu Vaghela and Pratik Kanani", "title": "Auto-Encoding Progressive Generative Adversarial Networks For 3D Multi\n  Object Scenes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  3D multi object generative models allow us to synthesize a large range of\nnovel 3D multi object scenes and also identify objects, shapes, layouts and\ntheir positions. But multi object scenes are difficult to create because of the\ndataset being multimodal in nature. The conventional 3D generative adversarial\nmodels are not efficient in generating multi object scenes, they usually tend\nto generate either one object or generate fuzzy results of multiple objects.\nAuto-encoder models have much scope in feature extraction and representation\nlearning using the unsupervised paradigm in probabilistic spaces. We try to\nmake use of this property in our proposed model. In this paper we propose a\nnovel architecture using 3DConvNets trained with the progressive training\nparadigm that has been able to generate realistic high resolution 3D scenes of\nrooms, bedrooms, offices etc. with various pieces of furniture and objects. We\nmake use of the adversarial auto-encoder along with the WGAN-GP loss parameter\nin our discriminator loss function. Finally this new approach to multi object\nscene generation has also been able to generate more number of objects per\nscene.\n", "versions": [{"version": "v1", "created": "Fri, 8 Mar 2019 14:53:16 GMT"}], "update_date": "2019-03-11", "authors_parsed": [["Singh", "Vedant", ""], ["Oza", "Manan", ""], ["Vaghela", "Himanshu", ""], ["Kanani", "Pratik", ""]]}, {"id": "1903.03479", "submitter": "Ovidiu Vaduvescu", "authors": "Dorian Gorgan, Ovidiu Vaduvescu, Teodor Stefanut, Victor Bacu, Adrian\n  Sabou, Denisa Copandean Balazs, Constantin Nandra, Costin Boldea, Afrodita\n  Boldea, Marian Predatu, Viktoria Pinter and Adrian Stanica", "title": "NEARBY Platform for Automatic Asteroids Detection and EURONEAR Surveys", "comments": "ESA NEO and Debris Detection Conference, ESA/ESOC, Darmstadt,\n  Germany, 22-24 Jan 2019", "journal-ref": "Published online by the ESA Space Safety Programme Office, 2019", "doi": null, "report-no": null, "categories": "astro-ph.IM cs.CV cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The survey of the nearby space and continuous monitoring of the Near Earth\nObjects (NEOs) and especially Near Earth Asteroids (NEAs) are essential for the\nfuture of our planet and should represent a priority for our solar system\nresearch and nearby space exploration. More computing power and sophisticated\ndigital tracking algorithms are needed to cope with the larger astronomy\nimaging cameras dedicated for survey telescopes. The paper presents the NEARBY\nplatform that aims to experiment new algorithms for automatic image reduction,\ndetection and validation of moving objects in astronomical surveys,\nspecifically NEAs. The NEARBY platform has been developed and experimented\nthrough a collaborative research work between the Technical University of\nCluj-Napoca (UTCN) and the University of Craiova, Romania, using observing\ninfrastructure of the Instituto de Astrofisica de Canarias (IAC) and Isaac\nNewton Group (ING), La Palma, Spain. The NEARBY platform has been developed and\ndeployed on the UTCN's cloud infrastructure and the acquired images are\nprocessed remotely by the astronomers who transfer it from ING through the web\ninterface of the NEARBY platform. The paper analyzes and highlights the main\naspects of the NEARBY platform development, and the results and conclusions on\nthe EURONEAR surveys.\n", "versions": [{"version": "v1", "created": "Fri, 8 Mar 2019 14:56:01 GMT"}], "update_date": "2019-03-11", "authors_parsed": [["Gorgan", "Dorian", ""], ["Vaduvescu", "Ovidiu", ""], ["Stefanut", "Teodor", ""], ["Bacu", "Victor", ""], ["Sabou", "Adrian", ""], ["Balazs", "Denisa Copandean", ""], ["Nandra", "Constantin", ""], ["Boldea", "Costin", ""], ["Boldea", "Afrodita", ""], ["Predatu", "Marian", ""], ["Pinter", "Viktoria", ""], ["Stanica", "Adrian", ""]]}, {"id": "1903.03491", "submitter": "Leif Bergerhoff", "authors": "Leif Bergerhoff and Marcelo C\\'ardenas and Joachim Weickert and Martin\n  Welk", "title": "Stable Backward Diffusion Models that Minimise Convex Energies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.CV cs.NA eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The inverse problem of backward diffusion is known to be ill-posed and highly\nunstable. Backward diffusion processes appear naturally in image enhancement\nand deblurring applications. It is therefore greatly desirable to establish a\nbackward diffusion model which implements a smart stabilisation approach that\ncan be used in combination with an easy to handle numerical scheme. So far,\nexisting stabilisation strategies in literature require sophisticated numerics\nto solve the underlying initial value problem. We derive a class of\nspace-discrete one-dimensional backward diffusion as gradient descent of\nenergies where we gain stability by imposing range constraints. Interestingly,\nthese energies are even convex. Furthermore, we establish a comprehensive\ntheory for the time-continuous evolution and we show that stability carries\nover to a simple explicit time discretisation of our model. Finally, we confirm\nthe stability and usefulness of our technique in experiments in which we\nenhance the contrast of digital greyscale and colour images.\n", "versions": [{"version": "v1", "created": "Fri, 8 Mar 2019 15:15:14 GMT"}, {"version": "v2", "created": "Wed, 17 Jun 2020 16:13:58 GMT"}], "update_date": "2020-06-18", "authors_parsed": [["Bergerhoff", "Leif", ""], ["C\u00e1rdenas", "Marcelo", ""], ["Weickert", "Joachim", ""], ["Welk", "Martin", ""]]}, {"id": "1903.03496", "submitter": "Simon Vandenhende", "authors": "Simon Vandenhende, Bert De Brabandere, Davy Neven and Luc Van Gool", "title": "A Three-Player GAN: Generating Hard Samples To Improve Classification\n  Networks", "comments": "Accepted for oral presentation at MVA2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a Three-Player Generative Adversarial Network to improve\nclassification networks. In addition to the game played between the\ndiscriminator and generator, a competition is introduced between the generator\nand the classifier. The generator's objective is to synthesize samples that are\nboth realistic and hard to label for the classifier. Even though we make no\nassumptions on the type of augmentations to learn, we find that the model is\nable to synthesize realistically looking examples that are hard for the\nclassification model. Furthermore, the classifier becomes more robust when\ntrained on these difficult samples. The method is evaluated on a public dataset\nfor traffic sign recognition.\n", "versions": [{"version": "v1", "created": "Fri, 8 Mar 2019 15:24:37 GMT"}], "update_date": "2019-03-11", "authors_parsed": [["Vandenhende", "Simon", ""], ["De Brabandere", "Bert", ""], ["Neven", "Davy", ""], ["Van Gool", "Luc", ""]]}, {"id": "1903.03503", "submitter": "Adrian Dalca", "authors": "Adrian V. Dalca, John Guttag, Mert R. Sabuncu", "title": "Unsupervised Data Imputation via Variational Inference of Deep Subspaces", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A wide range of systems exhibit high dimensional incomplete data. Accurate\nestimation of the missing data is often desired, and is crucial for many\ndownstream analyses. Many state-of-the-art recovery methods involve supervised\nlearning using datasets containing full observations. In contrast, we focus on\nunsupervised estimation of missing image data, where no full observations are\navailable - a common situation in practice. Unsupervised imputation methods for\nimages often employ a simple linear subspace to capture correlations between\ndata dimensions, omitting more complex relationships. In this work, we\nintroduce a general probabilistic model that describes sparse high dimensional\nimaging data as being generated by a deep non-linear embedding. We derive a\nlearning algorithm using a variational approximation based on convolutional\nneural networks and discuss its relationship to linear imputation models, the\nvariational auto encoder, and deep image priors. We introduce sparsity-aware\nnetwork building blocks that explicitly model observed and missing data. We\nanalyze proposed sparsity-aware network building blocks, evaluate our method on\npublic domain imaging datasets, and conclude by showing that our method enables\nimputation in an important real-world problem involving medical images. The\ncode is freely available as part of the \\verb|neuron| library at\nhttp://github.com/adalca/neuron.\n", "versions": [{"version": "v1", "created": "Fri, 8 Mar 2019 15:28:48 GMT"}], "update_date": "2019-03-15", "authors_parsed": [["Dalca", "Adrian V.", ""], ["Guttag", "John", ""], ["Sabuncu", "Mert R.", ""]]}, {"id": "1903.03509", "submitter": "David Castells-Rufas", "authors": "David Castells-Rufas, Jordi Carrabina", "title": "OpenCL-based FPGA accelerator for disparity map generation with\n  stereoscopic event cameras", "comments": "Presented at HIP3ES, 2019", "journal-ref": null, "doi": null, "report-no": "HIP3ES/2019/5", "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Although event-based cameras are already commercially available. Vision\nalgorithms based on them are still not common. As a consequence, there are few\nHardware Accelerators for them. In this work we present some experiments to\ncreate FPGA accelerators for a well-known vision algorithm using event-based\ncameras. We present a stereo matching algorithm to create a stream of disparity\nevents disparity map and implement several accelerators using the Intel FPGA\nOpenCL tool-chain. The results show that multiple designs can be easily tested\nand that a performance speedup of more than 8x can be achieved with simple code\ntransformations.\n", "versions": [{"version": "v1", "created": "Fri, 8 Mar 2019 15:39:27 GMT"}], "update_date": "2019-03-11", "authors_parsed": [["Castells-Rufas", "David", ""], ["Carrabina", "Jordi", ""]]}, {"id": "1903.03519", "submitter": "Ksenia Bittner", "authors": "Ksenia Bittner, Marco K\\\"orner, Peter Reinartz", "title": "DSM Building Shape Refinement from Combined Remote Sensing Images based\n  on Wnet-cGANs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe the workflow of a digital surface models (DSMs) refinement\nalgorithm using a hybrid conditional generative adversarial network (cGAN)\nwhere the generative part consists of two parallel networks merged at the last\nstage forming a WNet architecture. The inputs to the so-called WNet-cGAN are\nstereo DSMs and panchromatic (PAN) half-meter resolution satellite images.\nFusing these helps to propagate fine detailed information from a spectral image\nand complete the missing 3D knowledge from a stereo DSM about building shapes.\nBesides, it refines the building outlines and edges making them more\nrectangular and sharp.\n", "versions": [{"version": "v1", "created": "Fri, 8 Mar 2019 15:47:03 GMT"}], "update_date": "2019-03-11", "authors_parsed": [["Bittner", "Ksenia", ""], ["K\u00f6rner", "Marco", ""], ["Reinartz", "Peter", ""]]}, {"id": "1903.03545", "submitter": "Adrian Dalca", "authors": "Adrian V. Dalca, Guha Balakrishnan, John Guttag, Mert R. Sabuncu", "title": "Unsupervised Learning of Probabilistic Diffeomorphic Registration for\n  Images and Surfaces", "comments": "MedIA: Medical Image Analysis (MICCAI2018 Special Issue). Expands on\n  MICCAI 2018 paper (arXiv:1805.04605) by introducing an extension to\n  anatomical surface registration, new experiments, and analysis of\n  diffeomorphic implementations. Keywords: medical image registration;\n  diffeomorphic; invertible; probabilistic modeling; variational inference.\n  Code available at http://voxelmorph.csail.mit.edu. arXiv admin note: text\n  overlap with arXiv:1805.04605", "journal-ref": null, "doi": "10.1016/j.media.2019.07.006", "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Classical deformable registration techniques achieve impressive results and\noffer a rigorous theoretical treatment, but are computationally intensive since\nthey solve an optimization problem for each image pair. Recently,\nlearning-based methods have facilitated fast registration by learning spatial\ndeformation functions. However, these approaches use restricted deformation\nmodels, require supervised labels, or do not guarantee a diffeomorphic\n(topology-preserving) registration. Furthermore, learning-based registration\ntools have not been derived from a probabilistic framework that can offer\nuncertainty estimates.\n  In this paper, we build a connection between classical and learning-based\nmethods. We present a probabilistic generative model and derive an unsupervised\nlearning-based inference algorithm that uses insights from classical\nregistration methods and makes use of recent developments in convolutional\nneural networks (CNNs). We demonstrate our method on a 3D brain registration\ntask for both images and anatomical surfaces, and provide extensive empirical\nanalyses. Our principled approach results in state of the art accuracy and very\nfast runtimes, while providing diffeomorphic guarantees. Our implementation is\navailable at http://voxelmorph.csail.mit.edu.\n", "versions": [{"version": "v1", "created": "Fri, 8 Mar 2019 16:48:41 GMT"}, {"version": "v2", "created": "Tue, 23 Jul 2019 18:06:56 GMT"}], "update_date": "2019-07-26", "authors_parsed": [["Dalca", "Adrian V.", ""], ["Balakrishnan", "Guha", ""], ["Guttag", "John", ""], ["Sabuncu", "Mert R.", ""]]}, {"id": "1903.03546", "submitter": "Mira Rizkallah", "authors": "Mira Rizkallah, Thomas Maugey and Christine Guillemot", "title": "Prediction and Sampling with Local Graph Transforms for Quasi-Lossless\n  Light Field Compression", "comments": null, "journal-ref": null, "doi": "10.1109/TIP.2019.2959215", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph-based transforms have been shown to be powerful tools in terms of image\nenergy compaction. However, when the support increases to best capture signal\ndependencies, the computation of the basis functions becomes rapidly\nuntractable. This problem is in particular compelling for high dimensional\nimaging data such as light fields. The use of local transforms with limited\nsupports is a way to cope with this computational difficulty. Unfortunately,\nthe locality of the support may not allow us to fully exploit long term signal\ndependencies present in both the spatial and angular dimensions in the case of\nlight fields. This paper describes sampling and prediction schemes with local\ngraph-based transforms enabling to efficiently compact the signal energy and\nexploit dependencies beyond the local graph support. The proposed approach is\ninvestigated and is shown to be very efficient in the context of spatio-angular\ntransforms for quasi-lossless compression of light fields.\n", "versions": [{"version": "v1", "created": "Fri, 8 Mar 2019 16:49:49 GMT"}], "update_date": "2020-02-19", "authors_parsed": [["Rizkallah", "Mira", ""], ["Maugey", "Thomas", ""], ["Guillemot", "Christine", ""]]}, {"id": "1903.03556", "submitter": "Mira Rizkallah", "authors": "Mira Rizkallah, Xin Su, Thomas Maugey and Christine Guillemot", "title": "Geometry-Aware Graph Transforms for Light Field Compact Representation", "comments": null, "journal-ref": null, "doi": "10.1109/TIP.2019.2928873", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper addresses the problem of energy compaction of dense 4D light fields\nby designing geometry-aware local graph-based transforms. Local graphs are\nconstructed on super-rays that can be seen as a grouping of spatially and\ngeometry-dependent angularly correlated pixels. Both non separable and\nseparable transforms are considered. Despite the local support of limited size\ndefined by the super-rays, the Laplacian matrix of the non separable graph\nremains of high dimension and its diagonalization to compute the transform\neigen vectors remains computationally expensive. To solve this problem, we then\nperform the local spatio-angular transform in a separable manner. We show that\nwhen the shape of corresponding super-pixels in the different views is not\nisometric, the basis functions of the spatial transforms are not coherent,\nresulting in decreased correlation between spatial transform coefficients. We\nhence propose a novel transform optimization method that aims at preserving\nangular correlation even when the shapes of the super-pixels are not isometric.\nExperimental results show the benefit of the approach in terms of energy\ncompaction. A coding scheme is also described to assess the rate-distortion\nperfomances of the proposed transforms and is compared to state of the art\nencoders namely HEVC and JPEG Pleno VM 1.1.\n", "versions": [{"version": "v1", "created": "Fri, 8 Mar 2019 16:56:11 GMT"}], "update_date": "2019-10-23", "authors_parsed": [["Rizkallah", "Mira", ""], ["Su", "Xin", ""], ["Maugey", "Thomas", ""], ["Guillemot", "Christine", ""]]}, {"id": "1903.03691", "submitter": "Ayush Jaiswal", "authors": "Ayush Jaiswal, Shuai Xia, Iacopo Masi, Wael AbdAlmageed", "title": "RoPAD: Robust Presentation Attack Detection through Unsupervised\n  Adversarial Invariance", "comments": "To appear in Proceedings of International Conference on Biometrics\n  (ICB), 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For enterprise, personal and societal applications, there is now an\nincreasing demand for automated authentication of identity from images using\ncomputer vision. However, current authentication technologies are still\nvulnerable to presentation attacks. We present RoPAD, an end-to-end deep\nlearning model for presentation attack detection that employs unsupervised\nadversarial invariance to ignore visual distractors in images for increased\nrobustness and reduced overfitting. Experiments show that the proposed\nframework exhibits state-of-the-art performance on presentation attack\ndetection on several benchmark datasets.\n", "versions": [{"version": "v1", "created": "Fri, 8 Mar 2019 22:43:01 GMT"}, {"version": "v2", "created": "Wed, 20 Mar 2019 22:48:49 GMT"}], "update_date": "2019-03-22", "authors_parsed": [["Jaiswal", "Ayush", ""], ["Xia", "Shuai", ""], ["Masi", "Iacopo", ""], ["AbdAlmageed", "Wael", ""]]}, {"id": "1903.03695", "submitter": "Ashwini Tonge", "authors": "Ashwini Tonge and Cornelia Caragea", "title": "Image Privacy Prediction Using Deep Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CY", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Images today are increasingly shared online on social networking sites such\nas Facebook, Flickr, Foursquare, and Instagram. Despite that current social\nnetworking sites allow users to change their privacy preferences, this is often\na cumbersome task for the vast majority of users on the Web, who face\ndifficulties in assigning and managing privacy settings. Thus, automatically\npredicting images' privacy to warn users about private or sensitive content\nbefore uploading these images on social networking sites has become a necessity\nin our current interconnected world.\n  In this paper, we explore learning models to automatically predict\nappropriate images' privacy as private or public using carefully identified\nimage-specific features. We study deep visual semantic features that are\nderived from various layers of Convolutional Neural Networks (CNNs) as well as\ntextual features such as user tags and deep tags generated from deep CNNs.\nParticularly, we extract deep (visual and tag) features from four pre-trained\nCNN architectures for object recognition, i.e., AlexNet, GoogLeNet, VGG-16, and\nResNet, and compare their performance for image privacy prediction. Results of\nour experiments on a Flickr dataset of over thirty thousand images show that\nthe learning models trained on features extracted from ResNet outperform the\nstate-of-the-art models for image privacy prediction. We further investigate\nthe combination of user tags and deep tags derived from CNN architectures using\ntwo settings: (1) SVM on the bag-of-tags features; and (2) text-based CNN. Our\nresults show that even though the models trained on the visual features perform\nbetter than those trained on the tag features, the combination of deep visual\nfeatures with image tags shows improvements in performance over the individual\nfeature sets.\n", "versions": [{"version": "v1", "created": "Fri, 8 Mar 2019 23:12:12 GMT"}], "update_date": "2019-03-12", "authors_parsed": [["Tonge", "Ashwini", ""], ["Caragea", "Cornelia", ""]]}, {"id": "1903.03731", "submitter": "Hirak Jyoti Kashyap", "authors": "Hirak J Kashyap, Charless Fowlkes, Jeffrey L Krichmar", "title": "Sparse Representations for Object and Ego-motion Estimation in Dynamic\n  Scenes", "comments": "With supplementary material", "journal-ref": null, "doi": "10.1109/TNNLS.2020.3006467", "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dynamic scenes that contain both object motion and egomotion are a challenge\nfor monocular visual odometry (VO). Another issue with monocular VO is the\nscale ambiguity, i.e. these methods cannot estimate scene depth and camera\nmotion in real scale. Here, we propose a learning based approach to predict\ncamera motion parameters directly from optic flow, by marginalizing depthmap\nvariations and outliers. This is achieved by learning a sparse overcomplete\nbasis set of egomotion in an autoencoder network, which is able to eliminate\nirrelevant components of optic flow for the task of camera parameter or\nmotionfield estimation. The model is trained using a sparsity regularizer and a\nsupervised egomotion loss, and achieves the state-of-the-art performances on\ntrajectory prediction and camera rotation prediction tasks on KITTI and Virtual\nKITTI datasets, respectively. The sparse latent space egomotion representation\nlearned by the model is robust and requires only 5% of the hidden layer neurons\nto maintain the best trajectory prediction accuracy on KITTI dataset.\nAdditionally, in presence of depth information, the proposed method\ndemonstrates faithful object velocity prediction for wide range of object sizes\nand speeds by global compensation of predicted egomotion and a divisive\nnormalization procedure.\n", "versions": [{"version": "v1", "created": "Sat, 9 Mar 2019 03:56:53 GMT"}], "update_date": "2020-08-31", "authors_parsed": [["Kashyap", "Hirak J", ""], ["Fowlkes", "Charless", ""], ["Krichmar", "Jeffrey L", ""]]}, {"id": "1903.03736", "submitter": "Panwen Hu", "authors": "Panwen Hu, Zizheng Yan, Rui Huang and Feng Yin", "title": "How Effectively Can Indoor Wireless Positioning Relieve Visual Tracking\n  Pains: A Camera-Rao Bound Viewpoint", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual tracking is fragile in some difficult scenarios, for instance,\nappearance ambiguity and variation, occlusion can easily degrade most of visual\ntrackers to some extent. In this paper, visual tracking is empowered with\nwireless positioning to achieve high accuracy while maintaining robustness.\nFundamentally different from the previous works, this study does not involve\nany specific wireless positioning algorithms. Instead, we use the confidence\nregion derived from the wireless positioning Cramer-Rao bound (CRB) as the\nsearch region of visual trackers. The proposed framework is low-cost and very\nsimple to implement, yet readily leads to enhanced and robustified visual\ntracking performance in difficult scenarios as corroborated by our experimental\nresults. Most importantly, it is utmost valuable for the practioners to\npre-evaluate how effectively can the wireless resources available at hand\nalleviate the visual tracking pains.\n", "versions": [{"version": "v1", "created": "Sat, 9 Mar 2019 04:49:27 GMT"}], "update_date": "2019-03-12", "authors_parsed": [["Hu", "Panwen", ""], ["Yan", "Zizheng", ""], ["Huang", "Rui", ""], ["Yin", "Feng", ""]]}, {"id": "1903.03757", "submitter": "Yifei Shi", "authors": "Yifei Shi, Angel Xuan Chang, Zhelun Wu, Manolis Savva, Kai Xu", "title": "Hierarchy Denoising Recursive Autoencoders for 3D Scene Layout\n  Prediction", "comments": "CVPR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Indoor scenes exhibit rich hierarchical structure in 3D object layouts. Many\ntasks in 3D scene understanding can benefit from reasoning jointly about the\nhierarchical context of a scene, and the identities of objects. We present a\nvariational denoising recursive autoencoder (VDRAE) that generates and\niteratively refines a hierarchical representation of 3D object layouts,\ninterleaving bottom-up encoding for context aggregation and top-down decoding\nfor propagation. We train our VDRAE on large-scale 3D scene datasets to predict\nboth instance-level segmentations and a 3D object detections from an\nover-segmentation of an input point cloud. We show that our VDRAE improves\nobject detection performance on real-world 3D point cloud datasets compared to\nbaselines from prior work.\n", "versions": [{"version": "v1", "created": "Sat, 9 Mar 2019 07:56:21 GMT"}, {"version": "v2", "created": "Wed, 10 Apr 2019 15:38:09 GMT"}], "update_date": "2019-04-11", "authors_parsed": [["Shi", "Yifei", ""], ["Chang", "Angel Xuan", ""], ["Wu", "Zhelun", ""], ["Savva", "Manolis", ""], ["Xu", "Kai", ""]]}, {"id": "1903.03777", "submitter": "Xin Li Dr.", "authors": "Xin Li, Yiming Zhou, Zheng Pan, Jiashi Feng", "title": "Partial Order Pruning: for Best Speed/Accuracy Trade-off in Neural\n  Architecture Search", "comments": "Accepted to CVPR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Achieving good speed and accuracy trade-off on a target platform is very\nimportant in deploying deep neural networks in real world scenarios. However,\nmost existing automatic architecture search approaches only concentrate on high\nperformance. In this work, we propose an algorithm that can offer better\nspeed/accuracy trade-off of searched networks, which is termed \"Partial Order\nPruning\". It prunes the architecture search space with a partial order\nassumption to automatically search for the architectures with the best speed\nand accuracy trade-off. Our algorithm explicitly takes profile information\nabout the inference speed on the target platform into consideration. With the\nproposed algorithm, we present several Dongfeng (DF) networks that provide high\naccuracy and fast inference speed on various application GPU platforms. By\nfurther searching decoder architectures, our DF-Seg real-time segmentation\nnetworks yield state-of-the-art speed/accuracy trade-off on both the target\nembedded device and the high-end GPU.\n", "versions": [{"version": "v1", "created": "Sat, 9 Mar 2019 10:41:41 GMT"}, {"version": "v2", "created": "Fri, 5 Apr 2019 06:54:27 GMT"}], "update_date": "2019-04-08", "authors_parsed": [["Li", "Xin", ""], ["Zhou", "Yiming", ""], ["Pan", "Zheng", ""], ["Feng", "Jiashi", ""]]}, {"id": "1903.03785", "submitter": "Stylianos Ploumpis", "authors": "Stylianos Ploumpis, Haoyang Wang, Nick Pears, William A. P. Smith,\n  Stefanos Zafeiriou", "title": "Combining 3D Morphable Models: A Large scale Face-and-Head Model", "comments": "9 pages, 8 figures. To appear in the Proceedings of Computer Vision\n  and Pattern Recognition (CVPR), June 2019, Los Angeles, USA", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Three-dimensional Morphable Models (3DMMs) are powerful statistical tools for\nrepresenting the 3D surfaces of an object class. In this context, we identify\nan interesting question that has previously not received research attention: is\nit possible to combine two or more 3DMMs that (a) are built using different\ntemplates that perhaps only partly overlap, (b) have different representation\ncapabilities and (c) are built from different datasets that may not be\npublicly-available? In answering this question, we make two contributions.\nFirst, we propose two methods for solving this problem: i. use a regressor to\ncomplete missing parts of one model using the other, ii. use the Gaussian\nProcess framework to blend covariance matrices from multiple models. Second, as\nan example application of our approach, we build a new face-and-head shape\nmodel that combines the variability and facial detail of the LSFM with the full\nhead modelling of the LYHM. The resulting combined shape model achieves\nstate-of-the-art performance and outperforms existing head models by a large\nmargin. Finally, as an application experiment, we reconstruct full head\nrepresentations from single, unconstrained images by utilizing our proposed\nlarge-scale model in conjunction with the FaceWarehouse blendshapes for\nhandling expressions.\n", "versions": [{"version": "v1", "created": "Sat, 9 Mar 2019 11:35:30 GMT"}], "update_date": "2019-03-12", "authors_parsed": [["Ploumpis", "Stylianos", ""], ["Wang", "Haoyang", ""], ["Pears", "Nick", ""], ["Smith", "William A. P.", ""], ["Zafeiriou", "Stefanos", ""]]}, {"id": "1903.03793", "submitter": "Wenqi Shao", "authors": "Wenqi Shao, Tianjian Meng, Jingyu Li, Ruimao Zhang, Yudian Li,\n  Xiaogang Wang, Ping Luo", "title": "SSN: Learning Sparse Switchable Normalization via SparsestMax", "comments": "10 pages, 6 figures, accepted to CVPR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Normalization methods improve both optimization and generalization of\nConvNets. To further boost performance, the recently-proposed switchable\nnormalization (SN) provides a new perspective for deep learning: it learns to\nselect different normalizers for different convolution layers of a ConvNet.\nHowever, SN uses softmax function to learn importance ratios to combine\nnormalizers, leading to redundant computations compared to a single normalizer.\n  This work addresses this issue by presenting Sparse Switchable Normalization\n(SSN) where the importance ratios are constrained to be sparse. Unlike $\\ell_1$\nand $\\ell_0$ constraints that impose difficulties in optimization, we turn this\nconstrained optimization problem into feed-forward computation by proposing\nSparsestMax, which is a sparse version of softmax. SSN has several appealing\nproperties. (1) It inherits all benefits from SN such as applicability in\nvarious tasks and robustness to a wide range of batch sizes. (2) It is\nguaranteed to select only one normalizer for each normalization layer, avoiding\nredundant computations. (3) SSN can be transferred to various tasks in an\nend-to-end manner. Extensive experiments show that SSN outperforms its\ncounterparts on various challenging benchmarks such as ImageNet, Cityscapes,\nADE20K, and Kinetics.\n", "versions": [{"version": "v1", "created": "Sat, 9 Mar 2019 12:48:52 GMT"}], "update_date": "2019-03-12", "authors_parsed": [["Shao", "Wenqi", ""], ["Meng", "Tianjian", ""], ["Li", "Jingyu", ""], ["Zhang", "Ruimao", ""], ["Li", "Yudian", ""], ["Wang", "Xiaogang", ""], ["Luo", "Ping", ""]]}, {"id": "1903.03837", "submitter": "Mathias Unberath", "authors": "Laura Fink, Sing Chun Lee, Jie Ying Wu, Xingtong Liu, Tianyu Song,\n  Yordanka Stoyanova, Marc Stamminger, Nassir Navab, Mathias Unberath", "title": "LumiPath -- Towards Real-time Physically-based Rendering on Embedded\n  Devices", "comments": "To be presented at MICCAI 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the increasing computational power of today's workstations, real-time\nphysically-based rendering is within reach, rapidly gaining attention across a\nvariety of domains. These have expeditiously applied to medicine, where it is a\npowerful tool for intuitive 3D data visualization. Embedded devices such as\noptical see-through head-mounted displays (OST HMDs) have been a trend for\nmedical augmented reality. However, leveraging the obvious benefits of\nphysically-based rendering remains challenging on these devices because of\nlimited computational power, memory usage, and power consumption. We navigate\nthe compromise between device limitations and image quality to achieve\nreasonable rendering results by introducing a novel light field that can be\nsampled in real-time on embedded devices. We demonstrate its applications in\nmedicine and discuss limitations of the proposed method. An open-source version\nof this project is available at https://github.com/lorafib/LumiPath which\nprovides full insight on implementation and exemplary demonstrational material.\n", "versions": [{"version": "v1", "created": "Sat, 9 Mar 2019 17:49:44 GMT"}, {"version": "v2", "created": "Fri, 16 Aug 2019 14:07:40 GMT"}], "update_date": "2019-08-19", "authors_parsed": [["Fink", "Laura", ""], ["Lee", "Sing Chun", ""], ["Wu", "Jie Ying", ""], ["Liu", "Xingtong", ""], ["Song", "Tianyu", ""], ["Stoyanova", "Yordanka", ""], ["Stamminger", "Marc", ""], ["Navab", "Nassir", ""], ["Unberath", "Mathias", ""]]}, {"id": "1903.03838", "submitter": "Ali Harakeh", "authors": "Ali Harakeh, Michael Smart, Steven L. Waslander", "title": "BayesOD: A Bayesian Approach for Uncertainty Estimation in Deep Object\n  Detectors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  When incorporating deep neural networks into robotic systems, a major\nchallenge is the lack of uncertainty measures associated with their output\npredictions. Methods for uncertainty estimation in the output of deep object\ndetectors (DNNs) have been proposed in recent works, but have had limited\nsuccess due to 1) information loss at the detectors non-maximum suppression\n(NMS) stage, and 2) failure to take into account the multitask, many-to-one\nnature of anchor-based object detection. To that end, we introduce BayesOD, an\nuncertainty estimation approach that reformulates the standard object detector\ninference and Non-Maximum suppression components from a Bayesian perspective.\nExperiments performed on four common object detection datasets show that\nBayesOD provides uncertainty estimates that are better correlated with the\naccuracy of detections, manifesting as a significant reduction of\n9.77\\%-13.13\\% on the minimum Gaussian uncertainty error metric and a reduction\nof 1.63\\%-5.23\\% on the minimum Categorical uncertainty error metric. Code will\nbe released at {\\url{https://github.com/asharakeh/bayes-od-rc}}.\n", "versions": [{"version": "v1", "created": "Sat, 9 Mar 2019 17:56:31 GMT"}, {"version": "v2", "created": "Mon, 16 Sep 2019 15:55:05 GMT"}], "update_date": "2019-09-17", "authors_parsed": [["Harakeh", "Ali", ""], ["Smart", "Michael", ""], ["Waslander", "Steven L.", ""]]}, {"id": "1903.03878", "submitter": "Kuan Fang", "authors": "Kuan Fang, Alexander Toshev, Li Fei-Fei, Silvio Savarese", "title": "Scene Memory Transformer for Embodied Agents in Long-Horizon Tasks", "comments": "CVPR 2019 paper with supplementary material", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.RO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many robotic applications require the agent to perform long-horizon tasks in\npartially observable environments. In such applications, decision making at any\nstep can depend on observations received far in the past. Hence, being able to\nproperly memorize and utilize the long-term history is crucial. In this work,\nwe propose a novel memory-based policy, named Scene Memory Transformer (SMT).\nThe proposed policy embeds and adds each observation to a memory and uses the\nattention mechanism to exploit spatio-temporal dependencies. This model is\ngeneric and can be efficiently trained with reinforcement learning over long\nepisodes. On a range of visual navigation tasks, SMT demonstrates superior\nperformance to existing reactive and memory-based policies by a margin.\n", "versions": [{"version": "v1", "created": "Sat, 9 Mar 2019 22:03:02 GMT"}], "update_date": "2019-03-12", "authors_parsed": [["Fang", "Kuan", ""], ["Toshev", "Alexander", ""], ["Fei-Fei", "Li", ""], ["Savarese", "Silvio", ""]]}, {"id": "1903.03889", "submitter": "Yang Yang", "authors": "Yang Yang, Wenye Ma, Yin Zheng, Jian-Feng Cai, Weiyu Xu", "title": "Fast Single Image Reflection Suppression via Convex Optimization", "comments": "9 pages, 8 figures, IEEE Conference on Computer Vision and Pattern\n  Recognition (CVPR) 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Removing undesired reflections from images taken through the glass is of\ngreat importance in computer vision. It serves as a means to enhance the image\nquality for aesthetic purposes as well as to preprocess images in machine\nlearning and pattern recognition applications. We propose a convex model to\nsuppress the reflection from a single input image. Our model implies a partial\ndifferential equation with gradient thresholding, which is solved efficiently\nusing Discrete Cosine Transform. Extensive experiments on synthetic and\nreal-world images demonstrate that our approach achieves desirable reflection\nsuppression results and dramatically reduces the execution time.\n", "versions": [{"version": "v1", "created": "Sun, 10 Mar 2019 00:01:36 GMT"}, {"version": "v2", "created": "Sun, 28 Apr 2019 17:11:13 GMT"}, {"version": "v3", "created": "Fri, 10 May 2019 00:55:21 GMT"}], "update_date": "2019-05-13", "authors_parsed": [["Yang", "Yang", ""], ["Ma", "Wenye", ""], ["Zheng", "Yin", ""], ["Cai", "Jian-Feng", ""], ["Xu", "Weiyu", ""]]}, {"id": "1903.03893", "submitter": "Bin Wang", "authors": "Bin Wang, Yanan Sun, Bing Xue, Mengjie Zhang", "title": "A Hybrid GA-PSO Method for Evolving Architecture and Short Connections\n  of Deep Convolutional Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image classification is a difficult machine learning task, where\nConvolutional Neural Networks (CNNs) have been applied for over 20 years in\norder to solve the problem. In recent years, instead of the traditional way of\nonly connecting the current layer with its next layer, shortcut connections\nhave been proposed to connect the current layer with its forward layers apart\nfrom its next layer, which has been proved to be able to facilitate the\ntraining process of deep CNNs. However, there are various ways to build the\nshortcut connections, it is hard to manually design the best shortcut\nconnections when solving a particular problem, especially given the design of\nthe network architecture is already very challenging.\n  In this paper, a hybrid evolutionary computation (EC) method is proposed to\n\\textit{automatically} evolve both the architecture of deep CNNs and the\nshortcut connections. Three major contributions of this work are: Firstly, a\nnew encoding strategy is proposed to encode a CNN, where the architecture and\nthe shortcut connections are encoded separately; Secondly, a hybrid two-level\nEC method, which combines particle swarm optimisation and genetic algorithms,\nis developed to search for the optimal CNNs; Lastly, an adjustable learning\nrate is introduced for the fitness evaluations, which provides a better\nlearning rate for the training process given a fixed number of epochs. The\nproposed algorithm is evaluated on three widely used benchmark datasets of\nimage classification and compared with 12 peer Non-EC based competitors and one\nEC based competitor. The experimental results demonstrate that the proposed\nmethod outperforms all of the peer competitors in terms of classification\naccuracy.\n", "versions": [{"version": "v1", "created": "Sun, 10 Mar 2019 00:51:19 GMT"}], "update_date": "2019-03-12", "authors_parsed": [["Wang", "Bin", ""], ["Sun", "Yanan", ""], ["Xue", "Bing", ""], ["Zhang", "Mengjie", ""]]}, {"id": "1903.03896", "submitter": "Haofu Liao", "authors": "Haofu Liao, Wei-An Lin, Jiarui Zhang, Jingdan Zhang, Jiebo Luo, S.\n  Kevin Zhou", "title": "Multiview 2D/3D Rigid Registration via a Point-Of-Interest Network for\n  Tracking and Triangulation ($\\text{POINT}^2$)", "comments": "This work has been accepted to CVPR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose to tackle the problem of multiview 2D/3D rigid registration for\nintervention via a Point-Of-Interest Network for Tracking and Triangulation\n($\\text{POINT}^2$). $\\text{POINT}^2$ learns to establish 2D point-to-point\ncorrespondences between the pre- and intra-intervention images by tracking a\nset of random POIs. The 3D pose of the pre-intervention volume is then\nestimated through a triangulation layer. In $\\text{POINT}^2$, the unified\nframework of the POI tracker and the triangulation layer enables learning\ninformative 2D features and estimating 3D pose jointly. In contrast to existing\napproaches, $\\text{POINT}^2$ only requires a single forward-pass to achieve a\nreliable 2D/3D registration. As the POI tracker is shift-invariant,\n$\\text{POINT}^2$ is more robust to the initial pose of the 3D pre-intervention\nimage. Extensive experiments on a large-scale clinical cone-beam CT (CBCT)\ndataset show that the proposed $\\text{POINT}^2$ method outperforms the existing\nlearning-based method in terms of accuracy, robustness and running time.\nFurthermore, when used as an initial pose estimator, our method also improves\nthe robustness and speed of the state-of-the-art optimization-based approaches\nby ten folds.\n", "versions": [{"version": "v1", "created": "Sun, 10 Mar 2019 01:03:46 GMT"}, {"version": "v2", "created": "Mon, 8 Apr 2019 14:42:06 GMT"}, {"version": "v3", "created": "Mon, 27 May 2019 03:46:49 GMT"}, {"version": "v4", "created": "Tue, 22 Dec 2020 21:05:40 GMT"}], "update_date": "2020-12-24", "authors_parsed": [["Liao", "Haofu", ""], ["Lin", "Wei-An", ""], ["Zhang", "Jiarui", ""], ["Zhang", "Jingdan", ""], ["Luo", "Jiebo", ""], ["Zhou", "S. Kevin", ""]]}, {"id": "1903.03911", "submitter": "Kai Xu", "authors": "Xiaogang Wang, Bin Zhou, Yahao Shi, Xiaowu Chen, Qinping Zhao, Kai Xu", "title": "Shape2Motion: Joint Analysis of Motion Parts and Attributes from 3D\n  Shapes", "comments": "CVPR 2019 (oral presentation); Corresponding author: Kai Xu\n  (kevin.kai.xu@gmail.com); Project page:\n  www.kevinkaixu.net/projects/shape2motion.html", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For the task of mobility analysis of 3D shapes, we propose joint analysis for\nsimultaneous motion part segmentation and motion attribute estimation, taking a\nsingle 3D model as input. The problem is significantly different from those\ntackled in the existing works which assume the availability of either a\npre-existing shape segmentation or multiple 3D models in different motion\nstates. To that end, we develop Shape2Motion which takes a single 3D point\ncloud as input, and jointly computes a mobility-oriented segmentation and the\nassociated motion attributes. Shape2Motion is comprised of two deep neural\nnetworks designed for mobility proposal generation and mobility optimization,\nrespectively. The key contribution of these networks is the novel motion-driven\nfeatures and losses used in both motion part segmentation and motion attribute\nestimation. This is based on the observation that the movement of a functional\npart preserves the shape structure. We evaluate Shape2Motion with a newly\nproposed benchmark for mobility analysis of 3D shapes. Results demonstrate that\nour method achieves the state-of-the-art performance both in terms of motion\npart segmentation and motion attribute estimation.\n", "versions": [{"version": "v1", "created": "Sun, 10 Mar 2019 03:24:30 GMT"}, {"version": "v2", "created": "Tue, 12 Mar 2019 13:58:58 GMT"}], "update_date": "2019-03-13", "authors_parsed": [["Wang", "Xiaogang", ""], ["Zhou", "Bin", ""], ["Shi", "Yahao", ""], ["Chen", "Xiaowu", ""], ["Zhao", "Qinping", ""], ["Xu", "Kai", ""]]}, {"id": "1903.03927", "submitter": "Satyananda Kashyap", "authors": "Satyananda Kashyap, Honghai Zhang, Karan Rao, Milan Sonka", "title": "Learning-Based Cost Functions for 3D and 4D Multi-Surface Multi-Object\n  Segmentation of Knee MRI: Data from the Osteoarthritis Initiative", "comments": "IEEE Transactions in Medical Imaging, 11 pages", "journal-ref": "Published in: IEEE Transactions on Medical Imaging ( Volume: 37 ,\n  Issue: 5 , May 2018 )", "doi": "10.1109/TMI.2017.2781541", "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A fully automated knee MRI segmentation method to study osteoarthritis (OA)\nwas developed using a novel hierarchical set of random forests (RF) classifiers\nto learn the appearance of cartilage regions and their boundaries. A\nneighborhood approximation forest is used first to provide contextual feature\nto the second-level RF classifier that also considers local features and\nproduces location-specific costs for the layered optimal graph image\nsegmentation of multiple objects and surfaces (LOGISMOS) framework. Double echo\nsteady state (DESS) MRIs used in this work originated from the Osteoarthritis\nInitiative (OAI) study. Trained on 34 MRIs with varying degrees of OA, the\nperformance of the learning-based method tested on 108 MRIs showed a\nsignificant reduction in segmentation errors (\\emph{p}$<$0.05) compared with\nthe conventional gradient-based and single-stage RF-learned costs. The 3D\nLOGISMOS was extended to longitudinal-3D (4D) to simultaneously segment\nmultiple follow-up visits of the same patient. As such, data from all\ntime-points of the temporal sequence contribute information to a single optimal\nsolution that utilizes both spatial 3D and temporal contexts. 4D LOGISMOS\nvalidation on 108 MRIs from baseline and 12 month follow-up scans of 54\npatients showed a significant reduction in segmentation errors\n(\\emph{p}$<$0.01) compared to 3D. Finally, the potential of 4D LOGISMOS was\nfurther explored on the same 54 patients using 5 annual follow-up scans\ndemonstrating a significant improvement of measuring cartilage thickness\n(\\emph{p}$<$0.01) compared to the sequential 3D approach.\n", "versions": [{"version": "v1", "created": "Sun, 10 Mar 2019 05:48:53 GMT"}], "update_date": "2019-03-12", "authors_parsed": [["Kashyap", "Satyananda", ""], ["Zhang", "Honghai", ""], ["Rao", "Karan", ""], ["Sonka", "Milan", ""]]}, {"id": "1903.03929", "submitter": "Satyananda Kashyap", "authors": "Satyananda Kashyap, Ipek Oguz, Honghai Zhang, Milan Sonka", "title": "Automated Segmentation of Knee MRI Using Hierarchical Classifiers and\n  Just Enough Interaction Based Learning: Data from Osteoarthritis Initiative", "comments": "KEYWORDS: Graph based segmentation; Just enough interaction;\n  LOGISMOS; Neighborhood approximation forests; Osteoarthritis; Random forest\n  classifier; knee MRI PMID: 28626842 PMCID: PMC5471813", "journal-ref": "Med Image Comput Comput Assist Interv. 2016 Oct;9901:344-351. doi:\n  10.1007/978-3-319-46723-8_40. Epub 2016 Oct 2", "doi": "10.1007/978-3-319-46723-8_40", "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a fully automated learning-based approach for segmenting knee\ncartilage in the presence of osteoarthritis (OA). The algorithm employs a\nhierarchical set of two random forest classifiers. The first is a neighborhood\napproximation forest, the output probability map of which is utilized as a\nfeature set for the second random forest (RF) classifier. The output\nprobabilities of the hierarchical approach are used as cost functions in a\nLayered Optimal Graph Segmentation of Multiple Objects and Surfaces (LOGISMOS).\nIn this work, we highlight a novel post-processing interaction called\njust-enough interaction (JEI) which enables quick and accurate generation of a\nlarge set of training examples. Disjoint sets of 15 and 13 subjects were used\nfor training and tested on another disjoint set of 53 knee datasets. All images\nwere acquired using a double echo steady state (DESS) MRI sequence and are from\nthe osteoarthritis initiative (OAI) database. Segmentation performance using\nthe learning-based cost function showed significant reduction in segmentation\nerrors ($p< 0.05$) in comparison with conventional gradient-based cost\nfunctions.\n", "versions": [{"version": "v1", "created": "Sun, 10 Mar 2019 06:01:23 GMT"}], "update_date": "2019-03-12", "authors_parsed": [["Kashyap", "Satyananda", ""], ["Oguz", "Ipek", ""], ["Zhang", "Honghai", ""], ["Sonka", "Milan", ""]]}, {"id": "1903.03943", "submitter": "Bingbing Zhuang", "authors": "Bingbing Zhuang, Loong-Fah Cheong, Gim Hee Lee", "title": "Rolling-Shutter-Aware Differential SfM and Image Rectification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we develop a modified differential Structure from Motion (SfM)\nalgorithm that can estimate relative pose from two consecutive frames despite\nof Rolling Shutter (RS) artifacts. In particular, we show that under constant\nvelocity assumption, the errors induced by the rolling shutter effect can be\neasily rectified by a linear scaling operation on each optical flow. We further\npropose a 9-point algorithm to recover the relative pose of a rolling shutter\ncamera that undergoes constant acceleration motion. We demonstrate that the\ndense depth maps recovered from the relative pose of the RS camera can be used\nin a RS-aware warping for image rectification to recover high-quality Global\nShutter (GS) images. Experiments on both synthetic and real RS images show that\nour RS-aware differential SfM algorithm produces more accurate results on\nrelative pose estimation and 3D reconstruction from images distorted by RS\neffect compared to standard SfM algorithms that assume a GS camera model. We\nalso demonstrate that our RS-aware warping for image rectification method\noutperforms state-of-the-art commercial software products, i.e. Adobe After\nEffects and Apple Imovie, at removing RS artifacts.\n", "versions": [{"version": "v1", "created": "Sun, 10 Mar 2019 07:29:25 GMT"}, {"version": "v2", "created": "Sun, 4 Aug 2019 03:34:59 GMT"}], "update_date": "2019-08-06", "authors_parsed": [["Zhuang", "Bingbing", ""], ["Cheong", "Loong-Fah", ""], ["Lee", "Gim Hee", ""]]}, {"id": "1903.03953", "submitter": "Xinyi Ren", "authors": "Xinyi Ren, Jianlan Luo, Eugen Solowjow, Juan Aparicio Ojea, Abhishek\n  Gupta, Aviv Tamar, Pieter Abbeel", "title": "Domain Randomization for Active Pose Estimation", "comments": "Accepted at International Conference on Robotics and Automation\n  (ICRA) 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurate state estimation is a fundamental component of robotic control. In\nrobotic manipulation tasks, as is our focus in this work, state estimation is\nessential for identifying the positions of objects in the scene, forming the\nbasis of the manipulation plan. However, pose estimation typically requires\nexpensive 3D cameras or additional instrumentation such as fiducial markers to\nperform accurately. Recently, Tobin et al.~introduced an approach to pose\nestimation based on domain randomization, where a neural network is trained to\npredict pose directly from a 2D image of the scene. The network is trained on\ncomputer-generated images with a high variation in textures and lighting,\nthereby generalizing to real-world images. In this work, we investigate how to\nimprove the accuracy of domain randomization based pose estimation. Our main\nidea is that active perception -- moving the robot to get a better estimate of\npose -- can be trained in simulation and transferred to real using domain\nrandomization. In our approach, the robot trains in a domain-randomized\nsimulation how to estimate pose from a \\emph{sequence} of images. We show that\nour approach can significantly improve the accuracy of standard pose estimation\nin several scenarios: when the robot holding an object moves, when reference\nobjects are moved in the scene, or when the camera is moved around the object.\n", "versions": [{"version": "v1", "created": "Sun, 10 Mar 2019 08:33:14 GMT"}], "update_date": "2019-03-12", "authors_parsed": [["Ren", "Xinyi", ""], ["Luo", "Jianlan", ""], ["Solowjow", "Eugen", ""], ["Ojea", "Juan Aparicio", ""], ["Gupta", "Abhishek", ""], ["Tamar", "Aviv", ""], ["Abbeel", "Pieter", ""]]}, {"id": "1903.03956", "submitter": "Qianqian Xu", "authors": "Qianqian Xu, Zhiyong Yang, Yangbangyan Jiang, Xiaochun Cao, Qingming\n  Huang, Yuan Yao", "title": "Deep Robust Subjective Visual Property Prediction in Crowdsourcing", "comments": "9 pages, accepted by CVPR 2019 (Poster)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of estimating subjective visual properties (SVP) of images (e.g.,\nShoes A is more comfortable than B) is gaining rising attention. Due to its\nhighly subjective nature, different annotators often exhibit different\ninterpretations of scales when adopting absolute value tests. Therefore, recent\ninvestigations turn to collect pairwise comparisons via crowdsourcing\nplatforms. However, crowdsourcing data usually contains outliers. For this\npurpose, it is desired to develop a robust model for learning SVP from\ncrowdsourced noisy annotations. In this paper, we construct a deep SVP\nprediction model which not only leads to better detection of annotation\noutliers but also enables learning with extremely sparse annotations.\nSpecifically, we construct a comparison multi-graph based on the collected\nannotations, where different labeling results correspond to edges with\ndifferent directions between two vertexes. Then, we propose a generalized deep\nprobabilistic framework which consists of an SVP prediction module and an\noutlier modeling module that work collaboratively and are optimized jointly.\nExtensive experiments on various benchmark datasets demonstrate that our new\napproach guarantees promising results.\n", "versions": [{"version": "v1", "created": "Sun, 10 Mar 2019 09:29:53 GMT"}], "update_date": "2019-03-12", "authors_parsed": [["Xu", "Qianqian", ""], ["Yang", "Zhiyong", ""], ["Jiang", "Yangbangyan", ""], ["Cao", "Xiaochun", ""], ["Huang", "Qingming", ""], ["Yao", "Yuan", ""]]}, {"id": "1903.03989", "submitter": "Weiqi Ji", "authors": "Weiqi Ji, Zhuyin Ren and Chung K. Law", "title": "Uncertainty Propagation in Deep Neural Network Using Active Subspace", "comments": "Add link to github repo", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The inputs of deep neural network (DNN) from real-world data usually come\nwith uncertainties. Yet, it is challenging to propagate the uncertainty in the\ninput features to the DNN predictions at a low computational cost. This work\nemploys a gradient-based subspace method and response surface technique to\naccelerate the uncertainty propagation in DNN. Specifically, the active\nsubspace method is employed to identify the most important subspace in the\ninput features using the gradient of the DNN output to the inputs. Then the\nresponse surface within that low-dimensional subspace can be efficiently built,\nand the uncertainty of the prediction can be acquired by evaluating the\ncomputationally cheap response surface instead of the DNN models. In addition,\nthe subspace can help explain the adversarial examples. The approach is\ndemonstrated in MNIST datasets with a convolutional neural network. Code is\navailable at: https://github.com/jiweiqi/nnsubspace.\n", "versions": [{"version": "v1", "created": "Sun, 10 Mar 2019 13:38:43 GMT"}, {"version": "v2", "created": "Sat, 11 Jan 2020 22:34:28 GMT"}], "update_date": "2020-01-14", "authors_parsed": [["Ji", "Weiqi", ""], ["Ren", "Zhuyin", ""], ["Law", "Chung K.", ""]]}, {"id": "1903.04019", "submitter": "Zhang Zhaoxuan", "authors": "Xiaoguang Han, Zhaoxuan Zhang, Dong Du, Mingdai Yang, Jingming Yu, Pan\n  Pan, Xin Yang, Ligang Liu, Zixiang Xiong, Shuguang Cui", "title": "Deep Reinforcement Learning of Volume-guided Progressive View Inpainting\n  for 3D Point Scene Completion from a Single Depth Image", "comments": "Accepted as CVPR 2019 Oral", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a deep reinforcement learning method of progressive view\ninpainting for 3D point scene completion under volume guidance, achieving\nhigh-quality scene reconstruction from only a single depth image with severe\nocclusion. Our approach is end-to-end, consisting of three modules: 3D scene\nvolume reconstruction, 2D depth map inpainting, and multi-view selection for\ncompletion. Given a single depth image, our method first goes through the 3D\nvolume branch to obtain a volumetric scene reconstruction as a guide to the\nnext view inpainting step, which attempts to make up the missing information;\nthe third step involves projecting the volume under the same view of the input,\nconcatenating them to complete the current view depth, and integrating all\ndepth into the point cloud. Since the occluded areas are unavailable, we resort\nto a deep Q-Network to glance around and pick the next best view for large hole\ncompletion progressively until a scene is adequately reconstructed while\nguaranteeing validity. All steps are learned jointly to achieve robust and\nconsistent results. We perform qualitative and quantitative evaluations with\nextensive experiments on the SUNCG data, obtaining better results than the\nstate of the art.\n", "versions": [{"version": "v1", "created": "Sun, 10 Mar 2019 16:25:03 GMT"}, {"version": "v2", "created": "Tue, 12 Mar 2019 03:23:49 GMT"}], "update_date": "2019-03-13", "authors_parsed": [["Han", "Xiaoguang", ""], ["Zhang", "Zhaoxuan", ""], ["Du", "Dong", ""], ["Yang", "Mingdai", ""], ["Yu", "Jingming", ""], ["Pan", "Pan", ""], ["Yang", "Xin", ""], ["Liu", "Ligang", ""], ["Xiong", "Zixiang", ""], ["Cui", "Shuguang", ""]]}, {"id": "1903.04025", "submitter": "Xiaoyang Guo", "authors": "Xiaoyang Guo, Kai Yang, Wukui Yang, Xiaogang Wang, Hongsheng Li", "title": "Group-wise Correlation Stereo Network", "comments": "accepted to CVPR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Stereo matching estimates the disparity between a rectified image pair, which\nis of great importance to depth sensing, autonomous driving, and other related\ntasks. Previous works built cost volumes with cross-correlation or\nconcatenation of left and right features across all disparity levels, and then\na 2D or 3D convolutional neural network is utilized to regress the disparity\nmaps. In this paper, we propose to construct the cost volume by group-wise\ncorrelation. The left features and the right features are divided into groups\nalong the channel dimension, and correlation maps are computed among each group\nto obtain multiple matching cost proposals, which are then packed into a cost\nvolume. Group-wise correlation provides efficient representations for measuring\nfeature similarities and will not lose too much information like full\ncorrelation. It also preserves better performance when reducing parameters\ncompared with previous methods. The 3D stacked hourglass network proposed in\nprevious works is improved to boost the performance and decrease the inference\ncomputational cost. Experiment results show that our method outperforms\nprevious methods on Scene Flow, KITTI 2012, and KITTI 2015 datasets. The code\nis available at https://github.com/xy-guo/GwcNet\n", "versions": [{"version": "v1", "created": "Sun, 10 Mar 2019 17:34:54 GMT"}], "update_date": "2019-03-12", "authors_parsed": [["Guo", "Xiaoyang", ""], ["Yang", "Kai", ""], ["Yang", "Wukui", ""], ["Wang", "Xiaogang", ""], ["Li", "Hongsheng", ""]]}, {"id": "1903.04027", "submitter": "Satyananda Kashyap", "authors": "Satyananda Kashyap, Honghai Zhang, Milan Sonka", "title": "Just-Enough Interaction Approach to Knee MRI Segmentation: Data from the\n  Osteoarthritis Initiative", "comments": "Proceedings of the 3rd International Workshop on Interactive Medical\n  Image Computing (IMIC), Held in Conjunction with MICCAI, 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  State-of-the-art automated segmentation algorithms are not 100\\% accurate\nespecially when segmenting difficult to interpret datasets like those with\nsevere osteoarthritis (OA). We present a novel interactive method called\njust-enough interaction (JEI), which adds a fast correction step to the\nautomated layered optimal graph segmentation of multiple objects and surfaces\n(LOGISMOS). After LOGISMOS segmentation in knee MRI, the JEI user interaction\ndoes not modify boundary surfaces of the bones and cartilages directly. Local\ncosts of underlying graph nodes are modified instead and the graph is\nre-optimized, providing globally optimal corrected results. Significant\nperformance improvement ($p \\ll 0.001$) was observed when comparing\nJEI-corrected results to the automated. The algorithm was extended from 3D JEI\nto longitudinal multi-3D (4D) JEI allowing simultaneous visualization and\ninteraction of multiple-time points of the same patient.\n", "versions": [{"version": "v1", "created": "Sun, 10 Mar 2019 17:55:47 GMT"}], "update_date": "2019-03-12", "authors_parsed": [["Kashyap", "Satyananda", ""], ["Zhang", "Honghai", ""], ["Sonka", "Milan", ""]]}, {"id": "1903.04064", "submitter": "Chen-Yu Lee", "authors": "Chen-Yu Lee, Tanmay Batra, Mohammad Haris Baig, Daniel Ulbricht", "title": "Sliced Wasserstein Discrepancy for Unsupervised Domain Adaptation", "comments": "Accepted at CVPR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we connect two distinct concepts for unsupervised domain\nadaptation: feature distribution alignment between domains by utilizing the\ntask-specific decision boundary and the Wasserstein metric. Our proposed sliced\nWasserstein discrepancy (SWD) is designed to capture the natural notion of\ndissimilarity between the outputs of task-specific classifiers. It provides a\ngeometrically meaningful guidance to detect target samples that are far from\nthe support of the source and enables efficient distribution alignment in an\nend-to-end trainable fashion. In the experiments, we validate the effectiveness\nand genericness of our method on digit and sign recognition, image\nclassification, semantic segmentation, and object detection.\n", "versions": [{"version": "v1", "created": "Sun, 10 Mar 2019 21:56:45 GMT"}], "update_date": "2019-03-12", "authors_parsed": [["Lee", "Chen-Yu", ""], ["Batra", "Tanmay", ""], ["Baig", "Mohammad Haris", ""], ["Ulbricht", "Daniel", ""]]}, {"id": "1903.04090", "submitter": "Dinesh Kumar Vishwakarma Dr", "authors": "Tej Singh and Dinesh Kumar Vishwakarma", "title": "A Hybrid Framework for Action Recognition in Low-Quality Video Sequences", "comments": "13 pages, 9 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Vision-based activity recognition is essential for security, monitoring and\nsurveillance applications. Further, real-time analysis having low-quality video\nand contain less information about surrounding due to poor illumination, and\nocclusions. Therefore, it needs a more robust and integrated model for low\nquality and night security operations. In this context, we proposed a hybrid\nmodel for illumination invariant human activity recognition based on sub-image\nhistogram equalization enhancement and k-key pose human silhouettes. This\nfeature vector gives good average recognition accuracy on three low exposure\nvideo sequences subset of original actions video datasets. Finally, the\nperformance of the proposed approach is tested over three manually downgraded\nlow qualities Weizmann action, KTH, and Ballet Movement dataset. This model\noutperformed on low exposure videos over existing technique and achieved\ncomparable classification accuracy to similar state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Mon, 11 Mar 2019 00:57:20 GMT"}], "update_date": "2019-03-12", "authors_parsed": [["Singh", "Tej", ""], ["Vishwakarma", "Dinesh Kumar", ""]]}, {"id": "1903.04092", "submitter": "Osman Tursun", "authors": "Osman Tursun, Rui Zeng, Simon Denman, Sabesan Sivapalan, Sridha\n  Sridharan, Clinton Fookes", "title": "MTRNet: A Generic Scene Text Eraser", "comments": "Presented at ICDAR2019 Conference", "journal-ref": null, "doi": "10.1109/ICDAR.2019.00016", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Text removal algorithms have been proposed for uni-lingual scripts with\nregular shapes and layouts. However, to the best of our knowledge, a generic\ntext removal method which is able to remove all or user-specified text regions\nregardless of font, script, language or shape is not available. Developing such\na generic text eraser for real scenes is a challenging task, since it inherits\nall the challenges of multi-lingual and curved text detection and inpainting.\nTo fill this gap, we propose a mask-based text removal network (MTRNet). MTRNet\nis a conditional adversarial generative network (cGAN) with an auxiliary mask.\nThe introduced auxiliary mask not only makes the cGAN a generic text eraser,\nbut also enables stable training and early convergence on a challenging\nlarge-scale synthetic dataset, initially proposed for text detection in real\nscenes. What's more, MTRNet achieves state-of-the-art results on several\nreal-world datasets including ICDAR 2013, ICDAR 2017 MLT, and CTW1500, without\nbeing explicitly trained on this data, outperforming previous state-of-the-art\nmethods trained directly on these datasets.\n", "versions": [{"version": "v1", "created": "Mon, 11 Mar 2019 01:03:43 GMT"}, {"version": "v2", "created": "Tue, 12 Mar 2019 02:02:50 GMT"}, {"version": "v3", "created": "Tue, 22 Oct 2019 06:24:29 GMT"}], "update_date": "2020-02-11", "authors_parsed": [["Tursun", "Osman", ""], ["Zeng", "Rui", ""], ["Denman", "Simon", ""], ["Sivapalan", "Sabesan", ""], ["Sridharan", "Sridha", ""], ["Fookes", "Clinton", ""]]}, {"id": "1903.04104", "submitter": "Yixin Li", "authors": "Yixin Li, Shengqin Tang, Yun Ye, Jinwen Ma", "title": "Spatial-Aware Non-Local Attention for Fashion Landmark Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fashion landmark detection is a challenging task even using the current deep\nlearning techniques, due to the large variation and non-rigid deformation of\nclothes. In order to tackle these problems, we propose Spatial-Aware Non-Local\n(SANL) block, an attentive module in deep neural network which can utilize\nspatial information while capturing global dependency. Actually, the SANL block\nis constructed from the non-local block in the residual manner which can learn\nthe spatial related representation by taking a spatial attention map from\nGrad-CAM. We then establish our fashion landmark detection framework on feature\npyramid network, equipped with four SANL blocks in the backbone. It is\ndemonstrated by the experimental results on two large-scale fashion datasets\nthat our proposed fashion landmark detection approach with the SANL blocks\noutperforms the current state-of-the-art methods considerably. Some\nsupplementary experiments on fine-grained image classification also show the\neffectiveness of the proposed SANL block.\n", "versions": [{"version": "v1", "created": "Mon, 11 Mar 2019 02:35:39 GMT"}], "update_date": "2019-03-12", "authors_parsed": [["Li", "Yixin", ""], ["Tang", "Shengqin", ""], ["Ye", "Yun", ""], ["Ma", "Jinwen", ""]]}, {"id": "1903.04120", "submitter": "Pravendra Singh", "authors": "Pravendra Singh, Vinay Kumar Verma, Piyush Rai, Vinay P. Namboodiri", "title": "HetConv: Heterogeneous Kernel-Based Convolutions for Deep CNNs", "comments": "Accepted in CVPR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel deep learning architecture in which the convolution\noperation leverages heterogeneous kernels. The proposed HetConv (Heterogeneous\nKernel-Based Convolution) reduces the computation (FLOPs) and the number of\nparameters as compared to standard convolution operation while still\nmaintaining representational efficiency. To show the effectiveness of our\nproposed convolution, we present extensive experimental results on the standard\nconvolutional neural network (CNN) architectures such as VGG \\cite{vgg2014very}\nand ResNet \\cite{resnet}. We find that after replacing the standard\nconvolutional filters in these architectures with our proposed HetConv filters,\nwe achieve 3X to 8X FLOPs based improvement in speed while still maintaining\n(and sometimes improving) the accuracy. We also compare our proposed\nconvolutions with group/depth wise convolutions and show that it achieves more\nFLOPs reduction with significantly higher accuracy.\n", "versions": [{"version": "v1", "created": "Mon, 11 Mar 2019 04:20:38 GMT"}, {"version": "v2", "created": "Mon, 25 Mar 2019 09:52:55 GMT"}], "update_date": "2019-03-26", "authors_parsed": [["Singh", "Pravendra", ""], ["Verma", "Vinay Kumar", ""], ["Rai", "Piyush", ""], ["Namboodiri", "Vinay P.", ""]]}, {"id": "1903.04128", "submitter": "Stephen Tian", "authors": "Stephen Tian, Frederik Ebert, Dinesh Jayaraman, Mayur Mudigonda,\n  Chelsea Finn, Roberto Calandra, Sergey Levine", "title": "Manipulation by Feel: Touch-Based Control with Deep Predictive Models", "comments": "Accepted to ICRA 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Touch sensing is widely acknowledged to be important for dexterous robotic\nmanipulation, but exploiting tactile sensing for continuous, non-prehensile\nmanipulation is challenging. General purpose control techniques that are able\nto effectively leverage tactile sensing as well as accurate physics models of\ncontacts and forces remain largely elusive, and it is unclear how to even\nspecify a desired behavior in terms of tactile percepts. In this paper, we take\na step towards addressing these issues by combining high-resolution tactile\nsensing with data-driven modeling using deep neural network dynamics models. We\npropose deep tactile MPC, a framework for learning to perform tactile servoing\nfrom raw tactile sensor inputs, without manual supervision. We show that this\nmethod enables a robot equipped with a GelSight-style tactile sensor to\nmanipulate a ball, analog stick, and 20-sided die, learning from unsupervised\nautonomous interaction and then using the learned tactile predictive model to\nreposition each object to user-specified configurations, indicated by a goal\ntactile reading. Videos, visualizations and the code are available here:\nhttps://sites.google.com/view/deeptactilempc\n", "versions": [{"version": "v1", "created": "Mon, 11 Mar 2019 05:14:34 GMT"}], "update_date": "2019-03-12", "authors_parsed": [["Tian", "Stephen", ""], ["Ebert", "Frederik", ""], ["Jayaraman", "Dinesh", ""], ["Mudigonda", "Mayur", ""], ["Finn", "Chelsea", ""], ["Calandra", "Roberto", ""], ["Levine", "Sergey", ""]]}, {"id": "1903.04143", "submitter": "\\v{Z}iga Emer\\v{s}i\\v{c}", "authors": "\\v{Z}iga Emer\\v{s}i\\v{c}, Aruna Kumar S. V., B. S. Harish, Weronika\n  Gutfeter, Jalil Nourmohammadi Khiarak, Andrzej Pacut, Earnest Hansley,\n  Mauricio Pamplona Segundo, Sudeep Sarkar, Hyeonjung Park, Gi Pyo Nam, Ig-Jae\n  Kim, Sagar G. Sangodkar, \\\"Umit Ka\\c{c}ar, Murvet Kirci, Li Yuan, Jishou\n  Yuan, Haonan Zhao, Fei Lu, Junying Mao, Xiaoshuang Zhang, Dogucan Yaman,\n  Fevziye Irem Eyiokur, Kadir Bulut \\\"Ozler, Haz{\\i}m Kemal Ekenel, Debbrota\n  Paul Chowdhury, Sambit Bakshi, Pankaj K. Sa, Banshidhar Majhi, Peter Peer,\n  Vitomir \\v{S}truc", "title": "The Unconstrained Ear Recognition Challenge 2019 - ArXiv Version With\n  Appendix", "comments": "The content of this paper was published in ICB, 2019. This ArXiv\n  version is from before the peer review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a summary of the 2019 Unconstrained Ear Recognition\nChallenge (UERC), the second in a series of group benchmarking efforts centered\naround the problem of person recognition from ear images captured in\nuncontrolled settings. The goal of the challenge is to assess the performance\nof existing ear recognition techniques on a challenging large-scale ear dataset\nand to analyze performance of the technology from various viewpoints, such as\ngeneralization abilities to unseen data characteristics, sensitivity to\nrotations, occlusions and image resolution and performance bias on sub-groups\nof subjects, selected based on demographic criteria, i.e. gender and ethnicity.\nResearch groups from 12 institutions entered the competition and submitted a\ntotal of 13 recognition approaches ranging from descriptor-based methods to\ndeep-learning models. The majority of submissions focused on ensemble based\nmethods combining either representations from multiple deep models or\nhand-crafted with learned image descriptors. Our analysis shows that methods\nincorporating deep learning models clearly outperform techniques relying solely\non hand-crafted descriptors, even though both groups of techniques exhibit\nsimilar behaviour when it comes to robustness to various covariates, such\npresence of occlusions, changes in (head) pose, or variability in image\nresolution. The results of the challenge also show that there has been\nconsiderable progress since the first UERC in 2017, but that there is still\nample room for further research in this area.\n", "versions": [{"version": "v1", "created": "Mon, 11 Mar 2019 06:59:02 GMT"}, {"version": "v2", "created": "Tue, 12 Mar 2019 04:09:13 GMT"}, {"version": "v3", "created": "Thu, 14 Mar 2019 04:47:49 GMT"}], "update_date": "2019-03-15", "authors_parsed": [["Emer\u0161i\u010d", "\u017diga", ""], ["V.", "Aruna Kumar S.", ""], ["Harish", "B. S.", ""], ["Gutfeter", "Weronika", ""], ["Khiarak", "Jalil Nourmohammadi", ""], ["Pacut", "Andrzej", ""], ["Hansley", "Earnest", ""], ["Segundo", "Mauricio Pamplona", ""], ["Sarkar", "Sudeep", ""], ["Park", "Hyeonjung", ""], ["Nam", "Gi Pyo", ""], ["Kim", "Ig-Jae", ""], ["Sangodkar", "Sagar G.", ""], ["Ka\u00e7ar", "\u00dcmit", ""], ["Kirci", "Murvet", ""], ["Yuan", "Li", ""], ["Yuan", "Jishou", ""], ["Zhao", "Haonan", ""], ["Lu", "Fei", ""], ["Mao", "Junying", ""], ["Zhang", "Xiaoshuang", ""], ["Yaman", "Dogucan", ""], ["Eyiokur", "Fevziye Irem", ""], ["\u00d6zler", "Kadir Bulut", ""], ["Ekenel", "Haz\u0131m Kemal", ""], ["Chowdhury", "Debbrota Paul", ""], ["Bakshi", "Sambit", ""], ["Sa", "Pankaj K.", ""], ["Majhi", "Banshidhar", ""], ["Peer", "Peter", ""], ["\u0160truc", "Vitomir", ""]]}, {"id": "1903.04144", "submitter": "Shima Kamyab", "authors": "Shima Kamyab, Rasool Sabzi, Zohreh Azimifar", "title": "Deep Generative Models: Deterministic Prediction with an Application in\n  Inverse Rendering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep generative models are stochastic neural networks capable of learning the\ndistribution of data so as to generate new samples. Conditional Variational\nAutoencoder (CVAE) is a powerful deep generative model aiming at maximizing the\nlower bound of training data log-likelihood. In the CVAE structure, there is\nappropriate regularizer, which makes it applicable for suitably constraining\nthe solution space in solving ill-posed problems and providing high\ngeneralization power. Considering the stochastic prediction characteristic in\nCVAE, depending on the problem at hand, it is desirable to be able to control\nthe uncertainty in CVAE predictions. Therefore, in this paper we analyze the\nimpact of CVAE's condition on the diversity of solutions given by our designed\nCVAE in 3D shape inverse rendering as a prediction problem. The experimental\nresults using Modelnet10 and Shapenet datasets show the appropriate performance\nof our designed CVAE and verify the hypothesis: \\emph{\"The more informative the\nconditions in terms of object pose are, the less diverse the CVAE predictions\nare}\".\n", "versions": [{"version": "v1", "created": "Mon, 11 Mar 2019 07:01:51 GMT"}], "update_date": "2019-03-12", "authors_parsed": [["Kamyab", "Shima", ""], ["Sabzi", "Rasool", ""], ["Azimifar", "Zohreh", ""]]}, {"id": "1903.04147", "submitter": "Qiushan Guo", "authors": "Qiushan Guo, Yuan Dong, Yu Guo, Hongliang Bai", "title": "MSFD:Multi-Scale Receptive Field Face Detector", "comments": "Accepted by ICPR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We aim to study the multi-scale receptive fields of a single convolutional\nneural network to detect faces of varied scales. This paper presents our\nMulti-Scale Receptive Field Face Detector (MSFD), which has superior\nperformance on detecting faces at different scales and enjoys real-time\ninference speed. MSFD agglomerates context and texture by hierarchical\nstructure. More additional information and rich receptive field bring\nsignificant improvement but generate marginal time consumption. We\nsimultaneously propose an anchor assignment strategy which can cover faces with\na wide range of scales to improve the recall rate of small faces and rotated\nfaces. To reduce the false positive rate, we train our detector with focal loss\nwhich keeps the easy samples from overwhelming. As a result, MSFD reaches\nsuperior results on the FDDB, Pascal-Faces and WIDER FACE datasets, and can run\nat 31 FPS on GPU for VGA-resolution images.\n", "versions": [{"version": "v1", "created": "Mon, 11 Mar 2019 07:13:31 GMT"}], "update_date": "2019-03-12", "authors_parsed": [["Guo", "Qiushan", ""], ["Dong", "Yuan", ""], ["Guo", "Yu", ""], ["Bai", "Hongliang", ""]]}, {"id": "1903.04176", "submitter": "David M\\\"unch", "authors": "Patrick Schlosser, David M\\\"unch, Michael Arens", "title": "Investigation on Combining 3D Convolution of Image Data and Optical Flow\n  to Generate Temporal Action Proposals", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, several variants of two-stream architectures for temporal\naction proposal generation in long, untrimmed videos are presented. Inspired by\nthe recent advances in the field of human action recognition utilizing 3D\nconvolutions in combination with two-stream networks and based on the\nSingle-Stream Temporal Action Proposals (SST) architecture, four different\ntwo-stream architectures utilizing sequences of images on one stream and\nsequences of images of optical flow on the other stream are subsequently\ninvestigated. The four architectures fuse the two separate streams at different\ndepths in the model; for each of them, a broad range of parameters is\ninvestigated systematically as well as an optimal parametrization is\nempirically determined. The experiments on the THUMOS'14 dataset show that all\nfour two-stream architectures are able to outperform the original single-stream\nSST and achieve state of the art results. Additional experiments revealed that\nthe improvements are not restricted to a single method of calculating optical\nflow by exchanging the formerly used method of Brox with FlowNet2 and still\nachieving improvements.\n", "versions": [{"version": "v1", "created": "Mon, 11 Mar 2019 08:55:02 GMT"}, {"version": "v2", "created": "Thu, 14 Mar 2019 10:42:51 GMT"}], "update_date": "2019-03-15", "authors_parsed": [["Schlosser", "Patrick", ""], ["M\u00fcnch", "David", ""], ["Arens", "Michael", ""]]}, {"id": "1903.04191", "submitter": "Wouter Kouw", "authors": "Wouter M. Kouw, Silas N. {\\O}rting, Jens Petersen, Kim S. Pedersen,\n  Marleen de Bruijne", "title": "A cross-center smoothness prior for variational Bayesian brain tissue\n  segmentation", "comments": "12 pages, 2 figures, 1 table. Accepted to the International\n  Conference on Information Processing in Medical Imaging (2019)", "journal-ref": "International Conference on Information Processing in Medical\n  Imaging (IPMI), Hong Kong, 2019, pp. 360-371", "doi": "10.1007/978-3-030-20351-1_27", "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Suppose one is faced with the challenge of tissue segmentation in MR images,\nwithout annotators at their center to provide labeled training data. One option\nis to go to another medical center for a trained classifier. Sadly, tissue\nclassifiers do not generalize well across centers due to voxel intensity shifts\ncaused by center-specific acquisition protocols. However, certain aspects of\nsegmentations, such as spatial smoothness, remain relatively consistent and can\nbe learned separately. Here we present a smoothness prior that is fit to\nsegmentations produced at another medical center. This informative prior is\npresented to an unsupervised Bayesian model. The model clusters the voxel\nintensities, such that it produces segmentations that are similarly smooth to\nthose of the other medical center. In addition, the unsupervised Bayesian model\nis extended to a semi-supervised variant, which needs no visual interpretation\nof clusters into tissues.\n", "versions": [{"version": "v1", "created": "Mon, 11 Mar 2019 09:54:07 GMT"}], "update_date": "2019-08-28", "authors_parsed": [["Kouw", "Wouter M.", ""], ["\u00d8rting", "Silas N.", ""], ["Petersen", "Jens", ""], ["Pedersen", "Kim S.", ""], ["de Bruijne", "Marleen", ""]]}, {"id": "1903.04197", "submitter": "Yifan Liu", "authors": "Yifan Liu, Changyong Shun, Jingdong Wang, and Chunhua Shen", "title": "Structured Knowledge Distillation for Dense Prediction", "comments": "v1:10 pages cvpr2019 accepted; v2:15 pages for a journal version;\n  Code is available at:\n  https://github.com/irfanICMLL/structure_knowledge_distillation; fix typos", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we consider transferring the structure information from large\nnetworks to compact ones for dense prediction tasks in computer vision.\nPrevious knowledge distillation strategies used for dense prediction tasks\noften directly borrow the distillation scheme for image classification and\nperform knowledge distillation for each pixel separately, leading to\nsub-optimal performance. Here we propose to distill structured knowledge from\nlarge networks to compact networks, taking into account the fact that dense\nprediction is a structured prediction problem. Specifically, we study two\nstructured distillation schemes: i) pair-wise distillation that distills the\npair-wise similarities by building a static graph; and ii) holistic\ndistillation that uses adversarial training to distill holistic knowledge. The\neffectiveness of our knowledge distillation approaches is demonstrated by\nexperiments on three dense prediction tasks: semantic segmentation, depth\nestimation and object detection. Code is available at: https://git.io/StructKD\n", "versions": [{"version": "v1", "created": "Mon, 11 Mar 2019 10:05:09 GMT"}, {"version": "v2", "created": "Tue, 12 Mar 2019 00:38:30 GMT"}, {"version": "v3", "created": "Thu, 17 Oct 2019 00:12:14 GMT"}, {"version": "v4", "created": "Wed, 11 Dec 2019 05:32:03 GMT"}, {"version": "v5", "created": "Thu, 20 Feb 2020 23:52:50 GMT"}, {"version": "v6", "created": "Thu, 30 Apr 2020 10:25:50 GMT"}, {"version": "v7", "created": "Sun, 14 Jun 2020 13:37:24 GMT"}], "update_date": "2020-06-16", "authors_parsed": [["Liu", "Yifan", ""], ["Shun", "Changyong", ""], ["Wang", "Jingdong", ""], ["Shen", "Chunhua", ""]]}, {"id": "1903.04202", "submitter": "Andrea Pilzer", "authors": "Andrea Pilzer, St\\'ephane Lathuili\\`ere, Nicu Sebe, Elisa Ricci", "title": "Refine and Distill: Exploiting Cycle-Inconsistency and Knowledge\n  Distillation for Unsupervised Monocular Depth Estimation", "comments": "Accepted at CVPR2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nowadays, the majority of state of the art monocular depth estimation\ntechniques are based on supervised deep learning models. However, collecting\nRGB images with associated depth maps is a very time consuming procedure.\nTherefore, recent works have proposed deep architectures for addressing the\nmonocular depth prediction task as a reconstruction problem, thus avoiding the\nneed of collecting ground-truth depth. Following these works, we propose a\nnovel self-supervised deep model for estimating depth maps. Our framework\nexploits two main strategies: refinement via cycle-inconsistency and\ndistillation. Specifically, first a \\emph{student} network is trained to\npredict a disparity map such as to recover from a frame in a camera view the\nassociated image in the opposite view. Then, a backward cycle network is\napplied to the generated image to re-synthesize back the input image,\nestimating the opposite disparity. A third network exploits the inconsistency\nbetween the original and the reconstructed input frame in order to output a\nrefined depth map. Finally, knowledge distillation is exploited, such as to\ntransfer information from the refinement network to the student. Our extensive\nexperimental evaluation demonstrate the effectiveness of the proposed framework\nwhich outperforms state of the art unsupervised methods on the KITTI benchmark.\n", "versions": [{"version": "v1", "created": "Mon, 11 Mar 2019 10:29:29 GMT"}, {"version": "v2", "created": "Sat, 20 Apr 2019 13:52:36 GMT"}], "update_date": "2019-04-23", "authors_parsed": [["Pilzer", "Andrea", ""], ["Lathuili\u00e8re", "St\u00e9phane", ""], ["Sebe", "Nicu", ""], ["Ricci", "Elisa", ""]]}, {"id": "1903.04207", "submitter": "Samuel Remedios", "authors": "Samuel Remedios, Snehashis Roy, Justin Blaber, Camilo Bermudez,\n  Vishwesh Nath, Mayur B. Patel, John A. Butman, Bennett A. Landman, Dzung L.\n  Pham", "title": "Distributed deep learning for robust multi-site segmentation of CT\n  imaging after traumatic brain injury", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning models are becoming commonplace in the domain of medical\nimaging, and with these methods comes an ever-increasing need for more data.\nHowever, to preserve patient anonymity it is frequently impractical or\nprohibited to transfer protected health information (PHI) between institutions.\nAdditionally, due to the nature of some studies, there may not be a large\npublic dataset available on which to train models. To address this conundrum,\nwe analyze the efficacy of transferring the model itself in lieu of data\nbetween different sites. By doing so we accomplish two goals: 1) the model\ngains access to training on a larger dataset that it could not normally obtain\nand 2) the model better generalizes, having trained on data from separate\nlocations. In this paper, we implement multi-site learning with disparate\ndatasets from the National Institutes of Health (NIH) and Vanderbilt University\nMedical Center (VUMC) without compromising PHI. Three neural networks are\ntrained to convergence on a computed tomography (CT) brain hematoma\nsegmentation task: one only with NIH data,one only with VUMC data, and one\nmulti-site model alternating between NIH and VUMC data. Resultant lesion masks\nwith the multi-site model attain an average Dice similarity coefficient of 0.64\nand the automatically segmented hematoma volumes correlate to those done\nmanually with a Pearson correlation coefficient of 0.87,corresponding to an 8%\nand 5% improvement, respectively, over the single-site model counterparts.\n", "versions": [{"version": "v1", "created": "Mon, 11 Mar 2019 10:35:26 GMT"}], "update_date": "2019-03-12", "authors_parsed": [["Remedios", "Samuel", ""], ["Roy", "Snehashis", ""], ["Blaber", "Justin", ""], ["Bermudez", "Camilo", ""], ["Nath", "Vishwesh", ""], ["Patel", "Mayur B.", ""], ["Butman", "John A.", ""], ["Landman", "Bennett A.", ""], ["Pham", "Dzung L.", ""]]}, {"id": "1903.04227", "submitter": "Chuanxia Zheng", "authors": "Chuanxia Zheng, Tat-Jen Cham, Jianfei Cai", "title": "Pluralistic Image Completion", "comments": "21 pages, 16 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most image completion methods produce only one result for each masked input,\nalthough there may be many reasonable possibilities. In this paper, we present\nan approach for \\textbf{pluralistic image completion} -- the task of generating\nmultiple and diverse plausible solutions for image completion. A major\nchallenge faced by learning-based approaches is that usually only one ground\ntruth training instance per label. As such, sampling from conditional VAEs\nstill leads to minimal diversity. To overcome this, we propose a novel and\nprobabilistically principled framework with two parallel paths. One is a\nreconstructive path that utilizes the only one given ground truth to get prior\ndistribution of missing parts and rebuild the original image from this\ndistribution. The other is a generative path for which the conditional prior is\ncoupled to the distribution obtained in the reconstructive path. Both are\nsupported by GANs. We also introduce a new short+long term attention layer that\nexploits distant relations among decoder and encoder features, improving\nappearance consistency. When tested on datasets with buildings (Paris), faces\n(CelebA-HQ), and natural images (ImageNet), our method not only generated\nhigher-quality completion results, but also with multiple and diverse plausible\noutputs.\n", "versions": [{"version": "v1", "created": "Mon, 11 Mar 2019 11:44:56 GMT"}, {"version": "v2", "created": "Fri, 5 Apr 2019 15:24:19 GMT"}], "update_date": "2019-04-08", "authors_parsed": [["Zheng", "Chuanxia", ""], ["Cham", "Tat-Jen", ""], ["Cai", "Jianfei", ""]]}, {"id": "1903.04229", "submitter": "Caner Sahin", "authors": "Caner Sahin, Guillermo Garcia-Hernando, Juil Sock and Tae-Kyun Kim", "title": "Instance- and Category-level 6D Object Pose Estimation", "comments": "Book Chapter Submission. arXiv admin note: substantial text overlap\n  with arXiv:1706.03285", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  6D object pose estimation is an important task that determines the 3D\nposition and 3D rotation of an object in camera-centred coordinates. By\nutilizing such a task, one can propose promising solutions for various problems\nrelated to scene understanding, augmented reality, control and navigation of\nrobotics. Recent developments on visual depth sensors and low-cost availability\nof depth data significantly facilitate object pose estimation. Using depth\ninformation from RGB-D sensors, substantial progress has been made in the last\ndecade by the methods addressing the challenges such as viewpoint variability,\nocclusion and clutter, and similar looking distractors. Particularly, with the\nrecent advent of convolutional neural networks, RGB-only based solutions have\nbeen presented. However, improved results have only been reported for\nrecovering the pose of known instances, i.e., for the instance-level object\npose estimation tasks. More recently, state-of-the-art approaches target to\nsolve object pose estimation problem at the level of categories, recovering the\n6D pose of unknown instances. To this end, they address the challenges of the\ncategory-level tasks such as distribution shift among source and target\ndomains, high intra-class variations, and shape discrepancies between objects.\n", "versions": [{"version": "v1", "created": "Mon, 11 Mar 2019 11:45:46 GMT"}], "update_date": "2019-03-20", "authors_parsed": [["Sahin", "Caner", ""], ["Garcia-Hernando", "Guillermo", ""], ["Sock", "Juil", ""], ["Kim", "Tae-Kyun", ""]]}, {"id": "1903.04235", "submitter": "Zhao Kang", "authors": "Zhao Kang, Yiwei Lu, Yuanzhang Su, Changsheng Li, Zenglin Xu", "title": "Similarity Learning via Kernel Preserving Embedding", "comments": "Published in AAAI 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV cs.MM stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data similarity is a key concept in many data-driven applications. Many\nalgorithms are sensitive to similarity measures. To tackle this fundamental\nproblem, automatically learning of similarity information from data via\nself-expression has been developed and successfully applied in various models,\nsuch as low-rank representation, sparse subspace learning, semi-supervised\nlearning. However, it just tries to reconstruct the original data and some\nvaluable information, e.g., the manifold structure, is largely ignored. In this\npaper, we argue that it is beneficial to preserve the overall relations when we\nextract similarity information. Specifically, we propose a novel similarity\nlearning framework by minimizing the reconstruction error of kernel matrices,\nrather than the reconstruction error of original data adopted by existing work.\nTaking the clustering task as an example to evaluate our method, we observe\nconsiderable improvements compared to other state-of-the-art methods. More\nimportantly, our proposed framework is very general and provides a novel and\nfundamental building block for many other similarity-based tasks. Besides, our\nproposed kernel preserving opens up a large number of possibilities to embed\nhigh-dimensional data into low-dimensional space.\n", "versions": [{"version": "v1", "created": "Mon, 11 Mar 2019 11:58:40 GMT"}], "update_date": "2019-03-12", "authors_parsed": [["Kang", "Zhao", ""], ["Lu", "Yiwei", ""], ["Su", "Yuanzhang", ""], ["Li", "Changsheng", ""], ["Xu", "Zenglin", ""]]}, {"id": "1903.04246", "submitter": "Bastien Moysset", "authors": "Bastien Moysset and Ronaldo Messina", "title": "Manifold Mixup improves text recognition with CTC loss", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern handwritten text recognition techniques employ deep recurrent neural\nnetworks. The use of these techniques is especially efficient when a large\namount of annotated data is available for parameter estimation. Data\naugmentation can be used to enhance the performance of the systems when data is\nscarce. Manifold Mixup is a modern method of data augmentation that meld two\nimages or the feature maps corresponding to these images and the targets are\nfused accordingly. We propose to apply the Manifold Mixup to text recognition\nwhile adapting it to work with a Connectionist Temporal Classification cost. We\nshow that Manifold Mixup improves text recognition results on various languages\nand datasets.\n", "versions": [{"version": "v1", "created": "Mon, 11 Mar 2019 12:34:16 GMT"}], "update_date": "2019-03-12", "authors_parsed": [["Moysset", "Bastien", ""], ["Messina", "Ronaldo", ""]]}, {"id": "1903.04253", "submitter": "Georges Younes Mr.", "authors": "Georges Younes, Daniel Asmar, and John Zelek", "title": "A Unified Formulation for Visual Odometry", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Monocular Odometry systems can be broadly categorized as being either Direct,\nIndirect, or a hybrid of both. While Indirect systems process an alternative\nimage representation to compute geometric residuals, Direct methods process the\nimage pixels directly to generate photometric residuals. Both paradigms have\ndistinct but often complementary properties. This paper presents a Unified\nFormulation for Visual Odometry, referred to as UFVO, with the following key\ncontributions: (1) a tight coupling of photometric (Direct) and geometric\n(Indirect) measurements using a joint multi-objective optimization, (2) the use\nof a utility function as a decision maker that incorporates prior knowledge on\nboth paradigms, (3) descriptor sharing, where a feature can have more than one\ntype of descriptor and its different descriptors are used for tracking and\nmapping, (4) the depth estimation of both corner features and pixel features\nwithin the same map using an inverse depth parametrization, and (5) a corner\nand pixel selection strategy that extracts both types of information, while\npromoting a uniform distribution over the image domain. Experiments show that\nour proposed system can handle large inter-frame motions, inherits the\nsub-pixel accuracy of direct methods, can run efficiently in real-time, can\ngenerate an Indirect map representation at a marginal computational cost when\ncompared to traditional Indirect systems, all while outperforming state of the\nart in Direct, Indirect and hybrid systems.\n", "versions": [{"version": "v1", "created": "Mon, 11 Mar 2019 12:44:14 GMT"}], "update_date": "2019-03-12", "authors_parsed": [["Younes", "Georges", ""], ["Asmar", "Daniel", ""], ["Zelek", "John", ""]]}, {"id": "1903.04294", "submitter": "Yaxing Wang", "authors": "Yaxing Wang, Luis Herranz, Joost van de Weijer", "title": "Mix and match networks: cross-modal alignment for zero-pair\n  image-to-image translation", "comments": "Accepted by IJCV", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the problem of inferring unseen cross-modal\nimage-to-image translations between multiple modalities. We assume that only\nsome of the pairwise translations have been seen (i.e. trained) and infer the\nremaining unseen translations (where training pairs are not available). We\npropose mix and match networks, an approach where multiple encoders and\ndecoders are aligned in such a way that the desired translation can be obtained\nby simply cascading the source encoder and the target decoder, even when they\nhave not interacted during the training stage (i.e. unseen). The main challenge\nlies in the alignment of the latent representations at the bottlenecks of\nencoder-decoder pairs. We propose an architecture with several tools to\nencourage alignment, including autoencoders and robust side information and\nlatent consistency losses. We show the benefits of our approach in terms of\neffectiveness and scalability compared with other pairwise image-to-image\ntranslation approaches. We also propose zero-pair cross-modal image\ntranslation, a challenging setting where the objective is inferring semantic\nsegmentation from depth (and vice-versa) without explicit segmentation-depth\npairs, and only from two (disjoint) segmentation-RGB and depth-RGB training\nsets. We observe that a certain part of the shared information between unseen\nmodalities might not be reachable, so we further propose a variant that\nleverages pseudo-pairs which allows us to exploit this shared information\nbetween the unseen modalities.\n", "versions": [{"version": "v1", "created": "Fri, 8 Mar 2019 17:27:29 GMT"}, {"version": "v2", "created": "Fri, 15 May 2020 13:20:23 GMT"}], "update_date": "2020-05-18", "authors_parsed": [["Wang", "Yaxing", ""], ["Herranz", "Luis", ""], ["van de Weijer", "Joost", ""]]}, {"id": "1903.04352", "submitter": "Juan Eugenio Iglesias", "authors": "Juan Eugenio Iglesias and Koen Van Leemput and Polina Golland and\n  Anastasia Yendiki", "title": "Joint inference on structural and diffusion MRI for sequence-adaptive\n  Bayesian segmentation of thalamic nuclei with probabilistic atlases", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Segmentation of structural and diffusion MRI (sMRI/dMRI) is usually performed\nindependently in neuroimaging pipelines. However, some brain structures (e.g.,\nglobus pallidus, thalamus and its nuclei) can be extracted more accurately by\nfusing the two modalities. Following the framework of Bayesian segmentation\nwith probabilistic atlases and unsupervised appearance modeling, we present\nhere a novel algorithm to jointly segment multi-modal sMRI/dMRI data. We\npropose a hierarchical likelihood term for the dMRI defined on the unit ball,\nwhich combines the Beta and Dimroth-Scheidegger-Watson distributions to model\nthe data at each voxel. This term is integrated with a mixture of Gaussians for\nthe sMRI data, such that the resulting joint unsupervised likelihood enables\nthe analysis of multi-modal scans acquired with any type of MRI contrast,\nb-values, or number of directions, which enables wide applicability. We also\npropose an inference algorithm to estimate the maximum-a-posteriori model\nparameters from input images, and to compute the most likely segmentation.\nUsing a recently published atlas derived from histology, we apply our method to\nthalamic nuclei segmentation on two datasets: HCP (state of the art) and ADNI\n(legacy) - producing lower sample sizes than Bayesian segmentation with sMRI\nalone.\n", "versions": [{"version": "v1", "created": "Mon, 11 Mar 2019 15:11:28 GMT"}], "update_date": "2019-03-12", "authors_parsed": [["Iglesias", "Juan Eugenio", ""], ["Van Leemput", "Koen", ""], ["Golland", "Polina", ""], ["Yendiki", "Anastasia", ""]]}, {"id": "1903.04354", "submitter": "Dawood Al Chanti", "authors": "Dawood Al Chanti, Alice Caplier", "title": "ADS-ME: Anomaly Detection System for Micro-expression Spotting", "comments": "35 pages, 9 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Micro-expressions (MEs) are infrequent and uncontrollable facial events that\ncan highlight emotional deception and appear in a high-stakes environment. This\npaper propose an algorithm for spatiotemporal MEs spotting. Since MEs are\nunusual events, we treat them as abnormal patterns that diverge from expected\nNormal Facial Behaviour (NFBs) patterns. NFBs correspond to facial muscle\nactivations, eye blink/gaze events and mouth opening/closing movements that are\nall facial deformation but not MEs. We propose a probabilistic model to\nestimate the probability density function that models the spatiotemporal\ndistributions of NFBs patterns. To rank the outputs, we compute the negative\nlog-likelihood and we developed an adaptive thresholding technique to identify\nMEs from NFBs. While working only with NFBs data, the main challenge is to\ncapture intrinsic spatiotemoral features, hence we design a recurrent\nconvolutional autoencoder for feature representation. Finally, we show that our\nsystem is superior to previous works for MEs spotting.\n", "versions": [{"version": "v1", "created": "Mon, 11 Mar 2019 15:11:56 GMT"}], "update_date": "2019-03-12", "authors_parsed": [["Chanti", "Dawood Al", ""], ["Caplier", "Alice", ""]]}, {"id": "1903.04407", "submitter": "Pravendra Singh", "authors": "Pravendra Singh, Pratik Mazumder, Vinay P. Namboodiri", "title": "Accuracy Booster: Performance Boosting using Feature Map Re-calibration", "comments": "IEEE Winter Conference on Applications of Computer Vision (WACV),\n  2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolution Neural Networks (CNN) have been extremely successful in solving\nintensive computer vision tasks. The convolutional filters used in CNNs have\nplayed a major role in this success, by extracting useful features from the\ninputs. Recently researchers have tried to boost the performance of CNNs by\nre-calibrating the feature maps produced by these filters, e.g.,\nSqueeze-and-Excitation Networks (SENets). These approaches have achieved better\nperformance by Exciting up the important channels or feature maps while\ndiminishing the rest. However, in the process, architectural complexity has\nincreased. We propose an architectural block that introduces much lower\ncomplexity than the existing methods of CNN performance boosting while\nperforming significantly better than them. We carry out experiments on the\nCIFAR, ImageNet and MS-COCO datasets, and show that the proposed block can\nchallenge the state-of-the-art results. Our method boosts the ResNet-50\narchitecture to perform comparably to the ResNet-152 architecture, which is a\nthree times deeper network, on classification. We also show experimentally that\nour method is not limited to classification but also generalizes well to other\ntasks such as object detection.\n", "versions": [{"version": "v1", "created": "Mon, 11 Mar 2019 16:16:03 GMT"}, {"version": "v2", "created": "Tue, 7 Jan 2020 10:44:44 GMT"}], "update_date": "2020-01-08", "authors_parsed": [["Singh", "Pravendra", ""], ["Mazumder", "Pratik", ""], ["Namboodiri", "Vinay P.", ""]]}, {"id": "1903.04411", "submitter": "Zhewei Huang", "authors": "Zhewei Huang, Wen Heng, Shuchang Zhou", "title": "Learning to Paint With Model-based Deep Reinforcement Learning", "comments": "Accepted to ICCV 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show how to teach machines to paint like human painters, who can use a\nsmall number of strokes to create fantastic paintings. By employing a neural\nrenderer in model-based Deep Reinforcement Learning (DRL), our agents learn to\ndetermine the position and color of each stroke and make long-term plans to\ndecompose texture-rich images into strokes. Experiments demonstrate that\nexcellent visual effects can be achieved using hundreds of strokes. The\ntraining process does not require the experience of human painters or stroke\ntracking data. The code is available at\nhttps://github.com/hzwer/ICCV2019-LearningToPaint.\n", "versions": [{"version": "v1", "created": "Mon, 11 Mar 2019 16:21:46 GMT"}, {"version": "v2", "created": "Fri, 22 Mar 2019 20:36:19 GMT"}, {"version": "v3", "created": "Fri, 16 Aug 2019 08:02:02 GMT"}], "update_date": "2019-08-19", "authors_parsed": [["Huang", "Zhewei", ""], ["Heng", "Wen", ""], ["Zhou", "Shuchang", ""]]}, {"id": "1903.04473", "submitter": "Nikola Bani\\'c", "authors": "Nikola Bani\\'c, Karlo Ko\\v{s}{\\v{c}}evi\\'c, Marko Suba\\v{s}i\\'c, and\n  Sven Lon{\\v{c}}ari\\'c", "title": "The Past and the Present of the Color Checker Dataset Misuse", "comments": "5 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The pipelines of digital cameras contain a part for computational color\nconstancy, which aims to remove the influence of the illumination on the scene\ncolors. One of the best known and most widely used benchmark datasets for this\nproblem is the Color Checker dataset. However, due to the improper handling of\nthe black level in its images, this dataset has been widely misused and while\nsome recent publications tried to alleviate the problem, they nevertheless\nerred and created additional wrong data. This paper gives a history of the\nColor Checker dataset usage, it describes the origins and reasons for its\nmisuses, and it explains the old and new mistakes introduced in the most recent\npublications that tried to handle the issue. This should, hopefully, help to\nprevent similar future misuses.\n", "versions": [{"version": "v1", "created": "Mon, 11 Mar 2019 17:50:17 GMT"}], "update_date": "2019-03-12", "authors_parsed": [["Bani\u0107", "Nikola", ""], ["Ko\u0161{\u010d}evi\u0107", "Karlo", ""], ["Suba\u0161i\u0107", "Marko", ""], ["Lon{\u010d}ari\u0107", "Sven", ""]]}, {"id": "1903.04480", "submitter": "Junting Pan", "authors": "Junting Pan, Chengyu Wang, Xu Jia, Jing Shao, Lu Sheng, Junjie Yan and\n  Xiaogang Wang", "title": "Video Generation from Single Semantic Label Map", "comments": "Paper accepted at CVPR 2019. Source code and models available at\n  https://github.com/junting/seg2vid/tree/master", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes the novel task of video generation conditioned on a\nSINGLE semantic label map, which provides a good balance between flexibility\nand quality in the generation process. Different from typical end-to-end\napproaches, which model both scene content and dynamics in a single step, we\npropose to decompose this difficult task into two sub-problems. As current\nimage generation methods do better than video generation in terms of detail, we\nsynthesize high quality content by only generating the first frame. Then we\nanimate the scene based on its semantic meaning to obtain the temporally\ncoherent video, giving us excellent results overall. We employ a cVAE for\npredicting optical flow as a beneficial intermediate step to generate a video\nsequence conditioned on the initial single frame. A semantic label map is\nintegrated into the flow prediction module to achieve major improvements in the\nimage-to-video generation process. Extensive experiments on the Cityscapes\ndataset show that our method outperforms all competing methods.\n", "versions": [{"version": "v1", "created": "Mon, 11 Mar 2019 17:56:34 GMT"}], "update_date": "2019-03-12", "authors_parsed": [["Pan", "Junting", ""], ["Wang", "Chengyu", ""], ["Jia", "Xu", ""], ["Shao", "Jing", ""], ["Sheng", "Lu", ""], ["Yan", "Junjie", ""], ["Wang", "Xiaogang", ""]]}, {"id": "1903.04552", "submitter": "Nilaksh Das", "authors": "Nilaksh Das, Sanya Chaba, Renzhi Wu, Sakshi Gandhi, Duen Horng Chau,\n  Xu Chu", "title": "GOGGLES: Automatic Image Labeling with Affinity Coding", "comments": "Published at 2020 ACM SIGMOD International Conference on Management\n  of Data", "journal-ref": null, "doi": "10.1145/3318464.3380592", "report-no": null, "categories": "cs.CV cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generating large labeled training data is becoming the biggest bottleneck in\nbuilding and deploying supervised machine learning models. Recently, the data\nprogramming paradigm has been proposed to reduce the human cost in labeling\ntraining data. However, data programming relies on designing labeling functions\nwhich still requires significant domain expertise. Also, it is prohibitively\ndifficult to write labeling functions for image datasets as it is hard to\nexpress domain knowledge using raw features for images (pixels).\n  We propose affinity coding, a new domain-agnostic paradigm for automated\ntraining data labeling. The core premise of affinity coding is that the\naffinity scores of instance pairs belonging to the same class on average should\nbe higher than those of pairs belonging to different classes, according to some\naffinity functions. We build the GOGGLES system that implements affinity coding\nfor labeling image datasets by designing a novel set of reusable affinity\nfunctions for images, and propose a novel hierarchical generative model for\nclass inference using a small development set.\n  We compare GOGGLES with existing data programming systems on 5 image labeling\ntasks from diverse domains. GOGGLES achieves labeling accuracies ranging from a\nminimum of 71% to a maximum of 98% without requiring any extensive human\nannotation. In terms of end-to-end performance, GOGGLES outperforms the\nstate-of-the-art data programming system Snuba by 21% and a state-of-the-art\nfew-shot learning technique by 5%, and is only 7% away from the fully\nsupervised upper bound.\n", "versions": [{"version": "v1", "created": "Mon, 11 Mar 2019 19:19:30 GMT"}, {"version": "v2", "created": "Sun, 1 Mar 2020 02:43:24 GMT"}, {"version": "v3", "created": "Tue, 3 Mar 2020 06:30:24 GMT"}], "update_date": "2020-03-04", "authors_parsed": [["Das", "Nilaksh", ""], ["Chaba", "Sanya", ""], ["Wu", "Renzhi", ""], ["Gandhi", "Sakshi", ""], ["Chau", "Duen Horng", ""], ["Chu", "Xu", ""]]}, {"id": "1903.04586", "submitter": "Thomas Verelst", "authors": "Thomas Verelst, Matthew Blaschko and Maxim Berman", "title": "Generating superpixels using deep image representations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Superpixel algorithms are a common pre-processing step for computer vision\nalgorithms such as segmentation, object tracking and localization. Many\nsuperpixel methods only rely on colors features for segmentation, limiting\nperformance in low-contrast regions and applicability to infrared or medical\nimages where object boundaries have wide appearance variability. We study the\ninclusion of deep image features in the SLIC superpixel algorithm to exploit\nhigher-level image representations. In addition, we devise a trainable\nsuperpixel algorithm, yielding an intermediate domain-specific image\nrepresentation that can be applied to different tasks. A clustering-based\nsuperpixel algorithm is transformed into a pixel-wise classification task and\nsuperpixel training data is derived from semantic segmentation datasets. Our\nresults demonstrate that this approach is able to improve superpixel quality\nconsistently.\n", "versions": [{"version": "v1", "created": "Mon, 11 Mar 2019 20:24:30 GMT"}], "update_date": "2019-03-13", "authors_parsed": [["Verelst", "Thomas", ""], ["Blaschko", "Matthew", ""], ["Berman", "Maxim", ""]]}, {"id": "1903.04596", "submitter": "Ren Yang", "authors": "Ren Yang, Xiaoyan Sun, Mai Xu and Wenjun Zeng", "title": "Quality-Gated Convolutional LSTM for Enhancing Compressed Video", "comments": "Accepted to IEEE International Conference on Multimedia and Expo\n  (ICME) 2019", "journal-ref": null, "doi": "10.1109/ICME.2019.00098", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The past decade has witnessed great success in applying deep learning to\nenhance the quality of compressed video. However, the existing approaches aim\nat quality enhancement on a single frame, or only using fixed neighboring\nframes. Thus they fail to take full advantage of the inter-frame correlation in\nthe video. This paper proposes the Quality-Gated Convolutional Long Short-Term\nMemory (QG-ConvLSTM) network with bi-directional recurrent structure to fully\nexploit the advantageous information in a large range of frames. More\nimportantly, due to the obvious quality fluctuation among compressed frames,\nhigher quality frames can provide more useful information for other frames to\nenhance quality. Therefore, we propose learning the \"forget\" and \"input\" gates\nin the ConvLSTM cell from quality-related features. As such, the frames with\nvarious quality contribute to the memory in ConvLSTM with different importance,\nmaking the information of each frame reasonably and adequately used. Finally,\nthe experiments validate the effectiveness of our QG-ConvLSTM approach in\nadvancing the state-of-the-art quality enhancement of compressed video, and the\nablation study shows that our QG-ConvLSTM approach is learnt to make a\ntrade-off between quality and correlation when leveraging multi-frame\ninformation. The project page: https://github.com/ryangchn/QG-ConvLSTM.git.\n", "versions": [{"version": "v1", "created": "Mon, 11 Mar 2019 20:44:53 GMT"}, {"version": "v2", "created": "Thu, 28 Mar 2019 16:26:59 GMT"}, {"version": "v3", "created": "Sun, 14 Apr 2019 08:50:34 GMT"}], "update_date": "2019-08-13", "authors_parsed": [["Yang", "Ren", ""], ["Sun", "Xiaoyan", ""], ["Xu", "Mai", ""], ["Zeng", "Wenjun", ""]]}, {"id": "1903.04630", "submitter": "Xiaoshui Huang", "authors": "Xiaoshui Huang, Lixin Fan, Qiang Wu, Jian Zhang, Chun Yuan", "title": "Fast Registration for cross-source point clouds by using weak regional\n  affinity and pixel-wise refinement", "comments": "ICME 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many types of 3D acquisition sensors have emerged in recent years and point\ncloud has been widely used in many areas. Accurate and fast registration of\ncross-source 3D point clouds from different sensors is an emerged research\nproblem in computer vision. This problem is extremely challenging because\ncross-source point clouds contain a mixture of various variances, such as\ndensity, partial overlap, large noise and outliers, viewpoint changing. In this\npaper, an algorithm is proposed to align cross-source point clouds with both\nhigh accuracy and high efficiency. There are two main contributions: firstly,\ntwo components, the weak region affinity and pixel-wise refinement, are\nproposed to maintain the global and local information of 3D point clouds. Then,\nthese two components are integrated into an iterative tensor-based registration\nalgorithm to solve the cross-source point cloud registration problem. We\nconduct experiments on synthetic cross-source benchmark dataset and real\ncross-source datasets. Comparison with six state-of-the-art methods, the\nproposed method obtains both higher efficiency and accuracy.\n", "versions": [{"version": "v1", "created": "Mon, 11 Mar 2019 22:13:46 GMT"}], "update_date": "2019-03-13", "authors_parsed": [["Huang", "Xiaoshui", ""], ["Fan", "Lixin", ""], ["Wu", "Qiang", ""], ["Zhang", "Jian", ""], ["Yuan", "Chun", ""]]}, {"id": "1903.04687", "submitter": "Lei Zhang", "authors": "Lei Zhang and Xinbo Gao", "title": "Transfer Adaptation Learning: A Decade Survey", "comments": "26 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The world we see is ever-changing and it always changes with people, things,\nand the environment. Domain is referred to as the state of the world at a\ncertain moment. A research problem is characterized as transfer adaptation\nlearning (TAL) when it needs knowledge correspondence between different\nmoments/domains. Conventional machine learning aims to find a model with the\nminimum expected risk on test data by minimizing the regularized empirical risk\non the training data, which, however, supposes that the training and test data\nshare similar joint probability distribution. TAL aims to build models that can\nperform tasks of target domain by learning knowledge from a semantic related\nbut distribution different source domain. It is an energetic research filed of\nincreasing influence and importance, which is presenting a blowout publication\ntrend. This paper surveys the advances of TAL methodologies in the past decade,\nand the technical challenges and essential problems of TAL have been observed\nand discussed with deep insights and new perspectives. Broader solutions of\ntransfer adaptation learning being created by researchers are identified, i.e.,\ninstance re-weighting adaptation, feature adaptation, classifier adaptation,\ndeep network adaptation and adversarial adaptation, which are beyond the early\nsemi-supervised and unsupervised split. The survey helps researchers rapidly\nbut comprehensively understand and identify the research foundation, research\nstatus, theoretical limitations, future challenges and under-studied issues\n(universality, interpretability, and credibility) to be broken in the field\ntoward universal representation and safe applications in open-world scenarios.\n", "versions": [{"version": "v1", "created": "Tue, 12 Mar 2019 01:32:59 GMT"}, {"version": "v2", "created": "Sun, 22 Nov 2020 03:09:23 GMT"}], "update_date": "2020-11-24", "authors_parsed": [["Zhang", "Lei", ""], ["Gao", "Xinbo", ""]]}, {"id": "1903.04688", "submitter": "Chunhua Shen", "authors": "Tong He, Chunhua Shen, Zhi Tian, Dong Gong, Changming Sun, Youliang\n  Yan", "title": "Knowledge Adaptation for Efficient Semantic Segmentation", "comments": "Accepted to IEEE Conf. Computer Vision and Pattern Recognition, 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Both accuracy and efficiency are of significant importance to the task of\nsemantic segmentation. Existing deep FCNs suffer from heavy computations due to\na series of high-resolution feature maps for preserving the detailed knowledge\nin dense estimation. Although reducing the feature map resolution (i.e.,\napplying a large overall stride) via subsampling operations (e.g., pooling and\nconvolution striding) can instantly increase the efficiency, it dramatically\ndecreases the estimation accuracy. To tackle this dilemma, we propose a\nknowledge distillation method tailored for semantic segmentation to improve the\nperformance of the compact FCNs with large overall stride. To handle the\ninconsistency between the features of the student and teacher network, we\noptimize the feature similarity in a transferred latent domain formulated by\nutilizing a pre-trained autoencoder. Moreover, an affinity distillation module\nis proposed to capture the long-range dependency by calculating the non-local\ninteractions across the whole image. To validate the effectiveness of our\nproposed method, extensive experiments have been conducted on three popular\nbenchmarks: Pascal VOC, Cityscapes and Pascal Context. Built upon a highly\ncompetitive baseline, our proposed method can improve the performance of a\nstudent network by 2.5\\% (mIOU boosts from 70.2 to 72.7 on the cityscapes test\nset) and can train a better compact model with only 8\\% float operations\n(FLOPS) of a model that achieves comparable performances.\n", "versions": [{"version": "v1", "created": "Tue, 12 Mar 2019 01:34:39 GMT"}], "update_date": "2019-03-13", "authors_parsed": [["He", "Tong", ""], ["Shen", "Chunhua", ""], ["Tian", "Zhi", ""], ["Gong", "Dong", ""], ["Sun", "Changming", ""], ["Yan", "Youliang", ""]]}, {"id": "1903.04704", "submitter": "Jiapeng Tang", "authors": "Jiapeng Tang, Xiaoguang Han, Junyi Pan, Kui Jia, Xin Tong", "title": "A Skeleton-bridged Deep Learning Approach for Generating Meshes of\n  Complex Topologies from Single RGB Images", "comments": "8 pages paper, 3 pages supplementary material, CVPR Oral paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper focuses on the challenging task of learning 3D object surface\nreconstructions from single RGB images. Existing methods achieve varying\ndegrees of success by using different geometric representations. However, they\nall have their own drawbacks, and cannot well reconstruct those surfaces of\ncomplex topologies. To this end, we propose in this paper a skeleton-bridged,\nstage-wise learning approach to address the challenge. Our use of skeleton is\ndue to its nice property of topology preservation, while being of lower\ncomplexity to learn. To learn skeleton from an input image, we design a deep\narchitecture whose decoder is based on a novel design of parallel streams\nrespectively for synthesis of curve- and surface-like skeleton points. We use\ndifferent shape representations of point cloud, volume, and mesh in our\nstage-wise learning, in order to take their respective advantages. We also\npropose multi-stage use of the input image to correct prediction errors that\nare possibly accumulated in each stage. We conduct intensive experiments to\ninvestigate the efficacy of our proposed approach. Qualitative and quantitative\nresults on representative object categories of both simple and complex\ntopologies demonstrate the superiority of our approach over existing ones. We\nwill make our ShapeNet-Skeleton dataset publicly available.\n", "versions": [{"version": "v1", "created": "Tue, 12 Mar 2019 02:33:00 GMT"}, {"version": "v2", "created": "Wed, 10 Apr 2019 04:00:33 GMT"}], "update_date": "2019-04-11", "authors_parsed": [["Tang", "Jiapeng", ""], ["Han", "Xiaoguang", ""], ["Pan", "Junyi", ""], ["Jia", "Kui", ""], ["Tong", "Xin", ""]]}, {"id": "1903.04711", "submitter": "Wentao Zhu", "authors": "Wentao Zhu", "title": "Deep Learning for Automated Medical Image Analysis", "comments": "PhD Thesis", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Medical imaging is an essential tool in many areas of medical applications,\nused for both diagnosis and treatment. However, reading medical images and\nmaking diagnosis or treatment recommendations require specially trained medical\nspecialists. The current practice of reading medical images is labor-intensive,\ntime-consuming, costly, and error-prone. It would be more desirable to have a\ncomputer-aided system that can automatically make diagnosis and treatment\nrecommendations. Recent advances in deep learning enable us to rethink the ways\nof clinician diagnosis based on medical images. In this thesis, we will\nintroduce 1) mammograms for detecting breast cancers, the most frequently\ndiagnosed solid cancer for U.S. women, 2) lung CT images for detecting lung\ncancers, the most frequently diagnosed malignant cancer, and 3) head and neck\nCT images for automated delineation of organs at risk in radiotherapy. First,\nwe will show how to employ the adversarial concept to generate the hard\nexamples improving mammogram mass segmentation. Second, we will demonstrate how\nto use the weakly labeled data for the mammogram breast cancer diagnosis by\nefficiently design deep learning for multi-instance learning. Third, the thesis\nwill walk through DeepLung system which combines deep 3D ConvNets and GBM for\nautomated lung nodule detection and classification. Fourth, we will show how to\nuse weakly labeled data to improve existing lung nodule detection system by\nintegrating deep learning with a probabilistic graphic model. Lastly, we will\ndemonstrate the AnatomyNet which is thousands of times faster and more accurate\nthan previous methods on automated anatomy segmentation.\n", "versions": [{"version": "v1", "created": "Tue, 12 Mar 2019 03:28:37 GMT"}], "update_date": "2019-03-13", "authors_parsed": [["Zhu", "Wentao", ""]]}, {"id": "1903.04722", "submitter": "Manan Oza", "authors": "Manan Oza, Himanshu Vaghela, Kriti Srivastava", "title": "Progressive Generative Adversarial Binary Networks for Music Generation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.CV cs.LG eess.AS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recent improvements in generative adversarial network (GAN) training\ntechniques prove that progressively training a GAN drastically stabilizes the\ntraining and improves the quality of outputs produced. Adding layers after the\nprevious ones have converged has proven to help in better overall convergence\nand stability of the model as well as reducing the training time by a\nsufficient amount. Thus we use this training technique to train the model\nprogressively in the time and pitch domain i.e. starting from a very small time\nvalue and pitch range we gradually expand the matrix sizes until the end result\nis a completely trained model giving outputs having tensor sizes [4 (bar) x 96\n(time steps) x 84 (pitch values) x 8 (tracks)]. As proven in previously\nproposed models deterministic binary neurons also help in improving the\nresults. Thus we make use of a layer of deterministic binary neurons at the end\nof the generator to get binary valued outputs instead of fractional values\nexisting between 0 and 1.\n", "versions": [{"version": "v1", "created": "Tue, 12 Mar 2019 04:16:20 GMT"}], "update_date": "2019-03-13", "authors_parsed": [["Oza", "Manan", ""], ["Vaghela", "Himanshu", ""], ["Srivastava", "Kriti", ""]]}, {"id": "1903.04752", "submitter": "Yuhang Wu", "authors": "Yuhang Wu and Ioannis A. Kakadiaris", "title": "Occlusion-guided compact template learning for ensemble deep\n  network-based pose-invariant face recognition", "comments": "Accepted by International Conference on Biometrics (ICB 2019) as an\n  Oral presentation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Concatenation of the deep network representations extracted from different\nfacial patches helps to improve face recognition performance. However, the\nconcatenated facial template increases in size and contains redundant\ninformation. Previous solutions aim to reduce the dimensionality of the facial\ntemplate without considering the occlusion pattern of the facial patches. In\nthis paper, we propose an occlusion-guided compact template learning (OGCTL)\napproach that only uses the information from visible patches to construct the\ncompact template. The compact face representation is not sensitive to the\nnumber of patches that are used to construct the facial template and is more\nsuitable for incorporating the information from different view angles for\nimage-set based face recognition. Instead of using occlusion masks in face\nmatching (e.g., DPRFS [38]), the proposed method uses occlusion masks in\ntemplate construction and achieves significantly better image-set based face\nverification performance on a challenging database with a template size that is\nan order-of-magnitude smaller than DPRFS.\n", "versions": [{"version": "v1", "created": "Tue, 12 Mar 2019 07:24:33 GMT"}, {"version": "v2", "created": "Mon, 15 Apr 2019 08:32:38 GMT"}], "update_date": "2019-04-16", "authors_parsed": [["Wu", "Yuhang", ""], ["Kakadiaris", "Ioannis A.", ""]]}, {"id": "1903.04772", "submitter": "Arash Akbarinia", "authors": "Arash Akbarinia and Karl R. Gegenfurtner", "title": "Paradox in Deep Neural Networks: Similar yet Different while Different\n  yet Similar", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Machine learning is advancing towards a data-science approach, implying a\nnecessity to a line of investigation to divulge the knowledge learnt by deep\nneuronal networks. Limiting the comparison among networks merely to a\npredefined intelligent ability, according to ground truth, does not suffice, it\nshould be associated with innate similarity of these artificial entities. Here,\nwe analysed multiple instances of an identical architecture trained to classify\nobjects in static images (CIFAR and ImageNet data sets). We evaluated the\nperformance of the networks under various distortions and compared it to the\nintrinsic similarity between their constituent kernels. While we expected a\nclose correspondence between these two measures, we observed a puzzling\nphenomenon. Pairs of networks whose kernels' weights are over 99.9% correlated\ncan exhibit significantly different performances, yet other pairs with no\ncorrelation can reach quite compatible levels of performance. We show\nimplications of this for transfer learning, and argue its importance in our\ngeneral understanding of what intelligence is, whether natural or artificial.\n", "versions": [{"version": "v1", "created": "Tue, 12 Mar 2019 08:04:44 GMT"}], "update_date": "2019-03-13", "authors_parsed": [["Akbarinia", "Arash", ""], ["Gegenfurtner", "Karl R.", ""]]}, {"id": "1903.04778", "submitter": "Ziyuan Zhao", "authors": "Ziyuan Zhao, Xiaoman Zhang, Cen Chen, Wei Li, Songyou Peng, Jie Wang,\n  Xulei Yang, Le Zhang and Zeng Zeng", "title": "Semi-Supervised Self-Taught Deep Learning for Finger Bones Segmentation", "comments": "IEEE BHI 2019 accepted", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Segmentation stands at the forefront of many high-level vision tasks. In this\nstudy, we focus on segmenting finger bones within a newly introduced\nsemi-supervised self-taught deep learning framework which consists of a student\nnetwork and a stand-alone teacher module. The whole system is boosted in a\nlife-long learning manner wherein each step the teacher module provides a\nrefinement for the student network to learn with newly unlabeled data.\nExperimental results demonstrate the superiority of the proposed method over\nconventional supervised deep learning methods.\n", "versions": [{"version": "v1", "created": "Tue, 12 Mar 2019 08:32:33 GMT"}], "update_date": "2019-03-13", "authors_parsed": [["Zhao", "Ziyuan", ""], ["Zhang", "Xiaoman", ""], ["Chen", "Cen", ""], ["Li", "Wei", ""], ["Peng", "Songyou", ""], ["Wang", "Jie", ""], ["Yang", "Xulei", ""], ["Zhang", "Le", ""], ["Zeng", "Zeng", ""]]}, {"id": "1903.04814", "submitter": "Ji Hu", "authors": "Yaoqi Sun, Liang Li, Liang Zheng, Ji Hu, Yatong Jiang, Chenggang Yan", "title": "Image Classification base on PCA of Multi-view Deep Representation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the age of information explosion, image classification is the key\ntechnology of dealing with and organizing a large number of image data.\nCurrently, the classical image classification algorithms are mostly based on\nRGB images or grayscale images, and fail to make good use of the depth\ninformation about objects or scenes. The depth information in the images has a\nstrong complementary effect, which can enhance the classification accuracy\nsignificantly. In this paper, we propose an image classification technology\nusing principal component analysis based on multi-view depth characters. In\ndetail, firstly, the depth image of the original image is estimated; secondly,\ndepth characters are extracted from the RGB views and the depth view\nseparately, and then the reducing dimension operation through the PCA is\nimplemented. Eventually, the SVM is applied to image classification. The\nexperimental results show that the method has good performance.\n", "versions": [{"version": "v1", "created": "Tue, 12 Mar 2019 10:14:57 GMT"}], "update_date": "2019-03-13", "authors_parsed": [["Sun", "Yaoqi", ""], ["Li", "Liang", ""], ["Zheng", "Liang", ""], ["Hu", "Ji", ""], ["Jiang", "Yatong", ""], ["Yan", "Chenggang", ""]]}, {"id": "1903.04842", "submitter": "L\\'eo Maczyta", "authors": "L. Maczyta, P. Bouthemy, O. Le Meur", "title": "Unsupervised motion saliency map estimation based on optical flow\n  inpainting", "comments": null, "journal-ref": "International Conference on Image Processing (ICIP) 2019", "doi": "10.1109/ICIP.2019.8803542", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper addresses the problem of motion saliency in videos, that is,\nidentifying regions that undergo motion departing from its context. We propose\na new unsupervised paradigm to compute motion saliency maps. The key ingredient\nis the flow inpainting stage. Candidate regions are determined from the optical\nflow boundaries. The residual flow in these regions is given by the difference\nbetween the optical flow and the flow inpainted from the surrounding areas. It\nprovides the cue for motion saliency. The method is flexible and general by\nrelying on motion information only. Experimental results on the DAVIS 2016\nbenchmark demonstrate that the method compares favourably with state-of-the-art\nvideo saliency methods.\n", "versions": [{"version": "v1", "created": "Tue, 12 Mar 2019 11:15:14 GMT"}, {"version": "v2", "created": "Mon, 4 Nov 2019 10:06:40 GMT"}], "update_date": "2019-11-05", "authors_parsed": [["Maczyta", "L.", ""], ["Bouthemy", "P.", ""], ["Meur", "O. Le", ""]]}, {"id": "1903.04855", "submitter": "Chao Gou", "authors": "Chao Gou, Tianyu Shen, Wenbo Zheng, Huadan Xue, Hui Yu, Qiang Ji,\n  Zhengyu Jin, Fei-Yue Wang", "title": "Parallel Medical Imaging for Intelligent Medical Image Analysis:\n  Concepts, Methods, and Applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  There has been much progress in data-driven artificial intelligence\ntechnology for medical image analysis in the last decades. However, it still\nremains challenging due to its distinctive complexity of acquiring and\nannotating image data, extracting medical domain knowledge, and explaining the\ndiagnostic decision for medical image analysis. In this paper, we propose a\ndata-knowledge-driven framework termed as Parallel Medical Imaging (PMI) for\nintelligent medical image analysis based on the methodology of interactive\nACP-based parallel intelligence. In the PMI framework, computational\nexperiments with predictive learning in a data-driven way are conducted to\nextract medical knowledge for diagnostic decision support. Artificial imaging\nsystems are introduced to select and prescriptively generate medical image data\nin a knowledge-driven way to utilize medical domain knowledge. Through the\nclosed-loop optimization based on parallel execution, our proposed PMI\nframework can boost the generalization ability and alleviate the limitation of\nmedical interpretation for diagnostic decisions. Furthermore, we illustrate the\npreliminary implementation of PMI method through the case studies of mammogram\nanalysis and skin lesion image analysis. Experimental results on several public\nmedical image datasets demonstrate the effectiveness of proposed PMI.\n", "versions": [{"version": "v1", "created": "Tue, 12 Mar 2019 11:50:28 GMT"}, {"version": "v2", "created": "Thu, 26 Mar 2020 03:28:52 GMT"}, {"version": "v3", "created": "Tue, 29 Jun 2021 09:05:14 GMT"}], "update_date": "2021-06-30", "authors_parsed": [["Gou", "Chao", ""], ["Shen", "Tianyu", ""], ["Zheng", "Wenbo", ""], ["Xue", "Huadan", ""], ["Yu", "Hui", ""], ["Ji", "Qiang", ""], ["Jin", "Zhengyu", ""], ["Wang", "Fei-Yue", ""]]}, {"id": "1903.04933", "submitter": "Sander Dieleman", "authors": "Jeffrey De Fauw, Sander Dieleman, Karen Simonyan", "title": "Hierarchical Autoregressive Image Models with Auxiliary Decoders", "comments": "Updated: added human evaluation results, incorporated review feedback", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Autoregressive generative models of images tend to be biased towards\ncapturing local structure, and as a result they often produce samples which are\nlacking in terms of large-scale coherence. To address this, we propose two\nmethods to learn discrete representations of images which abstract away local\ndetail. We show that autoregressive models conditioned on these representations\ncan produce high-fidelity reconstructions of images, and that we can train\nautoregressive priors on these representations that produce samples with\nlarge-scale coherence. We can recursively apply the learning procedure,\nyielding a hierarchy of progressively more abstract image representations. We\ntrain hierarchical class-conditional autoregressive models on the ImageNet\ndataset and demonstrate that they are able to generate realistic images at\nresolutions of 128$\\times$128 and 256$\\times$256 pixels. We also perform a\nhuman evaluation study comparing our models with both adversarial and\nlikelihood-based state-of-the-art generative models.\n", "versions": [{"version": "v1", "created": "Wed, 6 Mar 2019 22:13:52 GMT"}, {"version": "v2", "created": "Tue, 8 Oct 2019 17:55:59 GMT"}], "update_date": "2019-10-09", "authors_parsed": [["De Fauw", "Jeffrey", ""], ["Dieleman", "Sander", ""], ["Simonyan", "Karen", ""]]}, {"id": "1903.04939", "submitter": "Ayan Chakrabarti", "authors": "Kyle Yee and Ayan Chakrabarti", "title": "Fast Deep Stereo with 2D Convolutional Processing of Cost Signatures", "comments": "Project site at https://projects.ayanc.org/fdscs/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern neural network-based algorithms are able to produce highly accurate\ndepth estimates from stereo image pairs, nearly matching the reliability of\nmeasurements from more expensive depth sensors. However, this accuracy comes\nwith a higher computational cost since these methods use network architectures\ndesigned to compute and process matching scores across all candidate matches at\nall locations, with floating point computations repeated across a match volume\nwith dimensions corresponding to both space and disparity. This leads to longer\nrunning times to process each image pair, making them impractical for real-time\nuse in robots and autonomous vehicles. We propose a new stereo algorithm that\nemploys a significantly more efficient network architecture. Our method builds\nan initial match cost volume using traditional matching costs that are fast to\ncompute, and trains a network to estimate disparity from this volume.\nCrucially, our network only employs per-pixel and two-dimensional convolution\noperations: to summarize the match information at each location as a\nlow-dimensional feature vector, and to spatially process these `cost-signature'\nfeatures to produce a dense disparity map. Experimental results on the KITTI\nbenchmark show that our method delivers competitive accuracy at significantly\nhigher speeds---running at 48 frames per second on a modern GPU.\n", "versions": [{"version": "v1", "created": "Fri, 8 Mar 2019 16:39:28 GMT"}], "update_date": "2019-03-13", "authors_parsed": [["Yee", "Kyle", ""], ["Chakrabarti", "Ayan", ""]]}, {"id": "1903.04963", "submitter": "Hanli Qiao", "authors": "Hanli Qiao", "title": "Discriminative Principal Component Analysis: A REVERSE THINKING", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a novel approach named by Discriminative Principal\nComponent Analysis which is abbreviated as Discriminative PCA in order to\nenhance separability of PCA by Linear Discriminant Analysis (LDA). The proposed\nmethod performs feature extraction by determining a linear projection that\ncaptures the most scattered discriminative information. The most innovation of\nDiscriminative PCA is performing PCA on discriminative matrix rather than\noriginal sample matrix. For calculating the required discriminative matrix\nunder low complexity, we exploit LDA on a converted matrix to obtain\nwithin-class matrix and between-class matrix thereof. During the computation\nprocess, we utilise direct linear discriminant analysis (DLDA) to solve the\nencountered SSS problem. For evaluating the performances of Discriminative PCA\nin face recognition, we analytically compare it with DLAD and PCA on four well\nknown facial databases, they are PIE, FERET, YALE and ORL respectively. Results\nin accuracy and running time obtained by nearest neighbour classifier are\ncompared when different number of training images per person used. Not only the\nsuperiority and outstanding performance of Discriminative PCA showed in\nrecognition rate, but also the comparable results of running time.\n", "versions": [{"version": "v1", "created": "Tue, 12 Mar 2019 14:43:12 GMT"}], "update_date": "2019-03-13", "authors_parsed": [["Qiao", "Hanli", ""]]}, {"id": "1903.04988", "submitter": "Breton Minnehan", "authors": "Breton Minnehan and Andreas Savakis", "title": "Cascaded Projection: End-to-End Network Compression and Acceleration", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a data-driven approach for deep convolutional neural network\ncompression that achieves high accuracy with high throughput and low memory\nrequirements. Current network compression methods either find a low-rank\nfactorization of the features that requires more memory, or select only a\nsubset of features by pruning entire filter channels. We propose the Cascaded\nProjection (CaP) compression method that projects the output and input filter\nchannels of successive layers to a unified low dimensional space based on a\nlow-rank projection. We optimize the projection to minimize classification loss\nand the difference between the next layer's features in the compressed and\nuncompressed networks. To solve this non-convex optimization problem we propose\na new optimization method of a proxy matrix using backpropagation and\nStochastic Gradient Descent (SGD) with geometric constraints. Our cascaded\nprojection approach leads to improvements in all critical areas of network\ncompression: high accuracy, low memory consumption, low parameter count and\nhigh processing speed. The proposed CaP method demonstrates state-of-the-art\nresults compressing VGG16 and ResNet networks with over 4x reduction in the\nnumber of computations and excellent performance in top-5 accuracy on the\nImageNet dataset before and after fine-tuning.\n", "versions": [{"version": "v1", "created": "Tue, 12 Mar 2019 15:20:10 GMT"}], "update_date": "2019-03-13", "authors_parsed": [["Minnehan", "Breton", ""], ["Savakis", "Andreas", ""]]}, {"id": "1903.04991", "submitter": "Andrzej Banburski", "authors": "Andrzej Banburski, Qianli Liao, Brando Miranda, Lorenzo Rosasco,\n  Fernanda De La Torre, Jack Hidary and Tomaso Poggio", "title": "Theory III: Dynamics and Generalization in Deep Networks", "comments": "47 pages, 11 figures. This replaces previous versions of Theory III,\n  that appeared on Arxiv [arXiv:1806.11379, arXiv:1801.00173] or on the CBMM\n  site. v5: Changes throughout the paper to the presentation and tightening\n  some of the statements", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The key to generalization is controlling the complexity of the network.\nHowever, there is no obvious control of complexity -- such as an explicit\nregularization term -- in the training of deep networks for classification. We\nwill show that a classical form of norm control -- but kind of hidden -- is\npresent in deep networks trained with gradient descent techniques on\nexponential-type losses. In particular, gradient descent induces a dynamics of\nthe normalized weights which converge for $t \\to \\infty$ to an equilibrium\nwhich corresponds to a minimum norm (or maximum margin) solution. For\nsufficiently large but finite $\\rho$ -- and thus finite $t$ -- the dynamics\nconverges to one of several margin maximizers, with the margin monotonically\nincreasing towards a limit stationary point of the flow. In the usual case of\nstochastic gradient descent, most of the stationary points are likely to be\nconvex minima corresponding to a constrained minimizer -- the network with\nnormalized weights-- which corresponds to vanishing regularization. The\nsolution has zero generalization gap, for fixed architecture, asymptotically\nfor $N \\to \\infty$, where $N$ is the number of training examples. Our approach\nextends some of the original results of Srebro from linear networks to deep\nnetworks and provides a new perspective on the implicit bias of gradient\ndescent. We believe that the elusive complexity control we describe is\nresponsible for the puzzling empirical finding of good predictive performance\nby deep networks, despite overparametrization.\n", "versions": [{"version": "v1", "created": "Tue, 12 Mar 2019 15:24:26 GMT"}, {"version": "v2", "created": "Thu, 11 Apr 2019 22:38:08 GMT"}, {"version": "v3", "created": "Thu, 13 Jun 2019 02:02:40 GMT"}, {"version": "v4", "created": "Wed, 3 Jul 2019 22:59:20 GMT"}, {"version": "v5", "created": "Sat, 11 Apr 2020 00:21:50 GMT"}], "update_date": "2020-04-14", "authors_parsed": [["Banburski", "Andrzej", ""], ["Liao", "Qianli", ""], ["Miranda", "Brando", ""], ["Rosasco", "Lorenzo", ""], ["De La Torre", "Fernanda", ""], ["Hidary", "Jack", ""], ["Poggio", "Tomaso", ""]]}, {"id": "1903.05014", "submitter": "Hanno Winter", "authors": "Hanno Winter, Stefan Luthardt, Volker Willert, J\\\"urgen Adamy", "title": "Generating Compact Geometric Track-Maps for Train Positioning\n  Applications", "comments": null, "journal-ref": null, "doi": "10.1109/IVS.2019.8813901", "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a method to generate compact geometric track-maps\nfor train-borne localization applications. Therefore, we first give a brief\noverview on the purpose of track maps in train-positioning applications. It\nbecomes apparent that there are hardly any adequate methods to generate\nsuitable geometric track-maps. This is why we present a novel map generation\nprocedure. It uses an optimization formulation to find the continuous sequence\nof track geometries that fits the available measurement data best. The\noptimization is initialized with the results from a localization filter\ndeveloped in our previous work. The localization filter also provides the\nrequired information for shape identification and measurement association. The\npresented approach will be evaluated on simulated data as well as on real\nmeasurements.\n", "versions": [{"version": "v1", "created": "Tue, 12 Mar 2019 16:00:01 GMT"}, {"version": "v2", "created": "Wed, 15 May 2019 08:39:26 GMT"}], "update_date": "2019-09-04", "authors_parsed": [["Winter", "Hanno", ""], ["Luthardt", "Stefan", ""], ["Willert", "Volker", ""], ["Adamy", "J\u00fcrgen", ""]]}, {"id": "1903.05027", "submitter": "Huanyu Liu", "authors": "Huanyu Liu, Chao Peng, Changqian Yu, Jingbo Wang, Xu Liu, Gang Yu, Wei\n  Jiang", "title": "An End-to-End Network for Panoptic Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Panoptic segmentation, which needs to assign a category label to each pixel\nand segment each object instance simultaneously, is a challenging topic.\nTraditionally, the existing approaches utilize two independent models without\nsharing features, which makes the pipeline inefficient to implement. In\naddition, a heuristic method is usually employed to merge the results. However,\nthe overlapping relationship between object instances is difficult to determine\nwithout sufficient context information during the merging process. To address\nthe problems, we propose a novel end-to-end network for panoptic segmentation,\nwhich can efficiently and effectively predict both the instance and stuff\nsegmentation in a single network. Moreover, we introduce a novel spatial\nranking module to deal with the occlusion problem between the predicted\ninstances. Extensive experiments have been done to validate the performance of\nour proposed method and promising results have been achieved on the COCO\nPanoptic benchmark.\n", "versions": [{"version": "v1", "created": "Tue, 12 Mar 2019 16:30:11 GMT"}, {"version": "v2", "created": "Wed, 13 Mar 2019 02:22:59 GMT"}], "update_date": "2019-03-14", "authors_parsed": [["Liu", "Huanyu", ""], ["Peng", "Chao", ""], ["Yu", "Changqian", ""], ["Wang", "Jingbo", ""], ["Liu", "Xu", ""], ["Yu", "Gang", ""], ["Jiang", "Wei", ""]]}, {"id": "1903.05044", "submitter": "S. Mazdak Abulnaga", "authors": "S. Mazdak Abulnaga, Esra Abaci Turk, Mikhail Bessmeltsev, P. Ellen\n  Grant, Justin Solomon, Polina Golland", "title": "Placental Flattening via Volumetric Parameterization", "comments": "MICCAI 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a volumetric mesh-based algorithm for flattening the placenta to a\ncanonical template to enable effective visualization of local anatomy and\nfunction. Monitoring placental function in vivo promises to support pregnancy\nassessment and to improve care outcomes. We aim to alleviate visualization and\ninterpretation challenges presented by the shape of the placenta when it is\nattached to the curved uterine wall. To do so, we flatten the volumetric mesh\nthat captures placental shape to resemble the well-studied ex vivo shape. We\nformulate our method as a map from the in vivo shape to a flattened template\nthat minimizes the symmetric Dirichlet energy to control distortion throughout\nthe volume. Local injectivity is enforced via constrained line search during\ngradient descent. We evaluate the proposed method on 28 placenta shapes\nextracted from MRI images in a clinical study of placental function. We achieve\nsub-voxel accuracy in mapping the boundary of the placenta to the template\nwhile successfully controlling distortion throughout the volume. We illustrate\nhow the resulting mapping of the placenta enhances visualization of placental\nanatomy and function. Our code is freely available at\nhttps://github.com/mabulnaga/placenta-flattening .\n", "versions": [{"version": "v1", "created": "Tue, 12 Mar 2019 16:48:23 GMT"}, {"version": "v2", "created": "Thu, 14 Mar 2019 23:52:38 GMT"}, {"version": "v3", "created": "Mon, 30 Sep 2019 18:30:15 GMT"}], "update_date": "2019-10-02", "authors_parsed": [["Abulnaga", "S. Mazdak", ""], ["Turk", "Esra Abaci", ""], ["Bessmeltsev", "Mikhail", ""], ["Grant", "P. Ellen", ""], ["Solomon", "Justin", ""], ["Golland", "Polina", ""]]}, {"id": "1903.05050", "submitter": "Yannis Avrithis", "authors": "Yann Lifchitz and Yannis Avrithis and Sylvaine Picard and Andrei\n  Bursuc", "title": "Dense Classification and Implanting for Few-Shot Learning", "comments": "CVPR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Training deep neural networks from few examples is a highly challenging and\nkey problem for many computer vision tasks. In this context, we are targeting\nknowledge transfer from a set with abundant data to other sets with few\navailable examples. We propose two simple and effective solutions: (i) dense\nclassification over feature maps, which for the first time studies local\nactivations in the domain of few-shot learning, and (ii) implanting, that is,\nattaching new neurons to a previously trained network to learn new,\ntask-specific features. On miniImageNet, we improve the prior state-of-the-art\non few-shot classification, i.e., we achieve 62.5%, 79.8% and 83.8% on 5-way\n1-shot, 5-shot and 10-shot settings respectively.\n", "versions": [{"version": "v1", "created": "Tue, 12 Mar 2019 16:58:08 GMT"}], "update_date": "2019-03-13", "authors_parsed": [["Lifchitz", "Yann", ""], ["Avrithis", "Yannis", ""], ["Picard", "Sylvaine", ""], ["Bursuc", "Andrei", ""]]}, {"id": "1903.05079", "submitter": "Yury Korolev", "authors": "Martin Burger, Yury Korolev, Carola-Bibiane Sch\\\"onlieb and Christiane\n  Stollenwerk", "title": "A total variation based regularizer promoting piecewise-Lipschitz\n  reconstructions", "comments": "12 pages, 4 figures, accepted for publication in SSVM conference\n  proceedings 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new regularizer in the total variation family that promotes\nreconstructions with a given Lipschitz constant (which can also vary\nspatially). We prove regularizing properties of this functional and investigate\nits connections to total variation and infimal convolution type regularizers\nTVLp and, in particular, establish topological equivalence. Our numerical\nexperiments show that the proposed regularizer can achieve similar performance\nas total generalized variation while having the advantage of a very intuitive\ninterpretation of its free parameter, which is just a local estimate of the\nnorm of the gradient. It also provides a natural approach to spatially adaptive\nregularization.\n", "versions": [{"version": "v1", "created": "Tue, 12 Mar 2019 17:57:13 GMT"}], "update_date": "2019-03-13", "authors_parsed": [["Burger", "Martin", ""], ["Korolev", "Yury", ""], ["Sch\u00f6nlieb", "Carola-Bibiane", ""], ["Stollenwerk", "Christiane", ""]]}, {"id": "1903.05134", "submitter": "Jiahui Yu", "authors": "Jiahui Yu, Thomas Huang", "title": "Universally Slimmable Networks and Improved Training Techniques", "comments": "Accepted in ICCV 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Slimmable networks are a family of neural networks that can instantly adjust\nthe runtime width. The width can be chosen from a predefined widths set to\nadaptively optimize accuracy-efficiency trade-offs at runtime. In this work, we\npropose a systematic approach to train universally slimmable networks\n(US-Nets), extending slimmable networks to execute at arbitrary width, and\ngeneralizing to networks both with and without batch normalization layers. We\nfurther propose two improved training techniques for US-Nets, named the\nsandwich rule and inplace distillation, to enhance training process and boost\ntesting accuracy. We show improved performance of universally slimmable\nMobileNet v1 and MobileNet v2 on ImageNet classification task, compared with\nindividually trained ones and 4-switch slimmable network baselines. We also\nevaluate the proposed US-Nets and improved training techniques on tasks of\nimage super-resolution and deep reinforcement learning. Extensive ablation\nexperiments on these representative tasks demonstrate the effectiveness of our\nproposed methods. Our discovery opens up the possibility to directly evaluate\nFLOPs-Accuracy spectrum of network architectures. Code and models are available\nat: https://github.com/JiahuiYu/slimmable_networks\n", "versions": [{"version": "v1", "created": "Tue, 12 Mar 2019 18:36:02 GMT"}, {"version": "v2", "created": "Sun, 20 Oct 2019 19:52:57 GMT"}], "update_date": "2019-10-22", "authors_parsed": [["Yu", "Jiahui", ""], ["Huang", "Thomas", ""]]}, {"id": "1903.05136", "submitter": "Zhijian Liu", "authors": "Zhenjia Xu, Zhijian Liu, Chen Sun, Kevin Murphy, William T. Freeman,\n  Joshua B. Tenenbaum, Jiajun Wu", "title": "Unsupervised Discovery of Parts, Structure, and Dynamics", "comments": "ICLR 2019. The first two authors contributed equally to this work", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Humans easily recognize object parts and their hierarchical structure by\nwatching how they move; they can then predict how each part moves in the\nfuture. In this paper, we propose a novel formulation that simultaneously\nlearns a hierarchical, disentangled object representation and a dynamics model\nfor object parts from unlabeled videos. Our Parts, Structure, and Dynamics\n(PSD) model learns to, first, recognize the object parts via a layered image\nrepresentation; second, predict hierarchy via a structural descriptor that\ncomposes low-level concepts into a hierarchical structure; and third, model the\nsystem dynamics by predicting the future. Experiments on multiple real and\nsynthetic datasets demonstrate that our PSD model works well on all three\ntasks: segmenting object parts, building their hierarchical structure, and\ncapturing their motion distributions.\n", "versions": [{"version": "v1", "created": "Tue, 12 Mar 2019 18:39:10 GMT"}], "update_date": "2019-03-14", "authors_parsed": [["Xu", "Zhenjia", ""], ["Liu", "Zhijian", ""], ["Sun", "Chen", ""], ["Murphy", "Kevin", ""], ["Freeman", "William T.", ""], ["Tenenbaum", "Joshua B.", ""], ["Wu", "Jiajun", ""]]}, {"id": "1903.05238", "submitter": "Sergiu Oprea", "authors": "Sergiu Oprea, Pablo Martinez-Gonzalez, Alberto Garcia-Garcia, John\n  Alejandro Castro-Vargas, Sergio Orts-Escolano, Jose Garcia-Rodriguez", "title": "A Visually Plausible Grasping System for Object Manipulation and\n  Interaction in Virtual Reality Environments", "comments": null, "journal-ref": null, "doi": "10.1016/j.cag.2019.07.003", "report-no": null, "categories": "cs.GR cs.CV cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Interaction in virtual reality (VR) environments is essential to achieve a\npleasant and immersive experience. Most of the currently existing VR\napplications, lack of robust object grasping and manipulation, which are the\ncornerstone of interactive systems. Therefore, we propose a realistic, flexible\nand robust grasping system that enables rich and real-time interactions in\nvirtual environments. It is visually realistic because it is completely\nuser-controlled, flexible because it can be used for different hand\nconfigurations, and robust because it allows the manipulation of objects\nregardless their geometry, i.e. hand is automatically fitted to the object\nshape. In order to validate our proposal, an exhaustive qualitative and\nquantitative performance analysis has been carried out. On the one hand,\nqualitative evaluation was used in the assessment of the abstract aspects such\nas: hand movement realism, interaction realism and motor control. On the other\nhand, for the quantitative evaluation a novel error metric has been proposed to\nvisually analyze the performed grips. This metric is based on the computation\nof the distance from the finger phalanges to the nearest contact point on the\nobject surface. These contact points can be used with different application\npurposes, mainly in the field of robotics. As a conclusion, system evaluation\nreports a similar performance between users with previous experience in virtual\nreality applications and inexperienced users, referring to a steep learning\ncurve.\n", "versions": [{"version": "v1", "created": "Tue, 12 Mar 2019 22:15:51 GMT"}], "update_date": "2020-04-13", "authors_parsed": [["Oprea", "Sergiu", ""], ["Martinez-Gonzalez", "Pablo", ""], ["Garcia-Garcia", "Alberto", ""], ["Castro-Vargas", "John Alejandro", ""], ["Orts-Escolano", "Sergio", ""], ["Garcia-Rodriguez", "Jose", ""]]}, {"id": "1903.05244", "submitter": "Jakub \\v{S}pa\\v{n}hel", "authors": "Jakub \\v{S}pa\\v{n}hel, Jakub Sochor, Roman Jur\\'anek, Petr Dobe\\v{s},\n  Vojt\\v{e}ch Bartl, Adam Herout", "title": "Learning Feature Aggregation in Temporal Domain for Re-Identification", "comments": "Under consideration at Computer Vision and Image Understanding", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Person re-identification is a standard and established problem in the\ncomputer vision community. In recent years, vehicle re-identification is also\ngetting more attention. In this paper, we focus on both these tasks and propose\na method for aggregation of features in temporal domain as it is common to have\nmultiple observations of the same object. The aggregation is based on weighting\ndifferent elements of the feature vectors by different weights and it is\ntrained in an end-to-end manner by a Siamese network. The experimental results\nshow that our method outperforms other existing methods for feature aggregation\nin temporal domain on both vehicle and person re-identification tasks.\nFurthermore, to push research in vehicle re-identification further, we\nintroduce a novel dataset CarsReId74k. The dataset is not limited to\nfrontal/rear viewpoints. It contains 17,681 unique vehicles, 73,976 observed\ntracks, and 277,236 positive pairs. The dataset was captured by 66 cameras from\nvarious angles.\n", "versions": [{"version": "v1", "created": "Tue, 12 Mar 2019 22:34:21 GMT"}], "update_date": "2019-03-14", "authors_parsed": [["\u0160pa\u0148hel", "Jakub", ""], ["Sochor", "Jakub", ""], ["Jur\u00e1nek", "Roman", ""], ["Dobe\u0161", "Petr", ""], ["Bartl", "Vojt\u011bch", ""], ["Herout", "Adam", ""]]}, {"id": "1903.05257", "submitter": "Hassan Muhammad", "authors": "Hassan Muhammad, Carlie S. Sigel, Gabriele Campanella, Thomas Boerner,\n  Linda M. Pak, Stefan B\\\"uttner, Jan N.M. IJzermans, Bas Groot Koerkamp,\n  Michael Doukas, William R. Jarnagin, Amber Simpson, Thomas J. Fuchs", "title": "Towards Unsupervised Cancer Subtyping: Predicting Prognosis Using A\n  Histologic Visual Dictionary", "comments": "10 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV q-bio.TO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unlike common cancers, such as those of the prostate and breast, tumor\ngrading in rare cancers is difficult and largely undefined because of small\nsample sizes, the sheer volume of time needed to undertake on such a task, and\nthe inherent difficulty of extracting human-observed patterns. One of the most\nchallenging examples is intrahepatic cholangiocarcinoma (ICC), a primary liver\ncancer arising from the biliary system, for which there is well-recognized\ntumor heterogeneity and no grading paradigm or prognostic biomarkers. In this\npaper, we propose a new unsupervised deep convolutional autoencoder-based\nclustering model that groups together cellular and structural morphologies of\ntumor in 246 ICC digitized whole slides, based on visual similarity. From this\nvisual dictionary of histologic patterns, we use the clusters as covariates to\ntrain Cox-proportional hazard survival models. In univariate analysis, three\nclusters were significantly associated with recurrence-free survival.\nCombinations of these clusters were significant in multivariate analysis. In a\nmultivariate analysis of all clusters, five showed significance to\nrecurrence-free survival, however the overall model was not measured to be\nsignificant. Finally, a pathologist assigned clinical terminology to the\nsignificant clusters in the visual dictionary and found evidence supporting the\nhypothesis that collagen-enriched fibrosis plays a role in disease severity.\nThese results offer insight into the future of cancer subtyping and show that\ncomputational pathology can contribute to disease prognostication, especially\nin rare cancers.\n", "versions": [{"version": "v1", "created": "Tue, 12 Mar 2019 23:24:23 GMT"}], "update_date": "2019-03-14", "authors_parsed": [["Muhammad", "Hassan", ""], ["Sigel", "Carlie S.", ""], ["Campanella", "Gabriele", ""], ["Boerner", "Thomas", ""], ["Pak", "Linda M.", ""], ["B\u00fcttner", "Stefan", ""], ["IJzermans", "Jan N. M.", ""], ["Koerkamp", "Bas Groot", ""], ["Doukas", "Michael", ""], ["Jarnagin", "William R.", ""], ["Simpson", "Amber", ""], ["Fuchs", "Thomas J.", ""]]}, {"id": "1903.05285", "submitter": "Di Xie", "authors": "Weijie Chen and Di Xie and Yuan Zhang and Shiliang Pu", "title": "All You Need is a Few Shifts: Designing Efficient Convolutional Neural\n  Networks for Image Classification", "comments": "CVPR2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Shift operation is an efficient alternative over depthwise separable\nconvolution. However, it is still bottlenecked by its implementation manner,\nnamely memory movement. To put this direction forward, a new and novel basic\ncomponent named Sparse Shift Layer (SSL) is introduced in this paper to\nconstruct efficient convolutional neural networks. In this family of\narchitectures, the basic block is only composed by 1x1 convolutional layers\nwith only a few shift operations applied to the intermediate feature maps. To\nmake this idea feasible, we introduce shift operation penalty during\noptimization and further propose a quantization-aware shift learning method to\nimpose the learned displacement more friendly for inference. Extensive ablation\nstudies indicate that only a few shift operations are sufficient to provide\nspatial information communication. Furthermore, to maximize the role of SSL, we\nredesign an improved network architecture to Fully Exploit the limited capacity\nof neural Network (FE-Net). Equipped with SSL, this network can achieve 75.0%\ntop-1 accuracy on ImageNet with only 563M M-Adds. It surpasses other\ncounterparts constructed by depthwise separable convolution and the networks\nsearched by NAS in terms of accuracy and practical speed.\n", "versions": [{"version": "v1", "created": "Wed, 13 Mar 2019 01:44:39 GMT"}], "update_date": "2019-03-14", "authors_parsed": [["Chen", "Weijie", ""], ["Xie", "Di", ""], ["Zhang", "Yuan", ""], ["Pu", "Shiliang", ""]]}, {"id": "1903.05358", "submitter": "Yanning Zhou", "authors": "Yanning Zhou, Omer Fahri Onder, Qi Dou, Efstratios Tsougenis, Hao Chen\n  and Pheng-Ann Heng", "title": "CIA-Net: Robust Nuclei Instance Segmentation with Contour-aware\n  Information Aggregation", "comments": "Accepted for the 26th Conference on Information Processing in Medical\n  Imaging (IPMI 2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurate segmenting nuclei instances is a crucial step in computer-aided\nimage analysis to extract rich features for cellular estimation and following\ndiagnosis as well as treatment. While it still remains challenging because the\nwide existence of nuclei clusters, along with the large morphological variances\namong different organs make nuclei instance segmentation susceptible to\nover-/under-segmentation. Additionally, the inevitably subjective annotating\nand mislabeling prevent the network learning from reliable samples and\neventually reduce the generalization capability for robustly segmenting unseen\norgan nuclei. To address these issues, we propose a novel deep neural network,\nnamely Contour-aware Informative Aggregation Network (CIA-Net) with multi-level\ninformation aggregation module between two task-specific decoders. Rather than\nindependent decoders, it leverages the merit of spatial and texture\ndependencies between nuclei and contour by bi-directionally aggregating\ntask-specific features. Furthermore, we proposed a novel smooth truncated loss\nthat modulates losses to reduce the perturbation from outliers. Consequently,\nthe network can focus on learning from reliable and informative samples, which\ninherently improves the generalization capability. Experiments on the 2018\nMICCAI challenge of Multi-Organ-Nuclei-Segmentation validated the effectiveness\nof our proposed method, surpassing all the other 35 competitive teams by a\nsignificant margin.\n", "versions": [{"version": "v1", "created": "Wed, 13 Mar 2019 08:43:01 GMT"}], "update_date": "2019-03-14", "authors_parsed": [["Zhou", "Yanning", ""], ["Onder", "Omer Fahri", ""], ["Dou", "Qi", ""], ["Tsougenis", "Efstratios", ""], ["Chen", "Hao", ""], ["Heng", "Pheng-Ann", ""]]}, {"id": "1903.05359", "submitter": "Zhan Yang", "authors": "Jun Long, WuQing Sun, Zhan Yang, Osolo Ian Raymond", "title": "Asymmetric Residual Neural Network for Accurate Human Activity\n  Recognition", "comments": "Accepted by Information", "journal-ref": null, "doi": "10.3390/info10060203", "report-no": null, "categories": "cs.CV cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human Activity Recognition (HAR) using deep neural network has become a hot\ntopic in human-computer interaction. Machine can effectively identify human\nnaturalistic activities by learning from a large collection of sensor data.\nActivity recognition is not only an interesting research problem, but also has\nmany real-world practical applications. Based on the success of residual\nnetworks in achieving a high level of aesthetic representation of the automatic\nlearning, we propose a novel \\textbf{A}symmetric \\textbf{R}esidual\n\\textbf{N}etwork, named ARN. ARN is implemented using two identical path\nframeworks consisting of (1) a short time window, which is used to capture\nspatial features, and (2) a long time window, which is used to capture fine\ntemporal features. The long time window path can be made very lightweight by\nreducing its channel capacity, yet still being able to learn useful temporal\nrepresentations for activity recognition. In this paper, we mainly focus on\nproposing a new model to improve the accuracy of HAR. In order to demonstrate\nthe effectiveness of ARN model, we carried out extensive experiments on\nbenchmark datasets (i.e., OPPORTUNITY, UniMiB-SHAR) and compared with some\nconventional and state-of-the-art learning-based methods. Then, we discuss the\ninfluence of networks parameters on performance to provide insights about its\noptimization. Results from our experiments show that ARN is effective in\nrecognizing human activities via wearable datasets.\n", "versions": [{"version": "v1", "created": "Wed, 13 Mar 2019 08:44:01 GMT"}, {"version": "v2", "created": "Wed, 29 May 2019 08:41:42 GMT"}, {"version": "v3", "created": "Tue, 11 Jun 2019 13:05:37 GMT"}], "update_date": "2019-06-12", "authors_parsed": [["Long", "Jun", ""], ["Sun", "WuQing", ""], ["Yang", "Zhan", ""], ["Raymond", "Osolo Ian", ""]]}, {"id": "1903.05369", "submitter": "Mingtao Pei", "authors": "Huiling Hao and Mingtao Pei", "title": "Face Liveness Detection Based on Client Identity Using Siamese Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Face liveness detection is an essential prerequisite for face recognition\napplications. Previous face liveness detection methods usually train a binary\nclassifier to differentiate between a fake face and a real face before face\nrecognition. The client identity information is not utilized in previous face\nliveness detection methods. However, in practical face recognition\napplications, face spoofing attacks are always aimed at a specific client, and\nthe client identity information can provide useful clues for face liveness\ndetection. In this paper, we propose a face liveness detection method based on\nthe client identity using Siamese network. We detect face liveness after face\nrecognition instead of before face recognition, that is, we detect face\nliveness with the client identity information. We train a Siamese network with\nimage pairs. Each image pair consists of two real face images or one real and\none fake face images. The face images in each pair come from a same client.\nGiven a test face image, the face image is firstly recognized by face\nrecognition system, then the real face image of the identified client is\nretrieved to help the face liveness detection. Experiment results demonstrate\nthe effectiveness of our method.\n", "versions": [{"version": "v1", "created": "Wed, 13 Mar 2019 09:12:38 GMT"}], "update_date": "2019-03-14", "authors_parsed": [["Hao", "Huiling", ""], ["Pei", "Mingtao", ""]]}, {"id": "1903.05421", "submitter": "Saif Imran", "authors": "Saif Imran, Yunfei Long, Xiaoming Liu, Daniel Morris", "title": "Depth Coefficients for Depth Completion", "comments": "to appear in Proc. IEEE Conf. Computer Vision and Pattern Recognition\n  (CVPR) 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Depth completion involves estimating a dense depth image from sparse depth\nmeasurements, often guided by a color image. While linear upsampling is\nstraight forward, it results in artifacts including depth pixels being\ninterpolated in empty space across discontinuities between objects. Current\nmethods use deep networks to upsample and \"complete\" the missing depth pixels.\nNevertheless, depth smearing between objects remains a challenge. We propose a\nnew representation for depth called Depth Coefficients (DC) to address this\nproblem. It enables convolutions to more easily avoid inter-object depth\nmixing. We also show that the standard Mean Squared Error (MSE) loss function\ncan promote depth mixing, and thus propose instead to use cross-entropy loss\nfor DC. With quantitative and qualitative evaluation on benchmarks, we show\nthat switching out sparse depth input and MSE loss with our DC representation\nand cross-entropy loss is a simple way to improve depth completion performance,\nand reduce pixel depth mixing, which leads to improved depth-based object\ndetection.\n", "versions": [{"version": "v1", "created": "Wed, 13 Mar 2019 11:39:00 GMT"}], "update_date": "2019-03-14", "authors_parsed": [["Imran", "Saif", ""], ["Long", "Yunfei", ""], ["Liu", "Xiaoming", ""], ["Morris", "Daniel", ""]]}, {"id": "1903.05434", "submitter": "Daqi Liu", "authors": "Daqi Liu, Miroslaw Bober and Josef Kittler", "title": "Visual Semantic Information Pursuit: A Survey", "comments": "Preliminary work. Under review by IEEE Transactions on Pattern\n  Analysis and Machine Intelligence (PAMI). Do not distribute", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual semantic information comprises two important parts: the meaning of\neach visual semantic unit and the coherent visual semantic relation conveyed by\nthese visual semantic units. Essentially, the former one is a visual perception\ntask while the latter one corresponds to visual context reasoning. Remarkable\nadvances in visual perception have been achieved due to the success of deep\nlearning. In contrast, visual semantic information pursuit, a visual scene\nsemantic interpretation task combining visual perception and visual context\nreasoning, is still in its early stage. It is the core task of many different\ncomputer vision applications, such as object detection, visual semantic\nsegmentation, visual relationship detection or scene graph generation. Since it\nhelps to enhance the accuracy and the consistency of the resulting\ninterpretation, visual context reasoning is often incorporated with visual\nperception in current deep end-to-end visual semantic information pursuit\nmethods. However, a comprehensive review for this exciting area is still\nlacking. In this survey, we present a unified theoretical paradigm for all\nthese methods, followed by an overview of the major developments and the future\ntrends in each potential direction. The common benchmark datasets, the\nevaluation metrics and the comparisons of the corresponding methods are also\nintroduced.\n", "versions": [{"version": "v1", "created": "Wed, 13 Mar 2019 12:01:12 GMT"}], "update_date": "2019-03-14", "authors_parsed": [["Liu", "Daqi", ""], ["Bober", "Miroslaw", ""], ["Kittler", "Josef", ""]]}, {"id": "1903.05454", "submitter": "Raffaele Imbriaco", "authors": "Raffaele Imbriaco, Clint Sebastian, Egor Bondarev, Peter de With", "title": "Towards Accurate Camera Geopositioning by Image Matching", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this work, we present a camera geopositioning system based on matching a\nquery image against a database with panoramic images. For matching, our system\nuses memory vectors aggregated from global image descriptors based on\nconvolutional features to facilitate fast searching in the database. To speed\nup searching, a clustering algorithm is used to balance geographical\npositioning and computation time. We refine the obtained position from the\nquery image using a new outlier removal algorithm. The matching of the query\nimage is obtained with a recall@5 larger than 90% for panorama-to-panorama\nmatching. We cluster available panoramas from geographically adjacent locations\ninto a single compact representation and observe computational gains of\napproximately 50% at the cost of only a small (approximately 3%) recall loss.\nFinally, we present a coordinate estimation algorithm that reduces the median\ngeopositioning error by up to 20%.\n", "versions": [{"version": "v1", "created": "Wed, 13 Mar 2019 12:37:42 GMT"}], "update_date": "2019-03-14", "authors_parsed": [["Imbriaco", "Raffaele", ""], ["Sebastian", "Clint", ""], ["Bondarev", "Egor", ""], ["de With", "Peter", ""]]}, {"id": "1903.05503", "submitter": "Wenzhao Zheng", "authors": "Wenzhao Zheng, Zhaodong Chen, Jiwen Lu, Jie Zhou", "title": "Hardness-Aware Deep Metric Learning", "comments": "Accepted as CVPR 2019 Oral. Source code available at\n  https://github.com/wzzheng/HDML", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a hardness-aware deep metric learning (HDML) framework.\nMost previous deep metric learning methods employ the hard negative mining\nstrategy to alleviate the lack of informative samples for training. However,\nthis mining strategy only utilizes a subset of training data, which may not be\nenough to characterize the global geometry of the embedding space\ncomprehensively. To address this problem, we perform linear interpolation on\nembeddings to adaptively manipulate their hard levels and generate\ncorresponding label-preserving synthetics for recycled training, so that\ninformation buried in all samples can be fully exploited and the metric is\nalways challenged with proper difficulty. Our method achieves very competitive\nperformance on the widely used CUB-200-2011, Cars196, and Stanford Online\nProducts datasets.\n", "versions": [{"version": "v1", "created": "Wed, 13 Mar 2019 14:14:54 GMT"}, {"version": "v2", "created": "Wed, 4 Dec 2019 13:10:32 GMT"}], "update_date": "2019-12-05", "authors_parsed": [["Zheng", "Wenzhao", ""], ["Chen", "Zhaodong", ""], ["Lu", "Jiwen", ""], ["Zhou", "Jie", ""]]}, {"id": "1903.05558", "submitter": "Mingming Li", "authors": "Ruirui Li, Mingming Li, Jiacheng Li, Yating Zhou", "title": "Connection Sensitive Attention U-NET for Accurate Retinal Vessel\n  Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a connection sensitive attention U-Net(CSAU) for accurate retinal\nvessel segmentation. This method improves the recent attention U-Net for\nsemantic segmentation with four key improvements: (1) connection sensitive loss\nthat models the structure properties to improve the accuracy of pixel-wise\nsegmentation; (2) attention gate with novel neural network structure and\nconcatenating DOWN-Link to effectively learn better attention weights on fine\nvessels; (3) integration of connection sensitive loss and attention gate to\nfurther improve the accuracy on detailed vessels by additionally concatenating\nattention weights to features before output; (4) metrics of connection\nsensitive accuracy to reflect the segmentation performance on boundaries and\nthin vessels.\n  Our method can effectively improve state-of-the-art vessel segmentation\nmethods that suffer from difficulties in presence of abnormalities, bifurcation\nand microvascular. This connection sensitive loss tightly integrates with the\nproposed attention U-Net to accurately (i) segment retinal vessels, and (ii)\nreserve the connectivity of thin vessels by modeling the structural properties.\nOur method achieves the leading position on DRIVE, STARE and HRF datasets among\nthe state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Wed, 13 Mar 2019 15:51:50 GMT"}, {"version": "v2", "created": "Tue, 23 Apr 2019 12:25:42 GMT"}], "update_date": "2019-04-24", "authors_parsed": [["Li", "Ruirui", ""], ["Li", "Mingming", ""], ["Li", "Jiacheng", ""], ["Zhou", "Yating", ""]]}, {"id": "1903.05572", "submitter": "Johannes Sch\\\"onberger", "authors": "Pablo Speciale, Johannes L. Sch\\\"onberger, Sing Bing Kang, Sudipta N.\n  Sinha, Marc Pollefeys", "title": "Privacy Preserving Image-Based Localization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image-based localization is a core component of many augmented/mixed reality\n(AR/MR) and autonomous robotic systems. Current localization systems rely on\nthe persistent storage of 3D point clouds of the scene to enable camera pose\nestimation, but such data reveals potentially sensitive scene information. This\ngives rise to significant privacy risks, especially as for many applications 3D\nmapping is a background process that the user might not be fully aware of. We\npose the following question: How can we avoid disclosing confidential\ninformation about the captured 3D scene, and yet allow reliable camera pose\nestimation? This paper proposes the first solution to what we call privacy\npreserving image-based localization. The key idea of our approach is to lift\nthe map representation from a 3D point cloud to a 3D line cloud. This novel\nrepresentation obfuscates the underlying scene geometry while providing\nsufficient geometric constraints to enable robust and accurate 6-DOF camera\npose estimation. Extensive experiments on several datasets and localization\nscenarios underline the high practical relevance of our proposed approach.\n", "versions": [{"version": "v1", "created": "Wed, 13 Mar 2019 16:12:04 GMT"}], "update_date": "2019-03-14", "authors_parsed": [["Speciale", "Pablo", ""], ["Sch\u00f6nberger", "Johannes L.", ""], ["Kang", "Sing Bing", ""], ["Sinha", "Sudipta N.", ""], ["Pollefeys", "Marc", ""]]}, {"id": "1903.05577", "submitter": "Dong Liu", "authors": "Haochen Zhang, Dong Liu, Zhiwei Xiong", "title": "Two-Stream Action Recognition-Oriented Video Super-Resolution", "comments": "Accepted to ICCV 2019. Code:\n  https://github.com/AlanZhang1995/TwoStreamSR", "journal-ref": null, "doi": "10.1109/ICCV.2019.00889", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the video super-resolution (SR) problem for facilitating video\nanalytics tasks, e.g. action recognition, instead of for visual quality. The\npopular action recognition methods based on convolutional networks, exemplified\nby two-stream networks, are not directly applicable on video of low spatial\nresolution. This can be remedied by performing video SR prior to recognition,\nwhich motivates us to improve the SR procedure for recognition accuracy.\nTailored for two-stream action recognition networks, we propose two video SR\nmethods for the spatial and temporal streams respectively. On the one hand, we\nobserve that regions with action are more important to recognition, and we\npropose an optical-flow guided weighted mean-squared-error loss for our\nspatial-oriented SR (SoSR) network to emphasize the reconstruction of moving\nobjects. On the other hand, we observe that existing video SR methods incur\ntemporal discontinuity between frames, which also worsens the recognition\naccuracy, and we propose a siamese network for our temporal-oriented SR (ToSR)\ntraining that emphasizes the temporal continuity between consecutive frames. We\nperform experiments using two state-of-the-art action recognition networks and\ntwo well-known datasets--UCF101 and HMDB51. Results demonstrate the\neffectiveness of our proposed SoSR and ToSR in improving recognition accuracy.\n", "versions": [{"version": "v1", "created": "Wed, 13 Mar 2019 16:22:36 GMT"}, {"version": "v2", "created": "Fri, 16 Aug 2019 01:40:03 GMT"}], "update_date": "2020-03-13", "authors_parsed": [["Zhang", "Haochen", ""], ["Liu", "Dong", ""], ["Xiong", "Zhiwei", ""]]}, {"id": "1903.05580", "submitter": "Jakub Nalepa", "authors": "Jakub Nalepa, Michal Myller, Michal Kawulok", "title": "Hyperspectral Data Augmentation", "comments": "Submitted to IEEE Geoscience and Remote Sensing Letters", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data augmentation is a popular technique which helps improve generalization\ncapabilities of deep neural networks. It plays a pivotal role in remote-sensing\nscenarios in which the amount of high-quality ground truth data is limited, and\nacquiring new examples is costly or impossible. This is a common problem in\nhyperspectral imaging, where manual annotation of image data is difficult,\nexpensive, and prone to human bias. In this letter, we propose online data\naugmentation of hyperspectral data which is executed during the inference\nrather than before the training of deep networks. This is in contrast to all\nother state-of-the-art hyperspectral augmentation algorithms which increase the\nsize (and representativeness) of training sets. Additionally, we introduce a\nnew principal component analysis based augmentation. The experiments revealed\nthat our data augmentation algorithms improve generalization of deep networks,\nwork in real-time, and the online approach can be effectively combined with\noffline techniques to enhance the classification accuracy.\n", "versions": [{"version": "v1", "created": "Wed, 13 Mar 2019 16:27:38 GMT"}], "update_date": "2019-03-14", "authors_parsed": [["Nalepa", "Jakub", ""], ["Myller", "Michal", ""], ["Kawulok", "Michal", ""]]}, {"id": "1903.05598", "submitter": "Clint Sebastian", "authors": "Clint Sebastian, Bas Boom, Egor Bondarev, Peter H.N. de With", "title": "LiDAR-assisted Large-scale Privacy Protection in Street-view Cycloramas", "comments": "Accepted at Electronic Imaging 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recently, privacy has a growing importance in several domains, especially in\nstreet-view images. The conventional way to achieve this is to automatically\ndetect and blur sensitive information from these images. However, the\nprocessing cost of blurring increases with the ever-growing resolution of\nimages. We propose a system that is cost-effective even after increasing the\nresolution by a factor of 2.5. The new system utilizes depth data obtained from\nLiDAR to significantly reduce the search space for detection, thereby reducing\nthe processing cost. Besides this, we test several detectors after reducing the\ndetection space and provide an alternative solution based on state-of-the-art\ndeep learning detectors to the existing HoG-SVM-Deep system that is faster and\nhas a higher performance.\n", "versions": [{"version": "v1", "created": "Wed, 13 Mar 2019 16:57:03 GMT"}], "update_date": "2019-03-14", "authors_parsed": [["Sebastian", "Clint", ""], ["Boom", "Bas", ""], ["Bondarev", "Egor", ""], ["de With", "Peter H. N.", ""]]}, {"id": "1903.05612", "submitter": "Carles Ventura", "authors": "Carles Ventura, Miriam Bellver, Andreu Girbau, Amaia Salvador, Ferran\n  Marques, Xavier Giro-i-Nieto", "title": "RVOS: End-to-End Recurrent Network for Video Object Segmentation", "comments": "CVPR 2019 camera ready. Project website:\n  https://imatge-upc.github.io/rvos/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multiple object video object segmentation is a challenging task, specially\nfor the zero-shot case, when no object mask is given at the initial frame and\nthe model has to find the objects to be segmented along the sequence. In our\nwork, we propose a Recurrent network for multiple object Video Object\nSegmentation (RVOS) that is fully end-to-end trainable. Our model incorporates\nrecurrence on two different domains: (i) the spatial, which allows to discover\nthe different object instances within a frame, and (ii) the temporal, which\nallows to keep the coherence of the segmented objects along time. We train RVOS\nfor zero-shot video object segmentation and are the first ones to report\nquantitative results for DAVIS-2017 and YouTube-VOS benchmarks. Further, we\nadapt RVOS for one-shot video object segmentation by using the masks obtained\nin previous time steps as inputs to be processed by the recurrent module. Our\nmodel reaches comparable results to state-of-the-art techniques in YouTube-VOS\nbenchmark and outperforms all previous video object segmentation methods not\nusing online learning in the DAVIS-2017 benchmark. Moreover, our model achieves\nfaster inference runtimes than previous methods, reaching 44ms/frame on a P100\nGPU.\n", "versions": [{"version": "v1", "created": "Wed, 13 Mar 2019 17:26:15 GMT"}, {"version": "v2", "created": "Tue, 21 May 2019 06:56:56 GMT"}], "update_date": "2019-05-22", "authors_parsed": [["Ventura", "Carles", ""], ["Bellver", "Miriam", ""], ["Girbau", "Andreu", ""], ["Salvador", "Amaia", ""], ["Marques", "Ferran", ""], ["Giro-i-Nieto", "Xavier", ""]]}, {"id": "1903.05625", "submitter": "Tim Meinhardt", "authors": "Philipp Bergmann, Tim Meinhardt, Laura Leal-Taixe", "title": "Tracking without bells and whistles", "comments": null, "journal-ref": null, "doi": "10.1109/ICCV.2019.00103", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of tracking multiple objects in a video sequence poses several\nchallenging tasks. For tracking-by-detection, these include object\nre-identification, motion prediction and dealing with occlusions. We present a\ntracker (without bells and whistles) that accomplishes tracking without\nspecifically targeting any of these tasks, in particular, we perform no\ntraining or optimization on tracking data. To this end, we exploit the bounding\nbox regression of an object detector to predict the position of an object in\nthe next frame, thereby converting a detector into a Tracktor. We demonstrate\nthe potential of Tracktor and provide a new state-of-the-art on three\nmulti-object tracking benchmarks by extending it with a straightforward\nre-identification and camera motion compensation. We then perform an analysis\non the performance and failure cases of several state-of-the-art tracking\nmethods in comparison to our Tracktor. Surprisingly, none of the dedicated\ntracking methods are considerably better in dealing with complex tracking\nscenarios, namely, small and occluded objects or missing detections. However,\nour approach tackles most of the easy tracking scenarios. Therefore, we\nmotivate our approach as a new tracking paradigm and point out promising future\nresearch directions. Overall, Tracktor yields superior tracking performance\nthan any current tracking method and our analysis exposes remaining and\nunsolved tracking challenges to inspire future research directions.\n", "versions": [{"version": "v1", "created": "Wed, 13 Mar 2019 17:45:49 GMT"}, {"version": "v2", "created": "Wed, 10 Apr 2019 15:33:57 GMT"}, {"version": "v3", "created": "Sat, 17 Aug 2019 14:40:56 GMT"}], "update_date": "2021-04-30", "authors_parsed": [["Bergmann", "Philipp", ""], ["Meinhardt", "Tim", ""], ["Leal-Taixe", "Laura", ""]]}, {"id": "1903.05628", "submitter": "Qi Mao", "authors": "Qi Mao, Hsin-Ying Lee, Hung-Yu Tseng, Siwei Ma, Ming-Hsuan Yang", "title": "Mode Seeking Generative Adversarial Networks for Diverse Image Synthesis", "comments": "CVPR 2019. Code: https://github.com/HelenMao/MSGAN", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most conditional generation tasks expect diverse outputs given a single\nconditional context. However, conditional generative adversarial networks\n(cGANs) often focus on the prior conditional information and ignore the input\nnoise vectors, which contribute to the output variations. Recent attempts to\nresolve the mode collapse issue for cGANs are usually task-specific and\ncomputationally expensive. In this work, we propose a simple yet effective\nregularization term to address the mode collapse issue for cGANs. The proposed\nmethod explicitly maximizes the ratio of the distance between generated images\nwith respect to the corresponding latent codes, thus encouraging the generators\nto explore more minor modes during training. This mode seeking regularization\nterm is readily applicable to various conditional generation tasks without\nimposing training overhead or modifying the original network structures. We\nvalidate the proposed algorithm on three conditional image synthesis tasks\nincluding categorical generation, image-to-image translation, and text-to-image\nsynthesis with different baseline models. Both qualitative and quantitative\nresults demonstrate the effectiveness of the proposed regularization method for\nimproving diversity without loss of quality.\n", "versions": [{"version": "v1", "created": "Wed, 13 Mar 2019 17:50:36 GMT"}, {"version": "v2", "created": "Mon, 18 Mar 2019 00:32:36 GMT"}, {"version": "v3", "created": "Wed, 3 Apr 2019 01:04:08 GMT"}, {"version": "v4", "created": "Mon, 22 Apr 2019 00:44:16 GMT"}, {"version": "v5", "created": "Wed, 24 Apr 2019 02:07:09 GMT"}, {"version": "v6", "created": "Sat, 4 May 2019 02:12:05 GMT"}], "update_date": "2019-05-07", "authors_parsed": [["Mao", "Qi", ""], ["Lee", "Hsin-Ying", ""], ["Tseng", "Hung-Yu", ""], ["Ma", "Siwei", ""], ["Yang", "Ming-Hsuan", ""]]}, {"id": "1903.05663", "submitter": "Laura Domin\\'e", "authors": "Laura Domin\\'e and Kazuhiro Terao", "title": "Scalable Deep Convolutional Neural Networks for Sparse, Locally Dense\n  Liquid Argon Time Projection Chamber Data", "comments": "Corrected URL to dataset Corrected figures and numbers", "journal-ref": "Phys. Rev. D 102, 012005 (2020)", "doi": "10.1103/PhysRevD.102.012005", "report-no": null, "categories": "hep-ex cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep convolutional neural networks (CNNs) show strong promise for analyzing\nscientific data in many domains including particle imaging detectors such as a\nliquid argon time projection chamber (LArTPC). Yet the high sparsity of LArTPC\ndata challenges traditional CNNs which were designed for dense data such as\nphotographs. A naive application of CNNs on LArTPC data results in inefficient\ncomputations and a poor scalability to large LArTPC detectors such as the Short\nBaseline Neutrino Program and Deep Underground Neutrino Experiment. Recently\nSubmanifold Sparse Convolutional Networks (SSCNs) have been proposed to address\nthis challenge. We report their performance on a 3D semantic segmentation task\non simulated LArTPC samples. In comparison with standard CNNs, we observe that\nthe computation memory and wall-time cost for inference are reduced by factor\nof 364 and 33 respectively without loss of accuracy. The same factors for 2D\nsamples are found to be 93 and 3.1 respectively. Using SSCN, we present the\nfirst machine learning-based approach to the reconstruction of Michel electrons\nusing public 3D LArTPC samples. We find a Michel electron identification\nefficiency of 93.9% with 96.7% of true positive rate. Reconstructed Michel\nelectron clusters yield 95.4% in average pixel clustering efficiency and 95.5%\nin purity. The results are compelling to show strong promise of scalable data\nreconstruction technique using deep neural networks for large scale LArTPC\ndetectors.\n", "versions": [{"version": "v1", "created": "Wed, 13 Mar 2019 18:25:20 GMT"}, {"version": "v2", "created": "Fri, 15 Mar 2019 09:12:45 GMT"}, {"version": "v3", "created": "Sat, 21 Dec 2019 03:21:11 GMT"}], "update_date": "2020-07-15", "authors_parsed": [["Domin\u00e9", "Laura", ""], ["Terao", "Kazuhiro", ""]]}, {"id": "1903.05684", "submitter": "Helge Rhodin", "authors": "Helge Rhodin, Victor Constantin, Isinsu Katircioglu, Mathieu Salzmann,\n  and Pascal Fua", "title": "Neural Scene Decomposition for Multi-Person Motion Capture", "comments": "CVPR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning general image representations has proven key to the success of many\ncomputer vision tasks. For example, many approaches to image understanding\nproblems rely on deep networks that were initially trained on ImageNet, mostly\nbecause the learned features are a valuable starting point to learn from\nlimited labeled data. However, when it comes to 3D motion capture of multiple\npeople, these features are only of limited use.\n  In this paper, we therefore propose an approach to learning features that are\nuseful for this purpose. To this end, we introduce a self-supervised approach\nto learning what we call a neural scene decomposition (NSD) that can be\nexploited for 3D pose estimation. NSD comprises three layers of abstraction to\nrepresent human subjects: spatial layout in terms of bounding-boxes and\nrelative depth; a 2D shape representation in terms of an instance segmentation\nmask; and subject-specific appearance and 3D pose information. By exploiting\nself-supervision coming from multiview data, our NSD model can be trained\nend-to-end without any 2D or 3D supervision. In contrast to previous\napproaches, it works for multiple persons and full-frame images. Because it\nencodes 3D geometry, NSD can then be effectively leveraged to train a 3D pose\nestimation network from small amounts of annotated data.\n", "versions": [{"version": "v1", "created": "Wed, 13 Mar 2019 19:02:46 GMT"}], "update_date": "2019-03-15", "authors_parsed": [["Rhodin", "Helge", ""], ["Constantin", "Victor", ""], ["Katircioglu", "Isinsu", ""], ["Salzmann", "Mathieu", ""], ["Fua", "Pascal", ""]]}, {"id": "1903.05690", "submitter": "Sifei Liu", "authors": "Xueting Li, Sifei Liu, Kihwan Kim, Xiaolong Wang, Ming-Hsuan Yang, Jan\n  Kautz", "title": "Putting Humans in a Scene: Learning Affordance in 3D Indoor Environments", "comments": "https://sites.google.com/view/3d-affordance-cvpr19", "journal-ref": "CVPR 2019", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Affordance modeling plays an important role in visual understanding. In this\npaper, we aim to predict affordances of 3D indoor scenes, specifically what\nhuman poses are afforded by a given indoor environment, such as sitting on a\nchair or standing on the floor. In order to predict valid affordances and learn\npossible 3D human poses in indoor scenes, we need to understand the semantic\nand geometric structure of a scene as well as its potential interactions with a\nhuman. To learn such a model, a large-scale dataset of 3D indoor affordances is\nrequired. In this work, we build a fully automatic 3D pose synthesizer that\nfuses semantic knowledge from a large number of 2D poses extracted from TV\nshows as well as 3D geometric knowledge from voxel representations of indoor\nscenes. With the data created by the synthesizer, we introduce a 3D pose\ngenerative model to predict semantically plausible and physically feasible\nhuman poses within a given scene (provided as a single RGB, RGB-D, or depth\nimage). We demonstrate that our human affordance prediction method consistently\noutperforms existing state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Wed, 13 Mar 2019 19:16:15 GMT"}, {"version": "v2", "created": "Fri, 15 Mar 2019 19:28:24 GMT"}], "update_date": "2019-03-19", "authors_parsed": [["Li", "Xueting", ""], ["Liu", "Sifei", ""], ["Kim", "Kihwan", ""], ["Wang", "Xiaolong", ""], ["Yang", "Ming-Hsuan", ""], ["Kautz", "Jan", ""]]}, {"id": "1903.05696", "submitter": "Aaron Hertzmann", "authors": "Aaron Hertzmann", "title": "Aesthetics of Neural Network Art", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a way to understand neural network artworks as\njuxtapositions of natural image cues. It is hypothesized that images with\nunusual combinations of realistic visual cues are interesting, and, neural\nmodels trained to model natural images are well-suited to creating interesting\nimages. Art using neural models produces new images similar to those of natural\nimages, but with weird and intriguing variations. This analysis is applied to\nneural art based on Generative Adversarial Networks, image stylization, Deep\nDreams, and Perception Engines.\n", "versions": [{"version": "v1", "created": "Wed, 13 Mar 2019 19:45:54 GMT"}, {"version": "v2", "created": "Mon, 18 Mar 2019 17:58:15 GMT"}], "update_date": "2019-03-19", "authors_parsed": [["Hertzmann", "Aaron", ""]]}, {"id": "1903.05711", "submitter": "Hunter Goforth", "authors": "Yasuhiro Aoki, Hunter Goforth, Rangaprasad Arun Srivatsan, Simon Lucey", "title": "PointNetLK: Robust & Efficient Point Cloud Registration using PointNet", "comments": "Accepted in CVPR 2019. v2: updated affiliations, additional result in\n  Fig 1", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  PointNet has revolutionized how we think about representing point clouds. For\nclassification and segmentation tasks, the approach and its subsequent\nextensions are state-of-the-art. To date, the successful application of\nPointNet to point cloud registration has remained elusive. In this paper we\nargue that PointNet itself can be thought of as a learnable \"imaging\" function.\nAs a consequence, classical vision algorithms for image alignment can be\napplied on the problem - namely the Lucas & Kanade (LK) algorithm. Our central\ninnovations stem from: (i) how to modify the LK algorithm to accommodate the\nPointNet imaging function, and (ii) unrolling PointNet and the LK algorithm\ninto a single trainable recurrent deep neural network. We describe the\narchitecture, and compare its performance against state-of-the-art in common\nregistration scenarios. The architecture offers some remarkable properties\nincluding: generalization across shape categories and computational efficiency\n- opening up new paths of exploration for the application of deep learning to\npoint cloud registration. Code and videos are available at\nhttps://github.com/hmgoforth/PointNetLK.\n", "versions": [{"version": "v1", "created": "Wed, 13 Mar 2019 20:56:03 GMT"}, {"version": "v2", "created": "Thu, 4 Apr 2019 16:27:35 GMT"}], "update_date": "2019-04-05", "authors_parsed": [["Aoki", "Yasuhiro", ""], ["Goforth", "Hunter", ""], ["Srivatsan", "Rangaprasad Arun", ""], ["Lucey", "Simon", ""]]}, {"id": "1903.05749", "submitter": "Changkyu Song", "authors": "Changkyu Song and Abdeslam Boularias", "title": "Inferring 3D Shapes of Unknown Rigid Objects in Clutter through Inverse\n  Physics Reasoning", "comments": null, "journal-ref": "The IEEE Robotics and Automation Letters (RA-L) with the IEEE\n  International Conference on Robotics and Automation (ICRA 2019)", "doi": "10.1109/LRA.2018.2885579", "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a probabilistic approach for building, on the fly, 3-D models of\nunknown objects while being manipulated by a robot. We specifically consider\nmanipulation tasks in piles of clutter that contain previously unseen objects.\nMost manipulation algorithms for performing such tasks require known geometric\nmodels of the objects in order to grasp or rearrange them robustly. One of the\nnovel aspects of this work is the utilization of a physics engine for verifying\nhypothesized geometries in simulation. The evidence provided by physics\nsimulations is used in a probabilistic framework that accounts for the fact\nthat mechanical properties of the objects are uncertain. We present an\nefficient algorithm for inferring occluded parts of objects based on their\nobserved motions and mutual interactions. Experiments using a robot show that\nthis approach is efficient for constructing physically realistic 3-D models,\nwhich can be useful for manipulation planning. Experiments also show that the\nproposed approach significantly outperforms alternative approaches in terms of\nshape accuracy.\n", "versions": [{"version": "v1", "created": "Wed, 13 Mar 2019 23:05:08 GMT"}], "update_date": "2019-03-15", "authors_parsed": [["Song", "Changkyu", ""], ["Boularias", "Abdeslam", ""]]}, {"id": "1903.05757", "submitter": "Xiaofeng Gao", "authors": "Xiaofeng Gao, Ran Gong, Tianmin Shu, Xu Xie, Shu Wang, Song-Chun Zhu", "title": "VRKitchen: an Interactive 3D Virtual Environment for Task-oriented\n  Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the main challenges of advancing task-oriented learning such as visual\ntask planning and reinforcement learning is the lack of realistic and\nstandardized environments for training and testing AI agents. Previously,\nresearchers often relied on ad-hoc lab environments. There have been recent\nadvances in virtual systems built with 3D physics engines and photo-realistic\nrendering for indoor and outdoor environments, but the embodied agents in those\nsystems can only conduct simple interactions with the world (e.g., walking\naround, moving objects, etc.). Most of the existing systems also do not allow\nhuman participation in their simulated environments. In this work, we design\nand implement a virtual reality (VR) system, VRKitchen, with integrated\nfunctions which i) enable embodied agents powered by modern AI methods (e.g.,\nplanning, reinforcement learning, etc.) to perform complex tasks involving a\nwide range of fine-grained object manipulations in a realistic environment, and\nii) allow human teachers to perform demonstrations to train agents (i.e.,\nlearning from demonstration). We also provide standardized evaluation\nbenchmarks and data collection tools to facilitate a broad use in research on\ntask-oriented learning and beyond.\n", "versions": [{"version": "v1", "created": "Wed, 13 Mar 2019 23:31:21 GMT"}], "update_date": "2019-03-15", "authors_parsed": [["Gao", "Xiaofeng", ""], ["Gong", "Ran", ""], ["Shu", "Tianmin", ""], ["Xie", "Xu", ""], ["Wang", "Shu", ""], ["Zhu", "Song-Chun", ""]]}, {"id": "1903.05761", "submitter": "Takao Yamanaka", "authors": "Reo Ogusu and Takao Yamanaka", "title": "LPM: Learnable Pooling Module for Efficient Full-Face Gaze Estimation", "comments": "FG2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gaze tracking is an important technology in many domains. Techniques such as\nConvolutional Neural Networks (CNN) has allowed the invention of gaze tracking\nmethod that relies only on commodity hardware such as the camera on a personal\ncomputer. It has been shown that the full-face region for gaze estimation can\nprovide better performance than from an eye image alone. However, a problem\nwith using the full-face image is the heavy computation due to the larger image\nsize. This study tackles this problem through compression of the input\nfull-face image by removing redundant information using a novel learnable\npooling module. The module can be trained end-to-end by backpropagation to\nlearn the size of the grid in the pooling filter. The learnable pooling module\nkeeps the resolution of valuable regions high and vice versa. This proposed\nmethod preserved the gaze estimation accuracy at a certain level when the image\nwas reduced to a smaller size.\n", "versions": [{"version": "v1", "created": "Wed, 13 Mar 2019 23:47:23 GMT"}, {"version": "v2", "created": "Thu, 21 Mar 2019 03:16:40 GMT"}], "update_date": "2019-03-22", "authors_parsed": [["Ogusu", "Reo", ""], ["Yamanaka", "Takao", ""]]}, {"id": "1903.05769", "submitter": "Oguzhan Gencoglu", "authors": "Umair Akhtar Hasan Khan, Carolin St\\\"urenberg, Oguzhan Gencoglu, Kevin\n  Sandeman, Timo Heikkinen, Antti Rannikko, Tuomas Mirtti", "title": "Improving Prostate Cancer Detection with Breast Histopathology Images", "comments": "9 pages, 2 figures", "journal-ref": null, "doi": "10.1007/978-3-030-23937-4_11", "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks have introduced significant advancements in the field of\nmachine learning-based analysis of digital pathology images including prostate\ntissue images. With the help of transfer learning, classification and\nsegmentation performance of neural network models have been further increased.\nHowever, due to the absence of large, extensively annotated, publicly available\nprostate histopathology datasets, several previous studies employ datasets from\nwell-studied computer vision tasks such as ImageNet dataset. In this work, we\npropose a transfer learning scheme from breast histopathology images to improve\nprostate cancer detection performance. We validate our approach on annotated\nprostate whole slide images by using a publicly available breast histopathology\ndataset as pre-training. We show that the proposed cross-cancer approach\noutperforms transfer learning from ImageNet dataset.\n", "versions": [{"version": "v1", "created": "Thu, 14 Mar 2019 00:09:14 GMT"}], "update_date": "2019-08-01", "authors_parsed": [["Khan", "Umair Akhtar Hasan", ""], ["St\u00fcrenberg", "Carolin", ""], ["Gencoglu", "Oguzhan", ""], ["Sandeman", "Kevin", ""], ["Heikkinen", "Timo", ""], ["Rannikko", "Antti", ""], ["Mirtti", "Tuomas", ""]]}, {"id": "1903.05784", "submitter": "Longguang Wang", "authors": "Longguang Wang, Yingqian Wang, Zhengfa Liang, Zaiping Lin, Jungang\n  Yang, Wei An, Yulan Guo", "title": "Learning Parallax Attention for Stereo Image Super-Resolution", "comments": "To appear in CVPR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stereo image pairs can be used to improve the performance of super-resolution\n(SR) since additional information is provided from a second viewpoint. However,\nit is challenging to incorporate this information for SR since disparities\nbetween stereo images vary significantly. In this paper, we propose a\nparallax-attention stereo superresolution network (PASSRnet) to integrate the\ninformation from a stereo image pair for SR. Specifically, we introduce a\nparallax-attention mechanism with a global receptive field along the epipolar\nline to handle different stereo images with large disparity variations. We also\npropose a new and the largest dataset for stereo image SR (namely, Flickr1024).\nExtensive experiments demonstrate that the parallax-attention mechanism can\ncapture correspondence between stereo images to improve SR performance with a\nsmall computational and memory cost. Comparative results show that our PASSRnet\nachieves the state-of-the-art performance on the Middlebury, KITTI 2012 and\nKITTI 2015 datasets.\n", "versions": [{"version": "v1", "created": "Thu, 14 Mar 2019 01:17:27 GMT"}, {"version": "v2", "created": "Fri, 15 Mar 2019 05:36:16 GMT"}, {"version": "v3", "created": "Tue, 19 Mar 2019 12:55:20 GMT"}], "update_date": "2019-03-20", "authors_parsed": [["Wang", "Longguang", ""], ["Wang", "Yingqian", ""], ["Liang", "Zhengfa", ""], ["Lin", "Zaiping", ""], ["Yang", "Jungang", ""], ["An", "Wei", ""], ["Guo", "Yulan", ""]]}, {"id": "1903.05789", "submitter": "Bin Dai", "authors": "Bin Dai and David Wipf", "title": "Diagnosing and Enhancing VAE Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although variational autoencoders (VAEs) represent a widely influential deep\ngenerative model, many aspects of the underlying energy function remain poorly\nunderstood. In particular, it is commonly believed that Gaussian\nencoder/decoder assumptions reduce the effectiveness of VAEs in generating\nrealistic samples. In this regard, we rigorously analyze the VAE objective,\ndifferentiating situations where this belief is and is not actually true. We\nthen leverage the corresponding insights to develop a simple VAE enhancement\nthat requires no additional hyperparameters or sensitive tuning.\nQuantitatively, this proposal produces crisp samples and stable FID scores that\nare actually competitive with a variety of GAN models, all while retaining\ndesirable attributes of the original VAE architecture. A shorter version of\nthis work will appear in the ICLR 2019 conference proceedings (Dai and Wipf,\n2019). The code for our model is available at https://github.com/daib13/\nTwoStageVAE.\n", "versions": [{"version": "v1", "created": "Thu, 14 Mar 2019 02:11:17 GMT"}, {"version": "v2", "created": "Wed, 30 Oct 2019 10:53:11 GMT"}], "update_date": "2019-10-31", "authors_parsed": [["Dai", "Bin", ""], ["Wipf", "David", ""]]}, {"id": "1903.05807", "submitter": "Xu Cao", "authors": "Xu Cao, Weimin Wang, Katashi Nagao", "title": "Neural Style Transfer for Point Clouds", "comments": "9 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  How can we edit or transform the geometric or color property of a point\ncloud? In this study, we propose a neural style transfer method for point\nclouds which allows us to transfer the style of geometry or color from one\npoint cloud either independently or simultaneously to another. This transfer is\nachieved by manipulating the content representations and Gram-based style\nrepresentations extracted from a pre-trained PointNet-based classification\nnetwork for colored point clouds. As Gram-based style representation is\ninvariant to the number or the order of points, the same method can be extended\nto transfer the style extracted from an image to the color expression of a\npoint cloud by merely treating the image as a set of pixels. Experimental\nresults demonstrate the capability of the proposed method for transferring\nstyle from either an image or a point cloud to another point cloud of a single\nobject or even an indoor scene.\n", "versions": [{"version": "v1", "created": "Thu, 14 Mar 2019 03:56:06 GMT"}], "update_date": "2019-03-15", "authors_parsed": [["Cao", "Xu", ""], ["Wang", "Weimin", ""], ["Nagao", "Katashi", ""]]}, {"id": "1903.05820", "submitter": "Yuxiao Yan", "authors": "Tongtong Zhao, Yuxiao Yan, Ibrahim Shehi Shehu, Xianping Fu, Huibing\n  Wang", "title": "Purifying Naturalistic Images through a Real-time Style Transfer\n  Semantics Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, the progress of learning-by-synthesis has proposed a training model\nfor synthetic images, which can effectively reduce the cost of human and\nmaterial resources. However, due to the different distribution of synthetic\nimages compared to real images, the desired performance cannot still be\nachieved. Real images consist of multiple forms of light orientation, while\nsynthetic images consist of a uniform light orientation. These features are\nconsidered to be characteristic of outdoor and indoor scenes, respectively. To\nsolve this problem, the previous method learned a model to improve the realism\nof the synthetic image. Different from the previous methods, this paper takes\nthe first step to purify real images. Through the style transfer task, the\ndistribution of outdoor real images is converted into indoor synthetic images,\nthereby reducing the influence of light. Therefore, this paper proposes a\nreal-time style transfer network that preserves image content information (eg,\ngaze direction, pupil center position) of an input image (real image) while\ninferring style information (eg, image color structure, semantic features) of\nstyle image (synthetic image). In addition, the network accelerates the\nconvergence speed of the model and adapts to multi-scale images. Experiments\nwere performed using mixed studies (qualitative and quantitative) methods to\ndemonstrate the possibility of purifying real images in complex directions.\nQualitatively, it compares the proposed method with the available methods in a\nseries of indoor and outdoor scenarios of the LPW dataset. In quantitative\nterms, it evaluates the purified image by training a gaze estimation model on\nthe cross data set. The results show a significant improvement over the\nbaseline method compared to the raw real image.\n", "versions": [{"version": "v1", "created": "Thu, 14 Mar 2019 05:33:08 GMT"}], "update_date": "2019-03-15", "authors_parsed": [["Zhao", "Tongtong", ""], ["Yan", "Yuxiao", ""], ["Shehu", "Ibrahim Shehi", ""], ["Fu", "Xianping", ""], ["Wang", "Huibing", ""]]}, {"id": "1903.05831", "submitter": "Naiyan Wang", "authors": "Yuntao Chen, Chenxia Han, Yanghao Li, Zehao Huang, Yi Jiang, Naiyan\n  Wang, Zhaoxiang Zhang", "title": "SimpleDet: A Simple and Versatile Distributed Framework for Object\n  Detection and Instance Recognition", "comments": "Tech Report", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Object detection and instance recognition play a central role in many AI\napplications like autonomous driving, video surveillance and medical image\nanalysis. However, training object detection models on large scale datasets\nremains computationally expensive and time consuming. This paper presents an\nefficient and open source object detection framework called SimpleDet which\nenables the training of state-of-the-art detection models on consumer grade\nhardware at large scale. SimpleDet supports up-to-date detection models with\nbest practice. SimpleDet also supports distributed training with near linear\nscaling out of box. Codes, examples and documents of SimpleDet can be found at\nhttps://github.com/tusimple/simpledet .\n", "versions": [{"version": "v1", "created": "Thu, 14 Mar 2019 06:40:29 GMT"}], "update_date": "2019-03-15", "authors_parsed": [["Chen", "Yuntao", ""], ["Han", "Chenxia", ""], ["Li", "Yanghao", ""], ["Huang", "Zehao", ""], ["Jiang", "Yi", ""], ["Wang", "Naiyan", ""], ["Zhang", "Zhaoxiang", ""]]}, {"id": "1903.05854", "submitter": "Tingting Qiao", "authors": "Tingting Qiao, Jing Zhang, Duanqing Xu and Dacheng Tao", "title": "MirrorGAN: Learning Text-to-image Generation by Redescription", "comments": "Accepted by CVPR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generating an image from a given text description has two goals: visual\nrealism and semantic consistency. Although significant progress has been made\nin generating high-quality and visually realistic images using generative\nadversarial networks, guaranteeing semantic consistency between the text\ndescription and visual content remains very challenging. In this paper, we\naddress this problem by proposing a novel global-local attentive and\nsemantic-preserving text-to-image-to-text framework called MirrorGAN. MirrorGAN\nexploits the idea of learning text-to-image generation by redescription and\nconsists of three modules: a semantic text embedding module (STEM), a\nglobal-local collaborative attentive module for cascaded image generation\n(GLAM), and a semantic text regeneration and alignment module (STREAM). STEM\ngenerates word- and sentence-level embeddings. GLAM has a cascaded architecture\nfor generating target images from coarse to fine scales, leveraging both local\nword attention and global sentence attention to progressively enhance the\ndiversity and semantic consistency of the generated images. STREAM seeks to\nregenerate the text description from the generated image, which semantically\naligns with the given text description. Thorough experiments on two public\nbenchmark datasets demonstrate the superiority of MirrorGAN over other\nrepresentative state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Thu, 14 Mar 2019 08:31:05 GMT"}], "update_date": "2019-03-15", "authors_parsed": [["Qiao", "Tingting", ""], ["Zhang", "Jing", ""], ["Xu", "Duanqing", ""], ["Tao", "Dacheng", ""]]}, {"id": "1903.05862", "submitter": "Yongliang Chen", "authors": "Yongliang Chen", "title": "Learning Orientation-Estimation Convolutional Neural Network for\n  Building Detection in Optical Remote Sensing Image", "comments": "Presented in DICTA2018; Further Improved including extra experiments", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Benefiting from the great success of deep learning in computer vision,\nCNN-based object detection methods have drawn significant attentions. Various\nframeworks have been proposed which show awesome and robust performance for a\nlarge range of datasets. However, for building detection in remote sensing\nimages, buildings always pose a diversity of orientations which makes it a\nchallenge for the application of off-the-shelf methods to building detection.\nIn this work, we aim to integrate orientation regression into the popular\naxis-aligned bounding-box detection method to tackle this problem. To adapt the\naxis-aligned bounding boxes to arbitrarily orientated ones, we also develop an\nalgorithm to estimate the Intersection over Union (IoU) overlap between any two\narbitrarily oriented boxes which is convenient to implement in Graphics\nProcessing Unit (GPU) for accelerating computation. The proposed method\nutilizes CNN for both robust feature extraction and rotated bounding box\nregression. We present our modelin an end-to-end fashion making it easy to\ntrain. The model is formulated and trained to predict orientation, location and\nextent simultaneously obtaining tighter bounding box and hence, higher mean\naverage precision (mAP). Experiments on remote sensing images of different\nscales shows a promising performance over the conventional one.\n", "versions": [{"version": "v1", "created": "Thu, 14 Mar 2019 09:02:59 GMT"}], "update_date": "2019-03-15", "authors_parsed": [["Chen", "Yongliang", ""]]}, {"id": "1903.05885", "submitter": "Thiemo Alldieck", "authors": "Thiemo Alldieck, Marcus Magnor, Bharat Lal Bhatnagar, Christian\n  Theobalt, Gerard Pons-Moll", "title": "Learning to Reconstruct People in Clothing from a Single RGB Camera", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a learning-based model to infer the personalized 3D shape of\npeople from a few frames (1-8) of a monocular video in which the person is\nmoving, in less than 10 seconds with a reconstruction accuracy of 5mm. Our\nmodel learns to predict the parameters of a statistical body model and instance\ndisplacements that add clothing and hair to the shape. The model achieves fast\nand accurate predictions based on two key design choices. First, by predicting\nshape in a canonical T-pose space, the network learns to encode the images of\nthe person into pose-invariant latent codes, where the information is fused.\nSecond, based on the observation that feed-forward predictions are fast but do\nnot always align with the input images, we predict using both, bottom-up and\ntop-down streams (one per view) allowing information to flow in both\ndirections. Learning relies only on synthetic 3D data. Once learned, the model\ncan take a variable number of frames as input, and is able to reconstruct\nshapes even from a single image with an accuracy of 6mm. Results on 3 different\ndatasets demonstrate the efficacy and accuracy of our approach.\n", "versions": [{"version": "v1", "created": "Thu, 14 Mar 2019 09:55:44 GMT"}, {"version": "v2", "created": "Mon, 8 Apr 2019 07:33:53 GMT"}], "update_date": "2019-04-09", "authors_parsed": [["Alldieck", "Thiemo", ""], ["Magnor", "Marcus", ""], ["Bhatnagar", "Bharat Lal", ""], ["Theobalt", "Christian", ""], ["Pons-Moll", "Gerard", ""]]}, {"id": "1903.05889", "submitter": "Jan Razlaw", "authors": "Jan Razlaw, Jan Quenzel and Sven Behnke", "title": "Detection and Tracking of Small Objects in Sparse 3D Laser Range Data", "comments": "Accepted for IEEE International Conference on Robotics and Automation\n  (ICRA), Montreal, Canada, to appear May 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Detection and tracking of dynamic objects is a key feature for autonomous\nbehavior in a continuously changing environment. With the increasing popularity\nand capability of micro aerial vehicles (MAVs) efficient algorithms have to be\nutilized to enable multi object tracking on limited hardware and data provided\nby lightweight sensors. We present a novel segmentation approach based on a\ncombination of median filters and an efficient pipeline for detection and\ntracking of small objects within sparse point clouds generated by a Velodyne\nVLP-16 sensor. We achieve real-time performance on a single core of our MAV\nhardware by exploiting the inherent structure of the data. Our approach is\nevaluated on simulated and real scans of in- and outdoor environments,\nobtaining results comparable to the state of the art. Additionally, we provide\nan application for filtering the dynamic and mapping the static part of the\ndata, generating further insights into the performance of the pipeline on\nunlabeled data.\n", "versions": [{"version": "v1", "created": "Thu, 14 Mar 2019 10:03:44 GMT"}], "update_date": "2019-03-15", "authors_parsed": [["Razlaw", "Jan", ""], ["Quenzel", "Jan", ""], ["Behnke", "Sven", ""]]}, {"id": "1903.05921", "submitter": "Shurun Wang", "authors": "Shurun Wang, Shiqi Wang, Xinfeng Zhang, Shanshe Wang, Siwei Ma, Wen\n  Gao", "title": "Scalable Facial Image Compression with Deep Feature Reconstruction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a scalable image compression scheme, including the\nbase layer for feature representation and enhancement layer for texture\nrepresentation. More specifically, the base layer is designed as the deep\nlearning feature for analysis purpose, and it can also be converted to the fine\nstructure with deep feature reconstruction. The enhancement layer, which serves\nto compress the residuals between the input image and the signals generated\nfrom the base layer, aims to faithfully reconstruct the input texture. The\nproposed scheme can feasibly inherit the advantages of both\ncompress-then-analyze and analyze-then-compress schemes in surveillance\napplications. The performance of this framework is validated with facial\nimages, and the conducted experiments provide useful evidences to show that the\nproposed framework can achieve better rate-accuracy and rate-distortion\nperformance over conventional image compression schemes.\n", "versions": [{"version": "v1", "created": "Thu, 14 Mar 2019 11:28:49 GMT"}, {"version": "v2", "created": "Sun, 25 Aug 2019 11:35:45 GMT"}], "update_date": "2019-08-27", "authors_parsed": [["Wang", "Shurun", ""], ["Wang", "Shiqi", ""], ["Zhang", "Xinfeng", ""], ["Wang", "Shanshe", ""], ["Ma", "Siwei", ""], ["Gao", "Wen", ""]]}, {"id": "1903.05942", "submitter": "Dong-Jin Kim", "authors": "Dong-Jin Kim, Jinsoo Choi, Tae-Hyun Oh, In So Kweon", "title": "Dense Relational Captioning: Triple-Stream Networks for\n  Relationship-Based Captioning", "comments": "CVPR 2019 (Under review for journal extension). Project page :\n  https://sites.google.com/view/relcap", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Our goal in this work is to train an image captioning model that generates\nmore dense and informative captions. We introduce \"relational captioning,\" a\nnovel image captioning task which aims to generate multiple captions with\nrespect to relational information between objects in an image. Relational\ncaptioning is a framework that is advantageous in both diversity and amount of\ninformation, leading to image understanding based on relationships. Part-of\nspeech (POS, i.e. subject-object-predicate categories) tags can be assigned to\nevery English word. We leverage the POS as a prior to guide the correct\nsequence of words in a caption. To this end, we propose a multi-task\ntriple-stream network (MTTSNet) which consists of three recurrent units for the\nrespective POS and jointly performs POS prediction and captioning. We\ndemonstrate more diverse and richer representations generated by the proposed\nmodel against several baselines and competing methods.\n", "versions": [{"version": "v1", "created": "Thu, 14 Mar 2019 12:36:01 GMT"}, {"version": "v2", "created": "Wed, 27 Mar 2019 05:06:16 GMT"}, {"version": "v3", "created": "Mon, 1 Apr 2019 05:07:54 GMT"}, {"version": "v4", "created": "Sun, 22 Sep 2019 07:24:51 GMT"}], "update_date": "2019-09-24", "authors_parsed": [["Kim", "Dong-Jin", ""], ["Choi", "Jinsoo", ""], ["Oh", "Tae-Hyun", ""], ["Kweon", "In So", ""]]}, {"id": "1903.05946", "submitter": "Tim C Kietzmann", "authors": "Tim C Kietzmann, Courtney J Spoerer, Lynn S\\\"orensen, Radoslaw M\n  Cichy, Olaf Hauk, and Nikolaus Kriegeskorte", "title": "Recurrence is required to capture the representational dynamics of the\n  human visual system", "comments": "https://www.pnas.org/content/early/2019/10/04/1905544116.short?rss=1", "journal-ref": "Proceedings of the National Academy of Sciences, p. 1-10 (2019)", "doi": "10.1073/pnas.1905544116", "report-no": null, "categories": "q-bio.NC cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The human visual system is an intricate network of brain regions that enables\nus to recognize the world around us. Despite its abundant lateral and feedback\nconnections, object processing is commonly viewed and studied as a feedforward\nprocess. Here, we measure and model the rapid representational dynamics across\nmultiple stages of the human ventral stream using time-resolved brain imaging\nand deep learning. We observe substantial representational transformations\nduring the first 300 ms of processing within and across ventral-stream regions.\nCategorical divisions emerge in sequence, cascading forward and in reverse\nacross regions, and Granger causality analysis suggests bidirectional\ninformation flow between regions. Finally, recurrent deep neural network models\nclearly outperform parameter-matched feedforward models in terms of their\nability to capture the multi-region cortical dynamics. Targeted virtual cooling\nexperiments on the recurrent deep network models further substantiate the\nimportance of their lateral and top-down connections. These results establish\nthat recurrent models are required to understand information processing in the\nhuman ventral stream.\n", "versions": [{"version": "v1", "created": "Thu, 14 Mar 2019 12:43:38 GMT"}, {"version": "v2", "created": "Tue, 8 Oct 2019 08:45:53 GMT"}], "update_date": "2019-10-09", "authors_parsed": [["Kietzmann", "Tim C", ""], ["Spoerer", "Courtney J", ""], ["S\u00f6rensen", "Lynn", ""], ["Cichy", "Radoslaw M", ""], ["Hauk", "Olaf", ""], ["Kriegeskorte", "Nikolaus", ""]]}, {"id": "1903.05962", "submitter": "Zhao Kang", "authors": "Zhao Kang, Liangjian Wen, Wenyu Chen, Zenglin Xu", "title": "Low-rank Kernel Learning for Graph-based Clustering", "comments": null, "journal-ref": "Knowledge-Based Systems, 2019", "doi": "10.1016/j.knosys.2018.09.009", "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Constructing the adjacency graph is fundamental to graph-based clustering.\nGraph learning in kernel space has shown impressive performance on a number of\nbenchmark data sets. However, its performance is largely determined by the\nchosen kernel matrix. To address this issue, the previous multiple kernel\nlearning algorithm has been applied to learn an optimal kernel from a group of\npredefined kernels. This approach might be sensitive to noise and limits the\nrepresentation ability of the consensus kernel. In contrast to existing\nmethods, we propose to learn a low-rank kernel matrix which exploits the\nsimilarity nature of the kernel matrix and seeks an optimal kernel from the\nneighborhood of candidate kernels. By formulating graph construction and kernel\nlearning in a unified framework, the graph and consensus kernel can be\niteratively enhanced by each other. Extensive experimental results validate the\nefficacy of the proposed method.\n", "versions": [{"version": "v1", "created": "Thu, 14 Mar 2019 12:59:52 GMT"}], "update_date": "2019-03-15", "authors_parsed": [["Kang", "Zhao", ""], ["Wen", "Liangjian", ""], ["Chen", "Wenyu", ""], ["Xu", "Zenglin", ""]]}, {"id": "1903.06010", "submitter": "Remi Giraud", "authors": "R\\'emi Giraud, Vinh-Thong Ta, Nicolas Papadakis", "title": "Superpixel-based Color Transfer", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we propose a fast superpixel-based color transfer method (SCT)\nbetween two images. Superpixels enable to decrease the image dimension and to\nextract a reduced set of color candidates. We propose to use a fast approximate\nnearest neighbor matching algorithm in which we enforce the match diversity by\nlimiting the selection of the same superpixels. A fusion framework is designed\nto transfer the matched colors, and we demonstrate the improvement obtained\nover exact matching results. Finally, we show that SCT is visually competitive\ncompared to state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Thu, 14 Mar 2019 14:07:51 GMT"}], "update_date": "2019-03-15", "authors_parsed": [["Giraud", "R\u00e9mi", ""], ["Ta", "Vinh-Thong", ""], ["Papadakis", "Nicolas", ""]]}, {"id": "1903.06031", "submitter": "Christopher Schymura", "authors": "Christopher Schymura and Dorothea Kolossa", "title": "Audiovisual Speaker Tracking using Nonlinear Dynamical Systems with\n  Dynamic Stream Weights", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data fusion plays an important role in many technical applications that\nrequire efficient processing of multimodal sensory observations. A prominent\nexample is audiovisual signal processing, which has gained increasing attention\nin automatic speech recognition, speaker localization and related tasks. If\nappropriately combined with acoustic information, additional visual cues can\nhelp to improve the performance in these applications, especially under adverse\nacoustic conditions. A dynamic weighting of acoustic and visual streams based\non instantaneous sensor reliability measures is an efficient approach to data\nfusion in this context. This paper presents a framework that extends the\nwell-established theory of nonlinear dynamical systems with the notion of\ndynamic stream weights for an arbitrary number of sensory observations. It\ncomprises a recursive state estimator based on the Gaussian filtering paradigm,\nwhich incorporates dynamic stream weights into a framework closely related to\nthe extended Kalman filter. Additionally, a convex optimization approach to\nestimate oracle dynamic stream weights in fully observed dynamical systems\nutilizing a Dirichlet prior is presented. This serves as a basis for a generic\nparameter learning framework of dynamic stream weight estimators. The proposed\nsystem is application-independent and can be easily adapted to specific tasks\nand requirements. A study using audiovisual speaker tracking tasks is\nconsidered as an exemplary application in this work. An improved tracking\nperformance of the dynamic stream weight-based estimation framework over\nstate-of-the-art methods is demonstrated in the experiments.\n", "versions": [{"version": "v1", "created": "Thu, 14 Mar 2019 14:23:32 GMT"}], "update_date": "2019-03-15", "authors_parsed": [["Schymura", "Christopher", ""], ["Kolossa", "Dorothea", ""]]}, {"id": "1903.06048", "submitter": "Animesh Karnewar", "authors": "Animesh Karnewar, Oliver Wang", "title": "MSG-GAN: Multi-Scale Gradients for Generative Adversarial Networks", "comments": "CVPR 2020 (Main Conference). Work sponsored by TomTom and Adobe. Code\n  repository: https://github.com/akanimax/msg-stylegan-tf", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While Generative Adversarial Networks (GANs) have seen huge successes in\nimage synthesis tasks, they are notoriously difficult to adapt to different\ndatasets, in part due to instability during training and sensitivity to\nhyperparameters. One commonly accepted reason for this instability is that\ngradients passing from the discriminator to the generator become uninformative\nwhen there isn't enough overlap in the supports of the real and fake\ndistributions. In this work, we propose the Multi-Scale Gradient Generative\nAdversarial Network (MSG-GAN), a simple but effective technique for addressing\nthis by allowing the flow of gradients from the discriminator to the generator\nat multiple scales. This technique provides a stable approach for high\nresolution image synthesis, and serves as an alternative to the commonly used\nprogressive growing technique. We show that MSG-GAN converges stably on a\nvariety of image datasets of different sizes, resolutions and domains, as well\nas different types of loss functions and architectures, all with the same set\nof fixed hyperparameters. When compared to state-of-the-art GANs, our approach\nmatches or exceeds the performance in most of the cases we tried.\n", "versions": [{"version": "v1", "created": "Thu, 14 Mar 2019 14:33:26 GMT"}, {"version": "v2", "created": "Fri, 22 Mar 2019 19:48:01 GMT"}, {"version": "v3", "created": "Fri, 29 Nov 2019 10:47:55 GMT"}, {"version": "v4", "created": "Fri, 12 Jun 2020 20:45:26 GMT"}], "update_date": "2020-06-16", "authors_parsed": [["Karnewar", "Animesh", ""], ["Wang", "Oliver", ""]]}, {"id": "1903.06056", "submitter": "Vishal Srivastava Dr", "authors": "Neeru Singla and Vishal Srivastava", "title": "Deep learning enabled multi-wavelength spatial coherence microscope for\n  the classification of malaria-infected stages with limited labelled data size", "comments": null, "journal-ref": null, "doi": "10.1016/j.optlastec.2020.106335", "report-no": null, "categories": "cs.CV physics.med-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Malaria is a life-threatening mosquito-borne blood disease, hence early\ndetection is very crucial for health. The conventional method for the detection\nis a microscopic examination of Giemsa-stained blood smears, which needs a\nhighly trained skilled technician. Automated classifications of different\nstages of malaria still a challenging task, especially having poor sensitivity\nin detecting the early trophozoite and late trophozoite or schizont stage with\nlimited labelled datasize. The study aims to develop a fast, robust and fully\nautomated system for the classification of different stages of malaria with\nlimited data size by using the pre-trained convolutional neural networks (CNNs)\nas a classifier and multi-wavelength to increase the sample size. We also\ncompare our customized CNN with other well-known CNNs and shows that our\nnetwork have a comparable performance with less computational time. We believe\nthat our proposed method can be applied to other limited labelled biological\ndatasets.\n", "versions": [{"version": "v1", "created": "Thu, 14 Mar 2019 14:51:42 GMT"}], "update_date": "2020-06-24", "authors_parsed": [["Singla", "Neeru", ""], ["Srivastava", "Vishal", ""]]}, {"id": "1903.06117", "submitter": "Simone Zini", "authors": "Simone Zini, Simone Bianco, Raimondo Schettini", "title": "Deep Residual Autoencoder for quality independent JPEG restoration", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose a deep residual autoencoder exploiting\nResidual-in-Residual Dense Blocks (RRDB) to remove artifacts in JPEG compressed\nimages that is independent from the Quality Factor (QF) used. The proposed\napproach leverages both the learning capacity of deep residual networks and\nprior knowledge of the JPEG compression pipeline. The proposed model operates\nin the YCbCr color space and performs JPEG artifact restoration in two phases\nusing two different autoencoders: the first one restores the luma channel\nexploiting 2D convolutions; the second one, using the restored luma channel as\na guide, restores the chroma channels explotining 3D convolutions. Extensive\nexperimental results on three widely used benchmark datasets (i.e. LIVE1,\nBDS500, and CLASSIC-5) show that our model is able to outperform the state of\nthe art with respect to all the evaluation metrics considered (i.e. PSNR,\nPSNR-B, and SSIM). This results is remarkable since the approaches in the state\nof the art use a different set of weights for each compression quality, while\nthe proposed model uses the same weights for all of them, making it applicable\nto images in the wild where the QF used for compression is unkwnown.\nFurthermore, the proposed model shows a greater robustness than\nstate-of-the-art methods when applied to compression qualities not seen during\ntraining.\n", "versions": [{"version": "v1", "created": "Thu, 14 Mar 2019 16:51:18 GMT"}], "update_date": "2019-03-15", "authors_parsed": [["Zini", "Simone", ""], ["Bianco", "Simone", ""], ["Schettini", "Raimondo", ""]]}, {"id": "1903.06133", "submitter": "Giuseppe G. Calvi", "authors": "Giuseppe G. Calvi, Ahmad Moniri, Mahmoud Mahfouz, Qibin Zhao, Danilo\n  P. Mandic", "title": "Compression and Interpretability of Deep Neural Networks via Tucker\n  Tensor Layer: From First Principles to Tensor Valued Back-Propagation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work aims to help resolve the two main stumbling blocks in the\napplication of Deep Neural Networks (DNNs), that is, the exceedingly large\nnumber of trainable parameters and their physical interpretability. This is\nachieved through a tensor valued approach, based on the proposed Tucker Tensor\nLayer (TTL), as an alternative to the dense weight-matrices of DNNs. This\nallows us to treat the weight-matrices of general DNNs as a matrix unfolding of\na higher order weight-tensor. By virtue of the compression properties of tensor\ndecompositions, this enables us to introduce a novel and efficient framework\nfor exploiting the multi-way nature of the weight-tensor in order to\ndramatically reduce the number of DNN parameters. We also derive the tensor\nvalued back-propagation algorithm within the TTL framework, by extending the\nnotion of matrix derivatives to tensors. In this way, the physical\ninterpretability of the Tucker decomposition is exploited to gain physical\ninsights into the NN training, through the process of computing gradients with\nrespect to each factor matrix. The proposed framework is validated on both\nsynthetic data, and the benchmark datasets MNIST, Fashion-MNIST, and CIFAR-10.\nOverall, through the ability to provide the relative importance of each data\nfeature in training, the TTL back-propagation is shown to help mitigate the\n\"black-box\" nature inherent to NNs. Experiments also illustrate that the TTL\nachieves a 66.63-fold compression on MNIST and Fashion-MNIST, while, by\nsimplifying the VGG-16 network, it achieves a 10\\% speed up in training time,\nat a comparable performance.\n", "versions": [{"version": "v1", "created": "Thu, 14 Mar 2019 17:19:38 GMT"}, {"version": "v2", "created": "Mon, 6 Jan 2020 10:41:42 GMT"}], "update_date": "2020-01-07", "authors_parsed": [["Calvi", "Giuseppe G.", ""], ["Moniri", "Ahmad", ""], ["Mahfouz", "Mahmoud", ""], ["Zhao", "Qibin", ""], ["Mandic", "Danilo P.", ""]]}, {"id": "1903.06150", "submitter": "Heliang Zheng", "authors": "Heliang Zheng, Jianlong Fu, Zheng-Jun Zha, Jiebo Luo", "title": "Looking for the Devil in the Details: Learning Trilinear Attention\n  Sampling Network for Fine-grained Image Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning subtle yet discriminative features (e.g., beak and eyes for a bird)\nplays a significant role in fine-grained image recognition. Existing\nattention-based approaches localize and amplify significant parts to learn\nfine-grained details, which often suffer from a limited number of parts and\nheavy computational cost. In this paper, we propose to learn such fine-grained\nfeatures from hundreds of part proposals by Trilinear Attention Sampling\nNetwork (TASN) in an efficient teacher-student manner. Specifically, TASN\nconsists of 1) a trilinear attention module, which generates attention maps by\nmodeling the inter-channel relationships, 2) an attention-based sampler which\nhighlights attended parts with high resolution, and 3) a feature distiller,\nwhich distills part features into a global one by weight sharing and feature\npreserving strategies. Extensive experiments verify that TASN yields the best\nperformance under the same settings with the most competitive approaches, in\niNaturalist-2017, CUB-Bird, and Stanford-Cars datasets.\n", "versions": [{"version": "v1", "created": "Thu, 14 Mar 2019 17:52:18 GMT"}, {"version": "v2", "created": "Tue, 11 Jun 2019 08:25:32 GMT"}], "update_date": "2019-06-12", "authors_parsed": [["Zheng", "Heliang", ""], ["Fu", "Jianlong", ""], ["Zha", "Zheng-Jun", ""], ["Luo", "Jiebo", ""]]}, {"id": "1903.06246", "submitter": "Baohua Sun", "authors": "Baohua Sun, Lin Yang, Wenhan Zhang, Michael Lin, Patrick Dong, Charles\n  Young, Jason Dong", "title": "SuperTML: Two-Dimensional Word Embedding for the Precognition on\n  Structured Tabular Data", "comments": "9 pages, 5 figures, 3 tables. Accepted by CVPR2019 Precognition\n  Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tabular data is the most commonly used form of data in industry. Gradient\nBoosting Trees, Support Vector Machine, Random Forest, and Logistic Regression\nare typically used for classification tasks on tabular data. DNN models using\ncategorical embeddings are also applied in this task, but all attempts thus far\nhave used one-dimensional embeddings. The recent work of Super Characters\nmethod using two-dimensional word embeddings achieved the state of art result\nin text classification tasks, showcasing the promise of this new approach. In\nthis paper, we propose the SuperTML method, which borrows the idea of Super\nCharacters method and two-dimensional embeddings to address the problem of\nclassification on tabular data. For each input of tabular data, the features\nare first projected into two-dimensional embeddings like an image, and then\nthis image is fed into fine-tuned two-dimensional CNN models for\nclassification. Experimental results have shown that the proposed SuperTML\nmethod had achieved state-of-the-art results on both large and small datasets.\n", "versions": [{"version": "v1", "created": "Tue, 26 Feb 2019 13:57:28 GMT"}, {"version": "v2", "created": "Fri, 22 Mar 2019 23:18:36 GMT"}, {"version": "v3", "created": "Tue, 4 Jun 2019 02:16:48 GMT"}], "update_date": "2019-06-05", "authors_parsed": [["Sun", "Baohua", ""], ["Yang", "Lin", ""], ["Zhang", "Wenhan", ""], ["Lin", "Michael", ""], ["Dong", "Patrick", ""], ["Young", "Charles", ""], ["Dong", "Jason", ""]]}, {"id": "1903.06249", "submitter": "Saeed Masoudnia", "authors": "Omid Mersa, Farhood Etaati, Saeed Masoudnia and Babak N. Araabi", "title": "Learning Representations from Persian Handwriting for Offline Signature\n  Verification, a Deep Transfer Learning Approach", "comments": null, "journal-ref": "2019 4th International Conference on Pattern Recognition and Image\n  Analysis (IPRIA)", "doi": "10.1109/PRIA.2019.8785979", "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Offline Signature Verification (OSV) is a challenging pattern recognition\ntask, especially when it is expected to generalize well on the skilled\nforgeries that are not available during the training. Its challenges also\ninclude small training sample and large intra-class variations. Considering the\nlimitations, we suggest a novel transfer learning approach from Persian\nhandwriting domain to multi-language OSV domain. We train two Residual CNNs on\nthe source domain separately based on two different tasks of word\nclassification and writer identification. Since identifying a person signature\nresembles identifying ones handwriting, it seems perfectly convenient to use\nhandwriting for the feature learning phase. The learned representation on the\nmore varied and plentiful handwriting dataset can compensate for the lack of\ntraining data in the original task, i.e. OSV, without sacrificing the\ngeneralizability. Our proposed OSV system includes two steps: learning\nrepresentation and verification of the input signature. For the first step, the\nsignature images are fed into the trained Residual CNNs. The output\nrepresentations are then used to train SVMs for the verification. We test our\nOSV system on three different signature datasets, including MCYT (a Spanish\nsignature dataset), UTSig (a Persian one) and GPDS-Synthetic (an artificial\ndataset). On UT-SIG, we achieved 9.80% Equal Error Rate (EER) which showed\nsubstantial improvement over the best EER in the literature, 17.45%. Our\nproposed method surpassed state-of-the-arts by 6% on GPDS-Synthetic, achieving\n6.81%. On MCYT, EER of 3.98% was obtained which is comparable to the best\npreviously reported results.\n", "versions": [{"version": "v1", "created": "Thu, 28 Feb 2019 08:13:55 GMT"}], "update_date": "2019-08-09", "authors_parsed": [["Mersa", "Omid", ""], ["Etaati", "Farhood", ""], ["Masoudnia", "Saeed", ""], ["Araabi", "Babak N.", ""]]}, {"id": "1903.06253", "submitter": "Marijana Kracunov", "authors": "Marijana Kracunov, Milica Bastica, Jovana Tesovic", "title": "Object tracking in video signals using Compressive Sensing", "comments": "Student paper submitted to \"The 8th Mediterranean Conference on\n  Embedded Computing\" - MECO'2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reducing the number of pixels in video signals while maintaining quality\nneeded for recovering the trace of an object using Compressive Sensing is main\nsubject of this work. Quality of frames, from video that contains moving\nobject, are gradually reduced by keeping different number of pixels in each\niteration, going from 45% all the way to 1%. Using algorithm for tracing\nobject, results were satisfactory and showed mere changes in trajectory graphs,\nobtained from original and reconstructed videos.\n", "versions": [{"version": "v1", "created": "Fri, 8 Feb 2019 17:41:02 GMT"}], "update_date": "2019-03-18", "authors_parsed": [["Kracunov", "Marijana", ""], ["Bastica", "Milica", ""], ["Tesovic", "Jovana", ""]]}, {"id": "1903.06254", "submitter": "Thomas Robins", "authors": "Thomas Robins, Antonio Stanziola, Kai Reimer, Peter Weinberg,\n  Meng-Xing Tang", "title": "Demonstration of Vector Flow Imaging using Convolutional Neural Networks", "comments": "2018 IEEE International Ultrasonics Symposium, Convolutional Neural\n  Network, DeepLearning, Echo-PIV, UltrasoundToolbox, FieldII", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Synthetic Aperture Vector Flow Imaging (SA-VFI) can visualize complex cardiac\nand vascular blood flow patterns at high temporal resolution with a large field\nof view. Convolutional neural networks (CNNs) are commonly used in image and\nvideo recognition and classification. However, most recently presented CNNs\nalso allow for making per-pixel predictions as needed in optical flow\nvelocimetry. To our knowledge we demonstrate here for the first time a CNN\narchitecture to produce 2D full flow field predictions from high frame rate SA\nultrasound images using supervised learning. The CNN was initially trained\nusing CFD-generated and augmented noiseless SA ultrasound data of a realistic\nvessel geometry. Subsequently, a mix of noisy simulated and real \\textit{in\nvivo} acquisitions were added to increase the network's robustness. The\nresulting flow field of the CNN resembled the ground truth accurately with an\nendpoint-error percentage between 6.5\\% to 14.5\\%. Furthermore, when confronted\nwith an unknown geometry of an arterial bifurcation, the CNN was able to\npredict an accurate flow field indicating its ability for generalization.\nRemarkably, the CNN also performed well for rotational flows, which usually\nrequires advanced, computationally intensive VFI methods. We have demonstrated\nthat convolutional neural networks can be used to estimate complex\nmultidirectional flow.\n", "versions": [{"version": "v1", "created": "Mon, 11 Mar 2019 13:01:38 GMT"}], "update_date": "2019-03-18", "authors_parsed": [["Robins", "Thomas", ""], ["Stanziola", "Antonio", ""], ["Reimer", "Kai", ""], ["Weinberg", "Peter", ""], ["Tang", "Meng-Xing", ""]]}, {"id": "1903.06255", "submitter": "Saeed Masoudnia", "authors": "Taraneh Younesian, Saeed Masoudnia, Reshad Hosseini, Babak N. Araabi", "title": "Active Transfer Learning for Persian Offline Signature Verification", "comments": null, "journal-ref": "2019 4th International Conference on Pattern Recognition and Image\n  Analysis (IPRIA)", "doi": "10.1109/PRIA.2019.8786013", "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Offline Signature Verification (OSV) remains a challenging pattern\nrecognition task, especially in the presence of skilled forgeries that are not\navailable during the training. This challenge is aggravated when there are\nsmall labeled training data available but with large intra-personal variations.\nIn this study, we address this issue by employing an active learning approach,\nwhich selects the most informative instances to label and therefore reduces the\nhuman labeling effort significantly. Our proposed OSV includes three steps:\nfeature learning, active learning, and final verification. We benefit from\ntransfer learning using a pre-trained CNN for feature learning. We also propose\nSVM-based active learning for each user to separate his genuine signatures from\nthe random forgeries. We finally used the SVMs to verify the authenticity of\nthe questioned signature. We examined our proposed active transfer learning\nmethod on UTSig: A Persian offline signature dataset. We achieved near 13%\nimprovement compared to the random selection of instances. Our results also\nshowed 1% improvement over the state-of-the-art method in which a fully\nsupervised setting with five more labeled instances per user was used.\n", "versions": [{"version": "v1", "created": "Thu, 28 Feb 2019 13:49:46 GMT"}], "update_date": "2019-08-09", "authors_parsed": [["Younesian", "Taraneh", ""], ["Masoudnia", "Saeed", ""], ["Hosseini", "Reshad", ""], ["Araabi", "Babak N.", ""]]}, {"id": "1903.06256", "submitter": "Haohan Wang", "authors": "Haohan Wang, Zexue He, Zachary C. Lipton, Eric P. Xing", "title": "Learning Robust Representations by Projecting Superficial Statistics Out", "comments": "To appear at ICLR 2019. Implementation:\n  https://github.com/HaohanWang/HEX", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite impressive performance as evaluated on i.i.d. holdout data, deep\nneural networks depend heavily on superficial statistics of the training data\nand are liable to break under distribution shift. For example, subtle changes\nto the background or texture of an image can break a seemingly powerful\nclassifier. Building on previous work on domain generalization, we hope to\nproduce a classifier that will generalize to previously unseen domains, even\nwhen domain identifiers are not available during training. This setting is\nchallenging because the model may extract many distribution-specific\n(superficial) signals together with distribution-agnostic (semantic) signals.\nTo overcome this challenge, we incorporate the gray-level co-occurrence matrix\n(GLCM) to extract patterns that our prior knowledge suggests are superficial:\nthey are sensitive to the texture but unable to capture the gestalt of an\nimage. Then we introduce two techniques for improving our networks'\nout-of-sample performance. The first method is built on the reverse gradient\nmethod that pushes our model to learn representations from which the GLCM\nrepresentation is not predictable. The second method is built on the\nindependence introduced by projecting the model's representation onto the\nsubspace orthogonal to GLCM representation's. We test our method on the battery\nof standard domain generalization data sets and, interestingly, achieve\ncomparable or better performance as compared to other domain generalization\nmethods that explicitly require samples from the target distribution for\ntraining.\n", "versions": [{"version": "v1", "created": "Sat, 2 Mar 2019 00:42:03 GMT"}], "update_date": "2019-03-18", "authors_parsed": [["Wang", "Haohan", ""], ["He", "Zexue", ""], ["Lipton", "Zachary C.", ""], ["Xing", "Eric P.", ""]]}, {"id": "1903.06257", "submitter": "Hyoung Suk Park Ph.D.", "authors": "Hyoung Suk Park, Jineon Baek, Sun Kyoung You, Jae Kyu Choi, and Jin\n  Keun Seo", "title": "Unpaired image denoising using a generative adversarial network in X-ray\n  CT", "comments": null, "journal-ref": "IEEE Access, 2019", "doi": "10.1109/ACCESS.2019.2934178", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a deep learning-based denoising method for noisy low-dose\ncomputerized tomography (CT) images in the absence of paired training data. The\nproposed method uses a fidelity-embedded generative adversarial network (GAN)\nto learn a denoising function from unpaired training data of low-dose CT (LDCT)\nand standard-dose CT (SDCT) images, where the denoising function is the optimal\ngenerator in the GAN framework. This paper analyzes the f-GAN objective to\nderive a suitable generator that is optimized by minimizing a weighted sum of\ntwo losses: the Kullback-Leibler divergence between an SDCT data distribution\nand a generated distribution, and the $\\ell_2$ loss between the LDCT image and\nthe corresponding generated images (or denoised image). The computed generator\nreflects the prior belief about SDCT data distribution through training. We\nobserved that the proposed method allows the preservation of fine anomalous\nfeatures while eliminating noise. The experimental results show that the\nproposed deep-learning method with unpaired datasets performs comparably to a\nmethod using paired datasets. A clinical experiment was also performed to show\nthe validity of the proposed method for noise arising in the low-dose X-ray CT.\n", "versions": [{"version": "v1", "created": "Mon, 4 Mar 2019 06:01:42 GMT"}, {"version": "v2", "created": "Thu, 8 Aug 2019 08:33:53 GMT"}], "update_date": "2019-08-13", "authors_parsed": [["Park", "Hyoung Suk", ""], ["Baek", "Jineon", ""], ["You", "Sun Kyoung", ""], ["Choi", "Jae Kyu", ""], ["Seo", "Jin Keun", ""]]}, {"id": "1903.06258", "submitter": "Alan JiaXiang Guo", "authors": "Yi Liang, Xin Zhao, Alan J.X. Guo, and Fei Zhu", "title": "Hyperspectral Image Classification with Deep Metric Learning and\n  Conditional Random Field", "comments": null, "journal-ref": null, "doi": "10.1109/LGRS.2019.2939356", "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To improve the classification performance in the context of hyperspectral\nimage processing, many works have been developed based on two common\nstrategies, namely the spatial-spectral information integration and the\nutilization of neural networks. However, both strategies typically require more\ntraining data than the classical algorithms, aggregating the shortage of\nlabeled samples. In this letter, we propose a novel framework that organically\ncombines the spectrum-based deep metric learning model and the conditional\nrandom field algorithm. The deep metric learning model is supervised by the\ncenter loss to produce spectrum-based features that gather more tightly in\nEuclidean space within classes. The conditional random field with Gaussian edge\npotentials, which is firstly proposed for image segmentation tasks, is\nintroduced to give the pixel-wise classification over the hyperspectral image\nby utilizing both the geographical distances between pixels and the Euclidean\ndistances between the features produced by the deep metric learning model. The\nproposed framework is trained by spectral pixels at the deep metric learning\nstage and utilizes the half handcrafted spatial features at the conditional\nrandom field stage. This settlement alleviates the shortage of training data to\nsome extent. Experiments on two real hyperspectral images demonstrate the\nadvantages of the proposed method in terms of both classification accuracy and\ncomputation cost.\n", "versions": [{"version": "v1", "created": "Mon, 4 Mar 2019 09:26:03 GMT"}, {"version": "v2", "created": "Tue, 16 Jul 2019 02:30:41 GMT"}], "update_date": "2020-06-24", "authors_parsed": [["Liang", "Yi", ""], ["Zhao", "Xin", ""], ["Guo", "Alan J. X.", ""], ["Zhu", "Fei", ""]]}, {"id": "1903.06259", "submitter": "Adeel Mufti", "authors": "Adeel Mufti, Biagio Antonelli, Julius Monello", "title": "Conditional GANs For Painting Generation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  We examined the use of modern Generative Adversarial Nets to generate novel\nimages of oil paintings using the Painter By Numbers dataset. We implemented\nSpectral Normalization GAN (SN-GAN) and Spectral Normalization GAN with\nGradient Penalty, and compared their outputs to a Deep Convolutional GAN.\nVisually, and quantitatively according to the Sliced Wasserstein Distance\nmetric, we determined that the SN-GAN produced paintings that were most\ncomparable to our training dataset. We then performed a series of experiments\nto add supervised conditioning to SN-GAN, the culmination of which is what we\nbelieve to be a novel architecture that can generate face paintings with\nuser-specified characteristics.\n", "versions": [{"version": "v1", "created": "Wed, 6 Mar 2019 19:47:56 GMT"}], "update_date": "2019-03-18", "authors_parsed": [["Mufti", "Adeel", ""], ["Antonelli", "Biagio", ""], ["Monello", "Julius", ""]]}, {"id": "1903.06260", "submitter": "Riddhish Bhalodia", "authors": "Tim Sodergren and Riddhish Bhalodia and Ross Whitaker and Joshua Cates\n  and Nassir Marrouche and Shireen Elhabian", "title": "Mixture Modeling of Global Shape Priors and Autoencoding Local Intensity\n  Priors for Left Atrium Segmentation", "comments": "Statistical Atlases and Computational Models of the Heart. Atrial\n  Segmentation and LV Quantification Challenges 2019", "journal-ref": "Statistical Atlases and Computational Models of the Heart. Atrial\n  Segmentation and LV Quantification Challenges, 2019, Springer International\n  Publishing, Cham 357--367,", "doi": null, "report-no": null, "categories": "cs.CV cs.CG cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Difficult image segmentation problems, for instance left atrium MRI, can be\naddressed by incorporating shape priors to find solutions that are consistent\nwith known objects. Nonetheless, a single multivariate Gaussian is not an\nadequate model in cases with significant nonlinear shape variation or where the\nprior distribution is multimodal. Nonparametric density estimation is more\ngeneral, but has a ravenous appetite for training samples and poses serious\nchallenges in optimization, especially in high dimensional spaces. Here, we\npropose a maximum-a-posteriori formulation that relies on a generative image\nmodel by incorporating both local intensity and global shape priors. We use\ndeep autoencoders to capture the complex intensity distribution while avoiding\nthe careful selection of hand-crafted features. We formulate the shape prior as\na mixture of Gaussians and learn the corresponding parameters in a\nhigh-dimensional shape space rather than pre-projecting onto a low-dimensional\nsubspace. In segmentation, we treat the identity of the mixture component as a\nlatent variable and marginalize it within a generalized\nexpectation-maximization framework. We present a conditional maximization-based\nscheme that alternates between a closed-form solution for component-specific\nshape parameters that provides a global update-based optimization strategy, and\nan intensity-based energy minimization that translates the global notion of a\nnonlinear shape prior into a set of local penalties. We demonstrate our\napproach on the left atrial segmentation from gadolinium-enhanced MRI, which is\nuseful in quantifying the atrial geometry in patients with atrial fibrillation.\n", "versions": [{"version": "v1", "created": "Wed, 6 Mar 2019 23:24:08 GMT"}], "update_date": "2019-03-18", "authors_parsed": [["Sodergren", "Tim", ""], ["Bhalodia", "Riddhish", ""], ["Whitaker", "Ross", ""], ["Cates", "Joshua", ""], ["Marrouche", "Nassir", ""], ["Elhabian", "Shireen", ""]]}, {"id": "1903.06261", "submitter": "Kunfang Zhang", "authors": "Mingming Lu, Kunfang Zhang, Haiying Liu and Naixue Xiong", "title": "Graph Hierarchical Convolutional Recurrent Neural Network (GHCRNN) for\n  Vehicle Condition Prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The prediction of urban vehicle flow and speed can greatly facilitate\npeople's travel, and also can provide reasonable advice for the decision-making\nof relevant government departments. However, due to the spatial, temporal and\nhierarchy of vehicle flow and many influencing factors such as weather, it is\ndifficult to prediction. Most of the existing research methods are to extract\nspatial structure information on the road network and extract time series\ninformation from the historical data. However, when extracting spatial\nfeatures, these methods have higher time and space complexity, and incorporate\na lot of noise. It is difficult to apply on large graphs, and only considers\nthe influence of surrounding connected road nodes on the central node, ignoring\na very important hierarchical relationship, namely, similar information of\nsimilar node features and road network structures. In response to these\nproblems, this paper proposes the Graph Hierarchical Convolutional Recurrent\nNeural Network (GHCRNN) model. The model uses GCN (Graph Convolutional\nNetworks) to extract spatial feature, GRU (Gated Recurrent Units) to extract\ntemporal feature, and uses the learnable Pooling to extract hierarchical\ninformation, eliminate redundant information and reduce complexity. Applying\nthis model to the vehicle flow and speed data of Shenzhen and Los Angeles has\nbeen well verified, and the time and memory consumption are effectively reduced\nunder the compared precision.\n", "versions": [{"version": "v1", "created": "Tue, 12 Mar 2019 01:24:47 GMT"}], "update_date": "2019-03-18", "authors_parsed": [["Lu", "Mingming", ""], ["Zhang", "Kunfang", ""], ["Liu", "Haiying", ""], ["Xiong", "Naixue", ""]]}, {"id": "1903.06262", "submitter": "Gladys Hilasaca", "authors": "Gladys M. Hilasaca, Wilson E. Marc\\'ilio-Jr, Danilo M. Eler, Rafael M.\n  Martins, and Fernando V. Paulovich", "title": "Overlap Removal of Dimensionality Reduction Scatterplot Layouts", "comments": "11 pages and 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dimensionality Reduction (DR) scatterplot layouts have become a ubiquitous\nvisualization tool for analyzing multidimensional data items with presence in\ndifferent areas. Despite its popularity, scatterplots suffer from occlusion,\nespecially when markers convey information, making it troublesome for users to\nestimate items' groups' sizes and, more importantly, potentially obfuscating\ncritical items for the analysis under execution. Different strategies have been\ndevised to address this issue, either producing overlap-free layouts, lacking\nthe powerful capabilities of contemporary DR techniques in uncover interesting\ndata patterns, or eliminating overlaps as a post-processing strategy. Despite\nthe good results of post-processing techniques, the best methods typically\nexpand or distort the scatterplot area, thus reducing markers' size (sometimes)\nto unreadable dimensions, defeating the purpose of removing overlaps. This\npaper presents a novel post-processing strategy to remove DR layouts' overlaps\nthat faithfully preserves the original layout's characteristics and markers'\nsizes. We show that the proposed strategy surpasses the state-of-the-art in\noverlap removal through an extensive comparative evaluation considering\nmultiple different metrics while it is 2 or 3 orders of magnitude faster for\nlarge datasets.\n", "versions": [{"version": "v1", "created": "Fri, 8 Mar 2019 18:20:36 GMT"}, {"version": "v2", "created": "Wed, 23 Dec 2020 03:41:36 GMT"}, {"version": "v3", "created": "Thu, 1 Apr 2021 13:08:56 GMT"}, {"version": "v4", "created": "Mon, 12 Apr 2021 03:27:50 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Hilasaca", "Gladys M.", ""], ["Marc\u00edlio-Jr", "Wilson E.", ""], ["Eler", "Danilo M.", ""], ["Martins", "Rafael M.", ""], ["Paulovich", "Fernando V.", ""]]}, {"id": "1903.06275", "submitter": "Dheeraj Peri", "authors": "Dheeraj Peri, Shagan Sah and Raymond Ptucha", "title": "Show, Translate and Tell", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Humans have an incredible ability to process and understand information from\nmultiple sources such as images, video, text, and speech. Recent success of\ndeep neural networks has enabled us to develop algorithms which give machines\nthe ability to understand and interpret this information. There is a need to\nboth broaden their applicability and develop methods which correlate visual\ninformation along with semantic content. We propose a unified model which\njointly trains on images and captions, and learns to generate new captions\ngiven either an image or a caption query. We evaluate our model on three\ndifferent tasks namely cross-modal retrieval, image captioning, and sentence\nparaphrasing. Our model gains insight into cross-modal vector embeddings,\ngeneralizes well on multiple tasks and is competitive to state of the art\nmethods on retrieval.\n", "versions": [{"version": "v1", "created": "Thu, 14 Mar 2019 21:50:09 GMT"}], "update_date": "2019-03-18", "authors_parsed": [["Peri", "Dheeraj", ""], ["Sah", "Shagan", ""], ["Ptucha", "Raymond", ""]]}, {"id": "1903.06315", "submitter": "Yang Li", "authors": "Yang Li, Yoshitaka Ushiku and Tatsuya Harada", "title": "Pose Graph Optimization for Unsupervised Monocular Visual Odometry", "comments": "Accepted to ICRA'2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unsupervised Learning based monocular visual odometry (VO) has lately drawn\nsignificant attention for its potential in label-free leaning ability and\nrobustness to camera parameters and environmental variations. However,\npartially due to the lack of drift correction technique, these methods are\nstill by far less accurate than geometric approaches for large-scale odometry\nestimation. In this paper, we propose to leverage graph optimization and loop\nclosure detection to overcome limitations of unsupervised learning based\nmonocular visual odometry. To this end, we propose a hybrid VO system which\ncombines an unsupervised monocular VO called NeuralBundler with a pose graph\noptimization back-end. NeuralBundler is a neural network architecture that uses\ntemporal and spatial photometric loss as main supervision and generates a\nwindowed pose graph consists of multi-view 6DoF constraints. We propose a novel\npose cycle consistency loss to relieve the tensions in the windowed pose graph,\nleading to improved performance and robustness. In the back-end, a global pose\ngraph is built from local and loop 6DoF constraints estimated by NeuralBundler\nand is optimized over SE(3). Empirical evaluation on the KITTI odometry dataset\ndemonstrates that 1) NeuralBundler achieves state-of-the-art performance on\nunsupervised monocular VO estimation, and 2) our whole approach can achieve\nefficient loop closing and show favorable overall translational accuracy\ncompared to established monocular SLAM systems.\n", "versions": [{"version": "v1", "created": "Fri, 15 Mar 2019 01:30:34 GMT"}], "update_date": "2019-03-18", "authors_parsed": [["Li", "Yang", ""], ["Ushiku", "Yoshitaka", ""], ["Harada", "Tatsuya", ""]]}, {"id": "1903.06319", "submitter": "Yanmei Dong", "authors": "Yanmei Dong, Mingtao Pei, Lijia Zhang, Bin Xu, Yuwei Wu, and Yunde Jia", "title": "Stitching Videos from a Fisheye Lens Camera and a Wide-Angle Lens Camera\n  for Telepresence Robots", "comments": "13 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many telepresence robots are equipped with a forward-facing camera for video\ncommunication and a downward-facing camera for navigation. In this paper, we\npropose to stitch videos from the FF-camera with a wide-angle lens and the\nDF-camera with a fisheye lens for telepresence robots. We aim at providing more\ncompact and efficient visual feedback for the user interface of telepresence\nrobots with user-friendly interactive experiences. To this end, we present a\nmulti-homography-based video stitching method which stitches videos from a\nwide-angle camera and a fisheye camera. The method consists of video image\nalignment, seam cutting, and image blending. We directly align the wide-angle\nvideo image and the fisheye video image based on the multi-homography alignment\nwithout calibration, distortion correction, and unwarping procedures. Thus, we\ncan obtain a stitched video with shape preservation in the non-overlapping\nregions and alignment in the overlapping area for telepresence. To alleviate\nghosting effects caused by moving objects and/or moving cameras during\ntelepresence robot driving, an optimal seam is found for aligned video\ncomposition, and the optimal seam will be updated in subsequent frames,\nconsidering spatial and temporal coherence. The final stitched video is created\nby image blending based on the optimal seam. We conducted a user study to\ndemonstrate the effectiveness of our method and the superiority of telepresence\nrobots with a stitched video as visual feedback.\n", "versions": [{"version": "v1", "created": "Fri, 15 Mar 2019 01:51:05 GMT"}], "update_date": "2019-03-18", "authors_parsed": [["Dong", "Yanmei", ""], ["Pei", "Mingtao", ""], ["Zhang", "Lijia", ""], ["Xu", "Bin", ""], ["Wu", "Yuwei", ""], ["Jia", "Yunde", ""]]}, {"id": "1903.06323", "submitter": "Tao Yu", "authors": "Tao Yu, Zerong Zheng, Yuan Zhong, Jianhui Zhao, Qionghai Dai, Gerard\n  Pons-Moll, Yebin Liu", "title": "SimulCap : Single-View Human Performance Capture with Cloth Simulation", "comments": "CVPR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a new method for live free-viewpoint human performance\ncapture with dynamic details (e.g., cloth wrinkles) using a single RGBD camera.\nOur main contributions are: (i) a multi-layer representation of garments and\nbody, and (ii) a physics-based performance capture procedure. We first digitize\nthe performer using multi-layer surface representation, which includes the\nundressed body surface and separate clothing meshes. For performance capture,\nwe perform skeleton tracking, cloth simulation, and iterative depth fitting\nsequentially for the incoming frame. By incorporating cloth simulation into the\nperformance capture pipeline, we can simulate plausible cloth dynamics and\ncloth-body interactions even in the occluded regions, which was not possible in\nprevious capture methods. Moreover, by formulating depth fitting as a physical\nprocess, our system produces cloth tracking results consistent with the depth\nobservation while still maintaining physical constraints. Results and\nevaluations show the effectiveness of our method. Our method also enables new\ntypes of applications such as cloth retargeting, free-viewpoint video rendering\nand animations.\n", "versions": [{"version": "v1", "created": "Fri, 15 Mar 2019 02:04:37 GMT"}, {"version": "v2", "created": "Mon, 18 Mar 2019 14:09:34 GMT"}], "update_date": "2019-03-19", "authors_parsed": [["Yu", "Tao", ""], ["Zheng", "Zerong", ""], ["Zhong", "Yuan", ""], ["Zhao", "Jianhui", ""], ["Dai", "Qionghai", ""], ["Pons-Moll", "Gerard", ""], ["Liu", "Yebin", ""]]}, {"id": "1903.06325", "submitter": "Hong-Xing Yu", "authors": "Hong-Xing Yu, Wei-Shi Zheng, Ancong Wu, Xiaowei Guo, Shaogang Gong,\n  Jian-Huang Lai", "title": "Unsupervised Person Re-identification by Soft Multilabel Learning", "comments": "CVPR19, oral", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although unsupervised person re-identification (RE-ID) has drawn increasing\nresearch attentions due to its potential to address the scalability problem of\nsupervised RE-ID models, it is very challenging to learn discriminative\ninformation in the absence of pairwise labels across disjoint camera views. To\novercome this problem, we propose a deep model for the soft multilabel learning\nfor unsupervised RE-ID. The idea is to learn a soft multilabel (real-valued\nlabel likelihood vector) for each unlabeled person by comparing (and\nrepresenting) the unlabeled person with a set of known reference persons from\nan auxiliary domain. We propose the soft multilabel-guided hard negative mining\nto learn a discriminative embedding for the unlabeled target domain by\nexploring the similarity consistency of the visual features and the soft\nmultilabels of unlabeled target pairs. Since most target pairs are cross-view\npairs, we develop the cross-view consistent soft multilabel learning to achieve\nthe learning goal that the soft multilabels are consistently good across\ndifferent camera views. To enable effecient soft multilabel learning, we\nintroduce the reference agent learning to represent each reference person by a\nreference agent in a joint embedding. We evaluate our unified deep model on\nMarket-1501 and DukeMTMC-reID. Our model outperforms the state-of-the-art\nunsupervised RE-ID methods by clear margins. Code is available at\nhttps://github.com/KovenYu/MAR.\n", "versions": [{"version": "v1", "created": "Fri, 15 Mar 2019 02:10:57 GMT"}, {"version": "v2", "created": "Mon, 8 Apr 2019 03:45:20 GMT"}], "update_date": "2019-04-09", "authors_parsed": [["Yu", "Hong-Xing", ""], ["Zheng", "Wei-Shi", ""], ["Wu", "Ancong", ""], ["Guo", "Xiaowei", ""], ["Gong", "Shaogang", ""], ["Lai", "Jian-Huang", ""]]}, {"id": "1903.06332", "submitter": "Yingqian Wang", "authors": "Yingqian Wang, Longguang Wang, Jungang Yang, Wei An, Yulan Guo", "title": "Flickr1024: A Large-Scale Dataset for Stereo Image Super-Resolution", "comments": "ICCV Workshop 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the popularity of dual cameras in recently released smart phones, a\ngrowing number of super-resolution (SR) methods have been proposed to enhance\nthe resolution of stereo image pairs. However, the lack of high-quality stereo\ndatasets has limited the research in this area. To facilitate the training and\nevaluation of novel stereo SR algorithms, in this paper, we present a\nlarge-scale stereo dataset named Flickr1024, which contains 1024 pairs of\nhigh-quality images and covers diverse scenarios. We first introduce the data\nacquisition and processing pipeline, and then compare several popular stereo\ndatasets. Finally, we conduct crossdataset experiments to investigate the\npotential benefits introduced by our dataset. Experimental results show that,\nas compared to the KITTI and Middlebury datasets, our Flickr1024 dataset can\nhelp to handle the over-fitting problem and significantly improves the\nperformance of stereo SR methods. The Flickr1024 dataset is available online\nat: https://yingqianwang.github.io/Flickr1024.\n", "versions": [{"version": "v1", "created": "Fri, 15 Mar 2019 02:43:44 GMT"}, {"version": "v2", "created": "Thu, 22 Aug 2019 14:54:34 GMT"}], "update_date": "2019-08-23", "authors_parsed": [["Wang", "Yingqian", ""], ["Wang", "Longguang", ""], ["Yang", "Jungang", ""], ["An", "Wei", ""], ["Guo", "Yulan", ""]]}, {"id": "1903.06342", "submitter": "Euijoon Ahn", "authors": "Euijoon Ahn, Ashnil Kumar, Dagan Feng, Michael Fulham, Jinman Kim", "title": "Unsupervised Deep Transfer Feature Learning for Medical Image\n  Classification", "comments": "4 pages, 1 figure, 3 tables, Accepted (Oral) as IEEE International\n  Symposium on Biomedical Imaging 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The accuracy and robustness of image classification with supervised deep\nlearning are dependent on the availability of large-scale, annotated training\ndata. However, there is a paucity of annotated data available due to the\ncomplexity of manual annotation. To overcome this problem, a popular approach\nis to use transferable knowledge across different domains by: 1) using a\ngeneric feature extractor that has been pre-trained on large-scale general\nimages (i.e., transfer-learned) but which not suited to capture characteristics\nfrom medical images; or 2) fine-tuning generic knowledge with a relatively\nsmaller number of annotated images. Our aim is to reduce the reliance on\nannotated training data by using a new hierarchical unsupervised feature\nextractor with a convolutional auto-encoder placed atop of a pre-trained\nconvolutional neural network. Our approach constrains the rich and generic\nimage features from the pre-trained domain to a sophisticated representation of\nthe local image characteristics from the unannotated medical image domain. Our\napproach has a higher classification accuracy than transfer-learned approaches\nand is competitive with state-of-the-art supervised fine-tuned methods.\n", "versions": [{"version": "v1", "created": "Fri, 15 Mar 2019 03:21:14 GMT"}, {"version": "v2", "created": "Tue, 26 Mar 2019 03:57:38 GMT"}], "update_date": "2019-03-27", "authors_parsed": [["Ahn", "Euijoon", ""], ["Kumar", "Ashnil", ""], ["Feng", "Dagan", ""], ["Fulham", "Michael", ""], ["Kim", "Jinman", ""]]}, {"id": "1903.06355", "submitter": "Wentao Liu", "authors": "Wei Feng, Wentao Liu, Tong Li, Jing Peng, Chen Qian, Xiaolin Hu", "title": "Turbo Learning Framework for Human-Object Interactions Recognition and\n  Human Pose Estimation", "comments": "AAAI2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human-object interactions (HOI) recognition and pose estimation are two\nclosely related tasks. Human pose is an essential cue for recognizing actions\nand localizing the interacted objects. Meanwhile, human action and their\ninteracted objects' localizations provide guidance for pose estimation. In this\npaper, we propose a turbo learning framework to perform HOI recognition and\npose estimation simultaneously. First, two modules are designed to enforce\nmessage passing between the tasks, i.e. pose aware HOI recognition module and\nHOI guided pose estimation module. Then, these two modules form a closed loop\nto utilize the complementary information iteratively, which can be trained in\nan end-to-end manner. The proposed method achieves the state-of-the-art\nperformance on two public benchmarks including Verbs in COCO (V-COCO) and\nHICO-DET datasets.\n", "versions": [{"version": "v1", "created": "Fri, 15 Mar 2019 04:32:35 GMT"}], "update_date": "2019-03-18", "authors_parsed": [["Feng", "Wei", ""], ["Liu", "Wentao", ""], ["Li", "Tong", ""], ["Peng", "Jing", ""], ["Qian", "Chen", ""], ["Hu", "Xiaolin", ""]]}, {"id": "1903.06391", "submitter": "Feras Dayoub", "authors": "Quazi Marufur Rahman and Niko S\\\"underhauf and Feras Dayoub", "title": "Did You Miss the Sign? A False Negative Alarm System for Traffic Sign\n  Detectors", "comments": "Submitted to the 2019 IEEE/RSJ International Conference on\n  Intelligent Robots and Systems (IROS 2019)", "journal-ref": null, "doi": "10.1109/IROS40897.2019.8968525", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Object detection is an integral part of an autonomous vehicle for its\nsafety-critical and navigational purposes. Traffic signs as objects play a\nvital role in guiding such systems. However, if the vehicle fails to locate any\ncritical sign, it might make a catastrophic failure. In this paper, we propose\nan approach to identify traffic signs that have been mistakenly discarded by\nthe object detector. The proposed method raises an alarm when it discovers a\nfailure by the object detector to detect a traffic sign. This approach can be\nuseful to evaluate the performance of the detector during the deployment phase.\nWe trained a single shot multi-box object detector to detect traffic signs and\nused its internal features to train a separate false negative detector (FND).\nDuring deployment, FND decides whether the traffic sign detector (TSD) has\nmissed a sign or not. We are using precision and recall to measure the accuracy\nof FND in two different datasets. For 80% recall, FND has achieved 89.9%\nprecision in Belgium Traffic Sign Detection dataset and 90.8% precision in\nGerman Traffic Sign Recognition Benchmark dataset respectively. To the best of\nour knowledge, our method is the first to tackle this critical aspect of false\nnegative detection in robotic vision. Such a fail-safe mechanism for object\ndetection can improve the engagement of robotic vision systems in our daily\nlife.\n", "versions": [{"version": "v1", "created": "Fri, 15 Mar 2019 07:34:32 GMT"}], "update_date": "2020-11-13", "authors_parsed": [["Rahman", "Quazi Marufur", ""], ["S\u00fcnderhauf", "Niko", ""], ["Dayoub", "Feras", ""]]}, {"id": "1903.06397", "submitter": "Yilun Zhang", "authors": "Yilun Zhang, Ty Nguyen, Ian D. Miller, Shreyas S. Shivakumar, Steven\n  Chen, Camillo J. Taylor, Vijay Kumar", "title": "DFineNet: Ego-Motion Estimation and Depth Refinement from Sparse, Noisy\n  Depth Input with RGB Guidance", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Depth estimation is an important capability for autonomous vehicles to\nunderstand and reconstruct 3D environments as well as avoid obstacles during\nthe execution. Accurate depth sensors such as LiDARs are often heavy, expensive\nand can only provide sparse depth while lighter depth sensors such as stereo\ncameras are noiser in comparison. We propose an end-to-end learning algorithm\nthat is capable of using sparse, noisy input depth for refinement and depth\ncompletion. Our model also produces the camera pose as a byproduct, making it a\ngreat solution for autonomous systems. We evaluate our approach on both indoor\nand outdoor datasets. Empirical results show that our method performs well on\nthe KITTI~\\cite{kitti_geiger2012we} dataset when compared to other competing\nmethods, while having superior performance in dealing with sparse, noisy input\ndepth on the TUM~\\cite{sturm12iros} dataset.\n", "versions": [{"version": "v1", "created": "Fri, 15 Mar 2019 07:50:18 GMT"}, {"version": "v2", "created": "Mon, 8 Apr 2019 22:02:06 GMT"}, {"version": "v3", "created": "Wed, 10 Apr 2019 19:59:40 GMT"}, {"version": "v4", "created": "Wed, 14 Aug 2019 07:00:09 GMT"}], "update_date": "2019-08-15", "authors_parsed": [["Zhang", "Yilun", ""], ["Nguyen", "Ty", ""], ["Miller", "Ian D.", ""], ["Shivakumar", "Shreyas S.", ""], ["Chen", "Steven", ""], ["Taylor", "Camillo J.", ""], ["Kumar", "Vijay", ""]]}, {"id": "1903.06399", "submitter": "Lei Chen", "authors": "Lei Chen, Le Wu, Zhenzhen Hu, Meng Wang", "title": "Quality-aware Unpaired Image-to-Image Translation", "comments": "IEEE Transactions on Multimedia", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative Adversarial Networks (GANs) have been widely used for the\nimage-to-image translation task. While these models rely heavily on the labeled\nimage pairs, recently some GAN variants have been proposed to tackle the\nunpaired image translation task. These models exploited supervision at the\ndomain level with a reconstruction process for unpaired image translation. On\nthe other hand, parallel works have shown that leveraging perceptual loss\nfunctions based on high level deep features could enhance the generated image\nquality. Nevertheless, as these GAN-based models either depended on the\npretrained deep network structure or relied on the labeled image pairs, they\ncould not be directly applied to the unpaired image translation task. Moreover,\ndespite the improvement of the introduced perceptual losses from deep neural\nnetworks, few researchers have explored the possibility of improving the\ngenerated image quality from classical image quality measures. To tackle the\nabove two challenges, in this paper, we propose a unified quality-aware\nGAN-based framework for unpaired image-to-image translation, where a\nquality-aware loss is explicitly incorporated by comparing each source image\nand the reconstructed image at the domain level. Specifically, we design two\ndetailed implementations of the quality loss. The first method is based on a\nclassical image quality assessment measure by defining a classical\nquality-aware loss. The second method proposes an adaptive deep network based\nloss. Finally, extensive experimental results on many real-world datasets\nclearly show the quality improvement of our proposed framework, and the\nsuperiority of leveraging classical image quality measures for unpaired image\ntranslation compared to the deep network based model.\n", "versions": [{"version": "v1", "created": "Fri, 15 Mar 2019 07:59:07 GMT"}], "update_date": "2019-03-18", "authors_parsed": [["Chen", "Lei", ""], ["Wu", "Le", ""], ["Hu", "Zhenzhen", ""], ["Wang", "Meng", ""]]}, {"id": "1903.06405", "submitter": "Jianwu Fang", "authors": "Jianru Xue, Jianwu Fang, Tao Li, Bohua Zhang, Pu Zhang, Zhen Ye, and\n  Jian Dou", "title": "BLVD: Building A Large-scale 5D Semantics Benchmark for Autonomous\n  Driving", "comments": "To appear in ICRA2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In autonomous driving community, numerous benchmarks have been established to\nassist the tasks of 3D/2D object detection, stereo vision, semantic/instance\nsegmentation. However, the more meaningful dynamic evolution of the surrounding\nobjects of ego-vehicle is rarely exploited, and lacks a large-scale dataset\nplatform. To address this, we introduce BLVD, a large-scale 5D semantics\nbenchmark which does not concentrate on the static detection or\nsemantic/instance segmentation tasks tackled adequately before. Instead, BLVD\naims to provide a platform for the tasks of dynamic 4D (3D+temporal) tracking,\n5D (4D+interactive) interactive event recognition and intention prediction.\nThis benchmark will boost the deeper understanding of traffic scenes than ever\nbefore. We totally yield 249,129 3D annotations, 4,902 independent individuals\nfor tracking with the length of overall 214,922 points, 6,004 valid fragments\nfor 5D interactive event recognition, and 4,900 individuals for 5D intention\nprediction. These tasks are contained in four kinds of scenarios depending on\nthe object density (low and high) and light conditions (daytime and nighttime).\nThe benchmark can be downloaded from our project site\nhttps://github.com/VCCIV/BLVD/.\n", "versions": [{"version": "v1", "created": "Fri, 15 Mar 2019 08:39:11 GMT"}], "update_date": "2019-03-18", "authors_parsed": [["Xue", "Jianru", ""], ["Fang", "Jianwu", ""], ["Li", "Tao", ""], ["Zhang", "Bohua", ""], ["Zhang", "Pu", ""], ["Ye", "Zhen", ""], ["Dou", "Jian", ""]]}, {"id": "1903.06473", "submitter": "Zerong Zheng", "authors": "Zerong Zheng, Tao Yu, Yixuan Wei, Qionghai Dai, Yebin Liu", "title": "DeepHuman: 3D Human Reconstruction from a Single Image", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose DeepHuman, an image-guided volume-to-volume translation CNN for 3D\nhuman reconstruction from a single RGB image. To reduce the ambiguities\nassociated with the surface geometry reconstruction, even for the\nreconstruction of invisible areas, we propose and leverage a dense semantic\nrepresentation generated from SMPL model as an additional input. One key\nfeature of our network is that it fuses different scales of image features into\nthe 3D space through volumetric feature transformation, which helps to recover\naccurate surface geometry. The visible surface details are further refined\nthrough a normal refinement network, which can be concatenated with the volume\ngeneration network using our proposed volumetric normal projection layer. We\nalso contribute THuman, a 3D real-world human model dataset containing about\n7000 models. The network is trained using training data generated from the\ndataset. Overall, due to the specific design of our network and the diversity\nin our dataset, our method enables 3D human model estimation given only a\nsingle image and outperforms state-of-the-art approaches.\n", "versions": [{"version": "v1", "created": "Fri, 15 Mar 2019 11:38:15 GMT"}, {"version": "v2", "created": "Thu, 28 Mar 2019 05:44:21 GMT"}], "update_date": "2019-03-29", "authors_parsed": [["Zheng", "Zerong", ""], ["Yu", "Tao", ""], ["Wei", "Yixuan", ""], ["Dai", "Qionghai", ""], ["Liu", "Yebin", ""]]}, {"id": "1903.06482", "submitter": "Shuaifeng Zhi", "authors": "Shuaifeng Zhi, Michael Bloesch, Stefan Leutenegger, Andrew J. Davison", "title": "SceneCode: Monocular Dense Semantic Reconstruction using Learned Encoded\n  Scene Representations", "comments": "To be published in Proceedings of the IEEE Conference on Computer\n  Vision and Pattern Recognition (CVPR 2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Systems which incrementally create 3D semantic maps from image sequences must\nstore and update representations of both geometry and semantic entities.\nHowever, while there has been much work on the correct formulation for\ngeometrical estimation, state-of-the-art systems usually rely on simple\nsemantic representations which store and update independent label estimates for\neach surface element (depth pixels, surfels, or voxels). Spatial correlation is\ndiscarded, and fused label maps are incoherent and noisy.\n  We introduce a new compact and optimisable semantic representation by\ntraining a variational auto-encoder that is conditioned on a colour image.\nUsing this learned latent space, we can tackle semantic label fusion by jointly\noptimising the low-dimenional codes associated with each of a set of\noverlapping images, producing consistent fused label maps which preserve\nspatial correlation. We also show how this approach can be used within a\nmonocular keyframe based semantic mapping system where a similar code approach\nis used for geometry. The probabilistic formulation allows a flexible\nformulation where we can jointly estimate motion, geometry and semantics in a\nunified optimisation.\n", "versions": [{"version": "v1", "created": "Fri, 15 Mar 2019 12:02:44 GMT"}, {"version": "v2", "created": "Mon, 18 Mar 2019 16:55:47 GMT"}], "update_date": "2019-03-19", "authors_parsed": [["Zhi", "Shuaifeng", ""], ["Bloesch", "Michael", ""], ["Leutenegger", "Stefan", ""], ["Davison", "Andrew J.", ""]]}, {"id": "1903.06495", "submitter": "Farzad Farshchi", "authors": "Farzad Farshchi, Qijing Huang, and Heechul Yun", "title": "Integrating NVIDIA Deep Learning Accelerator (NVDLA) with RISC-V SoC on\n  FireSim", "comments": "Presented at the 2nd Workshop on Energy Efficient Machine Learning\n  and Cognitive Computing for Embedded Applications (EMC2'19)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  NVDLA is an open-source deep neural network (DNN) accelerator which has\nreceived a lot of attention by the community since its introduction by Nvidia.\nIt is a full-featured hardware IP and can serve as a good reference for\nconducting research and development of SoCs with integrated accelerators.\nHowever, an expensive FPGA board is required to do experiments with this IP in\na real SoC. Moreover, since NVDLA is clocked at a lower frequency on an FPGA,\nit would be hard to do accurate performance analysis with such a setup. To\novercome these limitations, we integrate NVDLA into a real RISC-V SoC on the\nAmazon cloud FPGA using FireSim, a cycle-exact FPGA-accelerated simulator. We\nthen evaluate the performance of NVDLA by running YOLOv3 object-detection\nalgorithm. Our results show that NVDLA can sustain 7.5 fps when running YOLOv3.\nWe further analyze the performance by showing that sharing the last-level cache\nwith NVDLA can result in up to 1.56x speedup. We then identify that sharing the\nmemory system with the accelerator can result in unpredictable execution time\nfor the real-time tasks running on this platform. We believe this is an\nimportant issue that must be addressed in order for on-chip DNN accelerators to\nbe incorporated in real-time embedded systems.\n", "versions": [{"version": "v1", "created": "Tue, 5 Mar 2019 01:35:31 GMT"}, {"version": "v2", "created": "Fri, 6 Dec 2019 21:52:55 GMT"}], "update_date": "2019-12-10", "authors_parsed": [["Farshchi", "Farzad", ""], ["Huang", "Qijing", ""], ["Yun", "Heechul", ""]]}, {"id": "1903.06496", "submitter": "Valentin Vielzeuf", "authors": "Juan-Manuel P\\'erez-R\\'ua, Valentin Vielzeuf, St\\'ephane Pateux, Moez\n  Baccouche, Fr\\'ed\\'eric Jurie", "title": "MFAS: Multimodal Fusion Architecture Search", "comments": "CVPR 2019, Jun 2019, Long Beach, United States\n  http://cvpr2019.thecvf.com/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We tackle the problem of finding good architectures for multimodal\nclassification problems. We propose a novel and generic search space that spans\na large number of possible fusion architectures. In order to find an optimal\narchitecture for a given dataset in the proposed search space, we leverage an\nefficient sequential model-based exploration approach that is tailored for the\nproblem. We demonstrate the value of posing multimodal fusion as a neural\narchitecture search problem by extensive experimentation on a toy dataset and\ntwo other real multimodal datasets. We discover fusion architectures that\nexhibit state-of-the-art performance for problems with different domain and\ndataset size, including the NTU RGB+D dataset, the largest multi-modal action\nrecognition dataset available.\n", "versions": [{"version": "v1", "created": "Fri, 15 Mar 2019 12:45:13 GMT"}], "update_date": "2019-03-18", "authors_parsed": [["P\u00e9rez-R\u00faa", "Juan-Manuel", ""], ["Vielzeuf", "Valentin", ""], ["Pateux", "St\u00e9phane", ""], ["Baccouche", "Moez", ""], ["Jurie", "Fr\u00e9d\u00e9ric", ""]]}, {"id": "1903.06516", "submitter": "Philip Jackson", "authors": "Philip T. Jackson, Yinhai Wang, Sinead Knight, Hongming Chen, Thierry\n  Dorval, Martin Brown, Claus Bendtsen, Boguslaw Obara", "title": "Phenotypic Profiling of High Throughput Imaging Screens with Generic\n  Deep Convolutional Features", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While deep learning has seen many recent applications to drug discovery, most\nhave focused on predicting activity or toxicity directly from chemical\nstructure. Phenotypic changes exhibited in cellular images are also indications\nof the mechanism of action (MoA) of chemical compounds. In this paper, we show\nhow pre-trained convolutional image features can be used to assist scientists\nin discovering interesting chemical clusters for further investigation. Our\nmethod reduces the dimensionality of raw fluorescent stained images from a high\nthroughput imaging (HTI) screen, producing an embedding space that groups\ntogether images with similar cellular phenotypes. Running standard unsupervised\nclustering on this embedding space yields a set of distinct phenotypic\nclusters. This allows scientists to further select and focus on interesting\nclusters for downstream analyses. We validate the consistency of our embedding\nspace qualitatively with t-sne visualizations, and quantitatively by measuring\nembedding variance among images that are known to be similar. Results suggested\nthe usefulness of our proposed workflow using deep learning and clustering and\nit can lead to robust HTI screening and compound triage.\n", "versions": [{"version": "v1", "created": "Fri, 15 Mar 2019 13:11:03 GMT"}], "update_date": "2019-03-18", "authors_parsed": [["Jackson", "Philip T.", ""], ["Wang", "Yinhai", ""], ["Knight", "Sinead", ""], ["Chen", "Hongming", ""], ["Dorval", "Thierry", ""], ["Brown", "Martin", ""], ["Bendtsen", "Claus", ""], ["Obara", "Boguslaw", ""]]}, {"id": "1903.06519", "submitter": "Renata Rychtarikova", "authors": "Ganna Platonova, Dalibor Stys, Pavel Soucek, Kirill Lonhus, Jan\n  Valenta and Renata Rychtarikova", "title": "Spectroscopic Approach to Correction and Visualisation of Bright-Field\n  Light Transmission Microscopy Biological Data", "comments": "17 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The most realistic information about the transparent sample such as a live\ncell can be obtained only using bright-field light microscopy. At\nhigh-intensity pulsing LED illumination, we captured a primary\n12-bit-per-channel (bpc) response from an observed sample using a bright-field\nmicroscope equipped with a high-resolution (4872x3248) image sensor. In order\nto suppress data distortions originating from the light interactions with\nelements in the optical path, poor sensor reproduction (geometrical defects of\nthe camera sensor and some peculiarities of sensor sensitivity), we propose a\nspectroscopic approach for the correction of this uncompressed 12-bpc data by\nsimultaneous calibration of all parts of the experimental arrangement.\nMoreover, the final intensities of the corrected images are proportional to the\nphoton fluxes detected by a camera sensor. It can be visualized in 8-bpc\nintensity depth after the Least Information Loss compression [Lect. Notes\nBioinform. 9656, 527 (2016)].\n", "versions": [{"version": "v1", "created": "Thu, 14 Mar 2019 06:40:47 GMT"}, {"version": "v2", "created": "Mon, 31 May 2021 18:05:39 GMT"}, {"version": "v3", "created": "Thu, 8 Jul 2021 16:33:44 GMT"}], "update_date": "2021-07-09", "authors_parsed": [["Platonova", "Ganna", ""], ["Stys", "Dalibor", ""], ["Soucek", "Pavel", ""], ["Lonhus", "Kirill", ""], ["Valenta", "Jan", ""], ["Rychtarikova", "Renata", ""]]}, {"id": "1903.06528", "submitter": "William McNally", "authors": "William McNally, Kanav Vats, Tyler Pinto, Chris Dulhanty, John McPhee,\n  Alexander Wong", "title": "GolfDB: A Video Database for Golf Swing Sequencing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The golf swing is a complex movement requiring considerable full-body\ncoordination to execute proficiently. As such, it is the subject of frequent\nscrutiny and extensive biomechanical analyses. In this paper, we introduce the\nnotion of golf swing sequencing for detecting key events in the golf swing and\nfacilitating golf swing analysis. To enable consistent evaluation of golf swing\nsequencing performance, we also introduce the benchmark database GolfDB,\nconsisting of 1400 high-quality golf swing videos, each labeled with event\nframes, bounding box, player name and sex, club type, and view type.\nFurthermore, to act as a reference baseline for evaluating golf swing\nsequencing performance on GolfDB, we propose a lightweight deep neural network\ncalled SwingNet, which possesses a hybrid deep convolutional and recurrent\nneural network architecture. SwingNet correctly detects eight golf swing events\nat an average rate of 76.1%, and six out of eight events at a rate of 91.8%. In\nline with the proposed baseline SwingNet, we advocate the use of\ncomputationally efficient models in future research to promote in-the-field\nanalysis via deployment on readily-available mobile devices.\n", "versions": [{"version": "v1", "created": "Fri, 15 Mar 2019 13:20:16 GMT"}], "update_date": "2019-03-18", "authors_parsed": [["McNally", "William", ""], ["Vats", "Kanav", ""], ["Pinto", "Tyler", ""], ["Dulhanty", "Chris", ""], ["McPhee", "John", ""], ["Wong", "Alexander", ""]]}, {"id": "1903.06529", "submitter": "Nicolas Girard", "authors": "Nicolas Girard (UCA, TITANE), Guillaume Charpiat (TAU), Yuliya\n  Tarabalka (UCA, TITANE)", "title": "Noisy Supervision for Correcting Misaligned Cadaster Maps Without\n  Perfect Ground Truth Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In machine learning the best performance on a certain task is achieved by\nfully supervised methods when perfect ground truth labels are available.\nHowever, labels are often noisy, especially in remote sensing where manually\ncurated public datasets are rare. We study the multi-modal cadaster map\nalignment problem for which available annotations are mis-aligned polygons,\nresulting in noisy supervision. We subsequently set up a multiple-rounds\ntraining scheme which corrects the ground truth annotations at each round to\nbetter train the model at the next round. We show that it is possible to reduce\nthe noise of the dataset by iteratively training a better alignment model to\ncorrect the annotation alignment.\n", "versions": [{"version": "v1", "created": "Tue, 12 Mar 2019 14:38:39 GMT"}], "update_date": "2019-03-18", "authors_parsed": [["Girard", "Nicolas", "", "UCA, TITANE"], ["Charpiat", "Guillaume", "", "TAU"], ["Tarabalka", "Yuliya", "", "UCA, TITANE"]]}, {"id": "1903.06530", "submitter": "Seijoon Kim", "authors": "Seijoon Kim, Seongsik Park, Byunggook Na, Sungroh Yoon", "title": "Spiking-YOLO: Spiking Neural Network for Energy-Efficient Object\n  Detection", "comments": "Accepted to AAAI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Over the past decade, deep neural networks (DNNs) have demonstrated\nremarkable performance in a variety of applications. As we try to solve more\nadvanced problems, increasing demands for computing and power resources has\nbecome inevitable. Spiking neural networks (SNNs) have attracted widespread\ninterest as the third-generation of neural networks due to their event-driven\nand low-powered nature. SNNs, however, are difficult to train, mainly owing to\ntheir complex dynamics of neurons and non-differentiable spike operations.\nFurthermore, their applications have been limited to relatively simple tasks\nsuch as image classification. In this study, we investigate the performance\ndegradation of SNNs in a more challenging regression problem (i.e., object\ndetection). Through our in-depth analysis, we introduce two novel methods:\nchannel-wise normalization and signed neuron with imbalanced threshold, both of\nwhich provide fast and accurate information transmission for deep SNNs.\nConsequently, we present a first spiked-based object detection model, called\nSpiking-YOLO. Our experiments show that Spiking-YOLO achieves remarkable\nresults that are comparable (up to 98%) to those of Tiny YOLO on non-trivial\ndatasets, PASCAL VOC and MS COCO. Furthermore, Spiking-YOLO on a neuromorphic\nchip consumes approximately 280 times less energy than Tiny YOLO and converges\n2.3 to 4 times faster than previous SNN conversion methods.\n", "versions": [{"version": "v1", "created": "Tue, 12 Mar 2019 08:34:47 GMT"}, {"version": "v2", "created": "Sun, 24 Nov 2019 16:00:31 GMT"}], "update_date": "2019-11-26", "authors_parsed": [["Kim", "Seijoon", ""], ["Park", "Seongsik", ""], ["Na", "Byunggook", ""], ["Yoon", "Sungroh", ""]]}, {"id": "1903.06531", "submitter": "Liyuan Pan Miss", "authors": "Liyuan Pan, Richard Hartley, Cedric Scheerlinck, Miaomiao Liu, Xin Yu,\n  and Yuchao Dai", "title": "High Frame Rate Video Reconstruction based on an Event Camera", "comments": "TPAMI 2020. arXiv admin note: substantial text overlap with\n  arXiv:1811.10180", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Event-based cameras measure intensity changes (called `events') with\nmicrosecond accuracy under high-speed motion and challenging lighting\nconditions. With the `active pixel sensor' (APS), the `Dynamic and Active-pixel\nVision Sensor' (DAVIS) allows the simultaneous output of intensity frames and\nevents. However, the output images are captured at a relatively low frame rate\nand often suffer from motion blur. A blurred image can be regarded as the\nintegral of a sequence of latent images, while events indicate changes between\nthe latent images. Thus, we are able to model the blur-generation process by\nassociating event data to a latent sharp image. Based on the abundant event\ndata alongside a low frame rate, easily blurred images, we propose a simple yet\neffective approach to reconstruct high-quality and high frame rate sharp\nvideos. Starting with a single blurred frame and its event data from DAVIS, we\npropose the Event-based Double Integral (EDI) model and solve it by adding\nregularization terms. Then, we extend it to multiple Event-based Double\nIntegral (mEDI) model to get more smooth results based on multiple images and\ntheir events. Furthermore, we provide a new and more efficient solver to\nminimize the proposed energy model. By optimizing the energy function, we\nachieve significant improvements in removing blur and the reconstruction of a\nhigh temporal resolution video. The video generation is based on solving a\nsimple non-convex optimization problem in a single scalar variable.\nExperimental results on both synthetic and real datasets demonstrate the\nsuperiority of our mEDI model and optimization method compared to the\nstate-of-the-art.\n", "versions": [{"version": "v1", "created": "Tue, 12 Mar 2019 02:34:23 GMT"}, {"version": "v2", "created": "Tue, 23 Apr 2019 04:46:16 GMT"}, {"version": "v3", "created": "Wed, 11 Nov 2020 01:49:56 GMT"}], "update_date": "2020-11-13", "authors_parsed": [["Pan", "Liyuan", ""], ["Hartley", "Richard", ""], ["Scheerlinck", "Cedric", ""], ["Liu", "Miaomiao", ""], ["Yu", "Xin", ""], ["Dai", "Yuchao", ""]]}, {"id": "1903.06536", "submitter": "Saeed Masoudnia", "authors": "Saeed Masoudnia, Omid Mersa, Babak N. Araabi, Abdol-Hossein Vahabie,\n  Mohammad Amin Sadeghi, and Majid Nili Ahmadabadi", "title": "Multi-Representational Learning for Offline Signature Verification using\n  Multi-Loss Snapshot Ensemble of CNNs", "comments": null, "journal-ref": "Expert Systems with Applications, 2019, 133, 317-330", "doi": "10.1016/j.eswa.2019.03.040", "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Offline Signature Verification (OSV) is a challenging pattern recognition\ntask, especially in presence of skilled forgeries that are not available during\ntraining. This study aims to tackle its challenges and meet the substantial\nneed for generalization for OSV by examining different loss functions for\nConvolutional Neural Network (CNN). We adopt our new approach to OSV by asking\ntwo questions: 1. which classification loss provides more generalization for\nfeature learning in OSV? , and 2. How integration of different losses into a\nunified multi-loss function lead to an improved learning framework? These\nquestions are studied based on analysis of three loss functions, including\ncross entropy, Cauchy-Schwarz divergence, and hinge loss. According to\ncomplementary features of these losses, we combine them into a dynamic\nmulti-loss function and propose a novel ensemble framework for simultaneous use\nof them in CNN. Our proposed Multi-Loss Snapshot Ensemble (MLSE) consists of\nseveral sequential trials. In each trial, a dominant loss function is selected\nfrom the multi-loss set, and the remaining losses act as a regularizer.\nDifferent trials learn diverse representations for each input based on\nsignature identification task. This multi-representation set is then employed\nfor the verification task. An ensemble of SVMs is trained on these\nrepresentations, and their decisions are finally combined according to the\nselection of most generalizable SVM for each user. We conducted two sets of\nexperiments based on two different protocols of OSV, i.e., writer-dependent and\nwriter-independent on three signature datasets: GPDS-Synthetic, MCYT, and\nUT-SIG. Based on the writer-dependent OSV protocol, we achieved substantial\nimprovements over the best EERs in the literature. The results of the second\nset of experiments also confirmed the robustness to the arrival of new users\nenrolled in the OSV system.\n", "versions": [{"version": "v1", "created": "Mon, 11 Mar 2019 14:11:21 GMT"}], "update_date": "2019-08-09", "authors_parsed": [["Masoudnia", "Saeed", ""], ["Mersa", "Omid", ""], ["Araabi", "Babak N.", ""], ["Vahabie", "Abdol-Hossein", ""], ["Sadeghi", "Mohammad Amin", ""], ["Ahmadabadi", "Majid Nili", ""]]}, {"id": "1903.06538", "submitter": "Paresh Malalur", "authors": "Paresh Malalur and Tommi Jaakkola", "title": "Alignment Based Matching Networks for One-Shot Classification and\n  Open-Set Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning for object classification relies heavily on convolutional\nmodels. While effective, CNNs are rarely interpretable after the fact. An\nattention mechanism can be used to highlight the area of the image that the\nmodel focuses on thus offering a narrow view into the mechanism of\nclassification. We expand on this idea by forcing the method to explicitly\nalign images to be classified to reference images representing the classes. The\nmechanism of alignment is learned and therefore does not require that the\nreference objects are anything like those being classified. Beyond explanation,\nour exemplar based cross-alignment method enables classification with only a\nsingle example per category (one-shot). Our model cuts the 5-way, 1-shot error\nrate in Omniglot from 2.1% to 1.4% and in MiniImageNet from 53.5% to 46.5%\nwhile simultaneously providing point-wise alignment information providing some\nunderstanding on what the network is capturing. This method of alignment also\nenables the recognition of an unsupported class (open-set) in the one-shot\nsetting while maintaining an F1-score of above 0.5 for Omniglot even with 19\nother distracting classes while baselines completely fail to separate the\nopen-set class in the one-shot setting.\n", "versions": [{"version": "v1", "created": "Mon, 11 Mar 2019 02:50:27 GMT"}], "update_date": "2019-03-18", "authors_parsed": [["Malalur", "Paresh", ""], ["Jaakkola", "Tommi", ""]]}, {"id": "1903.06542", "submitter": "Satyananda Kashyap", "authors": "Alexandros Karargyris, Satyananda Kashyap, Joy T Wu, Arjun Sharma,\n  Mehdi Moradi, Tanveer Syeda-Mahmood", "title": "Age prediction using a large chest X-ray dataset", "comments": "Presented at SPIE Medical Imaging Conference, San Diego, 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Age prediction based on appearances of different anatomies in medical images\nhas been clinically explored for many decades. In this paper, we used deep\nlearning to predict a persons age on Chest X-Rays. Specifically, we trained a\nCNN in regression fashion on a large publicly available dataset. Moreover, for\ninterpretability, we explored activation maps to identify which areas of a CXR\nimage are important for the machine (i.e. CNN) to predict a patients age,\noffering insight. Overall, amongst correctly predicted CXRs, we see areas near\nthe clavicles, shoulders, spine, and mediastinum being most activated for age\nprediction, as one would expect biologically. Amongst incorrectly predicted\nCXRs, we have qualitatively identified disease patterns that could possibly\nmake the anatomies appear older or younger than expected. A further technical\nand clinical evaluation would improve this work. As CXR is the most commonly\nrequested imaging exam, a potential use case for estimating age may be found in\nthe preventative counseling of patient health status compared to their\nage-expected average, particularly when there is a large discrepancy between\npredicted age and the real patient age.\n", "versions": [{"version": "v1", "created": "Sat, 9 Mar 2019 00:12:42 GMT"}], "update_date": "2019-03-18", "authors_parsed": [["Karargyris", "Alexandros", ""], ["Kashyap", "Satyananda", ""], ["Wu", "Joy T", ""], ["Sharma", "Arjun", ""], ["Moradi", "Mehdi", ""], ["Syeda-Mahmood", "Tanveer", ""]]}, {"id": "1903.06548", "submitter": "Philip Sellars", "authors": "Philip Sellars, Angelica Aviles-Rivero, and Carola-Bibiane Sch\\\"onlieb", "title": "Superpixel Contracted Graph-Based Learning for Hyperspectral Image\n  Classification", "comments": "11 pages", "journal-ref": null, "doi": "10.1109/TGRS.2019.2961599", "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A central problem in hyperspectral image classification is obtaining high\nclassification accuracy when using a limited amount of labelled data. In this\npaper we present a novel graph-based framework, which aims to tackle this\nproblem in the presence of large scale data input. Our approach utilises a\nnovel superpixel method, specifically designed for hyperspectral data, to\ndefine meaningful local regions in an image, which with high probability share\nthe same classification label. We then extract spectral and spatial features\nfrom these regions and use these to produce a contracted weighted\ngraph-representation, where each node represents a region rather than a pixel.\nOur graph is then fed into a graph-based semi-supervised classifier which gives\nthe final classification. We show that using superpixels in a graph\nrepresentation is an effective tool for speeding up graphical classifiers\napplied to hyperspectral images. We demonstrate through exhaustive quantitative\nand qualitative results that our proposed method produces accurate\nclassifications when an incredibly small amount of labelled data is used. We\nshow that our approach mitigates the major drawbacks of existing approaches,\nresulting in our approach outperforming several comparative state-of-the-art\ntechniques.\n", "versions": [{"version": "v1", "created": "Thu, 14 Mar 2019 13:23:43 GMT"}, {"version": "v2", "created": "Mon, 18 Mar 2019 09:22:46 GMT"}, {"version": "v3", "created": "Tue, 19 Mar 2019 15:11:25 GMT"}], "update_date": "2020-06-24", "authors_parsed": [["Sellars", "Philip", ""], ["Aviles-Rivero", "Angelica", ""], ["Sch\u00f6nlieb", "Carola-Bibiane", ""]]}, {"id": "1903.06549", "submitter": "Naoya Sogi", "authors": "Naoya Sogi, Rui Zhu, Jing-Hao Xue, Kazuhiro Fukui", "title": "Constrained Mutual Convex Cone Method for Image Set Based Recognition", "comments": "arXiv admin note: substantial text overlap with arXiv:1805.12467", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a method for image-set classification based on\nconvex cone models. Image set classification aims to classify a set of images,\nwhich were usually obtained from video frames or multi-view cameras, into a\ntarget object. To accurately and stably classify a set, it is essential to\nrepresent structural information of the set accurately. There are various\nrepresentative image features, such as histogram based features, HLAC, and\nConvolutional Neural Network (CNN) features. We should note that most of them\nhave non-negativity and thus can be effectively represented by a convex cone.\nThis leads us to introduce the convex cone representation to image-set\nclassification. To establish a convex cone based framework, we mathematically\ndefine multiple angles between two convex cones, and then define the geometric\nsimilarity between the cones using the angles. Moreover, to enhance the\nframework, we introduce a discriminant space that maximizes the between-class\nvariance (gaps) and minimizes the within-class variance of the projected convex\ncones onto the discriminant space, similar to the Fisher discriminant analysis.\nFinally, the classification is performed based on the similarity between\nprojected convex cones. The effectiveness of the proposed method is\ndemonstrated experimentally by using five databases: CMU PIE dataset, ETH-80,\nCMU Motion of Body dataset, Youtube Celebrity dataset, and a private database\nof multi-view hand shapes.\n", "versions": [{"version": "v1", "created": "Thu, 14 Mar 2019 07:14:34 GMT"}], "update_date": "2019-03-18", "authors_parsed": [["Sogi", "Naoya", ""], ["Zhu", "Rui", ""], ["Xue", "Jing-Hao", ""], ["Fukui", "Kazuhiro", ""]]}, {"id": "1903.06562", "submitter": "Soumyabrata Dev", "authors": "Soumyabrata Dev, Shilpa Manandhar, Yee Hui Lee, and Stefan Winkler", "title": "Multi-label Cloud Segmentation Using a Deep Network", "comments": null, "journal-ref": "Published in Proc. IEEE AP-S Symposium on Antennas and Propagation\n  and USNC-URSI Radio Science Meeting, 2019", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Different empirical models have been developed for cloud detection. There is\na growing interest in using the ground-based sky/cloud images for this purpose.\nSeveral methods exist that perform binary segmentation of clouds. In this\npaper, we propose to use a deep learning architecture (U-Net) to perform\nmulti-label sky/cloud image segmentation. The proposed approach outperforms\nrecent literature by a large margin.\n", "versions": [{"version": "v1", "created": "Fri, 15 Mar 2019 14:09:49 GMT"}], "update_date": "2019-03-18", "authors_parsed": [["Dev", "Soumyabrata", ""], ["Manandhar", "Shilpa", ""], ["Lee", "Yee Hui", ""], ["Winkler", "Stefan", ""]]}, {"id": "1903.06571", "submitter": "Donghoon Lee", "authors": "Donghoon Lee, Tomas Pfister, Ming-Hsuan Yang", "title": "Inserting Videos into Videos", "comments": "CVPR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce a new problem of manipulating a given video by\ninserting other videos into it. Our main task is, given an object video and a\nscene video, to insert the object video at a user-specified location in the\nscene video so that the resulting video looks realistic. We aim to handle\ndifferent object motions and complex backgrounds without expensive segmentation\nannotations. As it is difficult to collect training pairs for this problem, we\nsynthesize fake training pairs that can provide helpful supervisory signals\nwhen training a neural network with unpaired real data. The proposed network\narchitecture can take both real and fake pairs as input and perform both\nsupervised and unsupervised training in an adversarial learning scheme. To\nsynthesize a realistic video, the network renders each frame based on the\ncurrent input and previous frames. Within this framework, we observe that\ninjecting noise into previous frames while generating the current frame\nstabilizes training. We conduct experiments on real-world videos in object\ntracking and person re-identification benchmark datasets. Experimental results\ndemonstrate that the proposed algorithm is able to synthesize long sequences of\nrealistic videos with a given object video inserted.\n", "versions": [{"version": "v1", "created": "Fri, 15 Mar 2019 14:30:22 GMT"}], "update_date": "2019-03-18", "authors_parsed": [["Lee", "Donghoon", ""], ["Pfister", "Tomas", ""], ["Yang", "Ming-Hsuan", ""]]}, {"id": "1903.06581", "submitter": "Duo Wang", "authors": "Duo Wang, Mateja Jamnik, Pietro Lio", "title": "Unsupervised and interpretable scene discovery with\n  Discrete-Attend-Infer-Repeat", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we present Discrete Attend Infer Repeat (Discrete-AIR), a\nRecurrent Auto-Encoder with structured latent distributions containing discrete\ncategorical distributions, continuous attribute distributions, and factorised\nspatial attention. While inspired by the original AIR model andretaining AIR\nmodel's capability in identifying objects in an image, Discrete-AIR provides\ndirect interpretability of the latent codes. We show that for Multi-MNIST and a\nmultiple-objects version of dSprites dataset, the Discrete-AIR model needs just\none categorical latent variable, one attribute variable (for Multi-MNIST only),\ntogether with spatial attention variables, for efficient inference. We perform\nanalysis to show that the learnt categorical distributions effectively capture\nthe categories of objects in the scene for Multi-MNIST and for Multi-Sprites.\n", "versions": [{"version": "v1", "created": "Thu, 14 Mar 2019 16:30:27 GMT"}], "update_date": "2019-03-18", "authors_parsed": [["Wang", "Duo", ""], ["Jamnik", "Mateja", ""], ["Lio", "Pietro", ""]]}, {"id": "1903.06586", "submitter": "Xiang Li", "authors": "Xiang Li, Wenhai Wang, Xiaolin Hu and Jian Yang", "title": "Selective Kernel Networks", "comments": "CVPR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In standard Convolutional Neural Networks (CNNs), the receptive fields of\nartificial neurons in each layer are designed to share the same size. It is\nwell-known in the neuroscience community that the receptive field size of\nvisual cortical neurons are modulated by the stimulus, which has been rarely\nconsidered in constructing CNNs. We propose a dynamic selection mechanism in\nCNNs that allows each neuron to adaptively adjust its receptive field size\nbased on multiple scales of input information. A building block called\nSelective Kernel (SK) unit is designed, in which multiple branches with\ndifferent kernel sizes are fused using softmax attention that is guided by the\ninformation in these branches. Different attentions on these branches yield\ndifferent sizes of the effective receptive fields of neurons in the fusion\nlayer. Multiple SK units are stacked to a deep network termed Selective Kernel\nNetworks (SKNets). On the ImageNet and CIFAR benchmarks, we empirically show\nthat SKNet outperforms the existing state-of-the-art architectures with lower\nmodel complexity. Detailed analyses show that the neurons in SKNet can capture\ntarget objects with different scales, which verifies the capability of neurons\nfor adaptively adjusting their receptive field sizes according to the input.\nThe code and models are available at https://github.com/implus/SKNet.\n", "versions": [{"version": "v1", "created": "Fri, 15 Mar 2019 15:04:22 GMT"}, {"version": "v2", "created": "Mon, 18 Mar 2019 03:08:24 GMT"}], "update_date": "2019-03-19", "authors_parsed": [["Li", "Xiang", ""], ["Wang", "Wenhai", ""], ["Hu", "Xiaolin", ""], ["Yang", "Jian", ""]]}, {"id": "1903.06593", "submitter": "Sven Kreiss", "authors": "Sven Kreiss, Lorenzo Bertoni, Alexandre Alahi", "title": "PifPaf: Composite Fields for Human Pose Estimation", "comments": "CVPR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new bottom-up method for multi-person 2D human pose estimation\nthat is particularly well suited for urban mobility such as self-driving cars\nand delivery robots. The new method, PifPaf, uses a Part Intensity Field (PIF)\nto localize body parts and a Part Association Field (PAF) to associate body\nparts with each other to form full human poses. Our method outperforms previous\nmethods at low resolution and in crowded, cluttered and occluded scenes thanks\nto (i) our new composite field PAF encoding fine-grained information and (ii)\nthe choice of Laplace loss for regressions which incorporates a notion of\nuncertainty. Our architecture is based on a fully convolutional, single-shot,\nbox-free design. We perform on par with the existing state-of-the-art bottom-up\nmethod on the standard COCO keypoint task and produce state-of-the-art results\non a modified COCO keypoint task for the transportation domain.\n", "versions": [{"version": "v1", "created": "Fri, 15 Mar 2019 15:14:42 GMT"}, {"version": "v2", "created": "Fri, 5 Apr 2019 11:46:33 GMT"}], "update_date": "2019-04-08", "authors_parsed": [["Kreiss", "Sven", ""], ["Bertoni", "Lorenzo", ""], ["Alahi", "Alexandre", ""]]}, {"id": "1903.06630", "submitter": "Guy Lemieux", "authors": "Guy G.F. Lemieux, Joe Edwards, Joel Vandergriendt, Aaron Severance,\n  Ryan De Iaco, Abdullah Raouf, Hussein Osman, Tom Watzka, Satwant Singh", "title": "TinBiNN: Tiny Binarized Neural Network Overlay in about 5,000 4-LUTs and\n  5mW", "comments": "Presented at 3rd International Workshop on Overlay Architectures for\n  FPGAs (OLAF 2017) arXiv:1704.08802", "journal-ref": null, "doi": null, "report-no": "OLAF/2017/06", "categories": "cs.DC cs.CV cs.OH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reduced-precision arithmetic improves the size, cost, power and performance\nof neural networks in digital logic. In convolutional neural networks, the use\nof 1b weights can achieve state-of-the-art error rates while eliminating\nmultiplication, reducing storage and improving power efficiency. The\nBinaryConnect binary-weighted system, for example, achieves 9.9% error using\nfloating-point activations on the CIFAR-10 dataset. In this paper, we introduce\nTinBiNN, a lightweight vector processor overlay for accelerating inference\ncomputations with 1b weights and 8b activations. The overlay is very small --\nit uses about 5,000 4-input LUTs and fits into a low cost iCE40 UltraPlus FPGA\nfrom Lattice Semiconductor. To show this can be useful, we build two embedded\n'person detector' systems by shrinking the original BinaryConnect network. The\nfirst is a 10-category classifier with a 89% smaller network that runs in\n1,315ms and achieves 13.6% error. The other is a 1-category classifier that is\neven smaller, runs in 195ms, and has only 0.4% error. In both classifiers, the\nerror can be attributed entirely to training and not reduced precision.\n", "versions": [{"version": "v1", "created": "Tue, 5 Mar 2019 14:51:36 GMT"}], "update_date": "2019-03-18", "authors_parsed": [["Lemieux", "Guy G. F.", ""], ["Edwards", "Joe", ""], ["Vandergriendt", "Joel", ""], ["Severance", "Aaron", ""], ["De Iaco", "Ryan", ""], ["Raouf", "Abdullah", ""], ["Osman", "Hussein", ""], ["Watzka", "Tom", ""], ["Singh", "Satwant", ""]]}, {"id": "1903.06646", "submitter": "Mai Bui", "authors": "Mai Bui, Christoph Baur, Nassir Navab, Slobodan Ilic and Shadi\n  Albarqouni", "title": "Adversarial Networks for Camera Pose Regression and Refinement", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite recent advances on the topic of direct camera pose regression using\nneural networks, accurately estimating the camera pose of a single RGB image\nstill remains a challenging task. To address this problem, we introduce a novel\nframework based, in its core, on the idea of implicitly learning the joint\ndistribution of RGB images and their corresponding camera poses using a\ndiscriminator network and adversarial learning. Our method allows not only to\nregress the camera pose from a single image, however, also offers a solely\nRGB-based solution for camera pose refinement using the discriminator network.\nFurther, we show that our method can effectively be used to optimize the\npredicted camera poses and thus improve the localization accuracy. To this end,\nwe validate our proposed method on the publicly available 7-Scenes dataset\nimproving upon the results of direct camera pose regression methods.\n", "versions": [{"version": "v1", "created": "Fri, 15 Mar 2019 16:32:51 GMT"}, {"version": "v2", "created": "Tue, 26 Mar 2019 13:56:35 GMT"}, {"version": "v3", "created": "Sun, 27 Oct 2019 21:17:06 GMT"}], "update_date": "2019-10-29", "authors_parsed": [["Bui", "Mai", ""], ["Baur", "Christoph", ""], ["Navab", "Nassir", ""], ["Ilic", "Slobodan", ""], ["Albarqouni", "Shadi", ""]]}, {"id": "1903.06649", "submitter": "Qiuwen Lou", "authors": "Qiuwen Lou, Indranil Palit, Tang Li, Andras Horvath, Michael Niemier,\n  X. Sharon Hu", "title": "Application-level Studies of Cellular Neural Network-based Hardware\n  Accelerators", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.ET cs.CV cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As cost and performance benefits associated with Moore's Law scaling slow,\nresearchers are studying alternative architectures (e.g., based on analog\nand/or spiking circuits) and/or computational models (e.g., convolutional and\nrecurrent neural networks) to perform application-level tasks faster, more\nenergy efficiently, and/or more accurately. We investigate cellular neural\nnetwork (CeNN)-based co-processors at the application-level for these metrics.\nWhile it is well-known that CeNNs can be well-suited for spatio-temporal\ninformation processing, few (if any) studies have quantified the\nenergy/delay/accuracy of a CeNN-friendly algorithm and compared the CeNN-based\napproach to the best von Neumann algorithm at the application level. We present\nan evaluation framework for such studies. As a case study, a CeNN-friendly\ntarget-tracking algorithm was developed and mapped to an array architecture\ndeveloped in conjunction with the algorithm. We compare the energy, delay, and\naccuracy of our architecture/algorithm (assuming all overheads) to the most\naccurate von Neumann algorithm (Struck). Von Neumann CPU data is measured on an\nIntel i5 chip. The CeNN approach is capable of matching the accuracy of Struck,\nand can offer approximately 1000x improvements in energy-delay product.\n", "versions": [{"version": "v1", "created": "Thu, 28 Feb 2019 17:07:33 GMT"}, {"version": "v2", "created": "Wed, 12 Jun 2019 21:23:42 GMT"}], "update_date": "2019-06-14", "authors_parsed": [["Lou", "Qiuwen", ""], ["Palit", "Indranil", ""], ["Li", "Tang", ""], ["Horvath", "Andras", ""], ["Niemier", "Michael", ""], ["Hu", "X. Sharon", ""]]}, {"id": "1903.06708", "submitter": "Ondrej Miksik", "authors": "Ondrej Miksik and Vibhav Vineet", "title": "Live Reconstruction of Large-Scale Dynamic Outdoor Worlds", "comments": "CVPR 2019 workshop on dynamic scene reconstruction", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Standard 3D reconstruction pipelines assume stationary world, therefore\nsuffer from `ghost artifacts' whenever dynamic objects are present in the\nscene. Recent approaches has started tackling this issue, however, they\ntypically either only discard dynamic information, represent it using bounding\nboxes or per-frame depth or rely on approaches that are inherently slow and not\nsuitable to online settings.\n  We propose an end-to-end system for live reconstruction of large-scale\noutdoor dynamic environments. We leverage recent advances in computationally\nefficient data-driven approaches for 6-DoF object pose estimation to segment\nthe scene into objects and stationary `background'. This allows us to represent\nthe scene using a time-dependent (dynamic) map, in which each object is\nexplicitly represented as a separate instance and reconstructed in its own\nvolume. For each time step, our dynamic map maintains a relative pose of each\nvolume with respect to the stationary background. Our system operates in\nincremental manner which is essential for on-line reconstruction, handles\nlarge-scale environments with objects at large distances and runs in (near)\nreal-time. We demonstrate the efficacy of our approach on the KITTI dataset,\nand provide qualitative and quantitative results showing high-quality dense 3D\nreconstructions of a number of dynamic scenes.\n", "versions": [{"version": "v1", "created": "Fri, 15 Mar 2019 15:50:59 GMT"}, {"version": "v2", "created": "Mon, 15 Apr 2019 18:40:16 GMT"}], "update_date": "2019-04-17", "authors_parsed": [["Miksik", "Ondrej", ""], ["Vineet", "Vibhav", ""]]}, {"id": "1903.06754", "submitter": "Ruohan Zhang", "authors": "Ruohan Zhang, Calen Walshe, Zhuode Liu, Lin Guan, Karl S. Muller, Jake\n  A. Whritner, Luxin Zhang, Mary M. Hayhoe, Dana H. Ballard", "title": "Atari-HEAD: Atari Human Eye-Tracking and Demonstration Dataset", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large-scale public datasets have been shown to benefit research in multiple\nareas of modern artificial intelligence. For decision-making research that\nrequires human data, high-quality datasets serve as important benchmarks to\nfacilitate the development of new methods by providing a common reproducible\nstandard. Many human decision-making tasks require visual attention to obtain\nhigh levels of performance. Therefore, measuring eye movements can provide a\nrich source of information about the strategies that humans use to solve\ndecision-making tasks. Here, we provide a large-scale, high-quality dataset of\nhuman actions with simultaneously recorded eye movements while humans play\nAtari video games. The dataset consists of 117 hours of gameplay data from a\ndiverse set of 20 games, with 8 million action demonstrations and 328 million\ngaze samples. We introduce a novel form of gameplay, in which the human plays\nin a semi-frame-by-frame manner. This leads to near-optimal game decisions and\ngame scores that are comparable or better than known human records. We\ndemonstrate the usefulness of the dataset through two simple applications:\npredicting human gaze and imitating human demonstrated actions. The quality of\nthe data leads to promising results in both tasks. Moreover, using a learned\nhuman gaze model to inform imitation learning leads to an 115\\% increase in\ngame performance. We interpret these results as highlighting the importance of\nincorporating human visual attention in models of decision making and\ndemonstrating the value of the current dataset to the research community. We\nhope that the scale and quality of this dataset can provide more opportunities\nto researchers in the areas of visual attention, imitation learning, and\nreinforcement learning.\n", "versions": [{"version": "v1", "created": "Fri, 15 Mar 2019 18:55:07 GMT"}, {"version": "v2", "created": "Sat, 7 Sep 2019 20:17:17 GMT"}], "update_date": "2019-09-10", "authors_parsed": [["Zhang", "Ruohan", ""], ["Walshe", "Calen", ""], ["Liu", "Zhuode", ""], ["Guan", "Lin", ""], ["Muller", "Karl S.", ""], ["Whritner", "Jake A.", ""], ["Zhang", "Luxin", ""], ["Hayhoe", "Mary M.", ""], ["Ballard", "Dana H.", ""]]}, {"id": "1903.06763", "submitter": "Tiziano Portenier", "authors": "Tiziano Portenier and Qiyang Hu and Paolo Favaro and Matthias Zwicker", "title": "Smart, Deep Copy-Paste", "comments": "12 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we propose a novel system for smart copy-paste, enabling the\nsynthesis of high-quality results given a masked source image content and a\ntarget image context as input. Our system naturally resolves both shading and\ngeometric inconsistencies between source and target image, resulting in a\nmerged result image that features the content from the pasted source image,\nseamlessly pasted into the target context. Our framework is based on a novel\ntraining image transformation procedure that allows to train a deep\nconvolutional neural network end-to-end to automatically learn a representation\nthat is suitable for copy-pasting. Our training procedure works with any image\ndataset without additional information such as labels, and we demonstrate the\neffectiveness of our system on two popular datasets, high-resolution face\nimages and the more complex Cityscapes dataset. Our technique outperforms the\ncurrent state of the art on face images, and we show promising results on the\nCityscapes dataset, demonstrating that our system generalizes to much higher\nresolution than the training data.\n", "versions": [{"version": "v1", "created": "Fri, 15 Mar 2019 19:07:34 GMT"}], "update_date": "2019-03-19", "authors_parsed": [["Portenier", "Tiziano", ""], ["Hu", "Qiyang", ""], ["Favaro", "Paolo", ""], ["Zwicker", "Matthias", ""]]}, {"id": "1903.06781", "submitter": "Shay Deutsch Dr.", "authors": "Shay Deutsch, Andrea Bertozzi, and Stefano Soatto", "title": "Zero Shot Learning with the Isoperimetric Loss", "comments": "Accepted to AAAI-20", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce the isoperimetric loss as a regularization criterion for\nlearning the map from a visual representation to a semantic embedding, to be\nused to transfer knowledge to unknown classes in a zero-shot learning setting.\nWe use a pre-trained deep neural network model as a visual representation of\nimage data, a Word2Vec embedding of class labels, and linear maps between the\nvisual and semantic embedding spaces. However, the spaces themselves are not\nlinear, and we postulate the sample embedding to be populated by noisy samples\nnear otherwise smooth manifolds. We exploit the graph structure defined by the\nsample points to regularize the estimates of the manifolds by inferring the\ngraph connectivity using a generalization of the isoperimetric inequalities\nfrom Riemannian geometry to graphs. Surprisingly, this regularization alone,\npaired with the simplest baseline model, outperforms the state-of-the-art among\nfully automated methods in zero-shot learning benchmarks such as AwA and CUB.\nThis improvement is achieved solely by learning the structure of the underlying\nspaces by imposing regularity.\n", "versions": [{"version": "v1", "created": "Fri, 15 Mar 2019 19:55:38 GMT"}, {"version": "v2", "created": "Tue, 3 Dec 2019 19:17:12 GMT"}], "update_date": "2019-12-05", "authors_parsed": [["Deutsch", "Shay", ""], ["Bertozzi", "Andrea", ""], ["Soatto", "Stefano", ""]]}, {"id": "1903.06791", "submitter": "Tao Sheng", "authors": "Chen Feng, Tao Sheng, Zhiyu Liang, Shaojie Zhuo, Xiaopeng Zhang, Liang\n  Shen, Matthew Ardi, Alexander C. Berg, Yiran Chen, Bo Chen, Kent Gauen,\n  Yung-Hsiang Lu", "title": "Low Power Inference for On-Device Visual Recognition with a\n  Quantization-Friendly Solution", "comments": "Accepted At The 2nd Workshop on Machine Learning on the Phone and\n  other Consumer Devices (MLPCD 2)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The IEEE Low-Power Image Recognition Challenge (LPIRC) is an annual\ncompetition started in 2015 that encourages joint hardware and software\nsolutions for computer vision systems with low latency and power. Track 1 of\nthe competition in 2018 focused on the innovation of software solutions with\nfixed inference engine and hardware. This decision allows participants to\nsubmit models online and not worry about building and bringing custom hardware\non-site, which attracted a historically large number of submissions. Among the\ndiverse solutions, the winning solution proposed a quantization-friendly\nframework for MobileNets that achieves an accuracy of 72.67% on the holdout\ndataset with an average latency of 27ms on a single CPU core of Google Pixel2\nphone, which is superior to the best real-time MobileNet models at the time.\n", "versions": [{"version": "v1", "created": "Tue, 12 Mar 2019 18:50:34 GMT"}], "update_date": "2019-03-19", "authors_parsed": [["Feng", "Chen", ""], ["Sheng", "Tao", ""], ["Liang", "Zhiyu", ""], ["Zhuo", "Shaojie", ""], ["Zhang", "Xiaopeng", ""], ["Shen", "Liang", ""], ["Ardi", "Matthew", ""], ["Berg", "Alexander C.", ""], ["Chen", "Yiran", ""], ["Chen", "Bo", ""], ["Gauen", "Kent", ""], ["Lu", "Yung-Hsiang", ""]]}, {"id": "1903.06811", "submitter": "Amy Tabb", "authors": "Amy Tabb and Henry Medeiros and Mitchell J. Feldmann and Thiago T.\n  Santos", "title": "Calibration of Asynchronous Camera Networks: CALICO", "comments": "11 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Camera network and multi-camera calibration for external parameters is a\nnecessary step for a variety of contexts in computer vision and robotics,\nranging from three-dimensional reconstruction to human activity tracking. This\npaper describes CALICO, a method for camera network and/or multi-camera\ncalibration suitable for challenging contexts: the cameras may not share a\ncommon field of view and the network may be asynchronous. The calibration\nobject required is one or more rigidly attached planar calibration patterns,\nwhich are distinguishable from one another, such as aruco or charuco patterns.\n  We formulate the camera network and/or multi-camera calibration problem using\nrigidity constraints, represented as a system of equations, and an approximate\nsolution is found through a two-step process. Simulated and real experiments,\nincluding an asynchronous camera network, multicamera system, and rotating\nimaging system, demonstrate the method in a variety of settings. Median\nreconstruction accuracy error was less than $0.41$ mm$^2$ for all datasets.\nThis method is suitable for novice users to calibrate a camera network, and the\nmodularity of the calibration object also allows for disassembly, shipping, and\nthe use of this method in a variety of large and small spaces.\n", "versions": [{"version": "v1", "created": "Fri, 15 Mar 2019 21:35:13 GMT"}, {"version": "v2", "created": "Thu, 14 Nov 2019 19:13:24 GMT"}], "update_date": "2019-11-18", "authors_parsed": [["Tabb", "Amy", ""], ["Medeiros", "Henry", ""], ["Feldmann", "Mitchell J.", ""], ["Santos", "Thiago T.", ""]]}, {"id": "1903.06814", "submitter": "Dominik Belter Mr.", "authors": "Karol Piaskowski, Rafal Staszak, Dominik Belter", "title": "Generate What You Can't See - a View-dependent Image Generation", "comments": "Submitted to IROS 2019. Copyright 2019 IEEE. Personal use of this\n  material is permitted. Permission from IEEE must be obtained for all other\n  uses. Supplementary video: https://youtu.be/gCAoJ7BM5F0", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In order to operate autonomously, a robot should explore the environment and\nbuild a model of each of the surrounding objects. A common approach is to\ncarefully scan the whole workspace. This is time-consuming. It is also often\nimpossible to reach all the viewpoints required to acquire full knowledge about\nthe environment. Humans can perform shape completion of occluded objects by\nrelying on past experience. Therefore, we propose a method that generates\nimages of an object from various viewpoints using a single input RGB image. A\ndeep neural network is trained to imagine the object appearance from many\nviewpoints. We present the whole pipeline, which takes a single RGB image as\ninput and returns a sequence of RGB and depth images of the object. The method\nutilizes a CNN-based object detector to extract the object from the natural\nscene. Then, the proposed network generates a set of RGB and depth images. We\nshow the results both on a synthetic dataset and on real images.\n", "versions": [{"version": "v1", "created": "Fri, 15 Mar 2019 21:50:54 GMT"}], "update_date": "2019-03-19", "authors_parsed": [["Piaskowski", "Karol", ""], ["Staszak", "Rafal", ""], ["Belter", "Dominik", ""]]}, {"id": "1903.06836", "submitter": "Lakshmanan Nataraj", "authors": "Lakshmanan Nataraj, Tajuddin Manhar Mohammed, Shivkumar\n  Chandrasekaran, Arjuna Flenner, Jawadul H. Bappy, Amit K. Roy-Chowdhury, B.\n  S. Manjunath", "title": "Detecting GAN generated Fake Images using Co-occurrence Matrices", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The advent of Generative Adversarial Networks (GANs) has brought about\ncompletely novel ways of transforming and manipulating pixels in digital\nimages. GAN based techniques such as Image-to-Image translations, DeepFakes,\nand other automated methods have become increasingly popular in creating fake\nimages. In this paper, we propose a novel approach to detect GAN generated fake\nimages using a combination of co-occurrence matrices and deep learning. We\nextract co-occurrence matrices on three color channels in the pixel domain and\ntrain a model using a deep convolutional neural network (CNN) framework.\nExperimental results on two diverse and challenging GAN datasets comprising\nmore than 56,000 images based on unpaired image-to-image translations (cycleGAN\n[1]) and facial attributes/expressions (StarGAN [2]) show that our approach is\npromising and achieves more than 99% classification accuracy in both datasets.\nFurther, our approach also generalizes well and achieves good results when\ntrained on one dataset and tested on the other.\n", "versions": [{"version": "v1", "created": "Fri, 15 Mar 2019 23:24:08 GMT"}, {"version": "v2", "created": "Thu, 3 Oct 2019 00:54:21 GMT"}], "update_date": "2019-10-04", "authors_parsed": [["Nataraj", "Lakshmanan", ""], ["Mohammed", "Tajuddin Manhar", ""], ["Chandrasekaran", "Shivkumar", ""], ["Flenner", "Arjuna", ""], ["Bappy", "Jawadul H.", ""], ["Roy-Chowdhury", "Amit K.", ""], ["Manjunath", "B. S.", ""]]}, {"id": "1903.06837", "submitter": "Mikhail Usvyatsov", "authors": "Mikhail Usvyatsov, Konrad Schindler", "title": "Visual recognition in the wild by sampling deep similarity functions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recognising relevant objects or object states in its environment is a basic\ncapability for an autonomous robot. The dominant approach to object recognition\nin images and range images is classification by supervised machine learning,\nnowadays mostly with deep convolutional neural networks (CNNs). This works well\nfor target classes whose variability can be completely covered with training\nexamples. However, a robot moving in the wild, i.e., in an environment that is\nnot known at the time the recognition system is trained, will often face\n\\emph{domain shift}: the training data cannot be assumed to exhaustively cover\nall the within-class variability that will be encountered in the test data. In\nthat situation, learning is in principle possible, since the training set does\ncapture the defining properties, respectively dissimilarities, of the target\nclasses. But directly training a CNN to predict class probabilities is prone to\noverfitting to irrelevant correlations between the class labels and the\nspecific subset of the target class that is represented in the training set. We\nexplore the idea to instead learn a Siamese CNN that acts as similarity\nfunction between pairs of training examples. Class predictions are then\nobtained by measuring the similarities between a new test instance and the\ntraining samples. We show that the CNN embedding correctly recovers the\nrelative similarities to arbitrary class exemplars in the training set. And\nthat therefore few, randomly picked training exemplars are sufficient to\nachieve good predictions, making the procedure efficient.\n", "versions": [{"version": "v1", "created": "Fri, 15 Mar 2019 23:26:13 GMT"}], "update_date": "2019-03-19", "authors_parsed": [["Usvyatsov", "Mikhail", ""], ["Schindler", "Konrad", ""]]}, {"id": "1903.06846", "submitter": "Kuangen Zhang", "authors": "Kuangen Zhang, Jing Wang, Chenglong Fu", "title": "Directional PointNet: 3D Environmental Classification for Wearable\n  Robotics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Environmental information can provide reliable prior information about human\nmotion intent, which can aid the subject with wearable robotics to walk in\ncomplex environments. Previous researchers have utilized 1D signal and 2D\nimages to classify environments, but they may face the problems of\nself-occlusion. Comparatively, 3D point cloud can be more appropriate to depict\nenvironments, thus we propose a directional PointNet to classify 3D point cloud\ndirectly. By utilizing the orientation information of the point cloud, the\ndirectional PointNet can classify daily terrains, including level ground, up\nstairs, and down stairs, and the classification accuracy achieves 99% for\ntesting set. Moreover, the directional PointNet is more efficient than the\nprevious PointNet because the T-net, which is utilized to estimate the\ntransformation of the point cloud, is removed in this research and the length\nof the global feature is optimized. The experimental results demonstrate that\nthe directional PointNet can classify the environments robustly and\nefficiently.\n", "versions": [{"version": "v1", "created": "Sat, 16 Mar 2019 00:14:20 GMT"}, {"version": "v2", "created": "Wed, 20 Mar 2019 18:19:32 GMT"}], "update_date": "2019-03-22", "authors_parsed": [["Zhang", "Kuangen", ""], ["Wang", "Jing", ""], ["Fu", "Chenglong", ""]]}, {"id": "1903.06855", "submitter": "Ali Oguz Uzman", "authors": "Ali Oguz Uzman and Jannis Horn and Sven Behnke", "title": "Learning Super-resolution 3D Segmentation of Plant Root MRI Images from\n  Few Examples", "comments": "27th European Symposium on Artificial Neural Networks, Computational\n  Intelligence and Machine Learning (ESANN), Bruges, Belgium, 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Analyzing plant roots is crucial to understand plant performance in different\nsoil environments. While magnetic resonance imaging (MRI) can be used to obtain\n3D images of plant roots, extracting the root structural model is challenging\ndue to highly noisy soil environments and low-resolution of MRI images. To\nimprove both contrast and resolution, we adapt the state-of-the-art method\nRefineNet for 3D segmentation of the plant root MRI images in super-resolution.\nThe networks are trained from few manual segmentations that are augmented by\ngeometric transformations, realistic noise, and other variabilities. The\nresulting segmentations contain most root structures, including branches not\nextracted by the human annotator.\n", "versions": [{"version": "v1", "created": "Sat, 16 Mar 2019 01:12:04 GMT"}], "update_date": "2019-03-19", "authors_parsed": [["Uzman", "Ali Oguz", ""], ["Horn", "Jannis", ""], ["Behnke", "Sven", ""]]}, {"id": "1903.06864", "submitter": "Tatiana Tommasi", "authors": "Fabio Maria Carlucci, Antonio D'Innocente, Silvia Bucci, Barbara\n  Caputo, Tatiana Tommasi", "title": "Domain Generalization by Solving Jigsaw Puzzles", "comments": "Accepted at CVPR 2019 (oral)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human adaptability relies crucially on the ability to learn and merge\nknowledge both from supervised and unsupervised learning: the parents point out\nfew important concepts, but then the children fill in the gaps on their own.\nThis is particularly effective, because supervised learning can never be\nexhaustive and thus learning autonomously allows to discover invariances and\nregularities that help to generalize. In this paper we propose to apply a\nsimilar approach to the task of object recognition across domains: our model\nlearns the semantic labels in a supervised fashion, and broadens its\nunderstanding of the data by learning from self-supervised signals how to solve\na jigsaw puzzle on the same images. This secondary task helps the network to\nlearn the concepts of spatial correlation while acting as a regularizer for the\nclassification task. Multiple experiments on the PACS, VLCS, Office-Home and\ndigits datasets confirm our intuition and show that this simple method\noutperforms previous domain generalization and adaptation solutions. An\nablation study further illustrates the inner workings of our approach.\n", "versions": [{"version": "v1", "created": "Sat, 16 Mar 2019 01:56:59 GMT"}, {"version": "v2", "created": "Sun, 14 Apr 2019 11:03:44 GMT"}], "update_date": "2019-08-09", "authors_parsed": [["Carlucci", "Fabio Maria", ""], ["D'Innocente", "Antonio", ""], ["Bucci", "Silvia", ""], ["Caputo", "Barbara", ""], ["Tommasi", "Tatiana", ""]]}, {"id": "1903.06874", "submitter": "Huan Ling", "authors": "Huan Ling and Jun Gao and Amlan Kar and Wenzheng Chen and Sanja Fidler", "title": "Fast Interactive Object Annotation with Curve-GCN", "comments": "In Computer Vision and Pattern Recognition (CVPR), Long Beach, US,\n  2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Manually labeling objects by tracing their boundaries is a laborious process.\nIn Polygon-RNN++ the authors proposed Polygon-RNN that produces polygonal\nannotations in a recurrent manner using a CNN-RNN architecture, allowing\ninteractive correction via humans-in-the-loop. We propose a new framework that\nalleviates the sequential nature of Polygon-RNN, by predicting all vertices\nsimultaneously using a Graph Convolutional Network (GCN). Our model is trained\nend-to-end. It supports object annotation by either polygons or splines,\nfacilitating labeling efficiency for both line-based and curved objects. We\nshow that Curve-GCN outperforms all existing approaches in automatic mode,\nincluding the powerful PSP-DeepLab and is significantly more efficient in\ninteractive mode than Polygon-RNN++. Our model runs at 29.3ms in automatic, and\n2.6ms in interactive mode, making it 10x and 100x faster than Polygon-RNN++.\n", "versions": [{"version": "v1", "created": "Sat, 16 Mar 2019 03:14:41 GMT"}], "update_date": "2019-03-19", "authors_parsed": [["Ling", "Huan", ""], ["Gao", "Jun", ""], ["Kar", "Amlan", ""], ["Chen", "Wenzheng", ""], ["Fidler", "Sanja", ""]]}, {"id": "1903.06879", "submitter": "Zhou Yang", "authors": "Lifang Wu, Zhou Yang, Jiaoyu He, Meng Jian, Yaowen Xu, Dezhong Xu and\n  Chang Wen Chen", "title": "Ontology Based Global and Collective Motion Patterns for Event\n  Classification in Basketball Videos", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In multi-person videos, especially team sport videos, a semantic event is\nusually represented as a confrontation between two teams of players, which can\nbe represented as collective motion. In broadcast basketball videos, specific\ncamera motions are used to present specific events. Therefore, a semantic event\nin broadcast basketball videos is closely related to both the global motion\n(camera motion) and the collective motion. A semantic event in basketball\nvideos can be generally divided into three stages: pre-event, event occurrence\n(event-occ), and post-event. In this paper, we propose an ontology-based global\nand collective motion pattern (On_GCMP) algorithm for basketball event\nclassification. First, a two-stage GCMP based event classification scheme is\nproposed. The GCMP is extracted using optical flow. The two-stage scheme\nprogressively combines a five-class event classification algorithm on\nevent-occs and a two-class event classification algorithm on pre-events. Both\nalgorithms utilize sequential convolutional neural networks (CNNs) and long\nshort-term memory (LSTM) networks to extract the spatial and temporal features\nof GCMP for event classification. Second, we utilize post-event segments to\npredict success/failure using deep features of images in the video frames\n(RGB_DF_VF) based algorithms. Finally the event classification results and\nsuccess/failure classification results are integrated to obtain the final\nresults. To evaluate the proposed scheme, we collected a new dataset called\nNCAA+, which is automatically obtained from the NCAA dataset by extending the\nfixed length of video clips forward and backward of the corresponding semantic\nevents. The experimental results demonstrate that the proposed scheme achieves\nthe mean average precision of 58.10% on NCAA+. It is higher by 6.50% than\nstate-of-the-art on NCAA.\n", "versions": [{"version": "v1", "created": "Sat, 16 Mar 2019 04:52:02 GMT"}, {"version": "v2", "created": "Tue, 19 Mar 2019 08:53:50 GMT"}], "update_date": "2019-03-20", "authors_parsed": [["Wu", "Lifang", ""], ["Yang", "Zhou", ""], ["He", "Jiaoyu", ""], ["Jian", "Meng", ""], ["Xu", "Yaowen", ""], ["Xu", "Dezhong", ""], ["Chen", "Chang Wen", ""]]}, {"id": "1903.06909", "submitter": "Elahe Mousavi", "authors": "Elahe Mousavi, Rahele Kafieh, Hossein Rabbani", "title": "Classification of dry age-related macular degeneration and diabetic\n  macular edema from optical coherence tomography images using dictionary\n  learning", "comments": "9 pages, IET journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Age-related Macular Degeneration (AMD) and Diabetic Macular Edema (DME) are\nthe major causes of vision loss in developed countries. Alteration of retinal\nlayer structure and appearance of exudate are the most significant signs of\nthese diseases. With the aim of automatic classification of DME, AMD and normal\nsubjects from Optical Coherence Tomography (OCT) images, we proposed a\nclassification algorithm. The two important issues intended in this approach\nare, not utilizing retinal layer segmentation which by itself is a challenging\ntask and attempting to identify diseases in their early stages, where the signs\nof diseases appear in a small fraction of B-Scans. We used a histogram of\noriented gradients (HOG) feature descriptor to well characterize the\ndistribution of local intensity gradients and edge directions. In order to\ncapture the structure of extracted features, we employed different dictionary\nlearning-based classifiers. Our dataset consists of 45 subjects: 15 patients\nwith AMD, 15 patients with DME and 15 normal subjects. The proposed classifier\nleads to an accuracy of 95.13%, 100.00%, and 100.00% for DME, AMD, and normal\nOCT images, respectively, only by considering the 4% of all B-Scans of a volume\nwhich outperforms the state of the art methods.\n", "versions": [{"version": "v1", "created": "Sat, 16 Mar 2019 11:17:57 GMT"}], "update_date": "2019-03-19", "authors_parsed": [["Mousavi", "Elahe", ""], ["Kafieh", "Rahele", ""], ["Rabbani", "Hossein", ""]]}, {"id": "1903.06916", "submitter": "M{\\aa}ns Larsson", "authors": "M{\\aa}ns Larsson, Erik Stenborg, Lars Hammarstrand, Torsten Sattler,\n  Mark Pollefeys, Fredrik Kahl", "title": "A Cross-Season Correspondence Dataset for Robust Semantic Segmentation", "comments": "In Proc. CVPR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a method to utilize 2D-2D point matches between\nimages taken during different image conditions to train a convolutional neural\nnetwork for semantic segmentation. Enforcing label consistency across the\nmatches makes the final segmentation algorithm robust to seasonal changes. We\ndescribe how these 2D-2D matches can be generated with little human interaction\nby geometrically matching points from 3D models built from images. Two\ncross-season correspondence datasets are created providing 2D-2D matches across\nseasonal changes as well as from day to night. The datasets are made publicly\navailable to facilitate further research. We show that adding the\ncorrespondences as extra supervision during training improves the segmentation\nperformance of the convolutional neural network, making it more robust to\nseasonal changes and weather conditions.\n", "versions": [{"version": "v1", "created": "Sat, 16 Mar 2019 13:01:23 GMT"}, {"version": "v2", "created": "Fri, 16 Aug 2019 06:41:21 GMT"}], "update_date": "2019-08-19", "authors_parsed": [["Larsson", "M\u00e5ns", ""], ["Stenborg", "Erik", ""], ["Hammarstrand", "Lars", ""], ["Sattler", "Torsten", ""], ["Pollefeys", "Mark", ""], ["Kahl", "Fredrik", ""]]}, {"id": "1903.06920", "submitter": "Uddeshya Upadhyay", "authors": "Uddeshya Upadhyay, Suyash P. Awate", "title": "Robust Super-Resolution GAN, with Manifold-based and Perception Loss", "comments": "IEEE International Symposium on Biomedical Imaging (ISBI)-2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Super-resolution using deep neural networks typically relies on highly\ncurated training sets that are often unavailable in clinical deployment\nscenarios. Using loss functions that assume Gaussian-distributed residuals\nmakes the learning sensitive to corruptions in clinical training sets. We\npropose novel loss functions that are robust to corruptions in training sets by\nmodeling heavy-tailed non-Gaussian distributions on the residuals. We propose a\nloss based on an autoencoder-based manifold-distance between the super-resolved\nand high-resolution images, to reproduce realistic textural content in\nsuper-resolved images. We propose to learn to super-resolve images to match\nhuman perceptions of structure, luminance, and contrast. Results on a large\nclinical dataset shows the advantages of each of our contributions, where our\nframework improves over the state of the art.\n", "versions": [{"version": "v1", "created": "Sat, 16 Mar 2019 13:45:09 GMT"}], "update_date": "2019-03-19", "authors_parsed": [["Upadhyay", "Uddeshya", ""], ["Awate", "Suyash P.", ""]]}, {"id": "1903.06922", "submitter": "Zhengeng Yang", "authors": "Zhengeng Yang, Hongshan Yu, Qiang Fu, Wei Sun, Wenyan Jia, Mingui Sun,\n  Zhi-Hong Mao", "title": "Real time backbone for semantic segmentation", "comments": "6pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The rapid development of autonomous driving in recent years presents lots of\nchallenges for scene understanding. As an essential step towards scene\nunderstanding, semantic segmentation thus received lots of attention in past\nfew years. Although deep learning based state-of-the-arts have achieved great\nsuccess in improving the segmentation accuracy, most of them suffer from an\ninefficiency problem and can hardly applied to practical applications. In this\npaper, we systematically analyze the computation cost of Convolutional Neural\nNetwork(CNN) and found that the inefficiency of CNN is mainly caused by its\nwide structure rather than the deep structure. In addition, the success of\npruning based model compression methods proved that there are many redundant\nchannels in CNN. Thus, we designed a very narrow while deep backbone network to\nimprove the efficiency of semantic segmentation. By casting our network to\nFCN32 segmentation architecture, the basic structure of most segmentation\nmethods, we achieved 60.6\\% mIoU on Cityscape val dataset with 54 frame per\nseconds(FPS) on $1024\\times2048$ inputs, which already outperforms one of the\nearliest real time deep learning based segmentation methods: ENet.\n", "versions": [{"version": "v1", "created": "Sat, 16 Mar 2019 13:51:15 GMT"}], "update_date": "2019-03-19", "authors_parsed": [["Yang", "Zhengeng", ""], ["Yu", "Hongshan", ""], ["Fu", "Qiang", ""], ["Sun", "Wei", ""], ["Jia", "Wenyan", ""], ["Sun", "Mingui", ""], ["Mao", "Zhi-Hong", ""]]}, {"id": "1903.06923", "submitter": "Rohan Ghosh", "authors": "Rohan Ghosh, Anupam Gupta, Siyi Tang, Alcimar Soares, Nitish Thakor", "title": "Spatiotemporal Feature Learning for Event-Based Vision", "comments": "Submitted to IEEE Transactions in Neural Networks and Learning\n  Systems", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unlike conventional frame-based sensors, event-based visual sensors output\ninformation through spikes at a high temporal resolution. By only encoding\nchanges in pixel intensity, they showcase a low-power consuming, low-latency\napproach to visual information sensing. To use this information for higher\nsensory tasks like object recognition and tracking, an essential simplification\nstep is the extraction and learning of features. An ideal feature descriptor\nmust be robust to changes involving (i) local transformations and (ii)\nre-appearances of a local event pattern. To that end, we propose a novel\nspatiotemporal feature representation learning algorithm based on slow feature\nanalysis (SFA). Using SFA, smoothly changing linear projections are learnt\nwhich are robust to local visual transformations. In order to determine if the\nfeatures can learn to be invariant to various visual transformations, feature\npoint tracking tasks are used for evaluation. Extensive experiments across two\ndatasets demonstrate the adaptability of the spatiotemporal feature learner to\ntranslation, scaling and rotational transformations of the feature points. More\nimportantly, we find that the obtained feature representations are able to\nexploit the high temporal resolution of such event-based cameras in generating\nbetter feature tracks.\n", "versions": [{"version": "v1", "created": "Sat, 16 Mar 2019 13:52:30 GMT"}], "update_date": "2019-03-19", "authors_parsed": [["Ghosh", "Rohan", ""], ["Gupta", "Anupam", ""], ["Tang", "Siyi", ""], ["Soares", "Alcimar", ""], ["Thakor", "Nitish", ""]]}, {"id": "1903.06946", "submitter": "Dominik Lorenz", "authors": "Dominik Lorenz, Leonard Bereska, Timo Milbich, Bj\\\"orn Ommer", "title": "Unsupervised Part-Based Disentangling of Object Shape and Appearance", "comments": "CVPR 2019 Oral", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large intra-class variation is the result of changes in multiple object\ncharacteristics. Images, however, only show the superposition of different\nvariable factors such as appearance or shape. Therefore, learning to\ndisentangle and represent these different characteristics poses a great\nchallenge, especially in the unsupervised case. Moreover, large object\narticulation calls for a flexible part-based model. We present an unsupervised\napproach for disentangling appearance and shape by learning parts consistently\nover all instances of a category. Our model for learning an object\nrepresentation is trained by simultaneously exploiting invariance and\nequivariance constraints between synthetically transformed images. Since no\npart annotation or prior information on an object class is required, the\napproach is applicable to arbitrary classes. We evaluate our approach on a wide\nrange of object categories and diverse tasks including pose prediction,\ndisentangled image synthesis, and video-to-video translation. The approach\noutperforms the state-of-the-art on unsupervised keypoint prediction and\ncompares favorably even against supervised approaches on the task of shape and\nappearance transfer.\n", "versions": [{"version": "v1", "created": "Sat, 16 Mar 2019 15:40:51 GMT"}, {"version": "v2", "created": "Wed, 24 Apr 2019 13:16:36 GMT"}, {"version": "v3", "created": "Mon, 17 Jun 2019 14:51:51 GMT"}], "update_date": "2019-06-18", "authors_parsed": [["Lorenz", "Dominik", ""], ["Bereska", "Leonard", ""], ["Milbich", "Timo", ""], ["Ommer", "Bj\u00f6rn", ""]]}, {"id": "1903.06949", "submitter": "Teofilo de Campos", "authors": "Luciano Walenty Xavier Cejnog, Roberto Marcondes Cesar Jr., Teofilo\n  Emidio de Campos and Valeria Meirelles Carril Elui", "title": "Hand range of motion evaluation for Rheumatoid Arthritis patients", "comments": "This preprint has been submitted to 4th IEEE International Conference\n  on Automatic Face and Gesture Recognition, FG2019. 5 pages, 7 figures, 1\n  table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a framework for dynamic evaluation of the fingers movements:\nflexion, extension, abduction and adduction. This framework estimates angle\nmeasurements from joints computed by a hand pose estimation algorithm using a\ndepth sensor (Realsense SR300). Given depth maps as input, our framework uses\nPose-REN, which is a state-of-art hand pose estimation method that estimates 3D\nhand joint positions using a deep convolutional neural network. The pose\nestimation algorithm runs in real-time, allowing users to visualise 3D skeleton\ntracking results at the same time as the depth images are acquired. Once 3D\njoint poses are obtained, our framework estimates a plane containing the wrist\nand MCP joints and measures flexion/extension and abduction/aduction angles by\napplying computational geometry operations with respect to this plane. We\nanalysed flexion and abduction movement patterns using real data, extracting\nthe movement trajectories. Our preliminary results show that this method allows\nan automatic discrimination of hands with Rheumatoid Arthritis (RA) and healthy\npatients. The angle between joints can be used as an indicative of current\nmovement capabilities and function. Although the measurements can be noisy and\nless accurate than those obtained statically through goniometry, the\nacquisition is much easier, non-invasive and patient-friendly, which shows the\npotential of our approach. The system can be used with and without orthosis.\nOur framework allows the acquisition of measurements with minimal intervention\nand significantly reduces the evaluation time.\n", "versions": [{"version": "v1", "created": "Sat, 16 Mar 2019 15:51:23 GMT"}], "update_date": "2019-03-19", "authors_parsed": [["Cejnog", "Luciano Walenty Xavier", ""], ["Cesar", "Roberto Marcondes", "Jr."], ["de Campos", "Teofilo Emidio", ""], ["Elui", "Valeria Meirelles Carril", ""]]}, {"id": "1903.06969", "submitter": "Teofilo de Campos", "authors": "Aloisio Dourado, Frederico Guth, Teofilo Emidio de Campos, Li Weigang", "title": "Domain adaptation for holistic skin detection", "comments": "11 pages, 10 figures, 6 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human skin detection in images is a widely studied topic of Computer Vision\nfor which it is commonly accepted that analysis of pixel color or local patches\nmay suffice. This is because skin regions appear to be relatively uniform and\nmany argue that there is a small chromatic variation among different samples.\nHowever, we found that there are strong biases in the datasets commonly used to\ntrain or tune skin detection methods. Furthermore, the lack of contextual\ninformation may hinder the performance of local approaches. In this paper we\npresent a comprehensive evaluation of holistic and local Convolutional Neural\nNetwork (CNN) approaches on in-domain and cross-domain experiments and compare\nwith state-of-the-art pixel-based approaches. We also propose a combination of\ninductive transfer learning and unsupervised domain adaptation methods, which\nare evaluated on different domains under several amounts of labelled data\navailability. We show a clear superiority of CNN over pixel-based approaches\neven without labelled training samples on the target domain. Furthermore, we\nprovide experimental support for the counter-intuitive superiority of holistic\nover local approaches for human skin detection.\n", "versions": [{"version": "v1", "created": "Sat, 16 Mar 2019 18:26:42 GMT"}, {"version": "v2", "created": "Sat, 28 Mar 2020 20:54:03 GMT"}], "update_date": "2020-03-31", "authors_parsed": [["Dourado", "Aloisio", ""], ["Guth", "Frederico", ""], ["de Campos", "Teofilo Emidio", ""], ["Weigang", "Li", ""]]}, {"id": "1903.06994", "submitter": "Peixi Xiong", "authors": "Peixi Xiong, Huayi Zhan, Xin Wang, Baivab Sinha, Ying Wu", "title": "Visual Query Answering by Entity-Attribute Graph Matching and Reasoning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual Query Answering (VQA) is of great significance in offering people\nconvenience: one can raise a question for details of objects, or high-level\nunderstanding about the scene, over an image. This paper proposes a novel\nmethod to address the VQA problem. In contrast to prior works, our method that\ntargets single scene VQA, replies on graph-based techniques and involves\nreasoning. In a nutshell, our approach is centered on three graphs. The first\ngraph, referred to as inference graph GI , is constructed via learning over\nlabeled data. The other two graphs, referred to as query graph Q and\nentity-attribute graph GEA, are generated from natural language query Qnl and\nimage Img, that are issued from users, respectively. As GEA often does not take\nsufficient information to answer Q, we develop techniques to infer missing\ninformation of GEA with GI . Based on GEA and Q, we provide techniques to find\nmatches of Q in GEA, as the answer of Qnl in Img. Unlike commonly used VQA\nmethods that are based on end-to-end neural networks, our graph-based method\nshows well-designed reasoning capability, and thus is highly interpretable. We\nalso create a dataset on soccer match (Soccer-VQA) with rich annotations. The\nexperimental results show that our approach outperforms the state-of-the-art\nmethod and has high potential for future investigation.\n", "versions": [{"version": "v1", "created": "Sat, 16 Mar 2019 21:58:04 GMT"}], "update_date": "2019-03-19", "authors_parsed": [["Xiong", "Peixi", ""], ["Zhan", "Huayi", ""], ["Wang", "Xin", ""], ["Sinha", "Baivab", ""], ["Wu", "Ying", ""]]}, {"id": "1903.06999", "submitter": "Yang Zheng", "authors": "Yang Zheng, Izzat H. Izzat, Shahrzad Ziaee", "title": "GFD-SSD: Gated Fusion Double SSD for Multispectral Pedestrian Detection", "comments": "10 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pedestrian detection is an essential task in autonomous driving research. In\naddition to typical color images, thermal images benefit the detection in dark\nenvironments. Hence, it is worthwhile to explore an integrated approach to take\nadvantage of both color and thermal images simultaneously. In this paper, we\npropose a novel approach to fuse color and thermal sensors using deep neural\nnetworks (DNN). Current state-of-the-art DNN object detectors vary from\ntwo-stage to one-stage mechanisms. Two-stage detectors, like Faster-RCNN,\nachieve higher accuracy, while one-stage detectors such as Single Shot Detector\n(SSD) demonstrate faster performance. To balance the trade-off, especially in\nthe consideration of autonomous driving applications, we investigate a fusion\nstrategy to combine two SSDs on color and thermal inputs. Traditional fusion\nmethods stack selected features from each channel and adjust their weights. In\nthis paper, we propose two variations of novel Gated Fusion Units (GFU), that\nlearn the combination of feature maps generated by the two SSD middle layers.\nLeveraging GFUs for the entire feature pyramid structure, we propose several\nmixed versions of both stack fusion and gated fusion. Experiments are conducted\non the KAIST multispectral pedestrian detection dataset. Our Gated Fusion\nDouble SSD (GFD-SSD) outperforms the stacked fusion and achieves the lowest\nmiss rate in the benchmark, at an inference speed that is two times faster than\nFaster-RCNN based fusion networks.\n", "versions": [{"version": "v1", "created": "Sat, 16 Mar 2019 22:55:47 GMT"}, {"version": "v2", "created": "Thu, 21 Mar 2019 22:48:57 GMT"}], "update_date": "2019-03-25", "authors_parsed": [["Zheng", "Yang", ""], ["Izzat", "Izzat H.", ""], ["Ziaee", "Shahrzad", ""]]}, {"id": "1903.07011", "submitter": "Hamid Tizhoosh", "authors": "Morteza Babaie, H.R. Tizhoosh", "title": "Deep Features for Tissue-Fold Detection in Histopathology Images", "comments": "Accepted for publication in the 15th European Congress on Digital\n  Pathology (ECDP 2019), University of Warwick, UK", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Whole slide imaging (WSI) refers to the digitization of a tissue specimen\nwhich enables pathologists to explore high-resolution images on a monitor\nrather than through a microscope. The formation of tissue folds occur during\ntissue processing. Their presence may not only cause out-of-focus digitization\nbut can also negatively affect the diagnosis in some cases. In this paper, we\nhave compared five pre-trained convolutional neural networks (CNNs) of\ndifferent depths as feature extractors to characterize tissue folds. We have\nalso explored common classifiers to discriminate folded tissue against the\nnormal tissue in hematoxylin and eosin (H\\&E) stained biopsy samples. In our\nexperiments, we manually select the folded area in roughly 2.5mm $\\times$ 2.5mm\npatches at $20$x magnification level as the training data. The ``DenseNet''\nwith 201 layers alongside an SVM classifier outperformed all other\nconfigurations. Based on the leave-one-out validation strategy, we achieved\n$96.3\\%$ accuracy, whereas with augmentation the accuracy increased to\n$97.2\\%$. We have tested the generalization of our method with five unseen WSIs\nfrom the NIH (National Cancer Institute) dataset. The accuracy for patch-wise\ndetection was $81\\%$. One folded patch within an image suffices to flag the\nentire specimen for visual inspection.\n", "versions": [{"version": "v1", "created": "Sun, 17 Mar 2019 01:25:10 GMT"}], "update_date": "2019-03-19", "authors_parsed": [["Babaie", "Morteza", ""], ["Tizhoosh", "H. R.", ""]]}, {"id": "1903.07013", "submitter": "Hamid Tizhoosh", "authors": "Wafa Chenni, Habib Herbi, Morteza Babaie, H.R.Tizhoosh", "title": "Patch Clustering for Representation of Histopathology Images", "comments": "Accepted for publication in the 15th European Congress on Digital\n  Pathology (ECDP 2019), University of Warwick, UK", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Whole Slide Imaging (WSI) has become an important topic during the last\ndecade. Even though significant progress in both medical image processing and\ncomputational resources has been achieved, there are still problems in WSI that\nneed to be solved. A major challenge is the scan size. The dimensions of\ndigitized tissue samples may exceed 100,000 by 100,000 pixels causing memory\nand efficiency obstacles for real-time processing. The main contribution of\nthis work is representing a WSI by selecting a small number of patches for\nalgorithmic processing (e.g., indexing and search). As a result, we reduced the\nsearch time and storage by various factors between ($50\\% - 90\\%$), while\nlosing only a few percentages in the patch retrieval accuracy. A\nself-organizing map (SOM) has been applied on local binary patterns (LBP) and\ndeep features of the KimiaPath24 dataset in order to cluster patches that share\nthe same characteristics. We used a Gaussian mixture model (GMM) to represent\neach class with a rather small ($10\\%-50\\%$) portion of patches. The results\nshowed that LBP features can outperform deep features. By selecting only $50\\%$\nof all patches after SOM clustering and GMM patch selection, we received $65\\%$\naccuracy for retrieval of the best match, while the maximum accuracy (using all\npatches) was $69\\%$.\n", "versions": [{"version": "v1", "created": "Sun, 17 Mar 2019 01:40:45 GMT"}], "update_date": "2019-03-19", "authors_parsed": [["Chenni", "Wafa", ""], ["Herbi", "Habib", ""], ["Babaie", "Morteza", ""], ["Tizhoosh", "H. R.", ""]]}, {"id": "1903.07027", "submitter": "James Gornet", "authors": "James Gornet, Kannan Umadevi Venkataraju, Arun Narasimhan, Nicholas\n  Turner, Kisuk Lee, H. Sebastian Seung, Pavel Osten, Uygar S\\\"umb\\\"ul", "title": "Reconstructing neuronal anatomy from whole-brain images", "comments": "2019 IEEE 16th International Symposium on Biomedical Imaging (ISBI\n  2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reconstructing multiple molecularly defined neurons from individual brains\nand across multiple brain regions can reveal organizational principles of the\nnervous system. However, high resolution imaging of the whole brain is a\ntechnically challenging and slow process. Recently, oblique light sheet\nmicroscopy has emerged as a rapid imaging method that can provide whole brain\nfluorescence microscopy at a voxel size of 0.4 by 0.4 by 2.5 cubic microns. On\nthe other hand, complex image artifacts due to whole-brain coverage produce\napparent discontinuities in neuronal arbors. Here, we present\nconnectivity-preserving methods and data augmentation strategies for supervised\nlearning of neuroanatomy from light microscopy using neural networks. We\nquantify the merit of our approach by implementing an end-to-end automated\ntracing pipeline. Lastly, we demonstrate a scalable, distributed implementation\nthat can reconstruct the large datasets that sub-micron whole-brain images\nproduce.\n", "versions": [{"version": "v1", "created": "Sun, 17 Mar 2019 05:06:29 GMT"}], "update_date": "2019-03-19", "authors_parsed": [["Gornet", "James", ""], ["Venkataraju", "Kannan Umadevi", ""], ["Narasimhan", "Arun", ""], ["Turner", "Nicholas", ""], ["Lee", "Kisuk", ""], ["Seung", "H. Sebastian", ""], ["Osten", "Pavel", ""], ["S\u00fcmb\u00fcl", "Uygar", ""]]}, {"id": "1903.07044", "submitter": "Saba Salehi", "authors": "Saba Salehi, Ahmad Mahmoodi-Aznaveh", "title": "Discriminating Original Region from Duplicated One in Copy-Move Forgery", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Since images are used as evidence in many cases, validation of digital images\nis essential. Copy-move forgery is a special kind of manipulation in which some\nparts of an image is copied and pasted into another part of the same image.\nVarious methods have been proposed to detect copy-move forgery, which have\nachieved promising results. In previous methods, a binary mask determining the\noriginal and forged region is presented as the final result. However, it is not\nspecified which part of the mask is the forged region. It should be noted that\ndiscriminating the original region from the duplicated one is not usually\nfeasible by human visual system(HVS). On the other hand, exact localizing the\nforged region can be helpful for automatic forgery detection especially in\ncombined forgeries. In real-world forgeries, some manipulations are performed\nin order to provide a visibly realistic scene. These modifications are usually\napplied on the boundary of the duplicated snippets. In this research, the\ntexture information of the border regions of both the original and copied\npatches have been statistically investigated. Based on this analysis, we\npropose a method to discriminated copied snippets from original ones. In order\nto validate our method, GRIP dataset is utilized since it contains more\nrealistic forged images which are not easily recognizable by HVS.\n", "versions": [{"version": "v1", "created": "Sun, 17 Mar 2019 08:52:55 GMT"}], "update_date": "2019-03-19", "authors_parsed": [["Salehi", "Saba", ""], ["Mahmoodi-Aznaveh", "Ahmad", ""]]}, {"id": "1903.07062", "submitter": "Massimiliano Mancini", "authors": "Massimiliano Mancini, Samuel Rota Bul\\`o, Barbara Caputo, Elisa Ricci", "title": "AdaGraph: Unifying Predictive and Continuous Domain Adaptation through\n  Graphs", "comments": "CVPR 2019 (oral)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ability to categorize is a cornerstone of visual intelligence, and a key\nfunctionality for artificial, autonomous visual machines. This problem will\nnever be solved without algorithms able to adapt and generalize across visual\ndomains. Within the context of domain adaptation and generalization, this paper\nfocuses on the predictive domain adaptation scenario, namely the case where no\ntarget data are available and the system has to learn to generalize from\nannotated source images plus unlabeled samples with associated metadata from\nauxiliary domains. Our contributionis the first deep architecture that tackles\npredictive domainadaptation, able to leverage over the information broughtby\nthe auxiliary domains through a graph. Moreover, we present a simple yet\neffective strategy that allows us to take advantage of the incoming target data\nat test time, in a continuous domain adaptation scenario. Experiments on three\nbenchmark databases support the value of our approach.\n", "versions": [{"version": "v1", "created": "Sun, 17 Mar 2019 11:56:45 GMT"}, {"version": "v2", "created": "Tue, 19 Mar 2019 10:33:00 GMT"}, {"version": "v3", "created": "Wed, 12 Jun 2019 23:01:27 GMT"}], "update_date": "2019-06-14", "authors_parsed": [["Mancini", "Massimiliano", ""], ["Bul\u00f2", "Samuel Rota", ""], ["Caputo", "Barbara", ""], ["Ricci", "Elisa", ""]]}, {"id": "1903.07067", "submitter": "Rohan Ghosh", "authors": "Rohan Ghosh, Anupam Gupta, Andrei Nakagawa, Alcimar Soares, Nitish\n  Thakor", "title": "Spatiotemporal Filtering for Event-Based Action Recognition", "comments": "Submitted to IEEE Transactions in Pattern Analysis and Machine\n  Intelligence", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we address the challenging problem of action recognition,\nusing event-based cameras. To recognise most gestural actions, often higher\ntemporal precision is required for sampling visual information. Actions are\ndefined by motion, and therefore, when using event-based cameras it is often\nunnecessary to re-sample the entire scene. Neuromorphic, event-based cameras\nhave presented an alternative to visual information acquisition by\nasynchronously time-encoding pixel intensity changes, through temporally\nprecise spikes (10 micro-second resolution), making them well equipped for\naction recognition. However, other challenges exist, which are intrinsic to\nevent-based imagers, such as higher signal-to-noise ratio, and a\nspatiotemporally sparse information. One option is to convert event-data into\nframes, but this could result in significant temporal precision loss. In this\nwork we introduce spatiotemporal filtering in the spike-event domain, as an\nalternative way of channeling spatiotemporal information through to a\nconvolutional neural network. The filters are local spatiotemporal weight\nmatrices, learned from the spike-event data, in an unsupervised manner. We find\nthat appropriate spatiotemporal filtering significantly improves CNN\nperformance beyond state-of-the-art on the event-based DVS Gesture dataset. On\nour newly recorded action recognition dataset, our method shows significant\nimprovement when compared with other, standard ways of generating the\nspatiotemporal filters.\n", "versions": [{"version": "v1", "created": "Sun, 17 Mar 2019 12:13:14 GMT"}], "update_date": "2019-03-19", "authors_parsed": [["Ghosh", "Rohan", ""], ["Gupta", "Anupam", ""], ["Nakagawa", "Andrei", ""], ["Soares", "Alcimar", ""], ["Thakor", "Nitish", ""]]}, {"id": "1903.07071", "submitter": "Hao Luo", "authors": "Hao Luo, Youzhi Gu, Xingyu Liao, Shenqi Lai, Wei Jiang", "title": "Bag of Tricks and A Strong Baseline for Deep Person Re-identification", "comments": "CVPR2019 Workshop. Code is available at\n  https://github.com/michuanhaohao/reid-strong-baseline", "journal-ref": "http://openaccess.thecvf.com/content_CVPRW_2019/papers/TRMTMCT/Luo_Bag_of_Tricks_and_a_Strong_Baseline_for_Deep_Person_CVPRW_2019_paper.pdf", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper explores a simple and efficient baseline for person\nre-identification (ReID). Person re-identification (ReID) with deep neural\nnetworks has made progress and achieved high performance in recent years.\nHowever, many state-of-the-arts methods design complex network structure and\nconcatenate multi-branch features. In the literature, some effective training\ntricks are briefly appeared in several papers or source codes. This paper will\ncollect and evaluate these effective training tricks in person ReID. By\ncombining these tricks together, the model achieves 94.5% rank-1 and 85.9% mAP\non Market1501 with only using global features. Our codes and models are\navailable at https://github.com/michuanhaohao/reid-strong-baseline.\n", "versions": [{"version": "v1", "created": "Sun, 17 Mar 2019 12:35:55 GMT"}, {"version": "v2", "created": "Tue, 26 Mar 2019 07:45:49 GMT"}, {"version": "v3", "created": "Fri, 19 Apr 2019 07:24:53 GMT"}], "update_date": "2019-06-19", "authors_parsed": [["Luo", "Hao", ""], ["Gu", "Youzhi", ""], ["Liao", "Xingyu", ""], ["Lai", "Shenqi", ""], ["Jiang", "Wei", ""]]}, {"id": "1903.07072", "submitter": "Hao Luo", "authors": "Hao Luo, Xing Fan, Chi Zhang and Wei Jiang", "title": "STNReID : Deep Convolutional Networks with Pairwise Spatial Transformer\n  Networks for Partial Person Re-identification", "comments": "Accepted by IEEE Transactions on Multimedia", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Partial person re-identification (ReID) is a challenging task because only\npartial information of person images is available for matching target persons.\nFew studies, especially on deep learning, have focused on matching partial\nperson images with holistic person images. This study presents a novel deep\npartial ReID framework based on pairwise spatial transformer networks\n(STNReID), which can be trained on existing holistic person datasets. STNReID\nincludes a spatial transformer network (STN) module and a ReID module. The STN\nmodule samples an affined image (a semantically corresponding patch) from the\nholistic image to match the partial image. The ReID module extracts the\nfeatures of the holistic, partial, and affined images. Competition (or\nconfrontation) is observed between the STN module and the ReID module, and\ntwo-stage training is applied to acquire a strong STNReID for partial ReID.\nExperimental results show that our STNReID obtains 66.7% and 54.6% rank-1\naccuracies on partial ReID and partial iLIDS datasets, respectively. These\nvalues are at par with those obtained with state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Sun, 17 Mar 2019 12:36:08 GMT"}, {"version": "v2", "created": "Tue, 7 Jan 2020 10:31:00 GMT"}], "update_date": "2020-01-08", "authors_parsed": [["Luo", "Hao", ""], ["Fan", "Xing", ""], ["Zhang", "Chi", ""], ["Jiang", "Wei", ""]]}, {"id": "1903.07145", "submitter": "Dejan Azinovi\\'c", "authors": "Dejan Azinovi\\'c, Tzu-Mao Li, Anton Kaplanyan, Matthias Nie{\\ss}ner", "title": "Inverse Path Tracing for Joint Material and Lighting Estimation", "comments": "CVPR'19 (Oral); Video: https://youtu.be/nC_t0t9u6ws", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern computer vision algorithms have brought significant advancement to 3D\ngeometry reconstruction. However, illumination and material reconstruction\nremain less studied, with current approaches assuming very simplified models\nfor materials and illumination. We introduce Inverse Path Tracing, a novel\napproach to jointly estimate the material properties of objects and light\nsources in indoor scenes by using an invertible light transport simulation. We\nassume a coarse geometry scan, along with corresponding images and camera\nposes. The key contribution of this work is an accurate and simultaneous\nretrieval of light sources and physically based material properties (e.g.,\ndiffuse reflectance, specular reflectance, roughness, etc.) for the purpose of\nediting and re-rendering the scene under new conditions. To this end, we\nintroduce a novel optimization method using a differentiable Monte Carlo\nrenderer that computes derivatives with respect to the estimated unknown\nillumination and material properties. This enables joint optimization for\nphysically correct light transport and material models using a tailored\nstochastic gradient descent.\n", "versions": [{"version": "v1", "created": "Sun, 17 Mar 2019 19:06:31 GMT"}], "update_date": "2019-03-19", "authors_parsed": [["Azinovi\u0107", "Dejan", ""], ["Li", "Tzu-Mao", ""], ["Kaplanyan", "Anton", ""], ["Nie\u00dfner", "Matthias", ""]]}, {"id": "1903.07146", "submitter": "Remi Giraud", "authors": "R\\'emi Giraud, Vinh-Thong Ta, Nicolas Papadakis", "title": "Robust Shape Regularity Criteria for Superpixel Evaluation", "comments": "International Conference on Image Processing 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Regular decompositions are necessary for most superpixel-based object\nrecognition or tracking applications. So far in the literature, the regularity\nor compactness of a superpixel shape is mainly measured by its circularity. In\nthis work, we first demonstrate that such measure is not adapted for superpixel\nevaluation, since it does not directly express regularity but circular\nappearance. Then, we propose a new metric that considers several shape\nregularity aspects: convexity, balanced repartition, and contour smoothness.\nFinally, we demonstrate that our measure is robust to scale and noise and\nenables to more relevantly compare superpixel methods.\n", "versions": [{"version": "v1", "created": "Sun, 17 Mar 2019 19:08:39 GMT"}], "update_date": "2019-03-19", "authors_parsed": [["Giraud", "R\u00e9mi", ""], ["Ta", "Vinh-Thong", ""], ["Papadakis", "Nicolas", ""]]}, {"id": "1903.07149", "submitter": "Remi Giraud", "authors": "R\\'emi Giraud, Vinh-Thong Ta, Nicolas Papadakis", "title": "SCALP: Superpixels with Contour Adherence using Linear Path", "comments": "International Conference on Pattern Recognition (ICPR) 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Superpixel decomposition methods are generally used as a pre-processing step\nto speed up image processing tasks. They group the pixels of an image into\nhomogeneous regions while trying to respect existing contours. For all\nstate-of-the-art superpixel decomposition methods, a trade-off is made between\n1) computational time, 2) adherence to image contours and 3) regularity and\ncompactness of the decomposition. In this paper, we propose a fast method to\ncompute Superpixels with Contour Adherence using Linear Path (SCALP) in an\niterative clustering framework. The distance computed when trying to associate\na pixel to a superpixel during the clustering is enhanced by considering the\nlinear path to the superpixel barycenter. The proposed framework produces\nregular and compact superpixels that adhere to the image contours. We provide a\ndetailed evaluation of SCALP on the standard Berkeley Segmentation Dataset. The\nobtained results outperform state-of-the-art methods in terms of standard\nsuperpixel and contour detection metrics.\n", "versions": [{"version": "v1", "created": "Sun, 17 Mar 2019 19:23:00 GMT"}], "update_date": "2019-03-19", "authors_parsed": [["Giraud", "R\u00e9mi", ""], ["Ta", "Vinh-Thong", ""], ["Papadakis", "Nicolas", ""]]}, {"id": "1903.07154", "submitter": "Dipan Pal", "authors": "Raied Aljadaany, Dipan K. Pal and Marios Savvides", "title": "Proximal Splitting Networks for Image Restoration", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image restoration problems are typically ill-posed requiring the design of\nsuitable priors. These priors are typically hand-designed and are fully\ninstantiated throughout the process. In this paper, we introduce a novel\nframework for handling inverse problems related to image restoration based on\nelements from the half quadratic splitting method and proximal operators.\nModeling the proximal operator as a convolutional network, we defined an\nimplicit prior on the image space as a function class during training. This is\nin contrast to the common practice in literature of having the prior to be\nfixed and fully instantiated even during training stages. Further, we allow\nthis proximal operator to be tuned differently for each iteration which greatly\nincreases modeling capacity and allows us to reduce the number of iterations by\nan order of magnitude as compared to other approaches. Our final network is an\nend-to-end one whose run time matches the previous fastest algorithms while\noutperforming them in recovery fidelity on two image restoration tasks. Indeed,\nwe find our approach achieves state-of-the-art results on benchmarks in image\ndenoising and image super resolution while recovering more complex and finer\ndetails.\n", "versions": [{"version": "v1", "created": "Sun, 17 Mar 2019 19:36:52 GMT"}], "update_date": "2019-03-19", "authors_parsed": [["Aljadaany", "Raied", ""], ["Pal", "Dipan K.", ""], ["Savvides", "Marios", ""]]}, {"id": "1903.07162", "submitter": "Remi Giraud", "authors": "R\\'emi Giraud, Vinh-Thong Ta, Nicolas Papadakis", "title": "Evaluation Framework of Superpixel Methods with a Global Regularity\n  Measure", "comments": "Journal of Electronic Imaging (JEI), 2017 Special issue on\n  Superpixels for Image Processing and Computer Vision", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the superpixel literature, the comparison of state-of-the-art methods can\nbe biased by the non-robustness of some metrics to decomposition aspects, such\nas the superpixel scale. Moreover, most recent decomposition methods allow to\nset a shape regularity parameter, which can have a substantial impact on the\nmeasured performances. In this paper, we introduce an evaluation framework,\nthat aims to unify the comparison process of superpixel methods. We investigate\nthe limitations of existing metrics, and propose to evaluate each of the three\ncore decomposition aspects: color homogeneity, respect of image objects and\nshape regularity. To measure the regularity aspect, we propose a new global\nregularity measure (GR), which addresses the non-robustness of state-of-the-art\nmetrics. We evaluate recent superpixel methods with these criteria, at several\nsuperpixel scales and regularity levels. The proposed framework reduces the\nbias in the comparison process of state-of-the-art superpixel methods. Finally,\nwe demonstrate that the proposed GR measure is correlated with the performances\nof various applications.\n", "versions": [{"version": "v1", "created": "Sun, 17 Mar 2019 20:29:55 GMT"}], "update_date": "2019-03-19", "authors_parsed": [["Giraud", "R\u00e9mi", ""], ["Ta", "Vinh-Thong", ""], ["Papadakis", "Nicolas", ""]]}, {"id": "1903.07165", "submitter": "Remi Giraud", "authors": "R\\'emi Giraud, Vinh-Thong Ta, Nicolas Papadakis, Jos\\'e V. Manj\\'on,\n  D. Louis Collins, Pierrick Coup\\'e, Alzheimer's Disease Neuroimaging\n  Initiative", "title": "An Optimized PatchMatch for Multi-scale and Multi-feature Label Fusion", "comments": "Neuroimage 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic segmentation methods are important tools for quantitative analysis\nof Magnetic Resonance Images (MRI). Recently, patch-based label fusion\napproaches have demonstrated state-of-the-art segmentation accuracy. In this\npaper, we introduce a new patch-based label fusion framework to perform\nsegmentation of anatomical structures. The proposed approach uses an Optimized\nPAtchMatch Label fusion (OPAL) strategy that drastically reduces the\ncomputation time required for the search of similar patches. The reduced\ncomputation time of OPAL opens the way for new strategies and facilitates\nprocessing on large databases. In this paper, we investigate new perspectives\noffered by OPAL, by introducing a new multi-scale and multi-feature framework.\nDuring our validation on hippocampus segmentation we use two datasets: young\nadults in the ICBM cohort and elderly adults in the EADC-ADNI dataset. For\nboth, OPAL is compared to state-of-the-art methods. Results show that OPAL\nobtained the highest median Dice coefficient (89.9% for ICBM and 90.1% for\nEADC-ADNI). Moreover, in both cases, OPAL produced a segmentation accuracy\nsimilar to inter-expert variability. On the EADC-ADNI dataset, we compare the\nhippocampal volumes obtained by manual and automatic segmentation. The volumes\nappear to be highly correlated that enables to perform more accurate separation\nof pathological populations.\n", "versions": [{"version": "v1", "created": "Sun, 17 Mar 2019 20:40:09 GMT"}], "update_date": "2019-03-19", "authors_parsed": [["Giraud", "R\u00e9mi", ""], ["Ta", "Vinh-Thong", ""], ["Papadakis", "Nicolas", ""], ["Manj\u00f3n", "Jos\u00e9 V.", ""], ["Collins", "D. Louis", ""], ["Coup\u00e9", "Pierrick", ""], ["Initiative", "Alzheimer's Disease Neuroimaging", ""]]}, {"id": "1903.07169", "submitter": "Remi Giraud", "authors": "R\\'emi Giraud, Vinh-Thong Ta, Aur\\'elie Bugeau, Pierrick Coup\\'e,\n  Nicolas Papadakis", "title": "SuperPatchMatch: an Algorithm for Robust Correspondences using\n  Superpixel Patches", "comments": "IEEE Transactions on Image Processing (TIP), 2017 Selected for\n  presentation at IEEE International Conference on Image Processing (ICIP) 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Superpixels have become very popular in many computer vision applications.\nNevertheless, they remain underexploited since the superpixel decomposition may\nproduce irregular and non stable segmentation results due to the dependency to\nthe image content. In this paper, we first introduce a novel structure, a\nsuperpixel-based patch, called SuperPatch. The proposed structure, based on\nsuperpixel neighborhood, leads to a robust descriptor since spatial information\nis naturally included. The generalization of the PatchMatch method to\nSuperPatches, named SuperPatchMatch, is introduced. Finally, we propose a\nframework to perform fast segmentation and labeling from an image database, and\ndemonstrate the potential of our approach since we outperform, in terms of\ncomputational cost and accuracy, the results of state-of-the-art methods on\nboth face labeling and medical image segmentation.\n", "versions": [{"version": "v1", "created": "Sun, 17 Mar 2019 21:08:38 GMT"}], "update_date": "2019-03-19", "authors_parsed": [["Giraud", "R\u00e9mi", ""], ["Ta", "Vinh-Thong", ""], ["Bugeau", "Aur\u00e9lie", ""], ["Coup\u00e9", "Pierrick", ""], ["Papadakis", "Nicolas", ""]]}, {"id": "1903.07173", "submitter": "Mostafa Mehdipour Ghazi", "authors": "Mostafa Mehdipour Ghazi, Mads Nielsen, Akshay Pai, M. Jorge Cardoso,\n  Marc Modat, Sebastien Ourselin, Lauge S{\\o}rensen", "title": "Training recurrent neural networks robust to incomplete data:\n  application to Alzheimer's disease progression modeling", "comments": "arXiv admin note: substantial text overlap with arXiv:1808.05500", "journal-ref": "Medical Image Analysis, Volume 53, Pages 39-46, 2019", "doi": "10.1016/j.media.2019.01.004", "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Disease progression modeling (DPM) using longitudinal data is a challenging\nmachine learning task. Existing DPM algorithms neglect temporal dependencies\namong measurements, make parametric assumptions about biomarker trajectories,\ndo not model multiple biomarkers jointly, and need an alignment of subjects'\ntrajectories. In this paper, recurrent neural networks (RNNs) are utilized to\naddress these issues. However, in many cases, longitudinal cohorts contain\nincomplete data, which hinders the application of standard RNNs and requires a\npre-processing step such as imputation of the missing values. Instead, we\npropose a generalized training rule for the most widely used RNN architecture,\nlong short-term memory (LSTM) networks, that can handle both missing predictor\nand target values. The proposed LSTM algorithm is applied to model the\nprogression of Alzheimer's disease (AD) using six volumetric magnetic resonance\nimaging (MRI) biomarkers, i.e., volumes of ventricles, hippocampus, whole\nbrain, fusiform, middle temporal gyrus, and entorhinal cortex, and it is\ncompared to standard LSTM networks with data imputation and a parametric,\nregression-based DPM method. The results show that the proposed algorithm\nachieves a significantly lower mean absolute error (MAE) than the alternatives\nwith p < 0.05 using Wilcoxon signed rank test in predicting values of almost\nall of the MRI biomarkers. Moreover, a linear discriminant analysis (LDA)\nclassifier applied to the predicted biomarker values produces a significantly\nlarger AUC of 0.90 vs. at most 0.84 with p < 0.001 using McNemar's test for\nclinical diagnosis of AD. Inspection of MAE curves as a function of the amount\nof missing data reveals that the proposed LSTM algorithm achieves the best\nperformance up until more than 74% missing values. Finally, it is illustrated\nhow the method can successfully be applied to data with varying time intervals.\n", "versions": [{"version": "v1", "created": "Sun, 17 Mar 2019 21:14:33 GMT"}], "update_date": "2019-03-19", "authors_parsed": [["Ghazi", "Mostafa Mehdipour", ""], ["Nielsen", "Mads", ""], ["Pai", "Akshay", ""], ["Cardoso", "M. Jorge", ""], ["Modat", "Marc", ""], ["Ourselin", "Sebastien", ""], ["S\u00f8rensen", "Lauge", ""]]}, {"id": "1903.07189", "submitter": "Orcun Goksel", "authors": "Yuanhao Gong and Orcun Goksel", "title": "Weighted Mean Curvature", "comments": "12 pages", "journal-ref": "Signal Processing 164 (2019) 329-339", "doi": "10.1016/j.sigpro.2019.06.020", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In image processing tasks, spatial priors are essential for robust\ncomputations, regularization, algorithmic design and Bayesian inference. In\nthis paper, we introduce weighted mean curvature (WMC) as a novel image prior\nand present an efficient computation scheme for its discretization in practical\nimage processing applications. We first demonstrate the favorable properties of\nWMC, such as sampling invariance, scale invariance, and contrast invariance\nwith Gaussian noise model; and we show the relation of WMC to area\nregularization. We further propose an efficient computation scheme for\ndiscretized WMC, which is demonstrated herein to process over 33.2\ngiga-pixels/second on GPU. This scheme yields itself to a convolutional neural\nnetwork representation. Finally, WMC is evaluated on synthetic and real images,\nshowing its superiority quantitatively to total-variation and mean curvature.\n", "versions": [{"version": "v1", "created": "Sun, 17 Mar 2019 22:28:44 GMT"}, {"version": "v2", "created": "Tue, 7 May 2019 09:26:33 GMT"}], "update_date": "2020-07-23", "authors_parsed": [["Gong", "Yuanhao", ""], ["Goksel", "Orcun", ""]]}, {"id": "1903.07190", "submitter": "Dan Casas", "authors": "Igor Santesteban, Miguel A. Otaduy, Dan Casas", "title": "Learning-Based Animation of Clothing for Virtual Try-On", "comments": "Eurographics 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a learning-based clothing animation method for highly\nefficient virtual try-on simulation. Given a garment, we preprocess a rich\ndatabase of physically-based dressed character simulations, for multiple body\nshapes and animations. Then, using this database, we train a learning-based\nmodel of cloth drape and wrinkles, as a function of body shape and dynamics. We\npropose a model that separates global garment fit, due to body shape, from\nlocal garment wrinkles, due to both pose dynamics and body shape. We use a\nrecurrent neural network to regress garment wrinkles, and we achieve highly\nplausible nonlinear effects, in contrast to the blending artifacts suffered by\nprevious methods. At runtime, dynamic virtual try-on animations are produced in\njust a few milliseconds for garments with thousands of triangles. We show\nqualitative and quantitative analysis of results\n", "versions": [{"version": "v1", "created": "Sun, 17 Mar 2019 22:43:28 GMT"}], "update_date": "2019-03-19", "authors_parsed": [["Santesteban", "Igor", ""], ["Otaduy", "Miguel A.", ""], ["Casas", "Dan", ""]]}, {"id": "1903.07193", "submitter": "Remi Giraud", "authors": "R\\'emi Giraud, Vinh-Thong Ta, Nicolas Papadakis", "title": "Robust superpixels using color and contour features along linear path", "comments": "Computer Vision and Image Understanding (CVIU), 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Superpixel decomposition methods are widely used in computer vision and image\nprocessing applications. By grouping homogeneous pixels, the accuracy can be\nincreased and the decrease of the number of elements to process can drastically\nreduce the computational burden. For most superpixel methods, a trade-off is\ncomputed between 1) color homogeneity, 2) adherence to the image contours and\n3) shape regularity of the decomposition. In this paper, we propose a framework\nthat jointly enforces all these aspects and provides accurate and regular\nSuperpixels with Contour Adherence using Linear Path (SCALP). During the\ndecomposition, we propose to consider color features along the linear path\nbetween the pixel and the corresponding superpixel barycenter. A contour prior\nis also used to prevent the crossing of image boundaries when associating a\npixel to a superpixel. Finally, in order to improve the decomposition accuracy\nand the robustness to noise, we propose to integrate the pixel neighborhood\ninformation, while preserving the same computational complexity. SCALP is\nextensively evaluated on standard segmentation dataset, and the obtained\nresults outperform the ones of the state-of-the-art methods. SCALP is also\nextended for supervoxel decomposition on MRI images.\n", "versions": [{"version": "v1", "created": "Sun, 17 Mar 2019 23:00:13 GMT"}], "update_date": "2019-03-19", "authors_parsed": [["Giraud", "R\u00e9mi", ""], ["Ta", "Vinh-Thong", ""], ["Papadakis", "Nicolas", ""]]}, {"id": "1903.07209", "submitter": "Alexander Wong", "authors": "Alexander Wong, Zhong Qiu Lin, and Brendan Chwyl", "title": "AttoNets: Compact and Efficient Deep Neural Networks for the Edge via\n  Human-Machine Collaborative Design", "comments": "15 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While deep neural networks have achieved state-of-the-art performance across\na large number of complex tasks, it remains a big challenge to deploy such\nnetworks for practical, on-device edge scenarios such as on mobile devices,\nconsumer devices, drones, and vehicles. In this study, we take a deeper\nexploration into a human-machine collaborative design approach for creating\nhighly efficient deep neural networks through a synergy between principled\nnetwork design prototyping and machine-driven design exploration. The efficacy\nof human-machine collaborative design is demonstrated through the creation of\nAttoNets, a family of highly efficient deep neural networks for on-device edge\ndeep learning. Each AttoNet possesses a human-specified network-level\nmacro-architecture comprising of custom modules with unique machine-designed\nmodule-level macro-architecture and micro-architecture designs, all driven by\nhuman-specified design requirements. Experimental results for the task of\nobject recognition showed that the AttoNets created via human-machine\ncollaborative design has significantly fewer parameters and computational costs\nthan state-of-the-art networks designed for efficiency while achieving\nnoticeably higher accuracy (with the smallest AttoNet achieving ~1.8% higher\naccuracy while requiring ~10x fewer multiply-add operations and parameters than\nMobileNet-V1). Furthermore, the efficacy of the AttoNets is demonstrated for\nthe task of instance-level object segmentation and object detection, where an\nAttoNet-based Mask R-CNN network was constructed with significantly fewer\nparameters and computational costs (~5x fewer multiply-add operations and ~2x\nfewer parameters) than a ResNet-50 based Mask R-CNN network.\n", "versions": [{"version": "v1", "created": "Mon, 18 Mar 2019 00:16:04 GMT"}, {"version": "v2", "created": "Mon, 15 Apr 2019 16:05:35 GMT"}], "update_date": "2019-04-16", "authors_parsed": [["Wong", "Alexander", ""], ["Lin", "Zhong Qiu", ""], ["Chwyl", "Brendan", ""]]}, {"id": "1903.07221", "submitter": "William Johnson", "authors": "William R. Johnson, Ajmal Mian, Mark A. Robinson, Jasper Verheul,\n  David G. Lloyd, Jacqueline A. Alderson", "title": "Multidimensional ground reaction forces and moments from wearable sensor\n  accelerations via deep learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Monitoring athlete internal workload exposure, including prevention of\ncatastrophic non-contact knee injuries, relies on the existence of a custom\nearly-warning detection system. This system must be able to estimate accurate,\nreliable, and valid musculoskeletal joint loads, for sporting maneuvers in near\nreal-time and during match play. However, current methods are constrained to\nlaboratory instrumentation, are labor and cost intensive, and require highly\ntrained specialist knowledge, thereby limiting their ecological validity and\nwider deployment. An informative next step towards this goal would be a new\nmethod to obtain ground kinetics in the field. Here we show that kinematic data\nobtained from wearable sensor accelerometers, in lieu of embedded force\nplatforms, can leverage recent supervised learning techniques to predict near\nreal-time multidimensional ground reaction forces and moments (GRF/M).\nCompeting convolutional neural network (CNN) deep learning models were trained\nusing laboratory-derived stance phase GRF/M data and simulated sensor\naccelerations for running and sidestepping maneuvers derived from nearly half a\nmillion legacy motion trials. Then, predictions were made from each model\ndriven by five sensor accelerations recorded during independent\ninter-laboratory data capture sessions. The proposed deep learning workbench\nachieved correlations to ground truth, by maximum discrete GRF component, of\nvertical Fz 0.97, anterior Fy 0.96 (both running), and lateral Fx 0.87\n(sidestepping), with the strongest mean recorded across GRF components 0.89,\nand for GRM 0.65 (both sidestepping). These best-case correlations indicate the\nplausibility of the approach although the range of results was disappointing.\nThe goal to accurately estimate near real-time on-field GRF/M will be improved\nby the lessons learned in this study [truncated].\n", "versions": [{"version": "v1", "created": "Mon, 18 Mar 2019 01:24:08 GMT"}, {"version": "v2", "created": "Mon, 1 Apr 2019 00:33:19 GMT"}, {"version": "v3", "created": "Fri, 3 Jul 2020 22:57:54 GMT"}], "update_date": "2020-07-07", "authors_parsed": [["Johnson", "William R.", ""], ["Mian", "Ajmal", ""], ["Robinson", "Mark A.", ""], ["Verheul", "Jasper", ""], ["Lloyd", "David G.", ""], ["Alderson", "Jacqueline A.", ""]]}, {"id": "1903.07224", "submitter": "Zhiqiang Gong", "authors": "Zhiqiang Gong, Ping Zhong, Weidong Hu, Fang Liu, and Bingwei Hui", "title": "An End-to-End Joint Unsupervised Learning of Deep Model and\n  Pseudo-Classes for Remote Sensing Scene Representation", "comments": "Submitted to IJCNN 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work develops a novel end-to-end deep unsupervised learning method based\non convolutional neural network (CNN) with pseudo-classes for remote sensing\nscene representation. First, we introduce center points as the centers of the\npseudo classes and the training samples can be allocated with pseudo labels\nbased on the center points. Therefore, the CNN model, which is used to extract\nfeatures from the scenes, can be trained supervised with the pseudo labels.\nMoreover, a pseudo-center loss is developed to decrease the variance between\nthe samples and the corresponding pseudo center point. The pseudo-center loss\nis important since it can update both the center points with the training\nsamples and the CNN model with the center points in the training process\nsimultaneously. Finally, joint learning of the pseudo-center loss and the\npseudo softmax loss which is formulated with the samples and the pseudo labels\nis developed for unsupervised remote sensing scene representation to obtain\ndiscriminative representations from the scenes. Experiments are conducted over\ntwo commonly used remote sensing scene datasets to validate the effectiveness\nof the proposed method and the experimental results show the superiority of the\nproposed method when compared with other state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Mon, 18 Mar 2019 01:41:33 GMT"}], "update_date": "2019-03-19", "authors_parsed": [["Gong", "Zhiqiang", ""], ["Zhong", "Ping", ""], ["Hu", "Weidong", ""], ["Liu", "Fang", ""], ["Hui", "Bingwei", ""]]}, {"id": "1903.07243", "submitter": "Changzhe Jiao", "authors": "Wenshuai Chen, Shuiping Gou, Xinlin Wang, Licheng Jiao, Changzhe Jiao,\n  Alina Zare", "title": "Complex Scene Classification of PolSAR Imagery based on a Self-paced\n  Learning Approach", "comments": null, "journal-ref": null, "doi": "10.1109/JSTARS.2018.2879440", "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing polarimetric synthetic aperture radar (PolSAR) image classification\nmethods cannot achieve satisfactory performance on complex scenes characterized\nby several types of land cover with significant levels of noise or similar\nscattering properties across land cover types. Hence, we propose a supervised\nclassification method aimed at constructing a classifier based on self-paced\nlearning (SPL). SPL has been demonstrated to be effective at dealing with\ncomplex data while providing classifier. In this paper, a novel Support Vector\nMachine (SVM) algorithm based on SPL with neighborhood constraints (SVM_SPLNC)\nis proposed. The proposed method leverages the easiest samples first to obtain\nan initial parameter vector. Then, more complex samples are gradually\nincorporated to update the parameter vector iteratively. Moreover, neighborhood\nconstraints are introduced during the training process to further improve\nperformance. Experimental results on three real PolSAR images show that the\nproposed method performs well on complex scenes.\n", "versions": [{"version": "v1", "created": "Mon, 18 Mar 2019 03:31:02 GMT"}], "update_date": "2019-03-19", "authors_parsed": [["Chen", "Wenshuai", ""], ["Gou", "Shuiping", ""], ["Wang", "Xinlin", ""], ["Jiao", "Licheng", ""], ["Jiao", "Changzhe", ""], ["Zare", "Alina", ""]]}, {"id": "1903.07254", "submitter": "Jiaxin Cheng", "authors": "Jiaxin Cheng, Yue Wu, Wael Abd-Almageed, Premkumar Natarajan", "title": "QATM: Quality-Aware Template Matching For Deep Learning", "comments": "Accepted as CVPR 2019 paper. Camera ready version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Finding a template in a search image is one of the core problems many\ncomputer vision, such as semantic image semantic, image-to-GPS verification\n\\etc. We propose a novel quality-aware template matching method, QATM, which is\nnot only used as a standalone template matching algorithm, but also a trainable\nlayer that can be easily embedded into any deep neural network. Specifically,\nwe assess the quality of a matching pair using soft-ranking among all matching\npairs, and thus different matching scenarios such as 1-to-1, 1-to-many, and\nmany-to-many will be all reflected to different values. Our extensive\nevaluation on classic template matching benchmarks and deep learning tasks\ndemonstrate the effectiveness of QATM. It not only outperforms state-of-the-art\ntemplate matching methods when used alone, but also largely improves existing\ndeep network solutions.\n", "versions": [{"version": "v1", "created": "Mon, 18 Mar 2019 04:58:51 GMT"}, {"version": "v2", "created": "Tue, 9 Apr 2019 23:11:15 GMT"}], "update_date": "2019-04-11", "authors_parsed": [["Cheng", "Jiaxin", ""], ["Wu", "Yue", ""], ["Abd-Almageed", "Wael", ""], ["Natarajan", "Premkumar", ""]]}, {"id": "1903.07256", "submitter": "Jia-Xing Zhong", "authors": "Jia-Xing Zhong, Nannan Li, Weijie Kong, Shan Liu, Thomas H. Li, Ge Li", "title": "Graph Convolutional Label Noise Cleaner: Train a Plug-and-play Action\n  Classifier for Anomaly Detection", "comments": "To appear in CVPR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video anomaly detection under weak labels is formulated as a typical\nmultiple-instance learning problem in previous works. In this paper, we provide\na new perspective, i.e., a supervised learning task under noisy labels. In such\na viewpoint, as long as cleaning away label noise, we can directly apply fully\nsupervised action classifiers to weakly supervised anomaly detection, and take\nmaximum advantage of these well-developed classifiers. For this purpose, we\ndevise a graph convolutional network to correct noisy labels. Based upon\nfeature similarity and temporal consistency, our network propagates supervisory\nsignals from high-confidence snippets to low-confidence ones. In this manner,\nthe network is capable of providing cleaned supervision for action classifiers.\nDuring the test phase, we only need to obtain snippet-wise predictions from the\naction classifier without any extra post-processing. Extensive experiments on 3\ndatasets at different scales with 2 types of action classifiers demonstrate the\nefficacy of our method. Remarkably, we obtain the frame-level AUC score of\n82.12% on UCF-Crime.\n", "versions": [{"version": "v1", "created": "Mon, 18 Mar 2019 05:07:09 GMT"}], "update_date": "2019-03-19", "authors_parsed": [["Zhong", "Jia-Xing", ""], ["Li", "Nannan", ""], ["Kong", "Weijie", ""], ["Liu", "Shan", ""], ["Li", "Thomas H.", ""], ["Li", "Ge", ""]]}, {"id": "1903.07291", "submitter": "Ming-Yu Liu", "authors": "Taesung Park, Ming-Yu Liu, Ting-Chun Wang, Jun-Yan Zhu", "title": "Semantic Image Synthesis with Spatially-Adaptive Normalization", "comments": "Accepted as a CVPR 2019 oral paper", "journal-ref": "CVPR 2019", "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose spatially-adaptive normalization, a simple but effective layer for\nsynthesizing photorealistic images given an input semantic layout. Previous\nmethods directly feed the semantic layout as input to the deep network, which\nis then processed through stacks of convolution, normalization, and\nnonlinearity layers. We show that this is suboptimal as the normalization\nlayers tend to ``wash away'' semantic information. To address the issue, we\npropose using the input layout for modulating the activations in normalization\nlayers through a spatially-adaptive, learned transformation. Experiments on\nseveral challenging datasets demonstrate the advantage of the proposed method\nover existing approaches, regarding both visual fidelity and alignment with\ninput layouts. Finally, our model allows user control over both semantic and\nstyle. Code is available at https://github.com/NVlabs/SPADE .\n", "versions": [{"version": "v1", "created": "Mon, 18 Mar 2019 08:12:23 GMT"}, {"version": "v2", "created": "Tue, 5 Nov 2019 15:41:27 GMT"}], "update_date": "2019-11-06", "authors_parsed": [["Park", "Taesung", ""], ["Liu", "Ming-Yu", ""], ["Wang", "Ting-Chun", ""], ["Zhu", "Jun-Yan", ""]]}, {"id": "1903.07296", "submitter": "Zhaokang Chen", "authors": "Zhaokang Chen and Bertram E. Shi", "title": "Appearance-Based Gaze Estimation Using Dilated-Convolutions", "comments": "16 pages, 7 figures. To appear in ACCV2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Appearance-based gaze estimation has attracted more and more attention\nbecause of its wide range of applications. The use of deep convolutional neural\nnetworks has improved the accuracy significantly. In order to improve the\nestimation accuracy further, we focus on extracting better features from eye\nimages. Relatively large changes in gaze angles may result in relatively small\nchanges in eye appearance. We argue that current architectures for gaze\nestimation may not be able to capture such small changes, as they apply\nmultiple pooling layers or other downsampling layers so that the spatial\nresolution of the high-level layers is reduced significantly. To evaluate\nwhether the use of features extracted at high resolution can benefit gaze\nestimation, we adopt dilated-convolutions to extract high-level features\nwithout reducing spatial resolution. In cross-subject experiments on the\nColumbia Gaze dataset for eye contact detection and the MPIIGaze dataset for 3D\ngaze vector regression, the resulting Dilated-Nets achieve significant (up to\n20.8%) gains when compared to similar networks without dilated-convolutions.\nOur proposed Dilated-Net achieves state-of-the-art results on both the Columbia\nGaze and the MPIIGaze datasets.\n", "versions": [{"version": "v1", "created": "Mon, 18 Mar 2019 08:29:32 GMT"}], "update_date": "2019-03-19", "authors_parsed": [["Chen", "Zhaokang", ""], ["Shi", "Bertram E.", ""]]}, {"id": "1903.07309", "submitter": "Alex Wong", "authors": "Alex Wong, Byung-Woo Hong, Stefano Soatto", "title": "Bilateral Cyclic Constraint and Adaptive Regularization for Unsupervised\n  Monocular Depth Prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Supervised learning methods to infer (hypothesize) depth of a scene from a\nsingle image require costly per-pixel ground-truth. We follow a geometric\napproach that exploits abundant stereo imagery to learn a model to hypothesize\nscene structure without direct supervision. Although we train a network with\nstereo pairs, we only require a single image at test time to hypothesize\ndisparity or depth. We propose a novel objective function that exploits the\nbilateral cyclic relationship between the left and right disparities and we\nintroduce an adaptive regularization scheme that allows the network to handle\nboth the co-visible and occluded regions in a stereo pair. This process\nultimately produces a model to generate hypotheses for the 3-dimensional\nstructure of the scene as viewed in a single image. When used to generate a\nsingle (most probable) estimate of depth, our method outperforms\nstate-of-the-art unsupervised monocular depth prediction methods on the KITTI\nbenchmarks. We show that our method generalizes well by applying our models\ntrained on KITTI to the Make3d dataset.\n", "versions": [{"version": "v1", "created": "Mon, 18 Mar 2019 08:53:48 GMT"}, {"version": "v2", "created": "Sat, 6 Apr 2019 05:31:01 GMT"}, {"version": "v3", "created": "Wed, 19 Jun 2019 15:58:41 GMT"}], "update_date": "2019-06-20", "authors_parsed": [["Wong", "Alex", ""], ["Hong", "Byung-Woo", ""], ["Soatto", "Stefano", ""]]}, {"id": "1903.07360", "submitter": "Shihua Huang", "authors": "Shihua Huang, Lu Wang", "title": "IvaNet: Learning to jointly detect and segment objets with the help of\n  Local Top-Down Modules", "comments": "5 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Driven by Convolutional Neural Networks, object detection and semantic\nsegmentation have gained significant improvements. However, existing methods on\nthe basis of a full top-down module have limited robustness in handling those\ntwo tasks simultaneously. To this end, we present a joint multi-task framework,\ntermed IvaNet. Different from existing methods, our IvaNet backwards abstract\nsemantic information from higher layers to augment lower layers using local\ntop-down modules. The comparisons against some counterparts on the PASCAL VOC\nand MS COCO datasets demonstrate the functionality of IvaNet.\n", "versions": [{"version": "v1", "created": "Mon, 18 Mar 2019 10:58:54 GMT"}], "update_date": "2019-03-19", "authors_parsed": [["Huang", "Shihua", ""], ["Wang", "Lu", ""]]}, {"id": "1903.07366", "submitter": "Guanying Chen", "authors": "Guanying Chen, Kai Han, Boxin Shi, Yasuyuki Matsushita, Kwan-Yee K.\n  Wong", "title": "Self-calibrating Deep Photometric Stereo Networks", "comments": "CVPR 2019 Oral, Project Page: http://guanyingc.github.io/SDPS-Net/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes an uncalibrated photometric stereo method for\nnon-Lambertian scenes based on deep learning. Unlike previous approaches that\nheavily rely on assumptions of specific reflectances and light source\ndistributions, our method is able to determine both shape and light directions\nof a scene with unknown arbitrary reflectances observed under unknown varying\nlight directions. To achieve this goal, we propose a two-stage deep learning\narchitecture, called SDPS-Net, which can effectively take advantage of\nintermediate supervision, resulting in reduced learning difficulty compared to\na single-stage model. Experiments on both synthetic and real datasets show that\nour proposed approach significantly outperforms previous uncalibrated\nphotometric stereo methods.\n", "versions": [{"version": "v1", "created": "Mon, 18 Mar 2019 11:16:29 GMT"}], "update_date": "2019-03-19", "authors_parsed": [["Chen", "Guanying", ""], ["Han", "Kai", ""], ["Shi", "Boxin", ""], ["Matsushita", "Yasuyuki", ""], ["Wong", "Kwan-Yee K.", ""]]}, {"id": "1903.07377", "submitter": "Johannes Michael", "authors": "Johannes Michael, Roger Labahn, Tobias Gr\\\"uning, Jochen Z\\\"ollner", "title": "Evaluating Sequence-to-Sequence Models for Handwritten Text Recognition", "comments": "8 pages, 1 figure, 8 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Encoder-decoder models have become an effective approach for sequence\nlearning tasks like machine translation, image captioning and speech\nrecognition, but have yet to show competitive results for handwritten text\nrecognition. To this end, we propose an attention-based sequence-to-sequence\nmodel. It combines a convolutional neural network as a generic feature\nextractor with a recurrent neural network to encode both the visual\ninformation, as well as the temporal context between characters in the input\nimage, and uses a separate recurrent neural network to decode the actual\ncharacter sequence. We make experimental comparisons between various attention\nmechanisms and positional encodings, in order to find an appropriate alignment\nbetween the input and output sequence. The model can be trained end-to-end and\nthe optional integration of a hybrid loss allows the encoder to retain an\ninterpretable and usable output, if desired. We achieve competitive results on\nthe IAM and ICFHR2016 READ data sets compared to the state-of-the-art without\nthe use of a language model, and we significantly improve over any recent\nsequence-to-sequence approaches.\n", "versions": [{"version": "v1", "created": "Mon, 18 Mar 2019 11:51:33 GMT"}, {"version": "v2", "created": "Mon, 15 Jul 2019 11:40:53 GMT"}], "update_date": "2019-07-16", "authors_parsed": [["Michael", "Johannes", ""], ["Labahn", "Roger", ""], ["Gr\u00fcning", "Tobias", ""], ["Z\u00f6llner", "Jochen", ""]]}, {"id": "1903.07414", "submitter": "Tak-Wai Hui", "authors": "Tak-Wai Hui, Xiaoou Tang, Chen Change Loy", "title": "A Lightweight Optical Flow CNN - Revisiting Data Fidelity and\n  Regularization", "comments": "Accepted to TPAMI 2020. arXiv admin note: substantial text overlap\n  with arXiv:1805.07036", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Over four decades, the majority addresses the problem of optical flow\nestimation using variational methods. With the advance of machine learning,\nsome recent works have attempted to address the problem using convolutional\nneural network (CNN) and have showed promising results. FlowNet2, the\nstate-of-the-art CNN, requires over 160M parameters to achieve accurate flow\nestimation. Our LiteFlowNet2 outperforms FlowNet2 on Sintel and KITTI\nbenchmarks, while being 25.3 times smaller in the model size and 3.1 times\nfaster in the running speed. LiteFlowNet2 is built on the foundation laid by\nconventional methods and resembles the corresponding roles as data fidelity and\nregularization in variational methods. We compute optical flow in a\nspatial-pyramid formulation as SPyNet but through a novel lightweight cascaded\nflow inference. It provides high flow estimation accuracy through early\ncorrection with seamless incorporation of descriptor matching. Flow\nregularization is used to ameliorate the issue of outliers and vague flow\nboundaries through feature-driven local convolutions. Our network also owns an\neffective structure for pyramidal feature extraction and embraces feature\nwarping rather than image warping as practiced in FlowNet2 and SPyNet.\nComparing to LiteFlowNet, LiteFlowNet2 improves the optical flow accuracy on\nSintel Clean by 23.3%, Sintel Final by 12.8%, KITTI 2012 by 19.6%, and KITTI\n2015 by 18.8%, while being 2.2 times faster. Our network protocol and trained\nmodels are made publicly available on https://github.com/twhui/LiteFlowNet2.\n", "versions": [{"version": "v1", "created": "Fri, 15 Mar 2019 04:20:58 GMT"}, {"version": "v2", "created": "Tue, 14 Jan 2020 04:08:22 GMT"}, {"version": "v3", "created": "Sun, 15 Mar 2020 02:32:44 GMT"}], "update_date": "2020-03-17", "authors_parsed": [["Hui", "Tak-Wai", ""], ["Tang", "Xiaoou", ""], ["Loy", "Chen Change", ""]]}, {"id": "1903.07427", "submitter": "Min-hwan Oh", "authors": "Min-hwan Oh, Peder A. Olsen, Karthikeyan Natesan Ramamurthy", "title": "Crowd Counting with Decomposed Uncertainty", "comments": "Accepted in AAAI 2020 (Main Technical Track)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Research in neural networks in the field of computer vision has achieved\nremarkable accuracy for point estimation. However, the uncertainty in the\nestimation is rarely addressed. Uncertainty quantification accompanied by point\nestimation can lead to a more informed decision, and even improve the\nprediction quality. In this work, we focus on uncertainty estimation in the\ndomain of crowd counting. With increasing occurrences of heavily crowded events\nsuch as political rallies, protests, concerts, etc., automated crowd analysis\nis becoming an increasingly crucial task. The stakes can be very high in many\nof these real-world applications. We propose a scalable neural network\nframework with quantification of decomposed uncertainty using a bootstrap\nensemble. We demonstrate that the proposed uncertainty quantification method\nprovides additional insight to the crowd counting problem and is simple to\nimplement. We also show that our proposed method exhibits the state of the art\nperformances in many benchmark crowd counting datasets.\n", "versions": [{"version": "v1", "created": "Fri, 15 Mar 2019 16:53:50 GMT"}, {"version": "v2", "created": "Wed, 15 May 2019 21:39:06 GMT"}, {"version": "v3", "created": "Wed, 22 Apr 2020 05:02:35 GMT"}], "update_date": "2020-04-23", "authors_parsed": [["Oh", "Min-hwan", ""], ["Olsen", "Peder A.", ""], ["Ramamurthy", "Karthikeyan Natesan", ""]]}, {"id": "1903.07437", "submitter": "Junyi Gao", "authors": "Junyi Gao, Weihao Tan, Liantao Ma, Yasha Wang, Wen Tang", "title": "MUSEFood: Multi-sensor-based Food Volume Estimation on Smartphones", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Researches have shown that diet recording can help people increase awareness\nof food intake and improve nutrition management, and thereby maintain a\nhealthier life. Recently, researchers have been working on smartphone-based\ndiet recording methods and applications that help users accomplish two tasks:\nrecord what they eat and how much they eat. Although the former task has made\ngreat progress through adopting image recognition technology, it is still a\nchallenge to estimate the volume of foods accurately and conveniently. In this\npaper, we propose a novel method, named MUSEFood, for food volume estimation.\nMUSEFood uses the camera to capture photos of the food, but unlike existing\nvolume measurement methods, MUSEFood requires neither training images with\nvolume information nor placing a reference object of known size while taking\nphotos. In addition, considering the impact of different containers on the\ncontour shape of foods, MUSEFood uses a multi-task learning framework to\nimprove the accuracy of food segmentation, and uses a differential model\napplicable for various containers to further reduce the negative impact of\ncontainer differences on volume estimation accuracy. Furthermore, MUSEFood uses\nthe microphone and the speaker to accurately measure the vertical distance from\nthe camera to the food in a noisy environment, thus scaling the size of food in\nthe image to its actual size. The experiments on real foods indicate that\nMUSEFood outperforms state-of-the-art approaches, and highly improves the speed\nof food volume estimation.\n", "versions": [{"version": "v1", "created": "Mon, 18 Mar 2019 13:40:23 GMT"}, {"version": "v2", "created": "Tue, 19 Mar 2019 07:44:41 GMT"}, {"version": "v3", "created": "Sat, 8 Jun 2019 17:55:16 GMT"}], "update_date": "2019-06-11", "authors_parsed": [["Gao", "Junyi", ""], ["Tan", "Weihao", ""], ["Ma", "Liantao", ""], ["Wang", "Yasha", ""], ["Tang", "Wen", ""]]}, {"id": "1903.07479", "submitter": "Phong Nguyen Huu", "authors": "Nguyen Huu Phong and Bernardete Ribeiro", "title": "Offline and Online Deep Learning for Image Recognition", "comments": "5 pages", "journal-ref": "2017 4th Experiment@International Conference (exp.at'17)", "doi": "10.1109/EXPAT.2017.7984421", "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image recognition using Deep Learning has been evolved for decades though\nadvances in the field through different settings is still a challenge. In this\npaper, we present our findings in searching for better image classifiers in\noffline and online environments. We resort to Convolutional Neural Network and\nits variations of fully connected Multi-layer Perceptron. Though still\npreliminary, these results are encouraging and may provide a better\nunderstanding about the field and directions toward future works.\n", "versions": [{"version": "v1", "created": "Mon, 18 Mar 2019 14:39:20 GMT"}], "update_date": "2019-03-19", "authors_parsed": [["Phong", "Nguyen Huu", ""], ["Ribeiro", "Bernardete", ""]]}, {"id": "1903.07499", "submitter": "YueFeng Chen", "authors": "Xiaofeng Mao, Yuefeng Chen, Yuhong Li, Tao Xiong, Yuan He, Hui Xue", "title": "Bilinear Representation for Language-based Image Editing Using\n  Conditional Generative Adversarial Networks", "comments": "To appear at ICASSP 2019. Implementation:\n  https://github.com/vtddggg/BilinearGAN_for_LBIE", "journal-ref": null, "doi": "10.1109/ICASSP.2019.8683008", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The task of Language-Based Image Editing (LBIE) aims at generating a target\nimage by editing the source image based on the given language description. The\nmain challenge of LBIE is to disentangle the semantics in image and text and\nthen combine them to generate realistic images. Therefore, the editing\nperformance is heavily dependent on the learned representation. In this work,\nconditional generative adversarial network (cGAN) is utilized for LBIE. We find\nthat existing conditioning methods in cGAN lack of representation power as they\ncannot learn the second-order correlation between two conditioning vectors. To\nsolve this problem, we propose an improved conditional layer named Bilinear\nResidual Layer (BRL) to learning more powerful representations for LBIE task.\nQualitative and quantitative comparisons demonstrate that our method can\ngenerate images with higher quality when compared to previous LBIE techniques.\n", "versions": [{"version": "v1", "created": "Mon, 18 Mar 2019 15:13:55 GMT"}], "update_date": "2019-11-15", "authors_parsed": [["Mao", "Xiaofeng", ""], ["Chen", "Yuefeng", ""], ["Li", "Yuhong", ""], ["Xiong", "Tao", ""], ["He", "Yuan", ""], ["Xue", "Hui", ""]]}, {"id": "1903.07504", "submitter": "Torsten Sattler", "authors": "Torsten Sattler, Qunjie Zhou, Marc Pollefeys, Laura Leal-Taixe", "title": "Understanding the Limitations of CNN-based Absolute Camera Pose\n  Regression", "comments": "Initial version of a paper accepted to CVPR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual localization is the task of accurate camera pose estimation in a known\nscene. It is a key problem in computer vision and robotics, with applications\nincluding self-driving cars, Structure-from-Motion, SLAM, and Mixed Reality.\nTraditionally, the localization problem has been tackled using 3D geometry.\nRecently, end-to-end approaches based on convolutional neural networks have\nbecome popular. These methods learn to directly regress the camera pose from an\ninput image. However, they do not achieve the same level of pose accuracy as 3D\nstructure-based methods. To understand this behavior, we develop a theoretical\nmodel for camera pose regression. We use our model to predict failure cases for\npose regression techniques and verify our predictions through experiments. We\nfurthermore use our model to show that pose regression is more closely related\nto pose approximation via image retrieval than to accurate pose estimation via\n3D structure. A key result is that current approaches do not consistently\noutperform a handcrafted image retrieval baseline. This clearly shows that\nadditional research is needed before pose regression algorithms are ready to\ncompete with structure-based methods.\n", "versions": [{"version": "v1", "created": "Mon, 18 Mar 2019 15:24:11 GMT"}], "update_date": "2019-03-19", "authors_parsed": [["Sattler", "Torsten", ""], ["Zhou", "Qunjie", ""], ["Pollefeys", "Marc", ""], ["Leal-Taixe", "Laura", ""]]}, {"id": "1903.07520", "submitter": "Anton Mitrokhin", "authors": "Anton Mitrokhin, Chengxi Ye, Cornelia Fermuller, Yiannis Aloimonos,\n  Tobi Delbruck", "title": "EV-IMO: Motion Segmentation Dataset and Learning Pipeline for Event\n  Cameras", "comments": "8 pages, 6 figures. Submitted to 2019 IEEE/RSJ International\n  Conference on Intelligent Robots and Systems (IROS 2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the first event-based learning approach for motion segmentation in\nindoor scenes and the first event-based dataset - EV-IMO - which includes\naccurate pixel-wise motion masks, egomotion and ground truth depth. Our\napproach is based on an efficient implementation of the SfM learning pipeline\nusing a low parameter neural network architecture on event data. In addition to\ncamera egomotion and a dense depth map, the network estimates pixel-wise\nindependently moving object segmentation and computes per-object 3D\ntranslational velocities for moving objects. We also train a shallow network\nwith just 40k parameters, which is able to compute depth and egomotion.\n  Our EV-IMO dataset features 32 minutes of indoor recording with up to 3 fast\nmoving objects simultaneously in the camera field of view. The objects and the\ncamera are tracked by the VICON motion capture system. By 3D scanning the room\nand the objects, accurate depth map ground truth and pixel-wise object masks\nare obtained, which are reliable even in poor lighting conditions and during\nfast motion. We then train and evaluate our learning pipeline on EV-IMO and\ndemonstrate that our approach far surpasses its rivals and is well suited for\nscene constrained robotics applications.\n", "versions": [{"version": "v1", "created": "Mon, 18 Mar 2019 15:51:25 GMT"}, {"version": "v2", "created": "Sun, 12 Jan 2020 23:58:10 GMT"}], "update_date": "2020-01-14", "authors_parsed": [["Mitrokhin", "Anton", ""], ["Ye", "Chengxi", ""], ["Fermuller", "Cornelia", ""], ["Aloimonos", "Yiannis", ""], ["Delbruck", "Tobi", ""]]}, {"id": "1903.07525", "submitter": "Sergiy Popovych", "authors": "Sergiy Popovych, Davit Buniatyan, Aleksandar Zlateski, Kai Li, H.\n  Sebastian Seung", "title": "PZnet: Efficient 3D ConvNet Inference on Manycore CPUs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.PF", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Convolutional nets have been shown to achieve state-of-the-art accuracy in\nmany biomedical image analysis tasks. Many tasks within biomedical analysis\ndomain involve analyzing volumetric (3D) data acquired by CT, MRI and\nMicroscopy acquisition methods. To deploy convolutional nets in practical\nworking systems, it is important to solve the efficient inference problem.\nNamely, one should be able to apply an already-trained convolutional network to\nmany large images using limited computational resources. In this paper we\npresent PZnet, a CPU-only engine that can be used to perform inference for a\nvariety of 3D convolutional net architectures. PZNet outperforms MKL-based CPU\nimplementations of PyTorch and Tensorflow by more than 3.5x for the popular\nU-net architecture. Moreover, for 3D convolutions with low featuremap numbers,\ncloud CPU inference with PZnet outperfroms cloud GPU inference in terms of cost\nefficiency.\n", "versions": [{"version": "v1", "created": "Mon, 18 Mar 2019 16:07:12 GMT"}], "update_date": "2019-03-19", "authors_parsed": [["Popovych", "Sergiy", ""], ["Buniatyan", "Davit", ""], ["Zlateski", "Aleksandar", ""], ["Li", "Kai", ""], ["Seung", "H. Sebastian", ""]]}, {"id": "1903.07546", "submitter": "Hongxin Wang", "authors": "Hongxin Wang, Jigen Peng, Qinbing Fu, Huatian Wang and Shigang Yue", "title": "Visual Cue Integration for Small Target Motion Detection in Natural\n  Cluttered Backgrounds", "comments": "7 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The robust detection of small targets against cluttered background is\nimportant for future artificial visual systems in searching and tracking\napplications. The insects' visual systems have demonstrated excellent ability\nto avoid predators, find prey or identify conspecifics - which always appear as\nsmall dim speckles in the visual field. Build a computational model of the\ninsects' visual pathways could provide effective solutions to detect small\nmoving targets. Although a few visual system models have been proposed, they\nonly make use of small-field visual features for motion detection and their\ndetection results often contain a number of false positives. To address this\nissue, we develop a new visual system model for small target motion detection\nagainst cluttered moving backgrounds. Compared to the existing models, the\nsmall-field and wide-field visual features are separately extracted by two\nmotion-sensitive neurons to detect small target motion and background motion.\nThese two types of motion information are further integrated to filter out\nfalse positives. Extensive experiments showed that the proposed model can\noutperform the existing models in terms of detection rates.\n", "versions": [{"version": "v1", "created": "Mon, 18 Mar 2019 16:33:47 GMT"}], "update_date": "2019-03-19", "authors_parsed": [["Wang", "Hongxin", ""], ["Peng", "Jigen", ""], ["Fu", "Qinbing", ""], ["Wang", "Huatian", ""], ["Yue", "Shigang", ""]]}, {"id": "1903.07563", "submitter": "Manjot Bilkhu", "authors": "Manjot Bilkhu, Hammababdullah Ayyubi", "title": "Human Activity Recognition for Edge Devices", "comments": "7 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video activity Recognition has recently gained a lot of momentum with the\nrelease of massive Kinetics (400 and 600) data. Architectures such as I3D and\nC3D networks have shown state-of-the-art performances for activity recognition.\nThe one major pitfall with these state-of-the-art networks is that they require\na lot of compute. In this paper we explore how we can achieve comparable\nresults to these state-of-the-art networks for devices-on-edge. We primarily\nexplore two architectures - I3D and Temporal Segment Network. We show that\ncomparable results can be achieved using one tenth the memory usage by changing\nthe testing procedure. We also report our results on Resnet architecture as our\nbackbone apart from the original Inception architecture. Specifically, we\nachieve 84.54\\% top-1 accuracy on UCF-101 dataset using only RGB frames.\n", "versions": [{"version": "v1", "created": "Mon, 18 Mar 2019 16:56:57 GMT"}], "update_date": "2019-03-19", "authors_parsed": [["Bilkhu", "Manjot", ""], ["Ayyubi", "Hammababdullah", ""]]}, {"id": "1903.07593", "submitter": "Allan Jabri", "authors": "Xiaolong Wang, Allan Jabri, Alexei A. Efros", "title": "Learning Correspondence from the Cycle-Consistency of Time", "comments": "CVPR 2019 Oral. Project page: http://ajabri.github.io/timecycle", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a self-supervised method for learning visual correspondence from\nunlabeled video. The main idea is to use cycle-consistency in time as free\nsupervisory signal for learning visual representations from scratch. At\ntraining time, our model learns a feature map representation to be useful for\nperforming cycle-consistent tracking. At test time, we use the acquired\nrepresentation to find nearest neighbors across space and time. We demonstrate\nthe generalizability of the representation -- without finetuning -- across a\nrange of visual correspondence tasks, including video object segmentation,\nkeypoint tracking, and optical flow. Our approach outperforms previous\nself-supervised methods and performs competitively with strongly supervised\nmethods.\n", "versions": [{"version": "v1", "created": "Mon, 18 Mar 2019 17:36:00 GMT"}, {"version": "v2", "created": "Tue, 2 Apr 2019 05:56:01 GMT"}], "update_date": "2019-04-03", "authors_parsed": [["Wang", "Xiaolong", ""], ["Jabri", "Allan", ""], ["Efros", "Alexei A.", ""]]}, {"id": "1903.07614", "submitter": "Laurent Duval", "authors": "Jean-Luc Peyrot and Laurent Duval and Fr\\'ed\\'eric Payan and Lauriane\n  Bouard and L\\'ena\\\"ic Chizat and S\\'ebastien Schneider and Marc Antonini", "title": "HexaShrink, an exact scalable framework for hexahedral meshes with\n  attributes and discontinuities: multiresolution rendering and storage of\n  geoscience models", "comments": null, "journal-ref": null, "doi": "10.1007/s10596-019-9816-2", "report-no": null, "categories": "cs.GR cs.CV cs.DS physics.data-an physics.geo-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With huge data acquisition progresses realized in the past decades and\nacquisition systems now able to produce high resolution grids and point clouds,\nthe digitization of physical terrains becomes increasingly more precise. Such\nextreme quantities of generated and modeled data greatly impact computational\nperformances on many levels of high-performance computing (HPC): storage media,\nmemory requirements, transfer capability, and finally simulation interactivity,\nnecessary to exploit this instance of big data. Efficient representations and\nstorage are thus becoming \"enabling technologies'' in HPC experimental and\nsimulation science. We propose HexaShrink, an original decomposition scheme for\nstructured hexahedral volume meshes. The latter are used for instance in\nbiomedical engineering, materials science, or geosciences. HexaShrink provides\na comprehensive framework allowing efficient mesh visualization and storage.\nIts exactly reversible multiresolution decomposition yields a hierarchy of\nmeshes of increasing levels of details, in terms of either geometry, continuous\nor categorical properties of cells. Starting with an overview of volume meshes\ncompression techniques, our contribution blends coherently different\nmultiresolution wavelet schemes in different dimensions. It results in a global\nframework preserving discontinuities (faults) across scales, implemented as a\nfully reversible upscaling at different resolutions. Experimental results are\nprovided on meshes of varying size and complexity. They emphasize the\nconsistency of the proposed representation, in terms of visualization,\nattribute downsampling and distribution at different resolutions. Finally,\nHexaShrink yields gains in storage space when combined to lossless compression\ntechniques.\n", "versions": [{"version": "v1", "created": "Sat, 16 Mar 2019 10:24:22 GMT"}, {"version": "v2", "created": "Sat, 4 May 2019 13:33:17 GMT"}], "update_date": "2019-05-07", "authors_parsed": [["Peyrot", "Jean-Luc", ""], ["Duval", "Laurent", ""], ["Payan", "Fr\u00e9d\u00e9ric", ""], ["Bouard", "Lauriane", ""], ["Chizat", "L\u00e9na\u00efc", ""], ["Schneider", "S\u00e9bastien", ""], ["Antonini", "Marc", ""]]}, {"id": "1903.07663", "submitter": "Tianchen Wang", "authors": "Tianchen Wang, Jinjun Xiong, Xiaowei Xu, Yiyu Shi", "title": "SCNN: A General Distribution based Statistical Convolutional Neural\n  Network with Application to Video Object Detection", "comments": "AAAI19", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Various convolutional neural networks (CNNs) were developed recently that\nachieved accuracy comparable with that of human beings in computer vision tasks\nsuch as image recognition, object detection and tracking, etc. Most of these\nnetworks, however, process one single frame of image at a time, and may not\nfully utilize the temporal and contextual correlation typically present in\nmultiple channels of the same image or adjacent frames from a video, thus\nlimiting the achievable throughput. This limitation stems from the fact that\nexisting CNNs operate on deterministic numbers. In this paper, we propose a\nnovel statistical convolutional neural network (SCNN), which extends existing\nCNN architectures but operates directly on correlated distributions rather than\ndeterministic numbers. By introducing a parameterized canonical model to model\ncorrelated data and defining corresponding operations as required for CNN\ntraining and inference, we show that SCNN can process multiple frames of\ncorrelated images effectively, hence achieving significant speedup over\nexisting CNN models. We use a CNN based video object detection as an example to\nillustrate the usefulness of the proposed SCNN as a general network model.\nExperimental results show that even a non-optimized implementation of SCNN can\nstill achieve 178% speedup over existing CNNs with slight accuracy degradation.\n", "versions": [{"version": "v1", "created": "Fri, 15 Mar 2019 16:00:23 GMT"}], "update_date": "2019-03-20", "authors_parsed": [["Wang", "Tianchen", ""], ["Xiong", "Jinjun", ""], ["Xu", "Xiaowei", ""], ["Shi", "Yiyu", ""]]}, {"id": "1903.07669", "submitter": "Pelin Dogan", "authors": "Pelin Dogan, Leonid Sigal, Markus Gross", "title": "Neural Sequential Phrase Grounding (SeqGROUND)", "comments": "Accepted at CVPR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an end-to-end approach for phrase grounding in images. Unlike\nprior methods that typically attempt to ground each phrase independently by\nbuilding an image-text embedding, our architecture formulates grounding of\nmultiple phrases as a sequential and contextual process. Specifically, we\nencode region proposals and all phrases into two stacks of LSTM cells, along\nwith so-far grounded phrase-region pairs. These LSTM stacks collectively\ncapture context for grounding of the next phrase. The resulting architecture,\nwhich we call SeqGROUND, supports many-to-many matching by allowing an image\nregion to be matched to multiple phrases and vice versa. We show competitive\nperformance on the Flickr30K benchmark dataset and, through ablation studies,\nvalidate the efficacy of sequential grounding as well as individual design\nchoices in our model architecture.\n", "versions": [{"version": "v1", "created": "Mon, 18 Mar 2019 18:52:09 GMT"}], "update_date": "2019-03-20", "authors_parsed": [["Dogan", "Pelin", ""], ["Sigal", "Leonid", ""], ["Gross", "Markus", ""]]}, {"id": "1903.07705", "submitter": "Xin Lei", "authors": "Xin Lei, Liangyu He, Yixuan Tan, Ken Xingze Wang, Xinggang Wang, Yihan\n  Du, Shanhui Fan, Zongfu Yu", "title": "Direct Object Recognition Without Line-of-Sight Using Optical Coherence", "comments": "Accepted to CVPR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV physics.optics", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual object recognition under situations in which the direct line-of-sight\nis blocked, such as when it is occluded around the corner, is of practical\nimportance in a wide range of applications. With coherent illumination, the\nlight scattered from diffusive walls forms speckle patterns that contain\ninformation of the hidden object. It is possible to realize non-line-of-sight\n(NLOS) recognition with these speckle patterns. We introduce a novel approach\nbased on speckle pattern recognition with deep neural network, which is simpler\nand more robust than other NLOS recognition methods. Simulations and\nexperiments are performed to verify the feasibility and performance of this\napproach.\n", "versions": [{"version": "v1", "created": "Mon, 18 Mar 2019 20:34:41 GMT"}], "update_date": "2019-04-17", "authors_parsed": [["Lei", "Xin", ""], ["He", "Liangyu", ""], ["Tan", "Yixuan", ""], ["Wang", "Ken Xingze", ""], ["Wang", "Xinggang", ""], ["Du", "Yihan", ""], ["Fan", "Shanhui", ""], ["Yu", "Zongfu", ""]]}, {"id": "1903.07740", "submitter": "Alexander Pashevich", "authors": "Alexander Pashevich, Robin Strudel, Igor Kalevatykh, Ivan Laptev,\n  Cordelia Schmid", "title": "Learning to Augment Synthetic Images for Sim2Real Policy Transfer", "comments": "7 pages", "journal-ref": "IROS 2019", "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Vision and learning have made significant progress that could improve\nrobotics policies for complex tasks and environments. Learning deep neural\nnetworks for image understanding, however, requires large amounts of\ndomain-specific visual data. While collecting such data from real robots is\npossible, such an approach limits the scalability as learning policies\ntypically requires thousands of trials. In this work we attempt to learn\nmanipulation policies in simulated environments. Simulators enable scalability\nand provide access to the underlying world state during training. Policies\nlearned in simulators, however, do not transfer well to real scenes given the\ndomain gap between real and synthetic data. We follow recent work on domain\nrandomization and augment synthetic images with sequences of random\ntransformations. Our main contribution is to optimize the augmentation strategy\nfor sim2real transfer and to enable domain-independent policy learning. We\ndesign an efficient search for depth image augmentations using object\nlocalization as a proxy task. Given the resulting sequence of random\ntransformations, we use it to augment synthetic depth images during policy\nlearning. Our augmentation strategy is policy-independent and enables policy\nlearning with no real images. We demonstrate our approach to significantly\nimprove accuracy on three manipulation tasks evaluated on a real robot.\n", "versions": [{"version": "v1", "created": "Mon, 18 Mar 2019 22:01:57 GMT"}, {"version": "v2", "created": "Tue, 30 Jul 2019 14:47:57 GMT"}], "update_date": "2019-07-31", "authors_parsed": [["Pashevich", "Alexander", ""], ["Strudel", "Robin", ""], ["Kalevatykh", "Igor", ""], ["Laptev", "Ivan", ""], ["Schmid", "Cordelia", ""]]}, {"id": "1903.07788", "submitter": "Jianxin Wu", "authors": "Kun Yi and Jianxin Wu", "title": "Probabilistic End-to-end Noise Correction for Learning with Noisy Labels", "comments": "CVPR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning has achieved excellent performance in various computer vision\ntasks, but requires a lot of training examples with clean labels. It is easy to\ncollect a dataset with noisy labels, but such noise makes networks overfit\nseriously and accuracies drop dramatically. To address this problem, we propose\nan end-to-end framework called PENCIL, which can update both network parameters\nand label estimations as label distributions. PENCIL is independent of the\nbackbone network structure and does not need an auxiliary clean dataset or\nprior information about noise, thus it is more general and robust than existing\nmethods and is easy to apply. PENCIL outperforms previous state-of-the-art\nmethods by large margins on both synthetic and real-world datasets with\ndifferent noise types and noise rates. Experiments show that PENCIL is robust\non clean datasets, too.\n", "versions": [{"version": "v1", "created": "Tue, 19 Mar 2019 01:38:08 GMT"}], "update_date": "2019-03-20", "authors_parsed": [["Yi", "Kun", ""], ["Wu", "Jianxin", ""]]}, {"id": "1903.07789", "submitter": "Junkai Sun", "authors": "Junkai Sun, Junbo Zhang, Qiaofei Li, Xiuwen Yi, Yuxuan Liang, Yu Zheng", "title": "Predicting Citywide Crowd Flows in Irregular Regions Using Multi-View\n  Graph Convolutional Networks", "comments": "12 pages, 13 figures, 5 tables. Published in IEEE TKDE, Date of\n  Publication: Jul. 13, 2020", "journal-ref": "IEEE Transactions on Knowledge and Data Engineering (TKDE), 2020", "doi": "10.1109/TKDE.2020.3008774", "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Being able to predict the crowd flows in each and every part of a city,\nespecially in irregular regions, is strategically important for traffic\ncontrol, risk assessment, and public safety. However, it is very challenging\nbecause of interactions and spatial correlations between different regions. In\naddition, it is affected by many factors: i) multiple temporal correlations\namong different time intervals: closeness, period, trend; ii) complex external\ninfluential factors: weather, events; iii) meta features: time of the day, day\nof the week, and so on. In this paper, we formulate crowd flow forecasting in\nirregular regions as a spatio-temporal graph (STG) prediction problem in which\neach node represents a region with time-varying flows. By extending graph\nconvolution to handle the spatial information, we propose using spatial graph\nconvolution to build a multi-view graph convolutional network (MVGCN) for the\ncrowd flow forecasting problem, where different views can capture different\nfactors as mentioned above. We evaluate MVGCN using four real-world datasets\n(taxicabs and bikes) and extensive experimental results show that our approach\noutperforms the adaptations of state-of-the-art methods. And we have developed\na crowd flow forecasting system for irregular regions that can now be used\ninternally.\n", "versions": [{"version": "v1", "created": "Tue, 19 Mar 2019 01:46:11 GMT"}, {"version": "v2", "created": "Fri, 17 Jul 2020 08:34:11 GMT"}], "update_date": "2020-07-20", "authors_parsed": [["Sun", "Junkai", ""], ["Zhang", "Junbo", ""], ["Li", "Qiaofei", ""], ["Yi", "Xiuwen", ""], ["Liang", "Yuxuan", ""], ["Zheng", "Yu", ""]]}, {"id": "1903.07801", "submitter": "YueFeng Chen", "authors": "Yuefeng Chen, Qing Wang", "title": "Robust Visual Tracking Using Dynamic Classifier Selection with Sparse\n  Representation of Label Noise", "comments": "accepted at ACCV2012, Oral", "journal-ref": null, "doi": "10.1007/978-3-642-37431-9_3", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently a category of tracking methods based on \"tracking-by-detection\" is\nwidely used in visual tracking problem. Most of these methods update the\nclassifier online using the samples generated by the tracker to handle the\nappearance changes. However, the self-updating scheme makes these methods\nsuffer from drifting problem because of the incorrect labels of weak\nclassifiers in training samples. In this paper, we split the class labels into\ntrue labels and noise labels and model them by sparse representation. A novel\ndynamic classifier selection method, robust to noisy training data, is\nproposed. Moreover, we apply the proposed classifier selection algorithm to\nvisual tracking by integrating a part based online boosting framework. We have\nevaluated our proposed method on 12 challenging sequences involving severe\nocclusions, significant illumination changes and large pose variations. Both\nthe qualitative and quantitative evaluations demonstrate that our approach\ntracks objects accurately and robustly and outperforms state-of-the-art\ntrackers.\n", "versions": [{"version": "v1", "created": "Tue, 19 Mar 2019 02:48:44 GMT"}], "update_date": "2019-03-20", "authors_parsed": [["Chen", "Yuefeng", ""], ["Wang", "Qing", ""]]}, {"id": "1903.07803", "submitter": "Rolando Estrada", "authors": "Aashis Khanal and Rolando Estrada", "title": "Dynamic Deep Networks for Retinal Vessel Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Segmenting the retinal vasculature entails a trade-off between how much of\nthe overall vascular structure we identify vs. how precisely we segment\nindividual vessels. In particular, state-of-the-art methods tend to\nunder-segment faint vessels, as well as pixels that lie on the edges of thicker\nvessels. Thus, they underestimate the width of individual vessels, as well as\nthe ratio of large to small vessels. More generally, many crucial\nbio-markers---including the artery-vein (AV) ratio, branching angles, number of\nbifurcation, fractal dimension, tortuosity, vascular length-to-diameter ratio\nand wall-to-lumen length---require precise measurements of individual vessels.\nTo address this limitation, we propose a novel, stochastic training scheme for\ndeep neural networks that better classifies the faint, ambiguous regions of the\nimage. Our approach relies on two key innovations. First, we train our deep\nnetworks with dynamic weights that fluctuate during each training iteration.\nThis stochastic approach forces the network to learn a mapping that robustly\nbalances precision and recall. Second, we decouple the segmentation process\ninto two steps. In the first half of our pipeline, we estimate the likelihood\nof every pixel and then use these likelihoods to segment pixels that are\nclearly vessel or background. In the latter part of our pipeline, we use a\nsecond network to classify the ambiguous regions in the image. Our proposed\nmethod obtained state-of-the-art results on five retinal datasets---DRIVE,\nSTARE, CHASE-DB, AV-WIDE, and VEVIO---by learning a robust balance between\nfalse positive and false negative rates. In addition, we are the first to\nreport segmentation results on the AV-WIDE dataset, and we have made the\nground-truth annotations for this dataset publicly available.\n", "versions": [{"version": "v1", "created": "Tue, 19 Mar 2019 03:02:24 GMT"}, {"version": "v2", "created": "Wed, 27 Mar 2019 14:37:17 GMT"}], "update_date": "2019-03-28", "authors_parsed": [["Khanal", "Aashis", ""], ["Estrada", "Rolando", ""]]}, {"id": "1903.07812", "submitter": "Huibing Wang", "authors": "Huibing Wang, Jinjia Peng and Xianping Fu", "title": "Self-Weighted Multiview Metric Learning by Maximizing the Cross\n  Correlations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the development of multimedia time, one sample can always be described\nfrom multiple views which contain compatible and complementary information.\nMost algorithms cannot take information from multiple views into considerations\nand fail to achieve desirable performance in most situations. For many\napplications, such as image retrieval, face recognition, etc., an appropriate\ndistance metric can better reflect the similarities between various samples.\nTherefore, how to construct a good distance metric learning methods which can\ndeal with multiview data has been an important topic during the last decade. In\nthis paper, we proposed a novel algorithm named Self-weighted Multiview Metric\nLearning (SM2L) which can finish this task by maximizing the cross correlations\nbetween different views. Furthermore, because multiple views have different\ncontributions to the learning procedure of SM2L, we adopt a self-weighted\nlearning framework to assign multiple views with different weights. Various\nexperiments on benchmark datasets can verify the performance of our proposed\nmethod.\n", "versions": [{"version": "v1", "created": "Tue, 19 Mar 2019 03:40:40 GMT"}], "update_date": "2019-03-20", "authors_parsed": [["Wang", "Huibing", ""], ["Peng", "Jinjia", ""], ["Fu", "Xianping", ""]]}, {"id": "1903.07820", "submitter": "Purva Tendulkar", "authors": "Purva Tendulkar, Kalpesh Krishna, Ramprasaath R. Selvaraju, Devi\n  Parikh", "title": "Trick or TReAT: Thematic Reinforcement for Artistic Typography", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An approach to make text visually appealing and memorable is semantic\nreinforcement - the use of visual cues alluding to the context or theme in\nwhich the word is being used to reinforce the message (e.g., Google Doodles).\nWe present a computational approach for semantic reinforcement called TReAT -\nThematic Reinforcement for Artistic Typography. Given an input word (e.g. exam)\nand a theme (e.g. education), the individual letters of the input word are\nreplaced by cliparts relevant to the theme which visually resemble the letters\n- adding creative context to the potentially boring input word. We use an\nunsupervised approach to learn a latent space to represent letters and cliparts\nand compute similarities between the two. Human studies show that participants\ncan reliably recognize the word as well as the theme in our outputs (TReATs)\nand find them more creative compared to meaningful baselines.\n", "versions": [{"version": "v1", "created": "Tue, 19 Mar 2019 04:08:51 GMT"}], "update_date": "2019-03-20", "authors_parsed": [["Tendulkar", "Purva", ""], ["Krishna", "Kalpesh", ""], ["Selvaraju", "Ramprasaath R.", ""], ["Parikh", "Devi", ""]]}, {"id": "1903.07824", "submitter": "Joseph Cheng", "authors": "Joseph Y. Cheng, Feiyu Chen, Christopher Sandino, Morteza Mardani,\n  John M. Pauly, Shreyas S. Vasanawala", "title": "Compressed Sensing: From Research to Clinical Practice with Data-Driven\n  Learning", "comments": "Submitted to the Special Issue on Computational MRI: Compressed\n  Sensing and Beyond in the IEEE Signal Processing Magazine", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG physics.med-ph stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Compressed sensing in MRI enables high subsampling factors while maintaining\ndiagnostic image quality. This technique enables shortened scan durations\nand/or improved image resolution. Further, compressed sensing can increase the\ndiagnostic information and value from each scan performed. Overall, compressed\nsensing has significant clinical impact in improving the diagnostic quality and\npatient experience for imaging exams. However, a number of challenges exist\nwhen moving compressed sensing from research to the clinic. These challenges\ninclude hand-crafted image priors, sensitive tuning parameters, and long\nreconstruction times. Data-driven learning provides a solution to address these\nchallenges. As a result, compressed sensing can have greater clinical impact.\nIn this tutorial, we will review the compressed sensing formulation and outline\nsteps needed to transform this formulation to a deep learning framework.\nSupplementary open source code in python will be used to demonstrate this\napproach with open databases. Further, we will discuss considerations in\napplying data-driven compressed sensing in the clinical setting.\n", "versions": [{"version": "v1", "created": "Tue, 19 Mar 2019 04:28:07 GMT"}], "update_date": "2019-03-20", "authors_parsed": [["Cheng", "Joseph Y.", ""], ["Chen", "Feiyu", ""], ["Sandino", "Christopher", ""], ["Mardani", "Morteza", ""], ["Pauly", "John M.", ""], ["Vasanawala", "Shreyas S.", ""]]}, {"id": "1903.07832", "submitter": "Zhe Chen", "authors": "Zhe Chen, Xiao-Jun Wu and Josef Kittler", "title": "Low-Rank Discriminative Least Squares Regression for Image\n  Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Latest least squares regression (LSR) methods mainly try to learn slack\nregression targets to replace strict zero-one labels. However, the difference\nof intra-class targets can also be highlighted when enlarging the distance\nbetween different classes, and roughly persuing relaxed targets may lead to the\nproblem of overfitting. To solve above problems, we propose a low-rank\ndiscriminative least squares regression model (LRDLSR) for multi-class image\nclassification. Specifically, LRDLSR class-wisely imposes low-rank constraint\non the intra-class regression targets to encourage its compactness and\nsimilarity. Moreover, LRDLSR introduces an additional regularization term on\nthe learned targets to avoid the problem of overfitting. These two improvements\nare helpful to learn a more discriminative projection for regression and thus\nachieving better classification performance. Experimental results over a range\nof image databases demonstrate the effectiveness of the proposed LRDLSR method.\n", "versions": [{"version": "v1", "created": "Tue, 19 Mar 2019 04:48:39 GMT"}, {"version": "v2", "created": "Tue, 16 Apr 2019 13:42:07 GMT"}, {"version": "v3", "created": "Tue, 14 May 2019 07:08:59 GMT"}, {"version": "v4", "created": "Tue, 8 Oct 2019 07:37:26 GMT"}], "update_date": "2019-10-09", "authors_parsed": [["Chen", "Zhe", ""], ["Wu", "Xiao-Jun", ""], ["Kittler", "Josef", ""]]}, {"id": "1903.07833", "submitter": "Zhe Chen", "authors": "Zhe Chen, Xiao-Jun Wu, and Josef Kittler", "title": "Fisher Discriminative Least Squares Regression for Image Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Discriminative least squares regression (DLSR) has been shown to achieve\npromising performance in multi-class image classification tasks. Its key idea\nis to force the regression labels of different classes to move in opposite\ndirections by means of the proposed the joint use of the $\\epsilon$-draggings\ntechnique, yielding discriminative regression model exhibiting wider margins,\nand the Fisher criterion. The $\\epsilon$-draggings technique ignores an\nimportant problem: its non-negative relaxation matrix is dynamically updated in\noptimization, which means the dragging values can also cause the labels from\nthe same class to be uncorrelated. In order to learn a more powerful\ndiscriminative projection, as well as regression labels, we propose a Fisher\nregularized DLSR (FDLSR) framework by constraining the relaxed labels using the\nFisher criterion. On one hand, the Fisher criterion improves the intra-class\ncompactness of the relaxed labels during relaxation learning. On the other\nhand, it is expected further to enhance the inter-class separability of\n$\\epsilon$-draggings technique. FDLSR for the first time ever attempts to\nintegrate the Fisher discriminant criterion and $\\epsilon$-draggings technique\ninto one unified model because they are absolutely complementary in learning\ndiscriminative projection. Extensive experiments on various datasets\ndemonstrate that the proposed FDLSR method achieves performance that is\nsuperior to other state-of-the-art classification methods. The Matlab codes of\nthis paper are available at https://github.com/chenzhe207/FDLSR.\n", "versions": [{"version": "v1", "created": "Tue, 19 Mar 2019 04:53:48 GMT"}, {"version": "v2", "created": "Sat, 28 Sep 2019 09:21:00 GMT"}, {"version": "v3", "created": "Tue, 4 Aug 2020 07:13:30 GMT"}], "update_date": "2020-08-05", "authors_parsed": [["Chen", "Zhe", ""], ["Wu", "Xiao-Jun", ""], ["Kittler", "Josef", ""]]}, {"id": "1903.07836", "submitter": "Zhe Chen", "authors": "Zhe Chen, Xiao-Jun Wu and Josef Kittler", "title": "Non-negative representation based discriminative dictionary learning for\n  face recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a non-negative representation based discriminative\ndictionary learning algorithm (NRDL) for multicategory face classification. In\ncontrast to traditional dictionary learning methods, NRDL investigates the use\nof non-negative representation (NR), which contributes to learning\ndiscriminative dictionary atoms. In order to make the learned dictionary more\nsuitable for classification, NRDL seamlessly incorporates nonnegative\nrepresentation constraint, discriminative dictionary learning and linear\nclassifier training into a unified model. Specifically, NRDL introduces a\npositive constraint on representation matrix to find distinct atoms from\nheterogeneous training samples, which results in sparse and discriminative\nrepresentation. Moreover, a discriminative dictionary encouraging function is\nproposed to enhance the uniqueness of class-specific sub-dictionaries.\nMeanwhile, an inter-class incoherence constraint and a compact graph based\nregularization term are constructed to respectively improve the\ndiscriminability of learned classifier. Experimental results on several\nbenchmark face data sets verify the advantages of our NRDL algorithm over the\nstate-of-the-art dictionary learning methods.\n", "versions": [{"version": "v1", "created": "Tue, 19 Mar 2019 05:08:21 GMT"}, {"version": "v2", "created": "Sat, 28 Sep 2019 09:23:04 GMT"}], "update_date": "2019-10-01", "authors_parsed": [["Chen", "Zhe", ""], ["Wu", "Xiao-Jun", ""], ["Kittler", "Josef", ""]]}, {"id": "1903.07840", "submitter": "John Skinner", "authors": "John Skinner, David Hall, Haoyang Zhang, Feras Dayoub, Niko\n  S\\\"underhauf", "title": "The Probabilistic Object Detection Challenge", "comments": "4 pages, workshop paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new challenge for computer and robotic vision, the first ACRV\nRobotic Vision Challenge, Probabilistic Object Detection. Probabilistic object\ndetection is a new variation on traditional object detection tasks, requiring\nestimates of spatial and semantic uncertainty. We extend the traditional\nbounding box format of object detection to express spatial uncertainty using\ngaussian distributions for the box corners. The challenge introduces a new test\ndataset of video sequences, which are designed to more closely resemble the\nkind of data available to a robotic system. We evaluate probabilistic\ndetections using a new probability-based detection quality (PDQ) measure. The\ngoal in creating this challenge is to draw the computer and robotic vision\ncommunities together, toward applying object detection solutions for practical\nrobotics applications.\n", "versions": [{"version": "v1", "created": "Tue, 19 Mar 2019 05:18:52 GMT"}, {"version": "v2", "created": "Mon, 8 Apr 2019 00:58:24 GMT"}], "update_date": "2019-04-09", "authors_parsed": [["Skinner", "John", ""], ["Hall", "David", ""], ["Zhang", "Haoyang", ""], ["Dayoub", "Feras", ""], ["S\u00fcnderhauf", "Niko", ""]]}, {"id": "1903.07864", "submitter": "Junting Zhang", "authors": "Junting Zhang, Jie Zhang, Shalini Ghosh, Dawei Li, Serafettin Tasci,\n  Larry Heck, Heming Zhang, C.-C. Jay Kuo", "title": "Class-incremental Learning via Deep Model Consolidation", "comments": "WACV 2020 camera-ready", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks (DNNs) often suffer from \"catastrophic forgetting\"\nduring incremental learning (IL) --- an abrupt degradation of performance on\nthe original set of classes when the training objective is adapted to a newly\nadded set of classes. Existing IL approaches tend to produce a model that is\nbiased towards either the old classes or new classes, unless with the help of\nexemplars of the old data. To address this issue, we propose a\nclass-incremental learning paradigm called Deep Model Consolidation (DMC),\nwhich works well even when the original training data is not available. The\nidea is to first train a separate model only for the new classes, and then\ncombine the two individual models trained on data of two distinct set of\nclasses (old classes and new classes) via a novel double distillation training\nobjective. The two existing models are consolidated by exploiting publicly\navailable unlabeled auxiliary data. This overcomes the potential difficulties\ndue to the unavailability of original training data. Compared to the\nstate-of-the-art techniques, DMC demonstrates significantly better performance\nin image classification (CIFAR-100 and CUB-200) and object detection (PASCAL\nVOC 2007) in the single-headed IL setting.\n", "versions": [{"version": "v1", "created": "Tue, 19 Mar 2019 07:20:38 GMT"}, {"version": "v2", "created": "Thu, 28 Mar 2019 00:06:32 GMT"}, {"version": "v3", "created": "Wed, 14 Aug 2019 06:15:25 GMT"}, {"version": "v4", "created": "Thu, 16 Jan 2020 02:21:49 GMT"}], "update_date": "2020-01-17", "authors_parsed": [["Zhang", "Junting", ""], ["Zhang", "Jie", ""], ["Ghosh", "Shalini", ""], ["Li", "Dawei", ""], ["Tasci", "Serafettin", ""], ["Heck", "Larry", ""], ["Zhang", "Heming", ""], ["Kuo", "C. -C. Jay", ""]]}, {"id": "1903.07868", "submitter": "Huibing Wang", "authors": "Jinjia Peng, Huibing Wang, Tongtong Zhao and Xianping Fu", "title": "Cross Domain Knowledge Transfer for Unsupervised Vehicle\n  Re-identification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Vehicle re-identification (reID) is to identify a target vehicle in different\ncameras with non-overlapping views. When deploy the well-trained model to a new\ndataset directly, there is a severe performance drop because of differences\namong datasets named domain bias. To address this problem, this paper proposes\nan domain adaptation framework which contains an image-to-image translation\nnetwork named vehicle transfer generative adversarial network (VTGAN) and an\nattention-based feature learning network (ATTNet). VTGAN could make images from\nthe source domain (well-labeled) have the style of target domain (unlabeled)\nand preserve identity information of source domain. To further improve the\ndomain adaptation ability for various backgrounds, ATTNet is proposed to train\ngenerated images with the attention structure for vehicle reID. Comprehensive\nexperimental results clearly demonstrate that our method achieves excellent\nperformance on VehicleID dataset.\n", "versions": [{"version": "v1", "created": "Tue, 19 Mar 2019 07:35:58 GMT"}], "update_date": "2019-03-20", "authors_parsed": [["Peng", "Jinjia", ""], ["Wang", "Huibing", ""], ["Zhao", "Tongtong", ""], ["Fu", "Xianping", ""]]}, {"id": "1903.07873", "submitter": "Rohan Ghosh", "authors": "Rohan Ghosh, Siyi Tang, Mahdi Rasouli, Nitish Thakor, Sunil Kukreja", "title": "Pose-Invariant Object Recognition for Event-Based Vision with Slow-ELM", "comments": "Appeared in 25th International Conference on Artificial Neural\n  Networks (ICANN), Barcelona, Spain", "journal-ref": null, "doi": "10.1007/978-3-319-44781-0_54", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neuromorphic image sensors produce activity-driven spiking output at every\npixel. These low-power consuming imagers which encode visual change information\nin the form of spikes help reduce computational overhead and realize complex\nreal-time systems; object recognition and pose-estimation to name a few.\nHowever, there exists a lack of algorithms in event-based vision aimed towards\ncapturing invariance to transformations. In this work, we propose a methodology\nfor recognizing objects invariant to their pose with the Dynamic Vision Sensor\n(DVS). A novel slow-ELM architecture is proposed which combines the\neffectiveness of Extreme Learning Machines and Slow Feature Analysis. The\nsystem, tested on an Intel Core i5-4590 CPU, can perform 10,000 classifications\nper second and achieves 1% classification error for 8 objects with views\naccumulated over 90 degrees of 2D pose.\n", "versions": [{"version": "v1", "created": "Tue, 19 Mar 2019 08:02:42 GMT"}], "update_date": "2019-03-20", "authors_parsed": [["Ghosh", "Rohan", ""], ["Tang", "Siyi", ""], ["Rasouli", "Mahdi", ""], ["Thakor", "Nitish", ""], ["Kukreja", "Sunil", ""]]}, {"id": "1903.07912", "submitter": "Vitaliy Lyudvichenko", "authors": "Vitaliy Lyudvichenko, Mikhail Erofeev, Alexander Ploshkin, Dmitriy\n  Vatolin", "title": "Improving Video Compression With Deep Visual-Attention Models", "comments": null, "journal-ref": "Proceedings of the 2019 International Conference on Intelligent\n  Medicine and Image Processing", "doi": "10.1145/3332340.3332358", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recent advances in deep learning have markedly improved the quality of\nvisual-attention modelling. In this work we apply these advances to video\ncompression.\n  We propose a compression method that uses a saliency model to adaptively\ncompress frame areas in accordance with their predicted saliency. We selected\nthree state-of-the-art saliency models, adapted them for video compression and\nanalyzed their results. The analysis includes objective evaluation of the\nmodels as well as objective and subjective evaluation of the compressed videos.\n  Our method, which is based on the x264 video codec, can produce videos with\nthe same visual quality as regular x264, but it reduces the bitrate by 25%\naccording to the objective evaluation and by 17% according to the subjective\none. Also, both the subjective and objective evaluations demonstrate that\nsaliency models can compete with gaze maps for a single observer.\n  Our method can extend to most video bitstream formats and can improve video\ncompression quality without requiring a switch to a new video encoding\nstandard.\n", "versions": [{"version": "v1", "created": "Tue, 19 Mar 2019 10:06:04 GMT"}], "update_date": "2019-07-25", "authors_parsed": [["Lyudvichenko", "Vitaliy", ""], ["Erofeev", "Mikhail", ""], ["Ploshkin", "Alexander", ""], ["Vatolin", "Dmitriy", ""]]}, {"id": "1903.07916", "submitter": "Rui Zeng", "authors": "Rui Zeng, Zongyuan Ge, Simon Denman, Sridha Sridharan, Clinton Fookes", "title": "Geometry-constrained Car Recognition Using a 3D Perspective Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel learning framework for vehicle recognition from a single\nRGB image. Unlike existing methods which only use attention mechanisms to\nlocate 2D discriminative information, our work learns a novel 3D perspective\nfeature representation of a vehicle, which is then fused with 2D appearance\nfeature to predict the category. The framework is composed of a global network\n(GN), a 3D perspective network (3DPN), and a fusion network. The GN is used to\nlocate the region of interest (RoI) and generate the 2D global feature. With\nthe assistance of the RoI, the 3DPN estimates the 3D bounding box under the\nguidance of the proposed vanishing point loss, which provides a perspective\ngeometry constraint. Then the proposed 3D representation is generated by\neliminating the viewpoint variance of the 3D bounding box using perspective\ntransformation. Finally, the 3D and 2D feature are fused to predict the\ncategory of the vehicle. We present qualitative and quantitative results on the\nvehicle classification and verification tasks in the BoxCars dataset. The\nresults demonstrate that, by learning such a concise 3D representation, we can\nachieve superior performance to methods that only use 2D information while\nretain 3D meaningful information without the challenge of requiring a 3D CAD\nmodel.\n", "versions": [{"version": "v1", "created": "Tue, 19 Mar 2019 10:17:47 GMT"}, {"version": "v2", "created": "Wed, 20 Mar 2019 05:09:01 GMT"}, {"version": "v3", "created": "Mon, 18 Nov 2019 00:16:24 GMT"}], "update_date": "2019-11-19", "authors_parsed": [["Zeng", "Rui", ""], ["Ge", "Zongyuan", ""], ["Denman", "Simon", ""], ["Sridharan", "Sridha", ""], ["Fookes", "Clinton", ""]]}, {"id": "1903.07933", "submitter": "Christoph Sch\\\"oller", "authors": "Christoph Sch\\\"oller, Vincent Aravantinos, Florian Lay, and Alois\n  Knoll", "title": "What the Constant Velocity Model Can Teach Us About Pedestrian Motion\n  Prediction", "comments": "Accepted for publication in the IEEE Robotics and Automation Letters\n  (RA-L) and for presentation at the 2020 International Conference on Robotics\n  and Automation (ICRA)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pedestrian motion prediction is a fundamental task for autonomous robots and\nvehicles to operate safely. In recent years many complex approaches based on\nneural networks have been proposed to address this problem. In this work we\nshow that - surprisingly - a simple Constant Velocity Model can outperform even\nstate-of-the-art neural models. This indicates that either neural networks are\nnot able to make use of the additional information they are provided with, or\nthat this information is not as relevant as commonly believed. Therefore, we\nanalyze how neural networks process their input and how it impacts their\npredictions. Our analysis reveals pitfalls in training neural networks for\npedestrian motion prediction and clarifies false assumptions about the problem\nitself. In particular, neural networks implicitly learn environmental priors\nthat negatively impact their generalization capability, the motion history of\npedestrians is irrelevant and interactions are too complex to predict. Our work\nshows how neural networks for pedestrian motion prediction can be thoroughly\nevaluated and our results indicate which research directions for neural motion\nprediction are promising in future.\n", "versions": [{"version": "v1", "created": "Tue, 19 Mar 2019 10:56:44 GMT"}, {"version": "v2", "created": "Tue, 17 Sep 2019 13:25:11 GMT"}, {"version": "v3", "created": "Wed, 22 Jan 2020 11:52:02 GMT"}], "update_date": "2020-01-23", "authors_parsed": [["Sch\u00f6ller", "Christoph", ""], ["Aravantinos", "Vincent", ""], ["Lay", "Florian", ""], ["Knoll", "Alois", ""]]}, {"id": "1903.07949", "submitter": "Xiangxiang Chu", "authors": "Hailong Ma and Xiangxiang Chu and Bo Zhang and Shaohua Wan and Bo\n  Zhang", "title": "A Matrix-in-matrix Neural Network for Image Super Resolution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, deep learning methods have achieved impressive results with\nhigher peak signal-to-noise ratio in single image super-resolution (SISR) tasks\nby utilizing deeper layers. However, their application is quite limited since\nthey require high computing power. In addition, most of the existing methods\nrarely take full advantage of the intermediate features which are helpful for\nrestoration. To address these issues, we propose a moderate-size SISR net work\nnamed matrixed channel attention network (MCAN) by constructing a matrix\nensemble of multi-connected channel attention blocks (MCAB). Several models of\ndifferent sizes are released to meet various practical requirements.\nConclusions can be drawn from our extensive benchmark experiments that the\nproposed models achieve better performance with much fewer multiply-adds and\nparameters. Our models will be made publicly available.\n", "versions": [{"version": "v1", "created": "Tue, 19 Mar 2019 11:39:54 GMT"}], "update_date": "2019-03-20", "authors_parsed": [["Ma", "Hailong", ""], ["Chu", "Xiangxiang", ""], ["Zhang", "Bo", ""], ["Wan", "Shaohua", ""], ["Zhang", "Bo", ""]]}, {"id": "1903.07973", "submitter": "Moshe Lichtenstein", "authors": "Moshe Lichtenstein, Gautam Pai, Ron Kimmel", "title": "Deep Eikonal Solvers", "comments": "Accepted for oral presentation at Seventh International Conference on\n  Scale Space and Variational Methods in Computer Vision (SSVM) 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A deep learning approach to numerically approximate the solution to the\nEikonal equation is introduced. The proposed method is built on the fast\nmarching scheme which comprises of two components: a local numerical solver and\nan update scheme. We replace the formulaic local numerical solver with a\ntrained neural network to provide highly accurate estimates of local distances\nfor a variety of different geometries and sampling conditions. Our learning\napproach generalizes not only to flat Euclidean domains but also to curved\nsurfaces enabled by the incorporation of certain invariant features in the\nneural network architecture. We show a considerable gain in performance,\nvalidated by smaller errors and higher orders of accuracy for the numerical\nsolutions of the Eikonal equation computed on different surfaces The proposed\napproach leverages the approximation power of neural networks to enhance the\nperformance of numerical algorithms, thereby, connecting the somewhat disparate\nthemes of numerical geometry and learning.\n", "versions": [{"version": "v1", "created": "Tue, 19 Mar 2019 13:03:51 GMT"}], "update_date": "2019-03-20", "authors_parsed": [["Lichtenstein", "Moshe", ""], ["Pai", "Gautam", ""], ["Kimmel", "Ron", ""]]}, {"id": "1903.07992", "submitter": "Manuel Fritsche", "authors": "Thomas Ziegler, Manuel Fritsche, Lorenz Kuhn, Konstantin Donhauser", "title": "Efficient Smoothing of Dilated Convolutions for Image Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dilated Convolutions have been shown to be highly useful for the task of\nimage segmentation. By introducing gaps into convolutional filters, they enable\nthe use of larger receptive fields without increasing the original kernel size.\nEven though this allows for the inexpensive capturing of features at different\nscales, the structure of the dilated convolutional filter leads to a loss of\ninformation. We hypothesise that inexpensive modifications to Dilated\nConvolutional Neural Networks, such as additional averaging layers, could\novercome this limitation. In this project we test this hypothesis by evaluating\nthe effect of these modifications for a state-of-the art image segmentation\nsystem and compare them to existing approaches with the same objective. Our\nexperiments show that our proposed methods improve the performance of dilated\nconvolutions for image segmentation. Crucially, our modifications achieve these\nresults at a much lower computational cost than previous smoothing approaches.\n", "versions": [{"version": "v1", "created": "Tue, 19 Mar 2019 13:32:27 GMT"}], "update_date": "2019-03-20", "authors_parsed": [["Ziegler", "Thomas", ""], ["Fritsche", "Manuel", ""], ["Kuhn", "Lorenz", ""], ["Donhauser", "Konstantin", ""]]}, {"id": "1903.08051", "submitter": "Jie Cai", "authors": "Jie Cai, Zibo Meng, Ahmed Shehab Khan, Zhiyuan Li, James O'Reilly,\n  Shizhong Han, Yan Tong", "title": "Identity-Free Facial Expression Recognition using conditional Generative\n  Adversarial Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A novel Identity-Free conditional Generative Adversarial Network (IF-GAN) was\nproposed for Facial Expression Recognition (FER) to explicitly reduce high\ninter-subject variations caused by identity-related facial attributes, e.g.,\nage, race, and gender. As part of an end-to-end system, a cGAN was designed to\ntransform a given input facial expression image to an \"average\" identity face\nwith the same expression as the input. Then, identity-free FER is possible\nsince the generated images have the same synthetic \"average\" identity and\ndiffer only in their displayed expressions. Experiments on four facial\nexpression datasets, one with spontaneous expressions, show that IF-GAN\noutperforms the baseline CNN and achieves state-of-the-art performance for FER.\n", "versions": [{"version": "v1", "created": "Tue, 19 Mar 2019 15:31:53 GMT"}, {"version": "v2", "created": "Thu, 20 May 2021 21:25:23 GMT"}], "update_date": "2021-05-24", "authors_parsed": [["Cai", "Jie", ""], ["Meng", "Zibo", ""], ["Khan", "Ahmed Shehab", ""], ["Li", "Zhiyuan", ""], ["O'Reilly", "James", ""], ["Han", "Shizhong", ""], ["Tong", "Yan", ""]]}, {"id": "1903.08066", "submitter": "Sambhav R. Jain", "authors": "Sambhav R. Jain, Albert Gural, Michael Wu, Chris H. Dick", "title": "Trained Quantization Thresholds for Accurate and Efficient Fixed-Point\n  Inference of Deep Neural Networks", "comments": "Link to Conference (Oral & Poster) Schedule -\n  https://mlsys.org/Conferences/2020/ScheduleMultitrack?event=1431", "journal-ref": "Proceedings of the 3rd Machine Learning and Systems (MLSys)\n  Conference, Austin, TX, USA, 2020", "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a method of training quantization thresholds (TQT) for uniform\nsymmetric quantizers using standard backpropagation and gradient descent.\nContrary to prior work, we show that a careful analysis of the straight-through\nestimator for threshold gradients allows for a natural range-precision\ntrade-off leading to better optima. Our quantizers are constrained to use\npower-of-2 scale-factors and per-tensor scaling of weights and activations to\nmake it amenable for hardware implementations. We present analytical support\nfor the general robustness of our methods and empirically validate them on\nvarious CNNs for ImageNet classification. We are able to achieve\nnear-floating-point accuracy on traditionally difficult networks such as\nMobileNets with less than 5 epochs of quantized (8-bit) retraining. Finally, we\npresent Graffitist, a framework that enables automatic quantization of\nTensorFlow graphs for TQT (available at https://github.com/Xilinx/graffitist ).\n", "versions": [{"version": "v1", "created": "Tue, 19 Mar 2019 15:50:24 GMT"}, {"version": "v2", "created": "Tue, 10 Sep 2019 06:24:16 GMT"}, {"version": "v3", "created": "Fri, 28 Feb 2020 18:21:29 GMT"}], "update_date": "2020-03-02", "authors_parsed": [["Jain", "Sambhav R.", ""], ["Gural", "Albert", ""], ["Wu", "Michael", ""], ["Dick", "Chris H.", ""]]}, {"id": "1903.08072", "submitter": "Samy Blusseau", "authors": "Yunxiang Zhang (CMM, LTCI), Samy Blusseau (CMM), Santiago\n  Velasco-Forero (CMM), Isabelle Bloch (LTCI), Jesus Angulo (CMM)", "title": "Max-plus Operators Applied to Filter Selection and Model Pruning in\n  Neural Networks", "comments": null, "journal-ref": "International Symposium on Mathematical Morphology, Jul 2019,\n  Saarbr{\\\"u}cken, Germany", "doi": null, "report-no": null, "categories": "math.ST cs.CV cs.LG cs.NE stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Following recent advances in morphological neural networks, we propose to\nstudy in more depth how Max-plus operators can be exploited to define\nmorphological units and how they behave when incorporated in layers of\nconventional neural networks. Besides showing that they can be easily\nimplemented with modern machine learning frameworks , we confirm and extend the\nobservation that a Max-plus layer can be used to select important filters and\nreduce redundancy in its previous layer, without incurring performance loss.\nExperimental results demonstrate that the filter selection strategy enabled by\na Max-plus is highly efficient and robust, through which we successfully\nperformed model pruning on different neural network architectures. We also\npoint out that there is a close connection between Maxout networks and our\npruned Max-plus networks by comparing their respective characteristics. The\ncode for reproducing our experiments is available online.\n", "versions": [{"version": "v1", "created": "Tue, 19 Mar 2019 15:58:43 GMT"}, {"version": "v2", "created": "Mon, 8 Apr 2019 12:51:57 GMT"}], "update_date": "2019-04-09", "authors_parsed": [["Zhang", "Yunxiang", "", "CMM, LTCI"], ["Blusseau", "Samy", "", "CMM"], ["Velasco-Forero", "Santiago", "", "CMM"], ["Bloch", "Isabelle", "", "LTCI"], ["Angulo", "Jesus", "", "CMM"]]}, {"id": "1903.08094", "submitter": "Clara Fernandez Labrador", "authors": "Clara Fernandez-Labrador, Jose M. Facil, Alejandro Perez-Yus, C\\'edric\n  Demonceaux, Javier Civera and Jose J. Guerrero", "title": "Corners for Layout: End-to-End Layout Recovery from 360 Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of 3D layout recovery in indoor scenes has been a core research\ntopic for over a decade. However, there are still several major challenges that\nremain unsolved. Among the most relevant ones, a major part of the\nstate-of-the-art methods make implicit or explicit assumptions on the scenes --\ne.g. box-shaped or Manhattan layouts. Also, current methods are computationally\nexpensive and not suitable for real-time applications like robot navigation and\nAR/VR. In this work we present CFL (Corners for Layout), the first end-to-end\nmodel for 3D layout recovery on 360 images. Our experimental results show that\nwe outperform the state of the art relaxing assumptions about the scene and at\na lower cost. We also show that our model generalizes better to camera position\nvariations than conventional approaches by using EquiConvs, a type of\nconvolution applied directly on the sphere projection and hence invariant to\nthe equirectangular distortions.\n  CFL Webpage: https://cfernandezlab.github.io/CFL/\n", "versions": [{"version": "v1", "created": "Tue, 19 Mar 2019 16:32:06 GMT"}, {"version": "v2", "created": "Mon, 25 Mar 2019 09:49:09 GMT"}], "update_date": "2019-03-26", "authors_parsed": [["Fernandez-Labrador", "Clara", ""], ["Facil", "Jose M.", ""], ["Perez-Yus", "Alejandro", ""], ["Demonceaux", "C\u00e9dric", ""], ["Civera", "Javier", ""], ["Guerrero", "Jose J.", ""]]}, {"id": "1903.08111", "submitter": "Marie-Caroline Corbineau", "authors": "Corbineau Marie-Caroline, Kouam\\'e Denis, Chouzenoux Emilie, Tourneret\n  Jean-Yves, Pesquet Jean-Christophe", "title": "Preconditioned P-ULA for Joint Deconvolution-Segmentation of Ultrasound\n  Images -- Extended Version", "comments": null, "journal-ref": "In IEEE Signal Processing Letters, vol.26, no.10, pp.1456-1460\n  (2019)", "doi": "10.1109/LSP.2019.2935610", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Joint deconvolution and segmentation of ultrasound images is a challenging\nproblem in medical imaging. By adopting a hierarchical Bayesian model, we\npropose an accelerated Markov chain Monte Carlo scheme where the tissue\nreflectivity function is sampled thanks to a recently introduced proximal\nunadjusted Langevin algorithm. This new approach is combined with a\nforward-backward step and a preconditioning strategy to accelerate the\nconvergence, and with a method based on the majorization-minimization principle\nto solve the inner nonconvex minimization problems. As demonstrated in\nnumerical experiments conducted on both simulated and in vivo ultrasound\nimages, the proposed method provides high-quality restoration and segmentation\nresults and is up to six times faster than an existing Hamiltonian Monte Carlo\nmethod.\n", "versions": [{"version": "v1", "created": "Tue, 19 Mar 2019 16:59:32 GMT"}, {"version": "v2", "created": "Fri, 14 Jun 2019 09:57:49 GMT"}, {"version": "v3", "created": "Thu, 1 Aug 2019 16:07:06 GMT"}, {"version": "v4", "created": "Tue, 21 Jan 2020 19:04:12 GMT"}], "update_date": "2020-01-23", "authors_parsed": [["Marie-Caroline", "Corbineau", ""], ["Denis", "Kouam\u00e9", ""], ["Emilie", "Chouzenoux", ""], ["Jean-Yves", "Tourneret", ""], ["Jean-Christophe", "Pesquet", ""]]}, {"id": "1903.08152", "submitter": "Huibing Wang", "authors": "Tongtong Zhao, Yuxiao Yan, Jinjia Peng, Huibing Wang and Xianping Fu", "title": "Mask-guided Style Transfer Network for Purifying Real Images", "comments": "arXiv admin note: substantial text overlap with arXiv:1903.05820", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, the progress of learning-by-synthesis has proposed a training model\nfor synthetic images, which can effectively reduce the cost of human and\nmaterial resources. However, due to the different distribution of synthetic\nimages compared with real images, the desired performance cannot be achieved.\nTo solve this problem, the previous method learned a model to improve the\nrealism of the synthetic images. Different from the previous methods, this\npaper try to purify real image by extracting discriminative and robust features\nto convert outdoor real images to indoor synthetic images. In this paper, we\nfirst introduce the segmentation masks to construct RGB-mask pairs as inputs,\nthen we design a mask-guided style transfer network to learn style features\nseparately from the attention and bkgd(background) regions and learn content\nfeatures from full and attention region. Moreover, we propose a novel\nregion-level task-guided loss to restrain the features learnt from style and\ncontent. Experiments were performed using mixed studies (qualitative and\nquantitative) methods to demonstrate the possibility of purifying real images\nin complex directions. We evaluate the proposed method on various public\ndatasets, including LPW, COCO and MPIIGaze. Experimental results show that the\nproposed method is effective and achieves the state-of-the-art results.\n", "versions": [{"version": "v1", "created": "Tue, 19 Mar 2019 03:54:55 GMT"}], "update_date": "2019-03-21", "authors_parsed": [["Zhao", "Tongtong", ""], ["Yan", "Yuxiao", ""], ["Peng", "Jinjia", ""], ["Wang", "Huibing", ""], ["Fu", "Xianping", ""]]}, {"id": "1903.08205", "submitter": "Fausto Milletari", "authors": "Tomas Sakinis, Fausto Milletari, Holger Roth, Panagiotis Korfiatis,\n  Petro Kostandy, Kenneth Philbrick, Zeynettin Akkus, Ziyue Xu, Daguang Xu,\n  Bradley J. Erickson", "title": "Interactive segmentation of medical images through fully convolutional\n  neural networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image segmentation plays an essential role in medicine for both diagnostic\nand interventional tasks. Segmentation approaches are either manual,\nsemi-automated or fully-automated. Manual segmentation offers full control over\nthe quality of the results, but is tedious, time consuming and prone to\noperator bias. Fully automated methods require no human effort, but often\ndeliver sub-optimal results without providing users with the means to make\ncorrections. Semi-automated approaches keep users in control of the results by\nproviding means for interaction, but the main challenge is to offer a good\ntrade-off between precision and required interaction. In this paper we present\na deep learning (DL) based semi-automated segmentation approach that aims to be\na \"smart\" interactive tool for region of interest delineation in medical\nimages. We demonstrate its use for segmenting multiple organs on computed\ntomography (CT) of the abdomen. Our approach solves some of the most pressing\nclinical challenges: (i) it requires only one to a few user clicks to deliver\nexcellent 2D segmentations in a fast and reliable fashion; (ii) it can\ngeneralize to previously unseen structures and \"corner cases\"; (iii) it\ndelivers results that can be corrected quickly in a smart and intuitive way up\nto an arbitrary degree of precision chosen by the user and (iv) ensures high\naccuracy. We present our approach and compare it to other techniques and\nprevious work to show the advantages brought by our method.\n", "versions": [{"version": "v1", "created": "Tue, 19 Mar 2019 18:28:49 GMT"}], "update_date": "2019-03-21", "authors_parsed": [["Sakinis", "Tomas", ""], ["Milletari", "Fausto", ""], ["Roth", "Holger", ""], ["Korfiatis", "Panagiotis", ""], ["Kostandy", "Petro", ""], ["Philbrick", "Kenneth", ""], ["Akkus", "Zeynettin", ""], ["Xu", "Ziyue", ""], ["Xu", "Daguang", ""], ["Erickson", "Bradley J.", ""]]}, {"id": "1903.08225", "submitter": "Dimitri Zhukov", "authors": "Dimitri Zhukov, Jean-Baptiste Alayrac, Ramazan Gokberk Cinbis, David\n  Fouhey, Ivan Laptev and Josef Sivic", "title": "Cross-task weakly supervised learning from instructional videos", "comments": "18 pages, 17 figures, to be published in proceedings of the CVPR,\n  2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we investigate learning visual models for the steps of ordinary\ntasks using weak supervision via instructional narrations and an ordered list\nof steps instead of strong supervision via temporal annotations. At the heart\nof our approach is the observation that weakly supervised learning may be\neasier if a model shares components while learning different steps: `pour egg'\nshould be trained jointly with other tasks involving `pour' and `egg'. We\nformalize this in a component model for recognizing steps and a weakly\nsupervised learning framework that can learn this model under temporal\nconstraints from narration and the list of steps. Past data does not permit\nsystematic studying of sharing and so we also gather a new dataset, CrossTask,\naimed at assessing cross-task sharing. Our experiments demonstrate that sharing\nacross tasks improves performance, especially when done at the component level\nand that our component model can parse previously unseen tasks by virtue of its\ncompositionality.\n", "versions": [{"version": "v1", "created": "Tue, 19 Mar 2019 19:30:29 GMT"}, {"version": "v2", "created": "Mon, 29 Apr 2019 10:08:57 GMT"}], "update_date": "2019-04-30", "authors_parsed": [["Zhukov", "Dimitri", ""], ["Alayrac", "Jean-Baptiste", ""], ["Cinbis", "Ramazan Gokberk", ""], ["Fouhey", "David", ""], ["Laptev", "Ivan", ""], ["Sivic", "Josef", ""]]}, {"id": "1903.08226", "submitter": "Aythami Morales", "authors": "R. Castrillon, A. Acien, J.R. Orozco-Arroyave, A. Morales, J.F.\n  Vargas, R.Vera-Rodr{\\i}guez, J. Fierrez, J. Ortega-Garcia, A. Villegas", "title": "Characterization of the Handwriting Skills as a Biomarker for Parkinson\n  Disease", "comments": "Accepted in 14th IEEE International Conference on Automatic Face &\n  Gesture Recognition (FG 2019) - Human Health Monitoring Based on Computer\n  Vision", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we evaluate the suitability of handwriting patterns as\npotential biomarkers to model Parkinson disease (PD). Although the study of PD\nis attracting the interest of many researchers around the world, databases to\nevaluate handwriting patterns are scarce and knowledge about patterns\nassociated to PD is limited and biased to the existing datasets. This paper\nintroduces a database with a total of 935 handwriting tasks collected from 55\nPD patients and 94 healthy controls (45 young and 49 old). Three feature sets\nare extracted from the signals: neuromotor, kinematic, and nonlinear dynamic.\nDifferent classifiers are used to discriminate between PD and healthy subjects:\nsupport vector machines, knearest neighbors, and a multilayer perceptron. The\nproposed features and classifiers enable to detect PD with accuracies between\n81% and 97%. Additionally, new insights are presented on the utility of the\nstudied features for monitoring and detecting PD.\n", "versions": [{"version": "v1", "created": "Tue, 19 Mar 2019 19:32:00 GMT"}], "update_date": "2019-03-21", "authors_parsed": [["Castrillon", "R.", ""], ["Acien", "A.", ""], ["Orozco-Arroyave", "J. R.", ""], ["Morales", "A.", ""], ["Vargas", "J. F.", ""], ["Vera-Rodr\u0131guez", "R.", ""], ["Fierrez", "J.", ""], ["Ortega-Garcia", "J.", ""], ["Villegas", "A.", ""]]}, {"id": "1903.08297", "submitter": "Nan Wu", "authors": "Nan Wu, Jason Phang, Jungkyu Park, Yiqiu Shen, Zhe Huang, Masha Zorin,\n  Stanis{\\l}aw Jastrz\\k{e}bski, Thibault F\\'evry, Joe Katsnelson, Eric Kim,\n  Stacey Wolfson, Ujas Parikh, Sushma Gaddam, Leng Leng Young Lin, Kara Ho,\n  Joshua D. Weinstein, Beatriu Reig, Yiming Gao, Hildegard Toth, Kristine\n  Pysarenko, Alana Lewin, Jiyon Lee, Krystal Airola, Eralda Mema, Stephanie\n  Chung, Esther Hwang, Naziya Samreen, S. Gene Kim, Laura Heacock, Linda Moy,\n  Kyunghyun Cho and Krzysztof J. Geras", "title": "Deep Neural Networks Improve Radiologists' Performance in Breast Cancer\n  Screening", "comments": "MIDL 2019 [arXiv:1907.08612]", "journal-ref": null, "doi": null, "report-no": "MIDL/2019/ExtendedAbstract/SkxYez76FE", "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a deep convolutional neural network for breast cancer screening\nexam classification, trained and evaluated on over 200,000 exams (over\n1,000,000 images). Our network achieves an AUC of 0.895 in predicting whether\nthere is a cancer in the breast, when tested on the screening population. We\nattribute the high accuracy of our model to a two-stage training procedure,\nwhich allows us to use a very high-capacity patch-level network to learn from\npixel-level labels alongside a network learning from macroscopic breast-level\nlabels. To validate our model, we conducted a reader study with 14 readers,\neach reading 720 screening mammogram exams, and find our model to be as\naccurate as experienced radiologists when presented with the same data.\nFinally, we show that a hybrid model, averaging probability of malignancy\npredicted by a radiologist with a prediction of our neural network, is more\naccurate than either of the two separately. To better understand our results,\nwe conduct a thorough analysis of our network's performance on different\nsubpopulations of the screening population, model design, training procedure,\nerrors, and properties of its internal representations.\n", "versions": [{"version": "v1", "created": "Wed, 20 Mar 2019 00:51:01 GMT"}], "update_date": "2019-08-08", "authors_parsed": [["Wu", "Nan", ""], ["Phang", "Jason", ""], ["Park", "Jungkyu", ""], ["Shen", "Yiqiu", ""], ["Huang", "Zhe", ""], ["Zorin", "Masha", ""], ["Jastrz\u0119bski", "Stanis\u0142aw", ""], ["F\u00e9vry", "Thibault", ""], ["Katsnelson", "Joe", ""], ["Kim", "Eric", ""], ["Wolfson", "Stacey", ""], ["Parikh", "Ujas", ""], ["Gaddam", "Sushma", ""], ["Lin", "Leng Leng Young", ""], ["Ho", "Kara", ""], ["Weinstein", "Joshua D.", ""], ["Reig", "Beatriu", ""], ["Gao", "Yiming", ""], ["Toth", "Hildegard", ""], ["Pysarenko", "Kristine", ""], ["Lewin", "Alana", ""], ["Lee", "Jiyon", ""], ["Airola", "Krystal", ""], ["Mema", "Eralda", ""], ["Chung", "Stephanie", ""], ["Hwang", "Esther", ""], ["Samreen", "Naziya", ""], ["Kim", "S. Gene", ""], ["Heacock", "Laura", ""], ["Moy", "Linda", ""], ["Cho", "Kyunghyun", ""], ["Geras", "Krzysztof J.", ""]]}, {"id": "1903.08347", "submitter": "Anant Gupta", "authors": "Anant Gupta, Atul Ingle, Andreas Velten, Mohit Gupta", "title": "Photon-Flooded Single-Photon 3D Cameras", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Single photon avalanche diodes (SPADs) are starting to play a pivotal role in\nthe development of photon-efficient, long-range LiDAR systems. However, due to\nnon-linearities in their image formation model, a high photon flux (e.g., due\nto strong sunlight) leads to distortion of the incident temporal waveform, and\npotentially, large depth errors. Operating SPADs in low flux regimes can\nmitigate these distortions, but, often requires attenuating the signal and\nthus, results in low signal-to-noise ratio. In this paper, we address the\nfollowing basic question: what is the optimal photon flux that a SPAD-based\nLiDAR should be operated in? We derive a closed form expression for the optimal\nflux, which is quasi-depth-invariant, and depends on the ambient light\nstrength. The optimal flux is lower than what a SPAD typically measures in real\nworld scenarios, but surprisingly, considerably higher than what is\nconventionally suggested for avoiding distortions. We propose a simple,\nadaptive approach for achieving the optimal flux by attenuating incident flux\nbased on an estimate of ambient light strength. Using extensive simulations and\na hardware prototype, we show that the optimal flux criterion holds for several\ndepth estimators, under a wide range of illumination conditions.\n", "versions": [{"version": "v1", "created": "Wed, 20 Mar 2019 05:29:18 GMT"}, {"version": "v2", "created": "Mon, 29 Apr 2019 04:51:28 GMT"}], "update_date": "2019-04-30", "authors_parsed": [["Gupta", "Anant", ""], ["Ingle", "Atul", ""], ["Velten", "Andreas", ""], ["Gupta", "Mohit", ""]]}, {"id": "1903.08352", "submitter": "Xiaotong Chen", "authors": "Xiaotong Chen, Rui Chen, Zhiqiang Sui, Zhefan Ye, Yanqi Liu, R. Iris\n  Bahar and Odest Chadwicke Jenkins", "title": "GRIP: Generative Robust Inference and Perception for Semantic Robot\n  Manipulation in Adversarial Environments", "comments": "9 pages, 7 figures, published on IROS 2019. contact: cxt@umich.edu", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advancements have led to a proliferation of machine learning systems\nused to assist humans in a wide range of tasks. However, we are still far from\naccurate, reliable, and resource-efficient operations of these systems. For\nrobot perception, convolutional neural networks (CNNs) for object detection and\npose estimation are recently coming into widespread use. However, neural\nnetworks are known to suffer overfitting during training process and are less\nrobust within unseen conditions, which are especially vulnerable to adversarial\nscenarios. In this work, we propose Generative Robust Inference and Perception\n(GRIP) as a two-stage object detection and pose estimation system that aims to\ncombine relative strengths of discriminative CNNs and generative inference\nmethods to achieve robust estimation. Our results show that a second stage of\nsample-based generative inference is able to recover from false object\ndetection by CNNs, and produce robust estimations in adversarial conditions. We\ndemonstrate the efficacy of GRIP robustness through comparison with\nstate-of-the-art learning-based pose estimators and pick-and-place manipulation\nin dark and cluttered environments.\n", "versions": [{"version": "v1", "created": "Wed, 20 Mar 2019 06:08:27 GMT"}, {"version": "v2", "created": "Sat, 17 Aug 2019 21:45:06 GMT"}, {"version": "v3", "created": "Thu, 19 Dec 2019 06:00:54 GMT"}], "update_date": "2019-12-20", "authors_parsed": [["Chen", "Xiaotong", ""], ["Chen", "Rui", ""], ["Sui", "Zhiqiang", ""], ["Ye", "Zhefan", ""], ["Liu", "Yanqi", ""], ["Bahar", "R. Iris", ""], ["Jenkins", "Odest Chadwicke", ""]]}, {"id": "1903.08362", "submitter": "Jie Zhang", "authors": "Jie Zhang, Junting Zhang, Shalini Ghosh, Dawei Li, Jingwen Zhu, Heming\n  Zhang, Yalin Wang", "title": "Regularize, Expand and Compress: Multi-task based Lifelong Learning via\n  NonExpansive AutoML", "comments": "9 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Lifelong learning, the problem of continual learning where tasks arrive in\nsequence, has been lately attracting more attention in the computer vision\ncommunity. The aim of lifelong learning is to develop a system that can learn\nnew tasks while maintaining the performance on the previously learned tasks.\nHowever, there are two obstacles for lifelong learning of deep neural networks:\ncatastrophic forgetting and capacity limitation. To solve the above issues,\ninspired by the recent breakthroughs in automatically learning good neural\nnetwork architectures, we develop a Multi-task based lifelong learning via\nnonexpansive AutoML framework termed Regularize, Expand and Compress (REC). REC\nis composed of three stages: 1) continually learns the sequential tasks without\nthe learned tasks' data via a newly proposed multi-task weight consolidation\n(MWC) algorithm; 2) expands the network to help the lifelong learning with\npotentially improved model capability and performance by network-transformation\nbased AutoML; 3) compresses the expanded model after learning every new task to\nmaintain model efficiency and performance. The proposed MWC and REC algorithms\nachieve superior performance over other lifelong learning algorithms on four\ndifferent datasets.\n", "versions": [{"version": "v1", "created": "Wed, 20 Mar 2019 07:16:58 GMT"}], "update_date": "2019-03-21", "authors_parsed": [["Zhang", "Jie", ""], ["Zhang", "Junting", ""], ["Ghosh", "Shalini", ""], ["Li", "Dawei", ""], ["Zhu", "Jingwen", ""], ["Zhang", "Heming", ""], ["Wang", "Yalin", ""]]}, {"id": "1903.08385", "submitter": "Shuang Wu", "authors": "Shuang Wu, Guanrui Wang, Pei Tang, Feng Chen, Luping Shi", "title": "Convolution with even-sized kernels and symmetric padding", "comments": "12 pages, 4 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Compact convolutional neural networks gain efficiency mainly through\ndepthwise convolutions, expanded channels and complex topologies, which\ncontrarily aggravate the training process. Besides, 3x3 kernels dominate the\nspatial representation in these models, whereas even-sized kernels (2x2, 4x4)\nare rarely adopted. In this work, we quantify the shift problem occurs in\neven-sized kernel convolutions by an information erosion hypothesis, and\neliminate it by proposing symmetric padding on four sides of the feature maps\n(C2sp, C4sp). Symmetric padding releases the generalization capabilities of\neven-sized kernels at little computational cost, making them outperform 3x3\nkernels in image classification and generation tasks. Moreover, C2sp obtains\ncomparable accuracy to emerging compact models with much less memory and time\nconsumption during training. Symmetric padding coupled with even-sized\nconvolutions can be neatly implemented into existing frameworks, providing\neffective elements for architecture designs, especially on online and continual\nlearning occasions where training efforts are emphasized.\n", "versions": [{"version": "v1", "created": "Wed, 20 Mar 2019 08:34:20 GMT"}, {"version": "v2", "created": "Wed, 22 May 2019 03:01:43 GMT"}], "update_date": "2019-05-23", "authors_parsed": [["Wu", "Shuang", ""], ["Wang", "Guanrui", ""], ["Tang", "Pei", ""], ["Chen", "Feng", ""], ["Shi", "Luping", ""]]}, {"id": "1903.08456", "submitter": "Alessio Sarullo", "authors": "Alessio Sarullo, Tingting Mu", "title": "On Class Imbalance and Background Filtering in Visual Relationship\n  Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we investigate the problems of class imbalance and irrelevant\nrelationships in Visual Relationship Detection (VRD). State-of-the-art deep VRD\nmodels still struggle to predict uncommon classes, limiting their\napplicability. Moreover, many methods are incapable of properly filtering out\nbackground relationships while predicting relevant ones. Although these\nproblems are very apparent, they have both been overlooked so far. We analyse\nwhy this is the case and propose modifications to both model and training to\nalleviate the aforementioned issues, as well as suggesting new measures to\ncomplement existing ones and give a more holistic picture of the efficacy of a\nmodel.\n", "versions": [{"version": "v1", "created": "Wed, 20 Mar 2019 11:46:24 GMT"}, {"version": "v2", "created": "Thu, 21 Mar 2019 18:42:41 GMT"}], "update_date": "2019-03-25", "authors_parsed": [["Sarullo", "Alessio", ""], ["Mu", "Tingting", ""]]}, {"id": "1903.08469", "submitter": "Marin Or\\v{s}i\\'c", "authors": "Marin Or\\v{s}i\\'c, Ivan Kre\\v{s}o, Petra Bevandi\\'c, Sini\\v{s}a\n  \\v{S}egvi\\'c", "title": "In Defense of Pre-trained ImageNet Architectures for Real-time Semantic\n  Segmentation of Road-driving Images", "comments": "Accepted to CVPR 2019. 8 pages, 8 figures, 5 tables. PyTorch source\n  is available at https://github.com/orsic/swiftnet", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent success of semantic segmentation approaches on demanding road driving\ndatasets has spurred interest in many related application fields. Many of these\napplications involve real-time prediction on mobile platforms such as cars,\ndrones and various kinds of robots. Real-time setup is challenging due to\nextraordinary computational complexity involved. Many previous works address\nthe challenge with custom lightweight architectures which decrease\ncomputational complexity by reducing depth, width and layer capacity with\nrespect to general purpose architectures. We propose an alternative approach\nwhich achieves a significantly better performance across a wide range of\ncomputing budgets. First, we rely on a light-weight general purpose\narchitecture as the main recognition engine. Then, we leverage light-weight\nupsampling with lateral connections as the most cost-effective solution to\nrestore the prediction resolution. Finally, we propose to enlarge the receptive\nfield by fusing shared features at multiple resolutions in a novel fashion.\nExperiments on several road driving datasets show a substantial advantage of\nthe proposed approach, either with ImageNet pre-trained parameters or when we\nlearn from scratch. Our Cityscapes test submission entitled SwiftNetRN-18\ndelivers 75.5% MIoU and achieves 39.9 Hz on 1024x2048 images on GTX1080Ti.\n", "versions": [{"version": "v1", "created": "Wed, 20 Mar 2019 12:23:21 GMT"}, {"version": "v2", "created": "Fri, 12 Apr 2019 09:46:06 GMT"}], "update_date": "2019-04-15", "authors_parsed": [["Or\u0161i\u0107", "Marin", ""], ["Kre\u0161o", "Ivan", ""], ["Bevandi\u0107", "Petra", ""], ["\u0160egvi\u0107", "Sini\u0161a", ""]]}, {"id": "1903.08478", "submitter": "Jiasong Wu", "authors": "Jiasong Wu, Ling Xu, Youyong Kong, Lotfi Senhadji, Huazhong Shu", "title": "Deep Octonion Networks", "comments": "27 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning is a research hot topic in the field of machine learning.\nReal-value neural networks (Real NNs), especially deep real networks (DRNs),\nhave been widely used in many research fields. In recent years, the deep\ncomplex networks (DCNs) and the deep quaternion networks (DQNs) have attracted\nmore and more attentions. The octonion algebra, which is an extension of\ncomplex algebra and quaternion algebra, can provide more efficient and compact\nexpression. This paper constructs a general framework of deep octonion networks\n(DONs) and provides the main building blocks of DONs such as octonion\nconvolution, octonion batch normalization and octonion weight initialization;\nDONs are then used in image classification tasks for CIFAR-10 and CIFAR-100\ndata sets. Compared with the DRNs, the DCNs, and the DQNs, the proposed DONs\nhave better convergence and higher classification accuracy. The success of DONs\nis also explained by multi-task learning.\n", "versions": [{"version": "v1", "created": "Wed, 20 Mar 2019 12:30:56 GMT"}], "update_date": "2019-03-21", "authors_parsed": [["Wu", "Jiasong", ""], ["Xu", "Ling", ""], ["Kong", "Youyong", ""], ["Senhadji", "Lotfi", ""], ["Shu", "Huazhong", ""]]}, {"id": "1903.08481", "submitter": "Jonathan Williams", "authors": "Jonathan Williams, Carola-Bibiane Sch\\\"onlieb, Tom Swinfield, Juheon\n  Lee, Xiaohao Cai, Lan Qie, David A. Coomes", "title": "Three-dimensional Segmentation of Trees Through a Flexible Multi-Class\n  Graph Cut Algorithm (MCGC)", "comments": null, "journal-ref": null, "doi": "10.1109/TGRS.2019.2940146", "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Developing a robust algorithm for automatic individual tree crown (ITC)\ndetection from laser scanning datasets is important for tracking the responses\nof trees to anthropogenic change. Such approaches allow the size, growth and\nmortality of individual trees to be measured, enabling forest carbon stocks and\ndynamics to be tracked and understood. Many algorithms exist for structurally\nsimple forests including coniferous forests and plantations. Finding a robust\nsolution for structurally complex, species-rich tropical forests remains a\nchallenge; existing segmentation algorithms often perform less well than simple\narea-based approaches when estimating plot-level biomass. Here we describe a\nMulti-Class Graph Cut (MCGC) approach to tree crown delineation. This uses\nlocal three-dimensional geometry and density information, alongside knowledge\nof crown allometries, to segment individual tree crowns from LiDAR point\nclouds. Our approach robustly identifies trees in the top and intermediate\nlayers of the canopy, but cannot recognise small trees. From these\nthree-dimensional crowns, we are able to measure individual tree biomass.\nComparing these estimates to those from permanent inventory plots, our\nalgorithm is able to produce robust estimates of hectare-scale carbon density,\ndemonstrating the power of ITC approaches in monitoring forests. The\nflexibility of our method to add additional dimensions of information, such as\nspectral reflectance, make this approach an obvious avenue for future\ndevelopment and extension to other sources of three-dimensional data, such as\nstructure from motion datasets.\n", "versions": [{"version": "v1", "created": "Wed, 20 Mar 2019 12:35:17 GMT"}], "update_date": "2019-10-04", "authors_parsed": [["Williams", "Jonathan", ""], ["Sch\u00f6nlieb", "Carola-Bibiane", ""], ["Swinfield", "Tom", ""], ["Lee", "Juheon", ""], ["Cai", "Xiaohao", ""], ["Qie", "Lan", ""], ["Coomes", "David A.", ""]]}, {"id": "1903.08527", "submitter": "Yu Deng", "authors": "Yu Deng, Jiaolong Yang, Sicheng Xu, Dong Chen, Yunde Jia, Xin Tong", "title": "Accurate 3D Face Reconstruction with Weakly-Supervised Learning: From\n  Single Image to Image Set", "comments": "minor revision of the layout; update contact information", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, deep learning based 3D face reconstruction methods have shown\npromising results in both quality and efficiency.However, training deep neural\nnetworks typically requires a large volume of data, whereas face images with\nground-truth 3D face shapes are scarce. In this paper, we propose a novel deep\n3D face reconstruction approach that 1) leverages a robust, hybrid loss\nfunction for weakly-supervised learning which takes into account both low-level\nand perception-level information for supervision, and 2) performs multi-image\nface reconstruction by exploiting complementary information from different\nimages for shape aggregation. Our method is fast, accurate, and robust to\nocclusion and large pose. We provide comprehensive experiments on three\ndatasets, systematically comparing our method with fifteen recent methods and\ndemonstrating its state-of-the-art performance.\n", "versions": [{"version": "v1", "created": "Wed, 20 Mar 2019 14:44:45 GMT"}, {"version": "v2", "created": "Thu, 9 Apr 2020 16:42:09 GMT"}], "update_date": "2020-04-10", "authors_parsed": [["Deng", "Yu", ""], ["Yang", "Jiaolong", ""], ["Xu", "Sicheng", ""], ["Chen", "Dong", ""], ["Jia", "Yunde", ""], ["Tong", "Xin", ""]]}, {"id": "1903.08536", "submitter": "Domen Tabernik", "authors": "Domen Tabernik, Samo \\v{S}ela, Jure Skvar\\v{c} and Danijel Sko\\v{c}aj", "title": "Segmentation-Based Deep-Learning Approach for Surface-Defect Detection", "comments": "Journal of Intelligent Manufacturing 2019", "journal-ref": null, "doi": "10.1007/s10845-019-01476-x", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automated surface-anomaly detection using machine learning has become an\ninteresting and promising area of research, with a very high and direct impact\non the application domain of visual inspection. Deep-learning methods have\nbecome the most suitable approaches for this task. They allow the inspection\nsystem to learn to detect the surface anomaly by simply showing it a number of\nexemplar images. This paper presents a segmentation-based deep-learning\narchitecture that is designed for the detection and segmentation of surface\nanomalies and is demonstrated on a specific domain of surface-crack detection.\nThe design of the architecture enables the model to be trained using a small\nnumber of samples, which is an important requirement for practical\napplications. The proposed model is compared with the related deep-learning\nmethods, including the state-of-the-art commercial software, showing that the\nproposed approach outperforms the related methods on the specific domain of\nsurface-crack detection. The large number of experiments also shed light on the\nrequired precision of the annotation, the number of required training samples\nand on the required computational cost. Experiments are performed on a newly\ncreated dataset based on a real-world quality control case and demonstrates\nthat the proposed approach is able to learn on a small number of defected\nsurfaces, using only approximately 25-30 defective training samples, instead of\nhundreds or thousands, which is usually the case in deep-learning applications.\nThis makes the deep-learning method practical for use in industry where the\nnumber of available defective samples is limited. The dataset is also made\npublicly available to encourage the development and evaluation of new methods\nfor surface-defect detection.\n", "versions": [{"version": "v1", "created": "Wed, 20 Mar 2019 15:03:17 GMT"}, {"version": "v2", "created": "Thu, 16 May 2019 08:47:24 GMT"}, {"version": "v3", "created": "Tue, 11 Jun 2019 10:07:09 GMT"}], "update_date": "2019-06-12", "authors_parsed": [["Tabernik", "Domen", ""], ["\u0160ela", "Samo", ""], ["Skvar\u010d", "Jure", ""], ["Sko\u010daj", "Danijel", ""]]}, {"id": "1903.08548", "submitter": "Maurice Quach", "authors": "Maurice Quach, Giuseppe Valenzise and Frederic Dufaux", "title": "Learning Convolutional Transforms for Lossy Point Cloud Geometry\n  Compression", "comments": "Published in ICIP 2019. The source code can be found at\n  https://github.com/mauriceqch/pcc_geo_cnn and the supplementary material can\n  be found at https://www.mauricequach.com/pcc_geo_cnn_samples", "journal-ref": null, "doi": "10.1109/ICIP.2019.8803413", "report-no": null, "categories": "cs.CV cs.LG eess.IV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Efficient point cloud compression is fundamental to enable the deployment of\nvirtual and mixed reality applications, since the number of points to code can\nrange in the order of millions. In this paper, we present a novel data-driven\ngeometry compression method for static point clouds based on learned\nconvolutional transforms and uniform quantization. We perform joint\noptimization of both rate and distortion using a trade-off parameter. In\naddition, we cast the decoding process as a binary classification of the point\ncloud occupancy map. Our method outperforms the MPEG reference solution in\nterms of rate-distortion on the Microsoft Voxelized Upper Bodies dataset with\n51.5% BDBR savings on average. Moreover, while octree-based methods face\nexponential diminution of the number of points at low bitrates, our method\nstill produces high resolution outputs even at low bitrates. Code and\nsupplementary material are available at\nhttps://github.com/mauriceqch/pcc_geo_cnn .\n", "versions": [{"version": "v1", "created": "Wed, 20 Mar 2019 15:14:15 GMT"}, {"version": "v2", "created": "Wed, 22 May 2019 15:56:14 GMT"}], "update_date": "2020-02-19", "authors_parsed": [["Quach", "Maurice", ""], ["Valenzise", "Giuseppe", ""], ["Dufaux", "Frederic", ""]]}, {"id": "1903.08549", "submitter": "Yi Zhang", "authors": "Peng Bao, Wenjun Xia, Kang Yang, Weiyan Chen, Mianyi Chen, Yan Xi,\n  Shanzhou Niu, Jiliu Zhou, He Zhang, Huaiqiang Sun, Zhangyang Wang, Yi Zhang", "title": "Convolutional Sparse Coding for Compressed Sensing CT Reconstruction", "comments": "Accepted by IEEE TMI", "journal-ref": null, "doi": "10.1109/TMI.2019.2906853", "report-no": null, "categories": "physics.med-ph cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Over the past few years, dictionary learning (DL)-based methods have been\nsuccessfully used in various image reconstruction problems. However,\ntraditional DL-based computed tomography (CT) reconstruction methods are\npatch-based and ignore the consistency of pixels in overlapped patches. In\naddition, the features learned by these methods always contain shifted versions\nof the same features. In recent years, convolutional sparse coding (CSC) has\nbeen developed to address these problems. In this paper, inspired by several\nsuccessful applications of CSC in the field of signal processing, we explore\nthe potential of CSC in sparse-view CT reconstruction. By directly working on\nthe whole image, without the necessity of dividing the image into overlapped\npatches in DL-based methods, the proposed methods can maintain more details and\navoid artifacts caused by patch aggregation. With predetermined filters, an\nalternating scheme is developed to optimize the objective function. Extensive\nexperiments with simulated and real CT data were performed to validate the\neffectiveness of the proposed methods. Qualitative and quantitative results\ndemonstrate that the proposed methods achieve better performance than several\nexisting state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Wed, 20 Mar 2019 15:14:58 GMT"}], "update_date": "2019-03-21", "authors_parsed": [["Bao", "Peng", ""], ["Xia", "Wenjun", ""], ["Yang", "Kang", ""], ["Chen", "Weiyan", ""], ["Chen", "Mianyi", ""], ["Xi", "Yan", ""], ["Niu", "Shanzhou", ""], ["Zhou", "Jiliu", ""], ["Zhang", "He", ""], ["Sun", "Huaiqiang", ""], ["Wang", "Zhangyang", ""], ["Zhang", "Yi", ""]]}, {"id": "1903.08550", "submitter": "Pramuditha Perera", "authors": "Pramuditha Perera, Ramesh Nallapati, Bing Xiang", "title": "OCGAN: One-class Novelty Detection Using GANs with Constrained Latent\n  Representations", "comments": "CVPR 2019 Accepted Paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel model called OCGAN for the classical problem of one-class\nnovelty detection, where, given a set of examples from a particular class, the\ngoal is to determine if a query example is from the same class. Our solution is\nbased on learning latent representations of in-class examples using a denoising\nauto-encoder network. The key contribution of our work is our proposal to\nexplicitly constrain the latent space to exclusively represent the given class.\nIn order to accomplish this goal, firstly, we force the latent space to have\nbounded support by introducing a tanh activation in the encoder's output layer.\nSecondly, using a discriminator in the latent space that is trained\nadversarially, we ensure that encoded representations of in-class examples\nresemble uniform random samples drawn from the same bounded space. Thirdly,\nusing a second adversarial discriminator in the input space, we ensure all\nrandomly drawn latent samples generate examples that look real. Finally, we\nintroduce a gradient-descent based sampling technique that explores points in\nthe latent space that generate potential out-of-class examples, which are fed\nback to the network to further train it to generate in-class examples from\nthose points. The effectiveness of the proposed method is measured across four\npublicly available datasets using two one-class novelty detection protocols\nwhere we achieve state-of-the-art results.\n", "versions": [{"version": "v1", "created": "Wed, 20 Mar 2019 15:15:05 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["Perera", "Pramuditha", ""], ["Nallapati", "Ramesh", ""], ["Xiang", "Bing", ""]]}, {"id": "1903.08558", "submitter": "Siyuan Li", "authors": "Siyuan Li, Iago Breno Araujo, Wenqi Ren, Zhangyang Wang, Eric K.\n  Tokuda, Roberto Hirata Junior, Roberto Cesar-Junior, Jiawan Zhang, Xiaojie\n  Guo and Xiaochun Cao", "title": "Single Image Deraining: A Comprehensive Benchmark Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a comprehensive study and evaluation of existing single image\nderaining algorithms, using a new large-scale benchmark consisting of both\nsynthetic and real-world rainy images.This dataset highlights diverse data\nsources and image contents, and is divided into three subsets (rain streak,\nrain drop, rain and mist), each serving different training or evaluation\npurposes. We further provide a rich variety of criteria for dehazing algorithm\nevaluation, ranging from full-reference metrics, to no-reference metrics, to\nsubjective evaluation and the novel task-driven evaluation. Experiments on the\ndataset shed light on the comparisons and limitations of state-of-the-art\nderaining algorithms, and suggest promising future directions.\n", "versions": [{"version": "v1", "created": "Wed, 20 Mar 2019 15:31:07 GMT"}], "update_date": "2019-03-21", "authors_parsed": [["Li", "Siyuan", ""], ["Araujo", "Iago Breno", ""], ["Ren", "Wenqi", ""], ["Wang", "Zhangyang", ""], ["Tokuda", "Eric K.", ""], ["Junior", "Roberto Hirata", ""], ["Cesar-Junior", "Roberto", ""], ["Zhang", "Jiawan", ""], ["Guo", "Xiaojie", ""], ["Cao", "Xiaochun", ""]]}, {"id": "1903.08583", "submitter": "Dmitry Kuznichov", "authors": "Dmitry Kuznichov, Alon Zvirin, Yaron Honen and Ron Kimmel", "title": "Data Augmentation for Leaf Segmentation and Counting Tasks in Rosette\n  Plants", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning techniques involving image processing and data analysis are\nconstantly evolving. Many domains adapt these techniques for object\nsegmentation, instantiation and classification. Recently, agricultural\nindustries adopted those techniques in order to bring automation to farmers\naround the globe. One analysis procedure required for automatic visual\ninspection in this domain is leaf count and segmentation. Collecting labeled\ndata from field crops and greenhouses is a complicated task due to the large\nvariety of crops, growth seasons, climate changes, phenotype diversity, and\nmore, especially when specific learning tasks require a large amount of labeled\ndata for training. Data augmentation for training deep neural networks is well\nestablished, examples include data synthesis, using generative semi-synthetic\nmodels, and applying various kinds of transformations. In this paper we propose\na method that preserves the geometric structure of the data objects, thus\nkeeping the physical appearance of the data-set as close as possible to imaged\nplants in real agricultural scenes. The proposed method provides state of the\nart results when applied to the standard benchmark in the field, namely, the\nongoing Leaf Segmentation Challenge hosted by Computer Vision Problems in Plant\nPhenotyping.\n", "versions": [{"version": "v1", "created": "Wed, 20 Mar 2019 16:13:10 GMT"}], "update_date": "2019-03-21", "authors_parsed": [["Kuznichov", "Dmitry", ""], ["Zvirin", "Alon", ""], ["Honen", "Yaron", ""], ["Kimmel", "Ron", ""]]}, {"id": "1903.08588", "submitter": "Heng Yang", "authors": "Heng Yang, Luca Carlone", "title": "A Polynomial-time Solution for Robust Registration with Extreme Outlier\n  Rates", "comments": "18 pages, Accepted for publication in Robotics: Science and Systems,\n  2019", "journal-ref": "Robotics: Science and Systems, 2019", "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a robust approach for the registration of two sets of 3D points in\nthe presence of a large amount of outliers. Our first contribution is to\nreformulate the registration problem using a Truncated Least Squares (TLS) cost\nthat makes the estimation insensitive to a large fraction of spurious\npoint-to-point correspondences. The second contribution is a general framework\nto decouple rotation, translation, and scale estimation, which allows solving\nin cascade for the three transformations. Since each subproblem (scale,\nrotation, and translation estimation) is still non-convex and combinatorial in\nnature, out third contribution is to show that (i) TLS scale and\n(component-wise) translation estimation can be solved exactly and in polynomial\ntime via an adaptive voting scheme, (ii) TLS rotation estimation can be relaxed\nto a semidefinite program and the relaxation is tight in practice, even in the\npresence of an extreme amount of outliers. We validate the proposed algorithm,\nnamed TEASER (Truncated least squares Estimation And SEmidefinite Relaxation),\nin standard registration benchmarks showing that the algorithm outperforms\nRANSAC and robust local optimization techniques, and favorably compares with\nBranch-and-Bound methods, while being a polynomial-time algorithm. TEASER can\ntolerate up to 99% outliers and returns highly-accurate solutions.\n", "versions": [{"version": "v1", "created": "Wed, 20 Mar 2019 16:16:28 GMT"}, {"version": "v2", "created": "Sun, 30 Jun 2019 15:37:56 GMT"}], "update_date": "2019-07-02", "authors_parsed": [["Yang", "Heng", ""], ["Carlone", "Luca", ""]]}, {"id": "1903.08589", "submitter": "Zhanchao Huang", "authors": "Zhanchao Huang and Jianlin Wang", "title": "DC-SPP-YOLO: Dense Connection and Spatial Pyramid Pooling Based YOLO for\n  Object Detection", "comments": "23 pages, 9 figures, 9 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although YOLOv2 approach is extremely fast on object detection; its backbone\nnetwork has the low ability on feature extraction and fails to make full use of\nmulti-scale local region features, which restricts the improvement of object\ndetection accuracy. Therefore, this paper proposed a DC-SPP-YOLO (Dense\nConnection and Spatial Pyramid Pooling Based YOLO) approach for ameliorating\nthe object detection accuracy of YOLOv2. Specifically, the dense connection of\nconvolution layers is employed in the backbone network of YOLOv2 to strengthen\nthe feature extraction and alleviate the vanishing-gradient problem. Moreover,\nan improved spatial pyramid pooling is introduced to pool and concatenate the\nmulti-scale local region features, so that the network can learn the object\nfeatures more comprehensively. The DC-SPP-YOLO model is established and trained\nbased on a new loss function composed of mean square error and cross entropy,\nand the object detection is realized. Experiments demonstrate that the mAP\n(mean Average Precision) of DC-SPP-YOLO proposed on PASCAL VOC datasets and\nUA-DETRAC datasets is higher than that of YOLOv2; the object detection accuracy\nof DC-SPP-YOLO is superior to YOLOv2 by strengthening feature extraction and\nusing the multi-scale local region features.\n", "versions": [{"version": "v1", "created": "Wed, 20 Mar 2019 16:19:20 GMT"}], "update_date": "2019-03-21", "authors_parsed": [["Huang", "Zhanchao", ""], ["Wang", "Jianlin", ""]]}, {"id": "1903.08616", "submitter": "Philip Schniter", "authors": "Rizwan Ahmad, Charles A. Bouman, Gregery T. Buzzard, Stanley Chan,\n  Sizhou Liu, Edward T. Reehorst, and Philip Schniter", "title": "Plug and play methods for magnetic resonance imaging (long version)", "comments": null, "journal-ref": null, "doi": "10.1109/MSP.2019.2949470", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Magnetic Resonance Imaging (MRI) is a non-invasive diagnostic tool that\nprovides excellent soft-tissue contrast without the use of ionizing radiation.\nCompared to other clinical imaging modalities (e.g., CT or ultrasound),\nhowever, the data acquisition process for MRI is inherently slow, which\nmotivates undersampling and thus drives the need for accurate, efficient\nreconstruction methods from undersampled datasets. In this article, we describe\nthe use of \"plug-and-play\" (PnP) algorithms for MRI image recovery. We first\ndescribe the linearly approximated inverse problem encountered in MRI. Then we\nreview several PnP methods, where the unifying commonality is to iteratively\ncall a denoising subroutine as one step of a larger optimization-inspired\nalgorithm. Next, we describe how the result of the PnP method can be\ninterpreted as a solution to an equilibrium equation, allowing convergence\nanalysis from the equilibrium perspective. Finally, we present illustrative\nexamples of PnP methods applied to MRI image recovery.\n", "versions": [{"version": "v1", "created": "Wed, 20 Mar 2019 16:59:04 GMT"}, {"version": "v2", "created": "Wed, 3 Jul 2019 13:34:22 GMT"}, {"version": "v3", "created": "Fri, 27 Sep 2019 15:02:18 GMT"}, {"version": "v4", "created": "Thu, 14 Nov 2019 20:50:03 GMT"}, {"version": "v5", "created": "Tue, 10 Dec 2019 19:19:39 GMT"}], "update_date": "2020-02-19", "authors_parsed": [["Ahmad", "Rizwan", ""], ["Bouman", "Charles A.", ""], ["Buzzard", "Gregery T.", ""], ["Chan", "Stanley", ""], ["Liu", "Sizhou", ""], ["Reehorst", "Edward T.", ""], ["Schniter", "Philip", ""]]}, {"id": "1903.08636", "submitter": "Patrick Geneva", "authors": "Patrick Geneva, James Maley, Guoquan Huang", "title": "An Efficient Schmidt-EKF for 3D Visual-Inertial SLAM", "comments": "Accepted to the 2019 Conference on Computer Vision and Pattern\n  Recognition (CVPR)", "journal-ref": "2019 Conference on Computer Vision and Pattern Recognition (CVPR)", "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It holds great implications for practical applications to enable\ncentimeter-accuracy positioning for mobile and wearable sensor systems. In this\npaper, we propose a novel, high-precision, efficient visual-inertial (VI)-SLAM\nalgorithm, termed Schmidt-EKF VI-SLAM (SEVIS), which optimally fuses IMU\nmeasurements and monocular images in a tightly-coupled manner to provide 3D\nmotion tracking with bounded error. In particular, we adapt the Schmidt Kalman\nfilter formulation to selectively include informative features in the state\nvector while treating them as nuisance parameters (or Schmidt states) once they\nbecome matured. This change in modeling allows for significant computational\nsavings by no longer needing to constantly update the Schmidt states (or their\ncovariance), while still allowing the EKF to correctly account for their\ncross-correlations with the active states. As a result, we achieve linear\ncomputational complexity in terms of map size, instead of quadratic as in the\nstandard SLAM systems. In order to fully exploit the map information to bound\nnavigation drifts, we advocate efficient keyframe-aided 2D-to-2D feature\nmatching to find reliable correspondences between current 2D visual\nmeasurements and 3D map features. The proposed SEVIS is extensively validated\nin both simulations and experiments.\n", "versions": [{"version": "v1", "created": "Wed, 20 Mar 2019 17:50:49 GMT"}], "update_date": "2019-03-21", "authors_parsed": [["Geneva", "Patrick", ""], ["Maley", "James", ""], ["Huang", "Guoquan", ""]]}, {"id": "1903.08642", "submitter": "Chen-Hsuan Lin", "authors": "Chen-Hsuan Lin, Oliver Wang, Bryan C. Russell, Eli Shechtman, Vladimir\n  G. Kim, Matthew Fisher, Simon Lucey", "title": "Photometric Mesh Optimization for Video-Aligned 3D Object Reconstruction", "comments": "Accepted to CVPR 2019 (project page & code:\n  https://chenhsuanlin.bitbucket.io/photometric-mesh-optim/)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we address the problem of 3D object mesh reconstruction from\nRGB videos. Our approach combines the best of multi-view geometric and\ndata-driven methods for 3D reconstruction by optimizing object meshes for\nmulti-view photometric consistency while constraining mesh deformations with a\nshape prior. We pose this as a piecewise image alignment problem for each mesh\nface projection. Our approach allows us to update shape parameters from the\nphotometric error without any depth or mask information. Moreover, we show how\nto avoid a degeneracy of zero photometric gradients via rasterizing from a\nvirtual viewpoint. We demonstrate 3D object mesh reconstruction results from\nboth synthetic and real-world videos with our photometric mesh optimization,\nwhich is unachievable with either na\\\"ive mesh generation networks or\ntraditional pipelines of surface reconstruction without heavy manual\npost-processing.\n", "versions": [{"version": "v1", "created": "Wed, 20 Mar 2019 17:58:38 GMT"}], "update_date": "2019-03-21", "authors_parsed": [["Lin", "Chen-Hsuan", ""], ["Wang", "Oliver", ""], ["Russell", "Bryan C.", ""], ["Shechtman", "Eli", ""], ["Kim", "Vladimir G.", ""], ["Fisher", "Matthew", ""], ["Lucey", "Simon", ""]]}, {"id": "1903.08649", "submitter": "Mohammad Nayeem Teli", "authors": "Mohammad Nayeem Teli, Bruce A. Draper, J. Ross Beveridge", "title": "Face Detection in Repeated Settings", "comments": "14 pages, 21 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Face detection is an important first step before face verification and\nrecognition. In unconstrained settings it is still an open challenge because of\nthe variation in pose, lighting, scale, background and location. However, for\nthe purposes of verification we can have a control on background and location.\nImages are primarily captured in places such as the entrance to a sensitive\nbuilding, in front of a door or some location where the background does not\nchange. We present a correlation based face detection algorithm to detect faces\nin such settings, where we control the location, and leave lighting, pose, and\nscale uncontrolled. In these scenarios the results indicate that our algorithm\nis easy and fast to train, outperforms Viola and Jones face detection accuracy\nand is faster to test.\n", "versions": [{"version": "v1", "created": "Wed, 20 Mar 2019 05:03:27 GMT"}], "update_date": "2019-03-22", "authors_parsed": [["Teli", "Mohammad Nayeem", ""], ["Draper", "Bruce A.", ""], ["Beveridge", "J. Ross", ""]]}, {"id": "1903.08671", "submitter": "Rahaf Aljundi", "authors": "Rahaf Aljundi, Min Lin, Baptiste Goujaud and Yoshua Bengio", "title": "Gradient based sample selection for online continual learning", "comments": "Neurips 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A continual learning agent learns online with a non-stationary and\nnever-ending stream of data. The key to such learning process is to overcome\nthe catastrophic forgetting of previously seen data, which is a well known\nproblem of neural networks. To prevent forgetting, a replay buffer is usually\nemployed to store the previous data for the purpose of rehearsal. Previous\nworks often depend on task boundary and i.i.d. assumptions to properly select\nsamples for the replay buffer. In this work, we formulate sample selection as a\nconstraint reduction problem based on the constrained optimization view of\ncontinual learning. The goal is to select a fixed subset of constraints that\nbest approximate the feasible region defined by the original constraints. We\nshow that it is equivalent to maximizing the diversity of samples in the replay\nbuffer with parameters gradient as the feature. We further develop a greedy\nalternative that is cheap and efficient. The advantage of the proposed method\nis demonstrated by comparing to other alternatives under the continual learning\nsetting. Further comparisons are made against state of the art methods that\nrely on task boundaries which show comparable or even better results for our\nmethod.\n", "versions": [{"version": "v1", "created": "Wed, 20 Mar 2019 18:01:55 GMT"}, {"version": "v2", "created": "Fri, 22 Mar 2019 13:20:35 GMT"}, {"version": "v3", "created": "Mon, 24 Jun 2019 09:00:19 GMT"}, {"version": "v4", "created": "Tue, 16 Jul 2019 15:52:08 GMT"}, {"version": "v5", "created": "Thu, 31 Oct 2019 14:45:47 GMT"}], "update_date": "2019-11-01", "authors_parsed": [["Aljundi", "Rahaf", ""], ["Lin", "Min", ""], ["Goujaud", "Baptiste", ""], ["Bengio", "Yoshua", ""]]}, {"id": "1903.08682", "submitter": "Yijun Li", "authors": "Yijun Li, Chen Fang, Aaron Hertzmann, Eli Shechtman, Ming-Hsuan Yang", "title": "Im2Pencil: Controllable Pencil Illustration from Photographs", "comments": "Accepted by CVPR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a high-quality photo-to-pencil translation method with\nfine-grained control over the drawing style. This is a challenging task due to\nmultiple stroke types (e.g., outline and shading), structural complexity of\npencil shading (e.g., hatching), and the lack of aligned training data pairs.\nTo address these challenges, we develop a two-branch model that learns separate\nfilters for generating sketchy outlines and tonal shading from a collection of\npencil drawings. We create training data pairs by extracting clean outlines and\ntonal illustrations from original pencil drawings using image filtering\ntechniques, and we manually label the drawing styles. In addition, our model\ncreates different pencil styles (e.g., line sketchiness and shading style) in a\nuser-controllable manner. Experimental results on different types of pencil\ndrawings show that the proposed algorithm performs favorably against existing\nmethods in terms of quality, diversity and user evaluations.\n", "versions": [{"version": "v1", "created": "Wed, 20 Mar 2019 18:25:04 GMT"}], "update_date": "2019-03-22", "authors_parsed": [["Li", "Yijun", ""], ["Fang", "Chen", ""], ["Hertzmann", "Aaron", ""], ["Shechtman", "Eli", ""], ["Yang", "Ming-Hsuan", ""]]}, {"id": "1903.08689", "submitter": "Yilun Du", "authors": "Yilun Du and Igor Mordatch", "title": "Implicit Generation and Generalization in Energy-Based Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Energy based models (EBMs) are appealing due to their generality and\nsimplicity in likelihood modeling, but have been traditionally difficult to\ntrain. We present techniques to scale MCMC based EBM training on continuous\nneural networks, and we show its success on the high-dimensional data domains\nof ImageNet32x32, ImageNet128x128, CIFAR-10, and robotic hand trajectories,\nachieving better samples than other likelihood models and nearing the\nperformance of contemporary GAN approaches, while covering all modes of the\ndata. We highlight some unique capabilities of implicit generation such as\ncompositionality and corrupt image reconstruction and inpainting. Finally, we\nshow that EBMs are useful models across a wide variety of tasks, achieving\nstate-of-the-art out-of-distribution classification, adversarially robust\nclassification, state-of-the-art continual online class learning, and coherent\nlong term predicted trajectory rollouts.\n", "versions": [{"version": "v1", "created": "Wed, 20 Mar 2019 18:34:29 GMT"}, {"version": "v2", "created": "Mon, 10 Jun 2019 02:54:05 GMT"}, {"version": "v3", "created": "Wed, 6 Nov 2019 19:54:22 GMT"}, {"version": "v4", "created": "Tue, 16 Jun 2020 20:53:53 GMT"}, {"version": "v5", "created": "Wed, 24 Jun 2020 01:52:12 GMT"}, {"version": "v6", "created": "Tue, 30 Jun 2020 03:25:59 GMT"}], "update_date": "2020-07-01", "authors_parsed": [["Du", "Yilun", ""], ["Mordatch", "Igor", ""]]}, {"id": "1903.08701", "submitter": "Ankit Laddha", "authors": "Gregory P. Meyer, Ankit Laddha, Eric Kee, Carlos Vallespi-Gonzalez,\n  Carl K. Wellington", "title": "LaserNet: An Efficient Probabilistic 3D Object Detector for Autonomous\n  Driving", "comments": "Accepted for publication at CVPR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.RO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we present LaserNet, a computationally efficient method for 3D\nobject detection from LiDAR data for autonomous driving. The efficiency results\nfrom processing LiDAR data in the native range view of the sensor, where the\ninput data is naturally compact. Operating in the range view involves well\nknown challenges for learning, including occlusion and scale variation, but it\nalso provides contextual information based on how the sensor data was captured.\nOur approach uses a fully convolutional network to predict a multimodal\ndistribution over 3D boxes for each point and then it efficiently fuses these\ndistributions to generate a prediction for each object. Experiments show that\nmodeling each detection as a distribution rather than a single deterministic\nbox leads to better overall detection performance. Benchmark results show that\nthis approach has significantly lower runtime than other recent detectors and\nthat it achieves state-of-the-art performance when compared on a large dataset\nthat has enough data to overcome the challenges of training on the range view.\n", "versions": [{"version": "v1", "created": "Wed, 20 Mar 2019 19:02:44 GMT"}], "update_date": "2019-03-22", "authors_parsed": [["Meyer", "Gregory P.", ""], ["Laddha", "Ankit", ""], ["Kee", "Eric", ""], ["Vallespi-Gonzalez", "Carlos", ""], ["Wellington", "Carl K.", ""]]}, {"id": "1903.08746", "submitter": "Chen Sun", "authors": "Chen Sun, Jean M. Uwabeza Vianney, Dongpu Cao", "title": "Affordance Learning In Direct Perception for Autonomous Driving", "comments": "9 pages, 13 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent development in autonomous driving involves high-level computer vision\nand detailed road scene understanding. Today, most autonomous vehicles are\nusing mediated perception approach for path planning and control, which highly\nrely on high-definition 3D maps and real time sensors. Recent research efforts\naim to substitute the massive HD maps with coarse road attributes. In this\npaper, we follow the direct perception based method to train a deep neural\nnetwork for affordance learning in autonomous driving. Our goal in this work is\nto develop the affordance learning model based on freely available Google\nStreet View panoramas and Open Street Map road vector attributes. Driving scene\nunderstanding can be achieved by learning affordances from the images captured\nby car-mounted cameras. Such scene understanding by learning affordances may be\nuseful for corroborating base maps such as HD maps so that the required data\nstorage space is minimized and available for processing in real time. We\ncompare capability in road attribute identification between human volunteers\nand our model by experimental evaluation. Our results indicate that this method\ncould act as a cheaper way for training data collection in autonomous driving.\nThe cross validation results also indicate the effectiveness of our model.\n", "versions": [{"version": "v1", "created": "Wed, 20 Mar 2019 21:15:08 GMT"}], "update_date": "2019-03-22", "authors_parsed": [["Sun", "Chen", ""], ["Vianney", "Jean M. Uwabeza", ""], ["Cao", "Dongpu", ""]]}, {"id": "1903.08773", "submitter": "Leixin Zhou", "authors": "Leixin Zhou, Wenxiang Deng, Xiaodong Wu", "title": "Robust Image Segmentation Quality Assessment", "comments": null, "journal-ref": null, "doi": null, "report-no": "MIDL/2020/ExtendedAbstract/nyhZXiaotm", "categories": "cs.CV", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Deep learning based image segmentation methods have achieved great success,\neven having human-level accuracy in some applications. However, due to the\nblack box nature of deep learning, the best method may fail in some situations.\nThus predicting segmentation quality without ground truth would be very crucial\nespecially in clinical practice. Recently, people proposed to train neural\nnetworks to estimate the quality score by regression. Although it can achieve\npromising prediction accuracy, the network suffers robustness problem, e.g. it\nis vulnerable to adversarial attacks. In this paper, we propose to alleviate\nthis problem by utilizing the difference between the input image and the\nreconstructed image, which is conditioned on the segmentation to be assessed,\nto lower the chance to overfit to the undesired image features from the\noriginal input image, and thus to increase the robustness. Results on ACDC17\ndataset demonstrated our method is promising.\n", "versions": [{"version": "v1", "created": "Wed, 20 Mar 2019 23:07:47 GMT"}, {"version": "v2", "created": "Tue, 26 May 2020 16:27:28 GMT"}, {"version": "v3", "created": "Wed, 27 May 2020 17:24:49 GMT"}], "update_date": "2020-05-28", "authors_parsed": [["Zhou", "Leixin", ""], ["Deng", "Wenxiang", ""], ["Wu", "Xiaodong", ""]]}, {"id": "1903.08811", "submitter": "Xu Han", "authors": "Zhengyang Shen, Xu Han, Zhenlin Xu, Marc Niethammer", "title": "Networks for Joint Affine and Non-parametric Image Registration", "comments": "Accepted to CVPR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce an end-to-end deep-learning framework for 3D medical image\nregistration. In contrast to existing approaches, our framework combines two\nregistration methods: an affine registration and a vector\nmomentum-parameterized stationary velocity field (vSVF) model. Specifically, it\nconsists of three stages. In the first stage, a multi-step affine network\npredicts affine transform parameters. In the second stage, we use a Unet-like\nnetwork to generate a momentum, from which a velocity field can be computed via\nsmoothing. Finally, in the third stage, we employ a self-iterable map-based\nvSVF component to provide a non-parametric refinement based on the current\nestimate of the transformation map. Once the model is trained, a registration\nis completed in one forward pass. To evaluate the performance, we conducted\nlongitudinal and cross-subject experiments on 3D magnetic resonance images\n(MRI) of the knee of the Osteoarthritis Initiative (OAI) dataset. Results show\nthat our framework achieves comparable performance to state-of-the-art medical\nimage registration approaches, but it is much faster, with a better control of\ntransformation regularity including the ability to produce approximately\nsymmetric transformations, and combining affine and non-parametric\nregistration.\n", "versions": [{"version": "v1", "created": "Thu, 21 Mar 2019 02:35:42 GMT"}], "update_date": "2019-03-22", "authors_parsed": [["Shen", "Zhengyang", ""], ["Han", "Xu", ""], ["Xu", "Zhenlin", ""], ["Niethammer", "Marc", ""]]}, {"id": "1903.08814", "submitter": "Md Sazzad Hossain", "authors": "M. S. Hossain, A. P. Paplinski, J. M. Betts", "title": "Prostate Segmentation from Ultrasound Images using Residual Fully\n  Convolutional Network", "comments": "6 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Medical imaging based prostate cancer diagnosis procedure uses\nintra-operative transrectal ultrasound (TRUS) imaging to visualize the prostate\nshape and location to collect tissue samples. Correct tissue sampling from\nprostate requires accurate prostate segmentation in TRUS images. To achieve\nthis, this study uses a novel residual connection based fully convolutional\nnetwork. The advantage of this segmentation technique is that it requires no\npre-processing of TRUS images to perform the segmentation. Thus, it offers a\nfaster and straightforward prostate segmentation from TRUS images. Results show\nthat the proposed technique can achieve around 86% Dice Similarity accuracy\nusing only few TRUS datasets.\n", "versions": [{"version": "v1", "created": "Thu, 21 Mar 2019 02:53:03 GMT"}], "update_date": "2019-03-22", "authors_parsed": [["Hossain", "M. S.", ""], ["Paplinski", "A. P.", ""], ["Betts", "J. M.", ""]]}, {"id": "1903.08817", "submitter": "Xing Liu", "authors": "Xing Liu, Masanori Suganuma, Zhun Sun and Takayuki Okatani", "title": "Dual Residual Networks Leveraging the Potential of Paired Operations for\n  Image Restoration", "comments": "i) Accepted to CVPR 2019 ii) Code, trained models and additional\n  results for visual comparison will be provided at\n  https://github.com/liu-vis/DualResidualNetworks", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study design of deep neural networks for tasks of image\nrestoration. We propose a novel style of residual connections dubbed \"dual\nresidual connection\", which exploits the potential of paired operations, e.g.,\nup- and down-sampling or convolution with large- and small-size kernels. We\ndesign a modular block implementing this connection style; it is equipped with\ntwo containers to which arbitrary paired operations are inserted. Adopting the\n\"unraveled\" view of the residual networks proposed by Veit et al., we point out\nthat a stack of the proposed modular blocks allows the first operation in a\nblock interact with the second operation in any subsequent blocks. Specifying\nthe two operations in each of the stacked blocks, we build a complete network\nfor each individual task of image restoration. We experimentally evaluate the\nproposed approach on five image restoration tasks using nine datasets. The\nresults show that the proposed networks with properly chosen paired operations\noutperform previous methods on almost all of the tasks and datasets.\n", "versions": [{"version": "v1", "created": "Thu, 21 Mar 2019 03:09:19 GMT"}, {"version": "v2", "created": "Sun, 7 Apr 2019 12:54:07 GMT"}], "update_date": "2019-04-09", "authors_parsed": [["Liu", "Xing", ""], ["Suganuma", "Masanori", ""], ["Sun", "Zhun", ""], ["Okatani", "Takayuki", ""]]}, {"id": "1903.08831", "submitter": "Jongbin Won", "authors": "Jongbin Won, Jong-Woong Park, and Do-Soo Moon", "title": "Non-target Structural Displacement Measurement Using Reference Frame\n  Based Deepflow", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Structural displacement is crucial for structural health monitoring, although\nit is very challenging to measure in field conditions. Most existing\ndisplacement measurement methods are costly, labor intensive, and\ninsufficiently accurate for measuring small dynamic displacements. Computer\nvision (CV) based methods incorporate optical devices with advanced image\nprocessing algorithms to accurately, cost-effectively, and remotely measure\nstructural displacement with easy installation. However, non-target based CV\nmethods are still limited by insufficient feature points, incorrect feature\npoint detection, occlusion, and drift induced by tracking error accumulation.\nThis paper presents a reference frame based Deepflow algorithm integrated with\nmasking and signal filtering for non-target based displacement measurements.\nThe proposed method allows the user to select points of interest for images\nwith a low gradient for displacement tracking and directly calculate\ndisplacement without drift accumulated by measurement error. The proposed\nmethod is experimentally validated on a cantilevered beam under ambient and\noccluded test conditions. The accuracy of the proposed method is compared with\nthat of a reference laser displacement sensor for validation. The significant\nadvantage of the proposed method is its flexibility in extracting structural\ndisplacement in any region on structures that do not have distinct natural\nfeatures.\n", "versions": [{"version": "v1", "created": "Thu, 21 Mar 2019 05:03:19 GMT"}], "update_date": "2019-03-22", "authors_parsed": [["Won", "Jongbin", ""], ["Park", "Jong-Woong", ""], ["Moon", "Do-Soo", ""]]}, {"id": "1903.08836", "submitter": "Zichuan Liu", "authors": "Zichuan Liu, Guosheng Lin, Sheng Yang, Fayao Liu, Weisi Lin and Wang\n  Ling Goh", "title": "Towards Robust Curve Text Detection with Conditional Spatial Expansion", "comments": "This paper has been accepted by IEEE International Conference on\n  Computer Vision and Pattern Recognition (CVPR 2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is challenging to detect curve texts due to their irregular shapes and\nvarying sizes. In this paper, we first investigate the deficiency of the\nexisting curve detection methods and then propose a novel Conditional Spatial\nExpansion (CSE) mechanism to improve the performance of curve text detection.\nInstead of regarding the curve text detection as a polygon regression or a\nsegmentation problem, we treat it as a region expansion process. Our CSE starts\nwith a seed arbitrarily initialized within a text region and progressively\nmerges neighborhood regions based on the extracted local features by a CNN and\ncontextual information of merged regions. The CSE is highly parameterized and\ncan be seamlessly integrated into existing object detection frameworks.\nEnhanced by the data-dependent CSE mechanism, our curve text detection system\nprovides robust instance-level text region extraction with minimal\npost-processing. The analysis experiment shows that our CSE can handle texts\nwith various shapes, sizes, and orientations, and can effectively suppress the\nfalse-positives coming from text-like textures or unexpected texts included in\nthe same RoI. Compared with the existing curve text detection algorithms, our\nmethod is more robust and enjoys a simpler processing flow. It also creates a\nnew state-of-art performance on curve text benchmarks with F-score of up to\n78.4$\\%$.\n", "versions": [{"version": "v1", "created": "Thu, 21 Mar 2019 05:46:15 GMT"}], "update_date": "2019-03-22", "authors_parsed": [["Liu", "Zichuan", ""], ["Lin", "Guosheng", ""], ["Yang", "Sheng", ""], ["Liu", "Fayao", ""], ["Lin", "Weisi", ""], ["Goh", "Wang Ling", ""]]}, {"id": "1903.08839", "submitter": "Junyi Lin", "authors": "Xipeng Chen, Kwan-Yee Lin, Wentao Liu, Chen Qian, Xiaogang Wang, Liang\n  Lin", "title": "Weakly-Supervised Discovery of Geometry-Aware Representation for 3D\n  Human Pose Estimation", "comments": "Accepted as a CVPR 2019 oral paper. Project page:\n  https://kwanyeelin.github.io/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent studies have shown remarkable advances in 3D human pose estimation\nfrom monocular images, with the help of large-scale in-door 3D datasets and\nsophisticated network architectures. However, the generalizability to different\nenvironments remains an elusive goal. In this work, we propose a geometry-aware\n3D representation for the human pose to address this limitation by using\nmultiple views in a simple auto-encoder model at the training stage and only 2D\nkeypoint information as supervision. A view synthesis framework is proposed to\nlearn the shared 3D representation between viewpoints with synthesizing the\nhuman pose from one viewpoint to the other one. Instead of performing a direct\ntransfer in the raw image-level, we propose a skeleton-based encoder-decoder\nmechanism to distil only pose-related representation in the latent space. A\nlearning-based representation consistency constraint is further introduced to\nfacilitate the robustness of latent 3D representation. Since the learnt\nrepresentation encodes 3D geometry information, mapping it to 3D pose will be\nmuch easier than conventional frameworks that use an image or 2D coordinates as\nthe input of 3D pose estimator. We demonstrate our approach on the task of 3D\nhuman pose estimation. Comprehensive experiments on three popular benchmarks\nshow that our model can significantly improve the performance of\nstate-of-the-art methods with simply injecting the representation as a robust\n3D prior.\n", "versions": [{"version": "v1", "created": "Thu, 21 Mar 2019 06:00:15 GMT"}, {"version": "v2", "created": "Wed, 27 Mar 2019 06:10:11 GMT"}], "update_date": "2019-03-28", "authors_parsed": [["Chen", "Xipeng", ""], ["Lin", "Kwan-Yee", ""], ["Liu", "Wentao", ""], ["Qian", "Chen", ""], ["Wang", "Xiaogang", ""], ["Lin", "Liang", ""]]}, {"id": "1903.08847", "submitter": "Harbi AlMahafzah", "authors": "Harbi AlMahafzah, Mohammad Imranand, Supreetha Gowda H.D.", "title": "Parametic Classification of Handvein Patterns Based on Texture Features", "comments": "8 pages, International Conference on Electrical, Electronics,\n  Materials and Applied Science (ICEEMAS). AIP: Proceedings International\n  Conference on Electrical, Electronics, Materials and Applied Science\n  (ICEEMAS),22nd and 23rd December 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we have developed Biometric recognition system adopting hand\nbased modality Handvein, which has the unique pattern for each individual and\nit is impossible to counterfeit and fabricate as it is an internal feature. We\nhave opted in choosing feature extraction algorithms such as LBP-visual\ndescriptor ,LPQ-blur insensitive texture operator, Log-Gabor-Texture\ndescriptor. We have chosen well known classifiers such as KNN and SVM for\nclassification. We have experimented and tabulated results of single algorithm\nrecognition rate for Handvein under different distance measures and kernel\noptions. The feature level fusion is carried out which increased the\nperformance level.\n", "versions": [{"version": "v1", "created": "Thu, 21 Mar 2019 06:58:12 GMT"}], "update_date": "2019-03-22", "authors_parsed": [["AlMahafzah", "Harbi", ""], ["Imranand", "Mohammad", ""], ["D.", "Supreetha Gowda H.", ""]]}, {"id": "1903.08858", "submitter": "Fuad Noman", "authors": "Chun-Ren Phang, Chee-Ming Ting, Fuad Noman, Hernando Ombao", "title": "Classification of EEG-Based Brain Connectivity Networks in Schizophrenia\n  Using a Multi-Domain Connectome Convolutional Neural Network", "comments": "15 pages, 9 figures", "journal-ref": null, "doi": "10.1109/JBHI.2019.2941222", "report-no": null, "categories": "cs.LG cs.CV q-bio.NC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We exploit altered patterns in brain functional connectivity as features for\nautomatic discriminative analysis of neuropsychiatric patients. Deep learning\nmethods have been introduced to functional network classification only very\nrecently for fMRI, and the proposed architectures essentially focused on a\nsingle type of connectivity measure. We propose a deep convolutional neural\nnetwork (CNN) framework for classification of electroencephalogram\n(EEG)-derived brain connectome in schizophrenia (SZ). To capture complementary\naspects of disrupted connectivity in SZ, we explore combination of various\nconnectivity features consisting of time and frequency-domain metrics of\neffective connectivity based on vector autoregressive model and partial\ndirected coherence, and complex network measures of network topology. We design\na novel multi-domain connectome CNN (MDC-CNN) based on a parallel ensemble of\n1D and 2D CNNs to integrate the features from various domains and dimensions\nusing different fusion strategies. Hierarchical latent representations learned\nby the multiple convolutional layers from EEG connectivity reveal apparent\ngroup differences between SZ and healthy controls (HC). Results on a large\nresting-state EEG dataset show that the proposed CNNs significantly outperform\ntraditional support vector machine classifiers. The MDC-CNN with combined\nconnectivity features further improves performance over single-domain CNNs\nusing individual features, achieving remarkable accuracy of $93.06\\%$ with a\ndecision-level fusion. The proposed MDC-CNN by integrating information from\ndiverse brain connectivity descriptors is able to accurately discriminate SZ\nfrom HC. The new framework is potentially useful for developing diagnostic\ntools for SZ and other disorders.\n", "versions": [{"version": "v1", "created": "Thu, 21 Mar 2019 07:35:54 GMT"}], "update_date": "2020-04-27", "authors_parsed": [["Phang", "Chun-Ren", ""], ["Ting", "Chee-Ming", ""], ["Noman", "Fuad", ""], ["Ombao", "Hernando", ""]]}, {"id": "1903.08863", "submitter": "Bertrand Girard", "authors": "Eduardo Sanchez (IRIT), Mathieu Serrurier (IRIT), Mathias Ortner", "title": "Learning Disentangled Representations of Satellite Image Time Series", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we investigate how to learn a suitable representation of\nsatellite image time series in an unsupervised manner by leveraging large\namounts of unlabeled data. Additionally , we aim to disentangle the\nrepresentation of time series into two representations: a shared representation\nthat captures the common information between the images of a time series and an\nexclusive representation that contains the specific information of each image\nof the time series. To address these issues, we propose a model that combines a\nnovel component called cross-domain autoencoders with the variational\nautoencoder (VAE) and generative ad-versarial network (GAN) methods. In order\nto learn disentangled representations of time series, our model learns the\nmultimodal image-to-image translation task. We train our model using satellite\nimage time series from the Sentinel-2 mission. Several experiments are carried\nout to evaluate the obtained representations. We show that these disentangled\nrepresentations can be very useful to perform multiple tasks such as image\nclassification, image retrieval, image segmentation and change detection.\n", "versions": [{"version": "v1", "created": "Thu, 21 Mar 2019 07:55:11 GMT"}], "update_date": "2019-03-22", "authors_parsed": [["Sanchez", "Eduardo", "", "IRIT"], ["Serrurier", "Mathieu", "", "IRIT"], ["Ortner", "Mathias", ""]]}, {"id": "1903.08871", "submitter": "Xiwei Tang", "authors": "Xiwei Tang, Xuan Bi and Annie Qu", "title": "Individualized Multilayer Tensor Learning with An Application in Imaging\n  Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work is motivated by multimodality breast cancer imaging data, which is\nquite challenging in that the signals of discrete tumor-associated\nmicrovesicles (TMVs) are randomly distributed with heterogeneous patterns. This\nimposes a significant challenge for conventional imaging regression and\ndimension reduction models assuming a homogeneous feature structure. We develop\nan innovative multilayer tensor learning method to incorporate heterogeneity to\na higher-order tensor decomposition and predict disease status effectively\nthrough utilizing subject-wise imaging features and multimodality information.\nSpecifically, we construct a multilayer decomposition which leverages an\nindividualized imaging layer in addition to a modality-specific tensor\nstructure. One major advantage of our approach is that we are able to\nefficiently capture the heterogeneous spatial features of signals that are not\ncharacterized by a population structure as well as integrating multimodality\ninformation simultaneously. To achieve scalable computing, we develop a new\nbi-level block improvement algorithm. In theory, we investigate both the\nalgorithm convergence property, tensor signal recovery error bound and\nasymptotic consistency for prediction model estimation. We also apply the\nproposed method for simulated and human breast cancer imaging data. Numerical\nresults demonstrate that the proposed method outperforms other existing\ncompeting methods.\n", "versions": [{"version": "v1", "created": "Thu, 21 Mar 2019 08:18:08 GMT"}], "update_date": "2019-03-22", "authors_parsed": [["Tang", "Xiwei", ""], ["Bi", "Xuan", ""], ["Qu", "Annie", ""]]}, {"id": "1903.08888", "submitter": "Jinshi Yu", "authors": "Jinshi Yu, Chao Li, Qibin Zhao, Guoxu Zhou", "title": "Tensor-Ring Nuclear Norm Minimization and Application for Visual Data\n  Completion", "comments": "This paper has been accepted by ICASSP 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tensor ring (TR) decomposition has been successfully used to obtain the\nstate-of-the-art performance in the visual data completion problem. However,\nthe existing TR-based completion methods are severely non-convex and\ncomputationally demanding. In addition, the determination of the optimal TR\nrank is a tough work in practice. To overcome these drawbacks, we first\nintroduce a class of new tensor nuclear norms by using tensor circular\nunfolding. Then we theoretically establish connection between the rank of the\ncircularly-unfolded matrices and the TR ranks. We also develop an efficient\ntensor completion algorithm by minimizing the proposed tensor nuclear norm.\nExtensive experimental results demonstrate that our proposed tensor completion\nmethod outperforms the conventional tensor completion methods in the\nimage/video in-painting problem with striped missing values.\n", "versions": [{"version": "v1", "created": "Thu, 21 Mar 2019 09:15:08 GMT"}], "update_date": "2019-03-22", "authors_parsed": [["Yu", "Jinshi", ""], ["Li", "Chao", ""], ["Zhao", "Qibin", ""], ["Zhou", "Guoxu", ""]]}, {"id": "1903.08890", "submitter": "Rui Lu", "authors": "Rui Lu, Menghan Zhou, Anlong Ming and Yu Zhou", "title": "Context-Constrained Accurate Contour Extraction for Occlusion Edge\n  Detection", "comments": "To appear in ICME 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Occlusion edge detection requires both accurate locations and context\nconstraints of the contour. Existing CNN-based pipeline does not utilize\nadaptive methods to filter the noise introduced by low-level features. To\naddress this dilemma, we propose a novel Context-constrained accurate Contour\nExtraction Network (CCENet). Spatial details are retained and contour-sensitive\ncontext is augmented through two extraction blocks, respectively. Then, an\nelaborately designed fusion module is available to integrate features, which\nplays a complementary role to restore details and remove clutter. Weight\nresponse of attention mechanism is eventually utilized to enhance occluded\ncontours and suppress noise. The proposed CCENet significantly surpasses\nstate-of-the-art methods on PIOD and BSDS ownership dataset of object edge\ndetection and occlusion orientation detection.\n", "versions": [{"version": "v1", "created": "Thu, 21 Mar 2019 09:21:41 GMT"}], "update_date": "2019-03-22", "authors_parsed": [["Lu", "Rui", ""], ["Zhou", "Menghan", ""], ["Ming", "Anlong", ""], ["Zhou", "Yu", ""]]}, {"id": "1903.08923", "submitter": "Lin Xu", "authors": "Lin Xu, Han Sun, Yuai Liu", "title": "Learning with Batch-wise Optimal Transport Loss for 3D Shape Recognition", "comments": "10 pages, 4 figures Accepted by CVPR2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep metric learning is essential for visual recognition. The widely used\npair-wise (or triplet) based loss objectives cannot make full use of semantical\ninformation in training samples or give enough attention to those hard samples\nduring optimization. Thus, they often suffer from a slow convergence rate and\ninferior performance. In this paper, we show how to learn an importance-driven\ndistance metric via optimal transport programming from batches of samples. It\ncan automatically emphasize hard examples and lead to significant improvements\nin convergence. We propose a new batch-wise optimal transport loss and combine\nit in an end-to-end deep metric learning manner. We use it to learn the\ndistance metric and deep feature representation jointly for recognition.\nEmpirical results on visual retrieval and classification tasks with six\nbenchmark datasets, i.e., MNIST, CIFAR10, SHREC13, SHREC14, ModelNet10, and\nModelNet40, demonstrate the superiority of the proposed method. It can\naccelerate the convergence rate significantly while achieving a\nstate-of-the-art recognition performance. For example, in 3D shape recognition\nexperiments, we show that our method can achieve better recognition performance\nwithin only 5 epochs than what can be obtained by mainstream 3D shape\nrecognition approaches after 200 epochs.\n", "versions": [{"version": "v1", "created": "Thu, 21 Mar 2019 11:10:05 GMT"}], "update_date": "2019-03-22", "authors_parsed": [["Xu", "Lin", ""], ["Sun", "Han", ""], ["Liu", "Yuai", ""]]}, {"id": "1903.08943", "submitter": "Soumyabrata Dev", "authors": "Soumyabrata Dev, Murhaf Hossari, Matthew Nicholson, Killian McCabe,\n  Atul Nautiyal, Clare Conran, Jian Tang, Wei Xu, and Fran\\c{c}ois Piti\\'e", "title": "The CASE Dataset of Candidate Spaces for Advert Implantation", "comments": "Published in Proc. International Conference on Machine Vision\n  Applications (MVA), 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the advent of faster internet services and growth of multimedia content,\nwe observe a massive growth in the number of online videos. The users generate\nthese video contents at an unprecedented rate, owing to the use of smart-phones\nand other hand-held video capturing devices. This creates immense potential for\nthe advertising and marketing agencies to create personalized content for the\nusers. In this paper, we attempt to assist the video editors to generate\naugmented video content, by proposing candidate spaces in video frames. We\npropose and release a large-scale dataset of outdoor scenes, along with\nmanually annotated maps for candidate spaces. We also benchmark several\ndeep-learning based semantic segmentation algorithms on this proposed dataset.\n", "versions": [{"version": "v1", "created": "Thu, 21 Mar 2019 12:13:46 GMT"}, {"version": "v2", "created": "Mon, 29 Apr 2019 09:32:48 GMT"}], "update_date": "2019-04-30", "authors_parsed": [["Dev", "Soumyabrata", ""], ["Hossari", "Murhaf", ""], ["Nicholson", "Matthew", ""], ["McCabe", "Killian", ""], ["Nautiyal", "Atul", ""], ["Conran", "Clare", ""], ["Tang", "Jian", ""], ["Xu", "Wei", ""], ["Piti\u00e9", "Fran\u00e7ois", ""]]}, {"id": "1903.08960", "submitter": "Lukas Hoyer", "authors": "Lukas Hoyer, Patrick Kesper, Anna Khoreva and Volker Fischer", "title": "Short-Term Prediction and Multi-Camera Fusion on Semantic Grids", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An environment representation (ER) is a substantial part of every autonomous\nsystem. It introduces a common interface between perception and other system\ncomponents, such as decision making, and allows downstream algorithms to deal\nwith abstracted data without knowledge of the used sensor. In this work, we\npropose and evaluate a novel architecture that generates an egocentric,\ngrid-based, predictive, and semantically-interpretable ER. In particular, we\nprovide a proof of concept for the spatio-temporal fusion of multiple camera\nsequences and short-term prediction in such an ER. Our design utilizes a strong\nsemantic segmentation network together with depth and egomotion estimates to\nfirst extract semantic information from multiple camera streams and then\ntransform these separately into egocentric temporally-aligned bird's-eye view\ngrids. A deep encoder-decoder network is trained to fuse a stack of these grids\ninto a unified semantic grid representation and to predict the dynamics of its\nsurrounding. We evaluate this representation on real-world sequences of the\nCityscapes dataset and show that our architecture can make accurate predictions\nin complex sensor fusion scenarios and significantly outperforms a model-driven\nbaseline in a category-based evaluation.\n", "versions": [{"version": "v1", "created": "Thu, 21 Mar 2019 12:49:31 GMT"}, {"version": "v2", "created": "Fri, 26 Jul 2019 18:31:06 GMT"}], "update_date": "2019-07-30", "authors_parsed": [["Hoyer", "Lukas", ""], ["Kesper", "Patrick", ""], ["Khoreva", "Anna", ""], ["Fischer", "Volker", ""]]}, {"id": "1903.09021", "submitter": "Ram Padhy", "authors": "Ram Prasad Padhy, Shahzad Ahmad, Sachin Verma, Pankaj Kumar Sa and\n  Sambit Bakshi", "title": "Localization of Unmanned Aerial Vehicles in Corridor Environments using\n  Deep Learning", "comments": "9 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Vision-based pose estimation of Unmanned Aerial Vehicles (UAV) in unknown\nenvironments is a rapidly growing research area in the field of robot vision.\nThe task becomes more complex when the only available sensor is a static single\ncamera (monocular vision). In this regard, we propose a monocular vision\nassisted localization algorithm, that will help a UAV to navigate safely in\nindoor corridor environments. Always, the aim is to navigate the UAV through a\ncorridor in the forward direction by keeping it at the center with no\norientation either to the left or right side. The algorithm makes use of the\nRGB image, captured from the UAV front camera, and passes it through a trained\ndeep neural network (DNN) to predict the position of the UAV as either on the\nleft or center or right side of the corridor. Depending upon the divergence of\nthe UAV with respect to the central bisector line (CBL) of the corridor, a\nsuitable command is generated to bring the UAV to the center. When the UAV is\nat the center of the corridor, a new image is passed through another trained\nDNN to predict the orientation of the UAV with respect to the CBL of the\ncorridor. If the UAV is either left or right tilted, an appropriate command is\ngenerated to rectify the orientation. We also propose a new corridor dataset,\nnamed NITRCorrV1, which contains images as captured by the UAV front camera\nwhen the UAV is at all possible locations of a variety of corridors. An\nexhaustive set of experiments in different corridors reveal the efficacy of the\nproposed algorithm.\n", "versions": [{"version": "v1", "created": "Thu, 21 Mar 2019 14:21:11 GMT"}], "update_date": "2019-03-22", "authors_parsed": [["Padhy", "Ram Prasad", ""], ["Ahmad", "Shahzad", ""], ["Verma", "Sachin", ""], ["Sa", "Pankaj Kumar", ""], ["Bakshi", "Sambit", ""]]}, {"id": "1903.09036", "submitter": "Omar Elgendy", "authors": "Abhiram Gnanasambandam and Omar Elgendy and Jiaju Ma and and Stanley\n  H. Chan", "title": "Megapixel Photon-Counting Color Imaging using Quanta Image Sensor", "comments": null, "journal-ref": null, "doi": "10.1364/OE.27.017298", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quanta Image Sensor (QIS) is a single-photon detector designed for extremely\nlow light imaging conditions. Majority of the existing QIS prototypes are\nmonochrome based on single-photon avalanche diodes (SPAD). Passive color\nimaging has not been demonstrated with single-photon detectors due to the\nintrinsic difficulty of shrinking the pixel size and increasing the spatial\nresolution while maintaining acceptable intra-pixel cross-talk. In this paper,\nwe present image reconstruction of the first color QIS with a resolution of\n$1024 \\times 1024$ pixels, supporting both single-bit and multi-bit photon\ncounting capability. Our color image reconstruction is enabled by a customized\njoint demosaicing-denoising algorithm, leveraging truncated Poisson statistics\nand variance stabilizing transforms. Experimental results of the new sensor and\nalgorithm demonstrate superior color imaging performance for very low-light\nconditions with a mean exposure of as low as a few photons per pixel in both\nreal and simulated images.\n", "versions": [{"version": "v1", "created": "Thu, 21 Mar 2019 14:49:07 GMT"}, {"version": "v2", "created": "Mon, 20 May 2019 20:38:55 GMT"}], "update_date": "2019-06-26", "authors_parsed": [["Gnanasambandam", "Abhiram", ""], ["Elgendy", "Omar", ""], ["Ma", "Jiaju", ""], ["Chan", "and Stanley H.", ""]]}, {"id": "1903.09067", "submitter": "Ji Zhao", "authors": "Ji Zhao", "title": "An Efficient Solution to Non-Minimal Case Essential Matrix Estimation", "comments": "IEEE Transactions on Pattern Analysis and Machine Intelligence", "journal-ref": null, "doi": "10.1109/TPAMI.2020.3030161", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Finding relative pose between two calibrated images is a fundamental task in\ncomputer vision. Given five point correspondences, the classical five-point\nmethods can be used to calculate the essential matrix efficiently. For the case\nof $N$ ($N > 5$) inlier point correspondences, which is called $N$-point\nproblem, existing methods are either inefficient or prone to local minima. In\nthis paper, we propose a certifiably globally optimal and efficient solver for\nthe $N$-point problem. First we formulate the problem as a quadratically\nconstrained quadratic program (QCQP). Then a certifiably globally optimal\nsolution to this problem is obtained by semidefinite relaxation. This allows us\nto obtain certifiably globally optimal solutions to the original non-convex\nQCQPs in polynomial time. The theoretical guarantees of the semidefinite\nrelaxation are also provided, including tightness and local stability. To deal\nwith outliers, we propose a robust $N$-point method using M-estimators. Though\nglobal optimality cannot be guaranteed for the overall robust framework, the\nproposed robust $N$-point method can achieve good performance when the outlier\nratio is not high. Extensive experiments on synthetic and real-world datasets\ndemonstrated that our $N$-point method is $2\\sim3$ orders of magnitude faster\nthan state-of-the-art methods. Moreover, our robust $N$-point method\noutperforms state-of-the-art methods in terms of robustness and accuracy.\n", "versions": [{"version": "v1", "created": "Thu, 21 Mar 2019 15:47:37 GMT"}, {"version": "v2", "created": "Mon, 14 Oct 2019 15:12:29 GMT"}, {"version": "v3", "created": "Wed, 7 Oct 2020 09:52:04 GMT"}], "update_date": "2020-12-23", "authors_parsed": [["Zhao", "Ji", ""]]}, {"id": "1903.09073", "submitter": "Thomas Mitchel", "authors": "Thomas W. Mitchel, Christian Wuelker, Jin Seob Kim, Sipu Ruan, Gregory\n  S. Chirikjian", "title": "Quotienting Impertinent Camera Kinematics for 3D Video Stabilization", "comments": "Added acknowledgements", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the recent advent of methods that allow for real-time computation, dense\n3D flows have become a viable basis for fast camera motion estimation. Most\nimportantly, dense flows are more robust than the sparse feature matching\ntechniques used by existing 3D stabilization methods, able to better handle\nlarge camera displacements and occlusions similar to those often found in\nconsumer videos. Here we introduce a framework for 3D video stabilization that\nrelies on dense scene flow alone. The foundation of this approach is a novel\ncamera motion model that allows for real-world camera poses to be recovered\ndirectly from 3D motion fields. Moreover, this model can be extended to\ndescribe certain types of non-rigid artifacts that are commonly found in\nvideos, such as those resulting from zooms. This framework gives rise to\nseveral robust regimes that produce high-quality stabilization of the kind\nachieved by prior full 3D methods while avoiding the fragility typically\npresent in feature-based approaches. As an added benefit, our framework is\nfast: the simplicity of our motion model and efficient flow calculations\ncombine to enable stabilization at a high frame rate.\n", "versions": [{"version": "v1", "created": "Thu, 21 Mar 2019 15:51:31 GMT"}, {"version": "v2", "created": "Mon, 5 Aug 2019 16:25:01 GMT"}], "update_date": "2019-08-06", "authors_parsed": [["Mitchel", "Thomas W.", ""], ["Wuelker", "Christian", ""], ["Kim", "Jin Seob", ""], ["Ruan", "Sipu", ""], ["Chirikjian", "Gregory S.", ""]]}, {"id": "1903.09107", "submitter": "Mubariz Zaffar", "authors": "Mubariz Zaffar, Ahmad Khaliq, Shoaib Ehsan, Michael Milford and Klaus\n  McDonald-Maier", "title": "Levelling the Playing Field: A Comprehensive Comparison of Visual Place\n  Recognition Approaches under Changing Conditions", "comments": "ICRA 2019 Workshop on Database Generation and Benchmarking of SLAM\n  Algorithms for Robotics and VR/AR", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years there has been significant improvement in the capability of\nVisual Place Recognition (VPR) methods, building on the success of both\nhand-crafted and learnt visual features, temporal filtering and usage of\nsemantic scene information. The wide range of approaches and the relatively\nrecent growth in interest in the field has meant that a wide range of datasets\nand assessment methodologies have been proposed, often with a focus only on\nprecision-recall type metrics, making comparison difficult. In this paper we\npresent a comprehensive approach to evaluating the performance of 10\nstate-of-the-art recently-developed VPR techniques, which utilizes three\nstandardized metrics: (a) Matching Performance b) Matching Time c) Memory\nFootprint. Together this analysis provides an up-to-date and widely\nencompassing snapshot of the various strengths and weaknesses of contemporary\napproaches to the VPR problem. The aim of this work is to help move this\nparticular research field towards a more mature and unified approach to the\nproblem, enabling better comparison and hence more progress to be made in\nfuture research.\n", "versions": [{"version": "v1", "created": "Thu, 21 Mar 2019 16:46:25 GMT"}, {"version": "v2", "created": "Mon, 29 Apr 2019 18:47:18 GMT"}], "update_date": "2019-05-01", "authors_parsed": [["Zaffar", "Mubariz", ""], ["Khaliq", "Ahmad", ""], ["Ehsan", "Shoaib", ""], ["Milford", "Michael", ""], ["McDonald-Maier", "Klaus", ""]]}, {"id": "1903.09115", "submitter": "Seong Hun Lee", "authors": "Seong Hun Lee, Javier Civera", "title": "Closed-Form Optimal Two-View Triangulation Based on Angular Errors", "comments": "Accepted to ICCV2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study closed-form optimal solutions to two-view\ntriangulation with known internal calibration and pose. By formulating the\ntriangulation problem as $L_1$ and $L_\\infty$ minimization of angular\nreprojection errors, we derive the exact closed-form solutions that guarantee\nglobal optimality under respective cost functions. To the best of our\nknowledge, we are the first to present such solutions. Since the angular error\nis rotationally invariant, our solutions can be applied for any type of central\ncameras, be it perspective, fisheye or omnidirectional. Our methods also\nrequire significantly less computation than the existing optimal methods.\nExperimental results on synthetic and real datasets validate our theoretical\nderivations.\n", "versions": [{"version": "v1", "created": "Thu, 21 Mar 2019 17:11:56 GMT"}, {"version": "v2", "created": "Fri, 22 Mar 2019 21:29:36 GMT"}, {"version": "v3", "created": "Mon, 29 Jul 2019 17:02:18 GMT"}], "update_date": "2019-07-30", "authors_parsed": [["Lee", "Seong Hun", ""], ["Civera", "Javier", ""]]}, {"id": "1903.09123", "submitter": "Tapabrata Chakraborti", "authors": "Tapabrata Chakraborti, Brendan McCane, Steven Mills, Umapada Pal", "title": "PProCRC: Probabilistic Collaboration of Image Patches", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a conditional probabilistic framework for collaborative\nrepresentation of image patches. It incorporates background compensation and\noutlier patch suppression into the main formulation itself, thus doing away\nwith the need for pre-processing steps to handle the same. A closed form\nnon-iterative solution of the cost function is derived. The proposed method\n(PProCRC) outperforms earlier CRC formulations: patch based (PCRC, GP-CRC) as\nwell as the state-of-the-art probabilistic (ProCRC and EProCRC) on three\nfine-grained species recognition datasets (Oxford Flowers, Oxford-IIIT Pets and\nCUB Birds) using two CNN backbones (Vgg-19 and ResNet-50).\n", "versions": [{"version": "v1", "created": "Thu, 21 Mar 2019 17:30:46 GMT"}, {"version": "v2", "created": "Wed, 11 Dec 2019 08:25:13 GMT"}, {"version": "v3", "created": "Mon, 9 Nov 2020 20:32:42 GMT"}], "update_date": "2020-11-11", "authors_parsed": [["Chakraborti", "Tapabrata", ""], ["McCane", "Brendan", ""], ["Mills", "Steven", ""], ["Pal", "Umapada", ""]]}, {"id": "1903.09126", "submitter": "Chaoxu Guo", "authors": "Chaoxu Guo, Bin Fan, Jie Gu, Qian Zhang, Shiming Xiang, Veronique\n  Prinet, Chunhong Pan", "title": "Progressive Sparse Local Attention for Video object detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transferring image-based object detectors to the domain of videos remains a\nchallenging problem. Previous efforts mostly exploit optical flow to propagate\nfeatures across frames, aiming to achieve a good trade-off between accuracy and\nefficiency. However, introducing an extra model to estimate optical flow can\nsignificantly increase the overall model size. The gap between optical flow and\nhigh-level features can also hinder it from establishing spatial correspondence\naccurately. Instead of relying on optical flow, this paper proposes a novel\nmodule called Progressive Sparse Local Attention (PSLA), which establishes the\nspatial correspondence between features across frames in a local region with\nprogressively sparser stride and uses the correspondence to propagate features.\nBased on PSLA, Recursive Feature Updating (RFU) and Dense Feature Transforming\n(DenseFT) are proposed to model temporal appearance and enrich feature\nrepresentation respectively in a novel video object detection framework.\nExperiments on ImageNet VID show that our method achieves the best accuracy\ncompared to existing methods with smaller model size and acceptable runtime\nspeed.\n", "versions": [{"version": "v1", "created": "Thu, 21 Mar 2019 17:33:22 GMT"}, {"version": "v2", "created": "Mon, 25 Mar 2019 04:09:55 GMT"}, {"version": "v3", "created": "Fri, 16 Aug 2019 13:08:37 GMT"}], "update_date": "2019-08-19", "authors_parsed": [["Guo", "Chaoxu", ""], ["Fan", "Bin", ""], ["Gu", "Jie", ""], ["Zhang", "Qian", ""], ["Xiang", "Shiming", ""], ["Prinet", "Veronique", ""], ["Pan", "Chunhong", ""]]}, {"id": "1903.09169", "submitter": "Michele Pratusevich", "authors": "Michele Pratusevich, Jason Chrisos, Shreyas Aditya", "title": "Quantitative Depth Quality Assessment of RGBD Cameras At Close Range\n  Using 3D Printed Fixtures", "comments": "4 pages; submitted to ICRA 2019 HAMM Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mobile robots that manipulate their environments require high-accuracy scene\nunderstanding at close range. Typically this understanding is achieved with\nRGBD cameras, but the evaluation process for selecting an appropriate RGBD\ncamera for the application is minimally quantitative. Limited\nmanufacturer-published metrics do not translate to observed quality in\nreal-world cluttered environments, since quality is application-specific. To\nbridge the gap, we present a method for quantitatively measuring depth quality\nusing a set of extendable 3D printed fixtures that approximate real-world\nconditions. By framing depth quality as point cloud density and root mean\nsquare error (RMSE) from a known geometry, we present a method that is\nextendable by other system integrators for custom environments. We show a\ncomparison of 3 cameras and present a case study for camera selection, provide\nreference meshes and analysis code, and discuss further extensions.\n", "versions": [{"version": "v1", "created": "Thu, 21 Mar 2019 18:03:29 GMT"}], "update_date": "2019-03-25", "authors_parsed": [["Pratusevich", "Michele", ""], ["Chrisos", "Jason", ""], ["Aditya", "Shreyas", ""]]}, {"id": "1903.09190", "submitter": "Adam Kubany", "authors": "Adam Kubany, Shimon Ben Ishay, Ruben-sacha Ohayon, Armin Shmilovici,\n  Lior Rokach, Tomer Doitshman", "title": "Comparison of State-of-the-Art Deep Learning APIs for Image Multi-Label\n  Classification using Semantic Metrics", "comments": null, "journal-ref": null, "doi": "10.1016/j.eswa.2020.113656", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image understanding heavily relies on accurate multi-label classification. In\nrecent years, deep learning algorithms have become very successful for such\ntasks, and various commercial and open-source APIs have been released for\npublic use. However, these APIs are often trained on different datasets, which,\nbesides affecting their performance, might pose a challenge to their\nperformance evaluation. This challenge concerns the different object-class\ndictionaries of the APIs' training dataset and the benchmark dataset, in which\nthe predicted labels are semantically similar to the benchmark labels but\nconsidered different simply because they have different wording in the\ndictionaries. To face this challenge, we propose semantic similarity metrics to\nobtain richer understating of the APIs predicted labels and thus their\nperformance. In this study, we evaluate and compare the performance of 13 of\nthe most prominent commercial and open-source APIs in a best-of-breed challenge\non the Visual Genome and Open Images benchmark datasets. Our findings\ndemonstrate that, while using traditional metrics, the Microsoft Computer\nVision, Imagga, and IBM APIs performed better than others. However, applying\nsemantic metrics also unveil the InceptionResNet-v2, Inception-v3, and ResNet50\nAPIs, which are trained only with the simple ImageNet dataset, as challengers\nfor top semantic performers.\n", "versions": [{"version": "v1", "created": "Thu, 21 Mar 2019 18:43:39 GMT"}, {"version": "v2", "created": "Fri, 5 Apr 2019 09:04:51 GMT"}, {"version": "v3", "created": "Wed, 10 Jul 2019 08:17:27 GMT"}, {"version": "v4", "created": "Thu, 11 Jun 2020 08:26:39 GMT"}], "update_date": "2020-07-06", "authors_parsed": [["Kubany", "Adam", ""], ["Ishay", "Shimon Ben", ""], ["Ohayon", "Ruben-sacha", ""], ["Shmilovici", "Armin", ""], ["Rokach", "Lior", ""], ["Doitshman", "Tomer", ""]]}, {"id": "1903.09199", "submitter": "Jiexiong Tang", "authors": "Jiexiong Tang, John Folkesson and Patric Jensfelt", "title": "Sparse2Dense: From direct sparse odometry to dense 3D reconstruction", "comments": "Accepted to ICRA 2019 (RA-L option), video demo available at\n  https://www.youtube.com/watch?v=3pbSHX72JC8&t=22s", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we proposed a new deep learning based dense monocular SLAM\nmethod. Compared to existing methods, the proposed framework constructs a dense\n3D model via a sparse to dense mapping using learned surface normals. With\nsingle view learned depth estimation as prior for monocular visual odometry, we\nobtain both accurate positioning and high quality depth reconstruction. The\ndepth and normal are predicted by a single network trained in a tightly coupled\nmanner.Experimental results show that our method significantly improves the\nperformance of visual tracking and depth prediction in comparison to the\nstate-of-the-art in deep monocular dense SLAM.\n", "versions": [{"version": "v1", "created": "Thu, 21 Mar 2019 19:01:37 GMT"}], "update_date": "2019-03-25", "authors_parsed": [["Tang", "Jiexiong", ""], ["Folkesson", "John", ""], ["Jensfelt", "Patric", ""]]}, {"id": "1903.09214", "submitter": "Sheng Jin", "authors": "Sheng Jin, Wentao Liu, Wanli Ouyang, Chen Qian", "title": "Multi-person Articulated Tracking with Spatial and Temporal Embeddings", "comments": "CVPR2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a unified framework for multi-person pose estimation and tracking.\nOur framework consists of two main components,~\\ie~SpatialNet and TemporalNet.\nThe SpatialNet accomplishes body part detection and part-level data association\nin a single frame, while the TemporalNet groups human instances in consecutive\nframes into trajectories. Specifically, besides body part detection heatmaps,\nSpatialNet also predicts the Keypoint Embedding (KE) and Spatial Instance\nEmbedding (SIE) for body part association. We model the grouping procedure into\na differentiable Pose-Guided Grouping (PGG) module to make the whole part\ndetection and grouping pipeline fully end-to-end trainable. TemporalNet extends\nspatial grouping of keypoints to temporal grouping of human instances. Given\nhuman proposals from two consecutive frames, TemporalNet exploits both\nappearance features encoded in Human Embedding (HE) and temporally consistent\ngeometric features embodied in Temporal Instance Embedding (TIE) for robust\ntracking. Extensive experiments demonstrate the effectiveness of our proposed\nmodel. Remarkably, we demonstrate substantial improvements over the\nstate-of-the-art pose tracking method from 65.4\\% to 71.8\\% Multi-Object\nTracking Accuracy (MOTA) on the ICCV'17 PoseTrack Dataset.\n", "versions": [{"version": "v1", "created": "Thu, 21 Mar 2019 19:42:27 GMT"}], "update_date": "2019-03-25", "authors_parsed": [["Jin", "Sheng", ""], ["Liu", "Wentao", ""], ["Ouyang", "Wanli", ""], ["Qian", "Chen", ""]]}, {"id": "1903.09233", "submitter": "Ilke Demir", "authors": "Ilke Demir, Camilla Hahn, Kathryn Leonard, Geraldine Morin, Dana\n  Rahbani, Athina Panotopoulou, Amelie Fondevilla, Elena Balashova, Bastien\n  Durix, Adam Kortylewski", "title": "SkelNetOn 2019: Dataset and Challenge on Deep Learning for Geometric\n  Shape Understanding", "comments": "Dataset paper for SkelNetOn Challenge, in association with Deep\n  Learning for Geometric Shape Understanding Workshop at CVPR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present SkelNetOn 2019 Challenge and Deep Learning for Geometric Shape\nUnderstanding workshop to utilize existing and develop novel deep learning\narchitectures for shape understanding. We observed that unlike traditional\nsegmentation and detection tasks, geometry understanding is still a new area\nfor deep learning techniques. SkelNetOn aims to bring together researchers from\ndifferent domains to foster learning methods on global shape understanding\ntasks. We aim to improve and evaluate the state-of-the-art shape understanding\napproaches, and to serve as reference benchmarks for future research. Similar\nto other challenges in computer vision, SkelNetOn proposes three datasets and\ncorresponding evaluation methodologies; all coherently bundled in three\ncompetitions with a dedicated workshop co-located with CVPR 2019 conference. In\nthis paper, we describe and analyze characteristics of datasets, define the\nevaluation criteria of the public competitions, and provide baselines for each\ntask.\n", "versions": [{"version": "v1", "created": "Thu, 21 Mar 2019 20:43:19 GMT"}, {"version": "v2", "created": "Tue, 14 May 2019 02:28:49 GMT"}, {"version": "v3", "created": "Sat, 22 Jun 2019 21:59:49 GMT"}], "update_date": "2019-06-25", "authors_parsed": [["Demir", "Ilke", ""], ["Hahn", "Camilla", ""], ["Leonard", "Kathryn", ""], ["Morin", "Geraldine", ""], ["Rahbani", "Dana", ""], ["Panotopoulou", "Athina", ""], ["Fondevilla", "Amelie", ""], ["Balashova", "Elena", ""], ["Durix", "Bastien", ""], ["Kortylewski", "Adam", ""]]}, {"id": "1903.09240", "submitter": "Subhashis Banerjee", "authors": "Subhashis Banerjee, Sushmita Mitra, Francesco Masulli, Stefano Rovetta", "title": "Deep Radiomics for Brain Tumor Detection and Classification from\n  Multi-Sequence MRI", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Glioma constitutes 80% of malignant primary brain tumors and is usually\nclassified as HGG and LGG. The LGG tumors are less aggressive, with slower\ngrowth rate as compared to HGG, and are responsive to therapy. Tumor biopsy\nbeing challenging for brain tumor patients, noninvasive imaging techniques like\nMagnetic Resonance Imaging (MRI) have been extensively employed in diagnosing\nbrain tumors. Therefore automated systems for the detection and prediction of\nthe grade of tumors based on MRI data becomes necessary for assisting doctors\nin the framework of augmented intelligence. In this paper, we thoroughly\ninvestigate the power of Deep ConvNets for classification of brain tumors using\nmulti-sequence MR images. We propose novel ConvNet models, which are trained\nfrom scratch, on MRI patches, slices, and multi-planar volumetric slices. The\nsuitability of transfer learning for the task is next studied by applying two\nexisting ConvNets models (VGGNet and ResNet) trained on ImageNet dataset,\nthrough fine-tuning of the last few layers. LOPO testing, and testing on the\nholdout dataset are used to evaluate the performance of the ConvNets. Results\ndemonstrate that the proposed ConvNets achieve better accuracy in all cases\nwhere the model is trained on the multi-planar volumetric dataset. Unlike\nconventional models, it obtains a testing accuracy of 95% for the low/high\ngrade glioma classification problem. A score of 97% is generated for\nclassification of LGG with/without 1p/19q codeletion, without any additional\neffort towards extraction and selection of features. We study the properties of\nself-learned kernels/ filters in different layers, through visualization of the\nintermediate layer outputs. We also compare the results with that of\nstate-of-the-art methods, demonstrating a maximum improvement of 7% on the\ngrading performance of ConvNets and 9% on the prediction of 1p/19q codeletion\nstatus.\n", "versions": [{"version": "v1", "created": "Thu, 21 Mar 2019 21:20:37 GMT"}], "update_date": "2019-03-25", "authors_parsed": [["Banerjee", "Subhashis", ""], ["Mitra", "Sushmita", ""], ["Masulli", "Francesco", ""], ["Rovetta", "Stefano", ""]]}, {"id": "1903.09254", "submitter": "Zheng Tang", "authors": "Zheng Tang, Milind Naphade, Ming-Yu Liu, Xiaodong Yang, Stan\n  Birchfield, Shuo Wang, Ratnesh Kumar, David Anastasiu, Jenq-Neng Hwang", "title": "CityFlow: A City-Scale Benchmark for Multi-Target Multi-Camera Vehicle\n  Tracking and Re-Identification", "comments": "Accepted for oral presentation at CVPR 2019 with review ratings of 2\n  strong accepts and 1 accept (work done during an internship at NVIDIA)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Urban traffic optimization using traffic cameras as sensors is driving the\nneed to advance state-of-the-art multi-target multi-camera (MTMC) tracking.\nThis work introduces CityFlow, a city-scale traffic camera dataset consisting\nof more than 3 hours of synchronized HD videos from 40 cameras across 10\nintersections, with the longest distance between two simultaneous cameras being\n2.5 km. To the best of our knowledge, CityFlow is the largest-scale dataset in\nterms of spatial coverage and the number of cameras/videos in an urban\nenvironment. The dataset contains more than 200K annotated bounding boxes\ncovering a wide range of scenes, viewing angles, vehicle models, and urban\ntraffic flow conditions. Camera geometry and calibration information are\nprovided to aid spatio-temporal analysis. In addition, a subset of the\nbenchmark is made available for the task of image-based vehicle\nre-identification (ReID). We conducted an extensive experimental evaluation of\nbaselines/state-of-the-art approaches in MTMC tracking, multi-target\nsingle-camera (MTSC) tracking, object detection, and image-based ReID on this\ndataset, analyzing the impact of different network architectures, loss\nfunctions, spatio-temporal models and their combinations on task effectiveness.\nAn evaluation server is launched with the release of our benchmark at the 2019\nAI City Challenge (https://www.aicitychallenge.org/) that allows researchers to\ncompare the performance of their newest techniques. We expect this dataset to\ncatalyze research in this field, propel the state-of-the-art forward, and lead\nto deployed traffic optimization(s) in the real world.\n", "versions": [{"version": "v1", "created": "Thu, 21 Mar 2019 22:03:25 GMT"}, {"version": "v2", "created": "Mon, 25 Mar 2019 06:32:44 GMT"}, {"version": "v3", "created": "Wed, 3 Apr 2019 18:41:37 GMT"}, {"version": "v4", "created": "Fri, 5 Apr 2019 22:52:17 GMT"}], "update_date": "2019-04-09", "authors_parsed": [["Tang", "Zheng", ""], ["Naphade", "Milind", ""], ["Liu", "Ming-Yu", ""], ["Yang", "Xiaodong", ""], ["Birchfield", "Stan", ""], ["Wang", "Shuo", ""], ["Kumar", "Ratnesh", ""], ["Anastasiu", "David", ""], ["Hwang", "Jenq-Neng", ""]]}, {"id": "1903.09263", "submitter": "Duc Duy Pham", "authors": "Duc Duy Pham, Gurbandurdy Dovletov, Sebastian Warwas, Stefan\n  Landgraeber, Marcus J\\\"ager, Josef Pauli", "title": "Deep Learning with Anatomical Priors: Imitating Enhanced Autoencoders in\n  Latent Space for Improved Pelvic Bone Segmentation in MRI", "comments": "Accepted for IEEE International Symposium on Biomedical Imaging\n  (ISBI) 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a 2D Encoder-Decoder based deep learning architecture for semantic\nsegmentation, that incorporates anatomical priors by imitating the encoder\ncomponent of an autoencoder in latent space. The autoencoder is additionally\nenhanced by means of hierarchical features, extracted by an U-Net module. Our\nsuggested architecture is trained in an end-to-end manner and is evaluated on\nthe example of pelvic bone segmentation in MRI. A comparison to the standard\nU-Net architecture shows promising improvements.\n", "versions": [{"version": "v1", "created": "Thu, 21 Mar 2019 22:28:14 GMT"}], "update_date": "2019-03-25", "authors_parsed": [["Pham", "Duc Duy", ""], ["Dovletov", "Gurbandurdy", ""], ["Warwas", "Sebastian", ""], ["Landgraeber", "Stefan", ""], ["J\u00e4ger", "Marcus", ""], ["Pauli", "Josef", ""]]}, {"id": "1903.09272", "submitter": "Shi Yin", "authors": "Shi Yin and Zhengqiang Zhang and Qinmu Peng and Xinge You", "title": "Fast and accurate reconstruction of HARDI using a 1D encoder-decoder\n  convolutional network", "comments": "4 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High angular resolution diffusion imaging (HARDI) demands a lager amount of\ndata measurements compared to diffusion tensor imaging, restricting its use in\npractice. In this work, we explore a learning-based approach to reconstruct\nHARDI from a smaller number of measurements in q-space. The approach aims to\ndirectly learn the mapping relationship between the measured and HARDI signals\nfrom the collecting HARDI acquisitions of other subjects. Specifically, the\nmapping is represented as a 1D encoder-decoder convolutional neural network\nunder the guidance of the compressed sensing (CS) theory for HARDI\nreconstruction. The proposed network architecture mainly consists of two parts:\nan encoder network produces the sparse coefficients and a decoder network\nyields a reconstruction result. Experiment results demonstrate we can robustly\nreconstruct HARDI signals with the accurate results and fast speed.\n", "versions": [{"version": "v1", "created": "Thu, 21 Mar 2019 23:56:16 GMT"}], "update_date": "2019-03-25", "authors_parsed": [["Yin", "Shi", ""], ["Zhang", "Zhengqiang", ""], ["Peng", "Qinmu", ""], ["You", "Xinge", ""]]}, {"id": "1903.09291", "submitter": "Shaohui Lin", "authors": "Shaohui Lin, Rongrong Ji, Chenqian Yan, Baochang Zhang, Liujuan Cao,\n  Qixiang Ye, Feiyue Huang, David Doermann", "title": "Towards Optimal Structured CNN Pruning via Generative Adversarial\n  Learning", "comments": "Proceedings of the IEEE Conference on Computer Vision and Pattern\n  Recognition (CVPR)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Structured pruning of filters or neurons has received increased focus for\ncompressing convolutional neural networks. Most existing methods rely on\nmulti-stage optimizations in a layer-wise manner for iteratively pruning and\nretraining which may not be optimal and may be computation intensive. Besides,\nthese methods are designed for pruning a specific structure, such as filter or\nblock structures without jointly pruning heterogeneous structures. In this\npaper, we propose an effective structured pruning approach that jointly prunes\nfilters as well as other structures in an end-to-end manner. To accomplish\nthis, we first introduce a soft mask to scale the output of these structures by\ndefining a new objective function with sparsity regularization to align the\noutput of baseline and network with this mask. We then effectively solve the\noptimization problem by generative adversarial learning (GAL), which learns a\nsparse soft mask in a label-free and an end-to-end manner. By forcing more\nscaling factors in the soft mask to zero, the fast iterative\nshrinkage-thresholding algorithm (FISTA) can be leveraged to fast and reliably\nremove the corresponding structures. Extensive experiments demonstrate the\neffectiveness of GAL on different datasets, including MNIST, CIFAR-10 and\nImageNet ILSVRC 2012. For example, on ImageNet ILSVRC 2012, the pruned\nResNet-50 achieves 10.88\\% Top-5 error and results in a factor of 3.7x speedup.\nThis significantly outperforms state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Fri, 22 Mar 2019 01:26:33 GMT"}], "update_date": "2019-03-25", "authors_parsed": [["Lin", "Shaohui", ""], ["Ji", "Rongrong", ""], ["Yan", "Chenqian", ""], ["Zhang", "Baochang", ""], ["Cao", "Liujuan", ""], ["Ye", "Qixiang", ""], ["Huang", "Feiyue", ""], ["Doermann", "David", ""]]}, {"id": "1903.09330", "submitter": "Cai Ning", "authors": "Cai Ning, Shi Fei, Hu Dianlin, Chen Yang", "title": "A resnet-based universal method for speckle reduction in optical\n  coherence tomography images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we propose a ResNet-based universal method for speckle reduction\nin optical coherence tomography (OCT) images. The proposed model contains 3\nmain modules: Convolution-BN-ReLU, Branch and Residual module. Unlike\ntraditional algorithms, the model can learn from training data instead of\nselecting parameters manually such as noise level. Application of this proposed\nmethod to the OCT images shows a more than 22 dB signal-to-noise ratio\nimprovement in speckle noise reduction with minimal structure blurring. The\nproposed method provides strong generalization ability and can process noisy\nother types of OCT images without retraining. It outperforms other filtering\nmethods in suppressing speckle noises and revealing subtle features.\n", "versions": [{"version": "v1", "created": "Fri, 22 Mar 2019 02:55:31 GMT"}], "update_date": "2019-03-25", "authors_parsed": [["Ning", "Cai", ""], ["Fei", "Shi", ""], ["Dianlin", "Hu", ""], ["Yang", "Chen", ""]]}, {"id": "1903.09331", "submitter": "Bibo Shi", "authors": "Chen Qin, Bibo Shi, Rui Liao, Tommaso Mansi, Daniel Rueckert, Ali\n  Kamen", "title": "Unsupervised Deformable Registration for Multi-Modal Images via\n  Disentangled Representations", "comments": "Accepted as an oral presentation in IPMI 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a fully unsupervised multi-modal deformable image registration\nmethod (UMDIR), which does not require any ground truth deformation fields or\nany aligned multi-modal image pairs during training. Multi-modal registration\nis a key problem in many medical image analysis applications. It is very\nchallenging due to complicated and unknown relationships between different\nmodalities. In this paper, we propose an unsupervised learning approach to\nreduce the multi-modal registration problem to a mono-modal one through image\ndisentangling. In particular, we decompose images of both modalities into a\ncommon latent shape space and separate latent appearance spaces via an\nunsupervised multi-modal image-to-image translation approach. The proposed\nregistration approach is then built on the factorized latent shape code, with\nthe assumption that the intrinsic shape deformation existing in original image\ndomain is preserved in this latent space. Specifically, two metrics have been\nproposed for training the proposed network: a latent similarity metric defined\nin the common shape space and a learningbased image similarity metric based on\nan adversarial loss. We examined different variations of our proposed approach\nand compared them with conventional state-of-the-art multi-modal registration\nmethods. Results show that our proposed methods achieve competitive performance\nagainst other methods at substantially reduced computation time.\n", "versions": [{"version": "v1", "created": "Fri, 22 Mar 2019 03:01:54 GMT"}], "update_date": "2019-03-25", "authors_parsed": [["Qin", "Chen", ""], ["Shi", "Bibo", ""], ["Liao", "Rui", ""], ["Mansi", "Tommaso", ""], ["Rueckert", "Daniel", ""], ["Kamen", "Ali", ""]]}, {"id": "1903.09339", "submitter": "Robert Grupp", "authors": "Robert B. Grupp, Rachel A. Hegeman, Ryan J. Murphy, Clayton P.\n  Alexander, Yoshito Otake, Benjamin A. McArthur, Mehran Armand, Russell H.\n  Taylor", "title": "Pose Estimation of Periacetabular Osteotomy Fragments with\n  Intraoperative X-Ray Navigation", "comments": "Accepted for publication in IEEE Transactions on Biomedical\n  Engineering", "journal-ref": "IEEE Transactions on Biomedical Engineering, vol. 67, no. 2, pp.\n  441-452, Feb. 2020", "doi": "10.1109/TBME.2019.2915165", "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Objective: State of the art navigation systems for pelvic osteotomies use\noptical systems with external fiducials. We propose the use of X-Ray navigation\nfor pose estimation of periacetabular fragments without fiducials. Methods: A\n2D/3D registration pipeline was developed to recover fragment pose. This\npipeline was tested through an extensive simulation study and 6 cadaveric\nsurgeries. Using osteotomy boundaries in the fluoroscopic images, the\npreoperative plan is refined to more accurately match the intraoperative shape.\nResults: In simulation, average fragment pose errors were 1.3{\\deg}/1.7 mm when\nthe planned fragment matched the intraoperative fragment, 2.2{\\deg}/2.1 mm when\nthe plan was not updated to match the true shape, and 1.9{\\deg}/2.0 mm when the\nfragment shape was intraoperatively estimated. In cadaver experiments, the\naverage pose errors were 2.2{\\deg}/2.2 mm, 3.8{\\deg}/2.5 mm, and 3.5{\\deg}/2.2\nmm when registering with the actual fragment shape, a preoperative plan, and an\nintraoperatively refined plan, respectively. Average errors of the lateral\ncenter edge angle were less than 2{\\deg} for all fragment shapes in simulation\nand cadaver experiments. Conclusion: The proposed pipeline is capable of\naccurately reporting femoral head coverage within a range clinically identified\nfor long-term joint survivability. Significance: Human interpretation of\nfragment pose is challenging and usually restricted to rotation about a single\nanatomical axis. The proposed pipeline provides an intraoperative estimate of\nrigid pose with respect to all anatomical axes, is compatible with minimally\ninvasive incisions, and has no dependence on external fiducials.\n", "versions": [{"version": "v1", "created": "Fri, 22 Mar 2019 03:21:15 GMT"}, {"version": "v2", "created": "Thu, 9 May 2019 22:23:34 GMT"}], "update_date": "2020-05-04", "authors_parsed": [["Grupp", "Robert B.", ""], ["Hegeman", "Rachel A.", ""], ["Murphy", "Ryan J.", ""], ["Alexander", "Clayton P.", ""], ["Otake", "Yoshito", ""], ["McArthur", "Benjamin A.", ""], ["Armand", "Mehran", ""], ["Taylor", "Russell H.", ""]]}, {"id": "1903.09344", "submitter": "Weihuang Xu", "authors": "Weihuang Xu, Guohao Yu, Alina Zare, Brendan Zurweller, Diane Rowland,\n  Joel Reyes-Cabrera, Felix B Fritschi, Roser Matamala, Thomas E. Juenger", "title": "Overcoming Small Minirhizotron Datasets Using Transfer Learning", "comments": null, "journal-ref": "Computers and Electronics in Agriculture, 175 (2020)", "doi": "10.1016/j.compag.2020.105466", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Minirhizotron technology is widely used for studying the development of\nroots. Such systems collect visible-wavelength color imagery of plant roots\nin-situ by scanning an imaging system within a clear tube driven into the soil.\nAutomated analysis of root systems could facilitate new scientific discoveries\nthat would be critical to address the world's pressing food, resource, and\nclimate issues. A key component of automated analysis of plant roots from\nimagery is the automated pixel-level segmentation of roots from their\nsurrounding soil. Supervised learning techniques appear to be an appropriate\ntool for the challenge due to varying local soil and root conditions, however,\nlack of enough annotated training data is a major limitation due to the\nerror-prone and time-consuming manually labeling process. In this paper, we\ninvestigate the use of deep neural networks based on the U-net architecture for\nautomated, precise pixel-wise root segmentation in minirhizotron imagery. We\ncompiled two minirhizotron image datasets to accomplish this study: one with\n17,550 peanut root images and another with 28 switchgrass root images. Both\ndatasets were paired with manually labeled ground truth masks. We trained three\nneural networks with different architectures on the larger peanut root dataset\nto explore the effect of the neural network depth on segmentation performance.\nTo tackle the more limited switchgrass root dataset, we showed that models\ninitialized with features pre-trained on the peanut dataset and then fine-tuned\non the switchgrass dataset can improve segmentation performance significantly.\nWe obtained 99\\% segmentation accuracy in switchgrass imagery using only 21\ntraining images. We also observed that features pre-trained on a closely\nrelated but relatively moderate size dataset like our peanut dataset are more\neffective than features pre-trained on the large but unrelated ImageNet\ndataset.\n", "versions": [{"version": "v1", "created": "Fri, 22 Mar 2019 03:40:25 GMT"}, {"version": "v2", "created": "Mon, 26 Aug 2019 17:53:20 GMT"}, {"version": "v3", "created": "Fri, 24 Apr 2020 15:13:48 GMT"}], "update_date": "2020-07-02", "authors_parsed": [["Xu", "Weihuang", ""], ["Yu", "Guohao", ""], ["Zare", "Alina", ""], ["Zurweller", "Brendan", ""], ["Rowland", "Diane", ""], ["Reyes-Cabrera", "Joel", ""], ["Fritschi", "Felix B", ""], ["Matamala", "Roser", ""], ["Juenger", "Thomas E.", ""]]}, {"id": "1903.09359", "submitter": "Xiaoguang Tu", "authors": "Xiaoguang Tu, Jian Zhao, Zihang Jiang, Yao Luo, Mei Xie, Yang Zhao,\n  Linxiao He, Zheng Ma and Jiashi Feng", "title": "3D Face Reconstruction from A Single Image Assisted by 2D Face Images in\n  the Wild", "comments": "10 pages, 7figures, 3 tables", "journal-ref": null, "doi": "10.1109/TMM.2020.2993962", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  3D face reconstruction from a single 2D image is a challenging problem with\nbroad applications. Recent methods typically aim to learn a CNN-based 3D face\nmodel that regresses coefficients of 3D Morphable Model (3DMM) from 2D images\nto render 3D face reconstruction or dense face alignment. However, the shortage\nof training data with 3D annotations considerably limits performance of those\nmethods. To alleviate this issue, we propose a novel 2D-assisted\nself-supervised learning (2DASL) method that can effectively use \"in-the-wild\"\n2D face images with noisy landmark information to substantially improve 3D face\nmodel learning. Specifically, taking the sparse 2D facial landmarks as\nadditional information, 2DSAL introduces four novel self-supervision schemes\nthat view the 2D landmark and 3D landmark prediction as a self-mapping process,\nincluding the 2D and 3D landmark self-prediction consistency, cycle-consistency\nover the 2D landmark prediction and self-critic over the predicted 3DMM\ncoefficients based on landmark predictions. Using these four self-supervision\nschemes, the 2DASL method significantly relieves demands on the the\nconventional paired 2D-to-3D annotations and gives much higher-quality 3D face\nmodels without requiring any additional 3D annotations. Experiments on multiple\nchallenging datasets show that our method outperforms state-of-the-arts for\nboth 3D face reconstruction and dense face alignment by a large margin.\n", "versions": [{"version": "v1", "created": "Fri, 22 Mar 2019 05:06:13 GMT"}, {"version": "v2", "created": "Sun, 3 May 2020 01:41:03 GMT"}], "update_date": "2020-06-05", "authors_parsed": [["Tu", "Xiaoguang", ""], ["Zhao", "Jian", ""], ["Jiang", "Zihang", ""], ["Luo", "Yao", ""], ["Xie", "Mei", ""], ["Zhao", "Yang", ""], ["He", "Linxiao", ""], ["Ma", "Zheng", ""], ["Feng", "Jiashi", ""]]}, {"id": "1903.09372", "submitter": "Tao Wang", "authors": "Tao Wang, Xiaopeng Zhang, Li Yuan, Jiashi Feng", "title": "Few-shot Adaptive Faster R-CNN", "comments": "Accepted at CVPR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To mitigate the detection performance drop caused by domain shift, we aim to\ndevelop a novel few-shot adaptation approach that requires only a few target\ndomain images with limited bounding box annotations. To this end, we first\nobserve several significant challenges. First, the target domain data is highly\ninsufficient, making most existing domain adaptation methods ineffective.\nSecond, object detection involves simultaneous localization and classification,\nfurther complicating the model adaptation process. Third, the model suffers\nfrom over-adaptation (similar to overfitting when training with a few data\nexample) and instability risk that may lead to degraded detection performance\nin the target domain. To address these challenges, we first introduce a pairing\nmechanism over source and target features to alleviate the issue of\ninsufficient target domain samples. We then propose a bi-level module to adapt\nthe source trained detector to the target domain: 1) the split pooling based\nimage level adaptation module uniformly extracts and aligns paired local patch\nfeatures over locations, with different scale and aspect ratio; 2) the instance\nlevel adaptation module semantically aligns paired object features while avoids\ninter-class confusion. Meanwhile, a source model feature regularization (SMFR)\nis applied to stabilize the adaptation process of the two modules. Combining\nthese contributions gives a novel few-shot adaptive Faster-RCNN framework,\ntermed FAFRCNN, which effectively adapts to target domain with a few labeled\nsamples. Experiments with multiple datasets show that our model achieves new\nstate-of-the-art performance under both the interested few-shot domain\nadaptation(FDA) and unsupervised domain adaptation(UDA) setting.\n", "versions": [{"version": "v1", "created": "Fri, 22 Mar 2019 06:32:12 GMT"}], "update_date": "2019-03-25", "authors_parsed": [["Wang", "Tao", ""], ["Zhang", "Xiaopeng", ""], ["Yuan", "Li", ""], ["Feng", "Jiashi", ""]]}, {"id": "1903.09410", "submitter": "Aupendu Kar", "authors": "Aupendu Kar and Prabir Kumar Biswas", "title": "Fast Bayesian Uncertainty Estimation and Reduction of Batch Normalized\n  Single Image Super-Resolution Network", "comments": "To appear in the Proceedings of the IEEE Conference on Computer\n  Vision and Pattern Recognition (CVPR 2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Convolutional neural network (CNN) has achieved unprecedented success in\nimage super-resolution tasks in recent years. However, the network's\nperformance depends on the distribution of the training sets and degrades on\nout-of-distribution samples. This paper adopts a Bayesian approach for\nestimating uncertainty associated with output and applies it in a deep image\nsuper-resolution model to address the concern mentioned above. We use the\nuncertainty estimation technique using the batch-normalization layer, where\nstochasticity of the batch mean and variance generate Monte-Carlo (MC) samples.\nThe MC samples, which are nothing but different super-resolved images using\ndifferent stochastic parameters, reconstruct the image, and provide a\nconfidence or uncertainty map of the reconstruction. We propose a faster\napproach for MC sample generation, and it allows the variable image size during\ntesting. Therefore, it will be useful for image reconstruction domain. Our\nexperimental findings show that this uncertainty map strongly relates to the\nquality of reconstruction generated by the deep CNN model and explains its\nlimitation. Furthermore, this paper proposes an approach to reduce the model's\nuncertainty for an input image, and it helps to defend the adversarial attacks\non the image super-resolution model. The proposed uncertainty reduction\ntechnique also improves the performance of the model for out-of-distribution\ntest images. To the best of our knowledge, we are the first to propose an\nadversarial defense mechanism in any image reconstruction domain.\n", "versions": [{"version": "v1", "created": "Fri, 22 Mar 2019 09:05:32 GMT"}, {"version": "v2", "created": "Sun, 28 Mar 2021 09:51:42 GMT"}, {"version": "v3", "created": "Wed, 19 May 2021 04:53:43 GMT"}], "update_date": "2021-05-20", "authors_parsed": [["Kar", "Aupendu", ""], ["Biswas", "Prabir Kumar", ""]]}, {"id": "1903.09467", "submitter": "Agisilaos Chartsias", "authors": "Agisilaos Chartsias, Thomas Joyce, Giorgos Papanastasiou, Michelle\n  Williams, David Newby, Rohan Dharmakumar, Sotirios A. Tsaftaris", "title": "Disentangled Representation Learning in Cardiac Image Analysis", "comments": null, "journal-ref": "Medical Image Analysis 58 (2019) 101535", "doi": "10.1016/j.media.2019.101535", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Typically, a medical image offers spatial information on the anatomy (and\npathology) modulated by imaging specific characteristics. Many imaging\nmodalities including Magnetic Resonance Imaging (MRI) and Computed Tomography\n(CT) can be interpreted in this way. We can venture further and consider that a\nmedical image naturally factors into some spatial factors depicting anatomy and\nfactors that denote the imaging characteristics. Here, we explicitly learn this\ndecomposed (disentangled) representation of imaging data, focusing in\nparticular on cardiac images. We propose Spatial Decomposition Network (SDNet),\nwhich factorises 2D medical images into spatial anatomical factors and\nnon-spatial modality factors. We demonstrate that this high-level\nrepresentation is ideally suited for several medical image analysis tasks, such\nas semi-supervised segmentation, multi-task segmentation and regression, and\nimage-to-image synthesis. Specifically, we show that our model can match the\nperformance of fully supervised segmentation models, using only a fraction of\nthe labelled images. Critically, we show that our factorised representation\nalso benefits from supervision obtained either when we use auxiliary tasks to\ntrain the model in a multi-task setting (e.g. regressing to known cardiac\nindices), or when aggregating multimodal data from different sources (e.g.\npooling together MRI and CT data). To explore the properties of the learned\nfactorisation, we perform latent-space arithmetic and show that we can\nsynthesise CT from MR and vice versa, by swapping the modality factors. We also\ndemonstrate that the factor holding image specific information can be used to\npredict the input modality with high accuracy. Code will be made available at\nhttps://github.com/agis85/anatomy_modality_decomposition.\n", "versions": [{"version": "v1", "created": "Fri, 22 Mar 2019 12:08:08 GMT"}, {"version": "v2", "created": "Tue, 9 Jul 2019 17:51:34 GMT"}, {"version": "v3", "created": "Sun, 4 Aug 2019 17:16:26 GMT"}, {"version": "v4", "created": "Mon, 16 Sep 2019 10:59:29 GMT"}], "update_date": "2020-04-21", "authors_parsed": [["Chartsias", "Agisilaos", ""], ["Joyce", "Thomas", ""], ["Papanastasiou", "Giorgos", ""], ["Williams", "Michelle", ""], ["Newby", "David", ""], ["Dharmakumar", "Rohan", ""], ["Tsaftaris", "Sotirios A.", ""]]}, {"id": "1903.09469", "submitter": "Clint Sebastian", "authors": "Raffaele Imbriaco, Clint Sebastian, Egor Bondarev, Peter H.N. de With", "title": "Aggregated Deep Local Features for Remote Sensing Image Retrieval", "comments": "Published in Remote Sensing. The first two authors have equal\n  contribution", "journal-ref": "Remote Sensing, 2019", "doi": "10.3390/rs11050493", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Remote Sensing Image Retrieval remains a challenging topic due to the special\nnature of Remote Sensing Imagery. Such images contain various different\nsemantic objects, which clearly complicates the retrieval task. In this paper,\nwe present an image retrieval pipeline that uses attentive, local convolutional\nfeatures and aggregates them using the Vector of Locally Aggregated Descriptors\n(VLAD) to produce a global descriptor. We study various system parameters such\nas the multiplicative and additive attention mechanisms and descriptor\ndimensionality. We propose a query expansion method that requires no external\ninputs. Experiments demonstrate that even without training, the local\nconvolutional features and global representation outperform other systems.\nAfter system tuning, we can achieve state-of-the-art or competitive results.\nFurthermore, we observe that our query expansion method increases overall\nsystem performance by about 3%, using only the top-three retrieved images.\nFinally, we show how dimensionality reduction produces compact descriptors with\nincreased retrieval performance and fast retrieval computation times, e.g. 50%\nfaster than the current systems.\n", "versions": [{"version": "v1", "created": "Fri, 22 Mar 2019 12:17:04 GMT"}], "update_date": "2019-03-25", "authors_parsed": [["Imbriaco", "Raffaele", ""], ["Sebastian", "Clint", ""], ["Bondarev", "Egor", ""], ["de With", "Peter H. N.", ""]]}, {"id": "1903.09555", "submitter": "Cristina Gonzalez-Gonzalo", "authors": "Cristina Gonz\\'alez-Gonzalo, Ver\\'onica S\\'anchez-Guti\\'errez, Paula\n  Hern\\'andez-Mart\\'inez, In\\'es Contreras, Yara T. Lechanteur, Artin Domanian,\n  Bram van Ginneken, Clara I. S\\'anchez", "title": "Evaluation of a deep learning system for the joint automated detection\n  of diabetic retinopathy and age-related macular degeneration", "comments": null, "journal-ref": "Acta Ophthalmologica. First published 26 November 2019", "doi": "10.1111/aos.14306", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Purpose: To validate the performance of a commercially-available,\nCE-certified deep learning (DL) system, RetCAD v.1.3.0 (Thirona, Nijmegen, The\nNetherlands), for the joint automatic detection of diabetic retinopathy (DR)\nand age-related macular degeneration (AMD) in color fundus (CF) images on a\ndataset with mixed presence of eye diseases.\n  Methods: Evaluation of joint detection of referable DR and AMD was performed\non a DR-AMD dataset with 600 images acquired during routine clinical practice,\ncontaining referable and non-referable cases of both diseases. Each image was\ngraded for DR and AMD by an experienced ophthalmologist to establish the\nreference standard (RS), and by four independent observers for comparison with\nhuman performance. Validation was furtherly assessed on Messidor (1200 images)\nfor individual identification of referable DR, and the Age-Related Eye Disease\nStudy (AREDS) dataset (133821 images) for referable AMD, against the\ncorresponding RS.\n  Results: Regarding joint validation on the DR-AMD dataset, the system\nachieved an area under the ROC curve (AUC) of 95.1% for detection of referable\nDR (SE=90.1%, SP=90.6%). For referable AMD, the AUC was 94.9% (SE=91.8%,\nSP=87.5%). Average human performance for DR was SE=61.5% and SP=97.8%; for AMD,\nSE=76.5% and SP=96.1%. Regarding detection of referable DR in Messidor, AUC was\n97.5% (SE=92.0%, SP=92.1%); for referable AMD in AREDS, AUC was 92.7%\n(SE=85.8%, SP=86.0%).\n  Conclusions: The validated system performs comparably to human experts at\nsimultaneous detection of DR and AMD. This shows that DL systems can facilitate\naccess to joint screening of eye diseases and become a quick and reliable\nsupport for ophthalmological experts.\n", "versions": [{"version": "v1", "created": "Fri, 22 Mar 2019 15:27:18 GMT"}], "update_date": "2020-10-29", "authors_parsed": [["Gonz\u00e1lez-Gonzalo", "Cristina", ""], ["S\u00e1nchez-Guti\u00e9rrez", "Ver\u00f3nica", ""], ["Hern\u00e1ndez-Mart\u00ednez", "Paula", ""], ["Contreras", "In\u00e9s", ""], ["Lechanteur", "Yara T.", ""], ["Domanian", "Artin", ""], ["van Ginneken", "Bram", ""], ["S\u00e1nchez", "Clara I.", ""]]}, {"id": "1903.09616", "submitter": "Xinshuo Weng", "authors": "Xinshuo Weng", "title": "On the Importance of Video Action Recognition for Visual Lipreading", "comments": "This paper is withdrawn by the author due to errors and there will be\n  no replacement in this thread", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We focus on the word-level visual lipreading, which requires to decode the\nword from the speaker's video. Recently, many state-of-the-art visual\nlipreading methods explore the end-to-end trainable deep models, involving the\nuse of 2D convolutional networks (e.g., ResNet) as the front-end visual feature\nextractor and the sequential model (e.g., Bi-LSTM or Bi-GRU) as the back-end.\nAlthough a deep 2D convolution neural network can provide informative\nimage-based features, it ignores the temporal motion existing between the\nadjacent frames. In this work, we investigate the spatial-temporal capacity\npower of I3D (Inflated 3D ConvNet) for visual lipreading. We demonstrate that,\nafter pre-trained on the large-scale video action recognition dataset (e.g.,\nKinetics), our models show a considerable improvement of performance on the\ntask of lipreading. A comparison between a set of video model architectures and\ninput data representation is also reported. Our extensive experiments on LRW\nshows that a two-stream I3D model with RGB video and optical flow as the inputs\nachieves the state-of-the-art performance.\n", "versions": [{"version": "v1", "created": "Fri, 22 Mar 2019 17:24:37 GMT"}, {"version": "v2", "created": "Mon, 16 Sep 2019 15:32:15 GMT"}], "update_date": "2019-09-17", "authors_parsed": [["Weng", "Xinshuo", ""]]}, {"id": "1903.09662", "submitter": "Zhen Zhao", "authors": "Zhen Zhao, Ashley Kleinhans, Gursharan Sandhu, Ishan Patel, K. P.\n  Unnikrishnan", "title": "Capsule Networks with Max-Min Normalization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Capsule Networks (CapsNet) use the Softmax function to convert the logits of\nthe routing coefficients into a set of normalized values that signify the\nassignment probabilities between capsules in adjacent layers. We show that the\nuse of Softmax prevents capsule layers from forming optimal couplings between\nlower and higher-level capsules. Softmax constrains the dynamic range of the\nrouting coefficients and leads to probabilities that remain mostly uniform\nafter several routing iterations. Instead, we propose the use of Max-Min\nnormalization. Max-Min performs a scale-invariant normalization of the logits\nthat allows each lower-level capsule to take on an independent value,\nconstrained only by the bounds of normalization. Max-Min provides consistent\nimprovement in test accuracy across five datasets and allows more routing\niterations without a decrease in network performance. A single CapsNet trained\nusing Max-Min achieves an improved test error of 0.20% on the MNIST dataset.\nWith a simple 3-model majority vote, we achieve a test error of 0.17% on MNIST.\n", "versions": [{"version": "v1", "created": "Fri, 22 Mar 2019 18:09:37 GMT"}], "update_date": "2019-03-26", "authors_parsed": [["Zhao", "Zhen", ""], ["Kleinhans", "Ashley", ""], ["Sandhu", "Gursharan", ""], ["Patel", "Ishan", ""], ["Unnikrishnan", "K. P.", ""]]}, {"id": "1903.09730", "submitter": "Shounak Datta", "authors": "Sankha Subhra Mullick, Shounak Datta, Swagatam Das", "title": "Generative Adversarial Minority Oversampling", "comments": "Codes are available at https://github.com/SankhaSubhra/GAMO", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Class imbalance is a long-standing problem relevant to a number of real-world\napplications of deep learning. Oversampling techniques, which are effective for\nhandling class imbalance in classical learning systems, can not be directly\napplied to end-to-end deep learning systems. We propose a three-player\nadversarial game between a convex generator, a multi-class classifier network,\nand a real/fake discriminator to perform oversampling in deep learning systems.\nThe convex generator generates new samples from the minority classes as convex\ncombinations of existing instances, aiming to fool both the discriminator as\nwell as the classifier into misclassifying the generated samples. Consequently,\nthe artificial samples are generated at critical locations near the peripheries\nof the classes. This, in turn, adjusts the classifier induced boundaries in a\nway which is more likely to reduce misclassification from the minority classes.\nExtensive experiments on multiple class imbalanced image datasets establish the\nefficacy of our proposal.\n", "versions": [{"version": "v1", "created": "Fri, 22 Mar 2019 23:13:14 GMT"}, {"version": "v2", "created": "Wed, 3 Apr 2019 03:13:19 GMT"}, {"version": "v3", "created": "Wed, 26 Aug 2020 18:05:57 GMT"}], "update_date": "2020-08-28", "authors_parsed": [["Mullick", "Sankha Subhra", ""], ["Datta", "Shounak", ""], ["Das", "Swagatam", ""]]}, {"id": "1903.09745", "submitter": "Bowen Lin", "authors": "Fengling Wang, Bowen Lin, Shujun Fu, Shiling Xie, Zhigang Zhao,\n  Yuliang Li", "title": "Fast LLMMSE filter for low-dose CT imaging", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Low-dose X-ray CT technology is one of important directions of current\nresearch and development of medical imaging equipment. A fast algorithm of\nblockwise sinogram filtering is presented for realtime low-dose CT imaging. A\nnonstationary Gaussian noise model of low-dose sinogram data is proposed in the\nlow-mA (tube current) CT protocol. Then, according to the linear minimum mean\nsquare error principle, an adaptive blockwise algorithm is built to filter\ncontaminated sinogram data caused by photon starvation. A moving sum technique\nis used to speed the algorithm into a linear time one, regardless of the block\nsize and thedata range. The proposedfast filtering givesa better performance in\nnoise reduction and detail preservation in the reconstructed images,which is\nverified in experiments on simulated and real data compared with some related\nfiltering methods.\n", "versions": [{"version": "v1", "created": "Sat, 23 Mar 2019 02:17:10 GMT"}], "update_date": "2019-03-26", "authors_parsed": [["Wang", "Fengling", ""], ["Lin", "Bowen", ""], ["Fu", "Shujun", ""], ["Xie", "Shiling", ""], ["Zhao", "Zhigang", ""], ["Li", "Yuliang", ""]]}, {"id": "1903.09746", "submitter": "Xiaoyu Chen", "authors": "Xiaoyu Chen, Xiaotian Lou, Lianfa Bai, Jing Han", "title": "Residual Pyramid Learning for Single-Shot Semantic Segmentation", "comments": "10 pages, 6 figures, 6 tables", "journal-ref": null, "doi": "10.1109/TITS.2019.2922252", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pixel-level semantic segmentation is a challenging task with a huge amount of\ncomputation, especially if the size of input is large. In the segmentation\nmodel, apart from the feature extraction, the extra decoder structure is often\nemployed to recover spatial information. In this paper, we put forward a method\nfor single-shot segmentation in a feature residual pyramid network (RPNet),\nwhich learns the main and residuals of segmentation by decomposing the label at\ndifferent levels of residual blocks. Specifically speaking, we use the residual\nfeatures to learn the edges and details, and the identity features to learn the\nmain part of targets. At testing time, the predicted residuals are used to\nenhance the details of the top-level prediction. Residual learning blocks split\nthe network into several shallow sub-networks which facilitates the training of\nthe RPNet. We then evaluate the proposed method and compare it with recent\nstate-of-the-art methods on CamVid and Cityscapes. The proposed single-shot\nsegmentation based on RPNet achieves impressive results with high efficiency on\npixel-level segmentation.\n", "versions": [{"version": "v1", "created": "Sat, 23 Mar 2019 02:48:07 GMT"}], "update_date": "2019-06-19", "authors_parsed": [["Chen", "Xiaoyu", ""], ["Lou", "Xiaotian", ""], ["Bai", "Lianfa", ""], ["Han", "Jing", ""]]}, {"id": "1903.09755", "submitter": "Ricardo Fabbri", "authors": "Ricardo Fabbri, Timothy Duff, Hongyi Fan, Margaret Regan, David da\n  Costa de Pinho, Elias Tsigaridas, Charles Wampler, Jonathan Hauenstein,\n  Benjamin Kimia, Anton Leykin, Tomas Pajdla", "title": "Trifocal Relative Pose from Lines at Points and its Efficient Solution", "comments": "This material is based upon work supported by the National Science\n  Foundation under Grant No. DMS-1439786 while most authors were in residence\n  at Brown University's Institute for Computational and Experimental Research\n  in Mathematics -- ICERM, in Providence, RI", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present a new minimal problem for relative pose estimation mixing point\nfeatures with lines incident at points observed in three views and its\nefficient homotopy continuation solver. We demonstrate the generality of the\napproach by analyzing and solving an additional problem with mixed point and\nline correspondences in three views. The minimal problems include\ncorrespondences of (i) three points and one line and (ii) three points and two\nlines through two of the points which is reported and analyzed here for the\nfirst time. These are difficult to solve, as they have 216 and - as shown here\n- 312 solutions, but cover important practical situations when line and point\nfeatures appear together, e.g., in urban scenes or when observing curves. We\ndemonstrate that even such difficult problems can be solved robustly using a\nsuitable homotopy continuation technique and we provide an implementation\noptimized for minimal problems that can be integrated into engineering\napplications. Our simulated and real experiments demonstrate our solvers in the\ncamera geometry computation task in structure from motion. We show that new\nsolvers allow for reconstructing challenging scenes where the standard two-view\ninitialization of structure from motion fails.\n", "versions": [{"version": "v1", "created": "Sat, 23 Mar 2019 04:26:57 GMT"}, {"version": "v2", "created": "Thu, 28 Mar 2019 19:53:50 GMT"}, {"version": "v3", "created": "Tue, 16 Apr 2019 03:46:48 GMT"}], "update_date": "2019-04-17", "authors_parsed": [["Fabbri", "Ricardo", ""], ["Duff", "Timothy", ""], ["Fan", "Hongyi", ""], ["Regan", "Margaret", ""], ["de Pinho", "David da Costa", ""], ["Tsigaridas", "Elias", ""], ["Wampler", "Charles", ""], ["Hauenstein", "Jonathan", ""], ["Kimia", "Benjamin", ""], ["Leykin", "Anton", ""], ["Pajdla", "Tomas", ""]]}, {"id": "1903.09760", "submitter": "Jaejun Yoo", "authors": "Jaejun Yoo, Youngjung Uh, Sanghyuk Chun, Byeongkyu Kang, Jung-Woo Ha", "title": "Photorealistic Style Transfer via Wavelet Transforms", "comments": "Accepted to ICCV 2019, Code and data: https://github.com/ClovaAI/WCT2", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent style transfer models have provided promising artistic results.\nHowever, given a photograph as a reference style, existing methods are limited\nby spatial distortions or unrealistic artifacts, which should not happen in\nreal photographs. We introduce a theoretically sound correction to the network\narchitecture that remarkably enhances photorealism and faithfully transfers the\nstyle. The key ingredient of our method is wavelet transforms that naturally\nfits in deep networks. We propose a wavelet corrected transfer based on\nwhitening and coloring transforms (WCT$^2$) that allows features to preserve\ntheir structural information and statistical properties of VGG feature space\nduring stylization. This is the first and the only end-to-end model that can\nstylize a $1024\\times1024$ resolution image in 4.7 seconds, giving a pleasing\nand photorealistic quality without any post-processing. Last but not least, our\nmodel provides a stable video stylization without temporal constraints. Our\ncode, generated images, and pre-trained models are all available at\nhttps://github.com/ClovaAI/WCT2.\n", "versions": [{"version": "v1", "created": "Sat, 23 Mar 2019 05:02:00 GMT"}, {"version": "v2", "created": "Sun, 29 Sep 2019 08:49:37 GMT"}], "update_date": "2019-10-01", "authors_parsed": [["Yoo", "Jaejun", ""], ["Uh", "Youngjung", ""], ["Chun", "Sanghyuk", ""], ["Kang", "Byeongkyu", ""], ["Ha", "Jung-Woo", ""]]}, {"id": "1903.09761", "submitter": "Anh Nguyen", "authors": "Anh Nguyen", "title": "Scene Understanding for Autonomous Manipulation with Deep Learning", "comments": "PhD Thesis, 88 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Over the past few years, deep learning techniques have achieved tremendous\nsuccess in many visual understanding tasks such as object detection, image\nsegmentation, and caption generation. Despite this thriving in computer vision\nand natural language processing, deep learning has not yet shown significant\nimpact in robotics. Due to the gap between theory and application, there are\nmany challenges when applying the results of deep learning to the real robotic\nsystems. In this study, our long-term goal is to bridge the gap between\ncomputer vision and robotics by developing visual methods that can be used in\nreal robots. In particular, this work tackles two fundamental visual problems\nfor autonomous robotic manipulation: affordance detection and fine-grained\naction understanding. Theoretically, we propose different deep architectures to\nfurther improves the state of the art in each problem. Empirically, we show\nthat the outcomes of our proposed methods can be applied in real robots and\nallow them to perform useful manipulation tasks.\n", "versions": [{"version": "v1", "created": "Sat, 23 Mar 2019 05:05:22 GMT"}], "update_date": "2019-03-26", "authors_parsed": [["Nguyen", "Anh", ""]]}, {"id": "1903.09766", "submitter": "Md Jahidul Islam", "authors": "Md Jahidul Islam, Youya Xia and Junaed Sattar", "title": "Fast Underwater Image Enhancement for Improved Visual Perception", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a conditional generative adversarial network-based\nmodel for real-time underwater image enhancement. To supervise the adversarial\ntraining, we formulate an objective function that evaluates the perceptual\nimage quality based on its global content, color, local texture, and style\ninformation. We also present EUVP, a large-scale dataset of a paired and\nunpaired collection of underwater images (of `poor' and `good' quality) that\nare captured using seven different cameras over various visibility conditions\nduring oceanic explorations and human-robot collaborative experiments. In\naddition, we perform several qualitative and quantitative evaluations which\nsuggest that the proposed model can learn to enhance underwater image quality\nfrom both paired and unpaired training. More importantly, the enhanced images\nprovide improved performances of standard models for underwater object\ndetection, human pose estimation, and saliency prediction. These results\nvalidate that it is suitable for real-time preprocessing in the autonomy\npipeline by visually-guided underwater robots. The model and associated\ntraining pipelines are available at https://github.com/xahidbuffon/funie-gan.\n", "versions": [{"version": "v1", "created": "Sat, 23 Mar 2019 05:21:05 GMT"}, {"version": "v2", "created": "Wed, 18 Dec 2019 23:40:48 GMT"}, {"version": "v3", "created": "Sun, 9 Feb 2020 02:06:40 GMT"}], "update_date": "2020-02-11", "authors_parsed": [["Islam", "Md Jahidul", ""], ["Xia", "Youya", ""], ["Sattar", "Junaed", ""]]}, {"id": "1903.09769", "submitter": "Tianyun Zhang", "authors": "Shaokai Ye, Xiaoyu Feng, Tianyun Zhang, Xiaolong Ma, Sheng Lin,\n  Zhengang Li, Kaidi Xu, Wujie Wen, Sijia Liu, Jian Tang, Makan Fardad, Xue\n  Lin, Yongpan Liu and Yanzhi Wang", "title": "Progressive DNN Compression: A Key to Achieve Ultra-High Weight Pruning\n  and Quantization Rates using ADMM", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Weight pruning and weight quantization are two important categories of DNN\nmodel compression. Prior work on these techniques are mainly based on\nheuristics. A recent work developed a systematic frame-work of DNN weight\npruning using the advanced optimization technique ADMM (Alternating Direction\nMethods of Multipliers), achieving one of state-of-art in weight pruning\nresults. In this work, we first extend such one-shot ADMM-based framework to\nguarantee solution feasibility and provide fast convergence rate, and\ngeneralize to weight quantization as well. We have further developed a\nmulti-step, progressive DNN weight pruning and quantization framework, with\ndual benefits of (i) achieving further weight pruning/quantization thanks to\nthe special property of ADMM regularization, and (ii) reducing the search space\nwithin each step. Extensive experimental results demonstrate the superior\nperformance compared with prior work. Some highlights: (i) we achieve 246x,36x,\nand 8x weight pruning on LeNet-5, AlexNet, and ResNet-50 models, respectively,\nwith (almost) zero accuracy loss; (ii) even a significant 61x weight pruning in\nAlexNet (ImageNet) results in only minor degradation in actual accuracy\ncompared with prior work; (iii) we are among the first to derive notable weight\npruning results for ResNet and MobileNet models; (iv) we derive the first\nlossless, fully binarized (for all layers) LeNet-5 for MNIST and VGG-16 for\nCIFAR-10; and (v) we derive the first fully binarized (for all layers) ResNet\nfor ImageNet with reasonable accuracy loss.\n", "versions": [{"version": "v1", "created": "Sat, 23 Mar 2019 05:54:26 GMT"}, {"version": "v2", "created": "Sat, 30 Mar 2019 03:27:38 GMT"}], "update_date": "2019-04-02", "authors_parsed": [["Ye", "Shaokai", ""], ["Feng", "Xiaoyu", ""], ["Zhang", "Tianyun", ""], ["Ma", "Xiaolong", ""], ["Lin", "Sheng", ""], ["Li", "Zhengang", ""], ["Xu", "Kaidi", ""], ["Wen", "Wujie", ""], ["Liu", "Sijia", ""], ["Tang", "Jian", ""], ["Fardad", "Makan", ""], ["Lin", "Xue", ""], ["Liu", "Yongpan", ""], ["Wang", "Yanzhi", ""]]}, {"id": "1903.09776", "submitter": "Yu Wu", "authors": "Ruijie Quan, Xuanyi Dong, Yu Wu, Linchao Zhu, Yi Yang", "title": "Auto-ReID: Searching for a Part-aware ConvNet for Person\n  Re-Identification", "comments": "Accepted to ICCV 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Prevailing deep convolutional neural networks (CNNs) for person\nre-IDentification (reID) are usually built upon ResNet or VGG backbones, which\nwere originally designed for classification. Because reID is different from\nclassification, the architecture should be modified accordingly. We propose to\nautomatically search for a CNN architecture that is specifically suitable for\nthe reID task. There are three aspects to be tackled. First, body structural\ninformation plays an important role in reID but it is not encoded in backbones.\nSecond, Neural Architecture Search (NAS) automates the process of architecture\ndesign without human effort, but no existing NAS methods incorporate the\nstructure information of input images. Third, reID is essentially a retrieval\ntask but current NAS algorithms are merely designed for classification. To\nsolve these problems, we propose a retrieval-based search algorithm over a\nspecifically designed reID search space, named Auto-ReID. Our Auto-ReID enables\nthe automated approach to find an efficient and effective CNN architecture for\nreID. Extensive experiments demonstrate that the searched architecture achieves\nstate-of-the-art performance while reducing 50% parameters and 53% FLOPs\ncompared to others.\n", "versions": [{"version": "v1", "created": "Sat, 23 Mar 2019 07:26:50 GMT"}, {"version": "v2", "created": "Tue, 26 Mar 2019 06:33:23 GMT"}, {"version": "v3", "created": "Sun, 21 Apr 2019 13:54:48 GMT"}, {"version": "v4", "created": "Tue, 20 Aug 2019 08:20:43 GMT"}], "update_date": "2019-08-21", "authors_parsed": [["Quan", "Ruijie", ""], ["Dong", "Xuanyi", ""], ["Wu", "Yu", ""], ["Zhu", "Linchao", ""], ["Yang", "Yi", ""]]}, {"id": "1903.09781", "submitter": "Jan Philip Klopp", "authors": "Keng-Chi Liu, Yi-Ting Shen, Jan P. Klopp, Liang-Gee Chen", "title": "What Synthesis is Missing: Depth Adaptation Integrated with Weak\n  Supervision for Indoor Scene Parsing", "comments": "10 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scene Parsing is a crucial step to enable autonomous systems to understand\nand interact with their surroundings. Supervised deep learning methods have\nmade great progress in solving scene parsing problems, however, come at the\ncost of laborious manual pixel-level annotation. To alleviate this effort\nsynthetic data as well as weak supervision have both been investigated.\nNonetheless, synthetically generated data still suffers from severe domain\nshift while weak labels are often imprecise. Moreover, most existing works for\nweakly supervised scene parsing are limited to salient foreground objects. The\naim of this work is hence twofold: Exploit synthetic data where feasible and\nintegrate weak supervision where necessary. More concretely, we address this\ngoal by utilizing depth as transfer domain because its synthetic-to-real\ndiscrepancy is much lower than for color. At the same time, we perform weak\nlocalization from easily obtainable image level labels and integrate both using\na novel contour-based scheme. Our approach is implemented as a teacher-student\nlearning framework to solve the transfer learning problem by generating a\npseudo ground truth. Using only depth-based adaptation, this approach already\noutperforms previous transfer learning approaches on the popular indoor scene\nparsing SUN RGB-D dataset. Our proposed two-stage integration more than halves\nthe gap towards fully supervised methods when compared to previous\nstate-of-the-art in transfer learning.\n", "versions": [{"version": "v1", "created": "Sat, 23 Mar 2019 08:26:34 GMT"}], "update_date": "2019-03-26", "authors_parsed": [["Liu", "Keng-Chi", ""], ["Shen", "Yi-Ting", ""], ["Klopp", "Jan P.", ""], ["Chen", "Liang-Gee", ""]]}, {"id": "1903.09784", "submitter": "Arushi Goel", "authors": "Arushi Goel, Keng Teck Ma and Cheston Tan", "title": "An End-to-End Network for Generating Social Relationship Graphs", "comments": null, "journal-ref": "CVPR 2019", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Socially-intelligent agents are of growing interest in artificial\nintelligence. To this end, we need systems that can understand social\nrelationships in diverse social contexts. Inferring the social context in a\ngiven visual scene not only involves recognizing objects, but also demands a\nmore in-depth understanding of the relationships and attributes of the people\ninvolved. To achieve this, one computational approach for representing human\nrelationships and attributes is to use an explicit knowledge graph, which\nallows for high-level reasoning. We introduce a novel end-to-end-trainable\nneural network that is capable of generating a Social Relationship Graph - a\nstructured, unified representation of social relationships and attributes -\nfrom a given input image. Our Social Relationship Graph Generation Network\n(SRG-GN) is the first to use memory cells like Gated Recurrent Units (GRUs) to\niteratively update the social relationship states in a graph using scene and\nattribute context. The neural network exploits the recurrent connections among\nthe GRUs to implement message passing between nodes and edges in the graph, and\nresults in significant improvement over previous methods for social\nrelationship recognition.\n", "versions": [{"version": "v1", "created": "Sat, 23 Mar 2019 08:36:46 GMT"}], "update_date": "2019-03-26", "authors_parsed": [["Goel", "Arushi", ""], ["Ma", "Keng Teck", ""], ["Tan", "Cheston", ""]]}, {"id": "1903.09798", "submitter": "Daiki Kimura", "authors": "Daiki Kimura, Minori Narita, Asim Munawar, Ryuki Tachibana", "title": "Spatially-weighted Anomaly Detection with Regression Model", "comments": "4 pages, published as an oral presentation paper at Meeting on Image\n  Recognition and Understanding (MIRU) 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual anomaly detection is common in several applications including medical\nscreening and production quality check. Although a definition of the anomaly is\nan unknown trend in data, in many cases some hints or samples of the anomaly\nclass can be given in advance. Conventional methods cannot use the available\nanomaly data, and also do not have a robustness of noise. In this paper, we\npropose a novel spatially-weighted reconstruction-loss-based anomaly detection\nwith a likelihood value from a regression model trained by all known data. The\nspatial weights are calculated by a region of interest generated from employing\nvisualization of the regression model. We introduce some ways to combine with\nvarious strategies to propose a state-of-the-art method. Comparing with other\nmethods on three different datasets, we empirically verify the proposed method\nperforms better than the others.\n", "versions": [{"version": "v1", "created": "Sat, 23 Mar 2019 11:07:37 GMT"}, {"version": "v2", "created": "Thu, 28 Mar 2019 08:59:17 GMT"}], "update_date": "2019-03-29", "authors_parsed": [["Kimura", "Daiki", ""], ["Narita", "Minori", ""], ["Munawar", "Asim", ""], ["Tachibana", "Ryuki", ""]]}, {"id": "1903.09799", "submitter": "Hao-Yun Chen", "authors": "Hao-Yun Chen, Jhao-Hong Liang, Shih-Chieh Chang, Jia-Yu Pan, Yu-Ting\n  Chen, Wei Wei, Da-Cheng Juan", "title": "Improving Adversarial Robustness via Guided Complement Entropy", "comments": "ICCV'19 Camera Ready", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.PF stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adversarial robustness has emerged as an important topic in deep learning as\ncarefully crafted attack samples can significantly disturb the performance of a\nmodel. Many recent methods have proposed to improve adversarial robustness by\nutilizing adversarial training or model distillation, which adds additional\nprocedures to model training. In this paper, we propose a new training paradigm\ncalled Guided Complement Entropy (GCE) that is capable of achieving\n\"adversarial defense for free,\" which involves no additional procedures in the\nprocess of improving adversarial robustness. In addition to maximizing model\nprobabilities on the ground-truth class like cross-entropy, we neutralize its\nprobabilities on the incorrect classes along with a \"guided\" term to balance\nbetween these two terms. We show in the experiments that our method achieves\nbetter model robustness with even better performance compared to the commonly\nused cross-entropy training objective. We also show that our method can be used\northogonal to adversarial training across well-known methods with noticeable\nrobustness gain. To the best of our knowledge, our approach is the first one\nthat improves model robustness without compromising performance.\n", "versions": [{"version": "v1", "created": "Sat, 23 Mar 2019 11:14:59 GMT"}, {"version": "v2", "created": "Tue, 30 Jul 2019 04:07:55 GMT"}, {"version": "v3", "created": "Wed, 7 Aug 2019 06:11:33 GMT"}], "update_date": "2019-08-08", "authors_parsed": [["Chen", "Hao-Yun", ""], ["Liang", "Jhao-Hong", ""], ["Chang", "Shih-Chieh", ""], ["Pan", "Jia-Yu", ""], ["Chen", "Yu-Ting", ""], ["Wei", "Wei", ""], ["Juan", "Da-Cheng", ""]]}, {"id": "1903.09807", "submitter": "Hyungjun Kim", "authors": "Hyungjun Kim, Yulhwa Kim, Sungju Ryu, and Jae-Joon Kim", "title": "BitSplit-Net: Multi-bit Deep Neural Network with Bitwise Activation\n  Function", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Significant computational cost and memory requirements for deep neural\nnetworks (DNNs) make it difficult to utilize DNNs in resource-constrained\nenvironments. Binary neural network (BNN), which uses binary weights and binary\nactivations, has been gaining interests for its hardware-friendly\ncharacteristics and minimal resource requirement. However, BNN usually suffers\nfrom accuracy degradation. In this paper, we introduce \"BitSplit-Net\", a neural\nnetwork which maintains the hardware-friendly characteristics of BNN while\nimproving accuracy by using multi-bit precision. In BitSplit-Net, each bit of\nmulti-bit activations propagates independently throughout the network before\nbeing merged at the end of the network. Thus, each bit path of the BitSplit-Net\nresembles BNN and hardware friendly features of BNN, such as bitwise binary\nactivation function, are preserved in our scheme. We demonstrate that the\nBitSplit version of LeNet-5, VGG-9, AlexNet, and ResNet-18 can be trained to\nhave similar classification accuracy at a lower computational cost compared to\nconventional multi-bit networks with low bit precision (<= 4-bit). We further\nevaluate BitSplit-Net on GPU with custom CUDA kernel, showing that BitSplit-Net\ncan achieve better hardware performance in comparison to conventional multi-bit\nnetworks.\n", "versions": [{"version": "v1", "created": "Sat, 23 Mar 2019 11:52:23 GMT"}], "update_date": "2019-03-26", "authors_parsed": [["Kim", "Hyungjun", ""], ["Kim", "Yulhwa", ""], ["Ryu", "Sungju", ""], ["Kim", "Jae-Joon", ""]]}, {"id": "1903.09809", "submitter": "Max-Heinrich Laves", "authors": "Max-Heinrich Laves, Sontje Ihler, L\\\"uder Alexander Kahrs, Tobias\n  Ortmaier", "title": "Semantic denoising autoencoders for retinal optical coherence tomography", "comments": "Accepted for publication at the SPIE/OSA European Conferences on\n  Biomedical Optics (ECBO) 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Noise in speckle-prone optical coherence tomography tends to obfuscate\nimportant details necessary for medical diagnosis. In this paper, a denoising\napproach that preserves disease characteristics on retinal optical coherence\ntomography images in ophthalmology is presented. By combining a deep\nconvolutional autoencoder with a priorly trained ResNet image classifier as\nregularizer, the perceptibility of delicate details is encouraged and only\ninformation-less background noise is filtered out. With our approach, higher\npeak signal-to-noise ratios with $ \\mathrm{PSNR} = 31.2\\,\\mathrm{dB} $ and\nhigher classification accuracy of $\\mathrm{ACC} = 85.0\\,\\%$ can be achieved for\ndenoised images compared to state-of-the-art denoising with $ \\mathrm{PSNR} =\n29.4\\,\\mathrm{dB} $ or $\\mathrm{ACC} = 70.3\\,\\%$, depending on the method. It\nis shown that regularized autoencoders are capable of denoising retinal OCT\nimages without blurring details of diseases.\n", "versions": [{"version": "v1", "created": "Sat, 23 Mar 2019 12:01:03 GMT"}], "update_date": "2019-03-26", "authors_parsed": [["Laves", "Max-Heinrich", ""], ["Ihler", "Sontje", ""], ["Kahrs", "L\u00fcder Alexander", ""], ["Ortmaier", "Tobias", ""]]}, {"id": "1903.09814", "submitter": "Zhen Li", "authors": "Zhen Li, Jinglei Yang, Zheng Liu, Xiaomin Yang, Gwanggil Jeon, Wei Wu", "title": "Feedback Network for Image Super-Resolution", "comments": "Accepted to CVPR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in image super-resolution (SR) explored the power of deep\nlearning to achieve a better reconstruction performance. However, the feedback\nmechanism, which commonly exists in human visual system, has not been fully\nexploited in existing deep learning based image SR methods. In this paper, we\npropose an image super-resolution feedback network (SRFBN) to refine low-level\nrepresentations with high-level information. Specifically, we use hidden states\nin an RNN with constraints to achieve such feedback manner. A feedback block is\ndesigned to handle the feedback connections and to generate powerful high-level\nrepresentations. The proposed SRFBN comes with a strong early reconstruction\nability and can create the final high-resolution image step by step. In\naddition, we introduce a curriculum learning strategy to make the network well\nsuitable for more complicated tasks, where the low-resolution images are\ncorrupted by multiple types of degradation. Extensive experimental results\ndemonstrate the superiority of the proposed SRFBN in comparison with the\nstate-of-the-art methods. Code is avaliable at\nhttps://github.com/Paper99/SRFBN_CVPR19.\n", "versions": [{"version": "v1", "created": "Sat, 23 Mar 2019 12:29:18 GMT"}, {"version": "v2", "created": "Fri, 28 Jun 2019 06:16:57 GMT"}], "update_date": "2019-07-01", "authors_parsed": [["Li", "Zhen", ""], ["Yang", "Jinglei", ""], ["Liu", "Zheng", ""], ["Yang", "Xiaomin", ""], ["Jeon", "Gwanggil", ""], ["Wu", "Wei", ""]]}, {"id": "1903.09823", "submitter": "Omar Elgendy", "authors": "Omar A. Elgendy and Stanley H. Chan", "title": "Color Filter Arrays for Quanta Image Sensors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quanta image sensor (QIS) is envisioned to be the next generation image\nsensor after CCD and CMOS. In this paper, we discuss how to design color filter\narrays for QIS and other small pixels. Designing color filter arrays for small\npixels is challenging because maximizing the light efficiency while suppressing\naliasing and crosstalk are conflicting tasks. We present an optimization-based\nframework which unifies several mainstream color filter array design\nmethodologies. Our method offers greater generality and flexibility. Compared\nto existing methods, the new framework can simultaneously handle luminance\nsensitivity, chrominance sensitivity, cross-talk, anti-aliasing,\nmanufacturability and orthogonality. Extensive experimental comparisons\ndemonstrate the effectiveness of the framework.\n", "versions": [{"version": "v1", "created": "Sat, 23 Mar 2019 13:35:10 GMT"}, {"version": "v2", "created": "Tue, 26 Mar 2019 14:26:09 GMT"}, {"version": "v3", "created": "Tue, 18 Jun 2019 19:03:45 GMT"}, {"version": "v4", "created": "Fri, 20 Dec 2019 06:14:06 GMT"}], "update_date": "2019-12-23", "authors_parsed": [["Elgendy", "Omar A.", ""], ["Chan", "Stanley H.", ""]]}, {"id": "1903.09834", "submitter": "Xian Wei", "authors": "Haitao Zhang, Lingguo Meng, Xian Wei, Xiaoliang Tang, Xuan Tang,\n  Xingping Wang, Bo Jin and Wei Yao", "title": "1D-Convolutional Capsule Network for Hyperspectral Image Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, convolutional neural networks (CNNs) have achieved excellent\nperformances in many computer vision tasks. Specifically, for hyperspectral\nimages (HSIs) classification, CNNs often require very complex structure due to\nthe high dimension of HSIs. The complex structure of CNNs results in\nprohibitive training efforts. Moreover, the common situation in HSIs\nclassification task is the lack of labeled samples, which results in accuracy\ndeterioration of CNNs. In this work, we develop an easy-to-implement capsule\nnetwork to alleviate the aforementioned problems, i.e., 1D-convolution capsule\nnetwork (1D-ConvCapsNet). Firstly, 1D-ConvCapsNet separately extracts spatial\nand spectral information on spatial and spectral domains, which is more\nlightweight than 3D-convolution due to fewer parameters. Secondly,\n1D-ConvCapsNet utilizes the capsule-wise constraint window method to reduce\nparameter amount and computational complexity of conventional capsule network.\nFinally, 1D-ConvCapsNet obtains accurate predictions with respect to input\nsamples via dynamic routing. The effectiveness of the 1D-ConvCapsNet is\nverified by three representative HSI datasets. Experimental results demonstrate\nthat 1D-ConvCapsNet is superior to state-of-the-art methods in both the\naccuracy and training effort.\n", "versions": [{"version": "v1", "created": "Sat, 23 Mar 2019 15:22:49 GMT"}], "update_date": "2019-03-26", "authors_parsed": [["Zhang", "Haitao", ""], ["Meng", "Lingguo", ""], ["Wei", "Xian", ""], ["Tang", "Xiaoliang", ""], ["Tang", "Xuan", ""], ["Wang", "Xingping", ""], ["Jin", "Bo", ""], ["Yao", "Wei", ""]]}, {"id": "1903.09837", "submitter": "Yingbin Zheng", "authors": "Zhao Zhou, Hao Ye, Luhui Chen, Yingbin Zheng", "title": "Detecting Curve Text with Local Segmentation Network and Curve\n  Connection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Curve text or arbitrary shape text is very common in real-world scenarios. In\nthis paper, we propose a novel framework with the local segmentation network\n(LSN) followed by the curve connection to detect text in horizontal, oriented\nand curved forms. The LSN is composed of two elements, i.e., proposal\ngeneration to get the horizontal rectangle proposals with high overlap with\ntext and text segmentation to find the arbitrary shape text region within\nproposals. The curve connection is then designed to connect the local mask to\nthe detection results. We conduct experiments using the proposed framework on\ntwo real-world curve text detection datasets and demonstrate the effectiveness\nover previous approaches.\n", "versions": [{"version": "v1", "created": "Sat, 23 Mar 2019 15:46:34 GMT"}, {"version": "v2", "created": "Mon, 22 Mar 2021 06:42:11 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Zhou", "Zhao", ""], ["Ye", "Hao", ""], ["Chen", "Luhui", ""], ["Zheng", "Yingbin", ""]]}, {"id": "1903.09839", "submitter": "Zhixin Zhang", "authors": "Zhixin Zhang, Xudong Chen, Jie Liu, Kaibo Zhou", "title": "Rotated Feature Network for multi-orientation object detection", "comments": "9 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  General detectors follow the pipeline that feature maps extracted from\nConvNets are shared between classification and regression tasks. However, there\nexists obvious conflicting requirements in multi-orientation object detection\nthat classification is insensitive to orientations, while regression is quite\nsensitive. To address this issue, we provide an Encoder-Decoder architecture,\ncalled Rotated Feature Network (RFN), which produces rotation-sensitive feature\nmaps (RS) for regression and rotation-invariant feature maps (RI) for\nclassification. Specifically, the Encoder unit assigns weights for rotated\nfeature maps. The Decoder unit extracts RS and RI by performing resuming\noperator on rotated and reweighed feature maps, respectively. To make the\nrotation-invariant characteristics more reliable, we adopt a metric to\nquantitatively evaluate the rotation-invariance by adding a constrain item in\nthe loss, yielding a promising detection performance. Compared with the\nstate-of-the-art methods, our method can achieve significant improvement on\nNWPU VHR-10 and RSOD datasets. We further evaluate the RFN on the scene\nclassification in remote sensing images and object detection in natural images,\ndemonstrating its good generalization ability. The proposed RFN can be\nintegrated into an existing framework, leading to great performance with only a\nslight increase in model complexity.\n", "versions": [{"version": "v1", "created": "Sat, 23 Mar 2019 16:10:11 GMT"}, {"version": "v2", "created": "Wed, 27 Mar 2019 03:06:33 GMT"}], "update_date": "2019-03-28", "authors_parsed": [["Zhang", "Zhixin", ""], ["Chen", "Xudong", ""], ["Liu", "Jie", ""], ["Zhou", "Kaibo", ""]]}, {"id": "1903.09847", "submitter": "Xinshuo Weng", "authors": "Xinshuo Weng and Kris Kitani", "title": "Monocular 3D Object Detection with Pseudo-LiDAR Point Cloud", "comments": "Camera Ready for ICCV Workshop on \"Road Scene Understanding and\n  Autonomous Driving\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Monocular 3D scene understanding tasks, such as object size estimation,\nheading angle estimation and 3D localization, is challenging. Successful modern\nday methods for 3D scene understanding require the use of a 3D sensor. On the\nother hand, single image based methods have significantly worse performance. In\nthis work, we aim at bridging the performance gap between 3D sensing and 2D\nsensing for 3D object detection by enhancing LiDAR-based algorithms to work\nwith single image input. Specifically, we perform monocular depth estimation\nand lift the input image to a point cloud representation, which we call\npseudo-LiDAR point cloud. Then we can train a LiDAR-based 3D detection network\nwith our pseudo-LiDAR end-to-end. Following the pipeline of two-stage 3D\ndetection algorithms, we detect 2D object proposals in the input image and\nextract a point cloud frustum from the pseudo-LiDAR for each proposal. Then an\noriented 3D bounding box is detected for each frustum. To handle the large\namount of noise in the pseudo-LiDAR, we propose two innovations: (1) use a\n2D-3D bounding box consistency constraint, adjusting the predicted 3D bounding\nbox to have a high overlap with its corresponding 2D proposal after projecting\nonto the image; (2) use the instance mask instead of the bounding box as the\nrepresentation of 2D proposals, in order to reduce the number of points not\nbelonging to the object in the point cloud frustum. Through our evaluation on\nthe KITTI benchmark, we achieve the top-ranked performance on both bird's eye\nview and 3D object detection among all monocular methods, effectively\nquadrupling the performance over previous state-of-the-art. Our code is\navailable at https://github.com/xinshuoweng/Mono3D_PLiDAR.\n", "versions": [{"version": "v1", "created": "Sat, 23 Mar 2019 17:23:44 GMT"}, {"version": "v2", "created": "Sun, 31 Mar 2019 02:12:18 GMT"}, {"version": "v3", "created": "Sat, 11 May 2019 15:40:13 GMT"}, {"version": "v4", "created": "Sat, 31 Aug 2019 00:47:13 GMT"}], "update_date": "2019-09-04", "authors_parsed": [["Weng", "Xinshuo", ""], ["Kitani", "Kris", ""]]}, {"id": "1903.09868", "submitter": "Mingfei Gao", "authors": "Mingfei Gao, Mingze Xu, Larry S. Davis, Richard Socher, Caiming Xiong", "title": "StartNet: Online Detection of Action Start in Untrimmed Videos", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose StartNet to address Online Detection of Action Start (ODAS) where\naction starts and their associated categories are detected in untrimmed,\nstreaming videos. Previous methods aim to localize action starts by learning\nfeature representations that can directly separate the start point from its\npreceding background. It is challenging due to the subtle appearance difference\nnear the action starts and the lack of training data. Instead, StartNet\ndecomposes ODAS into two stages: action classification (using ClsNet) and start\npoint localization (using LocNet). ClsNet focuses on per-frame labeling and\npredicts action score distributions online. Based on the predicted action\nscores of the past and current frames, LocNet conducts class-agnostic start\ndetection by optimizing long-term localization rewards using policy gradient\nmethods. The proposed framework is validated on two large-scale datasets,\nTHUMOS'14 and ActivityNet. The experimental results show that StartNet\nsignificantly outperforms the state-of-the-art by 15%-30% p-mAP under the\noffset tolerance of 1-10 seconds on THUMOS'14, and achieves comparable\nperformance on ActivityNet with 10 times smaller time offset.\n", "versions": [{"version": "v1", "created": "Sat, 23 Mar 2019 19:14:53 GMT"}], "update_date": "2019-03-26", "authors_parsed": [["Gao", "Mingfei", ""], ["Xu", "Mingze", ""], ["Davis", "Larry S.", ""], ["Socher", "Richard", ""], ["Xiong", "Caiming", ""]]}, {"id": "1903.09870", "submitter": "Ayzaan Wahid", "authors": "Ayzaan Wahid, Alexander Toshev, Marek Fiser, Tsang-Wei Edward Lee", "title": "Long Range Neural Navigation Policies for the Real World", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learned Neural Network based policies have shown promising results for robot\nnavigation. However, most of these approaches fall short of being used on a\nreal robot due to the extensive simulated training they require. These\nsimulations lack the visuals and dynamics of the real world, which makes it\ninfeasible to deploy on a real robot. We present a novel Neural Net based\npolicy, NavNet, which allows for easy deployment on a real robot. It consists\nof two sub policies -- a high level policy which can understand real images and\nperform long range planning expressed in high level commands; a low level\npolicy that can translate the long range plan into low level commands on a\nspecific platform in a safe and robust manner. For every new deployment, the\nhigh level policy is trained on an easily obtainable scan of the environment\nmodeling its visuals and layout. We detail the design of such an environment\nand how one can use it for training a final navigation policy. Further, we\ndemonstrate a learned low-level policy. We deploy the model in a large office\nbuilding and test it extensively, achieving $0.80$ success rate over long\nnavigation runs and outperforming SLAM-based models in the same settings.\n", "versions": [{"version": "v1", "created": "Sat, 23 Mar 2019 19:36:11 GMT"}, {"version": "v2", "created": "Wed, 28 Aug 2019 19:32:23 GMT"}], "update_date": "2019-08-30", "authors_parsed": [["Wahid", "Ayzaan", ""], ["Toshev", "Alexander", ""], ["Fiser", "Marek", ""], ["Lee", "Tsang-Wei Edward", ""]]}, {"id": "1903.09876", "submitter": "Hao Tang", "authors": "Hao Tang, Daniel R. Kim, Xiaohui Xie", "title": "Automated pulmonary nodule detection using 3D deep convolutional neural\n  networks", "comments": "2018 IEEE 15th International Symposium on Biomedical Imaging (ISBI\n  2018)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Early detection of pulmonary nodules in computed tomography (CT) images is\nessential for successful outcomes among lung cancer patients. Much attention\nhas been given to deep convolutional neural network (DCNN)-based approaches to\nthis task, but models have relied at least partly on 2D or 2.5D components for\ninherently 3D data. In this paper, we introduce a novel DCNN approach,\nconsisting of two stages, that is fully three-dimensional end-to-end and\nutilizes the state-of-the-art in object detection. First, nodule candidates are\nidentified with a U-Net-inspired 3D Faster R-CNN trained using online hard\nnegative mining. Second, false positive reduction is performed by 3D DCNN\nclassifiers trained on difficult examples produced during candidate screening.\nFinally, we introduce a method to ensemble models from both stages via\nconsensus to give the final predictions. By using this framework, we ranked\nfirst of 2887 teams in Season One of Alibaba's 2017 TianChi AI Competition for\nHealthcare.\n", "versions": [{"version": "v1", "created": "Sat, 23 Mar 2019 20:20:15 GMT"}], "update_date": "2019-03-26", "authors_parsed": [["Tang", "Hao", ""], ["Kim", "Daniel R.", ""], ["Xie", "Xiaohui", ""]]}, {"id": "1903.09879", "submitter": "Hao Tang", "authors": "Hao Tang, Chupeng Zhang, Xiaohui Xie", "title": "Automatic Pulmonary Lobe Segmentation Using Deep Learning", "comments": "2019 IEEE 16th International Symposium on Biomedical Imaging (ISBI\n  2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Pulmonary lobe segmentation is an important task for pulmonary disease\nrelated Computer Aided Diagnosis systems (CADs). Classical methods for lobe\nsegmentation rely on successful detection of fissures and other anatomical\ninformation such as the location of blood vessels and airways. With the success\nof deep learning in recent years, Deep Convolutional Neural Network (DCNN) has\nbeen widely applied to analyze medical images like Computed Tomography (CT) and\nMagnetic Resonance Imaging (MRI), which, however, requires a large number of\nground truth annotations. In this work, we release our manually labeled 50 CT\nscans which are randomly chosen from the LUNA16 dataset and explore the use of\ndeep learning on this task. We propose pre-processing CT image by cropping\nregion that is covered by the convex hull of the lungs in order to mitigate the\ninfluence of noise from outside the lungs. Moreover, we design a hybrid loss\nfunction with dice loss to tackle extreme class imbalance issue and focal loss\nto force model to focus on voxels that are hard to be discriminated. To\nvalidate the robustness and performance of our proposed framework trained with\na small number of training examples, we further tested our model on CT scans\nfrom an independent dataset. Experimental results show the robustness of the\nproposed approach, which consistently improves performance across different\ndatasets by a maximum of $5.87\\%$ as compared to a baseline model.\n", "versions": [{"version": "v1", "created": "Sat, 23 Mar 2019 20:31:45 GMT"}, {"version": "v2", "created": "Mon, 1 Apr 2019 18:04:51 GMT"}, {"version": "v3", "created": "Wed, 10 Apr 2019 06:41:19 GMT"}], "update_date": "2019-04-11", "authors_parsed": [["Tang", "Hao", ""], ["Zhang", "Chupeng", ""], ["Xie", "Xiaohui", ""]]}, {"id": "1903.09880", "submitter": "Hao Tang", "authors": "Hao Tang, Xingwei Liu, Xiaohui Xie", "title": "An End-to-end Framework For Integrated Pulmonary Nodule Detection and\n  False Positive Reduction", "comments": "2019 IEEE 16th International Symposium on Biomedical Imaging (ISBI\n  2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Pulmonary nodule detection using low-dose Computed Tomography (CT) is often\nthe first step in lung disease screening and diagnosis. Recently, algorithms\nbased on deep convolutional neural nets have shown great promise for automated\nnodule detection. Most of the existing deep learning nodule detection systems\nare constructed in two steps: a) nodule candidates screening and b) false\npositive reduction, using two different models trained separately. Although it\nis commonly adopted, the two-step approach not only imposes significant\nresource overhead on training two independent deep learning models, but also is\nsub-optimal because it prevents cross-talk between the two. In this work, we\npresent an end-to-end framework for nodule detection, integrating nodule\ncandidate screening and false positive reduction into one model, trained\njointly. We demonstrate that the end-to-end system improves the performance by\n3.88\\% over the two-step approach, while at the same time reducing model\ncomplexity by one third and cutting inference time by 3.6 fold. Code will be\nmade publicly available.\n", "versions": [{"version": "v1", "created": "Sat, 23 Mar 2019 20:38:51 GMT"}], "update_date": "2019-03-26", "authors_parsed": [["Tang", "Hao", ""], ["Liu", "Xingwei", ""], ["Xie", "Xiaohui", ""]]}, {"id": "1903.09887", "submitter": "Enmao Diao", "authors": "Enmao Diao, Jie Ding, Vahid Tarokh", "title": "DRASIC: Distributed Recurrent Autoencoder for Scalable Image Compression", "comments": null, "journal-ref": null, "doi": "10.1109/DCC47342.2020.00008", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new architecture for distributed image compression from a group\nof distributed data sources. The work is motivated by practical needs of\ndata-driven codec design, low power consumption, robustness, and data privacy.\nThe proposed architecture, which we refer to as Distributed Recurrent\nAutoencoder for Scalable Image Compression (DRASIC), is able to train\ndistributed encoders and one joint decoder on correlated data sources. Its\ncompression capability is much better than the method of training codecs\nseparately. Meanwhile, the performance of our distributed system with 10\ndistributed sources is only within 2 dB peak signal-to-noise ratio (PSNR) of\nthe performance of a single codec trained with all data sources. We experiment\ndistributed sources with different correlations and show how our data-driven\nmethodology well matches the Slepian-Wolf Theorem in Distributed Source Coding\n(DSC). To the best of our knowledge, this is the first data-driven DSC\nframework for general distributed code design with deep learning.\n", "versions": [{"version": "v1", "created": "Sat, 23 Mar 2019 21:42:17 GMT"}, {"version": "v2", "created": "Tue, 26 Mar 2019 15:27:48 GMT"}, {"version": "v3", "created": "Sat, 28 Dec 2019 01:51:24 GMT"}], "update_date": "2020-06-11", "authors_parsed": [["Diao", "Enmao", ""], ["Ding", "Jie", ""], ["Tarokh", "Vahid", ""]]}, {"id": "1903.09900", "submitter": "Andrew Hundt", "authors": "Andrew Hundt, Varun Jain, Gregory D. Hager", "title": "sharpDARTS: Faster and More Accurate Differentiable Architecture Search", "comments": "9 pages, 6 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural Architecture Search (NAS) has been a source of dramatic improvements\nin neural network design, with recent results meeting or exceeding the\nperformance of hand-tuned architectures. However, our understanding of how to\nrepresent the search space for neural net architectures and how to search that\nspace efficiently are both still in their infancy.\n  We have performed an in-depth analysis to identify limitations in a widely\nused search space and a recent architecture search method, Differentiable\nArchitecture Search (DARTS). These findings led us to introduce novel network\nblocks with a more general, balanced, and consistent design; a better-optimized\nCosine Power Annealing learning rate schedule; and other improvements. Our\nresulting sharpDARTS search is 50% faster with a 20-30% relative improvement in\nfinal model error on CIFAR-10 when compared to DARTS. Our best single model run\nhas 1.93% (1.98+/-0.07) validation error on CIFAR-10 and 5.5% error (5.8+/-0.3)\non the recently released CIFAR-10.1 test set. To our knowledge, both are state\nof the art for models of similar size. This model also generalizes\ncompetitively to ImageNet at 25.1% top-1 (7.8% top-5) error.\n  We found improvements for existing search spaces but does DARTS generalize to\nnew domains? We propose Differentiable Hyperparameter Grid Search and the\nHyperCuboid search space, which are representations designed to leverage DARTS\nfor more general parameter optimization. Here we find that DARTS fails to\ngeneralize when compared against a human's one shot choice of models. We look\nback to the DARTS and sharpDARTS search spaces to understand why, and an\nablation study reveals an unusual generalization gap. We finally propose Max-W\nregularization to solve this problem, which proves significantly better than\nthe handmade design. Code will be made available.\n", "versions": [{"version": "v1", "created": "Sat, 23 Mar 2019 23:20:44 GMT"}], "update_date": "2019-03-26", "authors_parsed": [["Hundt", "Andrew", ""], ["Jain", "Varun", ""], ["Hager", "Gregory D.", ""]]}, {"id": "1903.09917", "submitter": "Hongwei Dong", "authors": "Lamei Zhang and Hongwei Dong and Bin Zou", "title": "Efficiently utilizing complex-valued PolSAR image data via a multi-task\n  deep learning framework", "comments": null, "journal-ref": "ISPRS Journal of Photogrammetry and Remote Sensing, Volume 157,\n  November 2019, Pages 59-72", "doi": "10.1016/j.isprsjprs.2019.09.002", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional neural networks (CNNs) have been widely used to improve the\naccuracy of polarimetric synthetic aperture radar (PolSAR) image\nclassification. However, in most studies, the difference between PolSAR images\nand optical images is rarely considered. Most of the existing CNNs are not\ntailored for the task of PolSAR image classification, in which complex-valued\nPolSAR data have been simply equated to real-valued data to fit the optical\nimage processing architectures and avoid complex-valued operations. This is one\nof the reasons CNNs unable to perform their full capabilities in PolSAR\nclassification. To solve the above problem, the objective of this paper is to\ndevelop a tailored CNN framework for PolSAR image classification, which can be\nimplemented from two aspects: Seeking a better form of PolSAR data as the input\nof CNNs and building matched CNN architectures based on the proposed input\nform. In this paper, considering the properties of complex-valued numbers,\namplitude and phase of complex-valued PolSAR data are extracted as the input\nfor the first time to maintain the integrity of original information while\navoiding immature complex-valued operations. Then, a multi-task CNN (MCNN)\narchitecture is proposed to match the improved input form and achieve better\nclassification results. Furthermore, depthwise separable convolution is\nintroduced to the proposed architecture in order to better extract information\nfrom the phase information. Experiments on three PolSAR benchmark datasets not\nonly prove that using amplitude and phase as the input do contribute to the\nimprovement of PolSAR classification, but also verify the adaptability between\nthe improved input form and the well-designed architectures.\n", "versions": [{"version": "v1", "created": "Sun, 24 Mar 2019 03:45:44 GMT"}, {"version": "v2", "created": "Thu, 12 Sep 2019 08:56:09 GMT"}], "update_date": "2019-09-13", "authors_parsed": [["Zhang", "Lamei", ""], ["Dong", "Hongwei", ""], ["Zou", "Bin", ""]]}, {"id": "1903.09922", "submitter": "Nao Takano", "authors": "Nao Takano and Gita Alaghband", "title": "SRGAN: Training Dataset Matters", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative Adversarial Networks (GANs) in supervised settings can generate\nphoto-realistic corresponding output from low-definition input (SRGAN). Using\nthe architecture presented in the SRGAN original paper [2], we explore how\nselecting a dataset affects the outcome by using three different datasets to\nsee that SRGAN fundamentally learns objects, with their shape, color, and\ntexture, and redraws them in the output rather than merely attempting to\nsharpen edges. This is further underscored with our demonstration that once the\nnetwork learns the images of the dataset, it can generate a photo-like image\nwith even a slight hint of what it might look like for the original from a very\nblurry edged sketch. Given a set of inference images, the network trained with\nthe same dataset results in a better outcome over the one trained with\narbitrary set of images, and we report its significance numerically with\nFrechet Inception Distance score [22].\n", "versions": [{"version": "v1", "created": "Sun, 24 Mar 2019 04:28:58 GMT"}], "update_date": "2019-03-26", "authors_parsed": [["Takano", "Nao", ""], ["Alaghband", "Gita", ""]]}, {"id": "1903.09926", "submitter": "Kanav Vats", "authors": "Kanav Vats, Helmut Neher, Alexander Wong, David A. Clausi, and John\n  Zelek", "title": "KPTransfer: improved performance and faster convergence from keypoint\n  subset-wise domain transfer in human pose estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a novel approach called KPTransfer for improving\nmodeling performance for keypoint detection deep neural networks via domain\ntransfer between different keypoint subsets. This approach is motivated by the\nnotion that rich contextual knowledge can be transferred between different\nkeypoint subsets representing separate domains. In particular, the proposed\nmethod takes into account various keypoint subsets/domains by sequentially\nadding and removing keypoints. Contextual knowledge is transferred between two\nseparate domains via domain transfer. Experiments to demonstrate the efficacy\nof the proposed KPTransfer approach were performed for the task of human pose\nestimation on the MPII dataset, with comparisons against random initialization\nand frozen weight extraction configurations. Experimental results demonstrate\nthe efficacy of performing domain transfer between two different joint subsets\nresulting in a PCKh improvement of up to 1.1 over random initialization on\njoints such as wrists and knee in certain joint splits with an overall PCKh\nimprovement of 0.5. Domain transfer from a different set of joints not only\nresults in improved accuracy but also results in faster convergence because of\nmutual co-adaptations of weights resulting from the contextual knowledge of the\npose from a different set of joints.\n", "versions": [{"version": "v1", "created": "Sun, 24 Mar 2019 05:26:40 GMT"}], "update_date": "2019-03-26", "authors_parsed": [["Vats", "Kanav", ""], ["Neher", "Helmut", ""], ["Wong", "Alexander", ""], ["Clausi", "David A.", ""], ["Zelek", "John", ""]]}, {"id": "1903.09940", "submitter": "Mayank Mishra", "authors": "Vinay Kyatham, Mayank Mishra, Tarun Kumar Yadav, Deepak Mishra,\n  Prathosh AP", "title": "Variational Inference with Latent Space Quantization for Adversarial\n  Resilience", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite their tremendous success in modelling high-dimensional data\nmanifolds, deep neural networks suffer from the threat of adversarial attacks -\nExistence of perceptually valid input-like samples obtained through careful\nperturbation that lead to degradation in the performance of the underlying\nmodel. Major concerns with existing defense mechanisms include\nnon-generalizability across different attacks, models and large inference time.\nIn this paper, we propose a generalized defense mechanism capitalizing on the\nexpressive power of regularized latent space based generative models. We design\nan adversarial filter, devoid of access to classifier and adversaries, which\nmakes it usable in tandem with any classifier. The basic idea is to learn a\nLipschitz constrained mapping from the data manifold, incorporating adversarial\nperturbations, to a quantized latent space and re-map it to the true data\nmanifold. Specifically, we simultaneously auto-encode the data manifold and its\nperturbations implicitly through the perturbations of the regularized and\nquantized generative latent space, realized using variational inference. We\ndemonstrate the efficacy of the proposed formulation in providing resilience\nagainst multiple attack types (black and white box) and methods, while being\nalmost real-time. Our experiments show that the proposed method surpasses the\nstate-of-the-art techniques in several cases.\n", "versions": [{"version": "v1", "created": "Sun, 24 Mar 2019 07:47:01 GMT"}, {"version": "v2", "created": "Fri, 6 Sep 2019 06:48:07 GMT"}], "update_date": "2019-09-09", "authors_parsed": [["Kyatham", "Vinay", ""], ["Mishra", "Mayank", ""], ["Yadav", "Tarun Kumar", ""], ["Mishra", "Deepak", ""], ["AP", "Prathosh", ""]]}, {"id": "1903.09950", "submitter": "Ye Xia", "authors": "Ye Xia, Jinkyu Kim, John Canny, Karl Zipser, and David Whitney", "title": "Periphery-Fovea Multi-Resolution Driving Model guided by Human Attention", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inspired by human vision, we propose a new periphery-fovea multi-resolution\ndriving model that predicts vehicle speed from dash camera videos. The\nperipheral vision module of the model processes the full video frames in low\nresolution. Its foveal vision module selects sub-regions and uses\nhigh-resolution input from those regions to improve its driving performance. We\ntrain the fovea selection module with supervision from driver gaze. We show\nthat adding high-resolution input from predicted human driver gaze locations\nsignificantly improves the driving accuracy of the model. Our periphery-fovea\nmulti-resolution model outperforms a uni-resolution periphery-only model that\nhas the same amount of floating-point operations. More importantly, we\ndemonstrate that our driving model achieves a significantly higher performance\ngain in pedestrian-involved critical situations than in other non-critical\nsituations.\n", "versions": [{"version": "v1", "created": "Sun, 24 Mar 2019 09:13:52 GMT"}], "update_date": "2019-03-26", "authors_parsed": [["Xia", "Ye", ""], ["Kim", "Jinkyu", ""], ["Canny", "John", ""], ["Zipser", "Karl", ""], ["Whitney", "David", ""]]}, {"id": "1903.09973", "submitter": "Evgeny Ponomarev", "authors": "Julia Gusak, Maksym Kholiavchenko, Evgeny Ponomarev, Larisa Markeeva,\n  Ivan Oseledets, Andrzej Cichocki", "title": "MUSCO: Multi-Stage Compression of neural networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The low-rank tensor approximation is very promising for the compression of\ndeep neural networks. We propose a new simple and efficient iterative approach,\nwhich alternates low-rank factorization with a smart rank selection and\nfine-tuning. We demonstrate the efficiency of our method comparing to\nnon-iterative ones. Our approach improves the compression rate while\nmaintaining the accuracy for a variety of tasks.\n", "versions": [{"version": "v1", "created": "Sun, 24 Mar 2019 11:40:18 GMT"}, {"version": "v2", "created": "Thu, 9 May 2019 13:08:22 GMT"}, {"version": "v3", "created": "Mon, 13 May 2019 13:20:17 GMT"}, {"version": "v4", "created": "Fri, 15 Nov 2019 12:27:25 GMT"}], "update_date": "2019-11-18", "authors_parsed": [["Gusak", "Julia", ""], ["Kholiavchenko", "Maksym", ""], ["Ponomarev", "Evgeny", ""], ["Markeeva", "Larisa", ""], ["Oseledets", "Ivan", ""], ["Cichocki", "Andrzej", ""]]}, {"id": "1903.09977", "submitter": "Xian Wei", "authors": "Xian Wei, Hao Shen, Yuanxiang Li, Xuan Tang, Bo Jin, Lijun Zhao, Yi Lu\n  Murphey", "title": "Joint Learning of Discriminative Low-dimensional Image Representations\n  Based on Dictionary Learning and Two-layer Orthogonal Projections", "comments": "Some inappropriate descriptions have been found in this paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  There are some inadequacies in the language description of this paper that\nrequire further improvement. This paper is based on a revision of a conference\npaper. It is now necessary to further explain the difference between the\ncontributions of the two papers.\n", "versions": [{"version": "v1", "created": "Sun, 24 Mar 2019 12:06:49 GMT"}, {"version": "v2", "created": "Wed, 27 Mar 2019 14:56:23 GMT"}], "update_date": "2019-11-12", "authors_parsed": [["Wei", "Xian", ""], ["Shen", "Hao", ""], ["Li", "Yuanxiang", ""], ["Tang", "Xuan", ""], ["Jin", "Bo", ""], ["Zhao", "Lijun", ""], ["Murphey", "Yi Lu", ""]]}, {"id": "1903.09980", "submitter": "Zhijie Deng", "authors": "Zhijie Deng, Yucen Luo and Jun Zhu", "title": "Cluster Alignment with a Teacher for Unsupervised Domain Adaptation", "comments": "IEEE International Conference on Computer Vision 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning methods have shown promise in unsupervised domain adaptation,\nwhich aims to leverage a labeled source domain to learn a classifier for the\nunlabeled target domain with a different distribution. However, such methods\ntypically learn a domain-invariant representation space to match the marginal\ndistributions of the source and target domains, while ignoring their fine-level\nstructures. In this paper, we propose Cluster Alignment with a Teacher (CAT)\nfor unsupervised domain adaptation, which can effectively incorporate the\ndiscriminative clustering structures in both domains for better adaptation.\nTechnically, CAT leverages an implicit ensembling teacher model to reliably\ndiscover the class-conditional structure in the feature space for the unlabeled\ntarget domain. Then CAT forces the features of both the source and the target\ndomains to form discriminative class-conditional clusters and aligns the\ncorresponding clusters across domains. Empirical results demonstrate that CAT\nachieves state-of-the-art results in several unsupervised domain adaptation\nscenarios.\n", "versions": [{"version": "v1", "created": "Sun, 24 Mar 2019 12:16:59 GMT"}, {"version": "v2", "created": "Wed, 9 Oct 2019 05:12:15 GMT"}], "update_date": "2019-10-10", "authors_parsed": [["Deng", "Zhijie", ""], ["Luo", "Yucen", ""], ["Zhu", "Jun", ""]]}, {"id": "1903.10008", "submitter": "Kathl\\'en Kohn", "authors": "Timothy Duff and Kathl\\'en Kohn and Anton Leykin and Tomas Pajdla", "title": "PLMP -- Point-Line Minimal Problems in Complete Multi-View Visibility", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.NA math.AG math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a complete classification of all minimal problems for generic\narrangements of points and lines completely observed by calibrated perspective\ncameras. We show that there are only 30 minimal problems in total, no problems\nexist for more than 6 cameras, for more than 5 points, and for more than 6\nlines. We present a sequence of tests for detecting minimality starting with\ncounting degrees of freedom and ending with full symbolic and numeric\nverification of representative examples. For all minimal problems discovered,\nwe present their algebraic degrees, i.e. the number of solutions, which measure\ntheir intrinsic difficulty. It shows how exactly the difficulty of problems\ngrows with the number of views. Importantly, several new minimal problems have\nsmall degrees that might be practical in image matching and 3D reconstruction.\n", "versions": [{"version": "v1", "created": "Sun, 24 Mar 2019 16:07:49 GMT"}, {"version": "v2", "created": "Thu, 5 Sep 2019 10:43:11 GMT"}], "update_date": "2019-09-06", "authors_parsed": [["Duff", "Timothy", ""], ["Kohn", "Kathl\u00e9n", ""], ["Leykin", "Anton", ""], ["Pajdla", "Tomas", ""]]}, {"id": "1903.10019", "submitter": "Joel Brogan Joel R Brogan", "authors": "Joel Brogan, Aparna Bharati, Daniel Moreira, Kevin Bowyer, Patrick\n  Flynn, Anderson Rocha, Walter Scheirer", "title": "Dynamic Spatial Verification for Large-Scale Object-Level Image\n  Retrieval", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Images from social media can reflect diverse viewpoints, heated arguments,\nand expressions of creativity, adding new complexity to retrieval tasks.\nResearchers working onContent-Based Image Retrieval (CBIR) have traditionally\ntuned their algorithms to match filtered results with user search intent.\nHowever, we are now bombarded with composite images of unknown origin,\nauthenticity, and even meaning. With such uncertainty, users may not have an\ninitial idea of what the results of a search query should look like. For\ninstance, hidden people, spliced objects, and subtly altered scenes can be\ndifficult for a user to detect initially in a meme image, but may contribute\nsignificantly to its composition. We propose a new approach for spatial\nverification that aims at modeling object-level regions dynamically clustering\nkeypoints in a 2D Hough space, which are then used to accurately weight small\ncontributing objects within the results, without the need for costly object\ndetection steps. We call this method Objects in Scene to Objects in Scene\n(OS2OS) score, and it is optimized for fast matrix operations on CPUs. OS2OS\nperforms comparably to state-of-the-art methods in classic CBIR problems, on\nthe Oxford5K, Paris 6K, and Google-Landmarks datasets, without the need for\nbounding boxes. It also succeeds in emerging retrieval tasks such as image\ncomposite matching in the NIST MFC2018 dataset and meme-style composite imagery\nfromReddit.\n", "versions": [{"version": "v1", "created": "Sun, 24 Mar 2019 16:57:21 GMT"}, {"version": "v2", "created": "Mon, 8 Apr 2019 21:27:20 GMT"}, {"version": "v3", "created": "Wed, 10 Apr 2019 20:41:49 GMT"}, {"version": "v4", "created": "Mon, 2 Dec 2019 13:34:46 GMT"}], "update_date": "2019-12-03", "authors_parsed": [["Brogan", "Joel", ""], ["Bharati", "Aparna", ""], ["Moreira", "Daniel", ""], ["Bowyer", "Kevin", ""], ["Flynn", "Patrick", ""], ["Rocha", "Anderson", ""], ["Scheirer", "Walter", ""]]}, {"id": "1903.10035", "submitter": "Muhammed Talo", "authors": "Muhammed Talo", "title": "Automated Classification of Histopathology Images Using Transfer\n  Learning", "comments": null, "journal-ref": null, "doi": "10.1016/j.artmed.2019.101743", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  There is a strong need for automated systems to improve diagnostic quality\nand reduce the analysis time in histopathology image processing. Automated\ndetection and classification of pathological tissue characteristics with\ncomputer-aided diagnostic systems are a critical step in the early diagnosis\nand treatment of diseases. Once a pathology image is scanned by a microscope\nand loaded onto a computer, it can be used for automated detection and\nclassification of diseases. In this study, the DenseNet-161 and ResNet-50\npre-trained CNN models have been used to classify digital histopathology\npatches into the corresponding whole slide images via transfer learning\ntechnique. The proposed pre-trained models were tested on grayscale and color\nhistopathology images. The DenseNet-161 pre-trained model achieved a\nclassification accuracy of 97.89% using grayscale images and the ResNet-50\nmodel obtained the accuracy of 98.87% for color images. The proposed\npre-trained models outperform state-of-the-art methods in all performance\nmetrics to classify digital pathology patches into 24 categories.\n", "versions": [{"version": "v1", "created": "Sun, 24 Mar 2019 18:32:35 GMT"}, {"version": "v2", "created": "Thu, 21 Nov 2019 17:25:03 GMT"}], "update_date": "2019-11-22", "authors_parsed": [["Talo", "Muhammed", ""]]}, {"id": "1903.10082", "submitter": "Yulun Zhang", "authors": "Yulun Zhang, Kunpeng Li, Kai Li, Bineng Zhong and Yun Fu", "title": "Residual Non-local Attention Networks for Image Restoration", "comments": "To appear in ICLR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a residual non-local attention network for\nhigh-quality image restoration. Without considering the uneven distribution of\ninformation in the corrupted images, previous methods are restricted by local\nconvolutional operation and equal treatment of spatial- and channel-wise\nfeatures. To address this issue, we design local and non-local attention blocks\nto extract features that capture the long-range dependencies between pixels and\npay more attention to the challenging parts. Specifically, we design trunk\nbranch and (non-)local mask branch in each (non-)local attention block. The\ntrunk branch is used to extract hierarchical features. Local and non-local mask\nbranches aim to adaptively rescale these hierarchical features with mixed\nattentions. The local mask branch concentrates on more local structures with\nconvolutional operations, while non-local attention considers more about\nlong-range dependencies in the whole feature map. Furthermore, we propose\nresidual local and non-local attention learning to train the very deep network,\nwhich further enhance the representation ability of the network. Our proposed\nmethod can be generalized for various image restoration applications, such as\nimage denoising, demosaicing, compression artifacts reduction, and\nsuper-resolution. Experiments demonstrate that our method obtains comparable or\nbetter results compared with recently leading methods quantitatively and\nvisually.\n", "versions": [{"version": "v1", "created": "Sun, 24 Mar 2019 23:40:49 GMT"}], "update_date": "2019-03-26", "authors_parsed": [["Zhang", "Yulun", ""], ["Li", "Kunpeng", ""], ["Li", "Kai", ""], ["Zhong", "Bineng", ""], ["Fu", "Yun", ""]]}, {"id": "1903.10118", "submitter": "Keisuke Hagiwara", "authors": "Keisuke Hagiwara, Yusuke Mukuta, Tatsuya Harada", "title": "End-to-End Learning Using Cycle Consistency for Image-to-Caption\n  Transformations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  So far, research to generate captions from images has been carried out from\nthe viewpoint that a caption holds sufficient information for an image. If it\nis possible to generate an image that is close to the input image from a\ngenerated caption, i.e., if it is possible to generate a natural language\ncaption containing sufficient information to reproduce the image, then the\ncaption is considered to be faithful to the image. To make such regeneration\npossible, learning using the cycle-consistency loss is effective. In this\nstudy, we propose a method of generating captions by learning end-to-end mutual\ntransformations between images and texts. To evaluate our method, we perform\ncomparative experiments with and without the cycle consistency. The results are\nevaluated by an automatic evaluation and crowdsourcing, demonstrating that our\nproposed method is effective.\n", "versions": [{"version": "v1", "created": "Mon, 25 Mar 2019 03:40:15 GMT"}], "update_date": "2019-03-26", "authors_parsed": [["Hagiwara", "Keisuke", ""], ["Mukuta", "Yusuke", ""], ["Harada", "Tatsuya", ""]]}, {"id": "1903.10122", "submitter": "Yuan Li", "authors": "Christy Y. Li, Xiaodan Liang, Zhiting Hu, Eric P. Xing", "title": "Knowledge-driven Encode, Retrieve, Paraphrase for Medical Image Report\n  Generation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generating long and semantic-coherent reports to describe medical images\nposes great challenges towards bridging visual and linguistic modalities,\nincorporating medical domain knowledge, and generating realistic and accurate\ndescriptions. We propose a novel Knowledge-driven Encode, Retrieve, Paraphrase\n(KERP) approach which reconciles traditional knowledge- and retrieval-based\nmethods with modern learning-based methods for accurate and robust medical\nreport generation. Specifically, KERP decomposes medical report generation into\nexplicit medical abnormality graph learning and subsequent natural language\nmodeling. KERP first employs an Encode module that transforms visual features\ninto a structured abnormality graph by incorporating prior medical knowledge;\nthen a Retrieve module that retrieves text templates based on the detected\nabnormalities; and lastly, a Paraphrase module that rewrites the templates\naccording to specific cases. The core of KERP is a proposed generic\nimplementation unit---Graph Transformer (GTR) that dynamically transforms\nhigh-level semantics between graph-structured data of multiple domains such as\nknowledge graphs, images and sequences. Experiments show that the proposed\napproach generates structured and robust reports supported with accurate\nabnormality description and explainable attentive regions, achieving the\nstate-of-the-art results on two medical report benchmarks, with the best\nmedical abnormality and disease classification accuracy and improved human\nevaluation performance.\n", "versions": [{"version": "v1", "created": "Mon, 25 Mar 2019 03:54:11 GMT"}], "update_date": "2019-03-26", "authors_parsed": [["Li", "Christy Y.", ""], ["Liang", "Xiaodan", ""], ["Hu", "Zhiting", ""], ["Xing", "Eric P.", ""]]}, {"id": "1903.10128", "submitter": "Muhammad Haris", "authors": "Muhammad Haris, Greg Shakhnarovich, and Norimichi Ukita", "title": "Recurrent Back-Projection Network for Video Super-Resolution", "comments": "To appear in CVPR2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We proposed a novel architecture for the problem of video super-resolution.\nWe integrate spatial and temporal contexts from continuous video frames using a\nrecurrent encoder-decoder module, that fuses multi-frame information with the\nmore traditional, single frame super-resolution path for the target frame. In\ncontrast to most prior work where frames are pooled together by stacking or\nwarping, our model, the Recurrent Back-Projection Network (RBPN) treats each\ncontext frame as a separate source of information. These sources are combined\nin an iterative refinement framework inspired by the idea of back-projection in\nmultiple-image super-resolution. This is aided by explicitly representing\nestimated inter-frame motion with respect to the target, rather than explicitly\naligning frames. We propose a new video super-resolution benchmark, allowing\nevaluation at a larger scale and considering videos in different motion\nregimes. Experimental results demonstrate that our RBPN is superior to existing\nmethods on several datasets.\n", "versions": [{"version": "v1", "created": "Mon, 25 Mar 2019 04:13:36 GMT"}], "update_date": "2019-03-26", "authors_parsed": [["Haris", "Muhammad", ""], ["Shakhnarovich", "Greg", ""], ["Ukita", "Norimichi", ""]]}, {"id": "1903.10132", "submitter": "Yongqin Xian", "authors": "Yongqin Xian and Saurabh Sharma and Bernt Schiele and Zeynep Akata", "title": "f-VAEGAN-D2: A Feature Generating Framework for Any-Shot Learning", "comments": "Accepted at CVPR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When labeled training data is scarce, a promising data augmentation approach\nis to generate visual features of unknown classes using their attributes. To\nlearn the class conditional distribution of CNN features, these models rely on\npairs of image features and class attributes. Hence, they can not make use of\nthe abundance of unlabeled data samples. In this paper, we tackle any-shot\nlearning problems i.e. zero-shot and few-shot, in a unified feature generating\nframework that operates in both inductive and transductive learning settings.\nWe develop a conditional generative model that combines the strength of VAE and\nGANs and in addition, via an unconditional discriminator, learns the marginal\nfeature distribution of unlabeled images. We empirically show that our model\nlearns highly discriminative CNN features for five datasets, i.e. CUB, SUN, AWA\nand ImageNet, and establish a new state-of-the-art in any-shot learning, i.e.\ninductive and transductive (generalized) zero- and few-shot learning settings.\nWe also demonstrate that our learned features are interpretable: we visualize\nthem by inverting them back to the pixel space and we explain them by\ngenerating textual arguments of why they are associated with a certain label.\n", "versions": [{"version": "v1", "created": "Mon, 25 Mar 2019 04:35:03 GMT"}], "update_date": "2019-03-26", "authors_parsed": [["Xian", "Yongqin", ""], ["Sharma", "Saurabh", ""], ["Schiele", "Bernt", ""], ["Akata", "Zeynep", ""]]}, {"id": "1903.10139", "submitter": "Dwarikanath Mahapatra", "authors": "Dwarikanath Mahapatra and Zongyuan Ge", "title": "Training Data Independent Image Registration With GANs Using Transfer\n  Learning And Segmentation Information", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Registration is an important task in automated medical image analysis.\nAlthough deep learning (DL) based image registration methods out perform time\nconsuming conventional approaches, they are heavily dependent on training data\nand do not generalize well for new images types. We present a DL based approach\nthat can register an image pair which is different from the training images.\nThis is achieved by training generative adversarial networks (GANs) in\ncombination with segmentation information and transfer learning. Experiments on\nchest Xray and brain MR images show that our method gives better registration\nperformance over conventional methods.\n", "versions": [{"version": "v1", "created": "Mon, 25 Mar 2019 05:33:04 GMT"}, {"version": "v2", "created": "Thu, 11 Apr 2019 07:27:28 GMT"}], "update_date": "2019-04-12", "authors_parsed": [["Mahapatra", "Dwarikanath", ""], ["Ge", "Zongyuan", ""]]}, {"id": "1903.10140", "submitter": "Chunyang Feng", "authors": "Chunyang Feng, Yufeng Sun, Xin Li", "title": "Iris R-CNN: Accurate Iris Segmentation in Non-cooperative Environment", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the significant advances in iris segmentation, accomplishing accurate\niris segmentation in non-cooperative environment remains a grand challenge. In\nthis paper, we present a deep learning framework, referred to as Iris R-CNN, to\noffer superior accuracy for iris segmentation. The proposed framework is\nderived from Mask R-CNN, and several novel techniques are proposed to carefully\nexplore the unique characteristics of iris. First, we propose two novel\nnetworks: (i) Double-Circle Region Proposal Network (DC-RPN), and (ii)\nDouble-Circle Classification and Regression Network (DC-CRN) to take into\naccount the iris and pupil circles to maximize the accuracy for iris\nsegmentation. Second, we propose a novel normalization scheme for Regions of\nInterest (RoIs) to facilitate a radically new pooling operation over a\ndouble-circle region. Experimental results on two challenging iris databases,\nUBIRIS.v2 and MICHE, demonstrate the superior accuracy of the proposed approach\nover other state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Mon, 25 Mar 2019 05:33:06 GMT"}], "update_date": "2019-03-26", "authors_parsed": [["Feng", "Chunyang", ""], ["Sun", "Yufeng", ""], ["Li", "Xin", ""]]}, {"id": "1903.10143", "submitter": "Zhiwen Shao", "authors": "Zhiwen Shao, Jianfei Cai, Tat-Jen Cham, Xuequan Lu, Lizhuang Ma", "title": "Unconstrained Facial Action Unit Detection via Latent Feature Domain", "comments": "This paper has been accepted by IEEE Transactions on Affective\n  Computing", "journal-ref": null, "doi": "10.1109/TAFFC.2021.3091331", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Facial action unit (AU) detection in the wild is a challenging problem, due\nto the unconstrained variability in facial appearances and the lack of accurate\nannotations. Most existing methods depend on either impractical labor-intensive\nlabeling or inaccurate pseudo labels. In this paper, we propose an end-to-end\nunconstrained facial AU detection framework based on domain adaptation, which\ntransfers accurate AU labels from a constrained source domain to an\nunconstrained target domain by exploiting labels of AU-related facial\nlandmarks. Specifically, we map a source image with label and a target image\nwithout label into a latent feature domain by combining source landmark-related\nfeature with target landmark-free feature. Due to the combination of source\nAU-related information and target AU-free information, the latent feature\ndomain with transferred source label can be learned by maximizing the\ntarget-domain AU detection performance. Moreover, we introduce a novel landmark\nadversarial loss to disentangle the landmark-free feature from the\nlandmark-related feature by treating the adversarial learning as a multi-player\nminimax game. Our framework can also be naturally extended for use with\ntarget-domain pseudo AU labels. Extensive experiments show that our method\nsoundly outperforms lower-bounds and upper-bounds of the basic model, as well\nas state-of-the-art approaches on the challenging in-the-wild benchmarks. The\ncode is available at https://github.com/ZhiwenShao/ADLD.\n", "versions": [{"version": "v1", "created": "Mon, 25 Mar 2019 06:16:32 GMT"}, {"version": "v2", "created": "Tue, 27 Aug 2019 07:55:35 GMT"}, {"version": "v3", "created": "Fri, 10 Jan 2020 08:55:24 GMT"}, {"version": "v4", "created": "Sun, 20 Jun 2021 13:46:30 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Shao", "Zhiwen", ""], ["Cai", "Jianfei", ""], ["Cham", "Tat-Jen", ""], ["Lu", "Xuequan", ""], ["Ma", "Lizhuang", ""]]}, {"id": "1903.10145", "submitter": "Chunyuan Li", "authors": "Hao Fu, Chunyuan Li, Xiaodong Liu, Jianfeng Gao, Asli Celikyilmaz,\n  Lawrence Carin", "title": "Cyclical Annealing Schedule: A Simple Approach to Mitigating KL\n  Vanishing", "comments": "Published in NAACL 2019; The first two authors contribute equally;\n  Code: https://github.com/haofuml/cyclical_annealing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CL cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Variational autoencoders (VAEs) with an auto-regressive decoder have been\napplied for many natural language processing (NLP) tasks. The VAE objective\nconsists of two terms, (i) reconstruction and (ii) KL regularization, balanced\nby a weighting hyper-parameter \\beta. One notorious training difficulty is that\nthe KL term tends to vanish. In this paper we study scheduling schemes for\n\\beta, and show that KL vanishing is caused by the lack of good latent codes in\ntraining the decoder at the beginning of optimization. To remedy this, we\npropose a cyclical annealing schedule, which repeats the process of increasing\n\\beta multiple times. This new procedure allows the progressive learning of\nmore meaningful latent codes, by leveraging the informative representations of\nprevious cycles as warm re-starts. The effectiveness of cyclical annealing is\nvalidated on a broad range of NLP tasks, including language modeling, dialog\nresponse generation and unsupervised language pre-training.\n", "versions": [{"version": "v1", "created": "Mon, 25 Mar 2019 06:28:24 GMT"}, {"version": "v2", "created": "Sun, 26 May 2019 06:50:06 GMT"}, {"version": "v3", "created": "Mon, 10 Jun 2019 21:43:02 GMT"}], "update_date": "2019-06-12", "authors_parsed": [["Fu", "Hao", ""], ["Li", "Chunyuan", ""], ["Liu", "Xiaodong", ""], ["Gao", "Jianfeng", ""], ["Celikyilmaz", "Asli", ""], ["Carin", "Lawrence", ""]]}, {"id": "1903.10146", "submitter": "Sheng You", "authors": "Sheng You, Ning You, Minxue Pan", "title": "PI-REC: Progressive Image Reconstruction Network With Edge and Color\n  Domain", "comments": "15 pages, 13 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We propose a universal image reconstruction method to represent detailed\nimages purely from binary sparse edge and flat color domain. Inspired by the\nprocedures of painting, our framework, based on generative adversarial network,\nconsists of three phases: Imitation Phase aims at initializing networks,\nfollowed by Generating Phase to reconstruct preliminary images. Moreover,\nRefinement Phase is utilized to fine-tune preliminary images into final outputs\nwith details. This framework allows our model generating abundant high\nfrequency details from sparse input information. We also explore the defects of\ndisentangling style latent space implicitly from images, and demonstrate that\nexplicit color domain in our model performs better on controllability and\ninterpretability. In our experiments, we achieve outstanding results on\nreconstructing realistic images and translating hand drawn drafts into\nsatisfactory paintings. Besides, within the domain of edge-to-image\ntranslation, our model PI-REC outperforms existing state-of-the-art methods on\nevaluations of realism and accuracy, both quantitatively and qualitatively.\n", "versions": [{"version": "v1", "created": "Mon, 25 Mar 2019 06:29:36 GMT"}], "update_date": "2019-03-26", "authors_parsed": [["You", "Sheng", ""], ["You", "Ning", ""], ["Pan", "Minxue", ""]]}, {"id": "1903.10150", "submitter": "Tasfia Shermin", "authors": "Tasfia Shermin, Shyh Wei Teng, Manzur Murshed, Guojun Lu, Ferdous\n  Sohel, Manoranjan Paul", "title": "Enhanced Transfer Learning with ImageNet Trained Classification Layer", "comments": "14 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Parameter fine tuning is a transfer learning approach whereby learned\nparameters from pre-trained source network are transferred to the target\nnetwork followed by fine-tuning. Prior research has shown that this approach is\ncapable of improving task performance. However, the impact of the ImageNet\npre-trained classification layer in parameter fine-tuning is mostly unexplored\nin the literature. In this paper, we propose a fine-tuning approach with the\npre-trained classification layer. We employ layer-wise fine-tuning to determine\nwhich layers should be frozen for optimal performance. Our empirical analysis\ndemonstrates that the proposed fine-tuning performs better than traditional\nfine-tuning. This finding indicates that the pre-trained classification layer\nholds less category-specific or more global information than believed earlier.\nThus, we hypothesize that the presence of this layer is crucial for growing\nnetwork depth to adapt better to a new task. Our study manifests that careful\nnormalization and scaling are essential for creating harmony between the\npre-trained and new layers for target domain adaptation. We evaluate the\nproposed depth augmented networks for fine-tuning on several challenging\nbenchmark datasets and show that they can achieve higher classification\naccuracy than contemporary transfer learning approaches.\n", "versions": [{"version": "v1", "created": "Mon, 25 Mar 2019 06:43:26 GMT"}, {"version": "v2", "created": "Thu, 19 Sep 2019 13:33:05 GMT"}], "update_date": "2019-09-20", "authors_parsed": [["Shermin", "Tasfia", ""], ["Teng", "Shyh Wei", ""], ["Murshed", "Manzur", ""], ["Lu", "Guojun", ""], ["Sohel", "Ferdous", ""], ["Paul", "Manoranjan", ""]]}, {"id": "1903.10152", "submitter": "Xiaowei Hu", "authors": "Xiaowei Hu, Chi-Wing Fu, Lei Zhu, Tianyu Wang, Pheng-Ann Heng", "title": "SAC-Net: Spatial Attenuation Context for Salient Object Detection", "comments": null, "journal-ref": null, "doi": "10.1109/TCSVT.2020.2995220", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a new deep neural network design for salient object\ndetection by maximizing the integration of local and global image context\nwithin, around, and beyond the salient objects. Our key idea is to adaptively\npropagate and aggregate the image context features with variable attenuation\nover the entire feature maps. To achieve this, we design the spatial\nattenuation context (SAC) module to recurrently translate and aggregate the\ncontext features independently with different attenuation factors and then to\nattentively learn the weights to adaptively integrate the aggregated context\nfeatures. By further embedding the module to process individual layers in a\ndeep network, namely SAC-Net, we can train the network end-to-end and optimize\nthe context features for detecting salient objects. Compared with 29\nstate-of-the-art methods, experimental results show that our method performs\nfavorably over all the others on six common benchmark data, both quantitatively\nand visually.\n", "versions": [{"version": "v1", "created": "Mon, 25 Mar 2019 06:56:15 GMT"}, {"version": "v2", "created": "Tue, 9 Jul 2019 01:34:49 GMT"}, {"version": "v3", "created": "Tue, 12 May 2020 12:45:17 GMT"}], "update_date": "2020-05-21", "authors_parsed": [["Hu", "Xiaowei", ""], ["Fu", "Chi-Wing", ""], ["Zhu", "Lei", ""], ["Wang", "Tianyu", ""], ["Heng", "Pheng-Ann", ""]]}, {"id": "1903.10153", "submitter": "Fan Wu", "authors": "Pengfei Yao, Zheng Fang, Fan Wu, Yao Feng, Jiwei Li", "title": "DenseBody: Directly Regressing Dense 3D Human Pose and Shape From a\n  Single Color Image", "comments": "10 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recovering 3D human body shape and pose from 2D images is a challenging task\ndue to high complexity and flexibility of human body, and relatively less 3D\nlabeled data. Previous methods addressing these issues typically rely on\npredicting intermediate results such as body part segmentation, 2D/3D joints,\nsilhouette mask to decompose the problem into multiple sub-tasks in order to\nutilize more 2D labels. Most previous works incorporated parametric body shape\nmodel in their methods and predict parameters in low-dimensional space to\nrepresent human body. In this paper, we propose to directly regress the 3D\nhuman mesh from a single color image using Convolutional Neural Network(CNN).\nWe use an efficient representation of 3D human shape and pose which can be\npredicted through an encoder-decoder neural network. The proposed method\nachieves state-of-the-art performance on several 3D human body datasets\nincluding Human3.6M, SURREAL and UP-3D with even faster running speed.\n", "versions": [{"version": "v1", "created": "Mon, 25 Mar 2019 06:56:24 GMT"}, {"version": "v2", "created": "Tue, 26 Mar 2019 13:41:14 GMT"}, {"version": "v3", "created": "Thu, 28 Mar 2019 10:31:07 GMT"}], "update_date": "2019-03-29", "authors_parsed": [["Yao", "Pengfei", ""], ["Fang", "Zheng", ""], ["Wu", "Fan", ""], ["Feng", "Yao", ""], ["Li", "Jiwei", ""]]}, {"id": "1903.10157", "submitter": "Se Young Chun", "authors": "Dongwon Park and Jisoo Kim and Se Young Chun", "title": "Down-Scaling with Learned Kernels in Multi-Scale Deep Neural Networks\n  for Non-Uniform Single Image Deblurring", "comments": "10 pages, 7 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-scale approach has been used for blind image / video deblurring\nproblems to yield excellent performance for both conventional and recent\ndeep-learning-based state-of-the-art methods. Bicubic down-sampling is a\ntypical choice for multi-scale approach to reduce spatial dimension after\nfiltering with a fixed kernel. However, this fixed kernel may be sub-optimal\nsince it may destroy important information for reliable deblurring such as\nstrong edges. We propose convolutional neural network (CNN)-based down-scale\nmethods for multi-scale deep-learning-based non-uniform single image\ndeblurring. We argue that our CNN-based down-scaling effectively reduces the\nspatial dimension of the original image, while learned kernels with multiple\nchannels may well-preserve necessary details for deblurring tasks. For each\nscale, we adopt to use RCAN (Residual Channel Attention Networks) as a backbone\nnetwork to further improve performance. Our proposed method yielded\nstate-of-the-art performance on GoPro dataset by large margin. Our proposed\nmethod was able to achieve 2.59dB higher PSNR than the current state-of-the-art\nmethod by Tao. Our proposed CNN-based down-scaling was the key factor for this\nexcellent performance since the performance of our network without it was\ndecreased by 1.98dB. The same networks trained with GoPro set were also\nevaluated on large-scale Su dataset and our proposed method yielded 1.15dB\nbetter PSNR than the Tao's method. Qualitative comparisons on Lai dataset also\nconfirmed the superior performance of our proposed method over other\nstate-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Mon, 25 Mar 2019 07:23:07 GMT"}], "update_date": "2019-03-26", "authors_parsed": [["Park", "Dongwon", ""], ["Kim", "Jisoo", ""], ["Chun", "Se Young", ""]]}, {"id": "1903.10168", "submitter": "Silvio Giancola", "authors": "Jesus Zarzar, Silvio Giancola, Bernard Ghanem", "title": "Efficient Bird Eye View Proposals for 3D Siamese Tracking", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tracking vehicles in LIDAR point clouds is a challenging task due to the\nsparsity of the data and the dense search space. The lack of structure in point\nclouds impedes the use of convolution filters usually employed in 2D object\ntracking. In addition, structuring point clouds is cumbersome and implies\nlosing fine-grained information. As a result, generating proposals in 3D space\nis expensive and inefficient. In this paper, we leverage the dense and\nstructured Bird Eye View (BEV) representation of LIDAR point clouds to\nefficiently search for objects of interest. We use an efficient Region Proposal\nNetwork and generate a small number of object proposals in 3D. Successively, we\nrefine our selection of 3D object candidates by exploiting the similarity\ncapability of a 3D Siamese network. We regularize the latter 3D Siamese network\nfor shape completion to enhance its discrimination capability. Our method\nattempts to solve both for an efficient search space in the BEV space and a\nmeaningful selection using 3D LIDAR point cloud. We show that the Region\nProposal in the BEV outperforms Bayesian methods such as Kalman and Particle\nFilters in providing proposal by a significant margin and that such candidates\nare suitable for the 3D Siamese network. By training our method end-to-end, we\noutperform the previous baseline in vehicle tracking by 12% / 18% in Success\nand Precision when using only 16 candidates.\n", "versions": [{"version": "v1", "created": "Mon, 25 Mar 2019 08:09:13 GMT"}, {"version": "v2", "created": "Wed, 6 May 2020 06:41:26 GMT"}], "update_date": "2020-05-07", "authors_parsed": [["Zarzar", "Jesus", ""], ["Giancola", "Silvio", ""], ["Ghanem", "Bernard", ""]]}, {"id": "1903.10170", "submitter": "Kangxue Yin", "authors": "Kangxue Yin, Zhiqin Chen, Hui Huang, Daniel Cohen-Or, Hao Zhang", "title": "LOGAN: Unpaired Shape Transform in Latent Overcomplete Space", "comments": "Download supplementary material here ->\n  https://kangxue.org/papers/logan_supp.pdf", "journal-ref": "ACM Transactions on Graphics(Proc. of SIGGRAPH Asia), 38(6),\n  198:1-198:13, 2019", "doi": "10.1145/3355089.3356494", "report-no": null, "categories": "cs.CV cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce LOGAN, a deep neural network aimed at learning general-purpose\nshape transforms from unpaired domains. The network is trained on two sets of\nshapes, e.g., tables and chairs, while there is neither a pairing between\nshapes from the domains as supervision nor any point-wise correspondence\nbetween any shapes. Once trained, LOGAN takes a shape from one domain and\ntransforms it into the other. Our network consists of an autoencoder to encode\nshapes from the two input domains into a common latent space, where the latent\ncodes concatenate multi-scale shape features, resulting in an overcomplete\nrepresentation. The translator is based on a generative adversarial network\n(GAN), operating in the latent space, where an adversarial loss enforces\ncross-domain translation while a feature preservation loss ensures that the\nright shape features are preserved for a natural shape transform. We conduct\nablation studies to validate each of our key network designs and demonstrate\nsuperior capabilities in unpaired shape transforms on a variety of examples\nover baselines and state-of-the-art approaches. We show that LOGAN is able to\nlearn what shape features to preserve during shape translation, either local or\nnon-local, whether content or style, depending solely on the input domains for\ntraining.\n", "versions": [{"version": "v1", "created": "Mon, 25 Mar 2019 08:20:48 GMT"}, {"version": "v2", "created": "Mon, 29 Jul 2019 05:37:33 GMT"}, {"version": "v3", "created": "Tue, 3 Sep 2019 01:06:02 GMT"}], "update_date": "2019-09-04", "authors_parsed": [["Yin", "Kangxue", ""], ["Chen", "Zhiqin", ""], ["Huang", "Hui", ""], ["Cohen-Or", "Daniel", ""], ["Zhang", "Hao", ""]]}, {"id": "1903.10172", "submitter": "Mason Liu", "authors": "Mason Liu, Menglong Zhu, Marie White, Yinxiao Li, Dmitry Kalenichenko", "title": "Looking Fast and Slow: Memory-Guided Mobile Video Object Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With a single eye fixation lasting a fraction of a second, the human visual\nsystem is capable of forming a rich representation of a complex environment,\nreaching a holistic understanding which facilitates object recognition and\ndetection. This phenomenon is known as recognizing the \"gist\" of the scene and\nis accomplished by relying on relevant prior knowledge. This paper addresses\nthe analogous question of whether using memory in computer vision systems can\nnot only improve the accuracy of object detection in video streams, but also\nreduce the computation time. By interleaving conventional feature extractors\nwith extremely lightweight ones which only need to recognize the gist of the\nscene, we show that minimal computation is required to produce accurate\ndetections when temporal memory is present. In addition, we show that the\nmemory contains enough information for deploying reinforcement learning\nalgorithms to learn an adaptive inference policy. Our model achieves\nstate-of-the-art performance among mobile methods on the Imagenet VID 2015\ndataset, while running at speeds of up to 70+ FPS on a Pixel 3 phone.\n", "versions": [{"version": "v1", "created": "Mon, 25 Mar 2019 08:30:58 GMT"}], "update_date": "2019-03-26", "authors_parsed": [["Liu", "Mason", ""], ["Zhu", "Menglong", ""], ["White", "Marie", ""], ["Li", "Yinxiao", ""], ["Kalenichenko", "Dmitry", ""]]}, {"id": "1903.10175", "submitter": "Xuechen Li", "authors": "Yinlong Liu, Xuechen Li, Manning Wang, Guang Chen, Zhijian Song, Alois\n  Knoll", "title": "A Novel Method for the Absolute Pose Problem with Pairwise Constraints", "comments": "10 pages, 7figures", "journal-ref": null, "doi": "10.3390/rs11243007", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Absolute pose estimation is a fundamental problem in computer vision, and it\nis a typical parameter estimation problem, meaning that efforts to solve it\nwill always suffer from outlier-contaminated data. Conventionally, for a fixed\ndimensionality d and the number of measurements N, a robust estimation problem\ncannot be solved faster than O(N^d). Furthermore, it is almost impossible to\nremove d from the exponent of the runtime of a globally optimal algorithm.\nHowever, absolute pose estimation is a geometric parameter estimation problem,\nand thus has special constraints. In this paper, we consider pairwise\nconstraints and propose a globally optimal algorithm for solving the absolute\npose estimation problem. The proposed algorithm has a linear complexity in the\nnumber of correspondences at a given outlier ratio. Concretely, we first\ndecouple the rotation and the translation subproblems by utilizing the pairwise\nconstraints, and then we solve the rotation subproblem using the\nbranch-and-bound algorithm. Lastly, we estimate the translation based on the\nknown rotation by using another branch-and-bound algorithm. The advantages of\nour method are demonstrated via thorough testing on both synthetic and\nreal-world data\n", "versions": [{"version": "v1", "created": "Mon, 25 Mar 2019 08:37:49 GMT"}, {"version": "v2", "created": "Thu, 28 Mar 2019 06:18:58 GMT"}], "update_date": "2019-12-17", "authors_parsed": [["Liu", "Yinlong", ""], ["Li", "Xuechen", ""], ["Wang", "Manning", ""], ["Chen", "Guang", ""], ["Song", "Zhijian", ""], ["Knoll", "Alois", ""]]}, {"id": "1903.10176", "submitter": "Gary Mataev", "authors": "Gary Mataev, Michael Elad and Peyman Milanfar", "title": "DeepRED: Deep Image Prior Powered by RED", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inverse problems in imaging are extensively studied, with a variety of\nstrategies, tools, and theory that have been accumulated over the years.\nRecently, this field has been immensely influenced by the emergence of\ndeep-learning techniques. One such contribution, which is the focus of this\npaper, is the Deep Image Prior (DIP) work by Ulyanov, Vedaldi, and Lempitsky\n(2018). DIP offers a new approach towards the regularization of inverse\nproblems, obtained by forcing the recovered image to be synthesized from a\ngiven deep architecture. While DIP has been shown to be quite an effective\nunsupervised approach, its results still fall short when compared to\nstate-of-the-art alternatives.\n  In this work, we aim to boost DIP by adding an explicit prior, which enriches\nthe overall regularization effect in order to lead to better-recovered images.\nMore specifically, we propose to bring-in the concept of Regularization by\nDenoising (RED), which leverages existing denoisers for regularizing inverse\nproblems. Our work shows how the two (DIP and RED) can be merged into a highly\neffective unsupervised recovery process while avoiding the need to\ndifferentiate the chosen denoiser, and leading to very effective results,\ndemonstrated for several tested problems.\n", "versions": [{"version": "v1", "created": "Mon, 25 Mar 2019 08:39:53 GMT"}, {"version": "v2", "created": "Mon, 6 May 2019 19:41:00 GMT"}, {"version": "v3", "created": "Thu, 24 Oct 2019 10:50:39 GMT"}], "update_date": "2019-10-25", "authors_parsed": [["Mataev", "Gary", ""], ["Elad", "Michael", ""], ["Milanfar", "Peyman", ""]]}, {"id": "1903.10195", "submitter": "Xavier Gir\\'o-i-Nieto", "authors": "Amanda Duarte, Francisco Roldan, Miquel Tubau, Janna Escur, Santiago\n  Pascual, Amaia Salvador, Eva Mohedano, Kevin McGuinness, Jordi Torres and\n  Xavier Giro-i-Nieto", "title": "Wav2Pix: Speech-conditioned Face Generation using Generative Adversarial\n  Networks", "comments": "ICASSP 2019. Projevct website at\n  https://imatge-upc.github.io/wav2pix/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Speech is a rich biometric signal that contains information about the\nidentity, gender and emotional state of the speaker. In this work, we explore\nits potential to generate face images of a speaker by conditioning a Generative\nAdversarial Network (GAN) with raw speech input. We propose a deep neural\nnetwork that is trained from scratch in an end-to-end fashion, generating a\nface directly from the raw speech waveform without any additional identity\ninformation (e.g reference image or one-hot encoding). Our model is trained in\na self-supervised approach by exploiting the audio and visual signals naturally\naligned in videos. With the purpose of training from video data, we present a\nnovel dataset collected for this work, with high-quality videos of youtubers\nwith notable expressiveness in both the speech and visual signals.\n", "versions": [{"version": "v1", "created": "Mon, 25 Mar 2019 09:27:44 GMT"}], "update_date": "2019-03-26", "authors_parsed": [["Duarte", "Amanda", ""], ["Roldan", "Francisco", ""], ["Tubau", "Miquel", ""], ["Escur", "Janna", ""], ["Pascual", "Santiago", ""], ["Salvador", "Amaia", ""], ["Mohedano", "Eva", ""], ["McGuinness", "Kevin", ""], ["Torres", "Jordi", ""], ["Giro-i-Nieto", "Xavier", ""]]}, {"id": "1903.10203", "submitter": "Chaoyou Fu", "authors": "Chaoyou Fu, Xiang Wu, Yibo Hu, Huaibo Huang, Ran He", "title": "Dual Variational Generation for Low-Shot Heterogeneous Face Recognition", "comments": "Accepted by NeurIPS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Heterogeneous Face Recognition (HFR) is a challenging issue because of the\nlarge domain discrepancy and a lack of heterogeneous data. This paper considers\nHFR as a dual generation problem, and proposes a novel Dual Variational\nGeneration (DVG) framework. It generates large-scale new paired heterogeneous\nimages with the same identity from noise, for the sake of reducing the domain\ngap of HFR. Specifically, we first introduce a dual variational autoencoder to\nrepresent a joint distribution of paired heterogeneous images. Then, in order\nto ensure the identity consistency of the generated paired heterogeneous\nimages, we impose a distribution alignment in the latent space and a pairwise\nidentity preserving in the image space. Moreover, the HFR network reduces the\ndomain discrepancy by constraining the pairwise feature distances between the\ngenerated paired heterogeneous images. Extensive experiments on four HFR\ndatabases show that our method can significantly improve state-of-the-art\nresults. The related code is available at https://github.com/BradyFU/DVG.\n", "versions": [{"version": "v1", "created": "Mon, 25 Mar 2019 09:39:33 GMT"}, {"version": "v2", "created": "Tue, 26 Nov 2019 03:06:48 GMT"}, {"version": "v3", "created": "Sun, 1 Dec 2019 11:54:02 GMT"}], "update_date": "2019-12-03", "authors_parsed": [["Fu", "Chaoyou", ""], ["Wu", "Xiang", ""], ["Hu", "Yibo", ""], ["Huang", "Huaibo", ""], ["He", "Ran", ""]]}, {"id": "1903.10205", "submitter": "Haohao Hu", "authors": "Haohao Hu and Marc Sons and Christoph Stiller", "title": "Accurate Global Trajectory Alignment using Poles and Road Markings", "comments": "6 packages, 6 figures, conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Currently, digital maps are indispensable for automated driving. However, due\nto the low precision and reliability of GNSS particularly in urban areas,\nfusing trajectories of independent recording sessions and different regions is\na challenging task. To bypass the flaws from direct incorporation of GNSS\nmeasurements for geo-referencing, the usage of aerial imagery seems promising.\nFurthermore, more accurate geo-referencing improves the global map accuracy and\nallows to estimate the sensor calibration error. In this paper, we present a\nnovel geo-referencing approach to align trajectories to aerial imagery using\npoles and road markings. To match extracted features from sensor observations\nto aerial imagery landmarks robustly, a RANSAC-based matching approach is\napplied in a sliding window. For that, we assume that the trajectories are\nroughly referenced to the imagery which can be achieved by rough GNSS\nmeasurements from a low-cost GNSS receiver. Finally, we align the initial\ntrajectories precisely to the aerial imagery by minimizing a geometric cost\nfunction comprising all determined matches. Evaluations performed on data\nrecorded in Karlsruhe, Germany show that our algorithm yields trajectories\nwhich are accurately referenced to the used aerial imagery.\n", "versions": [{"version": "v1", "created": "Mon, 25 Mar 2019 09:40:11 GMT"}], "update_date": "2019-03-26", "authors_parsed": [["Hu", "Haohao", ""], ["Sons", "Marc", ""], ["Stiller", "Christoph", ""]]}, {"id": "1903.10210", "submitter": "Alex Gilbert", "authors": "Yunhao Ba, Alex Ross Gilbert, Franklin Wang, Jinfa Yang, Rui Chen,\n  Yiqin Wang, Lei Yan, Boxin Shi, Achuta Kadambi", "title": "Deep Shape from Polarization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper makes a first attempt to bring the Shape from Polarization (SfP)\nproblem to the realm of deep learning. The previous state-of-the-art methods\nfor SfP have been purely physics-based. We see value in these principled\nmodels, and blend these physical models as priors into a neural network\narchitecture. This proposed approach achieves results that exceed the previous\nstate-of-the-art on a challenging dataset we introduce. This dataset consists\nof polarization images taken over a range of object textures, paints, and\nlighting conditions. We report that our proposed method achieves the lowest\ntest error on each tested condition in our dataset, showing the value of\nblending data-driven and physics-driven approaches.\n", "versions": [{"version": "v1", "created": "Mon, 25 Mar 2019 09:55:08 GMT"}, {"version": "v2", "created": "Mon, 25 May 2020 05:36:22 GMT"}], "update_date": "2020-05-26", "authors_parsed": [["Ba", "Yunhao", ""], ["Gilbert", "Alex Ross", ""], ["Wang", "Franklin", ""], ["Yang", "Jinfa", ""], ["Chen", "Rui", ""], ["Wang", "Yiqin", ""], ["Yan", "Lei", ""], ["Shi", "Boxin", ""], ["Kadambi", "Achuta", ""]]}, {"id": "1903.10211", "submitter": "Shanshan Wang", "authors": "Lei Zhang, Shanshan Wang, Guang-Bin Huang, Wangmeng Zuo, Jian Yang,\n  David Zhang", "title": "Manifold Criterion Guided Transfer Learning via Intermediate Domain\n  Generation", "comments": "This paper has been accepted by IEEE Transactions on Neural Networks\n  and Learning Systems", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many practical transfer learning scenarios, the feature distribution is\ndifferent across the source and target domains (i.e. non-i.i.d.). Maximum mean\ndiscrepancy (MMD), as a domain discrepancy metric, has achieved promising\nperformance in unsupervised domain adaptation (DA). We argue that MMD-based DA\nmethods ignore the data locality structure, which, to some extent, would cause\nthe negative transfer effect. The locality plays an important role in\nminimizing the nonlinear local domain discrepancy underlying the marginal\ndistributions. For better exploiting the domain locality, a novel local\ngenerative discrepancy metric (LGDM) based intermediate domain generation\nlearning called Manifold Criterion guided Transfer Learning (MCTL) is proposed\nin this paper. The merits of the proposed MCTL are four-fold: 1) the concept of\nmanifold criterion (MC) is first proposed as a measure validating the\ndistribution matching across domains, and domain adaptation is achieved if the\nMC is satisfied; 2) the proposed MC can well guide the generation of the\nintermediate domain sharing similar distribution with the target domain, by\nminimizing the local domain discrepancy; 3) a global generative discrepancy\nmetric (GGDM) is presented, such that both the global and local discrepancy can\nbe effectively and positively reduced; 4) a simplified version of MCTL called\nMCTL-S is presented under a perfect domain generation assumption for more\ngeneric learning scenario. Experiments on a number of benchmark visual transfer\ntasks demonstrate the superiority of the proposed manifold criterion guided\ngenerative transfer method, by comparing with other state-of-the-art methods.\nThe source code is available in https://github.com/wangshanshanCQU/MCTL.\n", "versions": [{"version": "v1", "created": "Mon, 25 Mar 2019 09:58:51 GMT"}], "update_date": "2019-03-26", "authors_parsed": [["Zhang", "Lei", ""], ["Wang", "Shanshan", ""], ["Huang", "Guang-Bin", ""], ["Zuo", "Wangmeng", ""], ["Yang", "Jian", ""], ["Zhang", "David", ""]]}, {"id": "1903.10225", "submitter": "Wei Shen", "authors": "Wei Shen, Ziqiang Shi, Jun Sun", "title": "Learning from Adversarial Features for Few-Shot Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many recent few-shot learning methods concentrate on designing novel model\narchitectures. In this paper, we instead show that with a simple backbone\nconvolutional network we can even surpass state-of-the-art classification\naccuracy. The essential part that contributes to this superior performance is\nan adversarial feature learning strategy that improves the generalization\ncapability of our model. In this work, adversarial features are those features\nthat can cause the classifier uncertain about its prediction. In order to\ngenerate adversarial features, we firstly locate adversarial regions based on\nthe derivative of the entropy with respect to an averaging mask. Then we use\nthe adversarial region attention to aggregate the feature maps to obtain the\nadversarial features. In this way, we can explore and exploit the entire\nspatial area of the feature maps to mine more diverse discriminative knowledge.\nWe perform extensive model evaluations and analyses on miniImageNet and\ntieredImageNet datasets demonstrating the effectiveness of the proposed method.\n", "versions": [{"version": "v1", "created": "Mon, 25 Mar 2019 10:30:23 GMT"}], "update_date": "2019-03-26", "authors_parsed": [["Shen", "Wei", ""], ["Shi", "Ziqiang", ""], ["Sun", "Jun", ""]]}, {"id": "1903.10251", "submitter": "Lars Ailo Bongo", "authors": "Cristina J\\'acome, Johan Ravn, Einar Holsb{\\o}, Juan Carlos\n  Aviles-Solis, Hasse Melbye, Lars Ailo Bongo", "title": "Convolutional neural network for breathing phase detection in lung\n  sounds", "comments": null, "journal-ref": null, "doi": "10.3390/s19081798", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We applied deep learning to create an algorithm for breathing phase detection\nin lung sound recordings, and we compared the breathing phases detected by the\nalgorithm and manually annotated by two experienced lung sound researchers. Our\nalgorithm uses a convolutional neural network with spectrograms as the\nfeatures, removing the need to specify features explicitly. We trained and\nevaluated the algorithm using three subsets that are larger than previously\nseen in the literature. We evaluated the performance of the method using two\nmethods. First, discrete count of agreed breathing phases (using 50% overlap\nbetween a pair of boxes), shows a mean agreement with lung sound experts of 97%\nfor inspiration and 87% for expiration. Second, the fraction of time of\nagreement (in seconds) gives higher pseudo-kappa values for inspiration\n(0.73-0.88) than expiration (0.63-0.84), showing an average sensitivity of 97%\nand an average specificity of 84%. With both evaluation methods, the agreement\nbetween the annotators and the algorithm shows human level performance for the\nalgorithm. The developed algorithm is valid for detecting breathing phases in\nlung sound recordings.\n", "versions": [{"version": "v1", "created": "Mon, 25 Mar 2019 11:47:05 GMT"}], "update_date": "2019-04-18", "authors_parsed": [["J\u00e1come", "Cristina", ""], ["Ravn", "Johan", ""], ["Holsb\u00f8", "Einar", ""], ["Aviles-Solis", "Juan Carlos", ""], ["Melbye", "Hasse", ""], ["Bongo", "Lars Ailo", ""]]}, {"id": "1903.10258", "submitter": "Zechun Liu", "authors": "Zechun Liu, Haoyuan Mu, Xiangyu Zhang, Zichao Guo, Xin Yang, Tim\n  Kwang-Ting Cheng, Jian Sun", "title": "MetaPruning: Meta Learning for Automatic Neural Network Channel Pruning", "comments": "ICCV 2019 Camera ready version. Codes are available on\n  https://github.com/liuzechun/MetaPruning", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a novel meta learning approach for automatic\nchannel pruning of very deep neural networks. We first train a PruningNet, a\nkind of meta network, which is able to generate weight parameters for any\npruned structure given the target network. We use a simple stochastic structure\nsampling method for training the PruningNet. Then, we apply an evolutionary\nprocedure to search for good-performing pruned networks. The search is highly\nefficient because the weights are directly generated by the trained PruningNet\nand we do not need any finetuning at search time. With a single PruningNet\ntrained for the target network, we can search for various Pruned Networks under\ndifferent constraints with little human participation. Compared to the\nstate-of-the-art pruning methods, we have demonstrated superior performances on\nMobileNet V1/V2 and ResNet. Codes are available on\nhttps://github.com/liuzechun/MetaPruning.\n", "versions": [{"version": "v1", "created": "Mon, 25 Mar 2019 12:05:27 GMT"}, {"version": "v2", "created": "Wed, 3 Apr 2019 01:23:32 GMT"}, {"version": "v3", "created": "Wed, 14 Aug 2019 03:41:11 GMT"}], "update_date": "2019-08-15", "authors_parsed": [["Liu", "Zechun", ""], ["Mu", "Haoyuan", ""], ["Zhang", "Xiangyu", ""], ["Guo", "Zichao", ""], ["Yang", "Xin", ""], ["Cheng", "Tim Kwang-Ting", ""], ["Sun", "Jian", ""]]}, {"id": "1903.10297", "submitter": "Chenyang Zhu", "authors": "Chenyang Zhu, Kai Xu, Siddhartha Chaudhuri, Li Yi, Leonidas Guibas,\n  Hao Zhang", "title": "AdaCoSeg: Adaptive Shape Co-Segmentation with Group Consistency Loss", "comments": "CVPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce AdaCoSeg, a deep neural network architecture for adaptive\nco-segmentation of a set of 3D shapes represented as point clouds. Differently\nfrom the familiar single-instance segmentation problem, co-segmentation is\nintrinsically contextual: how a shape is segmented can vary depending on the\nset it is in. Hence, our network features an adaptive learning module to\nproduce a consistent shape segmentation which adapts to a set. Specifically,\ngiven an input set of unsegmented shapes, we first employ an offline\npre-trained part prior network to propose per-shape parts. Then, the\nco-segmentation network iteratively and} jointly optimizes the part labelings\nacross the set subjected to a novel group consistency loss defined by matrix\nranks. While the part prior network can be trained with noisy and\ninconsistently segmented shapes, the final output of AdaCoSeg is a consistent\npart labeling for the input set, with each shape segmented into up to (a\nuser-specified) K parts. Overall, our method is weakly supervised, producing\nsegmentations tailored to the test set, without consistent ground-truth\nsegmentations. We show qualitative and quantitative results from AdaCoSeg and\nevaluate it via ablation studies and comparisons to state-of-the-art\nco-segmentation methods.\n", "versions": [{"version": "v1", "created": "Mon, 25 Mar 2019 13:14:23 GMT"}, {"version": "v2", "created": "Tue, 26 Mar 2019 04:30:20 GMT"}, {"version": "v3", "created": "Wed, 27 Mar 2019 03:22:50 GMT"}, {"version": "v4", "created": "Sun, 19 Apr 2020 12:42:47 GMT"}, {"version": "v5", "created": "Wed, 25 Nov 2020 12:55:11 GMT"}], "update_date": "2020-11-26", "authors_parsed": [["Zhu", "Chenyang", ""], ["Xu", "Kai", ""], ["Chaudhuri", "Siddhartha", ""], ["Yi", "Li", ""], ["Guibas", "Leonidas", ""], ["Zhang", "Hao", ""]]}, {"id": "1903.10356", "submitter": "Chang-Hwan Son", "authors": "Hee-Jin Yu, Chang-Hwan Son", "title": "Apple Leaf Disease Identification through Region-of-Interest-Aware Deep\n  Convolutional Neural Network", "comments": null, "journal-ref": "Journal of Imaging Science and Technology, vol. 64, no. 2, pp.\n  20507-1-20507-10, Jan. 2020", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A new method of recognizing apple leaf diseases through\nregion-of-interest-aware deep convolutional neural network is proposed in this\npaper. The primary idea is that leaf disease symptoms appear in the leaf area\nwhereas the background region contains no useful information regarding leaf\ndiseases. To realize this idea, two subnetworks are first designed. One is for\nthe division of the input image into three areas: background, leaf area, and\nspot area indicating the leaf diseases, which is the region of interest, and\nthe other is for the classification of leaf diseases. The two subnetworks\nexhibit the architecture types of an encoder-decoder network and VGG network,\nrespectively; subsequently, they are trained separately through transfer\nlearning with a new training set containing class information, according to the\ntypes of leaf diseases and the ground truth images where the background, leaf\narea, and spot area are separated. Next, to connect these subnetworks and\nsubsequently train the connected whole network in an end-to-end manner, the\npredicted ROI feature map is stacked on the top of the input image through a\nfusion layer, and subsequently fed into the subnetwork used for the leaf\ndisease identification. The experimental results indicate that correct\nrecognition accuracy can be increased using the predicted ROI feature map. It\nis also shown that the proposed method obtains better performance than the\nconventional state-of-the-art methods: transfer-learning-based methods,\nbilinear model, and multiscale-based deep feature extraction, and pooling\napproach.\n", "versions": [{"version": "v1", "created": "Mon, 25 Mar 2019 14:15:40 GMT"}, {"version": "v2", "created": "Mon, 29 Jul 2019 12:23:35 GMT"}], "update_date": "2021-02-09", "authors_parsed": [["Yu", "Hee-Jin", ""], ["Son", "Chang-Hwan", ""]]}, {"id": "1903.10357", "submitter": "Wei Hu", "authors": "Wei Hu, Yangyu Huang, Fan Zhang and Ruirui Li", "title": "Noise-Tolerant Paradigm for Training Face Recognition CNNs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Benefit from large-scale training datasets, deep Convolutional Neural\nNetworks(CNNs) have achieved impressive results in face recognition(FR).\nHowever, tremendous scale of datasets inevitably lead to noisy data, which\nobviously reduce the performance of the trained CNN models. Kicking out wrong\nlabels from large-scale FR datasets is still very expensive, although some\ncleaning approaches are proposed. According to the analysis of the whole\nprocess of training CNN models supervised by angular margin based loss(AM-Loss)\nfunctions, we find that the $\\theta$ distribution of training samples\nimplicitly reflects their probability of being clean. Thus, we propose a novel\ntraining paradigm that employs the idea of weighting samples based on the above\nprobability. Without any prior knowledge of noise, we can train high\nperformance CNN models with large-scale FR datasets. Experiments demonstrate\nthe effectiveness of our training paradigm. The codes are available at\nhttps://github.com/huangyangyu/NoiseFace.\n", "versions": [{"version": "v1", "created": "Mon, 25 Mar 2019 14:16:06 GMT"}, {"version": "v2", "created": "Tue, 26 Mar 2019 05:57:35 GMT"}], "update_date": "2019-03-27", "authors_parsed": [["Hu", "Wei", ""], ["Huang", "Yangyu", ""], ["Zhang", "Fan", ""], ["Li", "Ruirui", ""]]}, {"id": "1903.10360", "submitter": "Kripasindhu Sarkar", "authors": "Kripasindhu Sarkar, Elizabeth Mathews, Didier Stricker", "title": "Structured 2D Representation of 3D Data for Shape Processing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We represent 3D shape by structured 2D representations of fixed length making\nit feasible to apply well investigated 2D convolutional neural networks (CNN)\nfor both discriminative and geometric tasks on 3D shapes. We first provide a\ngeneral introduction to such structured descriptors, analyze their different\nforms and show how a simple 2D CNN can be used to achieve good classification\nresult. With a specialized classification network for images and our structured\nrepresentation, we achieve the classification accuracy of 99.7\\% in the\nModelNet40 test set - improving the previous state-of-the-art by a large\nmargin. We finally provide a novel framework for performing the geometric task\nof 3D segmentation using 2D CNNs and the structured representation - concluding\nthe utility of such descriptors for both discriminative and geometric tasks.\n", "versions": [{"version": "v1", "created": "Mon, 25 Mar 2019 14:21:08 GMT"}], "update_date": "2019-03-26", "authors_parsed": [["Sarkar", "Kripasindhu", ""], ["Mathews", "Elizabeth", ""], ["Stricker", "Didier", ""]]}, {"id": "1903.10384", "submitter": "Shiyang Cheng", "authors": "Shiyang Cheng, Michael Bronstein, Yuxiang Zhou, Irene Kotsia, Maja\n  Pantic, Stefanos Zafeiriou", "title": "MeshGAN: Non-linear 3D Morphable Models of Faces", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Generative Adversarial Networks (GANs) are currently the method of choice for\ngenerating visual data. Certain GAN architectures and training methods have\ndemonstrated exceptional performance in generating realistic synthetic images\n(in particular, of human faces). However, for 3D object, GANs still fall short\nof the success they have had with images. One of the reasons is due to the fact\nthat so far GANs have been applied as 3D convolutional architectures to\ndiscrete volumetric representations of 3D objects. In this paper, we propose\nthe first intrinsic GANs architecture operating directly on 3D meshes (named as\nMeshGAN). Both quantitative and qualitative results are provided to show that\nMeshGAN can be used to generate high-fidelity 3D face with rich identities and\nexpressions.\n", "versions": [{"version": "v1", "created": "Mon, 25 Mar 2019 15:03:35 GMT"}], "update_date": "2019-03-26", "authors_parsed": [["Cheng", "Shiyang", ""], ["Bronstein", "Michael", ""], ["Zhou", "Yuxiang", ""], ["Kotsia", "Irene", ""], ["Pantic", "Maja", ""], ["Zafeiriou", "Stefanos", ""]]}, {"id": "1903.10412", "submitter": "Chongsheng Zhang", "authors": "Chongsheng Zhang and Guowen Peng and Yuefeng Tao and Feifei Fu and Wei\n  Jiang and George Almpanidis and Ke Chen", "title": "ShopSign: a Diverse Scene Text Dataset of Chinese Shop Signs in Street\n  Views", "comments": "10 pages, 2 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce the ShopSign dataset, which is a newly developed\nnatural scene text dataset of Chinese shop signs in street views. Although a\nfew scene text datasets are already publicly available (e.g. ICDAR2015,\nCOCO-Text), there are few images in these datasets that contain Chinese\ntexts/characters. Hence, we collect and annotate the ShopSign dataset to\nadvance research in Chinese scene text detection and recognition.\n  The new dataset has three distinctive characteristics: (1) large-scale: it\ncontains 25,362 Chinese shop sign images, with a total number of 196,010\ntext-lines. (2) diversity: the images in ShopSign were captured in different\nscenes, from downtown to developing regions, using more than 50 different\nmobile phones. (3) difficulty: the dataset is very sparse and imbalanced. It\nalso includes five categories of hard images (mirror, wooden, deformed, exposed\nand obscure). To illustrate the challenges in ShopSign, we run baseline\nexperiments using state-of-the-art scene text detection methods (including\nCTPN, TextBoxes++ and EAST), and cross-dataset validation to compare their\ncorresponding performance on the related datasets such as CTW, RCTW and ICPR\n2018 MTWI challenge dataset.\n  The sample images and detailed descriptions of our ShopSign dataset are\npublicly available at: https://github.com/chongshengzhang/shopsign.\n", "versions": [{"version": "v1", "created": "Mon, 25 Mar 2019 15:52:32 GMT"}], "update_date": "2019-03-26", "authors_parsed": [["Zhang", "Chongsheng", ""], ["Peng", "Guowen", ""], ["Tao", "Yuefeng", ""], ["Fu", "Feifei", ""], ["Jiang", "Wei", ""], ["Almpanidis", "George", ""], ["Chen", "Ke", ""]]}, {"id": "1903.10422", "submitter": "Waseem Abbas", "authors": "Waseem Abbas and David Masip Rodo", "title": "Locomotion and gesture tracking in mice and small animals for\n  neurosceince applications: A survey", "comments": "41 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Neuroscience has traditionally relied on manually observing lab animals in\ncontrolled environments. Researchers usually record animals behaving in free or\nrestrained manner and then annotate the data manually. The manual annotation is\nnot desirable for three reasons; one, it is time consuming, two, it is prone to\nhuman errors and three, no two human annotators will 100\\% agree on annotation,\nso it is not reproducible. Consequently, automated annotation of such data has\ngained traction because it is efficient and replicable. Usually, the automatic\nannotation of neuroscience data relies on computer vision and machine leaning\ntechniques. In this article, we have covered most of the approaches taken by\nresearchers for locomotion and gesture tracking of lab animals. We have divided\nthese papers in categories based upon the hardware they use and the software\napproach they take. We also have summarized their strengths and weaknesses.\n", "versions": [{"version": "v1", "created": "Mon, 25 Mar 2019 16:07:28 GMT"}], "update_date": "2019-03-26", "authors_parsed": [["Abbas", "Waseem", ""], ["Rodo", "David Masip", ""]]}, {"id": "1903.10427", "submitter": "Jaime Spencer Martin Mr.", "authors": "Jaime Spencer, Richard Bowden, Simon Hadfield", "title": "Scale-Adaptive Neural Dense Features: Learning via Hierarchical Context\n  Aggregation", "comments": "CVPR2019", "journal-ref": null, "doi": "10.1109/CVPR.2019.00636", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  How do computers and intelligent agents view the world around them? Feature\nextraction and representation constitutes one the basic building blocks towards\nanswering this question. Traditionally, this has been done with carefully\nengineered hand-crafted techniques such as HOG, SIFT or ORB. However, there is\nno ``one size fits all'' approach that satisfies all requirements. In recent\nyears, the rising popularity of deep learning has resulted in a myriad of\nend-to-end solutions to many computer vision problems. These approaches, while\nsuccessful, tend to lack scalability and can't easily exploit information\nlearned by other systems. Instead, we propose SAND features, a dedicated deep\nlearning solution to feature extraction capable of providing hierarchical\ncontext information. This is achieved by employing sparse relative labels\nindicating relationships of similarity/dissimilarity between image locations.\nThe nature of these labels results in an almost infinite set of dissimilar\nexamples to choose from. We demonstrate how the selection of negative examples\nduring training can be used to modify the feature space and vary it's\nproperties. To demonstrate the generality of this approach, we apply the\nproposed features to a multitude of tasks, each requiring different properties.\nThis includes disparity estimation, semantic segmentation, self-localisation\nand SLAM. In all cases, we show how incorporating SAND features results in\nbetter or comparable results to the baseline, whilst requiring little to no\nadditional training. Code can be found at:\nhttps://github.com/jspenmar/SAND_features\n", "versions": [{"version": "v1", "created": "Mon, 25 Mar 2019 16:12:20 GMT"}], "update_date": "2020-03-31", "authors_parsed": [["Spencer", "Jaime", ""], ["Bowden", "Richard", ""], ["Hadfield", "Simon", ""]]}, {"id": "1903.10442", "submitter": "Li Wang", "authors": "Li Wang, Yongbo Li, Xiangyang Xue", "title": "CODA: Counting Objects via Scale-aware Adversarial Density Adaption", "comments": "Accepted to ICME2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in crowd counting have achieved promising results with\nincreasingly complex convolutional neural network designs. However, due to the\nunpredictable domain shift, generalizing trained model to unseen scenarios is\noften suboptimal. Inspired by the observation that density maps of different\nscenarios share similar local structures, we propose a novel adversarial\nlearning approach in this paper, i.e., CODA (\\emph{Counting Objects via\nscale-aware adversarial Density Adaption}). To deal with different object\nscales and density distributions, we perform adversarial training with pyramid\npatches of multi-scales from both source- and target-domain. Along with a\nranking constraint across levels of the pyramid input, consistent object counts\ncan be produced for different scales. Extensive experiments demonstrate that\nour network produces much better results on unseen datasets compared with\nexisting counting adaption models. Notably, the performance of our CODA is\ncomparable with the state-of-the-art fully-supervised models that are trained\non the target dataset. Further analysis indicates that our density adaption\nframework can effortlessly extend to scenarios with different objects.\n\\emph{The code is available at https://github.com/Willy0919/CODA.}\n", "versions": [{"version": "v1", "created": "Mon, 25 Mar 2019 16:24:03 GMT"}], "update_date": "2019-03-26", "authors_parsed": [["Wang", "Li", ""], ["Li", "Yongbo", ""], ["Xue", "Xiangyang", ""]]}, {"id": "1903.10446", "submitter": "Sushrut Thorat", "authors": "Sushrut Thorat, Marcel van Gerven, Marius Peelen", "title": "The functional role of cue-driven feature-based feedback in object\n  recognition", "comments": "4 pages, 4 figures, published at the Conference on Cognitive\n  Computational Neuroscience (CCN) 2018", "journal-ref": null, "doi": "10.32470/CCN.2018.1044-0", "report-no": null, "categories": "q-bio.NC cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual object recognition is not a trivial task, especially when the objects\nare degraded or surrounded by clutter or presented briefly. External cues (such\nas verbal cues or visual context) can boost recognition performance in such\nconditions. In this work, we build an artificial neural network to model the\ninteraction between the object processing stream (OPS) and the cue. We study\nthe effects of varying neural and representational capacities of the OPS on the\nperformance boost provided by cue-driven feature-based feedback in the OPS. We\nobserve that the feedback provides performance boosts only if the\ncategory-specific features about the objects cannot be fully represented in the\nOPS. This representational limit is more dependent on task demands than neural\ncapacity. We also observe that the feedback scheme trained to maximise\nrecognition performance boost is not the same as tuning-based feedback, and\nactually performs better than tuning-based feedback.\n", "versions": [{"version": "v1", "created": "Mon, 25 Mar 2019 16:27:07 GMT"}], "update_date": "2019-03-26", "authors_parsed": [["Thorat", "Sushrut", ""], ["van Gerven", "Marcel", ""], ["Peelen", "Marius", ""]]}, {"id": "1903.10481", "submitter": "Zhihui Guo", "authors": "Zhihui Guo, Junjie Bai, Yi Lu, Xin Wang, Kunlin Cao, Qi Song, Milan\n  Sonka, Youbing Yin", "title": "DeepCenterline: a Multi-task Fully Convolutional Network for Centerline\n  Extraction", "comments": "Accepted by the international conference on Information Processing in\n  Medical Imaging (IPMI) 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A novel centerline extraction framework is reported which combines an\nend-to-end trainable multi-task fully convolutional network (FCN) with a\nminimal path extractor. The FCN simultaneously computes centerline distance\nmaps and detects branch endpoints. The method generates single-pixel-wide\ncenterlines with no spurious branches. It handles arbitrary tree-structured\nobject with no prior assumption regarding depth of the tree or its bifurcation\npattern. It is also robust to substantial scale changes across different parts\nof the target object and minor imperfections of the object's segmentation mask.\nTo the best of our knowledge, this is the first deep-learning based centerline\nextraction method that guarantees single-pixel-wide centerline for a complex\ntree-structured object. The proposed method is validated in coronary artery\ncenterline extraction on a dataset of 620 patients (400 of which used as test\nset). This application is challenging due to the large number of coronary\nbranches, branch tortuosity, and large variations in length, thickness, shape,\netc. The proposed method generates well-positioned centerlines, exhibiting\nlower number of missing branches and is more robust in the presence of minor\nimperfections of the object segmentation mask. Compared to a state-of-the-art\ntraditional minimal path approach, our method improves patient-level success\nrate of centerline extraction from 54.3% to 88.8% according to independent\nhuman expert review.\n", "versions": [{"version": "v1", "created": "Mon, 25 Mar 2019 17:29:30 GMT"}], "update_date": "2019-03-26", "authors_parsed": [["Guo", "Zhihui", ""], ["Bai", "Junjie", ""], ["Lu", "Yi", ""], ["Wang", "Xin", ""], ["Cao", "Kunlin", ""], ["Song", "Qi", ""], ["Sonka", "Milan", ""], ["Yin", "Youbing", ""]]}, {"id": "1903.10484", "submitter": "Nicholas Carlini", "authors": "J\\\"orn-Henrik Jacobsen and Jens Behrmannn and Nicholas Carlini and\n  Florian Tram\\`er and Nicolas Papernot", "title": "Exploiting Excessive Invariance caused by Norm-Bounded Adversarial\n  Robustness", "comments": "Accepted at the ICLR 2019 SafeML Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adversarial examples are malicious inputs crafted to cause a model to\nmisclassify them. Their most common instantiation, \"perturbation-based\"\nadversarial examples introduce changes to the input that leave its true label\nunchanged, yet result in a different model prediction. Conversely,\n\"invariance-based\" adversarial examples insert changes to the input that leave\nthe model's prediction unaffected despite the underlying input's label having\nchanged.\n  In this paper, we demonstrate that robustness to perturbation-based\nadversarial examples is not only insufficient for general robustness, but\nworse, it can also increase vulnerability of the model to invariance-based\nadversarial examples. In addition to analytical constructions, we empirically\nstudy vision classifiers with state-of-the-art robustness to perturbation-based\nadversaries constrained by an $\\ell_p$ norm. We mount attacks that exploit\nexcessive model invariance in directions relevant to the task, which are able\nto find adversarial examples within the $\\ell_p$ ball. In fact, we find that\nclassifiers trained to be $\\ell_p$-norm robust are more vulnerable to\ninvariance-based adversarial examples than their undefended counterparts.\n  Excessive invariance is not limited to models trained to be robust to\nperturbation-based $\\ell_p$-norm adversaries. In fact, we argue that the term\nadversarial example is used to capture a series of model limitations, some of\nwhich may not have been discovered yet. Accordingly, we call for a set of\nprecise definitions that taxonomize and address each of these shortcomings in\nlearning.\n", "versions": [{"version": "v1", "created": "Mon, 25 Mar 2019 17:29:52 GMT"}], "update_date": "2019-03-26", "authors_parsed": [["Jacobsen", "J\u00f6rn-Henrik", ""], ["Behrmannn", "Jens", ""], ["Carlini", "Nicholas", ""], ["Tram\u00e8r", "Florian", ""], ["Papernot", "Nicolas", ""]]}, {"id": "1903.10501", "submitter": "Lei Zhang", "authors": "Lei Zhang, Zhiqiang Lang, Peng Wang, Wei Wei, Shengcai Liao, Ling\n  Shao, Yanning Zhang", "title": "Pixel-aware Deep Function-mixture Network for Spectral Super-Resolution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Spectral super-resolution (SSR) aims at generating a hyperspectral image\n(HSI) from a given RGB image. Recently, a promising direction for SSR is to\nlearn a complicated mapping function from the RGB image to the HSI counterpart\nusing a deep convolutional neural network. This essentially involves mapping\nthe RGB context within a size-specific receptive field centered at each pixel\nto its spectrum in the HSI. The focus thereon is to appropriately determine the\nreceptive field size and establish the mapping function from RGB context to the\ncorresponding spectrum. Due to their differences in category or spatial\nposition, pixels in HSIs often require different-sized receptive fields and\ndistinct mapping functions. However, few efforts have been invested to\nexplicitly exploit this prior.\n  To address this problem, we propose a pixel-aware deep function-mixture\nnetwork for SSR, which is composed of a new class of modules, termed\nfunction-mixture (FM) blocks. Each FM block is equipped with some basis\nfunctions, i.e., parallel subnets of different-sized receptive fields. Besides,\nit incorporates an extra subnet as a mixing function to generate pixel-wise\nweights, and then linearly mixes the outputs of all basis functions with those\ngenerated weights. This enables us to pixel-wisely determine the receptive\nfield size and the mapping function. Moreover, we stack several such FM blocks\nto further increase the flexibility of the network in learning the pixel-wise\nmapping. To encourage feature reuse, intermediate features generated by the FM\nblocks are fused in late stage, which proves to be effective for boosting the\nSSR performance. Experimental results on three benchmark HSI datasets\ndemonstrate the superiority of the proposed method.\n", "versions": [{"version": "v1", "created": "Sun, 24 Mar 2019 13:42:05 GMT"}], "update_date": "2019-03-27", "authors_parsed": [["Zhang", "Lei", ""], ["Lang", "Zhiqiang", ""], ["Wang", "Peng", ""], ["Wei", "Wei", ""], ["Liao", "Shengcai", ""], ["Shao", "Ling", ""], ["Zhang", "Yanning", ""]]}, {"id": "1903.10520", "submitter": "Siyuan Qiao", "authors": "Siyuan Qiao, Huiyu Wang, Chenxi Liu, Wei Shen, Alan Yuille", "title": "Micro-Batch Training with Batch-Channel Normalization and Weight\n  Standardization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Batch Normalization (BN) has become an out-of-box technique to improve deep\nnetwork training. However, its effectiveness is limited for micro-batch\ntraining, i.e., each GPU typically has only 1-2 images for training, which is\ninevitable for many computer vision tasks, e.g., object detection and semantic\nsegmentation, constrained by memory consumption. To address this issue, we\npropose Weight Standardization (WS) and Batch-Channel Normalization (BCN) to\nbring two success factors of BN into micro-batch training: 1) the smoothing\neffects on the loss landscape and 2) the ability to avoid harmful elimination\nsingularities along the training trajectory. WS standardizes the weights in\nconvolutional layers to smooth the loss landscape by reducing the Lipschitz\nconstants of the loss and the gradients; BCN combines batch and channel\nnormalizations and leverages estimated statistics of the activations in\nconvolutional layers to keep networks away from elimination singularities. We\nvalidate WS and BCN on comprehensive computer vision tasks, including image\nclassification, object detection, instance segmentation, video recognition and\nsemantic segmentation. All experimental results consistently show that WS and\nBCN improve micro-batch training significantly. Moreover, using WS and BCN with\nmicro-batch training is even able to match or outperform the performances of BN\nwith large-batch training.\n", "versions": [{"version": "v1", "created": "Mon, 25 Mar 2019 18:00:05 GMT"}, {"version": "v2", "created": "Sun, 9 Aug 2020 21:25:15 GMT"}], "update_date": "2020-08-11", "authors_parsed": [["Qiao", "Siyuan", ""], ["Wang", "Huiyu", ""], ["Liu", "Chenxi", ""], ["Shen", "Wei", ""], ["Yuille", "Alan", ""]]}, {"id": "1903.10534", "submitter": "Francisco Raposo", "authors": "Francisco Afonso Raposo and David Martins de Matos and Ricardo Ribeiro", "title": "Learning Embodied Semantics via Music and Dance Semiotic Correlations", "comments": "24 pages, 1 figure, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Music semantics is embodied, in the sense that meaning is biologically\nmediated by and grounded in the human body and brain. This embodied cognition\nperspective also explains why music structures modulate kinetic and\nsomatosensory perception. We leverage this aspect of cognition, by considering\ndance as a proxy for music perception, in a statistical computational model\nthat learns semiotic correlations between music audio and dance video. We\nevaluate the ability of this model to effectively capture underlying semantics\nin a cross-modal retrieval task. Quantitative results, validated with\nstatistical significance testing, strengthen the body of evidence for embodied\ncognition in music and show the model can recommend music audio for dance video\nqueries and vice-versa.\n", "versions": [{"version": "v1", "created": "Mon, 25 Mar 2019 18:09:03 GMT"}], "update_date": "2019-03-27", "authors_parsed": [["Raposo", "Francisco Afonso", ""], ["de Matos", "David Martins", ""], ["Ribeiro", "Ricardo", ""]]}, {"id": "1903.10543", "submitter": "Muhamad Risqi U. Saputra", "authors": "Muhamad Risqi U. Saputra, Pedro P. B. de Gusmao, Sen Wang, Andrew\n  Markham, Niki Trigoni", "title": "Learning Monocular Visual Odometry through Geometry-Aware Curriculum\n  Learning", "comments": "accepted in IEEE ICRA 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inspired by the cognitive process of humans and animals, Curriculum Learning\n(CL) trains a model by gradually increasing the difficulty of the training\ndata. In this paper, we study whether CL can be applied to complex geometry\nproblems like estimating monocular Visual Odometry (VO). Unlike existing CL\napproaches, we present a novel CL strategy for learning the geometry of\nmonocular VO by gradually making the learning objective more difficult during\ntraining. To this end, we propose a novel geometry-aware objective function by\njointly optimizing relative and composite transformations over small windows\nvia bounded pose regression loss. A cascade optical flow network followed by\nrecurrent network with a differentiable windowed composition layer, termed\nCL-VO, is devised to learn the proposed objective. Evaluation on three\nreal-world datasets shows superior performance of CL-VO over state-of-the-art\nfeature-based and learning-based VO.\n", "versions": [{"version": "v1", "created": "Mon, 25 Mar 2019 18:26:06 GMT"}, {"version": "v2", "created": "Tue, 5 Nov 2019 21:22:03 GMT"}], "update_date": "2019-11-07", "authors_parsed": [["Saputra", "Muhamad Risqi U.", ""], ["de Gusmao", "Pedro P. B.", ""], ["Wang", "Sen", ""], ["Markham", "Andrew", ""], ["Trigoni", "Niki", ""]]}, {"id": "1903.10547", "submitter": "Yao-Hung Tsai", "authors": "Yao-Hung Hubert Tsai and Santosh Divvala and Louis-Philippe Morency\n  and Ruslan Salakhutdinov and Ali Farhadi", "title": "Video Relationship Reasoning using Gated Spatio-Temporal Energy Graph", "comments": "CVPR 2019. Supplementary included. Fixing a small typo", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual relationship reasoning is a crucial yet challenging task for\nunderstanding rich interactions across visual concepts. For example, a\nrelationship 'man, open, door' involves a complex relation 'open' between\nconcrete entities 'man, door'. While much of the existing work has studied this\nproblem in the context of still images, understanding visual relationships in\nvideos has received limited attention. Due to their temporal nature, videos\nenable us to model and reason about a more comprehensive set of visual\nrelationships, such as those requiring multiple (temporal) observations (e.g.,\n'man, lift up, box' vs. 'man, put down, box'), as well as relationships that\nare often correlated through time (e.g., 'woman, pay, money' followed by\n'woman, buy, coffee'). In this paper, we construct a Conditional Random Field\non a fully-connected spatio-temporal graph that exploits the statistical\ndependency between relational entities spatially and temporally. We introduce a\nnovel gated energy function parametrization that learns adaptive relations\nconditioned on visual observations. Our model optimization is computationally\nefficient, and its space computation complexity is significantly amortized\nthrough our proposed parameterization. Experimental results on benchmark video\ndatasets (ImageNet Video and Charades) demonstrate state-of-the-art performance\nacross three standard relationship reasoning tasks: Detection, Tagging, and\nRecognition.\n", "versions": [{"version": "v1", "created": "Mon, 25 Mar 2019 18:41:06 GMT"}, {"version": "v2", "created": "Wed, 27 Mar 2019 15:01:34 GMT"}], "update_date": "2019-03-28", "authors_parsed": [["Tsai", "Yao-Hung Hubert", ""], ["Divvala", "Santosh", ""], ["Morency", "Louis-Philippe", ""], ["Salakhutdinov", "Ruslan", ""], ["Farhadi", "Ali", ""]]}, {"id": "1903.10554", "submitter": "Jake Sganga", "authors": "Jake Sganga, David Eng, Chauncey Graetzel, David B. Camarillo", "title": "Deep Learning for Localization in the Lung", "comments": "35 pages double-spaced, 8 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Lung cancer is the leading cause of cancer-related death worldwide, and early\ndiagnosis is critical to improving patient outcomes. To diagnose cancer, a\nhighly trained pulmonologist must navigate a flexible bronchoscope deep into\nthe branched structure of the lung for biopsy. The biopsy fails to sample the\ntarget tissue in 26-33% of cases largely because of poor registration with the\npreoperative CT map. We developed two deep learning approaches to localize the\nbronchoscope in the preoperative CT map in real time and tested the algorithms\nacross 13 trajectories in a lung phantom and 68 trajectories in 11 human\ncadaver lungs. In the lung phantom, we observe performance reaching 95%\nprecision and recall of visible airways and 3 mm average position error. On a\nsuccessful cadaver lung sequence, the algorithms trained on simulation alone\nachieved 77%-94% precision and recall of visible airways and 4-6 mm average\nposition error. We also compare the effect of GAN-stylizing images and we look\nat aggregate statistics over the entire set of trajectories.\n", "versions": [{"version": "v1", "created": "Mon, 25 Mar 2019 19:11:39 GMT"}], "update_date": "2019-03-27", "authors_parsed": [["Sganga", "Jake", ""], ["Eng", "David", ""], ["Graetzel", "Chauncey", ""], ["Camarillo", "David B.", ""]]}, {"id": "1903.10578", "submitter": "David Hachuel", "authors": "David Hachuel, Akshay Jha, Deborah Estrin, Alfonso Martinez, Kyle\n  Staller, Christopher Velez", "title": "Augmenting Gastrointestinal Health: A Deep Learning Approach to Human\n  Stool Recognition and Characterization in Macroscopic Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Purpose - Functional bowel diseases, including irritable bowel syndrome,\nchronic constipation, and chronic diarrhea, are some of the most common\ndiseases seen in clinical practice. Many patients describe a range of triggers\nfor altered bowel consistency and symptoms. However, characterization of the\nrelationship between symptom triggers using bowel diaries is hampered by poor\ncompliance and lack of objective stool consistency measurements. We sought to\ndevelop a stool detection and tracking system using computer vision and deep\nconvolutional neural networks (CNN) that could be used by patients, providers,\nand researchers in the assessment of chronic gastrointestinal (GI) disease.\n", "versions": [{"version": "v1", "created": "Mon, 25 Mar 2019 20:08:17 GMT"}], "update_date": "2019-03-27", "authors_parsed": [["Hachuel", "David", ""], ["Jha", "Akshay", ""], ["Estrin", "Deborah", ""], ["Martinez", "Alfonso", ""], ["Staller", "Kyle", ""], ["Velez", "Christopher", ""]]}, {"id": "1903.10588", "submitter": "Zonglin Yang", "authors": "Zonglin Yang, Xinggang Wang", "title": "Reducing the dilution: An analysis of the information sensitiveness of\n  capsule network with a practical improvement method", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Capsule network has shown various advantages over convolutional neural\nnetwork (CNN). It keeps more precise spatial information than CNN and uses\nequivariance instead of invariance during inference and highly potential to be\na new effective tool for visual tasks. However, the current capsule networks\nhave incompatible performance with CNN when facing datasets with background and\ncomplex target objects and are lacking in universal and efficient\nregularization method.\n  We analyze a main reason of the incompatible performance as the conflict\nbetween information sensitiveness of capsule network and unreasonably higher\nactivation value distribution of capsules in primary capsule layer.\nCorrespondingly, we propose a practical improvement method by restraining the\nactivation value of capsules in primary capsule layer to suppress\nnon-informative capsules and highlight discriminative capsules. In the\nexperiments, the method has achieved better performances on various mainstream\ndatasets. In addition, the proposed improvement methods can be seen as a\nsuitable, simple and efficient regularization method that can be generally used\nin capsule network.\n", "versions": [{"version": "v1", "created": "Mon, 25 Mar 2019 20:28:44 GMT"}, {"version": "v2", "created": "Wed, 27 Mar 2019 13:54:26 GMT"}, {"version": "v3", "created": "Thu, 2 May 2019 21:42:46 GMT"}], "update_date": "2019-05-06", "authors_parsed": [["Yang", "Zonglin", ""], ["Wang", "Xinggang", ""]]}, {"id": "1903.10601", "submitter": "Qian Wang", "authors": "Qian Wang, Penghui Bu, Toby P. Breckon", "title": "Unifying Unsupervised Domain Adaptation and Zero-Shot Visual Recognition", "comments": "International Joint Conference on Neural Networks 2019, Budapest", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unsupervised domain adaptation aims to transfer knowledge from a source\ndomain to a target domain so that the target domain data can be recognized\nwithout any explicit labelling information for this domain. One limitation of\nthe problem setting is that testing data, despite having no labels, from the\ntarget domain is needed during training, which prevents the trained model being\ndirectly applied to classify unseen test instances. We formulate a new\ncross-domain classification problem arising from real-world scenarios where\nlabelled data is available for a subset of classes (known classes) in the\ntarget domain, and we expect to recognize new samples belonging to any class\n(known and unseen classes) once the model is learned. This is a generalized\nzero-shot learning problem where the side information comes from the source\ndomain in the form of labelled samples instead of class-level semantic\nrepresentations commonly used in traditional zero-shot learning. We present a\nunified domain adaptation framework for both unsupervised and zero-shot\nlearning conditions. Our approach learns a joint subspace from source and\ntarget domains so that the projections of both data in the subspace can be\ndomain invariant and easily separable. We use the supervised locality\npreserving projection (SLPP) as the enabling technique and conduct experiments\nunder both unsupervised and zero-shot learning conditions, achieving\nstate-of-the-art results on three domain adaptation benchmark datasets:\nOffice-Caltech, Office31 and Office-Home.\n", "versions": [{"version": "v1", "created": "Mon, 25 Mar 2019 21:34:39 GMT"}, {"version": "v2", "created": "Mon, 26 Aug 2019 11:56:24 GMT"}], "update_date": "2019-08-27", "authors_parsed": [["Wang", "Qian", ""], ["Bu", "Penghui", ""], ["Breckon", "Toby P.", ""]]}, {"id": "1903.10604", "submitter": "Qian Wang", "authors": "Qian Wang, Khalid N. Ismail, Toby P. Breckon", "title": "An Approach for Adaptive Automatic Threat Recognition Within 3D Computed\n  Tomography Images for Baggage Security Screening", "comments": "Technical Report, Durham University", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The screening of baggage using X-ray scanners is now routine in aviation\nsecurity with automatic threat detection approaches, based on 3D X-ray computed\ntomography (CT) images, known as Automatic Threat Recognition (ATR) within the\naviation security industry. These current strategies use pre-defined threat\nmaterial signatures in contrast to adaptability towards new and emerging threat\nsignatures. To address this issue, the concept of adaptive automatic threat\nrecognition (AATR) was proposed in previous work. In this paper, we present a\nsolution to AATR based on such X-ray CT baggage scan imagery. This aims to\naddress the issues of rapidly evolving threat signatures within the screening\nrequirements. Ideally, the detection algorithms deployed within the security\nscanners should be readily adaptable to different situations with varying\nrequirements of threat characteristics (e.g., threat material, physical\nproperties of objects). We tackle this issue using a novel adaptive machine\nlearning methodology with our solution consisting of a multi-scale 3D CT image\nsegmentation algorithm, a multi-class support vector machine (SVM) classifier\nfor object material recognition and a strategy to enable the adaptability of\nour approach. Experiments are conducted on both open and sequestered 3D CT\nbaggage image datasets specifically collected for the AATR study. Our proposed\napproach performs well on both recognition and adaptation. Overall our approach\ncan achieve the probability of detection around 90% with a probability of false\nalarm below 20%. Our AATR shows the capabilities of adapting to varying types\nof materials, even the unknown materials which are not available in the\ntraining data, adapting to varying required probability of detection and\nadapting to varying scales of the threat object.\n", "versions": [{"version": "v1", "created": "Mon, 25 Mar 2019 21:46:30 GMT"}, {"version": "v2", "created": "Mon, 18 Nov 2019 22:13:49 GMT"}], "update_date": "2019-11-20", "authors_parsed": [["Wang", "Qian", ""], ["Ismail", "Khalid N.", ""], ["Breckon", "Toby P.", ""]]}, {"id": "1903.10641", "submitter": "Jatavallabhula Krishna Murthy", "authors": "Shashank Srikanth and Junaid Ahmed Ansari and Karnik Ram R and Sarthak\n  Sharma and Krishna Murthy J. and Madhava Krishna K", "title": "INFER: INtermediate representations for FuturE pRediction", "comments": "Manuscript under review. Submitted to IROS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In urban driving scenarios, forecasting future trajectories of surrounding\nvehicles is of paramount importance. While several approaches for the problem\nhave been proposed, the best-performing ones tend to require extremely detailed\ninput representations (eg. image sequences). But, such methods do not\ngeneralize to datasets they have not been trained on. We propose intermediate\nrepresentations that are particularly well-suited for future prediction. As\nopposed to using texture (color) information, we rely on semantics and train an\nautoregressive model to accurately predict future trajectories of traffic\nparticipants (vehicles) (see fig. above). We demonstrate that using semantics\nprovides a significant boost over techniques that operate over raw pixel\nintensities/disparities. Uncharacteristic of state-of-the-art approaches, our\nrepresentations and models generalize to completely different datasets,\ncollected across several cities, and also across countries where people drive\non opposite sides of the road (left-handed vs right-handed driving).\nAdditionally, we demonstrate an application of our approach in multi-object\ntracking (data association). To foster further research in transferrable\nrepresentations and ensure reproducibility, we release all our code and data.\n", "versions": [{"version": "v1", "created": "Tue, 26 Mar 2019 00:32:02 GMT"}], "update_date": "2019-03-27", "authors_parsed": [["Srikanth", "Shashank", ""], ["Ansari", "Junaid Ahmed", ""], ["R", "Karnik Ram", ""], ["Sharma", "Sarthak", ""], ["J.", "Krishna Murthy", ""], ["K", "Madhava Krishna", ""]]}, {"id": "1903.10645", "submitter": "Fengze Liu", "authors": "Fengze Liu, Yingda Xia, Dong Yang, Alan Yuille, Daguang Xu", "title": "An Alarm System For Segmentation Algorithm Based On Shape Model", "comments": "Accepted to ICCV 2019 (10 pages, 4 figures)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is usually hard for a learning system to predict correctly on rare events\nthat never occur in the training data, and there is no exception for\nsegmentation algorithms. Meanwhile, manual inspection of each case to locate\nthe failures becomes infeasible due to the trend of large data scale and\nlimited human resource. Therefore, we build an alarm system that will set off\nalerts when the segmentation result is possibly unsatisfactory, assuming no\ncorresponding ground truth mask is provided. One plausible solution is to\nproject the segmentation results into a low dimensional feature space; then\nlearn classifiers/regressors to predict their qualities. Motivated by this, in\nthis paper, we learn a feature space using the shape information which is a\nstrong prior shared among different datasets and robust to the appearance\nvariation of input data.The shape feature is captured using a Variational\nAuto-Encoder (VAE) network that trained with only the ground truth masks.\nDuring testing, the segmentation results with bad shapes shall not fit the\nshape prior well, resulting in large loss values. Thus, the VAE is able to\nevaluate the quality of segmentation result on unseen data, without using\nground truth. Finally, we learn a regressor in the one-dimensional feature\nspace to predict the qualities of segmentation results. Our alarm system is\nevaluated on several recent state-of-art segmentation algorithms for 3D medical\nsegmentation tasks. Compared with other standard quality assessment methods,\nour system consistently provides more reliable prediction on the qualities of\nsegmentation results.\n", "versions": [{"version": "v1", "created": "Tue, 26 Mar 2019 00:55:49 GMT"}, {"version": "v2", "created": "Wed, 27 Mar 2019 03:02:29 GMT"}, {"version": "v3", "created": "Wed, 21 Aug 2019 00:26:53 GMT"}], "update_date": "2019-08-22", "authors_parsed": [["Liu", "Fengze", ""], ["Xia", "Yingda", ""], ["Yang", "Dong", ""], ["Yuille", "Alan", ""], ["Xu", "Daguang", ""]]}, {"id": "1903.10657", "submitter": "Chao Zhang", "authors": "Takumi Nakane, Takuya Akashi, Xuequan Lu, Chao Zhang", "title": "A Probabilistic Bitwise Genetic Algorithm for B-Spline based Image\n  Deformation Estimation", "comments": "GECCO2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel genetic algorithm to solve the image deformation\nestimation problem by preserving the genetic diversity. As a classical problem,\nthere is always a trade-off between the complexity of deformation models and\nthe difficulty of parameters search in image deformation. 2D cubic B-spline\nsurface is a highly free-form deformation model and is able to handle complex\ndeformations such as fluid image distortions. However, it is challenging to\nestimate an apposite global solution. To tackle this problem, we develop a\ngenetic operation named probabilistic bitwise operation (PBO) to replace the\ncrossover and mutation operations, which can preserve the diversity during\ngeneration iteration and achieve better coverage ratio of the solution space.\nFurthermore, a selection strategy named annealing selection is proposed to\ncontrol the convergence. Qualitative and quantitative results on synthetic data\nshow the effectiveness of our method.\n", "versions": [{"version": "v1", "created": "Tue, 26 Mar 2019 02:24:07 GMT"}], "update_date": "2019-03-27", "authors_parsed": [["Nakane", "Takumi", ""], ["Akashi", "Takuya", ""], ["Lu", "Xuequan", ""], ["Zhang", "Chao", ""]]}, {"id": "1903.10658", "submitter": "Jiuxiang Gu Mr", "authors": "Jiuxiang Gu, Shafiq Joty, Jianfei Cai, Handong Zhao, Xu Yang, Gang\n  Wang", "title": "Unpaired Image Captioning via Scene Graph Alignments", "comments": "Accepted in ICCV 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most of current image captioning models heavily rely on paired image-caption\ndatasets. However, getting large scale image-caption paired data is\nlabor-intensive and time-consuming. In this paper, we present a scene\ngraph-based approach for unpaired image captioning. Our framework comprises an\nimage scene graph generator, a sentence scene graph generator, a scene graph\nencoder, and a sentence decoder. Specifically, we first train the scene graph\nencoder and the sentence decoder on the text modality. To align the scene\ngraphs between images and sentences, we propose an unsupervised feature\nalignment method that maps the scene graph features from the image to the\nsentence modality. Experimental results show that our proposed model can\ngenerate quite promising results without using any image-caption training\npairs, outperforming existing methods by a wide margin.\n", "versions": [{"version": "v1", "created": "Tue, 26 Mar 2019 02:52:22 GMT"}, {"version": "v2", "created": "Thu, 4 Apr 2019 07:15:26 GMT"}, {"version": "v3", "created": "Wed, 14 Aug 2019 07:49:54 GMT"}, {"version": "v4", "created": "Sat, 17 Aug 2019 03:27:53 GMT"}], "update_date": "2019-08-20", "authors_parsed": [["Gu", "Jiuxiang", ""], ["Joty", "Shafiq", ""], ["Cai", "Jianfei", ""], ["Zhao", "Handong", ""], ["Yang", "Xu", ""], ["Wang", "Gang", ""]]}, {"id": "1903.10661", "submitter": "Zhiwei Liu", "authors": "Zhiwei Liu, Xiangyu Zhu, Guosheng Hu, Haiyun Guo, Ming Tang, Zhen Lei,\n  Neil M. Robertson, Jinqiao Wang", "title": "Semantic Alignment: Finding Semantically Consistent Ground-truth for\n  Facial Landmark Detection", "comments": "Accepted at CVPR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, deep learning based facial landmark detection has achieved great\nsuccess. Despite this, we notice that the semantic ambiguity greatly degrades\nthe detection performance. Specifically, the semantic ambiguity means that some\nlandmarks (e.g. those evenly distributed along the face contour) do not have\nclear and accurate definition, causing inconsistent annotations by annotators.\nAccordingly, these inconsistent annotations, which are usually provided by\npublic databases, commonly work as the ground-truth to supervise network\ntraining, leading to the degraded accuracy. To our knowledge, little research\nhas investigated this problem. In this paper, we propose a novel probabilistic\nmodel which introduces a latent variable, i.e. the 'real' ground-truth which is\nsemantically consistent, to optimize. This framework couples two parts (1)\ntraining landmark detection CNN and (2) searching the 'real' ground-truth.\nThese two parts are alternatively optimized: the searched 'real' ground-truth\nsupervises the CNN training; and the trained CNN assists the searching of\n'real' ground-truth. In addition, to recover the unconfidently predicted\nlandmarks due to occlusion and low quality, we propose a global heatmap\ncorrection unit (GHCU) to correct outliers by considering the global face shape\nas a constraint. Extensive experiments on both image-based (300W and AFLW) and\nvideo-based (300-VW) databases demonstrate that our method effectively improves\nthe landmark detection accuracy and achieves the state of the art performance.\n", "versions": [{"version": "v1", "created": "Tue, 26 Mar 2019 03:19:42 GMT"}], "update_date": "2019-03-27", "authors_parsed": [["Liu", "Zhiwei", ""], ["Zhu", "Xiangyu", ""], ["Hu", "Guosheng", ""], ["Guo", "Haiyun", ""], ["Tang", "Ming", ""], ["Lei", "Zhen", ""], ["Robertson", "Neil M.", ""], ["Wang", "Jinqiao", ""]]}, {"id": "1903.10663", "submitter": "ByungSoo Ko", "authors": "HeeJae Jun, Byungsoo Ko, Youngjoon Kim, Insik Kim, Jongtack Kim", "title": "Combination of Multiple Global Descriptors for Image Retrieval", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent studies in image retrieval task have shown that ensembling different\nmodels and combining multiple global descriptors lead to performance\nimprovement. However, training different models for the ensemble is not only\ndifficult but also inefficient with respect to time and memory. In this paper,\nwe propose a novel framework that exploits multiple global descriptors to get\nan ensemble effect while it can be trained in an end-to-end manner. The\nproposed framework is flexible and expandable by the global descriptor, CNN\nbackbone, loss, and dataset. Moreover, we investigate the effectiveness of\ncombining multiple global descriptors with quantitative and qualitative\nanalysis. Our extensive experiments show that the combined descriptor\noutperforms a single global descriptor, as it can utilize different types of\nfeature properties. In the benchmark evaluation, the proposed framework\nachieves the state-of-the-art performance on the CARS196, CUB200-2011, In-shop\nClothes, and Stanford Online Products on image retrieval tasks. Our model\nimplementations and pretrained models are publicly available.\n", "versions": [{"version": "v1", "created": "Tue, 26 Mar 2019 03:38:38 GMT"}, {"version": "v2", "created": "Thu, 18 Apr 2019 05:33:04 GMT"}, {"version": "v3", "created": "Sat, 27 Jul 2019 05:28:41 GMT"}, {"version": "v4", "created": "Thu, 23 Apr 2020 06:20:02 GMT"}], "update_date": "2020-04-24", "authors_parsed": [["Jun", "HeeJae", ""], ["Ko", "Byungsoo", ""], ["Kim", "Youngjoon", ""], ["Kim", "Insik", ""], ["Kim", "Jongtack", ""]]}, {"id": "1903.10667", "submitter": "Chao Zhang", "authors": "Chunzhi Gu, Xuequan Lu, Ying He, Chao Zhang", "title": "Blur Removal via Blurred-Noisy Image Pair", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Complex blur such as the mixup of space-variant and space-invariant blur,\nwhich is hard to model mathematically, widely exists in real images. In this\npaper, we propose a novel image deblurring method that does not need to\nestimate blur kernels. We utilize a pair of images that can be easily acquired\nin low-light situations: (1) a blurred image taken with low shutter speed and\nlow ISO noise; and (2) a noisy image captured with high shutter speed and high\nISO noise. Slicing the blurred image into patches, we extend the Gaussian\nmixture model (GMM) to model the underlying intensity distribution of each\npatch using the corresponding patches in the noisy image. We compute patch\ncorrespondences by analyzing the optical flow between the two images. The\nExpectation Maximization (EM) algorithm is utilized to estimate the parameters\nof GMM. To preserve sharp features, we add an additional bilateral term to the\nobjective function in the M-step. We eventually add a detail layer to the\ndeblurred image for refinement. Extensive experiments on both synthetic and\nreal-world data demonstrate that our method outperforms state-of-the-art\ntechniques, in terms of robustness, visual quality, and quantitative metrics.\n", "versions": [{"version": "v1", "created": "Tue, 26 Mar 2019 03:47:03 GMT"}, {"version": "v2", "created": "Wed, 27 Mar 2019 01:03:40 GMT"}, {"version": "v3", "created": "Mon, 13 Jan 2020 09:55:08 GMT"}, {"version": "v4", "created": "Mon, 16 Nov 2020 06:41:59 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["Gu", "Chunzhi", ""], ["Lu", "Xuequan", ""], ["He", "Ying", ""], ["Zhang", "Chao", ""]]}, {"id": "1903.10683", "submitter": "Xiaowei Hu", "authors": "Xiaowei Hu, Yitong Jiang, Chi-Wing Fu, Pheng-Ann Heng", "title": "Mask-ShadowGAN: Learning to Remove Shadows from Unpaired Data", "comments": "Accepted to ICCV 2019", "journal-ref": "IEEE International Conference on Computer Vision (ICCV), pp.\n  2472-2481, 2019", "doi": "10.1109/ICCV.2019.00256", "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a new method for shadow removal using unpaired data,\nenabling us to avoid tedious annotations and obtain more diverse training\nsamples. However, directly employing adversarial learning and cycle-consistency\nconstraints is insufficient to learn the underlying relationship between the\nshadow and shadow-free domains, since the mapping between shadow and\nshadow-free images is not simply one-to-one. To address the problem, we\nformulate Mask-ShadowGAN, a new deep framework that automatically learns to\nproduce a shadow mask from the input shadow image and then takes the mask to\nguide the shadow generation via re-formulated cycle-consistency constraints.\nParticularly, the framework simultaneously learns to produce shadow masks and\nlearns to remove shadows, to maximize the overall performance. Also, we\nprepared an unpaired dataset for shadow removal and demonstrated the\neffectiveness of Mask-ShadowGAN on various experiments, even it was trained on\nunpaired data.\n", "versions": [{"version": "v1", "created": "Tue, 26 Mar 2019 05:30:55 GMT"}, {"version": "v2", "created": "Tue, 30 Jul 2019 03:48:11 GMT"}, {"version": "v3", "created": "Thu, 8 Aug 2019 13:25:52 GMT"}], "update_date": "2020-05-19", "authors_parsed": [["Hu", "Xiaowei", ""], ["Jiang", "Yitong", ""], ["Fu", "Chi-Wing", ""], ["Heng", "Pheng-Ann", ""]]}, {"id": "1903.10744", "submitter": "Ebenezer Isaac", "authors": "Ebenezer R.H.P. Isaac, Susan Elias, Srinivasan Rajagopalan, and K.S.\n  Easwarakumar", "title": "Trait of Gait: A Survey on Gait Biometrics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gait analysis is the study of the systematic methods that assess and quantify\nanimal locomotion. The research on gait analysis has considerably evolved\nthrough time. It was an ancient art, and it still finds its application today\nin modern science and medicine. This paper describes how one's gait can be used\nas a biometric. It shall diversely cover salient research done within the field\nand explain the nuances and advances in each type of gait analysis. The\nprominent methods of gait recognition from the early era to the state of the\nart are covered. This survey also reviews the various gait datasets. The\noverall aim of this study is to provide a concise roadmap for anyone who wishes\nto do research in the field of gait biometrics.\n", "versions": [{"version": "v1", "created": "Tue, 26 Mar 2019 09:11:18 GMT"}], "update_date": "2019-03-27", "authors_parsed": [["Isaac", "Ebenezer R. H. P.", ""], ["Elias", "Susan", ""], ["Rajagopalan", "Srinivasan", ""], ["Easwarakumar", "K. S.", ""]]}, {"id": "1903.10750", "submitter": "Jie Zhou", "authors": "Jie Zhou, Xin Tan, Zhiwei Shao, Lizhuang Ma", "title": "FVNet: 3D Front-View Proposal Generation for Real-Time Object Detection\n  from Point Clouds", "comments": "10 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  3D object detection from raw and sparse point clouds has been far less\ntreated to date, compared with its 2D counterpart. In this paper, we propose a\nnovel framework called FVNet for 3D front-view proposal generation and object\ndetection from point clouds. It consists of two stages: generation of\nfront-view proposals and estimation of 3D bounding box parameters. Instead of\ngenerating proposals from camera images or bird's-eye-view maps, we first\nproject point clouds onto a cylindrical surface to generate front-view feature\nmaps which retains rich information. We then introduce a proposal generation\nnetwork to predict 3D region proposals from the generated maps and further\nextrude objects of interest from the whole point cloud. Finally, we present\nanother network to extract the point-wise features from the extruded object\npoints and regress the final 3D bounding box parameters in the canonical\ncoordinates. Our framework achieves real-time performance with 12ms per point\ncloud sample. Extensive experiments on the 3D detection benchmark KITTI show\nthat the proposed architecture outperforms state-of-the-art techniques which\ntake either camera images or point clouds as input, in terms of accuracy and\ninference time.\n", "versions": [{"version": "v1", "created": "Tue, 26 Mar 2019 09:26:46 GMT"}, {"version": "v2", "created": "Fri, 26 Apr 2019 08:51:47 GMT"}, {"version": "v3", "created": "Tue, 26 Nov 2019 07:54:29 GMT"}], "update_date": "2019-11-27", "authors_parsed": [["Zhou", "Jie", ""], ["Tan", "Xin", ""], ["Shao", "Zhiwei", ""], ["Ma", "Lizhuang", ""]]}, {"id": "1903.10762", "submitter": "Talha Qaiser", "authors": "Talha Qaiser, Nasir M. Rajpoot", "title": "Learning Where to See: A Novel Attention Model for Automated\n  Immunohistochemical Scoring", "comments": null, "journal-ref": null, "doi": "10.1109/TMI.2019.2907049", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimating over-amplification of human epidermal growth factor receptor 2\n(HER2) on invasive breast cancer (BC) is regarded as a significant predictive\nand prognostic marker. We propose a novel deep reinforcement learning (DRL)\nbased model that treats immunohistochemical (IHC) scoring of HER2 as a\nsequential learning task. For a given image tile sampled from multi-resolution\ngiga-pixel whole slide image (WSI), the model learns to sequentially identify\nsome of the diagnostically relevant regions of interest (ROIs) by following a\nparameterized policy. The selected ROIs are processed by recurrent and residual\nconvolution networks to learn the discriminative features for different HER2\nscores and predict the next location, without requiring to process all the\nsub-image patches of a given tile for predicting the HER2 score, mimicking the\nhistopathologist who would not usually analyze every part of the slide at the\nhighest magnification. The proposed model incorporates a task-specific\nregularization term and inhibition of return mechanism to prevent the model\nfrom revisiting the previously attended locations. We evaluated our model on\ntwo IHC datasets: a publicly available dataset from the HER2 scoring challenge\ncontest and another dataset consisting of WSIs of gastroenteropancreatic\nneuroendocrine tumor sections stained with Glo1 marker. We demonstrate that the\nproposed model outperforms other methods based on state-of-the-art deep\nconvolutional networks. To the best of our knowledge, this is the first study\nusing DRL for IHC scoring and could potentially lead to wider use of DRL in the\ndomain of computational pathology reducing the computational burden of the\nanalysis of large multigigapixel histology images.\n", "versions": [{"version": "v1", "created": "Tue, 26 Mar 2019 09:53:36 GMT"}], "update_date": "2019-03-27", "authors_parsed": [["Qaiser", "Talha", ""], ["Rajpoot", "Nasir M.", ""]]}, {"id": "1903.10764", "submitter": "Amir Atapour Abarghouei", "authors": "Amir Atapour-Abarghouei and Toby P. Breckon", "title": "Veritatem Dies Aperit- Temporally Consistent Depth Prediction Enabled by\n  a Multi-Task Geometric and Semantic Scene Understanding Approach", "comments": "CVPR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Robust geometric and semantic scene understanding is ever more important in\nmany real-world applications such as autonomous driving and robotic navigation.\nIn this paper, we propose a multi-task learning-based approach capable of\njointly performing geometric and semantic scene understanding, namely depth\nprediction (monocular depth estimation and depth completion) and semantic scene\nsegmentation. Within a single temporally constrained recurrent network, our\napproach uniquely takes advantage of a complex series of skip connections,\nadversarial training and the temporal constraint of sequential frame recurrence\nto produce consistent depth and semantic class labels simultaneously. Extensive\nexperimental evaluation demonstrates the efficacy of our approach compared to\nother contemporary state-of-the-art techniques.\n", "versions": [{"version": "v1", "created": "Tue, 26 Mar 2019 09:59:46 GMT"}, {"version": "v2", "created": "Fri, 19 Jul 2019 09:28:14 GMT"}], "update_date": "2019-07-22", "authors_parsed": [["Atapour-Abarghouei", "Amir", ""], ["Breckon", "Toby P.", ""]]}, {"id": "1903.10765", "submitter": "Michiel Verburg", "authors": "Michiel Verburg, Vlado Menkovski", "title": "Micro-expression detection in long videos using optical flow and\n  recurrent neural networks", "comments": "6 pages, 6 figures and 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Facial micro-expressions are subtle and involuntary expressions that can\nreveal concealed emotions. Micro-expressions are an invaluable source of\ninformation in application domains such as lie detection, mental health,\nsentiment analysis and more. One of the biggest challenges in this field of\nresearch is the small amount of available spontaneous micro-expression data.\nHowever, spontaneous data collection is burdened by time-consuming and\nexpensive annotation. Hence, methods are needed which can reduce the amount of\ndata that annotators have to review. This paper presents a novel\nmicro-expression spotting method using a recurrent neural network (RNN) on\noptical flow features. We extract Histogram of Oriented Optical Flow (HOOF)\nfeatures to encode the temporal changes in selected face regions. Finally, the\nRNN spots short intervals which are likely to contain occurrences of relevant\nfacial micro-movements. The proposed method is evaluated on the SAMM database.\nAny chance of subject bias is eliminated by training the RNN using\nLeave-One-Subject-Out cross-validation. Comparing the spotted intervals with\nthe labeled data shows that the method produced 1569 false positives while\nobtaining a recall of 0.4654. The initial results show that the proposed method\nwould reduce the video length by a factor of 3.5, while still retaining almost\nhalf of the relevant micro-movements. Lastly, as the model gets more data, it\nbecomes better at detecting intervals, which makes the proposed method suitable\nfor supporting the annotation process.\n", "versions": [{"version": "v1", "created": "Tue, 26 Mar 2019 10:01:46 GMT"}], "update_date": "2019-03-27", "authors_parsed": [["Verburg", "Michiel", ""], ["Menkovski", "Vlado", ""]]}, {"id": "1903.10826", "submitter": "Yujia Liu", "authors": "Yujia Liu, Seyed-Mohsen Moosavi-Dezfooli, Pascal Frossard", "title": "A geometry-inspired decision-based attack", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks have recently achieved tremendous success in image\nclassification. Recent studies have however shown that they are easily misled\ninto incorrect classification decisions by adversarial examples. Adversaries\ncan even craft attacks by querying the model in black-box settings, where no\ninformation about the model is released except its final decision. Such\ndecision-based attacks usually require lots of queries, while real-world image\nrecognition systems might actually restrict the number of queries. In this\npaper, we propose qFool, a novel decision-based attack algorithm that can\ngenerate adversarial examples using a small number of queries. The qFool method\ncan drastically reduce the number of queries compared to previous\ndecision-based attacks while reaching the same quality of adversarial examples.\nWe also enhance our method by constraining adversarial perturbations in\nlow-frequency subspace, which can make qFool even more computationally\nefficient. Altogether, we manage to fool commercial image recognition systems\nwith a small number of queries, which demonstrates the actual effectiveness of\nour new algorithm in practice.\n", "versions": [{"version": "v1", "created": "Tue, 26 Mar 2019 12:18:31 GMT"}], "update_date": "2019-03-27", "authors_parsed": [["Liu", "Yujia", ""], ["Moosavi-Dezfooli", "Seyed-Mohsen", ""], ["Frossard", "Pascal", ""]]}, {"id": "1903.10829", "submitter": "Hyun Jae Lee", "authors": "HyunJae Lee, Hyo-Eun Kim, Hyeonseob Nam", "title": "SRM : A Style-based Recalibration Module for Convolutional Neural\n  Networks", "comments": "11 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Following the advance of style transfer with Convolutional Neural Networks\n(CNNs), the role of styles in CNNs has drawn growing attention from a broader\nperspective. In this paper, we aim to fully leverage the potential of styles to\nimprove the performance of CNNs in general vision tasks. We propose a\nStyle-based Recalibration Module (SRM), a simple yet effective architectural\nunit, which adaptively recalibrates intermediate feature maps by exploiting\ntheir styles. SRM first extracts the style information from each channel of the\nfeature maps by style pooling, then estimates per-channel recalibration weight\nvia channel-independent style integration. By incorporating the relative\nimportance of individual styles into feature maps, SRM effectively enhances the\nrepresentational ability of a CNN. The proposed module is directly fed into\nexisting CNN architectures with negligible overhead. We conduct comprehensive\nexperiments on general image recognition as well as tasks related to styles,\nwhich verify the benefit of SRM over recent approaches such as\nSqueeze-and-Excitation (SE). To explain the inherent difference between SRM and\nSE, we provide an in-depth comparison of their representational properties.\n", "versions": [{"version": "v1", "created": "Tue, 26 Mar 2019 12:23:52 GMT"}], "update_date": "2019-03-27", "authors_parsed": [["Lee", "HyunJae", ""], ["Kim", "Hyo-Eun", ""], ["Nam", "Hyeonseob", ""]]}, {"id": "1903.10830", "submitter": "Stefan Popov", "authors": "Rodrigo Benenson, Stefan Popov, Vittorio Ferrari", "title": "Large-scale interactive object segmentation with human annotators", "comments": "Accepted at CVPR2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Manually annotating object segmentation masks is very time consuming.\nInteractive object segmentation methods offer a more efficient alternative\nwhere a human annotator and a machine segmentation model collaborate. In this\npaper we make several contributions to interactive segmentation: (1) we\nsystematically explore in simulation the design space of deep interactive\nsegmentation models and report new insights and caveats; (2) we execute a\nlarge-scale annotation campaign with real human annotators, producing masks for\n2.5M instances on the OpenImages dataset. We plan to release this data\npublicly, forming the largest existing dataset for instance segmentation.\nMoreover, by re-annotating part of the COCO dataset, we show that we can\nproduce instance masks 3 times faster than traditional polygon drawing tools\nwhile also providing better quality. (3) We present a technique for\nautomatically estimating the quality of the produced masks which exploits\nindirect signals from the annotation process.\n", "versions": [{"version": "v1", "created": "Tue, 26 Mar 2019 12:26:38 GMT"}, {"version": "v2", "created": "Wed, 17 Apr 2019 15:11:28 GMT"}], "update_date": "2019-04-18", "authors_parsed": [["Benenson", "Rodrigo", ""], ["Popov", "Stefan", ""], ["Ferrari", "Vittorio", ""]]}, {"id": "1903.10831", "submitter": "Liu Li", "authors": "Liu Li, Mai Xu, Xiaofei Wang, Lai Jiang and Hanruo Liu", "title": "Attention Based Glaucoma Detection: A Large-scale Database and CNN Model", "comments": "10 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, the attention mechanism has been successfully applied in\nconvolutional neural networks (CNNs), significantly boosting the performance of\nmany computer vision tasks. Unfortunately, few medical image recognition\napproaches incorporate the attention mechanism in the CNNs. In particular,\nthere exists high redundancy in fundus images for glaucoma detection, such that\nthe attention mechanism has potential in improving the performance of CNN-based\nglaucoma detection. This paper proposes an attention-based CNN for glaucoma\ndetection (AG-CNN). Specifically, we first establish a large-scale attention\nbased glaucoma (LAG) database, which includes 5,824 fundus images labeled with\neither positive glaucoma (2,392) or negative glaucoma (3,432). The attention\nmaps of the ophthalmologists are also collected in LAG database through a\nsimulated eye-tracking experiment. Then, a new structure of AG-CNN is designed,\nincluding an attention prediction subnet, a pathological area localization\nsubnet and a glaucoma classification subnet. Different from other\nattention-based CNN methods, the features are also visualized as the localized\npathological area, which can advance the performance of glaucoma detection.\nFinally, the experiment results show that the proposed AG-CNN approach\nsignificantly advances state-of-the-art glaucoma detection.\n", "versions": [{"version": "v1", "created": "Tue, 26 Mar 2019 12:29:36 GMT"}, {"version": "v2", "created": "Thu, 28 Mar 2019 02:46:30 GMT"}, {"version": "v3", "created": "Sun, 21 Apr 2019 15:07:48 GMT"}], "update_date": "2019-04-23", "authors_parsed": [["Li", "Liu", ""], ["Xu", "Mai", ""], ["Wang", "Xiaofei", ""], ["Jiang", "Lai", ""], ["Liu", "Hanruo", ""]]}, {"id": "1903.10836", "submitter": "Jizhe Zhou", "authors": "Jizhe Zhou, Chi-Man Pun and YingYu Wang", "title": "Pixelation is NOT Done in Videos Yet", "comments": "major modification on GP models", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces an algorithm to protect the privacy of individuals in\nstreaming video data by blurring faces such that face cannot be reliably\nrecognized. This thwarts any possible face recognition, but because all facial\ndetails are obscured, the result is of limited use. We propose a new clustering\nalgorithm to create raw trajectories for detected faces. Associating faces\nacross frames to form trajectories, it auto-generates cluster number and\ndiscovers new clusters through deep feature and position aggregated affinities.\nWe introduce a Gaussian Process to refine the raw trajectories. We conducted an\nonline experiment with 47 participants to evaluate the effectiveness of face\nblurring compared to the original photo (as-is), and users' experience\n(satisfaction, information sufficiency, enjoyment, social presence, and filter\nlikeability)\n", "versions": [{"version": "v1", "created": "Tue, 26 Mar 2019 12:38:45 GMT"}, {"version": "v2", "created": "Wed, 17 Apr 2019 16:43:03 GMT"}, {"version": "v3", "created": "Wed, 24 Apr 2019 09:08:24 GMT"}], "update_date": "2019-04-25", "authors_parsed": [["Zhou", "Jizhe", ""], ["Pun", "Chi-Man", ""], ["Wang", "YingYu", ""]]}, {"id": "1903.10863", "submitter": "Guo-Jun Qi", "authors": "Guo-Jun Qi, Liheng Zhang, Chang Wen Chen, Qi Tian", "title": "AVT: Unsupervised Learning of Transformation Equivariant Representations\n  by Autoencoding Variational Transformations", "comments": "http://maple-lab.net/projects/AVT.htm. arXiv admin note: text overlap\n  with arXiv:1901.04596", "journal-ref": "in Proceedings of International Conference in Computer Vision\n  (ICCV 2019), Seoul, Kore, Oct. 27 -- Nov. 2, 2019", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The learning of Transformation-Equivariant Representations (TERs), which is\nintroduced by Hinton et al. \\cite{hinton2011transforming}, has been considered\nas a principle to reveal visual structures under various transformations. It\ncontains the celebrated Convolutional Neural Networks (CNNs) as a special case\nthat only equivary to the translations. In contrast, we seek to train TERs for\na generic class of transformations and train them in an {\\em unsupervised}\nfashion. To this end, we present a novel principled method by Autoencoding\nVariational Transformations (AVT), compared with the conventional approach to\nautoencoding data. Formally, given transformed images, the AVT seeks to train\nthe networks by maximizing the mutual information between the transformations\nand representations. This ensures the resultant TERs of individual images\ncontain the {\\em intrinsic} information about their visual structures that\nwould equivary {\\em extricably} under various transformations in a generalized\n{\\em nonlinear} case. Technically, we show that the resultant optimization\nproblem can be efficiently solved by maximizing a variational lower-bound of\nthe mutual information. This variational approach introduces a transformation\ndecoder to approximate the intractable posterior of transformations, resulting\nin an autoencoding architecture with a pair of the representation encoder and\nthe transformation decoder. Experiments demonstrate the proposed AVT model sets\na new record for the performances on unsupervised tasks, greatly closing the\nperformance gap to the supervised models.\n", "versions": [{"version": "v1", "created": "Sat, 23 Mar 2019 21:31:10 GMT"}, {"version": "v2", "created": "Mon, 1 Apr 2019 22:16:02 GMT"}, {"version": "v3", "created": "Tue, 25 Jun 2019 04:17:15 GMT"}], "update_date": "2019-07-24", "authors_parsed": [["Qi", "Guo-Jun", ""], ["Zhang", "Liheng", ""], ["Chen", "Chang Wen", ""], ["Tian", "Qi", ""]]}, {"id": "1903.10869", "submitter": "Anh Nguyen", "authors": "Anh Nguyen, Thanh-Toan Do, Ian Reid, Darwin G. Caldwell, Nikos G.\n  Tsagarakis", "title": "V2CNet: A Deep Learning Framework to Translate Videos to Commands for\n  Robotic Manipulation", "comments": "15 pages. arXiv admin note: substantial text overlap with\n  arXiv:1710.00290", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose V2CNet, a new deep learning framework to automatically translate\nthe demonstration videos to commands that can be directly used in robotic\napplications. Our V2CNet has two branches and aims at understanding the\ndemonstration video in a fine-grained manner. The first branch has the\nencoder-decoder architecture to encode the visual features and sequentially\ngenerate the output words as a command, while the second branch uses a Temporal\nConvolutional Network (TCN) to learn the fine-grained actions. By jointly\ntraining both branches, the network is able to model the sequential information\nof the command, while effectively encodes the fine-grained actions. The\nexperimental results on our new large-scale dataset show that V2CNet\noutperforms recent state-of-the-art methods by a substantial margin, while its\noutput can be applied in real robotic applications. The source code and trained\nmodels will be made available.\n", "versions": [{"version": "v1", "created": "Sat, 23 Mar 2019 04:02:51 GMT"}], "update_date": "2019-03-27", "authors_parsed": [["Nguyen", "Anh", ""], ["Do", "Thanh-Toan", ""], ["Reid", "Ian", ""], ["Caldwell", "Darwin G.", ""], ["Tsagarakis", "Nikos G.", ""]]}, {"id": "1903.10873", "submitter": "Anpei Chen", "authors": "Anpei Chen, Zhang Chen, Guli Zhang, Ziheng Zhang, Kenny Mitchell,\n  Jingyi Yu", "title": "Photo-Realistic Facial Details Synthesis from Single Image", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a single-image 3D face synthesis technique that can handle\nchallenging facial expressions while recovering fine geometric details. Our\ntechnique employs expression analysis for proxy face geometry generation and\ncombines supervised and unsupervised learning for facial detail synthesis. On\nproxy generation, we conduct emotion prediction to determine a new\nexpression-informed proxy. On detail synthesis, we present a Deep Facial Detail\nNet (DFDN) based on Conditional Generative Adversarial Net (CGAN) that employs\nboth geometry and appearance loss functions. For geometry, we capture 366\nhigh-quality 3D scans from 122 different subjects under 3 facial expressions.\nFor appearance, we use additional 20K in-the-wild face images and apply\nimage-based rendering to accommodate lighting variations. Comprehensive\nexperiments demonstrate that our framework can produce high-quality 3D faces\nwith realistic details under challenging facial expressions.\n", "versions": [{"version": "v1", "created": "Tue, 26 Mar 2019 13:31:25 GMT"}, {"version": "v2", "created": "Wed, 8 May 2019 08:02:45 GMT"}, {"version": "v3", "created": "Tue, 13 Aug 2019 15:47:21 GMT"}, {"version": "v4", "created": "Mon, 14 Oct 2019 09:42:50 GMT"}, {"version": "v5", "created": "Tue, 3 Dec 2019 08:38:31 GMT"}], "update_date": "2019-12-04", "authors_parsed": [["Chen", "Anpei", ""], ["Chen", "Zhang", ""], ["Zhang", "Guli", ""], ["Zhang", "Ziheng", ""], ["Mitchell", "Kenny", ""], ["Yu", "Jingyi", ""]]}, {"id": "1903.10883", "submitter": "Markus Oberweger", "authors": "Markus Oberweger, Paul Wohlhart, Vincent Lepetit", "title": "Generalized Feedback Loop for Joint Hand-Object Pose Estimation", "comments": "arXiv admin note: substantial text overlap with arXiv:1609.09698", "journal-ref": "Transactions on Pattern Analysis and Machine Intelligence 2019", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an approach to estimating the 3D pose of a hand, possibly handling\nan object, given a depth image. We show that we can correct the mistakes made\nby a Convolutional Neural Network trained to predict an estimate of the 3D pose\nby using a feedback loop. The components of this feedback loop are also Deep\nNetworks, optimized using training data. This approach can be generalized to a\nhand interacting with an object. Therefore, we jointly estimate the 3D pose of\nthe hand and the 3D pose of the object. Our approach performs en-par with\nstate-of-the-art methods for 3D hand pose estimation, and outperforms\nstate-of-the-art methods for joint hand-object pose estimation when using depth\nimages only. Also, our approach is efficient as our implementation runs in\nreal-time on a single GPU.\n", "versions": [{"version": "v1", "created": "Mon, 25 Mar 2019 15:40:34 GMT"}], "update_date": "2019-03-27", "authors_parsed": [["Oberweger", "Markus", ""], ["Wohlhart", "Paul", ""], ["Lepetit", "Vincent", ""]]}, {"id": "1903.10885", "submitter": "Kripasindhu Sarkar", "authors": "Kripasindhu Sarkar, Kiran Varanasi, Didier Stricker", "title": "Learning Quadrangulated Patches For 3D Shape Processing", "comments": "arXiv admin note: substantial text overlap with arXiv:1709.06868", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a system for surface completion and inpainting of 3D shapes using\ngenerative models, learnt on local patches. Our method uses a novel encoding of\nheight map based local patches parameterized using 3D mesh quadrangulation of\nthe low resolution input shape. This provides us sufficient amount of local 3D\npatches to learn a generative model for the task of repairing moderate sized\nholes. Following the ideas from the recent progress in 2D inpainting, we\ninvestigated both linear dictionary based model and convolutional denoising\nautoencoders based model for the task for inpainting, and show our results to\nbe better than the previous geometry based method of surface inpainting. We\nvalidate our method on both synthetic shapes and real world scans.\n", "versions": [{"version": "v1", "created": "Mon, 25 Mar 2019 16:54:59 GMT"}], "update_date": "2019-03-27", "authors_parsed": [["Sarkar", "Kripasindhu", ""], ["Varanasi", "Kiran", ""], ["Stricker", "Didier", ""]]}, {"id": "1903.10920", "submitter": "Amir Rosenfeld", "authors": "Amir Rosenfeld, Richard Zemel, John K. Tsotsos", "title": "High-Level Perceptual Similarity is Enabled by Learning Diverse Tasks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Predicting human perceptual similarity is a challenging subject of ongoing\nresearch. The visual process underlying this aspect of human vision is thought\nto employ multiple different levels of visual analysis (shapes, objects,\ntexture, layout, color, etc). In this paper, we postulate that the perception\nof image similarity is not an explicitly learned capability, but rather one\nthat is a byproduct of learning others. This claim is supported by leveraging\nrepresentations learned from a diverse set of visual tasks and using them\njointly to predict perceptual similarity. This is done via simple feature\nconcatenation, without any further learning. Nevertheless, experiments\nperformed on the challenging Totally-Looks-Like (TLL) benchmark significantly\nsurpass recent baselines, closing much of the reported gap towards prediction\nof human perceptual similarity. We provide an analysis of these results and\ndiscuss them in a broader context of emergent visual capabilities and their\nimplications on the course of machine-vision research.\n", "versions": [{"version": "v1", "created": "Tue, 26 Mar 2019 14:32:02 GMT"}], "update_date": "2019-03-27", "authors_parsed": [["Rosenfeld", "Amir", ""], ["Zemel", "Richard", ""], ["Tsotsos", "John K.", ""]]}, {"id": "1903.10929", "submitter": "Andrea Romanoni", "authors": "Andrea Romanoni, Matteo Matteucci", "title": "TAPA-MVS: Textureless-Aware PAtchMatch Multi-View Stereo", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the most successful approaches in Multi-View Stereo estimates a depth\nmap and a normal map for each view via PatchMatch-based optimization and fuses\nthem into a consistent 3D points cloud. This approach relies on\nphoto-consistency to evaluate the goodness of a depth estimate. It generally\nproduces very accurate results; however, the reconstructed model often lacks\ncompleteness, especially in correspondence of broad untextured areas where the\nphoto-consistency metrics are unreliable. Assuming the untextured areas\npiecewise planar, in this paper we generate novel PatchMatch hypotheses so to\nexpand reliable depth estimates in neighboring untextured regions. At the same\ntime, we modify the photo-consistency measure such to favor standard or novel\nPatchMatch depth hypotheses depending on the textureness of the considered\narea. We also propose a depth refinement step to filter wrong estimates and to\nfill the gaps on both the depth maps and normal maps while preserving the\ndiscontinuities. The effectiveness of our new methods has been tested against\nseveral state of the art algorithms in the publicly available ETH3D dataset\ncontaining a wide variety of high and low-resolution images.\n", "versions": [{"version": "v1", "created": "Tue, 26 Mar 2019 14:39:11 GMT"}], "update_date": "2019-03-27", "authors_parsed": [["Romanoni", "Andrea", ""], ["Matteucci", "Matteo", ""]]}, {"id": "1903.10930", "submitter": "Philipp Oberdiek", "authors": "Fabian Wolf, Philipp Oberdiek, Gernot A. Fink", "title": "Exploring Confidence Measures for Word Spotting in Heterogeneous\n  Datasets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, convolutional neural networks (CNNs) took over the field of\ndocument analysis and they became the predominant model for word spotting.\nEspecially attribute CNNs, which learn the mapping between a word image and an\nattribute representation, showed exceptional performances. The drawback of this\napproach is the overconfidence of neural networks when used out of their\ntraining distribution. In this paper, we explore different metrics for\nquantifying the confidence of a CNN in its predictions, specifically on the\nretrieval problem of word spotting. With these confidence measures, we limit\nthe inability of a retrieval list to reject certain candidates. We investigate\nfour different approaches that are either based on the network's attribute\nestimations or make use of a surrogate model. Our approach also aims at\nanswering the question for which part of a dataset the retrieval system gives\nreliable results. We further show that there exists a direct relation between\nthe proposed confidence measures and the quality of an estimated attribute\nrepresentation.\n", "versions": [{"version": "v1", "created": "Tue, 26 Mar 2019 14:40:42 GMT"}], "update_date": "2019-03-27", "authors_parsed": [["Wolf", "Fabian", ""], ["Oberdiek", "Philipp", ""], ["Fink", "Gernot A.", ""]]}, {"id": "1903.10942", "submitter": "Helene Svane", "authors": "Helene Svane and Andrew du Plessis", "title": "Reconstruction of r-Regular Objects from Trinary Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study digital images of r-regular objects where a pixel is black if it is\ncompletely inside the object, white if it is completely inside the complement\nof the object, and grey otherwise. We call such images trinary. We discuss\npossible configurations of pixels in trinary images of r-regular objects at\ncertain resolutions and propose a method for reconstructing objects from such\nimages. We show that the reconstructed object is close to the original object\nin Hausdorff norm, and that there is a homeomorphism of the plane taking the\nreconstructed set to the original.\n", "versions": [{"version": "v1", "created": "Tue, 26 Mar 2019 15:05:00 GMT"}], "update_date": "2019-03-27", "authors_parsed": [["Svane", "Helene", ""], ["Plessis", "Andrew du", ""]]}, {"id": "1903.10955", "submitter": "Buyu Li", "authors": "Buyu Li, Wanli Ouyang, Lu Sheng, Xingyu Zeng and Xiaogang Wang", "title": "GS3D: An Efficient 3D Object Detection Framework for Autonomous Driving", "comments": "Accepted by CVPR2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an efficient 3D object detection framework based on a single RGB\nimage in the scenario of autonomous driving. Our efforts are put on extracting\nthe underlying 3D information in a 2D image and determining the accurate 3D\nbounding box of the object without point cloud or stereo data. Leveraging the\noff-the-shelf 2D object detector, we propose an artful approach to efficiently\nobtain a coarse cuboid for each predicted 2D box. The coarse cuboid has enough\naccuracy to guide us to determine the 3D box of the object by refinement. In\ncontrast to previous state-of-the-art methods that only use the features\nextracted from the 2D bounding box for box refinement, we explore the 3D\nstructure information of the object by employing the visual features of visible\nsurfaces. The new features from surfaces are utilized to eliminate the problem\nof representation ambiguity brought by only using a 2D bounding box. Moreover,\nwe investigate different methods of 3D box refinement and discover that a\nclassification formulation with quality aware loss has much better performance\nthan regression. Evaluated on the KITTI benchmark, our approach outperforms\ncurrent state-of-the-art methods for single RGB image based 3D object\ndetection.\n", "versions": [{"version": "v1", "created": "Tue, 26 Mar 2019 15:25:26 GMT"}, {"version": "v2", "created": "Wed, 27 Mar 2019 02:34:23 GMT"}], "update_date": "2019-03-28", "authors_parsed": [["Li", "Buyu", ""], ["Ouyang", "Wanli", ""], ["Sheng", "Lu", ""], ["Zeng", "Xingyu", ""], ["Wang", "Xiaogang", ""]]}, {"id": "1903.10974", "submitter": "Michael Jones", "authors": "Esra Ataer-Cansizoglu and Michael Jones and Ziming Zhang and Alan\n  Sullivan", "title": "Verification of Very Low-Resolution Faces Using An Identity-Preserving\n  Deep Face Super-Resolution Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Face super-resolution methods usually aim at producing visually appealing\nresults rather than preserving distinctive features for further face\nidentification. In this work, we propose a deep learning method for face\nverification on very low-resolution face images that involves\nidentity-preserving face super-resolution. Our framework includes a\nsuper-resolution network and a feature extraction network. We train a VGG-based\ndeep face recognition network (Parkhi et al. 2015) to be used as feature\nextractor. Our super-resolution network is trained to minimize the feature\ndistance between the high resolution ground truth image and the super-resolved\nimage, where features are extracted using our pre-trained feature extraction\nnetwork. We carry out experiments on FRGC, Multi-PIE, LFW-a, and MegaFace\ndatasets to evaluate our method in controlled and uncontrolled settings. The\nresults show that the presented method outperforms conventional\nsuper-resolution methods in low-resolution face verification.\n", "versions": [{"version": "v1", "created": "Tue, 26 Mar 2019 16:01:23 GMT"}], "update_date": "2019-03-27", "authors_parsed": [["Ataer-Cansizoglu", "Esra", ""], ["Jones", "Michael", ""], ["Zhang", "Ziming", ""], ["Sullivan", "Alan", ""]]}, {"id": "1903.10979", "submitter": "Chen Yukang", "authors": "Yukang Chen, Tong Yang, Xiangyu Zhang, Gaofeng Meng, Xinyu Xiao, Jian\n  Sun", "title": "DetNAS: Backbone Search for Object Detection", "comments": "In NeurIPS 2019. Code and models are available at\n  https://github.com/megvii-model/DetNAS", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Object detectors are usually equipped with backbone networks designed for\nimage classification. It might be sub-optimal because of the gap between the\ntasks of image classification and object detection. In this work, we present\nDetNAS to use Neural Architecture Search (NAS) for the design of better\nbackbones for object detection. It is non-trivial because detection training\ntypically needs ImageNet pre-training while NAS systems require accuracies on\nthe target detection task as supervisory signals. Based on the technique of\none-shot supernet, which contains all possible networks in the search space, we\npropose a framework for backbone search on object detection. We train the\nsupernet under the typical detector training schedule: ImageNet pre-training\nand detection fine-tuning. Then, the architecture search is performed on the\ntrained supernet, using the detection task as the guidance. This framework\nmakes NAS on backbones very efficient. In experiments, we show the\neffectiveness of DetNAS on various detectors, for instance, one-stage RetinaNet\nand the two-stage FPN. We empirically find that networks searched on object\ndetection shows consistent superiority compared to those searched on ImageNet\nclassification. The resulting architecture achieves superior performance than\nhand-crafted networks on COCO with much less FLOPs complexity.\n", "versions": [{"version": "v1", "created": "Tue, 26 Mar 2019 16:08:30 GMT"}, {"version": "v2", "created": "Mon, 10 Jun 2019 15:00:52 GMT"}, {"version": "v3", "created": "Mon, 28 Oct 2019 09:30:23 GMT"}, {"version": "v4", "created": "Mon, 30 Dec 2019 14:04:15 GMT"}], "update_date": "2020-01-01", "authors_parsed": [["Chen", "Yukang", ""], ["Yang", "Tong", ""], ["Zhang", "Xiangyu", ""], ["Meng", "Gaofeng", ""], ["Xiao", "Xinyu", ""], ["Sun", "Jian", ""]]}, {"id": "1903.10995", "submitter": "Dengxin Dai", "authors": "Simon Hecker and Dengxin Dai and Luc Van Gool", "title": "Learning Accurate, Comfortable and Human-like Driving", "comments": "submitted to a conference, 10 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Autonomous vehicles are more likely to be accepted if they drive accurately,\ncomfortably, but also similar to how human drivers would. This is especially\ntrue when autonomous and human-driven vehicles need to share the same road. The\nmain research focus thus far, however, is still on improving driving accuracy\nonly. This paper formalizes the three concerns with the aim of accurate,\ncomfortable and human-like driving. Three contributions are made in this paper.\nFirst, numerical map data from HERE Technologies are employed for more accurate\ndriving; a set of map features which are believed to be relevant to driving are\nengineered to navigate better. Second, the learning procedure is improved from\na pointwise prediction to a sequence-based prediction and passengers' comfort\nmeasures are embedded into the learning algorithm. Finally, we take advantage\nof the advances in adversary learning to learn human-like driving;\nspecifically, the standard L1 or L2 loss is augmented by an adversary loss\nwhich is based on a discriminator trained to distinguish between human driving\nand machine driving. Our model is trained and evaluated on the Drive360\ndataset, which features 60 hours and 3000 km of real-world driving data.\nExtensive experiments show that our driving model is more accurate, more\ncomfortable and behaves more like a human driver than previous methods. The\nresources of this work will be released on the project page.\n", "versions": [{"version": "v1", "created": "Tue, 26 Mar 2019 16:36:15 GMT"}], "update_date": "2019-03-27", "authors_parsed": [["Hecker", "Simon", ""], ["Dai", "Dengxin", ""], ["Van Gool", "Luc", ""]]}, {"id": "1903.11020", "submitter": "Shuo Zhou", "authors": "Shuo Zhou, Wenwen Li, Christopher R. Cox, and Haiping Lu", "title": "Domain Independent SVM for Transfer Learning in Brain Decoding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Brain imaging data are important in brain sciences yet expensive to obtain,\nwith big volume (i.e., large p) but small sample size (i.e., small n). To\ntackle this problem, transfer learning is a promising direction that leverages\nsource data to improve performance on related, target data. Most transfer\nlearning methods focus on minimizing data distribution mismatch. However, a big\nchallenge in brain imaging is the large domain discrepancies in cognitive\nexperiment designs and subject-specific structures and functions. A recent\ntransfer learning approach minimizes domain dependence to learn common features\nacross domains, via the Hilbert-Schmidt Independence Criterion (HSIC). Inspired\nby this method, we propose a new Domain Independent Support Vector Machine\n(DI-SVM) for transfer learning in brain condition decoding. Specifically,\nDI-SVM simultaneously minimizes the SVM empirical risk and the dependence on\ndomain information via a simplified HSIC. We use public data to construct 13\ntransfer learning tasks in brain decoding, including three interesting\nmulti-source transfer tasks. Experiments show that DI-SVM's superior\nperformance over eight competing methods on these tasks, particularly an\nimprovement of more than 24% on multi-source transfer tasks.\n", "versions": [{"version": "v1", "created": "Tue, 26 Mar 2019 17:04:44 GMT"}], "update_date": "2019-03-27", "authors_parsed": [["Zhou", "Shuo", ""], ["Li", "Wenwen", ""], ["Cox", "Christopher R.", ""], ["Lu", "Haiping", ""]]}, {"id": "1903.11027", "submitter": "Holger Caesar", "authors": "Holger Caesar, Varun Bankiti, Alex H. Lang, Sourabh Vora, Venice Erin\n  Liong, Qiang Xu, Anush Krishnan, Yu Pan, Giancarlo Baldan, Oscar Beijbom", "title": "nuScenes: A multimodal dataset for autonomous driving", "comments": "CVPR 2020 camera ready incl. supplementary material", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.RO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Robust detection and tracking of objects is crucial for the deployment of\nautonomous vehicle technology. Image based benchmark datasets have driven\ndevelopment in computer vision tasks such as object detection, tracking and\nsegmentation of agents in the environment. Most autonomous vehicles, however,\ncarry a combination of cameras and range sensors such as lidar and radar. As\nmachine learning based methods for detection and tracking become more\nprevalent, there is a need to train and evaluate such methods on datasets\ncontaining range sensor data along with images. In this work we present\nnuTonomy scenes (nuScenes), the first dataset to carry the full autonomous\nvehicle sensor suite: 6 cameras, 5 radars and 1 lidar, all with full 360 degree\nfield of view. nuScenes comprises 1000 scenes, each 20s long and fully\nannotated with 3D bounding boxes for 23 classes and 8 attributes. It has 7x as\nmany annotations and 100x as many images as the pioneering KITTI dataset. We\ndefine novel 3D detection and tracking metrics. We also provide careful dataset\nanalysis as well as baselines for lidar and image based detection and tracking.\nData, development kit and more information are available online.\n", "versions": [{"version": "v1", "created": "Tue, 26 Mar 2019 17:19:56 GMT"}, {"version": "v2", "created": "Tue, 3 Sep 2019 10:06:43 GMT"}, {"version": "v3", "created": "Fri, 22 Nov 2019 09:01:24 GMT"}, {"version": "v4", "created": "Wed, 8 Jan 2020 10:30:05 GMT"}, {"version": "v5", "created": "Tue, 5 May 2020 09:13:24 GMT"}], "update_date": "2020-05-06", "authors_parsed": [["Caesar", "Holger", ""], ["Bankiti", "Varun", ""], ["Lang", "Alex H.", ""], ["Vora", "Sourabh", ""], ["Liong", "Venice Erin", ""], ["Xu", "Qiang", ""], ["Krishnan", "Anush", ""], ["Pan", "Yu", ""], ["Baldan", "Giancarlo", ""], ["Beijbom", "Oscar", ""]]}, {"id": "1903.11029", "submitter": "Noel Mizzi", "authors": "Noel Mizzi, Adrian Muscat", "title": "Optimising the Input Image to Improve Visual Relationship Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual Relationship Detection is defined as, given an image composed of a\nsubject and an object, the correct relation is predicted. To improve the visual\npart of this difficult problem, ten preprocessing methods were tested to\ndetermine whether the widely used Union method yields the optimal results.\nTherefore, focusing solely on predicate prediction, no object detection and\nlinguistic knowledge were used to prevent them from affecting the comparison\nresults. Once fine-tuned, the Visual Geometry Group models were evaluated using\nRecall@1, per-predicate recall, activation maximisations, class activation\nmaps, and error analysis. From this research it was found that using\npreprocessing methods such as the Union-Without-Background-and-with-Binary-mask\n(Union-WB-and-B) method yields significantly better results than the widely\nused Union method since, as designed, it enables the Convolutional Neural\nNetwork to also identify the subject and object in the convolutional layers\ninstead of solely in the fully-connected layers.\n", "versions": [{"version": "v1", "created": "Tue, 26 Mar 2019 17:21:06 GMT"}], "update_date": "2019-03-27", "authors_parsed": [["Mizzi", "Noel", ""], ["Muscat", "Adrian", ""]]}, {"id": "1903.11059", "submitter": "Linnan Wang", "authors": "Linnan Wang, Yiyang Zhao, Yuu Jinnai, Yuandong Tian, Rodrigo Fonseca", "title": "AlphaX: eXploring Neural Architectures with Deep Neural Networks and\n  Monte Carlo Tree Search", "comments": "another search algorithm for NAS. arXiv admin note: substantial text\n  overlap with arXiv:1805.07440", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural Architecture Search (NAS) has shown great success in automating the\ndesign of neural networks, but the prohibitive amount of computations behind\ncurrent NAS methods requires further investigations in improving the sample\nefficiency and the network evaluation cost to get better results in a shorter\ntime. In this paper, we present a novel scalable Monte Carlo Tree Search (MCTS)\nbased NAS agent, named AlphaX, to tackle these two aspects. AlphaX improves the\nsearch efficiency by adaptively balancing the exploration and exploitation at\nthe state level, and by a Meta-Deep Neural Network (DNN) to predict network\naccuracies for biasing the search toward a promising region. To amortize the\nnetwork evaluation cost, AlphaX accelerates MCTS rollouts with a distributed\ndesign and reduces the number of epochs in evaluating a network by transfer\nlearning guided with the tree structure in MCTS. In 12 GPU days and 1000\nsamples, AlphaX found an architecture that reaches 97.84\\% top-1 accuracy on\nCIFAR-10, and 75.5\\% top-1 accuracy on ImageNet, exceeding SOTA NAS methods in\nboth the accuracy and sampling efficiency. Particularly, we also evaluate\nAlphaX on NASBench-101, a large scale NAS dataset; AlphaX is 3x and 2.8x more\nsample efficient than Random Search and Regularized Evolution in finding the\nglobal optimum. Finally, we show the searched architecture improves a variety\nof vision applications from Neural Style Transfer, to Image Captioning and\nObject Detection.\n", "versions": [{"version": "v1", "created": "Tue, 26 Mar 2019 04:54:53 GMT"}, {"version": "v2", "created": "Wed, 2 Oct 2019 02:04:45 GMT"}], "update_date": "2019-10-03", "authors_parsed": [["Wang", "Linnan", ""], ["Zhao", "Yiyang", ""], ["Jinnai", "Yuu", ""], ["Tian", "Yuandong", ""], ["Fonseca", "Rodrigo", ""]]}, {"id": "1903.11114", "submitter": "Felix M. Riese", "authors": "Felix M. Riese and Sina Keller", "title": "SuSi: Supervised Self-Organizing Maps for Regression and Classification\n  in Python", "comments": "An extended and peer-reviewed version exists at\n  doi:10.3390/rs12010007", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many research fields, the sizes of the existing datasets vary widely.\nHence, there is a need for machine learning techniques which are well-suited\nfor these different datasets. One possible technique is the self-organizing map\n(SOM), a type of artificial neural network which is, so far, weakly represented\nin the field of machine learning. The SOM's unique characteristic is the\nneighborhood relationship of the output neurons. This relationship improves the\nability of generalization on small datasets. SOMs are mostly applied in\nunsupervised learning and few studies focus on using SOMs as supervised\nlearning approach. Furthermore, no appropriate SOM package is available with\nrespect to machine learning standards and in the widely used programming\nlanguage Python. In this paper, we introduce the freely available Supervised\nSelf-organizing maps (SuSi) Python package which performs supervised regression\nand classification. The implementation of SuSi is described with respect to the\nunderlying mathematics. Then, we present first evaluations of the SOM for\nregression and classification datasets from two different domains of geospatial\nimage analysis. Despite the early stage of its development, the SuSi framework\nperforms well and is characterized by only small performance differences\nbetween the training and the test datasets. A comparison of the SuSi framework\nwith existing Python and R packages demonstrates the importance of the SuSi\nframework. In future work, the SuSi framework will be extended, optimized and\nupgraded e.g. with tools to better understand and visualize the input data as\nwell as the handling of missing and incomplete data.\n", "versions": [{"version": "v1", "created": "Tue, 26 Mar 2019 18:52:45 GMT"}, {"version": "v2", "created": "Thu, 28 Mar 2019 17:09:17 GMT"}, {"version": "v3", "created": "Wed, 8 Jan 2020 13:06:43 GMT"}], "update_date": "2020-01-09", "authors_parsed": [["Riese", "Felix M.", ""], ["Keller", "Sina", ""]]}, {"id": "1903.11149", "submitter": "Felix Petersen", "authors": "Felix Petersen, Amit H. Bermano, Oliver Deussen, Daniel Cohen-Or", "title": "Pix2Vex: Image-to-Geometry Reconstruction using a Smooth Differentiable\n  Renderer", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The long-coveted task of reconstructing 3D geometry from images is still a\nstanding problem. In this paper, we build on the power of neural networks and\nintroduce Pix2Vex, a network trained to convert camera-captured images into 3D\ngeometry. We present a novel differentiable renderer ($DR$) as a forward\nvalidation means during training. Our key insight is that $DR$s produce images\nof a particular appearance, different from typical input images. Hence, we\npropose adding an image-to-image translation component, converting between\nthese rendering styles. This translation closes the training loop, while\nallowing to use minimal supervision only, without needing any 3D model as\nground truth. Unlike state-of-the-art methods, our $DR$ is $C^\\infty$ smooth\nand thus does not display any discontinuities at occlusions or dis-occlusions.\nThrough our novel training scheme, our network can train on different types of\nimages, where previous work can typically only train on images of a similar\nappearance to those rendered by a $DR$.\n", "versions": [{"version": "v1", "created": "Tue, 26 Mar 2019 20:34:32 GMT"}, {"version": "v2", "created": "Sun, 26 May 2019 17:05:53 GMT"}], "update_date": "2019-05-28", "authors_parsed": [["Petersen", "Felix", ""], ["Bermano", "Amit H.", ""], ["Deussen", "Oliver", ""], ["Cohen-Or", "Daniel", ""]]}, {"id": "1903.11174", "submitter": "Rogerio Bonatti", "authors": "Wenshan Wang, Aayush Ahuja, Yanfu Zhang, Rogerio Bonatti, Sebastian\n  Scherer", "title": "Improved Generalization of Heading Direction Estimation for Aerial\n  Filming Using Semi-supervised Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the task of Autonomous aerial filming of a moving actor (e.g. a person or\na vehicle), it is crucial to have a good heading direction estimation for the\nactor from the visual input. However, the models obtained in other similar\ntasks, such as pedestrian collision risk analysis and human-robot interaction,\nare very difficult to generalize to the aerial filming task, because of the\ndifference in data distributions. Towards improving generalization with less\namount of labeled data, this paper presents a semi-supervised algorithm for\nheading direction estimation problem. We utilize temporal continuity as the\nunsupervised signal to regularize the model and achieve better generalization\nability. This semi-supervised algorithm is applied to both training and testing\nphases, which increases the testing performance by a large margin. We show that\nby leveraging unlabeled sequences, the amount of labeled data required can be\nsignificantly reduced. We also discuss several important details on improving\nthe performance by balancing labeled and unlabeled loss, and making good\ncombinations. Experimental results show that our approach robustly outputs the\nheading direction for different types of actor. The aesthetic value of the\nvideo is also improved in the aerial filming task.\n", "versions": [{"version": "v1", "created": "Tue, 26 Mar 2019 22:05:05 GMT"}], "update_date": "2019-03-28", "authors_parsed": [["Wang", "Wenshan", ""], ["Ahuja", "Aayush", ""], ["Zhang", "Yanfu", ""], ["Bonatti", "Rogerio", ""], ["Scherer", "Sebastian", ""]]}, {"id": "1903.11207", "submitter": "Ranjay Krishna", "authors": "Ranjay Krishna, Michael Bernstein, Li Fei-Fei", "title": "Information Maximizing Visual Question Generation", "comments": "CVPR 2019", "journal-ref": "IEEE Conference on Computer Vision and Pattern Recognition, 2019", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Though image-to-sequence generation models have become overwhelmingly popular\nin human-computer communications, they suffer from strongly favoring safe\ngeneric questions (\"What is in this picture?\"). Generating uninformative but\nrelevant questions is not sufficient or useful. We argue that a good question\nis one that has a tightly focused purpose --- one that is aimed at expecting a\nspecific type of response. We build a model that maximizes mutual information\nbetween the image, the expected answer and the generated question. To overcome\nthe non-differentiability of discrete natural language tokens, we introduce a\nvariational continuous latent space onto which the expected answers project. We\nregularize this latent space with a second latent space that ensures clustering\nof similar answers. Even when we don't know the expected answer, this second\nlatent space can generate goal-driven questions specifically aimed at\nextracting objects (\"what is the person throwing\"), attributes, (\"What kind of\nshirt is the person wearing?\"), color (\"what color is the frisbee?\"), material\n(\"What material is the frisbee?\"), etc. We quantitatively show that our model\nis able to retain information about an expected answer category, resulting in\nmore diverse, goal-driven questions. We launch our model on a set of real world\nimages and extract previously unseen visual concepts.\n", "versions": [{"version": "v1", "created": "Wed, 27 Mar 2019 00:57:25 GMT"}], "update_date": "2019-03-28", "authors_parsed": [["Krishna", "Ranjay", ""], ["Bernstein", "Michael", ""], ["Fei-Fei", "Li", ""]]}, {"id": "1903.11210", "submitter": "Junaid Malik", "authors": "Junaid Malik, Serkan Kiranyaz, Suchitra Kunhoth, Turker Ince, Somaya\n  Al-Maadeed, Ridha Hamila, Moncef Gabbouj", "title": "Colorectal cancer diagnosis from histology images: A comparative study", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Computer-aided diagnosis (CAD) based on histopathological imaging has\nprogressed rapidly in recent years with the rise of machine learning based\nmethodologies. Traditional approaches consist of training a classification\nmodel using features extracted from the images, based on textures or\nmorphological properties. Recently, deep-learning based methods have been\napplied directly to the raw (unprocessed) data. However, their usability is\nimpacted by the paucity of annotated data in the biomedical sector. In order to\nleverage the learning capabilities of deep Convolutional Neural Nets (CNNs)\nwithin the confines of limited labelled data, in this study we shall\ninvestigate the transfer learning approaches that aim to apply the knowledge\ngained from solving a source (e.g., non-medical) problem, to learn better\npredictive models for the target (e.g., biomedical) task. As an alternative, we\nshall further propose a new adaptive and compact CNN based architecture that\ncan be trained from scratch even on scarce and low-resolution data. Moreover,\nwe conduct quantitative comparative evaluations among the traditional methods,\ntransfer learning-based methods and the proposed adaptive approach for the\nparticular task of cancer detection and identification from scarce and\nlow-resolution histology images. Over the largest benchmark dataset formed for\nthis purpose, the proposed adaptive approach achieved a higher cancer detection\naccuracy with a significant gap, whereas the deep CNNs with transfer learning\nachieved a superior cancer identification.\n", "versions": [{"version": "v1", "created": "Wed, 27 Mar 2019 01:02:44 GMT"}, {"version": "v2", "created": "Thu, 28 Mar 2019 03:56:18 GMT"}], "update_date": "2019-03-29", "authors_parsed": [["Malik", "Junaid", ""], ["Kiranyaz", "Serkan", ""], ["Kunhoth", "Suchitra", ""], ["Ince", "Turker", ""], ["Al-Maadeed", "Somaya", ""], ["Hamila", "Ridha", ""], ["Gabbouj", "Moncef", ""]]}, {"id": "1903.11215", "submitter": "Bas Peters", "authors": "Bas Peters, Eldad Haber, Justin Granek", "title": "Neural-networks for geophysicists and their application to seismic data\n  interpretation", "comments": "8 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.geo-ph cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural-networks have seen a surge of interest for the interpretation of\nseismic images during the last few years. Network-based learning methods can\nprovide fast and accurate automatic interpretation, provided there are\nsufficiently many training labels. We provide an introduction to the field\naimed at geophysicists that are familiar with the framework of forward modeling\nand inversion. We explain the similarities and differences between deep\nnetworks to other geophysical inverse problems and show their utility in\nsolving problems such as lithology interpolation between wells, horizon\ntracking and segmentation of seismic images. The benefits of our approach are\ndemonstrated on field data from the Sea of Ireland and the North Sea.\n", "versions": [{"version": "v1", "created": "Wed, 27 Mar 2019 01:26:41 GMT"}], "update_date": "2019-03-28", "authors_parsed": [["Peters", "Bas", ""], ["Haber", "Eldad", ""], ["Granek", "Justin", ""]]}, {"id": "1903.11228", "submitter": "Zhiqin Chen", "authors": "Zhiqin Chen, Kangxue Yin, Matthew Fisher, Siddhartha Chaudhuri, Hao\n  Zhang", "title": "BAE-NET: Branched Autoencoder for Shape Co-Segmentation", "comments": "Accepted to ICCV 2019. Code: https://github.com/czq142857/BAE-NET\n  Supplementary material: https://www.sfu.ca/~zhiqinc/imseg/sup.pdf", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We treat shape co-segmentation as a representation learning problem and\nintroduce BAE-NET, a branched autoencoder network, for the task. The\nunsupervised BAE-NET is trained with a collection of un-segmented shapes, using\na shape reconstruction loss, without any ground-truth labels. Specifically, the\nnetwork takes an input shape and encodes it using a convolutional neural\nnetwork, whereas the decoder concatenates the resulting feature code with a\npoint coordinate and outputs a value indicating whether the point is\ninside/outside the shape. Importantly, the decoder is branched: each branch\nlearns a compact representation for one commonly recurring part of the shape\ncollection, e.g., airplane wings. By complementing the shape reconstruction\nloss with a label loss, BAE-NET is easily tuned for one-shot learning. We show\nunsupervised, weakly supervised, and one-shot learning results by BAE-NET,\ndemonstrating that using only a couple of exemplars, our network can generally\noutperform state-of-the-art supervised methods trained on hundreds of segmented\nshapes. Code is available at https://github.com/czq142857/BAE-NET.\n", "versions": [{"version": "v1", "created": "Wed, 27 Mar 2019 02:33:20 GMT"}, {"version": "v2", "created": "Tue, 13 Aug 2019 20:38:10 GMT"}], "update_date": "2019-08-15", "authors_parsed": [["Chen", "Zhiqin", ""], ["Yin", "Kangxue", ""], ["Fisher", "Matthew", ""], ["Chaudhuri", "Siddhartha", ""], ["Zhang", "Hao", ""]]}, {"id": "1903.11233", "submitter": "Jizong Peng", "authors": "Jizong Peng, Guillermo Estrada, Marco Pedersoli, Christian Desrosiers", "title": "Deep Co-Training for Semi-Supervised Image Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we aim to improve the performance of semantic image\nsegmentation in a semi-supervised setting in which training is effectuated with\na reduced set of annotated images and additional non-annotated images. We\npresent a method based on an ensemble of deep segmentation models. Each model\nis trained on a subset of the annotated data, and uses the non-annotated images\nto exchange information with the other models, similar to co-training. Even if\neach model learns on the same non-annotated images, diversity is preserved with\nthe use of adversarial samples. Our results show that this ability to\nsimultaneously train models, which exchange knowledge while preserving\ndiversity, leads to state-of-the-art results on two challenging medical image\ndatasets.\n", "versions": [{"version": "v1", "created": "Wed, 27 Mar 2019 03:10:36 GMT"}, {"version": "v2", "created": "Thu, 15 Aug 2019 20:01:53 GMT"}, {"version": "v3", "created": "Wed, 30 Oct 2019 17:48:03 GMT"}], "update_date": "2019-10-31", "authors_parsed": [["Peng", "Jizong", ""], ["Estrada", "Guillermo", ""], ["Pedersoli", "Marco", ""], ["Desrosiers", "Christian", ""]]}, {"id": "1903.11236", "submitter": "Chunhua Shen", "authors": "Bohan Zhuang, Lingqiao Liu, Mingkui Tan, Chunhua Shen, Ian Reid", "title": "Training Quantized Neural Networks with a Full-precision Auxiliary\n  Module", "comments": "Accepted to Proc. IEEE Conf. Computer Vision and Pattern Recognition\n  (CVPR 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In this paper, we seek to tackle a challenge in training low-precision\nnetworks: the notorious difficulty in propagating gradient through a\nlow-precision network due to the non-differentiable quantization function. We\npropose a solution by training the low-precision network with a fullprecision\nauxiliary module. Specifically, during training, we construct a mix-precision\nnetwork by augmenting the original low-precision network with the full\nprecision auxiliary module. Then the augmented mix-precision network and the\nlow-precision network are jointly optimized. This strategy creates additional\nfull-precision routes to update the parameters of the low-precision model, thus\nmaking the gradient back-propagates more easily. At the inference time, we\ndiscard the auxiliary module without introducing any computational complexity\nto the low-precision network. We evaluate the proposed method on image\nclassification and object detection over various quantization approaches and\nshow consistent performance increase. In particular, we achieve near lossless\nperformance to the full-precision model by using a 4-bit detector, which is of\ngreat practical value.\n", "versions": [{"version": "v1", "created": "Wed, 27 Mar 2019 03:42:31 GMT"}, {"version": "v2", "created": "Wed, 27 Nov 2019 05:21:50 GMT"}, {"version": "v3", "created": "Wed, 18 Mar 2020 08:06:50 GMT"}], "update_date": "2020-03-19", "authors_parsed": [["Zhuang", "Bohan", ""], ["Liu", "Lingqiao", ""], ["Tan", "Mingkui", ""], ["Shen", "Chunhua", ""], ["Reid", "Ian", ""]]}, {"id": "1903.11239", "submitter": "Andy Zeng", "authors": "Andy Zeng, Shuran Song, Johnny Lee, Alberto Rodriguez, Thomas\n  Funkhouser", "title": "TossingBot: Learning to Throw Arbitrary Objects with Residual Physics", "comments": "Summary Video: https://youtu.be/f5Zn2Up2RjQ Project webpage:\n  https://tossingbot.cs.princeton.edu", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate whether a robot arm can learn to pick and throw arbitrary\nobjects into selected boxes quickly and accurately. Throwing has the potential\nto increase the physical reachability and picking speed of a robot arm.\nHowever, precisely throwing arbitrary objects in unstructured settings presents\nmany challenges: from acquiring reliable pre-throw conditions (e.g. initial\npose of object in manipulator) to handling varying object-centric properties\n(e.g. mass distribution, friction, shape) and dynamics (e.g. aerodynamics). In\nthis work, we propose an end-to-end formulation that jointly learns to infer\ncontrol parameters for grasping and throwing motion primitives from visual\nobservations (images of arbitrary objects in a bin) through trial and error.\nWithin this formulation, we investigate the synergies between grasping and\nthrowing (i.e., learning grasps that enable more accurate throws) and between\nsimulation and deep learning (i.e., using deep networks to predict residuals on\ntop of control parameters predicted by a physics simulator). The resulting\nsystem, TossingBot, is able to grasp and throw arbitrary objects into boxes\nlocated outside its maximum reach range at 500+ mean picks per hour (600+\ngrasps per hour with 85% throwing accuracy); and generalizes to new objects and\ntarget locations. Videos are available at https://tossingbot.cs.princeton.edu\n", "versions": [{"version": "v1", "created": "Wed, 27 Mar 2019 04:04:28 GMT"}, {"version": "v2", "created": "Wed, 3 Jul 2019 19:16:12 GMT"}, {"version": "v3", "created": "Sat, 30 May 2020 15:59:12 GMT"}], "update_date": "2020-06-02", "authors_parsed": [["Zeng", "Andy", ""], ["Song", "Shuran", ""], ["Lee", "Johnny", ""], ["Rodriguez", "Alberto", ""], ["Funkhouser", "Thomas", ""]]}, {"id": "1903.11248", "submitter": "Jun Gao", "authors": "Jun Gao, Xiao Li, Liwei Wang, Sanja Fidler, Stephen Lin", "title": "Mimicking the In-Camera Color Pipeline for Camera-Aware Object\n  Compositing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We present a method for compositing virtual objects into a photograph such\nthat the object colors appear to have been processed by the photo's camera\nimaging pipeline. Compositing in such a camera-aware manner is essential for\nhigh realism, and it requires the color transformation in the photo's pipeline\nto be inferred, which is challenging due to the inherent one-to-many mapping\nthat exists from a scene to a photo. To address this problem for the case of a\nsingle photo taken from an unknown camera, we propose a dual-learning approach\nin which the reverse color transformation (from the photo to the scene) is\njointly estimated. Learning of the reverse transformation is used to facilitate\nlearning of the forward mapping, by enforcing cycle consistency of the two\nprocesses. We additionally employ a feature sharing schema to extract evidence\nfrom the target photo in the reverse mapping to guide the forward color\ntransformation. Our dual-learning approach achieves object compositing results\nthat surpass those of alternative techniques.\n", "versions": [{"version": "v1", "created": "Wed, 27 Mar 2019 04:41:19 GMT"}], "update_date": "2019-03-28", "authors_parsed": [["Gao", "Jun", ""], ["Li", "Xiao", ""], ["Wang", "Liwei", ""], ["Fidler", "Sanja", ""], ["Lin", "Stephen", ""]]}, {"id": "1903.11249", "submitter": "Varun Kannadi Valloli", "authors": "Varun Kannadi Valloli and Kinal Mehta", "title": "W-Net: Reinforced U-Net for Density Map Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Crowd management is of paramount importance when it comes to preventing\nstampedes and saving lives, especially in a countries like China and India\nwhere the combined population is a third of the global population. Millions of\npeople convene annually all around the nation to celebrate a myriad of events\nand crowd count estimation is the linchpin of the crowd management system that\ncould prevent stampedes and save lives. We present a network for crowd counting\nwhich reports state of the art results on crowd counting benchmarks. Our\ncontributions are, first, a U-Net inspired model which affords us to report\nstate of the art results. Second, we propose an independent decoding\nReinforcement branch which helps the network converge much earlier and also\nenables the network to estimate density maps with high Structural Similarity\nIndex (SSIM). Third, we discuss the drawbacks of the contemporary architectures\nand empirically show that even though our architecture achieves state of the\nart results, the merit may be due to the encoder-decoder pipeline instead.\nFinally, we report the error analysis which shows that the contemporary line of\nwork is at saturation and leaves certain prominent problems unsolved.\n", "versions": [{"version": "v1", "created": "Wed, 27 Mar 2019 04:46:23 GMT"}, {"version": "v2", "created": "Fri, 29 Mar 2019 09:19:46 GMT"}], "update_date": "2019-04-01", "authors_parsed": [["Valloli", "Varun Kannadi", ""], ["Mehta", "Kinal", ""]]}, {"id": "1903.11250", "submitter": "Mingkui Tan", "authors": "Yong Guo, Qi Chen, Jian Chen, Qingyao Wu, Qinfeng Shi, and Mingkui Tan", "title": "Auto-Embedding Generative Adversarial Networks for High Resolution Image\n  Synthesis", "comments": "Accepted by IEEE Transactions on Multimedia", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generating images via the generative adversarial network (GAN) has attracted\nmuch attention recently. However, most of the existing GAN-based methods can\nonly produce low-resolution images of limited quality. Directly generating\nhigh-resolution images using GANs is nontrivial, and often produces problematic\nimages with incomplete objects. To address this issue, we develop a novel GAN\ncalled Auto-Embedding Generative Adversarial Network (AEGAN), which\nsimultaneously encodes the global structure features and captures the\nfine-grained details. In our network, we use an autoencoder to learn the\nintrinsic high-level structure of real images and design a novel denoiser\nnetwork to provide photo-realistic details for the generated images. In the\nexperiments, we are able to produce 512x512 images of promising quality\ndirectly from the input noise. The resultant images exhibit better perceptual\nphoto-realism, i.e., with sharper structure and richer details, than other\nbaselines on several datasets, including Oxford-102 Flowers, Caltech-UCSD Birds\n(CUB), High-Quality Large-scale CelebFaces Attributes (CelebA-HQ), Large-scale\nScene Understanding (LSUN) and ImageNet.\n", "versions": [{"version": "v1", "created": "Wed, 27 Mar 2019 05:13:29 GMT"}, {"version": "v2", "created": "Fri, 29 Mar 2019 01:35:14 GMT"}], "update_date": "2019-04-01", "authors_parsed": [["Guo", "Yong", ""], ["Chen", "Qi", ""], ["Chen", "Jian", ""], ["Wu", "Qingyao", ""], ["Shi", "Qinfeng", ""], ["Tan", "Mingkui", ""]]}, {"id": "1903.11260", "submitter": "Guo-Jun Qi", "authors": "Guo-Jun Qi, Jiebo Luo", "title": "Small Data Challenges in Big Data Era: A Survey of Recent Progress on\n  Unsupervised and Semi-Supervised Methods", "comments": "published in IEEE Transactions on Pattern Analysis and Machine\n  Intelligence", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Representation learning with small labeled data have emerged in many\nproblems, since the success of deep neural networks often relies on the\navailability of a huge amount of labeled data that is expensive to collect. To\naddress it, many efforts have been made on training sophisticated models with\nfew labeled data in an unsupervised and semi-supervised fashion. In this paper,\nwe will review the recent progresses on these two major categories of methods.\nA wide spectrum of models will be categorized in a big picture, where we will\nshow how they interplay with each other to motivate explorations of new ideas.\nWe will review the principles of learning the transformation equivariant,\ndisentangled, self-supervised and semi-supervised representations, all of which\nunderpin the foundation of recent progresses. Many implementations of\nunsupervised and semi-supervised generative models have been developed on the\nbasis of these criteria, greatly expanding the territory of existing\nautoencoders, generative adversarial nets (GANs) and other deep networks by\nexploring the distribution of unlabeled data for more powerful representations.\nWe will discuss emerging topics by revealing the intrinsic connections between\nunsupervised and semi-supervised learning, and propose in future directions to\nbridge the algorithmic and theoretical gap between transformation equivariance\nfor unsupervised learning and supervised invariance for supervised learning,\nand unify unsupervised pretraining and supervised finetuning. We will also\nprovide a broader outlook of future directions to unify transformation and\ninstance equivariances for representation learning, connect unsupervised and\nsemi-supervised augmentations, and explore the role of the self-supervised\nregularization for many learning problems.\n", "versions": [{"version": "v1", "created": "Wed, 27 Mar 2019 05:50:28 GMT"}, {"version": "v2", "created": "Sat, 2 Jan 2021 16:11:08 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Qi", "Guo-Jun", ""], ["Luo", "Jiebo", ""]]}, {"id": "1903.11279", "submitter": "Huasha Zhao Mr", "authors": "Xiaojing Liu, Feiyu Gao, Qiong Zhang, Huasha Zhao", "title": "Graph Convolution for Multimodal Information Extraction from Visually\n  Rich Documents", "comments": "naacl'19 accepted paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visually rich documents (VRDs) are ubiquitous in daily business and life.\nExamples are purchase receipts, insurance policy documents, custom declaration\nforms and so on. In VRDs, visual and layout information is critical for\ndocument understanding, and texts in such documents cannot be serialized into\nthe one-dimensional sequence without losing information. Classic information\nextraction models such as BiLSTM-CRF typically operate on text sequences and do\nnot incorporate visual features. In this paper, we introduce a graph\nconvolution based model to combine textual and visual information presented in\nVRDs. Graph embeddings are trained to summarize the context of a text segment\nin the document, and further combined with text embeddings for entity\nextraction. Extensive experiments have been conducted to show that our method\noutperforms BiLSTM-CRF baselines by significant margins, on two real-world\ndatasets. Additionally, ablation studies are also performed to evaluate the\neffectiveness of each component of our model.\n", "versions": [{"version": "v1", "created": "Wed, 27 Mar 2019 07:47:12 GMT"}], "update_date": "2019-03-28", "authors_parsed": [["Liu", "Xiaojing", ""], ["Gao", "Feiyu", ""], ["Zhang", "Qiong", ""], ["Zhao", "Huasha", ""]]}, {"id": "1903.11286", "submitter": "Beomjun Kim", "authors": "Beomjun Kim, Jean Ponce, Bumsub Ham", "title": "Deformable kernel networks for guided depth map upsampling", "comments": "conference submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of upsampling a low-resolution (LR) depth map using a\nregistered high-resolution (HR) color image of the same scene. Previous methods\nbased on convolutional neural networks (CNNs) combine nonlinear activations of\nspatially-invariant kernels to estimate structural details from LR depth and HR\ncolor images, and regress upsampling results directly from the networks. In\nthis paper, we revisit the weighted averaging process that has been widely used\nto transfer structural details from hand-crafted visual features to LR depth\nmaps. We instead learn explicitly sparse and spatially-variant kernels for this\ntask. To this end, we propose a CNN architecture and its efficient\nimplementation, called the deformable kernel network (DKN), that outputs sparse\nsets of neighbors and the corresponding weights adaptively for each pixel. We\nalso propose a fast version of DKN (FDKN) that runs about 17 times faster (0.01\nseconds for a HR image of size 640 x 480). Experimental results on standard\nbenchmarks demonstrate the effectiveness of our approach. In particular, we\nshow that the weighted averaging process with 3 x 3 kernels (i.e., aggregating\n9 samples sparsely chosen) outperforms the state of the art by a significant\nmargin.\n", "versions": [{"version": "v1", "created": "Wed, 27 Mar 2019 08:21:09 GMT"}], "update_date": "2019-03-28", "authors_parsed": [["Kim", "Beomjun", ""], ["Ponce", "Jean", ""], ["Ham", "Bumsub", ""]]}, {"id": "1903.11299", "submitter": "Maxime Portaz", "authors": "Maxime Portaz, Hicham Randrianarivo (CEDRIC), Adrien Nivaggioli,\n  Estelle Maudet, Christophe Servan (LIUM), Sylvain Peyronnet (ELM)", "title": "Image search using multilingual texts: a cross-modal learning approach\n  between image and text", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multilingual (or cross-lingual) embeddings represent several languages in a\nunique vector space. Using a common embedding space enables for a shared\nsemantic between words from different languages. In this paper, we propose to\nembed images and texts into a unique distributional vector space, enabling to\nsearch images by using text queries expressing information needs related to the\n(visual) content of images, as well as using image similarity. Our framework\nforces the representation of an image to be similar to the representation of\nthe text that describes it. Moreover, by using multilingual embeddings we\nensure that words from two different languages have close descriptors and thus\nare attached to similar images. We provide experimental evidence of the\nefficiency of our approach by experimenting it on two datasets: Common Objects\nin COntext (COCO) [19] and Multi30K [7].\n", "versions": [{"version": "v1", "created": "Wed, 27 Mar 2019 09:02:41 GMT"}, {"version": "v2", "created": "Tue, 2 Apr 2019 09:19:45 GMT"}, {"version": "v3", "created": "Tue, 14 May 2019 09:34:25 GMT"}], "update_date": "2019-05-15", "authors_parsed": [["Portaz", "Maxime", "", "CEDRIC"], ["Randrianarivo", "Hicham", "", "CEDRIC"], ["Nivaggioli", "Adrien", "", "LIUM"], ["Maudet", "Estelle", "", "LIUM"], ["Servan", "Christophe", "", "LIUM"], ["Peyronnet", "Sylvain", "", "ELM"]]}, {"id": "1903.11303", "submitter": "Zhaoqiang Xia", "authors": "Lei Li, Zhaoqiang Xia, Xiaoyue Jiang, Yupeng Ma, Fabio Roli, Xiaoyi\n  Feng", "title": "3D Face Mask Presentation Attack Detection Based on Intrinsic Image\n  Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Face presentation attacks have become a major threat to face recognition\nsystems and many countermeasures have been proposed in the past decade.\nHowever, most of them are devoted to 2D face presentation attacks, rather than\n3D face masks. Unlike the real face, the 3D face mask is usually made of resin\nmaterials and has a smooth surface, resulting in reflectance differences. So,\nwe propose a novel detection method for 3D face mask presentation attack by\nmodeling reflectance differences based on intrinsic image analysis. In the\nproposed method, the face image is first processed with intrinsic image\ndecomposition to compute its reflectance image. Then, the intensity\ndistribution histograms are extracted from three orthogonal planes to represent\nthe intensity differences of reflectance images between the real face and 3D\nface mask. After that, the 1D convolutional network is further used to capture\nthe information for describing different materials or surfaces react\ndifferently to changes in illumination. Extensive experiments on the 3DMAD\ndatabase demonstrate the effectiveness of our proposed method in distinguishing\na face mask from the real one and show that the detection performance\noutperforms other state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Wed, 27 Mar 2019 09:13:08 GMT"}], "update_date": "2019-03-28", "authors_parsed": [["Li", "Lei", ""], ["Xia", "Zhaoqiang", ""], ["Jiang", "Xiaoyue", ""], ["Ma", "Yupeng", ""], ["Roli", "Fabio", ""], ["Feng", "Xiaoyi", ""]]}, {"id": "1903.11306", "submitter": "Zhongdao Wang", "authors": "Zhongdao Wang, Liang Zheng, Yali Li and Shengjin Wang", "title": "Linkage Based Face Clustering via Graph Convolution Network", "comments": "To appear in CVPR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present an accurate and scalable approach to the face\nclustering task. We aim at grouping a set of faces by their potential\nidentities. We formulate this task as a link prediction problem: a link exists\nbetween two faces if they are of the same identity. The key idea is that we\nfind the local context in the feature space around an instance (face) contains\nrich information about the linkage relationship between this instance and its\nneighbors. By constructing sub-graphs around each instance as input data, which\ndepict the local context, we utilize the graph convolution network (GCN) to\nperform reasoning and infer the likelihood of linkage between pairs in the\nsub-graphs. Experiments show that our method is more robust to the complex\ndistribution of faces than conventional methods, yielding favorably comparable\nresults to state-of-the-art methods on standard face clustering benchmarks, and\nis scalable to large datasets. Furthermore, we show that the proposed method\ndoes not need the number of clusters as prior, is aware of noises and outliers,\nand can be extended to a multi-view version for more accurate clustering\naccuracy.\n", "versions": [{"version": "v1", "created": "Wed, 27 Mar 2019 09:36:23 GMT"}, {"version": "v2", "created": "Fri, 29 Mar 2019 07:05:22 GMT"}, {"version": "v3", "created": "Mon, 8 Apr 2019 05:35:15 GMT"}], "update_date": "2019-04-09", "authors_parsed": [["Wang", "Zhongdao", ""], ["Zheng", "Liang", ""], ["Li", "Yali", ""], ["Wang", "Shengjin", ""]]}, {"id": "1903.11323", "submitter": "Rizwan Ahmed Khan", "authors": "Hamza Sharif, Rizwan Ahmed Khan", "title": "A novel machine learning based framework for detection of Autism\n  Spectrum Disorder (ASD)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computer vision and machine learning are the linchpin of field of automation.\nThe medicine industry has adopted numerous methods to discover the root causes\nof many diseases in order to automate detection process. But, the biomarkers of\nAutism Spectrum Disorder (ASD) are still unknown, let alone automating its\ndetection. Studies from the neuroscience domain highlighted the fact that\ncorpus callosum and intracranial brain volume holds significant information for\ndetection of ASD. Such results and studies are not tested and verified by\nscientists working in the domain of computer vision / machine learning. Thus,\nin this study we have proposed a machine learning based framework for automatic\ndetection of ASD using features extracted from corpus callosum and intracranial\nbrain volume from ABIDE dataset. Corpus callosum and intracranial brain volume\ndata is obtained from T1-weighted MRI scans. Our proposed framework first\ncalculates weights of features extracted from Corpus callosum and intracranial\nbrain volume data. This step ensures to utilize discriminative capabilities of\nonly those features that will help in robust recognition of ASD. Then,\nconventional machine learning algorithm (conventional refers to algorithms\nother than deep learning) is applied on features that are most significant in\nterms of discriminative capabilities for recognition of ASD. Finally, for\nbenchmarking and to verify potential of deep learning on analyzing neuroimaging\ndata i.e. T1-weighted MRI scans, we have done experiment with state of the art\ndeep learning architecture i.e. VGG16 . We have used transfer learning approach\nto use already trained VGG16 model for detection of ASD. This is done to help\nreaders understand benefits and bottlenecks of using deep learning approach for\nanalyzing neuroimaging data which is difficult to record in large enough\nquantity for deep learning.\n", "versions": [{"version": "v1", "created": "Wed, 27 Mar 2019 10:02:08 GMT"}, {"version": "v2", "created": "Sat, 28 Dec 2019 14:38:52 GMT"}, {"version": "v3", "created": "Thu, 2 Jan 2020 14:58:23 GMT"}], "update_date": "2020-01-03", "authors_parsed": [["Sharif", "Hamza", ""], ["Khan", "Rizwan Ahmed", ""]]}, {"id": "1903.11326", "submitter": "Yining Li", "authors": "Yining Li, Chen Huang and Chen Change Loy", "title": "Dense Intrinsic Appearance Flow for Human Pose Transfer", "comments": "CVPR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel approach for the task of human pose transfer, which aims\nat synthesizing a new image of a person from an input image of that person and\na target pose. We address the issues of limited correspondences identified\nbetween keypoints only and invisible pixels due to self-occlusion. Unlike\nexisting methods, we propose to estimate dense and intrinsic 3D appearance flow\nto better guide the transfer of pixels between poses. In particular, we wish to\ngenerate the 3D flow from just the reference and target poses. Training a\nnetwork for this purpose is non-trivial, especially when the annotations for 3D\nappearance flow are scarce by nature. We address this problem through a flow\nsynthesis stage. This is achieved by fitting a 3D model to the given pose pair\nand project them back to the 2D plane to compute the dense appearance flow for\ntraining. The synthesized ground-truths are then used to train a feedforward\nnetwork for efficient mapping from the input and target skeleton poses to the\n3D appearance flow. With the appearance flow, we perform feature warping on the\ninput image and generate a photorealistic image of the target pose. Extensive\nresults on DeepFashion and Market-1501 datasets demonstrate the effectiveness\nof our approach over existing methods. Our code is available at\nhttp://mmlab.ie.cuhk.edu.hk/projects/pose-transfer\n", "versions": [{"version": "v1", "created": "Wed, 27 Mar 2019 10:11:09 GMT"}], "update_date": "2019-03-28", "authors_parsed": [["Li", "Yining", ""], ["Huang", "Chen", ""], ["Loy", "Chen Change", ""]]}, {"id": "1903.11328", "submitter": "Mayu Otani", "authors": "Mayu Otani, Yuta Nakashima, Esa Rahtu, Janne Heikkil\\\"a", "title": "Rethinking the Evaluation of Video Summaries", "comments": "CVPR'19 poster", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video summarization is a technique to create a short skim of the original\nvideo while preserving the main stories/content. There exists a substantial\ninterest in automatizing this process due to the rapid growth of the available\nmaterial. The recent progress has been facilitated by public benchmark\ndatasets, which enable easy and fair comparison of methods. Currently the\nestablished evaluation protocol is to compare the generated summary with\nrespect to a set of reference summaries provided by the dataset. In this paper,\nwe will provide in-depth assessment of this pipeline using two popular\nbenchmark datasets. Surprisingly, we observe that randomly generated summaries\nachieve comparable or better performance to the state-of-the-art. In some\ncases, the random summaries outperform even the human generated summaries in\nleave-one-out experiments. Moreover, it turns out that the video segmentation,\nwhich is often considered as a fixed pre-processing method, has the most\nsignificant impact on the performance measure. Based on our observations, we\npropose alternative approaches for assessing the importance scores as well as\nan intuitive visualization of correlation between the estimated scoring and\nhuman annotations.\n", "versions": [{"version": "v1", "created": "Wed, 27 Mar 2019 10:15:19 GMT"}, {"version": "v2", "created": "Thu, 11 Apr 2019 07:52:09 GMT"}], "update_date": "2019-04-12", "authors_parsed": [["Otani", "Mayu", ""], ["Nakashima", "Yuta", ""], ["Rahtu", "Esa", ""], ["Heikkil\u00e4", "Janne", ""]]}, {"id": "1903.11332", "submitter": "Jacques Manderscheid", "authors": "Jacques Manderscheid, Amos Sironi, Nicolas Bourdis, Davide Migliore\n  and Vincent Lepetit", "title": "Speed Invariant Time Surface for Learning to Detect Corner Points with\n  Event-Based Cameras", "comments": "8 pages, 7 figures, accepted at CVPR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a learning approach to corner detection for event-based cameras\nthat is stable even under fast and abrupt motions. Event-based cameras offer\nhigh temporal resolution, power efficiency, and high dynamic range. However,\nthe properties of event-based data are very different compared to standard\nintensity images, and simple extensions of corner detection methods designed\nfor these images do not perform well on event-based data. We first introduce an\nefficient way to compute a time surface that is invariant to the speed of the\nobjects. We then show that we can train a Random Forest to recognize events\ngenerated by a moving corner from our time surface. Random Forests are also\nextremely efficient, and therefore a good choice to deal with the high capture\nfrequency of event-based cameras ---our implementation processes up to 1.6Mev/s\non a single CPU. Thanks to our time surface formulation and this learning\napproach, our method is significantly more robust to abrupt changes of\ndirection of the corners compared to previous ones. Our method also naturally\nassigns a confidence score for the corners, which can be useful for\npostprocessing. Moreover, we introduce a high-resolution dataset suitable for\nquantitative evaluation and comparison of corner detection methods for\nevent-based cameras. We call our approach SILC, for Speed Invariant Learned\nCorners, and compare it to the state-of-the-art with extensive experiments,\nshowing better performance.\n", "versions": [{"version": "v1", "created": "Wed, 27 Mar 2019 10:19:40 GMT"}, {"version": "v2", "created": "Tue, 30 Apr 2019 13:20:01 GMT"}], "update_date": "2019-05-01", "authors_parsed": [["Manderscheid", "Jacques", ""], ["Sironi", "Amos", ""], ["Bourdis", "Nicolas", ""], ["Migliore", "Davide", ""], ["Lepetit", "Vincent", ""]]}, {"id": "1903.11341", "submitter": "Nikita Dvornik", "authors": "Nikita Dvornik, Cordelia Schmid, Julien Mairal", "title": "Diversity with Cooperation: Ensemble Methods for Few-Shot Classification", "comments": "Added experiments for different network architectures across\n  different input image resolutions", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Few-shot classification consists of learning a predictive model that is able\nto effectively adapt to a new class, given only a few annotated samples. To\nsolve this challenging problem, meta-learning has become a popular paradigm\nthat advocates the ability to \"learn to adapt\". Recent works have shown,\nhowever, that simple learning strategies without meta-learning could be\ncompetitive. In this paper, we go a step further and show that by addressing\nthe fundamental high-variance issue of few-shot learning classifiers, it is\npossible to significantly outperform current meta-learning techniques. Our\napproach consists of designing an ensemble of deep networks to leverage the\nvariance of the classifiers, and introducing new strategies to encourage the\nnetworks to cooperate, while encouraging prediction diversity. Evaluation is\nconducted on the mini-ImageNet and CUB datasets, where we show that even a\nsingle network obtained by distillation yields state-of-the-art results.\n", "versions": [{"version": "v1", "created": "Wed, 27 Mar 2019 10:53:22 GMT"}, {"version": "v2", "created": "Fri, 30 Aug 2019 09:34:59 GMT"}], "update_date": "2019-09-02", "authors_parsed": [["Dvornik", "Nikita", ""], ["Schmid", "Cordelia", ""], ["Mairal", "Julien", ""]]}, {"id": "1903.11359", "submitter": "Francesco Croce", "authors": "Francesco Croce, Jonas Rauber, Matthias Hein", "title": "Scaling up the randomized gradient-free adversarial attack reveals\n  overestimation of robustness using established attacks", "comments": "Accepted at International Journal of Computer Vision", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.CV cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern neural networks are highly non-robust against adversarial\nmanipulation. A significant amount of work has been invested in techniques to\ncompute lower bounds on robustness through formal guarantees and to build\nprovably robust models. However, it is still difficult to get guarantees for\nlarger networks or robustness against larger perturbations. Thus attack\nstrategies are needed to provide tight upper bounds on the actual robustness.\nWe significantly improve the randomized gradient-free attack for ReLU networks\n[9], in particular by scaling it up to large networks. We show that our attack\nachieves similar or significantly smaller robust accuracy than state-of-the-art\nattacks like PGD or the one of Carlini and Wagner, thus revealing an\noverestimation of the robustness by these state-of-the-art methods. Our attack\nis not based on a gradient descent scheme and in this sense gradient-free,\nwhich makes it less sensitive to the choice of hyperparameters as no careful\nselection of the stepsize is required.\n", "versions": [{"version": "v1", "created": "Wed, 27 Mar 2019 11:41:27 GMT"}, {"version": "v2", "created": "Wed, 25 Sep 2019 17:04:43 GMT"}], "update_date": "2019-09-26", "authors_parsed": [["Croce", "Francesco", ""], ["Rauber", "Jonas", ""], ["Hein", "Matthias", ""]]}, {"id": "1903.11394", "submitter": "Kuldeep Purohit", "authors": "Kuldeep Purohit and A. N. Rajagopalan", "title": "Region-Adaptive Dense Network for Efficient Motion Deblurring", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we address the problem of dynamic scene deblurring in the\npresence of motion blur. Restoration of images affected by severe blur\nnecessitates a network design with a large receptive field, which existing\nnetworks attempt to achieve through simple increment in the number of generic\nconvolution layers, kernel-size, or the scales at which the image is processed.\nHowever, these techniques ignore the non-uniform nature of blur, and they come\nat the expense of an increase in model size and inference time. We present a\nnew architecture composed of region adaptive dense deformable modules that\nimplicitly discover the spatially varying shifts responsible for non-uniform\nblur in the input image and learn to modulate the filters. This capability is\ncomplemented by a self-attentive module which captures non-local spatial\nrelationships among the intermediate features and enhances the\nspatially-varying processing capability. We incorporate these modules into a\ndensely connected encoder-decoder design which utilizes pre-trained Densenet\nfilters to further improve the performance. Our network facilitates\ninterpretable modeling of the spatially-varying deblurring process while\ndispensing with multi-scale processing and large filters entirely. Extensive\ncomparisons with prior art on benchmark dynamic scene deblurring datasets\nclearly demonstrate the superiority of the proposed networks via significant\nimprovements in accuracy and speed, enabling almost real-time deblurring.\n", "versions": [{"version": "v1", "created": "Mon, 25 Mar 2019 21:13:48 GMT"}, {"version": "v2", "created": "Fri, 29 Mar 2019 20:05:00 GMT"}, {"version": "v3", "created": "Fri, 20 Sep 2019 20:05:39 GMT"}], "update_date": "2019-09-24", "authors_parsed": [["Purohit", "Kuldeep", ""], ["Rajagopalan", "A. N.", ""]]}, {"id": "1903.11412", "submitter": "Xiaohang Zhan", "authors": "Xiaohang Zhan, Xingang Pan, Ziwei Liu, Dahua Lin, Chen Change Loy", "title": "Self-Supervised Learning via Conditional Motion Propagation", "comments": "In CVPR 2019. More details at the project page:\n  http://mmlab.ie.cuhk.edu.hk/projects/CMP/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Intelligent agent naturally learns from motion. Various self-supervised\nalgorithms have leveraged motion cues to learn effective visual\nrepresentations. The hurdle here is that motion is both ambiguous and complex,\nrendering previous works either suffer from degraded learning efficacy, or\nresort to strong assumptions on object motions. In this work, we design a new\nlearning-from-motion paradigm to bridge these gaps. Instead of explicitly\nmodeling the motion probabilities, we design the pretext task as a conditional\nmotion propagation problem. Given an input image and several sparse flow\nguidance vectors on it, our framework seeks to recover the full-image motion.\nCompared to other alternatives, our framework has several appealing properties:\n(1) Using sparse flow guidance during training resolves the inherent motion\nambiguity, and thus easing feature learning. (2) Solving the pretext task of\nconditional motion propagation encourages the emergence of kinematically-sound\nrepresentations that poss greater expressive power. Extensive experiments\ndemonstrate that our framework learns structural and coherent features; and\nachieves state-of-the-art self-supervision performance on several downstream\ntasks including semantic segmentation, instance segmentation, and human\nparsing. Furthermore, our framework is successfully extended to several useful\napplications such as semi-automatic pixel-level annotation. Project page:\n\"http://mmlab.ie.cuhk.edu.hk/projects/CMP/\".\n", "versions": [{"version": "v1", "created": "Wed, 27 Mar 2019 13:24:46 GMT"}, {"version": "v2", "created": "Wed, 3 Apr 2019 12:08:58 GMT"}, {"version": "v3", "created": "Thu, 25 Apr 2019 03:52:57 GMT"}], "update_date": "2019-04-26", "authors_parsed": [["Zhan", "Xiaohang", ""], ["Pan", "Xingang", ""], ["Liu", "Ziwei", ""], ["Lin", "Dahua", ""], ["Loy", "Chen Change", ""]]}, {"id": "1903.11421", "submitter": "Richard Jiang", "authors": "Ziping Jiang, Paul L. Chazot, M. Emre Celebi, Danny Crookes and\n  Richard Jiang", "title": "Social Behavioral Phenotyping of Drosophila with a2D-3D Hybrid CNN\n  Framework", "comments": null, "journal-ref": "IEEE Access 2019", "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.ET cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Behavioural phenotyping of Drosophila is an important means in biological and\nmedical research to identify genetic, pathologic or psychologic impact on\nanimal behaviour.\n", "versions": [{"version": "v1", "created": "Wed, 27 Mar 2019 13:41:17 GMT"}], "update_date": "2019-08-05", "authors_parsed": [["Jiang", "Ziping", ""], ["Chazot", "Paul L.", ""], ["Celebi", "M. Emre", ""], ["Crookes", "Danny", ""], ["Jiang", "Richard", ""]]}, {"id": "1903.11444", "submitter": "Xinzhu Ma", "authors": "Xinzhu Ma, Zhihui Wang, Haojie Li, Pengbo Zhang, Xin Fan, Wanli Ouyang", "title": "Accurate Monocular Object Detection via Color-Embedded 3D Reconstruction\n  for Autonomous Driving", "comments": "To appear in ICCV'19", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a monocular 3D object detection framework in the\ndomain of autonomous driving. Unlike previous image-based methods which focus\non RGB feature extracted from 2D images, our method solves this problem in the\nreconstructed 3D space in order to exploit 3D contexts explicitly. To this end,\nwe first leverage a stand-alone module to transform the input data from 2D\nimage plane to 3D point clouds space for a better input representation, then we\nperform the 3D detection using PointNet backbone net to obtain objects 3D\nlocations, dimensions and orientations. To enhance the discriminative\ncapability of point clouds, we propose a multi-modal feature fusion module to\nembed the complementary RGB cue into the generated point clouds representation.\nWe argue that it is more effective to infer the 3D bounding boxes from the\ngenerated 3D scene space (i.e., X,Y, Z space) compared to the image plane\n(i.e., R,G,B image plane). Evaluation on the challenging KITTI dataset shows\nthat our approach boosts the performance of state-of-the-art monocular approach\nby a large margin.\n", "versions": [{"version": "v1", "created": "Wed, 27 Mar 2019 14:23:44 GMT"}, {"version": "v2", "created": "Mon, 1 Apr 2019 12:15:39 GMT"}, {"version": "v3", "created": "Mon, 12 Aug 2019 10:09:16 GMT"}, {"version": "v4", "created": "Tue, 30 Mar 2021 09:14:19 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Ma", "Xinzhu", ""], ["Wang", "Zhihui", ""], ["Li", "Haojie", ""], ["Zhang", "Pengbo", ""], ["Fan", "Xin", ""], ["Ouyang", "Wanli", ""]]}, {"id": "1903.11508", "submitter": "Steffen Eger", "authors": "Steffen Eger and G\\\"ozde G\\\"ul \\c{S}ahin and Andreas R\\\"uckl\\'e and\n  Ji-Ung Lee and Claudia Schulz and Mohsen Mesgar and Krishnkant Swarnkar and\n  Edwin Simpson and Iryna Gurevych", "title": "Text Processing Like Humans Do: Visually Attacking and Shielding NLP\n  Systems", "comments": "Accepted as long paper at NAACL-2019; fixed one ungrammatical\n  sentence", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CR cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual modifications to text are often used to obfuscate offensive comments\nin social media (e.g., \"!d10t\") or as a writing style (\"1337\" in \"leet speak\"),\namong other scenarios. We consider this as a new type of adversarial attack in\nNLP, a setting to which humans are very robust, as our experiments with both\nsimple and more difficult visual input perturbations demonstrate. We then\ninvestigate the impact of visual adversarial attacks on current NLP systems on\ncharacter-, word-, and sentence-level tasks, showing that both neural and\nnon-neural models are, in contrast to humans, extremely sensitive to such\nattacks, suffering performance decreases of up to 82\\%. We then explore three\nshielding methods---visual character embeddings, adversarial training, and\nrule-based recovery---which substantially improve the robustness of the models.\nHowever, the shielding methods still fall behind performances achieved in\nnon-attack scenarios, which demonstrates the difficulty of dealing with visual\nattacks.\n", "versions": [{"version": "v1", "created": "Wed, 27 Mar 2019 16:01:18 GMT"}, {"version": "v2", "created": "Wed, 10 Jun 2020 12:20:04 GMT"}], "update_date": "2020-06-11", "authors_parsed": [["Eger", "Steffen", ""], ["\u015eahin", "G\u00f6zde G\u00fcl", ""], ["R\u00fcckl\u00e9", "Andreas", ""], ["Lee", "Ji-Ung", ""], ["Schulz", "Claudia", ""], ["Mesgar", "Mohsen", ""], ["Swarnkar", "Krishnkant", ""], ["Simpson", "Edwin", ""], ["Gurevych", "Iryna", ""]]}, {"id": "1903.11532", "submitter": "Clint Sebastian", "authors": "Ries Uittenbogaard, Clint Sebastian, Julien Vijverberg, Bas Boom,\n  Dariu M. Gavrila, Peter H.N. de With", "title": "Privacy Protection in Street-View Panoramas using Depth and Multi-View\n  Imagery", "comments": "Accepted to CVPR 2019. Dataset (and provided link) will be made\n  available before the CVPR", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The current paradigm in privacy protection in street-view images is to detect\nand blur sensitive information. In this paper, we propose a framework that is\nan alternative to blurring, which automatically removes and inpaints moving\nobjects (e.g. pedestrians, vehicles) in street-view imagery. We propose a novel\nmoving object segmentation algorithm exploiting consistencies in depth across\nmultiple street-view images that are later combined with the results of a\nsegmentation network. The detected moving objects are removed and inpainted\nwith information from other views, to obtain a realistic output image such that\nthe moving object is not visible anymore. We evaluate our results on a dataset\nof 1000 images to obtain a peak noise-to-signal ratio (PSNR) and L1 loss of\n27.2 dB and 2.5%, respectively. To ensure the subjective quality, To assess\noverall quality, we also report the results of a survey conducted on 35\nprofessionals, asked to visually inspect the images whether object removal and\ninpainting had taken place. The inpainting dataset will be made publicly\navailable for scientific benchmarking purposes at\nhttps://research.cyclomedia.com\n", "versions": [{"version": "v1", "created": "Wed, 27 Mar 2019 16:34:16 GMT"}], "update_date": "2019-03-28", "authors_parsed": [["Uittenbogaard", "Ries", ""], ["Sebastian", "Clint", ""], ["Vijverberg", "Julien", ""], ["Boom", "Bas", ""], ["Gavrila", "Dariu M.", ""], ["de With", "Peter H. N.", ""]]}, {"id": "1903.11552", "submitter": "Alessandro Borgia", "authors": "Alessandro Borgia and Yang Hua and Elyor Kodirov and Neil M. Robertson", "title": "GAN-based Pose-aware Regulation for Video-based Person Re-identification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Video-based person re-identification deals with the inherent difficulty of\nmatching unregulated sequences with different length and with incomplete target\npose/viewpoint structure. Common approaches operate either by reducing the\nproblem to the still images case, facing a significant information loss, or by\nexploiting inter-sequence temporal dependencies as in Siamese Recurrent Neural\nNetworks or in gait analysis. However, in all cases, the inter-sequences\npose/viewpoint misalignment is not considered, and the existing spatial\napproaches are mostly limited to the still images context. To this end, we\npropose a novel approach that can exploit more effectively the rich video\ninformation, by accounting for the role that the changing pose/viewpoint factor\nplays in the sequences matching process. Specifically, our approach consists of\ntwo components. The first one attempts to complement the original\npose-incomplete information carried by the sequences with synthetic\nGAN-generated images, and fuse their feature vectors into a more discriminative\nviewpoint-insensitive embedding, namely Weighted Fusion (WF). Another one\nperforms an explicit pose-based alignment of sequence pairs to promote coherent\nfeature matching, namely Weighted-Pose Regulation (WPR). Extensive experiments\non two large video-based benchmark datasets show that our approach outperforms\nconsiderably existing methods.\n", "versions": [{"version": "v1", "created": "Wed, 27 Mar 2019 17:14:51 GMT"}], "update_date": "2019-03-28", "authors_parsed": [["Borgia", "Alessandro", ""], ["Hua", "Yang", ""], ["Kodirov", "Elyor", ""], ["Robertson", "Neil M.", ""]]}, {"id": "1903.11593", "submitter": "Stephen Baek", "authors": "Stephen Baek, Yusen He, Bryan G. Allen, John M. Buatti, Brian J.\n  Smith, Ling Tong, Zhiyu Sun, Jia Wu, Maximilian Diehn, Billy W. Loo, Kristin\n  A. Plichta, Steven N. Seyedin, Maggie Gannon, Katherine R. Cabel, Yusung Kim,\n  Xiaodong Wu", "title": "Deep segmentation networks predict survival of non-small cell lung\n  cancer", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.AI cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Non-small-cell lung cancer (NSCLC) represents approximately 80-85% of lung\ncancer diagnoses and is the leading cause of cancer-related death worldwide.\nRecent studies indicate that image-based radiomics features from positron\nemission tomography-computed tomography (PET/CT) images have predictive power\non NSCLC outcomes. To this end, easily calculated functional features such as\nthe maximum and the mean of standard uptake value (SUV) and total lesion\nglycolysis (TLG) are most commonly used for NSCLC prognostication, but their\nprognostic value remains controversial. Meanwhile, convolutional neural\nnetworks (CNN) are rapidly emerging as a new premise for cancer image analysis,\nwith significantly enhanced predictive power compared to other hand-crafted\nradiomics features. Here we show that CNN trained to perform the tumor\nsegmentation task, with no other information than physician contours, identify\na rich set of survival-related image features with remarkable prognostic value.\nIn a retrospective study on 96 NSCLC patients before stereotactic-body\nradiotherapy (SBRT), we found that the CNN segmentation algorithm (U-Net)\ntrained for tumor segmentation in PET/CT images, contained features having\nstrong correlation with 2- and 5-year overall and disease-specific survivals.\nThe U-net algorithm has not seen any other clinical information (e.g. survival,\nage, smoking history) than the images and the corresponding tumor contours\nprovided by physicians. Furthermore, through visualization of the U-Net, we\nalso found convincing evidence that the regions of progression appear to match\nwith the regions where the U-Net features identified patterns that predicted\nhigher likelihood of death. We anticipate our findings will be a starting point\nfor more sophisticated non-intrusive patient specific cancer prognosis\ndetermination.\n", "versions": [{"version": "v1", "created": "Tue, 26 Mar 2019 19:55:44 GMT"}, {"version": "v2", "created": "Fri, 8 Nov 2019 22:42:56 GMT"}], "update_date": "2019-11-12", "authors_parsed": [["Baek", "Stephen", ""], ["He", "Yusen", ""], ["Allen", "Bryan G.", ""], ["Buatti", "John M.", ""], ["Smith", "Brian J.", ""], ["Tong", "Ling", ""], ["Sun", "Zhiyu", ""], ["Wu", "Jia", ""], ["Diehn", "Maximilian", ""], ["Loo", "Billy W.", ""], ["Plichta", "Kristin A.", ""], ["Seyedin", "Steven N.", ""], ["Gannon", "Maggie", ""], ["Cabel", "Katherine R.", ""], ["Kim", "Yusung", ""], ["Wu", "Xiaodong", ""]]}, {"id": "1903.11633", "submitter": "Joseph Robinson", "authors": "Joseph P Robinson and Yuncheng Li and Ning Zhang and Yun Fu and and\n  Sergey Tulyakov", "title": "Laplace Landmark Localization", "comments": "International Conference on Computer Vision (ICCV), 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Landmark localization in images and videos is a classic problem solved in\nvarious ways. Nowadays, with deep networks prevailing throughout machine\nlearning, there are revamped interests in pushing facial landmark detection\ntechnologies to handle more challenging data. Most efforts use network\nobjectives based on L1 or L2 norms, which have several disadvantages. First of\nall, the locations of landmarks are determined from generated heatmaps (i.e.,\nconfidence maps) from which predicted landmark locations (i.e., the means) get\npenalized without accounting for the spread: a high scatter corresponds to low\nconfidence and vice-versa. For this, we introduce a LaplaceKL objective that\npenalizes for a low confidence. Another issue is a dependency on labeled data,\nwhich are expensive to obtain and susceptible to error. To address both issues\nwe propose an adversarial training framework that leverages unlabeled data to\nimprove model performance. Our method claims state-of-the-art on all of the\n300W benchmarks and ranks second-to-best on the Annotated Facial Landmarks in\nthe Wild (AFLW) dataset. Furthermore, our model is robust with a reduced size:\n1/8 the number of channels (i.e., 0.0398MB) is comparable to state-of-that-art\nin real-time on CPU. Thus, we show that our method is of high practical value\nto real-life application.\n", "versions": [{"version": "v1", "created": "Wed, 27 Mar 2019 18:14:50 GMT"}, {"version": "v2", "created": "Wed, 14 Aug 2019 20:16:35 GMT"}], "update_date": "2019-08-16", "authors_parsed": [["Robinson", "Joseph P", ""], ["Li", "Yuncheng", ""], ["Zhang", "Ning", ""], ["Fu", "Yun", ""], ["Tulyakov", "and Sergey", ""]]}, {"id": "1903.11649", "submitter": "Samyak Datta", "authors": "Samyak Datta, Karan Sikka, Anirban Roy, Karuna Ahuja, Devi Parikh,\n  Ajay Divakaran", "title": "Align2Ground: Weakly Supervised Phrase Grounding Guided by Image-Caption\n  Alignment", "comments": "v2 contains phrase localization results on Flickr30k Entities.\n  Accepted for publication at ICCV 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of grounding free-form textual phrases by using weak\nsupervision from image-caption pairs. We propose a novel end-to-end model that\nuses caption-to-image retrieval as a `downstream' task to guide the process of\nphrase localization. Our method, as a first step, infers the latent\ncorrespondences between regions-of-interest (RoIs) and phrases in the caption\nand creates a discriminative image representation using these matched RoIs. In\na subsequent step, this (learned) representation is aligned with the caption.\nOur key contribution lies in building this `caption-conditioned' image encoding\nwhich tightly couples both the tasks and allows the weak supervision to\neffectively guide visual grounding. We provide an extensive empirical and\nqualitative analysis to investigate the different components of our proposed\nmodel and compare it with competitive baselines. For phrase localization, we\nreport an improvement of 4.9% (absolute) over the prior state-of-the-art on the\nVisualGenome dataset. We also report results that are at par with the\nstate-of-the-art on the downstream caption-to-image retrieval task on COCO and\nFlickr30k datasets.\n", "versions": [{"version": "v1", "created": "Wed, 27 Mar 2019 18:55:32 GMT"}, {"version": "v2", "created": "Tue, 15 Oct 2019 17:09:03 GMT"}], "update_date": "2019-10-16", "authors_parsed": [["Datta", "Samyak", ""], ["Sikka", "Karan", ""], ["Roy", "Anirban", ""], ["Ahuja", "Karuna", ""], ["Parikh", "Devi", ""], ["Divakaran", "Ajay", ""]]}, {"id": "1903.11683", "submitter": "Vasileios Tzoumas", "authors": "Vasileios Tzoumas, Pasquale Antonante, Luca Carlone", "title": "Outlier-Robust Spatial Perception: Hardness, General-Purpose Algorithms,\n  and Guarantees", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG cs.RO cs.SY stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spatial perception is the backbone of many robotics applications, and spans a\nbroad range of research problems, including localization and mapping, point\ncloud alignment, and relative pose estimation from camera images. Robust\nspatial perception is jeopardized by the presence of incorrect data\nassociation, and in general, outliers. Although techniques to handle outliers\ndo exist, they can fail in unpredictable manners (e.g., RANSAC, robust\nestimators), or can have exponential runtime (e.g., branch-and-bound). In this\npaper, we advance the state of the art in outlier rejection by making three\ncontributions. First, we show that even a simple linear instance of outlier\nrejection is inapproximable: in the worst-case one cannot design a\nquasi-polynomial time algorithm that computes an approximate solution\nefficiently. Our second contribution is to provide the first per-instance\nsub-optimality bounds to assess the approximation quality of a given outlier\nrejection outcome. Our third contribution is to propose a simple\ngeneral-purpose algorithm, named adaptive trimming, to remove outliers. Our\nalgorithm leverages recently-proposed global solvers that are able to solve\noutlier-free problems, and iteratively removes measurements with large errors.\nWe demonstrate the proposed algorithm on three spatial perception problems: 3D\nregistration, two-view geometry, and SLAM. The results show that our algorithm\noutperforms several state-of-the-art methods across applications while being a\ngeneral-purpose method.\n", "versions": [{"version": "v1", "created": "Wed, 27 Mar 2019 20:12:37 GMT"}, {"version": "v2", "created": "Mon, 29 Jul 2019 19:46:50 GMT"}], "update_date": "2019-07-31", "authors_parsed": [["Tzoumas", "Vasileios", ""], ["Antonante", "Pasquale", ""], ["Carlone", "Luca", ""]]}, {"id": "1903.11701", "submitter": "Debasmit Das", "authors": "Debasmit Das and C. S. George Lee", "title": "Zero-shot Image Recognition Using Relational Matching, Adaptation and\n  Calibration", "comments": "International Joint Conference on Neural Networks (IJCNN) 2019.\n  Copyright 2019 IEEE", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Zero-shot learning (ZSL) for image classification focuses on recognizing\nnovel categories that have no labeled data available for training. The learning\nis generally carried out with the help of mid-level semantic descriptors\nassociated with each class. This semantic-descriptor space is generally shared\nby both seen and unseen categories. However, ZSL suffers from hubness, domain\ndiscrepancy and biased-ness towards seen classes. To tackle these problems, we\npropose a three-step approach to zero-shot learning. Firstly, a mapping is\nlearned from the semantic-descriptor space to the image-feature space. This\nmapping learns to minimize both one-to-one and pairwise distances between\nsemantic embeddings and the image features of the corresponding classes.\nSecondly, we propose test-time domain adaptation to adapt the semantic\nembedding of the unseen classes to the test data. This is achieved by finding\ncorrespondences between the semantic descriptors and the image features.\nThirdly, we propose scaled calibration on the classification scores of the seen\nclasses. This is necessary because the ZSL model is biased towards seen classes\nas the unseen classes are not used in the training. Finally, to validate the\nproposed three-step approach, we performed experiments on four benchmark\ndatasets where the proposed method outperformed previous results. We also\nstudied and analyzed the performance of each component of our proposed ZSL\nframework.\n", "versions": [{"version": "v1", "created": "Wed, 27 Mar 2019 21:07:00 GMT"}], "update_date": "2019-03-29", "authors_parsed": [["Das", "Debasmit", ""], ["Lee", "C. S. George", ""]]}, {"id": "1903.11728", "submitter": "Jiahui Yu", "authors": "Jiahui Yu, Thomas Huang", "title": "AutoSlim: Towards One-Shot Architecture Search for Channel Numbers", "comments": "tech report", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study how to set channel numbers in a neural network to achieve better\naccuracy under constrained resources (e.g., FLOPs, latency, memory footprint or\nmodel size). A simple and one-shot solution, named AutoSlim, is presented.\nInstead of training many network samples and searching with reinforcement\nlearning, we train a single slimmable network to approximate the network\naccuracy of different channel configurations. We then iteratively evaluate the\ntrained slimmable model and greedily slim the layer with minimal accuracy drop.\nBy this single pass, we can obtain the optimized channel configurations under\ndifferent resource constraints. We present experiments with MobileNet v1,\nMobileNet v2, ResNet-50 and RL-searched MNasNet on ImageNet classification. We\nshow significant improvements over their default channel configurations. We\nalso achieve better accuracy than recent channel pruning methods and neural\narchitecture search methods.\n  Notably, by setting optimized channel numbers, our AutoSlim-MobileNet-v2 at\n305M FLOPs achieves 74.2% top-1 accuracy, 2.4% better than default MobileNet-v2\n(301M FLOPs), and even 0.2% better than RL-searched MNasNet (317M FLOPs). Our\nAutoSlim-ResNet-50 at 570M FLOPs, without depthwise convolutions, achieves 1.3%\nbetter accuracy than MobileNet-v1 (569M FLOPs). Code and models will be\navailable at: https://github.com/JiahuiYu/slimmable_networks\n", "versions": [{"version": "v1", "created": "Wed, 27 Mar 2019 23:17:28 GMT"}, {"version": "v2", "created": "Sat, 1 Jun 2019 03:19:54 GMT"}], "update_date": "2019-06-04", "authors_parsed": [["Yu", "Jiahui", ""], ["Huang", "Thomas", ""]]}, {"id": "1903.11741", "submitter": "Saeid Asgari Taghanaki", "authors": "Saeid Asgari Taghanaki, Mohammad Havaei, Tess Berthier, Francis Dutil,\n  Lisa Di Jorio, Ghassan Hamarneh, Yoshua Bengio", "title": "InfoMask: Masked Variational Latent Representation to Localize Chest\n  Disease", "comments": "Accepted to MICCAI 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The scarcity of richly annotated medical images is limiting supervised deep\nlearning based solutions to medical image analysis tasks, such as localizing\ndiscriminatory radiomic disease signatures. Therefore, it is desirable to\nleverage unsupervised and weakly supervised models. Most recent weakly\nsupervised localization methods apply attention maps or region proposals in a\nmultiple instance learning formulation. While attention maps can be noisy,\nleading to erroneously highlighted regions, it is not simple to decide on an\noptimal window/bag size for multiple instance learning approaches. In this\npaper, we propose a learned spatial masking mechanism to filter out irrelevant\nbackground signals from attention maps. The proposed method minimizes mutual\ninformation between a masked variational representation and the input while\nmaximizing the information between the masked representation and class labels.\nThis results in more accurate localization of discriminatory regions. We tested\nthe proposed model on the ChestX-ray8 dataset to localize pneumonia from chest\nX-ray images without using any pixel-level or bounding-box annotations.\n", "versions": [{"version": "v1", "created": "Thu, 28 Mar 2019 00:39:34 GMT"}, {"version": "v2", "created": "Fri, 7 Jun 2019 01:42:13 GMT"}], "update_date": "2019-06-10", "authors_parsed": [["Taghanaki", "Saeid Asgari", ""], ["Havaei", "Mohammad", ""], ["Berthier", "Tess", ""], ["Dutil", "Francis", ""], ["Di Jorio", "Lisa", ""], ["Hamarneh", "Ghassan", ""], ["Bengio", "Yoshua", ""]]}, {"id": "1903.11752", "submitter": "Zheng Qin", "authors": "Zheng Qin, Zeming Li, Zhaoning Zhang, Yiping Bao, Gang Yu, Yuxing\n  Peng, Jian Sun", "title": "ThunderNet: Towards Real-time Generic Object Detection", "comments": "10 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Real-time generic object detection on mobile platforms is a crucial but\nchallenging computer vision task. However, previous CNN-based detectors suffer\nfrom enormous computational cost, which hinders them from real-time inference\nin computation-constrained scenarios. In this paper, we investigate the\neffectiveness of two-stage detectors in real-time generic detection and propose\na lightweight two-stage detector named ThunderNet. In the backbone part, we\nanalyze the drawbacks in previous lightweight backbones and present a\nlightweight backbone designed for object detection. In the detection part, we\nexploit an extremely efficient RPN and detection head design. To generate more\ndiscriminative feature representation, we design two efficient architecture\nblocks, Context Enhancement Module and Spatial Attention Module. At last, we\ninvestigate the balance between the input resolution, the backbone, and the\ndetection head. Compared with lightweight one-stage detectors, ThunderNet\nachieves superior performance with only 40% of the computational cost on PASCAL\nVOC and COCO benchmarks. Without bells and whistles, our model runs at 24.1 fps\non an ARM-based device. To the best of our knowledge, this is the first\nreal-time detector reported on ARM platforms. Code will be released for paper\nreproduction.\n", "versions": [{"version": "v1", "created": "Thu, 28 Mar 2019 01:48:09 GMT"}, {"version": "v2", "created": "Fri, 26 Jul 2019 03:24:07 GMT"}], "update_date": "2019-07-29", "authors_parsed": [["Qin", "Zheng", ""], ["Li", "Zeming", ""], ["Zhang", "Zhaoning", ""], ["Bao", "Yiping", ""], ["Yu", "Gang", ""], ["Peng", "Yuxing", ""], ["Sun", "Jian", ""]]}, {"id": "1903.11779", "submitter": "Brent Griffin Dr", "authors": "Brent A. Griffin and Jason J. Corso", "title": "BubbleNets: Learning to Select the Guidance Frame in Video Object\n  Segmentation by Deep Sorting Frames", "comments": "CVPR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semi-supervised video object segmentation has made significant progress on\nreal and challenging videos in recent years. The current paradigm for\nsegmentation methods and benchmark datasets is to segment objects in video\nprovided a single annotation in the first frame. However, we find that\nsegmentation performance across the entire video varies dramatically when\nselecting an alternative frame for annotation. This paper address the problem\nof learning to suggest the single best frame across the video for user\nannotation-this is, in fact, never the first frame of video. We achieve this by\nintroducing BubbleNets, a novel deep sorting network that learns to select\nframes using a performance-based loss function that enables the conversion of\nexpansive amounts of training examples from already existing datasets. Using\nBubbleNets, we are able to achieve an 11% relative improvement in segmentation\nperformance on the DAVIS benchmark without any changes to the underlying method\nof segmentation.\n", "versions": [{"version": "v1", "created": "Thu, 28 Mar 2019 03:42:30 GMT"}, {"version": "v2", "created": "Tue, 24 Nov 2020 01:52:04 GMT"}], "update_date": "2020-11-25", "authors_parsed": [["Griffin", "Brent A.", ""], ["Corso", "Jason J.", ""]]}, {"id": "1903.11785", "submitter": "Jun Chen", "authors": "Jun Chen, Ryosuke Watanabe, Keisuke Nonaka, Tomoaki Konno, Hiroshi\n  Sankoh, Sei Naito", "title": "A Fast Free-viewpoint Video Synthesis Algorithm for Sports Scenes", "comments": "7 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we report on a parallel freeviewpoint video synthesis\nalgorithm that can efficiently reconstruct a high-quality 3D scene\nrepresentation of sports scenes. The proposed method focuses on a scene that is\ncaptured by multiple synchronized cameras featuring wide-baselines. The\nfollowing strategies are introduced to accelerate the production of a\nfree-viewpoint video taking the improvement of visual quality into account: (1)\na sparse point cloud is reconstructed using a volumetric visual hull approach,\nand an exact 3D ROI is found for each object using an efficient connected\ncomponents labeling algorithm. Next, the reconstruction of a dense point cloud\nis accelerated by implementing visual hull only in the ROIs; (2) an accurate\npolyhedral surface mesh is built by estimating the exact intersections between\ngrid cells and the visual hull; (3) the appearance of the reconstructed\npresentation is reproduced in a view-dependent manner that respectively renders\nthe non-occluded and occluded region with the nearest camera and its\nneighboring cameras. The production for volleyball and judo sequences\ndemonstrates the effectiveness of our method in terms of both execution time\nand visual quality.\n", "versions": [{"version": "v1", "created": "Thu, 28 Mar 2019 05:02:30 GMT"}, {"version": "v2", "created": "Wed, 31 Jul 2019 02:19:06 GMT"}], "update_date": "2019-08-01", "authors_parsed": [["Chen", "Jun", ""], ["Watanabe", "Ryosuke", ""], ["Nonaka", "Keisuke", ""], ["Konno", "Tomoaki", ""], ["Sankoh", "Hiroshi", ""], ["Naito", "Sei", ""]]}, {"id": "1903.11800", "submitter": "Xuebo Liu", "authors": "Jingchao Liu, Xuebo Liu, Jie Sheng, Ding Liang, Xin Li, Qingjie Liu", "title": "Pyramid Mask Text Detector", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scene text detection, an essential step of scene text recognition system, is\nto locate text instances in natural scene images automatically. Some recent\nattempts benefiting from Mask R-CNN formulate scene text detection task as an\ninstance segmentation problem and achieve remarkable performance. In this\npaper, we present a new Mask R-CNN based framework named Pyramid Mask Text\nDetector (PMTD) to handle the scene text detection. Instead of binary text mask\ngenerated by the existing Mask R-CNN based methods, our PMTD performs\npixel-level regression under the guidance of location-aware supervision,\nyielding a more informative soft text mask for each text instance. As for the\ngeneration of text boxes, PMTD reinterprets the obtained 2D soft mask into 3D\nspace and introduces a novel plane clustering algorithm to derive the optimal\ntext box on the basis of 3D shape. Experiments on standard datasets demonstrate\nthat the proposed PMTD brings consistent and noticeable gain and clearly\noutperforms state-of-the-art methods. Specifically, it achieves an F-measure of\n80.13% on ICDAR 2017 MLT dataset.\n", "versions": [{"version": "v1", "created": "Thu, 28 Mar 2019 06:38:21 GMT"}], "update_date": "2019-03-29", "authors_parsed": [["Liu", "Jingchao", ""], ["Liu", "Xuebo", ""], ["Sheng", "Jie", ""], ["Liang", "Ding", ""], ["Li", "Xin", ""], ["Liu", "Qingjie", ""]]}, {"id": "1903.11816", "submitter": "Huikai Wu", "authors": "Huikai Wu, Junge Zhang, Kaiqi Huang, Kongming Liang, Yizhou Yu", "title": "FastFCN: Rethinking Dilated Convolution in the Backbone for Semantic\n  Segmentation", "comments": "Code is available in https://github.com/wuhuikai/FastFCN", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Modern approaches for semantic segmentation usually employ dilated\nconvolutions in the backbone to extract high-resolution feature maps, which\nbrings heavy computation complexity and memory footprint. To replace the time\nand memory consuming dilated convolutions, we propose a novel joint upsampling\nmodule named Joint Pyramid Upsampling (JPU) by formulating the task of\nextracting high-resolution feature maps into a joint upsampling problem. With\nthe proposed JPU, our method reduces the computation complexity by more than\nthree times without performance loss. Experiments show that JPU is superior to\nother upsampling modules, which can be plugged into many existing approaches to\nreduce computation complexity and improve performance. By replacing dilated\nconvolutions with the proposed JPU module, our method achieves the\nstate-of-the-art performance in Pascal Context dataset (mIoU of 53.13%) and\nADE20K dataset (final score of 0.5584) while running 3 times faster.\n", "versions": [{"version": "v1", "created": "Thu, 28 Mar 2019 07:49:36 GMT"}], "update_date": "2019-03-29", "authors_parsed": [["Wu", "Huikai", ""], ["Zhang", "Junge", ""], ["Huang", "Kaiqi", ""], ["Liang", "Kongming", ""], ["Yu", "Yizhou", ""]]}, {"id": "1903.11834", "submitter": "Chen Xueying", "authors": "Xueying Chen, Rong Zhang, Pingkun Yan", "title": "Feature Fusion Encoder Decoder Network For Automatic Liver Lesion\n  Segmentation", "comments": "4 pages, 2 figures, conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Liver lesion segmentation is a difficult yet critical task for medical image\nanalysis. Recently, deep learning based image segmentation methods have\nachieved promising performance, which can be divided into three categories: 2D,\n2.5D and 3D, based on the dimensionality of the models. However, 2.5D and 3D\nmethods can have very high complexity and 2D methods may not perform\nsatisfactorily. To obtain competitive performance with low complexity, in this\npaper, we propose a Feature-fusion Encoder-Decoder Network (FED-Net) based 2D\nsegmentation model to tackle the challenging problem of liver lesion\nsegmentation from CT images. Our feature fusion method is based on the\nattention mechanism, which fuses high-level features carrying semantic\ninformation with low-level features having image details. Additionally, to\ncompensate for the information loss during the upsampling process, a dense\nupsampling convolution and a residual convolutional structure are proposed. We\ntested our method on the dataset of MICCAI 2017 Liver Tumor Segmentation (LiTS)\nChallenge and achieved competitive results compared with other state-of-the-art\nmethods.\n", "versions": [{"version": "v1", "created": "Thu, 28 Mar 2019 08:39:49 GMT"}], "update_date": "2019-03-29", "authors_parsed": [["Chen", "Xueying", ""], ["Zhang", "Rong", ""], ["Yan", "Pingkun", ""]]}, {"id": "1903.11851", "submitter": "Hongyang Li", "authors": "Hongyang Li and Bo Dai and Shaoshuai Shi and Wanli Ouyang and Xiaogang\n  Wang", "title": "Feature Intertwiner for Object Detection", "comments": "ICLR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  A well-trained model should classify objects with a unanimous score for every\ncategory. This requires the high-level semantic features should be as much\nalike as possible among samples. To achive this, previous works focus on\nre-designing the loss or proposing new regularization constraints. In this\npaper, we provide a new perspective. For each category, it is assumed that\nthere are two feature sets: one with reliable information and the other with\nless reliable source. We argue that the reliable set could guide the feature\nlearning of the less reliable set during training - in spirit of student\nmimicking teacher behavior and thus pushing towards a more compact class\ncentroid in the feature space. Such a scheme also benefits the reliable set\nsince samples become closer within the same category - implying that it is\neasier for the classifier to identify. We refer to this mutual learning process\nas feature intertwiner and embed it into object detection. It is well-known\nthat objects of low resolution are more difficult to detect due to the loss of\ndetailed information during network forward pass (e.g., RoI operation). We thus\nregard objects of high resolution as the reliable set and objects of low\nresolution as the less reliable set. Specifically, an intertwiner is designed\nto minimize the distribution divergence between two sets. The choice of\ngenerating an effective feature representation for the reliable set is further\ninvestigated, where we introduce the optimal transport (OT) theory into the\nframework. Samples in the less reliable set are better aligned with aid of OT\nmetric. Incorporated with such a plug-and-play intertwiner, we achieve an\nevident improvement over previous state-of-the-arts.\n", "versions": [{"version": "v1", "created": "Thu, 28 Mar 2019 09:31:04 GMT"}], "update_date": "2019-03-29", "authors_parsed": [["Li", "Hongyang", ""], ["Dai", "Bo", ""], ["Shi", "Shaoshuai", ""], ["Ouyang", "Wanli", ""], ["Wang", "Xiaogang", ""]]}, {"id": "1903.11862", "submitter": "Yannis Avrithis", "authors": "Hanwei Zhang, Yannis Avrithis, Teddy Furon, Laurent Amsaleg", "title": "Smooth Adversarial Examples", "comments": null, "journal-ref": null, "doi": "10.1186/s13635-020-00112-z", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper investigates the visual quality of the adversarial examples.\nRecent papers propose to smooth the perturbations to get rid of high frequency\nartefacts. In this work, smoothing has a different meaning as it perceptually\nshapes the perturbation according to the visual content of the image to be\nattacked. The perturbation becomes locally smooth on the flat areas of the\ninput image, but it may be noisy on its textured areas and sharp across its\nedges.\n  This operation relies on Laplacian smoothing, well-known in graph signal\nprocessing, which we integrate in the attack pipeline. We benchmark several\nattacks with and without smoothing under a white-box scenario and evaluate\ntheir transferability. Despite the additional constraint of smoothness, our\nattack has the same probability of success at lower distortion.\n", "versions": [{"version": "v1", "created": "Thu, 28 Mar 2019 09:50:28 GMT"}], "update_date": "2021-01-13", "authors_parsed": [["Zhang", "Hanwei", ""], ["Avrithis", "Yannis", ""], ["Furon", "Teddy", ""], ["Amsaleg", "Laurent", ""]]}, {"id": "1903.11891", "submitter": "Guangcun Shan", "authors": "Tian Wang, Zichen Miao, Yuxin Chen, Yi Zhou, Guangcun Shan, Hichem\n  Snoussi", "title": "AED-Net: An Abnormal Event Detection Network", "comments": "14 pages, 7 figures", "journal-ref": "Engineering, 2019", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  It is challenging to detect the anomaly in crowded scenes for quite a long\ntime. In this paper, a self-supervised framework, abnormal event detection\nnetwork (AED-Net), which is composed of PCAnet and kernel principal component\nanalysis (kPCA), is proposed to address this problem. Using surveillance video\nsequences of different scenes as raw data, PCAnet is trained to extract\nhigh-level semantics of crowd's situation. Next, kPCA,a one-class classifier,\nis trained to determine anomaly of the scene. In contrast to some prevailing\ndeep learning methods,the framework is completely self-supervised because it\nutilizes only video sequences in a normal situation. Experiments of global and\nlocal abnormal event detection are carried out on UMN and UCSD datasets, and\ncompetitive results with higher EER and AUC compared to other state-of-the-art\nmethods are observed. Furthermore, by adding local response normalization (LRN)\nlayer, we propose an improvement to original AED-Net. And it is proved to\nperform better by promoting the framework's generalization capacity according\nto the experiments.\n", "versions": [{"version": "v1", "created": "Thu, 28 Mar 2019 10:49:57 GMT"}], "update_date": "2019-03-29", "authors_parsed": [["Wang", "Tian", ""], ["Miao", "Zichen", ""], ["Chen", "Yuxin", ""], ["Zhou", "Yi", ""], ["Shan", "Guangcun", ""], ["Snoussi", "Hichem", ""]]}, {"id": "1903.11900", "submitter": "Riccardo Volpi", "authors": "Riccardo Volpi, Vittorio Murino", "title": "Addressing Model Vulnerability to Distributional Shifts over Image\n  Transformation Sets", "comments": "ICCV 2019 (camera ready)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We are concerned with the vulnerability of computer vision models to\ndistributional shifts. We formulate a combinatorial optimization problem that\nallows evaluating the regions in the image space where a given model is more\nvulnerable, in terms of image transformations applied to the input, and face it\nwith standard search algorithms. We further embed this idea in a training\nprocedure, where we define new data augmentation rules according to the image\ntransformations that the current model is most vulnerable to, over iterations.\nAn empirical evaluation on classification and semantic segmentation problems\nsuggests that the devised algorithm allows to train models that are more robust\nagainst content-preserving image manipulations and, in general, against\ndistributional shifts.\n", "versions": [{"version": "v1", "created": "Thu, 28 Mar 2019 11:24:38 GMT"}, {"version": "v2", "created": "Tue, 20 Aug 2019 09:51:24 GMT"}], "update_date": "2019-08-21", "authors_parsed": [["Volpi", "Riccardo", ""], ["Murino", "Vittorio", ""]]}, {"id": "1903.11991", "submitter": "Maximus Mutschler", "authors": "Maximus Mutschler and Andreas Zell", "title": "Parabolic Approximation Line Search for DNNs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A major challenge in current optimization research for deep learning is to\nautomatically find optimal step sizes for each update step. The optimal step\nsize is closely related to the shape of the loss in the update step direction.\nHowever, this shape has not yet been examined in detail. This work shows\nempirically that the batch loss over lines in negative gradient direction is\nmostly convex locally and well suited for one-dimensional parabolic\napproximations. By exploiting this parabolic property we introduce a simple and\nrobust line search approach, which performs loss-shape dependent update steps.\nOur approach combines well-known methods such as parabolic approximation, line\nsearch and conjugate gradient, to perform efficiently. It surpasses other step\nsize estimating methods and competes with common optimization methods on a\nlarge variety of experiments without the need of hand-designed step size\nschedules. Thus, it is of interest for objectives where step-size schedules are\nunknown or do not perform well. Our extensive evaluation includes multiple\ncomprehensive hyperparameter grid searches on several datasets and\narchitectures. Finally, we provide a general investigation of exact line\nsearches in the context of batch losses and exact losses, including their\nrelation to our line search approach.\n", "versions": [{"version": "v1", "created": "Thu, 28 Mar 2019 14:13:21 GMT"}, {"version": "v2", "created": "Tue, 26 Nov 2019 10:03:30 GMT"}, {"version": "v3", "created": "Wed, 3 Jun 2020 11:17:22 GMT"}, {"version": "v4", "created": "Wed, 28 Oct 2020 14:06:03 GMT"}], "update_date": "2020-10-29", "authors_parsed": [["Mutschler", "Maximus", ""], ["Zell", "Andreas", ""]]}, {"id": "1903.12003", "submitter": "Chaoyou Fu", "authors": "Chaoyou Fu, Yibo Hu, Xiang Wu, Guoli Wang, Qian Zhang, Ran He", "title": "High Fidelity Face Manipulation with Extreme Poses and Expressions", "comments": "Accepted by IEEE Transactions on Information Forensics and Security\n  (TIFS)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Face manipulation has shown remarkable advances with the flourish of\nGenerative Adversarial Networks. However, due to the difficulties of\ncontrolling structures and textures, it is challenging to model poses and\nexpressions simultaneously, especially for the extreme manipulation at\nhigh-resolution. In this paper, we propose a novel framework that simplifies\nface manipulation into two correlated stages: a boundary prediction stage and a\ndisentangled face synthesis stage. The first stage models poses and expressions\njointly via boundary images. Specifically, a conditional encoder-decoder\nnetwork is employed to predict the boundary image of the target face in a\nsemi-supervised way. Pose and expression estimators are introduced to improve\nthe prediction performance. In the second stage, the predicted boundary image\nand the input face image are encoded into the structure and the texture latent\nspace by two encoder networks, respectively. A proxy network and a feature\nthreshold loss are further imposed to disentangle the latent space.\nFurthermore, due to the lack of high-resolution face manipulation databases to\nverify the effectiveness of our method, we collect a new high-quality\nMulti-View Face (MVF-HQ) database. It contains 120,283 images at 6000x4000\nresolution from 479 identities with diverse poses, expressions, and\nilluminations. MVF-HQ is much larger in scale and much higher in resolution\nthan publicly available high-resolution face manipulation databases. We will\nrelease MVF-HQ soon to push forward the advance of face manipulation.\nQualitative and quantitative experiments on four databases show that our method\ndramatically improves the synthesis quality.\n", "versions": [{"version": "v1", "created": "Thu, 28 Mar 2019 14:25:04 GMT"}, {"version": "v2", "created": "Tue, 26 Nov 2019 08:23:53 GMT"}, {"version": "v3", "created": "Mon, 20 Jul 2020 09:46:51 GMT"}, {"version": "v4", "created": "Sat, 16 Jan 2021 09:39:48 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Fu", "Chaoyou", ""], ["Hu", "Yibo", ""], ["Wu", "Xiang", ""], ["Wang", "Guoli", ""], ["Zhang", "Qian", ""], ["He", "Ran", ""]]}, {"id": "1903.12020", "submitter": "Qingzhong Wang", "authors": "Qingzhong Wang and Antoni B. Chan", "title": "Describing like humans: on diversity in image captioning", "comments": "Accepted by CVPR2019. In this version, we correct the label of y axis\n  in figure 8", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, the state-of-the-art models for image captioning have overtaken\nhuman performance based on the most popular metrics, such as BLEU, METEOR,\nROUGE, and CIDEr. Does this mean we have solved the task of image captioning?\nThe above metrics only measure the similarity of the generated caption to the\nhuman annotations, which reflects its accuracy. However, an image contains many\nconcepts and multiple levels of detail, and thus there is a variety of captions\nthat express different concepts and details that might be interesting for\ndifferent humans. Therefore only evaluating accuracy is not sufficient for\nmeasuring the performance of captioning models --- the diversity of the\ngenerated captions should also be considered. In this paper, we proposed a new\nmetric for measuring the diversity of image captions, which is derived from\nlatent semantic analysis and kernelized to use CIDEr similarity. We conduct\nextensive experiments to re-evaluate recent captioning models in the context of\nboth diversity and accuracy. We find that there is still a large gap between\nthe model and human performance in terms of both accuracy and diversity and the\nmodels that have optimized accuracy (CIDEr) have low diversity. We also show\nthat balancing the cross-entropy loss and CIDEr reward in reinforcement\nlearning during training can effectively control the tradeoff between diversity\nand accuracy of the generated captions.\n", "versions": [{"version": "v1", "created": "Thu, 28 Mar 2019 14:49:14 GMT"}, {"version": "v2", "created": "Mon, 1 Apr 2019 14:45:54 GMT"}, {"version": "v3", "created": "Wed, 15 May 2019 03:09:16 GMT"}], "update_date": "2019-05-16", "authors_parsed": [["Wang", "Qingzhong", ""], ["Chan", "Antoni B.", ""]]}, {"id": "1903.12049", "submitter": "Hughes Perreault", "authors": "Hughes Perreault, Guillaume-Alexandre Bilodeau, Nicolas Saunier,\n  Pierre Gravel", "title": "Road User Detection in Videos", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Successive frames of a video are highly redundant, and the most popular\nobject detection methods do not take advantage of this fact. Using multiple\nconsecutive frames can improve detection of small objects or difficult examples\nand can improve speed and detection consistency in a video sequence, for\ninstance by interpolating features between frames. In this work, a novel\napproach is introduced to perform online video object detection using two\nconsecutive frames of video sequences involving road users. Two new models,\nRetinaNet-Double and RetinaNet-Flow, are proposed, based respectively on the\nconcatenation of a target frame with a preceding frame, and the concatenation\nof the optical flow with the target frame. The models are trained and evaluated\non three public datasets. Experiments show that using a preceding frame\nimproves performance over single frame detectors, but using explicit optical\nflow usually does not.\n", "versions": [{"version": "v1", "created": "Thu, 28 Mar 2019 15:28:07 GMT"}], "update_date": "2019-03-29", "authors_parsed": [["Perreault", "Hughes", ""], ["Bilodeau", "Guillaume-Alexandre", ""], ["Saunier", "Nicolas", ""], ["Gravel", "Pierre", ""]]}, {"id": "1903.12061", "submitter": "Dizhong Zhu", "authors": "Dizhong Zhu, William A.P. Smith", "title": "Depth from a polarisation + RGB stereo pair", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a hybrid depth imaging system in which a\npolarisation camera is augmented by a second image from a standard digital\ncamera. For this modest increase in equipment complexity over conventional\nshape-from-polarisation, we obtain a number of benefits that enable us to\novercome longstanding problems with the polarisation shape cue. The stereo cue\nprovides a depth map which, although coarse, is metrically accurate. This is\nused as a guide surface for disambiguation of the polarisation surface normal\nestimates using a higher order graphical model. In turn, these are used to\nestimate diffuse albedo. By extending a previous shape-from-polarisation method\nto the perspective case, we show how to compute dense, detailed maps of\nabsolute depth, while retaining a linear formulation. We show that our hybrid\nmethod is able to recover dense 3D geometry that is superior to\nstate-of-the-art shape-from-polarisation or two view stereo alone.\n", "versions": [{"version": "v1", "created": "Thu, 28 Mar 2019 15:45:53 GMT"}, {"version": "v2", "created": "Wed, 3 Apr 2019 13:02:25 GMT"}], "update_date": "2019-04-04", "authors_parsed": [["Zhu", "Dizhong", ""], ["Smith", "William A. P.", ""]]}, {"id": "1903.12063", "submitter": "Johannes Lotz", "authors": "Johannes Lotz, Nick Weiss and Stefan Heldmann", "title": "Robust, fast and accurate: a 3-step method for automatic histological\n  image registration", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We present a 3-step registration pipeline for differently stained\nhistological serial sections that consists of 1) a robust pre-alignment, 2) a\nparametric registration computed on coarse resolution images, and 3) an\naccurate nonlinear registration. In all three steps the NGF distance measure is\nminimized with respect to an increasingly flexible transformation. We apply the\nmethod in the ANHIR image registration challenge and evaluate its performance\non the training data. The presented method is robust (error reduction in 99.6%\nof the cases), fast (runtime 4 seconds) and accurate (median relative target\nregistration error 0.19%).\n", "versions": [{"version": "v1", "created": "Thu, 28 Mar 2019 15:48:02 GMT"}, {"version": "v2", "created": "Fri, 29 Mar 2019 09:32:55 GMT"}], "update_date": "2019-04-01", "authors_parsed": [["Lotz", "Johannes", ""], ["Weiss", "Nick", ""], ["Heldmann", "Stefan", ""]]}, {"id": "1903.12088", "submitter": "Suiyi Ling", "authors": "Suiyi Ling, Jing Li, Junle Wang, Patrick Le Callet", "title": "GANs-NQM: A Generative Adversarial Networks based No Reference Quality\n  Assessment Metric for RGB-D Synthesized Views", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we proposed a no-reference (NR) quality metric for RGB plus\nimage-depth (RGB-D) synthesis images based on Generative Adversarial Networks\n(GANs), namely GANs-NQM. Due to the failure of the inpainting on dis-occluded\nregions in RGB-D synthesis process, to capture the non-uniformly distributed\nlocal distortions and to learn their impact on perceptual quality are\nchallenging tasks for objective quality metrics. In our study, based on the\ncharacteristics of GANs, we proposed i) a novel training strategy of GANs for\nRGB-D synthesis images using existing large-scale computer vision datasets\nrather than RGB-D dataset; ii) a referenceless quality metric based on the\ntrained discriminator by learning a `Bag of Distortion Word' (BDW) codebook and\na local distortion regions selector; iii) a hole filling inpainter, i.e., the\ngenerator of the trained GANs, for RGB-D dis-occluded regions as a side\noutcome. According to the experimental results on IRCCyN/IVC DIBR database, the\nproposed model outperforms the state-of-the-art quality metrics, in addition,\nis more applicable in real scenarios. The corresponding context inpainter also\nshows appealing results over other inpainting algorithms.\n", "versions": [{"version": "v1", "created": "Thu, 28 Mar 2019 16:11:42 GMT"}], "update_date": "2019-03-29", "authors_parsed": [["Ling", "Suiyi", ""], ["Li", "Jing", ""], ["Wang", "Junle", ""], ["Callet", "Patrick Le", ""]]}, {"id": "1903.12117", "submitter": "Gjorgji Strezoski", "authors": "Gjorgji Strezoski, Nanne van Noord, Marcel Worring", "title": "Many Task Learning with Task Routing", "comments": "8 Pages, 5 Figures, 2 Tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Typical multi-task learning (MTL) methods rely on architectural adjustments\nand a large trainable parameter set to jointly optimize over several tasks.\nHowever, when the number of tasks increases so do the complexity of the\narchitectural adjustments and resource requirements. In this paper, we\nintroduce a method which applies a conditional feature-wise transformation over\nthe convolutional activations that enables a model to successfully perform a\nlarge number of tasks. To distinguish from regular MTL, we introduce Many Task\nLearning (MaTL) as a special case of MTL where more than 20 tasks are performed\nby a single model. Our method dubbed Task Routing (TR) is encapsulated in a\nlayer we call the Task Routing Layer (TRL), which applied in an MaTL scenario\nsuccessfully fits hundreds of classification tasks in one model. We evaluate\nour method on 5 datasets against strong baselines and state-of-the-art\napproaches.\n", "versions": [{"version": "v1", "created": "Thu, 28 Mar 2019 17:03:04 GMT"}], "update_date": "2019-03-29", "authors_parsed": [["Strezoski", "Gjorgji", ""], ["van Noord", "Nanne", ""], ["Worring", "Marcel", ""]]}, {"id": "1903.12139", "submitter": "Sze Teng Liong", "authors": "Sze-Teng Liong, Y.S. Gan, Yen-Chang Huang, Chang-Ann Yuan, Hsiu-Chi\n  Chang", "title": "Automatic Defect Segmentation on Leather with Deep Learning", "comments": "13 pages, 14 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Leather is a natural and durable material created through a process of\ntanning of hides and skins of animals. The price of the leather is subjective\nas it is highly sensitive to its quality and surface defects condition. In the\nliterature, there are very few works investigating on the defects detection for\nleather using automatic image processing techniques. The manual defect\ninspection process is essential in an leather production industry to control\nthe quality of the finished products. However, it is tedious, as it is labour\nintensive, time consuming, causes eye fatigue and often prone to human error.\nIn this paper, a fully automatic defect detection and marking system on a calf\nleather is proposed. The proposed system consists of a piece of leather, LED\nlight, high resolution camera and a robot arm. Succinctly, a machine vision\nmethod is presented to identify the position of the defects on the leather\nusing a deep learning architecture. Then, a series of processes are conducted\nto predict the defect instances, including elicitation of the leather images\nwith a robot arm, train and test the images using a deep learning architecture\nand determination of the boundary of the defects using mathematical derivation\nof the geometry. Note that, all the processes do not involve human\nintervention, except for the defect ground truths construction stage. The\nproposed algorithm is capable to exhibit 91.5% segmentation accuracy on the\ntrain data and 70.35% on the test data. We also report confusion matrix,\nF1-score, precision and specificity, sensitivity performance metrics to further\nverify the effectiveness of the proposed approach.\n", "versions": [{"version": "v1", "created": "Thu, 28 Mar 2019 17:24:30 GMT"}], "update_date": "2019-03-29", "authors_parsed": [["Liong", "Sze-Teng", ""], ["Gan", "Y. S.", ""], ["Huang", "Yen-Chang", ""], ["Yuan", "Chang-Ann", ""], ["Chang", "Hsiu-Chi", ""]]}, {"id": "1903.12141", "submitter": "Xinshao Wang Dr", "authors": "Xinshao Wang, Yang Hua, Elyor Kodirov, Neil M. Robertson", "title": "IMAE for Noise-Robust Learning: Mean Absolute Error Does Not Treat\n  Examples Equally and Gradient Magnitude's Variance Matters", "comments": "Updated Version. IMAE for Noise-Robust Learning: Mean Absolute Error\n  Does Not Treat Examples Equally and Gradient Magnitude's Variance Matters\n  Code:\n  \\url{https://github.com/XinshaoAmosWang/Improving-Mean-Absolute-Error-against-CCE}.\n  Please feel free to contact for discussions or implementation problems", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we study robust deep learning against abnormal training data\nfrom the perspective of example weighting built in empirical loss functions,\ni.e., gradient magnitude with respect to logits, an angle that is not\nthoroughly studied so far. Consequently, we have two key findings: (1) Mean\nAbsolute Error (MAE) Does Not Treat Examples Equally. We present new\nobservations and insightful analysis about MAE, which is theoretically proved\nto be noise-robust. First, we reveal its underfitting problem in practice.\nSecond, we analyse that MAE's noise-robustness is from emphasising on uncertain\nexamples instead of treating training samples equally, as claimed in prior\nwork. (2) The Variance of Gradient Magnitude Matters. We propose an effective\nand simple solution to enhance MAE's fitting ability while preserving its\nnoise-robustness. Without changing MAE's overall weighting scheme, i.e., what\nexamples get higher weights, we simply change its weighting variance\nnon-linearly so that the impact ratio between two examples are adjusted. Our\nsolution is termed Improved MAE (IMAE). We prove IMAE's effectiveness using\nextensive experiments: image classification under clean labels, synthetic label\nnoise, and real-world unknown noise. We conclude IMAE is superior to CCE, the\nmost popular loss for training DNNs.\n", "versions": [{"version": "v1", "created": "Thu, 28 Mar 2019 17:27:05 GMT"}, {"version": "v2", "created": "Sun, 31 Mar 2019 12:23:00 GMT"}, {"version": "v3", "created": "Fri, 19 Apr 2019 10:30:04 GMT"}, {"version": "v4", "created": "Tue, 13 Aug 2019 21:51:20 GMT"}, {"version": "v5", "created": "Fri, 18 Oct 2019 15:44:53 GMT"}, {"version": "v6", "created": "Tue, 17 Dec 2019 13:02:56 GMT"}, {"version": "v7", "created": "Sat, 11 Jan 2020 23:44:10 GMT"}, {"version": "v8", "created": "Mon, 27 Jan 2020 11:59:02 GMT"}, {"version": "v9", "created": "Sun, 15 Nov 2020 09:38:15 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["Wang", "Xinshao", ""], ["Hua", "Yang", ""], ["Kodirov", "Elyor", ""], ["Robertson", "Neil M.", ""]]}, {"id": "1903.12152", "submitter": "Yuankai Huo", "authors": "Yuankai Huo, Zhoubing Xu, Yunxi Xiong, Katherine Aboud, Prasanna\n  Parvathaneni, Shunxing Bao, Camilo Bermudez, Susan M. Resnick, Laurie E.\n  Cutting, Bennett A. Landman", "title": "3D Whole Brain Segmentation using Spatially Localized Atlas Network\n  Tiles", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Detailed whole brain segmentation is an essential quantitative technique,\nwhich provides a non-invasive way of measuring brain regions from a structural\nmagnetic resonance imaging (MRI). Recently, deep convolution neural network\n(CNN) has been applied to whole brain segmentation. However, restricted by\ncurrent GPU memory, 2D based methods, downsampling based 3D CNN methods, and\npatch-based high-resolution 3D CNN methods have been the de facto standard\nsolutions. 3D patch-based high resolution methods typically yield superior\nperformance among CNN approaches on detailed whole brain segmentation (>100\nlabels), however, whose performance are still commonly inferior compared with\nmulti-atlas segmentation methods (MAS) due to the following challenges: (1) a\nsingle network is typically used to learn both spatial and contextual\ninformation for the patches, (2) limited manually traced whole brain volumes\nare available (typically less than 50) for training a network. In this work, we\npropose the spatially localized atlas network tiles (SLANT) method to\ndistribute multiple independent 3D fully convolutional networks (FCN) for\nhigh-resolution whole brain segmentation. To address the first challenge,\nmultiple spatially distributed networks were used in the SLANT method, in which\neach network learned contextual information for a fixed spatial location. To\naddress the second challenge, auxiliary labels on 5111 initially unlabeled\nscans were created by multi-atlas segmentation for training. Since the method\nintegrated multiple traditional medical image processing methods with deep\nlearning, we developed a containerized pipeline to deploy the end-to-end\nsolution. From the results, the proposed method achieved superior performance\ncompared with multi-atlas segmentation methods, while reducing the\ncomputational time from >30 hours to 15 minutes\n(https://github.com/MASILab/SLANTbrainSeg).\n", "versions": [{"version": "v1", "created": "Thu, 28 Mar 2019 17:40:32 GMT"}], "update_date": "2019-03-29", "authors_parsed": [["Huo", "Yuankai", ""], ["Xu", "Zhoubing", ""], ["Xiong", "Yunxi", ""], ["Aboud", "Katherine", ""], ["Parvathaneni", "Prasanna", ""], ["Bao", "Shunxing", ""], ["Bermudez", "Camilo", ""], ["Resnick", "Susan M.", ""], ["Cutting", "Laurie E.", ""], ["Landman", "Bennett A.", ""]]}, {"id": "1903.12161", "submitter": "Sergi Caelles", "authors": "Sergi Caelles, Albert Pumarola, Francesc Moreno-Noguer, Alberto\n  Sanfeliu, Luc Van Gool", "title": "Fast video object segmentation with Spatio-Temporal GANs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning descriptive spatio-temporal object models from data is paramount for\nthe task of semi-supervised video object segmentation. Most existing approaches\nmainly rely on models that estimate the segmentation mask based on a reference\nmask at the first frame (aided sometimes by optical flow or the previous mask).\nThese models, however, are prone to fail under rapid appearance changes or\nocclusions due to their limitations in modelling the temporal component. On the\nother hand, very recently, other approaches learned long-term features using a\nconvolutional LSTM to leverage the information from all previous video frames.\nEven though these models achieve better temporal representations, they still\nhave to be fine-tuned for every new video sequence. In this paper, we present\nan intermediate solution and devise a novel GAN architecture, FaSTGAN, to learn\nspatio-temporal object models over finite temporal windows. To achieve this, we\nconcentrate all the heavy computational load to the training phase with two\ncritics that enforce spatial and temporal mask consistency over the last K\nframes. Then at test time, we only use a relatively light regressor, which\nreduces the inference time considerably. As a result, our approach combines a\nhigh resiliency to sudden geometric and photometric object changes with\nefficiency at test time (no need for fine-tuning nor post-processing). We\ndemonstrate that the accuracy of our method is on par with state-of-the-art\ntechniques on the challenging YouTube-VOS and DAVIS datasets, while running at\n32 fps, about 4x faster than the closest competitor.\n", "versions": [{"version": "v1", "created": "Thu, 28 Mar 2019 17:45:09 GMT"}], "update_date": "2019-03-29", "authors_parsed": [["Caelles", "Sergi", ""], ["Pumarola", "Albert", ""], ["Moreno-Noguer", "Francesc", ""], ["Sanfeliu", "Alberto", ""], ["Van Gool", "Luc", ""]]}, {"id": "1903.12174", "submitter": "Xinlei Chen", "authors": "Xinlei Chen, Ross Girshick, Kaiming He, Piotr Doll\\'ar", "title": "TensorMask: A Foundation for Dense Object Segmentation", "comments": "accepted to ICCV", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sliding-window object detectors that generate bounding-box object predictions\nover a dense, regular grid have advanced rapidly and proven popular. In\ncontrast, modern instance segmentation approaches are dominated by methods that\nfirst detect object bounding boxes, and then crop and segment these regions, as\npopularized by Mask R-CNN. In this work, we investigate the paradigm of dense\nsliding-window instance segmentation, which is surprisingly under-explored. Our\ncore observation is that this task is fundamentally different than other dense\nprediction tasks such as semantic segmentation or bounding-box object\ndetection, as the output at every spatial location is itself a geometric\nstructure with its own spatial dimensions. To formalize this, we treat dense\ninstance segmentation as a prediction task over 4D tensors and present a\ngeneral framework called TensorMask that explicitly captures this geometry and\nenables novel operators on 4D tensors. We demonstrate that the tensor view\nleads to large gains over baselines that ignore this structure, and leads to\nresults comparable to Mask R-CNN. These promising results suggest that\nTensorMask can serve as a foundation for novel advances in dense mask\nprediction and a more complete understanding of the task. Code will be made\navailable.\n", "versions": [{"version": "v1", "created": "Thu, 28 Mar 2019 17:59:33 GMT"}, {"version": "v2", "created": "Tue, 27 Aug 2019 22:59:25 GMT"}], "update_date": "2019-08-29", "authors_parsed": [["Chen", "Xinlei", ""], ["Girshick", "Ross", ""], ["He", "Kaiming", ""], ["Doll\u00e1r", "Piotr", ""]]}, {"id": "1903.12206", "submitter": "Zenglin Shi", "authors": "Zenglin Shi, Pascal Mettes, and Cees G. M. Snoek", "title": "Counting with Focus for Free", "comments": "ICCV, 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper aims to count arbitrary objects in images. The leading counting\napproaches start from point annotations per object from which they construct\ndensity maps. Then, their training objective transforms input images to density\nmaps through deep convolutional networks. We posit that the point annotations\nserve more supervision purposes than just constructing density maps. We\nintroduce ways to repurpose the points for free. First, we propose supervised\nfocus from segmentation, where points are converted into binary maps. The\nbinary maps are combined with a network branch and accompanying loss function\nto focus on areas of interest. Second, we propose supervised focus from global\ndensity, where the ratio of point annotations to image pixels is used in\nanother branch to regularize the overall density estimation. To assist both the\ndensity estimation and the focus from segmentation, we also introduce an\nimproved kernel size estimator for the point annotations. Experiments on six\ndatasets show that all our contributions reduce the counting error, regardless\nof the base network, resulting in state-of-the-art accuracy using only a single\nnetwork. Finally, we are the first to count on WIDER FACE, allowing us to show\nthe benefits of our approach in handling varying object scales and crowding\nlevels. Code is available at\nhttps://github.com/shizenglin/Counting-with-Focus-for-Free\n", "versions": [{"version": "v1", "created": "Thu, 28 Mar 2019 18:16:35 GMT"}, {"version": "v2", "created": "Tue, 6 Aug 2019 14:30:58 GMT"}], "update_date": "2019-08-07", "authors_parsed": [["Shi", "Zenglin", ""], ["Mettes", "Pascal", ""], ["Snoek", "Cees G. M.", ""]]}, {"id": "1903.12212", "submitter": "Wilson Chang", "authors": "Wei-Lun Chang, Hui-Po Wang, Wen-Hsiao Peng, Wei-Chen Chiu", "title": "All about Structure: Adapting Structural Information across Domains for\n  Boosting Semantic Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper we tackle the problem of unsupervised domain adaptation for the\ntask of semantic segmentation, where we attempt to transfer the knowledge\nlearned upon synthetic datasets with ground-truth labels to real-world images\nwithout any annotation. With the hypothesis that the structural content of\nimages is the most informative and decisive factor to semantic segmentation and\ncan be readily shared across domains, we propose a Domain Invariant Structure\nExtraction (DISE) framework to disentangle images into domain-invariant\nstructure and domain-specific texture representations, which can further\nrealize image-translation across domains and enable label transfer to improve\nsegmentation performance. Extensive experiments verify the effectiveness of our\nproposed DISE model and demonstrate its superiority over several\nstate-of-the-art approaches.\n", "versions": [{"version": "v1", "created": "Tue, 26 Mar 2019 03:23:30 GMT"}], "update_date": "2019-04-01", "authors_parsed": [["Chang", "Wei-Lun", ""], ["Wang", "Hui-Po", ""], ["Peng", "Wen-Hsiao", ""], ["Chiu", "Wei-Chen", ""]]}, {"id": "1903.12220", "submitter": "Maithra Raghu", "authors": "Maithra Raghu, Katy Blumer, Greg Corrado, Jon Kleinberg, Ziad\n  Obermeyer, Sendhil Mullainathan", "title": "The Algorithmic Automation Problem: Prediction, Triage, and Human Effort", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a wide array of areas, algorithms are matching and surpassing the\nperformance of human experts, leading to consideration of the roles of human\njudgment and algorithmic prediction in these domains. The discussion around\nthese developments, however, has implicitly equated the specific task of\nprediction with the general task of automation. We argue here that automation\nis broader than just a comparison of human versus algorithmic performance on a\ntask; it also involves the decision of which instances of the task to give to\nthe algorithm in the first place. We develop a general framework that poses\nthis latter decision as an optimization problem, and we show how basic\nheuristics for this optimization problem can lead to performance gains even on\nheavily-studied applications of AI in medicine. Our framework also serves to\nhighlight how effective automation depends crucially on estimating both\nalgorithmic and human error on an instance-by-instance basis, and our results\nshow how improvements in these error estimation problems can yield significant\ngains for automation as well.\n", "versions": [{"version": "v1", "created": "Thu, 28 Mar 2019 18:53:58 GMT"}], "update_date": "2019-04-01", "authors_parsed": [["Raghu", "Maithra", ""], ["Blumer", "Katy", ""], ["Corrado", "Greg", ""], ["Kleinberg", "Jon", ""], ["Obermeyer", "Ziad", ""], ["Mullainathan", "Sendhil", ""]]}, {"id": "1903.12230", "submitter": "Zhangjie Cao", "authors": "Zhangjie Cao, Kaichao You, Mingsheng Long, Jianmin Wang, and Qiang\n  Yang", "title": "Learning to Transfer Examples for Partial Domain Adaptation", "comments": "CVPR 2019 accepted", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Domain adaptation is critical for learning in new and unseen environments.\nWith domain adversarial training, deep networks can learn disentangled and\ntransferable features that effectively diminish the dataset shift between the\nsource and target domains for knowledge transfer. In the era of Big Data, the\nready availability of large-scale labeled datasets has stimulated wide interest\nin partial domain adaptation (PDA), which transfers a recognizer from a labeled\nlarge domain to an unlabeled small domain. It extends standard domain\nadaptation to the scenario where target labels are only a subset of source\nlabels. Under the condition that target labels are unknown, the key challenge\nof PDA is how to transfer relevant examples in the shared classes to promote\npositive transfer, and ignore irrelevant ones in the specific classes to\nmitigate negative transfer. In this work, we propose a unified approach to PDA,\nExample Transfer Network (ETN), which jointly learns domain-invariant\nrepresentations across the source and target domains, and a progressive\nweighting scheme that quantifies the transferability of source examples while\ncontrolling their importance to the learning task in the target domain. A\nthorough evaluation on several benchmark datasets shows that our approach\nachieves state-of-the-art results for partial domain adaptation tasks.\n", "versions": [{"version": "v1", "created": "Thu, 28 Mar 2019 19:29:29 GMT"}, {"version": "v2", "created": "Sun, 7 Apr 2019 17:43:37 GMT"}], "update_date": "2019-04-09", "authors_parsed": [["Cao", "Zhangjie", ""], ["You", "Kaichao", ""], ["Long", "Mingsheng", ""], ["Wang", "Jianmin", ""], ["Yang", "Qiang", ""]]}, {"id": "1903.12239", "submitter": "Nicholas Weir", "authors": "Nicholas Weir, David Lindenbaum, Alexei Bastidas, Adam Van Etten, Sean\n  McPherson, Jacob Shermeyer, Varun Kumar, Hanlin Tang", "title": "SpaceNet MVOI: a Multi-View Overhead Imagery Dataset", "comments": "Accepted into IEEE International Conference on Computer Vision (ICCV)\n  2019", "journal-ref": null, "doi": "10.1109/ICCV.2019.00108", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Detection and segmentation of objects in overheard imagery is a challenging\ntask. The variable density, random orientation, small size, and\ninstance-to-instance heterogeneity of objects in overhead imagery calls for\napproaches distinct from existing models designed for natural scene datasets.\nThough new overhead imagery datasets are being developed, they almost\nuniversally comprise a single view taken from directly overhead (\"at nadir\"),\nfailing to address a critical variable: look angle. By contrast, views vary in\nreal-world overhead imagery, particularly in dynamic scenarios such as natural\ndisasters where first looks are often over 40 degrees off-nadir. This\nrepresents an important challenge to computer vision methods, as changing view\nangle adds distortions, alters resolution, and changes lighting. At present,\nthe impact of these perturbations for algorithmic detection and segmentation of\nobjects is untested. To address this problem, we present an open source\nMulti-View Overhead Imagery dataset, termed SpaceNet MVOI, with 27 unique looks\nfrom a broad range of viewing angles (-32.5 degrees to 54.0 degrees). Each of\nthese images cover the same 665 square km geographic extent and are annotated\nwith 126,747 building footprint labels, enabling direct assessment of the\nimpact of viewpoint perturbation on model performance. We benchmark multiple\nleading segmentation and object detection models on: (1) building detection,\n(2) generalization to unseen viewing angles and resolutions, and (3)\nsensitivity of building footprint extraction to changes in resolution. We find\nthat state of the art segmentation and object detection models struggle to\nidentify buildings in off-nadir imagery and generalize poorly to unseen views,\npresenting an important benchmark to explore the broadly relevant challenge of\ndetecting small, heterogeneous target objects in visually dynamic contexts.\n", "versions": [{"version": "v1", "created": "Thu, 28 Mar 2019 19:51:02 GMT"}, {"version": "v2", "created": "Thu, 15 Aug 2019 18:43:42 GMT"}], "update_date": "2021-02-25", "authors_parsed": [["Weir", "Nicholas", ""], ["Lindenbaum", "David", ""], ["Bastidas", "Alexei", ""], ["Van Etten", "Adam", ""], ["McPherson", "Sean", ""], ["Shermeyer", "Jacob", ""], ["Kumar", "Varun", ""], ["Tang", "Hanlin", ""]]}, {"id": "1903.12255", "submitter": "Zeyi Huang", "authors": "Zeyi Huang, Wei Ke and Dong Huang", "title": "Improving Object Detection with Inverted Attention", "comments": "9 pages, 7 figures, 6 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Improving object detectors against occlusion, blur and noise is a critical\nstep to deploy detectors in real applications. Since it is not possible to\nexhaust all image defects through data collection, many researchers seek to\ngenerate hard samples in training. The generated hard samples are either images\nor feature maps with coarse patches dropped out in the spatial dimensions.\nSignificant overheads are required in training the extra hard samples and/or\nestimating drop-out patches using extra network branches. In this paper, we\nimprove object detectors using a highly efficient and fine-grain mechanism\ncalled Inverted Attention (IA). Different from the original detector network\nthat only focuses on the dominant part of objects, the detector network with IA\niteratively inverts attention on feature maps and puts more attention on\ncomplementary object parts, feature channels and even context. Our approach (1)\noperates along both the spatial and channels dimensions of the feature maps;\n(2) requires no extra training on hard samples, no extra network parameters for\nattention estimation, and no testing overheads. Experiments show that our\napproach consistently improved both two-stage and single-stage detectors on\nbenchmark databases.\n", "versions": [{"version": "v1", "created": "Thu, 28 Mar 2019 20:41:11 GMT"}], "update_date": "2019-04-01", "authors_parsed": [["Huang", "Zeyi", ""], ["Ke", "Wei", ""], ["Huang", "Dong", ""]]}, {"id": "1903.12261", "submitter": "Dan Hendrycks", "authors": "Dan Hendrycks, Thomas Dietterich", "title": "Benchmarking Neural Network Robustness to Common Corruptions and\n  Perturbations", "comments": "ICLR 2019 camera-ready; datasets available at\n  https://github.com/hendrycks/robustness ; this article supersedes\n  arXiv:1807.01697", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we establish rigorous benchmarks for image classifier\nrobustness. Our first benchmark, ImageNet-C, standardizes and expands the\ncorruption robustness topic, while showing which classifiers are preferable in\nsafety-critical applications. Then we propose a new dataset called ImageNet-P\nwhich enables researchers to benchmark a classifier's robustness to common\nperturbations. Unlike recent robustness research, this benchmark evaluates\nperformance on common corruptions and perturbations not worst-case adversarial\nperturbations. We find that there are negligible changes in relative corruption\nrobustness from AlexNet classifiers to ResNet classifiers. Afterward we\ndiscover ways to enhance corruption and perturbation robustness. We even find\nthat a bypassed adversarial defense provides substantial common perturbation\nrobustness. Together our benchmarks may aid future work toward networks that\nrobustly generalize.\n", "versions": [{"version": "v1", "created": "Thu, 28 Mar 2019 20:56:37 GMT"}], "update_date": "2019-04-01", "authors_parsed": [["Hendrycks", "Dan", ""], ["Dietterich", "Thomas", ""]]}, {"id": "1903.12266", "submitter": "Maciej Zamorski", "authors": "Maciej Zamorski, Adrian Zdobylak, Maciej Zi\\k{e}ba, Jerzy\n  \\'Swi\\k{a}tek", "title": "Generative Adversarial Networks: recent developments", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In traditional generative modeling, good data representation is very often a\nbase for a good machine learning model. It can be linked to good\nrepresentations encoding more explanatory factors that are hidden in the\noriginal data. With the invention of Generative Adversarial Networks (GANs), a\nsubclass of generative models that are able to learn representations in an\nunsupervised and semi-supervised fashion, we are now able to adversarially\nlearn good mappings from a simple prior distribution to a target data\ndistribution. This paper presents an overview of recent developments in GANs\nwith a focus on learning latent space representations.\n", "versions": [{"version": "v1", "created": "Sat, 16 Mar 2019 18:10:35 GMT"}], "update_date": "2019-04-01", "authors_parsed": [["Zamorski", "Maciej", ""], ["Zdobylak", "Adrian", ""], ["Zi\u0119ba", "Maciej", ""], ["\u015awi\u0105tek", "Jerzy", ""]]}, {"id": "1903.12269", "submitter": "Adnan Siraj Rakin", "authors": "Adnan Siraj Rakin and Zhezhi He and Deliang Fan", "title": "Bit-Flip Attack: Crushing Neural Network with Progressive Bit Search", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Several important security issues of Deep Neural Network (DNN) have been\nraised recently associated with different applications and components. The most\nwidely investigated security concern of DNN is from its malicious input, a.k.a\nadversarial example. Nevertheless, the security challenge of DNN's parameters\nis not well explored yet. In this work, we are the first to propose a novel DNN\nweight attack methodology called Bit-Flip Attack (BFA) which can crush a neural\nnetwork through maliciously flipping extremely small amount of bits within its\nweight storage memory system (i.e., DRAM). The bit-flip operations could be\nconducted through well-known Row-Hammer attack, while our main contribution is\nto develop an algorithm to identify the most vulnerable bits of DNN weight\nparameters (stored in memory as binary bits), that could maximize the accuracy\ndegradation with a minimum number of bit-flips. Our proposed BFA utilizes a\nProgressive Bit Search (PBS) method which combines gradient ranking and\nprogressive search to identify the most vulnerable bit to be flipped. With the\naid of PBS, we can successfully attack a ResNet-18 fully malfunction (i.e.,\ntop-1 accuracy degrade from 69.8% to 0.1%) only through 13 bit-flips out of 93\nmillion bits, while randomly flipping 100 bits merely degrades the accuracy by\nless than 1%.\n", "versions": [{"version": "v1", "created": "Thu, 28 Mar 2019 21:07:48 GMT"}, {"version": "v2", "created": "Sun, 7 Apr 2019 21:20:53 GMT"}], "update_date": "2019-04-09", "authors_parsed": [["Rakin", "Adnan Siraj", ""], ["He", "Zhezhi", ""], ["Fan", "Deliang", ""]]}, {"id": "1903.12290", "submitter": "Wenbin Li", "authors": "Wenbin Li, Lei Wang, Jinglin Xu, Jing Huo, Yang Gao and Jiebo Luo", "title": "Revisiting Local Descriptor based Image-to-Class Measure for Few-shot\n  Learning", "comments": "accepted by CVPR 2019. The code link:\n  https://github.com/WenbinLee/DN4.git", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Few-shot learning in image classification aims to learn a classifier to\nclassify images when only few training examples are available for each class.\nRecent work has achieved promising classification performance, where an\nimage-level feature based measure is usually used. In this paper, we argue that\na measure at such a level may not be effective enough in light of the scarcity\nof examples in few-shot learning. Instead, we think a local descriptor based\nimage-to-class measure should be taken, inspired by its surprising success in\nthe heydays of local invariant features. Specifically, building upon the recent\nepisodic training mechanism, we propose a Deep Nearest Neighbor Neural Network\n(DN4 in short) and train it in an end-to-end manner. Its key difference from\nthe literature is the replacement of the image-level feature based measure in\nthe final layer by a local descriptor based image-to-class measure. This\nmeasure is conducted online via a $k$-nearest neighbor search over the deep\nlocal descriptors of convolutional feature maps. The proposed DN4 not only\nlearns the optimal deep local descriptors for the image-to-class measure, but\nalso utilizes the higher efficiency of such a measure in the case of example\nscarcity, thanks to the exchangeability of visual patterns across the images in\nthe same class. Our work leads to a simple, effective, and computationally\nefficient framework for few-shot learning. Experimental study on benchmark\ndatasets consistently shows its superiority over the related state-of-the-art,\nwith the largest absolute improvement of $17\\%$ over the next best. The source\ncode can be available from \\UrlFont{https://github.com/WenbinLee/DN4.git}.\n", "versions": [{"version": "v1", "created": "Thu, 28 Mar 2019 22:02:24 GMT"}, {"version": "v2", "created": "Wed, 10 Apr 2019 02:14:33 GMT"}], "update_date": "2019-04-11", "authors_parsed": [["Li", "Wenbin", ""], ["Wang", "Lei", ""], ["Xu", "Jinglin", ""], ["Huo", "Jing", ""], ["Gao", "Yang", ""], ["Luo", "Jiebo", ""]]}, {"id": "1903.12294", "submitter": "Franz Sauer", "authors": "Franz Sauer and Kwan-Liu Ma", "title": "Multifaceted 4D Feature Segmentation and Extraction in Point and\n  Field-based Datasets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The use of large-scale multifaceted data is common in a wide variety of\nscientific applications. In many cases, this multifaceted data takes the form\nof a field-based (Eulerian) and point/trajectory-based (Lagrangian)\nrepresentation as each has a unique set of advantages in characterizing a\nsystem of study. Furthermore, studying the increasing scale and complexity of\nthese multifaceted datasets is limited by perceptual ability and available\ncomputational resources, necessitating sophisticated data reduction and feature\nextraction techniques. In this work, we present a new 4D feature\nsegmentation/extraction scheme that can operate on both the field and\npoint/trajectory data types simultaneously. The resulting features are\ntime-varying data subsets that have both a field and point-based component, and\nwere extracted based on underlying patterns from both data types. This enables\nresearchers to better explore both the spatial and temporal interplay between\nthe two data representations and study underlying phenomena from new\nperspectives. We parallelize our approach using GPU acceleration and apply it\nto real world multifaceted datasets to illustrate the types of features that\ncan be extracted and explored.\n", "versions": [{"version": "v1", "created": "Thu, 28 Mar 2019 22:32:46 GMT"}], "update_date": "2019-04-01", "authors_parsed": [["Sauer", "Franz", ""], ["Ma", "Kwan-Liu", ""]]}, {"id": "1903.12296", "submitter": "Hao Tang", "authors": "Hao Tang, Dan Xu, Nicu Sebe, Yan Yan", "title": "Attention-Guided Generative Adversarial Networks for Unsupervised\n  Image-to-Image Translation", "comments": "8 pages, 7 figures, Accepted to IJCNN 2019", "journal-ref": "IJCNN 2019", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The state-of-the-art approaches in Generative Adversarial Networks (GANs) are\nable to learn a mapping function from one image domain to another with unpaired\nimage data. However, these methods often produce artifacts and can only be able\nto convert low-level information, but fail to transfer high-level semantic part\nof images. The reason is mainly that generators do not have the ability to\ndetect the most discriminative semantic part of images, which thus makes the\ngenerated images with low-quality. To handle the limitation, in this paper we\npropose a novel Attention-Guided Generative Adversarial Network (AGGAN), which\ncan detect the most discriminative semantic object and minimize changes of\nunwanted part for semantic manipulation problems without using extra data and\nmodels. The attention-guided generators in AGGAN are able to produce attention\nmasks via a built-in attention mechanism, and then fuse the input image with\nthe attention mask to obtain a target image with high-quality. Moreover, we\npropose a novel attention-guided discriminator which only considers attended\nregions. The proposed AGGAN is trained by an end-to-end fashion with an\nadversarial loss, cycle-consistency loss, pixel loss and attention loss. Both\nqualitative and quantitative results demonstrate that our approach is effective\nto generate sharper and more accurate images than existing models. The code is\navailable at https://github.com/Ha0Tang/AttentionGAN.\n", "versions": [{"version": "v1", "created": "Thu, 28 Mar 2019 22:59:47 GMT"}, {"version": "v2", "created": "Sun, 14 Apr 2019 14:36:26 GMT"}, {"version": "v3", "created": "Tue, 27 Aug 2019 21:35:27 GMT"}], "update_date": "2019-08-29", "authors_parsed": [["Tang", "Hao", ""], ["Xu", "Dan", ""], ["Sebe", "Nicu", ""], ["Yan", "Yan", ""]]}, {"id": "1903.12302", "submitter": "Ferenc Balint-Benczedi", "authors": "Ferenc Balint-Benczedi, Michael Beetz", "title": "Amortized Object and Scene Perception for Long-term Robot Manipulation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mobile robots, performing long-term manipulation activities in human\nenvironments, have to perceive a wide variety of objects possessing very\ndifferent visual characteristics and need to reliably keep track of these\nthroughout the execution of a task. In order to be efficient, robot perception\ncapabilities need to go beyond what is currently perceivable and should be able\nto answer queries about both current and past scenes. In this paper we\ninvestigate a perception system for long-term robot manipulation that keeps\ntrack of the changing environment and builds a representation of the perceived\nworld. Specifically we introduce an amortized component that spreads perception\ntasks throughout the execution cycle. The resulting query driven perception\nsystem asynchronously integrates results from logged images into a symbolic and\nnumeric (what we call sub-symbolic) representation that forms the perceptual\nbelief state of the robot.\n", "versions": [{"version": "v1", "created": "Thu, 28 Mar 2019 23:36:47 GMT"}], "update_date": "2019-04-01", "authors_parsed": [["Balint-Benczedi", "Ferenc", ""], ["Beetz", "Michael", ""]]}, {"id": "1903.12305", "submitter": "Jingwei Huang", "authors": "Jingwei Huang and Yichao Zhou and Thomas Funkhouser and Leonidas\n  Guibas", "title": "FrameNet: Learning Local Canonical Frames of 3D Surfaces from a Single\n  RGB Image", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we introduce the novel problem of identifying dense canonical\n3D coordinate frames from a single RGB image. We observe that each pixel in an\nimage corresponds to a surface in the underlying 3D geometry, where a canonical\nframe can be identified as represented by three orthogonal axes, one along its\nnormal direction and two in its tangent plane. We propose an algorithm to\npredict these axes from RGB. Our first insight is that canonical frames\ncomputed automatically with recently introduced direction field synthesis\nmethods can provide training data for the task. Our second insight is that\nnetworks designed for surface normal prediction provide better results when\ntrained jointly to predict canonical frames, and even better when trained to\nalso predict 2D projections of canonical frames. We conjecture this is because\nprojections of canonical tangent directions often align with local gradients in\nimages, and because those directions are tightly linked to 3D canonical frames\nthrough projective geometry and orthogonality constraints. In our experiments,\nwe find that our method predicts 3D canonical frames that can be used in\napplications ranging from surface normal estimation, feature matching, and\naugmented reality.\n", "versions": [{"version": "v1", "created": "Fri, 29 Mar 2019 00:42:52 GMT"}], "update_date": "2019-04-01", "authors_parsed": [["Huang", "Jingwei", ""], ["Zhou", "Yichao", ""], ["Funkhouser", "Thomas", ""], ["Guibas", "Leonidas", ""]]}, {"id": "1903.12314", "submitter": "Zhe Gan", "authors": "Linjie Li, Zhe Gan, Yu Cheng, Jingjing Liu", "title": "Relation-Aware Graph Attention Network for Visual Question Answering", "comments": "To appear in ICCV 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In order to answer semantically-complicated questions about an image, a\nVisual Question Answering (VQA) model needs to fully understand the visual\nscene in the image, especially the interactive dynamics between different\nobjects. We propose a Relation-aware Graph Attention Network (ReGAT), which\nencodes each image into a graph and models multi-type inter-object relations\nvia a graph attention mechanism, to learn question-adaptive relation\nrepresentations. Two types of visual object relations are explored: (i)\nExplicit Relations that represent geometric positions and semantic interactions\nbetween objects; and (ii) Implicit Relations that capture the hidden dynamics\nbetween image regions. Experiments demonstrate that ReGAT outperforms prior\nstate-of-the-art approaches on both VQA 2.0 and VQA-CP v2 datasets. We further\nshow that ReGAT is compatible to existing VQA architectures, and can be used as\na generic relation encoder to boost the model performance for VQA.\n", "versions": [{"version": "v1", "created": "Fri, 29 Mar 2019 01:24:19 GMT"}, {"version": "v2", "created": "Thu, 15 Aug 2019 03:59:32 GMT"}, {"version": "v3", "created": "Wed, 9 Oct 2019 18:34:49 GMT"}], "update_date": "2019-10-11", "authors_parsed": [["Li", "Linjie", ""], ["Gan", "Zhe", ""], ["Cheng", "Yu", ""], ["Liu", "Jingjing", ""]]}, {"id": "1903.12331", "submitter": "Weiwei Zong", "authors": "Weiwei Zong, Joon Lee, Chang Liu, Eric Carver, Aharon Feldman,\n  Branislava Janic, Mohamed Elshaikh, Milan Pantelic, David Hearshen, Indrin\n  Chetty, Benjamin Movsas, Ning Wen", "title": "A Deep Dive into Understanding Tumor Foci Classification using\n  Multiparametric MRI Based on Convolutional Neural Network", "comments": null, "journal-ref": null, "doi": "10.1002/mp.14255", "report-no": null, "categories": "cs.CV eess.IV q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning models have had a great success in disease classifications\nusing large data pools of skin cancer images or lung X-rays. However, data\nscarcity has been the roadblock of applying deep learning models directly on\nprostate multiparametric MRI (mpMRI). Although model interpretation has been\nheavily studied for natural images for the past few years, there has been a\nlack of interpretation of deep learning models trained on medical images. This\nwork designs a customized workflow for the small and imbalanced data set of\nprostate mpMRI where features were extracted from a deep learning model and\nthen analyzed by a traditional machine learning classifier. In addition, this\nwork contributes to revealing how deep learning models interpret mpMRI for\nprostate cancer patients stratification.\n", "versions": [{"version": "v1", "created": "Fri, 29 Mar 2019 02:38:37 GMT"}, {"version": "v2", "created": "Thu, 4 Apr 2019 19:43:24 GMT"}, {"version": "v3", "created": "Thu, 14 May 2020 15:28:55 GMT"}], "update_date": "2021-01-27", "authors_parsed": [["Zong", "Weiwei", ""], ["Lee", "Joon", ""], ["Liu", "Chang", ""], ["Carver", "Eric", ""], ["Feldman", "Aharon", ""], ["Janic", "Branislava", ""], ["Elshaikh", "Mohamed", ""], ["Pantelic", "Milan", ""], ["Hearshen", "David", ""], ["Chetty", "Indrin", ""], ["Movsas", "Benjamin", ""], ["Wen", "Ning", ""]]}, {"id": "1903.12337", "submitter": "Jingbo Lin", "authors": "Jingbo Lin, Weipeng Jing, Houbing Song, and Guangsheng Chen", "title": "ESFNet: Efficient Network for Building Extraction from High-Resolution\n  Aerial Images", "comments": "10 pages, 3 figures, 4 tables. Accepted for IEEE Access", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Building footprint extraction from high-resolution aerial images is always an\nessential part of urban dynamic monitoring, planning and management. It has\nalso been a challenging task in remote sensing research. In recent years, deep\nneural networks have made great achievement in improving accuracy of building\nextraction from remote sensing imagery. However, most of existing approaches\nusually require large amount of parameters and floating point operations for\nhigh accuracy, it leads to high memory consumption and low inference speed\nwhich are harmful to research. In this paper, we proposed a novel efficient\nnetwork named ESFNet which employs separable factorized residual block and\nutilizes the dilated convolutions, aiming to preserve slight accuracy loss with\nlow computational cost and memory consumption. Our ESFNet obtains a better\ntrade-off between accuracy and efficiency, it can run at over 100 FPS on single\nTesla V100, requires 6x fewer FLOPs and has 18x fewer parameters than\nstate-of-the-art real-time architecture ERFNet while preserving similar\naccuracy without any additional context module, post-processing and pre-trained\nscheme. We evaluated our networks on WHU Building Dataset and compared it with\nother state-of-the-art architectures. The result and comprehensive analysis\nshow that our networks are benefit for efficient remote sensing researches, and\nthe idea can be further extended to other areas. The code is public available\nat: https://github.com/mrluin/ESFNet-Pytorch\n", "versions": [{"version": "v1", "created": "Fri, 29 Mar 2019 03:13:19 GMT"}, {"version": "v2", "created": "Fri, 19 Apr 2019 13:03:33 GMT"}], "update_date": "2019-04-22", "authors_parsed": [["Lin", "Jingbo", ""], ["Jing", "Weipeng", ""], ["Song", "Houbing", ""], ["Chen", "Guangsheng", ""]]}, {"id": "1903.12351", "submitter": "Liu Liu", "authors": "Liu Liu and Hongdong Li", "title": "Lending Orientation to Neural Networks for Cross-view Geo-localization", "comments": "CVPR2019. Codes and datasets are available at\n  https://github.com/Liumouliu/OriCNN", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies image-based geo-localization (IBL) problem using\nground-to-aerial cross-view matching. The goal is to predict the spatial\nlocation of a ground-level query image by matching it to a large geotagged\naerial image database (e.g., satellite imagery). This is a challenging task due\nto the drastic differences in their viewpoints and visual appearances. Existing\ndeep learning methods for this problem have been focused on maximizing feature\nsimilarity between spatially close-by image pairs, while minimizing other\nimages pairs which are far apart. They do so by deep feature embedding based on\nvisual appearance in those ground-and-aerial images. However, in everyday life,\nhumans commonly use {\\em orientation} information as an important cue for the\ntask of spatial localization. Inspired by this insight, this paper proposes a\nnovel method which endows deep neural networks with the `commonsense' of\norientation. Given a ground-level spherical panoramic image as query input (and\na large georeferenced satellite image database), we design a Siamese network\nwhich explicitly encodes the orientation (i.e., spherical directions) of each\npixel of the images. Our method significantly boosts the discriminative power\nof the learned deep features, leading to a much higher recall and precision\noutperforming all previous methods. Our network is also more compact using only\n1/5th number of parameters than a previously best-performing network. To\nevaluate the generalization of our method, we also created a large-scale\ncross-view localization benchmark containing 100K geotagged ground-aerial pairs\ncovering a city. Our codes and datasets are available at\n\\url{https://github.com/Liumouliu/OriCNN}.\n", "versions": [{"version": "v1", "created": "Fri, 29 Mar 2019 04:40:01 GMT"}], "update_date": "2019-04-01", "authors_parsed": [["Liu", "Liu", ""], ["Li", "Hongdong", ""]]}, {"id": "1903.12355", "submitter": "Chengxu Zhuang", "authors": "Chengxu Zhuang, Alex Lin Zhai, Daniel Yamins", "title": "Local Aggregation for Unsupervised Learning of Visual Embeddings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unsupervised approaches to learning in neural networks are of substantial\ninterest for furthering artificial intelligence, both because they would enable\nthe training of networks without the need for large numbers of expensive\nannotations, and because they would be better models of the kind of\ngeneral-purpose learning deployed by humans. However, unsupervised networks\nhave long lagged behind the performance of their supervised counterparts,\nespecially in the domain of large-scale visual recognition. Recent developments\nin training deep convolutional embeddings to maximize non-parametric instance\nseparation and clustering objectives have shown promise in closing this gap.\nHere, we describe a method that trains an embedding function to maximize a\nmetric of local aggregation, causing similar data instances to move together in\nthe embedding space, while allowing dissimilar instances to separate. This\naggregation metric is dynamic, allowing soft clusters of different scales to\nemerge. We evaluate our procedure on several large-scale visual recognition\ndatasets, achieving state-of-the-art unsupervised transfer learning performance\non object recognition in ImageNet, scene recognition in Places 205, and object\ndetection in PASCAL VOC.\n", "versions": [{"version": "v1", "created": "Fri, 29 Mar 2019 05:05:41 GMT"}, {"version": "v2", "created": "Wed, 10 Apr 2019 06:37:42 GMT"}], "update_date": "2019-04-11", "authors_parsed": [["Zhuang", "Chengxu", ""], ["Zhai", "Alex Lin", ""], ["Yamins", "Daniel", ""]]}, {"id": "1903.12363", "submitter": "Xiaohui Zhao Ph.D.", "authors": "Xiaohui Zhao, Endi Niu, Zhuo Wu, and Xiaoguang Wang", "title": "CUTIE: Learning to Understand Documents with Convolutional Universal\n  Text Information Extractor", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Extracting key information from documents, such as receipts or invoices, and\npreserving the interested texts to structured data is crucial in the\ndocument-intensive streamline processes of office automation in areas that\nincludes but not limited to accounting, financial, and taxation areas. To avoid\ndesigning expert rules for each specific type of document, some published works\nattempt to tackle the problem by learning a model to explore the semantic\ncontext in text sequences based on the Named Entity Recognition (NER) method in\nthe NLP field. In this paper, we propose to harness the effective information\nfrom both semantic meaning and spatial distribution of texts in documents.\nSpecifically, our proposed model, Convolutional Universal Text Information\nExtractor (CUTIE), applies convolutional neural networks on gridded texts where\ntexts are embedded as features with semantical connotations. We further explore\nthe effect of employing different structures of convolutional neural network\nand propose a fast and portable structure. We demonstrate the effectiveness of\nthe proposed method on a dataset with up to $4,484$ labelled receipts, without\nany pre-training or post-processing, achieving state of the art performance\nthat is much better than the NER based methods in terms of either speed and\naccuracy. Experimental results also demonstrate that the proposed CUTIE model\nbeing able to achieve good performance with a much smaller amount of training\ndata.\n", "versions": [{"version": "v1", "created": "Fri, 29 Mar 2019 06:23:06 GMT"}, {"version": "v2", "created": "Thu, 4 Apr 2019 05:50:09 GMT"}, {"version": "v3", "created": "Fri, 24 May 2019 03:42:21 GMT"}, {"version": "v4", "created": "Thu, 20 Jun 2019 01:56:57 GMT"}], "update_date": "2019-06-21", "authors_parsed": [["Zhao", "Xiaohui", ""], ["Niu", "Endi", ""], ["Wu", "Zhuo", ""], ["Wang", "Xiaoguang", ""]]}, {"id": "1903.12364", "submitter": "Andre Ivan", "authors": "Andre Ivan, Williem, In Kyu Park", "title": "Synthesizing a 4D Spatio-Angular Consistent Light Field from a Single\n  Image", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Synthesizing a densely sampled light field from a single image is highly\nbeneficial for many applications. The conventional method reconstructs a depth\nmap and relies on physical-based rendering and a secondary network to improve\nthe synthesized novel views. Simple pixel-based loss also limits the network by\nmaking it rely on pixel intensity cue rather than geometric reasoning. In this\nstudy, we show that a different geometric representation, namely, appearance\nflow, can be used to synthesize a light field from a single image robustly and\ndirectly. A single end-to-end deep neural network that does not require a\nphysical-based approach nor a post-processing subnetwork is proposed. Two novel\nloss functions based on known light field domain knowledge are presented to\nenable the network to preserve the spatio-angular consistency between\nsub-aperture images effectively. Experimental results show that the proposed\nmodel successfully synthesizes dense light fields and qualitatively and\nquantitatively outperforms the previous model . The method can be generalized\nto arbitrary scenes, rather than focusing on a particular class of object. The\nsynthesized light field can be used for various applications, such as depth\nestimation and refocusing.\n", "versions": [{"version": "v1", "created": "Fri, 29 Mar 2019 06:24:56 GMT"}], "update_date": "2019-04-01", "authors_parsed": [["Ivan", "Andre", ""], ["Williem", "", ""], ["Park", "In Kyu", ""]]}, {"id": "1903.12368", "submitter": "Zihao Bo", "authors": "Zihao Bo, Hao Zhang, Junhai Yong, Feng Xu", "title": "DenseAttentionSeg: Segment Hands from Interacted Objects Using Depth\n  Input", "comments": null, "journal-ref": null, "doi": "10.1016/j.asoc.2020.106297", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a real-time DNN-based technique to segment hand and object of\ninteracting motions from depth inputs. Our model is called DenseAttentionSeg,\nwhich contains a dense attention mechanism to fuse information in different\nscales and improves the results quality with skip-connections. Besides, we\nintroduce a contour loss in model training, which helps to generate accurate\nhand and object boundaries. Finally, we propose and release our InterSegHands\ndataset, a fine-scale hand segmentation dataset containing about 52k depth maps\nof hand-object interactions. Our experiments evaluate the effectiveness of our\ntechniques and datasets, and indicate that our method outperforms the current\nstate-of-the-art deep segmentation methods on interaction segmentation.\n", "versions": [{"version": "v1", "created": "Fri, 29 Mar 2019 06:38:22 GMT"}, {"version": "v2", "created": "Tue, 23 Apr 2019 10:59:17 GMT"}], "update_date": "2020-12-08", "authors_parsed": [["Bo", "Zihao", ""], ["Zhang", "Hao", ""], ["Yong", "Junhai", ""], ["Xu", "Feng", ""]]}, {"id": "1903.12370", "submitter": "Mitch Hill", "authors": "Erik Nijkamp, Mitch Hill, Tian Han, Song-Chun Zhu, Ying Nian Wu", "title": "On the Anatomy of MCMC-Based Maximum Likelihood Learning of Energy-Based\n  Models", "comments": "Code available at: https://github.com/point0bar1/ebm-anatomy", "journal-ref": "AAAI 2020", "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This study investigates the effects of Markov chain Monte Carlo (MCMC)\nsampling in unsupervised Maximum Likelihood (ML) learning. Our attention is\nrestricted to the family of unnormalized probability densities for which the\nnegative log density (or energy function) is a ConvNet. We find that many of\nthe techniques used to stabilize training in previous studies are not\nnecessary. ML learning with a ConvNet potential requires only a few\nhyper-parameters and no regularization. Using this minimal framework, we\nidentify a variety of ML learning outcomes that depend solely on the\nimplementation of MCMC sampling.\n  On one hand, we show that it is easy to train an energy-based model which can\nsample realistic images with short-run Langevin. ML can be effective and stable\neven when MCMC samples have much higher energy than true steady-state samples\nthroughout training. Based on this insight, we introduce an ML method with\npurely noise-initialized MCMC, high-quality short-run synthesis, and the same\nbudget as ML with informative MCMC initialization such as CD or PCD. Unlike\nprevious models, our energy model can obtain realistic high-diversity samples\nfrom a noise signal after training.\n  On the other hand, ConvNet potentials learned with non-convergent MCMC do not\nhave a valid steady-state and cannot be considered approximate unnormalized\ndensities of the training data because long-run MCMC samples differ greatly\nfrom observed images. We show that it is much harder to train a ConvNet\npotential to learn a steady-state over realistic images. To our knowledge,\nlong-run MCMC samples of all previous models lose the realism of short-run\nsamples. With correct tuning of Langevin noise, we train the first ConvNet\npotentials for which long-run and steady-state MCMC samples are realistic\nimages.\n", "versions": [{"version": "v1", "created": "Fri, 29 Mar 2019 06:45:03 GMT"}, {"version": "v2", "created": "Thu, 11 Apr 2019 00:14:15 GMT"}, {"version": "v3", "created": "Thu, 19 Sep 2019 08:09:22 GMT"}, {"version": "v4", "created": "Wed, 27 Nov 2019 20:16:29 GMT"}], "update_date": "2019-12-02", "authors_parsed": [["Nijkamp", "Erik", ""], ["Hill", "Mitch", ""], ["Han", "Tian", ""], ["Zhu", "Song-Chun", ""], ["Wu", "Ying Nian", ""]]}, {"id": "1903.12395", "submitter": "Yang Wang", "authors": "Lin Wu, Yang Wang, Hongzhi Yin, Meng Wang, Ling Shao", "title": "Few-Shot Deep Adversarial Learning for Video-based Person\n  Re-identification", "comments": "Appearing at IEEE Transactions on Image Processing", "journal-ref": null, "doi": "10.1109/TIP.2019.2940684", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video-based person re-identification (re-ID) refers to matching people across\ncamera views from arbitrary unaligned video footages. Existing methods rely on\nsupervision signals to optimise a projected space under which the distances\nbetween inter/intra-videos are maximised/minimised. However, this demands\nexhaustively labelling people across camera views, rendering them unable to be\nscaled in large networked cameras. Also, it is noticed that learning effective\nvideo representations with view invariance is not explicitly addressed for\nwhich features exhibit different distributions otherwise. Thus, matching videos\nfor person re-ID demands flexible models to capture the dynamics in time-series\nobservations and learn view-invariant representations with access to limited\nlabeled training samples. In this paper, we propose a novel few-shot deep\nlearning approach to video-based person re-ID, to learn comparable\nrepresentations that are discriminative and view-invariant. The proposed method\nis developed on the variational recurrent neural networks (VRNNs) and trained\nadversarially to produce latent variables with temporal dependencies that are\nhighly discriminative yet view-invariant in matching persons. Through extensive\nexperiments conducted on three benchmark datasets, we empirically show the\ncapability of our method in creating view-invariant temporal features and\nstate-of-the-art performance achieved by our method.\n", "versions": [{"version": "v1", "created": "Fri, 29 Mar 2019 08:45:59 GMT"}, {"version": "v2", "created": "Tue, 23 Jul 2019 02:47:47 GMT"}, {"version": "v3", "created": "Thu, 12 Sep 2019 05:23:08 GMT"}], "update_date": "2019-09-13", "authors_parsed": [["Wu", "Lin", ""], ["Wang", "Yang", ""], ["Yin", "Hongzhi", ""], ["Wang", "Meng", ""], ["Shao", "Ling", ""]]}, {"id": "1903.12473", "submitter": "Enze Xie", "authors": "Wenhai Wang, Enze Xie, Xiang Li, Wenbo Hou, Tong Lu, Gang Yu, Shuai\n  Shao", "title": "Shape Robust Text Detection with Progressive Scale Expansion Network", "comments": "Accepted by CVPR 2019. arXiv admin note: substantial text overlap\n  with arXiv:1806.02559", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scene text detection has witnessed rapid progress especially with the recent\ndevelopment of convolutional neural networks. However, there still exists two\nchallenges which prevent the algorithm into industry applications. On the one\nhand, most of the state-of-art algorithms require quadrangle bounding box which\nis in-accurate to locate the texts with arbitrary shape. On the other hand, two\ntext instances which are close to each other may lead to a false detection\nwhich covers both instances. Traditionally, the segmentation-based approach can\nrelieve the first problem but usually fail to solve the second challenge. To\naddress these two challenges, in this paper, we propose a novel Progressive\nScale Expansion Network (PSENet), which can precisely detect text instances\nwith arbitrary shapes. More specifically, PSENet generates the different scale\nof kernels for each text instance, and gradually expands the minimal scale\nkernel to the text instance with the complete shape. Due to the fact that there\nare large geometrical margins among the minimal scale kernels, our method is\neffective to split the close text instances, making it easier to use\nsegmentation-based methods to detect arbitrary-shaped text instances. Extensive\nexperiments on CTW1500, Total-Text, ICDAR 2015 and ICDAR 2017 MLT validate the\neffectiveness of PSENet. Notably, on CTW1500, a dataset full of long curve\ntexts, PSENet achieves a F-measure of 74.3% at 27 FPS, and our best F-measure\n(82.2%) outperforms state-of-art algorithms by 6.6%. The code will be released\nin the future.\n", "versions": [{"version": "v1", "created": "Thu, 28 Mar 2019 06:04:44 GMT"}, {"version": "v2", "created": "Mon, 29 Jul 2019 12:00:59 GMT"}], "update_date": "2019-07-30", "authors_parsed": [["Wang", "Wenhai", ""], ["Xie", "Enze", ""], ["Li", "Xiang", ""], ["Hou", "Wenbo", ""], ["Lu", "Tong", ""], ["Yu", "Gang", ""], ["Shao", "Shuai", ""]]}, {"id": "1903.12476", "submitter": "Yun Liu", "authors": "Yun Liu and Ming-Ming Cheng and Xinyu Zhang and Guang-Yu Nie and Meng\n  Wang", "title": "DNA: Deeply-supervised Nonlinear Aggregation for Salient Object\n  Detection", "comments": "arXiv admin note: text overlap with arXiv:1812.10956", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent progress on salient object detection mainly aims at exploiting how to\neffectively integrate multi-scale convolutional features in convolutional\nneural networks (CNNs). Many popular methods impose deep supervision to perform\nside-output predictions that are linearly aggregated for final saliency\nprediction. In this paper, we theoretically and experimentally demonstrate that\nlinear aggregation of side-output predictions is suboptimal, and it only makes\nlimited use of the side-output information obtained by deep supervision. To\nsolve this problem, we propose Deeply-supervised Nonlinear Aggregation (DNA)\nfor better leveraging the complementary information of various side-outputs.\nCompared with existing methods, it i) aggregates side-output features rather\nthan predictions, and ii) adopts nonlinear instead of linear transformations.\nExperiments demonstrate that DNA can successfully break through the bottleneck\nof current linear approaches. Specifically, the proposed saliency detector, a\nmodified U-Net architecture with DNA, performs favorably against\nstate-of-the-art methods on various datasets and evaluation metrics without\nbells and whistles.\n", "versions": [{"version": "v1", "created": "Thu, 28 Mar 2019 01:41:40 GMT"}, {"version": "v2", "created": "Tue, 3 Mar 2020 08:02:22 GMT"}, {"version": "v3", "created": "Mon, 10 Aug 2020 03:24:08 GMT"}, {"version": "v4", "created": "Wed, 20 Jan 2021 06:22:08 GMT"}], "update_date": "2021-01-21", "authors_parsed": [["Liu", "Yun", ""], ["Cheng", "Ming-Ming", ""], ["Zhang", "Xinyu", ""], ["Nie", "Guang-Yu", ""], ["Wang", "Meng", ""]]}, {"id": "1903.12493", "submitter": "Zhan Yang", "authors": "Zhan Yang, Osolo Ian Raymond, WuQing Sun, Jun Long", "title": "Asymmetric Deep Semantic Quantization for Image Retrieval", "comments": "Accepted to IEEE ACCESS. arXiv admin note: text overlap with\n  arXiv:1812.01404", "journal-ref": null, "doi": "10.1109/ACCESS.2019.2920712", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to its fast retrieval and storage efficiency capabilities, hashing has\nbeen widely used in nearest neighbor retrieval tasks. By using deep learning\nbased techniques, hashing can outperform non-learning based hashing technique\nin many applications. However, we argue that the current deep learning based\nhashing methods ignore some critical problems (e.g., the learned hash codes are\nnot discriminative due to the hashing methods being unable to discover rich\nsemantic information and the training strategy having difficulty optimizing the\ndiscrete binary codes). In this paper, we propose a novel image hashing method,\ntermed as \\textbf{\\underline{A}}symmetric \\textbf{\\underline{D}}eep\n\\textbf{\\underline{S}}emantic \\textbf{\\underline{Q}}uantization\n(\\textbf{ADSQ}). \\textbf{ADSQ} is implemented using three stream frameworks,\nwhich consist of one \\emph{LabelNet} and two \\emph{ImgNets}. The\n\\emph{LabelNet} leverages the power of three fully-connected layers, which are\nused to capture rich semantic information between image pairs. For the two\n\\emph{ImgNets}, they each adopt the same convolutional neural network\nstructure, but with different weights (i.e., asymmetric convolutional neural\nnetworks). The two \\emph{ImgNets} are used to generate discriminative compact\nhash codes. Specifically, the function of the \\emph{LabelNet} is to capture\nrich semantic information that is used to guide the two \\emph{ImgNets} in\nminimizing the gap between the real-continuous features and the discrete binary\ncodes. Furthermore, \\textbf{ADSQ} can utilize the most critical semantic\ninformation to guide the feature learning process and consider the consistency\nof the common semantic space and Hamming space. Experimental results on three\nbenchmarks (i.e., CIFAR-10, NUS-WIDE, and ImageNet) demonstrate that the\nproposed \\textbf{ADSQ} can outperforms current state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Fri, 29 Mar 2019 13:00:03 GMT"}, {"version": "v2", "created": "Sat, 1 Jun 2019 02:05:24 GMT"}], "update_date": "2019-06-04", "authors_parsed": [["Yang", "Zhan", ""], ["Raymond", "Osolo Ian", ""], ["Sun", "WuQing", ""], ["Long", "Jun", ""]]}, {"id": "1903.12520", "submitter": "Anurag Illendula", "authors": "Anurag Illendula, Amit Sheth", "title": "Multimodal Emotion Classification", "comments": "Accepted at the 2nd Emoji Workshop co-located with The Web Conference\n  2019", "journal-ref": "Companion Proceedings of the 2019 World Wide Web Conference", "doi": "10.1145/3308560.3316549", "report-no": null, "categories": "cs.CV cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Most NLP and Computer Vision tasks are limited to scarcity of labelled data.\nIn social media emotion classification and other related tasks, hashtags have\nbeen used as indicators to label data. With the rapid increase in emoji usage\nof social media, emojis are used as an additional feature for major social NLP\ntasks. However, this is less explored in case of multimedia posts on social\nmedia where posts are composed of both image and text. At the same time, w.e\nhave seen a surge in the interest to incorporate domain knowledge to improve\nmachine understanding of text. In this paper, we investigate whether domain\nknowledge for emoji can improve the accuracy of emotion classification task. We\nexploit the importance of different modalities from social media post for\nemotion classification task using state-of-the-art deep learning architectures.\nOur experiments demonstrate that the three modalities (text, emoji and images)\nencode different information to express emotion and therefore can complement\neach other. Our results also demonstrate that emoji sense depends on the\ntextual context, and emoji combined with text encodes better information than\nconsidered separately. The highest accuracy of 71.98\\% is achieved with a\ntraining data of 550k posts.\n", "versions": [{"version": "v1", "created": "Wed, 13 Mar 2019 07:54:29 GMT"}], "update_date": "2019-04-01", "authors_parsed": [["Illendula", "Anurag", ""], ["Sheth", "Amit", ""]]}, {"id": "1903.12529", "submitter": "Kai Zhang", "authors": "Kai Zhang, Wangmeng Zuo, Lei Zhang", "title": "Deep Plug-and-Play Super-Resolution for Arbitrary Blur Kernels", "comments": "Accepted to CVPR2019; code is available at\n  https://github.com/cszn/DPSR", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While deep neural networks (DNN) based single image super-resolution (SISR)\nmethods are rapidly gaining popularity, they are mainly designed for the\nwidely-used bicubic degradation, and there still remains the fundamental\nchallenge for them to super-resolve low-resolution (LR) image with arbitrary\nblur kernels. In the meanwhile, plug-and-play image restoration has been\nrecognized with high flexibility due to its modular structure for easy plug-in\nof denoiser priors. In this paper, we propose a principled formulation and\nframework by extending bicubic degradation based deep SISR with the help of\nplug-and-play framework to handle LR images with arbitrary blur kernels.\nSpecifically, we design a new SISR degradation model so as to take advantage of\nexisting blind deblurring methods for blur kernel estimation. To optimize the\nnew degradation induced energy function, we then derive a plug-and-play\nalgorithm via variable splitting technique, which allows us to plug any\nsuper-resolver prior rather than the denoiser prior as a modular part.\nQuantitative and qualitative evaluations on synthetic and real LR images\ndemonstrate that the proposed deep plug-and-play super-resolution framework is\nflexible and effective to deal with blurry LR images.\n", "versions": [{"version": "v1", "created": "Fri, 29 Mar 2019 14:11:36 GMT"}], "update_date": "2019-04-01", "authors_parsed": [["Zhang", "Kai", ""], ["Zuo", "Wangmeng", ""], ["Zhang", "Lei", ""]]}, {"id": "1903.12530", "submitter": "Zhe He", "authors": "Zhe He, Adrian Spurr, Xucong Zhang, Otmar Hilliges", "title": "Photo-Realistic Monocular Gaze Redirection Using Generative Adversarial\n  Networks", "comments": "Published on ICCV2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gaze redirection is the task of changing the gaze to a desired direction for\na given monocular eye patch image. Many applications such as videoconferencing,\nfilms, games, and generation of training data for gaze estimation require\nredirecting the gaze, without distorting the appearance of the area surrounding\nthe eye and while producing photo-realistic images. Existing methods lack the\nability to generate perceptually plausible images. In this work, we present a\nnovel method to alleviate this problem by leveraging generative adversarial\ntraining to synthesize an eye image conditioned on a target gaze direction. Our\nmethod ensures perceptual similarity and consistency of synthesized images to\nthe real images. Furthermore, a gaze estimation loss is used to control the\ngaze direction accurately. To attain high-quality images, we incorporate\nperceptual and cycle consistency losses into our architecture. In extensive\nevaluations we show that the proposed method outperforms state-of-the-art\napproaches in terms of both image quality and redirection precision. Finally,\nwe show that generated images can bring significant improvement for the gaze\nestimation task if used to augment real training data.\n", "versions": [{"version": "v1", "created": "Fri, 29 Mar 2019 14:12:01 GMT"}, {"version": "v2", "created": "Tue, 15 Oct 2019 07:31:53 GMT"}, {"version": "v3", "created": "Sun, 17 Nov 2019 11:05:26 GMT"}, {"version": "v4", "created": "Wed, 20 Nov 2019 10:23:17 GMT"}], "update_date": "2019-11-21", "authors_parsed": [["He", "Zhe", ""], ["Spurr", "Adrian", ""], ["Zhang", "Xucong", ""], ["Hilliges", "Otmar", ""]]}, {"id": "1903.12561", "submitter": "Kaidi Xu", "authors": "Shaokai Ye, Kaidi Xu, Sijia Liu, Jan-Henrik Lambrechts, Huan Zhang,\n  Aojun Zhou, Kaisheng Ma, Yanzhi Wang, Xue Lin", "title": "Adversarial Robustness vs Model Compression, or Both?", "comments": "Accepted by ICCV 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is well known that deep neural networks (DNNs) are vulnerable to\nadversarial attacks, which are implemented by adding crafted perturbations onto\nbenign examples. Min-max robust optimization based adversarial training can\nprovide a notion of security against adversarial attacks. However, adversarial\nrobustness requires a significantly larger capacity of the network than that\nfor the natural training with only benign examples. This paper proposes a\nframework of concurrent adversarial training and weight pruning that enables\nmodel compression while still preserving the adversarial robustness and\nessentially tackles the dilemma of adversarial training. Furthermore, this work\nstudies two hypotheses about weight pruning in the conventional setting and\nfinds that weight pruning is essential for reducing the network model size in\nthe adversarial setting, training a small model from scratch even with\ninherited initialization from the large model cannot achieve both adversarial\nrobustness and high standard accuracy. Code is available at\nhttps://github.com/yeshaokai/Robustness-Aware-Pruning-ADMM.\n", "versions": [{"version": "v1", "created": "Fri, 29 Mar 2019 15:06:41 GMT"}, {"version": "v2", "created": "Thu, 8 Aug 2019 19:43:19 GMT"}, {"version": "v3", "created": "Fri, 25 Oct 2019 01:20:16 GMT"}, {"version": "v4", "created": "Mon, 28 Oct 2019 17:43:19 GMT"}, {"version": "v5", "created": "Tue, 22 Jun 2021 15:16:04 GMT"}], "update_date": "2021-06-23", "authors_parsed": [["Ye", "Shaokai", ""], ["Xu", "Kaidi", ""], ["Liu", "Sijia", ""], ["Lambrechts", "Jan-Henrik", ""], ["Zhang", "Huan", ""], ["Zhou", "Aojun", ""], ["Ma", "Kaisheng", ""], ["Wang", "Yanzhi", ""], ["Lin", "Xue", ""]]}, {"id": "1903.12564", "submitter": "Changhee Han", "authors": "Changhee Han, Leonardo Rundo, Ryosuke Araki, Yujiro Furukawa,\n  Giancarlo Mauri, Hideki Nakayama, Hideaki Hayashi", "title": "Infinite Brain MR Images: PGGAN-based Data Augmentation for Tumor\n  Detection", "comments": "13 pages, 6 figures, Accepted to Neural Approaches to Dynamics of\n  Signal Exchanges as a Springer book chapter", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to the lack of available annotated medical images, accurate\ncomputer-assisted diagnosis requires intensive Data Augmentation (DA)\ntechniques, such as geometric/intensity transformations of original images;\nhowever, those transformed images intrinsically have a similar distribution to\nthe original ones, leading to limited performance improvement. To fill the data\nlack in the real image distribution, we synthesize brain contrast-enhanced\nMagnetic Resonance (MR) images---realistic but completely different from the\noriginal ones---using Generative Adversarial Networks (GANs). This study\nexploits Progressive Growing of GANs (PGGANs), a multi-stage generative\ntraining method, to generate original-sized 256 X 256 MR images for\nConvolutional Neural Network-based brain tumor detection, which is challenging\nvia conventional GANs; difficulties arise due to unstable GAN training with\nhigh resolution and a variety of tumors in size, location, shape, and contrast.\nOur preliminary results show that this novel PGGAN-based DA method can achieve\npromising performance improvement, when combined with classical DA, in tumor\ndetection and also in other medical imaging tasks.\n", "versions": [{"version": "v1", "created": "Fri, 29 Mar 2019 15:16:15 GMT"}], "update_date": "2019-04-01", "authors_parsed": [["Han", "Changhee", ""], ["Rundo", "Leonardo", ""], ["Araki", "Ryosuke", ""], ["Furukawa", "Yujiro", ""], ["Mauri", "Giancarlo", ""], ["Nakayama", "Hideki", ""], ["Hayashi", "Hideaki", ""]]}, {"id": "1903.12571", "submitter": "Changhee Han", "authors": "Leonardo Rundo, Changhee Han, Jin Zhang, Ryuichiro Hataya, Yudai\n  Nagano, Carmelo Militello, Claudio Ferretti, Marco S. Nobile, Andrea\n  Tangherloni, Maria Carla Gilardi, Salvatore Vitabile, Hideki Nakayama,\n  Giancarlo Mauri", "title": "CNN-based Prostate Zonal Segmentation on T2-weighted MR Images: A\n  Cross-dataset Study", "comments": "12 pages, 3 figures, Accepted to Neural Approaches to Dynamics of\n  Signal Exchanges as a Springer book chapter", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Prostate cancer is the most common cancer among US men. However, prostate\nimaging is still challenging despite the advances in multi-parametric Magnetic\nResonance Imaging (MRI), which provides both morphologic and functional\ninformation pertaining to the pathological regions. Along with whole prostate\ngland segmentation, distinguishing between the Central Gland (CG) and\nPeripheral Zone (PZ) can guide towards differential diagnosis, since the\nfrequency and severity of tumors differ in these regions; however, their\nboundary is often weak and fuzzy. This work presents a preliminary study on\nDeep Learning to automatically delineate the CG and PZ, aiming at evaluating\nthe generalization ability of Convolutional Neural Networks (CNNs) on two\nmulti-centric MRI prostate datasets. Especially, we compared three CNN-based\narchitectures: SegNet, U-Net, and pix2pix. In such a context, the segmentation\nperformances achieved with/without pre-training were compared in 4-fold\ncross-validation. In general, U-Net outperforms the other methods, especially\nwhen training and testing are performed on multiple datasets.\n", "versions": [{"version": "v1", "created": "Fri, 29 Mar 2019 15:30:38 GMT"}], "update_date": "2019-04-01", "authors_parsed": [["Rundo", "Leonardo", ""], ["Han", "Changhee", ""], ["Zhang", "Jin", ""], ["Hataya", "Ryuichiro", ""], ["Nagano", "Yudai", ""], ["Militello", "Carmelo", ""], ["Ferretti", "Claudio", ""], ["Nobile", "Marco S.", ""], ["Tangherloni", "Andrea", ""], ["Gilardi", "Maria Carla", ""], ["Vitabile", "Salvatore", ""], ["Nakayama", "Hideki", ""], ["Mauri", "Giancarlo", ""]]}, {"id": "1903.12581", "submitter": "Nikola Bani\\'c", "authors": "Nikola Bani\\'c, Karlo Ko\\v{s}\\v{c}evi\\'c, Marko Suba\\v{s}i\\'c, Sven\n  Lon\\v{c}ari\\'c", "title": "CroP: Color Constancy Benchmark Dataset Generator", "comments": "10 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Implementing color constancy as a pre-processing step in contemporary digital\ncameras is of significant importance as it removes the influence of scene\nillumination on object colors. Several benchmark color constancy datasets have\nbeen created for the purpose of developing and testing new color constancy\nmethods. However, they all have numerous drawbacks including a small number of\nimages, erroneously extracted ground-truth illuminations, long histories of\nmisuses, violations of their stated assumptions, etc. To overcome such and\nsimilar problems, in this paper a color constancy benchmark dataset generator\nis proposed. For a given camera sensor it enables generation of any number of\nrealistic raw images taken in a subset of the real world, namely images of\nprinted photographs. Datasets with such images share many positive features\nwith other existing real-world datasets, while some of the negative features\nare completely eliminated. The generated images can be successfully used to\ntrain methods that afterward achieve high accuracy on real-world datasets. This\nopens the way for creating large enough datasets for advanced deep learning\ntechniques. Experimental results are presented and discussed. The source code\nis available at http://www.fer.unizg.hr/ipg/resources/color_constancy/.\n", "versions": [{"version": "v1", "created": "Fri, 29 Mar 2019 15:39:44 GMT"}], "update_date": "2019-04-01", "authors_parsed": [["Bani\u0107", "Nikola", ""], ["Ko\u0161\u010devi\u0107", "Karlo", ""], ["Suba\u0161i\u0107", "Marko", ""], ["Lon\u010dari\u0107", "Sven", ""]]}, {"id": "1903.12648", "submitter": "Kibok Lee", "authors": "Kibok Lee, Kimin Lee, Jinwoo Shin, Honglak Lee", "title": "Overcoming Catastrophic Forgetting with Unlabeled Data in the Wild", "comments": "ICCV 2019; v3 updated Figure 1", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Lifelong learning with deep neural networks is well-known to suffer from\ncatastrophic forgetting: the performance on previous tasks drastically degrades\nwhen learning a new task. To alleviate this effect, we propose to leverage a\nlarge stream of unlabeled data easily obtainable in the wild. In particular, we\ndesign a novel class-incremental learning scheme with (a) a new distillation\nloss, termed global distillation, (b) a learning strategy to avoid overfitting\nto the most recent task, and (c) a confidence-based sampling method to\neffectively leverage unlabeled external data. Our experimental results on\nvarious datasets, including CIFAR and ImageNet, demonstrate the superiority of\nthe proposed methods over prior methods, particularly when a stream of\nunlabeled data is accessible: our method shows up to 15.8% higher accuracy and\n46.5% less forgetting compared to the state-of-the-art method. The code is\navailable at https://github.com/kibok90/iccv2019-inc.\n", "versions": [{"version": "v1", "created": "Fri, 29 Mar 2019 17:48:15 GMT"}, {"version": "v2", "created": "Thu, 22 Aug 2019 08:43:25 GMT"}, {"version": "v3", "created": "Sat, 26 Oct 2019 17:17:50 GMT"}], "update_date": "2019-10-29", "authors_parsed": [["Lee", "Kibok", ""], ["Lee", "Kimin", ""], ["Shin", "Jinwoo", ""], ["Lee", "Honglak", ""]]}]