[{"id": "1306.0139", "submitter": "Firas Ajil Jassim", "authors": "Firas A. Jassim", "title": "Image Inpainting by Kriging Interpolation Technique", "comments": "6 pages, 9 figures, 1 table", "journal-ref": "World of Computer Science and Information Technology Journal\n  (WCSIT),Vol. 3, No. 5, pp.91-96, 2013", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  Image inpainting is the art of predicting damaged regions of an image. The\nmanual way of image inpainting is a time consuming. Therefore, there must be an\nautomatic digital method for image inpainting that recovers the image from the\ndamaged regions. In this paper, a novel statistical image inpainting algorithm\nbased on Kriging interpolation technique was proposed. Kriging technique\nautomatically fills the damaged region in an image using the information\navailable from its surrounding regions in such away that it uses the spatial\ncorrelation structure of points inside the k-by-k block. Kriging has the\nability to face the challenge of keeping the structure and texture information\nas the size of damaged region heighten. Experimental results showed that,\nKriging has a high PSNR value when recovering a variety of test images from\nscratches and text as damaged regions.\n", "versions": [{"version": "v1", "created": "Sat, 1 Jun 2013 19:16:43 GMT"}], "update_date": "2013-06-04", "authors_parsed": [["Jassim", "Firas A.", ""]]}, {"id": "1306.0152", "submitter": "Eugenio Culurciello Eugenio Culurciello", "authors": "Eugenio Culurciello, Jonghoon Jin, Aysegul Dundar, Jordan Bates", "title": "An Analysis of the Connections Between Layers of Deep Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an analysis of different techniques for selecting the connection\nbe- tween layers of deep neural networks. Traditional deep neural networks use\nran- dom connection tables between layers to keep the number of connections\nsmall and tune to different image features. This kind of connection performs\nadequately in supervised deep networks because their values are refined during\nthe training. On the other hand, in unsupervised learning, one cannot rely on\nback-propagation techniques to learn the connections between layers. In this\nwork, we tested four different techniques for connecting the first layer of the\nnetwork to the second layer on the CIFAR and SVHN datasets and showed that the\naccuracy can be im- proved up to 3% depending on the technique used. We also\nshowed that learning the connections based on the co-occurrences of the\nfeatures does not confer an advantage over a random connection table in small\nnetworks. This work is helpful to improve the efficiency of connections between\nthe layers of unsupervised deep neural networks.\n", "versions": [{"version": "v1", "created": "Sat, 1 Jun 2013 21:37:25 GMT"}], "update_date": "2013-06-04", "authors_parsed": [["Culurciello", "Eugenio", ""], ["Jin", "Jonghoon", ""], ["Dundar", "Aysegul", ""], ["Bates", "Jordan", ""]]}, {"id": "1306.0178", "submitter": "Riadh Bouslimi", "authors": "Riadh Bouslimi, Abir Messaoudi, Jalel Akaichi", "title": "Using a bag of Words for Automatic Medical Image Annotation with a\n  Latent Semantic", "comments": "10 pages, 6 figures", "journal-ref": "International Journal of Artificial Intelligence &\n  Applications(IJAIA), Vol 4, No.3 May 2013", "doi": null, "report-no": null, "categories": "cs.IR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present in this paper a new approach for the automatic annotation of\nmedical images, using the approach of \"bag-of-words\" to represent the visual\ncontent of the medical image combined with text descriptors based approach\ntf.idf and reduced by latent semantic to extract the co-occurrence between\nterms and visual terms. A medical report is composed of a text describing a\nmedical image. First, we are interested to index the text and extract all\nrelevant terms using a thesaurus containing MeSH medical concepts. In a second\nphase, the medical image is indexed while recovering areas of interest which\nare invariant to change in scale, light and tilt. To annotate a new medical\nimage, we use the approach of \"bagof-words\" to recover the feature vector.\nIndeed, we use the vector space model to retrieve similar medical image from\nthe database training. The calculation of the relevance value of an image to\nthe query image is based on the cosine function. We conclude with an experiment\ncarried out on five types of radiological imaging to evaluate the performance\nof our system of medical annotation. The results showed that our approach works\nbetter with more images from the radiology of the skull.\n", "versions": [{"version": "v1", "created": "Sun, 2 Jun 2013 06:57:33 GMT"}, {"version": "v2", "created": "Tue, 4 Jun 2013 06:27:30 GMT"}], "update_date": "2013-06-05", "authors_parsed": [["Bouslimi", "Riadh", ""], ["Messaoudi", "Abir", ""], ["Akaichi", "Jalel", ""]]}, {"id": "1306.0404", "submitter": "Jun He", "authors": "Jun He, Dejiao Zhang, Laura Balzano, Tao Tao", "title": "Iterative Grassmannian Optimization for Robust Image Alignment", "comments": "Preprint submitted to the special issue of the Image and Vision\n  Computing Journal on the theme \"The Best of Face and Gesture 2013\"", "journal-ref": "Image and Vision Computing, 32(10), 800-813, 2014", "doi": "10.1016/j.imavis.2014.02.015", "report-no": null, "categories": "cs.CV math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Robust high-dimensional data processing has witnessed an exciting development\nin recent years, as theoretical results have shown that it is possible using\nconvex programming to optimize data fit to a low-rank component plus a sparse\noutlier component. This problem is also known as Robust PCA, and it has found\napplication in many areas of computer vision. In image and video processing and\nface recognition, the opportunity to process massive image databases is\nemerging as people upload photo and video data online in unprecedented volumes.\nHowever, data quality and consistency is not controlled in any way, and the\nmassiveness of the data poses a serious computational challenge. In this paper\nwe present t-GRASTA, or \"Transformed GRASTA (Grassmannian Robust Adaptive\nSubspace Tracking Algorithm)\". t-GRASTA iteratively performs incremental\ngradient descent constrained to the Grassmann manifold of subspaces in order to\nsimultaneously estimate a decomposition of a collection of images into a\nlow-rank subspace, a sparse part of occlusions and foreground objects, and a\ntransformation such as rotation or translation of the image. We show that\nt-GRASTA is 4 $\\times$ faster than state-of-the-art algorithms, has half the\nmemory requirement, and can achieve alignment for face images as well as\njittered camera surveillance images.\n", "versions": [{"version": "v1", "created": "Mon, 3 Jun 2013 13:49:14 GMT"}, {"version": "v2", "created": "Thu, 20 Jun 2013 14:08:47 GMT"}], "update_date": "2015-04-21", "authors_parsed": [["He", "Jun", ""], ["Zhang", "Dejiao", ""], ["Balzano", "Laura", ""], ["Tao", "Tao", ""]]}, {"id": "1306.0974", "submitter": "Jiuqing Wan", "authors": "Jiuqing Wan, Li Liu", "title": "Distributed Bayesian inference for consistent labeling of tracked\n  objects in non-overlapping camera networks", "comments": "19 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the fundamental requirements for visual surveillance using\nnon-overlapping camera networks is the correct labeling of tracked objects on\neach camera in a consistent way,in the sense that the captured tracklets, or\nobservations in this paper, of the same object at different cameras should be\nassigned with the same label. In this paper, we formulate this task as a\nBayesian inference problem and propose a distributed inference framework in\nwhich the posterior distribution of labeling variable corresponding to each\nobservation, conditioned on all history appearance and spatio-temporal evidence\nmade in the whole networks, is calculated based solely on local information\nprocessing on each camera and mutual information exchanging between neighboring\ncameras. In our framework, the number of objects presenting in the monitored\nregion, i.e. the sampling space of labeling variables, does not need to be\nspecified beforehand. Instead, it can be determined automatically on the fly.\nIn addition, we make no assumption about the appearance distribution of a\nsingle object, but use similarity scores between appearance pairs, given by\nadvanced object re-identification algorithm, as appearance likelihood for\ninference. This feature makes our method very flexible and competitive when\nobserving condition undergoes large changes across camera views. To cope with\nthe problem of missing detection, which is critical for distributed inference,\nwe consider an enlarged neighborhood of each camera during inference and use a\nmixture model to describe the higher order spatio-temporal constraints. The\nrobustness of the algorithm against missing detection is improved at the cost\nof slightly increased computation and communication burden at each camera node.\nFinally, we demonstrate the effectiveness of our method through experiments on\nan indoor Office Building dataset and an outdoor Campus Garden dataset.\n", "versions": [{"version": "v1", "created": "Wed, 5 Jun 2013 03:50:58 GMT"}], "update_date": "2013-06-06", "authors_parsed": [["Wan", "Jiuqing", ""], ["Liu", "Li", ""]]}, {"id": "1306.1023", "submitter": "Eckhard Hitzer", "authors": "Eckhard Hitzer", "title": "Quaternion Fourier Transform on Quaternion Fields and Generalizations", "comments": "21 pages", "journal-ref": "Advances in Applied Clifford Algebras, olume 17, Issue 3 , pp.\n  497-517 (2007)", "doi": "10.1007/s00006-007-0037-8", "report-no": null, "categories": "math.RA cs.CV math-ph math.MP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We treat the quaternionic Fourier transform (QFT) applied to quaternion\nfields and investigate QFT properties useful for applications. Different forms\nof the QFT lead us to different Plancherel theorems. We relate the QFT\ncomputation for quaternion fields to the QFT of real signals. We research the\ngeneral linear ($GL$) transformation behavior of the QFT with matrices,\nClifford geometric algebra and with examples. We finally arrive at wide-ranging\nnon-commutative multivector FT generalizations of the QFT. Examples given are\nnew volume-time and spacetime algebra Fourier transformations.\n", "versions": [{"version": "v1", "created": "Wed, 5 Jun 2013 09:10:52 GMT"}], "update_date": "2013-06-06", "authors_parsed": [["Hitzer", "Eckhard", ""]]}, {"id": "1306.1034", "submitter": "Mehul Bhatt", "authors": "Mehul Bhatt and Jakob Suchan and Christian Freksa", "title": "ROTUNDE - A Smart Meeting Cinematography Initiative: Tools, Datasets,\n  and Benchmarks for Cognitive Interpretation and Control", "comments": "Appears in AAAI-2013 Workshop on: Space, Time, and Ambient\n  Intelligence (STAMI 2013)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CV cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We construe smart meeting cinematography with a focus on professional\nsituations such as meetings and seminars, possibly conducted in a distributed\nmanner across socio-spatially separated groups. The basic objective in smart\nmeeting cinematography is to interpret professional interactions involving\npeople, and automatically produce dynamic recordings of discussions, debates,\npresentations etc in the presence of multiple communication modalities. Typical\nmodalities include gestures (e.g., raising one's hand for a question,\napplause), voice and interruption, electronic apparatus (e.g., pressing a\nbutton), movement (e.g., standing-up, moving around) etc. ROTUNDE, an instance\nof smart meeting cinematography concept, aims to: (a) develop\nfunctionality-driven benchmarks with respect to the interpretation and control\ncapabilities of human-cinematographers, real-time video editors, surveillance\npersonnel, and typical human performance in everyday situations; (b) Develop\ngeneral tools for the commonsense cognitive interpretation of dynamic scenes\nfrom the viewpoint of visuo-spatial cognition centred perceptual\nnarrativisation. Particular emphasis is placed on declarative representations\nand interfacing mechanisms that seamlessly integrate within large-scale\ncognitive (interaction) systems and companion technologies consisting of\ndiverse AI sub-components. For instance, the envisaged tools would provide\ngeneral capabilities for high-level commonsense reasoning about space, events,\nactions, change, and interaction.\n", "versions": [{"version": "v1", "created": "Wed, 5 Jun 2013 09:40:24 GMT"}], "update_date": "2013-06-06", "authors_parsed": [["Bhatt", "Mehul", ""], ["Suchan", "Jakob", ""], ["Freksa", "Christian", ""]]}, {"id": "1306.1083", "submitter": "Puneet Kumar", "authors": "Pierre-Yves Baudin (INRIA Saclay - Ile de France), Danny Goodman,\n  Puneet Kumar (INRIA Saclay - Ile de France, CVN), Noura Azzabou (MIRCEN,\n  UPMC), Pierre G. Carlier (UPMC), Nikos Paragios (INRIA Saclay - Ile de\n  France, LIGM, ENPC, MAS), M. Pawan Kumar (INRIA Saclay - Ile de France, CVN)", "title": "Discriminative Parameter Estimation for Random Walks Segmentation:\n  Technical Report", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Random Walks (RW) algorithm is one of the most e - cient and easy-to-use\nprobabilistic segmentation methods. By combining contrast terms with prior\nterms, it provides accurate segmentations of medical images in a fully\nautomated manner. However, one of the main drawbacks of using the RW algorithm\nis that its parameters have to be hand-tuned. we propose a novel discriminative\nlearning framework that estimates the parameters using a training dataset. The\nmain challenge we face is that the training samples are not fully supervised.\nSpeci cally, they provide a hard segmentation of the images, instead of a\nproba-bilistic segmentation. We overcome this challenge by treating the optimal\nprobabilistic segmentation that is compatible with the given hard segmentation\nas a latent variable. This allows us to employ the latent support vector\nmachine formulation for parameter estimation. We show that our approach signi\ncantly outperforms the baseline methods on a challenging dataset consisting of\nreal clinical 3D MRI volumes of skeletal muscles.\n", "versions": [{"version": "v1", "created": "Wed, 5 Jun 2013 12:48:02 GMT"}], "update_date": "2013-06-06", "authors_parsed": [["Baudin", "Pierre-Yves", "", "INRIA Saclay - Ile de France"], ["Goodman", "Danny", "", "INRIA Saclay - Ile de France, CVN"], ["Kumar", "Puneet", "", "INRIA Saclay - Ile de France, CVN"], ["Azzabou", "Noura", "", "MIRCEN,\n  UPMC"], ["Carlier", "Pierre G.", "", "UPMC"], ["Paragios", "Nikos", "", "INRIA Saclay - Ile de\n  France, LIGM, ENPC, MAS"], ["Kumar", "M. Pawan", "", "INRIA Saclay - Ile de France, CVN"]]}, {"id": "1306.1301", "submitter": "Joyeeta Singha", "authors": "Joyeeta Singha and Karen Das", "title": "Recognition of Indian Sign Language in Live Video", "comments": "6 pages, 5 figures", "journal-ref": "International Journal of Computer Applications 70(19):17-22, May\n  2013", "doi": "10.5120/12174-7306", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sign Language Recognition has emerged as one of the important area of\nresearch in Computer Vision. The difficulty faced by the researchers is that\nthe instances of signs vary with both motion and appearance. Thus, in this\npaper a novel approach for recognizing various alphabets of Indian Sign\nLanguage is proposed where continuous video sequences of the signs have been\nconsidered. The proposed system comprises of three stages: Preprocessing stage,\nFeature Extraction and Classification. Preprocessing stage includes skin\nfiltering, histogram matching. Eigen values and Eigen Vectors were considered\nfor feature extraction stage and finally Eigen value weighted Euclidean\ndistance is used to recognize the sign. It deals with bare hands, thus allowing\nthe user to interact with the system in natural way. We have considered 24\ndifferent alphabets in the video sequences and attained a success rate of\n96.25%.\n", "versions": [{"version": "v1", "created": "Thu, 6 Jun 2013 05:40:06 GMT"}], "update_date": "2013-06-07", "authors_parsed": [["Singha", "Joyeeta", ""], ["Das", "Karen", ""]]}, {"id": "1306.1358", "submitter": "Eckhard Hitzer", "authors": "Eckhard Hitzer", "title": "Geometric operations implemented by conformal geometric algebra neural\n  nodes", "comments": "6 pages, 2 tables, 10 figures", "journal-ref": "Proc. SICE Symposium on Systems and Information 2008, 26-28 Nov.\n  2008, Himeji, Japan, pp. 357-362 (2008)", "doi": null, "report-no": null, "categories": "cs.CV cs.NE math.RA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Geometric algebra is an optimal frame work for calculating with vectors. The\ngeometric algebra of a space includes elements that represent all the its\nsubspaces (lines, planes, volumes, ...). Conformal geometric algebra expands\nthis approach to elementary representations of arbitrary points, point pairs,\nlines, circles, planes and spheres. Apart from including curved objects,\nconformal geometric algebra has an elegant unified quaternion like\nrepresentation for all proper and improper Euclidean transformations, including\nreflections at spheres, general screw transformations and scaling. Expanding\nthe concepts of real and complex neurons we arrive at the new powerful concept\nof conformal geometric algebra neurons. These neurons can easily take the above\nmentioned geometric objects or sets of these objects as inputs and apply a wide\nrange of geometric transformations via the geometric algebra valued weights.\n", "versions": [{"version": "v1", "created": "Thu, 6 Jun 2013 09:48:49 GMT"}], "update_date": "2013-06-07", "authors_parsed": [["Hitzer", "Eckhard", ""]]}, {"id": "1306.1392", "submitter": "Alessandro Mirone", "authors": "Alessandro Mirone, Emmanuelle Gouillart, Emmanuel Brun, Paul\n  Tafforeau, Jerome Kieffer", "title": "PyHST2: an hybrid distributed code for high speed tomographic\n  reconstruction with iterative reconstruction and a priori knowledge\n  capabilities", "comments": null, "journal-ref": null, "doi": "10.1016/j.nimb.2013.09.030", "report-no": null, "categories": "math.NA cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the PyHST2 code which is in service at ESRF for phase-contrast and\nabsorption tomography. This code has been engineered to sustain the high data\nflow typical of the third generation synchrotron facilities (10 terabytes per\nexperiment) by adopting a distributed and pipelined architecture. The code\nimplements, beside a default filtered backprojection reconstruction, iterative\nreconstruction techniques with a-priori knowledge. These latter are used to\nimprove the reconstruction quality or in order to reduce the required data\nvolume and reach a given quality goal. The implemented a-priori knowledge\ntechniques are based on the total variation penalisation and a new recently\nfound convex functional which is based on overlapping patches.\n  We give details of the different methods and their implementations while the\ncode is distributed under free license.\n  We provide methods for estimating, in the absence of ground-truth data, the\noptimal parameters values for a-priori techniques.\n", "versions": [{"version": "v1", "created": "Thu, 6 Jun 2013 12:29:57 GMT"}], "update_date": "2015-06-02", "authors_parsed": [["Mirone", "Alessandro", ""], ["Gouillart", "Emmanuelle", ""], ["Brun", "Emmanuel", ""], ["Tafforeau", "Paul", ""], ["Kieffer", "Jerome", ""]]}, {"id": "1306.1462", "submitter": "Kanika Bansal", "authors": "Kanika Bansal, Rajiv Kumar", "title": "K-Algorithm A Modified Technique for Noise Removal in Handwritten\n  Documents", "comments": null, "journal-ref": "International Journal of Information Sciences and Techniques, May\n  2013, Volume 3, Number 3", "doi": "10.5121/ijist.2013.3301", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  OCR has been an active research area since last few decades. OCR performs the\nrecognition of the text in the scanned document image and converts it into\neditable form. The OCR process can have several stages like pre-processing,\nsegmentation, recognition and post processing. The pre-processing stage is a\ncrucial stage for the success of OCR, which mainly deals with noise removal. In\nthe present paper, a modified technique for noise removal named as K-Algorithm\nhas been proposed, which has two stages as filtering and binarization. The\nproposed technique shows improvised results in comparison to median filtering\ntechnique.\n", "versions": [{"version": "v1", "created": "Thu, 6 Jun 2013 16:30:57 GMT"}], "update_date": "2013-06-07", "authors_parsed": [["Bansal", "Kanika", ""], ["Kumar", "Rajiv", ""]]}, {"id": "1306.1603", "submitter": "Ognjen Arandjelovi\\'c PhD", "authors": "Reza Shoja Ghiass, Ognjen Arandjelovic, Hakim Bendada, and Xavier\n  Maldague", "title": "Infrared face recognition: a literature review", "comments": "International Joint Conference on Neural Networks, 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic face recognition (AFR) is an area with immense practical potential\nwhich includes a wide range of commercial and law enforcement applications, and\nit continues to be one of the most active research areas of computer vision.\nEven after over three decades of intense research, the state-of-the-art in AFR\ncontinues to improve, benefiting from advances in a range of different fields\nincluding image processing, pattern recognition, computer graphics and\nphysiology. However, systems based on visible spectrum images continue to face\nchallenges in the presence of illumination, pose and expression changes, as\nwell as facial disguises, all of which can significantly decrease their\naccuracy. Amongst various approaches which have been proposed in an attempt to\novercome these limitations, the use of infrared (IR) imaging has emerged as a\nparticularly promising research direction. This paper presents a comprehensive\nand timely review of the literature on this subject.\n", "versions": [{"version": "v1", "created": "Fri, 7 Jun 2013 03:57:53 GMT"}], "update_date": "2013-06-10", "authors_parsed": [["Ghiass", "Reza Shoja", ""], ["Arandjelovic", "Ognjen", ""], ["Bendada", "Hakim", ""], ["Maldague", "Xavier", ""]]}, {"id": "1306.1609", "submitter": "Ognjen Arandjelovi\\'c PhD", "authors": "Reza Shoja Ghiass, Ognjen Arandjelovic, Hakim Bendada, and Xavier\n  Maldague", "title": "Vesselness features and the inverse compositional AAM for robust face\n  recognition using thermal IR", "comments": null, "journal-ref": "AAAI Conference on Artificial Intelligence, 2013", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Over the course of the last decade, infrared (IR) and particularly thermal IR\nimaging based face recognition has emerged as a promising complement to\nconventional, visible spectrum based approaches which continue to struggle when\napplied in the real world. While inherently insensitive to visible spectrum\nillumination changes, IR images introduce specific challenges of their own,\nmost notably sensitivity to factors which affect facial heat emission patterns,\ne.g. emotional state, ambient temperature, and alcohol intake. In addition,\nfacial expression and pose changes are more difficult to correct in IR images\nbecause they are less rich in high frequency detail which is an important cue\nfor fitting any deformable model. We describe a novel method which addresses\nthese challenges. To normalize for pose and facial expression changes we\ngenerate a synthetic frontal image of a face in a canonical, neutral facial\nexpression from an image of the face in an arbitrary pose and facial\nexpression. This is achieved by piecewise affine warping which follows active\nappearance model (AAM) fitting. This is the first publication which explores\nthe use of an AAM on thermal IR images; we propose a pre-processing step which\nenhances detail in thermal images, making AAM convergence faster and more\naccurate. To overcome the problem of thermal IR image sensitivity to the\npattern of facial temperature emissions we describe a representation based on\nreliable anatomical features. In contrast to previous approaches, our\nrepresentation is not binary; rather, our method accounts for the reliability\nof the extracted features. This makes the proposed representation much more\nrobust both to pose and scale changes. The effectiveness of the proposed\napproach is demonstrated on the largest public database of thermal IR images of\nfaces on which it achieved 100% identification, significantly outperforming\nprevious methods.\n", "versions": [{"version": "v1", "created": "Fri, 7 Jun 2013 05:03:16 GMT"}], "update_date": "2013-06-10", "authors_parsed": [["Ghiass", "Reza Shoja", ""], ["Arandjelovic", "Ognjen", ""], ["Bendada", "Hakim", ""], ["Maldague", "Xavier", ""]]}, {"id": "1306.1619", "submitter": "Ji Won Yoon Ph.D.", "authors": "Ji Won Yoon", "title": "Statistical Denoising for single molecule fluorescence microscopic\n  images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Single molecule fluorescence microscopy is a powerful technique for\nuncovering detailed information about biological systems, both in vitro and in\nvivo. In such experiments, the inherently low signal to noise ratios mean that\naccurate algorithms to separate true signal and background noise are essential\nto generate meaningful results. To this end, we have developed a new and robust\nmethod to reduce noise in single molecule fluorescence images by using a\nGaussian Markov Random Field (GMRF) prior in a Bayesian framework. Two\ndifferent strategies are proposed to build the prior - an intrinsic GMRF, with\na stationary relationship between pixels and a heterogeneous intrinsic GMRF,\nwith a differently weighted relationship between pixels classified as molecules\nand background. Testing with synthetic and real experimental fluorescence\nimages demonstrates that the heterogeneous intrinsic GMRF is superior to other\nconventional de-noising approaches.\n", "versions": [{"version": "v1", "created": "Fri, 7 Jun 2013 05:39:48 GMT"}], "update_date": "2013-06-10", "authors_parsed": [["Yoon", "Ji Won", ""]]}, {"id": "1306.1650", "submitter": "Eckhard Hitzer", "authors": "Eckhard Hitzer", "title": "OPS-QFTs: A new type of quaternion Fourier transforms based on the\n  orthogonal planes split with one or two general pure quaternions", "comments": "4 pages", "journal-ref": "Numerical Analysis and and Applied Mathematics ICNAAM 2011:\n  International Conference on Numerical Analysis and Applied Mathematics. AIP\n  Conference Proceedings, Volume 1389, pp. 280-283 (2011)", "doi": "10.1063/1.3636721", "report-no": null, "categories": "math.RA cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We explain the orthogonal planes split (OPS) of quaternions based on the\narbitrary choice of one or two linearly independent pure unit quaternions\n$f,g$. Next we systematically generalize the quaternionic Fourier transform\n(QFT) applied to quaternion fields to conform with the OPS determined by $f,g$,\nor by only one pure unit quaternion $f$, comment on their geometric meaning,\nand establish inverse transformations.\n  Keywords: Clifford geometric algebra, quaternion geometry, quaternion Fourier\ntransform, inverse Fourier transform, orthogonal planes split\n", "versions": [{"version": "v1", "created": "Fri, 7 Jun 2013 08:17:56 GMT"}], "update_date": "2013-06-10", "authors_parsed": [["Hitzer", "Eckhard", ""]]}, {"id": "1306.1653", "submitter": "Eckhard Hitzer", "authors": "Eckhard Hitzer", "title": "Non-constant bounded holomorphic functions of hyperbolic numbers -\n  Candidates for hyperbolic activation functions", "comments": "6 pages, 2 figures", "journal-ref": "in Y. Kuroe, T. Nitta (eds.), Proceedings of the First SICE\n  Symposium on Computational Intelligence [Concentrating on Clifford Neural\n  Computing], 30 Sep. 2011, KIT, Kyoto, Japan, catalogue no. 11PG0009, pp. 23 -\n  28, 2011", "doi": null, "report-no": null, "categories": "cs.NE cs.CV math.RA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Liouville theorem states that bounded holomorphic complex functions are\nnecessarily constant. Holomorphic functions fulfill the socalled Cauchy-Riemann\n(CR) conditions. The CR conditions mean that a complex $z$-derivative is\nindependent of the direction. Holomorphic functions are ideal for activation\nfunctions of complex neural networks, but the Liouville theorem makes them\nuseless. Yet recently the use of hyperbolic numbers, lead to the construction\nof hyperbolic number neural networks. We will describe the Cauchy-Riemann\nconditions for hyperbolic numbers and show that there exists a new interesting\ntype of bounded holomorphic functions of hyperbolic numbers, which are not\nconstant. We give examples of such functions. They therefore substantially\nexpand the available candidates for holomorphic activation functions for\nhyperbolic number neural networks.\n  Keywords: Hyperbolic numbers, Liouville theorem, Cauchy-Riemann conditions,\nbounded holomorphic functions\n", "versions": [{"version": "v1", "created": "Fri, 7 Jun 2013 08:43:11 GMT"}], "update_date": "2013-06-10", "authors_parsed": [["Hitzer", "Eckhard", ""]]}, {"id": "1306.1669", "submitter": "Eckhard Hitzer", "authors": "Eckhard Hitzer", "title": "Quaternionic Fourier-Mellin Transform", "comments": "11 pages, 9 figures", "journal-ref": "in T. Sugawa (ed.), Proc. of the The 19th Int. Conf. on Finite or\n  Infinite Dim. Complex Analysis and Appl. (ICFIDCAA), 11-15 December 2011,\n  Hiroshima, Japan, Tohoku Univ. Press, Sendai (2013), pp. ii, 123-131", "doi": null, "report-no": null, "categories": "math.RA cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this contribution we generalize the classical Fourier Mellin transform [S.\nDorrode and F. Ghorbel, Robust and efficient Fourier-Mellin transform\napproximations for gray-level image reconstruction and complete invariant\ndescription, Computer Vision and Image Understanding, 83(1) (2001), 57-78, DOI\n10.1006/cviu.2001.0922.], which transforms functions $f$ representing, e.g., a\ngray level image defined over a compact set of $\\mathbb{R}^2$. The quaternionic\nFourier Mellin transform (QFMT) applies to functions $f: \\mathbb{R}^2\n\\rightarrow \\mathbb{H}$, for which $|f|$ is summable over $\\mathbb{R}_+^*\n\\times \\mathbb{S}^1$ under the measure $d\\theta \\frac{dr}{r}$. $\\mathbb{R}_+^*$\nis the multiplicative group of positive and non-zero real numbers. We\ninvestigate the properties of the QFMT similar to the investigation of the\nquaternionic Fourier Transform (QFT) in [E. Hitzer, Quaternion Fourier\nTransform on Quaternion Fields and Generalizations, Advances in Applied\nClifford Algebras, 17(3) (2007), 497-517.; E. Hitzer, Directional Uncertainty\nPrinciple for Quaternion Fourier Transforms, Advances in Applied Clifford\nAlgebras, 20(2) (2010), 271-284, online since 08 July 2009.].\n", "versions": [{"version": "v1", "created": "Fri, 7 Jun 2013 09:32:25 GMT"}], "update_date": "2013-06-10", "authors_parsed": [["Hitzer", "Eckhard", ""]]}, {"id": "1306.1676", "submitter": "Eckhard Hitzer", "authors": "Eckhard Hitzer", "title": "Algebraic foundations of split hypercomplex nonlinear adaptive filtering", "comments": "14 pages, 1 figure", "journal-ref": "Mathematical Methods in the Applied Sciences, Volume 36, Issue 9,\n  pages 1042-1055, June 2013", "doi": "10.1002/mma.2660", "report-no": null, "categories": "cs.CV math.RA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A split hypercomplex learning algorithm for the training of nonlinear finite\nimpulse response adaptive filters for the processing of hypercomplex signals of\nany dimension is proposed. The derivation strictly takes into account the laws\nof hypercomplex algebra and hypercomplex calculus, some of which have been\nneglected in existing learning approaches (e.g. for quaternions). Already in\nthe case of quaternions we can predict improvements in performance of\nhypercomplex processes. The convergence of the proposed algorithms is\nrigorously analyzed.\n  Keywords: Quaternionic adaptive filtering, Hypercomplex adaptive filtering,\nNonlinear adaptive filtering, Hypercomplex Multilayer Perceptron, Clifford\ngeometric algebra\n", "versions": [{"version": "v1", "created": "Fri, 7 Jun 2013 09:55:29 GMT"}], "update_date": "2013-06-10", "authors_parsed": [["Hitzer", "Eckhard", ""]]}, {"id": "1306.1679", "submitter": "Eckhard Hitzer", "authors": "Eckhard Hitzer", "title": "Clifford Fourier-Mellin transform with two real square roots of -1 in\n  Cl(p,q), p+q=2", "comments": "6 pages, 2 figures", "journal-ref": "9th International conference on mathematical problems in\n  engineering, aerospace and sciences: ICNPAA 2012. AIP Conference Proceedings,\n  Volume 1493, pp. 480-485 (2012)", "doi": "10.1063/1.4765531", "report-no": null, "categories": "math.RA cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe a non-commutative generalization of the complex Fourier-Mellin\ntransform to Clifford algebra valued signal functions over the domain\n$\\R^{p,q}$ taking values in Cl(p,q), p+q=2.\n  Keywords: algebra, Fourier transforms; Logic, set theory, and algebra,\nFourier analysis, Integral transforms\n", "versions": [{"version": "v1", "created": "Fri, 7 Jun 2013 10:10:59 GMT"}], "update_date": "2013-06-10", "authors_parsed": [["Hitzer", "Eckhard", ""]]}, {"id": "1306.1822", "submitter": "Ognjen Arandjelovi\\'c PhD", "authors": "Reza Shoja Ghiass, Ognjen Arandjelovic, Hakim Bendada, Xavier Maldague", "title": "Illumination-invariant face recognition from a single image across\n  extreme pose using a dual dimension AAM ensemble in the thermal infrared\n  spectrum", "comments": "International Joint Conference on Neural Networks, 2013. arXiv admin\n  note: substantial text overlap with arXiv:1306.1609", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Over the course of the last decade, infrared (IR) and particularly thermal IR\nimaging based face recognition has emerged as a promising complement to\nconventional, visible spectrum based approaches which continue to struggle when\napplied in practice. While inherently insensitive to visible spectrum\nillumination changes, IR data introduces specific challenges of its own, most\nnotably sensitivity to factors which affect facial heat emission patterns, e.g.\nemotional state, ambient temperature, and alcohol intake. In addition, facial\nexpression and pose changes are more difficult to correct in IR images because\nthey are less rich in high frequency detail which is an important cue for\nfitting any deformable model. In this paper we describe a novel method which\naddresses these major challenges. Specifically, when comparing two thermal IR\nimages of faces, we mutually normalize their poses and facial expressions by\nusing an active appearance model (AAM) to generate synthetic images of the two\nfaces with a neutral facial expression and in the same view (the average of the\ntwo input views). This is achieved by piecewise affine warping which follows\nAAM fitting. A major contribution of our work is the use of an AAM ensemble in\nwhich each AAM is specialized to a particular range of poses and a particular\nregion of the thermal IR face space. Combined with the contributions from our\nprevious work which addressed the problem of reliable AAM fitting in the\nthermal IR spectrum, and the development of a person-specific representation\nrobust to transient changes in the pattern of facial temperature emissions, the\nproposed ensemble framework accurately matches faces across the full range of\nyaw from frontal to profile, even in the presence of scale variation (e.g. due\nto the varying distance of a subject from the camera).\n", "versions": [{"version": "v1", "created": "Fri, 7 Jun 2013 04:17:25 GMT"}], "update_date": "2013-06-11", "authors_parsed": [["Ghiass", "Reza Shoja", ""], ["Arandjelovic", "Ognjen", ""], ["Bendada", "Hakim", ""], ["Maldague", "Xavier", ""]]}, {"id": "1306.1894", "submitter": "Alejandro Frery", "authors": "Mar\\'ia Elena Buemi and Alejandro C. Frery and Heitor S. Ramos", "title": "Speckle Reduction with Adaptive Stack Filters", "comments": "Accepted for publication on Pattern Recognition Letters. arXiv admin\n  note: substantial text overlap with arXiv:1207.4308", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stack filters are a special case of non-linear filters. They have a good\nperformance for filtering images with different types of noise while preserving\nedges and details. A stack filter decomposes an input image into stacks of\nbinary images according to a set of thresholds. Each binary image is then\nfiltered by a Boolean function, which characterizes the filter. Adaptive stack\nfilters can be computed by training using a prototype (ideal) image and its\ncorrupted version, leading to optimized filters with respect to a loss\nfunction. In this work we propose the use of training with selected samples for\nthe estimation of the optimal Boolean function. We study the performance of\nadaptive stack filters when they are applied to speckled imagery, in particular\nto Synthetic Aperture Radar (SAR) images. This is done by evaluating the\nquality of the filtered images through the use of suitable image quality\nindexes and by measuring the classification accuracy of the resulting images.\nWe used SAR images as input, since they are affected by speckle noise that\nmakes classification a difficult task.\n", "versions": [{"version": "v1", "created": "Sat, 8 Jun 2013 08:21:06 GMT"}], "update_date": "2013-06-11", "authors_parsed": [["Buemi", "Mar\u00eda Elena", ""], ["Frery", "Alejandro C.", ""], ["Ramos", "Heitor S.", ""]]}, {"id": "1306.1913", "submitter": "Zoltan Szabo", "authors": "Andras Lorincz, Laszlo Jeni, Zoltan Szabo, Jeffrey Cohn, Takeo Kanade", "title": "Emotional Expression Classification using Time-Series Kernels", "comments": "IEEE International Workshop on Analysis and Modeling of Faces and\n  Gestures, Portland, Oregon, 28 June 2013 (accepted)", "journal-ref": null, "doi": "10.1109/CVPRW.2013.131", "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimation of facial expressions, as spatio-temporal processes, can take\nadvantage of kernel methods if one considers facial landmark positions and\ntheir motion in 3D space. We applied support vector classification with kernels\nderived from dynamic time-warping similarity measures. We achieved over 99%\naccuracy - measured by area under ROC curve - using only the 'motion pattern'\nof the PCA compressed representation of the marker point vector, the so-called\nshape parameters. Beyond the classification of full motion patterns, several\nexpressions were recognized with over 90% accuracy in as few as 5-6 frames from\ntheir onset, about 200 milliseconds.\n", "versions": [{"version": "v1", "created": "Sat, 8 Jun 2013 12:57:39 GMT"}], "update_date": "2016-11-17", "authors_parsed": [["Lorincz", "Andras", ""], ["Jeni", "Laszlo", ""], ["Szabo", "Zoltan", ""], ["Cohn", "Jeffrey", ""], ["Kanade", "Takeo", ""]]}, {"id": "1306.2003", "submitter": "Abraao D. C. Nacimento", "authors": "Abra\\~ao D. C. Nascimento, Michelle M. Horta, Alejandro C. Frery, and\n  Renato J. Cintra", "title": "Comparing Edge Detection Methods based on Stochastic Entropies and\n  Distances for PolSAR Imagery", "comments": "12 pages, 14 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.CV eess.IV stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Polarimetric synthetic aperture radar (PolSAR) has achieved a prominent\nposition as a remote imaging method. However, PolSAR images are contaminated by\nspeckle noise due to the coherent illumination employed during the data\nacquisition. This noise provides a granular aspect to the image, making its\nprocessing and analysis (such as in edge detection) hard tasks. This paper\ndiscusses seven methods for edge detection in multilook PolSAR images. In all\nmethods, the basic idea consists in detecting transition points in the finest\npossible strip of data which spans two regions. The edge is contoured using the\ntransitions points and a B-spline curve. Four stochastic distances, two\ndifferences of entropies, and the maximum likelihood criterion were used under\nthe scaled complex Wishart distribution; the first six stem from the h-phi\nclass of measures. The performance of the discussed detection methods was\nquantified and analyzed by the computational time and probability of correct\nedge detection, with respect to the number of looks, the backscatter matrix as\na whole, the SPAN, the covariance an the spatial resolution. The detection\nprocedures were applied to three real PolSAR images. Results provide evidence\nthat the methods based on the Bhattacharyya distance and the difference of\nShannon entropies outperform the other techniques.\n", "versions": [{"version": "v1", "created": "Sun, 9 Jun 2013 10:40:20 GMT"}], "update_date": "2020-06-16", "authors_parsed": [["Nascimento", "Abra\u00e3o D. C.", ""], ["Horta", "Michelle M.", ""], ["Frery", "Alejandro C.", ""], ["Cintra", "Renato J.", ""]]}, {"id": "1306.2081", "submitter": "Bo Li", "authors": "Bo Li and Henry Johan", "title": "3D model retrieval using global and local radial distances", "comments": "6", "journal-ref": "The International Workshop on Advanced Image Technology\n  (IWAIT2010), 2010", "doi": null, "report-no": null, "categories": "cs.GR cs.CV cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  3D model retrieval techniques can be classified as histogram-based,\nview-based and graph-based approaches. We propose a hybrid shape descriptor\nwhich combines the global and local radial distance features by utilizing the\nhistogram-based and view-based approaches respectively. We define an\narea-weighted global radial distance with respect to the center of the bounding\nsphere of the model and encode its distribution into a 2D histogram as the\nglobal radial distance shape descriptor. We then uniformly divide the bounding\ncube of a 3D model into a set of small cubes and define their centers as local\ncenters. Then, we compute the local radial distance of a point based on the\nnearest local center. By sparsely sampling a set of views and encoding the\nlocal radial distance feature on the rendered views by color coding, we extract\nthe local radial distance shape descriptor. Based on these two shape\ndescriptors, we develop a hybrid radial distance shape descriptor for 3D model\nretrieval. Experiment results show that our hybrid shape descriptor outperforms\nseveral typical histogram-based and view-based approaches.\n", "versions": [{"version": "v1", "created": "Mon, 10 Jun 2013 01:38:09 GMT"}], "update_date": "2013-06-11", "authors_parsed": [["Li", "Bo", ""], ["Johan", "Henry", ""]]}, {"id": "1306.2100", "submitter": "Ognjen Arandjelovi\\'c PhD", "authors": "Ognjen Arandjelovic", "title": "Discriminative extended canonical correlation analysis for pattern set\n  matching", "comments": "Machine Learning, 2013", "journal-ref": null, "doi": "10.1007/s10994-013-5380-5", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we address the problem of matching sets of vectors embedded in\nthe same input space. We propose an approach which is motivated by canonical\ncorrelation analysis (CCA), a statistical technique which has proven successful\nin a wide variety of pattern recognition problems. Like CCA when applied to the\nmatching of sets, our extended canonical correlation analysis (E-CCA) aims to\nextract the most similar modes of variability within two sets. Our first major\ncontribution is the formulation of a principled framework for robust inference\nof such modes from data in the presence of uncertainty associated with noise\nand sampling randomness. E-CCA retains the efficiency and closed form\ncomputability of CCA, but unlike it, does not possess free parameters which\ncannot be inferred directly from data (inherent data dimensionality, and the\nnumber of canonical correlations used for set similarity computation). Our\nsecond major contribution is to show that in contrast to CCA, E-CCA is readily\nadapted to match sets in a discriminative learning scheme which we call\ndiscriminative extended canonical correlation analysis (DE-CCA). Theoretical\ncontributions of this paper are followed by an empirical evaluation of its\npremises on the task of face recognition from sets of rasterized appearance\nimages. The results demonstrate that our approach, E-CCA, already outperforms\nboth CCA and its quasi-discriminative counterpart constrained CCA (C-CCA), for\nall values of their free parameters. An even greater improvement is achieved\nwith the discriminative variant, DE-CCA.\n", "versions": [{"version": "v1", "created": "Mon, 10 Jun 2013 04:41:37 GMT"}], "update_date": "2013-06-11", "authors_parsed": [["Arandjelovic", "Ognjen", ""]]}, {"id": "1306.2102", "submitter": "Ognjen Arandjelovi\\'c PhD", "authors": "Ognjen Arandjelovic", "title": "Discriminative k-means clustering", "comments": null, "journal-ref": "International Joint Conference on Neural Networks, 2013", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The k-means algorithm is a partitional clustering method. Over 60 years old,\nit has been successfully used for a variety of problems. The popularity of\nk-means is in large part a consequence of its simplicity and efficiency. In\nthis paper we are inspired by these appealing properties of k-means in the\ndevelopment of a clustering algorithm which accepts the notion of \"positively\"\nand \"negatively\" labelled data. The goal is to discover the cluster structure\nof both positive and negative data in a manner which allows for the\ndiscrimination between the two sets. The usefulness of this idea is\ndemonstrated practically on the problem of face recognition, where the task of\nlearning the scope of a person's appearance should be done in a manner which\nallows this face to be differentiated from others.\n", "versions": [{"version": "v1", "created": "Mon, 10 Jun 2013 04:59:05 GMT"}], "update_date": "2013-06-11", "authors_parsed": [["Arandjelovic", "Ognjen", ""]]}, {"id": "1306.2159", "submitter": "Mikhail Kharinov Vyacheslavovich", "authors": "M.Kharinov", "title": "Image segmentation by optimal and hierarchical piecewise constant\n  approximations", "comments": "4 pages, 5 formulas, 3 figures, 1 table, submitted to the Eleventh\n  International Conference on Pattern Recognition and Image Analysis September\n  23-28, 2013, Samara, Russia", "journal-ref": "Proc. of the 11-th. Int. Conf. (PRIA-11-2013), Russia, Samara,\n  September 23-28, 2013. Vol. 1, pp. 213-216", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Piecewise constant image approximations of sequential number of segments or\nclusters of disconnected pixels are treated. The method of majorizing of\noptimal approximation sequence by hierarchical sequence of image approximations\nis proposed. A generalization for multidimensional case of color and\nmultispectral images is foreseen.\n", "versions": [{"version": "v1", "created": "Mon, 10 Jun 2013 10:35:26 GMT"}], "update_date": "2013-10-02", "authors_parsed": [["Kharinov", "M.", ""]]}, {"id": "1306.2599", "submitter": "Joyeeta Singha", "authors": "Joyeeta Singha and Karen Das", "title": "Hand Gesture Recognition Based on Karhunen-Loeve Transform", "comments": "7 pages,9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we have proposed a system based on K-L Transform to recognize\ndifferent hand gestures. The system consists of five steps: skin filtering,\npalm cropping, edge detection, feature extraction, and classification. Firstly\nthe hand is detected using skin filtering and palm cropping was performed to\nextract out only the palm portion of the hand. The extracted image was then\nprocessed using the Canny Edge Detection technique to extract the outline\nimages of palm. After palm extraction, the features of hand were extracted\nusing K-L Transform technique and finally the input gesture was recognized\nusing proper classifier. In our system, we have tested for 10 different hand\ngestures, and recognizing rate obtained was 96%. Hence we propose an easy\napproach to recognize different hand gestures.\n", "versions": [{"version": "v1", "created": "Tue, 11 Jun 2013 18:03:06 GMT"}], "update_date": "2013-06-12", "authors_parsed": [["Singha", "Joyeeta", ""], ["Das", "Karen", ""]]}, {"id": "1306.2624", "submitter": "Yasel Garc\\'es Su\\'arez", "authors": "Yasel Garc\\'es Su\\'arez, Esley Torres, Osvaldo Pereira, Claudia\n  P\\'erez, and Roberto Rogr\\'iguez", "title": "Stopping Criterion for the Mean Shift Iterative Algorithm", "comments": "Have 8 pages. Is the first version of the more general paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV math.RA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image segmentation is a critical step in computer vision tasks constituting\nan essential issue for pattern recognition and visual interpretation. In this\npaper, we propose a new stopping criterion for the mean shift iterative\nalgorithm by using images defined in Zn ring, with the goal of reaching a\nbetter segmentation. We carried out also a study on the weak and strong of\nequivalence classes between two images. An analysis on the convergence with\nthis new stopping criterion is carried out too.\n", "versions": [{"version": "v1", "created": "Tue, 11 Jun 2013 19:41:45 GMT"}], "update_date": "2013-06-12", "authors_parsed": [["Su\u00e1rez", "Yasel Garc\u00e9s", ""], ["Torres", "Esley", ""], ["Pereira", "Osvaldo", ""], ["P\u00e9rez", "Claudia", ""], ["Rogr\u00edguez", "Roberto", ""]]}, {"id": "1306.2727", "submitter": "Tanaya Guha", "authors": "Tanaya Guha, Ehsan Nezhadarya, Rabab K Ward", "title": "Sparse Representation-based Image Quality Assessment", "comments": "10 pages, 3 figures, 3 tables, submitted to a journal", "journal-ref": null, "doi": "10.1016/j.image.2014.09.010", "report-no": null, "categories": "cs.CV cs.MM eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A successful approach to image quality assessment involves comparing the\nstructural information between a distorted and its reference image. However,\nextracting structural information that is perceptually important to our visual\nsystem is a challenging task. This paper addresses this issue by employing a\nsparse representation-based approach and proposes a new metric called the\n\\emph{sparse representation-based quality} (SPARQ) \\emph{index}. The proposed\nmethod learns the inherent structures of the reference image as a set of basis\nvectors, such that any structure in the image can be represented by a linear\ncombination of only a few of those basis vectors. This sparse strategy is\nemployed because it is known to generate basis vectors that are qualitatively\nsimilar to the receptive field of the simple cells present in the mammalian\nprimary visual cortex. The visual quality of the distorted image is estimated\nby comparing the structures of the reference and the distorted images in terms\nof the learnt basis vectors resembling cortical cells. Our approach is\nevaluated on six publicly available subject-rated image quality assessment\ndatasets. The proposed SPARQ index consistently exhibits high correlation with\nthe subjective ratings on all datasets and performs better or at par with the\nstate-of-the-art.\n", "versions": [{"version": "v1", "created": "Wed, 12 Jun 2013 07:22:36 GMT"}], "update_date": "2021-03-17", "authors_parsed": [["Guha", "Tanaya", ""], ["Nezhadarya", "Ehsan", ""], ["Ward", "Rabab K", ""]]}, {"id": "1306.2795", "submitter": "Ronan Collobert", "authors": "Pedro H. O. Pinheiro, Ronan Collobert", "title": "Recurrent Convolutional Neural Networks for Scene Parsing", "comments": null, "journal-ref": null, "doi": null, "report-no": "Idiap-RR-22-2013", "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scene parsing is a technique that consist on giving a label to all pixels in\nan image according to the class they belong to. To ensure a good visual\ncoherence and a high class accuracy, it is essential for a scene parser to\ncapture image long range dependencies. In a feed-forward architecture, this can\nbe simply achieved by considering a sufficiently large input context patch,\naround each pixel to be labeled. We propose an approach consisting of a\nrecurrent convolutional neural network which allows us to consider a large\ninput context, while limiting the capacity of the model. Contrary to most\nstandard approaches, our method does not rely on any segmentation methods, nor\nany task-specific features. The system is trained in an end-to-end manner over\nraw pixels, and models complex spatial dependencies with low inference cost. As\nthe context size increases with the built-in recurrence, the system identifies\nand corrects its own errors. Our approach yields state-of-the-art performance\non both the Stanford Background Dataset and the SIFT Flow Dataset, while\nremaining very fast at test time.\n", "versions": [{"version": "v1", "created": "Wed, 12 Jun 2013 11:56:57 GMT"}], "update_date": "2013-06-13", "authors_parsed": [["Pinheiro", "Pedro H. O.", ""], ["Collobert", "Ronan", ""]]}, {"id": "1306.2967", "submitter": "Mohsen Joneidi", "authors": "Mohsen Joneidi, Mostafa Sadeghi", "title": "Optimization of Clustering for Clustering-based Image Denoising", "comments": "The paper have some problems that is needed to be re-written. it has\n  been withdrawn", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  In this paper, the problem of de-noising of an image contaminated with\nadditive white Gaussian noise (AWGN) is studied. This subject has been\ncontinued to be an open problem in signal processing for more than 50 years. In\nthe present paper, we suggest a method based on global clustering of image\nconstructing blocks. Noting that the type of clustering plays an important role\nin clustering-based de-noising methods, we address two questions about the\nclustering. First, which parts of data should be considered for clustering?\nSecond, what data clustering method is suitable for de-noising? Clustering is\nexploited to learn an over complete dictionary. By obtaining sparse\ndecomposition of the noisy image blocks in terms of the dictionary atoms, the\nde-noised version is achieved. Experimental results show that our dictionary\nlearning framework outperforms traditional dictionary learning methods such as\nK-SVD.\n", "versions": [{"version": "v1", "created": "Wed, 12 Jun 2013 20:23:07 GMT"}, {"version": "v2", "created": "Thu, 17 Oct 2013 08:36:03 GMT"}, {"version": "v3", "created": "Mon, 28 Oct 2013 18:54:09 GMT"}], "update_date": "2013-10-29", "authors_parsed": [["Joneidi", "Mohsen", ""], ["Sadeghi", "Mostafa", ""]]}, {"id": "1306.3003", "submitter": "Xuhui Fan", "authors": "Xuhui Fan, Yiling Zeng, Longbing Cao", "title": "Non-parametric Power-law Data Clustering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It has always been a great challenge for clustering algorithms to\nautomatically determine the cluster numbers according to the distribution of\ndatasets. Several approaches have been proposed to address this issue,\nincluding the recent promising work which incorporate Bayesian Nonparametrics\ninto the $k$-means clustering procedure. This approach shows simplicity in\nimplementation and solidity in theory, while it also provides a feasible way to\ninference in large scale datasets. However, several problems remains unsolved\nin this pioneering work, including the power-law data applicability, mechanism\nto merge centers to avoid the over-fitting problem, clustering order problem,\ne.t.c.. To address these issues, the Pitman-Yor Process based k-means (namely\n\\emph{pyp-means}) is proposed in this paper. Taking advantage of the Pitman-Yor\nProcess, \\emph{pyp-means} treats clusters differently by dynamically and\nadaptively changing the threshold to guarantee the generation of power-law\nclustering results. Also, one center agglomeration procedure is integrated into\nthe implementation to be able to merge small but close clusters and then\nadaptively determine the cluster number. With more discussion on the clustering\norder, the convergence proof, complexity analysis and extension to spectral\nclustering, our approach is compared with traditional clustering algorithm and\nvariational inference methods. The advantages and properties of pyp-means are\nvalidated by experiments on both synthetic datasets and real world datasets.\n", "versions": [{"version": "v1", "created": "Thu, 13 Jun 2013 01:20:50 GMT"}], "update_date": "2013-06-14", "authors_parsed": [["Fan", "Xuhui", ""], ["Zeng", "Yiling", ""], ["Cao", "Longbing", ""]]}, {"id": "1306.3032", "submitter": "Kazutaka Kurihara", "authors": "Kazutaka Kurihara, Masakazu Takasu, Kazuhiro Sasao, Hal Seki, Takayuki\n  Narabu, Mitsuo Yamamoto, Satoshi Iida, Hiroyuki Yamamoto", "title": "A Face-like Structure Detection on Planet and Satellite Surfaces using\n  Image Processing", "comments": "4 pages", "journal-ref": "ACE 2013, LNCS 8253, Springer, pp. 564-567, 2013", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper demonstrates that face-like structures are everywhere, and can be\nde-tected automatically even with computers. Huge amount of satellite images of\nthe Earth, the Moon, the Mars are explored and many interesting face-like\nstructure are detected. Throughout this fact, we believe that science and\ntechnologies can alert people not to easily become an occultist.\n", "versions": [{"version": "v1", "created": "Thu, 13 Jun 2013 06:28:07 GMT"}], "update_date": "2013-11-20", "authors_parsed": [["Kurihara", "Kazutaka", ""], ["Takasu", "Masakazu", ""], ["Sasao", "Kazuhiro", ""], ["Seki", "Hal", ""], ["Narabu", "Takayuki", ""], ["Yamamoto", "Mitsuo", ""], ["Iida", "Satoshi", ""], ["Yamamoto", "Hiroyuki", ""]]}, {"id": "1306.3084", "submitter": "Doriane Ibarra", "authors": "Jorge Hernandez (CMM), Beatriz Marcotegui (CMM)", "title": "Segmentation et Interpr\\'etation de Nuages de Points pour la\n  Mod\\'elisation d'Environnements Urbains", "comments": null, "journal-ref": "Revue fran\\c{c}aise de photogrammetrie et de t\\'el\\'edection 191\n  (2008) 28-35", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dans cet article, nous pr\\'esentons une m\\'ethode pour la d\\'etection et la\nclassification d'artefacts au niveau du sol, comme phase de filtrage\npr\\'ealable \\`a la mod\\'elisation d'environnements urbains. La m\\'ethode de\nd\\'etection est r\\'ealis\\'ee sur l'image profondeur, une projection de nuage de\npoints sur un plan image o\\`u la valeur du pixel correspond \\`a la distance du\npoint au plan. En faisant l'hypoth\\`ese que les artefacts sont situ\\'es au sol,\nils sont d\\'etect\\'es par une transformation de chapeau haut de forme par\nremplissage de trous sur l'image de profondeur. Les composantes connexes ainsi\nobtenues, sont ensuite caract\\'eris\\'ees et une analyse des variables est\nutilis\\'ee pour la s\\'election des caract\\'eristiques les plus discriminantes.\nLes composantes connexes sont donc classifi\\'ees en quatre cat\\'egories\n(lampadaires, pi\\'etons, voitures et \"Reste\") \\`a l'aide d'un algorithme\nd'apprentissage supervis\\'e. La m\\'ethode a \\'et\\'e test\\'ee sur des nuages de\npoints de la ville de Paris, en montrant de bons r\\'esultats de d\\'etection et\nde classification dans l'ensemble de donn\\'ees.---In this article, we present a\nmethod for detection and classification of artifacts at the street level, in\norder to filter cloud point, facilitating the urban modeling process. Our\napproach exploits 3D information by using range image, a projection of 3D\npoints onto an image plane where the pixel intensity is a function of the\nmeasured distance between 3D points and the plane. By assuming that the\nartifacts are on the ground, they are detected using a Top-Hat of the hole\nfilling algorithm of range images. Then, several features are extracted from\nthe detected connected components and a stepwise forward variable/model\nselection by using the Wilk's Lambda criterion is performed. Afterward, CCs are\nclassified in four categories (lampposts, pedestrians, cars and others) by\nusing a supervised machine learning method. The proposed method was tested on\ncloud points of Paris, and have shown satisfactory results on the whole\ndataset.\n", "versions": [{"version": "v1", "created": "Thu, 13 Jun 2013 11:27:58 GMT"}], "update_date": "2013-06-14", "authors_parsed": [["Hernandez", "Jorge", "", "CMM"], ["Marcotegui", "Beatriz", "", "CMM"]]}, {"id": "1306.3162", "submitter": "Kishore Konda", "authors": "Kishore Reddy Konda, Roland Memisevic, Vincent Michalski", "title": "Learning to encode motion using spatio-temporal synchrony", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the task of learning to extract motion from videos. To this end,\nwe show that the detection of spatial transformations can be viewed as the\ndetection of synchrony between the image sequence and a sequence of features\nundergoing the motion we wish to detect. We show that learning about synchrony\nis possible using very fast, local learning rules, by introducing\nmultiplicative \"gating\" interactions between hidden units across frames. This\nmakes it possible to achieve competitive performance in a wide variety of\nmotion estimation tasks, using a small fraction of the time required to learn\nfeatures, and to outperform hand-crafted spatio-temporal features by a large\nmargin. We also show how learning about synchrony can be viewed as performing\ngreedy parameter estimation in the well-known motion energy model.\n", "versions": [{"version": "v1", "created": "Thu, 13 Jun 2013 16:46:03 GMT"}, {"version": "v2", "created": "Mon, 16 Dec 2013 20:14:24 GMT"}, {"version": "v3", "created": "Mon, 10 Feb 2014 11:19:23 GMT"}], "update_date": "2014-02-11", "authors_parsed": [["Konda", "Kishore Reddy", ""], ["Memisevic", "Roland", ""], ["Michalski", "Vincent", ""]]}, {"id": "1306.3294", "submitter": "Quan Wang", "authors": "Quan Wang, Kim L. Boyer", "title": "Feature Learning by Multidimensional Scaling and its Applications in\n  Object Recognition", "comments": "To appear in SIBGRAPI 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the MDS feature learning framework, in which multidimensional\nscaling (MDS) is applied on high-level pairwise image distances to learn\nfixed-length vector representations of images. The aspects of the images that\nare captured by the learned features, which we call MDS features, completely\ndepend on what kind of image distance measurement is employed. With properly\nselected semantics-sensitive image distances, the MDS features provide rich\nsemantic information about the images that is not captured by other feature\nextraction techniques. In our work, we introduce the iterated\nLevenberg-Marquardt algorithm for solving MDS, and study the MDS feature\nlearning with IMage Euclidean Distance (IMED) and Spatial Pyramid Matching\n(SPM) distance. We present experiments on both synthetic data and real images\n--- the publicly accessible UIUC car image dataset. The MDS features based on\nSPM distance achieve exceptional performance for the car recognition task.\n", "versions": [{"version": "v1", "created": "Fri, 14 Jun 2013 04:43:40 GMT"}], "update_date": "2013-06-17", "authors_parsed": [["Wang", "Quan", ""], ["Boyer", "Kim L.", ""]]}, {"id": "1306.3297", "submitter": "Ognjen Arandjelovi\\'c PhD", "authors": "Ognjen Arandjelovic", "title": "Matching objects across the textured-smooth continuum", "comments": "Australasian Conference on Robotics and Automation, 2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of 3D object recognition is of immense practical importance, with\nthe last decade witnessing a number of breakthroughs in the state of the art.\nMost of the previous work has focused on the matching of textured objects using\nlocal appearance descriptors extracted around salient image points. The\nrecently proposed bag of boundaries method was the first to address directly\nthe problem of matching smooth objects using boundary features. However, no\nprevious work has attempted to achieve a holistic treatment of the problem by\njointly using textural and shape features which is what we describe herein. Due\nto the complementarity of the two modalities, we fuse the corresponding\nmatching scores and learn their relative weighting in a data specific manner by\noptimizing discriminative performance on synthetically distorted data. For the\ntextural description of an object we adopt a representation in the form of a\nhistogram of SIFT based visual words. Similarly the apparent shape of an object\nis represented by a histogram of discretized features capturing local shape. On\na large public database of a diverse set of objects, the proposed method is\nshown to outperform significantly both purely textural and purely shape based\napproaches for matching across viewpoint variation.\n", "versions": [{"version": "v1", "created": "Fri, 14 Jun 2013 05:52:58 GMT"}], "update_date": "2013-06-17", "authors_parsed": [["Arandjelovic", "Ognjen", ""]]}, {"id": "1306.3309", "submitter": "Henry O. Jacobs", "authors": "Henry Jacobs", "title": "Symmetries in LDDMM with higher order momentum distributions", "comments": "12 pages, accepted to the 4th MICCAI workshop on Mathematical\n  Foundations of Computational Anatomy", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.DS cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In some implementations of the Large Deformation Diffeomorphic Metric Mapping\nformulation for image registration we consider the motion of particles which\nlocally translate image data. We then lift the motion of the particles to\nobtain a motion on the entire image. However, it is certainly possible to\nconsider particles which do more than translate, and this is what will be\ndescribed in this paper. As the unreduced Lagrangian associated to EPDiff\npossesses $\\Diff(M)$ symmetry, it must also exhibit $G \\subset \\Diff(M)$\nsymmetry, for any Lie subgroup. In this paper we will describe a tower of Lie\ngroups $G^{(0)} \\subseteq G^{(1)} \\subseteq G^{(2)} \\subseteq...$ which\ncorrespond to preserving $k$-th order jet-data. The reduced configuration\nspaces $Q^{(k)} := \\Diff(M) / G^{(k)}$ will be finite-dimensional (in\nparticular, $Q^{(0)}$ is the configuration manifold for $N$ particles in $M$).\nWe will observe that $G^{(k)}$ is a normal subgroup of $G^{(0)}$ and so the\nquotient $G^{(0)} / G^{(k)}$ is itself a (finite dimensional) Lie group which\nacts on $Q^{(k)}$. This makes $Q^{(k)}$ a principle bundle over $Q^{(0)}$ and\nthe reduced geodesic equations on $Q^{(k)}$ will possess $G^{(0)} /\nG^{(k)}$-symmetry. Noether's theorem implies the existence of conserved momenta\nfor the reduced system on $T^{\\ast}Q^{(k)}$.\n", "versions": [{"version": "v1", "created": "Fri, 14 Jun 2013 07:05:58 GMT"}, {"version": "v2", "created": "Wed, 17 Jul 2013 12:24:02 GMT"}], "update_date": "2015-01-20", "authors_parsed": [["Jacobs", "Henry", ""]]}, {"id": "1306.3415", "submitter": "Ognjen Arandjelovi\\'c PhD", "authors": "Ognjen Arandjelovic", "title": "Live-wire 3D medical images segmentation", "comments": "University of Oxford B.A. Thesis, 2003", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This report describes the design, implementation, evaluation and original\nenhancements to the Live-Wire method for 2D and 3D image segmentation.\nLive-Wire 2D employs a semi-automatic paradigm; the user is asked to select a\nfew boundary points of the object to segment, to steer the process in the right\ndirection, while the result is displayed in real time. In our implementation\nsegmentation is extended to three dimensions by performing this process on a\nslice-by-slice basis. User's time and involvement is further reduced by\nallowing him to specify object contours in planes orthogonal to the slices. If\nthese planes are chosen strategically, Live-Wire 3D can perform 2D segmentation\nin the plane of each slice automatically. This report also proposes two\nimprovements to the original method, path heating and a new graph edge feature\nfunction based on variance of path properties along the boundary. We show that\nthese improvements lead up to a 33% reduction in interaction with the user, and\nimproved delineation in presence of strong interfering edges.\n", "versions": [{"version": "v1", "created": "Fri, 14 Jun 2013 14:52:10 GMT"}], "update_date": "2013-06-17", "authors_parsed": [["Arandjelovic", "Ognjen", ""]]}, {"id": "1306.3476", "submitter": "James Bergstra", "authors": "James Bergstra and David D. Cox", "title": "Hyperparameter Optimization and Boosting for Classifying Facial\n  Expressions: How good can a \"Null\" Model be?", "comments": "Presented at the Workshop on Representation and Learning, ICML 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the goals of the ICML workshop on representation and learning is to\nestablish benchmark scores for a new data set of labeled facial expressions.\nThis paper presents the performance of a \"Null\" model consisting of\nconvolutions with random weights, PCA, pooling, normalization, and a linear\nreadout. Our approach focused on hyperparameter optimization rather than novel\nmodel components. On the Facial Expression Recognition Challenge held by the\nKaggle website, our hyperparameter optimization approach achieved a score of\n60% accuracy on the test data. This paper also introduces a new ensemble\nconstruction variant that combines hyperparameter optimization with the\nconstruction of ensembles. This algorithm constructed an ensemble of four\nmodels that scored 65.5% accuracy. These scores rank 12th and 5th respectively\namong the 56 challenge participants. It is worth noting that our approach was\ndeveloped prior to the release of the data set, and applied without\nmodification; our strong competition performance suggests that the TPE\nhyperparameter optimization algorithm and domain expertise encoded in our Null\nmodel can generalize to new image classification data sets.\n", "versions": [{"version": "v1", "created": "Fri, 14 Jun 2013 18:28:52 GMT"}], "update_date": "2013-06-17", "authors_parsed": [["Bergstra", "James", ""], ["Cox", "David D.", ""]]}, {"id": "1306.3560", "submitter": "Carlo Ciliberto", "authors": "Sean Ryan Fanello, Carlo Ciliberto, Matteo Santoro, Lorenzo Natale,\n  Giorgio Metta, Lorenzo Rosasco, Francesca Odone", "title": "iCub World: Friendly Robots Help Building Good Vision Data-Sets", "comments": "CVPR2013 Workshop: Ground Truth - What is a good dataset?. Portland,\n  USA (June 28, 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present and start analyzing the iCub World data-set, an\nobject recognition data-set, we acquired using a Human-Robot Interaction (HRI)\nscheme and the iCub humanoid robot platform. Our set up allows for rapid\nacquisition and annotation of data with corresponding ground truth. While more\nconstrained in its scopes -- the iCub world is essentially a robotics research\nlab -- we demonstrate how the proposed data-set poses challenges to current\nrecognition systems. The iCubWorld data-set is publicly available. The data-set\ncan be downloaded from: http://www.iit.it/en/projects/data-sets.html.\n", "versions": [{"version": "v1", "created": "Sat, 15 Jun 2013 09:27:17 GMT"}], "update_date": "2013-06-18", "authors_parsed": [["Fanello", "Sean Ryan", ""], ["Ciliberto", "Carlo", ""], ["Santoro", "Matteo", ""], ["Natale", "Lorenzo", ""], ["Metta", "Giorgio", ""], ["Rosasco", "Lorenzo", ""], ["Odone", "Francesca", ""]]}, {"id": "1306.3828", "submitter": "Haichao Zhang Haichao Zhang", "authors": "Haichao Zhang and David Wipf", "title": "Non-Uniform Blind Deblurring with a Spatially-Adaptive Sparse Prior", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Typical blur from camera shake often deviates from the standard uniform\nconvolutional script, in part because of problematic rotations which create\ngreater blurring away from some unknown center point. Consequently, successful\nblind deconvolution requires the estimation of a spatially-varying or\nnon-uniform blur operator. Using ideas from Bayesian inference and convex\nanalysis, this paper derives a non-uniform blind deblurring algorithm with\nseveral desirable, yet previously-unexplored attributes. The underlying\nobjective function includes a spatially adaptive penalty which couples the\nlatent sharp image, non-uniform blur operator, and noise level together. This\ncoupling allows the penalty to automatically adjust its shape based on the\nestimated degree of local blur and image structure such that regions with large\nblur or few prominent edges are discounted. Remaining regions with modest blur\nand revealing edges therefore dominate the overall estimation process without\nexplicitly incorporating structure-selection heuristics. The algorithm can be\nimplemented using a majorization-minimization strategy that is virtually\nparameter free. Detailed theoretical analysis and empirical validation on real\nimages serve to validate the proposed method.\n", "versions": [{"version": "v1", "created": "Mon, 17 Jun 2013 12:12:22 GMT"}], "update_date": "2013-06-18", "authors_parsed": [["Zhang", "Haichao", ""], ["Wipf", "David", ""]]}, {"id": "1306.3855", "submitter": "Dmytro Mishkin", "authors": "Dmytro Mishkin, Michal Perdoch and Jiri Matas", "title": "Two-View Matching with View Synthesis Revisited", "comments": "25 pages, 14 figures", "journal-ref": null, "doi": null, "report-no": "CTU--CMP--2013--15", "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Wide-baseline matching focussing on problems with extreme viewpoint change is\nconsidered. We introduce the use of view synthesis with affine-covariant\ndetectors to solve such problems and show that matching with the Hessian-Affine\nor MSER detectors outperforms the state-of-the-art ASIFT.\n  To minimise the loss of speed caused by view synthesis, we propose the\nMatching On Demand with view Synthesis algorithm (MODS) that uses progressively\nmore synthesized images and more (time-consuming) detectors until reliable\nestimation of geometry is possible. We show experimentally that the MODS\nalgorithm solves problems beyond the state-of-the-art and yet is comparable in\nspeed to standard wide-baseline matchers on simpler problems.\n  Minor contributions include an improved method for tentative correspondence\nselection, applicable both with and without view synthesis and a view synthesis\nsetup greatly improving MSER robustness to blur and scale change that increase\nits running time by 10% only.\n", "versions": [{"version": "v1", "created": "Mon, 17 Jun 2013 13:44:25 GMT"}, {"version": "v2", "created": "Mon, 11 Nov 2013 17:41:22 GMT"}], "update_date": "2013-11-12", "authors_parsed": [["Mishkin", "Dmytro", ""], ["Perdoch", "Michal", ""], ["Matas", "Jiri", ""]]}, {"id": "1306.3874", "submitter": "KyungHyun Cho", "authors": "Kyunghyun Cho and Xi Chen", "title": "Classifying and Visualizing Motion Capture Sequences using Deep Neural\n  Networks", "comments": "VISAPP 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The gesture recognition using motion capture data and depth sensors has\nrecently drawn more attention in vision recognition. Currently most systems\nonly classify dataset with a couple of dozens different actions. Moreover,\nfeature extraction from the data is often computational complex. In this paper,\nwe propose a novel system to recognize the actions from skeleton data with\nsimple, but effective, features using deep neural networks. Features are\nextracted for each frame based on the relative positions of joints (PO),\ntemporal differences (TD), and normalized trajectories of motion (NT). Given\nthese features a hybrid multi-layer perceptron is trained, which simultaneously\nclassifies and reconstructs input data. We use deep autoencoder to visualize\nlearnt features, and the experiments show that deep neural networks can capture\nmore discriminative information than, for instance, principal component\nanalysis can. We test our system on a public database with 65 classes and more\nthan 2,000 motion sequences. We obtain an accuracy above 95% which is, to our\nknowledge, the state of the art result for such a large dataset.\n", "versions": [{"version": "v1", "created": "Mon, 17 Jun 2013 14:26:52 GMT"}, {"version": "v2", "created": "Mon, 1 Sep 2014 16:03:02 GMT"}], "update_date": "2014-09-02", "authors_parsed": [["Cho", "Kyunghyun", ""], ["Chen", "Xi", ""]]}, {"id": "1306.3946", "submitter": "Hong Jiang", "authors": "Hong Jiang, Gang Huang and Paul Wilford", "title": "Multi-view in Lensless Compressive Imaging", "comments": "Accepted for presentation at PCS 2013 as Paper #1021; 4 pages, 4\n  figures. arXiv admin note: text overlap with arXiv:1302.1789", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.CV math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-view images are acquired by a lensless compressive imaging\narchitecture, which consists of an aperture assembly and multiple sensors. The\naperture assembly consists of a two dimensional array of aperture elements\nwhose transmittance can be individually controlled to implement a compressive\nsensing matrix. For each transmittance pattern of the aperture assembly, each\nof the sensors takes a measurement. The measurement vectors from the multiple\nsensors represent multi-view images of the same scene. We present theoretical\nframework for multi-view reconstruction and experimental results for enhancing\nquality of image using multi-view.\n", "versions": [{"version": "v1", "created": "Mon, 17 Jun 2013 18:14:59 GMT"}, {"version": "v2", "created": "Thu, 12 Sep 2013 16:01:12 GMT"}], "update_date": "2013-09-13", "authors_parsed": [["Jiang", "Hong", ""], ["Huang", "Gang", ""], ["Wilford", "Paul", ""]]}, {"id": "1306.4079", "submitter": "Zeng Jie", "authors": "Zeng Jie", "title": "A Novel Block-DCT and PCA Based Image Perceptual Hashing Algorithm", "comments": "7 pages, 5 figrues", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image perceptual hashing finds applications in content indexing, large-scale\nimage database management, certification and authentication and digital\nwatermarking. We propose a Block-DCT and PCA based image perceptual hash in\nthis article and explore the algorithm in the application of tamper detection.\nThe main idea of the algorithm is to integrate color histogram and DCT\ncoefficients of image blocks as perceptual feature, then to compress perceptual\nfeatures as inter-feature with PCA, and to threshold to create a robust hash.\nThe robustness and discrimination properties of the proposed algorithm are\nevaluated in detail. Our algorithms first construct a secondary image, derived\nfrom input image by pseudo-randomly extracting features that approximately\ncapture semi-global geometric characteristics. From the secondary image (which\ndoes not perceptually resemble the input), we further extract the final\nfeatures which can be used as a hash value (and can be further suitably\nquantized). In this paper, we use spectral matrix invariants as embodied by\nSingular Value Decomposition. Surprisingly, formation of the secondary image\nturns out be quite important since it not only introduces further robustness,\nbut also enhances the security properties. Indeed, our experiments reveal that\nour hashing algorithms extract most of the geometric information from the\nimages and hence are robust to severe perturbations (e.g. up to %50 cropping by\narea with 20 degree rotations) on images while avoiding misclassification.\nExperimental results show that the proposed image perceptual hash algorithm can\neffectively address the tamper detection problem with advantageous robustness\nand discrimination.\n", "versions": [{"version": "v1", "created": "Tue, 18 Jun 2013 06:58:58 GMT"}], "update_date": "2013-06-19", "authors_parsed": [["Jie", "Zeng", ""]]}, {"id": "1306.4345", "submitter": "Vishakha Metre VAM", "authors": "Vishakha Metre and Jayshree Ghorpade", "title": "An Overview of the Research on Texture Based Plant Leaf Classification", "comments": "12 pages,5 figures and 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Plant classification has a broad application prospective in agriculture and\nmedicine, and is especially significant to the biology diversity research. As\nplants are vitally important for environmental protection, it is more important\nto identify and classify them accurately. Plant leaf classification is a\ntechnique where leaf is classified based on its different morphological\nfeatures. The goal of this paper is to provide an overview of different aspects\nof texture based plant leaf classification and related things. At last we will\nbe concluding about the efficient method i.e. the method that gives better\nperformance compared to the other methods.\n", "versions": [{"version": "v1", "created": "Tue, 18 Jun 2013 20:38:07 GMT"}], "update_date": "2013-06-20", "authors_parsed": [["Metre", "Vishakha", ""], ["Ghorpade", "Jayshree", ""]]}, {"id": "1306.4478", "submitter": "Stefanie Wuhrer", "authors": "Stefanie Wuhrer, Jochen Lang, Motahareh Tekieh and Chang Shu", "title": "Finite Element Based Tracking of Deforming Surfaces", "comments": "additional experiments", "journal-ref": "Graphical Models, 77(1), pp. 1-17, 2015", "doi": "10.1016/j.gmod.2014.10.002", "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an approach to robustly track the geometry of an object that\ndeforms over time from a set of input point clouds captured from a single\nviewpoint. The deformations we consider are caused by applying forces to known\nlocations on the object's surface. Our method combines the use of prior\ninformation on the geometry of the object modeled by a smooth template and the\nuse of a linear finite element method to predict the deformation. This allows\nthe accurate reconstruction of both the observed and the unobserved sides of\nthe object. We present tracking results for noisy low-quality point clouds\nacquired by either a stereo camera or a depth camera, and simulations with\npoint clouds corrupted by different error terms. We show that our method is\nalso applicable to large non-linear deformations.\n", "versions": [{"version": "v1", "created": "Wed, 19 Jun 2013 10:25:24 GMT"}, {"version": "v2", "created": "Tue, 18 Mar 2014 19:45:52 GMT"}, {"version": "v3", "created": "Tue, 28 Oct 2014 18:40:01 GMT"}], "update_date": "2015-03-31", "authors_parsed": [["Wuhrer", "Stefanie", ""], ["Lang", "Jochen", ""], ["Tekieh", "Motahareh", ""], ["Shu", "Chang", ""]]}, {"id": "1306.4592", "submitter": "Tirtharaj Dash", "authors": "Tirtharaj Dash", "title": "Time Efficient Approach To Offline Hand Written Character Recognition\n  Using Associative Memory Net", "comments": null, "journal-ref": "International Journal of Computing and Business Research (IJCBR)\n  ISSN (Online) : 2229-6166; Volume 3, Issue 3; September 2012", "doi": null, "report-no": null, "categories": "cs.NE cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, an efficient Offline Hand Written Character Recognition\nalgorithm is proposed based on Associative Memory Net (AMN). The AMN used in\nthis work is basically auto associative. The implementation is carried out\ncompletely in 'C' language. To make the system perform to its best with minimal\ncomputation time, a Parallel algorithm is also developed using an API package\nOpenMP. Characters are mainly English alphabets (Small (26), Capital (26))\ncollected from system (52) and from different persons (52). The characters\ncollected from system are used to train the AMN and characters collected from\ndifferent persons are used for testing the recognition ability of the net. The\ndetailed analysis showed that the network recognizes the hand written\ncharacters with recognition rate of 72.20% in average case. However, in best\ncase, it recognizes the collected hand written characters with 88.5%. The\ndeveloped network consumes 3.57 sec (average) in Serial implementation and 1.16\nsec (average) in Parallel implementation using OpenMP.\n", "versions": [{"version": "v1", "created": "Wed, 19 Jun 2013 16:01:29 GMT"}], "update_date": "2013-06-20", "authors_parsed": [["Dash", "Tirtharaj", ""]]}, {"id": "1306.4629", "submitter": "Tirtharaj Dash", "authors": "Tirtharaj Dash and Tanistha Nayak", "title": "Non-Correlated Character Recognition using Artificial Neural Network", "comments": "appeared in: proceedings of National Conference on Dynamics and\n  Prospects of Data Mining: Theory and Practices (DPDM)-2012; September 30,\n  2012, India; Publisher: OITS-BLS, Balasore Chapter; Proceeding ISBN:\n  987-93-81361-31-6, pp. 79-83", "journal-ref": "proc. National Conference on Dynamics and Prospects of Data\n  Mining: Theory and Practices (DPDM)-2012; September 30, 2012, India; ISBN:\n  987-93-81361-31-6, pp. 79-83", "doi": null, "report-no": null, "categories": "cs.NE cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper investigates a method of Handwritten English Character Recognition\nusing Artificial Neural Network (ANN). This work has been done in offline\nEnvironment for non correlated characters, which do not possess any linear\nrelationships among them. We test that whether the particular tested character\nbelongs to a cluster or not. The implementation is carried out in Matlab\nenvironment and successfully tested. Fifty-two sets of English alphabets are\nused to train the ANN and test the network. The algorithms are tested with 26\ncapital letters and 26 small letters. The testing result showed that the\nproposed ANN based algorithm showed a maximum recognition rate of 85%.\n", "versions": [{"version": "v1", "created": "Wed, 19 Jun 2013 17:57:00 GMT"}], "update_date": "2013-06-24", "authors_parsed": [["Dash", "Tirtharaj", ""], ["Nayak", "Tanistha", ""]]}, {"id": "1306.4724", "submitter": "Ognjen Arandjelovi\\'c PhD", "authors": "Ognjen Arandjelovic", "title": "Computer simulation based parameter selection for resistance exercise", "comments": "In Modelling and Simulation, 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In contrast to most scientific disciplines, sports science research has been\ncharacterized by comparatively little effort investment in the development of\nrelevant phenomenological models. Scarcer yet is the application of said models\nin practice. We present a framework which allows resistance training\npractitioners to employ a recently proposed neuromuscular model in actual\ntraining program design. The first novelty concerns the monitoring aspect of\ncoaching. A method for extracting training performance characteristics from\nloosely constrained video sequences, effortlessly and with minimal human input,\nusing computer vision is described. The extracted data is subsequently used to\nfit the underlying neuromuscular model. This is achieved by solving an inverse\ndynamics problem corresponding to a particular exercise. Lastly, a computer\nsimulation of hypothetical training bouts, using athlete-specific capability\nparameters, is used to predict the effected adaptation and changes in\nperformance. The software described here allows the practitioner to manipulate\nhypothetical training parameters and immediately see their effect on predicted\nadaptation for a specific athlete. Thus, this work presents a holistic view of\nthe monitoring-assessment-adjustment loop.\n", "versions": [{"version": "v1", "created": "Thu, 20 Jun 2013 00:47:21 GMT"}], "update_date": "2013-06-21", "authors_parsed": [["Arandjelovic", "Ognjen", ""]]}, {"id": "1306.4746", "submitter": "Daniel Barrett", "authors": "Daniel Paul Barrett and Jeffrey Mark Siskind", "title": "Felzenszwalb-Baum-Welch: Event Detection by Changing Appearance", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a method which can detect events in videos by modeling the change\nin appearance of the event participants over time. This method makes it\npossible to detect events which are characterized not by motion, but by the\nchanging state of the people or objects involved. This is accomplished by using\nobject detectors as output models for the states of a hidden Markov model\n(HMM). The method allows an HMM to model the sequence of poses of the event\nparticipants over time, and is effective for poses of humans and inanimate\nobjects. The ability to use existing object-detection methods as part of an\nevent model makes it possible to leverage ongoing work in the object-detection\ncommunity. A novel training method uses an EM loop to simultaneously learn the\ntemporal structure and object models automatically, without the need to specify\neither the individual poses to be modeled or the frames in which they occur.\nThe E-step estimates the latent assignment of video frames to HMM states, while\nthe M-step estimates both the HMM transition probabilities and state output\nmodels, including the object detectors, which are trained on the weighted\nsubset of frames assigned to their state. A new dataset was gathered because\nlittle work has been done on events characterized by changing object pose, and\nsuitable datasets are not available. Our method produced results superior to\nthat of comparison systems on this dataset.\n", "versions": [{"version": "v1", "created": "Thu, 20 Jun 2013 03:22:19 GMT"}], "update_date": "2013-06-21", "authors_parsed": [["Barrett", "Daniel Paul", ""], ["Siskind", "Jeffrey Mark", ""]]}, {"id": "1306.4758", "submitter": "Payal Gulati Ms", "authors": "Payal Gulati, A. K. Sharma", "title": "Analysing Word Importance for Image Annotation", "comments": "4 pages, 3 figures, Published in IJCSI (International Journal of\n  Computer Science Issues) Journal, Volume 10, Issue 1, No 2, January 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image annotation provides several keywords automatically for a given image\nbased on various tags to describe its contents which is useful in Image\nretrieval. Various researchers are working on text based and content based\nimage annotations [7,9]. It is seen, in traditional Image annotation\napproaches, annotation words are treated equally without considering the\nimportance of each word in real world. In context of this, in this work, images\nare annotated with keywords based on their frequency count and word\ncorrelation. Moreover this work proposes an approach to compute importance\nscore of candidate keywords, having same frequency count.\n", "versions": [{"version": "v1", "created": "Thu, 20 Jun 2013 05:42:58 GMT"}], "update_date": "2013-06-21", "authors_parsed": [["Gulati", "Payal", ""], ["Sharma", "A. K.", ""]]}, {"id": "1306.4966", "submitter": "Rui Hu", "authors": "Rui Hu and Stephen M. Watt", "title": "Determining Points on Handwritten Mathematical Symbols", "comments": "16 pages; 19 figures; Conferences on Intelligent Computer Mathematics\n  (CICM2013), July 8-12, 2013, University of Bath, Bath, UK", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a variety of applications, such as handwritten mathematics and diagram\nlabelling, it is common to have symbols of many different sizes in use and for\nthe writing not to follow simple baselines. In order to understand the scale\nand relative positioning of individual characters, it is necessary to identify\nthe location of certain expected features. These are typically identified by\nparticular points in the symbols, for example, the baseline of a lower case \"p\"\nwould be identified by the lowest part of the bowl, ignoring the descender. We\ninvestigate how to find these special points automatically so they may be used\nin a number of problems, such as improving two-dimensional mathematical\nrecognition and in handwriting neatening, while preserving the original style.\n", "versions": [{"version": "v1", "created": "Thu, 20 Jun 2013 19:38:07 GMT"}], "update_date": "2013-06-21", "authors_parsed": [["Hu", "Rui", ""], ["Watt", "Stephen M.", ""]]}, {"id": "1306.5096", "submitter": "Marko Velic", "authors": "Marko Velic, Ivan Padavic, Sinisa Car", "title": "Computer Aided ECG Analysis - State of the Art and Upcoming Challenges", "comments": "7 pages, 3 figures, IEEE EUROCON 2013 International conference on\n  computer as a tool, 1-4 July 2013, Zagreb, Croatia", "journal-ref": null, "doi": "10.1109/EUROCON.2013.6625218", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present current achievements in computer aided ECG analysis\nand their applicability in real world medical diagnosis process. Most of the\ncurrent work is covering problems of removing noise, detecting heartbeats and\nrhythm-based analysis. There are some advancements in particular ECG segments\ndetection and beat classifications but with limited evaluations and without\nclinical approvals. This paper presents state of the art advancements in those\nareas till present day. Besides this short computer science and signal\nprocessing literature review, paper covers future challenges regarding the ECG\nsignal morphology analysis deriving from the medical literature review. Paper\nis concluded with identified gaps in current advancements and testing, upcoming\nchallenges for future research and a bullseye test is suggested for morphology\nanalysis evaluation.\n", "versions": [{"version": "v1", "created": "Fri, 21 Jun 2013 11:09:18 GMT"}], "update_date": "2016-11-15", "authors_parsed": [["Velic", "Marko", ""], ["Padavic", "Ivan", ""], ["Car", "Sinisa", ""]]}, {"id": "1306.5151", "submitter": "Andrea Vedaldi", "authors": "Subhransu Maji and Esa Rahtu and Juho Kannala and Matthew Blaschko and\n  Andrea Vedaldi", "title": "Fine-Grained Visual Classification of Aircraft", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces FGVC-Aircraft, a new dataset containing 10,000 images\nof aircraft spanning 100 aircraft models, organised in a three-level hierarchy.\nAt the finer level, differences between models are often subtle but always\nvisually measurable, making visual recognition challenging but possible. A\nbenchmark is obtained by defining corresponding classification tasks and\nevaluation protocols, and baseline results are presented. The construction of\nthis dataset was made possible by the work of aircraft enthusiasts, a strategy\nthat can extend to the study of number of other object classes. Compared to the\ndomains usually considered in fine-grained visual classification (FGVC), for\nexample animals, aircraft are rigid and hence less deformable. They, however,\npresent other interesting modes of variation, including purpose, size,\ndesignation, structure, historical style, and branding.\n", "versions": [{"version": "v1", "created": "Fri, 21 Jun 2013 14:31:57 GMT"}], "update_date": "2013-06-24", "authors_parsed": [["Maji", "Subhransu", ""], ["Rahtu", "Esa", ""], ["Kannala", "Juho", ""], ["Blaschko", "Matthew", ""], ["Vedaldi", "Andrea", ""]]}, {"id": "1306.5226", "submitter": "Kunal Narayan Chaudhury", "authors": "Kunal N. Chaudhury, Yuehaw Khoo, Amit Singer", "title": "Global registration of multiple point clouds using semidefinite\n  programming", "comments": "33 pages, 12 figures. To appear in SIAM Journal on Optimization", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.NA math.NA math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider $N$ points in $\\mathbb{R}^d$ and $M$ local coordinate systems that\nare related through unknown rigid transforms. For each point we are given\n(possibly noisy) measurements of its local coordinates in some of the\ncoordinate systems. Alternatively, for each coordinate system, we observe the\ncoordinates of a subset of the points. The problem of estimating the global\ncoordinates of the $N$ points (up to a rigid transform) from such measurements\ncomes up in distributed approaches to molecular conformation and sensor network\nlocalization, and also in computer vision and graphics.\n  The least-squares formulation of this problem, though non-convex, has a well\nknown closed-form solution when $M=2$ (based on the singular value\ndecomposition). However, no closed form solution is known for $M\\geq 3$.\n  In this paper, we demonstrate how the least-squares formulation can be\nrelaxed into a convex program, namely a semidefinite program (SDP). By setting\nup connections between the uniqueness of this SDP and results from rigidity\ntheory, we prove conditions for exact and stable recovery for the SDP\nrelaxation. In particular, we prove that the SDP relaxation can guarantee\nrecovery under more adversarial conditions compared to earlier proposed\nspectral relaxations, and derive error bounds for the registration error\nincurred by the SDP relaxation.\n  We also present results of numerical experiments on simulated data to confirm\nthe theoretical findings. We empirically demonstrate that (a) unlike the\nspectral relaxation, the relaxation gap is mostly zero for the semidefinite\nprogram (i.e., we are able to solve the original non-convex least-squares\nproblem) up to a certain noise threshold, and (b) the semidefinite program\nperforms significantly better than spectral and manifold-optimization methods,\nparticularly at large noise levels.\n", "versions": [{"version": "v1", "created": "Fri, 21 Jun 2013 19:55:40 GMT"}, {"version": "v2", "created": "Sun, 21 Jul 2013 15:36:47 GMT"}, {"version": "v3", "created": "Tue, 3 Sep 2013 01:51:11 GMT"}, {"version": "v4", "created": "Fri, 4 Jul 2014 18:41:18 GMT"}, {"version": "v5", "created": "Tue, 23 Dec 2014 08:01:28 GMT"}], "update_date": "2014-12-24", "authors_parsed": [["Chaudhury", "Kunal N.", ""], ["Khoo", "Yuehaw", ""], ["Singer", "Amit", ""]]}, {"id": "1306.5263", "submitter": "Haonan Yu", "authors": "Haonan Yu, Jeffrey Mark Siskind", "title": "Discriminative Training: Learning to Describe Video with Sentences, from\n  Video Described with Sentences", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a method for learning word meanings from complex and realistic\nvideo clips by discriminatively training (DT) positive sentential labels\nagainst negative ones, and then use the trained word models to generate\nsentential descriptions for new video. This new work is inspired by recent work\nwhich adopts a maximum likelihood (ML) framework to address the same problem\nusing only positive sentential labels. The new method, like the ML-based one,\nis able to automatically determine which words in the sentence correspond to\nwhich concepts in the video (i.e., ground words to meanings) in a weakly\nsupervised fashion. While both DT and ML yield comparable results with\nsufficient training data, DT outperforms ML significantly with smaller training\nsets because it can exploit negative training labels to better constrain the\nlearning problem.\n", "versions": [{"version": "v1", "created": "Fri, 21 Jun 2013 22:09:36 GMT"}], "update_date": "2013-06-25", "authors_parsed": [["Yu", "Haonan", ""], ["Siskind", "Jeffrey Mark", ""]]}, {"id": "1306.5293", "submitter": "Aruna Mastani S", "authors": "S. Aruna Mastani, K. Shilpa", "title": "New Approach of Estimating PSNR-B For De-blocked Images", "comments": "7 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  Measurement of image quality is very crucial to many image processing\napplications. Quality metrics are used to measure the quality of improvement in\nthe images after they are processed and compared with the original images.\nCompression is one of the applications where it is required to monitor the\nquality of decompressed or decoded image. JPEG compression is the lossy\ncompression which is most prevalent technique for image codecs. But it suffers\nfrom blocking artifacts. Various deblocking filters are used to reduce blocking\nartifacts. The efficiency of deblocking filters which improves visual signals\ndegraded by blocking artifacts from compression will also be studied. Objective\nquality metrics like PSNR, SSIM, and PSNRB for analyzing the quality of\ndeblocked images will be studied. We introduce a new approach of PSNR-B for\nanalyzing quality of deblocked images. Simulation results show that new\napproach of PSNR-B called modified PSNR-B. it gives even better results\ncompared to existing well known blockiness specific indices\n", "versions": [{"version": "v1", "created": "Sat, 22 Jun 2013 06:12:26 GMT"}], "update_date": "2013-06-25", "authors_parsed": [["Mastani", "S. Aruna", ""], ["Shilpa", "K.", ""]]}, {"id": "1306.5308", "submitter": "Mehul Bhatt", "authors": "Mehul Bhatt, Jakob Suchan, Carl Schultz", "title": "Cognitive Interpretation of Everyday Activities: Toward Perceptual\n  Narrative Based Visuo-Spatial Scene Interpretation", "comments": "To appear at: Computational Models of Narrative (CMN) 2013., a\n  satellite event of CogSci 2013: The 35th meeting of the Cognitive Science\n  Society", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CV cs.HC cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We position a narrative-centred computational model for high-level knowledge\nrepresentation and reasoning in the context of a range of assistive\ntechnologies concerned with \"visuo-spatial perception and cognition\" tasks. Our\nproposed narrative model encompasses aspects such as \\emph{space, events,\nactions, change, and interaction} from the viewpoint of commonsense reasoning\nand learning in large-scale cognitive systems. The broad focus of this paper is\non the domain of \"human-activity interpretation\" in smart environments, ambient\nintelligence etc. In the backdrop of a \"smart meeting cinematography\" domain,\nwe position the proposed narrative model, preliminary work on perceptual\nnarrativisation, and the immediate outlook on constructing general-purpose\nopen-source tools for perceptual narrativisation.\n  ACM Classification: I.2 Artificial Intelligence: I.2.0 General -- Cognitive\nSimulation, I.2.4 Knowledge Representation Formalisms and Methods, I.2.10\nVision and Scene Understanding: Architecture and control structures, Motion,\nPerceptual reasoning, Shape, Video analysis\n  General keywords: cognitive systems; human-computer interaction; spatial\ncognition and computation; commonsense reasoning; spatial and temporal\nreasoning; assistive technologies\n", "versions": [{"version": "v1", "created": "Sat, 22 Jun 2013 10:37:34 GMT"}], "update_date": "2013-06-25", "authors_parsed": [["Bhatt", "Mehul", ""], ["Suchan", "Jakob", ""], ["Schultz", "Carl", ""]]}, {"id": "1306.5390", "submitter": "Tejaswi Agarwal", "authors": "Tejaswi Agarwal, Saurabh Jha and B. Rajesh Kanna", "title": "P-HGRMS: A Parallel Hypergraph Based Root Mean Square Algorithm for\n  Image Denoising", "comments": "2 pages, 2 figures. Published as poster at the 22nd ACM International\n  Symposium on High Performance Parallel and Distributed Systems, HPDC 2013,\n  New York, USA. Won the Best Poster Award at HPDC 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a parallel Salt and Pepper (SP) noise removal algorithm\nin a grey level digital image based on the Hypergraph Based Root Mean Square\n(HGRMS) approach. HGRMS is generic algorithm for identifying noisy pixels in\nany digital image using a two level hierarchical serial approach. However, for\nSP noise removal, we reduce this algorithm to a parallel model by introducing a\ncardinality matrix and an iteration factor, k, which helps us reduce the\ndependencies in the existing approach. We also observe that the performance of\nthe serial implementation is better on smaller images, but once the threshold\nis achieved in terms of image resolution, its computational complexity\nincreases drastically. We test P-HGRMS using standard images from the Berkeley\nSegmentation dataset on NVIDIAs Compute Unified Device Architecture (CUDA) for\nnoise identification and attenuation. We also compare the noise removal\nefficiency of the proposed algorithm using Peak Signal to Noise Ratio (PSNR) to\nthe existing approach. P-HGRMS maintains the noise removal efficiency and\noutperforms its sequential counterpart by 6 to 18 times (6x - 18x) in\ncomputational efficiency.\n", "versions": [{"version": "v1", "created": "Sun, 23 Jun 2013 09:36:08 GMT"}, {"version": "v2", "created": "Sat, 29 Jun 2013 01:32:41 GMT"}], "update_date": "2013-07-02", "authors_parsed": [["Agarwal", "Tejaswi", ""], ["Jha", "Saurabh", ""], ["Kanna", "B. Rajesh", ""]]}, {"id": "1306.5480", "submitter": "Benjamin Kunsberg", "authors": "Benjamin Kunsberg and Steven W. Zucker", "title": "Characterizing Ambiguity in Light Source Invariant Shape from Shading", "comments": "34 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Shape from shading is a classical inverse problem in computer vision. This\nshape reconstruction problem is inherently ill-defined; it depends on the\nassumed light source direction. We introduce a novel mathematical formulation\nfor calculating local surface shape based on covariant derivatives of the\nshading flow field, rather than the customary integral minimization or P.D.E\napproaches. On smooth surfaces, we show second derivatives of brightness are\nindependent of the light sources and can be directly related to surface\nproperties. We use these measurements to define the matching local family of\nsurfaces that can result from any given shading patch, changing the emphasis to\ncharacterizing ambiguity in the problem. We give an example of how these local\nsurface ambiguities collapse along certain image contours and how this can be\nused for the reconstruction problem.\n", "versions": [{"version": "v1", "created": "Sun, 23 Jun 2013 22:08:07 GMT"}], "update_date": "2013-06-25", "authors_parsed": [["Kunsberg", "Benjamin", ""], ["Zucker", "Steven W.", ""]]}, {"id": "1306.6058", "submitter": "Reza Farrahi Moghaddam", "authors": "Reza Farrahi Moghaddam, Shaohua Chen, Rachid Hedjam, Mohamed Cheriet", "title": "A maximal-information color to gray conversion method for document\n  images: Toward an optimal grayscale representation for document image\n  binarization", "comments": "36 page, the uncompressed version is available on Synchromedia\n  website", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A novel method to convert color/multi-spectral images to gray-level images is\nintroduced to increase the performance of document binarization methods. The\nmethod uses the distribution of the pixel data of the input document image in a\ncolor space to find a transformation, called the dual transform, which balances\nthe amount of information on all color channels. Furthermore, in order to\nreduce the intensity variations on the gray output, a color reduction\npreprocessing step is applied. Then, a channel is selected as the gray value\nrepresentation of the document image based on the homogeneity criterion on the\ntext regions. In this way, the proposed method can provide a\nluminance-independent contrast enhancement. The performance of the method is\nevaluated against various images from two databases, the ICDAR'03 Robust\nReading, the KAIST and the DIBCO'09 datasets, subjectively and objectively with\npromising results. The ground truth images for the images from the ICDAR'03\nRobust Reading dataset have been created manually by the authors.\n", "versions": [{"version": "v1", "created": "Tue, 25 Jun 2013 18:41:04 GMT"}, {"version": "v2", "created": "Wed, 26 Jun 2013 12:20:20 GMT"}], "update_date": "2013-06-27", "authors_parsed": [["Moghaddam", "Reza Farrahi", ""], ["Chen", "Shaohua", ""], ["Hedjam", "Rachid", ""], ["Cheriet", "Mohamed", ""]]}, {"id": "1306.6263", "submitter": "Hossein Ziaei Nafchi", "authors": "Seyed Morteza Ayatollahi, Hossein Ziaei Nafchi", "title": "Persian Heritage Image Binarization Competition (PHIBC 2012)", "comments": "4 pages, 2 figures, conference", "journal-ref": null, "doi": "10.1109/PRIA.2013.6528442", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The first competition on the binarization of historical Persian documents and\nmanuscripts (PHIBC 2012) has been organized in conjunction with the first\nIranian conference on pattern recognition and image analysis (PRIA 2013). The\nmain objective of PHIBC 2012 is to evaluate performance of the binarization\nmethodologies, when applied on the Persian heritage images. This paper provides\na report on the methodology and performance of the three submitted algorithms\nbased on evaluation measures has been used.\n", "versions": [{"version": "v1", "created": "Wed, 26 Jun 2013 14:56:00 GMT"}, {"version": "v2", "created": "Thu, 21 Apr 2016 00:04:34 GMT"}], "update_date": "2016-04-22", "authors_parsed": [["Ayatollahi", "Seyed Morteza", ""], ["Nafchi", "Hossein Ziaei", ""]]}, {"id": "1306.6269", "submitter": "Aditya Tatu Dr.", "authors": "Sumukh Bansal and Aditya Tatu", "title": "Active Contour Models for Manifold Valued Image Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image segmentation is the process of partitioning a image into different\nregions or groups based on some characteristics like color, texture, motion or\nshape etc. Active contours is a popular variational method for object\nsegmentation in images, in which the user initializes a contour which evolves\nin order to optimize an objective function designed such that the desired\nobject boundary is the optimal solution. Recently, imaging modalities that\nproduce Manifold valued images have come up, for example, DT-MRI images, vector\nfields. The traditional active contour model does not work on such images. In\nthis paper, we generalize the active contour model to work on Manifold valued\nimages. As expected, our algorithm detects regions with similar Manifold values\nin the image. Our algorithm also produces expected results on usual gray-scale\nimages, since these are nothing but trivial examples of Manifold valued images.\nAs another application of our general active contour model, we perform texture\nsegmentation on gray-scale images by first creating an appropriate Manifold\nvalued image. We demonstrate segmentation results for manifold valued images\nand texture images.\n", "versions": [{"version": "v1", "created": "Wed, 26 Jun 2013 15:16:54 GMT"}, {"version": "v2", "created": "Mon, 11 Nov 2013 08:02:41 GMT"}], "update_date": "2013-11-12", "authors_parsed": [["Bansal", "Sumukh", ""], ["Tatu", "Aditya", ""]]}, {"id": "1306.6281", "submitter": "Zachary Harmany", "authors": "Zachary T. Harmany, Roummel F. Marcia, Rebecca M. Willett", "title": "Compressive Coded Aperture Keyed Exposure Imaging with Optical Flow\n  Reconstruction", "comments": "13 pages, 4 figures, Submitted to IEEE Transactions on Image\n  Processing. arXiv admin note: substantial text overlap with arXiv:1111.7247", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.CV math.IT stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes a coded aperture and keyed exposure approach to\ncompressive video measurement which admits a small physical platform, high\nphoton efficiency, high temporal resolution, and fast reconstruction\nalgorithms. The proposed projections satisfy the Restricted Isometry Property\n(RIP), and hence compressed sensing theory provides theoretical guarantees on\nthe video reconstruction quality. Moreover, the projections can be easily\nimplemented using existing optical elements such as spatial light modulators\n(SLMs). We extend these coded mask designs to novel dual-scale masks (DSMs)\nwhich enable the recovery of a coarse-resolution estimate of the scene with\nnegligible computational cost. We develop fast numerical algorithms which\nutilize both temporal correlations and optical flow in the video sequence as\nwell as the innovative structure of the projections. Our numerical experiments\ndemonstrate the efficacy of the proposed approach on short-wave infrared data.\n", "versions": [{"version": "v1", "created": "Wed, 26 Jun 2013 15:53:55 GMT"}], "update_date": "2013-06-27", "authors_parsed": [["Harmany", "Zachary T.", ""], ["Marcia", "Roummel F.", ""], ["Willett", "Rebecca M.", ""]]}, {"id": "1306.6726", "submitter": "Aditya Tatu Dr.", "authors": "Aditya Tatu and Sumukh Bansal", "title": "A Novel Active Contour Model for Texture Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Texture is intuitively defined as a repeated arrangement of a basic pattern\nor object in an image. There is no mathematical definition of a texture though.\nThe human visual system is able to identify and segment different textures in a\ngiven image. Automating this task for a computer is far from trivial. There are\nthree major components of any texture segmentation algorithm: (a) The features\nused to represent a texture, (b) the metric induced on this representation\nspace and (c) the clustering algorithm that runs over these features in order\nto segment a given image into different textures. In this paper, we propose an\nactive contour based novel unsupervised algorithm for texture segmentation. We\nuse intensity covariance matrices of regions as the defining feature of\ntextures and find regions that have the most inter-region dissimilar covariance\nmatrices using active contours. Since covariance matrices are symmetric\npositive definite, we use geodesic distance defined on the manifold of\nsymmetric positive definite matrices PD(n) as a measure of dissimlarity between\nsuch matrices. We demonstrate performance of our algorithm on both artificial\nand real texture images.\n", "versions": [{"version": "v1", "created": "Fri, 28 Jun 2013 06:32:42 GMT"}], "update_date": "2013-07-01", "authors_parsed": [["Tatu", "Aditya", ""], ["Bansal", "Sumukh", ""]]}, {"id": "1306.6737", "submitter": "Minati Mishra", "authors": "Minati Mishra, Flt. Lt. Dr. M. C. Adhikary", "title": "Digital Image Tamper Detection Techniques - A Comprehensive Study", "comments": "12 pages available online via\n  http://ijcsbi.org/index.php/ijcsbi/article/view/50", "journal-ref": "IJCSBI Vol. 2, No. 1. June 2013", "doi": null, "report-no": null, "categories": "cs.CR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Photographs are considered to be the most powerful and trustworthy media of\nexpression. For a long time, those were accepted as proves of evidences in\nvaried fields such as journalism, forensic investigations, military\nintelligence, scientific research and publications, crime detection and legal\nproceedings, investigation of insurance claims, medical imaging etc. Today,\ndigital images have completely replaced the conventional photographs from every\nsphere of life but unfortunately, they seldom enjoy the credibility of their\nconventional counterparts, thanks to the rapid advancements in the field of\ndigital image processing. The increasing availability of low cost and sometimes\nfree of cost image editing software such as Photoshop, Corel Paint Shop,\nPhotoscape, PhotoPlus, GIMP and Pixelmator have made the tampering of digital\nimages even more easier and a common practice. Now it has become quite\nimpossible to say whether a photograph is a genuine camera output or a\nmanipulated version of it just by looking at it. As a result, photographs have\nalmost lost their reliability and place as proves of evidences in all fields.\nThis is why digital image tamper detection has emerged as an important research\narea to establish the authenticity of digital photographs by separating the\ntampered lots from the original ones. This paper gives a brief history of image\ntampering and a state-of-the-art review of the tamper detection techniques.\n", "versions": [{"version": "v1", "created": "Fri, 28 Jun 2013 07:22:20 GMT"}], "update_date": "2013-07-01", "authors_parsed": [["Mishra", "Minati", ""], ["Adhikary", "Flt. Lt. Dr. M. C.", ""]]}, {"id": "1306.6842", "submitter": "Dimitris Arabadjis", "authors": "Dimitris Arabadjis, Fotios Giannopoulos, Constantin Papaodysseus,\n  Solomon Zannos, Panayiotis Rousopoulos, Michail Panagopoulos, Christopher\n  Blackwell", "title": "New Mathematical and Algorithmic Schemes for Pattern Classification with\n  Application to the Identification of Writers of Important Ancient Documents", "comments": null, "journal-ref": "Pattern Recognition, Volume 46, Issue 8, Pages 2278-2296, August\n  2013", "doi": "10.1016/j.patcog.2013.01.019", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, a novel approach is introduced for classifying curves into\nproper families, according to their similarity. First, a mathematical quantity\nwe call plane curvature is introduced and a number of propositions are stated\nand proved. Proper similarity measures of two curves are introduced and a\nsubsequent statistical analysis is applied. First, the efficiency of the curve\nfitting process has been tested on 2 shapes datasets of reference. Next, the\nmethodology has been applied to the very important problem of classifying 23\nByzantine codices and 46 Ancient inscriptions to their writers, thus achieving\ncorrect dating of their content. The inscriptions have been attributed to ten\nindividual hands and the Byzantine codices to four writers.\n", "versions": [{"version": "v1", "created": "Fri, 28 Jun 2013 13:51:18 GMT"}], "update_date": "2013-07-01", "authors_parsed": [["Arabadjis", "Dimitris", ""], ["Giannopoulos", "Fotios", ""], ["Papaodysseus", "Constantin", ""], ["Zannos", "Solomon", ""], ["Rousopoulos", "Panayiotis", ""], ["Panagopoulos", "Michail", ""], ["Blackwell", "Christopher", ""]]}]