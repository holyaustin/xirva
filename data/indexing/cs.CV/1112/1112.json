[{"id": "1112.0059", "submitter": "Sancho McCann", "authors": "Sancho McCann, David G. Lowe", "title": "Local Naive Bayes Nearest Neighbor for Image Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": "TR-2011-11", "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present Local Naive Bayes Nearest Neighbor, an improvement to the NBNN\nimage classification algorithm that increases classification accuracy and\nimproves its ability to scale to large numbers of object classes. The key\nobservation is that only the classes represented in the local neighborhood of a\ndescriptor contribute significantly and reliably to their posterior probability\nestimates. Instead of maintaining a separate search structure for each class,\nwe merge all of the reference data together into one search structure, allowing\nquick identification of a descriptor's local neighborhood. We show an increase\nin classification accuracy when we ignore adjustments to the more distant\nclasses and show that the run time grows with the log of the number of classes\nrather than linearly in the number of classes as did the original. This gives a\n100 times speed-up over the original method on the Caltech 256 dataset. We also\nprovide the first head-to-head comparison of NBNN against spatial pyramid\nmethods using a common set of input features. We show that local NBNN\noutperforms all previous NBNN based methods and the original spatial pyramid\nmodel. However, we find that local NBNN, while competitive with, does not beat\nstate-of-the-art spatial pyramid methods that use local soft assignment and\nmax-pooling.\n", "versions": [{"version": "v1", "created": "Thu, 1 Dec 2011 01:19:08 GMT"}], "update_date": "2011-12-02", "authors_parsed": [["McCann", "Sancho", ""], ["Lowe", "David G.", ""]]}, {"id": "1112.0655", "submitter": "Andr\\'as Gelencs\\'er", "authors": "Andras Gelencser, Themistoklis Prodromakis, Christofer Toumazou and\n  Tamas Roska", "title": "A Biomimetic Model of the Outer Plexiform Layer by Incorporating\n  Memristive Devices", "comments": "24 pages, 12 figures", "journal-ref": null, "doi": "10.1103/PhysRevE.85.041918", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present a biorealistic model for the first part of the early\nvision processing by incorporating memristive nanodevices. The architecture of\nthe proposed network is based on the organisation and functioning of the outer\nplexiform layer (OPL) in the vertebrate retina. We demonstrate that memristive\ndevices are indeed a valuable building block for neuromorphic architectures, as\ntheir highly non-linear and adaptive response could be exploited for\nestablishing ultra-dense networks with similar dynamics to their biological\ncounterparts. We particularly show that hexagonal memristive grids can be\nemployed for faithfully emulating the smoothing-effect occurring at the OPL for\nenhancing the dynamic range of the system. In addition, we employ a\nmemristor-based thresholding scheme for detecting the edges of grayscale\nimages, while the proposed system is also evaluated for its adaptation and\nfault tolerance capacity against different light or noise conditions as well as\ndistinct device yields.\n", "versions": [{"version": "v1", "created": "Sat, 3 Dec 2011 13:53:54 GMT"}], "update_date": "2015-06-03", "authors_parsed": [["Gelencser", "Andras", ""], ["Prodromakis", "Themistoklis", ""], ["Toumazou", "Christofer", ""], ["Roska", "Tamas", ""]]}, {"id": "1112.0974", "submitter": "Jan Lellmann", "authors": "Jan Lellmann, Frank Lenzen, and Christoph Schn\\\"orr", "title": "Optimality Bounds for a Variational Relaxation of the Image Partitioning\n  Problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV math.CO math.FA math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a variational convex relaxation of a class of optimal\npartitioning and multiclass labeling problems, which has recently proven quite\nsuccessful and can be seen as a continuous analogue of Linear Programming (LP)\nrelaxation methods for finite-dimensional problems. While for the latter case\nseveral optimality bounds are known, to our knowledge no such bounds exist in\nthe continuous setting. We provide such a bound by analyzing a probabilistic\nrounding method, showing that it is possible to obtain an integral solution of\nthe original partitioning problem from a solution of the relaxed problem with\nan a priori upper bound on the objective, ensuring the quality of the result\nfrom the viewpoint of optimization. The approach has a natural interpretation\nas an approximate, multiclass variant of the celebrated coarea formula.\n", "versions": [{"version": "v1", "created": "Mon, 5 Dec 2011 16:05:32 GMT"}], "update_date": "2011-12-06", "authors_parsed": [["Lellmann", "Jan", ""], ["Lenzen", "Frank", ""], ["Schn\u00f6rr", "Christoph", ""]]}, {"id": "1112.1120", "submitter": "Joan Bruna", "authors": "Joan Bruna and St\\'ephane Mallat", "title": "Classification with Invariant Scattering Representations", "comments": "6 pages, 2 figures; IVMSP Workshop, 2011 IEEE 10th", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV math.FA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A scattering transform defines a signal representation which is invariant to\ntranslations and Lipschitz continuous relatively to deformations. It is\nimplemented with a non-linear convolution network that iterates over wavelet\nand modulus operators. Lipschitz continuity locally linearizes deformations.\nComplex classes of signals and textures can be modeled with low-dimensional\naffine spaces, computed with a PCA in the scattering domain. Classification is\nperformed with a penalized model selection. State of the art results are\nobtained for handwritten digit recognition over small training sets, and for\ntexture classification.\n", "versions": [{"version": "v1", "created": "Mon, 5 Dec 2011 23:25:07 GMT"}], "update_date": "2011-12-07", "authors_parsed": [["Bruna", "Joan", ""], ["Mallat", "St\u00e9phane", ""]]}, {"id": "1112.1187", "submitter": "Andres Almansa", "authors": "Neus Sabater (CMLA), Andr\\'es Almansa (LTCI), Jean-Michel Morel (CMLA)", "title": "Meaningful Matches in Stereovision", "comments": "IEEE Transactions on Pattern Analysis and Machine Intelligence 99,\n  Preprints (2011) 1-12", "journal-ref": null, "doi": "10.1109/TPAMI.2011.207", "report-no": null, "categories": "cs.CV stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a statistical method to decide whether two blocks in a\npair of of images match reliably. The method ensures that the selected block\nmatches are unlikely to have occurred \"just by chance.\" The new approach is\nbased on the definition of a simple but faithful statistical \"background model\"\nfor image blocks learned from the image itself. A theorem guarantees that under\nthis model not more than a fixed number of wrong matches occurs (on average)\nfor the whole image. This fixed number (the number of false alarms) is the only\nmethod parameter. Furthermore, the number of false alarms associated with each\nmatch measures its reliability. This \"a contrario\" block-matching method,\nhowever, cannot rule out false matches due to the presence of periodic objects\nin the images. But it is successfully complemented by a parameterless\n\"self-similarity threshold.\" Experimental evidence shows that the proposed\nmethod also detects occlusions and incoherent motions due to vehicles and\npedestrians in non simultaneous stereo.\n", "versions": [{"version": "v1", "created": "Tue, 6 Dec 2011 08:06:45 GMT"}], "update_date": "2017-12-08", "authors_parsed": [["Sabater", "Neus", "", "CMLA"], ["Almansa", "Andr\u00e9s", "", "LTCI"], ["Morel", "Jean-Michel", "", "CMLA"]]}, {"id": "1112.1200", "submitter": "Duc Phu Chau", "authors": "Duc Phu Chau (INRIA Sophia Antipolis), Fran\\c{c}ois Bremond (INRIA\n  Sophia Antipolis), Monique Thonnat (INRIA Sophia Antipolis)", "title": "A multi-feature tracking algorithm enabling adaptation to context\n  variations", "comments": "The International Conference on Imaging for Crime Detection and\n  Prevention (ICDP) (2011)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose in this paper a tracking algorithm which is able to adapt itself\nto different scene contexts. A feature pool is used to compute the matching\nscore between two detected objects. This feature pool includes 2D, 3D\ndisplacement distances, 2D sizes, color histogram, histogram of oriented\ngradient (HOG), color covariance and dominant color. An offline learning\nprocess is proposed to search for useful features and to estimate their weights\nfor each context. In the online tracking process, a temporal window is defined\nto establish the links between the detected objects. This enables to find the\nobject trajectories even if the objects are misdetected in some frames. A\ntrajectory filter is proposed to remove noisy trajectories. Experimentation on\ndifferent contexts is shown. The proposed tracker has been tested in videos\nbelonging to three public datasets and to the Caretaker European project. The\nexperimental results prove the effect of the proposed feature weight learning,\nand the robustness of the proposed tracker compared to some methods in the\nstate of the art. The contributions of our approach over the state of the art\ntrackers are: (i) a robust tracking algorithm based on a feature pool, (ii) a\nsupervised learning scheme to learn feature weights for each context, (iii) a\nnew method to quantify the reliability of HOG descriptor, (iv) a combination of\ncolor covariance and dominant color features with spatial pyramid distance to\nmanage the case of object occlusion.\n", "versions": [{"version": "v1", "created": "Tue, 6 Dec 2011 09:19:17 GMT"}], "update_date": "2011-12-07", "authors_parsed": [["Chau", "Duc Phu", "", "INRIA Sophia Antipolis"], ["Bremond", "Fran\u00e7ois", "", "INRIA\n  Sophia Antipolis"], ["Thonnat", "Monique", "", "INRIA Sophia Antipolis"]]}, {"id": "1112.1484", "submitter": "Sudam Sekhar panda", "authors": "S.S. Panda, M.S.R.S Prasad, G. Jena", "title": "POCS Based Super-Resolution Image Reconstruction Using an Adaptive\n  Regularization Parameter", "comments": "4 pages,2 fig,2 tables,Published in IJCSI International Journal of\n  Computer Science Issues, Vol. 8, Issue 5, No 2, September 2011 ISSN (Online):\n  1694-0814", "journal-ref": "IJCSI International Journal of Computer Science Issues, Vol. 8,\n  Issue 5, No 2, September 2011 ISSN (Online): 1694-0814", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Crucial information barely visible to the human eye is often embedded in a\nseries of low-resolution images taken of the same scene. Super-resolution\nenables the extraction of this information by reconstructing a single image, at\na high resolution than is present in any of the individual images. This is\nparticularly useful in forensic imaging, where the extraction of minute details\nin an image can help to solve a crime. Super-resolution image restoration has\nbeen one of the most important research areas in recent years which goals to\nobtain a high resolution (HR) image from several low resolutions (LR) blurred,\nnoisy, under sampled and displaced images. Relation of the HR image and LR\nimages can be modeled by a linear system using a transformation matrix and\nadditive noise. However, a unique solution may not be available because of the\nsingularity of transformation matrix. To overcome this problem, POCS method has\nbeen used. However, their performance is not good because the effect of noise\nenergy has been ignored. In this paper, we propose an adaptive regularization\napproach based on the fact that the regularization parameter should be a linear\nfunction of noise variance. The performance of the proposed approach has been\ntested on several images and the obtained results demonstrate the superiority\nof our approach compared with existing methods.\n", "versions": [{"version": "v1", "created": "Wed, 7 Dec 2011 06:29:07 GMT"}], "update_date": "2011-12-08", "authors_parsed": [["Panda", "S. S.", ""], ["Prasad", "M. S. R. S", ""], ["Jena", "G.", ""]]}, {"id": "1112.1496", "submitter": "Kaihua Zhang", "authors": "Kaihua Zhang, Lei Zhang, Huihui Song, David Zhang", "title": "Re-initialization Free Level Set Evolution via Reaction Diffusion", "comments": "IEEE Trans. on Image Processing, to appear", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  This paper presents a novel reaction-diffusion (RD) method for implicit\nactive contours, which is completely free of the costly re-initialization\nprocedure in level set evolution (LSE). A diffusion term is introduced into\nLSE, resulting in a RD-LSE equation, to which a piecewise constant solution can\nbe derived. In order to have a stable numerical solution of the RD based LSE,\nwe propose a two-step splitting method (TSSM) to iteratively solve the RD-LSE\nequation: first iterating the LSE equation, and then solving the diffusion\nequation. The second step regularizes the level set function obtained in the\nfirst step to ensure stability, and thus the complex and costly\nre-initialization procedure is completely eliminated from LSE. By successfully\napplying diffusion to LSE, the RD-LSE model is stable by means of the simple\nfinite difference method, which is very easy to implement. The proposed RD\nmethod can be generalized to solve the LSE for both variational level set\nmethod and PDE-based level set method. The RD-LSE method shows very good\nperformance on boundary anti-leakage, and it can be readily extended to high\ndimensional level set method. The extensive and promising experimental results\non synthetic and real images validate the effectiveness of the proposed RD-LSE\napproach.\n", "versions": [{"version": "v1", "created": "Wed, 7 Dec 2011 08:16:48 GMT"}, {"version": "v2", "created": "Tue, 7 Aug 2012 01:20:11 GMT"}, {"version": "v3", "created": "Wed, 8 Aug 2012 01:28:14 GMT"}], "update_date": "2012-08-09", "authors_parsed": [["Zhang", "Kaihua", ""], ["Zhang", "Lei", ""], ["Song", "Huihui", ""], ["Zhang", "David", ""]]}, {"id": "1112.2095", "submitter": "Sao Mai Nguyen", "authors": "Sao Mai Nguyen (INRIA Bordeaux - Sud-Ouest), Masaki Ogino, Minoru\n  Asada", "title": "Real-time face swapping as a tool for understanding infant\n  self-recognition", "comments": null, "journal-ref": "International Conference on Epigenetic Robotics, Glumslov : Sweden\n  (2010)", "doi": null, "report-no": null, "categories": "cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To study the preference of infants for contingency of movements and\nfamiliarity of faces during self-recognition task, we built, as an accurate and\ninstantaneous imitator, a real-time face- swapper for videos. We present a\nnon-constraint face-swapper based on 3D visual tracking that achieves real-time\nperformance through parallel computing. Our imitator system is par- ticularly\nsuited for experiments involving children with Autistic Spectrum Disorder who\nare often strongly disturbed by the constraints of other methods.\n", "versions": [{"version": "v1", "created": "Fri, 9 Dec 2011 13:19:21 GMT"}], "update_date": "2011-12-12", "authors_parsed": [["Nguyen", "Sao Mai", "", "INRIA Bordeaux - Sud-Ouest"], ["Ogino", "Masaki", ""], ["Asada", "Minoru", ""]]}, {"id": "1112.2386", "submitter": "Omid Pakdelazar", "authors": "'Omid Pakdelazar' and 'Gholamali Rezai-rad'", "title": "Improvement of BM3D Algorithm and Employment to Satellite and CFA Images\n  Denoising", "comments": "11 pages, 7 figure", "journal-ref": null, "doi": "10.5121/ijist.2011.1303", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a new procedure in order to improve the performance of\nblock matching and 3-D filtering (BM3D) image denoising algorithm. It is\ndemonstrated that it is possible to achieve a better performance than that of\nBM3D algorithm in a variety of noise levels. This method changes BM3D algorithm\nparameter values according to noise level, removes prefiltering, which is used\nin high noise level; therefore Peak Signal-to-Noise Ratio (PSNR) and visual\nquality get improved, and BM3D complexities and processing time are reduced.\nThis improved BM3D algorithm is extended and used to denoise satellite and\ncolor filter array (CFA) images. Output results show that the performance has\nupgraded in comparison with current methods of denoising satellite and CFA\nimages. In this regard this algorithm is compared with Adaptive PCA algorithm,\nthat has led to superior performance for denoising CFA images, on the subject\nof PSNR and visual quality. Also the processing time has decreased\nsignificantly.\n", "versions": [{"version": "v1", "created": "Sun, 11 Dec 2011 18:57:10 GMT"}], "update_date": "2011-12-13", "authors_parsed": [["Pakdelazar'", "'Omid", ""], ["Rezai-rad'", "'Gholamali", ""]]}, {"id": "1112.2903", "submitter": "Shai Bagon", "authors": "Shai Bagon and Meirav Galun", "title": "Large Scale Correlation Clustering Optimization", "comments": "9 pages, 6 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Clustering is a fundamental task in unsupervised learning. The focus of this\npaper is the Correlation Clustering functional which combines positive and\nnegative affinities between the data points. The contribution of this paper is\ntwo fold: (i) Provide a theoretic analysis of the functional. (ii) New\noptimization algorithms which can cope with large scale problems (>100K\nvariables) that are infeasible using existing methods. Our theoretic analysis\nprovides a probabilistic generative interpretation for the functional, and\njustifies its intrinsic \"model-selection\" capability. Furthermore, we draw an\nanalogy between optimizing this functional and the well known Potts energy\nminimization. This analogy allows us to suggest several new optimization\nalgorithms, which exploit the intrinsic \"model-selection\" capability of the\nfunctional to automatically recover the underlying number of clusters. We\ncompare our algorithms to existing methods on both synthetic and real data. In\naddition we suggest two new applications that are made possible by our\nalgorithms: unsupervised face identification and interactive multi-object\nsegmentation by rough boundary delineation.\n", "versions": [{"version": "v1", "created": "Tue, 13 Dec 2011 14:28:12 GMT"}], "update_date": "2011-12-14", "authors_parsed": [["Bagon", "Shai", ""], ["Galun", "Meirav", ""]]}, {"id": "1112.2988", "submitter": "Tsvi Achler", "authors": "Tsvi Achler", "title": "Supervised Generative Reconstruction: An Efficient Way To Flexibly Store\n  and Recognize Patterns", "comments": "2 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Matching animal-like flexibility in recognition and the ability to quickly\nincorporate new information remains difficult. Limits are yet to be adequately\naddressed in neural models and recognition algorithms. This work proposes a\nconfiguration for recognition that maintains the same function of conventional\nalgorithms but avoids combinatorial problems. Feedforward recognition\nalgorithms such as classical artificial neural networks and machine learning\nalgorithms are known to be subject to catastrophic interference and forgetting.\nModifying or learning new information (associations between patterns and\nlabels) causes loss of previously learned information. I demonstrate using\nmathematical analysis how supervised generative models, with feedforward and\nfeedback connections, can emulate feedforward algorithms yet avoid catastrophic\ninterference and forgetting. Learned information in generative models is stored\nin a more intuitive form that represents the fixed points or solutions of the\nnetwork and moreover displays similar difficulties as cognitive phenomena.\nBrain-like capabilities and limits associated with generative models suggest\nthe brain may perform recognition and store information using a similar\napproach. Because of the central role of recognition, progress understanding\nthe underlying principles may reveal significant insight on how to better study\nand integrate with the brain.\n", "versions": [{"version": "v1", "created": "Tue, 13 Dec 2011 18:10:11 GMT"}, {"version": "v2", "created": "Sat, 23 Jun 2012 16:35:37 GMT"}], "update_date": "2012-06-26", "authors_parsed": [["Achler", "Tsvi", ""]]}, {"id": "1112.3010", "submitter": "Karthik Gurumoorthy", "authors": "Karthik S. Gurumoorthy and Anand Rangarajan", "title": "A new variational principle for the Euclidean distance function: Linear\n  approach to the non-linear eikonal problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a fast convolution-based technique for computing an approximate,\nsigned Euclidean distance function $S$ on a set of 2D and 3D grid locations.\nInstead of solving the non-linear, static Hamilton-Jacobi equation ($\\|\\nabla\nS\\|=1$), our solution stems from first solving for a scalar field $\\phi$ in a\nlinear differential equation and then deriving the solution for $S$ by taking\nthe negative logarithm. In other words, when $S$ and $\\phi$ are related by\n$\\phi = \\exp \\left(-\\frac{S}{\\tau} \\right)$ and $\\phi$ satisfies a specific\nlinear differential equation corresponding to the extremum of a variational\nproblem, we obtain the approximate Euclidean distance function $S = -\\tau\n\\log(\\phi)$ which converges to the true solution in the limit as $\\tau\n\\rightarrow 0$. This is in sharp contrast to techniques like the fast marching\nand fast sweeping methods which directly solve the Hamilton-Jacobi equation by\nthe Godunov upwind discretization scheme. Our linear formulation results in a\nclosed-form solution to the approximate Euclidean distance function expressible\nas a discrete convolution, and hence efficiently computable using the fast\nFourier transform (FFT). Our solution also circumvents the need for spatial\ndiscretization of the derivative operator. As $\\tau\\rightarrow0$ we show the\nconvergence of our results to the true solution and also bound the error for a\ngiven value of $\\tau$. The differentiability of our solution allows us to\ncompute---using a set of convolutions---the first and second derivatives of the\napproximate distance function. In order to determine the sign of the distance\nfunction (defined to be positive inside a closed region and negative outside),\nwe compute the winding number in 2D and the topological degree in 3D, whose\ncomputations can also be performed via fast convolutions. We demonstrate the\nefficacy of our method through a set of experimental results.\n", "versions": [{"version": "v1", "created": "Tue, 13 Dec 2011 20:03:33 GMT"}, {"version": "v2", "created": "Fri, 1 Nov 2013 06:26:59 GMT"}, {"version": "v3", "created": "Fri, 26 Sep 2014 13:03:03 GMT"}, {"version": "v4", "created": "Sun, 8 Feb 2015 10:06:36 GMT"}], "update_date": "2015-02-10", "authors_parsed": [["Gurumoorthy", "Karthik S.", ""], ["Rangarajan", "Anand", ""]]}, {"id": "1112.3059", "submitter": "Robert Hovden", "authors": "Paul Cueva, Robert Hovden, Julia A. Mundy, Huolin L. Xin, and David A.\n  Muller", "title": "Data Processing For Atomic Resolution EELS", "comments": null, "journal-ref": "Microscopy and Microanalysis, Vol. 18 pp 667-675 (2012)", "doi": "10.1017/S1431927612000244", "report-no": null, "categories": "cond-mat.mtrl-sci cs.CV physics.data-an", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  The high beam current and sub-angstrom resolution of aberration-corrected\nscanning transmission electron microscopes has enabled electron energy loss\nspectroscopic (EELS) mapping with atomic resolution. These spectral maps are\noften dose-limited and spatially oversampled, leading to low counts/channel and\nare thus highly sensitive to errors in background estimation. However, by\ntaking advantage of redundancy in the dataset map one can improve background\nestimation and increase chemical sensitivity. We consider two such approaches-\nlinear combination of power laws and local background averaging-that reduce\nbackground error and improve signal extraction. Principal components analysis\n(PCA) can also be used to analyze spectrum images, but the poor\npeak-to-background ratio in EELS can lead to serious artifacts if raw EELS data\nis PCA filtered. We identify common artifacts and discuss alternative\napproaches. These algorithms are implemented within the Cornell Spectrum\nImager, an open source software package for spectroscopic analysis.\n", "versions": [{"version": "v1", "created": "Tue, 13 Dec 2011 22:18:52 GMT"}], "update_date": "2014-11-20", "authors_parsed": [["Cueva", "Paul", ""], ["Hovden", "Robert", ""], ["Mundy", "Julia A.", ""], ["Xin", "Huolin L.", ""], ["Muller", "David A.", ""]]}, {"id": "1112.3110", "submitter": "Andrew Ensor", "authors": "Andrew Ensor, Seth Hall", "title": "GPU-based Image Analysis on Mobile Devices", "comments": "Proceedings of Image and Vision Computing New Zealand 2011", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the rapid advances in mobile technology many mobile devices are capable\nof capturing high quality images and video with their embedded camera. This\npaper investigates techniques for real-time processing of the resulting images,\nparticularly on-device utilizing a graphical processing unit. Issues and\nlimitations of image processing on mobile devices are discussed, and the\nperformance of graphical processing units on a range of devices measured\nthrough a programmable shader implementation of Canny edge detection.\n", "versions": [{"version": "v1", "created": "Wed, 14 Dec 2011 03:46:46 GMT"}], "update_date": "2011-12-15", "authors_parsed": [["Ensor", "Andrew", ""], ["Hall", "Seth", ""]]}, {"id": "1112.3166", "submitter": "Stefan Sommer", "authors": "Stefan Sommer, Mads Nielsen, Sune Darkner, Xavier Pennec", "title": "Higher-Order Momentum Distributions and Locally Affine LDDMM\n  Registration", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To achieve sparse parametrizations that allows intuitive analysis, we aim to\nrepresent deformation with a basis containing interpretable elements, and we\nwish to use elements that have the description capacity to represent the\ndeformation compactly. To accomplish this, we introduce in this paper\nhigher-order momentum distributions in the LDDMM registration framework. While\nthe zeroth order moments previously used in LDDMM only describe local\ndisplacement, the first-order momenta that are proposed here represent a basis\nthat allows local description of affine transformations and subsequent compact\ndescription of non-translational movement in a globally non-rigid deformation.\nThe resulting representation contains directly interpretable information from\nboth mathematical and modeling perspectives. We develop the mathematical\nconstruction of the registration framework with higher-order momenta, we show\nthe implications for sparse image registration and deformation description, and\nwe provide examples of how the parametrization enables registration with a very\nlow number of parameters. The capacity and interpretability of the\nparametrization using higher-order momenta lead to natural modeling of\narticulated movement, and the method promises to be useful for quantifying\nventricle expansion and progressing atrophy during Alzheimer's disease.\n", "versions": [{"version": "v1", "created": "Wed, 14 Dec 2011 11:10:08 GMT"}, {"version": "v2", "created": "Thu, 29 Nov 2012 09:12:24 GMT"}], "update_date": "2012-11-30", "authors_parsed": [["Sommer", "Stefan", ""], ["Nielsen", "Mads", ""], ["Darkner", "Sune", ""], ["Pennec", "Xavier", ""]]}, {"id": "1112.3173", "submitter": "Achim Tresch", "authors": "Ramin Norousi, Stephan Wickles, Thomas Becker, Roland Beckmann, Volker\n  J. Schmid, and Achim Tresch", "title": "Automatic post-picking improves particle image detection from Cryo-EM\n  micrographs", "comments": "14 pages, 5 figures", "journal-ref": "Journal of Structural Biology 2013. 182(2)", "doi": "10.1016/j.jsb.2013.02.008", "report-no": null, "categories": "cs.CV q-bio.BM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cryo-electron microscopy (cryo-EM) studies using single particle\nreconstruction is extensively used to reveal structural information of\nmacromolecular complexes. Aiming at the highest achievable resolution, state of\nthe art electron microscopes acquire thousands of high-quality images. Having\ncollected these data, each single particle must be detected and windowed out.\nSeveral fully- or semi-automated approaches have been developed for the\nselection of particle images from digitized micrographs. However they still\nrequire laborious manual post processing, which will become the major\nbottleneck for next generation of electron microscopes. Instead of focusing on\nimprovements in automated particle selection from micrographs, we propose a\npost-picking step for classifying small windowed images, which are output by\ncommon picking software. A supervised strategy for the classification of\nwindowed micrograph images into particles and non-particles reduces the manual\nworkload by orders of magnitude. The method builds on new powerful image\nfeatures, and the proper training of an ensemble classifier. A few hundred\ntraining samples are enough to achieve a human-like classification performance.\n", "versions": [{"version": "v1", "created": "Wed, 14 Dec 2011 11:38:34 GMT"}, {"version": "v2", "created": "Mon, 2 Jan 2012 12:39:20 GMT"}], "update_date": "2020-05-04", "authors_parsed": [["Norousi", "Ramin", ""], ["Wickles", "Stephan", ""], ["Becker", "Thomas", ""], ["Beckmann", "Roland", ""], ["Schmid", "Volker J.", ""], ["Tresch", "Achim", ""]]}, {"id": "1112.3697", "submitter": "Alexander Binder", "authors": "Alexander Binder and Shinichi Nakajima and Marius Kloft and Christina\n  M\\\"uller and Wojciech Samek and Ulf Brefeld and Klaus-Robert M\\\"uller and\n  Motoaki Kawanabe", "title": "Insights from Classifying Visual Concepts with Multiple Kernel Learning", "comments": "18 pages, 8 tables, 4 figures, format deviating from plos one\n  submission format requirements for aesthetic reasons", "journal-ref": "PLoS ONE 7(8): e38897, 2012", "doi": "10.1371/journal.pone.0038897", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Combining information from various image features has become a standard\ntechnique in concept recognition tasks. However, the optimal way of fusing the\nresulting kernel functions is usually unknown in practical applications.\nMultiple kernel learning (MKL) techniques allow to determine an optimal linear\ncombination of such similarity matrices. Classical approaches to MKL promote\nsparse mixtures. Unfortunately, so-called 1-norm MKL variants are often\nobserved to be outperformed by an unweighted sum kernel. The contribution of\nthis paper is twofold: We apply a recently developed non-sparse MKL variant to\nstate-of-the-art concept recognition tasks within computer vision. We provide\ninsights on benefits and limits of non-sparse MKL and compare it against its\ndirect competitors, the sum kernel SVM and the sparse MKL. We report empirical\nresults for the PASCAL VOC 2009 Classification and ImageCLEF2010 Photo\nAnnotation challenge data sets. About to be submitted to PLoS ONE.\n", "versions": [{"version": "v1", "created": "Fri, 16 Dec 2011 01:06:47 GMT"}], "update_date": "2012-11-26", "authors_parsed": [["Binder", "Alexander", ""], ["Nakajima", "Shinichi", ""], ["Kloft", "Marius", ""], ["M\u00fcller", "Christina", ""], ["Samek", "Wojciech", ""], ["Brefeld", "Ulf", ""], ["M\u00fcller", "Klaus-Robert", ""], ["Kawanabe", "Motoaki", ""]]}, {"id": "1112.3972", "submitter": "Serguei Mokhov", "authors": "Emil Vassev and Serguei A. Mokhov", "title": "Developing Autonomic Properties for Distributed Pattern-Recognition\n  Systems with ASSL: A Distributed MARF Case Study", "comments": "28 pages; 16 figures; Submitted and accepted in 2010; to appear in\n  \"E. Vassev and S. A. Mokhov. Development and evaluation of autonomic\n  properties for pattern-recognition systems with ASSL -- a distributed MARF\n  case study. Transactions on Computational Science, Special Issue on Advances\n  in Autonomic Computing: Formal Engineering Methods for Nature-Inspired\n  Computing Systems, XV (LNCS7050).\"", "journal-ref": "J. Trans. on Comput. Sci. XV, Springer-Verlag,130-157", "doi": "10.1007/978-3-642-28525-7_5", "report-no": null, "categories": "cs.DC cs.CV cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we discuss our research towards developing special properties\nthat introduce autonomic behavior in pattern-recognition systems. In our\napproach we use ASSL (Autonomic System Specification Language) to formally\ndevelop such properties for DMARF (Distributed Modular Audio Recognition\nFramework). These properties enhance DMARF with an autonomic middleware that\nmanages the four stages of the framework's pattern-recognition pipeline. DMARF\nis a biologically inspired system employing pattern recognition, signal\nprocessing, and natural language processing helping us process audio, textual,\nor imagery data needed by a variety of scientific applications, e.g., biometric\napplications. In that context, the notion go autonomic DMARF (ADMARF) can be\nemployed by autonomous and robotic systems that theoretically require\nless-to-none human intervention other than data collection for pattern analysis\nand observing the results. In this article, we explain the ASSL specification\nmodels for the autonomic properties of DMARF.\n", "versions": [{"version": "v1", "created": "Fri, 16 Dec 2011 21:12:17 GMT"}], "update_date": "2013-07-08", "authors_parsed": [["Vassev", "Emil", ""], ["Mokhov", "Serguei A.", ""]]}, {"id": "1112.4060", "submitter": "Bartlomiej Placzek", "authors": "Bart{\\l}omiej P{\\l}aczek", "title": "A real time vehicles detection algorithm for vision based sensors", "comments": "The final publication is available at http://www.springerlink.com", "journal-ref": "P{\\l}aczek B., A real time vehicles detection algorithm for vision\n  based sensors, Lecture Notes in Computer Science 6375, Springer-Verlag,\n  Berlin Heidelberg, 2010, pp. 211-218", "doi": "10.1007/978-3-642-15907-7_26", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A vehicle detection plays an important role in the traffic control at\nsignalised intersections. This paper introduces a vision-based algorithm for\nvehicles presence recognition in detection zones. The algorithm uses linguistic\nvariables to evaluate local attributes of an input image. The image attributes\nare categorised as vehicle, background or unknown features. Experimental\nresults on complex traffic scenes show that the proposed algorithm is effective\nfor a real-time vehicles detection.\n", "versions": [{"version": "v1", "created": "Sat, 17 Dec 2011 14:50:50 GMT"}], "update_date": "2011-12-20", "authors_parsed": [["P\u0142aczek", "Bart\u0142omiej", ""]]}, {"id": "1112.4064", "submitter": "Bartlomiej Placzek", "authors": "Bart{\\l}omiej P{\\l}aczek", "title": "Vehicles Recognition Using Fuzzy Descriptors of Image Segments", "comments": "The final publication is available at http://www.springerlink.com", "journal-ref": "P{\\l}aczek, B.: Vehicles Recognition Using Fuzzy Descriptors of\n  Image Segments. Computer Recognition Systems 3, Advances in Intelligent and\n  Soft Computing , vol. 57/2009, pp. 79--86. Springer-Verlag, Berlin Heidelberg\n  (2009)", "doi": "10.1007/978-3-540-93905-4_10", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper a vision-based vehicles recognition method is presented.\nProposed method uses fuzzy description of image segments for automatic\nrecognition of vehicles recorded in image data. The description takes into\naccount selected geometrical properties and shape coefficients determined for\nsegments of reference image (vehicle model). The proposed method was\nimplemented using reasoning system with fuzzy rules. A vehicles recognition\nalgorithm was developed based on the fuzzy rules describing shape and\narrangement of the image segments that correspond to visible parts of a\nvehicle. An extension of the algorithm with set of fuzzy rules defined for\ndifferent reference images (and various vehicle shapes) enables vehicles\nclassification in traffic scenes. The devised method is suitable for\napplication in video sensors for road traffic control and surveillance systems.\n", "versions": [{"version": "v1", "created": "Sat, 17 Dec 2011 15:21:22 GMT"}], "update_date": "2011-12-20", "authors_parsed": [["P\u0142aczek", "Bart\u0142omiej", ""]]}, {"id": "1112.4135", "submitter": "Hocine Cherifi", "authors": "Abdelkaher Ait Abdelouahad (GSCM-LRIT), Mohammed El Hassouni (DESTEC),\n  Hocine Cherifi (Le2i), Driss Aboutajdine (GSCM-LRIT)", "title": "A Reduced Reference Image Quality Measure Using Bessel K Forms Model for\n  Tetrolet Coefficients", "comments": null, "journal-ref": "Journal of Convergence Information Technology 6 (2011) 216, 224", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce a Reduced Reference Image Quality Assessment\n(RRIQA) measure based on the natural image statistic approach. A new adaptive\ntransform called \"Tetrolet\" is applied to both reference and distorted images.\nTo model the marginal distribution of tetrolet coefficients Bessel K Forms\n(BKF) density is proposed. Estimating the parameters of this distribution\nallows to summarize the reference image with a small amount of side\ninformation. Five distortion measures based on the BKF parameters of the\noriginal and processed image are used to predict quality scores. A comparison\nbetween these measures is presented showing a good consistency with human\njudgment.\n", "versions": [{"version": "v1", "created": "Sun, 18 Dec 2011 08:11:59 GMT"}], "update_date": "2011-12-20", "authors_parsed": [["Abdelouahad", "Abdelkaher Ait", "", "GSCM-LRIT"], ["Hassouni", "Mohammed El", "", "DESTEC"], ["Cherifi", "Hocine", "", "Le2i"], ["Aboutajdine", "Driss", "", "GSCM-LRIT"]]}, {"id": "1112.4164", "submitter": "Shervin Minaee", "authors": "Shervin Minaee, Mehran Fotouhi and Babak Hossein Khalaj", "title": "A Geometric Approach For Fully Automatic Chromosome Segmentation", "comments": "This paper has been revised", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A fundamental task in human chromosome analysis is chromosome segmentation.\nSegmentation plays an important role in chromosome karyotyping. The first step\nin segmentation is to remove intrusive objects such as stain debris and other\nnoises. The next step is detection of touching and overlapping chromosomes, and\nthe final step is separation of such chromosomes. Common methods for separation\nbetween touching chromosomes are interactive and require human intervention for\ncorrect separation between touching and overlapping chromosomes. In this paper,\na geometric-based method is used for automatic detection of touching and\noverlapping chromosomes and separating them. The proposed scheme performs\nsegmentation in two phases. In the first phase, chromosome clusters are\ndetected using three geometric criteria, and in the second phase, chromosome\nclusters are separated using a cut-line. Most of earlier methods did not work\nproperly in case of chromosome clusters that contained more than two\nchromosomes. Our method, on the other hand, is quite efficient in separation of\nsuch chromosome clusters. At each step, one separation will be performed and\nthis algorithm is repeated until all individual chromosomes are separated.\nAnother important point about the proposed method is that it uses the geometric\nfeatures of chromosomes which are independent of the type of images and it can\neasily be applied to any type of images such as binary images and does not\nrequire multispectral images as well. We have applied our method to a database\ncontaining 62 touching and partially overlapping chromosomes and a success rate\nof 91.9% is achieved.\n", "versions": [{"version": "v1", "created": "Sun, 18 Dec 2011 15:46:18 GMT"}, {"version": "v2", "created": "Tue, 22 May 2012 14:28:33 GMT"}, {"version": "v3", "created": "Sun, 11 May 2014 17:15:50 GMT"}, {"version": "v4", "created": "Wed, 20 Aug 2014 21:29:55 GMT"}, {"version": "v5", "created": "Mon, 1 Sep 2014 04:53:23 GMT"}], "update_date": "2014-09-02", "authors_parsed": [["Minaee", "Shervin", ""], ["Fotouhi", "Mehran", ""], ["Khalaj", "Babak Hossein", ""]]}, {"id": "1112.4434", "submitter": "Joseph  Salmon", "authors": "Ery Arias-Castro, Joseph Salmon, and Rebecca Willett", "title": "Oracle inequalities and minimax rates for non-local means and related\n  adaptive kernel-based methods", "comments": "49 pages, 15 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.CV cs.IT math.IT stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes a novel theoretical characterization of the performance\nof non-local means (NLM) for noise removal. NLM has proven effective in a\nvariety of empirical studies, but little is understood fundamentally about how\nit performs relative to classical methods based on wavelets or how various\nparameters (e.g., patch size) should be chosen. For cartoon images and images\nwhich may contain thin features and regular textures, the error decay rates of\nNLM are derived and compared with those of linear filtering, oracle estimators,\nvariable-bandwidth kernel methods, Yaroslavsky's filter and wavelet\nthresholding estimators. The trade-off between global and local search for\nmatching patches is examined, and the bias reduction associated with the local\npolynomial regression version of NLM is analyzed. The theoretical results are\nvalidated via simulations for 2D images corrupted by additive white Gaussian\nnoise.\n", "versions": [{"version": "v1", "created": "Mon, 19 Dec 2011 18:55:22 GMT"}, {"version": "v2", "created": "Thu, 26 Apr 2012 00:09:35 GMT"}], "update_date": "2012-04-27", "authors_parsed": [["Arias-Castro", "Ery", ""], ["Salmon", "Joseph", ""], ["Willett", "Rebecca", ""]]}, {"id": "1112.5298", "submitter": "Tomas Werner", "authors": "Tomas Werner", "title": "Zero-Temperature Limit of a Convergent Algorithm to Minimize the Bethe\n  Free Energy", "comments": "Research Report", "journal-ref": null, "doi": null, "report-no": "CTU--CMP--2011--14", "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  After the discovery that fixed points of loopy belief propagation coincide\nwith stationary points of the Bethe free energy, several researchers proposed\nprovably convergent algorithms to directly minimize the Bethe free energy.\nThese algorithms were formulated only for non-zero temperature (thus finding\nfixed points of the sum-product algorithm) and their possible extension to zero\ntemperature is not obvious. We present the zero-temperature limit of the\ndouble-loop algorithm by Heskes, which converges a max-product fixed point. The\ninner loop of this algorithm is max-sum diffusion. Under certain conditions,\nthe algorithm combines the complementary advantages of the max-product belief\npropagation and max-sum diffusion (LP relaxation): it yields good approximation\nof both ground states and max-marginals.\n", "versions": [{"version": "v1", "created": "Thu, 22 Dec 2011 13:10:05 GMT"}], "update_date": "2011-12-23", "authors_parsed": [["Werner", "Tomas", ""]]}, {"id": "1112.5638", "submitter": "Elif Vural", "authors": "Elif Vural, Pascal Frossard", "title": "Discretization of Parametrizable Signal Manifolds", "comments": null, "journal-ref": "IEEE Transactions on Image Processing, vol. 20, no. 12, Dec. 2011", "doi": "10.1109/TIP.2011.2155077", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transformation-invariant analysis of signals often requires the computation\nof the distance from a test pattern to a transformation manifold. In\nparticular, the estimation of the distances between a transformed query signal\nand several transformation manifolds representing different classes provides\nessential information for the classification of the signal. In many\napplications the computation of the exact distance to the manifold is costly,\nwhereas an efficient practical solution is the approximation of the manifold\ndistance with the aid of a manifold grid. In this paper, we consider a setting\nwith transformation manifolds of known parameterization. We first present an\nalgorithm for the selection of samples from a single manifold that permits to\nminimize the average error in the manifold distance estimation. Then we propose\na method for the joint discretization of multiple manifolds that represent\ndifferent signal classes, where we optimize the transformation-invariant\nclassification accuracy yielded by the discrete manifold representation.\nExperimental results show that sampling each manifold individually by\nminimizing the manifold distance estimation error outperforms baseline sampling\nsolutions with respect to registration and classification accuracy. Performing\nan additional joint optimization on all samples improves the classification\nperformance further. Moreover, given a fixed total number of samples to be\nselected from all manifolds, an asymmetric distribution of samples to different\nmanifolds depending on their geometric structures may also increase the\nclassification accuracy in comparison with the equal distribution of samples.\n", "versions": [{"version": "v1", "created": "Fri, 23 Dec 2011 19:08:10 GMT"}], "update_date": "2011-12-26", "authors_parsed": [["Vural", "Elif", ""], ["Frossard", "Pascal", ""]]}, {"id": "1112.5640", "submitter": "Elif Vural", "authors": "Elif Vural, Pascal Frossard", "title": "Learning Smooth Pattern Transformation Manifolds", "comments": null, "journal-ref": "IEEE Transactions on Image Processing, vol. 22, no. 4, pp.\n  1311-1325, 2013", "doi": "10.1109/TIP.2012.2227768", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Manifold models provide low-dimensional representations that are useful for\nprocessing and analyzing data in a transformation-invariant way. In this paper,\nwe study the problem of learning smooth pattern transformation manifolds from\nimage sets that represent observations of geometrically transformed signals. In\norder to construct a manifold, we build a representative pattern whose\ntransformations accurately fit various input images. We examine two objectives\nof the manifold building problem, namely, approximation and classification. For\nthe approximation problem, we propose a greedy method that constructs a\nrepresentative pattern by selecting analytic atoms from a continuous dictionary\nmanifold. We present a DC (Difference-of-Convex) optimization scheme that is\napplicable to a wide range of transformation and dictionary models, and\ndemonstrate its application to transformation manifolds generated by rotation,\ntranslation and anisotropic scaling of a reference pattern. Then, we generalize\nthis approach to a setting with multiple transformation manifolds, where each\nmanifold represents a different class of signals. We present an iterative\nmultiple manifold building algorithm such that the classification accuracy is\npromoted in the learning of the representative patterns. Experimental results\nsuggest that the proposed methods yield high accuracy in the approximation and\nclassification of data compared to some reference methods, while the invariance\nto geometric transformations is achieved due to the transformation manifold\nmodel.\n", "versions": [{"version": "v1", "created": "Fri, 23 Dec 2011 19:13:31 GMT"}, {"version": "v2", "created": "Wed, 16 May 2012 21:21:15 GMT"}, {"version": "v3", "created": "Fri, 3 Aug 2012 13:43:46 GMT"}, {"version": "v4", "created": "Tue, 16 Oct 2012 09:34:37 GMT"}, {"version": "v5", "created": "Tue, 30 Oct 2012 13:29:16 GMT"}], "update_date": "2013-05-20", "authors_parsed": [["Vural", "Elif", ""], ["Frossard", "Pascal", ""]]}, {"id": "1112.5771", "submitter": "Xu Zhiqiang", "authors": "Zuowei Shen, Zhiqiang Xu", "title": "On B-spline framelets derived from the unitary extension principle", "comments": "28 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.FA cs.CV cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spline wavelet tight frames of Ron-Shen have been used widely in frame based\nimage analysis and restorations. However, except for the tight frame property\nand the approximation order of the truncated series, there are few other\nproperties of this family of spline wavelet tight frames to be known. This\npaper is to present a few new properties of this family that will provide\nfurther understanding of it and, hopefully, give some indications why it is\nefficient in image analysis and restorations. In particular, we present a\nrecurrence formula of computing generators of higher order spline wavelet tight\nframes from the lower order ones. We also represent each generator of spline\nwavelet tight frames as certain order of derivative of some univariate box\nspline. With this, we further show that each generator of sufficiently high\norder spline wavelet tight frames is close to a right order of derivative of a\nproperly scaled Gaussian function. This leads to the result that the wavelet\nsystem generated by a finitely many consecutive derivatives of a properly\nscaled Gaussian function forms a frame whose frame bounds can be almost tight.\n", "versions": [{"version": "v1", "created": "Sun, 25 Dec 2011 07:07:18 GMT"}, {"version": "v2", "created": "Thu, 12 Jan 2012 07:00:44 GMT"}], "update_date": "2012-01-13", "authors_parsed": [["Shen", "Zuowei", ""], ["Xu", "Zhiqiang", ""]]}, {"id": "1112.5895", "submitter": "Guoshen Yu", "authors": "Julio Duarte-Carvajalino, Guillermo Sapiro, Guoshen Yu, Lawrence Carin", "title": "Online Adaptive Statistical Compressed Sensing of Gaussian Mixture\n  Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A framework of online adaptive statistical compressed sensing is introduced\nfor signals following a mixture model. The scheme first uses non-adaptive\nmeasurements, from which an online decoding scheme estimates the model\nselection. As soon as a candidate model has been selected, an optimal sensing\nscheme for the selected model continues to apply. The final signal\nreconstruction is calculated from the ensemble of both the non-adaptive and the\nadaptive measurements. For signals generated from a Gaussian mixture model, the\nonline adaptive sensing algorithm is given and its performance is analyzed. On\nboth synthetic and real image data, the proposed adaptive scheme considerably\nreduces the average reconstruction error with respect to standard statistical\ncompressed sensing that uses fully random measurements, at a marginally\nincreased computational complexity.\n", "versions": [{"version": "v1", "created": "Mon, 26 Dec 2011 21:42:22 GMT"}], "update_date": "2011-12-30", "authors_parsed": [["Duarte-Carvajalino", "Julio", ""], ["Sapiro", "Guillermo", ""], ["Yu", "Guoshen", ""], ["Carin", "Lawrence", ""]]}, {"id": "1112.5997", "submitter": "Shervin Minaee", "authors": "Sina Akbari Mistani, Shervin Minaee and Emad Fatemizadeh", "title": "Multispectral Palmprint Recognition Using a Hybrid Feature", "comments": "6 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Personal identification problem has been a major field of research in recent\nyears. Biometrics-based technologies that exploit fingerprints, iris, face,\nvoice and palmprints, have been in the center of attention to solve this\nproblem. Palmprints can be used instead of fingerprints that have been of the\nearliest of these biometrics technologies. A palm is covered with the same skin\nas the fingertips but has a larger surface, giving us more information than the\nfingertips. The major features of the palm are palm-lines, including principal\nlines, wrinkles and ridges. Using these lines is one of the most popular\napproaches towards solving the palmprint recognition problem. Another robust\nfeature is the wavelet energy of palms. In this paper we used a hybrid feature\nwhich combines both of these features. %Moreover, multispectral analysis is\napplied to improve the performance of the system. At the end, minimum distance\nclassifier is used to match test images with one of the training samples. The\nproposed algorithm has been tested on a well-known multispectral palmprint\ndataset and achieved an average accuracy of 98.8\\%.\n", "versions": [{"version": "v1", "created": "Tue, 27 Dec 2011 18:19:04 GMT"}, {"version": "v2", "created": "Fri, 25 Sep 2015 14:56:31 GMT"}, {"version": "v3", "created": "Fri, 11 Dec 2015 22:52:06 GMT"}], "update_date": "2015-12-15", "authors_parsed": [["Mistani", "Sina Akbari", ""], ["Minaee", "Shervin", ""], ["Fatemizadeh", "Emad", ""]]}, {"id": "1112.6269", "submitter": "Dhananjay Maktedar D", "authors": "Dhananjay D. M., C.V. Guru Rao, and I.V.Muralikrishna", "title": "Automated PolyU Palmprint sample Registration and Coarse Classification", "comments": "6 PAGES", "journal-ref": "IJCSI International Journal of Computer Science Issues, Vol. 8,\n  Issue 6, No 3, November 2011 ISSN (Online): 1694-0814 www.IJCSI.org", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Biometric based authentication for secured access to resources has gained\nimportance, due to their reliable, invariant and discriminating features.\nPalmprint is one such biometric entity. Prior to classification and\nidentification registering a sample palmprint is an important activity. In this\npaper we propose a computationally effective method for automated registration\nof samples from PlolyU palmprint database. In our approach we preprocess the\nsample and trace the border to find the nearest point from center of sample.\nAngle between vector representing the nearest point and vector passing through\nthe center is used for automated palm sample registration. The angle of\ninclination between start and end point of heart line and life line is used for\nbasic classification of palmprint samples in left class and right class.\n", "versions": [{"version": "v1", "created": "Thu, 29 Dec 2011 10:35:01 GMT"}], "update_date": "2011-12-30", "authors_parsed": [["M.", "Dhananjay D.", ""], ["Rao", "C. V. Guru", ""], ["Muralikrishna", "I. V.", ""]]}, {"id": "1112.6291", "submitter": "Jonathan Masci", "authors": "Jonathan Masci, Davide Migliore, Michael M. Bronstein, J\\\"urgen\n  Schmidhuber", "title": "Descriptor learning for omnidirectional image matching", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Feature matching in omnidirectional vision systems is a challenging problem,\nmainly because complicated optical systems make the theoretical modelling of\ninvariance and construction of invariant feature descriptors hard or even\nimpossible. In this paper, we propose learning invariant descriptors using a\ntraining set of similar and dissimilar descriptor pairs. We use the\nsimilarity-preserving hashing framework, in which we are trying to map the\ndescriptor data to the Hamming space preserving the descriptor similarity on\nthe training set. A neural network is used to solve the underlying optimization\nproblem. Our approach outperforms not only straightforward descriptor matching,\nbut also state-of-the-art similarity-preserving hashing methods.\n", "versions": [{"version": "v1", "created": "Thu, 29 Dec 2011 12:34:43 GMT"}], "update_date": "2011-12-30", "authors_parsed": [["Masci", "Jonathan", ""], ["Migliore", "Davide", ""], ["Bronstein", "Michael M.", ""], ["Schmidhuber", "J\u00fcrgen", ""]]}, {"id": "1112.6371", "submitter": "Odemir Bruno PhD", "authors": "Ricardo Fabbri, Wesley N. Gon\\c{c}alves, Francisco J. P. Lopes, Odemir\n  M. Bruno", "title": "Multi-q Analysis of Image Patterns", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.data-an cs.AI cs.CV physics.comp-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies the use of the Tsallis Entropy versus the classic\nBoltzmann-Gibbs-Shannon entropy for classifying image patterns. Given a\ndatabase of 40 pattern classes, the goal is to determine the class of a given\nimage sample. Our experiments show that the Tsallis entropy encoded in a\nfeature vector for different $q$ indices has great advantage over the\nBoltzmann-Gibbs-Shannon entropy for pattern classification, boosting\nrecognition rates by a factor of 3. We discuss the reasons behind this success,\nshedding light on the usefulness of the Tsallis entropy.\n", "versions": [{"version": "v1", "created": "Thu, 29 Dec 2011 18:38:34 GMT"}], "update_date": "2011-12-30", "authors_parsed": [["Fabbri", "Ricardo", ""], ["Gon\u00e7alves", "Wesley N.", ""], ["Lopes", "Francisco J. P.", ""], ["Bruno", "Odemir M.", ""]]}]