[{"id": "1411.0022", "submitter": "Varun Panaganti", "authors": "Varun Panaganti", "title": "Generalized Adaptive Dictionary Learning via Domain Shift Minimization", "comments": "6 pages, 2 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual data driven dictionaries have been successfully employed for various\nobject recognition and classification tasks. However, the task becomes more\nchallenging if the training and test data are from contrasting domains. In this\npaper, we propose a novel and generalized approach towards learning an adaptive\nand common dictionary for multiple domains. Precisely, we project the data from\ndifferent domains onto a low dimensional space while preserving the intrinsic\nstructure of data from each domain. We also minimize the domain-shift among the\ndata from each pair of domains. Simultaneously, we learn a common adaptive\ndictionary. Our algorithm can also be modified to learn class-specific\ndictionaries which can be used for classification. We additionally propose a\ndiscriminative manifold regularization which imposes the intrinsic structure of\nclass specific features onto the sparse coefficients. Experiments on image\nclassification show that our approach fares better compared to the existing\nstate-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Fri, 31 Oct 2014 20:42:36 GMT"}], "update_date": "2014-11-04", "authors_parsed": [["Panaganti", "Varun", ""]]}, {"id": "1411.0085", "submitter": "Atul Kanaujia", "authors": "Atul Kanaujia, Tae Eun Choe, Hongli Deng", "title": "Complex Events Recognition under Uncertainty in a Sensor Network", "comments": "13 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automated extraction of semantic information from a network of sensors for\ncognitive analysis and human-like reasoning is a desired capability in future\nground surveillance systems. We tackle the problem of complex decision making\nunder uncertainty in network information environment, where lack of effective\nvisual processing tools, incomplete domain knowledge frequently cause\nuncertainty in the visual primitives, leading to sub-optimal decisions. While\nstate-of-the-art vision techniques exist in detecting visual entities (humans,\nvehicles and scene elements) in an image, a missing functionality is the\nability to merge the information to reveal meaningful information for high\nlevel inference. In this work, we develop a probabilistic first order predicate\nlogic(FOPL) based reasoning system for recognizing complex events in\nsynchronized stream of videos, acquired from sensors with non-overlapping\nfields of view. We adopt Markov Logic Network(MLN) as a tool to model\nuncertainty in observations, and fuse information extracted from heterogeneous\ndata in a probabilistically consistent way. MLN overcomes strong dependence on\npure empirical learning by incorporating domain knowledge, in the form of\nuser-defined rules and confidences associated with them. This work demonstrates\nthat the MLN based decision control system can be made scalable to model\nstatistical relations between a variety of entities and over long video\nsequences. Experiments with real-world data, under a variety of settings,\nillustrate the mathematical soundness and wide-ranging applicability of our\napproach.\n", "versions": [{"version": "v1", "created": "Sat, 1 Nov 2014 09:04:45 GMT"}], "update_date": "2014-11-04", "authors_parsed": [["Kanaujia", "Atul", ""], ["Choe", "Tae Eun", ""], ["Deng", "Hongli", ""]]}, {"id": "1411.0126", "submitter": "GowthamRangarajan Raman", "authors": "Gowtham Rangarajan Raman", "title": "Detection of texts in natural images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  A framework that makes use of Connected components and supervised Support\nmachine to recognise texts is proposed. The image is preprocessed and and edge\ngraph is calculated using a probabilistic framework to compensate for\nphotometric noise. Connected components over the resultant image is calculated,\nwhich is bounded and then pruned using geometric constraints. Finally a Gabor\nFeature based SVM is used to classify the presence of text in the candidates.\nThe proposed method was tested with ICDAR 10 dataset and few other images\navailable on the internet. It resulted in a recall and precision metric of 0.72\nand 0.88 comfortably better than the benchmark Eiphstein's algorithm. The\nproposed method recorded a 0.70 and 0.74 in natural images which is\nsignificantly better than current methods on natural images. The proposed\nmethod also scales almost linearly for high resolution, cluttered images.\n", "versions": [{"version": "v1", "created": "Sat, 1 Nov 2014 15:06:23 GMT"}], "update_date": "2014-11-04", "authors_parsed": [["Raman", "Gowtham Rangarajan", ""]]}, {"id": "1411.0130", "submitter": "Balint Antal", "authors": "Balint Antal, Andras Hajdu, Zsuzsanna Maros-Szabo, Zsolt Torok,\n  Adrienne Csutak, Tunde Peto", "title": "A Two-phase Decision Support Framework for the Automatic Screening of\n  Digital Fundus Images", "comments": null, "journal-ref": "Journal of Computational Science, Elsevier, Volume 3, Issue 5,\n  September 2012, Pages 262-268", "doi": "10.1016/j.jocs.2012.01.001", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we give a brief review on the present status of automated\ndetection systems describe for the screening of diabetic retinopathy. We\nfurther detail an enhanced detection procedure that consists of two steps.\nFirst, a pre-screening algorithm is considered to classify the input digital\nfundus images based on the severity of abnormalities. If an image is found to\nbe seriously abnormal, it will not be analysed further with robust lesion\ndetector algorithms. As a further improvement, we introduce a novel feature\nextraction approach based on clinical observations. The second step of the\nproposed method detects regions of interest with possible lesions on the images\nthat previously passed the pre-screening step. These regions will serve as\ninput to the specific lesion detectors for detailed analysis. This procedure\ncan increase the computational performance of a screening system. Experimental\nresults show that both two steps of the proposed approach are capable to\nefficiently exclude a large amount of data from further processing, thus, to\ndecrease the computational burden of the automatic screening system.\n", "versions": [{"version": "v1", "created": "Sat, 1 Nov 2014 15:52:29 GMT"}], "update_date": "2014-11-04", "authors_parsed": [["Antal", "Balint", ""], ["Hajdu", "Andras", ""], ["Maros-Szabo", "Zsuzsanna", ""], ["Torok", "Zsolt", ""], ["Csutak", "Adrienne", ""], ["Peto", "Tunde", ""]]}, {"id": "1411.0161", "submitter": "Paul Honeine", "authors": "Paul Honeine", "title": "Entropy of Overcomplete Kernel Dictionaries", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.CV cs.LG cs.NE math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In signal analysis and synthesis, linear approximation theory considers a\nlinear decomposition of any given signal in a set of atoms, collected into a\nso-called dictionary. Relevant sparse representations are obtained by relaxing\nthe orthogonality condition of the atoms, yielding overcomplete dictionaries\nwith an extended number of atoms. More generally than the linear decomposition,\novercomplete kernel dictionaries provide an elegant nonlinear extension by\ndefining the atoms through a mapping kernel function (e.g., the gaussian\nkernel). Models based on such kernel dictionaries are used in neural networks,\ngaussian processes and online learning with kernels.\n  The quality of an overcomplete dictionary is evaluated with a diversity\nmeasure the distance, the approximation, the coherence and the Babel measures.\nIn this paper, we develop a framework to examine overcomplete kernel\ndictionaries with the entropy from information theory. Indeed, a higher value\nof the entropy is associated to a further uniform spread of the atoms over the\nspace. For each of the aforementioned diversity measures, we derive lower\nbounds on the entropy. Several definitions of the entropy are examined, with an\nextensive analysis in both the input space and the mapped feature space.\n", "versions": [{"version": "v1", "created": "Sat, 1 Nov 2014 19:41:14 GMT"}], "update_date": "2014-11-04", "authors_parsed": [["Honeine", "Paul", ""]]}, {"id": "1411.0296", "submitter": "Aasa Feragen", "authors": "Aasa Feragen, Francois Lauze, S{\\o}ren Hauberg", "title": "Geodesic Exponential Kernels: When Curvature and Linearity Conflict", "comments": "13 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider kernel methods on general geodesic metric spaces and provide both\nnegative and positive results. First we show that the common Gaussian kernel\ncan only be generalized to a positive definite kernel on a geodesic metric\nspace if the space is flat. As a result, for data on a Riemannian manifold, the\ngeodesic Gaussian kernel is only positive definite if the Riemannian manifold\nis Euclidean. This implies that any attempt to design geodesic Gaussian kernels\non curved Riemannian manifolds is futile. However, we show that for spaces with\nconditionally negative definite distances the geodesic Laplacian kernel can be\ngeneralized while retaining positive definiteness. This implies that geodesic\nLaplacian kernels can be generalized to some curved spaces, including spheres\nand hyperbolic spaces. Our theoretical results are verified empirically.\n", "versions": [{"version": "v1", "created": "Sun, 2 Nov 2014 19:08:14 GMT"}, {"version": "v2", "created": "Mon, 17 Nov 2014 06:22:19 GMT"}], "update_date": "2014-11-18", "authors_parsed": [["Feragen", "Aasa", ""], ["Lauze", "Francois", ""], ["Hauberg", "S\u00f8ren", ""]]}, {"id": "1411.0326", "submitter": "Corneliu Florea", "authors": "Corneliu Florea and Constantin Vertan and Laura Florea", "title": "High Dynamic Range Imaging by Perceptual Logarithmic Exposure Merging", "comments": "14 pages 8 figures. Accepted at AMCS journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we emphasize a similarity between the Logarithmic-Type Image\nProcessing (LTIP) model and the Naka-Rushton model of the Human Visual System\n(HVS). LTIP is a derivation of the Logarithmic Image Processing (LIP), which\nfurther replaces the logarithmic function with a ratio of polynomial functions.\nBased on this similarity, we show that it is possible to present an unifying\nframework for the High Dynamic Range (HDR) imaging problem, namely that\nperforming exposure merging under the LTIP model is equivalent to standard\nirradiance map fusion. The resulting HDR algorithm is shown to provide high\nquality in both subjective and objective evaluations.\n", "versions": [{"version": "v1", "created": "Sun, 2 Nov 2014 21:45:41 GMT"}, {"version": "v2", "created": "Wed, 3 Jun 2015 11:51:09 GMT"}], "update_date": "2015-06-04", "authors_parsed": [["Florea", "Corneliu", ""], ["Vertan", "Constantin", ""], ["Florea", "Laura", ""]]}, {"id": "1411.0392", "submitter": "Roozbeh Rajabi", "authors": "Roozbeh Rajabi, Hassan Ghassemian", "title": "Sparsity Constrained Graph Regularized NMF for Spectral Unmixing of\n  Hyperspectral Data", "comments": "10 pages, Journal", "journal-ref": null, "doi": "10.1007/s12524-014-0408-2", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hyperspectral images contain mixed pixels due to low spatial resolution of\nhyperspectral sensors. Mixed pixels are pixels containing more than one\ndistinct material called endmembers. The presence percentages of endmembers in\nmixed pixels are called abundance fractions. Spectral unmixing problem refers\nto decomposing these pixels into a set of endmembers and abundance fractions.\nDue to nonnegativity constraint on abundance fractions, nonnegative matrix\nfactorization methods (NMF) have been widely used for solving spectral unmixing\nproblem. In this paper we have used graph regularized NMF (GNMF) method\ncombined with sparseness constraint to decompose mixed pixels in hyperspectral\nimagery. This method preserves the geometrical structure of data while\nrepresenting it in low dimensional space. Adaptive regularization parameter\nbased on temperature schedule in simulated annealing method also has been used\nin this paper for the sparseness term. Proposed algorithm is applied on\nsynthetic and real datasets. Synthetic data is generated based on endmembers\nfrom USGS spectral library. AVIRIS Cuprite dataset is used as real dataset for\nevaluation of proposed method. Results are quantified based on spectral angle\ndistance (SAD) and abundance angle distance (AAD) measures. Results in\ncomparison with other methods show that the proposed method can unmix data more\neffectively. Specifically for the Cuprite dataset, performance of the proposed\nmethod is approximately 10% better than the VCA and Sparse NMF in terms of root\nmean square of SAD.\n", "versions": [{"version": "v1", "created": "Mon, 3 Nov 2014 08:41:32 GMT"}], "update_date": "2014-11-04", "authors_parsed": [["Rajabi", "Roozbeh", ""], ["Ghassemian", "Hassan", ""]]}, {"id": "1411.0442", "submitter": "Sharmila Kumari M PROFESSOR", "authors": "Abdullah Gubbi, Mohammad Fazle Azeem, M Sharmila Kumari", "title": "Non Binary Local Gradient Contours for Face Recognition", "comments": "12 pages", "journal-ref": "International Journal of Information Processing, 8(3), 63-74, 2014", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As the features from the traditional Local Binary Patterns (LBP) and Local\nDirectional Patterns (LDP) are found to be ineffective for face recognition, we\nhave proposed a new approach derived on the basis of Information sets whereby\nthe loss of information that occurs during the binarization is eliminated. The\ninformation sets expand the scope of fuzzy sets by connecting the attribute and\nthe corresponding membership function value as a product. Since face is having\nsmooth texture in a limited area, the extracted features must be highly\ndiscernible. To limit the number of features, we consider only the non\noverlapping windows. By the application of the information set theory we can\nreduce the number of feature of an image. The derived features are shown to\nwork fairly well over eigenface, fisherface and LBP methods.\n", "versions": [{"version": "v1", "created": "Mon, 3 Nov 2014 11:52:53 GMT"}], "update_date": "2014-11-04", "authors_parsed": [["Gubbi", "Abdullah", ""], ["Azeem", "Mohammad Fazle", ""], ["Kumari", "M Sharmila", ""]]}, {"id": "1411.0582", "submitter": "Giuseppe Boccignone", "authors": "Jonathan Vitale, Mary-Anne Williams, Benjamin Johnston, Giuseppe\n  Boccignone", "title": "Affective Facial Expression Processing via Simulation: A Probabilistic\n  Model", "comments": "Annual International Conference on Biologically Inspired Cognitive\n  Architectures - BICA 2014", "journal-ref": "Biologically Inspired Cognitive Architectures, Volume 10, October\n  2014, Pages 30-41", "doi": "10.1016/j.bica.2014.11.005", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding the mental state of other people is an important skill for\nintelligent agents and robots to operate within social environments. However,\nthe mental processes involved in `mind-reading' are complex. One explanation of\nsuch processes is Simulation Theory - it is supported by a large body of\nneuropsychological research. Yet, determining the best computational model or\ntheory to use in simulation-style emotion detection, is far from being\nunderstood.\n  In this work, we use Simulation Theory and neuroscience findings on\nMirror-Neuron Systems as the basis for a novel computational model, as a way to\nhandle affective facial expressions. The model is based on a probabilistic\nmapping of observations from multiple identities onto a single fixed identity\n(`internal transcoding of external stimuli'), and then onto a latent space\n(`phenomenological response'). Together with the proposed architecture we\npresent some promising preliminary results\n", "versions": [{"version": "v1", "created": "Mon, 3 Nov 2014 17:50:49 GMT"}], "update_date": "2019-11-05", "authors_parsed": [["Vitale", "Jonathan", ""], ["Williams", "Mary-Anne", ""], ["Johnston", "Benjamin", ""], ["Boccignone", "Giuseppe", ""]]}, {"id": "1411.0740", "submitter": "Ahmadreza Baghaie", "authors": "Ahmadreza Baghaie, Roshan M. D'souza, Zeyun Yu", "title": "State-of-the-Art in Retinal Optical Coherence Tomography Image Analysis", "comments": "Added references, corrected typos", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Optical Coherence Tomography (OCT) is one of the most emerging imaging\nmodalities that has been used widely in the field of biomedical imaging. From\nits emergence in 1990's, plenty of hardware and software improvements have been\nmade. Its applications range from ophthalmology to dermatology to coronary\nimaging etc. Here, the focus is on applications of OCT in ophthalmology and\nretinal imaging. OCT is able to non-invasively produce cross-sectional volume\nimages of the tissues which are further used for analysis of the tissue\nstructure and its properties. Due to the underlying physics, OCT images usually\nsuffer from a granular pattern, called speckle noise, which restricts the\nprocess of interpretation, hence requiring specialized noise reduction\ntechniques to remove the noise while preserving image details. Also, given the\nfact that OCT images are in the $\\mu m$ -level, further analysis in needed to\ndistinguish between the different structures in the imaged volume. Therefore\nthe use of different segmentation techniques are of high importance. The\nmovement of the tissue under imaging or the progression of disease in the\ntissue also imposes further implications both on the quality and the proper\ninterpretation of the acquired images. Thus, use of image registration\ntechniques can be very helpful. In this work, an overview of such image\nanalysis techniques will be given.\n", "versions": [{"version": "v1", "created": "Tue, 4 Nov 2014 00:10:57 GMT"}, {"version": "v2", "created": "Mon, 17 Nov 2014 20:36:38 GMT"}], "update_date": "2014-11-18", "authors_parsed": [["Baghaie", "Ahmadreza", ""], ["D'souza", "Roshan M.", ""], ["Yu", "Zeyun", ""]]}, {"id": "1411.0763", "submitter": "Xu Yang Dr.", "authors": "Xu Yang, Hong Qiao, and Zhi-Yong Liu", "title": "A Weighted Common Subgraph Matching Algorithm", "comments": "6 pages, 5 figures, the second round revision in IEEE TNNLS", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a weighted common subgraph (WCS) matching algorithm to find the\nmost similar subgraphs in two labeled weighted graphs. WCS matching, as a\nnatural generalization of the equal-sized graph matching or subgraph matching,\nfinds wide applications in many computer vision and machine learning tasks. In\nthis paper, the WCS matching is first formulated as a combinatorial\noptimization problem over the set of partial permutation matrices. Then it is\napproximately solved by a recently proposed combinatorial optimization\nframework - Graduated NonConvexity and Concavity Procedure (GNCCP).\nExperimental comparisons on both synthetic graphs and real world images\nvalidate its robustness against noise level, problem size, outlier number, and\nedge density.\n", "versions": [{"version": "v1", "created": "Tue, 4 Nov 2014 02:39:46 GMT"}], "update_date": "2014-11-05", "authors_parsed": [["Yang", "Xu", ""], ["Qiao", "Hong", ""], ["Liu", "Zhi-Yong", ""]]}, {"id": "1411.0791", "submitter": "Xiao Liu", "authors": "Xiao Liu, Congying Han, Tiande Guo", "title": "A Robust Point Sets Matching Method", "comments": "9 pages, 3 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Point sets matching method is very important in computer vision, feature\nextraction, fingerprint matching, motion estimation and so on. This paper\nproposes a robust point sets matching method. We present an iterative algorithm\nthat is robust to noise case. Firstly, we calculate all transformations between\ntwo points. Then similarity matrix are computed to measure the possibility that\ntwo transformation are both true. We iteratively update the matching score\nmatrix by using the similarity matrix. By using matching algorithm on graph, we\nobtain the matching result. Experimental results obtained by our approach show\nrobustness to outlier and jitter.\n", "versions": [{"version": "v1", "created": "Tue, 4 Nov 2014 05:53:45 GMT"}], "update_date": "2014-11-05", "authors_parsed": [["Liu", "Xiao", ""], ["Han", "Congying", ""], ["Guo", "Tiande", ""]]}, {"id": "1411.0802", "submitter": "Lu Ma", "authors": "Lu Ma, Mahsa Ghafarianzadeh, Dave Coleman, Nikolaus Correll, and Gabe\n  Sibley", "title": "Simultaneous Localization, Mapping, and Manipulation for Unsupervised\n  Object Discovery", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an unsupervised framework for simultaneous appearance-based object\ndiscovery, detection, tracking and reconstruction using RGBD cameras and a\nrobot manipulator. The system performs dense 3D simultaneous localization and\nmapping concurrently with unsupervised object discovery. Putative objects that\nare spatially and visually coherent are manipulated by the robot to gain\nadditional motion-cues. The robot uses appearance alone, followed by structure\nand motion cues, to jointly discover, verify, learn and improve models of\nobjects. Induced motion segmentation reinforces learned models which are\nrepresented implicitly as 2D and 3D level sets to capture both shape and\nappearance. We compare three different approaches for appearance-based object\ndiscovery and find that a novel form of spatio-temporal super-pixels gives the\nhighest quality candidate object models in terms of precision and recall. Live\nexperiments with a Baxter robot demonstrate a holistic pipeline capable of\nautomatic discovery, verification, detection, tracking and reconstruction of\nunknown objects.\n", "versions": [{"version": "v1", "created": "Tue, 4 Nov 2014 06:46:30 GMT"}], "update_date": "2014-11-05", "authors_parsed": [["Ma", "Lu", ""], ["Ghafarianzadeh", "Mahsa", ""], ["Coleman", "Dave", ""], ["Correll", "Nikolaus", ""], ["Sibley", "Gabe", ""]]}, {"id": "1411.0814", "submitter": "Yiguang Liu", "authors": "Yiguang Liu", "title": "A random algorithm for low-rank decomposition of large-scale matrices\n  with missing entries", "comments": null, "journal-ref": null, "doi": "10.1109/TIP.2015.2458176", "report-no": null, "categories": "cs.NA cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A Random SubMatrix method (RSM) is proposed to calculate the low-rank\ndecomposition of large-scale matrices with known entry percentage \\rho. RSM is\nvery fast as the floating-point operations (flops) required are compared\nfavorably with the state-of-the-art algorithms. Meanwhile RSM is very\nmemory-saving. With known entries homogeneously distributed in the given\nmatrix, sub-matrices formed by known entries are randomly selected. According\nto the just proved theorem that subspace related to smaller singular values is\nless perturbed by noise, the null vectors or the right singular vectors\nassociated with the minor singular values are calculated for each submatrix.\nThe vectors are the null vectors of the corresponding submatrix in the ground\ntruth of the given large-scale matrix. If enough sub-matrices are randomly\nchosen, the low-rank decomposition is estimated. The experimental results on\nrandom synthetical matrices with sizes such as 131072X1024 and on real data\nsets indicate that RSM is much faster and memory-saving, and, meanwhile, has\nconsiderable high precision achieving or approximating to the best.\n", "versions": [{"version": "v1", "created": "Tue, 4 Nov 2014 07:43:15 GMT"}], "update_date": "2015-10-28", "authors_parsed": [["Liu", "Yiguang", ""]]}, {"id": "1411.1045", "submitter": "Matthias K\\\"ummerer", "authors": "Matthias K\\\"ummerer and Lucas Theis and Matthias Bethge", "title": "Deep Gaze I: Boosting Saliency Prediction with Feature Maps Trained on\n  ImageNet", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV q-bio.NC stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent results suggest that state-of-the-art saliency models perform far from\noptimal in predicting fixations. This lack in performance has been attributed\nto an inability to model the influence of high-level image features such as\nobjects. Recent seminal advances in applying deep neural networks to tasks like\nobject recognition suggests that they are able to capture this kind of\nstructure. However, the enormous amount of training data necessary to train\nthese networks makes them difficult to apply directly to saliency prediction.\nWe present a novel way of reusing existing neural networks that have been\npretrained on the task of object recognition in models of fixation prediction.\nUsing the well-known network of Krizhevsky et al. (2012), we come up with a new\nsaliency model that significantly outperforms all state-of-the-art models on\nthe MIT Saliency Benchmark. We show that the structure of this network allows\nnew insights in the psychophysics of fixation selection and potentially their\nneural implementation. To train our network, we build on recent work on the\nmodeling of saliency as point processes.\n", "versions": [{"version": "v1", "created": "Tue, 4 Nov 2014 20:56:51 GMT"}, {"version": "v2", "created": "Thu, 18 Dec 2014 17:22:49 GMT"}, {"version": "v3", "created": "Thu, 26 Feb 2015 19:05:11 GMT"}, {"version": "v4", "created": "Thu, 9 Apr 2015 09:48:11 GMT"}], "update_date": "2015-04-10", "authors_parsed": [["K\u00fcmmerer", "Matthias", ""], ["Theis", "Lucas", ""], ["Bethge", "Matthias", ""]]}, {"id": "1411.1091", "submitter": "Jonathan Long", "authors": "Jonathan Long, Ning Zhang, Trevor Darrell", "title": "Do Convnets Learn Correspondence?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional neural nets (convnets) trained from massive labeled datasets\nhave substantially improved the state-of-the-art in image classification and\nobject detection. However, visual understanding requires establishing\ncorrespondence on a finer level than object category. Given their large pooling\nregions and training from whole-image labels, it is not clear that convnets\nderive their success from an accurate correspondence model which could be used\nfor precise localization. In this paper, we study the effectiveness of convnet\nactivation features for tasks requiring correspondence. We present evidence\nthat convnet features localize at a much finer scale than their receptive field\nsizes, that they can be used to perform intraclass alignment as well as\nconventional hand-engineered features, and that they outperform conventional\nfeatures in keypoint prediction on objects from PASCAL VOC 2011.\n", "versions": [{"version": "v1", "created": "Tue, 4 Nov 2014 21:35:55 GMT"}], "update_date": "2015-03-10", "authors_parsed": [["Long", "Jonathan", ""], ["Zhang", "Ning", ""], ["Darrell", "Trevor", ""]]}, {"id": "1411.1171", "submitter": "Rui  Zeng", "authors": "Rui Zeng, Jiasong Wu, Zhuhong Shao, Lotfi Senhadji, and Huazhong Shu", "title": "Multilinear Principal Component Analysis Network for Tensor Object\n  Classification", "comments": "4 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The recently proposed principal component analysis network (PCANet) has been\nproved high performance for visual content classification. In this letter, we\ndevelop a tensorial extension of PCANet, namely, multilinear principal analysis\ncomponent network (MPCANet), for tensor object classification. Compared to\nPCANet, the proposed MPCANet uses the spatial structure and the relationship\nbetween each dimension of tensor objects much more efficiently. Experiments\nwere conducted on different visual content datasets including UCF sports action\nvideo sequences database and UCF11 database. The experimental results have\nrevealed that the proposed MPCANet achieves higher classification accuracy than\nPCANet for tensor object classification.\n", "versions": [{"version": "v1", "created": "Wed, 5 Nov 2014 07:27:08 GMT"}], "update_date": "2014-11-06", "authors_parsed": [["Zeng", "Rui", ""], ["Wu", "Jiasong", ""], ["Shao", "Zhuhong", ""], ["Senhadji", "Lotfi", ""], ["Shu", "Huazhong", ""]]}, {"id": "1411.1172", "submitter": "Rui  Zeng", "authors": "Rui Zeng, Jiasong Wu, Lotfi Senhadji, Huazhong Shu", "title": "Tensor object classification via multilinear discriminant analysis\n  network", "comments": "5 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a multilinear discriminant analysis network (MLDANet) for\nthe recognition of multidimensional objects, known as tensor objects. The\nMLDANet is a variation of linear discriminant analysis network (LDANet) and\nprincipal component analysis network (PCANet), both of which are the recently\nproposed deep learning algorithms. The MLDANet consists of three parts: 1) The\nencoder learned by MLDA from tensor data. 2) Features maps ob-tained from\ndecoder. 3) The use of binary hashing and histogram for feature pooling. A\nlearning algorithm for MLDANet is described. Evaluations on UCF11 database\nindicate that the proposed MLDANet outperforms the PCANet, LDANet, MPCA + LDA,\nand MLDA in terms of classification for tensor objects.\n", "versions": [{"version": "v1", "created": "Wed, 5 Nov 2014 07:36:04 GMT"}], "update_date": "2014-11-06", "authors_parsed": [["Zeng", "Rui", ""], ["Wu", "Jiasong", ""], ["Senhadji", "Lotfi", ""], ["Shu", "Huazhong", ""]]}, {"id": "1411.1297", "submitter": "Osvaldo Pereira", "authors": "Osvaldo Pereira, Esley Torre, Yasel Garc\\'es and Roberto Rodr\\'iguez", "title": "Edge Detection based on Kernel Density Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Edges of an image are considered a crucial type of information. These can be\nextracted by applying edge detectors with different methodology. Edge detection\nis a vital step in computer vision tasks, because it is an essential issue for\npattern recognition and visual interpretation. In this paper, we propose a new\nmethod for edge detection in images, based on the estimation by kernel of the\nprobability density function. In our algorithm, pixels in the image with\nminimum value of density function are labeled as edges. The boundary between\ntwo homogeneous regions is defined in two domains: the spatial/lattice domain\nand the range/color domain. Extensive experimental evaluations proved that our\nedge detection method is significantly a competitive algorithm.\n", "versions": [{"version": "v1", "created": "Wed, 5 Nov 2014 15:38:11 GMT"}], "update_date": "2016-08-10", "authors_parsed": [["Pereira", "Osvaldo", ""], ["Torre", "Esley", ""], ["Garc\u00e9s", "Yasel", ""], ["Rodr\u00edguez", "Roberto", ""]]}, {"id": "1411.1372", "submitter": "Nima Keivan", "authors": "Nima Keivan and Gabe Sibley", "title": "Online SLAM with Any-time Self-calibration and Automatic Change\n  Detection", "comments": "8 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A framework for online simultaneous localization, mapping and\nself-calibration is presented which can detect and handle significant change in\nthe calibration parameters. Estimates are computed in constant-time by\nfactoring the problem and focusing on segments of the trajectory that are most\ninformative for the purposes of calibration. A novel technique is presented to\ndetect the probability that a significant change is present in the calibration\nparameters. The system is then able to re-calibrate. Maximum likelihood\ntrajectory and map estimates are computed using an asynchronous and adaptive\noptimization. The system requires no prior information and is able to\ninitialize without any special motions or routines, or in the case where\nobservability over calibration parameters is delayed. The system is\nexperimentally validated to calibrate camera intrinsic parameters for a\nnonlinear camera model on a monocular dataset featuring a significant zoom\nevent partway through, and achieves high accuracy despite unknown initial\ncalibration parameters. Self-calibration and re-calibration parameters are\nshown to closely match estimates computed using a calibration target. The\naccuracy of the system is demonstrated with SLAM results that achieve sub-1%\ndistance-travel error even in the presence of significant re-calibration\nevents.\n", "versions": [{"version": "v1", "created": "Wed, 5 Nov 2014 19:39:41 GMT"}], "update_date": "2014-11-06", "authors_parsed": [["Keivan", "Nima", ""], ["Sibley", "Gabe", ""]]}, {"id": "1411.1442", "submitter": "Wei Wang", "authors": "Wei Wang", "title": "Optical Character Recognition, Using K-Nearest Neighbors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  The problem of optical character recognition, OCR, has been widely discussed\nin the literature. Having a hand-written text, the program aims at recognizing\nthe text. Even though there are several approaches to this issue, it is still\nan open problem. In this paper we would like to propose an approach that uses\nK-nearest neighbors algorithm, and has the accuracy of more than 90%. The\ntraining and run time is also very short.\n", "versions": [{"version": "v1", "created": "Wed, 5 Nov 2014 22:56:10 GMT"}], "update_date": "2014-11-07", "authors_parsed": [["Wang", "Wei", ""]]}, {"id": "1411.1446", "submitter": "Wei Wang", "authors": "Wei Wang", "title": "Electrocardiography Separation of Mother and Baby", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  Extraction of Electrocardiography (ECG or EKG) signals of mother and baby is\na challenging task, because one single device is used and it receives a mixture\nof multiple heart beats. In this paper, we would like to design a filter to\nseparate the signals from each other.\n", "versions": [{"version": "v1", "created": "Wed, 5 Nov 2014 23:14:10 GMT"}], "update_date": "2014-11-07", "authors_parsed": [["Wang", "Wei", ""]]}, {"id": "1411.1509", "submitter": "Zetao Chen", "authors": "Zetao Chen, Obadiah Lam, Adam Jacobson and Michael Milford", "title": "Convolutional Neural Network-based Place Recognition", "comments": "8 pages, 11 figures, this paper has been accepted by 2014\n  Australasian Conference on Robotics and Automation (ACRA 2014) to be held in\n  University of Melbourne, Dec 2~4", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently Convolutional Neural Networks (CNNs) have been shown to achieve\nstate-of-the-art performance on various classification tasks. In this paper, we\npresent for the first time a place recognition technique based on CNN models,\nby combining the powerful features learnt by CNNs with a spatial and sequential\nfilter. Applying the system to a 70 km benchmark place recognition dataset we\nachieve a 75% increase in recall at 100% precision, significantly outperforming\nall previous state of the art techniques. We also conduct a comprehensive\nperformance comparison of the utility of features from all 21 layers for place\nrecognition, both for the benchmark dataset and for a second dataset with more\nsignificant viewpoint changes.\n", "versions": [{"version": "v1", "created": "Thu, 6 Nov 2014 07:03:15 GMT"}], "update_date": "2014-11-07", "authors_parsed": [["Chen", "Zetao", ""], ["Lam", "Obadiah", ""], ["Jacobson", "Adam", ""], ["Milford", "Michael", ""]]}, {"id": "1411.1537", "submitter": "Fei Sha", "authors": "Boqing Gong, Wei-lun Chao, Kristen Grauman and Fei Sha", "title": "Large-Margin Determinantal Point Processes", "comments": "15 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Determinantal point processes (DPPs) offer a powerful approach to modeling\ndiversity in many applications where the goal is to select a diverse subset. We\nstudy the problem of learning the parameters (the kernel matrix) of a DPP from\nlabeled training data. We make two contributions. First, we show how to\nreparameterize a DPP's kernel matrix with multiple kernel functions, thus\nenhancing modeling flexibility. Second, we propose a novel parameter estimation\ntechnique based on the principle of large margin separation. In contrast to the\nstate-of-the-art method of maximum likelihood estimation, our large-margin loss\nfunction explicitly models errors in selecting the target subsets, and it can\nbe customized to trade off different types of errors (precision vs. recall).\nExtensive empirical studies validate our contributions, including applications\non challenging document and video summarization, where flexibility in modeling\nthe kernel matrix and balancing different errors is indispensable.\n", "versions": [{"version": "v1", "created": "Thu, 6 Nov 2014 09:14:02 GMT"}, {"version": "v2", "created": "Fri, 7 Nov 2014 05:21:03 GMT"}], "update_date": "2014-11-10", "authors_parsed": [["Gong", "Boqing", ""], ["Chao", "Wei-lun", ""], ["Grauman", "Kristen", ""], ["Sha", "Fei", ""]]}, {"id": "1411.1668", "submitter": "Partha Bhowmick", "authors": "Sahadev Bera, Shyamosree Pal, Partha Bhowmick, Bhargab B. Bhattacharya", "title": "On Chord and Sagitta in ${\\mathbb Z}^2$: An Analysis towards Fast and\n  Robust Circular Arc Detection", "comments": "34 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although chord and sagitta, when considered in tandem, may reflect many\nunderlying geometric properties of circles on the Euclidean plane, their\nimplications on the digital plane are not yet well-understood. In this paper,\nwe explore some of their fundamental properties on the digital plane that have\na strong bearing on the unsupervised detection of circles and circular arcs in\na digital image. We show that although the chord-and-sagitta properties of a\nreal circle do not readily migrate to the digital plane, they can indeed be\nused for the analysis in the discrete domain based on certain bounds on their\ndeviations, which are derived from the real domain. In particular, we derive an\nupper bound on the circumferential angular deviation of a point in the context\nof chord property, and an upper bound on the relative error in radius\nestimation with regard to the sagitta property. Using these two bounds, we\ndesign a novel algorithm for the detection and parameterization of circles and\ncircular arcs, which does not require any heuristic initialization or manual\ntuning. The chord property is deployed for the detection of circular arcs,\nwhereas the sagitta property is used to estimate their centers and radii.\nFinally, to improve the accuracy of estimation, the notion of restricted Hough\ntransform is used. Experimental results demonstrate superior efficiency and\nrobustness of the proposed methodology compared to existing techniques.\n", "versions": [{"version": "v1", "created": "Sun, 26 Oct 2014 17:45:02 GMT"}], "update_date": "2014-11-07", "authors_parsed": [["Bera", "Sahadev", ""], ["Pal", "Shyamosree", ""], ["Bhowmick", "Partha", ""], ["Bhattacharya", "Bhargab B.", ""]]}, {"id": "1411.1752", "submitter": "Adarsh Prasad", "authors": "Adarsh Prasad, Stefanie Jegelka and Dhruv Batra", "title": "Submodular meets Structured: Finding Diverse Subsets in\n  Exponentially-Large Structured Item Sets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To cope with the high level of ambiguity faced in domains such as Computer\nVision or Natural Language processing, robust prediction methods often search\nfor a diverse set of high-quality candidate solutions or proposals. In\nstructured prediction problems, this becomes a daunting task, as the solution\nspace (image labelings, sentence parses, etc.) is exponentially large. We study\ngreedy algorithms for finding a diverse subset of solutions in\nstructured-output spaces by drawing new connections between submodular\nfunctions over combinatorial item sets and High-Order Potentials (HOPs) studied\nfor graphical models. Specifically, we show via examples that when marginal\ngains of submodular diversity functions allow structured representations, this\nenables efficient (sub-linear time) approximate maximization by reducing the\ngreedy augmentation step to inference in a factor graph with appropriately\nconstructed HOPs. We discuss benefits, tradeoffs, and show that our\nconstructions lead to significantly better proposals.\n", "versions": [{"version": "v1", "created": "Thu, 6 Nov 2014 20:07:37 GMT"}], "update_date": "2014-11-10", "authors_parsed": [["Prasad", "Adarsh", ""], ["Jegelka", "Stefanie", ""], ["Batra", "Dhruv", ""]]}, {"id": "1411.1784", "submitter": "Mehdi Mirza", "authors": "Mehdi Mirza, Simon Osindero", "title": "Conditional Generative Adversarial Nets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative Adversarial Nets [8] were recently introduced as a novel way to\ntrain generative models. In this work we introduce the conditional version of\ngenerative adversarial nets, which can be constructed by simply feeding the\ndata, y, we wish to condition on to both the generator and discriminator. We\nshow that this model can generate MNIST digits conditioned on class labels. We\nalso illustrate how this model could be used to learn a multi-modal model, and\nprovide preliminary examples of an application to image tagging in which we\ndemonstrate how this approach can generate descriptive tags which are not part\nof training labels.\n", "versions": [{"version": "v1", "created": "Thu, 6 Nov 2014 22:33:22 GMT"}], "update_date": "2014-11-10", "authors_parsed": [["Mirza", "Mehdi", ""], ["Osindero", "Simon", ""]]}, {"id": "1411.1971", "submitter": "Jiaxin Zhang", "authors": "Xiangyang Zhou, Jiaxin Zhang, Brian Kulis", "title": "Power-Law Graph Cuts", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Algorithms based on spectral graph cut objectives such as normalized cuts,\nratio cuts and ratio association have become popular in recent years because\nthey are widely applicable and simple to implement via standard eigenvector\ncomputations. Despite strong performance for a number of clustering tasks,\nspectral graph cut algorithms still suffer from several limitations: first,\nthey require the number of clusters to be known in advance, but this\ninformation is often unknown a priori; second, they tend to produce clusters\nwith uniform sizes. In some cases, the true clusters exhibit a known size\ndistribution; in image segmentation, for instance, human-segmented images tend\nto yield segment sizes that follow a power-law distribution. In this paper, we\npropose a general framework of power-law graph cut algorithms that produce\nclusters whose sizes are power-law distributed, and also does not fix the\nnumber of clusters upfront. To achieve our goals, we treat the Pitman-Yor\nexchangeable partition probability function (EPPF) as a regularizer to graph\ncut objectives. Because the resulting objectives cannot be solved by relaxing\nvia eigenvectors, we derive a simple iterative algorithm to locally optimize\nthe objectives. Moreover, we show that our proposed algorithm can be viewed as\nperforming MAP inference on a particular Pitman-Yor mixture model. Our\nexperiments on various data sets show the effectiveness of our algorithms.\n", "versions": [{"version": "v1", "created": "Wed, 29 Oct 2014 20:46:20 GMT"}, {"version": "v2", "created": "Tue, 25 Nov 2014 21:41:07 GMT"}], "update_date": "2014-11-27", "authors_parsed": [["Zhou", "Xiangyang", ""], ["Zhang", "Jiaxin", ""], ["Kulis", "Brian", ""]]}, {"id": "1411.2090", "submitter": "Nagaraja S", "authors": "Nagaraja S., Prabhakar C.J. and Praveen Kumar P.U", "title": "Parallax Effect Free Mosaicing of Underwater Video Sequence Based on\n  Texture Features", "comments": "13 pages, 7 figures, Signal & Image Processing : An International\n  Journal (SIPIJ), Vol.5, No.5, October 2014", "journal-ref": null, "doi": "10.5121/sipij.2014.5502", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present feature-based technique for construction of mosaic\nimage from underwater video sequence, which suffers from parallax distortion\ndue to propagation properties of light in the underwater environment. The most\nof the available mosaic tools and underwater image mosaicing techniques yields\nfinal result with some artifacts such as blurring, ghosting and seam due to\npresence of parallax in the input images. The removal of parallax from input\nimages may not reduce its effects instead it must be corrected in successive\nsteps of mosaicing. Thus, our approach minimizes the parallax effects by\nadopting an efficient local alignment technique after global registration. We\nextract texture features using Centre Symmetric Local Binary Pattern (CS-LBP)\ndescriptor in order to find feature correspondences, which are used further for\nestimation of homography through RANSAC. In order to increase the accuracy of\nglobal registration, we perform preprocessing such as colour alignment between\ntwo selected frames based on colour distribution adjustment. Because of\nexistence of 100% overlap in consecutive frames of underwater video, we select\nframes with minimum overlap based on mutual offset in order to reduce the\ncomputation cost during mosaicing. Our approach minimizes the parallax effects\nconsiderably in final mosaic constructed using our own underwater video\nsequences.\n", "versions": [{"version": "v1", "created": "Sat, 8 Nov 2014 06:15:44 GMT"}], "update_date": "2014-11-11", "authors_parsed": [["S.", "Nagaraja", ""], ["J.", "Prabhakar C.", ""], ["U", "Praveen Kumar P.", ""]]}, {"id": "1411.2141", "submitter": "Ahmadreza Baghaie", "authors": "Ahmadreza Baghaie, Zeyun Yu, Roshan M. D'souza", "title": "Fast Mesh-Based Medical Image Registration", "comments": "Accepted manuscript for ISVC'2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper a fast triangular mesh based registration method is proposed.\nHaving Template and Reference images as inputs, the template image is\ntriangulated using a content adaptive mesh generation algorithm. Considering\nthe pixel values at mesh nodes, interpolated using spline interpolation method\nfor both of the images, the energy functional needed for image registration is\nminimized. The minimization process was achieved using a mesh based\ndiscretization of the distance measure and regularization term which resulted\nin a sparse system of linear equations, which due to the smaller size in\ncomparison to the pixel-wise registration method, can be solved directly. Mean\nSquared Difference (MSD) is used as a metric for evaluating the results. Using\nthe mesh based technique, higher speed was achieved compared to pixel-based\ncurvature registration technique with fast DCT solver. The implementation was\ndone in MATLAB without any specific optimization. Higher speeds can be achieved\nusing C/C++ implementations.\n", "versions": [{"version": "v1", "created": "Sat, 8 Nov 2014 17:40:11 GMT"}], "update_date": "2016-11-25", "authors_parsed": [["Baghaie", "Ahmadreza", ""], ["Yu", "Zeyun", ""], ["D'souza", "Roshan M.", ""]]}, {"id": "1411.2173", "submitter": "Julieta Martinez", "authors": "Julieta Martinez, Holger H. Hoos, James J. Little", "title": "Stacked Quantizers for Compositional Vector Compression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, Babenko and Lempitsky introduced Additive Quantization (AQ), a\ngeneralization of Product Quantization (PQ) where a non-independent set of\ncodebooks is used to compress vectors into small binary codes. Unfortunately,\nunder this scheme encoding cannot be done independently in each codebook, and\noptimal encoding is an NP-hard problem. In this paper, we observe that PQ and\nAQ are both compositional quantizers that lie on the extremes of the codebook\ndependence-independence assumption, and explore an intermediate approach that\nexploits a hierarchical structure in the codebooks. This results in a method\nthat achieves quantization error on par with or lower than AQ, while being\nseveral orders of magnitude faster. We perform a complexity analysis of PQ, AQ\nand our method, and evaluate our approach on standard benchmarks of SIFT and\nGIST descriptors, as well as on new datasets of features obtained from\nstate-of-the-art convolutional neural networks.\n", "versions": [{"version": "v1", "created": "Sat, 8 Nov 2014 22:51:12 GMT"}], "update_date": "2014-11-11", "authors_parsed": [["Martinez", "Julieta", ""], ["Hoos", "Holger H.", ""], ["Little", "James J.", ""]]}, {"id": "1411.2214", "submitter": "Babak Saleh", "authors": "Babak Saleh, Ali Farhadi, Ahmed Elgammal", "title": "Abnormal Object Recognition: A Comprehensive Study", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When describing images, humans tend not to talk about the obvious, but rather\nmention what they find interesting. We argue that abnormalities and deviations\nfrom typicalities are among the most important components that form what is\nworth mentioning. In this paper we introduce the abnormality detection as a\nrecognition problem and show how to model typicalities and, consequently,\nmeaningful deviations from prototypical properties of categories. Our model can\nrecognize abnormalities and report the main reasons of any recognized\nabnormality. We introduce the abnormality detection dataset and show\ninteresting results on how to reason about abnormalities.\n", "versions": [{"version": "v1", "created": "Sun, 9 Nov 2014 09:51:06 GMT"}], "update_date": "2014-11-11", "authors_parsed": [["Saleh", "Babak", ""], ["Farhadi", "Ali", ""], ["Elgammal", "Ahmed", ""]]}, {"id": "1411.2316", "submitter": "Vishnu Naresh Boddeti", "authors": "Joseph A. Fernandez, Vishnu Naresh Boddeti, Andres Rodriguez, B. V. K.\n  Vijaya Kumar", "title": "Zero-Aliasing Correlation Filters for Object Recognition", "comments": "14 pages, to appear in IEEE Transactions on Pattern Analysis and\n  Machine Intelligence (PAMI)", "journal-ref": null, "doi": "10.1109/TPAMI.2014.2375215", "report-no": null, "categories": "cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Correlation filters (CFs) are a class of classifiers that are attractive for\nobject localization and tracking applications. Traditionally, CFs have been\ndesigned in the frequency domain using the discrete Fourier transform (DFT),\nwhere correlation is efficiently implemented. However, existing CF designs do\nnot account for the fact that the multiplication of two DFTs in the frequency\ndomain corresponds to a circular correlation in the time/spatial domain.\nBecause this was previously unaccounted for, prior CF designs are not truly\noptimal, as their optimization criteria do not accurately quantify their\noptimization intention. In this paper, we introduce new zero-aliasing\nconstraints that completely eliminate this aliasing problem by ensuring that\nthe optimization criterion for a given CF corresponds to a linear correlation\nrather than a circular correlation. This means that previous CF designs can be\nsignificantly improved by this reformulation. We demonstrate the benefits of\nthis new CF design approach with several important CFs. We present experimental\nresults on diverse data sets and present solutions to the computational\nchallenges associated with computing these CFs. Code for the CFs described in\nthis paper and their respective zero-aliasing versions is available at\nhttp://vishnu.boddeti.net/projects/correlation-filters.html\n", "versions": [{"version": "v1", "created": "Mon, 10 Nov 2014 03:48:21 GMT"}, {"version": "v2", "created": "Wed, 19 Nov 2014 15:10:22 GMT"}], "update_date": "2016-11-18", "authors_parsed": [["Fernandez", "Joseph A.", ""], ["Boddeti", "Vishnu Naresh", ""], ["Rodriguez", "Andres", ""], ["Kumar", "B. V. K. Vijaya", ""]]}, {"id": "1411.2335", "submitter": "Kriti Kumar", "authors": "Kriti Kumar, Ashley Varghese, Pavan K Reddy, N Narendra, Prashanth\n  Swamy, M Girish Chandra and P Balamuralidhar", "title": "An Improved Tracking using IMU and Vision Fusion for Mobile Augmented\n  Reality Applications", "comments": null, "journal-ref": "International Journal of Multimedia and its Applications, ISSN:\n  0975-5578 (online); 0975-5934 (print), October 2014, Volume 6, Number 5", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mobile Augmented Reality (MAR) is becoming an important cyber-physical system\napplication given the ubiquitous availability of mobile phones. With the need\nto operate in unprepared environments, accurate and robust registration and\ntracking has become an important research problem to solve. In fact, when MAR\nis used for tele-interactive applications involving large distances, say from\nan accident site to insurance office, tracking at both the ends is desirable\nand further it is essential to appropriately fuse inertial and vision sensors\ndata. In this paper, we present results and discuss some insights gained in\nmarker-less tracking during the development of a prototype pertaining to an\nexample use case related to breakdown or damage assessment of a vehicle. The\nnovelty of this paper is in bringing together different components and modules\nwith appropriate enhancements towards a complete working system.\n", "versions": [{"version": "v1", "created": "Mon, 10 Nov 2014 06:15:42 GMT"}], "update_date": "2014-11-11", "authors_parsed": [["Kumar", "Kriti", ""], ["Varghese", "Ashley", ""], ["Reddy", "Pavan K", ""], ["Narendra", "N", ""], ["Swamy", "Prashanth", ""], ["Chandra", "M Girish", ""], ["Balamuralidhar", "P", ""]]}, {"id": "1411.2539", "submitter": "Ryan Kiros", "authors": "Ryan Kiros, Ruslan Salakhutdinov, Richard S. Zemel", "title": "Unifying Visual-Semantic Embeddings with Multimodal Neural Language\n  Models", "comments": "13 pages. NIPS 2014 deep learning workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inspired by recent advances in multimodal learning and machine translation,\nwe introduce an encoder-decoder pipeline that learns (a): a multimodal joint\nembedding space with images and text and (b): a novel language model for\ndecoding distributed representations from our space. Our pipeline effectively\nunifies joint image-text embedding models with multimodal neural language\nmodels. We introduce the structure-content neural language model that\ndisentangles the structure of a sentence to its content, conditioned on\nrepresentations produced by the encoder. The encoder allows one to rank images\nand sentences while the decoder can generate novel descriptions from scratch.\nUsing LSTM to encode sentences, we match the state-of-the-art performance on\nFlickr8K and Flickr30K without using object detections. We also set new best\nresults when using the 19-layer Oxford convolutional network. Furthermore we\nshow that with linear encoders, the learned embedding space captures multimodal\nregularities in terms of vector space arithmetic e.g. *image of a blue car* -\n\"blue\" + \"red\" is near images of red cars. Sample captions generated for 800\nimages are made available for comparison.\n", "versions": [{"version": "v1", "created": "Mon, 10 Nov 2014 19:09:41 GMT"}], "update_date": "2014-11-11", "authors_parsed": [["Kiros", "Ryan", ""], ["Salakhutdinov", "Ruslan", ""], ["Zemel", "Richard S.", ""]]}, {"id": "1411.2584", "submitter": "Gianluca Vinti Prof.", "authors": "Danilo Costarelli, Federico Cluni, Anna Maria Minotti and Gianluca\n  Vinti", "title": "Applications of sampling Kantorovich operators to thermographic images\n  for seismic engineering", "comments": "16 pages, 5 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present some applications of the multivariate sampling\nKantorovich operators $S_w$ to seismic engineering. The mathematical theory of\nthese operators, both in the space of continuous functions and in Orlicz\nspaces, show how it is possible to approximate/reconstruct multivariate\nsignals, such as images. In particular, to obtain applications for\nthermographic images a mathematical algorithm is developed using MATLAB and\nmatrix calculus. The setting of Orlicz spaces is important since allow us to\nreconstruct not necessarily continuous signals by means of $S_w$. The\nreconstruction of thermographic images of buildings by our sampling Kantorovich\nalgorithm allow us to obtain models for the simulation of the behavior of\nstructures under seismic action. We analyze a real world case study in term of\nstructural analysis and we compare the behavior of the building under seismic\naction using various models.\n", "versions": [{"version": "v1", "created": "Sun, 9 Nov 2014 18:38:41 GMT"}], "update_date": "2014-11-12", "authors_parsed": [["Costarelli", "Danilo", ""], ["Cluni", "Federico", ""], ["Minotti", "Anna Maria", ""], ["Vinti", "Gianluca", ""]]}, {"id": "1411.2861", "submitter": "Xiaodan Liang", "authors": "Xiaodan Liang, Si Liu, Yunchao Wei, Luoqi Liu, Liang Lin, Shuicheng\n  Yan", "title": "Computational Baby Learning", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Intuitive observations show that a baby may inherently possess the capability\nof recognizing a new visual concept (e.g., chair, dog) by learning from only\nvery few positive instances taught by parent(s) or others, and this recognition\ncapability can be gradually further improved by exploring and/or interacting\nwith the real instances in the physical world. Inspired by these observations,\nwe propose a computational model for slightly-supervised object detection,\nbased on prior knowledge modelling, exemplar learning and learning with video\ncontexts. The prior knowledge is modeled with a pre-trained Convolutional\nNeural Network (CNN). When very few instances of a new concept are given, an\ninitial concept detector is built by exemplar learning over the deep features\nfrom the pre-trained CNN. Simulating the baby's interaction with physical\nworld, the well-designed tracking solution is then used to discover more\ndiverse instances from the massive online unlabeled videos. Once a positive\ninstance is detected/identified with high score in each video, more variable\ninstances possibly from different view-angles and/or different distances are\ntracked and accumulated. Then the concept detector can be fine-tuned based on\nthese new instances. This process can be repeated again and again till we\nobtain a very mature concept detector. Extensive experiments on Pascal\nVOC-07/10/12 object detection datasets well demonstrate the effectiveness of\nour framework. It can beat the state-of-the-art full-training based\nperformances by learning from very few samples for each object category, along\nwith about 20,000 unlabeled videos.\n", "versions": [{"version": "v1", "created": "Tue, 11 Nov 2014 16:00:59 GMT"}, {"version": "v2", "created": "Wed, 12 Nov 2014 13:59:59 GMT"}, {"version": "v3", "created": "Mon, 4 May 2015 02:33:26 GMT"}], "update_date": "2015-05-05", "authors_parsed": [["Liang", "Xiaodan", ""], ["Liu", "Si", ""], ["Wei", "Yunchao", ""], ["Liu", "Luoqi", ""], ["Lin", "Liang", ""], ["Yan", "Shuicheng", ""]]}, {"id": "1411.2942", "submitter": "Xiaowei Zhou", "authors": "Xiaowei Zhou, Spyridon Leonardos, Xiaoyan Hu, Kostas Daniilidis", "title": "3D Shape Estimation from 2D Landmarks: A Convex Relaxation Approach", "comments": "In Proceedings of CVPR 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the problem of estimating the 3D shape of an object, given a\nset of 2D landmarks in a single image. To alleviate the reconstruction\nambiguity, a widely-used approach is to confine the unknown 3D shape within a\nshape space built upon existing shapes. While this approach has proven to be\nsuccessful in various applications, a challenging issue remains, i.e., the\njoint estimation of shape parameters and camera-pose parameters requires to\nsolve a nonconvex optimization problem. The existing methods often adopt an\nalternating minimization scheme to locally update the parameters, and\nconsequently the solution is sensitive to initialization. In this paper, we\npropose a convex formulation to address this problem and develop an efficient\nalgorithm to solve the proposed convex program. We demonstrate the exact\nrecovery property of the proposed method, its merits compared to alternative\nmethods, and the applicability in human pose and car shape estimation.\n", "versions": [{"version": "v1", "created": "Tue, 11 Nov 2014 19:56:17 GMT"}, {"version": "v2", "created": "Mon, 17 Nov 2014 20:45:46 GMT"}, {"version": "v3", "created": "Mon, 1 Dec 2014 20:27:51 GMT"}, {"version": "v4", "created": "Tue, 2 Jun 2015 03:53:46 GMT"}], "update_date": "2015-06-03", "authors_parsed": [["Zhou", "Xiaowei", ""], ["Leonardos", "Spyridon", ""], ["Hu", "Xiaoyan", ""], ["Daniilidis", "Kostas", ""]]}, {"id": "1411.3041", "submitter": "Ramakrishna  Vedantam", "authors": "Ramakrishna Vedantam, C. Lawrence Zitnick, and Devi Parikh", "title": "Collecting Image Description Datasets using Crowdsourcing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe our two new datasets with images described by humans. Both the\ndatasets were collected using Amazon Mechanical Turk, a crowdsourcing platform.\nThe two datasets contain significantly more descriptions per image than other\nexisting datasets. One is based on a popular image description dataset called\nthe UIUC Pascal Sentence Dataset, whereas the other is based on the Abstract\nScenes dataset con- taining images made from clipart objects. In this paper we\ndescribe our interfaces, analyze some properties of and show example\ndescriptions from our two datasets.\n", "versions": [{"version": "v1", "created": "Wed, 12 Nov 2014 01:34:46 GMT"}], "update_date": "2014-11-13", "authors_parsed": [["Vedantam", "Ramakrishna", ""], ["Zitnick", "C. Lawrence", ""], ["Parikh", "Devi", ""]]}, {"id": "1411.3159", "submitter": "Marcel Simon", "authors": "Marcel Simon, Erik Rodner, Joachim Denzler", "title": "Part Detector Discovery in Deep Convolutional Neural Networks", "comments": "Accepted for publication on Asian Conference on Computer Vision\n  (ACCV) 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current fine-grained classification approaches often rely on a robust\nlocalization of object parts to extract localized feature representations\nsuitable for discrimination. However, part localization is a challenging task\ndue to the large variation of appearance and pose. In this paper, we show how\npre-trained convolutional neural networks can be used for robust and efficient\nobject part discovery and localization without the necessity to actually train\nthe network on the current dataset. Our approach called \"part detector\ndiscovery\" (PDD) is based on analyzing the gradient maps of the network outputs\nand finding activation centers spatially related to annotated semantic parts or\nbounding boxes.\n  This allows us not just to obtain excellent performance on the CUB200-2011\ndataset, but in contrast to previous approaches also to perform detection and\nbird classification jointly without requiring a given bounding box annotation\nduring testing and ground-truth parts during training. The code is available at\nhttp://www.inf-cv.uni-jena.de/part_discovery and\nhttps://github.com/cvjena/PartDetectorDisovery.\n", "versions": [{"version": "v1", "created": "Wed, 12 Nov 2014 12:42:54 GMT"}, {"version": "v2", "created": "Fri, 14 Nov 2014 11:57:27 GMT"}], "update_date": "2014-11-17", "authors_parsed": [["Simon", "Marcel", ""], ["Rodner", "Erik", ""], ["Denzler", "Joachim", ""]]}, {"id": "1411.3169", "submitter": "Ali Ghaderi Ph.D", "authors": "Ali Ghaderi", "title": "On Coarse Graining of Information and Its Application to Pattern\n  Recognition", "comments": null, "journal-ref": null, "doi": "10.1063/1.4906011", "report-no": null, "categories": "cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a method based on finite mixture models for classifying a set of\nobservations into number of different categories. In order to demonstrate the\nmethod, we show how the component densities for the mixture model can be\nderived by using the maximum entropy method in conjunction with conservation of\nPythagorean means. Several examples of distributions belonging to the\nPythagorean family are derived. A discussion on estimation of model parameters\nand the number of categories is also given.\n", "versions": [{"version": "v1", "created": "Wed, 12 Nov 2014 13:21:48 GMT"}], "update_date": "2015-06-23", "authors_parsed": [["Ghaderi", "Ali", ""]]}, {"id": "1411.3229", "submitter": "Tian Cao", "authors": "Tian Cao, Christopher Zach, Shannon Modla, Debbie Powell, Kirk Czymmek\n  and Marc Niethammer", "title": "Multi-modal Image Registration for Correlative Microscopy", "comments": "24 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Correlative microscopy is a methodology combining the functionality of light\nmicroscopy with the high resolution of electron microscopy and other microscopy\ntechnologies. Image registration for correlative microscopy is quite\nchallenging because it is a multi-modal, multi-scale and multi-dimensional\nregistration problem. In this report, I introduce two methods of image\nregistration for correlative microscopy. The first method is based on fiducials\n(beads). I generate landmarks from the fiducials and compute the similarity\ntransformation matrix based on three pairs of nearest corresponding landmarks.\nA least-squares matching process is applied afterwards to further refine the\nregistration. The second method is inspired by the image analogies approach. I\nintroduce the sparse representation model into image analogies. I first train\nrepresentative image patches (dictionaries) for pre-registered datasets from\ntwo different modalities, and then I use the sparse coding technique to\ntransfer a given image to a predicted image from one modality to another based\non the learned dictionaries. The final image registration is between the\npredicted image and the original image corresponding to the given image in the\ndifferent modality. The method transforms a multi-modal registration problem to\na mono-modal one. I test my approaches on Transmission Electron Microscopy\n(TEM) and confocal microscopy images. Experimental results of the methods are\nalso shown in this report.\n", "versions": [{"version": "v1", "created": "Wed, 12 Nov 2014 16:32:17 GMT"}, {"version": "v2", "created": "Tue, 13 Jan 2015 15:44:08 GMT"}], "update_date": "2015-01-14", "authors_parsed": [["Cao", "Tian", ""], ["Zach", "Christopher", ""], ["Modla", "Shannon", ""], ["Powell", "Debbie", ""], ["Czymmek", "Kirk", ""], ["Niethammer", "Marc", ""]]}, {"id": "1411.3230", "submitter": "Julien Mairal", "authors": "Julien Mairal (Inria), Francis Bach (Inria) and Jean Ponce (Ecole\n  Normale Sup\\'erieure)", "title": "Sparse Modeling for Image and Vision Processing", "comments": "205 pages, to appear in Foundations and Trends in Computer Graphics\n  and Vision", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, a large amount of multi-disciplinary research has been\nconducted on sparse models and their applications. In statistics and machine\nlearning, the sparsity principle is used to perform model selection---that is,\nautomatically selecting a simple model among a large collection of them. In\nsignal processing, sparse coding consists of representing data with linear\ncombinations of a few dictionary elements. Subsequently, the corresponding\ntools have been widely adopted by several scientific communities such as\nneuroscience, bioinformatics, or computer vision. The goal of this monograph is\nto offer a self-contained view of sparse modeling for visual recognition and\nimage processing. More specifically, we focus on applications where the\ndictionary is learned and adapted to data, yielding a compact representation\nthat has been successful in various contexts.\n", "versions": [{"version": "v1", "created": "Wed, 12 Nov 2014 16:33:37 GMT"}, {"version": "v2", "created": "Sat, 6 Dec 2014 14:39:07 GMT"}], "update_date": "2014-12-09", "authors_parsed": [["Mairal", "Julien", "", "Inria"], ["Bach", "Francis", "", "Inria"], ["Ponce", "Jean", "", "Ecole\n  Normale Sup\u00e9rieure"]]}, {"id": "1411.3285", "submitter": "Martin Welk", "authors": "Martin Welk", "title": "Amoeba Techniques for Shape and Texture Analysis", "comments": "38 pages, 19 figures v2: minor corrections and rephrasing, Section 5\n  (pre-smoothing) extended", "journal-ref": null, "doi": "10.1007/978-3-319-24726-7_4", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Morphological amoebas are image-adaptive structuring elements for\nmorphological and other local image filters introduced by Lerallut et al. Their\nconstruction is based on combining spatial distance with contrast information\ninto an image-dependent metric. Amoeba filters show interesting parallels to\nimage filtering methods based on partial differential equations (PDEs), which\ncan be confirmed by asymptotic equivalence results. In computing amoebas, graph\nstructures are generated that hold information about local image texture. This\npaper reviews and summarises the work of the author and his coauthors on\nmorphological amoebas, particularly their relations to PDE filters and texture\nanalysis. It presents some extensions and points out directions for future\ninvestigation on the subject.\n", "versions": [{"version": "v1", "created": "Wed, 12 Nov 2014 19:14:30 GMT"}, {"version": "v2", "created": "Wed, 10 Jun 2015 15:28:31 GMT"}], "update_date": "2017-09-22", "authors_parsed": [["Welk", "Martin", ""]]}, {"id": "1411.3410", "submitter": "Kwangchol Jang", "authors": "Kwangchol Jang, Sokmin Han, Insong Kim", "title": "Person Re-identification Based on Color Histogram and Spatial\n  Configuration of Dominant Color Regions", "comments": "12 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/publicdomain/", "abstract": "  There is a requirement to determine whether a given person of interest has\nalready been observed over a network of cameras in video surveillance systems.\nA human appearance obtained in one camera is usually different from the ones\nobtained in another camera due to difference in illumination, pose and\nviewpoint, camera parameters. Being related to appearance-based approaches for\nperson re-identification, we propose a novel method based on the dominant color\nhistogram and spatial configuration of dominant color regions on human body\nparts. Dominant color histogram and spatial configuration of the dominant color\nregions based on dominant color descriptor(DCD) can be considered to be robust\nto illumination and pose, viewpoint changes. The proposed method is evaluated\nusing benchmark video datasets. Experimental results using the cumulative\nmatching characteristic(CMC) curve demonstrate the effectiveness of our\napproach for person re-identification.\n", "versions": [{"version": "v1", "created": "Thu, 13 Nov 2014 00:55:48 GMT"}], "update_date": "2014-11-14", "authors_parsed": [["Jang", "Kwangchol", ""], ["Han", "Sokmin", ""], ["Kim", "Insong", ""]]}, {"id": "1411.3423", "submitter": "Ghulam Mubashar Hassan", "authors": "Ghulam Mubashar Hassan, Arcady V. Dyskin and Cara K. MacNish", "title": "A Comparative Study of Techniques of Distant Reconstruction of\n  Displacement Fields by using DISTRESS Simulator", "comments": "in review process of a Journal", "journal-ref": null, "doi": "10.1016/j.ijleo.2016.09.026", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reconstruction and monitoring of displacement and strain fields is an\nimportant problem in engineering. We analyze the remote and non-obtrusive\nmethods of strain measurement based on photogrammetry and Digital Image\nCorrelation (DIC). The method is based on covering the photographed surface\nwith a pattern of speckles and comparing the images taken before and after the\ndeformation. In this study, a comprehensive literature review and comparative\nanalysis of photogrammetric solutions is presented. The analysis is based on a\nspecially developed Digital Image Synthesizer To Reconstruct Strain in Solids\n(DISTRESS) Simulator to generate synthetic images of displacement and stress\nfields in order to investigate the intrinsic accuracy of the existing variants\nof DIC. We investigated the Basic DIC and a commercial software VIC 2D, both\nbased on displacement field reconstruction with post processing strain\ndetermination based on numerical differentiation. We also investigated what we\ncall the Extended DIC where the strain field is determined independently of the\ndisplacement field. While the Basic DIC and VIC 2D are faster, the Extended DIC\ndelivers the best accuracy of strain reconstruction. The speckle pattern is\nfound to be playing a critical role in achieving high accuracy for DIC.\nIncrease in subset size for DIC does not significantly improves the accuracy,\nwhile the smallest subset size depends on the speckle pattern and speckle size.\nIncrease in the overall image size provides more details but does not play\nsignificant role in improving the accuracy, while significantly increasing the\ncomputation cost.\n", "versions": [{"version": "v1", "created": "Thu, 13 Nov 2014 02:38:42 GMT"}], "update_date": "2016-09-28", "authors_parsed": [["Hassan", "Ghulam Mubashar", ""], ["Dyskin", "Arcady V.", ""], ["MacNish", "Cara K.", ""]]}, {"id": "1411.3519", "submitter": "Mohamed Hussein", "authors": "Marwan Torki, Mohamed E. Hussein, Ahmed Elsallamy, Mahmoud Fayyaz,\n  Shehab Yaser", "title": "Window-Based Descriptors for Arabic Handwritten Alphabet Recognition: A\n  Comparative Study on a Novel Dataset", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a comparative study for window-based descriptors on the\napplication of Arabic handwritten alphabet recognition. We show a detailed\nexperimental evaluation of different descriptors with several classifiers. The\nobjective of the paper is to evaluate different window-based descriptors on the\nproblem of Arabic letter recognition. Our experiments clearly show that they\nperform very well. Moreover, we introduce a novel spatial pyramid partitioning\nscheme that enhances the recognition accuracy for most descriptors. In\naddition, we introduce a novel dataset for Arabic handwritten isolated alphabet\nletters, which can serve as a benchmark for future research.\n", "versions": [{"version": "v1", "created": "Thu, 13 Nov 2014 12:22:57 GMT"}, {"version": "v2", "created": "Mon, 17 Nov 2014 17:55:32 GMT"}], "update_date": "2014-11-18", "authors_parsed": [["Torki", "Marwan", ""], ["Hussein", "Mohamed E.", ""], ["Elsallamy", "Ahmed", ""], ["Fayyaz", "Mahmoud", ""], ["Yaser", "Shehab", ""]]}, {"id": "1411.3525", "submitter": "Alessandro Roncone", "authors": "Alessandro Roncone, Ugo Pattacini, Giorgio Metta and Lorenzo Natale", "title": "Gaze Stabilization for Humanoid Robots: a Comprehensive Framework", "comments": "6 pages, appears in 2014 IEEE-RAS International Conference on\n  Humanoid Robots", "journal-ref": null, "doi": "10.1109/HUMANOIDS.2014.7041369", "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gaze stabilization is an important requisite for humanoid robots. Previous\nwork on this topic has focused on the integration of inertial and visual\ninformation. Little attention has been given to a third component, which is the\nknowledge that the robot has about its own movement. In this work we propose a\ncomprehensive framework for gaze stabilization in a humanoid robot. We focus on\nthe problem of compensating for disturbances induced in the cameras due to\nself-generated movements of the robot. In this work we employ two separate\nsignals for stabilization: (1) an anticipatory term obtained from the velocity\ncommands sent to the joints while the robot moves autonomously; (2) a feedback\nterm from the on board gyroscope, which compensates unpredicted external\ndisturbances. We first provide the mathematical formulation to derive the\nforward and the differential kinematics of the fixation point of the stereo\nsystem. We finally test our method on the iCub robot. We show that the\nstabilization consistently reduces the residual optical flow during the\nmovement of the robot and in presence of external disturbances. We also\ndemonstrate that proper integration of the neck DoF is crucial to achieve\ncorrect stabilization.\n", "versions": [{"version": "v1", "created": "Thu, 13 Nov 2014 12:39:42 GMT"}], "update_date": "2016-11-18", "authors_parsed": [["Roncone", "Alessandro", ""], ["Pattacini", "Ugo", ""], ["Metta", "Giorgio", ""], ["Natale", "Lorenzo", ""]]}, {"id": "1411.3815", "submitter": "Mingmin Zhao", "authors": "Mingmin Zhao, Chengxu Zhuang, Yizhou Wang, Tai Sing Lee", "title": "Predictive Encoding of Contextual Relationships for Perceptual\n  Inference, Interpolation and Prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new neurally-inspired model that can learn to encode the global\nrelationship context of visual events across time and space and to use the\ncontextual information to modulate the analysis by synthesis process in a\npredictive coding framework. The model learns latent contextual representations\nby maximizing the predictability of visual events based on local and global\ncontextual information through both top-down and bottom-up processes. In\ncontrast to standard predictive coding models, the prediction error in this\nmodel is used to update the contextual representation but does not alter the\nfeedforward input for the next layer, and is thus more consistent with\nneurophysiological observations. We establish the computational feasibility of\nthis model by demonstrating its ability in several aspects. We show that our\nmodel can outperform state-of-art performances of gated Boltzmann machines\n(GBM) in estimation of contextual information. Our model can also interpolate\nmissing events or predict future events in image sequences while simultaneously\nestimating contextual information. We show it achieves state-of-art\nperformances in terms of prediction accuracy in a variety of tasks and\npossesses the ability to interpolate missing frames, a function that is lacking\nin GBM.\n", "versions": [{"version": "v1", "created": "Fri, 14 Nov 2014 07:38:45 GMT"}, {"version": "v2", "created": "Sun, 21 Dec 2014 00:08:05 GMT"}, {"version": "v3", "created": "Wed, 24 Dec 2014 12:05:56 GMT"}, {"version": "v4", "created": "Mon, 2 Mar 2015 12:50:42 GMT"}, {"version": "v5", "created": "Fri, 10 Apr 2015 17:52:12 GMT"}, {"version": "v6", "created": "Thu, 16 Apr 2015 15:57:36 GMT"}], "update_date": "2015-04-17", "authors_parsed": [["Zhao", "Mingmin", ""], ["Zhuang", "Chengxu", ""], ["Wang", "Yizhou", ""], ["Lee", "Tai Sing", ""]]}, {"id": "1411.4005", "submitter": "Miguel Sim\\~oes", "authors": "Miguel Sim\\~oes, Jos\\'e Bioucas-Dias, Luis B. Almeida, Jocelyn\n  Chanussot", "title": "A convex formulation for hyperspectral image superresolution via\n  subspace-based regularization", "comments": "IEEE Trans. Geosci. Remote Sens., to be published", "journal-ref": null, "doi": "10.1109/TGRS.2014.2375320", "report-no": null, "categories": "cs.CV physics.data-an stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hyperspectral remote sensing images (HSIs) usually have high spectral\nresolution and low spatial resolution. Conversely, multispectral images (MSIs)\nusually have low spectral and high spatial resolutions. The problem of\ninferring images which combine the high spectral and high spatial resolutions\nof HSIs and MSIs, respectively, is a data fusion problem that has been the\nfocus of recent active research due to the increasing availability of HSIs and\nMSIs retrieved from the same geographical area.\n  We formulate this problem as the minimization of a convex objective function\ncontaining two quadratic data-fitting terms and an edge-preserving regularizer.\nThe data-fitting terms account for blur, different resolutions, and additive\nnoise. The regularizer, a form of vector Total Variation, promotes\npiecewise-smooth solutions with discontinuities aligned across the\nhyperspectral bands.\n  The downsampling operator accounting for the different spatial resolutions,\nthe non-quadratic and non-smooth nature of the regularizer, and the very large\nsize of the HSI to be estimated lead to a hard optimization problem. We deal\nwith these difficulties by exploiting the fact that HSIs generally \"live\" in a\nlow-dimensional subspace and by tailoring the Split Augmented Lagrangian\nShrinkage Algorithm (SALSA), which is an instance of the Alternating Direction\nMethod of Multipliers (ADMM), to this optimization problem, by means of a\nconvenient variable splitting. The spatial blur and the spectral linear\noperators linked, respectively, with the HSI and MSI acquisition processes are\nalso estimated, and we obtain an effective algorithm that outperforms the\nstate-of-the-art, as illustrated in a series of experiments with simulated and\nreal-life data.\n", "versions": [{"version": "v1", "created": "Fri, 14 Nov 2014 18:36:31 GMT"}], "update_date": "2015-06-23", "authors_parsed": [["Sim\u00f5es", "Miguel", ""], ["Bioucas-Dias", "Jos\u00e9", ""], ["Almeida", "Luis B.", ""], ["Chanussot", "Jocelyn", ""]]}, {"id": "1411.4006", "submitter": "Zhongwen Xu", "authors": "Zhongwen Xu, Yi Yang and Alexander G. Hauptmann", "title": "A Discriminative CNN Video Representation for Event Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a discriminative video representation for event\ndetection over a large scale video dataset when only limited hardware resources\nare available. The focus of this paper is to effectively leverage deep\nConvolutional Neural Networks (CNNs) to advance event detection, where only\nframe level static descriptors can be extracted by the existing CNN toolkit.\nThis paper makes two contributions to the inference of CNN video\nrepresentation. First, while average pooling and max pooling have long been the\nstandard approaches to aggregating frame level static features, we show that\nperformance can be significantly improved by taking advantage of an appropriate\nencoding method. Second, we propose using a set of latent concept descriptors\nas the frame descriptor, which enriches visual information while keeping it\ncomputationally affordable. The integration of the two contributions results in\na new state-of-the-art performance in event detection over the largest video\ndatasets. Compared to improved Dense Trajectories, which has been recognized as\nthe best video representation for event detection, our new representation\nimproves the Mean Average Precision (mAP) from 27.6% to 36.8% for the TRECVID\nMEDTest 14 dataset and from 34.0% to 44.6% for the TRECVID MEDTest 13 dataset.\nThis work is the core part of the winning solution of our CMU-Informedia team\nin TRECVID MED 2014 competition.\n", "versions": [{"version": "v1", "created": "Fri, 14 Nov 2014 18:37:31 GMT"}], "update_date": "2014-11-17", "authors_parsed": [["Xu", "Zhongwen", ""], ["Yang", "Yi", ""], ["Hauptmann", "Alexander G.", ""]]}, {"id": "1411.4033", "submitter": "Ahmadreza Baghaie", "authors": "Ahmadreza Baghaie, Roshan M. D'souza, Zeyun Yu", "title": "Sparse And Low Rank Decomposition Based Batch Image Alignment for\n  Speckle Reduction of retinal OCT Images", "comments": "Accepted for presentation at ISBI'15", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Optical Coherence Tomography (OCT) is an emerging technique in the field of\nbiomedical imaging, with applications in ophthalmology, dermatology, coronary\nimaging etc. Due to the underlying physics, OCT images usually suffer from a\ngranular pattern, called speckle noise, which restricts the process of\ninterpretation. Here, a sparse and low rank decomposition based method is used\nfor speckle reduction in retinal OCT images. This technique works on input data\nthat consists of several B-scans of the same location. The next step is the\nbatch alignment of the images using a sparse and low-rank decomposition based\ntechnique. Finally the denoised image is created by median filtering of the\nlow-rank component of the processed data. Simultaneous decomposition and\nalignment of the images result in better performance in comparison to simple\nregistration-based methods that are used in the literature for noise reduction\nof OCT images.\n", "versions": [{"version": "v1", "created": "Fri, 14 Nov 2014 20:24:31 GMT"}, {"version": "v2", "created": "Sun, 23 Nov 2014 20:32:48 GMT"}, {"version": "v3", "created": "Mon, 9 Feb 2015 01:58:07 GMT"}], "update_date": "2015-02-10", "authors_parsed": [["Baghaie", "Ahmadreza", ""], ["D'souza", "Roshan M.", ""], ["Yu", "Zeyun", ""]]}, {"id": "1411.4038", "submitter": "Jonathan Long", "authors": "Jonathan Long, Evan Shelhamer, Trevor Darrell", "title": "Fully Convolutional Networks for Semantic Segmentation", "comments": "to appear in CVPR (2015)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional networks are powerful visual models that yield hierarchies of\nfeatures. We show that convolutional networks by themselves, trained\nend-to-end, pixels-to-pixels, exceed the state-of-the-art in semantic\nsegmentation. Our key insight is to build \"fully convolutional\" networks that\ntake input of arbitrary size and produce correspondingly-sized output with\nefficient inference and learning. We define and detail the space of fully\nconvolutional networks, explain their application to spatially dense prediction\ntasks, and draw connections to prior models. We adapt contemporary\nclassification networks (AlexNet, the VGG net, and GoogLeNet) into fully\nconvolutional networks and transfer their learned representations by\nfine-tuning to the segmentation task. We then define a novel architecture that\ncombines semantic information from a deep, coarse layer with appearance\ninformation from a shallow, fine layer to produce accurate and detailed\nsegmentations. Our fully convolutional network achieves state-of-the-art\nsegmentation of PASCAL VOC (20% relative improvement to 62.2% mean IU on 2012),\nNYUDv2, and SIFT Flow, while inference takes one third of a second for a\ntypical image.\n", "versions": [{"version": "v1", "created": "Fri, 14 Nov 2014 20:46:03 GMT"}, {"version": "v2", "created": "Sun, 8 Mar 2015 22:16:40 GMT"}], "update_date": "2015-03-10", "authors_parsed": [["Long", "Jonathan", ""], ["Shelhamer", "Evan", ""], ["Darrell", "Trevor", ""]]}, {"id": "1411.4046", "submitter": "Mohammad Ali Keyvanrad", "authors": "Mohammad Ali Keyvanrad, Mohammad Mehdi Homayounpour", "title": "Deep Belief Network Training Improvement Using Elite Samples Minimizing\n  Free Energy", "comments": "18 pages. arXiv admin note: substantial text overlap with\n  arXiv:1408.3264", "journal-ref": "Int. J. Patt. Recogn. Artif. Intell. 29, 1551006 (2015)", "doi": "10.1142/S0218001415510064", "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nowadays this is very popular to use deep architectures in machine learning.\nDeep Belief Networks (DBNs) are deep architectures that use stack of Restricted\nBoltzmann Machines (RBM) to create a powerful generative model using training\ndata. In this paper we present an improvement in a common method that is\nusually used in training of RBMs. The new method uses free energy as a\ncriterion to obtain elite samples from generative model. We argue that these\nsamples can more accurately compute gradient of log probability of training\ndata. According to the results, an error rate of 0.99% was achieved on MNIST\ntest set. This result shows that the proposed method outperforms the method\npresented in the first paper introducing DBN (1.25% error rate) and general\nclassification methods such as SVM (1.4% error rate) and KNN (with 1.6% error\nrate). In another test using ISOLET dataset, letter classification error\ndropped to 3.59% compared to 5.59% error rate achieved in those papers using\nthis dataset. The implemented method is available online at\n\"http://ceit.aut.ac.ir/~keyvanrad/DeeBNet Toolbox.html\".\n", "versions": [{"version": "v1", "created": "Fri, 14 Nov 2014 16:57:48 GMT"}], "update_date": "2015-08-21", "authors_parsed": [["Keyvanrad", "Mohammad Ali", ""], ["Homayounpour", "Mohammad Mehdi", ""]]}, {"id": "1411.4064", "submitter": "Haonan Yu", "authors": "Haonan Yu and Daniel P. Barrett and Jeffrey Mark Siskind", "title": "A Faster Method for Tracking and Scoring Videos Corresponding to\n  Sentences", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Prior work presented the sentence tracker, a method for scoring how well a\nsentence describes a video clip or alternatively how well a video clip depicts\na sentence. We present an improved method for optimizing the same cost function\nemployed by this prior work, reducing the space complexity from exponential in\nthe sentence length to polynomial, as well as producing a qualitatively\nidentical result in time polynomial in the sentence length instead of\nexponential. Since this new method is plug-compatible with the prior method, it\ncan be used for the same applications: video retrieval with sentential queries,\ngenerating sentential descriptions of video clips, and focusing the attention\nof a tracker with a sentence, while allowing these applications to scale with\nsignificantly larger numbers of object detections, word meanings modeled with\nHMMs with significantly larger numbers of states, and significantly longer\nsentences, with no appreciable degradation in quality of results.\n", "versions": [{"version": "v1", "created": "Fri, 14 Nov 2014 21:40:37 GMT"}], "update_date": "2014-11-18", "authors_parsed": [["Yu", "Haonan", ""], ["Barrett", "Daniel P.", ""], ["Siskind", "Jeffrey Mark", ""]]}, {"id": "1411.4080", "submitter": "Miriam Redi", "authors": "Miriam Redi, Neil O Hare, Rossano Schifanella, Michele Trevisiol,\n  Alejandro Jaimes", "title": "6 Seconds of Sound and Vision: Creativity in Micro-Videos", "comments": "8 pages, 1 figures, conference IEEE CVPR 2014", "journal-ref": null, "doi": "10.1109/CVPR.2014.544", "report-no": null, "categories": "cs.MM cs.CV cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The notion of creativity, as opposed to related concepts such as beauty or\ninterestingness, has not been studied from the perspective of automatic\nanalysis of multimedia content. Meanwhile, short online videos shared on social\nmedia platforms, or micro-videos, have arisen as a new medium for creative\nexpression. In this paper we study creative micro-videos in an effort to\nunderstand the features that make a video creative, and to address the problem\nof automatic detection of creative content. Defining creative videos as those\nthat are novel and have aesthetic value, we conduct a crowdsourcing experiment\nto create a dataset of over 3,800 micro-videos labelled as creative and\nnon-creative. We propose a set of computational features that we map to the\ncomponents of our definition of creativity, and conduct an analysis to\ndetermine which of these features correlate most with creative video. Finally,\nwe evaluate a supervised approach to automatically detect creative video, with\npromising results, showing that it is necessary to model both aesthetic value\nand novelty to achieve optimal classification accuracy.\n", "versions": [{"version": "v1", "created": "Fri, 14 Nov 2014 23:29:18 GMT"}], "update_date": "2014-11-18", "authors_parsed": [["Redi", "Miriam", ""], ["Hare", "Neil O", ""], ["Schifanella", "Rossano", ""], ["Trevisiol", "Michele", ""], ["Jaimes", "Alejandro", ""]]}, {"id": "1411.4098", "submitter": "Rahul Sawhney", "authors": "Rahul Sawhney, Fuxin Li and Henrik I. Christensen", "title": "GASP : Geometric Association with Surface Patches", "comments": "International Conference on 3D Vision, 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A fundamental challenge to sensory processing tasks in perception and\nrobotics is the problem of obtaining data associations across views. We present\na robust solution for ascertaining potentially dense surface patch (superpixel)\nassociations, requiring just range information. Our approach involves\ndecomposition of a view into regularized surface patches. We represent them as\nsequences expressing geometry invariantly over their superpixel neighborhoods,\nas uniquely consistent partial orderings. We match these representations\nthrough an optimal sequence comparison metric based on the Damerau-Levenshtein\ndistance - enabling robust association with quadratic complexity (in contrast\nto hitherto employed joint matching formulations which are NP-complete). The\napproach is able to perform under wide baselines, heavy rotations, partial\noverlaps, significant occlusions and sensor noise.\n  The technique does not require any priors -- motion or otherwise, and does\nnot make restrictive assumptions on scene structure and sensor movement. It\ndoes not require appearance -- is hence more widely applicable than appearance\nreliant methods, and invulnerable to related ambiguities such as textureless or\naliased content. We present promising qualitative and quantitative results\nunder diverse settings, along with comparatives with popular approaches based\non range as well as RGB-D data.\n", "versions": [{"version": "v1", "created": "Sat, 15 Nov 2014 01:31:55 GMT"}], "update_date": "2014-11-18", "authors_parsed": [["Sawhney", "Rahul", ""], ["Li", "Fuxin", ""], ["Christensen", "Henrik I.", ""]]}, {"id": "1411.4101", "submitter": "Rahul Mohan Mr.", "authors": "Rahul Mohan", "title": "Deep Deconvolutional Networks for Scene Parsing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  Scene parsing is an important and challenging prob- lem in computer vision.\nIt requires labeling each pixel in an image with the category it belongs to.\nTradition- ally, it has been approached with hand-engineered features from\ncolor information in images. Recently convolutional neural networks (CNNs),\nwhich automatically learn hierar- chies of features, have achieved record\nperformance on the task. These approaches typically include a post-processing\ntechnique, such as superpixels, to produce the final label- ing. In this paper,\nwe propose a novel network architecture that combines deep deconvolutional\nneural networks with CNNs. Our experiments show that deconvolutional neu- ral\nnetworks are capable of learning higher order image structure beyond edge\nprimitives in comparison to CNNs. The new network architecture is employed for\nmulti-patch training, introduced as part of this work. Multi-patch train- ing\nmakes it possible to effectively learn spatial priors from scenes. The proposed\napproach yields state-of-the-art per- formance on four scene parsing datasets,\nnamely Stanford Background, SIFT Flow, CamVid, and KITTI. In addition, our\nsystem has the added advantage of having a training system that can be\ncompletely automated end-to-end with- out requiring any post-processing.\n", "versions": [{"version": "v1", "created": "Sat, 15 Nov 2014 02:03:14 GMT"}], "update_date": "2014-11-18", "authors_parsed": [["Mohan", "Rahul", ""]]}, {"id": "1411.4102", "submitter": "Rahul Sawhney", "authors": "Rahul Sawhney, Henrik I. Christensen and Gary R. Bradski", "title": "Anisotropic Agglomerative Adaptive Mean-Shift", "comments": "British Machine Vision Conference, 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mean Shift today, is widely used for mode detection and clustering. The\ntechnique though, is challenged in practice due to assumptions of isotropicity\nand homoscedasticity. We present an adaptive Mean Shift methodology that allows\nfor full anisotropic clustering, through unsupervised local bandwidth\nselection. The bandwidth matrices evolve naturally, adapting locally through\nagglomeration, and in turn guiding further agglomeration. The online\nmethodology is practical and effecive for low-dimensional feature spaces,\npreserving better detail and clustering salience. Additionally, conventional\nMean Shift either critically depends on a per instance choice of bandwidth, or\nrelies on offline methods which are inflexible and/or again data instance\nspecific. The presented approach, due to its adaptive design, also alleviates\nthis issue - with a default form performing generally well. The methodology\nthough, allows for effective tuning of results.\n", "versions": [{"version": "v1", "created": "Sat, 15 Nov 2014 02:05:22 GMT"}], "update_date": "2014-11-18", "authors_parsed": [["Sawhney", "Rahul", ""], ["Christensen", "Henrik I.", ""], ["Bradski", "Gary R.", ""]]}, {"id": "1411.4114", "submitter": "Jongwon Ha", "authors": "Ha Jong Won, Li Gwang Chol, Kim Hyok Chol, Li Kum Song (College of\n  Computer Science, Kim Il Sung University)", "title": "Definition of Visual Speech Element and Research on a Method of\n  Extracting Feature Vector for Korean Lip-Reading", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV cs.LG", "license": "http://creativecommons.org/licenses/publicdomain/", "abstract": "  In this paper, we defined the viseme (visual speech element) and described\nabout the method of extracting visual feature vector. We defined the 10 visemes\nbased on vowel by analyzing of Korean utterance and proposed the method of\nextracting the 20-dimensional visual feature vector, combination of static\nfeatures and dynamic features. Lastly, we took an experiment in recognizing\nwords based on 3-viseme HMM and evaluated the efficiency.\n", "versions": [{"version": "v1", "created": "Sat, 15 Nov 2014 05:44:10 GMT"}], "update_date": "2014-11-19", "authors_parsed": [["Won", "Ha Jong", "", "College of\n  Computer Science, Kim Il Sung University"], ["Chol", "Li Gwang", "", "College of\n  Computer Science, Kim Il Sung University"], ["Chol", "Kim Hyok", "", "College of\n  Computer Science, Kim Il Sung University"], ["Song", "Li Kum", "", "College of\n  Computer Science, Kim Il Sung University"]]}, {"id": "1411.4199", "submitter": "Ke Jiang", "authors": "Ke Jiang, Qichao Que, Brian Kulis", "title": "Revisiting Kernelized Locality-Sensitive Hashing for Improved\n  Large-Scale Image Retrieval", "comments": "15 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a simple but powerful reinterpretation of kernelized\nlocality-sensitive hashing (KLSH), a general and popular method developed in\nthe vision community for performing approximate nearest-neighbor searches in an\narbitrary reproducing kernel Hilbert space (RKHS). Our new perspective is based\non viewing the steps of the KLSH algorithm in an appropriately projected space,\nand has several key theoretical and practical benefits. First, it eliminates\nthe problematic conceptual difficulties that are present in the existing\nmotivation of KLSH. Second, it yields the first formal retrieval performance\nbounds for KLSH. Third, our analysis reveals two techniques for boosting the\nempirical performance of KLSH. We evaluate these extensions on several\nlarge-scale benchmark image retrieval data sets, and show that our analysis\nleads to improved recall performance of at least 12%, and sometimes much\nhigher, over the standard KLSH method.\n", "versions": [{"version": "v1", "created": "Sun, 16 Nov 2014 00:08:24 GMT"}], "update_date": "2014-11-18", "authors_parsed": [["Jiang", "Ke", ""], ["Que", "Qichao", ""], ["Kulis", "Brian", ""]]}, {"id": "1411.4229", "submitter": "Kaiming He", "authors": "Xiangyu Zhang, Jianhua Zou, Xiang Ming, Kaiming He, Jian Sun", "title": "Efficient and Accurate Approximations of Nonlinear Convolutional\n  Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper aims to accelerate the test-time computation of deep convolutional\nneural networks (CNNs). Unlike existing methods that are designed for\napproximating linear filters or linear responses, our method takes the\nnonlinear units into account. We minimize the reconstruction error of the\nnonlinear responses, subject to a low-rank constraint which helps to reduce the\ncomplexity of filters. We develop an effective solution to this constrained\nnonlinear optimization problem. An algorithm is also presented for reducing the\naccumulated error when multiple layers are approximated. A whole-model speedup\nratio of 4x is demonstrated on a large network trained for ImageNet, while the\ntop-5 error rate is only increased by 0.9%. Our accelerated model has a\ncomparably fast speed as the \"AlexNet\", but is 4.7% more accurate.\n", "versions": [{"version": "v1", "created": "Sun, 16 Nov 2014 08:37:25 GMT"}], "update_date": "2014-11-18", "authors_parsed": [["Zhang", "Xiangyu", ""], ["Zou", "Jianhua", ""], ["Ming", "Xiang", ""], ["He", "Kaiming", ""], ["Sun", "Jian", ""]]}, {"id": "1411.4280", "submitter": "Jonathan Tompson", "authors": "Jonathan Tompson, Ross Goroshin, Arjun Jain, Yann LeCun, Christopher\n  Bregler", "title": "Efficient Object Localization Using Convolutional Networks", "comments": "8 pages with 1 page of citations", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent state-of-the-art performance on human-body pose estimation has been\nachieved with Deep Convolutional Networks (ConvNets). Traditional ConvNet\narchitectures include pooling and sub-sampling layers which reduce\ncomputational requirements, introduce invariance and prevent over-training.\nThese benefits of pooling come at the cost of reduced localization accuracy. We\nintroduce a novel architecture which includes an efficient `position\nrefinement' model that is trained to estimate the joint offset location within\na small region of the image. This refinement model is jointly trained in\ncascade with a state-of-the-art ConvNet model to achieve improved accuracy in\nhuman joint location estimation. We show that the variance of our detector\napproaches the variance of human annotations on the FLIC dataset and\noutperforms all existing approaches on the MPII-human-pose dataset.\n", "versions": [{"version": "v1", "created": "Sun, 16 Nov 2014 17:23:02 GMT"}, {"version": "v2", "created": "Mon, 20 Apr 2015 16:55:05 GMT"}, {"version": "v3", "created": "Tue, 9 Jun 2015 12:29:21 GMT"}], "update_date": "2015-06-10", "authors_parsed": [["Tompson", "Jonathan", ""], ["Goroshin", "Ross", ""], ["Jain", "Arjun", ""], ["LeCun", "Yann", ""], ["Bregler", "Christopher", ""]]}, {"id": "1411.4296", "submitter": "Rui Guerreiro", "authors": "Rui F. C. Guerreiro", "title": "Combining contextual and local edges for line segment extraction in\n  cluttered images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic extraction methods typically assume that line segments are\npronounced, thin, few and far between, do not cross each other, and are noise\nand clutter-free. Since these assumptions often fail in realistic scenarios,\nmany line segments are not detected or are fragmented. In more severe cases,\ni.e., many who use the Hough Transform, extraction can fail entirely. In this\npaper, we propose a method that tackles these issues. Its key aspect is the\ncombination of thresholded image derivatives obtained with filters of large and\nsmall footprints, which we denote as contextual and local edges, respectively.\nContextual edges are robust to noise and we use them to select valid local\nedges, i.e., local edges that are of the same type as contextual ones:\ndark-to-bright transition of vice-versa. If the distance between valid local\nedges does not exceed a maximum distance threshold, we enforce connectivity by\nmarking them and the pixels in between as edge points. This originates\nconnected edge maps that are robust and well localized. We use a powerful\ntwo-sample statistical test to compute contextual edges, which we introduce\nbriefly, as they are unfamiliar to the image processing community. Finally, we\npresent experiments that illustrate, with synthetic and real images, how our\nmethod is efficient in extracting complete segments of all lengths and widths\nin several situations where current methods fail.\n", "versions": [{"version": "v1", "created": "Sun, 16 Nov 2014 19:40:32 GMT"}], "update_date": "2014-11-18", "authors_parsed": [["Guerreiro", "Rui F. C.", ""]]}, {"id": "1411.4304", "submitter": "Rodrigo Benenson", "authors": "Rodrigo Benenson, Mohamed Omran, Jan Hosang, Bernt Schiele", "title": "Ten Years of Pedestrian Detection, What Have We Learned?", "comments": "To appear in ECCV 2014 CVRSUAD workshop proceedings", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Paper-by-paper results make it easy to miss the forest for the trees.We\nanalyse the remarkable progress of the last decade by discussing the main ideas\nexplored in the 40+ detectors currently present in the Caltech pedestrian\ndetection benchmark. We observe that there exist three families of approaches,\nall currently reaching similar detection quality. Based on our analysis, we\nstudy the complementarity of the most promising ideas by combining multiple\npublished strategies. This new decision forest detector achieves the current\nbest known performance on the challenging Caltech-USA dataset.\n", "versions": [{"version": "v1", "created": "Sun, 16 Nov 2014 21:25:53 GMT"}], "update_date": "2014-11-18", "authors_parsed": [["Benenson", "Rodrigo", ""], ["Omran", "Mohamed", ""], ["Hosang", "Jan", ""], ["Schiele", "Bernt", ""]]}, {"id": "1411.4331", "submitter": "Jie Shen", "authors": "Weipeng Zhang, Jie Shen, Guangcan Liu, Yong Yu", "title": "A Latent Clothing Attribute Approach for Human Pose Estimation", "comments": "accepted to ACCV 2014, preceding work http://arxiv.org/abs/1404.4923", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As a fundamental technique that concerns several vision tasks such as image\nparsing, action recognition and clothing retrieval, human pose estimation (HPE)\nhas been extensively investigated in recent years. To achieve accurate and\nreliable estimation of the human pose, it is well-recognized that the clothing\nattributes are useful and should be utilized properly. Most previous\napproaches, however, require to manually annotate the clothing attributes and\nare therefore very costly. In this paper, we shall propose and explore a\n\\emph{latent} clothing attribute approach for HPE. Unlike previous approaches,\nour approach models the clothing attributes as latent variables and thus\nrequires no explicit labeling for the clothing attributes. The inference of the\nlatent variables are accomplished by utilizing the framework of latent\nstructured support vector machines (LSSVM). We employ the strategy of\n\\emph{alternating direction} to train the LSSVM model: In each iteration, one\nkind of variables (e.g., human pose or clothing attribute) are fixed and the\nothers are optimized. Our extensive experiments on two real-world benchmarks\nshow the state-of-the-art performance of our proposed approach.\n", "versions": [{"version": "v1", "created": "Sun, 16 Nov 2014 23:47:59 GMT"}], "update_date": "2014-11-18", "authors_parsed": [["Zhang", "Weipeng", ""], ["Shen", "Jie", ""], ["Liu", "Guangcan", ""], ["Yu", "Yong", ""]]}, {"id": "1411.4389", "submitter": "Jeff Donahue", "authors": "Jeff Donahue, Lisa Anne Hendricks, Marcus Rohrbach, Subhashini\n  Venugopalan, Sergio Guadarrama, Kate Saenko, Trevor Darrell", "title": "Long-term Recurrent Convolutional Networks for Visual Recognition and\n  Description", "comments": "Originally presented at CVPR 2015 (oral). Updated version (accepted\n  as a TPAMI journal article) includes additional results", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Models based on deep convolutional networks have dominated recent image\ninterpretation tasks; we investigate whether models which are also recurrent,\nor \"temporally deep\", are effective for tasks involving sequences, visual and\notherwise. We develop a novel recurrent convolutional architecture suitable for\nlarge-scale visual learning which is end-to-end trainable, and demonstrate the\nvalue of these models on benchmark video recognition tasks, image description\nand retrieval problems, and video narration challenges. In contrast to current\nmodels which assume a fixed spatio-temporal receptive field or simple temporal\naveraging for sequential processing, recurrent convolutional models are \"doubly\ndeep\"' in that they can be compositional in spatial and temporal \"layers\". Such\nmodels may have advantages when target concepts are complex and/or training\ndata are limited. Learning long-term dependencies is possible when\nnonlinearities are incorporated into the network state updates. Long-term RNN\nmodels are appealing in that they directly can map variable-length inputs\n(e.g., video frames) to variable length outputs (e.g., natural language text)\nand can model complex temporal dynamics; yet they can be optimized with\nbackpropagation. Our recurrent long-term models are directly connected to\nmodern visual convnet models and can be jointly trained to simultaneously learn\ntemporal dynamics and convolutional perceptual representations. Our results\nshow such models have distinct advantages over state-of-the-art models for\nrecognition or generation which are separately defined and/or optimized.\n", "versions": [{"version": "v1", "created": "Mon, 17 Nov 2014 08:25:17 GMT"}, {"version": "v2", "created": "Tue, 18 Nov 2014 07:37:44 GMT"}, {"version": "v3", "created": "Tue, 17 Feb 2015 23:59:08 GMT"}, {"version": "v4", "created": "Tue, 31 May 2016 22:57:33 GMT"}], "update_date": "2016-06-02", "authors_parsed": [["Donahue", "Jeff", ""], ["Hendricks", "Lisa Anne", ""], ["Rohrbach", "Marcus", ""], ["Venugopalan", "Subhashini", ""], ["Guadarrama", "Sergio", ""], ["Saenko", "Kate", ""], ["Darrell", "Trevor", ""]]}, {"id": "1411.4419", "submitter": "Xi Peng", "authors": "Xi Peng, Jiwen Lu, Zhang Yi, Rui Yan", "title": "Automatic Subspace Learning via Principal Coefficients Embedding", "comments": "IEEE Trans. on Cybernetics, 2016", "journal-ref": "IEEE Trans. on Cybernetics, 2016", "doi": "10.1109/TCYB.2016.2572306.", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we address two challenging problems in unsupervised subspace\nlearning: 1) how to automatically identify the feature dimension of the learned\nsubspace (i.e., automatic subspace learning), and 2) how to learn the\nunderlying subspace in the presence of Gaussian noise (i.e., robust subspace\nlearning). We show that these two problems can be simultaneously solved by\nproposing a new method (called principal coefficients embedding, PCE). For a\ngiven data set $\\mathbf{D}\\in \\mathds{R}^{m\\times n}$, PCE recovers a clean\ndata set $\\mathbf{D}_{0}\\in \\mathds{R}^{m\\times n}$ from $\\mathbf{D}$ and\nsimultaneously learns a global reconstruction relation $\\mathbf{C}\\in\n\\mathbf{R}^{n\\times n}$ of $\\mathbf{D}_{0}$. By preserving $\\mathbf{C}$ into an\n$m^{\\prime}$-dimensional space, the proposed method obtains a projection matrix\nthat can capture the latent manifold structure of $\\mathbf{D}_{0}$, where\n$m^{\\prime}\\ll m$ is automatically determined by the rank of $\\mathbf{C}$ with\ntheoretical guarantees. PCE has three advantages: 1) it can automatically\ndetermine the feature dimension even though data are sampled from a union of\nmultiple linear subspaces in presence of the Gaussian noise, 2) Although the\nobjective function of PCE only considers the Gaussian noise, experimental\nresults show that it is robust to the non-Gaussian noise (\\textit{e.g.}, random\npixel corruption) and real disguises, 3) Our method has a closed-form solution\nand can be calculated very fast. Extensive experimental results show the\nsuperiority of PCE on a range of databases with respect to the classification\naccuracy, robustness and efficiency.\n", "versions": [{"version": "v1", "created": "Mon, 17 Nov 2014 10:23:26 GMT"}, {"version": "v2", "created": "Wed, 13 May 2015 05:29:02 GMT"}, {"version": "v3", "created": "Fri, 15 May 2015 05:43:50 GMT"}, {"version": "v4", "created": "Sun, 29 May 2016 06:53:33 GMT"}, {"version": "v5", "created": "Mon, 17 Oct 2016 03:17:24 GMT"}], "update_date": "2017-05-17", "authors_parsed": [["Peng", "Xi", ""], ["Lu", "Jiwen", ""], ["Yi", "Zhang", ""], ["Yan", "Rui", ""]]}, {"id": "1411.4423", "submitter": "Sotirios Chatzis", "authors": "Sotirios P. Chatzis", "title": "A Nonparametric Bayesian Approach Toward Stacked Convolutional\n  Independent Component Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unsupervised feature learning algorithms based on convolutional formulations\nof independent components analysis (ICA) have been demonstrated to yield\nstate-of-the-art results in several action recognition benchmarks. However,\nexisting approaches do not allow for the number of latent components (features)\nto be automatically inferred from the data in an unsupervised manner. This is a\nsignificant disadvantage of the state-of-the-art, as it results in considerable\nburden imposed on researchers and practitioners, who must resort to tedious\ncross-validation procedures to obtain the optimal number of latent features. To\nresolve these issues, in this paper we introduce a convolutional nonparametric\nBayesian sparse ICA architecture for overcomplete feature learning from\nhigh-dimensional data. Our method utilizes an Indian buffet process prior to\nfacilitate inference of the appropriate number of latent features under a\nhybrid variational inference algorithm, scalable to massive datasets. As we\nshow, our model can be naturally used to obtain deep unsupervised hierarchical\nfeature extractors, by greedily stacking successive model layers, similar to\nexisting approaches. In addition, inference for this model is completely\nheuristics-free; thus, it obviates the need of tedious parameter tuning, which\nis a major challenge most deep learning approaches are faced with. We evaluate\nour method on several action recognition benchmarks, and exhibit its advantages\nover the state-of-the-art.\n", "versions": [{"version": "v1", "created": "Mon, 17 Nov 2014 10:35:09 GMT"}, {"version": "v2", "created": "Mon, 31 Aug 2015 07:40:21 GMT"}, {"version": "v3", "created": "Tue, 1 Sep 2015 12:03:38 GMT"}, {"version": "v4", "created": "Thu, 3 Sep 2015 10:32:05 GMT"}, {"version": "v5", "created": "Wed, 23 Sep 2015 20:22:53 GMT"}], "update_date": "2015-09-25", "authors_parsed": [["Chatzis", "Sotirios P.", ""]]}, {"id": "1411.4464", "submitter": "Kai Kang", "authors": "Kai Kang, Xiaogang Wang", "title": "Fully Convolutional Neural Networks for Crowd Segmentation", "comments": "9 pages,7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  In this paper, we propose a fast fully convolutional neural network (FCNN)\nfor crowd segmentation. By replacing the fully connected layers in CNN with 1\nby 1 convolution kernels, FCNN takes whole images as inputs and directly\noutputs segmentation maps by one pass of forward propagation. It has the\nproperty of translation invariance like patch-by-patch scanning but with much\nlower computation cost. Once FCNN is learned, it can process input images of\nany sizes without warping them to a standard size. These attractive properties\nmake it extendable to other general image segmentation problems. Based on FCNN,\na multi-stage deep learning is proposed to integrate appearance and motion cues\nfor crowd segmentation. Both appearance filters and motion filers are\npretrained stage-by-stage and then jointly optimized. Different combination\nmethods are investigated. The effectiveness of our approach and component-wise\nanalysis are evaluated on two crowd segmentation datasets created by us, which\ninclude image frames from 235 and 11 scenes, respectively. They are currently\nthe largest crowd segmentation datasets and will be released to the public.\n", "versions": [{"version": "v1", "created": "Mon, 17 Nov 2014 13:09:09 GMT"}], "update_date": "2014-11-18", "authors_parsed": [["Kang", "Kai", ""], ["Wang", "Xiaogang", ""]]}, {"id": "1411.4491", "submitter": "Basura Fernando", "authors": "Basura Fernando and Tatiana Tommasi and Tinne Tuytelaars", "title": "Joint cross-domain classification and subspace learning for unsupervised\n  adaptation", "comments": "Paper is under consideration at Pattern Recognition Letters", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Domain adaptation aims at adapting the knowledge acquired on a source domain\nto a new different but related target domain. Several approaches have\nbeenproposed for classification tasks in the unsupervised scenario, where no\nlabeled target data are available. Most of the attention has been dedicated to\nsearching a new domain-invariant representation, leaving the definition of the\nprediction function to a second stage. Here we propose to learn both jointly.\nSpecifically we learn the source subspace that best matches the target subspace\nwhile at the same time minimizing a regularized misclassification loss. We\nprovide an alternating optimization technique based on stochastic sub-gradient\ndescent to solve the learning problem and we demonstrate its performance on\nseveral domain adaptation tasks.\n", "versions": [{"version": "v1", "created": "Mon, 17 Nov 2014 14:29:35 GMT"}, {"version": "v2", "created": "Tue, 18 Nov 2014 15:55:50 GMT"}, {"version": "v3", "created": "Wed, 29 Apr 2015 02:51:00 GMT"}], "update_date": "2015-04-30", "authors_parsed": [["Fernando", "Basura", ""], ["Tommasi", "Tatiana", ""], ["Tuytelaars", "Tinne", ""]]}, {"id": "1411.4555", "submitter": "Samy Bengio", "authors": "Oriol Vinyals, Alexander Toshev, Samy Bengio, Dumitru Erhan", "title": "Show and Tell: A Neural Image Caption Generator", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatically describing the content of an image is a fundamental problem in\nartificial intelligence that connects computer vision and natural language\nprocessing. In this paper, we present a generative model based on a deep\nrecurrent architecture that combines recent advances in computer vision and\nmachine translation and that can be used to generate natural sentences\ndescribing an image. The model is trained to maximize the likelihood of the\ntarget description sentence given the training image. Experiments on several\ndatasets show the accuracy of the model and the fluency of the language it\nlearns solely from image descriptions. Our model is often quite accurate, which\nwe verify both qualitatively and quantitatively. For instance, while the\ncurrent state-of-the-art BLEU-1 score (the higher the better) on the Pascal\ndataset is 25, our approach yields 59, to be compared to human performance\naround 69. We also show BLEU-1 score improvements on Flickr30k, from 56 to 66,\nand on SBU, from 19 to 28. Lastly, on the newly released COCO dataset, we\nachieve a BLEU-4 of 27.7, which is the current state-of-the-art.\n", "versions": [{"version": "v1", "created": "Mon, 17 Nov 2014 17:15:41 GMT"}, {"version": "v2", "created": "Mon, 20 Apr 2015 22:26:11 GMT"}], "update_date": "2015-04-22", "authors_parsed": [["Vinyals", "Oriol", ""], ["Toshev", "Alexander", ""], ["Bengio", "Samy", ""], ["Erhan", "Dumitru", ""]]}, {"id": "1411.4568", "submitter": "Yannick Verdie", "authors": "Yannick Verdie, Kwang Moo Yi, Pascal Fua, Vincent Lepetit", "title": "TILDE: A Temporally Invariant Learned DEtector", "comments": null, "journal-ref": null, "doi": "10.1109/CVPR.2015.7299165", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a learning-based approach to detect repeatable keypoints under\ndrastic imaging changes of weather and lighting conditions to which\nstate-of-the-art keypoint detectors are surprisingly sensitive. We first\nidentify good keypoint candidates in multiple training images taken from the\nsame viewpoint. We then train a regressor to predict a score map whose maxima\nare those points so that they can be found by simple non-maximum suppression.\nAs there are no standard datasets to test the influence of these kinds of\nchanges, we created our own, which we will make publicly available. We will\nshow that our method significantly outperforms the state-of-the-art methods in\nsuch challenging conditions, while still achieving state-of-the-art performance\non the untrained standard Oxford dataset.\n", "versions": [{"version": "v1", "created": "Mon, 17 Nov 2014 17:44:21 GMT"}, {"version": "v2", "created": "Wed, 11 Feb 2015 14:22:39 GMT"}, {"version": "v3", "created": "Thu, 12 Mar 2015 20:07:01 GMT"}], "update_date": "2015-11-16", "authors_parsed": [["Verdie", "Yannick", ""], ["Yi", "Kwang Moo", ""], ["Fua", "Pascal", ""], ["Lepetit", "Vincent", ""]]}, {"id": "1411.4670", "submitter": "Mohamed Hussein", "authors": "Mohamed E. Hussein and Marwan Torki and Ahmed Elsallamy and Mahmoud\n  Fayyaz", "title": "AlexU-Word: A New Dataset for Isolated-Word Closed-Vocabulary Offline\n  Arabic Handwriting Recognition", "comments": "6 pages, 8 figure, and 6 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce the first phase of a new dataset for offline\nArabic handwriting recognition. The aim is to collect a very large dataset of\nisolated Arabic words that covers all letters of the alphabet in all possible\nshapes using a small number of simple words. The end goal is to collect a very\nlarge dataset of segmented letter images, which can be used to build and\nevaluate Arabic handwriting recognition systems that are based on segmented\nletter recognition. The current version of the dataset contains $25114$ samples\nof $109$ unique Arabic words that cover all possible shapes of all alphabet\nletters. The samples were collected from $907$ writers. In its current form,\nthe dataset can be used for the problem of closed-vocabulary word recognition.\nWe evaluated a number of window-based descriptors and classifiers on this task\nand obtained an accuracy of $92.16\\%$ using a SIFT-based descriptor and ANN.\n", "versions": [{"version": "v1", "created": "Mon, 17 Nov 2014 21:23:26 GMT"}], "update_date": "2014-11-19", "authors_parsed": [["Hussein", "Mohamed E.", ""], ["Torki", "Marwan", ""], ["Elsallamy", "Ahmed", ""], ["Fayyaz", "Mahmoud", ""]]}, {"id": "1411.4701", "submitter": "Zhiding Yu", "authors": "Zhiding Yu, Wende Zhang, B. V. K. Vijaya Kumar, Dan Levi", "title": "Structured Hough Voting for Vision-based Highway Border Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a vision-based highway border detection algorithm using structured\nHough voting. Our approach takes advantage of the geometric relationship\nbetween highway road borders and highway lane markings. It uses a strategy\nwhere a number of trained road border and lane marking detectors are triggered,\nfollowed by Hough voting to generate corresponding detection of the border and\nlane marking. Since the initially triggered detectors usually result in large\nnumber of positives, conventional frame-wise Hough voting is not able to always\ngenerate robust border and lane marking results. Therefore, we formulate this\nproblem as a joint detection-and-tracking problem under the structured Hough\nvoting model, where tracking refers to exploiting inter-frame structural\ninformation to stabilize the detection results. Both qualitative and\nquantitative evaluations show the superiority of the proposed structured Hough\nvoting model over a number of baseline methods.\n", "versions": [{"version": "v1", "created": "Tue, 18 Nov 2014 00:37:35 GMT"}], "update_date": "2014-11-19", "authors_parsed": [["Yu", "Zhiding", ""], ["Zhang", "Wende", ""], ["Kumar", "B. V. K. Vijaya", ""], ["Levi", "Dan", ""]]}, {"id": "1411.4734", "submitter": "David Eigen", "authors": "David Eigen and Rob Fergus", "title": "Predicting Depth, Surface Normals and Semantic Labels with a Common\n  Multi-Scale Convolutional Architecture", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we address three different computer vision tasks using a single\nbasic architecture: depth prediction, surface normal estimation, and semantic\nlabeling. We use a multiscale convolutional network that is able to adapt\neasily to each task using only small modifications, regressing from the input\nimage to the output map directly. Our method progressively refines predictions\nusing a sequence of scales, and captures many image details without any\nsuperpixels or low-level segmentation. We achieve state-of-the-art performance\non benchmarks for all three tasks.\n", "versions": [{"version": "v1", "created": "Tue, 18 Nov 2014 04:49:08 GMT"}, {"version": "v2", "created": "Wed, 3 Dec 2014 19:00:25 GMT"}, {"version": "v3", "created": "Fri, 30 Oct 2015 05:05:30 GMT"}, {"version": "v4", "created": "Thu, 17 Dec 2015 03:19:36 GMT"}], "update_date": "2015-12-18", "authors_parsed": [["Eigen", "David", ""], ["Fergus", "Rob", ""]]}, {"id": "1411.4894", "submitter": "Ayan Chakrabarti", "authors": "Ayan Chakrabarti, Ying Xiong, Steven J. Gortler, Todd Zickler", "title": "Low-level Vision by Consensus in a Spatial Hierarchy of Regions", "comments": "Accepted to CVPR 2015. Project page:\n  http://www.ttic.edu/chakrabarti/consensus/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a multi-scale framework for low-level vision, where the goal is\nestimating physical scene values from image data---such as depth from stereo\nimage pairs. The framework uses a dense, overlapping set of image regions at\nmultiple scales and a \"local model,\" such as a slanted-plane model for stereo\ndisparity, that is expected to be valid piecewise across the visual field.\nEstimation is cast as optimization over a dichotomous mixture of variables,\nsimultaneously determining which regions are inliers with respect to the local\nmodel (binary variables) and the correct co-ordinates in the local model space\nfor each inlying region (continuous variables). When the regions are organized\ninto a multi-scale hierarchy, optimization can occur in an efficient and\nparallel architecture, where distributed computational units iteratively\nperform calculations and share information through sparse connections between\nparents and children. The framework performs well on a standard benchmark for\nbinocular stereo, and it produces a distributional scene representation that is\nappropriate for combining with higher-level reasoning and other low-level cues.\n", "versions": [{"version": "v1", "created": "Tue, 18 Nov 2014 16:23:06 GMT"}, {"version": "v2", "created": "Tue, 14 Apr 2015 15:57:52 GMT"}], "update_date": "2015-04-15", "authors_parsed": [["Chakrabarti", "Ayan", ""], ["Xiong", "Ying", ""], ["Gortler", "Steven J.", ""], ["Zickler", "Todd", ""]]}, {"id": "1411.4952", "submitter": "Piotr Doll\\'ar", "authors": "Hao Fang and Saurabh Gupta and Forrest Iandola and Rupesh Srivastava\n  and Li Deng and Piotr Doll\\'ar and Jianfeng Gao and Xiaodong He and Margaret\n  Mitchell and John C. Platt and C. Lawrence Zitnick and Geoffrey Zweig", "title": "From Captions to Visual Concepts and Back", "comments": "version corresponding to CVPR15 paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a novel approach for automatically generating image\ndescriptions: visual detectors, language models, and multimodal similarity\nmodels learnt directly from a dataset of image captions. We use multiple\ninstance learning to train visual detectors for words that commonly occur in\ncaptions, including many different parts of speech such as nouns, verbs, and\nadjectives. The word detector outputs serve as conditional inputs to a\nmaximum-entropy language model. The language model learns from a set of over\n400,000 image descriptions to capture the statistics of word usage. We capture\nglobal semantics by re-ranking caption candidates using sentence-level features\nand a deep multimodal similarity model. Our system is state-of-the-art on the\nofficial Microsoft COCO benchmark, producing a BLEU-4 score of 29.1%. When\nhuman judges compare the system captions to ones written by other people on our\nheld-out test set, the system captions have equal or better quality 34% of the\ntime.\n", "versions": [{"version": "v1", "created": "Tue, 18 Nov 2014 18:23:45 GMT"}, {"version": "v2", "created": "Fri, 21 Nov 2014 20:19:56 GMT"}, {"version": "v3", "created": "Tue, 14 Apr 2015 18:05:07 GMT"}], "update_date": "2016-02-22", "authors_parsed": [["Fang", "Hao", ""], ["Gupta", "Saurabh", ""], ["Iandola", "Forrest", ""], ["Srivastava", "Rupesh", ""], ["Deng", "Li", ""], ["Doll\u00e1r", "Piotr", ""], ["Gao", "Jianfeng", ""], ["He", "Xiaodong", ""], ["Mitchell", "Margaret", ""], ["Platt", "John C.", ""], ["Zitnick", "C. Lawrence", ""], ["Zweig", "Geoffrey", ""]]}, {"id": "1411.4958", "submitter": "Xiaolong Wang", "authors": "Xiaolong Wang, David F. Fouhey, Abhinav Gupta", "title": "Designing Deep Networks for Surface Normal Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the past few years, convolutional neural nets (CNN) have shown incredible\npromise for learning visual representations. In this paper, we use CNNs for the\ntask of predicting surface normals from a single image. But what is the right\narchitecture we should use? We propose to build upon the decades of hard work\nin 3D scene understanding, to design new CNN architecture for the task of\nsurface normal estimation. We show by incorporating several constraints\n(man-made, manhattan world) and meaningful intermediate representations (room\nlayout, edge labels) in the architecture leads to state of the art performance\non surface normal estimation. We also show that our network is quite robust and\nshow state of the art results on other datasets as well without any\nfine-tuning.\n", "versions": [{"version": "v1", "created": "Tue, 18 Nov 2014 18:39:48 GMT"}], "update_date": "2014-11-19", "authors_parsed": [["Wang", "Xiaolong", ""], ["Fouhey", "David F.", ""], ["Gupta", "Abhinav", ""]]}, {"id": "1411.5057", "submitter": "Chen Chen", "authors": "Chen Chen, Junzhou Huang, Lei He, Hongsheng Li", "title": "Fast Iteratively Reweighted Least Squares Algorithms for Analysis-Based\n  Sparsity Reconstruction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a novel algorithm for analysis-based sparsity\nreconstruction. It can solve the generalized problem by structured sparsity\nregularization with an orthogonal basis and total variation regularization. The\nproposed algorithm is based on the iterative reweighted least squares (IRLS)\nmodel, which is further accelerated by the preconditioned conjugate gradient\nmethod. The convergence rate of the proposed algorithm is almost the same as\nthat of the traditional IRLS algorithms, that is, exponentially fast. Moreover,\nwith the specifically devised preconditioner, the computational cost for each\niteration is significantly less than that of traditional IRLS algorithms, which\nenables our approach to handle large scale problems. In addition to the fast\nconvergence, it is straightforward to apply our method to standard sparsity,\ngroup sparsity, overlapping group sparsity and TV based problems. Experiments\nare conducted on a practical application: compressive sensing magnetic\nresonance imaging. Extensive results demonstrate that the proposed algorithm\nachieves superior performance over 14 state-of-the-art algorithms in terms of\nboth accuracy and computational cost.\n", "versions": [{"version": "v1", "created": "Tue, 18 Nov 2014 22:53:09 GMT"}, {"version": "v2", "created": "Fri, 20 Mar 2015 01:45:51 GMT"}, {"version": "v3", "created": "Tue, 28 Apr 2015 19:43:12 GMT"}], "update_date": "2015-04-29", "authors_parsed": [["Chen", "Chen", ""], ["Huang", "Junzhou", ""], ["He", "Lei", ""], ["Li", "Hongsheng", ""]]}, {"id": "1411.5065", "submitter": "Chen Chen", "authors": "Chen Chen, Yeqing Li, Wei Liu, and Junzhou Huang", "title": "SIRF: Simultaneous Image Registration and Fusion in A Unified Framework", "comments": null, "journal-ref": null, "doi": "10.1109/TIP.2015.2456415", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a novel method for image fusion with a\nhigh-resolution panchromatic image and a low-resolution multispectral image at\nthe same geographical location. The fusion is formulated as a convex\noptimization problem which minimizes a linear combination of a least-squares\nfitting term and a dynamic gradient sparsity regularizer. The former is to\npreserve accurate spectral information of the multispectral image, while the\nlatter is to keep sharp edges of the high-resolution panchromatic image. We\nfurther propose to simultaneously register the two images during the fusing\nprocess, which is naturally achieved by virtue of the dynamic gradient sparsity\nproperty. An efficient algorithm is then devised to solve the optimization\nproblem, accomplishing a linear computational complexity in the size of the\noutput image in each iteration. We compare our method against seven\nstate-of-the-art image fusion methods on multispectral image datasets from four\nsatellites. Extensive experimental results demonstrate that the proposed method\nsubstantially outperforms the others in terms of both spatial and spectral\nqualities. We also show that our method can provide high-quality products from\ncoarsely registered real-world datasets. Finally, a MATLAB implementation is\nprovided to facilitate future research.\n", "versions": [{"version": "v1", "created": "Tue, 18 Nov 2014 23:26:37 GMT"}, {"version": "v2", "created": "Thu, 1 Jan 2015 22:00:10 GMT"}], "update_date": "2015-10-28", "authors_parsed": [["Chen", "Chen", ""], ["Li", "Yeqing", ""], ["Liu", "Wei", ""], ["Huang", "Junzhou", ""]]}, {"id": "1411.5140", "submitter": "Qian Wang", "authors": "Qian Wang, Jiaxing Zhang, Sen Song, Zheng Zhang", "title": "Attentional Neural Network: Feature Selection Using Cognitive Feedback", "comments": "Poster in Neural Information Processing Systems (NIPS) 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Attentional Neural Network is a new framework that integrates top-down\ncognitive bias and bottom-up feature extraction in one coherent architecture.\nThe top-down influence is especially effective when dealing with high noise or\ndifficult segmentation problems. Our system is modular and extensible. It is\nalso easy to train and cheap to run, and yet can accommodate complex behaviors.\nWe obtain classification accuracy better than or competitive with state of art\nresults on the MNIST variation dataset, and successfully disentangle overlaid\ndigits with high success rates. We view such a general purpose framework as an\nessential foundation for a larger system emulating the cognitive abilities of\nthe whole brain.\n", "versions": [{"version": "v1", "created": "Wed, 19 Nov 2014 08:33:28 GMT"}], "update_date": "2014-11-20", "authors_parsed": [["Wang", "Qian", ""], ["Zhang", "Jiaxing", ""], ["Song", "Sen", ""], ["Zhang", "Zheng", ""]]}, {"id": "1411.5190", "submitter": "Mateusz Malinowski", "authors": "Mateusz Malinowski and Mario Fritz", "title": "A Pooling Approach to Modelling Spatial Relations for Image Retrieval\n  and Annotation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Over the last two decades we have witnessed strong progress on modeling\nvisual object classes, scenes and attributes that have significantly\ncontributed to automated image understanding. On the other hand, surprisingly\nlittle progress has been made on incorporating a spatial representation and\nreasoning in the inference process. In this work, we propose a pooling\ninterpretation of spatial relations and show how it improves image retrieval\nand annotations tasks involving spatial language. Due to the complexity of the\nspatial language, we argue for a learning-based approach that acquires a\nrepresentation of spatial relations by learning parameters of the pooling\noperator. We show improvements on previous work on two datasets and two\ndifferent tasks as well as provide additional insights on a new dataset with an\nexplicit focus on spatial relations.\n", "versions": [{"version": "v1", "created": "Wed, 19 Nov 2014 11:44:24 GMT"}, {"version": "v2", "created": "Tue, 5 May 2015 17:55:23 GMT"}], "update_date": "2015-05-06", "authors_parsed": [["Malinowski", "Mateusz", ""], ["Fritz", "Mario", ""]]}, {"id": "1411.5268", "submitter": "Alex James Dr", "authors": "Swathikiran Sudhakarana, Alex Pappachen James", "title": "Sparse distributed localized gradient fused features of objects", "comments": "Pages 13", "journal-ref": "Pattern Recognition, Available online 31 October 2014", "doi": "10.1016/j.patcog.2014.10.002", "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  The sparse, hierarchical, and modular processing of natural signals is\nrelated to the ability of humans to recognize objects with high accuracy. In\nthis study, we report a sparse feature processing and encoding method, which\nimproved the recognition performance of an automated object recognition system.\nRandomly distributed localized gradient enhanced features were selected before\nemploying aggregate functions for representation, where we used a modular and\nhierarchical approach to detect the object features. These object features were\ncombined with a minimum distance classifier, thereby obtaining object\nrecognition system accuracies of 93% using the Amsterdam library of object\nimages (ALOI) database, 92% using the Columbia object image library (COIL)-100\ndatabase, and 69% using the PASCAL visual object challenge 2007 database. The\nobject recognition performance was shown to be robust to variations in noise,\nobject scaling, and object shifts. Finally, a comparison with eight existing\nobject recognition methods indicated that our new method improved the\nrecognition accuracy by 10% with ALOI, 8% with the COIL-100 database, and 10%\nwith the PASCAL visual object challenge 2007 database.\n", "versions": [{"version": "v1", "created": "Wed, 19 Nov 2014 15:57:02 GMT"}], "update_date": "2014-11-20", "authors_parsed": [["Sudhakarana", "Swathikiran", ""], ["James", "Alex Pappachen", ""]]}, {"id": "1411.5307", "submitter": "Robinson Piramuthu Robinson Piramuthu", "authors": "Kevin Shih, Wei Di, Vignesh Jagadeesh, Robinson Piramuthu", "title": "Efficient Media Retrieval from Non-Cooperative Queries", "comments": "8 pages, 9 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Text is ubiquitous in the artificial world and easily attainable when it\ncomes to book title and author names. Using the images from the book cover set\nfrom the Stanford Mobile Visual Search dataset and additional book covers and\nmetadata from openlibrary.org, we construct a large scale book cover retrieval\ndataset, complete with 100K distractor covers and title and author strings for\neach. Because our query images are poorly conditioned for clean text\nextraction, we propose a method for extracting a matching noisy and erroneous\nOCR readings and matching it against clean author and book title strings in a\nstandard document look-up problem setup. Finally, we demonstrate how to use\nthis text-matching as a feature in conjunction with popular retrieval features\nsuch as VLAD using a simple learning setup to achieve significant improvements\nin retrieval accuracy over that of either VLAD or the text alone.\n", "versions": [{"version": "v1", "created": "Wed, 19 Nov 2014 18:34:28 GMT"}], "update_date": "2014-11-20", "authors_parsed": [["Shih", "Kevin", ""], ["Di", "Wei", ""], ["Jagadeesh", "Vignesh", ""], ["Piramuthu", "Robinson", ""]]}, {"id": "1411.5309", "submitter": "David Eigen", "authors": "Li Wan and David Eigen and Rob Fergus", "title": "End-to-End Integration of a Convolutional Network, Deformable Parts\n  Model and Non-Maximum Suppression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deformable Parts Models and Convolutional Networks each have achieved notable\nperformance in object detection. Yet these two approaches find their strengths\nin complementary areas: DPMs are well-versed in object composition, modeling\nfine-grained spatial relationships between parts; likewise, ConvNets are adept\nat producing powerful image features, having been discriminatively trained\ndirectly on the pixels. In this paper, we propose a new model that combines\nthese two approaches, obtaining the advantages of each. We train this model\nusing a new structured loss function that considers all bounding boxes within\nan image, rather than isolated object instances. This enables the non-maximal\nsuppression (NMS) operation, previously treated as a separate post-processing\nstage, to be integrated into the model. This allows for discriminative training\nof our combined Convnet + DPM + NMS model in end-to-end fashion. We evaluate\nour system on PASCAL VOC 2007 and 2011 datasets, achieving competitive results\non both benchmarks.\n", "versions": [{"version": "v1", "created": "Wed, 19 Nov 2014 18:36:09 GMT"}], "update_date": "2014-11-20", "authors_parsed": [["Wan", "Li", ""], ["Eigen", "David", ""], ["Fergus", "Rob", ""]]}, {"id": "1411.5319", "submitter": "Kota Hara", "authors": "Kota Hara, Vignesh Jagadeesh, Robinson Piramuthu", "title": "Fashion Apparel Detection: The Role of Deep Convolutional Neural Network\n  and Pose-dependent Priors", "comments": "Accepted for publication at IEEE Winter Conference on Applications of\n  Computer Vision (WACV) 2016", "journal-ref": null, "doi": "10.1109/WACV.2016.7477611", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we propose and address a new computer vision task, which we\ncall fashion item detection, where the aim is to detect various fashion items a\nperson in the image is wearing or carrying. The types of fashion items we\nconsider in this work include hat, glasses, bag, pants, shoes and so on. The\ndetection of fashion items can be an important first step of various e-commerce\napplications for fashion industry. Our method is based on state-of-the-art\nobject detection method pipeline which combines object proposal methods with a\nDeep Convolutional Neural Network. Since the locations of fashion items are in\nstrong correlation with the locations of body joints positions, we incorporate\ncontextual information from body poses in order to improve the detection\nperformance. Through the experiments, we demonstrate the effectiveness of the\nproposed method.\n", "versions": [{"version": "v1", "created": "Wed, 19 Nov 2014 19:09:00 GMT"}, {"version": "v2", "created": "Sun, 24 Jan 2016 19:45:37 GMT"}], "update_date": "2016-11-17", "authors_parsed": [["Hara", "Kota", ""], ["Jagadeesh", "Vignesh", ""], ["Piramuthu", "Robinson", ""]]}, {"id": "1411.5328", "submitter": "Robinson Piramuthu Robinson Piramuthu", "authors": "Bolei Zhou, Vignesh Jagadeesh, Robinson Piramuthu", "title": "ConceptLearner: Discovering Visual Concepts from Weakly Labeled Image\n  Collections", "comments": "9 pages, 8 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Discovering visual knowledge from weakly labeled data is crucial to scale up\ncomputer vision recognition system, since it is expensive to obtain fully\nlabeled data for a large number of concept categories. In this paper, we\npropose ConceptLearner, which is a scalable approach to discover visual\nconcepts from weakly labeled image collections. Thousands of visual concept\ndetectors are learned automatically, without human in the loop for additional\nannotation. We show that these learned detectors could be applied to recognize\nconcepts at image-level and to detect concepts at image region-level\naccurately. Under domain-specific supervision, we further evaluate the learned\nconcepts for scene recognition on SUN database and for object detection on\nPascal VOC 2007. ConceptLearner shows promising performance compared to fully\nsupervised and weakly supervised methods.\n", "versions": [{"version": "v1", "created": "Wed, 19 Nov 2014 19:35:39 GMT"}], "update_date": "2014-11-20", "authors_parsed": [["Zhou", "Bolei", ""], ["Jagadeesh", "Vignesh", ""], ["Piramuthu", "Robinson", ""]]}, {"id": "1411.5331", "submitter": "Michelle Greene", "authors": "Michelle R. Greene, Abraham P. Botros, Diane M. Beck and Li Fei-Fei", "title": "Visual Noise from Natural Scene Statistics Reveals Human Scene Category\n  Representations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  Our perceptions are guided both by the bottom-up information entering our\neyes, as well as our top-down expectations of what we will see. Although\nbottom-up visual processing has been extensively studied, comparatively little\nis known about top-down signals. Here, we describe REVEAL (Representations\nEnvisioned Via Evolutionary ALgorithm), a method for visualizing an observer's\ninternal representation of a complex, real-world scene, allowing us to, for the\nfirst time, visualize the top-down information in an observer's mind. REVEAL\nrests on two innovations for solving this high dimensional problem: visual\nnoise that samples from natural image statistics, and a computer algorithm that\ncollaborates with human observers to efficiently obtain a solution. In this\nwork, we visualize observers' internal representations of a visual scene\ncategory (street) using an experiment in which the observer views the\nnaturalistic visual noise and collaborates with the algorithm to externalize\nhis internal representation. As no scene information was presented, observers\nhad to use their internal knowledge of the target, matching it with the visual\nfeatures in the noise. We matched reconstructed images with images of\nreal-world street scenes to enhance visualization. Critically, we show that the\nvisualized mental images can be used to predict rapid scene detection\nperformance, as each observer had faster and more accurate responses to\ndetecting real-world images that were the most similar to his reconstructed\nstreet templates. These results show that it is possible to visualize\npreviously unobservable mental representations of real world stimuli. More\nbroadly, REVEAL provides a general method for objectively examining the content\nof previously private, subjective mental experiences.\n", "versions": [{"version": "v1", "created": "Wed, 19 Nov 2014 19:38:50 GMT"}], "update_date": "2014-11-20", "authors_parsed": [["Greene", "Michelle R.", ""], ["Botros", "Abraham P.", ""], ["Beck", "Diane M.", ""], ["Fei-Fei", "Li", ""]]}, {"id": "1411.5340", "submitter": "Michelle Greene", "authors": "Michelle R. Greene, Christopher Baldassano, Andre Esteva, Diane M.\n  Beck and Li Fei-Fei", "title": "Affordances Provide a Fundamental Categorization Principle for Visual\n  Scenes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.CV cs.HC", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  How do we know that a kitchen is a kitchen by looking? Relatively little is\nknown about how we conceptualize and categorize different visual environments.\nTraditional models of visual perception posit that scene categorization is\nachieved through the recognition of a scene's objects, yet these models cannot\naccount for the mounting evidence that human observers are relatively\ninsensitive to the local details in an image. Psychologists have long theorized\nthat the affordances, or actionable possibilities of a stimulus are pivotal to\nits perception. To what extent are scene categories created from similar\naffordances? Using a large-scale experiment using hundreds of scene categories,\nwe show that the activities afforded by a visual scene provide a fundamental\ncategorization principle. Affordance-based similarity explained the majority of\nthe structure in the human scene categorization patterns, outperforming\nalternative similarities based on objects or visual features. We all models\nwere combined, affordances provided the majority of the predictive power in the\ncombined model, and nearly half of the total explained variance is captured\nonly by affordances. These results challenge many existing models of high-level\nvisual perception, and provide immediately testable hypotheses for the\nfunctional organization of the human perceptual system.\n", "versions": [{"version": "v1", "created": "Wed, 19 Nov 2014 19:58:59 GMT"}], "update_date": "2014-11-20", "authors_parsed": [["Greene", "Michelle R.", ""], ["Baldassano", "Christopher", ""], ["Esteva", "Andre", ""], ["Beck", "Diane M.", ""], ["Fei-Fei", "Li", ""]]}, {"id": "1411.5555", "submitter": "Andrey Savchenko", "authors": "Andrey Savchenko", "title": "Maximum Likelihood Directed Enumeration Method in Piecewise-Regular\n  Object Recognition", "comments": "13 pages, 6 figures, 20 references", "journal-ref": null, "doi": "10.1016/j.patcog.2016.08.015", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We explore the problems of classification of composite object (images, speech\nsignals) with low number of models per class. We study the question of\nimproving recognition performance for medium-sized database (thousands of\nclasses). The key issue of fast approximate nearest-neighbor methods widely\napplied in this task is their heuristic nature. It is possible to strongly\nprove their efficiency by using the theory of algorithms only for simple\nsimilarity measures and artificially generated tasks. On the contrary, in this\npaper we propose an alternative, statistically optimal greedy algorithm. At\neach step of this algorithm joint density (likelihood) of distances to\npreviously checked models is estimated for each class. The next model to check\nis selected from the class with the maximal likelihood. The latter is estimated\nbased on the asymptotic properties of the Kullback-Leibler information\ndiscrimination and mathematical model of piecewise-regular object with\ndistribution of each regular segment of exponential type. Experimental results\nin face recognition for FERET dataset prove that the proposed method is much\nmore effective than not only brute force and the baseline (directed enumeration\nmethod) but also approximate nearest neighbor methods from FLANN and\nNonMetricSpaceLib libraries (randomized kd-tree, composite index, perm-sort).\n", "versions": [{"version": "v1", "created": "Thu, 20 Nov 2014 14:22:15 GMT"}], "update_date": "2017-09-19", "authors_parsed": [["Savchenko", "Andrey", ""]]}, {"id": "1411.5654", "submitter": "Xinlei Chen", "authors": "Xinlei Chen and C. Lawrence Zitnick", "title": "Learning a Recurrent Visual Representation for Image Caption Generation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we explore the bi-directional mapping between images and their\nsentence-based descriptions. We propose learning this mapping using a recurrent\nneural network. Unlike previous approaches that map both sentences and images\nto a common embedding, we enable the generation of novel sentences given an\nimage. Using the same model, we can also reconstruct the visual features\nassociated with an image given its visual description. We use a novel recurrent\nvisual memory that automatically learns to remember long-term visual concepts\nto aid in both sentence generation and visual feature reconstruction. We\nevaluate our approach on several tasks. These include sentence generation,\nsentence retrieval and image retrieval. State-of-the-art results are shown for\nthe task of generating novel image descriptions. When compared to human\ngenerated captions, our automatically generated captions are preferred by\nhumans over $19.8\\%$ of the time. Results are better than or comparable to\nstate-of-the-art results on the image and sentence retrieval tasks for methods\nusing similar visual features.\n", "versions": [{"version": "v1", "created": "Thu, 20 Nov 2014 19:50:27 GMT"}], "update_date": "2014-11-21", "authors_parsed": [["Chen", "Xinlei", ""], ["Zitnick", "C. Lawrence", ""]]}, {"id": "1411.5726", "submitter": "Ramakrishna  Vedantam", "authors": "Ramakrishna Vedantam, C. Lawrence Zitnick and Devi Parikh", "title": "CIDEr: Consensus-based Image Description Evaluation", "comments": "To appear in CVPR 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatically describing an image with a sentence is a long-standing\nchallenge in computer vision and natural language processing. Due to recent\nprogress in object detection, attribute classification, action recognition,\netc., there is renewed interest in this area. However, evaluating the quality\nof descriptions has proven to be challenging. We propose a novel paradigm for\nevaluating image descriptions that uses human consensus. This paradigm consists\nof three main parts: a new triplet-based method of collecting human annotations\nto measure consensus, a new automated metric (CIDEr) that captures consensus,\nand two new datasets: PASCAL-50S and ABSTRACT-50S that contain 50 sentences\ndescribing each image. Our simple metric captures human judgment of consensus\nbetter than existing metrics across sentences generated by various sources. We\nalso evaluate five state-of-the-art image description approaches using this new\nprotocol and provide a benchmark for future comparisons. A version of CIDEr\nnamed CIDEr-D is available as a part of MS COCO evaluation server to enable\nsystematic evaluation and benchmarking.\n", "versions": [{"version": "v1", "created": "Thu, 20 Nov 2014 23:54:35 GMT"}, {"version": "v2", "created": "Wed, 3 Jun 2015 01:42:20 GMT"}], "update_date": "2015-06-04", "authors_parsed": [["Vedantam", "Ramakrishna", ""], ["Zitnick", "C. Lawrence", ""], ["Parikh", "Devi", ""]]}, {"id": "1411.5731", "submitter": "Suleyman Cetintas", "authors": "Can Xu, Suleyman Cetintas, Kuang-Chih Lee, Li-Jia Li", "title": "Visual Sentiment Prediction with Deep Convolutional Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Images have become one of the most popular types of media through which users\nconvey their emotions within online social networks. Although vast amount of\nresearch is devoted to sentiment analysis of textual data, there has been very\nlimited work that focuses on analyzing sentiment of image data. In this work,\nwe propose a novel visual sentiment prediction framework that performs image\nunderstanding with Deep Convolutional Neural Networks (CNN). Specifically, the\nproposed sentiment prediction framework performs transfer learning from a CNN\nwith millions of parameters, which is pre-trained on large-scale data for\nobject recognition. Experiments conducted on two real-world datasets from\nTwitter and Tumblr demonstrate the effectiveness of the proposed visual\nsentiment analysis framework.\n", "versions": [{"version": "v1", "created": "Fri, 21 Nov 2014 00:39:43 GMT"}], "update_date": "2014-11-24", "authors_parsed": [["Xu", "Can", ""], ["Cetintas", "Suleyman", ""], ["Lee", "Kuang-Chih", ""], ["Li", "Li-Jia", ""]]}, {"id": "1411.5752", "submitter": "Bharath Hariharan", "authors": "Bharath Hariharan and Pablo Arbel\\'aez and Ross Girshick and Jitendra\n  Malik", "title": "Hypercolumns for Object Segmentation and Fine-grained Localization", "comments": "CVPR Camera ready", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recognition algorithms based on convolutional networks (CNNs) typically use\nthe output of the last layer as feature representation. However, the\ninformation in this layer may be too coarse to allow precise localization. On\nthe contrary, earlier layers may be precise in localization but will not\ncapture semantics. To get the best of both worlds, we define the hypercolumn at\na pixel as the vector of activations of all CNN units above that pixel. Using\nhypercolumns as pixel descriptors, we show results on three fine-grained\nlocalization tasks: simultaneous detection and segmentation[22], where we\nimprove state-of-the-art from 49.7[22] mean AP^r to 60.0, keypoint\nlocalization, where we get a 3.3 point boost over[20] and part labeling, where\nwe show a 6.6 point gain over a strong baseline.\n", "versions": [{"version": "v1", "created": "Fri, 21 Nov 2014 03:12:33 GMT"}, {"version": "v2", "created": "Sat, 25 Apr 2015 23:08:59 GMT"}], "update_date": "2015-04-28", "authors_parsed": [["Hariharan", "Bharath", ""], ["Arbel\u00e1ez", "Pablo", ""], ["Girshick", "Ross", ""], ["Malik", "Jitendra", ""]]}, {"id": "1411.5825", "submitter": "Mitko Veta", "authors": "Mitko Veta, Paul J. van Diest, Stefan M. Willems, Haibo Wang, Anant\n  Madabhushi, Angel Cruz-Roa, Fabio Gonzalez, Anders B. L. Larsen, Jacob S.\n  Vestergaard, Anders B. Dahl, Dan C. Cire\\c{s}an, J\\\"urgen Schmidhuber,\n  Alessandro Giusti, Luca M. Gambardella, F. Boray Tek, Thomas Walter,\n  Ching-Wei Wang, Satoshi Kondo, Bogdan J. Matuszewski, Frederic Precioso,\n  Violet Snell, Josef Kittler, Teofilo E. de Campos, Adnan M. Khan, Nasir M.\n  Rajpoot, Evdokia Arkoumani, Miangela M. Lacle, Max A. Viergever, Josien P.W.\n  Pluim", "title": "Assessment of algorithms for mitosis detection in breast cancer\n  histopathology images", "comments": "23 pages, 5 figures, accepted for publication in the journal Medical\n  Image Analysis", "journal-ref": null, "doi": "10.1016/j.media.2014.11.010", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The proliferative activity of breast tumors, which is routinely estimated by\ncounting of mitotic figures in hematoxylin and eosin stained histology\nsections, is considered to be one of the most important prognostic markers.\nHowever, mitosis counting is laborious, subjective and may suffer from low\ninter-observer agreement. With the wider acceptance of whole slide images in\npathology labs, automatic image analysis has been proposed as a potential\nsolution for these issues. In this paper, the results from the Assessment of\nMitosis Detection Algorithms 2013 (AMIDA13) challenge are described. The\nchallenge was based on a data set consisting of 12 training and 11 testing\nsubjects, with more than one thousand annotated mitotic figures by multiple\nobservers. Short descriptions and results from the evaluation of eleven methods\nare presented. The top performing method has an error rate that is comparable\nto the inter-observer agreement among pathologists.\n", "versions": [{"version": "v1", "created": "Fri, 21 Nov 2014 11:00:38 GMT"}], "update_date": "2014-12-02", "authors_parsed": [["Veta", "Mitko", ""], ["van Diest", "Paul J.", ""], ["Willems", "Stefan M.", ""], ["Wang", "Haibo", ""], ["Madabhushi", "Anant", ""], ["Cruz-Roa", "Angel", ""], ["Gonzalez", "Fabio", ""], ["Larsen", "Anders B. L.", ""], ["Vestergaard", "Jacob S.", ""], ["Dahl", "Anders B.", ""], ["Cire\u015fan", "Dan C.", ""], ["Schmidhuber", "J\u00fcrgen", ""], ["Giusti", "Alessandro", ""], ["Gambardella", "Luca M.", ""], ["Tek", "F. Boray", ""], ["Walter", "Thomas", ""], ["Wang", "Ching-Wei", ""], ["Kondo", "Satoshi", ""], ["Matuszewski", "Bogdan J.", ""], ["Precioso", "Frederic", ""], ["Snell", "Violet", ""], ["Kittler", "Josef", ""], ["de Campos", "Teofilo E.", ""], ["Khan", "Adnan M.", ""], ["Rajpoot", "Nasir M.", ""], ["Arkoumani", "Evdokia", ""], ["Lacle", "Miangela M.", ""], ["Viergever", "Max A.", ""], ["Pluim", "Josien P. W.", ""]]}, {"id": "1411.5878", "submitter": "Ming-Ming Cheng Prof.", "authors": "Ali Borji, Ming-Ming Cheng, Qibin Hou, Huaizu Jiang, Jia Li", "title": "Salient Object Detection: A Survey", "comments": null, "journal-ref": "Computational Visual Media, 5(2):117-150, 2019", "doi": "10.1007/s41095-019-0149-9", "report-no": null, "categories": "cs.CV cs.AI q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Detecting and segmenting salient objects from natural scenes, often referred\nto as salient object detection, has attracted great interest in computer\nvision. While many models have been proposed and several applications have\nemerged, a deep understanding of achievements and issues remains lacking. We\naim to provide a comprehensive review of recent progress in salient object\ndetection and situate this field among other closely related areas such as\ngeneric scene segmentation, object proposal generation, and saliency for\nfixation prediction. Covering 228 publications, we survey i) roots, key\nconcepts, and tasks, ii) core techniques and main modeling trends, and iii)\ndatasets and evaluation metrics for salient object detection. We also discuss\nopen problems such as evaluation metrics and dataset bias in model performance,\nand suggest future research directions.\n", "versions": [{"version": "v1", "created": "Tue, 18 Nov 2014 22:41:24 GMT"}, {"version": "v2", "created": "Mon, 7 Aug 2017 17:17:03 GMT"}, {"version": "v3", "created": "Wed, 16 Aug 2017 21:45:20 GMT"}, {"version": "v4", "created": "Mon, 4 Sep 2017 02:51:33 GMT"}, {"version": "v5", "created": "Thu, 6 Sep 2018 19:31:29 GMT"}, {"version": "v6", "created": "Mon, 1 Jul 2019 11:13:07 GMT"}], "update_date": "2019-07-02", "authors_parsed": [["Borji", "Ali", ""], ["Cheng", "Ming-Ming", ""], ["Hou", "Qibin", ""], ["Jiang", "Huaizu", ""], ["Li", "Jia", ""]]}, {"id": "1411.5879", "submitter": "Sung Ju Hwang", "authors": "Sung Ju Hwang and Leonid Sigal", "title": "A Unified Semantic Embedding: Relating Taxonomies and Attributes", "comments": "To Appear in NIPS 2014 Learning Semantics Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a method that learns a discriminative yet semantic space for\nobject categorization, where we also embed auxiliary semantic entities such as\nsupercategories and attributes. Contrary to prior work which only utilized them\nas side information, we explicitly embed the semantic entities into the same\nspace where we embed categories, which enables us to represent a category as\ntheir linear combination. By exploiting such a unified model for semantics, we\nenforce each category to be represented by a supercategory + sparse combination\nof attributes, with an additional exclusive regularization to learn\ndiscriminative composition.\n", "versions": [{"version": "v1", "created": "Tue, 18 Nov 2014 17:03:20 GMT"}, {"version": "v2", "created": "Tue, 9 Dec 2014 18:00:13 GMT"}], "update_date": "2014-12-10", "authors_parsed": [["Hwang", "Sung Ju", ""], ["Sigal", "Leonid", ""]]}, {"id": "1411.5908", "submitter": "Karel Lenc", "authors": "Karel Lenc, Andrea Vedaldi", "title": "Understanding image representations by measuring their equivariance and\n  equivalence", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the importance of image representations such as histograms of\noriented gradients and deep Convolutional Neural Networks (CNN), our\ntheoretical understanding of them remains limited. Aiming at filling this gap,\nwe investigate three key mathematical properties of representations:\nequivariance, invariance, and equivalence. Equivariance studies how\ntransformations of the input image are encoded by the representation,\ninvariance being a special case where a transformation has no effect.\nEquivalence studies whether two representations, for example two different\nparametrisations of a CNN, capture the same visual information or not. A number\nof methods to establish these properties empirically are proposed, including\nintroducing transformation and stitching layers in CNNs. These methods are then\napplied to popular representations to reveal insightful aspects of their\nstructure, including clarifying at which layers in a CNN certain geometric\ninvariances are achieved. While the focus of the paper is theoretical, direct\napplications to structured-output regression are demonstrated too.\n", "versions": [{"version": "v1", "created": "Fri, 21 Nov 2014 15:14:42 GMT"}, {"version": "v2", "created": "Sat, 20 Jun 2015 18:35:37 GMT"}], "update_date": "2015-06-23", "authors_parsed": [["Lenc", "Karel", ""], ["Vedaldi", "Andrea", ""]]}, {"id": "1411.5928", "submitter": "Alexey Dosovitskiy", "authors": "Alexey Dosovitskiy, Jost Tobias Springenberg, Maxim Tatarchenko and\n  Thomas Brox", "title": "Learning to Generate Chairs, Tables and Cars with Convolutional Networks", "comments": "v4: final PAMI version. New architecture figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We train generative 'up-convolutional' neural networks which are able to\ngenerate images of objects given object style, viewpoint, and color. We train\nthe networks on rendered 3D models of chairs, tables, and cars. Our experiments\nshow that the networks do not merely learn all images by heart, but rather find\na meaningful representation of 3D models allowing them to assess the similarity\nof different models, interpolate between given views to generate the missing\nones, extrapolate views, and invent new objects not present in the training set\nby recombining training instances, or even two different object classes.\nMoreover, we show that such generative networks can be used to find\ncorrespondences between different objects from the dataset, outperforming\nexisting approaches on this task.\n", "versions": [{"version": "v1", "created": "Fri, 21 Nov 2014 16:01:04 GMT"}, {"version": "v2", "created": "Mon, 5 Jan 2015 12:31:49 GMT"}, {"version": "v3", "created": "Thu, 3 Dec 2015 09:49:23 GMT"}, {"version": "v4", "created": "Wed, 2 Aug 2017 20:53:43 GMT"}], "update_date": "2017-08-04", "authors_parsed": [["Dosovitskiy", "Alexey", ""], ["Springenberg", "Jost Tobias", ""], ["Tatarchenko", "Maxim", ""], ["Brox", "Thomas", ""]]}, {"id": "1411.5935", "submitter": "M. Zeeshan Zia", "authors": "M.Zeeshan Zia, Michael Stark, Konrad Schindler", "title": "Towards Scene Understanding with Detailed 3D Object Representations", "comments": "International Journal of Computer Vision (appeared online on 4\n  November 2014). Online version:\n  http://link.springer.com/article/10.1007/s11263-014-0780-y", "journal-ref": null, "doi": "10.1007/s11263-014-0780-y", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current approaches to semantic image and scene understanding typically employ\nrather simple object representations such as 2D or 3D bounding boxes. While\nsuch coarse models are robust and allow for reliable object detection, they\ndiscard much of the information about objects' 3D shape and pose, and thus do\nnot lend themselves well to higher-level reasoning. Here, we propose to base\nscene understanding on a high-resolution object representation. An object class\n- in our case cars - is modeled as a deformable 3D wireframe, which enables\nfine-grained modeling at the level of individual vertices and faces. We augment\nthat model to explicitly include vertex-level occlusion, and embed all\ninstances in a common coordinate frame, in order to infer and exploit\nobject-object interactions. Specifically, from a single view we jointly\nestimate the shapes and poses of multiple objects in a common 3D frame. A\nground plane in that frame is estimated by consensus among different objects,\nwhich significantly stabilizes monocular 3D pose estimation. The fine-grained\nmodel, in conjunction with the explicit 3D scene model, further allows one to\ninfer part-level occlusions between the modeled objects, as well as occlusions\nby other, unmodeled scene elements. To demonstrate the benefits of such\ndetailed object class models in the context of scene understanding we\nsystematically evaluate our approach on the challenging KITTI street scene\ndataset. The experiments show that the model's ability to utilize image\nevidence at the level of individual parts improves monocular 3D pose estimation\nw.r.t. both location and (continuous) viewpoint.\n", "versions": [{"version": "v1", "created": "Tue, 18 Nov 2014 15:07:19 GMT"}], "update_date": "2014-11-24", "authors_parsed": [["Zia", "M. Zeeshan", ""], ["Stark", "Michael", ""], ["Schindler", "Konrad", ""]]}, {"id": "1411.6031", "submitter": "Georgia Gkioxari", "authors": "Georgia Gkioxari and Jitendra Malik", "title": "Finding Action Tubes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of action detection in videos. Driven by the latest\nprogress in object detection from 2D images, we build action models using rich\nfeature hierarchies derived from shape and kinematic cues. We incorporate\nappearance and motion in two ways. First, starting from image region proposals\nwe select those that are motion salient and thus are more likely to contain the\naction. This leads to a significant reduction in the number of regions being\nprocessed and allows for faster computations. Second, we extract\nspatio-temporal feature representations to build strong classifiers using\nConvolutional Neural Networks. We link our predictions to produce detections\nconsistent in time, which we call action tubes. We show that our approach\noutperforms other techniques in the task of action detection.\n", "versions": [{"version": "v1", "created": "Fri, 21 Nov 2014 21:38:15 GMT"}], "update_date": "2014-11-25", "authors_parsed": [["Gkioxari", "Georgia", ""], ["Malik", "Jitendra", ""]]}, {"id": "1411.6067", "submitter": "Shubham Tulsiani", "authors": "Shubham Tulsiani and Jitendra Malik", "title": "Viewpoints and Keypoints", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We characterize the problem of pose estimation for rigid objects in terms of\ndetermining viewpoint to explain coarse pose and keypoint prediction to capture\nthe finer details. We address both these tasks in two different settings - the\nconstrained setting with known bounding boxes and the more challenging\ndetection setting where the aim is to simultaneously detect and correctly\nestimate pose of objects. We present Convolutional Neural Network based\narchitectures for these and demonstrate that leveraging viewpoint estimates can\nsubstantially improve local appearance based keypoint predictions. In addition\nto achieving significant improvements over state-of-the-art in the above tasks,\nwe analyze the error modes and effect of object characteristics on performance\nto guide future efforts towards this goal.\n", "versions": [{"version": "v1", "created": "Sat, 22 Nov 2014 03:14:21 GMT"}, {"version": "v2", "created": "Sun, 26 Apr 2015 04:07:17 GMT"}], "update_date": "2015-04-28", "authors_parsed": [["Tulsiani", "Shubham", ""], ["Malik", "Jitendra", ""]]}, {"id": "1411.6069", "submitter": "Abhishek Kar", "authors": "Abhishek Kar, Shubham Tulsiani, Jo\\~ao Carreira, Jitendra Malik", "title": "Category-Specific Object Reconstruction from a Single Image", "comments": "First two authors contributed equally. To appear at CVPR 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Object reconstruction from a single image -- in the wild -- is a problem\nwhere we can make progress and get meaningful results today. This is the main\nmessage of this paper, which introduces an automated pipeline with pixels as\ninputs and 3D surfaces of various rigid categories as outputs in images of\nrealistic scenes. At the core of our approach are deformable 3D models that can\nbe learned from 2D annotations available in existing object detection datasets,\nthat can be driven by noisy automatic object segmentations and which we\ncomplement with a bottom-up module for recovering high-frequency shape details.\nWe perform a comprehensive quantitative analysis and ablation study of our\napproach using the recently introduced PASCAL 3D+ dataset and show very\nencouraging automatic reconstructions on PASCAL VOC.\n", "versions": [{"version": "v1", "created": "Sat, 22 Nov 2014 03:15:29 GMT"}, {"version": "v2", "created": "Wed, 6 May 2015 21:42:41 GMT"}], "update_date": "2015-05-08", "authors_parsed": [["Kar", "Abhishek", ""], ["Tulsiani", "Shubham", ""], ["Carreira", "Jo\u00e3o", ""], ["Malik", "Jitendra", ""]]}, {"id": "1411.6091", "submitter": "Joao Carreira", "authors": "Jo\\~ao Carreira, Abhishek Kar, Shubham Tulsiani and Jitendra Malik", "title": "Virtual View Networks for Object Reconstruction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  All that structure from motion algorithms \"see\" are sets of 2D points. We\nshow that these impoverished views of the world can be faked for the purpose of\nreconstructing objects in challenging settings, such as from a single image, or\nfrom a few ones far apart, by recognizing the object and getting help from a\ncollection of images of other objects from the same class. We synthesize\nvirtual views by computing geodesics on novel networks connecting objects with\nsimilar viewpoints, and introduce techniques to increase the specificity and\nrobustness of factorization-based object reconstruction in this setting. We\nreport accurate object shape reconstruction from a single image on challenging\nPASCAL VOC data, which suggests that the current domain of applications of\nrigid structure-from-motion techniques may be significantly extended.\n", "versions": [{"version": "v1", "created": "Sat, 22 Nov 2014 07:07:53 GMT"}], "update_date": "2014-11-25", "authors_parsed": [["Carreira", "Jo\u00e3o", ""], ["Kar", "Abhishek", ""], ["Tulsiani", "Shubham", ""], ["Malik", "Jitendra", ""]]}, {"id": "1411.6206", "submitter": "Dornoosh  Zonoobi", "authors": "Dornoosh Zonoobi, Shahrooz Faghih Roohi, Ashraf A. Kassim", "title": "Low-Rank and Sparse Matrix Decomposition with a-priori knowledge for\n  Dynamic 3D MRI reconstruction", "comments": "conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It has been recently shown that incorporating priori knowledge significantly\nimproves the performance of basic compressive sensing based approaches. We have\nmanaged to successfully exploit this idea for recovering a matrix as a\nsummation of a Low-rank and a Sparse component from compressive measurements.\nWhen applied to the problem of construction of 4D Cardiac MR image sequences in\nreal-time from highly under-sampled $k-$space data, our proposed method\nachieves superior reconstruction quality compared to the other state-of-the-art\nmethods.\n", "versions": [{"version": "v1", "created": "Sun, 23 Nov 2014 08:19:11 GMT"}], "update_date": "2014-11-25", "authors_parsed": [["Zonoobi", "Dornoosh", ""], ["Roohi", "Shahrooz Faghih", ""], ["Kassim", "Ashraf A.", ""]]}, {"id": "1411.6228", "submitter": "Pedro O. Pinheiro", "authors": "Pedro O. Pinheiro and Ronan Collobert", "title": "From Image-level to Pixel-level Labeling with Convolutional Networks", "comments": "CVPR2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We are interested in inferring object segmentation by leveraging only object\nclass information, and by considering only minimal priors on the object\nsegmentation task. This problem could be viewed as a kind of weakly supervised\nsegmentation task, and naturally fits the Multiple Instance Learning (MIL)\nframework: every training image is known to have (or not) at least one pixel\ncorresponding to the image class label, and the segmentation task can be\nrewritten as inferring the pixels belonging to the class of the object (given\none image, and its object class). We propose a Convolutional Neural\nNetwork-based model, which is constrained during training to put more weight on\npixels which are important for classifying the image. We show that at test\ntime, the model has learned to discriminate the right pixels well enough, such\nthat it performs very well on an existing segmentation benchmark, by adding\nonly few smoothing priors. Our system is trained using a subset of the Imagenet\ndataset and the segmentation experiments are performed on the challenging\nPascal VOC dataset (with no fine-tuning of the model on Pascal VOC). Our model\nbeats the state of the art results in weakly supervised object segmentation\ntask by a large margin. We also compare the performance of our model with state\nof the art fully-supervised segmentation approaches.\n", "versions": [{"version": "v1", "created": "Sun, 23 Nov 2014 12:06:36 GMT"}, {"version": "v2", "created": "Mon, 26 Jan 2015 13:11:43 GMT"}, {"version": "v3", "created": "Fri, 24 Apr 2015 07:26:01 GMT"}], "update_date": "2015-04-27", "authors_parsed": [["Pinheiro", "Pedro O.", ""], ["Collobert", "Ronan", ""]]}, {"id": "1411.6275", "submitter": "Hebert P\\'erez-Ros\\'es PhD", "authors": "Miguel Casta\\~neda-Garay, Oscar Belmonte-Fern\\'andez, Hebert\n  P\\'erez-Ros\\'es, Antonio Diaz-Tula", "title": "Detection of Non-Stationary Photometric Perturbations on Projection\n  Screens", "comments": "20 pages, Journal of Research and Practice in Information Technology,\n  vol. 44, num. 4, 2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Interfaces based on projection screens have become increasingly more popular\nin recent years, mainly due to the large screen size and resolution that they\nprovide, as well as their stereo-vision capabilities. This work shows a local\nmethod for real-time detection of non-stationary photometric perturbations in\nprojected images by means of computer vision techniques. The method is based on\nthe computation of differences between the images in the projector's frame\nbuffer and the corresponding images on the projection screen observed by the\ncamera. It is robust under spatial variations in the intensity of light emitted\nby the projector on the projection surface and also robust under stationary\nphotometric perturbations caused by external factors. Moreover, we describe the\nexperiments carried out to show the reliability of the method.\n", "versions": [{"version": "v1", "created": "Sun, 23 Nov 2014 18:17:42 GMT"}], "update_date": "2014-11-25", "authors_parsed": [["Casta\u00f1eda-Garay", "Miguel", ""], ["Belmonte-Fern\u00e1ndez", "Oscar", ""], ["P\u00e9rez-Ros\u00e9s", "Hebert", ""], ["Diaz-Tula", "Antonio", ""]]}, {"id": "1411.6326", "submitter": "Shreyansh Daftry", "authors": "Debadeepta Dey, Kumar Shaurya Shankar, Sam Zeng, Rupesh Mehta, M.\n  Talha Agcayazi, Christopher Eriksen, Shreyansh Daftry, Martial Hebert, and J.\n  Andrew Bagnell", "title": "Vision and Learning for Deliberative Monocular Cluttered Flight", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cameras provide a rich source of information while being passive, cheap and\nlightweight for small and medium Unmanned Aerial Vehicles (UAVs). In this work\nwe present the first implementation of receding horizon control, which is\nwidely used in ground vehicles, with monocular vision as the only sensing mode\nfor autonomous UAV flight in dense clutter. We make it feasible on UAVs via a\nnumber of contributions: novel coupling of perception and control via relevant\nand diverse, multiple interpretations of the scene around the robot, leveraging\nrecent advances in machine learning to showcase anytime budgeted cost-sensitive\nfeature selection, and fast non-linear regression for monocular depth\nprediction. We empirically demonstrate the efficacy of our novel pipeline via\nreal world experiments of more than 2 kms through dense trees with a quadrotor\nbuilt from off-the-shelf parts. Moreover our pipeline is designed to combine\ninformation from other modalities like stereo and lidar as well if available.\n", "versions": [{"version": "v1", "created": "Mon, 24 Nov 2014 02:09:59 GMT"}], "update_date": "2014-11-25", "authors_parsed": [["Dey", "Debadeepta", ""], ["Shankar", "Kumar Shaurya", ""], ["Zeng", "Sam", ""], ["Mehta", "Rupesh", ""], ["Agcayazi", "M. Talha", ""], ["Eriksen", "Christopher", ""], ["Daftry", "Shreyansh", ""], ["Hebert", "Martial", ""], ["Bagnell", "J. Andrew", ""]]}, {"id": "1411.6340", "submitter": "Thalaiyasingam Ajanthan", "authors": "Thalaiyasingam Ajanthan, Richard Hartley, Mathieu Salzmann and\n  Hongdong Li", "title": "Iteratively Reweighted Graph Cut for Multi-label MRFs with Non-convex\n  Priors", "comments": "9 pages, 5 figures and 6 tables", "journal-ref": "CVPR, June 2015", "doi": "10.1109/CVPR.2015.7299150", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While widely acknowledged as highly effective in computer vision, multi-label\nMRFs with non-convex priors are difficult to optimize. To tackle this, we\nintroduce an algorithm that iteratively approximates the original energy with\nan appropriately weighted surrogate energy that is easier to minimize. Our\nalgorithm guarantees that the original energy decreases at each iteration. In\nparticular, we consider the scenario where the global minimizer of the weighted\nsurrogate energy can be obtained by a multi-label graph cut algorithm, and show\nthat our algorithm then lets us handle of large variety of non-convex priors.\nWe demonstrate the benefits of our method over state-of-the-art MRF energy\nminimization techniques on stereo and inpainting problems.\n", "versions": [{"version": "v1", "created": "Mon, 24 Nov 2014 03:35:34 GMT"}], "update_date": "2016-11-30", "authors_parsed": [["Ajanthan", "Thalaiyasingam", ""], ["Hartley", "Richard", ""], ["Salzmann", "Mathieu", ""], ["Li", "Hongdong", ""]]}, {"id": "1411.6365", "submitter": "Jongwon Ha", "authors": "Ha Jong Won, Choe Chun Hwa, Li Kum Song", "title": "On the mathematic modeling of non-parametric curves based on cubic\n  B\\'ezier curves", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  B\\'ezier splines are widely available in various systems with the curves and\nsurface designs. In general, the B\\'ezier spline can be specified with the\nB\\'ezier curve segments and a B\\'ezier curve segment can be fitted to any\nnumber of control points. The number of control points determines the degree of\nthe B\\'ezier polynomial. This paper presents a method which determines control\npoints for B\\'ezier curves approximating segments of obtained image\noutline(non-parametric curve) by using the properties of cubic B\\'ezier curves.\nProposed method is a technique to determine the control points that has\ngenerality and reduces the error of the B\\'ezier curve approximation. Main\nadvantage of proposed method is that it has higher accuracy and compression\nrate than previous methods. The cubic B\\'ezier spline is obtained from cubic\nB\\'ezier curve segments. To demonstrate the various performances of the\nproposed algorithm, experimental results are compared.\n", "versions": [{"version": "v1", "created": "Mon, 24 Nov 2014 07:19:17 GMT"}], "update_date": "2014-11-25", "authors_parsed": [["Won", "Ha Jong", ""], ["Hwa", "Choe Chun", ""], ["Song", "Li Kum", ""]]}, {"id": "1411.6369", "submitter": "Yichong Xu", "authors": "Yichong Xu, Tianjun Xiao, Jiaxing Zhang, Kuiyuan Yang, Zheng Zhang", "title": "Scale-Invariant Convolutional Neural Networks", "comments": "This paper is submitted for CVPR2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Even though convolutional neural networks (CNN) has achieved near-human\nperformance in various computer vision tasks, its ability to tolerate scale\nvariations is limited. The popular practise is making the model bigger first,\nand then train it with data augmentation using extensive scale-jittering. In\nthis paper, we propose a scaleinvariant convolutional neural network (SiCNN), a\nmodeldesigned to incorporate multi-scale feature exaction and classification\ninto the network structure. SiCNN uses a multi-column architecture, with each\ncolumn focusing on a particular scale. Unlike previous multi-column strategies,\nthese columns share the same set of filter parameters by a scale transformation\namong them. This design deals with scale variation without blowing up the model\nsize. Experimental results show that SiCNN detects features at various scales,\nand the classification result exhibits strong robustness against object scale\nvariations.\n", "versions": [{"version": "v1", "created": "Mon, 24 Nov 2014 07:28:21 GMT"}], "update_date": "2014-11-25", "authors_parsed": [["Xu", "Yichong", ""], ["Xiao", "Tianjun", ""], ["Zhang", "Jiaxing", ""], ["Yang", "Kuiyuan", ""], ["Zhang", "Zheng", ""]]}, {"id": "1411.6382", "submitter": "Chunhua Shen", "authors": "Yao Li, Lingqiao Liu, Chunhua Shen, Anton van den Hengel", "title": "Mid-level Deep Pattern Mining", "comments": "Published in Proc. IEEE Conf. Computer Vision and Pattern Recognition\n  2015", "journal-ref": null, "doi": "10.1109/CVPR.2015.7298699", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mid-level visual element discovery aims to find clusters of image patches\nthat are both representative and discriminative. In this work, we study this\nproblem from the prospective of pattern mining while relying on the recently\npopularized Convolutional Neural Networks (CNNs). Specifically, we find that\nfor an image patch, activations extracted from the first fully-connected layer\nof CNNs have two appealing properties which enable its seamless integration\nwith pattern mining. Patterns are then discovered from a large number of CNN\nactivations of image patches through the well-known association rule mining.\nWhen we retrieve and visualize image patches with the same pattern,\nsurprisingly, they are not only visually similar but also semantically\nconsistent. We apply our approach to scene and object classification tasks, and\ndemonstrate that our approach outperforms all previous works on mid-level\nvisual element discovery by a sizeable margin with far fewer elements being\nused. Our approach also outperforms or matches recent works using CNN for these\ntasks. Source code of the complete system is available online.\n", "versions": [{"version": "v1", "created": "Mon, 24 Nov 2014 08:57:16 GMT"}, {"version": "v2", "created": "Wed, 3 Dec 2014 23:35:01 GMT"}, {"version": "v3", "created": "Thu, 9 Apr 2015 03:40:41 GMT"}], "update_date": "2016-11-17", "authors_parsed": [["Li", "Yao", ""], ["Liu", "Lingqiao", ""], ["Shen", "Chunhua", ""], ["Hengel", "Anton van den", ""]]}, {"id": "1411.6387", "submitter": "Chunhua Shen", "authors": "Fayao Liu, Chunhua Shen, Guosheng Lin", "title": "Deep Convolutional Neural Fields for Depth Estimation from a Single\n  Image", "comments": "fixed some typos. in CVPR15 proceedings", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of depth estimation from a single monocular image in\nthis work. It is a challenging task as no reliable depth cues are available,\ne.g., stereo correspondences, motions, etc. Previous efforts have been focusing\non exploiting geometric priors or additional sources of information, with all\nusing hand-crafted features. Recently, there is mounting evidence that features\nfrom deep convolutional neural networks (CNN) are setting new records for\nvarious vision applications. On the other hand, considering the continuous\ncharacteristic of the depth values, depth estimations can be naturally\nformulated into a continuous conditional random field (CRF) learning problem.\nTherefore, we in this paper present a deep convolutional neural field model for\nestimating depths from a single image, aiming to jointly explore the capacity\nof deep CNN and continuous CRF. Specifically, we propose a deep structured\nlearning scheme which learns the unary and pairwise potentials of continuous\nCRF in a unified deep CNN framework.\n  The proposed method can be used for depth estimations of general scenes with\nno geometric priors nor any extra information injected. In our case, the\nintegral of the partition function can be analytically calculated, thus we can\nexactly solve the log-likelihood optimization. Moreover, solving the MAP\nproblem for predicting depths of a new image is highly efficient as closed-form\nsolutions exist. We experimentally demonstrate that the proposed method\noutperforms state-of-the-art depth estimation methods on both indoor and\noutdoor scene datasets.\n", "versions": [{"version": "v1", "created": "Mon, 24 Nov 2014 09:13:00 GMT"}, {"version": "v2", "created": "Thu, 18 Dec 2014 04:11:14 GMT"}], "update_date": "2015-10-01", "authors_parsed": [["Liu", "Fayao", ""], ["Shen", "Chunhua", ""], ["Lin", "Guosheng", ""]]}, {"id": "1411.6406", "submitter": "Chunhua Shen", "authors": "Lingqiao Liu, Chunhua Shen, Lei Wang, Anton van den Hengel, Chao Wang", "title": "Encoding High Dimensional Local Features by Sparse Coding Based Fisher\n  Vectors", "comments": "Appearing in Proc. Advances in Neural Information Processing Systems\n  (NIPS) 2014, Montreal, Canada", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deriving from the gradient vector of a generative model of local features,\nFisher vector coding (FVC) has been identified as an effective coding method\nfor image classification. Most, if not all, % FVC implementations employ the\nGaussian mixture model (GMM) to characterize the generation process of local\nfeatures. This choice has shown to be sufficient for traditional low\ndimensional local features, e.g., SIFT; and typically, good performance can be\nachieved with only a few hundred Gaussian distributions. However, the same\nnumber of Gaussians is insufficient to model the feature space spanned by\nhigher dimensional local features, which have become popular recently. In order\nto improve the modeling capacity for high dimensional features, it turns out to\nbe inefficient and computationally impractical to simply increase the number of\nGaussians. In this paper, we propose a model in which each local feature is\ndrawn from a Gaussian distribution whose mean vector is sampled from a\nsubspace. With certain approximation, this model can be converted to a sparse\ncoding procedure and the learning/inference problems can be readily solved by\nstandard sparse coding methods. By calculating the gradient vector of the\nproposed model, we derive a new fisher vector encoding strategy, termed Sparse\nCoding based Fisher Vector Coding (SCFVC). Moreover, we adopt the recently\ndeveloped Deep Convolutional Neural Network (CNN) descriptor as a high\ndimensional local feature and implement image classification with the proposed\nSCFVC. Our experimental evaluations demonstrate that our method not only\nsignificantly outperforms the traditional GMM based Fisher vector encoding but\nalso achieves the state-of-the-art performance in generic object recognition,\nindoor scene, and fine-grained image classification problems.\n", "versions": [{"version": "v1", "created": "Mon, 24 Nov 2014 10:48:47 GMT"}], "update_date": "2014-11-25", "authors_parsed": [["Liu", "Lingqiao", ""], ["Shen", "Chunhua", ""], ["Wang", "Lei", ""], ["Hengel", "Anton van den", ""], ["Wang", "Chao", ""]]}, {"id": "1411.6447", "submitter": "Tianjun Xiao", "authors": "Tianjun Xiao, Yichong Xu, Kuiyuan Yang, Jiaxing Zhang, Yuxin Peng,\n  Zheng Zhang", "title": "The Application of Two-level Attention Models in Deep Convolutional\n  Neural Network for Fine-grained Image Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fine-grained classification is challenging because categories can only be\ndiscriminated by subtle and local differences. Variances in the pose, scale or\nrotation usually make the problem more difficult. Most fine-grained\nclassification systems follow the pipeline of finding foreground object or\nobject parts (where) to extract discriminative features (what).\n  In this paper, we propose to apply visual attention to fine-grained\nclassification task using deep neural network. Our pipeline integrates three\ntypes of attention: the bottom-up attention that propose candidate patches, the\nobject-level top-down attention that selects relevant patches to a certain\nobject, and the part-level top-down attention that localizes discriminative\nparts. We combine these attentions to train domain-specific deep nets, then use\nit to improve both the what and where aspects. Importantly, we avoid using\nexpensive annotations like bounding box or part information from end-to-end.\nThe weak supervision constraint makes our work easier to generalize.\n  We have verified the effectiveness of the method on the subsets of ILSVRC2012\ndataset and CUB200_2011 dataset. Our pipeline delivered significant\nimprovements and achieved the best accuracy under the weakest supervision\ncondition. The performance is competitive against other methods that rely on\nadditional annotations.\n", "versions": [{"version": "v1", "created": "Mon, 24 Nov 2014 13:30:07 GMT"}], "update_date": "2014-11-25", "authors_parsed": [["Xiao", "Tianjun", ""], ["Xu", "Yichong", ""], ["Yang", "Kuiyuan", ""], ["Zhang", "Jiaxing", ""], ["Peng", "Yuxin", ""], ["Zhang", "Zheng", ""]]}, {"id": "1411.6509", "submitter": "Ali Sharif Razavian", "authors": "Ali Sharif Razavian, Hossein Azizpour, Atsuto Maki, Josephine\n  Sullivan, Carl Henrik Ek, Stefan Carlsson", "title": "Persistent Evidence of Local Image Properties in Generic ConvNets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Supervised training of a convolutional network for object classification\nshould make explicit any information related to the class of objects and\ndisregard any auxiliary information associated with the capture of the image or\nthe variation within the object class. Does this happen in practice? Although\nthis seems to pertain to the very final layers in the network, if we look at\nearlier layers we find that this is not the case. Surprisingly, strong spatial\ninformation is implicit. This paper addresses this, in particular, exploiting\nthe image representation at the first fully connected layer, i.e. the global\nimage descriptor which has been recently shown to be most effective in a range\nof visual recognition tasks. We empirically demonstrate evidences for the\nfinding in the contexts of four different tasks: 2d landmark detection, 2d\nobject keypoints prediction, estimation of the RGB values of input image, and\nrecovery of semantic label of each pixel. We base our investigation on a simple\nframework with ridge rigression commonly across these tasks, and show results\nwhich all support our insight. Such spatial information can be used for\ncomputing correspondence of landmarks to a good accuracy, but should\npotentially be useful for improving the training of the convolutional nets for\nclassification purposes.\n", "versions": [{"version": "v1", "created": "Mon, 24 Nov 2014 16:17:15 GMT"}], "update_date": "2014-11-25", "authors_parsed": [["Razavian", "Ali Sharif", ""], ["Azizpour", "Hossein", ""], ["Maki", "Atsuto", ""], ["Sullivan", "Josephine", ""], ["Ek", "Carl Henrik", ""], ["Carlsson", "Stefan", ""]]}, {"id": "1411.6660", "submitter": "Zhenzhong Lan", "authors": "Zhenzhong Lan, Ming Lin, Xuanchong Li, Alexander G. Hauptmann, Bhiksha\n  Raj", "title": "Beyond Gaussian Pyramid: Multi-skip Feature Stacking for Action\n  Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most state-of-the-art action feature extractors involve differential\noperators, which act as highpass filters and tend to attenuate low frequency\naction information. This attenuation introduces bias to the resulting features\nand generates ill-conditioned feature matrices. The Gaussian Pyramid has been\nused as a feature enhancing technique that encodes scale-invariant\ncharacteristics into the feature space in an attempt to deal with this\nattenuation. However, at the core of the Gaussian Pyramid is a convolutional\nsmoothing operation, which makes it incapable of generating new features at\ncoarse scales. In order to address this problem, we propose a novel feature\nenhancing technique called Multi-skIp Feature Stacking (MIFS), which stacks\nfeatures extracted using a family of differential filters parameterized with\nmultiple time skips and encodes shift-invariance into the frequency space. MIFS\ncompensates for information lost from using differential operators by\nrecapturing information at coarse scales. This recaptured information allows us\nto match actions at different speeds and ranges of motion. We prove that MIFS\nenhances the learnability of differential-based features exponentially. The\nresulting feature matrices from MIFS have much smaller conditional numbers and\nvariances than those from conventional methods. Experimental results show\nsignificantly improved performance on challenging action recognition and event\ndetection tasks. Specifically, our method exceeds the state-of-the-arts on\nHollywood2, UCF101 and UCF50 datasets and is comparable to state-of-the-arts on\nHMDB51 and Olympics Sports datasets. MIFS can also be used as a speedup\nstrategy for feature extraction with minimal or no accuracy cost.\n", "versions": [{"version": "v1", "created": "Mon, 24 Nov 2014 21:40:09 GMT"}, {"version": "v2", "created": "Sat, 21 Mar 2015 19:22:51 GMT"}, {"version": "v3", "created": "Fri, 10 Apr 2015 19:25:22 GMT"}, {"version": "v4", "created": "Sun, 19 Apr 2015 19:13:42 GMT"}], "update_date": "2015-07-14", "authors_parsed": [["Lan", "Zhenzhong", ""], ["Lin", "Ming", ""], ["Li", "Xuanchong", ""], ["Hauptmann", "Alexander G.", ""], ["Raj", "Bhiksha", ""]]}, {"id": "1411.6836", "submitter": "Mircea Cimpoi", "authors": "Mircea Cimpoi, Subhransu Maji, Andrea Vedaldi", "title": "Deep convolutional filter banks for texture recognition and segmentation", "comments": "Accepted to CVPR15", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Research in texture recognition often concentrates on the problem of material\nrecognition in uncluttered conditions, an assumption rarely met by\napplications. In this work we conduct a first study of material and describable\ntexture at- tributes recognition in clutter, using a new dataset derived from\nthe OpenSurface texture repository. Motivated by the challenge posed by this\nproblem, we propose a new texture descriptor, D-CNN, obtained by Fisher Vector\npooling of a Convolutional Neural Network (CNN) filter bank. D-CNN\nsubstantially improves the state-of-the-art in texture, mate- rial and scene\nrecognition. Our approach achieves 82.3% accuracy on Flickr material dataset\nand 81.1% accuracy on MIT indoor scenes, providing absolute gains of more than\n10% over existing approaches. D-CNN easily trans- fers across domains without\nrequiring feature adaptation as for methods that build on the fully-connected\nlayers of CNNs. Furthermore, D-CNN can seamlessly incorporate multi-scale\ninformation and describe regions of arbitrary shapes and sizes. Our approach is\nparticularly suited at lo- calizing stuff categories and obtains\nstate-of-the-art re- sults on MSRC segmentation dataset, as well as promising\nresults on recognizing materials and surface attributes in clutter on the\nOpenSurfaces dataset.\n", "versions": [{"version": "v1", "created": "Tue, 25 Nov 2014 12:36:23 GMT"}, {"version": "v2", "created": "Thu, 9 Jul 2015 18:25:43 GMT"}], "update_date": "2015-07-10", "authors_parsed": [["Cimpoi", "Mircea", ""], ["Maji", "Subhransu", ""], ["Vedaldi", "Andrea", ""]]}, {"id": "1411.6850", "submitter": "Khalid Jebari hassani", "authors": "Amina Dik, Khalid Jebari, Abdelaziz Bouroumi and Aziz Ettouhami", "title": "Similarity- based approach for outlier detection", "comments": "International Journal of Computer Science Issues 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a new approach for detecting outliers by introducing the\nnotion of object's proximity. The main idea is that normal point has similar\ncharacteristics with several neighbors. So the point in not an outlier if it\nhas a high degree of proximity and its neighbors are several. The performance\nof this approach is illustrated through real datasets\n", "versions": [{"version": "v1", "created": "Tue, 25 Nov 2014 13:13:47 GMT"}], "update_date": "2014-11-26", "authors_parsed": [["Dik", "Amina", ""], ["Jebari", "Khalid", ""], ["Bouroumi", "Abdelaziz", ""], ["Ettouhami", "Aziz", ""]]}, {"id": "1411.6880", "submitter": "William Gray Roncal", "authors": "William Gray Roncal, Dean M. Kleissas, Joshua T. Vogelstein, Priya\n  Manavalan, Kunal Lillaney, Michael Pekala, Randal Burns, R. Jacob Vogelstein,\n  Carey E. Priebe, Mark A. Chevillet, Gregory D. Hager", "title": "An Automated Images-to-Graphs Framework for High Resolution Connectomics", "comments": "13 pages, first two authors contributed equally V2: Added additional\n  experiments and clarifications; added information on infrastructure and\n  pipeline environment", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reconstructing a map of neuronal connectivity is a critical challenge in\ncontemporary neuroscience. Recent advances in high-throughput serial section\nelectron microscopy (EM) have produced massive 3D image volumes of nanoscale\nbrain tissue for the first time. The resolution of EM allows for individual\nneurons and their synaptic connections to be directly observed. Recovering\nneuronal networks by manually tracing each neuronal process at this scale is\nunmanageable, and therefore researchers are developing automated image\nprocessing modules. Thus far, state-of-the-art algorithms focus only on the\nsolution to a particular task (e.g., neuron segmentation or synapse\nidentification).\n  In this manuscript we present the first fully automated images-to-graphs\npipeline (i.e., a pipeline that begins with an imaged volume of neural tissue\nand produces a brain graph without any human interaction). To evaluate overall\nperformance and select the best parameters and methods, we also develop a\nmetric to assess the quality of the output graphs. We evaluate a set of\nalgorithms and parameters, searching possible operating points to identify the\nbest available brain graph for our assessment metric. Finally, we deploy a\nreference end-to-end version of the pipeline on a large, publicly available\ndata set. This provides a baseline result and framework for community analysis\nand future algorithm development and testing. All code and data derivatives\nhave been made publicly available toward eventually unlocking new biofidelic\ncomputational primitives and understanding of neuropathologies.\n", "versions": [{"version": "v1", "created": "Tue, 25 Nov 2014 14:37:47 GMT"}, {"version": "v2", "created": "Thu, 30 Apr 2015 06:04:10 GMT"}], "update_date": "2015-05-01", "authors_parsed": [["Roncal", "William Gray", ""], ["Kleissas", "Dean M.", ""], ["Vogelstein", "Joshua T.", ""], ["Manavalan", "Priya", ""], ["Lillaney", "Kunal", ""], ["Pekala", "Michael", ""], ["Burns", "Randal", ""], ["Vogelstein", "R. Jacob", ""], ["Priebe", "Carey E.", ""], ["Chevillet", "Mark A.", ""], ["Hager", "Gregory D.", ""]]}, {"id": "1411.6909", "submitter": "Aaron Hertzmann", "authors": "Hamid Izadinia, Ali Farhadi, Aaron Hertzmann, Matthew D. Hoffman", "title": "Image Classification and Retrieval from User-Supplied Tags", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes direct learning of image classification from\nuser-supplied tags, without filtering. Each tag is supplied by the user who\nshared the image online. Enormous numbers of these tags are freely available\nonline, and they give insight about the image categories important to users and\nto image classification. Our approach is complementary to the conventional\napproach of manual annotation, which is extremely costly. We analyze of the\nFlickr 100 Million Image dataset, making several useful observations about the\nstatistics of these tags. We introduce a large-scale robust classification\nalgorithm, in order to handle the inherent noise in these tags, and a\ncalibration procedure to better predict objective annotations. We show that\nfreely available, user-supplied tags can obtain similar or superior results to\nlarge databases of costly manual annotations.\n", "versions": [{"version": "v1", "created": "Tue, 25 Nov 2014 16:17:09 GMT"}], "update_date": "2014-11-26", "authors_parsed": [["Izadinia", "Hamid", ""], ["Farhadi", "Ali", ""], ["Hertzmann", "Aaron", ""], ["Hoffman", "Matthew D.", ""]]}, {"id": "1411.6970", "submitter": "Stephan Saalfeld", "authors": "Philipp Hanslovsky, John A. Bogovic, Stephan Saalfeld (HHMI Janelia\n  Research Campus)", "title": "Post-acquisition image based compensation for thickness variation in\n  microscopy section series", "comments": null, "journal-ref": "IEEE International Symposium on Biomedical Imaging, 2015, pages\n  507--511", "doi": "10.1109/ISBI.2015.7163922", "report-no": null, "categories": "cs.CV q-bio.QM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Serial section Microscopy is an established method for volumetric anatomy\nreconstruction. Section series imaged with Electron Microscopy are currently\nvital for the reconstruction of the synaptic connectivity of entire animal\nbrains such as that of Drosophila melanogaster. The process of removing\nultrathin layers from a solid block containing the specimen, however, is a\nfragile procedure and has limited precision with respect to section thickness.\nWe have developed a method to estimate the relative z-position of each\nindividual section as a function of signal change across the section series.\nFirst experiments show promising results on both serial section Transmission\nElectron Microscopy (ssTEM) data and Focused Ion Beam Scanning Electron\nMicroscopy (FIB-SEM) series. We made our solution available as Open Source\nplugins for the TrakEM2 software and the ImageJ distribution Fiji.\n", "versions": [{"version": "v1", "created": "Tue, 25 Nov 2014 19:01:12 GMT"}, {"version": "v2", "created": "Wed, 27 May 2015 19:39:10 GMT"}], "update_date": "2016-06-20", "authors_parsed": [["Hanslovsky", "Philipp", "", "HHMI Janelia\n  Research Campus"], ["Bogovic", "John A.", "", "HHMI Janelia\n  Research Campus"], ["Saalfeld", "Stephan", "", "HHMI Janelia\n  Research Campus"]]}, {"id": "1411.7113", "submitter": "Mohamed Aly", "authors": "Mohamed Aly", "title": "Real time Detection of Lane Markers in Urban Streets", "comments": "6 pages", "journal-ref": "IEEE Intelligent Vehicles Symposium, Eindhoven, The Netherlands,\n  June 2008", "doi": "10.1109/IVS.2008.4621152", "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a robust and real time approach to lane marker detection in urban\nstreets. It is based on generating a top view of the road, filtering using\nselective oriented Gaussian filters, using RANSAC line fitting to give initial\nguesses to a new and fast RANSAC algorithm for fitting Bezier Splines, which is\nthen followed by a post-processing step. Our algorithm can detect all lanes in\nstill images of the street in various conditions, while operating at a rate of\n50 Hz and achieving comparable results to previous techniques.\n", "versions": [{"version": "v1", "created": "Wed, 26 Nov 2014 05:50:02 GMT"}], "update_date": "2014-11-27", "authors_parsed": [["Aly", "Mohamed", ""]]}, {"id": "1411.7336", "submitter": "Mohammed  alzaidi", "authors": "Mohammed A. Talab, Siti Norul Huda Sheikh Abdullah, Bilal Bataineh", "title": "Edge direction matrixes-based local binar patterns descriptor for shape\n  pattern recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  Shapes and texture image recognition usage is an essential branch of pattern\nrecognition. It is made up of techniques that aim at extracting information\nfrom images via human knowledge and works. Local Binary Pattern (LBP) ensures\nencoding global and local information and scaling invariance by introducing a\nlook-up table to reflect the uniformity structure of an object. However, edge\ndirection matrixes (EDMS) only apply global invariant descriptor which employs\nfirst and secondary order relationships. The main idea behind this methodology\nis the need of improved recognition capabilities, a goal achieved by the\ncombinative use of these descriptors. This collaboration aims to make use of\nthe major advantages each one presents, by simultaneously complementing each\nother, in order to elevate their weak points. By using multiple classifier\napproaches such as random forest and multi-layer perceptron neural network, the\nproposed combinative descriptor are compared with the state of the art\ncombinative methods based on Gray-Level Co-occurrence matrix (GLCM with EDMS),\nLBP and moment invariant on four benchmark dataset MPEG-7 CE-Shape-1, KTH-TIPS\nimage, Enghlishfnt and Arabic calligraphy . The experiments have shown the\nsuperiority of the introduced descriptor over the GLCM with EDMS, LBP and\nmoment invariants and other well-known descriptor such as Scale Invariant\nFeature Transform from the literature.\n", "versions": [{"version": "v1", "created": "Wed, 26 Nov 2014 19:12:33 GMT"}], "update_date": "2014-11-27", "authors_parsed": [["Talab", "Mohammed A.", ""], ["Abdullah", "Siti Norul Huda Sheikh", ""], ["Bataineh", "Bilal", ""]]}, {"id": "1411.7399", "submitter": "Lior Wolf", "authors": "Benjamin Klein, Guy Lev, Gil Sadeh, Lior Wolf", "title": "Fisher Vectors Derived from Hybrid Gaussian-Laplacian Mixture Models for\n  Image Annotation", "comments": "new version includes text synthesis by an RNN and experiments with\n  the COCO benchmark", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the traditional object recognition pipeline, descriptors are densely\nsampled over an image, pooled into a high dimensional non-linear representation\nand then passed to a classifier. In recent years, Fisher Vectors have proven\nempirically to be the leading representation for a large variety of\napplications. The Fisher Vector is typically taken as the gradients of the\nlog-likelihood of descriptors, with respect to the parameters of a Gaussian\nMixture Model (GMM). Motivated by the assumption that different distributions\nshould be applied for different datasets, we present two other Mixture Models\nand derive their Expectation-Maximization and Fisher Vector expressions. The\nfirst is a Laplacian Mixture Model (LMM), which is based on the Laplacian\ndistribution. The second Mixture Model presented is a Hybrid Gaussian-Laplacian\nMixture Model (HGLMM) which is based on a weighted geometric mean of the\nGaussian and Laplacian distribution. An interesting property of the\nExpectation-Maximization algorithm for the latter is that in the maximization\nstep, each dimension in each component is chosen to be either a Gaussian or a\nLaplacian. Finally, by using the new Fisher Vectors derived from HGLMMs, we\nachieve state-of-the-art results for both the image annotation and the image\nsearch by a sentence tasks.\n", "versions": [{"version": "v1", "created": "Wed, 26 Nov 2014 21:21:51 GMT"}, {"version": "v2", "created": "Sat, 24 Jan 2015 20:03:50 GMT"}], "update_date": "2015-01-27", "authors_parsed": [["Klein", "Benjamin", ""], ["Lev", "Guy", ""], ["Sadeh", "Gil", ""], ["Wolf", "Lior", ""]]}, {"id": "1411.7445", "submitter": "Chao Xu", "authors": "Tao Han, Chao Xu, Ryan Loxton, Lei Xie", "title": "Bi-objective Optimization for Robust RGB-D Visual Odometry", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers a new bi-objective optimization formulation for robust\nRGB-D visual odometry. We investigate two methods for solving the proposed\nbi-objective optimization problem: the weighted sum method (in which the\nobjective functions are combined into a single objective function) and the\nbounded objective method (in which one of the objective functions is optimized\nand the value of the other objective function is bounded via a constraint). Our\nexperimental results for the open source TUM RGB-D dataset show that the new\nbi-objective optimization formulation is superior to several existing RGB-D\nodometry methods. In particular, the new formulation yields more accurate\nmotion estimates and is more robust when textural or structural features in the\nimage sequence are lacking.\n", "versions": [{"version": "v1", "created": "Thu, 27 Nov 2014 02:37:41 GMT"}], "update_date": "2014-12-01", "authors_parsed": [["Han", "Tao", ""], ["Xu", "Chao", ""], ["Loxton", "Ryan", ""], ["Xie", "Lei", ""]]}, {"id": "1411.7466", "submitter": "Chunhua Shen", "authors": "Lingqiao Liu, Chunhua Shen, Anton van den Hengel", "title": "The Treasure beneath Convolutional Layers: Cross-convolutional-layer\n  Pooling for Image Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A number of recent studies have shown that a Deep Convolutional Neural\nNetwork (DCNN) pretrained on a large dataset can be adopted as a universal\nimage description which leads to astounding performance in many visual\nclassification tasks. Most of these studies, if not all, adopt activations of\nthe fully-connected layer of a DCNN as the image or region representation and\nit is believed that convolutional layer activations are less discriminative.\n  This paper, however, advocates that if used appropriately convolutional layer\nactivations can be turned into a powerful image representation which enjoys\nmany advantages over fully-connected layer activations. This is achieved by\nadopting a new technique proposed in this paper called\ncross-convolutional-layer pooling. More specifically, it extracts subarrays of\nfeature maps of one convolutional layer as local features and pools the\nextracted features with the guidance of feature maps of the successive\nconvolutional layer. Compared with exising methods that apply DCNNs in the\nlocal feature setting, the proposed method is significantly faster since it\nrequires much fewer times of DCNN forward computation. Moreover, it avoids the\ndomain mismatch issue which is usually encountered when applying fully\nconnected layer activations to describe local regions. By applying our method\nto four popular visual classification tasks, it is demonstrated that the\nproposed method can achieve comparable or in some cases significantly better\nperformance than existing fully-connected layer based image representations\nwhile incurring much lower computational cost.\n", "versions": [{"version": "v1", "created": "Thu, 27 Nov 2014 04:12:57 GMT"}], "update_date": "2014-12-01", "authors_parsed": [["Liu", "Lingqiao", ""], ["Shen", "Chunhua", ""], ["Hengel", "Anton van den", ""]]}, {"id": "1411.7564", "submitter": "Chunhua Shen", "authors": "Peng Wang, Chunhua Shen, Anton van den Hengel, Philip H. S. Torr", "title": "Large-scale Binary Quadratic Optimization Using Semidefinite Relaxation\n  and Applications", "comments": "Fixed some typos. 18 pages. Accepted to IEEE Transactions on Pattern\n  Analysis and Machine Intelligence", "journal-ref": null, "doi": "10.1109/TPAMI.2016.2541146", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In computer vision, many problems such as image segmentation, pixel\nlabelling, and scene parsing can be formulated as binary quadratic programs\n(BQPs). For submodular problems, cuts based methods can be employed to\nefficiently solve large-scale problems. However, general nonsubmodular problems\nare significantly more challenging to solve. Finding a solution when the\nproblem is of large size to be of practical interest, however, typically\nrequires relaxation. Two standard relaxation methods are widely used for\nsolving general BQPs--spectral methods and semidefinite programming (SDP), each\nwith their own advantages and disadvantages. Spectral relaxation is simple and\neasy to implement, but its bound is loose. Semidefinite relaxation has a\ntighter bound, but its computational complexity is high, especially for large\nscale problems. In this work, we present a new SDP formulation for BQPs, with\ntwo desirable properties. First, it has a similar relaxation bound to\nconventional SDP formulations. Second, compared with conventional SDP methods,\nthe new SDP formulation leads to a significantly more efficient and scalable\ndual optimization approach, which has the same degree of complexity as spectral\nmethods. We then propose two solvers, namely, quasi-Newton and smoothing Newton\nmethods, for the dual problem. Both of them are significantly more efficiently\nthan standard interior-point methods. In practice, the smoothing Newton solver\nis faster than the quasi-Newton solver for dense or medium-sized problems,\nwhile the quasi-Newton solver is preferable for large sparse/structured\nproblems. Our experiments on a few computer vision applications including\nclustering, image segmentation, co-segmentation and registration show the\npotential of our SDP formulation for solving large-scale BQPs.\n", "versions": [{"version": "v1", "created": "Thu, 27 Nov 2014 12:05:06 GMT"}, {"version": "v2", "created": "Sun, 22 Nov 2015 23:27:30 GMT"}, {"version": "v3", "created": "Tue, 23 Feb 2016 02:19:18 GMT"}, {"version": "v4", "created": "Mon, 2 May 2016 00:32:58 GMT"}], "update_date": "2016-11-17", "authors_parsed": [["Wang", "Peng", ""], ["Shen", "Chunhua", ""], ["Hengel", "Anton van den", ""], ["Torr", "Philip H. S.", ""]]}, {"id": "1411.7591", "submitter": "Yedid Hoshen", "authors": "Yedid Hoshen and Shmuel Peleg", "title": "An Egocentric Look at Video Photographer Identity", "comments": null, "journal-ref": "Proc. CVPR'16, Las Vegas, June 2016, pp. 4284-4292", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Egocentric cameras are being worn by an increasing number of users, among\nthem many security forces worldwide. GoPro cameras already penetrated the mass\nmarket, reporting substantial increase in sales every year. As head-worn\ncameras do not capture the photographer, it may seem that the anonymity of the\nphotographer is preserved even when the video is publicly distributed.\n  We show that camera motion, as can be computed from the egocentric video,\nprovides unique identity information. The photographer can be reliably\nrecognized from a few seconds of video captured when walking. The proposed\nmethod achieves more than 90% recognition accuracy in cases where the random\nsuccess rate is only 3%.\n  Applications can include theft prevention by locking the camera when not worn\nby its rightful owner. Searching video sharing services (e.g. YouTube) for\negocentric videos shot by a specific photographer may also become possible. An\nimportant message in this paper is that photographers should be aware that\nsharing egocentric video will compromise their anonymity, even when their face\nis not visible.\n", "versions": [{"version": "v1", "created": "Thu, 27 Nov 2014 13:30:53 GMT"}, {"version": "v2", "created": "Sat, 25 Apr 2015 21:06:52 GMT"}, {"version": "v3", "created": "Sun, 8 Nov 2015 16:37:31 GMT"}], "update_date": "2017-01-05", "authors_parsed": [["Hoshen", "Yedid", ""], ["Peleg", "Shmuel", ""]]}, {"id": "1411.7655", "submitter": "Hocine Cherifi", "authors": "Mounir Omari, Mohammed El Hassouni, Abdelkaher Ait Abdelouahad, Hocine\n  Cherifi", "title": "A statistical reduced-reference method for color image quality\n  assessment", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although color is a fundamental feature of human visual perception, it has\nbeen largely unexplored in the reduced-reference (RR) image quality assessment\n(IQA) schemes. In this paper, we propose a natural scene statistic (NSS)\nmethod, which efficiently uses this information. It is based on the statistical\ndeviation between the steerable pyramid coefficients of the reference color\nimage and the degraded one. We propose and analyze the multivariate generalized\nGaussian distribution (MGGD) to model the underlying statistics. In order to\nquantify the degradation, we develop and evaluate two measures based\nrespectively on the Geodesic distance between two MGGDs and on the closed-form\nof the Kullback Leibler divergence. We performed an extensive evaluation of\nboth metrics in various color spaces (RGB, HSV, CIELAB and YCrCb) using the TID\n2008 benchmark and the FRTV Phase I validation process. Experimental results\ndemonstrate the effectiveness of the proposed framework to achieve a good\nconsistency with human visual perception. Furthermore, the best configuration\nis obtained with CIELAB color space associated to KLD deviation measure.\n", "versions": [{"version": "v1", "created": "Thu, 27 Nov 2014 17:24:59 GMT"}], "update_date": "2014-12-01", "authors_parsed": [["Omari", "Mounir", ""], ["Hassouni", "Mohammed El", ""], ["Abdelouahad", "Abdelkaher Ait", ""], ["Cherifi", "Hocine", ""]]}, {"id": "1411.7676", "submitter": "Stefano Soatto", "authors": "Stefano Soatto and Alessandro Chiuso", "title": "Visual Representations: Defining Properties and Deep Approximations", "comments": "UCLA CSD TR140023, Nov. 12, 2014, revised April 13, 2015, November\n  13, 2015, February 28, 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual representations are defined in terms of minimal sufficient statistics\nof visual data, for a class of tasks, that are also invariant to nuisance\nvariability. Minimal sufficiency guarantees that we can store a representation\nin lieu of raw data with smallest complexity and no performance loss on the\ntask at hand. Invariance guarantees that the statistic is constant with respect\nto uninformative transformations of the data. We derive analytical expressions\nfor such representations and show they are related to feature descriptors\ncommonly used in computer vision, as well as to convolutional neural networks.\nThis link highlights the assumptions and approximations tacitly assumed by\nthese methods and explains empirical practices such as clamping, pooling and\njoint normalization.\n", "versions": [{"version": "v1", "created": "Thu, 27 Nov 2014 18:42:49 GMT"}, {"version": "v2", "created": "Sat, 20 Dec 2014 04:21:08 GMT"}, {"version": "v3", "created": "Mon, 13 Apr 2015 16:40:34 GMT"}, {"version": "v4", "created": "Thu, 16 Apr 2015 02:47:26 GMT"}, {"version": "v5", "created": "Fri, 17 Apr 2015 18:56:39 GMT"}, {"version": "v6", "created": "Tue, 9 Jun 2015 19:09:01 GMT"}, {"version": "v7", "created": "Sat, 21 Nov 2015 15:42:18 GMT"}, {"version": "v8", "created": "Sun, 3 Jan 2016 22:29:22 GMT"}, {"version": "v9", "created": "Mon, 29 Feb 2016 18:37:32 GMT"}], "update_date": "2016-06-29", "authors_parsed": [["Soatto", "Stefano", ""], ["Chiuso", "Alessandro", ""]]}, {"id": "1411.7682", "submitter": "Hocine Cherifi", "authors": "Mounir Omari, Mohammed El Hassouni, Hocine Cherifi and Abdelkaher Ait\n  Abdelouahad", "title": "On color image quality assessment using natural image statistics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Color distortion can introduce a significant damage in visual quality\nperception, however, most of existing reduced-reference quality measures are\ndesigned for grayscale images. In this paper, we consider a basic extension of\nwell-known image-statistics based quality assessment measures to color images.\nIn order to evaluate the impact of color information on the measures\nefficiency, two color spaces are investigated: RGB and CIELAB. Results of an\nextensive evaluation using TID 2013 benchmark demonstrates that significant\nimprovement can be achieved for a great number of distortion type when the\nCIELAB color representation is used.\n", "versions": [{"version": "v1", "created": "Thu, 27 Nov 2014 19:04:30 GMT"}], "update_date": "2014-12-01", "authors_parsed": [["Omari", "Mounir", ""], ["Hassouni", "Mohammed El", ""], ["Cherifi", "Hocine", ""], ["Abdelouahad", "Abdelkaher Ait", ""]]}, {"id": "1411.7714", "submitter": "Marius Leordeanu", "authors": "Marius Leordeanu and Alexandra Radu and Rahul Sukthankar", "title": "Features in Concert: Discriminative Feature Selection meets Unsupervised\n  Clustering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Feature selection is an essential problem in computer vision, important for\ncategory learning and recognition. Along with the rapid development of a wide\nvariety of visual features and classifiers, there is a growing need for\nefficient feature selection and combination methods, to construct powerful\nclassifiers for more complex and higher-level recognition tasks. We propose an\nalgorithm that efficiently discovers sparse, compact representations of input\nfeatures or classifiers, from a vast sea of candidates, with important\noptimality properties, low computational cost and excellent accuracy in\npractice. Different from boosting, we start with a discriminant linear\nclassification formulation that encourages sparse solutions. Then we obtain an\nequivalent unsupervised clustering problem that jointly discovers ensembles of\ndiverse features. They are independently valuable but even more powerful when\nunited in a cluster of classifiers. We evaluate our method on the task of\nlarge-scale recognition in video and show that it significantly outperforms\nclassical selection approaches, such as AdaBoost and greedy forward-backward\nselection, and powerful classifiers such as SVMs, in speed of training and\nperformance, especially in the case of limited training data.\n", "versions": [{"version": "v1", "created": "Thu, 27 Nov 2014 22:37:58 GMT"}], "update_date": "2014-12-01", "authors_parsed": [["Leordeanu", "Marius", ""], ["Radu", "Alexandra", ""], ["Sukthankar", "Rahul", ""]]}, {"id": "1411.7715", "submitter": "Artem Rozantsev Mr.", "authors": "Artem Rozantsev, Vincent Lepetit, Pascal Fua", "title": "Flying Objects Detection from a Single Moving Camera", "comments": null, "journal-ref": null, "doi": "10.1109/CVPR.2015.7299040", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an approach to detect flying objects such as UAVs and aircrafts\nwhen they occupy a small portion of the field of view, possibly moving against\ncomplex backgrounds, and are filmed by a camera that itself moves.\n  Solving such a difficult problem requires combining both appearance and\nmotion cues. To this end we propose a regression-based approach to motion\nstabilization of local image patches that allows us to achieve effective\nclassification on spatio-temporal image cubes and outperform state-of-the-art\ntechniques.\n  As the problem is relatively new, we collected two challenging datasets for\nUAVs and Aircrafts, which can be used as benchmarks for flying objects\ndetection and vision-guided collision avoidance.\n", "versions": [{"version": "v1", "created": "Thu, 27 Nov 2014 22:39:50 GMT"}], "update_date": "2015-12-14", "authors_parsed": [["Rozantsev", "Artem", ""], ["Lepetit", "Vincent", ""], ["Fua", "Pascal", ""]]}, {"id": "1411.7766", "submitter": "Ziwei Liu", "authors": "Ziwei Liu, Ping Luo, Xiaogang Wang, Xiaoou Tang", "title": "Deep Learning Face Attributes in the Wild", "comments": "To appear in International Conference on Computer Vision (ICCV) 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Predicting face attributes in the wild is challenging due to complex face\nvariations. We propose a novel deep learning framework for attribute prediction\nin the wild. It cascades two CNNs, LNet and ANet, which are fine-tuned jointly\nwith attribute tags, but pre-trained differently. LNet is pre-trained by\nmassive general object categories for face localization, while ANet is\npre-trained by massive face identities for attribute prediction. This framework\nnot only outperforms the state-of-the-art with a large margin, but also reveals\nvaluable facts on learning face representation.\n  (1) It shows how the performances of face localization (LNet) and attribute\nprediction (ANet) can be improved by different pre-training strategies.\n  (2) It reveals that although the filters of LNet are fine-tuned only with\nimage-level attribute tags, their response maps over entire images have strong\nindication of face locations. This fact enables training LNet for face\nlocalization with only image-level annotations, but without face bounding boxes\nor landmarks, which are required by all attribute recognition works.\n  (3) It also demonstrates that the high-level hidden neurons of ANet\nautomatically discover semantic concepts after pre-training with massive face\nidentities, and such concepts are significantly enriched after fine-tuning with\nattribute tags. Each attribute can be well explained with a sparse linear\ncombination of these concepts.\n", "versions": [{"version": "v1", "created": "Fri, 28 Nov 2014 07:13:54 GMT"}, {"version": "v2", "created": "Thu, 10 Sep 2015 06:05:30 GMT"}, {"version": "v3", "created": "Thu, 24 Sep 2015 13:52:26 GMT"}], "update_date": "2015-09-25", "authors_parsed": [["Liu", "Ziwei", ""], ["Luo", "Ping", ""], ["Wang", "Xiaogang", ""], ["Tang", "Xiaoou", ""]]}, {"id": "1411.7798", "submitter": "Ran He", "authors": "Ran He and Man Zhang and Liang Wang and Ye Ji and Qiyue Yin", "title": "Cross-Modal Learning via Pairwise Constraints", "comments": "12 pages, 5 figures, 70 references", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In multimedia applications, the text and image components in a web document\nform a pairwise constraint that potentially indicates the same semantic\nconcept. This paper studies cross-modal learning via the pairwise constraint,\nand aims to find the common structure hidden in different modalities. We first\npropose a compound regularization framework to deal with the pairwise\nconstraint, which can be used as a general platform for developing cross-modal\nalgorithms. For unsupervised learning, we propose a cross-modal subspace\nclustering method to learn a common structure for different modalities. For\nsupervised learning, to reduce the semantic gap and the outliers in pairwise\nconstraints, we propose a cross-modal matching method based on compound ?21\nregularization along with an iteratively reweighted algorithm to find the\nglobal optimum. Extensive experiments demonstrate the benefits of joint text\nand image modeling with semantically induced pairwise constraints, and show\nthat the proposed cross-modal methods can further reduce the semantic gap\nbetween different modalities and improve the clustering/retrieval accuracy.\n", "versions": [{"version": "v1", "created": "Fri, 28 Nov 2014 10:11:03 GMT"}], "update_date": "2014-12-01", "authors_parsed": [["He", "Ran", ""], ["Zhang", "Man", ""], ["Wang", "Liang", ""], ["Ji", "Ye", ""], ["Yin", "Qiyue", ""]]}, {"id": "1411.7855", "submitter": "\\\"Orjan Stenflo", "authors": "Franklin Mendivil and \\\"Orjan Stenflo", "title": "V-variable image compression", "comments": "15 pages, 22 figures", "journal-ref": "Fractals, 23, no 02, (2015)", "doi": "10.1142/S0218348X15500073", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  V-variable fractals, where $V$ is a positive integer, are intuitively\nfractals with at most $V$ different \"forms\" or \"shapes\" at all levels of\nmagnification. In this paper we describe how V-variable fractals can be used\nfor the purpose of image compression.\n", "versions": [{"version": "v1", "created": "Fri, 28 Nov 2014 13:18:22 GMT"}], "update_date": "2015-06-03", "authors_parsed": [["Mendivil", "Franklin", ""], ["Stenflo", "\u00d6rjan", ""]]}, {"id": "1411.7883", "submitter": "Luca Del Pero", "authors": "Luca Del Pero, Susanna Ricco, Rahul Sukthankar, Vittorio Ferrari", "title": "Articulated motion discovery using pairs of trajectories", "comments": "10 pages, 5 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an unsupervised approach for discovering characteristic motion\npatterns in videos of highly articulated objects performing natural, unscripted\nbehaviors, such as tigers in the wild. We discover consistent patterns in a\nbottom-up manner by analyzing the relative displacements of large numbers of\nordered trajectory pairs through time, such that each trajectory is attached to\na different moving part on the object. The pairs of trajectories descriptor\nrelies entirely on motion and is more discriminative than state-of-the-art\nfeatures that employ single trajectories. Our method generates temporal video\nintervals, each automatically trimmed to one instance of the discovered\nbehavior, and clusters them by type (e.g., running, turning head, drinking\nwater). We present experiments on two datasets: dogs from YouTube-Objects and a\nnew dataset of National Geographic tiger videos. Results confirm that our\nproposed descriptor outperforms existing appearance- and trajectory-based\ndescriptors (e.g., HOG and DTFs) on both datasets and enables us to segment\nunconstrained animal video into intervals containing single behaviors.\n", "versions": [{"version": "v1", "created": "Fri, 28 Nov 2014 14:43:03 GMT"}, {"version": "v2", "created": "Tue, 16 Dec 2014 13:56:07 GMT"}, {"version": "v3", "created": "Fri, 24 Apr 2015 15:29:06 GMT"}], "update_date": "2015-04-27", "authors_parsed": [["Del Pero", "Luca", ""], ["Ricco", "Susanna", ""], ["Sukthankar", "Rahul", ""], ["Ferrari", "Vittorio", ""]]}, {"id": "1411.7889", "submitter": "Aliakbar Jafarpour", "authors": "Aliakbar Jafarpour", "title": "Open-source code for manifold-based 3D rotation recovery of X-ray\n  scattering patterns", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.optics cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Single particle 3D imaging with ultrashort X-ray laser pulses is based on\ncollecting and combining the information content of 2D scattering patterns of\nan object at different orientations. Typical sample-delivery schemes leave\nlittle or no room for controlling the orientations. As such, the orientation\nassociated with a given snapshot should be estimated after the experiment. Here\nwe present an open-source code for the most rigorous technique having been\nreported in this context. Some practical issues along with proposed solutions\nare also discussed.\n", "versions": [{"version": "v1", "created": "Wed, 26 Nov 2014 18:16:54 GMT"}], "update_date": "2014-12-01", "authors_parsed": [["Jafarpour", "Aliakbar", ""]]}, {"id": "1411.7911", "submitter": "Artem Rozantsev Mr.", "authors": "Artem Rozantsev, Vincent Lepetit, Pascal Fua", "title": "On Rendering Synthetic Images for Training an Object Detector", "comments": null, "journal-ref": null, "doi": "10.1016/j.cviu.2014.12.006", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel approach to synthesizing images that are effective for\ntraining object detectors. Starting from a small set of real images, our\nalgorithm estimates the rendering parameters required to synthesize similar\nimages given a coarse 3D model of the target object. These parameters can then\nbe reused to generate an unlimited number of training images of the object of\ninterest in arbitrary 3D poses, which can then be used to increase\nclassification performances.\n  A key insight of our approach is that the synthetically generated images\nshould be similar to real images, not in terms of image quality, but rather in\nterms of features used during the detector training. We show in the context of\ndrone, plane, and car detection that using such synthetically generated images\nyields significantly better performances than simply perturbing real images or\neven synthesizing images in such way that they look very realistic, as is often\ndone when only limited amounts of training data are available.\n", "versions": [{"version": "v1", "created": "Fri, 28 Nov 2014 15:41:11 GMT"}], "update_date": "2015-06-30", "authors_parsed": [["Rozantsev", "Artem", ""], ["Lepetit", "Vincent", ""], ["Fua", "Pascal", ""]]}, {"id": "1411.7923", "submitter": "Dong Yi", "authors": "Dong Yi, Zhen Lei, Shengcai Liao, and Stan Z. Li", "title": "Learning Face Representation from Scratch", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pushing by big data and deep convolutional neural network (CNN), the\nperformance of face recognition is becoming comparable to human. Using private\nlarge scale training datasets, several groups achieve very high performance on\nLFW, i.e., 97% to 99%. While there are many open source implementations of CNN,\nnone of large scale face dataset is publicly available. The current situation\nin the field of face recognition is that data is more important than algorithm.\nTo solve this problem, this paper proposes a semi-automatical way to collect\nface images from Internet and builds a large scale dataset containing about\n10,000 subjects and 500,000 images, called CASIAWebFace. Based on the database,\nwe use a 11-layer CNN to learn discriminative representation and obtain\nstate-of-theart accuracy on LFW and YTF. The publication of CASIAWebFace will\nattract more research groups entering this field and accelerate the development\nof face recognition in the wild.\n", "versions": [{"version": "v1", "created": "Fri, 28 Nov 2014 16:05:18 GMT"}], "update_date": "2014-12-01", "authors_parsed": [["Yi", "Dong", ""], ["Lei", "Zhen", ""], ["Liao", "Shengcai", ""], ["Li", "Stan Z.", ""]]}, {"id": "1411.7935", "submitter": "Laura Leal-Taix\\'e", "authors": "Laura Leal-Taix\\'e", "title": "Multiple object tracking with context awareness", "comments": "PhD thesis, Leibniz University Hannover, Germany", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multiple people tracking is a key problem for many applications such as\nsurveillance, animation or car navigation, and a key input for tasks such as\nactivity recognition. In crowded environments occlusions and false detections\nare common, and although there have been substantial advances in recent years,\ntracking is still a challenging task. Tracking is typically divided into two\nsteps: detection, i.e., locating the pedestrians in the image, and data\nassociation, i.e., linking detections across frames to form complete\ntrajectories.\n  For the data association task, approaches typically aim at developing new,\nmore complex formulations, which in turn put the focus on the optimization\ntechniques required to solve them. However, they still utilize very basic\ninformation such as distance between detections. In this thesis, I focus on the\ndata association task and argue that there is contextual information that has\nnot been fully exploited yet in the tracking community, mainly social context\nand spatial context coming from different views.\n", "versions": [{"version": "v1", "created": "Mon, 24 Nov 2014 09:24:24 GMT"}, {"version": "v2", "created": "Wed, 30 Nov 2016 12:23:11 GMT"}], "update_date": "2016-12-01", "authors_parsed": [["Leal-Taix\u00e9", "Laura", ""]]}, {"id": "1411.7964", "submitter": "Tal Hassner", "authors": "Tal Hassner, Shai Harel, Eran Paz, Roee Enbar", "title": "Effective Face Frontalization in Unconstrained Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  \"Frontalization\" is the process of synthesizing frontal facing views of faces\nappearing in single unconstrained photos. Recent reports have suggested that\nthis process may substantially boost the performance of face recognition\nsystems. This, by transforming the challenging problem of recognizing faces\nviewed from unconstrained viewpoints to the easier problem of recognizing faces\nin constrained, forward facing poses. Previous frontalization methods did this\nby attempting to approximate 3D facial shapes for each query image. We observe\nthat 3D face shape estimation from unconstrained photos may be a harder problem\nthan frontalization and can potentially introduce facial misalignments.\nInstead, we explore the simpler approach of using a single, unmodified, 3D\nsurface as an approximation to the shape of all input faces. We show that this\nleads to a straightforward, efficient and easy to implement method for\nfrontalization. More importantly, it produces aesthetic new frontal views and\nis surprisingly effective when used for face recognition and gender estimation.\n", "versions": [{"version": "v1", "created": "Fri, 28 Nov 2014 18:22:56 GMT"}], "update_date": "2014-12-01", "authors_parsed": [["Hassner", "Tal", ""], ["Harel", "Shai", ""], ["Paz", "Eran", ""], ["Enbar", "Roee", ""]]}]