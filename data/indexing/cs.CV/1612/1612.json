[{"id": "1612.00005", "submitter": "Anh Nguyen", "authors": "Anh Nguyen, Jeff Clune, Yoshua Bengio, Alexey Dosovitskiy, Jason\n  Yosinski", "title": "Plug & Play Generative Networks: Conditional Iterative Generation of\n  Images in Latent Space", "comments": "CVPR camera-ready", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generating high-resolution, photo-realistic images has been a long-standing\ngoal in machine learning. Recently, Nguyen et al. (2016) showed one interesting\nway to synthesize novel images by performing gradient ascent in the latent\nspace of a generator network to maximize the activations of one or multiple\nneurons in a separate classifier network. In this paper we extend this method\nby introducing an additional prior on the latent code, improving both sample\nquality and sample diversity, leading to a state-of-the-art generative model\nthat produces high quality images at higher resolutions (227x227) than previous\ngenerative models, and does so for all 1000 ImageNet categories. In addition,\nwe provide a unified probabilistic interpretation of related activation\nmaximization methods and call the general class of models \"Plug and Play\nGenerative Networks\". PPGNs are composed of 1) a generator network G that is\ncapable of drawing a wide range of image types and 2) a replaceable \"condition\"\nnetwork C that tells the generator what to draw. We demonstrate the generation\nof images conditioned on a class (when C is an ImageNet or MIT Places\nclassification network) and also conditioned on a caption (when C is an image\ncaptioning network). Our method also improves the state of the art of\nMultifaceted Feature Visualization, which generates the set of synthetic inputs\nthat activate a neuron in order to better understand how deep neural networks\noperate. Finally, we show that our model performs reasonably well at the task\nof image inpainting. While image models are used in this paper, the approach is\nmodality-agnostic and can be applied to many types of data.\n", "versions": [{"version": "v1", "created": "Wed, 30 Nov 2016 20:56:36 GMT"}, {"version": "v2", "created": "Wed, 12 Apr 2017 06:39:52 GMT"}], "update_date": "2017-04-13", "authors_parsed": [["Nguyen", "Anh", ""], ["Clune", "Jeff", ""], ["Bengio", "Yoshua", ""], ["Dosovitskiy", "Alexey", ""], ["Yosinski", "Jason", ""]]}, {"id": "1612.00056", "submitter": "Dario Prandi", "authors": "Jean-Paul Gauthier and Dario Prandi", "title": "Generalized Fourier-Bessel operator and almost-periodic interpolation\n  and approximation", "comments": "15 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.CV math.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider functions $f$ of two real variables, given as trigonometric\nfunctions over a finite set $F$ of frequencies. This set is assumed to be\nclosed under rotations in the frequency plane of angle $\\frac{2k\\pi}{M}$ for\nsome integer $M$. Firstly, we address the problem of evaluating these functions\nover a similar finite set $E$ in the space plane and, secondly, we address the\nproblems of interpolating or approximating a function $g$ of two variables by\nsuch an $f$ over the grid $E.$ In particular, for this aim, we establish an\nabstract factorization theorem for the evaluation function, which is a key\npoint for an efficient numerical solution to these problems. This result is\nbased on the very special structure of the group $SE(2,N)$, subgroup of the\ngroup $SE(2)$ of motions of the plane corresponding to discrete rotations,\nwhich is a maximally almost periodic group.\n  Although the motivation of this paper comes from our previous works on\nbiomimetic image reconstruction and pattern recognition, where these questions\nappear naturally, this topic is related with several classical problems: the\nFFT in polar coordinates, the Non Uniform FFT, the evaluation of general\ntrigonometric polynomials, and so on.\n", "versions": [{"version": "v1", "created": "Wed, 23 Nov 2016 08:12:34 GMT"}], "update_date": "2016-12-02", "authors_parsed": [["Gauthier", "Jean-Paul", ""], ["Prandi", "Dario", ""]]}, {"id": "1612.00085", "submitter": "Woo Hyun Nam", "authors": "Il Jun Ahn (1) and Woo Hyun Nam (1) ((1) Digital Media &\n  Communications R&D Center, Samsung Electronics, Seoul, Korea)", "title": "Texture Enhancement via High-Resolution Style Transfer for Single-Image\n  Super-Resolution", "comments": "Il Jun Ahn and Woo Hyun Nam contributed equally to this work.\n  Submitted to IEEE Transactions on Consumer Electronics", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, various deep-neural-network (DNN)-based approaches have been\nproposed for single-image super-resolution (SISR). Despite their promising\nresults on major structure regions such as edges and lines, they still suffer\nfrom limited performance on texture regions that consist of very complex and\nfine patterns. This is because, during the acquisition of a low-resolution (LR)\nimage via down-sampling, these regions lose most of the high frequency\ninformation necessary to represent the texture details. In this paper, we\npresent a novel texture enhancement framework for SISR to effectively improve\nthe spatial resolution in the texture regions as well as edges and lines. We\ncall our method, high-resolution (HR) style transfer algorithm. Our framework\nconsists of three steps: (i) generate an initial HR image from an interpolated\nLR image via an SISR algorithm, (ii) generate an HR style image from the\ninitial HR image via down-scaling and tiling, and (iii) combine the HR style\nimage with the initial HR image via a customized style transfer algorithm.\nHere, the HR style image is obtained by down-scaling the initial HR image and\nthen repetitively tiling it into an image of the same size as the HR image.\nThis down-scaling and tiling process comes from the idea that texture regions\nare often composed of small regions that similar in appearance albeit sometimes\ndifferent in scale. This process creates an HR style image that is rich in\ndetails, which can be used to restore high-frequency texture details back into\nthe initial HR image via the style transfer algorithm. Experimental results on\na number of texture datasets show that our proposed HR style transfer algorithm\nprovides more visually pleasing results compared with competitive methods.\n", "versions": [{"version": "v1", "created": "Thu, 1 Dec 2016 00:15:02 GMT"}], "update_date": "2016-12-02", "authors_parsed": [["Ahn", "Il Jun", ""], ["Nam", "Woo Hyun", ""]]}, {"id": "1612.00089", "submitter": "Luka \\v{C}ehovin", "authors": "Luka \\v{C}ehovin Zajc, Alan Luke\\v{z}i\\v{c}, Ale\\v{s} Leonardis, Matej\n  Kristan", "title": "Beyond standard benchmarks: Parameterizing performance evaluation in\n  visual object tracking", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Object-to-camera motion produces a variety of apparent motion patterns that\nsignificantly affect performance of short-term visual trackers. Despite being\ncrucial for designing robust trackers, their influence is poorly explored in\nstandard benchmarks due to weakly defined, biased and overlapping attribute\nannotations. In this paper we propose to go beyond pre-recorded benchmarks with\npost-hoc annotations by presenting an approach that utilizes omnidirectional\nvideos to generate realistic, consistently annotated, short-term tracking\nscenarios with exactly parameterized motion patterns. We have created an\nevaluation system, constructed a fully annotated dataset of omnidirectional\nvideos and the generators for typical motion patterns. We provide an in-depth\nanalysis of major tracking paradigms which is complementary to the standard\nbenchmarks and confirms the expressiveness of our evaluation approach.\n", "versions": [{"version": "v1", "created": "Thu, 1 Dec 2016 00:26:03 GMT"}, {"version": "v2", "created": "Sat, 25 Mar 2017 18:13:40 GMT"}], "update_date": "2017-03-28", "authors_parsed": [["Zajc", "Luka \u010cehovin", ""], ["Luke\u017ei\u010d", "Alan", ""], ["Leonardis", "Ale\u0161", ""], ["Kristan", "Matej", ""]]}, {"id": "1612.00101", "submitter": "Matthias Nie{\\ss}ner", "authors": "Angela Dai, Charles Ruizhongtai Qi, Matthias Nie{\\ss}ner", "title": "Shape Completion using 3D-Encoder-Predictor CNNs and Shape Synthesis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a data-driven approach to complete partial 3D shapes through a\ncombination of volumetric deep neural networks and 3D shape synthesis. From a\npartially-scanned input shape, our method first infers a low-resolution -- but\ncomplete -- output. To this end, we introduce a 3D-Encoder-Predictor Network\n(3D-EPN) which is composed of 3D convolutional layers. The network is trained\nto predict and fill in missing data, and operates on an implicit surface\nrepresentation that encodes both known and unknown space. This allows us to\npredict global structure in unknown areas at high accuracy. We then correlate\nthese intermediary results with 3D geometry from a shape database at test time.\nIn a final pass, we propose a patch-based 3D shape synthesis method that\nimposes the 3D geometry from these retrieved shapes as constraints on the\ncoarsely-completed mesh. This synthesis process enables us to reconstruct\nfine-scale detail and generate high-resolution output while respecting the\nglobal mesh structure obtained by the 3D-EPN. Although our 3D-EPN outperforms\nstate-of-the-art completion method, the main contribution in our work lies in\nthe combination of a data-driven shape predictor and analytic 3D shape\nsynthesis. In our results, we show extensive evaluations on a newly-introduced\nshape completion benchmark for both real-world and synthetic data.\n", "versions": [{"version": "v1", "created": "Thu, 1 Dec 2016 01:10:41 GMT"}, {"version": "v2", "created": "Tue, 11 Apr 2017 20:48:53 GMT"}], "update_date": "2017-04-13", "authors_parsed": [["Dai", "Angela", ""], ["Qi", "Charles Ruizhongtai", ""], ["Nie\u00dfner", "Matthias", ""]]}, {"id": "1612.00119", "submitter": "Xiaojie Jin Mr.", "authors": "Xiaojie Jin, Xin Li, Huaxin Xiao, Xiaohui Shen, Zhe Lin, Jimei Yang,\n  Yunpeng Chen, Jian Dong, Luoqi Liu, Zequn Jie, Jiashi Feng, Shuicheng Yan", "title": "Video Scene Parsing with Predictive Feature Learning", "comments": "15 pages, 7 figures, 5 tables, currently v2", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we address the challenging video scene parsing problem by\ndeveloping effective representation learning methods given limited parsing\nannotations. In particular, we contribute two novel methods that constitute a\nunified parsing framework. (1) \\textbf{Predictive feature learning}} from\nnearly unlimited unlabeled video data. Different from existing methods learning\nfeatures from single frame parsing, we learn spatiotemporal discriminative\nfeatures by enforcing a parsing network to predict future frames and their\nparsing maps (if available) given only historical frames. In this way, the\nnetwork can effectively learn to capture video dynamics and temporal context,\nwhich are critical clues for video scene parsing, without requiring extra\nmanual annotations. (2) \\textbf{Prediction steering parsing}} architecture that\neffectively adapts the learned spatiotemporal features to scene parsing tasks\nand provides strong guidance for any off-the-shelf parsing model to achieve\nbetter video scene parsing performance. Extensive experiments over two\nchallenging datasets, Cityscapes and Camvid, have demonstrated the\neffectiveness of our methods by showing significant improvement over\nwell-established baselines.\n", "versions": [{"version": "v1", "created": "Thu, 1 Dec 2016 02:48:48 GMT"}, {"version": "v2", "created": "Tue, 13 Dec 2016 04:55:42 GMT"}], "update_date": "2016-12-14", "authors_parsed": [["Jin", "Xiaojie", ""], ["Li", "Xin", ""], ["Xiao", "Huaxin", ""], ["Shen", "Xiaohui", ""], ["Lin", "Zhe", ""], ["Yang", "Jimei", ""], ["Chen", "Yunpeng", ""], ["Dong", "Jian", ""], ["Liu", "Luoqi", ""], ["Jie", "Zequn", ""], ["Feng", "Jiashi", ""], ["Yan", "Shuicheng", ""]]}, {"id": "1612.00125", "submitter": "arXiv Admin", "authors": "Lei Shi, Rui Guo, Yuchen Ma", "title": "A Novel Artificial Fish Swarm Algorithm for Pattern Recognition with\n  Convex Optimization", "comments": "arXiv admin note: submission has been withdrawn by arXiv\n  administrators due to inappropriate text reuse from external sources", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image pattern recognition is an important area in digital image processing.\nAn efficient pattern recognition algorithm should be able to provide correct\nrecognition at a reduced computational time. Off late amongst the machine\nlearning pattern recognition algorithms, Artificial fish swarm algorithm is one\nof the swarm intelligence optimization algorithms that works based on\npopulation and stochastic search. In order to achieve acceptable result, there\nare many parameters needs to be adjusted in AFSA. Among these parameters,\nvisual and step are very significant in view of the fact that artificial fish\nbasically move based on these parameters. In standard AFSA, these two\nparameters remain constant until the algorithm termination. Large values of\nthese parameters increase the capability of algorithm in global search, while\nsmall values improve the local search ability of the algorithm. In this paper,\nwe empirically study the performance of the AFSA and different approaches to\nbalance between local and global exploration have been tested based on the\nadaptive modification of visual and step during algorithm execution. The\nproposed approaches have been evaluated based on the four well-known benchmark\nfunctions. Experimental results show considerable positive impact on the\nperformance of AFSA. A Convex optimization has been integrated into the\nproposed work to have an ideal segmentation of the input image which is a MR\nbrain image.\n", "versions": [{"version": "v1", "created": "Thu, 1 Dec 2016 03:18:46 GMT"}, {"version": "v2", "created": "Wed, 7 Dec 2016 15:05:48 GMT"}], "update_date": "2016-12-08", "authors_parsed": [["Shi", "Lei", ""], ["Guo", "Rui", ""], ["Ma", "Yuchen", ""]]}, {"id": "1612.00132", "submitter": "Jiajun Lu", "authors": "Jiajun Lu, Aditya Deshpande, David Forsyth", "title": "CDVAE: Co-embedding Deep Variational Auto Encoder for Conditional\n  Variational Generation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Problems such as predicting a new shading field (Y) for an image (X) are\nambiguous: many very distinct solutions are good. Representing this ambiguity\nrequires building a conditional model P(Y|X) of the prediction, conditioned on\nthe image. Such a model is difficult to train, because we do not usually have\ntraining data containing many different shadings for the same image. As a\nresult, we need different training examples to share data to produce good\nmodels. This presents a danger we call \"code space collapse\" - the training\nprocedure produces a model that has a very good loss score, but which\nrepresents the conditional distribution poorly. We demonstrate an improved\nmethod for building conditional models by exploiting a metric constraint on\ntraining data that prevents code space collapse. We demonstrate our model on\ntwo example tasks using real data: image saturation adjustment, image\nrelighting. We describe quantitative metrics to evaluate ambiguous generation\nresults. Our results quantitatively and qualitatively outperform different\nstrong baselines.\n", "versions": [{"version": "v1", "created": "Thu, 1 Dec 2016 03:40:42 GMT"}, {"version": "v2", "created": "Tue, 28 Mar 2017 03:21:34 GMT"}], "update_date": "2017-03-29", "authors_parsed": [["Lu", "Jiajun", ""], ["Deshpande", "Aditya", ""], ["Forsyth", "David", ""]]}, {"id": "1612.00137", "submitter": "Cewu Lu", "authors": "Hao-Shu Fang, Shuqin Xie, Yu-Wing Tai and Cewu Lu", "title": "RMPE: Regional Multi-person Pose Estimation", "comments": "Models & Codes available at https://github.com/MVIG-SJTU/RMPE or\n  https://github.com/Fang-Haoshu/RMPE", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-person pose estimation in the wild is challenging. Although\nstate-of-the-art human detectors have demonstrated good performance, small\nerrors in localization and recognition are inevitable. These errors can cause\nfailures for a single-person pose estimator (SPPE), especially for methods that\nsolely depend on human detection results. In this paper, we propose a novel\nregional multi-person pose estimation (RMPE) framework to facilitate pose\nestimation in the presence of inaccurate human bounding boxes. Our framework\nconsists of three components: Symmetric Spatial Transformer Network (SSTN),\nParametric Pose Non-Maximum-Suppression (NMS), and Pose-Guided Proposals\nGenerator (PGPG). Our method is able to handle inaccurate bounding boxes and\nredundant detections, allowing it to achieve a 17% increase in mAP over the\nstate-of-the-art methods on the MPII (multi person) dataset.Our model and\nsource codes are publicly available.\n", "versions": [{"version": "v1", "created": "Thu, 1 Dec 2016 04:36:52 GMT"}, {"version": "v2", "created": "Tue, 7 Feb 2017 09:34:28 GMT"}, {"version": "v3", "created": "Wed, 19 Apr 2017 16:25:22 GMT"}, {"version": "v4", "created": "Sat, 2 Sep 2017 00:16:36 GMT"}, {"version": "v5", "created": "Sun, 4 Feb 2018 04:27:56 GMT"}], "update_date": "2018-02-06", "authors_parsed": [["Fang", "Hao-Shu", ""], ["Xie", "Shuqin", ""], ["Tai", "Yu-Wing", ""], ["Lu", "Cewu", ""]]}, {"id": "1612.00138", "submitter": "Andras Rozsa", "authors": "Andras Rozsa, Manuel Gunther, and Terrance E. Boult", "title": "Towards Robust Deep Neural Networks with BANG", "comments": "Accepted to the IEEE Winter Conference on Applications of Computer\n  Vision (WACV), 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning models, including state-of-the-art deep neural networks, are\nvulnerable to small perturbations that cause unexpected classification errors.\nThis unexpected lack of robustness raises fundamental questions about their\ngeneralization properties and poses a serious concern for practical\ndeployments. As such perturbations can remain imperceptible - the formed\nadversarial examples demonstrate an inherent inconsistency between vulnerable\nmachine learning models and human perception - some prior work casts this\nproblem as a security issue. Despite the significance of the discovered\ninstabilities and ensuing research, their cause is not well understood and no\neffective method has been developed to address the problem. In this paper, we\npresent a novel theory to explain why this unpleasant phenomenon exists in deep\nneural networks. Based on that theory, we introduce a simple, efficient, and\neffective training approach, Batch Adjusted Network Gradients (BANG), which\nsignificantly improves the robustness of machine learning models. While the\nBANG technique does not rely on any form of data augmentation or the\nutilization of adversarial images for training, the resultant classifiers are\nmore resistant to adversarial perturbations while maintaining or even enhancing\nthe overall classification performance.\n", "versions": [{"version": "v1", "created": "Thu, 1 Dec 2016 04:49:45 GMT"}, {"version": "v2", "created": "Tue, 8 Aug 2017 21:57:46 GMT"}, {"version": "v3", "created": "Tue, 30 Jan 2018 20:59:56 GMT"}], "update_date": "2018-02-01", "authors_parsed": [["Rozsa", "Andras", ""], ["Gunther", "Manuel", ""], ["Boult", "Terrance E.", ""]]}, {"id": "1612.00144", "submitter": "Anirban Santara", "authors": "Anirban Santara, Kaustubh Mani, Pranoot Hatwar, Ankit Singh, Ankur\n  Garg, Kirti Padia and Pabitra Mitra", "title": "BASS Net: Band-Adaptive Spectral-Spatial Feature Learning Neural Network\n  for Hyperspectral Image Classification", "comments": "8 pages, 10 figures, Submitted to IEEE TGRS, Code available at:\n  https://github.com/kaustubh0mani/BASS-Net", "journal-ref": null, "doi": "10.1109/TGRS.2017.2705073", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning based landcover classification algorithms have recently been\nproposed in literature. In hyperspectral images (HSI) they face the challenges\nof large dimensionality, spatial variability of spectral signatures and\nscarcity of labeled data. In this article we propose an end-to-end deep\nlearning architecture that extracts band specific spectral-spatial features and\nperforms landcover classification. The architecture has fewer independent\nconnection weights and thus requires lesser number of training data. The method\nis found to outperform the highest reported accuracies on popular hyperspectral\nimage data sets.\n", "versions": [{"version": "v1", "created": "Thu, 1 Dec 2016 05:00:02 GMT"}, {"version": "v2", "created": "Fri, 2 Dec 2016 09:02:51 GMT"}], "update_date": "2017-06-28", "authors_parsed": [["Santara", "Anirban", ""], ["Mani", "Kaustubh", ""], ["Hatwar", "Pranoot", ""], ["Singh", "Ankit", ""], ["Garg", "Ankur", ""], ["Padia", "Kirti", ""], ["Mitra", "Pabitra", ""]]}, {"id": "1612.00155", "submitter": "Pedro Tabacof", "authors": "Pedro Tabacof, Julia Tavares, Eduardo Valle", "title": "Adversarial Images for Variational Autoencoders", "comments": "Workshop on Adversarial Training, NIPS 2016, Barcelona, Spain", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate adversarial attacks for autoencoders. We propose a procedure\nthat distorts the input image to mislead the autoencoder in reconstructing a\ncompletely different target image. We attack the internal latent\nrepresentations, attempting to make the adversarial input produce an internal\nrepresentation as similar as possible as the target's. We find that\nautoencoders are much more robust to the attack than classifiers: while some\nexamples have tolerably small input distortion, and reasonable similarity to\nthe target image, there is a quasi-linear trade-off between those aims. We\nreport results on MNIST and SVHN datasets, and also test regular deterministic\nautoencoders, reaching similar conclusions in all cases. Finally, we show that\nthe usual adversarial attack for classifiers, while being much easier, also\npresents a direct proportion between distortion on the input, and misdirection\non the output. That proportionality however is hidden by the normalization of\nthe output, which maps a linear layer into non-linear probabilities.\n", "versions": [{"version": "v1", "created": "Thu, 1 Dec 2016 05:59:57 GMT"}], "update_date": "2016-12-02", "authors_parsed": [["Tabacof", "Pedro", ""], ["Tavares", "Julia", ""], ["Valle", "Eduardo", ""]]}, {"id": "1612.00181", "submitter": "Michael Miller", "authors": "Michael Snow and Jan Van lent", "title": "Monge's Optimal Transport Distance for Image Classification", "comments": "15 pages, 14 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper focuses on a similarity measure, known as the Wasserstein\ndistance, with which to compare images. The Wasserstein distance results from a\npartial differential equation (PDE) formulation of Monge's optimal transport\nproblem. We present an efficient numerical solution method for solving Monge's\nproblem. To demonstrate the measure's discriminatory power when comparing\nimages, we use a $1$-Nearest Neighbour ($1$-NN) machine learning algorithm to\nillustrate the measure's potential benefits over other more traditional\ndistance metrics and also the Tangent Space distance, designed to perform\nexcellently on the well-known MNIST dataset. To our knowledge, the PDE\nformulation of the Wasserstein metric has not been presented for dealing with\nimage comparison, nor has the Wasserstein distance been used within the\n$1$-nearest neighbour architecture.\n", "versions": [{"version": "v1", "created": "Thu, 1 Dec 2016 09:05:30 GMT"}, {"version": "v2", "created": "Sun, 8 Apr 2018 10:12:32 GMT"}], "update_date": "2018-04-10", "authors_parsed": [["Snow", "Michael", ""], ["Van lent", "Jan", ""]]}, {"id": "1612.00192", "submitter": "Artem Rozantsev Dr.", "authors": "Artem Rozantsev and Sudipta N. Sinha and Debadeepta Dey and Pascal Fua", "title": "Flight Dynamics-based Recovery of a UAV Trajectory using Ground Cameras", "comments": null, "journal-ref": null, "doi": "10.1109/CVPR.2017.266", "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new method to estimate the 6-dof trajectory of a flying object\nsuch as a quadrotor UAV within a 3D airspace monitored using multiple fixed\nground cameras. It is based on a new structure from motion formulation for the\n3D reconstruction of a single moving point with known motion dynamics. Our main\ncontribution is a new bundle adjustment procedure which in addition to\noptimizing the camera poses, regularizes the point trajectory using a prior\nbased on motion dynamics (or specifically flight dynamics). Furthermore, we can\ninfer the underlying control input sent to the UAV's autopilot that determined\nits flight trajectory.\n  Our method requires neither perfect single-view tracking nor appearance\nmatching across views. For robustness, we allow the tracker to generate\nmultiple detections per frame in each video. The true detections and the data\nassociation across videos is estimated using robust multi-view triangulation\nand subsequently refined during our bundle adjustment procedure. Quantitative\nevaluation on simulated data and experiments on real videos from indoor and\noutdoor scenes demonstrates the effectiveness of our method.\n", "versions": [{"version": "v1", "created": "Thu, 1 Dec 2016 10:22:41 GMT"}, {"version": "v2", "created": "Tue, 21 Nov 2017 11:25:44 GMT"}], "update_date": "2017-11-22", "authors_parsed": [["Rozantsev", "Artem", ""], ["Sinha", "Sudipta N.", ""], ["Dey", "Debadeepta", ""], ["Fua", "Pascal", ""]]}, {"id": "1612.00197", "submitter": "Christian Rupprecht", "authors": "Christian Rupprecht, Iro Laina, Robert DiPietro, Maximilian Baust,\n  Federico Tombari, Nassir Navab, Gregory D. Hager", "title": "Learning in an Uncertain World: Representing Ambiguity Through Multiple\n  Hypotheses", "comments": "ICCV 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many prediction tasks contain uncertainty. In some cases, uncertainty is\ninherent in the task itself. In future prediction, for example, many distinct\noutcomes are equally valid. In other cases, uncertainty arises from the way\ndata is labeled. For example, in object detection, many objects of interest\noften go unlabeled, and in human pose estimation, occluded joints are often\nlabeled with ambiguous values. In this work we focus on a principled approach\nfor handling such scenarios. In particular, we propose a framework for\nreformulating existing single-prediction models as multiple hypothesis\nprediction (MHP) models and an associated meta loss and optimization procedure\nto train them. To demonstrate our approach, we consider four diverse\napplications: human pose estimation, future prediction, image classification\nand segmentation. We find that MHP models outperform their single-hypothesis\ncounterparts in all cases, and that MHP models simultaneously expose valuable\ninsights into the variability of predictions.\n", "versions": [{"version": "v1", "created": "Thu, 1 Dec 2016 10:50:21 GMT"}, {"version": "v2", "created": "Tue, 21 Mar 2017 09:17:38 GMT"}, {"version": "v3", "created": "Tue, 22 Aug 2017 13:55:17 GMT"}], "update_date": "2017-08-23", "authors_parsed": [["Rupprecht", "Christian", ""], ["Laina", "Iro", ""], ["DiPietro", "Robert", ""], ["Baust", "Maximilian", ""], ["Tombari", "Federico", ""], ["Navab", "Nassir", ""], ["Hager", "Gregory D.", ""]]}, {"id": "1612.00212", "submitter": "Shuchang Zhou", "authors": "He Wen, Shuchang Zhou, Zhe Liang, Yuxiang Zhang, Dieqiao Feng, Xinyu\n  Zhou, Cong Yao", "title": "Training Bit Fully Convolutional Network for Fast Semantic Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fully convolutional neural networks give accurate, per-pixel prediction for\ninput images and have applications like semantic segmentation. However, a\ntypical FCN usually requires lots of floating point computation and large\nrun-time memory, which effectively limits its usability. We propose a method to\ntrain Bit Fully Convolution Network (BFCN), a fully convolutional neural\nnetwork that has low bit-width weights and activations. Because most of its\ncomputation-intensive convolutions are accomplished between low bit-width\nnumbers, a BFCN can be accelerated by an efficient bit-convolution\nimplementation. On CPU, the dot product operation between two bit vectors can\nbe reduced to bitwise operations and popcounts, which can offer much higher\nthroughput than 32-bit multiplications and additions.\n  To validate the effectiveness of BFCN, we conduct experiments on the PASCAL\nVOC 2012 semantic segmentation task and Cityscapes. Our BFCN with 1-bit weights\nand 2-bit activations, which runs 7.8x faster on CPU or requires less than 1\\%\nresources on FPGA, can achieve comparable performance as the 32-bit\ncounterpart.\n", "versions": [{"version": "v1", "created": "Thu, 1 Dec 2016 11:56:15 GMT"}], "update_date": "2016-12-02", "authors_parsed": [["Wen", "He", ""], ["Zhou", "Shuchang", ""], ["Liang", "Zhe", ""], ["Zhang", "Yuxiang", ""], ["Feng", "Dieqiao", ""], ["Zhou", "Xinyu", ""], ["Yao", "Cong", ""]]}, {"id": "1612.00215", "submitter": "Aykut Erdem", "authors": "Levent Karacan, Zeynep Akata, Aykut Erdem, Erkut Erdem", "title": "Learning to Generate Images of Outdoor Scenes from Attributes and\n  Semantic Layouts", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic image synthesis research has been rapidly growing with deep\nnetworks getting more and more expressive. In the last couple of years, we have\nobserved images of digits, indoor scenes, birds, chairs, etc. being\nautomatically generated. The expressive power of image generators have also\nbeen enhanced by introducing several forms of conditioning variables such as\nobject names, sentences, bounding box and key-point locations. In this work, we\npropose a novel deep conditional generative adversarial network architecture\nthat takes its strength from the semantic layout and scene attributes\nintegrated as conditioning variables. We show that our architecture is able to\ngenerate realistic outdoor scene images under different conditions, e.g.\nday-night, sunny-foggy, with clear object boundaries.\n", "versions": [{"version": "v1", "created": "Thu, 1 Dec 2016 12:07:55 GMT"}], "update_date": "2016-12-02", "authors_parsed": [["Karacan", "Levent", ""], ["Akata", "Zeynep", ""], ["Erdem", "Aykut", ""], ["Erdem", "Erkut", ""]]}, {"id": "1612.00220", "submitter": "Mark Marsden", "authors": "Mark Marsden, Kevin McGuinness, Suzanne Little, Noel E. O'Connor", "title": "Fully Convolutional Crowd Counting On Highly Congested Scenes", "comments": "7 pages , VISAPP 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we advance the state-of-the-art for crowd counting in high\ndensity scenes by further exploring the idea of a fully convolutional crowd\ncounting model introduced by (Zhang et al., 2016). Producing an accurate and\nrobust crowd count estimator using computer vision techniques has attracted\nsignificant research interest in recent years. Applications for crowd counting\nsystems exist in many diverse areas including city planning, retail, and of\ncourse general public safety. Developing a highly generalised counting model\nthat can be deployed in any surveillance scenario with any camera perspective\nis the key objective for research in this area. Techniques developed in the\npast have generally performed poorly in highly congested scenes with several\nthousands of people in frame (Rodriguez et al., 2011). Our approach, influenced\nby the work of (Zhang et al., 2016), consists of the following contributions:\n(1) A training set augmentation scheme that minimises redundancy among training\nsamples to improve model generalisation and overall counting performance; (2) a\ndeep, single column, fully convolutional network (FCN) architecture; (3) a\nmulti-scale averaging step during inference. The developed technique can\nanalyse images of any resolution or aspect ratio and achieves state-of-the-art\ncounting performance on the Shanghaitech Part B and UCF CC 50 datasets as well\nas competitive performance on Shanghaitech Part A.\n", "versions": [{"version": "v1", "created": "Thu, 1 Dec 2016 12:24:35 GMT"}, {"version": "v2", "created": "Tue, 17 Jan 2017 15:00:46 GMT"}], "update_date": "2017-01-18", "authors_parsed": [["Marsden", "Mark", ""], ["McGuinness", "Kevin", ""], ["Little", "Suzanne", ""], ["O'Connor", "Noel E.", ""]]}, {"id": "1612.00234", "submitter": "Xiang Long", "authors": "Xiang Long, Chuang Gan, Gerard de Melo", "title": "Video Captioning with Multi-Faceted Attention", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, video captioning has been attracting an increasing amount of\ninterest, due to its potential for improving accessibility and information\nretrieval. While existing methods rely on different kinds of visual features\nand model structures, they do not fully exploit relevant semantic information.\nWe present an extensible approach to jointly leverage several sorts of visual\nfeatures and semantic attributes. Our novel architecture builds on LSTMs for\nsentence generation, with several attention layers and two multimodal layers.\nThe attention mechanism learns to automatically select the most salient visual\nfeatures or semantic attributes, and the multimodal layer yields overall\nrepresentations for the input and outputs of the sentence generation component.\nExperimental results on the challenging MSVD and MSR-VTT datasets show that our\nframework outperforms the state-of-the-art approaches, while ground truth based\nsemantic attributes are able to further elevate the output quality to a\nnear-human level.\n", "versions": [{"version": "v1", "created": "Thu, 1 Dec 2016 13:11:29 GMT"}], "update_date": "2016-12-02", "authors_parsed": [["Long", "Xiang", ""], ["Gan", "Chuang", ""], ["de Melo", "Gerard", ""]]}, {"id": "1612.00334", "submitter": "Yanjun  Qi Dr.", "authors": "Beilun Wang, Ji Gao, Yanjun Qi", "title": "A Theoretical Framework for Robustness of (Deep) Classifiers against\n  Adversarial Examples", "comments": "38 pages , ICLR 2017 Workshop Track", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most machine learning classifiers, including deep neural networks, are\nvulnerable to adversarial examples. Such inputs are typically generated by\nadding small but purposeful modifications that lead to incorrect outputs while\nimperceptible to human eyes. The goal of this paper is not to introduce a\nsingle method, but to make theoretical steps towards fully understanding\nadversarial examples. By using concepts from topology, our theoretical analysis\nbrings forth the key reasons why an adversarial example can fool a classifier\n($f_1$) and adds its oracle ($f_2$, like human eyes) in such analysis. By\ninvestigating the topological relationship between two (pseudo)metric spaces\ncorresponding to predictor $f_1$ and oracle $f_2$, we develop necessary and\nsufficient conditions that can determine if $f_1$ is always robust\n(strong-robust) against adversarial examples according to $f_2$. Interestingly\nour theorems indicate that just one unnecessary feature can make $f_1$ not\nstrong-robust, and the right feature representation learning is the key to\ngetting a classifier that is both accurate and strong-robust.\n", "versions": [{"version": "v1", "created": "Thu, 1 Dec 2016 16:20:39 GMT"}, {"version": "v10", "created": "Thu, 9 Mar 2017 22:00:56 GMT"}, {"version": "v11", "created": "Thu, 27 Apr 2017 14:36:40 GMT"}, {"version": "v12", "created": "Wed, 27 Sep 2017 16:02:48 GMT"}, {"version": "v2", "created": "Mon, 5 Dec 2016 17:07:35 GMT"}, {"version": "v3", "created": "Tue, 17 Jan 2017 22:23:55 GMT"}, {"version": "v4", "created": "Sat, 21 Jan 2017 16:37:24 GMT"}, {"version": "v5", "created": "Thu, 26 Jan 2017 15:32:06 GMT"}, {"version": "v6", "created": "Wed, 1 Feb 2017 17:30:50 GMT"}, {"version": "v7", "created": "Thu, 2 Feb 2017 14:39:50 GMT"}, {"version": "v8", "created": "Fri, 3 Feb 2017 16:06:39 GMT"}, {"version": "v9", "created": "Mon, 27 Feb 2017 20:18:26 GMT"}], "update_date": "2017-09-28", "authors_parsed": [["Wang", "Beilun", ""], ["Gao", "Ji", ""], ["Qi", "Yanjun", ""]]}, {"id": "1612.00338", "submitter": "Zohre Kohan", "authors": "Zohreh Kohan, Hamidreza Farhidzadeh, Reza Azmi, Behrouz Gholizadeh", "title": "Hippocampus Temporal Lobe Epilepsy Detection using a Combination of\n  Shape-based Features and Spherical Harmonics Representation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most of the temporal lobe epilepsy detection approaches are based on\nhippocampus deformation and use complicated features, resulting, detection is\ndone with complicated features extraction and pre-processing task. In this\npaper, a new detection method based on shape-based features and spherical\nharmonics is proposed which can analysis the hippocampus shape anomaly and\ndetection asymmetry. This method consisted of two main parts; (1) shape feature\nextraction, and (2) image classification. For evaluation, HFH database is used\nwhich is publicly available in this field. Nine different geometry and 256\nspherical harmonic features are introduced then selected Eighteen of them that\ndetect the asymmetry in hippocampus significantly in a randomly selected subset\nof the dataset. Then a support vector machine (SVM) classifier was employed to\nclassify the remaining images of the dataset to normal and epileptic images\nusing our selected features. On a dataset of 25 images, 12 images were used for\nfeature extraction and the rest 13 for classification. The results show that\nthe proposed method has accuracy, specificity and sensitivity of, respectively,\n84%, 100%, and 80%. Therefore, the proposed approach shows acceptable result\nand is straightforward also; complicated pre-processing steps were omitted\ncompared to other methods.\n", "versions": [{"version": "v1", "created": "Thu, 1 Dec 2016 16:27:59 GMT"}, {"version": "v2", "created": "Wed, 28 Dec 2016 00:18:26 GMT"}], "update_date": "2016-12-30", "authors_parsed": [["Kohan", "Zohreh", ""], ["Farhidzadeh", "Hamidreza", ""], ["Azmi", "Reza", ""], ["Gholizadeh", "Behrouz", ""]]}, {"id": "1612.00343", "submitter": "Da Chen", "authors": "Da Chen and Jean-Marie Mirebeau and Laurent D. Cohen", "title": "Global Minimum for a Finsler Elastica Minimal Path Approach", "comments": "Improve the clarity of this manuscript", "journal-ref": "International Journal of Computer Vision, Volume 122, Issue 3, pp\n  458-483, 2017", "doi": "10.1007/s11263-016-0975-5", "report-no": null, "categories": "cs.CG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a novel curvature-penalized minimal path model via\nan orientation-lifted Finsler metric and the Euler elastica curve. The original\nminimal path model computes the globally minimal geodesic by solving an Eikonal\npartial differential equation (PDE). Essentially, this first-order model is\nunable to penalize curvature which is related to the path rigidity property in\nthe classical active contour models. To solve this problem, we present an\nEikonal PDE-based Finsler elastica minimal path approach to address the\ncurvature-penalized geodesic energy minimization problem. We were successful at\nadding the curvature penalization to the classical geodesic energy. The basic\nidea of this work is to interpret the Euler elastica bending energy via a novel\nFinsler elastica metric that embeds a curvature penalty. This metric is\nnon-Riemannian, anisotropic and asymmetric, and is defined over an\norientation-lifted space by adding to the image domain the orientation as an\nextra space dimension. Based on this orientation lifting, the proposed minimal\npath model can benefit from both the curvature and orientation of the paths.\nThanks to the fast marching method, the global minimum of the\ncurvature-penalized geodesic energy can be computed efficiently. We introduce\ntwo anisotropic image data-driven speed functions that are computed by\nsteerable filters. Based on these orientation-dependent speed functions, we can\napply the proposed Finsler elastica minimal path model to the applications of\nclosed contour detection, perceptual grouping and tubular structure extraction.\nNumerical experiments on both synthetic and real images show that these\napplications of the proposed model indeed obtain promising results.\n", "versions": [{"version": "v1", "created": "Thu, 1 Dec 2016 16:45:30 GMT"}, {"version": "v2", "created": "Tue, 23 May 2017 16:57:04 GMT"}, {"version": "v3", "created": "Mon, 21 May 2018 11:44:20 GMT"}], "update_date": "2018-05-22", "authors_parsed": [["Chen", "Da", ""], ["Mirebeau", "Jean-Marie", ""], ["Cohen", "Laurent D.", ""]]}, {"id": "1612.00356", "submitter": "Kwame Kutten", "authors": "Kwame S. Kutten, Nicolas Charon, Michael I. Miller, J. T. Ratnanather,\n  Jordan Matelsky, Alexander D. Baden, Kunal Lillaney, Karl Deisseroth, Li Ye,\n  and Joshua T. Vogelstein", "title": "A Large Deformation Diffeomorphic Approach to Registration of CLARITY\n  Images via Mutual Information", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  CLARITY is a method for converting biological tissues into translucent and\nporous hydrogel-tissue hybrids. This facilitates interrogation with light sheet\nmicroscopy and penetration of molecular probes while avoiding physical slicing.\nIn this work, we develop a pipeline for registering CLARIfied mouse brains to\nan annotated brain atlas. Due to the novelty of this microscopy technique it is\nimpractical to use absolute intensity values to align these images to existing\nstandard atlases. Thus we adopt a large deformation diffeomorphic approach for\nregistering images via mutual information matching. Furthermore we show how a\ncascaded multi-resolution approach can improve registration quality while\nreducing algorithm run time. As acquired image volumes were over a terabyte in\nsize, they were far too large for work on personal computers. Therefore the\nNeuroData computational infrastructure was deployed for multi-resolution\nstorage and visualization of these images and aligned annotations on the web.\n", "versions": [{"version": "v1", "created": "Thu, 1 Dec 2016 17:31:04 GMT"}, {"version": "v2", "created": "Fri, 16 Dec 2016 20:05:28 GMT"}, {"version": "v3", "created": "Fri, 11 Aug 2017 15:52:28 GMT"}], "update_date": "2017-08-14", "authors_parsed": [["Kutten", "Kwame S.", ""], ["Charon", "Nicolas", ""], ["Miller", "Michael I.", ""], ["Ratnanather", "J. T.", ""], ["Matelsky", "Jordan", ""], ["Baden", "Alexander D.", ""], ["Lillaney", "Kunal", ""], ["Deisseroth", "Karl", ""], ["Ye", "Li", ""], ["Vogelstein", "Joshua T.", ""]]}, {"id": "1612.00370", "submitter": "Siqi Liu", "authors": "Siqi Liu, Zhenhai Zhu, Ning Ye, Sergio Guadarrama, Kevin Murphy", "title": "Improved Image Captioning via Policy Gradient optimization of SPIDEr", "comments": "Accepted at ICCV 2017", "journal-ref": null, "doi": "10.1109/ICCV.2017.100", "report-no": null, "categories": "cs.CV cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current image captioning methods are usually trained via (penalized) maximum\nlikelihood estimation. However, the log-likelihood score of a caption does not\ncorrelate well with human assessments of quality. Standard syntactic evaluation\nmetrics, such as BLEU, METEOR and ROUGE, are also not well correlated. The\nnewer SPICE and CIDEr metrics are better correlated, but have traditionally\nbeen hard to optimize for. In this paper, we show how to use a policy gradient\n(PG) method to directly optimize a linear combination of SPICE and CIDEr (a\ncombination we call SPIDEr): the SPICE score ensures our captions are\nsemantically faithful to the image, while CIDEr score ensures our captions are\nsyntactically fluent. The PG method we propose improves on the prior MIXER\napproach, by using Monte Carlo rollouts instead of mixing MLE training with PG.\nWe show empirically that our algorithm leads to easier optimization and\nimproved results compared to MIXER. Finally, we show that using our PG method\nwe can optimize any of the metrics, including the proposed SPIDEr metric which\nresults in image captions that are strongly preferred by human raters compared\nto captions generated by the same model but trained to optimize MLE or the COCO\nmetrics.\n", "versions": [{"version": "v1", "created": "Thu, 1 Dec 2016 18:10:48 GMT"}, {"version": "v2", "created": "Wed, 14 Dec 2016 08:36:58 GMT"}, {"version": "v3", "created": "Sat, 18 Mar 2017 09:24:38 GMT"}, {"version": "v4", "created": "Mon, 12 Mar 2018 18:53:06 GMT"}], "update_date": "2018-03-14", "authors_parsed": [["Liu", "Siqi", ""], ["Zhu", "Zhenhai", ""], ["Ye", "Ning", ""], ["Guadarrama", "Sergio", ""], ["Murphy", "Kevin", ""]]}, {"id": "1612.00380", "submitter": "Nantas Nardelli", "authors": "Shehroze Bhatti, Alban Desmaison, Ondrej Miksik, Nantas Nardelli, N.\n  Siddharth, Philip H. S. Torr", "title": "Playing Doom with SLAM-Augmented Deep Reinforcement Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A number of recent approaches to policy learning in 2D game domains have been\nsuccessful going directly from raw input images to actions. However when\nemployed in complex 3D environments, they typically suffer from challenges\nrelated to partial observability, combinatorial exploration spaces, path\nplanning, and a scarcity of rewarding scenarios. Inspired from prior work in\nhuman cognition that indicates how humans employ a variety of semantic concepts\nand abstractions (object categories, localisation, etc.) to reason about the\nworld, we build an agent-model that incorporates such abstractions into its\npolicy-learning framework. We augment the raw image input to a Deep Q-Learning\nNetwork (DQN), by adding details of objects and structural elements\nencountered, along with the agent's localisation. The different components are\nautomatically extracted and composed into a topological representation using\non-the-fly object detection and 3D-scene reconstruction.We evaluate the\nefficacy of our approach in Doom, a 3D first-person combat game that exhibits a\nnumber of challenges discussed, and show that our augmented framework\nconsistently learns better, more effective policies.\n", "versions": [{"version": "v1", "created": "Thu, 1 Dec 2016 18:54:51 GMT"}], "update_date": "2016-12-02", "authors_parsed": [["Bhatti", "Shehroze", ""], ["Desmaison", "Alban", ""], ["Miksik", "Ondrej", ""], ["Nardelli", "Nantas", ""], ["Siddharth", "N.", ""], ["Torr", "Philip H. S.", ""]]}, {"id": "1612.00385", "submitter": "Wenjie Pei", "authors": "Wenjie Pei, Tadas Baltru\\v{s}aitis, David M.J. Tax, Louis-Philippe\n  Morency", "title": "Temporal Attention-Gated Model for Robust Sequence Classification", "comments": "Accepted by CVPR 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Typical techniques for sequence classification are designed for\nwell-segmented sequences which have been edited to remove noisy or irrelevant\nparts. Therefore, such methods cannot be easily applied on noisy sequences\nexpected in real-world applications. In this paper, we present the Temporal\nAttention-Gated Model (TAGM) which integrates ideas from attention models and\ngated recurrent networks to better deal with noisy or unsegmented sequences.\nSpecifically, we extend the concept of attention model to measure the relevance\nof each observation (time step) of a sequence. We then use a novel gated\nrecurrent network to learn the hidden representation for the final prediction.\nAn important advantage of our approach is interpretability since the temporal\nattention weights provide a meaningful value for the salience of each time step\nin the sequence. We demonstrate the merits of our TAGM approach, both for\nprediction accuracy and interpretability, on three different tasks: spoken\ndigit recognition, text-based sentiment analysis and visual event recognition.\n", "versions": [{"version": "v1", "created": "Thu, 1 Dec 2016 19:11:24 GMT"}, {"version": "v2", "created": "Sat, 15 Apr 2017 12:53:28 GMT"}], "update_date": "2017-04-18", "authors_parsed": [["Pei", "Wenjie", ""], ["Baltru\u0161aitis", "Tadas", ""], ["Tax", "David M. J.", ""], ["Morency", "Louis-Philippe", ""]]}, {"id": "1612.00390", "submitter": "Andreas Savakis", "authors": "Jefferson Ryan Medel, Andreas Savakis", "title": "Anomaly Detection in Video Using Predictive Convolutional Long\n  Short-Term Memory Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automating the detection of anomalous events within long video sequences is\nchallenging due to the ambiguity of how such events are defined. We approach\nthe problem by learning generative models that can identify anomalies in videos\nusing limited supervision. We propose end-to-end trainable composite\nConvolutional Long Short-Term Memory (Conv-LSTM) networks that are able to\npredict the evolution of a video sequence from a small number of input frames.\nRegularity scores are derived from the reconstruction errors of a set of\npredictions with abnormal video sequences yielding lower regularity scores as\nthey diverge further from the actual sequence over time. The models utilize a\ncomposite structure and examine the effects of conditioning in learning more\nmeaningful representations. The best model is chosen based on the\nreconstruction and prediction accuracy. The Conv-LSTM models are evaluated both\nqualitatively and quantitatively, demonstrating competitive results on anomaly\ndetection datasets. Conv-LSTM units are shown to be an effective tool for\nmodeling and predicting video sequences.\n", "versions": [{"version": "v1", "created": "Thu, 1 Dec 2016 19:28:59 GMT"}, {"version": "v2", "created": "Thu, 15 Dec 2016 16:39:32 GMT"}], "update_date": "2016-12-16", "authors_parsed": [["Medel", "Jefferson Ryan", ""], ["Savakis", "Andreas", ""]]}, {"id": "1612.00404", "submitter": "Shubham Tulsiani", "authors": "Shubham Tulsiani, Hao Su, Leonidas J. Guibas, Alexei A. Efros,\n  Jitendra Malik", "title": "Learning Shape Abstractions by Assembling Volumetric Primitives", "comments": "Project url: https://shubhtuls.github.io/volumetricPrimitives/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a learning framework for abstracting complex shapes by learning to\nassemble objects using 3D volumetric primitives. In addition to generating\nsimple and geometrically interpretable explanations of 3D objects, our\nframework also allows us to automatically discover and exploit consistent\nstructure in the data. We demonstrate that using our method allows predicting\nshape representations which can be leveraged for obtaining a consistent parsing\nacross the instances of a shape collection and constructing an interpretable\nshape similarity measure. We also examine applications for image-based\nprediction as well as shape manipulation.\n", "versions": [{"version": "v1", "created": "Thu, 1 Dec 2016 20:05:27 GMT"}, {"version": "v2", "created": "Wed, 29 Mar 2017 20:41:33 GMT"}, {"version": "v3", "created": "Sat, 24 Jun 2017 16:48:05 GMT"}, {"version": "v4", "created": "Thu, 2 Aug 2018 18:54:32 GMT"}], "update_date": "2018-08-06", "authors_parsed": [["Tulsiani", "Shubham", ""], ["Su", "Hao", ""], ["Guibas", "Leonidas J.", ""], ["Efros", "Alexei A.", ""], ["Malik", "Jitendra", ""]]}, {"id": "1612.00408", "submitter": "Imon Banerjee", "authors": "Imon Banerjee, Lewis Hahn, Geoffrey Sonn, Richard Fan, Daniel L. Rubin", "title": "Computerized Multiparametric MR image Analysis for Prostate Cancer\n  Aggressiveness-Assessment", "comments": "NIPS 2016 Workshop on Machine Learning for Health (NIPS ML4HC)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We propose an automated method for detecting aggressive prostate cancer(CaP)\n(Gleason score >=7) based on a comprehensive analysis of the lesion and the\nsurrounding normal prostate tissue which has been simultaneously captured in\nT2-weighted MR images, diffusion-weighted images (DWI) and apparent diffusion\ncoefficient maps (ADC). The proposed methodology was tested on a dataset of 79\npatients (40 aggressive, 39 non-aggressive). We evaluated the performance of a\nwide range of popular quantitative imaging features on the characterization of\naggressive versus non-aggressive CaP. We found that a group of 44\ndiscriminative predictors among 1464 quantitative imaging features can be used\nto produce an area under the ROC curve of 0.73.\n", "versions": [{"version": "v1", "created": "Thu, 1 Dec 2016 20:10:37 GMT"}], "update_date": "2016-12-02", "authors_parsed": [["Banerjee", "Imon", ""], ["Hahn", "Lewis", ""], ["Sonn", "Geoffrey", ""], ["Fan", "Richard", ""], ["Rubin", "Daniel L.", ""]]}, {"id": "1612.00423", "submitter": "Shenlong Wang", "authors": "Shenlong Wang, Min Bai, Gellert Mattyus, Hang Chu, Wenjie Luo, Bin\n  Yang, Justin Liang, Joel Cheverie, Sanja Fidler, Raquel Urtasun", "title": "TorontoCity: Seeing the World with a Million Eyes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper we introduce the TorontoCity benchmark, which covers the full\ngreater Toronto area (GTA) with 712.5 $km^2$ of land, 8439 $km$ of road and\naround 400,000 buildings. Our benchmark provides different perspectives of the\nworld captured from airplanes, drones and cars driving around the city.\nManually labeling such a large scale dataset is infeasible. Instead, we propose\nto utilize different sources of high-precision maps to create our ground truth.\nTowards this goal, we develop algorithms that allow us to align all data\nsources with the maps while requiring minimal human supervision. We have\ndesigned a wide variety of tasks including building height estimation\n(reconstruction), road centerline and curb extraction, building instance\nsegmentation, building contour extraction (reorganization), semantic labeling\nand scene type classification (recognition). Our pilot study shows that most of\nthese tasks are still difficult for modern convolutional neural networks.\n", "versions": [{"version": "v1", "created": "Thu, 1 Dec 2016 20:39:49 GMT"}], "update_date": "2016-12-02", "authors_parsed": [["Wang", "Shenlong", ""], ["Bai", "Min", ""], ["Mattyus", "Gellert", ""], ["Chu", "Hang", ""], ["Luo", "Wenjie", ""], ["Yang", "Bin", ""], ["Liang", "Justin", ""], ["Cheverie", "Joel", ""], ["Fidler", "Sanja", ""], ["Urtasun", "Raquel", ""]]}, {"id": "1612.00437", "submitter": "Julian Yarkony", "authors": "Shaofei Wang, Chong Zhang, Miguel A. Gonzalez-Ballester, Julian\n  Yarkony", "title": "Efficient Pose and Cell Segmentation using Column Generation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problems of multi-person pose segmentation in natural images and\ninstance segmentation in biological images with crowded cells. We formulate\nthese distinct tasks as integer programs where variables correspond to\nposes/cells. To optimize, we propose a generic relaxation scheme for solving\nthese combinatorial problems using a column generation formulation where the\nprogram for generating a column is solved via exact optimization of very small\nscale integer programs. This results in efficient exploration of the spaces of\nposes and cells.\n", "versions": [{"version": "v1", "created": "Thu, 1 Dec 2016 20:56:17 GMT"}], "update_date": "2016-12-02", "authors_parsed": [["Wang", "Shaofei", ""], ["Zhang", "Chong", ""], ["Gonzalez-Ballester", "Miguel A.", ""], ["Yarkony", "Julian", ""]]}, {"id": "1612.00472", "submitter": "Andrew Jaegle", "authors": "Andrew Jaegle, Stephen Phillips, Daphne Ippolito, Kostas Daniilidis", "title": "Understanding image motion with group representations", "comments": "Published as a conference paper at ICLR 2018; 14 pages, including\n  references and supplement", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motion is an important signal for agents in dynamic environments, but\nlearning to represent motion from unlabeled video is a difficult and\nunderconstrained problem. We propose a model of motion based on elementary\ngroup properties of transformations and use it to train a representation of\nimage motion. While most methods of estimating motion are based on pixel-level\nconstraints, we use these group properties to constrain the abstract\nrepresentation of motion itself. We demonstrate that a deep neural network\ntrained using this method captures motion in both synthetic 2D sequences and\nreal-world sequences of vehicle motion, without requiring any labels. Networks\ntrained to respect these constraints implicitly identify the image\ncharacteristic of motion in different sequence types. In the context of vehicle\nmotion, this method extracts information useful for localization, tracking, and\nodometry. Our results demonstrate that this representation is useful for\nlearning motion in the general setting where explicit labels are difficult to\nobtain.\n", "versions": [{"version": "v1", "created": "Thu, 1 Dec 2016 21:18:45 GMT"}, {"version": "v2", "created": "Mon, 26 Feb 2018 17:33:45 GMT"}], "update_date": "2018-02-27", "authors_parsed": [["Jaegle", "Andrew", ""], ["Phillips", "Stephen", ""], ["Ippolito", "Daphne", ""], ["Daniilidis", "Kostas", ""]]}, {"id": "1612.00478", "submitter": "Noranart Vesdapunt", "authors": "Jonathan Shen, Noranart Vesdapunt, Vishnu N. Boddeti, Kris M. Kitani", "title": "In Teacher We Trust: Learning Compressed Models for Pedestrian Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep convolutional neural networks continue to advance the state-of-the-art\nin many domains as they grow bigger and more complex. It has been observed that\nmany of the parameters of a large network are redundant, allowing for the\npossibility of learning a smaller network that mimics the outputs of the large\nnetwork through a process called Knowledge Distillation. We show, however, that\nstandard Knowledge Distillation is not effective for learning small models for\nthe task of pedestrian detection. To improve this process, we introduce a\nhigher-dimensional hint layer to increase information flow. We also estimate\nthe variance in the outputs of the large network and propose a loss function to\nincorporate this uncertainty. Finally, we attempt to boost the complexity of\nthe small network without increasing its size by using as input hand-designed\nfeatures that have been demonstrated to be effective for pedestrian detection.\nWe succeed in training a model that contains $400\\times$ fewer parameters than\nthe large network while outperforming AlexNet on the Caltech Pedestrian\nDataset.\n", "versions": [{"version": "v1", "created": "Thu, 1 Dec 2016 21:37:19 GMT"}], "update_date": "2016-12-05", "authors_parsed": [["Shen", "Jonathan", ""], ["Vesdapunt", "Noranart", ""], ["Boddeti", "Vishnu N.", ""], ["Kitani", "Kris M.", ""]]}, {"id": "1612.00496", "submitter": "Arsalan Mousavian", "authors": "Arsalan Mousavian, Dragomir Anguelov, John Flynn, Jana Kosecka", "title": "3D Bounding Box Estimation Using Deep Learning and Geometry", "comments": "To appear in IEEE Conference on Computer Vision and Pattern\n  Recognition (CVPR) 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a method for 3D object detection and pose estimation from a single\nimage. In contrast to current techniques that only regress the 3D orientation\nof an object, our method first regresses relatively stable 3D object properties\nusing a deep convolutional neural network and then combines these estimates\nwith geometric constraints provided by a 2D object bounding box to produce a\ncomplete 3D bounding box. The first network output estimates the 3D object\norientation using a novel hybrid discrete-continuous loss, which significantly\noutperforms the L2 loss. The second output regresses the 3D object dimensions,\nwhich have relatively little variance compared to alternatives and can often be\npredicted for many object types. These estimates, combined with the geometric\nconstraints on translation imposed by the 2D bounding box, enable us to recover\na stable and accurate 3D object pose. We evaluate our method on the challenging\nKITTI object detection benchmark both on the official metric of 3D orientation\nestimation and also on the accuracy of the obtained 3D bounding boxes. Although\nconceptually simple, our method outperforms more complex and computationally\nexpensive approaches that leverage semantic segmentation, instance level\nsegmentation and flat ground priors and sub-category detection. Our\ndiscrete-continuous loss also produces state of the art results for 3D\nviewpoint estimation on the Pascal 3D+ dataset.\n", "versions": [{"version": "v1", "created": "Thu, 1 Dec 2016 22:16:48 GMT"}, {"version": "v2", "created": "Mon, 10 Apr 2017 19:05:46 GMT"}], "update_date": "2017-04-12", "authors_parsed": [["Mousavian", "Arsalan", ""], ["Anguelov", "Dragomir", ""], ["Flynn", "John", ""], ["Kosecka", "Jana", ""]]}, {"id": "1612.00500", "submitter": "Ruohan Gao", "authors": "Ruohan Gao, Dinesh Jayaraman, Kristen Grauman", "title": "Object-Centric Representation Learning from Unlabeled Videos", "comments": "In Proceedings of the Asian Conference on Computer Vision (ACCV),\n  2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Supervised (pre-)training currently yields state-of-the-art performance for\nrepresentation learning for visual recognition, yet it comes at the cost of (1)\nintensive manual annotations and (2) an inherent restriction in the scope of\ndata relevant for learning. In this work, we explore unsupervised feature\nlearning from unlabeled video. We introduce a novel object-centric approach to\ntemporal coherence that encourages similar representations to be learned for\nobject-like regions segmented from nearby frames. Our framework relies on a\nSiamese-triplet network to train a deep convolutional neural network (CNN)\nrepresentation. Compared to existing temporal coherence methods, our idea has\nthe advantage of lightweight preprocessing of the unlabeled video (no tracking\nrequired) while still being able to extract object-level regions from which to\nlearn invariances. Furthermore, as we show in results on several standard\ndatasets, our method typically achieves substantial accuracy gains over\ncompeting unsupervised methods for image classification and retrieval tasks.\n", "versions": [{"version": "v1", "created": "Thu, 1 Dec 2016 22:36:20 GMT"}], "update_date": "2016-12-05", "authors_parsed": [["Gao", "Ruohan", ""], ["Jayaraman", "Dinesh", ""], ["Grauman", "Kristen", ""]]}, {"id": "1612.00522", "submitter": "Jiajun Lu", "authors": "Jiajun Lu, Kalyan Sunkavalli, Nathan Carr, Sunil Hadap, David Forsyth", "title": "A Visual Representation for Editing Face Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new approach for editing face images, which enables numerous\nexciting applications including face relighting, makeup transfer and face\ndetail editing. Our face edits are based on a visual representation, which\nincludes geometry, face segmentation, albedo, illumination and detail map. To\nrecover our visual representation, we start by estimating geometry using a\nmorphable face model, then decompose the face image to recover the albedo, and\nthen shade the geometry with the albedo and illumination. The residual between\nour shaded geometry and the input image produces our detail map, which carries\nhigh frequency information that is either insufficiently or incorrectly\ncaptured by our shading process. By manipulating the detail map, we can edit\nface images with reality and identity preserved. Our representation allows\nvarious applications. First, it allows a user to directly manipulate various\nillumination. Second, it allows non-parametric makeup transfer with input\nface's distinctive identity features preserved. Third, it allows non-parametric\nmodifications to the face appearance by transferring details. For face\nrelighting and detail editing, we evaluate via a user study and our method\noutperforms other methods. For makeup transfer, we evaluate via an online\nattractiveness evaluation system, and can reliably make people look younger and\nmore attractive. We also show extensive qualitative comparisons to existing\nmethods, and have significant improvements over previous techniques.\n", "versions": [{"version": "v1", "created": "Fri, 2 Dec 2016 00:07:38 GMT"}], "update_date": "2016-12-05", "authors_parsed": [["Lu", "Jiajun", ""], ["Sunkavalli", "Kalyan", ""], ["Carr", "Nathan", ""], ["Hadap", "Sunil", ""], ["Forsyth", "David", ""]]}, {"id": "1612.00523", "submitter": "Shunsuke Saito", "authors": "Shunsuke Saito, Lingyu Wei, Liwen Hu, Koki Nagano, Hao Li", "title": "Photorealistic Facial Texture Inference Using Deep Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a data-driven inference method that can synthesize a\nphotorealistic texture map of a complete 3D face model given a partial 2D view\nof a person in the wild. After an initial estimation of shape and low-frequency\nalbedo, we compute a high-frequency partial texture map, without the shading\ncomponent, of the visible face area. To extract the fine appearance details\nfrom this incomplete input, we introduce a multi-scale detail analysis\ntechnique based on mid-layer feature correlations extracted from a deep\nconvolutional neural network. We demonstrate that fitting a convex combination\nof feature correlations from a high-resolution face database can yield a\nsemantically plausible facial detail description of the entire face. A complete\nand photorealistic texture map can then be synthesized by iteratively\noptimizing for the reconstructed feature correlations. Using these\nhigh-resolution textures and a commercial rendering framework, we can produce\nhigh-fidelity 3D renderings that are visually comparable to those obtained with\nstate-of-the-art multi-view face capture systems. We demonstrate successful\nface reconstructions from a wide range of low resolution input images,\nincluding those of historical figures. In addition to extensive evaluations, we\nvalidate the realism of our results using a crowdsourced user study.\n", "versions": [{"version": "v1", "created": "Fri, 2 Dec 2016 00:14:12 GMT"}], "update_date": "2016-12-05", "authors_parsed": [["Saito", "Shunsuke", ""], ["Wei", "Lingyu", ""], ["Hu", "Liwen", ""], ["Nagano", "Koki", ""], ["Li", "Hao", ""]]}, {"id": "1612.00534", "submitter": "Bo Li", "authors": "Bo Li, Tianfu Wu, Shuai Shao, Lun Zhang and Rufeng Chu", "title": "Object Detection via Aspect Ratio and Context Aware Region-based\n  Convolutional Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Jointly integrating aspect ratio and context has been extensively studied and\nshown performance improvement in traditional object detection systems such as\nthe DPMs. It, however, has been largely ignored in deep neural network based\ndetection systems. This paper presents a method of integrating a mixture of\nobject models and region-based convolutional networks for accurate object\ndetection. Each mixture component accounts for both object aspect ratio and\nmulti-scale contextual information explicitly: (i) it exploits a mixture of\ntiling configurations in the RoI pooling to remedy the warping artifacts caused\nby a single type RoI pooling (e.g., with equally-sized 7 x 7 cells), and to\nrespect the underlying object shapes more; (ii) it \"looks from both the inside\nand the outside of a RoI\" by incorporating contextual information at two\nscales: global context pooled from the whole image and local context pooled\nfrom the surrounding of a RoI. To facilitate accurate detection, this paper\nproposes a multi-stage detection scheme for integrating the mixture of object\nmodels, which utilizes the detection results of the model at the previous stage\nas the proposals for the current in both training and testing. The proposed\nmethod is called the aspect ratio and context aware region-based convolutional\nnetwork (ARC-R-CNN). In experiments, ARC-R-CNN shows very competitive results\nwith Faster R-CNN [41] and R-FCN [10] on two datasets: the PASCAL VOC and the\nMicrosoft COCO. It obtains significantly better mAP performance using high IoU\nthresholds on both datasets.\n", "versions": [{"version": "v1", "created": "Fri, 2 Dec 2016 01:20:02 GMT"}, {"version": "v2", "created": "Wed, 22 Mar 2017 16:28:24 GMT"}], "update_date": "2017-03-23", "authors_parsed": [["Li", "Bo", ""], ["Wu", "Tianfu", ""], ["Shao", "Shuai", ""], ["Zhang", "Lun", ""], ["Chu", "Rufeng", ""]]}, {"id": "1612.00542", "submitter": "Daniel L\\'evy", "authors": "Daniel L\\'evy, Arzav Jain", "title": "Breast Mass Classification from Mammograms using Deep Convolutional\n  Neural Networks", "comments": "NIPS 2016 ML4HC Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mammography is the most widely used method to screen breast cancer. Because\nof its mostly manual nature, variability in mass appearance, and low\nsignal-to-noise ratio, a significant number of breast masses are missed or\nmisdiagnosed. In this work, we present how Convolutional Neural Networks can be\nused to directly classify pre-segmented breast masses in mammograms as benign\nor malignant, using a combination of transfer learning, careful pre-processing\nand data augmentation to overcome limited training data. We achieve\nstate-of-the-art results on the DDSM dataset, surpassing human performance, and\nshow interpretability of our model.\n", "versions": [{"version": "v1", "created": "Fri, 2 Dec 2016 02:06:15 GMT"}], "update_date": "2016-12-05", "authors_parsed": [["L\u00e9vy", "Daniel", ""], ["Jain", "Arzav", ""]]}, {"id": "1612.00558", "submitter": "Basura Fernando", "authors": "Basura Fernando, Sareh Shirazi and Stephen Gould", "title": "Unsupervised Human Action Detection by Action Matching", "comments": "IEEE International Conference on Computer Vision and Pattern\n  Recognition CVPR 2017 Workshops", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new task of unsupervised action detection by action matching.\nGiven two long videos, the objective is to temporally detect all pairs of\nmatching video segments. A pair of video segments are matched if they share the\nsame human action. The task is category independent---it does not matter what\naction is being performed---and no supervision is used to discover such video\nsegments. Unsupervised action detection by action matching allows us to align\nvideos in a meaningful manner. As such, it can be used to discover new action\ncategories or as an action proposal technique within, say, an action detection\npipeline. Moreover, it is a useful pre-processing step for generating video\nhighlights, e.g., from sports videos.\n  We present an effective and efficient method for unsupervised action\ndetection. We use an unsupervised temporal encoding method and exploit the\ntemporal consistency in human actions to obtain candidate action segments. We\nevaluate our method on this challenging task using three activity recognition\nbenchmarks, namely, the MPII Cooking activities dataset, the THUMOS15 action\ndetection benchmark and a new dataset called the IKEA dataset. On the MPII\nCooking dataset we detect action segments with a precision of 21.6% and recall\nof 11.7% over 946 long video pairs and over 5000 ground truth action segments.\nSimilarly, on THUMOS dataset we obtain 18.4% precision and 25.1% recall over\n5094 ground truth action segment pairs.\n", "versions": [{"version": "v1", "created": "Fri, 2 Dec 2016 03:39:38 GMT"}, {"version": "v2", "created": "Mon, 3 Apr 2017 03:36:17 GMT"}, {"version": "v3", "created": "Wed, 5 Apr 2017 06:18:22 GMT"}, {"version": "v4", "created": "Tue, 16 May 2017 00:56:24 GMT"}], "update_date": "2017-05-17", "authors_parsed": [["Fernando", "Basura", ""], ["Shirazi", "Sareh", ""], ["Gould", "Stephen", ""]]}, {"id": "1612.00560", "submitter": "Bo Zhao", "authors": "Bo Zhao, Botong Wu, Tianfu Wu, Yizhou Wang", "title": "Zero-Shot Learning posed as a Missing Data Problem", "comments": null, "journal-ref": "2017 IEEE International Conference on Computer Vision Workshops\n  (ICCVW)", "doi": "10.1109/ICCVW.2017.310", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a method of zero-shot learning (ZSL) which poses ZSL as\nthe missing data problem, rather than the missing label problem. Specifically,\nmost existing ZSL methods focus on learning mapping functions from the image\nfeature space to the label embedding space. Whereas, the proposed method\nexplores a simple yet effective transductive framework in the reverse way \\---\nour method estimates data distribution of unseen classes in the image feature\nspace by transferring knowledge from the label embedding space. In experiments,\nour method outperforms the state-of-the-art on two popular datasets.\n", "versions": [{"version": "v1", "created": "Fri, 2 Dec 2016 03:49:23 GMT"}, {"version": "v2", "created": "Tue, 21 Feb 2017 02:37:56 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Zhao", "Bo", ""], ["Wu", "Botong", ""], ["Wu", "Tianfu", ""], ["Wang", "Yizhou", ""]]}, {"id": "1612.00563", "submitter": "Steven Rennie", "authors": "Steven J. Rennie, Etienne Marcheret, Youssef Mroueh, Jarret Ross and\n  Vaibhava Goel", "title": "Self-critical Sequence Training for Image Captioning", "comments": "CVPR 2017 + additional analysis + fixed baseline results, 16 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently it has been shown that policy-gradient methods for reinforcement\nlearning can be utilized to train deep end-to-end systems directly on\nnon-differentiable metrics for the task at hand. In this paper we consider the\nproblem of optimizing image captioning systems using reinforcement learning,\nand show that by carefully optimizing our systems using the test metrics of the\nMSCOCO task, significant gains in performance can be realized. Our systems are\nbuilt using a new optimization approach that we call self-critical sequence\ntraining (SCST). SCST is a form of the popular REINFORCE algorithm that, rather\nthan estimating a \"baseline\" to normalize the rewards and reduce variance,\nutilizes the output of its own test-time inference algorithm to normalize the\nrewards it experiences. Using this approach, estimating the reward signal (as\nactor-critic methods must do) and estimating normalization (as REINFORCE\nalgorithms typically do) is avoided, while at the same time harmonizing the\nmodel with respect to its test-time inference procedure. Empirically we find\nthat directly optimizing the CIDEr metric with SCST and greedy decoding at\ntest-time is highly effective. Our results on the MSCOCO evaluation sever\nestablish a new state-of-the-art on the task, improving the best result in\nterms of CIDEr from 104.9 to 114.7.\n", "versions": [{"version": "v1", "created": "Fri, 2 Dec 2016 04:37:22 GMT"}, {"version": "v2", "created": "Thu, 16 Nov 2017 02:38:37 GMT"}], "update_date": "2017-11-17", "authors_parsed": [["Rennie", "Steven J.", ""], ["Marcheret", "Etienne", ""], ["Mroueh", "Youssef", ""], ["Ross", "Jarret", ""], ["Goel", "Vaibhava", ""]]}, {"id": "1612.00576", "submitter": "Peter Anderson", "authors": "Peter Anderson, Basura Fernando, Mark Johnson, Stephen Gould", "title": "Guided Open Vocabulary Image Captioning with Constrained Beam Search", "comments": "EMNLP 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing image captioning models do not generalize well to out-of-domain\nimages containing novel scenes or objects. This limitation severely hinders the\nuse of these models in real world applications dealing with images in the wild.\nWe address this problem using a flexible approach that enables existing deep\ncaptioning architectures to take advantage of image taggers at test time,\nwithout re-training. Our method uses constrained beam search to force the\ninclusion of selected tag words in the output, and fixed, pretrained word\nembeddings to facilitate vocabulary expansion to previously unseen tag words.\nUsing this approach we achieve state of the art results for out-of-domain\ncaptioning on MSCOCO (and improved results for in-domain captioning). Perhaps\nsurprisingly, our results significantly outperform approaches that incorporate\nthe same tag predictions into the learning algorithm. We also show that we can\nsignificantly improve the quality of generated ImageNet captions by leveraging\nground-truth labels.\n", "versions": [{"version": "v1", "created": "Fri, 2 Dec 2016 06:50:49 GMT"}, {"version": "v2", "created": "Wed, 19 Jul 2017 22:01:27 GMT"}], "update_date": "2017-07-21", "authors_parsed": [["Anderson", "Peter", ""], ["Fernando", "Basura", ""], ["Johnson", "Mark", ""], ["Gould", "Stephen", ""]]}, {"id": "1612.00593", "submitter": "Charles Ruizhongtai Qi", "authors": "Charles R. Qi, Hao Su, Kaichun Mo, Leonidas J. Guibas", "title": "PointNet: Deep Learning on Point Sets for 3D Classification and\n  Segmentation", "comments": "CVPR 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Point cloud is an important type of geometric data structure. Due to its\nirregular format, most researchers transform such data to regular 3D voxel\ngrids or collections of images. This, however, renders data unnecessarily\nvoluminous and causes issues. In this paper, we design a novel type of neural\nnetwork that directly consumes point clouds and well respects the permutation\ninvariance of points in the input. Our network, named PointNet, provides a\nunified architecture for applications ranging from object classification, part\nsegmentation, to scene semantic parsing. Though simple, PointNet is highly\nefficient and effective. Empirically, it shows strong performance on par or\neven better than state of the art. Theoretically, we provide analysis towards\nunderstanding of what the network has learnt and why the network is robust with\nrespect to input perturbation and corruption.\n", "versions": [{"version": "v1", "created": "Fri, 2 Dec 2016 08:40:40 GMT"}, {"version": "v2", "created": "Mon, 10 Apr 2017 22:25:25 GMT"}], "update_date": "2017-04-12", "authors_parsed": [["Qi", "Charles R.", ""], ["Su", "Hao", ""], ["Mo", "Kaichun", ""], ["Guibas", "Leonidas J.", ""]]}, {"id": "1612.00596", "submitter": "Li Cheng", "authors": "Yu Zhang, Chi Xu, Li Cheng", "title": "Learning to Search on Manifolds for 3D Pose Estimation of Articulated\n  Objects", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper focuses on the challenging problem of 3D pose estimation of a\ndiverse spectrum of articulated objects from single depth images. A novel\nstructured prediction approach is considered, where 3D poses are represented as\nskeletal models that naturally operate on manifolds. Given an input depth\nimage, the problem of predicting the most proper articulation of underlying\nskeletal model is thus formulated as sequentially searching for the optimal\nskeletal configuration. This is subsequently addressed by convolutional neural\nnets trained end-to-end to render sequential prediction of the joint locations\nas regressing a set of tangent vectors of the underlying manifolds. Our\napproach is examined on various articulated objects including human hand,\nmouse, and fish benchmark datasets. Empirically it is shown to deliver highly\ncompetitive performance with respect to the state-of-the-arts, while operating\nin real-time (over 30 FPS).\n", "versions": [{"version": "v1", "created": "Fri, 2 Dec 2016 08:54:28 GMT"}], "update_date": "2016-12-05", "authors_parsed": [["Zhang", "Yu", ""], ["Xu", "Chi", ""], ["Cheng", "Li", ""]]}, {"id": "1612.00603", "submitter": "Hao Su", "authors": "Haoqiang Fan, Hao Su, Leonidas Guibas", "title": "A Point Set Generation Network for 3D Object Reconstruction from a\n  Single Image", "comments": "The first two authors contributed equally", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generation of 3D data by deep neural network has been attracting increasing\nattention in the research community. The majority of extant works resort to\nregular representations such as volumetric grids or collection of images;\nhowever, these representations obscure the natural invariance of 3D shapes\nunder geometric transformations and also suffer from a number of other issues.\nIn this paper we address the problem of 3D reconstruction from a single image,\ngenerating a straight-forward form of output -- point cloud coordinates. Along\nwith this problem arises a unique and interesting issue, that the groundtruth\nshape for an input image may be ambiguous. Driven by this unorthodox output\nform and the inherent ambiguity in groundtruth, we design architecture, loss\nfunction and learning paradigm that are novel and effective. Our final solution\nis a conditional shape sampler, capable of predicting multiple plausible 3D\npoint clouds from an input image. In experiments not only can our system\noutperform state-of-the-art methods on single image based 3d reconstruction\nbenchmarks; but it also shows a strong performance for 3d shape completion and\npromising ability in making multiple plausible predictions.\n", "versions": [{"version": "v1", "created": "Fri, 2 Dec 2016 09:20:09 GMT"}, {"version": "v2", "created": "Wed, 7 Dec 2016 01:12:53 GMT"}], "update_date": "2016-12-08", "authors_parsed": [["Fan", "Haoqiang", ""], ["Su", "Hao", ""], ["Guibas", "Leonidas", ""]]}, {"id": "1612.00604", "submitter": "Andrii Maksai", "authors": "Andrii Maksai, Xinchao Wang, Francois Fleuret, and Pascal Fua", "title": "Globally Consistent Multi-People Tracking using Motion Patterns", "comments": "8 pages, 7 figures. 11 pages supplementary", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many state-of-the-art approaches to people tracking rely on detecting them in\neach frame independently, grouping detections into short but reliable\ntrajectory segments, and then further grouping them into full trajectories.\nThis grouping typically relies on imposing local smoothness constraints but\nalmost never on enforcing more global constraints on the trajectories. In this\npaper, we propose an approach to imposing global consistency by first inferring\nbehavioral patterns from the ground truth and then using them to guide the\ntracking algorithm. When used in conjunction with several state-of-the-art\nalgorithms, this further increases their already good performance. Furthermore,\nwe propose an unsupervised scheme that yields almost similar improvements\nwithout the need for ground truth.\n", "versions": [{"version": "v1", "created": "Fri, 2 Dec 2016 09:24:30 GMT"}], "update_date": "2016-12-05", "authors_parsed": [["Maksai", "Andrii", ""], ["Wang", "Xinchao", ""], ["Fleuret", "Francois", ""], ["Fua", "Pascal", ""]]}, {"id": "1612.00606", "submitter": "Li Yi", "authors": "Li Yi, Hao Su, Xingwen Guo, Leonidas Guibas", "title": "SyncSpecCNN: Synchronized Spectral CNN for 3D Shape Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study the problem of semantic annotation on 3D models that\nare represented as shape graphs. A functional view is taken to represent\nlocalized information on graphs, so that annotations such as part segment or\nkeypoint are nothing but 0-1 indicator vertex functions. Compared with images\nthat are 2D grids, shape graphs are irregular and non-isomorphic data\nstructures. To enable the prediction of vertex functions on them by\nconvolutional neural networks, we resort to spectral CNN method that enables\nweight sharing by parameterizing kernels in the spectral domain spanned by\ngraph laplacian eigenbases. Under this setting, our network, named SyncSpecCNN,\nstrive to overcome two key challenges: how to share coefficients and conduct\nmulti-scale analysis in different parts of the graph for a single shape, and\nhow to share information across related but different shapes that may be\nrepresented by very different graphs. Towards these goals, we introduce a\nspectral parameterization of dilated convolutional kernels and a spectral\ntransformer network. Experimentally we tested our SyncSpecCNN on various tasks,\nincluding 3D shape part segmentation and 3D keypoint prediction.\nState-of-the-art performance has been achieved on all benchmark datasets.\n", "versions": [{"version": "v1", "created": "Fri, 2 Dec 2016 09:27:34 GMT"}], "update_date": "2016-12-05", "authors_parsed": [["Yi", "Li", ""], ["Su", "Hao", ""], ["Guo", "Xingwen", ""], ["Guibas", "Leonidas", ""]]}, {"id": "1612.00625", "submitter": "Vijendra Singh", "authors": "Singh Vijendra, Nisha Vasudeva and Hem Jyotsana Parashar", "title": "Recognition of Text Image Using Multilayer Perceptron", "comments": "2011 IEEE 3rd International Conference on Machine Learning and\n  Computing (ICMLC 2011, Singapore, PP 547-550", "journal-ref": null, "doi": null, "report-no": "978-1-4244-925 3-4 /11/IEEE", "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The biggest challenge in the field of image processing is to recognize\ndocuments both in printed and handwritten format. Optical Character Recognition\nOCR is a type of document image analysis where scanned digital image that\ncontains either machine printed or handwritten script input into an OCR\nsoftware engine and translating it into an editable machine readable digital\ntext format. A Neural network is designed to model the way in which the brain\nperforms a particular task or function of interest: The neural network is\nsimulated in software on a digital computer. Character Recognition refers to\nthe process of converting printed Text documents into translated Unicode Text.\nThe printed documents available in the form of books, papers, magazines, etc.\nare scanned using standard scanners which produce an image of the scanned\ndocument. Lines are identifying by an algorithm where we identify top and\nbottom of line. Then in each line character boundaries are calculated by an\nalgorithm then using these calculation, characters is isolated from the image\nand then we classify each character by basic back propagation. Each image\ncharacter is comprised of 30*20 pixels. We have used the Back propagation\nNeural Network for efficient recognition where the errors were corrected\nthrough back propagation and rectified neuron values were transmitted by\nfeed-forward method in the neural network of multiple layers.\n", "versions": [{"version": "v1", "created": "Fri, 2 Dec 2016 10:43:04 GMT"}], "update_date": "2016-12-05", "authors_parsed": [["Vijendra", "Singh", ""], ["Vasudeva", "Nisha", ""], ["Parashar", "Hem Jyotsana", ""]]}, {"id": "1612.00645", "submitter": "Martins Irhebhude", "authors": "Martins E. Irhebhude, Philip O. Odion and Darius T. Chinyio", "title": "Centrog Feature technique for vehicle type recognition at day and night\n  times", "comments": "14 pages, 8 figures, Journal article", "journal-ref": null, "doi": "10.5121/ijaia.2016.7604", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work proposes a feature-based technique to recognize vehicle types\nwithin day and night times. Support vector machine (SVM) classifier is applied\non image histogram and CENsus Transformed histogRam Oriented Gradient (CENTROG)\nfeatures in order to classify vehicle types during the day and night. Thermal\nimages were used for the night time experiments. Although thermal images suffer\nfrom low image resolution, lack of colour and poor texture information, they\noffer the advantage of being unaffected by high intensity light sources such as\nvehicle headlights which tend to render normal images unsuitable for night time\nimage capturing and subsequent analysis. Since contour is useful in shape based\ncategorisation and the most distinctive feature within thermal images, CENTROG\nis used to capture this feature information and is used within the experiments.\nThe experimental results so obtained were compared with those obtained by\nemploying the CENsus TRansformed hISTogram (CENTRIST). Experimental results\nrevealed that CENTROG offers better recognition accuracies for both day and\nnight times vehicle types recognition.\n", "versions": [{"version": "v1", "created": "Fri, 2 Dec 2016 11:51:50 GMT"}], "update_date": "2016-12-05", "authors_parsed": [["Irhebhude", "Martins E.", ""], ["Odion", "Philip O.", ""], ["Chinyio", "Darius T.", ""]]}, {"id": "1612.00667", "submitter": "Santi Puch", "authors": "Santi Puch, Asier Aduriz, Adri\\`a Casamitjana, Veronica Vilaplana,\n  Paula Petrone, Gr\\'egory Operto, Raffaele Cacciaglia, Stavros Skouras, Carles\n  Falcon, Jos\\'e Luis Molinuevo, Juan Domingo Gispert", "title": "Voxelwise nonlinear regression toolbox for neuroimage analysis:\n  Application to aging and neurodegenerative disease modeling", "comments": "4 pages + 1 page for acknowledgements and references. NIPS 2016\n  Workshop on Machine Learning for Health (NIPS ML4HC)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG q-bio.NC stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes a new neuroimaging analysis toolbox that allows for the\nmodeling of nonlinear effects at the voxel level, overcoming limitations of\nmethods based on linear models like the GLM. We illustrate its features using a\nrelevant example in which distinct nonlinear trajectories of Alzheimer's\ndisease related brain atrophy patterns were found across the full biological\nspectrum of the disease. The open-source toolbox presented in this paper is\navailable at https://github.com/imatge-upc/VNeAT.\n", "versions": [{"version": "v1", "created": "Fri, 2 Dec 2016 12:59:11 GMT"}, {"version": "v2", "created": "Sun, 5 Mar 2017 10:58:16 GMT"}, {"version": "v3", "created": "Tue, 18 Apr 2017 20:12:16 GMT"}], "update_date": "2017-04-20", "authors_parsed": [["Puch", "Santi", ""], ["Aduriz", "Asier", ""], ["Casamitjana", "Adri\u00e0", ""], ["Vilaplana", "Veronica", ""], ["Petrone", "Paula", ""], ["Operto", "Gr\u00e9gory", ""], ["Cacciaglia", "Raffaele", ""], ["Skouras", "Stavros", ""], ["Falcon", "Carles", ""], ["Molinuevo", "Jos\u00e9 Luis", ""], ["Gispert", "Juan Domingo", ""]]}, {"id": "1612.00686", "submitter": "Philipp Seeb\\\"ock", "authors": "Philipp Seeb\\\"ock, Sebastian Waldstein, Sophie Klimscha, Bianca S.\n  Gerendas, Ren\\'e Donner, Thomas Schlegl, Ursula Schmidt-Erfurth and Georg\n  Langs", "title": "Identifying and Categorizing Anomalies in Retinal Imaging Data", "comments": "Extended Abstract, Accepted for NIPS 2016 Workshop \"Machine Learning\n  for Health\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The identification and quantification of markers in medical images is\ncritical for diagnosis, prognosis and management of patients in clinical\npractice. Supervised- or weakly supervised training enables the detection of\nfindings that are known a priori. It does not scale well, and a priori\ndefinition limits the vocabulary of markers to known entities reducing the\naccuracy of diagnosis and prognosis. Here, we propose the identification of\nanomalies in large-scale medical imaging data using healthy examples as a\nreference. We detect and categorize candidates for anomaly findings untypical\nfor the observed data. A deep convolutional autoencoder is trained on healthy\nretinal images. The learned model generates a new feature representation, and\nthe distribution of healthy retinal patches is estimated by a one-class support\nvector machine. Results demonstrate that we can identify pathologic regions in\nimages without using expert annotations. A subsequent clustering categorizes\nfindings into clinically meaningful classes. In addition the learned features\noutperform standard embedding approaches in a classification task.\n", "versions": [{"version": "v1", "created": "Fri, 2 Dec 2016 14:05:49 GMT"}], "update_date": "2016-12-05", "authors_parsed": [["Seeb\u00f6ck", "Philipp", ""], ["Waldstein", "Sebastian", ""], ["Klimscha", "Sophie", ""], ["Gerendas", "Bianca S.", ""], ["Donner", "Ren\u00e9", ""], ["Schlegl", "Thomas", ""], ["Schmidt-Erfurth", "Ursula", ""], ["Langs", "Georg", ""]]}, {"id": "1612.00738", "submitter": "Hakan Bilen", "authors": "Hakan Bilen, Basura Fernando, Efstratios Gavves, Andrea Vedaldi", "title": "Action Recognition with Dynamic Image Networks", "comments": "14 pages, 9 figures, 9 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce the concept of \"dynamic image\", a novel compact representation\nof videos useful for video analysis, particularly in combination with\nconvolutional neural networks (CNNs). A dynamic image encodes temporal data\nsuch as RGB or optical flow videos by using the concept of `rank pooling'. The\nidea is to learn a ranking machine that captures the temporal evolution of the\ndata and to use the parameters of the latter as a representation. When a linear\nranking machine is used, the resulting representation is in the form of an\nimage, which we call dynamic because it summarizes the video dynamics in\naddition of appearance. This is a powerful idea because it allows to convert\nany video to an image so that existing CNN models pre-trained for the analysis\nof still images can be immediately extended to videos. We also present an\nefficient and effective approximate rank pooling operator, accelerating\nstandard rank pooling algorithms by orders of magnitude, and formulate that as\na CNN layer. This new layer allows generalizing dynamic images to dynamic\nfeature maps. We demonstrate the power of the new representations on standard\nbenchmarks in action recognition achieving state-of-the-art performance.\n", "versions": [{"version": "v1", "created": "Fri, 2 Dec 2016 16:33:06 GMT"}, {"version": "v2", "created": "Sat, 19 Aug 2017 20:54:07 GMT"}], "update_date": "2017-08-22", "authors_parsed": [["Bilen", "Hakan", ""], ["Fernando", "Basura", ""], ["Gavves", "Efstratios", ""], ["Vedaldi", "Andrea", ""]]}, {"id": "1612.00799", "submitter": "David V\\'azquez", "authors": "David V\\'azquez, Jorge Bernal, F. Javier S\\'anchez, Gloria\n  Fern\\'andez-Esparrach, Antonio M. L\\'opez, Adriana Romero, Michal Drozdzal\n  and Aaron Courville", "title": "A Benchmark for Endoluminal Scene Segmentation of Colonoscopy Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Colorectal cancer (CRC) is the third cause of cancer death worldwide.\nCurrently, the standard approach to reduce CRC-related mortality is to perform\nregular screening in search for polyps and colonoscopy is the screening tool of\nchoice. The main limitations of this screening procedure are polyp miss-rate\nand inability to perform visual assessment of polyp malignancy. These drawbacks\ncan be reduced by designing Decision Support Systems (DSS) aiming to help\nclinicians in the different stages of the procedure by providing endoluminal\nscene segmentation. Thus, in this paper, we introduce an extended benchmark of\ncolonoscopy image, with the hope of establishing a new strong benchmark for\ncolonoscopy image analysis research. We provide new baselines on this dataset\nby training standard fully convolutional networks (FCN) for semantic\nsegmentation and significantly outperforming, without any further\npost-processing, prior results in endoluminal scene segmentation.\n", "versions": [{"version": "v1", "created": "Fri, 2 Dec 2016 19:25:44 GMT"}], "update_date": "2016-12-05", "authors_parsed": [["V\u00e1zquez", "David", ""], ["Bernal", "Jorge", ""], ["S\u00e1nchez", "F. Javier", ""], ["Fern\u00e1ndez-Esparrach", "Gloria", ""], ["L\u00f3pez", "Antonio M.", ""], ["Romero", "Adriana", ""], ["Drozdzal", "Michal", ""], ["Courville", "Aaron", ""]]}, {"id": "1612.00814", "submitter": "Xinchen Yan", "authors": "Xinchen Yan, Jimei Yang, Ersin Yumer, Yijie Guo, Honglak Lee", "title": "Perspective Transformer Nets: Learning Single-View 3D Object\n  Reconstruction without 3D Supervision", "comments": "published at NIPS 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Understanding the 3D world is a fundamental problem in computer vision.\nHowever, learning a good representation of 3D objects is still an open problem\ndue to the high dimensionality of the data and many factors of variation\ninvolved. In this work, we investigate the task of single-view 3D object\nreconstruction from a learning agent's perspective. We formulate the learning\nprocess as an interaction between 3D and 2D representations and propose an\nencoder-decoder network with a novel projection loss defined by the perspective\ntransformation. More importantly, the projection loss enables the unsupervised\nlearning using 2D observation without explicit 3D supervision. We demonstrate\nthe ability of the model in generating 3D volume from a single 2D image with\nthree sets of experiments: (1) learning from single-class objects; (2) learning\nfrom multi-class objects and (3) testing on novel object classes. Results show\nsuperior performance and better generalization ability for 3D object\nreconstruction when the projection loss is involved.\n", "versions": [{"version": "v1", "created": "Thu, 1 Dec 2016 05:51:37 GMT"}, {"version": "v2", "created": "Thu, 16 Mar 2017 07:08:48 GMT"}, {"version": "v3", "created": "Sun, 13 Aug 2017 02:40:50 GMT"}], "update_date": "2017-08-15", "authors_parsed": [["Yan", "Xinchen", ""], ["Yang", "Jimei", ""], ["Yumer", "Ersin", ""], ["Guo", "Yijie", ""], ["Lee", "Honglak", ""]]}, {"id": "1612.00835", "submitter": "Patsorn Sangkloy", "authors": "Patsorn Sangkloy, Jingwan Lu, Chen Fang, Fisher Yu, James Hays", "title": "Scribbler: Controlling Deep Image Synthesis with Sketch and Color", "comments": "13 pages, 14 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, there have been several promising methods to generate realistic\nimagery from deep convolutional networks. These methods sidestep the\ntraditional computer graphics rendering pipeline and instead generate imagery\nat the pixel level by learning from large collections of photos (e.g. faces or\nbedrooms). However, these methods are of limited utility because it is\ndifficult for a user to control what the network produces. In this paper, we\npropose a deep adversarial image synthesis architecture that is conditioned on\nsketched boundaries and sparse color strokes to generate realistic cars,\nbedrooms, or faces. We demonstrate a sketch based image synthesis system which\nallows users to 'scribble' over the sketch to indicate preferred color for\nobjects. Our network can then generate convincing images that satisfy both the\ncolor and the sketch constraints of user. The network is feed-forward which\nallows users to see the effect of their edits in real time. We compare to\nrecent work on sketch to image synthesis and show that our approach can\ngenerate more realistic, more diverse, and more controllable outputs. The\narchitecture is also effective at user-guided colorization of grayscale images.\n", "versions": [{"version": "v1", "created": "Fri, 2 Dec 2016 20:53:01 GMT"}, {"version": "v2", "created": "Mon, 5 Dec 2016 20:06:57 GMT"}], "update_date": "2016-12-06", "authors_parsed": [["Sangkloy", "Patsorn", ""], ["Lu", "Jingwan", ""], ["Fang", "Chen", ""], ["Yu", "Fisher", ""], ["Hays", "James", ""]]}, {"id": "1612.00837", "submitter": "Yash Goyal", "authors": "Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, Devi Parikh", "title": "Making the V in VQA Matter: Elevating the Role of Image Understanding in\n  Visual Question Answering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Problems at the intersection of vision and language are of significant\nimportance both as challenging research questions and for the rich set of\napplications they enable. However, inherent structure in our world and bias in\nour language tend to be a simpler signal for learning than visual modalities,\nresulting in models that ignore visual information, leading to an inflated\nsense of their capability.\n  We propose to counter these language priors for the task of Visual Question\nAnswering (VQA) and make vision (the V in VQA) matter! Specifically, we balance\nthe popular VQA dataset by collecting complementary images such that every\nquestion in our balanced dataset is associated with not just a single image,\nbut rather a pair of similar images that result in two different answers to the\nquestion. Our dataset is by construction more balanced than the original VQA\ndataset and has approximately twice the number of image-question pairs. Our\ncomplete balanced dataset is publicly available at www.visualqa.org as part of\nthe 2nd iteration of the Visual Question Answering Dataset and Challenge (VQA\nv2.0).\n  We further benchmark a number of state-of-art VQA models on our balanced\ndataset. All models perform significantly worse on our balanced dataset,\nsuggesting that these models have indeed learned to exploit language priors.\nThis finding provides the first concrete empirical evidence for what seems to\nbe a qualitative sense among practitioners.\n  Finally, our data collection protocol for identifying complementary images\nenables us to develop a novel interpretable model, which in addition to\nproviding an answer to the given (image, question) pair, also provides a\ncounter-example based explanation. Specifically, it identifies an image that is\nsimilar to the original image, but it believes has a different answer to the\nsame question. This can help in building trust for machines among their users.\n", "versions": [{"version": "v1", "created": "Fri, 2 Dec 2016 20:57:07 GMT"}, {"version": "v2", "created": "Fri, 14 Apr 2017 18:20:13 GMT"}, {"version": "v3", "created": "Mon, 15 May 2017 17:58:49 GMT"}], "update_date": "2017-05-16", "authors_parsed": [["Goyal", "Yash", ""], ["Khot", "Tejas", ""], ["Summers-Stay", "Douglas", ""], ["Batra", "Dhruv", ""], ["Parikh", "Devi", ""]]}, {"id": "1612.00874", "submitter": "Suhas Sreehari", "authors": "Suhas Sreehari, S. V. Venkatakrishnan, Katherine L. Bouman, Jeffrey P.\n  Simmons, Lawrence F. Drummy, Charles A. Bouman", "title": "Multi-resolution Data Fusion for Super-Resolution Electron Microscopy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Perhaps surprisingly, the total electron microscopy (EM) data collected to\ndate is less than a cubic millimeter. Consequently, there is an enormous demand\nin the materials and biological sciences to image at greater speed and lower\ndosage, while maintaining resolution. Traditional EM imaging based on\nhomogeneous raster-order scanning severely limits the volume of high-resolution\ndata that can be collected, and presents a fundamental limitation to\nunderstanding physical processes such as material deformation, crack\npropagation, and pyrolysis.\n  We introduce a novel multi-resolution data fusion (MDF) method for\nsuper-resolution computational EM. Our method combines innovative data\nacquisition with novel algorithmic techniques to dramatically improve the\nresolution/volume/speed trade-off. The key to our approach is to collect the\nentire sample at low resolution, while simultaneously collecting a small\nfraction of data at high resolution. The high-resolution measurements are then\nused to create a material-specific patch-library that is used within the\n\"plug-and-play\" framework to dramatically improve super-resolution of the\nlow-resolution data. We present results using FEI electron microscope data that\ndemonstrate super-resolution factors of 4x, 8x, and 16x, while substantially\nmaintaining high image quality and reducing dosage.\n", "versions": [{"version": "v1", "created": "Mon, 28 Nov 2016 17:35:10 GMT"}], "update_date": "2016-12-06", "authors_parsed": [["Sreehari", "Suhas", ""], ["Venkatakrishnan", "S. V.", ""], ["Bouman", "Katherine L.", ""], ["Simmons", "Jeffrey P.", ""], ["Drummy", "Lawrence F.", ""], ["Bouman", "Charles A.", ""]]}, {"id": "1612.00881", "submitter": "Cesar Roberto de Souza", "authors": "C\\'esar Roberto de Souza, Adrien Gaidon, Yohann Cabon, Antonio Manuel\n  L\\'opez Pe\\~na", "title": "Procedural Generation of Videos to Train Deep Action Recognition\n  Networks", "comments": "Accepted for publication at CVPR 2017. http://adas.cvc.uab.es/phav/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning for human action recognition in videos is making significant\nprogress, but is slowed down by its dependency on expensive manual labeling of\nlarge video collections. In this work, we investigate the generation of\nsynthetic training data for action recognition, as it has recently shown\npromising results for a variety of other computer vision tasks. We propose an\ninterpretable parametric generative model of human action videos that relies on\nprocedural generation and other computer graphics techniques of modern game\nengines. We generate a diverse, realistic, and physically plausible dataset of\nhuman action videos, called PHAV for \"Procedural Human Action Videos\". It\ncontains a total of 39,982 videos, with more than 1,000 examples for each\naction of 35 categories. Our approach is not limited to existing motion capture\nsequences, and we procedurally define 14 synthetic actions. We introduce a deep\nmulti-task representation learning architecture to mix synthetic and real\nvideos, even if the action categories differ. Our experiments on the UCF101 and\nHMDB51 benchmarks suggest that combining our large set of synthetic videos with\nsmall real-world datasets can boost recognition performance, significantly\noutperforming fine-tuning state-of-the-art unsupervised generative models of\nvideos.\n", "versions": [{"version": "v1", "created": "Fri, 2 Dec 2016 22:24:24 GMT"}, {"version": "v2", "created": "Wed, 19 Jul 2017 10:34:36 GMT"}], "update_date": "2017-07-20", "authors_parsed": [["de Souza", "C\u00e9sar Roberto", ""], ["Gaidon", "Adrien", ""], ["Cabon", "Yohann", ""], ["Pe\u00f1a", "Antonio Manuel L\u00f3pez", ""]]}, {"id": "1612.00891", "submitter": "Jonathan Cox", "authors": "Jonathan A. Cox", "title": "Parameter Compression of Recurrent Neural Networks and Degradation of\n  Short-term Memory", "comments": "Accepted to IJCNN 2017. Final camera ready paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The significant computational costs of deploying neural networks in\nlarge-scale or resource constrained environments, such as data centers and\nmobile devices, has spurred interest in model compression, which can achieve a\nreduction in both arithmetic operations and storage memory. Several techniques\nhave been proposed for reducing or compressing the parameters for feed-forward\nand convolutional neural networks, but less is understood about the effect of\nparameter compression on recurrent neural networks (RNN). In particular, the\nextent to which the recurrent parameters can be compressed and the impact on\nshort-term memory performance, is not well understood. In this paper, we study\nthe effect of complexity reduction, through singular value decomposition rank\nreduction, on RNN and minimal gated recurrent unit (MGRU) networks for several\ntasks. We show that considerable rank reduction is possible when compressing\nrecurrent weights, even without fine tuning. Furthermore, we propose a\nperturbation model for the effect of general perturbations, such as a\ncompression, on the recurrent parameters of RNNs. The model is tested against a\nnoiseless memorization experiment that elucidates the short-term memory\nperformance. In this way, we demonstrate that the effect of compression of\nrecurrent parameters is dependent on the degree of temporal coherence present\nin the data and task. This work can guide on-the-fly RNN compression for novel\nenvironments or tasks, and provides insight for applying RNN compression in\nlow-power devices, such as hearing aids.\n", "versions": [{"version": "v1", "created": "Fri, 2 Dec 2016 23:11:10 GMT"}, {"version": "v2", "created": "Fri, 24 Feb 2017 18:22:30 GMT"}], "update_date": "2017-02-27", "authors_parsed": [["Cox", "Jonathan A.", ""]]}, {"id": "1612.00901", "submitter": "Mark Yatskar", "authors": "Mark Yatskar, Vicente Ordonez, Luke Zettlemoyer, Ali Farhadi", "title": "Commonly Uncommon: Semantic Sparsity in Situation Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semantic sparsity is a common challenge in structured visual classification\nproblems; when the output space is complex, the vast majority of the possible\npredictions are rarely, if ever, seen in the training set. This paper studies\nsemantic sparsity in situation recognition, the task of producing structured\nsummaries of what is happening in images, including activities, objects and the\nroles objects play within the activity. For this problem, we find empirically\nthat most object-role combinations are rare, and current state-of-the-art\nmodels significantly underperform in this sparse data regime. We avoid many\nsuch errors by (1) introducing a novel tensor composition function that learns\nto share examples across role-noun combinations and (2) semantically augmenting\nour training data with automatically gathered examples of rarely observed\noutputs using web data. When integrated within a complete CRF-based structured\nprediction model, the tensor-based approach outperforms existing state of the\nart by a relative improvement of 2.11% and 4.40% on top-5 verb and noun-role\naccuracy, respectively. Adding 5 million images with our semantic augmentation\ntechniques gives further relative improvements of 6.23% and 9.57% on top-5 verb\nand noun-role accuracy.\n", "versions": [{"version": "v1", "created": "Sat, 3 Dec 2016 00:31:52 GMT"}], "update_date": "2016-12-06", "authors_parsed": [["Yatskar", "Mark", ""], ["Ordonez", "Vicente", ""], ["Zettlemoyer", "Luke", ""], ["Farhadi", "Ali", ""]]}, {"id": "1612.00940", "submitter": "Alex Fedorov", "authors": "Alex Fedorov, Jeremy Johnson, Eswar Damaraju, Alexei Ozerin, Vince\n  Calhoun, Sergey Plis", "title": "End-to-end learning of brain tissue segmentation from imperfect labeling", "comments": "Published as a conference paper at IJCNN 2017 Preprint version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Segmenting a structural magnetic resonance imaging (MRI) scan is an important\npre-processing step for analytic procedures and subsequent inferences about\nlongitudinal tissue changes. Manual segmentation defines the current gold\nstandard in quality but is prohibitively expensive. Automatic approaches are\ncomputationally intensive, incredibly slow at scale, and error prone due to\nusually involving many potentially faulty intermediate steps. In order to\nstreamline the segmentation, we introduce a deep learning model that is based\non volumetric dilated convolutions, subsequently reducing both processing time\nand errors. Compared to its competitors, the model has a reduced set of\nparameters and thus is easier to train and much faster to execute. The contrast\nin performance between the dilated network and its competitors becomes obvious\nwhen both are tested on a large dataset of unprocessed human brain volumes. The\ndilated network consistently outperforms not only another state-of-the-art deep\nlearning approach, the up convolutional network, but also the ground truth on\nwhich it was trained. Not only can the incredible speed of our model make large\nscale analyses much easier but we also believe it has great potential in a\nclinical setting where, with little to no substantial delay, a patient and\nprovider can go over test results.\n", "versions": [{"version": "v1", "created": "Sat, 3 Dec 2016 08:43:33 GMT"}, {"version": "v2", "created": "Mon, 5 Jun 2017 20:14:37 GMT"}], "update_date": "2017-06-07", "authors_parsed": [["Fedorov", "Alex", ""], ["Johnson", "Jeremy", ""], ["Damaraju", "Eswar", ""], ["Ozerin", "Alexei", ""], ["Calhoun", "Vince", ""], ["Plis", "Sergey", ""]]}, {"id": "1612.00979", "submitter": "Stepan Tulyakov", "authors": "Stepan Tulyakov and Anton Ivanov and Francois Fleuret", "title": "Semi-supervised learning of deep metrics for stereo reconstruction", "comments": "11 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep-learning metrics have recently demonstrated extremely good performance\nto match image patches for stereo reconstruction. However, training such\nmetrics requires large amount of labeled stereo images, which can be difficult\nor costly to collect for certain applications. The main contribution of our\nwork is a new semi-supervised method for learning deep metrics from unlabeled\nstereo images, given coarse information about the scenes and the optical\nsystem. Our method alternatively optimizes the metric with a standard\nstochastic gradient descent, and applies stereo constraints to regularize its\nprediction. Experiments on reference data-sets show that, for a given network\narchitecture, training with this new method without ground-truth produces a\nmetric with performance as good as state-of-the-art baselines trained with the\nsaid ground-truth. This work has three practical implications. Firstly, it\nhelps to overcome limitations of training sets, in particular noisy ground\ntruth. Secondly it allows to use much more training data during learning.\nThirdly, it allows to tune deep metric for a particular stereo system, even if\nground truth is not available.\n", "versions": [{"version": "v1", "created": "Sat, 3 Dec 2016 16:09:32 GMT"}], "update_date": "2016-12-06", "authors_parsed": [["Tulyakov", "Stepan", ""], ["Ivanov", "Anton", ""], ["Fleuret", "Francois", ""]]}, {"id": "1612.00983", "submitter": "Yuzhen Lu", "authors": "Yuzhen Lu", "title": "Food Image Recognition by Using Convolutional Neural Networks (CNNs)", "comments": "6 pages, 5 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Food image recognition is one of the promising applications of visual object\nrecognition in computer vision. In this study, a small-scale dataset consisting\nof 5822 images of ten categories and a five-layer CNN was constructed to\nrecognize these images. The bag-of-features (BoF) model coupled with support\nvector machine (SVM) was first evaluated for image classification, resulting in\nan overall accuracy of 56%; while the CNN model performed much better with an\noverall accuracy of 74%. Data augmentation techniques based on geometric\ntransformation were applied to increase the size of training images, which\nachieved a significantly improved accuracy of more than 90% while preventing\nthe overfitting issue that occurred to the CNN based on raw training data.\nFurther improvements can be expected by collecting more images and optimizing\nthe network architecture and hyper-parameters.\n", "versions": [{"version": "v1", "created": "Sat, 3 Dec 2016 16:22:59 GMT"}, {"version": "v2", "created": "Mon, 25 Feb 2019 15:22:55 GMT"}], "update_date": "2019-02-26", "authors_parsed": [["Lu", "Yuzhen", ""]]}, {"id": "1612.00986", "submitter": "Orazio Gallo", "authors": "Suren Jayasuriya, Orazio Gallo, Jinwei Gu, Jan Kautz", "title": "Deep Learning with Energy-efficient Binary Gradient Cameras", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Power consumption is a critical factor for the deployment of embedded\ncomputer vision systems. We explore the use of computational cameras that\ndirectly output binary gradient images to reduce the portion of the power\nconsumption allocated to image sensing. We survey the accuracy of binary\ngradient cameras on a number of computer vision tasks using deep learning.\nThese include object recognition, head pose regression, face detection, and\ngesture recognition. We show that, for certain applications, accuracy can be on\npar or even better than what can be achieved on traditional images. We are also\nthe first to recover intensity information from binary spatial gradient\nimages--useful for applications with a human observer in the loop, such as\nsurveillance. Our results, which we validate with a prototype binary gradient\ncamera, point to the potential of gradient-based computer vision systems.\n", "versions": [{"version": "v1", "created": "Sat, 3 Dec 2016 17:04:52 GMT"}], "update_date": "2016-12-06", "authors_parsed": [["Jayasuriya", "Suren", ""], ["Gallo", "Orazio", ""], ["Gu", "Jinwei", ""], ["Kautz", "Jan", ""]]}, {"id": "1612.00991", "submitter": "Yaxing Wang", "authors": "Yaxing Wang, Lichao Zhang, Joost van de Weijer", "title": "Ensembles of Generative Adversarial Networks", "comments": "accepted NIPS 2016 Workshop on Adversarial Training", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Ensembles are a popular way to improve results of discriminative CNNs. The\ncombination of several networks trained starting from different initializations\nimproves results significantly. In this paper we investigate the usage of\nensembles of GANs. The specific nature of GANs opens up several new ways to\nconstruct ensembles. The first one is based on the fact that in the minimax\ngame which is played to optimize the GAN objective the generator network keeps\non changing even after the network can be considered optimal. As such ensembles\nof GANs can be constructed based on the same network initialization but just\ntaking models which have different amount of iterations. These so-called self\nensembles are much faster to train than traditional ensembles. The second\nmethod, called cascade GANs, redirects part of the training data which is badly\nmodeled by the first GAN to another GAN. In experiments on the CIFAR10 dataset\nwe show that ensembles of GANs obtain model probability distributions which\nbetter model the data distribution. In addition, we show that these improved\nresults can be obtained at little additional computational cost.\n", "versions": [{"version": "v1", "created": "Sat, 3 Dec 2016 17:49:02 GMT"}], "update_date": "2016-12-06", "authors_parsed": [["Wang", "Yaxing", ""], ["Zhang", "Lichao", ""], ["van de Weijer", "Joost", ""]]}, {"id": "1612.00992", "submitter": "Mark Howison", "authors": "David Berenbaum, Dwyer Deighan, Thomas Marlow, Ashley Lee, Scott\n  Frickel, Mark Howison", "title": "Mining Spatio-temporal Data on Industrialization from Historical\n  Registries", "comments": null, "journal-ref": "Journal of Environmental Informatics 34(1): 28-34 (2019)", "doi": "10.3808/jei.201700381", "report-no": null, "categories": "cs.CV cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the growing availability of big data in many fields, historical data\non socioevironmental phenomena are often not available due to a lack of\nautomated and scalable approaches for collecting, digitizing, and assembling\nthem. We have developed a data-mining method for extracting tabulated, geocoded\ndata from printed directories. While scanning and optical character recognition\n(OCR) can digitize printed text, these methods alone do not capture the\nstructure of the underlying data. Our pipeline integrates both page layout\nanalysis and OCR to extract tabular, geocoded data from structured text. We\ndemonstrate the utility of this method by applying it to scanned manufacturing\nregistries from Rhode Island that record 41 years of industrial land use. The\nresulting spatio-temporal data can be used for socioenvironmental analyses of\nindustrialization at a resolution that was not previously possible. In\nparticular, we find strong evidence for the dispersion of manufacturing from\nthe urban core of Providence, the state's capital, along the Interstate 95\ncorridor to the north and south.\n", "versions": [{"version": "v1", "created": "Sat, 3 Dec 2016 17:54:03 GMT"}], "update_date": "2021-04-21", "authors_parsed": [["Berenbaum", "David", ""], ["Deighan", "Dwyer", ""], ["Marlow", "Thomas", ""], ["Lee", "Ashley", ""], ["Frickel", "Scott", ""], ["Howison", "Mark", ""]]}, {"id": "1612.01006", "submitter": "Mojtaba Kazemi", "authors": "Mojtaba Kazemi, Ehsan Mohammadi.P, Parichehr shahidi sadeghi, Mohamad\n  B. Menhaj", "title": "A Non-Local Means Approach for Gaussian Noise Removal from Images using\n  a Modified Weighting Kernel", "comments": "2017 25th Iranian Conference on Electrical Engineering (ICEE)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gaussian noise removal is an interesting area in digital image processing not\nonly to improve the visual quality, but for its impact on other post-processing\nalgorithms like image registration or segmentation. Many presented\nstate-of-the-art denoising methods are based on the self-similarity or\npatch-based image processing. Specifically, Non-Local Means (NLM) as a\npatch-based filter has gained increasing attention in recent years.\nEssentially, this filter tends to obtain the noise-less signal value by\ncomputing the Gaussian-weighted Euclidean distance between the patch\nunder-processing and other patches inside the image. However, the NLM filter is\nsensitive to the outliers (pixels that their intensity values are far away from\nother pixels) inside the patch, meaning that the pixels with the symmetric\nlocations in the patch are assigned the same weight. This can lead to\nsub-optimal denoising performance when the destructive nature of noise\ngenerates some outliers inside patches. In this paper, we propose a new\nweighting approach to modify the Gaussian kernel of the NLM filter. Our\napproach employs the geometric distance between image intensities to come up\nwith new weights for each pixel of a patch, lowering the impact of outliers on\nthe denoising performance. Experiments on a set of standard images and\ndifferent noise levels show that our proposed method outperforms the other\ncompared denoising filters.\n", "versions": [{"version": "v1", "created": "Sat, 3 Dec 2016 19:00:18 GMT"}], "update_date": "2016-12-06", "authors_parsed": [["Kazemi", "Mojtaba", ""], ["P", "Ehsan Mohammadi.", ""], ["sadeghi", "Parichehr shahidi", ""], ["Menhaj", "Mohamad B.", ""]]}, {"id": "1612.01022", "submitter": "Yuankai Wu Yuankai Wu", "authors": "Yuankai Wu and Huachun Tan", "title": "Short-term traffic flow forecasting with spatial-temporal correlation in\n  a hybrid deep learning framework", "comments": "14 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning approaches have reached a celebrity status in artificial\nintelligence field, its success have mostly relied on Convolutional Networks\n(CNN) and Recurrent Networks. By exploiting fundamental spatial properties of\nimages and videos, the CNN always achieves dominant performance on visual\ntasks. And the Recurrent Networks (RNN) especially long short-term memory\nmethods (LSTM) can successfully characterize the temporal correlation, thus\nexhibits superior capability for time series tasks. Traffic flow data have\nplentiful characteristics on both time and space domain. However, applications\nof CNN and LSTM approaches on traffic flow are limited. In this paper, we\npropose a novel deep architecture combined CNN and LSTM to forecast future\ntraffic flow (CLTFP). An 1-dimension CNN is exploited to capture spatial\nfeatures of traffic flow, and two LSTMs are utilized to mine the short-term\nvariability and periodicities of traffic flow. Given those meaningful features,\nthe feature-level fusion is performed to achieve short-term forecasting. The\nproposed CLTFP is compared with other popular forecasting methods on an open\ndatasets. Experimental results indicate that the CLTFP has considerable\nadvantages in traffic flow forecasting. in additional, the proposed CLTFP is\nanalyzed from the view of Granger Causality, and several interesting properties\nof CLTFP are discovered and discussed .\n", "versions": [{"version": "v1", "created": "Sat, 3 Dec 2016 21:30:26 GMT"}], "update_date": "2016-12-06", "authors_parsed": [["Wu", "Yuankai", ""], ["Tan", "Huachun", ""]]}, {"id": "1612.01033", "submitter": "Marco Pedersoli M", "authors": "Marco Pedersoli, Thomas Lucas, Cordelia Schmid, Jakob Verbeek", "title": "Areas of Attention for Image Captioning", "comments": "Accepted in ICCV 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose \"Areas of Attention\", a novel attention-based model for automatic\nimage captioning. Our approach models the dependencies between image regions,\ncaption words, and the state of an RNN language model, using three pairwise\ninteractions. In contrast to previous attention-based approaches that associate\nimage regions only to the RNN state, our method allows a direct association\nbetween caption words and image regions. During training these associations are\ninferred from image-level captions, akin to weakly-supervised object detector\ntraining. These associations help to improve captioning by localizing the\ncorresponding regions during testing. We also propose and compare different\nways of generating attention areas: CNN activation grids, object proposals, and\nspatial transformers nets applied in a convolutional fashion. Spatial\ntransformers give the best results. They allow for image specific attention\nareas, and can be trained jointly with the rest of the network. Our attention\nmechanism and spatial transformer attention areas together yield\nstate-of-the-art results on the MSCOCO dataset.o meaningful latent semantic\nstructure in the generated captions.\n", "versions": [{"version": "v1", "created": "Sat, 3 Dec 2016 23:01:36 GMT"}, {"version": "v2", "created": "Fri, 25 Aug 2017 14:36:01 GMT"}], "update_date": "2017-08-28", "authors_parsed": [["Pedersoli", "Marco", ""], ["Lucas", "Thomas", ""], ["Schmid", "Cordelia", ""], ["Verbeek", "Jakob", ""]]}, {"id": "1612.01035", "submitter": "Lex Fridman", "authors": "Lex Fridman, Bryan Reimer", "title": "Semi-Automated Annotation of Discrete States in Large Video Datasets", "comments": "To be presented at AAAI 2017. arXiv admin note: text overlap with\n  arXiv:1508.04028", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a framework for semi-automated annotation of video frames where\nthe video is of an object that at any point in time can be labeled as being in\none of a finite number of discrete states. A Hidden Markov Model (HMM) is used\nto model (1) the behavior of the underlying object and (2) the noisy\nobservation of its state through an image processing algorithm. The key insight\nof this approach is that the annotation of frame-by-frame video can be reduced\nfrom a problem of labeling every single image to a problem of detecting a\ntransition between states of the underlying objected being recording on video.\nThe performance of the framework is evaluated on a driver gaze classification\ndataset composed of 16,000,000 images that were fully annotated over 6,000\nhours of direct manual annotation labor. On this dataset, we achieve a 13x\nreduction in manual annotation for an average accuracy of 99.1% and a 84x\nreduction for an average accuracy of 91.2%.\n", "versions": [{"version": "v1", "created": "Sat, 3 Dec 2016 23:40:14 GMT"}], "update_date": "2016-12-06", "authors_parsed": [["Fridman", "Lex", ""], ["Reimer", "Bryan", ""]]}, {"id": "1612.01051", "submitter": "Bichen Wu", "authors": "Bichen Wu, Alvin Wan, Forrest Iandola, Peter H. Jin, Kurt Keutzer", "title": "SqueezeDet: Unified, Small, Low Power Fully Convolutional Neural\n  Networks for Real-Time Object Detection for Autonomous Driving", "comments": "The supplementary material of this paper, which discusses the energy\n  efficiency of SqueezeDet, is attached after the main paper. The source code\n  of this work is open-source released at\n  https://github.com/BichenWuUCB/squeezeDet", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Object detection is a crucial task for autonomous driving. In addition to\nrequiring high accuracy to ensure safety, object detection for autonomous\ndriving also requires real-time inference speed to guarantee prompt vehicle\ncontrol, as well as small model size and energy efficiency to enable embedded\nsystem deployment.\n  In this work, we propose SqueezeDet, a fully convolutional neural network for\nobject detection that aims to simultaneously satisfy all of the above\nconstraints. In our network, we use convolutional layers not only to extract\nfeature maps but also as the output layer to compute bounding boxes and class\nprobabilities. The detection pipeline of our model only contains a single\nforward pass of a neural network, thus it is extremely fast. Our model is\nfully-convolutional, which leads to a small model size and better energy\nefficiency. While achieving the same accuracy as previous baselines, our model\nis 30.4x smaller, 19.7x faster, and consumes 35.2x lower energy. The code is\nopen-sourced at \\url{https://github.com/BichenWuUCB/squeezeDet}.\n", "versions": [{"version": "v1", "created": "Sun, 4 Dec 2016 02:12:22 GMT"}, {"version": "v2", "created": "Thu, 22 Dec 2016 00:04:37 GMT"}, {"version": "v3", "created": "Wed, 29 Nov 2017 07:02:33 GMT"}, {"version": "v4", "created": "Tue, 11 Jun 2019 05:02:58 GMT"}], "update_date": "2019-06-12", "authors_parsed": [["Wu", "Bichen", ""], ["Wan", "Alvin", ""], ["Iandola", "Forrest", ""], ["Jin", "Peter H.", ""], ["Keutzer", "Kurt", ""]]}, {"id": "1612.01057", "submitter": "Tianshui Chen", "authors": "Tianshui Chen, Liang Lin, Xian Wu, Nong Xiao, Xiaonan Luo", "title": "Learning to Segment Object Candidates via Recursive Neural Networks", "comments": "Accepted at TIP", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To avoid the exhaustive search over locations and scales, current\nstate-of-the-art object detection systems usually involve a crucial component\ngenerating a batch of candidate object proposals from images. In this paper, we\npresent a simple yet effective approach for segmenting object proposals via a\ndeep architecture of recursive neural networks (ReNNs), which hierarchically\ngroups regions for detecting object candidates over scales. Unlike traditional\nmethods that mainly adopt fixed similarity measures for merging regions or\nfinding object proposals, our approach adaptively learns the region merging\nsimilarity and the objectness measure during the process of hierarchical region\ngrouping. Specifically, guided by a structured loss, the ReNN model jointly\noptimizes the cross-region similarity metric with the region merging process as\nwell as the objectness prediction. During inference of the object proposal\ngeneration, we introduce randomness into the greedy search to cope with the\nambiguity of grouping regions. Extensive experiments on standard benchmarks,\ne.g., PASCAL VOC and ImageNet, suggest that our approach is capable of\nproducing object proposals with high recall while well preserving the object\nboundaries and outperforms other existing methods in both accuracy and\nefficiency.\n", "versions": [{"version": "v1", "created": "Sun, 4 Dec 2016 03:35:35 GMT"}, {"version": "v2", "created": "Tue, 6 Dec 2016 07:02:41 GMT"}, {"version": "v3", "created": "Mon, 2 Jul 2018 09:02:27 GMT"}, {"version": "v4", "created": "Sun, 29 Jul 2018 02:17:35 GMT"}], "update_date": "2018-07-31", "authors_parsed": [["Chen", "Tianshui", ""], ["Lin", "Liang", ""], ["Wu", "Xian", ""], ["Xiao", "Nong", ""], ["Luo", "Xiaonan", ""]]}, {"id": "1612.01072", "submitter": "Gang Chen", "authors": "Gang Chen, Yawei Li and Sargur N. Srihari", "title": "Word Recognition with Deep Conditional Random Fields", "comments": "5 pages, published in ICIP 2016. arXiv admin note: substantial text\n  overlap with arXiv:1412.3397", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Recognition of handwritten words continues to be an important problem in\ndocument analysis and recognition. Existing approaches extract hand-engineered\nfeatures from word images--which can perform poorly with new data sets.\nRecently, deep learning has attracted great attention because of the ability to\nlearn features from raw data. Moreover they have yielded state-of-the-art\nresults in classification tasks including character recognition and scene\nrecognition. On the other hand, word recognition is a sequential problem where\nwe need to model the correlation between characters. In this paper, we propose\nusing deep Conditional Random Fields (deep CRFs) for word recognition.\nBasically, we combine CRFs with deep learning, in which deep features are\nlearned and sequences are labeled in a unified framework. We pre-train the deep\nstructure with stacked restricted Boltzmann machines (RBMs) for feature\nlearning and optimize the entire network with an online learning algorithm. The\nproposed model was evaluated on two datasets, and seen to perform significantly\nbetter than competitive baseline models. The source code is available at\nhttps://github.com/ganggit/deepCRFs.\n", "versions": [{"version": "v1", "created": "Sun, 4 Dec 2016 05:39:42 GMT"}], "update_date": "2016-12-06", "authors_parsed": [["Chen", "Gang", ""], ["Li", "Yawei", ""], ["Srihari", "Sargur N.", ""]]}, {"id": "1612.01074", "submitter": "Yunzhu Li", "authors": "Yunzhu Li, Andre Esteva, Brett Kuprel, Rob Novoa, Justin Ko, Sebastian\n  Thrun", "title": "Skin Cancer Detection and Tracking using Data Synthesis and Deep\n  Learning", "comments": "4 pages, 5 figures, Yunzhu Li and Andre Esteva contributed equally to\n  this work", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dense object detection and temporal tracking are needed across applications\ndomains ranging from people-tracking to analysis of satellite imagery over\ntime. The detection and tracking of malignant skin cancers and benign moles\nposes a particularly challenging problem due to the general uniformity of large\nskin patches, the fact that skin lesions vary little in their appearance, and\nthe relatively small amount of data available. Here we introduce a novel data\nsynthesis technique that merges images of individual skin lesions with\nfull-body images and heavily augments them to generate significant amounts of\ndata. We build a convolutional neural network (CNN) based system, trained on\nthis synthetic data, and demonstrate superior performance to traditional\ndetection and tracking techniques. Additionally, we compare our system to\nhumans trained with simple criteria. Our system is intended for potential\nclinical use to augment the capabilities of healthcare providers. While\ndomain-specific, we believe the methods invoked in this work will be useful in\napplying CNNs across domains that suffer from limited data availability.\n", "versions": [{"version": "v1", "created": "Sun, 4 Dec 2016 05:47:47 GMT"}], "update_date": "2016-12-06", "authors_parsed": [["Li", "Yunzhu", ""], ["Esteva", "Andre", ""], ["Kuprel", "Brett", ""], ["Novoa", "Rob", ""], ["Ko", "Justin", ""], ["Thrun", "Sebastian", ""]]}, {"id": "1612.01075", "submitter": "Gang Chen", "authors": "Gang Chen, Yawei Li and Sargur N. Srihari", "title": "Joint Visual Denoising and Classification using Deep Learning", "comments": "5 pages, 7 figures, ICIP 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Visual restoration and recognition are traditionally addressed in pipeline\nfashion, i.e. denoising followed by classification. Instead, observing\ncorrelations between the two tasks, for example clearer image will lead to\nbetter categorization and vice visa, we propose a joint framework for visual\nrestoration and recognition for handwritten images, inspired by advances in\ndeep autoencoder and multi-modality learning. Our model is a 3-pathway deep\narchitecture with a hidden-layer representation which is shared by multi-inputs\nand outputs, and each branch can be composed of a multi-layer deep model. Thus,\nvisual restoration and classification can be unified using shared\nrepresentation via non-linear mapping, and model parameters can be learnt via\nbackpropagation. Using MNIST and USPS data corrupted with structured noise, the\nproposed framework performs at least 20\\% better in classification than\nseparate pipelines, as well as clearer recovered images. The noise model and\nthe reproducible source code is available at\n{\\url{https://github.com/ganggit/jointmodel}}.\n", "versions": [{"version": "v1", "created": "Sun, 4 Dec 2016 05:51:55 GMT"}], "update_date": "2016-12-06", "authors_parsed": [["Chen", "Gang", ""], ["Li", "Yawei", ""], ["Srihari", "Sargur N.", ""]]}, {"id": "1612.01079", "submitter": "Yang Gao", "authors": "Huazhe Xu, Yang Gao, Fisher Yu, Trevor Darrell", "title": "End-to-end Learning of Driving Models from Large-scale Video Datasets", "comments": "camera ready for CVPR2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Robust perception-action models should be learned from training data with\ndiverse visual appearances and realistic behaviors, yet current approaches to\ndeep visuomotor policy learning have been generally limited to in-situ models\nlearned from a single vehicle or a simulation environment. We advocate learning\na generic vehicle motion model from large scale crowd-sourced video data, and\ndevelop an end-to-end trainable architecture for learning to predict a\ndistribution over future vehicle egomotion from instantaneous monocular camera\nobservations and previous vehicle state. Our model incorporates a novel\nFCN-LSTM architecture, which can be learned from large-scale crowd-sourced\nvehicle action data, and leverages available scene segmentation side tasks to\nimprove performance under a privileged learning paradigm.\n", "versions": [{"version": "v1", "created": "Sun, 4 Dec 2016 07:02:52 GMT"}, {"version": "v2", "created": "Sun, 23 Jul 2017 10:10:56 GMT"}], "update_date": "2017-07-25", "authors_parsed": [["Xu", "Huazhe", ""], ["Gao", "Yang", ""], ["Yu", "Fisher", ""], ["Darrell", "Trevor", ""]]}, {"id": "1612.01082", "submitter": "Junjie Zhang", "authors": "Junjie Zhang, Qi Wu, Chunhua Shen, Jian Zhang, Jianfeng Lu", "title": "Multi-Label Image Classification with Regional Latent Semantic\n  Dependencies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep convolution neural networks (CNN) have demonstrated advanced performance\non single-label image classification, and various progress also have been made\nto apply CNN methods on multi-label image classification, which requires to\nannotate objects, attributes, scene categories etc. in a single shot. Recent\nstate-of-the-art approaches to multi-label image classification exploit the\nlabel dependencies in an image, at global level, largely improving the labeling\ncapacity. However, predicting small objects and visual concepts is still\nchallenging due to the limited discrimination of the global visual features. In\nthis paper, we propose a Regional Latent Semantic Dependencies model (RLSD) to\naddress this problem. The utilized model includes a fully convolutional\nlocalization architecture to localize the regions that may contain multiple\nhighly-dependent labels. The localized regions are further sent to the\nrecurrent neural networks (RNN) to characterize the latent semantic\ndependencies at the regional level. Experimental results on several benchmark\ndatasets show that our proposed model achieves the best performance compared to\nthe state-of-the-art models, especially for predicting small objects occurred\nin the images. In addition, we set up an upper bound model (RLSD+ft-RPN) using\nbounding box coordinates during training, the experimental results also show\nthat our RLSD can approach the upper bound without using the bounding-box\nannotations, which is more realistic in the real world.\n", "versions": [{"version": "v1", "created": "Sun, 4 Dec 2016 07:25:25 GMT"}, {"version": "v2", "created": "Wed, 4 Jan 2017 04:44:29 GMT"}, {"version": "v3", "created": "Sun, 12 Mar 2017 23:41:23 GMT"}], "update_date": "2017-03-14", "authors_parsed": [["Zhang", "Junjie", ""], ["Wu", "Qi", ""], ["Shen", "Chunhua", ""], ["Zhang", "Jian", ""], ["Lu", "Jianfeng", ""]]}, {"id": "1612.01105", "submitter": "Hengshuang Zhao", "authors": "Hengshuang Zhao, Jianping Shi, Xiaojuan Qi, Xiaogang Wang, Jiaya Jia", "title": "Pyramid Scene Parsing Network", "comments": "CVPR 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scene parsing is challenging for unrestricted open vocabulary and diverse\nscenes. In this paper, we exploit the capability of global context information\nby different-region-based context aggregation through our pyramid pooling\nmodule together with the proposed pyramid scene parsing network (PSPNet). Our\nglobal prior representation is effective to produce good quality results on the\nscene parsing task, while PSPNet provides a superior framework for pixel-level\nprediction tasks. The proposed approach achieves state-of-the-art performance\non various datasets. It came first in ImageNet scene parsing challenge 2016,\nPASCAL VOC 2012 benchmark and Cityscapes benchmark. A single PSPNet yields new\nrecord of mIoU accuracy 85.4% on PASCAL VOC 2012 and accuracy 80.2% on\nCityscapes.\n", "versions": [{"version": "v1", "created": "Sun, 4 Dec 2016 11:46:22 GMT"}, {"version": "v2", "created": "Thu, 27 Apr 2017 12:15:17 GMT"}], "update_date": "2017-04-28", "authors_parsed": [["Zhao", "Hengshuang", ""], ["Shi", "Jianping", ""], ["Qi", "Xiaojuan", ""], ["Wang", "Xiaogang", ""], ["Jia", "Jiaya", ""]]}, {"id": "1612.01131", "submitter": "Amelia Carolina Sparavigna", "authors": "Amelia Carolina Sparavigna", "title": "A method for the segmentation of images based on thresholding and\n  applied to vesicular textures", "comments": "Keywords: Segmentation, Edge Detection, Image Analysis, 2D Textures,\n  Texture Functions", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In image processing, a segmentation is a process of partitioning an image\ninto multiple sets of pixels, that are defined as super-pixels. Each\nsuper-pixel is characterized by a label or parameter. Here, we are proposing a\nmethod for determining the super-pixels based on the thresholding of the image.\nThis approach is quite useful for studying the images showing vesicular\ntextures.\n", "versions": [{"version": "v1", "created": "Sun, 4 Dec 2016 15:47:27 GMT"}], "update_date": "2016-12-06", "authors_parsed": [["Sparavigna", "Amelia Carolina", ""]]}, {"id": "1612.01160", "submitter": "Matthew Trager", "authors": "Matthew Trager, Bernd Sturmfels, John Canny, Martial Hebert, Jean\n  Ponce", "title": "General models for rational cameras and the case of two-slit projections", "comments": "9 pages + supplementary material", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The rational camera model recently introduced in [19] provides a general\nmethodology for studying abstract nonlinear imaging systems and their\nmulti-view geometry. This paper builds on this framework to study \"physical\nrealizations\" of rational cameras. More precisely, we give an explicit account\nof the mapping between between physical visual rays and image points (missing\nin the original description), which allows us to give simple analytical\nexpressions for direct and inverse projections. We also consider \"primitive\"\ncamera models, that are orbits under the action of various projective\ntransformations, and lead to a general notion of intrinsic parameters. The\nmethodology is general, but it is illustrated concretely by an in-depth study\nof two-slit cameras, that we model using pairs of linear projections. This\nsimple analytical form allows us to describe models for the corresponding\nprimitive cameras, to introduce intrinsic parameters with a clear geometric\nmeaning, and to define an epipolar tensor characterizing two-view\ncorrespondences. In turn, this leads to new algorithms for structure from\nmotion and self-calibration.\n", "versions": [{"version": "v1", "created": "Sun, 4 Dec 2016 18:22:36 GMT"}, {"version": "v2", "created": "Thu, 8 Dec 2016 14:32:41 GMT"}, {"version": "v3", "created": "Fri, 9 Dec 2016 09:21:33 GMT"}, {"version": "v4", "created": "Tue, 11 Apr 2017 11:55:04 GMT"}], "update_date": "2017-04-12", "authors_parsed": [["Trager", "Matthew", ""], ["Sturmfels", "Bernd", ""], ["Canny", "John", ""], ["Hebert", "Martial", ""], ["Ponce", "Jean", ""]]}, {"id": "1612.01175", "submitter": "Benjamin Eysenbach", "authors": "Benjamin Eysenbach, Carl Vondrick, Antonio Torralba", "title": "Who is Mistaken?", "comments": "See project website at: http://people.csail.mit.edu/bce/mistaken/ .\n  (Edit: fixed typos and references)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recognizing when people have false beliefs is crucial for understanding their\nactions. We introduce the novel problem of identifying when people in abstract\nscenes have incorrect beliefs. We present a dataset of scenes, each visually\ndepicting an 8-frame story in which a character has a mistaken belief. We then\ncreate a representation of characters' beliefs for two tasks in human action\nunderstanding: predicting who is mistaken, and when they are mistaken.\nExperiments suggest that our method for identifying mistaken characters\nperforms better on these tasks than simple baselines. Diagnostics on our model\nsuggest it learns important cues for recognizing mistaken beliefs, such as\ngaze. We believe models of people's beliefs will have many applications in\naction understanding, robotics, and healthcare.\n", "versions": [{"version": "v1", "created": "Sun, 4 Dec 2016 20:45:42 GMT"}, {"version": "v2", "created": "Fri, 31 Mar 2017 16:36:53 GMT"}], "update_date": "2017-04-03", "authors_parsed": [["Eysenbach", "Benjamin", ""], ["Vondrick", "Carl", ""], ["Torralba", "Antonio", ""]]}, {"id": "1612.01194", "submitter": "Haroon Idrees", "authors": "Khurram Soomro, Haroon Idrees, and Mubarak Shah", "title": "Online Localization and Prediction of Actions and Interactions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a person-centric and online approach to the challenging\nproblem of localization and prediction of actions and interactions in videos.\nTypically, localization or recognition is performed in an offline manner where\nall the frames in the video are processed together. This prevents timely\nlocalization and prediction of actions and interactions - an important\nconsideration for many tasks including surveillance and human-machine\ninteraction.\n  In our approach, we estimate human poses at each frame and train\ndiscriminative appearance models using the superpixels inside the pose bounding\nboxes. Since the pose estimation per frame is inherently noisy, the conditional\nprobability of pose hypotheses at current time-step (frame) is computed using\npose estimations in the current frame and their consistency with poses in the\nprevious frames. Next, both the superpixel and pose-based foreground\nlikelihoods are used to infer the location of actors at each time through a\nConditional Random. The issue of visual drift is handled by updating the\nappearance models, and refining poses using motion smoothness on joint\nlocations, in an online manner. For online prediction of action (interaction)\nconfidences, we propose an approach based on Structural SVM that operates on\nshort video segments, and is trained with the objective that confidence of an\naction or interaction increases as time progresses. Lastly, we quantify the\nperformance of both detection and prediction together, and analyze how the\nprediction accuracy varies as a time function of observed action (interaction)\nat different levels of detection performance. Our experiments on several\ndatasets suggest that despite using only a few frames to localize actions\n(interactions) at each time instant, we are able to obtain competitive results\nto state-of-the-art offline methods.\n", "versions": [{"version": "v1", "created": "Sun, 4 Dec 2016 22:16:55 GMT"}], "update_date": "2016-12-06", "authors_parsed": [["Soomro", "Khurram", ""], ["Idrees", "Haroon", ""], ["Shah", "Mubarak", ""]]}, {"id": "1612.01202", "submitter": "R{\\i}za Alp Guler", "authors": "R{\\i}za Alp G\\\"uler, George Trigeorgis, Epameinondas Antonakos,\n  Patrick Snape, Stefanos Zafeiriou, Iasonas Kokkinos", "title": "DenseReg: Fully Convolutional Dense Shape Regression In-the-Wild", "comments": "CVPR 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose to learn a mapping from image pixels into a dense\ntemplate grid through a fully convolutional network. We formulate this task as\na regression problem and train our network by leveraging upon manually\nannotated facial landmarks \"in-the-wild\". We use such landmarks to establish a\ndense correspondence field between a three-dimensional object template and the\ninput image, which then serves as the ground-truth for training our regression\nsystem. We show that we can combine ideas from semantic segmentation with\nregression networks, yielding a highly-accurate \"quantized regression\"\narchitecture.\n  Our system, called DenseReg, allows us to estimate dense image-to-template\ncorrespondences in a fully convolutional manner. As such our network can\nprovide useful correspondence information as a stand-alone system, while when\nused as an initialization for Statistical Deformable Models we obtain landmark\nlocalization results that largely outperform the current state-of-the-art on\nthe challenging 300W benchmark. We thoroughly evaluate our method on a host of\nfacial analysis tasks and also provide qualitative results for dense human body\ncorrespondence. We make our code available at http://alpguler.com/DenseReg.html\nalong with supplementary materials.\n", "versions": [{"version": "v1", "created": "Sun, 4 Dec 2016 23:08:06 GMT"}, {"version": "v2", "created": "Mon, 19 Jun 2017 18:51:03 GMT"}], "update_date": "2017-06-21", "authors_parsed": [["G\u00fcler", "R\u0131za Alp", ""], ["Trigeorgis", "George", ""], ["Antonakos", "Epameinondas", ""], ["Snape", "Patrick", ""], ["Zafeiriou", "Stefanos", ""], ["Kokkinos", "Iasonas", ""]]}, {"id": "1612.01213", "submitter": "Hyun Oh Song", "authors": "Hyun Oh Song, Stefanie Jegelka, Vivek Rathod, Kevin Murphy", "title": "Deep Metric Learning via Facility Location", "comments": "Submission accepted at CVPR 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning the representation and the similarity metric in an end-to-end\nfashion with deep networks have demonstrated outstanding results for clustering\nand retrieval. However, these recent approaches still suffer from the\nperformance degradation stemming from the local metric training procedure which\nis unaware of the global structure of the embedding space.\n  We propose a global metric learning scheme for optimizing the deep metric\nembedding with the learnable clustering function and the clustering metric\n(NMI) in a novel structured prediction framework.\n  Our experiments on CUB200-2011, Cars196, and Stanford online products\ndatasets show state of the art performance both on the clustering and retrieval\ntasks measured in the NMI and Recall@K evaluation metrics.\n", "versions": [{"version": "v1", "created": "Mon, 5 Dec 2016 01:09:35 GMT"}, {"version": "v2", "created": "Tue, 11 Apr 2017 17:24:42 GMT"}], "update_date": "2017-04-12", "authors_parsed": [["Song", "Hyun Oh", ""], ["Jegelka", "Stefanie", ""], ["Rathod", "Vivek", ""], ["Murphy", "Kevin", ""]]}, {"id": "1612.01225", "submitter": "Chen Liu", "authors": "Chen Liu, Jiajun Wu, Pushmeet Kohli, Yasutaka Furukawa", "title": "Deep Multi-Modal Image Correspondence Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inference of correspondences between images from different modalities is an\nextremely important perceptual ability that enables humans to understand and\nrecognize cross-modal concepts. In this paper, we consider an instance of this\nproblem that involves matching photographs of building interiors with their\ncorresponding floorplan. This is a particularly challenging problem because a\nfloorplan, as a stylized architectural drawing, is very different in appearance\nfrom a color photograph. Furthermore, individual photographs by themselves\ndepict only a part of a floorplan (e.g., kitchen, bathroom, and living room).\nWe propose the use of a number of different neural network architectures for\nthis task, which are trained and evaluated on a novel large-scale dataset of 5\nmillion floorplan images and 80 million associated photographs. Experimental\nevaluation reveals that our neural network architectures are able to identify\nvisual cues that result in reliable matches across these two quite different\nmodalities. In fact, the trained networks are able to even outperform human\nsubjects in several challenging image matching problems. Our result implies\nthat neural networks are effective at perceptual tasks that require long\nperiods of reasoning even for humans to solve.\n", "versions": [{"version": "v1", "created": "Mon, 5 Dec 2016 02:16:09 GMT"}], "update_date": "2016-12-06", "authors_parsed": [["Liu", "Chen", ""], ["Wu", "Jiajun", ""], ["Kohli", "Pushmeet", ""], ["Furukawa", "Yasutaka", ""]]}, {"id": "1612.01227", "submitter": "Kede Ma", "authors": "Kede Ma, Huan Fu, Tongliang Liu, Zhou Wang, Dacheng Tao", "title": "Deep Blur Mapping: Exploiting High-Level Semantics by Deep Neural\n  Networks", "comments": "Tech report", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The human visual system excels at detecting local blur of visual images, but\nthe underlying mechanism is not well understood. Traditional views of blur such\nas reduction in energy at high frequencies and loss of phase coherence at\nlocalized features have fundamental limitations. For example, they cannot well\ndiscriminate flat regions from blurred ones. Here we propose that high-level\nsemantic information is critical in successfully identifying local blur.\nTherefore, we resort to deep neural networks that are proficient at learning\nhigh-level features and propose the first end-to-end local blur mapping\nalgorithm based on a fully convolutional network. By analyzing various\narchitectures with different depths and design philosophies, we empirically\nshow that high-level features of deeper layers play a more important role than\nlow-level features of shallower layers in resolving challenging ambiguities for\nthis task. We test the proposed method on a standard blur detection benchmark\nand demonstrate that it significantly advances the state-of-the-art (ODS\nF-score of 0.853). Furthermore, we explore the use of the generated blur maps\nin three applications, including blur region segmentation, blur degree\nestimation, and blur magnification.\n", "versions": [{"version": "v1", "created": "Mon, 5 Dec 2016 02:21:39 GMT"}, {"version": "v2", "created": "Tue, 12 Jun 2018 02:43:21 GMT"}], "update_date": "2018-06-13", "authors_parsed": [["Ma", "Kede", ""], ["Fu", "Huan", ""], ["Liu", "Tongliang", ""], ["Wang", "Zhou", ""], ["Tao", "Dacheng", ""]]}, {"id": "1612.01230", "submitter": "Yoshihiro Yamada", "authors": "Yoshihiro Yamada, Masakazu Iwamura, Koichi Kise", "title": "Deep Pyramidal Residual Networks with Separated Stochastic Depth", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  On general object recognition, Deep Convolutional Neural Networks (DCNNs)\nachieve high accuracy. In particular, ResNet and its improvements have broken\nthe lowest error rate records. In this paper, we propose a method to\nsuccessfully combine two ResNet improvements, ResDrop and PyramidNet. We\nconfirmed that the proposed network outperformed the conventional methods; on\nCIFAR-100, the proposed network achieved an error rate of 16.18% in contrast to\nPiramidNet achieving that of 18.29% and ResNeXt 17.31%.\n", "versions": [{"version": "v1", "created": "Mon, 5 Dec 2016 02:41:18 GMT"}], "update_date": "2016-12-06", "authors_parsed": [["Yamada", "Yoshihiro", ""], ["Iwamura", "Masakazu", ""], ["Kise", "Koichi", ""]]}, {"id": "1612.01234", "submitter": "Chen Liu", "authors": "Chen Liu, Hang Yan, Pushmeet Kohli, Yasutaka Furukawa", "title": "Multi-way Particle Swarm Fusion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a novel MAP inference framework for Markov Random Field\n(MRF) in parallel computing environments. The inference framework, dubbed Swarm\nFusion, is a natural generalization of the Fusion Move method. Every thread (in\na case of multi-threading environments) maintains and updates a solution. At\neach iteration, a thread can generate arbitrary number of solution proposals\nand take arbitrary number of concurrent solutions from the other threads to\nperform multi-way fusion in updating its solution. The framework is general,\nmaking popular existing inference techniques such as alpha-expansion, fusion\nmove, parallel alpha-expansion, and hierarchical fusion, its special cases. We\nhave evaluated the effectiveness of our approach against competing methods on\nthree problems of varying difficulties, in particular, the stereo, the optical\nflow, and the layered depthmap estimation problems.\n", "versions": [{"version": "v1", "created": "Mon, 5 Dec 2016 03:05:21 GMT"}], "update_date": "2016-12-06", "authors_parsed": [["Liu", "Chen", ""], ["Yan", "Hang", ""], ["Kohli", "Pushmeet", ""], ["Furukawa", "Yasutaka", ""]]}, {"id": "1612.01235", "submitter": "Hang Yan", "authors": "Hang Yan, Yebin Liu, Yasutaka Furukawa", "title": "Turning an Urban Scene Video into a Cinemagraph", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes an algorithm that turns a regular video capturing urban\nscenes into a high-quality endless animation, known as a Cinemagraph. The\ncreation of a Cinemagraph usually requires a static camera in a carefully\nconfigured scene. The task becomes challenging for a regular video with a\nmoving camera and objects. Our approach first warps an input video into the\nviewpoint of a reference camera. Based on the warped video, we propose\neffective temporal analysis algorithms to detect regions with static geometry\nand dynamic appearance, where geometric modeling is reliable and visually\nattractive animations can be created. Lastly, the algorithm applies a sequence\nof video processing techniques to produce a Cinemagraph movie. We have tested\nthe proposed approach on numerous challenging real scenes. To our knowledge,\nthis work is the first to automatically generate Cinemagraph animations from\nregular movies in the wild.\n", "versions": [{"version": "v1", "created": "Mon, 5 Dec 2016 03:09:27 GMT"}], "update_date": "2016-12-06", "authors_parsed": [["Yan", "Hang", ""], ["Liu", "Yebin", ""], ["Furukawa", "Yasutaka", ""]]}, {"id": "1612.01237", "submitter": "Pegah Faridi", "authors": "Pegah Faridi, Habibollah Danyali, Mohammad Sadegh Helfroush, and\n  Mojgan Akbarzadeh Jahromi", "title": "Cancerous Nuclei Detection and Scoring in Breast Cancer\n  Histopathological Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Early detection and prognosis of breast cancer are feasible by utilizing\nhistopathological grading of biopsy specimens. This research is focused on\ndetection and grading of nuclear pleomorphism in histopathological images of\nbreast cancer. The proposed method consists of three internal steps. First,\nunmixing colors of H&E is used in the preprocessing step. Second, nuclei\nboundaries are extracted incorporating the center of cancerous nuclei which are\ndetected by applying morphological operations and Difference of Gaussian filter\non the preprocessed image. Finally, segmented nuclei are scored to accomplish\none parameter of the Nottingham grading system for breast cancer. In this\napproach, the nuclei area, chromatin density, contour regularity, and nucleoli\npresence, are features for nuclear pleomorphism scoring. Experimental results\nshowed that the proposed algorithm, with an accuracy of 86.6%, made significant\nadvancement in detecting cancerous nuclei compared to existing methods in the\nrelated literature.\n", "versions": [{"version": "v1", "created": "Mon, 5 Dec 2016 03:27:21 GMT"}], "update_date": "2016-12-06", "authors_parsed": [["Faridi", "Pegah", ""], ["Danyali", "Habibollah", ""], ["Helfroush", "Mohammad Sadegh", ""], ["Jahromi", "Mojgan Akbarzadeh", ""]]}, {"id": "1612.01253", "submitter": "Yen-Chang Hsu", "authors": "Yen-Chang Hsu, Zhaoyang Lv, Zsolt Kira", "title": "Deep Image Category Discovery using a Transferred Similarity Function", "comments": "13 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatically discovering image categories in unlabeled natural images is one\nof the important goals of unsupervised learning. However, the task is\nchallenging and even human beings define visual categories based on a large\namount of prior knowledge. In this paper, we similarly utilize prior knowledge\nto facilitate the discovery of image categories. We present a novel end-to-end\nnetwork to map unlabeled images to categories as a clustering network. We\npropose that this network can be learned with contrastive loss which is only\nbased on weak binary pair-wise constraints. Such binary constraints can be\nlearned from datasets in other domains as transferred similarity functions,\nwhich mimic a simple knowledge transfer. We first evaluate our experiments on\nthe MNIST dataset as a proof of concept, based on predicted similarities\ntrained on Omniglot, showing a 99\\% accuracy which significantly outperforms\nclustering based approaches. Then we evaluate the discovery performance on\nCifar-10, STL-10, and ImageNet, which achieves both state-of-the-art accuracy\nand shows it can be scalable to various large natural images.\n", "versions": [{"version": "v1", "created": "Mon, 5 Dec 2016 05:41:26 GMT"}], "update_date": "2016-12-06", "authors_parsed": [["Hsu", "Yen-Chang", ""], ["Lv", "Zhaoyang", ""], ["Kira", "Zsolt", ""]]}, {"id": "1612.01256", "submitter": "Yasutaka Furukawa", "authors": "Satoshi Ikehata and Ivaylo Boyadzhiev and Qi Shan and Yasutaka\n  Furukawa", "title": "Panoramic Structure from Motion via Geometric Relationship Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the problem of Structure from Motion (SfM) for indoor\npanoramic image streams, extremely challenging even for the state-of-the-art\ndue to the lack of textures and minimal parallax. The key idea is the fusion of\nsingle-view and multi-view reconstruction techniques via geometric relationship\ndetection (e.g., detecting 2D lines as coplanar in 3D). Rough geometry suffices\nto perform such detection, and our approach utilizes rough surface normal\nestimates from an image-to-normal deep network to discover geometric\nrelationships among lines. The detected relationships provide exact geometric\nconstraints in our line-based linear SfM formulation. A constrained linear\nleast squares is used to reconstruct a 3D model and camera motions, followed by\nthe bundle adjustment. We have validated our algorithm on challenging datasets,\noutperforming various state-of-the-art reconstruction techniques.\n", "versions": [{"version": "v1", "created": "Mon, 5 Dec 2016 06:24:10 GMT"}], "update_date": "2016-12-06", "authors_parsed": [["Ikehata", "Satoshi", ""], ["Boyadzhiev", "Ivaylo", ""], ["Shan", "Qi", ""], ["Furukawa", "Yasutaka", ""]]}, {"id": "1612.01288", "submitter": "Wim Abbeloos", "authors": "Wim Abbeloos, Toon Goedem\\'e", "title": "Point Pair Feature based Object Detection for Random Bin Picking", "comments": null, "journal-ref": null, "doi": "10.1109/CRV.2016.59", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Point pair features are a popular representation for free form 3D object\ndetection and pose estimation. In this paper, their performance in an\nindustrial random bin picking context is investigated. A new method to generate\nrepresentative synthetic datasets is proposed. This allows to investigate the\ninfluence of a high degree of clutter and the presence of self similar\nfeatures, which are typical to our application. We provide an overview of\nsolutions proposed in literature and discuss their strengths and weaknesses. A\nsimple heuristic method to drastically reduce the computational complexity is\nintroduced, which results in improved robustness, speed and accuracy compared\nto the naive approach.\n", "versions": [{"version": "v1", "created": "Mon, 5 Dec 2016 09:57:45 GMT"}], "update_date": "2016-12-07", "authors_parsed": [["Abbeloos", "Wim", ""], ["Goedem\u00e9", "Toon", ""]]}, {"id": "1612.01294", "submitter": "Arnab Ghosh", "authors": "Arnab Ghosh and Viveka Kulharia and Vinay Namboodiri", "title": "Message Passing Multi-Agent GANs", "comments": "The first 2 authors contributed equally for this work", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Communicating and sharing intelligence among agents is an important facet of\nachieving Artificial General Intelligence. As a first step towards this\nchallenge, we introduce a novel framework for image generation: Message Passing\nMulti-Agent Generative Adversarial Networks (MPM GANs). While GANs have\nrecently been shown to be very effective for image generation and other tasks,\nthese networks have been limited to mostly single generator-discriminator\nnetworks. We show that we can obtain multi-agent GANs that communicate through\nmessage passing to achieve better image generation. The objectives of the\nindividual agents in this framework are two fold: a co-operation objective and\na competing objective. The co-operation objective ensures that the message\nsharing mechanism guides the other generator to generate better than itself\nwhile the competing objective encourages each generator to generate better than\nits counterpart. We analyze and visualize the messages that these GANs share\namong themselves in various scenarios. We quantitatively show that the message\nsharing formulation serves as a regularizer for the adversarial training.\nQualitatively, we show that the different generators capture different traits\nof the underlying data distribution.\n", "versions": [{"version": "v1", "created": "Mon, 5 Dec 2016 10:10:13 GMT"}], "update_date": "2016-12-06", "authors_parsed": [["Ghosh", "Arnab", ""], ["Kulharia", "Viveka", ""], ["Namboodiri", "Vinay", ""]]}, {"id": "1612.01323", "submitter": "Sankaraganesh Jonna", "authors": "Sankaraganesh Jonna and Sukla Satapathy and Rajiv R. Sahay", "title": "Stereo image de-fencing using smartphones", "comments": "Under review as a conference paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conventional approaches to image de-fencing have limited themselves to using\nonly image data in adjacent frames of the captured video of an approximately\nstatic scene. In this work, we present a method to harness disparity using a\nstereo pair of fenced images in order to detect fence pixels. Tourists and\namateur photographers commonly carry smartphones/phablets which can be used to\ncapture a short video sequence of the fenced scene. We model the formation of\nthe occluded frames in the captured video. Furthermore, we propose an\noptimization framework to estimate the de-fenced image using the total\nvariation prior to regularize the ill-posed problem.\n", "versions": [{"version": "v1", "created": "Mon, 5 Dec 2016 11:56:56 GMT"}], "update_date": "2016-12-06", "authors_parsed": [["Jonna", "Sankaraganesh", ""], ["Satapathy", "Sukla", ""], ["Sahay", "Rajiv R.", ""]]}, {"id": "1612.01337", "submitter": "Dimitrios Marmanis", "authors": "Dimitrios Marmanis, Konrad Schindler, Jan Dirk Wegner, Silvano\n  Galliani, Mihai Datcu, Uwe Stilla", "title": "Classification With an Edge: Improving Semantic Image Segmentation with\n  Boundary Detection", "comments": null, "journal-ref": "ISPRS Journal of Photogrammetry and Remote Sensing, Volume 135,\n  January 2018, Pages 158-172, ISSN 0924-2716,\n  https://doi.org/10.1016/j.isprsjprs.2017.11.009", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an end-to-end trainable deep convolutional neural network (DCNN)\nfor semantic segmentation with built-in awareness of semantically meaningful\nboundaries. Semantic segmentation is a fundamental remote sensing task, and\nmost state-of-the-art methods rely on DCNNs as their workhorse. A major reason\nfor their success is that deep networks learn to accumulate contextual\ninformation over very large windows (receptive fields). However, this success\ncomes at a cost, since the associated loss of effecive spatial resolution\nwashes out high-frequency details and leads to blurry object boundaries. Here,\nwe propose to counter this effect by combining semantic segmentation with\nsemantically informed edge detection, thus making class-boundaries explicit in\nthe model, First, we construct a comparatively simple, memory-efficient model\nby adding boundary detection to the Segnet encoder-decoder architecture.\nSecond, we also include boundary detection in FCN-type models and set up a\nhigh-end classifier ensemble. We show that boundary detection significantly\nimproves semantic segmentation with CNNs. Our high-end ensemble achieves > 90%\noverall accuracy on the ISPRS Vaihingen benchmark.\n", "versions": [{"version": "v1", "created": "Mon, 5 Dec 2016 13:12:24 GMT"}, {"version": "v2", "created": "Thu, 21 Dec 2017 21:36:14 GMT"}], "update_date": "2017-12-25", "authors_parsed": [["Marmanis", "Dimitrios", ""], ["Schindler", "Konrad", ""], ["Wegner", "Jan Dirk", ""], ["Galliani", "Silvano", ""], ["Datcu", "Mihai", ""], ["Stilla", "Uwe", ""]]}, {"id": "1612.01341", "submitter": "Hanxiao Wang", "authors": "Hanxiao Wang, Shaogang Gong, Tao Xiang", "title": "Highly Efficient Regression for Scalable Person Re-Identification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing person re-identification models are poor for scaling up to large\ndata required in real-world applications due to: (1) Complexity: They employ\ncomplex models for optimal performance resulting in high computational cost for\ntraining at a large scale; (2) Inadaptability: Once trained, they are\nunsuitable for incremental update to incorporate any new data available. This\nwork proposes a truly scalable solution to re-id by addressing both problems.\nSpecifically, a Highly Efficient Regression (HER) model is formulated by\nembedding the Fisher's criterion to a ridge regression model for very fast\nre-id model learning with scalable memory/storage usage. Importantly, this new\nHER model supports faster than real-time incremental model updates therefore\nmaking real-time active learning feasible in re-id with human-in-the-loop.\nExtensive experiments show that such a simple and fast model not only\noutperforms notably the state-of-the-art re-id methods, but also is more\nscalable to large data with additional benefits to active learning for reducing\nhuman labelling effort in re-id deployment.\n", "versions": [{"version": "v1", "created": "Mon, 5 Dec 2016 13:23:42 GMT"}], "update_date": "2016-12-06", "authors_parsed": [["Wang", "Hanxiao", ""], ["Gong", "Shaogang", ""], ["Xiang", "Tao", ""]]}, {"id": "1612.01345", "submitter": "Hanxiao Wang", "authors": "Hanxiao Wang, Shaogang Gong, Xiatian Zhu, Tao Xiang", "title": "Human-In-The-Loop Person Re-Identification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current person re-identification (re-id) methods assume that (1) pre-labelled\ntraining data are available for every camera pair, (2) the gallery size for\nre-identification is moderate. Both assumptions scale poorly to real-world\napplications when camera network size increases and gallery size becomes large.\nHuman verification of automatic model ranked re-id results becomes inevitable.\nIn this work, a novel human-in-the-loop re-id model based on Human Verification\nIncremental Learning (HVIL) is formulated which does not require any\npre-labelled training data to learn a model, therefore readily scalable to new\ncamera pairs. This HVIL model learns cumulatively from human feedback to\nprovide instant improvement to re-id ranking of each probe on-the-fly enabling\nthe model scalable to large gallery sizes. We further formulate a Regularised\nMetric Ensemble Learning (RMEL) model to combine a series of incrementally\nlearned HVIL models into a single ensemble model to be used when human feedback\nbecomes unavailable.\n", "versions": [{"version": "v1", "created": "Mon, 5 Dec 2016 13:29:47 GMT"}, {"version": "v2", "created": "Fri, 4 May 2018 19:56:18 GMT"}], "update_date": "2018-05-08", "authors_parsed": [["Wang", "Hanxiao", ""], ["Gong", "Shaogang", ""], ["Zhu", "Xiatian", ""], ["Xiang", "Tao", ""]]}, {"id": "1612.01380", "submitter": "Ruohan Gao", "authors": "Ruohan Gao and Kristen Grauman", "title": "On-Demand Learning for Deep Image Restoration", "comments": "International Conference on Computer Vision (ICCV), Venice, Italy,\n  Oct. 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While machine learning approaches to image restoration offer great promise,\ncurrent methods risk training models fixated on performing well only for image\ncorruption of a particular level of difficulty---such as a certain level of\nnoise or blur. First, we examine the weakness of conventional \"fixated\" models\nand demonstrate that training general models to handle arbitrary levels of\ncorruption is indeed non-trivial. Then, we propose an on-demand learning\nalgorithm for training image restoration models with deep convolutional neural\nnetworks. The main idea is to exploit a feedback mechanism to self-generate\ntraining instances where they are needed most, thereby learning models that can\ngeneralize across difficulty levels. On four restoration tasks---image\ninpainting, pixel interpolation, image deblurring, and image denoising---and\nthree diverse datasets, our approach consistently outperforms both the status\nquo training procedure and curriculum learning alternatives.\n", "versions": [{"version": "v1", "created": "Mon, 5 Dec 2016 14:53:23 GMT"}, {"version": "v2", "created": "Mon, 27 Mar 2017 19:08:16 GMT"}, {"version": "v3", "created": "Wed, 2 Aug 2017 14:48:27 GMT"}], "update_date": "2017-08-03", "authors_parsed": [["Gao", "Ruohan", ""], ["Grauman", "Kristen", ""]]}, {"id": "1612.01400", "submitter": "Shrisha Rao", "authors": "Apoorva Honnegowda Roopa and Shrisha Rao", "title": "A Distance Function for Comparing Straight-Edge Geometric Figures", "comments": "29 pages, 12 figures including appendices", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper defines a distance function that measures the dissimilarity\nbetween planar geometric figures formed with straight lines. This function can\nin turn be used in partial matching of different geometric figures. For a given\npair of geometric figures that are graphically isomorphic, one function\nmeasures the angular dissimilarity and another function measures the edge\nlength disproportionality. The distance function is then defined as the convex\nsum of these two functions. The novelty of the presented function is that it\nsatisfies all properties of a distance function and the computation of the same\nis done by projecting appropriate features to a cartesian plane. To compute the\ndeviation from the angular similarity property, the Euclidean distance between\nthe given angular pairs and the corresponding points on the $y=x$ line is\nmeasured. Further while computing the deviation from the edge length\nproportionality property, the best fit line, for the set of edge lengths, which\npasses through the origin is found, and the Euclidean distance between the\ngiven edge length pairs and the corresponding point on a $y=mx$ line is\ncalculated. Iterative Proportional Fitting Procedure (IPFP) is used to find\nthis best fit line. We demonstrate the behavior of the defined function for\nsome sample pairs of figures.\n", "versions": [{"version": "v1", "created": "Fri, 25 Nov 2016 01:19:55 GMT"}], "update_date": "2016-12-06", "authors_parsed": [["Roopa", "Apoorva Honnegowda", ""], ["Rao", "Shrisha", ""]]}, {"id": "1612.01452", "submitter": "Marcel Simon", "authors": "Marcel Simon, Erik Rodner, Joachim Denzler", "title": "ImageNet pre-trained models with batch normalization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional neural networks (CNN) pre-trained on ImageNet are the backbone\nof most state-of-the-art approaches. In this paper, we present a new set of\npre-trained models with popular state-of-the-art architectures for the Caffe\nframework. The first release includes Residual Networks (ResNets) with\ngeneration script as well as the batch-normalization-variants of AlexNet and\nVGG19. All models outperform previous models with the same architecture. The\nmodels and training code are available at\nhttp://www.inf-cv.uni-jena.de/Research/CNN+Models.html and\nhttps://github.com/cvjena/cnn-models\n", "versions": [{"version": "v1", "created": "Mon, 5 Dec 2016 17:49:41 GMT"}, {"version": "v2", "created": "Tue, 6 Dec 2016 12:20:00 GMT"}], "update_date": "2016-12-07", "authors_parsed": [["Simon", "Marcel", ""], ["Rodner", "Erik", ""], ["Denzler", "Joachim", ""]]}, {"id": "1612.01465", "submitter": "Eldar Insafutdinov", "authors": "Eldar Insafutdinov, Mykhaylo Andriluka, Leonid Pishchulin, Siyu Tang,\n  Evgeny Levinkov, Bjoern Andres, Bernt Schiele", "title": "ArtTrack: Articulated Multi-person Tracking in the Wild", "comments": "Accepted to CVPR 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose an approach for articulated tracking of multiple\npeople in unconstrained videos. Our starting point is a model that resembles\nexisting architectures for single-frame pose estimation but is substantially\nfaster. We achieve this in two ways: (1) by simplifying and sparsifying the\nbody-part relationship graph and leveraging recent methods for faster\ninference, and (2) by offloading a substantial share of computation onto a\nfeed-forward convolutional architecture that is able to detect and associate\nbody joints of the same person even in clutter. We use this model to generate\nproposals for body joint locations and formulate articulated tracking as\nspatio-temporal grouping of such proposals. This allows to jointly solve the\nassociation problem for all people in the scene by propagating evidence from\nstrong detections through time and enforcing constraints that each proposal can\nbe assigned to one person only. We report results on a public MPII Human Pose\nbenchmark and on a new MPII Video Pose dataset of image sequences with multiple\npeople. We demonstrate that our model achieves state-of-the-art results while\nusing only a fraction of time and is able to leverage temporal information to\nimprove state-of-the-art for crowded scenes.\n", "versions": [{"version": "v1", "created": "Mon, 5 Dec 2016 18:38:56 GMT"}, {"version": "v2", "created": "Thu, 22 Dec 2016 11:49:21 GMT"}, {"version": "v3", "created": "Tue, 9 May 2017 09:56:46 GMT"}], "update_date": "2017-05-10", "authors_parsed": [["Insafutdinov", "Eldar", ""], ["Andriluka", "Mykhaylo", ""], ["Pishchulin", "Leonid", ""], ["Tang", "Siyu", ""], ["Levinkov", "Evgeny", ""], ["Andres", "Bjoern", ""], ["Schiele", "Bernt", ""]]}, {"id": "1612.01479", "submitter": "Jason Rock", "authors": "Jason Rock, Theerasit Issaranon, Aditya Deshpande, David Forsyth", "title": "Authoring image decompositions with generative models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show how to extend traditional intrinsic image decompositions to\nincorporate further layers above albedo and shading. It is hard to obtain data\nto learn a multi-layer decomposition. Instead, we can learn to decompose an\nimage into layers that are \"like this\" by authoring generative models for each\nlayer using proxy examples that capture the Platonic ideal (Mondrian images for\nalbedo; rendered 3D primitives for shading; material swatches for shading\ndetail). Our method then generates image layers, one from each model, that\nexplain the image. Our approach rests on innovation in generative models for\nimages. We introduce a Convolutional Variational Auto Encoder (conv-VAE), a\nnovel VAE architecture that can reconstruct high fidelity images. The approach\nis general, and does not require that layers admit a physical interpretation.\n", "versions": [{"version": "v1", "created": "Mon, 5 Dec 2016 18:59:53 GMT"}], "update_date": "2016-12-06", "authors_parsed": [["Rock", "Jason", ""], ["Issaranon", "Theerasit", ""], ["Deshpande", "Aditya", ""], ["Forsyth", "David", ""]]}, {"id": "1612.01495", "submitter": "Ondrej Miksik", "authors": "Ondrej Miksik, Juan-Manuel P\\'erez-R\\'ua, Philip H. S. Torr, Patrick\n  P\\'erez", "title": "ROAM: a Rich Object Appearance Model with Application to Rotoscoping", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Rotoscoping, the detailed delineation of scene elements through a video shot,\nis a painstaking task of tremendous importance in professional post-production\npipelines. While pixel-wise segmentation techniques can help for this task,\nprofessional rotoscoping tools rely on parametric curves that offer the artists\na much better interactive control on the definition, editing and manipulation\nof the segments of interest. Sticking to this prevalent rotoscoping paradigm,\nwe propose a novel framework to capture and track the visual aspect of an\narbitrary object in a scene, given a first closed outline of this object. This\nmodel combines a collection of local foreground/background appearance models\nspread along the outline, a global appearance model of the enclosed object and\na set of distinctive foreground landmarks. The structure of this rich\nappearance model allows simple initialization, efficient iterative optimization\nwith exact minimization at each step, and on-line adaptation in videos. We\ndemonstrate qualitatively and quantitatively the merit of this framework\nthrough comparisons with tools based on either dynamic segmentation with a\nclosed curve or pixel-wise binary labelling.\n", "versions": [{"version": "v1", "created": "Mon, 5 Dec 2016 20:03:18 GMT"}], "update_date": "2016-12-06", "authors_parsed": [["Miksik", "Ondrej", ""], ["P\u00e9rez-R\u00faa", "Juan-Manuel", ""], ["Torr", "Philip H. S.", ""], ["P\u00e9rez", "Patrick", ""]]}, {"id": "1612.01543", "submitter": "Yoojin Choi", "authors": "Yoojin Choi, Mostafa El-Khamy, and Jungwon Lee", "title": "Towards the Limit of Network Quantization", "comments": "Published as a conference paper at ICLR 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Network quantization is one of network compression techniques to reduce the\nredundancy of deep neural networks. It reduces the number of distinct network\nparameter values by quantization in order to save the storage for them. In this\npaper, we design network quantization schemes that minimize the performance\nloss due to quantization given a compression ratio constraint. We analyze the\nquantitative relation of quantization errors to the neural network loss\nfunction and identify that the Hessian-weighted distortion measure is locally\nthe right objective function for the optimization of network quantization. As a\nresult, Hessian-weighted k-means clustering is proposed for clustering network\nparameters to quantize. When optimal variable-length binary codes, e.g.,\nHuffman codes, are employed for further compression, we derive that the network\nquantization problem can be related to the entropy-constrained scalar\nquantization (ECSQ) problem in information theory and consequently propose two\nsolutions of ECSQ for network quantization, i.e., uniform quantization and an\niterative solution similar to Lloyd's algorithm. Finally, using the simple\nuniform quantization followed by Huffman coding, we show from our experiments\nthat the compression ratios of 51.25, 22.17 and 40.65 are achievable for LeNet,\n32-layer ResNet and AlexNet, respectively.\n", "versions": [{"version": "v1", "created": "Mon, 5 Dec 2016 21:04:17 GMT"}, {"version": "v2", "created": "Mon, 13 Nov 2017 19:44:32 GMT"}], "update_date": "2017-11-15", "authors_parsed": [["Choi", "Yoojin", ""], ["El-Khamy", "Mostafa", ""], ["Lee", "Jungwon", ""]]}, {"id": "1612.01594", "submitter": "Homa Foroughi", "authors": "Homa Foroughi, Nilanjan Ray and Hong Zhang", "title": "Object Classification with Joint Projection and Low-rank Dictionary\n  Learning", "comments": "arXiv admin note: text overlap with arXiv:1603.07697; text overlap\n  with arXiv:1404.3606 by other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For an object classification system, the most critical obstacles towards\nreal-world applications are often caused by large intra-class variability,\narising from different lightings, occlusion and corruption, in limited sample\nsets. Most methods in the literature would fail when the training samples are\nheavily occluded, corrupted or have significant illumination or viewpoint\nvariations. Besides, most of the existing methods and especially deep\nlearning-based methods, need large training sets to achieve a satisfactory\nrecognition performance. Although using the pre-trained network on a generic\nlarge-scale dataset and fine-tune it to the small-sized target dataset is a\nwidely used technique, this would not help when the content of base and target\ndatasets are very different. To address these issues, we propose a joint\nprojection and low-rank dictionary learning method using dual graph constraints\n(JP-LRDL). The proposed joint learning method would enable us to learn the\nfeatures on top of which dictionaries can be better learned, from the data with\nlarge intra-class variability. Specifically, a structured class-specific\ndictionary is learned and the discrimination is further improved by imposing a\ngraph constraint on the coding coefficients, that maximizes the intra-class\ncompactness and inter-class separability. We also enforce low-rank and\nstructural incoherence constraints on sub-dictionaries to make them more\ncompact and robust to variations and outliers and reduce the redundancy among\nthem, respectively. To preserve the intrinsic structure of data and penalize\nunfavourable relationship among training samples simultaneously, we introduce a\nprojection graph into the framework, which significantly enhances the\ndiscriminative ability of the projection matrix and makes the method robust to\nsmall-sized and high-dimensional datasets.\n", "versions": [{"version": "v1", "created": "Mon, 5 Dec 2016 23:49:26 GMT"}], "update_date": "2016-12-07", "authors_parsed": [["Foroughi", "Homa", ""], ["Ray", "Nilanjan", ""], ["Zhang", "Hong", ""]]}, {"id": "1612.01601", "submitter": "David Stutz", "authors": "David Stutz and Alexander Hermans and Bastian Leibe", "title": "Superpixels: An Evaluation of the State-of-the-Art", "comments": null, "journal-ref": null, "doi": "10.1016/j.cviu.2017.03.007", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Superpixels group perceptually similar pixels to create visually meaningful\nentities while heavily reducing the number of primitives for subsequent\nprocessing steps. As of these properties, superpixel algorithms have received\nmuch attention since their naming in 2003. By today, publicly available\nsuperpixel algorithms have turned into standard tools in low-level vision. As\nsuch, and due to their quick adoption in a wide range of applications,\nappropriate benchmarks are crucial for algorithm selection and comparison.\nUntil now, the rapidly growing number of algorithms as well as varying\nexperimental setups hindered the development of a unifying benchmark. We\npresent a comprehensive evaluation of 28 state-of-the-art superpixel algorithms\nutilizing a benchmark focussing on fair comparison and designed to provide new\ninsights relevant for applications. To this end, we explicitly discuss\nparameter optimization and the importance of strictly enforcing connectivity.\nFurthermore, by extending well-known metrics, we are able to summarize\nalgorithm performance independent of the number of generated superpixels,\nthereby overcoming a major limitation of available benchmarks. Furthermore, we\ndiscuss runtime, robustness against noise, blur and affine transformations,\nimplementation details as well as aspects of visual quality. Finally, we\npresent an overall ranking of superpixel algorithms which redefines the\nstate-of-the-art and enables researchers to easily select appropriate\nalgorithms and the corresponding implementations which themselves are made\npublicly available as part of our benchmark at\ndavidstutz.de/projects/superpixel-benchmark/.\n", "versions": [{"version": "v1", "created": "Tue, 6 Dec 2016 00:26:54 GMT"}, {"version": "v2", "created": "Thu, 15 Dec 2016 00:35:24 GMT"}, {"version": "v3", "created": "Wed, 19 Apr 2017 20:40:48 GMT"}], "update_date": "2017-04-21", "authors_parsed": [["Stutz", "David", ""], ["Hermans", "Alexander", ""], ["Leibe", "Bastian", ""]]}, {"id": "1612.01611", "submitter": "Jingxin Xu", "authors": "Jingxin Xu, Clinton Fookes, Sridha Sridharan", "title": "Automatic Event Detection for Signal-based Surveillance", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Signal-based Surveillance systems such as Closed Circuits Televisions (CCTV)\nhave been widely installed in public places. Those systems are normally used to\nfind the events with security interest, and play a significant role in public\nsafety. Though such systems are still heavily reliant on human labour to\nmonitor the captured information, there have been a number of automatic\ntechniques proposed to analysing the data. This article provides an overview of\nautomatic surveillance event detection techniques . Despite it's popularity in\nresearch, it is still too challenging a problem to be realised in a real world\ndeployment. The challenges come from not only the detection techniques such as\nsignal processing and machine learning, but also the experimental design with\nfactors such as data collection, evaluation protocols, and ground-truth\nannotation. Finally, this article propose that multi-disciplinary research is\nthe path towards a solution to this problem.\n", "versions": [{"version": "v1", "created": "Tue, 6 Dec 2016 00:54:45 GMT"}], "update_date": "2016-12-07", "authors_parsed": [["Xu", "Jingxin", ""], ["Fookes", "Clinton", ""], ["Sridharan", "Sridha", ""]]}, {"id": "1612.01635", "submitter": "Ning Yu", "authors": "Ning Yu, Xiaohui Shen, Zhe Lin, Radomir Mech, Connelly Barnes", "title": "Learning to Detect Multiple Photographic Defects", "comments": "Accepted to WACV'18", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce the problem of simultaneously detecting multiple\nphotographic defects. We aim at detecting the existence, severity, and\npotential locations of common photographic defects related to color, noise,\nblur and composition. The automatic detection of such defects could be used to\nprovide users with suggestions for how to improve photos without the need to\nlaboriously try various correction methods. Defect detection could also help\nusers select photos of higher quality while filtering out those with severe\ndefects in photo curation and summarization.\n  To investigate this problem, we collected a large-scale dataset of user\nannotations on seven common photographic defects, which allows us to evaluate\nalgorithms by measuring their consistency with human judgments. Our new dataset\nenables us to formulate the problem as a multi-task learning problem and train\na multi-column deep convolutional neural network (CNN) to simultaneously\npredict the severity of all the defects. Unlike some existing single-defect\nestimation methods that rely on low-level statistics and may fail in many cases\non natural photographs, our model is able to understand image contents and\nquality at a higher level. As a result, in our experiments, we show that our\nmodel has predictions with much higher consistency with human judgments than\nlow-level methods as well as several baseline CNN models. Our model also\nperforms better than an average human from our user study.\n", "versions": [{"version": "v1", "created": "Tue, 6 Dec 2016 02:32:22 GMT"}, {"version": "v2", "created": "Fri, 26 May 2017 08:05:02 GMT"}, {"version": "v3", "created": "Mon, 9 Oct 2017 20:15:48 GMT"}, {"version": "v4", "created": "Sat, 13 Jan 2018 01:24:53 GMT"}, {"version": "v5", "created": "Thu, 8 Mar 2018 07:05:30 GMT"}], "update_date": "2018-03-09", "authors_parsed": [["Yu", "Ning", ""], ["Shen", "Xiaohui", ""], ["Lin", "Zhe", ""], ["Mech", "Radomir", ""], ["Barnes", "Connelly", ""]]}, {"id": "1612.01655", "submitter": "Xin Yang", "authors": "Xin Yang, Lequan Yu, Lingyun Wu, Yi Wang, Dong Ni, Jing Qin, Pheng-Ann\n  Heng", "title": "Fine-grained Recurrent Neural Networks for Automatic Prostate\n  Segmentation in Ultrasound Images", "comments": "To appear in AAAI Conference 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Boundary incompleteness raises great challenges to automatic prostate\nsegmentation in ultrasound images. Shape prior can provide strong guidance in\nestimating the missing boundary, but traditional shape models often suffer from\nhand-crafted descriptors and local information loss in the fitting procedure.\nIn this paper, we attempt to address those issues with a novel framework. The\nproposed framework can seamlessly integrate feature extraction and shape prior\nexploring, and estimate the complete boundary with a sequential manner. Our\nframework is composed of three key modules. Firstly, we serialize the static 2D\nprostate ultrasound images into dynamic sequences and then predict prostate\nshapes by sequentially exploring shape priors. Intuitively, we propose to learn\nthe shape prior with the biologically plausible Recurrent Neural Networks\n(RNNs). This module is corroborated to be effective in dealing with the\nboundary incompleteness. Secondly, to alleviate the bias caused by different\nserialization manners, we propose a multi-view fusion strategy to merge shape\npredictions obtained from different perspectives. Thirdly, we further implant\nthe RNN core into a multiscale Auto-Context scheme to successively refine the\ndetails of the shape prediction map. With extensive validation on challenging\nprostate ultrasound images, our framework bridges severe boundary\nincompleteness and achieves the best performance in prostate boundary\ndelineation when compared with several advanced methods. Additionally, our\napproach is general and can be extended to other medical image segmentation\ntasks, where boundary incompleteness is one of the main challenges.\n", "versions": [{"version": "v1", "created": "Tue, 6 Dec 2016 03:56:07 GMT"}], "update_date": "2016-12-07", "authors_parsed": [["Yang", "Xin", ""], ["Yu", "Lequan", ""], ["Wu", "Lingyun", ""], ["Wang", "Yi", ""], ["Ni", "Dong", ""], ["Qin", "Jing", ""], ["Heng", "Pheng-Ann", ""]]}, {"id": "1612.01657", "submitter": "Yang Yang", "authors": "Ruicong Xu, Yang Yang, Yadan Luo, Fumin Shen, Zi Huang, Heng Tao Shen", "title": "Binary Subspace Coding for Query-by-Image Video Retrieval", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The query-by-image video retrieval (QBIVR) task has been attracting\nconsiderable research attention recently. However, most existing methods\nrepresent a video by either aggregating or projecting all its frames into a\nsingle datum point, which may easily cause severe information loss. In this\npaper, we propose an efficient QBIVR framework to enable an effective and\nefficient video search with image query. We first define a\nsimilarity-preserving distance metric between an image and its orthogonal\nprojection in the subspace of the video, which can be equivalently transformed\nto a Maximum Inner Product Search (MIPS) problem.\n  Besides, to boost the efficiency of solving the MIPS problem, we propose two\nasymmetric hashing schemes, which bridge the domain gap of images and videos.\nThe first approach, termed Inner-product Binary Coding (IBC), preserves the\ninner relationships of images and videos in a common Hamming space. To further\nimprove the retrieval efficiency, we devise a Bilinear Binary Coding (BBC)\napproach, which employs compact bilinear projections instead of a single large\nprojection matrix. Extensive experiments have been conducted on four real-world\nvideo datasets to verify the effectiveness of our proposed approaches as\ncompared to the state-of-the-arts.\n", "versions": [{"version": "v1", "created": "Tue, 6 Dec 2016 04:01:17 GMT"}], "update_date": "2016-12-07", "authors_parsed": [["Xu", "Ruicong", ""], ["Yang", "Yang", ""], ["Luo", "Yadan", ""], ["Shen", "Fumin", ""], ["Huang", "Zi", ""], ["Shen", "Heng Tao", ""]]}, {"id": "1612.01669", "submitter": "Jonghwan Mun", "authors": "Jonghwan Mun, Paul Hongsuck Seo, Ilchae Jung, Bohyung Han", "title": "MarioQA: Answering Questions by Watching Gameplay Videos", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a framework to analyze various aspects of models for video\nquestion answering (VideoQA) using customizable synthetic datasets, which are\nconstructed automatically from gameplay videos. Our work is motivated by the\nfact that existing models are often tested only on datasets that require\nexcessively high-level reasoning or mostly contain instances accessible through\nsingle frame inferences. Hence, it is difficult to measure capacity and\nflexibility of trained models, and existing techniques often rely on ad-hoc\nimplementations of deep neural networks without clear insight into datasets and\nmodels. We are particularly interested in understanding temporal relationships\nbetween video events to solve VideoQA problems; this is because reasoning\ntemporal dependency is one of the most distinct components in videos from\nimages. To address this objective, we automatically generate a customized\nsynthetic VideoQA dataset using {\\em Super Mario Bros.} gameplay videos so that\nit contains events with different levels of reasoning complexity. Using the\ndataset, we show that properly constructed datasets with events in various\ncomplexity levels are critical to learn effective models and improve overall\nperformance.\n", "versions": [{"version": "v1", "created": "Tue, 6 Dec 2016 05:23:52 GMT"}, {"version": "v2", "created": "Sun, 13 Aug 2017 07:49:55 GMT"}], "update_date": "2017-08-15", "authors_parsed": [["Mun", "Jonghwan", ""], ["Seo", "Paul Hongsuck", ""], ["Jung", "Ilchae", ""], ["Han", "Bohyung", ""]]}, {"id": "1612.01689", "submitter": "Ra\\'ul D\\'iaz", "authors": "Ra\\'ul D\\'iaz, Charless C. Fowlkes", "title": "Cluster-Wise Ratio Tests for Fast Camera Localization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Feature point matching for camera localization suffers from scalability\nproblems. Even when feature descriptors associated with 3D scene points are\nlocally unique, as coverage grows, similar or repeated features become\nincreasingly common. As a result, the standard distance ratio-test used to\nidentify reliable image feature points is overly restrictive and rejects many\ngood candidate matches. We propose a simple coarse-to-fine strategy that uses\nconservative approximations to robust local ratio-tests that can be computed\nefficiently using global approximate k-nearest neighbor search. We treat these\nforward matches as votes in camera pose space and use them to prioritize\nback-matching within candidate camera pose clusters, exploiting feature\nco-visibility captured by clustering the 3D model camera pose graph. This\napproach achieves state-of-the-art camera localization results on a variety of\npopular benchmarks, outperforming several methods that use more complicated\ndata structures and that make more restrictive assumptions on camera pose. We\nalso carry out diagnostic analyses on a difficult test dataset containing\nglobally repetitive structure that suggest our approach successfully adapts to\nthe challenges of large-scale image localization.\n", "versions": [{"version": "v1", "created": "Tue, 6 Dec 2016 07:35:24 GMT"}, {"version": "v2", "created": "Sat, 20 May 2017 18:02:46 GMT"}], "update_date": "2017-05-23", "authors_parsed": [["D\u00edaz", "Ra\u00fal", ""], ["Fowlkes", "Charless C.", ""]]}, {"id": "1612.01697", "submitter": "Sebastian Bosse", "authors": "Sebastian Bosse, Dominique Maniry, Klaus-Robert M\\\"uller, Thomas\n  Wiegand, and Wojciech Samek", "title": "Deep Neural Networks for No-Reference and Full-Reference Image Quality\n  Assessment", "comments": null, "journal-ref": "IEEE Transactions on Image Processing, 27(1):206-219, 2018", "doi": "10.1109/TIP.2017.2760518", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a deep neural network-based approach to image quality assessment\n(IQA). The network is trained end-to-end and comprises ten convolutional layers\nand five pooling layers for feature extraction, and two fully connected layers\nfor regression, which makes it significantly deeper than related IQA models.\nUnique features of the proposed architecture are that: 1) with slight\nadaptations it can be used in a no-reference (NR) as well as in a\nfull-reference (FR) IQA setting and 2) it allows for joint learning of local\nquality and local weights, i.e., relative importance of local quality to the\nglobal quality estimate, in an unified framework. Our approach is purely\ndata-driven and does not rely on hand-crafted features or other types of prior\ndomain knowledge about the human visual system or image statistics. We evaluate\nthe proposed approach on the LIVE, CISQ, and TID2013 databases as well as the\nLIVE In the wild image quality challenge database and show superior performance\nto state-of-the-art NR and FR IQA methods. Finally, cross-database evaluation\nshows a high ability to generalize between different databases, indicating a\nhigh robustness of the learned features.\n", "versions": [{"version": "v1", "created": "Tue, 6 Dec 2016 08:15:03 GMT"}, {"version": "v2", "created": "Thu, 7 Dec 2017 20:36:50 GMT"}], "update_date": "2017-12-11", "authors_parsed": [["Bosse", "Sebastian", ""], ["Maniry", "Dominique", ""], ["M\u00fcller", "Klaus-Robert", ""], ["Wiegand", "Thomas", ""], ["Samek", "Wojciech", ""]]}, {"id": "1612.01725", "submitter": "Ron Slossberg", "authors": "Ron Slossberg, Aaron Wetzler and Ron Kimmel", "title": "Deep Stereo Matching with Dense CRF Priors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stereo reconstruction from rectified images has recently been revisited\nwithin the context of deep learning. Using a deep Convolutional Neural Network\nto obtain patch-wise matching cost volumes has resulted in state of the art\nstereo reconstruction on classic datasets like Middlebury and Kitti. By\nintroducing this cost into a classical stereo pipeline, the final results are\nimproved dramatically over non-learning based cost models. However these\npipelines typically include hand engineered post processing steps to\neffectively regularize and clean the result. Here, we show that it is possible\nto take a more holistic approach by training a fully end-to-end network which\ndirectly includes regularization in the form of a densely connected Conditional\nRandom Field (CRF) that acts as a prior on inter-pixel interactions. We\ndemonstrate that our approach on both synthetic and real world datasets\noutperforms an alternative end-to-end network and compares favorably to more\nhand engineered approaches.\n", "versions": [{"version": "v1", "created": "Tue, 6 Dec 2016 09:51:21 GMT"}, {"version": "v2", "created": "Tue, 24 Jan 2017 20:08:28 GMT"}], "update_date": "2017-01-26", "authors_parsed": [["Slossberg", "Ron", ""], ["Wetzler", "Aaron", ""], ["Kimmel", "Ron", ""]]}, {"id": "1612.01756", "submitter": "Francesco Cricri", "authors": "Francesco Cricri, Xingyang Ni, Mikko Honkala, Emre Aksu, Moncef\n  Gabbouj", "title": "Video Ladder Networks", "comments": "This version extends the paper accepted at the NIPS 2016 workshop on\n  ML for Spatiotemporal Forecasting, with more details and more experimental\n  results", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the Video Ladder Network (VLN) for efficiently generating future\nvideo frames. VLN is a neural encoder-decoder model augmented at all layers by\nboth recurrent and feedforward lateral connections. At each layer, these\nconnections form a lateral recurrent residual block, where the feedforward\nconnection represents a skip connection and the recurrent connection represents\nthe residual. Thanks to the recurrent connections, the decoder can exploit\ntemporal summaries generated from all layers of the encoder. This way, the top\nlayer is relieved from the pressure of modeling lower-level spatial and\ntemporal details. Furthermore, we extend the basic version of VLN to\nincorporate ResNet-style residual blocks in the encoder and decoder, which help\nimproving the prediction results. VLN is trained in self-supervised regime on\nthe Moving MNIST dataset, achieving competitive results while having very\nsimple structure and providing fast inference.\n", "versions": [{"version": "v1", "created": "Tue, 6 Dec 2016 11:15:28 GMT"}, {"version": "v2", "created": "Wed, 7 Dec 2016 11:35:22 GMT"}, {"version": "v3", "created": "Fri, 30 Dec 2016 09:01:02 GMT"}], "update_date": "2017-01-02", "authors_parsed": [["Cricri", "Francesco", ""], ["Ni", "Xingyang", ""], ["Honkala", "Mikko", ""], ["Aksu", "Emre", ""], ["Gabbouj", "Moncef", ""]]}, {"id": "1612.01810", "submitter": "Ming-Ming Cheng Prof.", "authors": "Jiaxing Zhao, Ren Bo, Qibin Hou, Ming-Ming Cheng, Paul L. Rosin", "title": "FLIC: Fast Linear Iterative Clustering with Active Search", "comments": "AAAI 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Benefiting from its high efficiency and simplicity, Simple Linear Iterative\nClustering (SLIC) remains one of the most popular over-segmentation tools.\nHowever, due to explicit enforcement of spatial similarity for region\ncontinuity, the boundary adaptation of SLIC is sub-optimal. It also has\ndrawbacks on convergence rate as a result of both the fixed search region and\nseparately doing the assignment step and the update step. In this paper, we\npropose an alternative approach to fix the inherent limitations of SLIC. In our\napproach, each pixel actively searches its corresponding segment under the help\nof its neighboring pixels, which naturally enables region coherence without\nbeing harmful to boundary adaptation. We also jointly perform the assignment\nand update steps, allowing high convergence rate. Extensive evaluations on\nBerkeley segmentation benchmark verify that our method outperforms competitive\nmethods under various evaluation metrics. It also has the lowest time cost\namong existing methods (approximately 30fps for a 481x321 image on a single CPU\ncore).\n", "versions": [{"version": "v1", "created": "Tue, 6 Dec 2016 14:16:19 GMT"}, {"version": "v2", "created": "Tue, 21 Nov 2017 03:29:42 GMT"}, {"version": "v3", "created": "Fri, 5 Oct 2018 13:42:55 GMT"}], "update_date": "2018-10-09", "authors_parsed": [["Zhao", "Jiaxing", ""], ["Bo", "Ren", ""], ["Hou", "Qibin", ""], ["Cheng", "Ming-Ming", ""], ["Rosin", "Paul L.", ""]]}, {"id": "1612.01820", "submitter": "Jie Yang", "authors": "Jie Yang, Elsa D. Angelini, Benjamin M. Smith, John H.M. Austin, Eric\n  A. Hoffman, David A. Bluemke, R. Graham Barr, and Andrew F. Laine", "title": "Explaining Radiological Emphysema Subtypes with Unsupervised Texture\n  Prototypes: MESA COPD Study", "comments": "MICCAI workshop on Medical Computer Vision: Algorithms for Big Data\n  (2016)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pulmonary emphysema is traditionally subcategorized into three subtypes,\nwhich have distinct radiological appearances on computed tomography (CT) and\ncan help with the diagnosis of chronic obstructive pulmonary disease (COPD).\nAutomated texture-based quantification of emphysema subtypes has been\nsuccessfully implemented via supervised learning of these three emphysema\nsubtypes. In this work, we demonstrate that unsupervised learning on a large\nheterogeneous database of CT scans can generate texture prototypes that are\nvisually homogeneous and distinct, reproducible across subjects, and capable of\npredicting accurately the three standard radiological subtypes. These texture\nprototypes enable automated labeling of lung volumes, and open the way to new\ninterpretations of lung CT scans with finer subtyping of emphysema.\n", "versions": [{"version": "v1", "created": "Mon, 5 Dec 2016 20:42:20 GMT"}], "update_date": "2016-12-07", "authors_parsed": [["Yang", "Jie", ""], ["Angelini", "Elsa D.", ""], ["Smith", "Benjamin M.", ""], ["Austin", "John H. M.", ""], ["Hoffman", "Eric A.", ""], ["Bluemke", "David A.", ""], ["Barr", "R. Graham", ""], ["Laine", "Andrew F.", ""]]}, {"id": "1612.01834", "submitter": "Beidi Chen", "authors": "Beidi Chen, Anshumali Shrivastava", "title": "Revisiting Winner Take All (WTA) Hashing for Sparse Datasets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  WTA (Winner Take All) hashing has been successfully applied in many large\nscale vision applications. This hashing scheme was tailored to take advantage\nof the comparative reasoning (or order based information), which showed\nsignificant accuracy improvements. In this paper, we identify a subtle issue\nwith WTA, which grows with the sparsity of the datasets. This issue limits the\ndiscriminative power of WTA. We then propose a solution for this problem based\non the idea of Densification which provably fixes the issue. Our experiments\nshow that Densified WTA Hashing outperforms Vanilla WTA both in image\nclassification and retrieval tasks consistently and significantly.\n", "versions": [{"version": "v1", "created": "Tue, 6 Dec 2016 14:51:37 GMT"}, {"version": "v2", "created": "Wed, 7 Dec 2016 08:50:26 GMT"}], "update_date": "2016-12-08", "authors_parsed": [["Chen", "Beidi", ""], ["Shrivastava", "Anshumali", ""]]}, {"id": "1612.01887", "submitter": "Jiasen Lu", "authors": "Jiasen Lu, Caiming Xiong, Devi Parikh, Richard Socher", "title": "Knowing When to Look: Adaptive Attention via A Visual Sentinel for Image\n  Captioning", "comments": "12 pages, 11 figures, CVPR2017 camera ready", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Attention-based neural encoder-decoder frameworks have been widely adopted\nfor image captioning. Most methods force visual attention to be active for\nevery generated word. However, the decoder likely requires little to no visual\ninformation from the image to predict non-visual words such as \"the\" and \"of\".\nOther words that may seem visual can often be predicted reliably just from the\nlanguage model e.g., \"sign\" after \"behind a red stop\" or \"phone\" following\n\"talking on a cell\". In this paper, we propose a novel adaptive attention model\nwith a visual sentinel. At each time step, our model decides whether to attend\nto the image (and if so, to which regions) or to the visual sentinel. The model\ndecides whether to attend to the image and where, in order to extract\nmeaningful information for sequential word generation. We test our method on\nthe COCO image captioning 2015 challenge dataset and Flickr30K. Our approach\nsets the new state-of-the-art by a significant margin.\n", "versions": [{"version": "v1", "created": "Tue, 6 Dec 2016 16:03:50 GMT"}, {"version": "v2", "created": "Tue, 6 Jun 2017 06:59:15 GMT"}], "update_date": "2017-06-07", "authors_parsed": [["Lu", "Jiasen", ""], ["Xiong", "Caiming", ""], ["Parikh", "Devi", ""], ["Socher", "Richard", ""]]}, {"id": "1612.01895", "submitter": "Xin Wang", "authors": "Xin Wang, Geoffrey Oxholm, Da Zhang, Yuan-Fang Wang", "title": "Multimodal Transfer: A Hierarchical Deep Convolutional Neural Network\n  for Fast Artistic Style Transfer", "comments": "Accepted by CVPR 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transferring artistic styles onto everyday photographs has become an\nextremely popular task in both academia and industry. Recently, offline\ntraining has replaced on-line iterative optimization, enabling nearly real-time\nstylization. When those stylization networks are applied directly to\nhigh-resolution images, however, the style of localized regions often appears\nless similar to the desired artistic style. This is because the transfer\nprocess fails to capture small, intricate textures and maintain correct texture\nscales of the artworks. Here we propose a multimodal convolutional neural\nnetwork that takes into consideration faithful representations of both color\nand luminance channels, and performs stylization hierarchically with multiple\nlosses of increasing scales. Compared to state-of-the-art networks, our network\ncan also perform style transfer in nearly real-time by conducting much more\nsophisticated training offline. By properly handling style and texture cues at\nmultiple scales using several modalities, we can transfer not just large-scale,\nobvious style cues but also subtle, exquisite ones. That is, our scheme can\ngenerate results that are visually pleasing and more similar to multiple\ndesired artistic styles with color and texture cues at multiple scales.\n", "versions": [{"version": "v1", "created": "Thu, 17 Nov 2016 07:48:25 GMT"}, {"version": "v2", "created": "Tue, 11 Apr 2017 05:27:13 GMT"}], "update_date": "2017-12-01", "authors_parsed": [["Wang", "Xin", ""], ["Oxholm", "Geoffrey", ""], ["Zhang", "Da", ""], ["Wang", "Yuan-Fang", ""]]}, {"id": "1612.01922", "submitter": "Kofi Boakye", "authors": "Kofi Boakye, Sachin Farfade, Hamid Izadinia, Yannis Kalantidis, and\n  Pierre Garrigues", "title": "Tag Prediction at Flickr: a View from the Darkroom", "comments": "Presented at the ACM Multimedia Thematic Workshops, 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automated photo tagging has established itself as one of the most compelling\napplications of deep learning. While deep convolutional neural networks have\nrepeatedly demonstrated top performance on standard datasets for\nclassification, there are a number of often overlooked but important\nconsiderations when deploying this technology in a real-world scenario. In this\npaper, we present our efforts in developing a large-scale photo tagging system\nfor Flickr photo search. We discuss topics including how to 1) select the tags\nthat matter most to our users; 2) develop lightweight, high-performance models\nfor tag prediction; and 3) leverage the power of large amounts of noisy data\nfor training. Our results demonstrate that, for real-world datasets, training\nexclusively with this noisy data yields performance on par with the standard\nparadigm of first pre-training on clean data and then fine-tuning. In addition,\nwe observe that the models trained with user-generated data can yield better\nfine-tuning results when a small amount of clean data is available. As such, we\nadvocate for the approach of harnessing user-generated data in large-scale\nsystems.\n", "versions": [{"version": "v1", "created": "Tue, 6 Dec 2016 17:39:49 GMT"}, {"version": "v2", "created": "Wed, 7 Dec 2016 21:06:18 GMT"}, {"version": "v3", "created": "Tue, 19 Dec 2017 23:37:04 GMT"}], "update_date": "2017-12-21", "authors_parsed": [["Boakye", "Kofi", ""], ["Farfade", "Sachin", ""], ["Izadinia", "Hamid", ""], ["Kalantidis", "Yannis", ""], ["Garrigues", "Pierre", ""]]}, {"id": "1612.01925", "submitter": "Eddy Ilg", "authors": "Eddy Ilg, Nikolaus Mayer, Tonmoy Saikia, Margret Keuper, Alexey\n  Dosovitskiy, Thomas Brox", "title": "FlowNet 2.0: Evolution of Optical Flow Estimation with Deep Networks", "comments": "Including supplementary material. For the video see:\n  http://lmb.informatik.uni-freiburg.de/Publications/2016/IMKDB16/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The FlowNet demonstrated that optical flow estimation can be cast as a\nlearning problem. However, the state of the art with regard to the quality of\nthe flow has still been defined by traditional methods. Particularly on small\ndisplacements and real-world data, FlowNet cannot compete with variational\nmethods. In this paper, we advance the concept of end-to-end learning of\noptical flow and make it work really well. The large improvements in quality\nand speed are caused by three major contributions: first, we focus on the\ntraining data and show that the schedule of presenting data during training is\nvery important. Second, we develop a stacked architecture that includes warping\nof the second image with intermediate optical flow. Third, we elaborate on\nsmall displacements by introducing a sub-network specializing on small motions.\nFlowNet 2.0 is only marginally slower than the original FlowNet but decreases\nthe estimation error by more than 50%. It performs on par with state-of-the-art\nmethods, while running at interactive frame rates. Moreover, we present faster\nvariants that allow optical flow computation at up to 140fps with accuracy\nmatching the original FlowNet.\n", "versions": [{"version": "v1", "created": "Tue, 6 Dec 2016 17:52:47 GMT"}], "update_date": "2016-12-07", "authors_parsed": [["Ilg", "Eddy", ""], ["Mayer", "Nikolaus", ""], ["Saikia", "Tonmoy", ""], ["Keuper", "Margret", ""], ["Dosovitskiy", "Alexey", ""], ["Brox", "Thomas", ""]]}, {"id": "1612.01928", "submitter": "Dmitriy Serdyuk", "authors": "Dmitriy Serdyuk, Kartik Audhkhasi, Phil\\'emon Brakel, Bhuvana\n  Ramabhadran, Samuel Thomas, Yoshua Bengio", "title": "Invariant Representations for Noisy Speech Recognition", "comments": "5 pages, 1 figure, 1 table, NIPS workshop on end-to-end speech\n  recognition", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV cs.LG cs.SD stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern automatic speech recognition (ASR) systems need to be robust under\nacoustic variability arising from environmental, speaker, channel, and\nrecording conditions. Ensuring such robustness to variability is a challenge in\nmodern day neural network-based ASR systems, especially when all types of\nvariability are not seen during training. We attempt to address this problem by\nencouraging the neural network acoustic model to learn invariant feature\nrepresentations. We use ideas from recent research on image generation using\nGenerative Adversarial Networks and domain adaptation ideas extending\nadversarial gradient-based training. A recent work from Ganin et al. proposes\nto use adversarial training for image domain adaptation by using an\nintermediate representation from the main target classification network to\ndeteriorate the domain classifier performance through a separate neural\nnetwork. Our work focuses on investigating neural architectures which produce\nrepresentations invariant to noise conditions for ASR. We evaluate the proposed\narchitecture on the Aurora-4 task, a popular benchmark for noise robust ASR. We\nshow that our method generalizes better than the standard multi-condition\ntraining especially when only a few noise categories are seen during training.\n", "versions": [{"version": "v1", "created": "Sun, 27 Nov 2016 22:20:51 GMT"}], "update_date": "2016-12-07", "authors_parsed": [["Serdyuk", "Dmitriy", ""], ["Audhkhasi", "Kartik", ""], ["Brakel", "Phil\u00e9mon", ""], ["Ramabhadran", "Bhuvana", ""], ["Thomas", "Samuel", ""], ["Bengio", "Yoshua", ""]]}, {"id": "1612.01939", "submitter": "Baochen Sun", "authors": "Baochen Sun, Jiashi Feng, Kate Saenko", "title": "Correlation Alignment for Unsupervised Domain Adaptation", "comments": "Introduction to CORAL, CORAL-LDA, and Deep CORAL. arXiv admin note:\n  text overlap with arXiv:1511.05547", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this chapter, we present CORrelation ALignment (CORAL), a simple yet\neffective method for unsupervised domain adaptation. CORAL minimizes domain\nshift by aligning the second-order statistics of source and target\ndistributions, without requiring any target labels. In contrast to subspace\nmanifold methods, it aligns the original feature distributions of the source\nand target domains, rather than the bases of lower-dimensional subspaces. It is\nalso much simpler than other distribution matching methods. CORAL performs\nremarkably well in extensive evaluations on standard benchmark datasets. We\nfirst describe a solution that applies a linear transformation to source\nfeatures to align them with target features before classifier training. For\nlinear classifiers, we propose to equivalently apply CORAL to the classifier\nweights, leading to added efficiency when the number of classifiers is small\nbut the number and dimensionality of target examples are very high. The\nresulting CORAL Linear Discriminant Analysis (CORAL-LDA) outperforms LDA by a\nlarge margin on standard domain adaptation benchmarks. Finally, we extend CORAL\nto learn a nonlinear transformation that aligns correlations of layer\nactivations in deep neural networks (DNNs). The resulting Deep CORAL approach\nworks seamlessly with DNNs and achieves state-of-the-art performance on\nstandard benchmark datasets. Our code is available\nat:~\\url{https://github.com/VisionLearningGroup/CORAL}\n", "versions": [{"version": "v1", "created": "Tue, 6 Dec 2016 18:31:57 GMT"}], "update_date": "2016-12-07", "authors_parsed": [["Sun", "Baochen", ""], ["Feng", "Jiashi", ""], ["Saenko", "Kate", ""]]}, {"id": "1612.01958", "submitter": "Aditya Deshpande", "authors": "Aditya Deshpande, Jiajun Lu, Mao-Chuang Yeh, Min Jin Chong, David\n  Forsyth", "title": "Learning Diverse Image Colorization", "comments": "This revision to appear in CVPR17", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Colorization is an ambiguous problem, with multiple viable colorizations for\na single grey-level image. However, previous methods only produce the single\nmost probable colorization. Our goal is to model the diversity intrinsic to the\nproblem of colorization and produce multiple colorizations that display\nlong-scale spatial co-ordination. We learn a low dimensional embedding of color\nfields using a variational autoencoder (VAE). We construct loss terms for the\nVAE decoder that avoid blurry outputs and take into account the uneven\ndistribution of pixel colors. Finally, we build a conditional model for the\nmulti-modal distribution between grey-level image and the color field\nembeddings. Samples from this conditional model result in diverse colorization.\nWe demonstrate that our method obtains better diverse colorizations than a\nstandard conditional variational autoencoder (CVAE) model, as well as a\nrecently proposed conditional generative adversarial network (cGAN).\n", "versions": [{"version": "v1", "created": "Tue, 6 Dec 2016 19:28:22 GMT"}, {"version": "v2", "created": "Thu, 27 Apr 2017 14:34:28 GMT"}], "update_date": "2017-04-28", "authors_parsed": [["Deshpande", "Aditya", ""], ["Lu", "Jiajun", ""], ["Yeh", "Mao-Chuang", ""], ["Chong", "Min Jin", ""], ["Forsyth", "David", ""]]}, {"id": "1612.01981", "submitter": "Manohar Karki", "authors": "Manohar Karki, Robert DiBiano, Saikat Basu, Supratik Mukhopadhyay", "title": "Core Sampling Framework for Pixel Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The intermediate map responses of a Convolutional Neural Network (CNN)\ncontain information about an image that can be used to extract contextual\nknowledge about it. In this paper, we present a core sampling framework that is\nable to use these activation maps from several layers as features to another\nneural network using transfer learning to provide an understanding of an input\nimage. Our framework creates a representation that combines features from the\ntest data and the contextual knowledge gained from the responses of a\npretrained network, processes it and feeds it to a separate Deep Belief\nNetwork. We use this representation to extract more information from an image\nat the pixel level, hence gaining understanding of the whole image. We\nexperimentally demonstrate the usefulness of our framework using a pretrained\nVGG-16 model to perform segmentation on the BAERI dataset of Synthetic Aperture\nRadar(SAR) imagery and the CAMVID dataset.\n", "versions": [{"version": "v1", "created": "Tue, 6 Dec 2016 20:28:44 GMT"}], "update_date": "2016-12-07", "authors_parsed": [["Karki", "Manohar", ""], ["DiBiano", "Robert", ""], ["Basu", "Saikat", ""], ["Mukhopadhyay", "Supratik", ""]]}, {"id": "1612.01991", "submitter": "Mohammadreza Mostajabi", "authors": "Mohammadreza Mostajabi, Nicholas Kolkin, Gregory Shakhnarovich", "title": "Diverse Sampling for Self-Supervised Learning of Semantic Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an approach for learning category-level semantic segmentation\npurely from image-level classification tags indicating presence of categories.\nIt exploits localization cues that emerge from training classification-tasked\nconvolutional networks, to drive a \"self-supervision\" process that\nautomatically labels a sparse, diverse training set of points likely to belong\nto classes of interest. Our approach has almost no hyperparameters, is modular,\nand allows for very fast training of segmentation in less than 3 minutes. It\nobtains competitive results on the VOC 2012 segmentation benchmark. More,\nsignificantly the modularity and fast training of our framework allows new\nclasses to efficiently added for inference.\n", "versions": [{"version": "v1", "created": "Tue, 6 Dec 2016 20:54:18 GMT"}], "update_date": "2016-12-07", "authors_parsed": [["Mostajabi", "Mohammadreza", ""], ["Kolkin", "Nicholas", ""], ["Shakhnarovich", "Gregory", ""]]}, {"id": "1612.02095", "submitter": "Evan Racah Mr.", "authors": "Evan Racah, Christopher Beckham, Tegan Maharaj, Samira Ebrahimi Kahou,\n  Prabhat, Christopher Pal", "title": "ExtremeWeather: A large-scale climate dataset for semi-supervised\n  detection, localization, and understanding of extreme weather events", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Then detection and identification of extreme weather events in large-scale\nclimate simulations is an important problem for risk management, informing\ngovernmental policy decisions and advancing our basic understanding of the\nclimate system. Recent work has shown that fully supervised convolutional\nneural networks (CNNs) can yield acceptable accuracy for classifying well-known\ntypes of extreme weather events when large amounts of labeled data are\navailable. However, many different types of spatially localized climate\npatterns are of interest including hurricanes, extra-tropical cyclones, weather\nfronts, and blocking events among others. Existing labeled data for these\npatterns can be incomplete in various ways, such as covering only certain years\nor geographic areas and having false negatives. This type of climate data\ntherefore poses a number of interesting machine learning challenges. We present\na multichannel spatiotemporal CNN architecture for semi-supervised bounding box\nprediction and exploratory data analysis. We demonstrate that our approach is\nable to leverage temporal information and unlabeled data to improve the\nlocalization of extreme weather events. Further, we explore the representations\nlearned by our model in order to better understand this important data. We\npresent a dataset, ExtremeWeather, to encourage machine learning research in\nthis area and to help facilitate further work in understanding and mitigating\nthe effects of climate change. The dataset is available at\nextremeweatherdataset.github.io and the code is available at\nhttps://github.com/eracah/hur-detect.\n", "versions": [{"version": "v1", "created": "Wed, 7 Dec 2016 01:46:09 GMT"}, {"version": "v2", "created": "Sat, 25 Nov 2017 23:44:46 GMT"}], "update_date": "2017-11-28", "authors_parsed": [["Racah", "Evan", ""], ["Beckham", "Christopher", ""], ["Maharaj", "Tegan", ""], ["Kahou", "Samira Ebrahimi", ""], ["Prabhat", "", ""], ["Pal", "Christopher", ""]]}, {"id": "1612.02101", "submitter": "Ming-Ming Cheng Dr", "authors": "Qinbin Hou, Puneet Kumar Dokania, Daniela Massiceti, Yunchao Wei,\n  Ming-Ming Cheng, Philip Torr", "title": "Bottom-Up Top-Down Cues for Weakly-Supervised Semantic Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the task of learning a classifier for semantic segmentation using\nweak supervision in the form of image labels which specify the object classes\npresent in the image. Our method uses deep convolutional neural networks (CNNs)\nand adopts an Expectation-Maximization (EM) based approach. We focus on the\nfollowing three aspects of EM: (i) initialization; (ii) latent posterior\nestimation (E-step) and (iii) the parameter update (M-step). We show that\nsaliency and attention maps, our bottom-up and top-down cues respectively, of\nsimple images provide very good cues to learn an initialization for the\nEM-based algorithm. Intuitively, we show that before trying to learn to segment\ncomplex images, it is much easier and highly effective to first learn to\nsegment a set of simple images and then move towards the complex ones. Next, in\norder to update the parameters, we propose minimizing the combination of the\nstandard softmax loss and the KL divergence between the true latent posterior\nand the likelihood given by the CNN. We argue that this combination is more\nrobust to wrong predictions made by the expectation step of the EM method. We\nsupport this argument with empirical and visual results. Extensive experiments\nand discussions show that: (i) our method is very simple and intuitive; (ii)\nrequires only image-level labels; and (iii) consistently outperforms other\nweakly-supervised state-of-the-art methods with a very high margin on the\nPASCAL VOC 2012 dataset.\n", "versions": [{"version": "v1", "created": "Wed, 7 Dec 2016 02:37:51 GMT"}, {"version": "v2", "created": "Fri, 23 Dec 2016 03:51:04 GMT"}, {"version": "v3", "created": "Sun, 9 Apr 2017 08:32:03 GMT"}], "update_date": "2017-11-07", "authors_parsed": [["Hou", "Qinbin", ""], ["Dokania", "Puneet Kumar", ""], ["Massiceti", "Daniela", ""], ["Wei", "Yunchao", ""], ["Cheng", "Ming-Ming", ""], ["Torr", "Philip", ""]]}, {"id": "1612.02103", "submitter": "Ming-Ming Cheng Prof.", "authors": "Yun Liu, Ming-Ming Cheng, Xiaowei Hu, Kai Wang, Xiang Bai", "title": "Richer Convolutional Features for Edge Detection", "comments": null, "journal-ref": "IEEE TPAMI, 41(8):1939-1946, 2019", "doi": "10.1109/TPAMI.2018.2878849", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose an accurate edge detector using richer\nconvolutional features (RCF). Since objects in nature images have various\nscales and aspect ratios, the automatically learned rich hierarchical\nrepresentations by CNNs are very critical and effective to detect edges and\nobject boundaries. And the convolutional features gradually become coarser with\nreceptive fields increasing. Based on these observations, our proposed network\narchitecture makes full use of multiscale and multi-level information to\nperform the image-to-image edge prediction by combining all of the useful\nconvolutional features into a holistic framework. It is the first attempt to\nadopt such rich convolutional features in computer vision tasks. Using VGG16\nnetwork, we achieve \\sArt results on several available datasets. When\nevaluating on the well-known BSDS500 benchmark, we achieve ODS F-measure of\n\\textbf{.811} while retaining a fast speed (\\textbf{8} FPS). Besides, our fast\nversion of RCF achieves ODS F-measure of \\textbf{.806} with \\textbf{30} FPS.\n", "versions": [{"version": "v1", "created": "Wed, 7 Dec 2016 02:57:03 GMT"}, {"version": "v2", "created": "Sun, 9 Apr 2017 08:38:41 GMT"}, {"version": "v3", "created": "Wed, 3 Jul 2019 04:44:45 GMT"}], "update_date": "2019-07-04", "authors_parsed": [["Liu", "Yun", ""], ["Cheng", "Ming-Ming", ""], ["Hu", "Xiaowei", ""], ["Wang", "Kai", ""], ["Bai", "Xiang", ""]]}, {"id": "1612.02136", "submitter": "Yanran Li", "authors": "Tong Che, Yanran Li, Athul Paul Jacob, Yoshua Bengio, Wenjie Li", "title": "Mode Regularized Generative Adversarial Networks", "comments": "Published as a conference paper at ICLR 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although Generative Adversarial Networks achieve state-of-the-art results on\na variety of generative tasks, they are regarded as highly unstable and prone\nto miss modes. We argue that these bad behaviors of GANs are due to the very\nparticular functional shape of the trained discriminators in high dimensional\nspaces, which can easily make training stuck or push probability mass in the\nwrong direction, towards that of higher concentration than that of the data\ngenerating distribution. We introduce several ways of regularizing the\nobjective, which can dramatically stabilize the training of GAN models. We also\nshow that our regularizers can help the fair distribution of probability mass\nacross the modes of the data generating distribution, during the early phases\nof training and thus providing a unified solution to the missing modes problem.\n", "versions": [{"version": "v1", "created": "Wed, 7 Dec 2016 07:45:38 GMT"}, {"version": "v2", "created": "Fri, 9 Dec 2016 06:08:37 GMT"}, {"version": "v3", "created": "Sun, 18 Dec 2016 05:55:22 GMT"}, {"version": "v4", "created": "Mon, 20 Feb 2017 05:01:27 GMT"}, {"version": "v5", "created": "Thu, 2 Mar 2017 06:28:13 GMT"}], "update_date": "2017-03-03", "authors_parsed": [["Che", "Tong", ""], ["Li", "Yanran", ""], ["Jacob", "Athul Paul", ""], ["Bengio", "Yoshua", ""], ["Li", "Wenjie", ""]]}, {"id": "1612.02141", "submitter": "Aditya Balu", "authors": "Aditya Balu, Sambit Ghadai, Kin Gwn Lore, Gavin Young, Adarsh\n  Krishnamurthy and Soumik Sarkar", "title": "Learning Localized Geometric Features Using 3D-CNN: An Application to\n  Manufacturability Analysis of Drilled Holes", "comments": "9 Pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  3D convolutional neural networks (3D-CNN) have been used for object\nrecognition based on the voxelized shape of an object. In this paper, we\npresent a 3D-CNN based method to learn distinct local geometric features of\ninterest within an object. In this context, the voxelized representation may\nnot be sufficient to capture the distinguishing information about such local\nfeatures. To enable efficient learning, we augment the voxel data with surface\nnormals of the object boundary. We then train a 3D-CNN with this augmented data\nand identify the local features critical for decision-making using 3D\ngradient-weighted class activation maps. An application of this feature\nidentification framework is to recognize difficult-to-manufacture drilled hole\nfeatures in a complex CAD geometry. The framework can be extended to identify\ndifficult-to-manufacture features at multiple spatial scales leading to a\nreal-time decision support system for design for manufacturability.\n", "versions": [{"version": "v1", "created": "Wed, 7 Dec 2016 08:07:05 GMT"}, {"version": "v2", "created": "Thu, 22 Jun 2017 01:28:06 GMT"}], "update_date": "2017-07-18", "authors_parsed": [["Balu", "Aditya", ""], ["Ghadai", "Sambit", ""], ["Lore", "Kin Gwn", ""], ["Young", "Gavin", ""], ["Krishnamurthy", "Adarsh", ""], ["Sarkar", "Soumik", ""]]}, {"id": "1612.02155", "submitter": "Haroon Idrees", "authors": "Shayan Modiri Assari, Haroon Idrees and Mubarak Shah", "title": "Re-identification of Humans in Crowds using Personal, Social and\n  Environmental Constraints", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the problem of human re-identification across\nnon-overlapping cameras in crowds.Re-identification in crowded scenes is a\nchallenging problem due to large number of people and frequent occlusions,\ncoupled with changes in their appearance due to different properties and\nexposure of cameras. To solve this problem, we model multiple Personal, Social\nand Environmental (PSE) constraints on human motion across cameras. The\npersonal constraints include appearance and preferred speed of each individual\nassumed to be similar across the non-overlapping cameras. The social influences\n(constraints) are quadratic in nature, i.e. occur between pairs of individuals,\nand modeled through grouping and collision avoidance. Finally, the\nenvironmental constraints capture the transition probabilities between gates\n(entrances / exits) in different cameras, defined as multi-modal distributions\nof transition time and destination between all pairs of gates. We incorporate\nthese constraints into an energy minimization framework for solving human\nre-identification. Assigning $1-1$ correspondence while modeling PSE\nconstraints is NP-hard. We present a stochastic local search algorithm to\nrestrict the search space of hypotheses, and obtain $1-1$ solution in the\npresence of linear and quadratic PSE constraints. Moreover, we present an\nalternate optimization using Frank-Wolfe algorithm that solves the convex\napproximation of the objective function with linear relaxation on binary\nvariables, and yields an order of magnitude speed up over stochastic local\nsearch with minor drop in performance. We evaluate our approach using\nCumulative Matching Curves as well $1-1$ assignment on several thousand frames\nof Grand Central, PRID and DukeMTMC datasets, and obtain significantly better\nresults compared to existing re-identification methods.\n", "versions": [{"version": "v1", "created": "Wed, 7 Dec 2016 09:03:11 GMT"}], "update_date": "2016-12-08", "authors_parsed": [["Assari", "Shayan Modiri", ""], ["Idrees", "Haroon", ""], ["Shah", "Mubarak", ""]]}, {"id": "1612.02166", "submitter": "Dwarikanath Mahapatra", "authors": "Dwarikanath Mahapatra", "title": "Consensus Based Medical Image Segmentation Using Semi-Supervised\n  Learning And Graph Cuts", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Medical image segmentation requires consensus ground truth segmentations to\nbe derived from multiple expert annotations. A novel approach is proposed that\nobtains consensus segmentations from experts using graph cuts (GC) and semi\nsupervised learning (SSL). Popular approaches use iterative Expectation\nMaximization (EM) to estimate the final annotation and quantify annotator's\nperformance. Such techniques pose the risk of getting trapped in local minima.\nWe propose a self consistency (SC) score to quantify annotator consistency\nusing low level image features. SSL is used to predict missing annotations by\nconsidering global features and local image consistency. The SC score also\nserves as the penalty cost in a second order Markov random field (MRF) cost\nfunction optimized using graph cuts to derive the final consensus label. Graph\ncut obtains a global maximum without an iterative procedure. Experimental\nresults on synthetic images, real data of Crohn's disease patients and retinal\nimages show our final segmentation to be accurate and more consistent than\ncompeting methods.\n", "versions": [{"version": "v1", "created": "Wed, 7 Dec 2016 09:38:11 GMT"}, {"version": "v2", "created": "Fri, 16 Dec 2016 04:30:40 GMT"}, {"version": "v3", "created": "Mon, 21 May 2018 05:36:59 GMT"}], "update_date": "2018-05-22", "authors_parsed": [["Mahapatra", "Dwarikanath", ""]]}, {"id": "1612.02177", "submitter": "Seungjun Nah", "authors": "Seungjun Nah, Tae Hyun Kim, Kyoung Mu Lee", "title": "Deep Multi-scale Convolutional Neural Network for Dynamic Scene\n  Deblurring", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Non-uniform blind deblurring for general dynamic scenes is a challenging\ncomputer vision problem as blurs arise not only from multiple object motions\nbut also from camera shake, scene depth variation. To remove these complicated\nmotion blurs, conventional energy optimization based methods rely on simple\nassumptions such that blur kernel is partially uniform or locally linear.\nMoreover, recent machine learning based methods also depend on synthetic blur\ndatasets generated under these assumptions. This makes conventional deblurring\nmethods fail to remove blurs where blur kernel is difficult to approximate or\nparameterize (e.g. object motion boundaries). In this work, we propose a\nmulti-scale convolutional neural network that restores sharp images in an\nend-to-end manner where blur is caused by various sources. Together, we present\nmulti-scale loss function that mimics conventional coarse-to-fine approaches.\nFurthermore, we propose a new large-scale dataset that provides pairs of\nrealistic blurry image and the corresponding ground truth sharp image that are\nobtained by a high-speed camera. With the proposed model trained on this\ndataset, we demonstrate empirically that our method achieves the\nstate-of-the-art performance in dynamic scene deblurring not only\nqualitatively, but also quantitatively.\n", "versions": [{"version": "v1", "created": "Wed, 7 Dec 2016 10:08:33 GMT"}, {"version": "v2", "created": "Mon, 7 May 2018 06:15:57 GMT"}], "update_date": "2018-05-08", "authors_parsed": [["Nah", "Seungjun", ""], ["Kim", "Tae Hyun", ""], ["Lee", "Kyoung Mu", ""]]}, {"id": "1612.02183", "submitter": "Wim Abbeloos", "authors": "Wim Abbeloos, Toon Goedem\\'e", "title": "Fusion of Range and Thermal Images for Person Detection", "comments": "VII International Conference on Electrical Engineering FIE 2014,\n  Santiago de Cuba", "journal-ref": "Proceedings Conferencia Internacional de Ingenier\\'ia El\\'ectrica\n  7 (2014) 1-4", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Detecting people in images is a challenging problem. Differences in pose,\nclothing and lighting, along with other factors, cause a lot of variation in\ntheir appearance. To overcome these issues, we propose a system based on fused\nrange and thermal infrared images. These measurements show considerably less\nvariation and provide more meaningful information. We provide a brief\nintroduction to the sensor technology used and propose a calibration method.\nSeveral data fusion algorithms are compared and their performance is assessed\non a simulated data set. The results of initial experiments on real data are\nanalyzed and the measurement errors and the challenges they present are\ndiscussed. The resulting fused data are used to efficiently detect people in a\nfixed camera set-up. The system is extended to include person tracking.\n", "versions": [{"version": "v1", "created": "Wed, 7 Dec 2016 10:29:54 GMT"}], "update_date": "2016-12-08", "authors_parsed": [["Abbeloos", "Wim", ""], ["Goedem\u00e9", "Toon", ""]]}, {"id": "1612.02184", "submitter": "Roey Mechrez", "authors": "Roey Mechrez, Eli Shechtman and Lihi Zelnik-Manor", "title": "Saliency Driven Image Manipulation", "comments": "to appear in WACV'18", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Have you ever taken a picture only to find out that an unimportant background\nobject ended up being overly salient? Or one of those team sports photos where\nyour favorite player blends with the rest? Wouldn't it be nice if you could\ntweak these pictures just a little bit so that the distractor would be\nattenuated and your favorite player will stand-out among her peers?\nManipulating images in order to control the saliency of objects is the goal of\nthis paper. We propose an approach that considers the internal color and\nsaliency properties of the image. It changes the saliency map via an\noptimization framework that relies on patch-based manipulation using only\npatches from within the same image to achieve realistic looking results.\nApplications include object enhancement, distractors attenuation and background\ndecluttering. Comparing our method to previous ones shows significant\nimprovement, both in the achieved saliency manipulation and in the realistic\nappearance of the resulting images.\n", "versions": [{"version": "v1", "created": "Wed, 7 Dec 2016 10:30:44 GMT"}, {"version": "v2", "created": "Thu, 16 Mar 2017 09:57:34 GMT"}, {"version": "v3", "created": "Wed, 17 Jan 2018 07:34:12 GMT"}], "update_date": "2018-01-18", "authors_parsed": [["Mechrez", "Roey", ""], ["Shechtman", "Eli", ""], ["Zelnik-Manor", "Lihi", ""]]}, {"id": "1612.02190", "submitter": "Roey Mechrez", "authors": "Itamar Talmi, Roey Mechrez and Lihi Zelnik-Manor", "title": "Template Matching with Deformable Diversity Similarity", "comments": "accepted to CVPR2017 (spotlight)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel measure for template matching named Deformable Diversity\nSimilarity -- based on the diversity of feature matches between a target image\nwindow and the template. We rely on both local appearance and geometric\ninformation that jointly lead to a powerful approach for matching. Our key\ncontribution is a similarity measure, that is robust to complex deformations,\nsignificant background clutter, and occlusions. Empirical evaluation on the\nmost up-to-date benchmark shows that our method outperforms the current\nstate-of-the-art in its detection accuracy while improving computational\ncomplexity.\n", "versions": [{"version": "v1", "created": "Wed, 7 Dec 2016 10:43:55 GMT"}, {"version": "v2", "created": "Tue, 18 Apr 2017 12:03:06 GMT"}], "update_date": "2017-04-19", "authors_parsed": [["Talmi", "Itamar", ""], ["Mechrez", "Roey", ""], ["Zelnik-Manor", "Lihi", ""]]}, {"id": "1612.02203", "submitter": "Enrique S\\'anchez Lozano", "authors": "Enrique S\\'anchez-Lozano, Georgios Tzimiropoulos, Brais Martinez,\n  Fernando De la Torre and Michel Valstar", "title": "A Functional Regression approach to Facial Landmark Tracking", "comments": "Accepted at IEEE TPAMI. This is authors' version. 0162-8828\n  \\copyright 2017 IEEE. Personal use is permitted, but\n  republication/redistribution requires IEEE permission. See\n  http://www.ieee.org/publications_standards/publications/rights/index.html for\n  more information", "journal-ref": "IEEE Transactions on Pattern Analysis and Machine Intelligence,\n  2017", "doi": "10.1109/TPAMI.2017.2745568", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Linear regression is a fundamental building block in many face detection and\ntracking algorithms, typically used to predict shape displacements from image\nfeatures through a linear mapping. This paper presents a Functional Regression\nsolution to the least squares problem, which we coin Continuous Regression,\nresulting in the first real-time incremental face tracker. Contrary to prior\nwork in Functional Regression, in which B-splines or Fourier series were used,\nwe propose to approximate the input space by its first-order Taylor expansion,\nyielding a closed-form solution for the continuous domain of displacements. We\nthen extend the continuous least squares problem to correlated variables, and\ndemonstrate the generalisation of our approach. We incorporate Continuous\nRegression into the cascaded regression framework, and show its computational\nbenefits for both training and testing. We then present a fast approach for\nincremental learning within Cascaded Continuous Regression, coined iCCR, and\nshow that its complexity allows real-time face tracking, being 20 times faster\nthan the state of the art. To the best of our knowledge, this is the first\nincremental face tracker that is shown to operate in real-time. We show that\niCCR achieves state-of-the-art performance on the 300-VW dataset, the most\nrecent, large-scale benchmark for face tracking.\n", "versions": [{"version": "v1", "created": "Wed, 7 Dec 2016 11:34:36 GMT"}, {"version": "v2", "created": "Wed, 20 Sep 2017 15:58:32 GMT"}], "update_date": "2017-09-21", "authors_parsed": [["S\u00e1nchez-Lozano", "Enrique", ""], ["Tzimiropoulos", "Georgios", ""], ["Martinez", "Brais", ""], ["De la Torre", "Fernando", ""], ["Valstar", "Michel", ""]]}, {"id": "1612.02218", "submitter": "Wim Abbeloos", "authors": "Stef Van Wolputte, Wim Abbeloos, Stijn Helsen, Abdellatif\n  Bey-Temsamani, Toon Goedem\\'e", "title": "Embedded Line Scan Image Sensors: The Low Cost Alternative for High\n  Speed Imaging", "comments": "2015 International Conference on Image Processing Theory, Tools and\n  Applications (IPTA)", "journal-ref": "Proceedings of the International Conference on Image Processing\n  Theory, Tools and Applications (2015) 543-549", "doi": "10.1109/IPTA.2015.7367207", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose a low-cost high-speed imaging line scan system. We\nreplace an expensive industrial line scan camera and illumination with a\ncustom-built set-up of cheap off-the-shelf components, yielding a measurement\nsystem with comparative quality while costing about 20 times less. We use a\nlow-cost linear (1D) image sensor, cheap optics including a LED-based or\nLASER-based lighting and an embedded platform to process the images. A\nstep-by-step method to design such a custom high speed imaging system and\nselect proper components is proposed. Simulations allowing to predict the final\nimage quality to be obtained by the set-up has been developed. Finally, we\napplied our method in a lab, closely representing the real-life cases. Our\nresults shows that our simulations are very accurate and that our low-cost line\nscan set-up acquired image quality compared to the high-end commercial vision\nsystem, for a fraction of the price.\n", "versions": [{"version": "v1", "created": "Wed, 7 Dec 2016 12:27:26 GMT"}], "update_date": "2016-12-08", "authors_parsed": [["Van Wolputte", "Stef", ""], ["Abbeloos", "Wim", ""], ["Helsen", "Stijn", ""], ["Bey-Temsamani", "Abdellatif", ""], ["Goedem\u00e9", "Toon", ""]]}, {"id": "1612.02219", "submitter": "Wim Abbeloos", "authors": "Matthias Faes, Wim Abbeloos, Frederik Vogeler, Hans Valkenaers, Kurt\n  Coppens, Toon Goedem\\'e, Eleonora Ferraris", "title": "Process Monitoring of Extrusion Based 3D Printing via Laser Scanning", "comments": "International Conference on Polymers and Moulds Innovations(PMI) 2014", "journal-ref": "Conference Proceedings PMI 6 (2014) 363-367", "doi": "10.13140/2.1.5175.0081", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Extrusion based 3D Printing (E3DP) is an Additive Manufacturing (AM)\ntechnique that extrudes thermoplastic polymer in order to build up components\nusing a layerwise approach. Hereby, AM typically requires long production times\nin comparison to mass production processes such as Injection Molding. Failures\nduring the AM process are often only noticed after build completion and\nfrequently lead to part rejection because of dimensional inaccuracy or lack of\nmechanical performance, resulting in an important loss of time and material. A\nsolution to improve the accuracy and robustness of a manufacturing technology\nis the integration of sensors to monitor and control process state-variables\nonline. In this way, errors can be rapidly detected and possibly compensated at\nan early stage. To achieve this, we integrated a modular 2D laser triangulation\nscanner into an E3DP machine and analyzed feedback signals. A 2D laser\ntriangulation scanner was selected here owing to the very compact size,\nachievable accuracy and the possibility of capturing geometrical 3D data. Thus,\nour implemented system is able to provide both quantitative and qualitative\ninformation. Also, in this work, first steps towards the development of a\nquality control loop for E3DP processes are presented and opportunities are\ndiscussed.\n", "versions": [{"version": "v1", "created": "Wed, 7 Dec 2016 12:29:46 GMT"}], "update_date": "2016-12-08", "authors_parsed": [["Faes", "Matthias", ""], ["Abbeloos", "Wim", ""], ["Vogeler", "Frederik", ""], ["Valkenaers", "Hans", ""], ["Coppens", "Kurt", ""], ["Goedem\u00e9", "Toon", ""], ["Ferraris", "Eleonora", ""]]}, {"id": "1612.02223", "submitter": "Wim Abbeloos", "authors": "Wim Abbeloos, Toon Goedem\\'e", "title": "Exploring the potential of combining time of flight and thermal infrared\n  cameras for person detection", "comments": "Proceedings of the 10th International Conference on Informatics in\n  Control, Automation and Robotics", "journal-ref": "Proceedings of the International Conference on Informatics in\n  Control, Automation and Robotics (2013) 464-470", "doi": "10.5220/0004595704640470", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Combining new, low-cost thermal infrared and time-of-flight range sensors\nprovides new opportunities. In this position paper we explore the possibilities\nof combining these sensors and using their fused data for person detection. The\nproposed calibration approach for this sensor combination differs from the\ntraditional stereo camera calibration in two fundamental ways. A first\ndistinction is that the spectral sensitivity of the two sensors differs\nsignificantly. In fact, there is no sensitivity range overlap at all. A second\ndistinction is that their resolution is typically very low, which requires\nspecial attention. We assume a situation in which the sensors' relative\nposition is known, but their orientation is unknown. In addition, some of the\ntypical measurement errors are discussed, and methods to compensate for them\nare proposed. We discuss how the fused data could allow increased accuracy and\nrobustness without the need for complex algorithms requiring large amounts of\ncomputational power and training data.\n", "versions": [{"version": "v1", "created": "Wed, 7 Dec 2016 12:35:45 GMT"}], "update_date": "2016-12-08", "authors_parsed": [["Abbeloos", "Wim", ""], ["Goedem\u00e9", "Toon", ""]]}, {"id": "1612.02287", "submitter": "Frank Michel", "authors": "Frank Michel, Alexander Kirillov, Eric Brachmann, Alexander Krull,\n  Stefan Gumhold, Bogdan Savchynskyy, Carsten Rother", "title": "Global Hypothesis Generation for 6D Object Pose Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the task of estimating the 6D pose of a known 3D object\nfrom a single RGB-D image. Most modern approaches solve this task in three\nsteps: i) Compute local features; ii) Generate a pool of pose-hypotheses; iii)\nSelect and refine a pose from the pool. This work focuses on the second step.\nWhile all existing approaches generate the hypotheses pool via local reasoning,\ne.g. RANSAC or Hough-voting, we are the first to show that global reasoning is\nbeneficial at this stage. In particular, we formulate a novel fully-connected\nConditional Random Field (CRF) that outputs a very small number of\npose-hypotheses. Despite the potential functions of the CRF being non-Gaussian,\nwe give a new and efficient two-step optimization procedure, with some\nguarantees for optimality. We utilize our global hypotheses generation\nprocedure to produce results that exceed state-of-the-art for the challenging\n\"Occluded Object Dataset\".\n", "versions": [{"version": "v1", "created": "Wed, 7 Dec 2016 15:23:12 GMT"}, {"version": "v2", "created": "Thu, 8 Dec 2016 08:50:37 GMT"}, {"version": "v3", "created": "Mon, 2 Jan 2017 09:09:03 GMT"}], "update_date": "2017-01-03", "authors_parsed": [["Michel", "Frank", ""], ["Kirillov", "Alexander", ""], ["Brachmann", "Eric", ""], ["Krull", "Alexander", ""], ["Gumhold", "Stefan", ""], ["Savchynskyy", "Bogdan", ""], ["Rother", "Carsten", ""]]}, {"id": "1612.02297", "submitter": "Michael Figurnov", "authors": "Michael Figurnov, Maxwell D. Collins, Yukun Zhu, Li Zhang, Jonathan\n  Huang, Dmitry Vetrov, Ruslan Salakhutdinov", "title": "Spatially Adaptive Computation Time for Residual Networks", "comments": "CVPR 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a deep learning architecture based on Residual Network\nthat dynamically adjusts the number of executed layers for the regions of the\nimage. This architecture is end-to-end trainable, deterministic and\nproblem-agnostic. It is therefore applicable without any modifications to a\nwide range of computer vision problems such as image classification, object\ndetection and image segmentation. We present experimental results showing that\nthis model improves the computational efficiency of Residual Networks on the\nchallenging ImageNet classification and COCO object detection datasets.\nAdditionally, we evaluate the computation time maps on the visual saliency\ndataset cat2000 and find that they correlate surprisingly well with human eye\nfixation positions.\n", "versions": [{"version": "v1", "created": "Wed, 7 Dec 2016 15:37:57 GMT"}, {"version": "v2", "created": "Sun, 2 Jul 2017 17:49:27 GMT"}], "update_date": "2017-07-04", "authors_parsed": [["Figurnov", "Michael", ""], ["Collins", "Maxwell D.", ""], ["Zhu", "Yukun", ""], ["Zhang", "Li", ""], ["Huang", "Jonathan", ""], ["Vetrov", "Dmitry", ""], ["Salakhutdinov", "Ruslan", ""]]}, {"id": "1612.02335", "submitter": "Yu-Chuan Su", "authors": "Yu-Chuan Su, Dinesh Jayaraman, Kristen Grauman", "title": "Pano2Vid: Automatic Cinematography for Watching 360$^{\\circ}$ Videos", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce the novel task of Pano2Vid $-$ automatic cinematography in\npanoramic 360$^{\\circ}$ videos. Given a 360$^{\\circ}$ video, the goal is to\ndirect an imaginary camera to virtually capture natural-looking normal\nfield-of-view (NFOV) video. By selecting \"where to look\" within the panorama at\neach time step, Pano2Vid aims to free both the videographer and the end viewer\nfrom the task of determining what to watch. Towards this goal, we first compile\na dataset of 360$^{\\circ}$ videos downloaded from the web, together with\nhuman-edited NFOV camera trajectories to facilitate evaluation. Next, we\npropose AutoCam, a data-driven approach to solve the Pano2Vid task. AutoCam\nleverages NFOV web video to discriminatively identify space-time \"glimpses\" of\ninterest at each time instant, and then uses dynamic programming to select\noptimal human-like camera trajectories. Through experimental evaluation on\nmultiple newly defined Pano2Vid performance measures against several baselines,\nwe show that our method successfully produces informative videos that could\nconceivably have been captured by human videographers.\n", "versions": [{"version": "v1", "created": "Wed, 7 Dec 2016 17:20:09 GMT"}], "update_date": "2016-12-08", "authors_parsed": [["Su", "Yu-Chuan", ""], ["Jayaraman", "Dinesh", ""], ["Grauman", "Kristen", ""]]}, {"id": "1612.02372", "submitter": "Jia Xue", "authors": "Jia Xue, Hang Zhang, Kristin Dana, Ko Nishino", "title": "Differential Angular Imaging for Material Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Material recognition for real-world outdoor surfaces has become increasingly\nimportant for computer vision to support its operation \"in the wild.\"\nComputational surface modeling that underlies material recognition has\ntransitioned from reflectance modeling using in-lab controlled radiometric\nmeasurements to image-based representations based on internet-mined images of\nmaterials captured in the scene. We propose to take a middle-ground approach\nfor material recognition that takes advantage of both rich radiometric cues and\nflexible image capture. We realize this by developing a framework for\ndifferential angular imaging, where small angular variations in image capture\nprovide an enhanced appearance representation and significant recognition\nimprovement. We build a large-scale material database, Ground Terrain in\nOutdoor Scenes (GTOS) database, geared towards real use for autonomous agents.\nThe database consists of over 30,000 images covering 40 classes of outdoor\nground terrain under varying weather and lighting conditions. We develop a\nnovel approach for material recognition called a Differential Angular Imaging\nNetwork (DAIN) to fully leverage this large dataset. With this novel network\narchitecture, we extract characteristics of materials encoded in the angular\nand spatial gradients of their appearance. Our results show that DAIN achieves\nrecognition performance that surpasses single view or coarsely quantized\nmultiview images. These results demonstrate the effectiveness of differential\nangular imaging as a means for flexible, in-place material recognition.\n", "versions": [{"version": "v1", "created": "Wed, 7 Dec 2016 18:59:19 GMT"}, {"version": "v2", "created": "Fri, 14 Jul 2017 00:43:10 GMT"}], "update_date": "2017-07-17", "authors_parsed": [["Xue", "Jia", ""], ["Zhang", "Hang", ""], ["Dana", "Kristin", ""], ["Nishino", "Ko", ""]]}, {"id": "1612.02374", "submitter": "Shashank Jaiswal", "authors": "Shashank Jaiswal, Michel Valstar, Alinda Gillott, David Daley", "title": "Automatic Detection of ADHD and ASD from Expressive Behaviour in RGBD\n  Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Attention Deficit Hyperactivity Disorder (ADHD) and Autism Spectrum Disorder\n(ASD) are neurodevelopmental conditions which impact on a significant number of\nchildren and adults. Currently, the diagnosis of such disorders is done by\nexperts who employ standard questionnaires and look for certain behavioural\nmarkers through manual observation. Such methods for their diagnosis are not\nonly subjective, difficult to repeat, and costly but also extremely time\nconsuming. In this work, we present a novel methodology to aid diagnostic\npredictions about the presence/absence of ADHD and ASD by automatic visual\nanalysis of a person's behaviour. To do so, we conduct the questionnaires in a\ncomputer-mediated way while recording participants with modern RGBD\n(Colour+Depth) sensors. In contrast to previous automatic approaches which have\nfocussed only detecting certain behavioural markers, our approach provides a\nfully automatic end-to-end system for directly predicting ADHD and ASD in\nadults. Using state of the art facial expression analysis based on Dynamic Deep\nLearning and 3D analysis of behaviour, we attain classification rates of 96%\nfor Controls vs Condition (ADHD/ASD) group and 94% for Comorbid (ADHD+ASD) vs\nASD only group. We show that our system is a potentially useful time saving\ncontribution to the diagnostic field of ADHD and ASD.\n", "versions": [{"version": "v1", "created": "Wed, 7 Dec 2016 19:01:17 GMT"}], "update_date": "2016-12-08", "authors_parsed": [["Jaiswal", "Shashank", ""], ["Valstar", "Michel", ""], ["Gillott", "Alinda", ""], ["Daley", "David", ""]]}, {"id": "1612.02401", "submitter": "Benjamin Ummenhofer", "authors": "Benjamin Ummenhofer, Huizhong Zhou, Jonas Uhrig, Nikolaus Mayer, Eddy\n  Ilg, Alexey Dosovitskiy, Thomas Brox", "title": "DeMoN: Depth and Motion Network for Learning Monocular Stereo", "comments": "Camera ready version for CVPR 2017. Supplementary material included.\n  Project page:\n  http://lmb.informatik.uni-freiburg.de/people/ummenhof/depthmotionnet/", "journal-ref": null, "doi": "10.1109/CVPR.2017.596", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we formulate structure from motion as a learning problem. We\ntrain a convolutional network end-to-end to compute depth and camera motion\nfrom successive, unconstrained image pairs. The architecture is composed of\nmultiple stacked encoder-decoder networks, the core part being an iterative\nnetwork that is able to improve its own predictions. The network estimates not\nonly depth and motion, but additionally surface normals, optical flow between\nthe images and confidence of the matching. A crucial component of the approach\nis a training loss based on spatial relative differences. Compared to\ntraditional two-frame structure from motion methods, results are more accurate\nand more robust. In contrast to the popular depth-from-single-image networks,\nDeMoN learns the concept of matching and, thus, better generalizes to\nstructures not seen during training.\n", "versions": [{"version": "v1", "created": "Wed, 7 Dec 2016 20:26:53 GMT"}, {"version": "v2", "created": "Tue, 11 Apr 2017 09:14:10 GMT"}], "update_date": "2018-01-18", "authors_parsed": [["Ummenhofer", "Benjamin", ""], ["Zhou", "Huizhong", ""], ["Uhrig", "Jonas", ""], ["Mayer", "Nikolaus", ""], ["Ilg", "Eddy", ""], ["Dosovitskiy", "Alexey", ""], ["Brox", "Thomas", ""]]}, {"id": "1612.02493", "submitter": "XiaoJie Shi", "authors": "Xiaojie Shi, Yijun Shao", "title": "Research on the Multiple Feature Fusion Image Retrieval Algorithm based\n  on Texture Feature and Rough Set Theory", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, we have witnessed the explosive growth of images with complex\ninformation and content. In order to effectively and precisely retrieve desired\nimages from a large-scale image database with low time-consuming, we propose\nthe multiple feature fusion image retrieval algorithm based on the texture\nfeature and rough set theory in this paper. In contrast to the conventional\napproaches that only use the single feature or standard, we fuse the different\nfeatures with operation of normalization. The rough set theory will assist us\nto enhance the robustness of retrieval system when facing with incomplete data\nwarehouse. To enhance the texture extraction paradigm, we use the wavelet Gabor\nfunction that holds better robustness. In addition, from the perspectives of\nthe internal and external normalization, we re-organize extracted feature with\nthe better combination. The numerical experiment has verified general\nfeasibility of our methodology. We enhance the overall accuracy compared with\nthe other state-of-the-art algorithms.\n", "versions": [{"version": "v1", "created": "Thu, 8 Dec 2016 00:08:23 GMT"}], "update_date": "2016-12-09", "authors_parsed": [["Shi", "Xiaojie", ""], ["Shao", "Yijun", ""]]}, {"id": "1612.02498", "submitter": "Odemir Bruno PhD", "authors": "Jo\\~ao B. Florindo, Odemir M. Bruno", "title": "Discrete Schroedinger Transform For Texture Recognition", "comments": "15 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV physics.data-an", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work presents a new procedure to extract features of grey-level texture\nimages based on the discrete Schroedinger transform. This is a non-linear\ntransform where the image is mapped as the initial probability distribution of\na wave function and such distribution evolves in time following the\nSchroedinger equation from Quantum Mechanics. The features are provided by\nstatistical moments of the distribution measured at different times. The\nproposed method is applied to the classification of three databases of textures\nused for benchmark and compared to other well-known texture descriptors in the\nliterature, such as textons, local binary patterns, multifractals, among\nothers. All of them are outperformed by the proposed method in terms of\npercentage of images correctly classified. The proposal is also applied to the\nidentification of plant species using scanned images of leaves and again it\noutperforms other texture methods. A test with images affected by Gaussian and\n\"salt \\& pepper\" noise is also carried out, also with the best performance\nachieved by the Schroedinger descriptors.\n", "versions": [{"version": "v1", "created": "Thu, 8 Dec 2016 00:49:18 GMT"}], "update_date": "2016-12-09", "authors_parsed": [["Florindo", "Jo\u00e3o B.", ""], ["Bruno", "Odemir M.", ""]]}, {"id": "1612.02513", "submitter": "Hang Duong Viet", "authors": "Viet-Hang Duong, Yuan-Shan Lee, Bach-Tung Pham, Seksan\n  Mathulaprangsan, Pham The Bao, and Jia-Ching Wang", "title": "Complex Matrix Factorization for Face Recognition", "comments": "4 pages,3 figures,4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work developed novel complex matrix factorization methods for face\nrecognition; the methods were complex matrix factorization (CMF), sparse\ncomplex matrix factorization (SpaCMF), and graph complex matrix factorization\n(GraCMF). After real-valued data are transformed into a complex field, the\ncomplex-valued matrix will be decomposed into two matrices of bases and\ncoefficients, which are derived from solutions to an optimization problem in a\ncomplex domain. The generated objective function is the real-valued function of\nthe reconstruction error, which produces a parametric description. Factorizing\nthe matrix of complex entries directly transformed the constrained optimization\nproblem into an unconstrained optimization problem. Additionally, a complex\nvector space with N dimensions can be regarded as a 2N-dimensional real vector\nspace. Accordingly, all real analytic properties can be exploited in the\ncomplex field. The ability to exploit these important characteristics motivated\nthe development herein of a simpler framework that can provide better\nrecognition results. The effectiveness of this framework will be clearly\nelucidated in later sections in this paper.\n", "versions": [{"version": "v1", "created": "Thu, 8 Dec 2016 02:42:20 GMT"}], "update_date": "2016-12-09", "authors_parsed": [["Duong", "Viet-Hang", ""], ["Lee", "Yuan-Shan", ""], ["Pham", "Bach-Tung", ""], ["Mathulaprangsan", "Seksan", ""], ["Bao", "Pham The", ""], ["Wang", "Jia-Ching", ""]]}, {"id": "1612.02521", "submitter": "Kaihua Zhang", "authors": "Huihui Song and Yuhui Zheng and Kaihua Zhang", "title": "An Efficient Algorithm for the Piecewise-Smooth Model with Approximately\n  Explicit Solutions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents an efficient approach to image segmentation that\napproximates the piecewise-smooth (PS) functional in [12] with explicit\nsolutions. By rendering some rational constraints on the initial conditions and\nthe final solutions of the PS functional, we propose two novel formulations\nwhich can be approximated to be the explicit solutions of the evolution partial\ndifferential equations (PDEs) of the PS model, in which only one PDE needs to\nbe solved efficiently. Furthermore, an energy term that regularizes the level\nset function to be a signed distance function is incorporated into our\nevolution formulation, and the time-consuming re-initialization is avoided.\nExperiments on synthetic and real images show that our method is more efficient\nthan both the PS model and the local binary fitting (LBF) model [4], while\nhaving similar segmentation accuracy as the LBF model.\n", "versions": [{"version": "v1", "created": "Thu, 8 Dec 2016 03:25:59 GMT"}], "update_date": "2016-12-09", "authors_parsed": [["Song", "Huihui", ""], ["Zheng", "Yuhui", ""], ["Zhang", "Kaihua", ""]]}, {"id": "1612.02534", "submitter": "Xiaofang Wang", "authors": "Xiaofang Wang, Kris M. Kitani and Martial Hebert", "title": "Contextual Visual Similarity", "comments": "Submitted to CVPR 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Measuring visual similarity is critical for image understanding. But what\nmakes two images similar? Most existing work on visual similarity assumes that\nimages are similar because they contain the same object instance or category.\nHowever, the reason why images are similar is much more complex. For example,\nfrom the perspective of category, a black dog image is similar to a white dog\nimage. However, in terms of color, a black dog image is more similar to a black\nhorse image than the white dog image. This example serves to illustrate that\nvisual similarity is ambiguous but can be made precise when given an explicit\ncontextual perspective. Based on this observation, we propose the concept of\ncontextual visual similarity. To be concrete, we examine the concept of\ncontextual visual similarity in the application domain of image search. Instead\nof providing only a single image for image similarity search (\\eg, Google image\nsearch), we require three images. Given a query image, a second positive image\nand a third negative image, dissimilar to the first two images, we define a\ncontextualized similarity search criteria. In particular, we learn feature\nweights over all the feature dimensions of each image such that the distance\nbetween the query image and the positive image is small and their distances to\nthe negative image are large after reweighting their features. The learned\nfeature weights encode the contextualized visual similarity specified by the\nuser and can be used for attribute specific image search. We also show the\nusefulness of our contextualized similarity weighting scheme for different\ntasks, such as answering visual analogy questions and unsupervised attribute\ndiscovery.\n", "versions": [{"version": "v1", "created": "Thu, 8 Dec 2016 05:53:16 GMT"}], "update_date": "2016-12-09", "authors_parsed": [["Wang", "Xiaofang", ""], ["Kitani", "Kris M.", ""], ["Hebert", "Martial", ""]]}, {"id": "1612.02541", "submitter": "Yuxin Peng", "authors": "Jian Zhang and Yuxin Peng", "title": "Query-adaptive Image Retrieval by Deep Weighted Hashing", "comments": "13 pages, submitted to IEEE Transactions On Multimedia", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hashing methods have attracted much attention for large scale image\nretrieval. Some deep hashing methods have achieved promising results by taking\nadvantage of the strong representation power of deep networks recently.\nHowever, existing deep hashing methods treat all hash bits equally. On one\nhand, a large number of images share the same distance to a query image due to\nthe discrete Hamming distance, which raises a critical issue of image retrieval\nwhere fine-grained rankings are very important. On the other hand, different\nhash bits actually contribute to the image retrieval differently, and treating\nthem equally greatly affects the retrieval accuracy of image. To address the\nabove two problems, we propose the query-adaptive deep weighted hashing (QaDWH)\napproach, which can perform fine-grained ranking for different queries by\nweighted Hamming distance. First, a novel deep hashing network is proposed to\nlearn the hash codes and corresponding class-wise weights jointly, so that the\nlearned weights can reflect the importance of different hash bits for different\nimage classes. Second, a query-adaptive image retrieval method is proposed,\nwhich rapidly generates hash bit weights for different query images by fusing\nits semantic probability and the learned class-wise weights. Fine-grained image\nretrieval is then performed by the weighted Hamming distance, which can provide\nmore accurate ranking than the traditional Hamming distance. Experiments on\nfour widely used datasets show that the proposed approach outperforms eight\nstate-of-the-art hashing methods.\n", "versions": [{"version": "v1", "created": "Thu, 8 Dec 2016 06:20:03 GMT"}, {"version": "v2", "created": "Tue, 9 May 2017 02:40:20 GMT"}], "update_date": "2017-05-10", "authors_parsed": [["Zhang", "Jian", ""], ["Peng", "Yuxin", ""]]}, {"id": "1612.02559", "submitter": "Mandar Dixit", "authors": "Mandar Dixit, Roland Kwitt, Marc Niethammer, Nuno Vasconcelos", "title": "AGA: Attribute Guided Augmentation", "comments": "CVPR 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of data augmentation, i.e., generating artificial\nsamples to extend a given corpus of training data. Specifically, we propose\nattributed-guided augmentation (AGA) which learns a mapping that allows to\nsynthesize data such that an attribute of a synthesized sample is at a desired\nvalue or strength. This is particularly interesting in situations where little\ndata with no attribute annotation is available for learning, but we have access\nto a large external corpus of heavily annotated samples. While prior works\nprimarily augment in the space of images, we propose to perform augmentation in\nfeature space instead. We implement our approach as a deep encoder-decoder\narchitecture that learns the synthesis function in an end-to-end manner. We\ndemonstrate the utility of our approach on the problems of (1) one-shot object\nrecognition in a transfer-learning setting where we have no prior knowledge of\nthe new classes, as well as (2) object-based one-shot scene recognition. As\nexternal data, we leverage 3D depth and pose information from the SUN RGB-D\ndataset. Our experiments show that attribute-guided augmentation of high-level\nCNN features considerably improves one-shot recognition performance on both\nproblems.\n", "versions": [{"version": "v1", "created": "Thu, 8 Dec 2016 08:23:09 GMT"}, {"version": "v2", "created": "Sat, 26 Aug 2017 19:52:46 GMT"}], "update_date": "2017-08-29", "authors_parsed": [["Dixit", "Mandar", ""], ["Kwitt", "Roland", ""], ["Niethammer", "Marc", ""], ["Vasconcelos", "Nuno", ""]]}, {"id": "1612.02562", "submitter": "Ioannis Papavasileiou", "authors": "Ioannis Papavasileiou (1), Wenlong Zhang (2), Xin Wang (3), Jinbo Bi\n  (1), Li Zhang (4) and Song Han (1) ((1) University of Connecticut, (2)\n  Arizona State University, (3) Philips Research North America, (4) Nanjing\n  Brain Hospital, P. R. China)", "title": "Classification of Neurological Gait Disorders Using Multi-task Feature\n  Learning", "comments": "shorter version of this paper is submitted to CHASE '17 conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As our population ages, neurological impairments and degeneration of the\nmusculoskeletal system yield gait abnormalities, which can significantly reduce\nquality of life. Gait rehabilitative therapy has been widely adopted to help\npatients maximize community participation and living independence. To further\nimprove the precision and efficiency of rehabilitative therapy, more objective\nmethods need to be developed based on sensory data. In this paper, an\nalgorithmic framework is proposed to provide classification of gait disorders\ncaused by two common neurological diseases, stroke and Parkinson's Disease\n(PD), from ground contact force (GCF) data. An advanced machine learning\nmethod, multi-task feature learning (MTFL), is used to jointly train\nclassification models of a subject's gait in three classes, post-stroke, PD and\nhealthy gait. Gait parameters related to mobility, balance, strength and rhythm\nare used as features for the classification. Out of all the features used, the\nMTFL models capture the more important ones per disease, which will help\nprovide better objective assessment and therapy progress tracking. To evaluate\nthe proposed methodology we use data from a human participant study, which\nincludes five PD patients, three post-stroke patients, and three healthy\nsubjects. Despite the diversity of abnormalities, the evaluation shows that the\nproposed approach can successfully distinguish post-stroke and PD gait from\nhealthy gait, as well as post-stroke from PD gait, with Area Under the Curve\n(AUC) score of at least 0.96. Moreover, the methodology helps select important\ngait features to better understand the key characteristics that distinguish\nabnormal gaits and design personalized treatment.\n", "versions": [{"version": "v1", "created": "Thu, 8 Dec 2016 08:43:42 GMT"}, {"version": "v2", "created": "Tue, 24 Jan 2017 03:50:45 GMT"}, {"version": "v3", "created": "Wed, 1 Feb 2017 03:50:17 GMT"}], "update_date": "2017-02-02", "authors_parsed": [["Papavasileiou", "Ioannis", ""], ["Zhang", "Wenlong", ""], ["Wang", "Xin", ""], ["Bi", "Jinbo", ""], ["Zhang", "Li", ""], ["Han", "Song", ""]]}, {"id": "1612.02572", "submitter": "Giovanni Montana", "authors": "James H Cole, Rudra PK Poudel, Dimosthenis Tsagkrasoulis, Matthan WA\n  Caan, Claire Steves, Tim D Spector, Giovanni Montana", "title": "Predicting brain age with deep learning from raw imaging data results in\n  a reliable and heritable biomarker", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning analysis of neuroimaging data can accurately predict\nchronological age in healthy people and deviations from healthy brain ageing\nhave been associated with cognitive impairment and disease. Here we sought to\nfurther establish the credentials of \"brain-predicted age\" as a biomarker of\nindividual differences in the brain ageing process, using a predictive\nmodelling approach based on deep learning, and specifically convolutional\nneural networks (CNN), and applied to both pre-processed and raw T1-weighted\nMRI data. Firstly, we aimed to demonstrate the accuracy of CNN brain-predicted\nage using a large dataset of healthy adults (N = 2001). Next, we sought to\nestablish the heritability of brain-predicted age using a sample of monozygotic\nand dizygotic female twins (N = 62). Thirdly, we examined the test-retest and\nmulti-centre reliability of brain-predicted age using two samples\n(within-scanner N = 20; between-scanner N = 11). CNN brain-predicted ages were\ngenerated and compared to a Gaussian Process Regression (GPR) approach, on all\ndatasets. Input data were grey matter (GM) or white matter (WM) volumetric maps\ngenerated by Statistical Parametric Mapping (SPM) or raw data. Brain-predicted\nage represents an accurate, highly reliable and genetically-valid phenotype,\nthat has potential to be used as a biomarker of brain ageing. Moreover, age\npredictions can be accurately generated on raw T1-MRI data, substantially\nreducing computation time for novel data, bringing the process closer to giving\nreal-time information on brain health in clinical settings.\n", "versions": [{"version": "v1", "created": "Thu, 8 Dec 2016 09:26:08 GMT"}], "update_date": "2016-12-09", "authors_parsed": [["Cole", "James H", ""], ["Poudel", "Rudra PK", ""], ["Tsagkrasoulis", "Dimosthenis", ""], ["Caan", "Matthan WA", ""], ["Steves", "Claire", ""], ["Spector", "Tim D", ""], ["Montana", "Giovanni", ""]]}, {"id": "1612.02575", "submitter": "Prasad Sudhakar", "authors": "Rahul Venkataramani and Sheshadri Thiruvenkadam and Prasad Sudhakar\n  and Hariharan Ravishankar and Vivek Vaidya", "title": "Filter sharing: Efficient learning of parameters for volumetric\n  convolutions", "comments": "6 pages, 2 figures. Published in NIPS 2016 workshop on Machine\n  Learning for Health, December 2016, Barcelona", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Typical convolutional neural networks (CNNs) have several millions of\nparameters and require a large amount of annotated data to train them. In\nmedical applications where training data is hard to come by, these\nsophisticated machine learning models are difficult to train. In this paper, we\npropose a method to reduce the inherent complexity of CNNs during training by\nexploiting the significant redundancy that is noticed in the learnt CNN\nfilters. Our method relies on finding a small set of filters and mixing\ncoefficients to derive every filter in each convolutional layer at the time of\ntraining itself, thereby reducing the number of parameters to be trained. We\nconsider the problem of 3D lung nodule segmentation in CT images and\ndemonstrate the effectiveness of our method in achieving good results with only\nfew training examples.\n", "versions": [{"version": "v1", "created": "Thu, 8 Dec 2016 09:35:13 GMT"}], "update_date": "2016-12-09", "authors_parsed": [["Venkataramani", "Rahul", ""], ["Thiruvenkadam", "Sheshadri", ""], ["Sudhakar", "Prasad", ""], ["Ravishankar", "Hariharan", ""], ["Vaidya", "Vivek", ""]]}, {"id": "1612.02583", "submitter": "Dong Gong", "authors": "Dong Gong, Jie Yang, Lingqiao Liu, Yanning Zhang, Ian Reid, Chunhua\n  Shen, Anton van den Hengel, Qinfeng Shi", "title": "From Motion Blur to Motion Flow: a Deep Learning Solution for Removing\n  Heterogeneous Motion Blur", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Removing pixel-wise heterogeneous motion blur is challenging due to the\nill-posed nature of the problem. The predominant solution is to estimate the\nblur kernel by adding a prior, but the extensive literature on the subject\nindicates the difficulty in identifying a prior which is suitably informative,\nand general. Rather than imposing a prior based on theory, we propose instead\nto learn one from the data. Learning a prior over the latent image would\nrequire modeling all possible image content. The critical observation\nunderpinning our approach is thus that learning the motion flow instead allows\nthe model to focus on the cause of the blur, irrespective of the image content.\nThis is a much easier learning task, but it also avoids the iterative process\nthrough which latent image priors are typically applied. Our approach directly\nestimates the motion flow from the blurred image through a fully-convolutional\ndeep neural network (FCN) and recovers the unblurred image from the estimated\nmotion flow. Our FCN is the first universal end-to-end mapping from the blurred\nimage to the dense motion flow. To train the FCN, we simulate motion flows to\ngenerate synthetic blurred-image-motion-flow pairs thus avoiding the need for\nhuman labeling. Extensive experiments on challenging realistic blurred images\ndemonstrate that the proposed method outperforms the state-of-the-art.\n", "versions": [{"version": "v1", "created": "Thu, 8 Dec 2016 10:05:57 GMT"}], "update_date": "2016-12-09", "authors_parsed": [["Gong", "Dong", ""], ["Yang", "Jie", ""], ["Liu", "Lingqiao", ""], ["Zhang", "Yanning", ""], ["Reid", "Ian", ""], ["Shen", "Chunhua", ""], ["Hengel", "Anton van den", ""], ["Shi", "Qinfeng", ""]]}, {"id": "1612.02589", "submitter": "Stergios Christodoulidis Mr.", "authors": "Stergios Christodoulidis, Marios Anthimopoulos, Lukas Ebner, Andreas\n  Christe and Stavroula Mougiakakou", "title": "Multi-source Transfer Learning with Convolutional Neural Networks for\n  Lung Pattern Analysis", "comments": null, "journal-ref": null, "doi": "10.1109/JBHI.2016.2636929", "report-no": null, "categories": "cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Early diagnosis of interstitial lung diseases is crucial for their treatment,\nbut even experienced physicians find it difficult, as their clinical\nmanifestations are similar. In order to assist with the diagnosis,\ncomputer-aided diagnosis (CAD) systems have been developed. These commonly rely\non a fixed scale classifier that scans CT images, recognizes textural lung\npatterns and generates a map of pathologies. In a previous study, we proposed a\nmethod for classifying lung tissue patterns using a deep convolutional neural\nnetwork (CNN), with an architecture designed for the specific problem. In this\nstudy, we present an improved method for training the proposed network by\ntransferring knowledge from the similar domain of general texture\nclassification. Six publicly available texture databases are used to pretrain\nnetworks with the proposed architecture, which are then fine-tuned on the lung\ntissue data. The resulting CNNs are combined in an ensemble and their fused\nknowledge is compressed back to a network with the original architecture. The\nproposed approach resulted in an absolute increase of about 2% in the\nperformance of the proposed CNN. The results demonstrate the potential of\ntransfer learning in the field of medical image analysis, indicate the textural\nnature of the problem and show that the method used for training a network can\nbe as important as designing its architecture.\n", "versions": [{"version": "v1", "created": "Thu, 8 Dec 2016 10:43:03 GMT"}], "update_date": "2016-12-13", "authors_parsed": [["Christodoulidis", "Stergios", ""], ["Anthimopoulos", "Marios", ""], ["Ebner", "Lukas", ""], ["Christe", "Andreas", ""], ["Mougiakakou", "Stavroula", ""]]}, {"id": "1612.02590", "submitter": "Zike Yan", "authors": "Zike Yan, Xuezhi Xiang", "title": "Scene Flow Estimation: A Survey", "comments": "51 pages, 12 figures, 10 tables, 108 references", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper is the first to review the scene flow estimation field, which\nanalyzes and compares methods, technical challenges, evaluation methodologies\nand performance of scene flow estimation. Existing algorithms are categorized\nin terms of scene representation, data source, and calculation scheme, and the\npros and cons in each category are compared briefly. The datasets and\nevaluation protocols are enumerated, and the performance of the most\nrepresentative methods is presented. A future vision is illustrated with few\nquestions arisen for discussion. This survey presents a general introduction\nand analysis of scene flow estimation.\n", "versions": [{"version": "v1", "created": "Thu, 8 Dec 2016 10:44:03 GMT"}, {"version": "v2", "created": "Mon, 12 Dec 2016 03:25:34 GMT"}, {"version": "v3", "created": "Sun, 21 May 2017 09:52:02 GMT"}], "update_date": "2017-05-23", "authors_parsed": [["Yan", "Zike", ""], ["Xiang", "Xuezhi", ""]]}, {"id": "1612.02631", "submitter": "Yuliya Tarabalka", "authors": "Seong-Gyun Jeong, Yuliya Tarabalka, Nicolas Nisse and Josiane Zerubia", "title": "Progressive Tree-like Curvilinear Structure Reconstruction with\n  Structured Ranking Learning and Graph Algorithm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel tree-like curvilinear structure reconstruction algorithm\nbased on supervised learning and graph theory. In this work we analyze image\npatches to obtain the local major orientations and the rankings that correspond\nto the curvilinear structure. To extract local curvilinear features, we compute\noriented gradient information using steerable filters. We then employ\nStructured Support Vector Machine for ordinal regression of the input image\npatches, where the ordering is determined by shape similarity to latent\ncurvilinear structure. Finally, we progressively reconstruct the curvilinear\nstructure by looking for geodesic paths connecting remote vertices in the graph\nbuilt on the structured output rankings. Experimental results show that the\nproposed algorithm faithfully provides topological features of the curvilinear\nstructures using minimal pixels for various datasets.\n", "versions": [{"version": "v1", "created": "Thu, 8 Dec 2016 13:13:01 GMT"}], "update_date": "2016-12-09", "authors_parsed": [["Jeong", "Seong-Gyun", ""], ["Tarabalka", "Yuliya", ""], ["Nisse", "Nicolas", ""], ["Zerubia", "Josiane", ""]]}, {"id": "1612.02646", "submitter": "Federico Perazzi", "authors": "Anna Khoreva, Federico Perazzi, Rodrigo Benenson, Bernt Schiele,\n  Alexander Sorkine-Hornung", "title": "Learning Video Object Segmentation from Static Images", "comments": "Submitted to CVPR 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inspired by recent advances of deep learning in instance segmentation and\nobject tracking, we introduce video object segmentation problem as a concept of\nguided instance segmentation. Our model proceeds on a per-frame basis, guided\nby the output of the previous frame towards the object of interest in the next\nframe. We demonstrate that highly accurate object segmentation in videos can be\nenabled by using a convnet trained with static images only. The key ingredient\nof our approach is a combination of offline and online learning strategies,\nwhere the former serves to produce a refined mask from the previous frame\nestimate and the latter allows to capture the appearance of the specific object\ninstance. Our method can handle different types of input annotations: bounding\nboxes and segments, as well as incorporate multiple annotated frames, making\nthe system suitable for diverse applications. We obtain competitive results on\nthree different datasets, independently from the type of input annotation.\n", "versions": [{"version": "v1", "created": "Thu, 8 Dec 2016 13:59:18 GMT"}], "update_date": "2019-02-05", "authors_parsed": [["Khoreva", "Anna", ""], ["Perazzi", "Federico", ""], ["Benenson", "Rodrigo", ""], ["Schiele", "Bernt", ""], ["Sorkine-Hornung", "Alexander", ""]]}, {"id": "1612.02649", "submitter": "Judy Hoffman", "authors": "Judy Hoffman, Dequan Wang, Fisher Yu, Trevor Darrell", "title": "FCNs in the Wild: Pixel-level Adversarial and Constraint-based\n  Adaptation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fully convolutional models for dense prediction have proven successful for a\nwide range of visual tasks. Such models perform well in a supervised setting,\nbut performance can be surprisingly poor under domain shifts that appear mild\nto a human observer. For example, training on one city and testing on another\nin a different geographic region and/or weather condition may result in\nsignificantly degraded performance due to pixel-level distribution shift. In\nthis paper, we introduce the first domain adaptive semantic segmentation\nmethod, proposing an unsupervised adversarial approach to pixel prediction\nproblems. Our method consists of both global and category specific adaptation\ntechniques. Global domain alignment is performed using a novel semantic\nsegmentation network with fully convolutional domain adversarial learning. This\ninitially adapted space then enables category specific adaptation through a\ngeneralization of constrained weak learning, with explicit transfer of the\nspatial layout from the source to the target domains. Our approach outperforms\nbaselines across different settings on multiple large-scale datasets, including\nadapting across various real city environments, different synthetic\nsub-domains, from simulated to real environments, and on a novel large-scale\ndash-cam dataset.\n", "versions": [{"version": "v1", "created": "Thu, 8 Dec 2016 14:11:10 GMT"}], "update_date": "2016-12-09", "authors_parsed": [["Hoffman", "Judy", ""], ["Wang", "Dequan", ""], ["Yu", "Fisher", ""], ["Darrell", "Trevor", ""]]}, {"id": "1612.02675", "submitter": "Karthik Gopinath", "authors": "Karthik Gopinath and Jayanthi Sivaswamy", "title": "Domain knowledge assisted cyst segmentation in OCT retinal images", "comments": "The paper was accepted as an oral presentation in MICCAI-2015 OPTIMA\n  Cyst Segmentation Challenge", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  3D imaging modalities are becoming increasingly popular and relevant in\nretinal imaging owing to their effectiveness in highlighting structures in\nsub-retinal layers. OCT is one such modality which has great importance in the\ncontext of analysis of cystoid structures in subretinal layers. Signal to noise\nratio(SNR) of the images obtained from OCT is less and hence automated and\naccurate determination of cystoid structures from OCT is a challenging task. We\npropose an automated method for detecting/segmenting cysts in 3D OCT volumes.\nThe proposed method is biologically inspired and fast aided by the domain\nknowledge about the cystoid structures. An ensemble learning methodRandom\nforests is learnt for classification of detected region into cyst region. The\nmethod achieves detection and segmentation in a unified setting. We believe the\nproposed approach with further improvements can be a promising starting point\nfor more robust approach. This method is validated against the training set\nachieves a mean dice coefficient of 0.3893 with a standard deviation of 0.2987\n", "versions": [{"version": "v1", "created": "Thu, 8 Dec 2016 14:59:07 GMT"}], "update_date": "2016-12-09", "authors_parsed": [["Gopinath", "Karthik", ""], ["Sivaswamy", "Jayanthi", ""]]}, {"id": "1612.02699", "submitter": "Chi Li", "authors": "Chi Li, M. Zeeshan Zia, Quoc-Huy Tran, Xiang Yu, Gregory D. Hager and\n  Manmohan Chandraker", "title": "Deep Supervision with Shape Concepts for Occlusion-Aware 3D Object\n  Parsing", "comments": "Accepted in CVPR 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Monocular 3D object parsing is highly desirable in various scenarios\nincluding occlusion reasoning and holistic scene interpretation. We present a\ndeep convolutional neural network (CNN) architecture to localize semantic parts\nin 2D image and 3D space while inferring their visibility states, given a\nsingle RGB image. Our key insight is to exploit domain knowledge to regularize\nthe network by deeply supervising its hidden layers, in order to sequentially\ninfer intermediate concepts associated with the final task. To acquire training\ndata in desired quantities with ground truth 3D shape and relevant concepts, we\nrender 3D object CAD models to generate large-scale synthetic data and simulate\nchallenging occlusion configurations between objects. We train the network only\non synthetic data and demonstrate state-of-the-art performances on real image\nbenchmarks including an extended version of KITTI, PASCAL VOC, PASCAL3D+ and\nIKEA for 2D and 3D keypoint localization and instance segmentation. The\nempirical results substantiate the utility of our deep supervision scheme by\ndemonstrating effective transfer of knowledge from synthetic data to real\nimages, resulting in less overfitting compared to standard end-to-end training.\n", "versions": [{"version": "v1", "created": "Thu, 8 Dec 2016 15:33:19 GMT"}, {"version": "v2", "created": "Tue, 4 Apr 2017 20:58:33 GMT"}, {"version": "v3", "created": "Thu, 20 Apr 2017 20:19:33 GMT"}], "update_date": "2017-04-24", "authors_parsed": [["Li", "Chi", ""], ["Zia", "M. Zeeshan", ""], ["Tran", "Quoc-Huy", ""], ["Yu", "Xiang", ""], ["Hager", "Gregory D.", ""], ["Chandraker", "Manmohan", ""]]}, {"id": "1612.02709", "submitter": "Menghua Zhai", "authors": "Menghua Zhai and Zachary Bessinger and Scott Workman and Nathan Jacobs", "title": "Predicting Ground-Level Scene Layout from Aerial Imagery", "comments": "13 pages including appendix", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a novel strategy for learning to extract semantically meaningful\nfeatures from aerial imagery. Instead of manually labeling the aerial imagery,\nwe propose to predict (noisy) semantic features automatically extracted from\nco-located ground imagery. Our network architecture takes an aerial image as\ninput, extracts features using a convolutional neural network, and then applies\nan adaptive transformation to map these features into the ground-level\nperspective. We use an end-to-end learning approach to minimize the difference\nbetween the semantic segmentation extracted directly from the ground image and\nthe semantic segmentation predicted solely based on the aerial image. We show\nthat a model learned using this strategy, with no additional training, is\nalready capable of rough semantic labeling of aerial imagery. Furthermore, we\ndemonstrate that by finetuning this model we can achieve more accurate semantic\nsegmentation than two baseline initialization strategies. We use our network to\naddress the task of estimating the geolocation and geoorientation of a ground\nimage. Finally, we show how features extracted from an aerial image can be used\nto hallucinate a plausible ground-level panorama.\n", "versions": [{"version": "v1", "created": "Thu, 8 Dec 2016 16:12:23 GMT"}], "update_date": "2016-12-09", "authors_parsed": [["Zhai", "Menghua", ""], ["Bessinger", "Zachary", ""], ["Workman", "Scott", ""], ["Jacobs", "Nathan", ""]]}, {"id": "1612.02742", "submitter": "Xiaoming Deng", "authors": "Xiaoming Deng, Ye Yuan, Yinda Zhang, Ping Tan, Liang Chang, Shuo Yang,\n  Hongan Wang", "title": "Joint Hand Detection and Rotation Estimation by Using CNN", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hand detection is essential for many hand related tasks, e.g. parsing hand\npose, understanding gesture, which are extremely useful for robotics and\nhuman-computer interaction. However, hand detection in uncontrolled\nenvironments is challenging due to the flexibility of wrist joint and cluttered\nbackground. We propose a deep learning based approach which detects hands and\ncalibrates in-plane rotation under supervision at the same time. To guarantee\nthe recall, we propose a context aware proposal generation algorithm which\nsignificantly outperforms the selective search. We then design a convolutional\nneural network(CNN) which handles object rotation explicitly to jointly solve\nthe object detection and rotation estimation tasks. Experiments show that our\nmethod achieves better results than state-of-the-art detection models on\nwidely-used benchmarks such as Oxford and Egohands database. We further show\nthat rotation estimation and classification can mutually benefit each other.\n", "versions": [{"version": "v1", "created": "Thu, 8 Dec 2016 17:53:54 GMT"}], "update_date": "2016-12-09", "authors_parsed": [["Deng", "Xiaoming", ""], ["Yuan", "Ye", ""], ["Zhang", "Yinda", ""], ["Tan", "Ping", ""], ["Chang", "Liang", ""], ["Yang", "Shuo", ""], ["Wang", "Hongan", ""]]}, {"id": "1612.02761", "submitter": "Yuelong Li", "authors": "Yuelong Li, Chul Lee and Vishal Monga", "title": "A Maximum A Posteriori Estimation Framework for Robust High Dynamic\n  Range Video Synthesis", "comments": null, "journal-ref": null, "doi": "10.1109/TIP.2016.2642790", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High dynamic range (HDR) image synthesis from multiple low dynamic range\n(LDR) exposures continues to be actively researched. The extension to HDR video\nsynthesis is a topic of significant current interest due to potential cost\nbenefits. For HDR video, a stiff practical challenge presents itself in the\nform of accurate correspondence estimation of objects between video frames. In\nparticular, loss of data resulting from poor exposures and varying intensity\nmake conventional optical flow methods highly inaccurate. We avoid exact\ncorrespondence estimation by proposing a statistical approach via maximum a\nposterior (MAP) estimation, and under appropriate statistical assumptions and\nchoice of priors and models, we reduce it to an optimization problem of solving\nfor the foreground and background of the target frame. We obtain the background\nthrough rank minimization and estimate the foreground via a novel multiscale\nadaptive kernel regression technique, which implicitly captures local structure\nand temporal motion by solving an unconstrained optimization problem. Extensive\nexperimental results on both real and synthetic datasets demonstrate that our\nalgorithm is more capable of delivering high-quality HDR videos than current\nstate-of-the-art methods, under both subjective and objective assessments.\nFurthermore, a thorough complexity analysis reveals that our algorithm achieves\nbetter complexity-performance trade-off than conventional methods.\n", "versions": [{"version": "v1", "created": "Thu, 8 Dec 2016 18:33:08 GMT"}], "update_date": "2017-04-05", "authors_parsed": [["Li", "Yuelong", ""], ["Lee", "Chul", ""], ["Monga", "Vishal", ""]]}, {"id": "1612.02766", "submitter": "Xianming Liu", "authors": "Xianming Liu, Amy Zhang, Tobias Tiecke, Andreas Gros, Thomas S. Huang", "title": "Feedback Neural Network for Weakly Supervised Geo-Semantic Segmentation", "comments": "9 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning from weakly-supervised data is one of the main challenges in machine\nlearning and computer vision, especially for tasks such as image semantic\nsegmentation where labeling is extremely expensive and subjective. In this\npaper, we propose a novel neural network architecture to perform\nweakly-supervised learning by suppressing irrelevant neuron activations. It\nlocalizes objects of interest by learning from image-level categorical labels\nin an end-to-end manner. We apply this algorithm to a practical challenge of\ntransforming satellite images into a map of settlements and individual\nbuildings. Experimental results show that the proposed algorithm achieves\nsuperior performance and efficiency when compared with various baseline models.\n", "versions": [{"version": "v1", "created": "Thu, 8 Dec 2016 18:46:39 GMT"}], "update_date": "2016-12-09", "authors_parsed": [["Liu", "Xianming", ""], ["Zhang", "Amy", ""], ["Tiecke", "Tobias", ""], ["Gros", "Andreas", ""], ["Huang", "Thomas S.", ""]]}, {"id": "1612.02808", "submitter": "Evangelos Kalogerakis", "authors": "Evangelos Kalogerakis, Melinos Averkiou, Subhransu Maji, Siddhartha\n  Chaudhuri", "title": "3D Shape Segmentation with Projective Convolutional Networks", "comments": "This is an updated version of our CVPR 2017 paper. We incorporated\n  new experiments that demonstrate ShapePFCN performance under the case of\n  consistent *upright* orientation and an additional input channel in our\n  rendered images for encoding height from the ground plane (upright axis\n  coordinate values). Performance is improved in this setting", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a deep architecture for segmenting 3D objects into\ntheir labeled semantic parts. Our architecture combines image-based Fully\nConvolutional Networks (FCNs) and surface-based Conditional Random Fields\n(CRFs) to yield coherent segmentations of 3D shapes. The image-based FCNs are\nused for efficient view-based reasoning about 3D object parts. Through a\nspecial projection layer, FCN outputs are effectively aggregated across\nmultiple views and scales, then are projected onto the 3D object surfaces.\nFinally, a surface-based CRF combines the projected outputs with geometric\nconsistency cues to yield coherent segmentations. The whole architecture\n(multi-view FCNs and CRF) is trained end-to-end. Our approach significantly\noutperforms the existing state-of-the-art methods in the currently largest\nsegmentation benchmark (ShapeNet). Finally, we demonstrate promising\nsegmentation results on noisy 3D shapes acquired from consumer-grade depth\ncameras.\n", "versions": [{"version": "v1", "created": "Thu, 8 Dec 2016 20:46:32 GMT"}, {"version": "v2", "created": "Sun, 16 Apr 2017 01:48:57 GMT"}, {"version": "v3", "created": "Mon, 13 Nov 2017 05:46:17 GMT"}], "update_date": "2017-11-15", "authors_parsed": [["Kalogerakis", "Evangelos", ""], ["Averkiou", "Melinos", ""], ["Maji", "Subhransu", ""], ["Chaudhuri", "Siddhartha", ""]]}, {"id": "1612.02844", "submitter": "Hang Zhang", "authors": "Hang Zhang, Jia Xue, Kristin Dana", "title": "Deep TEN: Texture Encoding Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a Deep Texture Encoding Network (Deep-TEN) with a novel Encoding\nLayer integrated on top of convolutional layers, which ports the entire\ndictionary learning and encoding pipeline into a single model. Current methods\nbuild from distinct components, using standard encoders with separate\noff-the-shelf features such as SIFT descriptors or pre-trained CNN features for\nmaterial recognition. Our new approach provides an end-to-end learning\nframework, where the inherent visual vocabularies are learned directly from the\nloss function. The features, dictionaries and the encoding representation for\nthe classifier are all learned simultaneously. The representation is orderless\nand therefore is particularly useful for material and texture recognition. The\nEncoding Layer generalizes robust residual encoders such as VLAD and Fisher\nVectors, and has the property of discarding domain specific information which\nmakes the learned convolutional features easier to transfer. Additionally,\njoint training using multiple datasets of varied sizes and class labels is\nsupported resulting in increased recognition performance. The experimental\nresults show superior performance as compared to state-of-the-art methods using\ngold-standard databases such as MINC-2500, Flickr Material Database,\nKTH-TIPS-2b, and two recent databases 4D-Light-Field-Material and GTOS. The\nsource code for the complete system are publicly available.\n", "versions": [{"version": "v1", "created": "Thu, 8 Dec 2016 21:27:31 GMT"}], "update_date": "2016-12-12", "authors_parsed": [["Zhang", "Hang", ""], ["Xue", "Jia", ""], ["Dana", "Kristin", ""]]}, {"id": "1612.02859", "submitter": "Erik Wijmans", "authors": "Erik Wijmans and Yasutaka Furukawa", "title": "Exploiting 2D Floorplan for Building-scale Panorama RGBD Alignment", "comments": null, "journal-ref": null, "doi": "10.1109/CVPR.2017.156", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a novel algorithm that utilizes a 2D floorplan to align\npanorama RGBD scans. While effective panorama RGBD alignment techniques exist,\nsuch a system requires extremely dense RGBD image sampling. Our approach can\nsignificantly reduce the number of necessary scans with the aid of a floorplan\nimage. We formulate a novel Markov Random Field inference problem as a scan\nplacement over the floorplan, as opposed to the conventional scan-to-scan\nalignment. The technical contributions lie in multi-modal image correspondence\ncues (between scans and schematic floorplan) as well as a novel coverage\npotential avoiding an inherent stacking bias. The proposed approach has been\nevaluated on five challenging large indoor spaces. To the best of our\nknowledge, we present the first effective system that utilizes a 2D floorplan\nimage for building-scale 3D pointcloud alignment. The source code and the data\nwill be shared with the community to further enhance indoor mapping research.\n", "versions": [{"version": "v1", "created": "Thu, 8 Dec 2016 22:22:14 GMT"}], "update_date": "2019-05-21", "authors_parsed": [["Wijmans", "Erik", ""], ["Furukawa", "Yasutaka", ""]]}, {"id": "1612.02880", "submitter": "Jingang Zhong", "authors": "Zibang Zhang, Xueying Wang, Jingang Zhong", "title": "Fast Fourier single-pixel imaging using binary illumination", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fourier single-pixel imaging (FSI) has proven capable of reconstructing\nhigh-quality two-dimensional and three-dimensional images. The utilization of\nthe sparsity of natural images in Fourier domain allows high-resolution images\nto be reconstructed from far fewer measurements than effective image pixels.\nHowever, applying original FSI in digital micro-mirror device (DMD) based\nhigh-speed imaging system turns out to be challenging, because the original FSI\nuses grayscale Fourier basis patterns for illumination while DMDs generate\ngrayscale patterns at a relatively low rate. DMDs are a binary device which can\nonly generate a black-and-white pattern at each instance. In this paper, we\nadopt binary Fourier patterns for illumination to achieve DMD-based high-speed\nsingle-pixel imaging. Binary Fourier patterns are generated by upsampling and\nthen applying error diffusion based dithering to the grayscale patterns.\nExperiments demonstrate the proposed technique able to achieve static imaging\nwith high quality and dynamic imaging in real time. The proposed technique\npotentially allows high-quality and high-speed imaging over broad wavebands.\n", "versions": [{"version": "v1", "created": "Fri, 9 Dec 2016 01:02:37 GMT"}], "update_date": "2016-12-12", "authors_parsed": [["Zhang", "Zibang", ""], ["Wang", "Xueying", ""], ["Zhong", "Jingang", ""]]}, {"id": "1612.02889", "submitter": "Yubo Zhang", "authors": "Yubo Zhang, Vishnu Naresh Boddeti, Kris M. Kitani", "title": "Gesture-based Bootstrapping for Egocentric Hand Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurately identifying hands in images is a key sub-task for human activity\nunderstanding with wearable first-person point-of-view cameras. Traditional\nhand segmentation approaches rely on a large corpus of manually labeled data to\ngenerate robust hand detectors. However, these approaches still face challenges\nas the appearance of the hand varies greatly across users, tasks, environments\nor illumination conditions. A key observation in the case of many wearable\napplications and interfaces is that, it is only necessary to accurately detect\nthe user's hands in a specific situational context. Based on this observation,\nwe introduce an interactive approach to learn a person-specific hand\nsegmentation model that does not require any manually labeled training data.\nOur approach proceeds in two steps, an interactive bootstrapping step for\nidentifying moving hand regions, followed by learning a personalized user\nspecific hand appearance model. Concretely, our approach uses two convolutional\nneural networks: (1) a gesture network that uses pre-defined motion information\nto detect the hand region; and (2) an appearance network that learns a person\nspecific model of the hand region based on the output of the gesture network.\nDuring training, to make the appearance network robust to errors in the gesture\nnetwork, the loss function of the former network incorporates the confidence of\nthe gesture network while learning. Experiments demonstrate the robustness of\nour approach with an F1 score over 0.8 on all challenging datasets across a\nwide range of illumination and hand appearance variations, improving over a\nbaseline approach by over 10%.\n", "versions": [{"version": "v1", "created": "Fri, 9 Dec 2016 01:49:41 GMT"}, {"version": "v2", "created": "Mon, 11 Jun 2018 18:15:06 GMT"}], "update_date": "2018-06-18", "authors_parsed": [["Zhang", "Yubo", ""], ["Boddeti", "Vishnu Naresh", ""], ["Kitani", "Kris M.", ""]]}, {"id": "1612.02903", "submitter": "Christopher Pramerdorfer", "authors": "Christopher Pramerdorfer and Martin Kampel", "title": "Facial Expression Recognition using Convolutional Neural Networks: State\n  of the Art", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ability to recognize facial expressions automatically enables novel\napplications in human-computer interaction and other areas. Consequently, there\nhas been active research in this field, with several recent works utilizing\nConvolutional Neural Networks (CNNs) for feature extraction and inference.\nThese works differ significantly in terms of CNN architectures and other\nfactors. Based on the reported results alone, the performance impact of these\nfactors is unclear. In this paper, we review the state of the art in\nimage-based facial expression recognition using CNNs and highlight algorithmic\ndifferences and their performance impact. On this basis, we identify existing\nbottlenecks and consequently directions for advancing this research field.\nFurthermore, we demonstrate that overcoming one of these bottlenecks - the\ncomparatively basic architectures of the CNNs utilized in this field - leads to\na substantial performance increase. By forming an ensemble of modern deep CNNs,\nwe obtain a FER2013 test accuracy of 75.2%, outperforming previous works\nwithout requiring auxiliary training data or face registration.\n", "versions": [{"version": "v1", "created": "Fri, 9 Dec 2016 03:59:31 GMT"}], "update_date": "2016-12-12", "authors_parsed": [["Pramerdorfer", "Christopher", ""], ["Kampel", "Martin", ""]]}, {"id": "1612.02954", "submitter": "Frank Nielsen", "authors": "Frank Nielsen and Richard Nock", "title": "A series of maximum entropy upper bounds of the differential entropy", "comments": "18 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.CV cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a series of closed-form maximum entropy upper bounds for the\ndifferential entropy of a continuous univariate random variable and study the\nproperties of that series. We then show how to use those generic bounds for\nupper bounding the differential entropy of Gaussian mixture models. This\nrequires to calculate the raw moments and raw absolute moments of Gaussian\nmixtures in closed-form that may also be handy in statistical machine learning\nand information theory. We report on our experiments and discuss on the\ntightness of those bounds.\n", "versions": [{"version": "v1", "created": "Fri, 9 Dec 2016 09:36:14 GMT"}], "update_date": "2016-12-12", "authors_parsed": [["Nielsen", "Frank", ""], ["Nock", "Richard", ""]]}, {"id": "1612.03019", "submitter": "Alberto Pretto", "authors": "Maurilio Di Cicco, Ciro Potena, Giorgio Grisetti and Alberto Pretto", "title": "Automatic Model Based Dataset Generation for Fast and Accurate Crop and\n  Weeds Detection", "comments": "To appear in IEEE/RSJ IROS 2017", "journal-ref": null, "doi": "10.1109/IROS.2017.8206408", "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Selective weeding is one of the key challenges in the field of agriculture\nrobotics. To accomplish this task, a farm robot should be able to accurately\ndetect plants and to distinguish them between crop and weeds. Most of the\npromising state-of-the-art approaches make use of appearance-based models\ntrained on large annotated datasets. Unfortunately, creating large agricultural\ndatasets with pixel-level annotations is an extremely time consuming task,\nactually penalizing the usage of data-driven techniques. In this paper, we face\nthis problem by proposing a novel and effective approach that aims to\ndramatically minimize the human intervention needed to train the detection and\nclassification algorithms. The idea is to procedurally generate large synthetic\ntraining datasets randomizing the key features of the target environment (i.e.,\ncrop and weed species, type of soil, light conditions). More specifically, by\ntuning these model parameters, and exploiting a few real-world textures, it is\npossible to render a large amount of realistic views of an artificial\nagricultural scenario with no effort. The generated data can be directly used\nto train the model or to supplement real-world images. We validate the proposed\nmethodology by using as testbed a modern deep learning based image segmentation\narchitecture. We compare the classification results obtained using both real\nand synthetic images as training data. The reported results confirm the\neffectiveness and the potentiality of our approach.\n", "versions": [{"version": "v1", "created": "Fri, 9 Dec 2016 13:43:19 GMT"}, {"version": "v2", "created": "Fri, 3 Mar 2017 18:07:02 GMT"}, {"version": "v3", "created": "Sun, 6 Aug 2017 17:33:20 GMT"}], "update_date": "2017-12-19", "authors_parsed": [["Di Cicco", "Maurilio", ""], ["Potena", "Ciro", ""], ["Grisetti", "Giorgio", ""], ["Pretto", "Alberto", ""]]}, {"id": "1612.03052", "submitter": "Joe Yue-Hei Ng", "authors": "Joe Yue-Hei Ng, Jonghyun Choi, Jan Neumann, Larry S. Davis", "title": "ActionFlowNet: Learning Motion Representation for Action Recognition", "comments": "WACV 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Even with the recent advances in convolutional neural networks (CNN) in\nvarious visual recognition tasks, the state-of-the-art action recognition\nsystem still relies on hand crafted motion feature such as optical flow to\nachieve the best performance. We propose a multitask learning model\nActionFlowNet to train a single stream network directly from raw pixels to\njointly estimate optical flow while recognizing actions with convolutional\nneural networks, capturing both appearance and motion in a single model. We\nadditionally provide insights to how the quality of the learned optical flow\naffects the action recognition. Our model significantly improves action\nrecognition accuracy by a large margin 31% compared to state-of-the-art\nCNN-based action recognition models trained without external large scale data\nand additional optical flow input. Without pretraining on large external\nlabeled datasets, our model, by well exploiting the motion information,\nachieves competitive recognition accuracy to the models trained with large\nlabeled datasets such as ImageNet and Sport-1M.\n", "versions": [{"version": "v1", "created": "Fri, 9 Dec 2016 15:20:23 GMT"}, {"version": "v2", "created": "Fri, 21 Apr 2017 01:45:42 GMT"}, {"version": "v3", "created": "Fri, 16 Feb 2018 22:15:25 GMT"}], "update_date": "2018-02-20", "authors_parsed": [["Ng", "Joe Yue-Hei", ""], ["Choi", "Jonghyun", ""], ["Neumann", "Jan", ""], ["Davis", "Larry S.", ""]]}, {"id": "1612.03094", "submitter": "Adri\\`a Recasens", "authors": "Adri\\`a Recasens, Carl Vondrick, Aditya Khosla, Antonio Torralba", "title": "Following Gaze Across Views", "comments": "9 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Following the gaze of people inside videos is an important signal for\nunderstanding people and their actions. In this paper, we present an approach\nfor following gaze across views by predicting where a particular person is\nlooking throughout a scene. We collect VideoGaze, a new dataset which we use as\na benchmark to both train and evaluate models. Given one view with a person in\nit and a second view of the scene, our model estimates a density for gaze\nlocation in the second view. A key aspect of our approach is an end-to-end\nmodel that solves the following sub-problems: saliency, gaze pose, and\ngeometric relationships between views. Although our model is supervised only\nwith gaze, we show that the model learns to solve these subproblems\nautomatically without supervision. Experiments suggest that our approach\nfollows gaze better than standard baselines and produces plausible results for\neveryday situations.\n", "versions": [{"version": "v1", "created": "Fri, 9 Dec 2016 17:20:17 GMT"}], "update_date": "2016-12-12", "authors_parsed": [["Recasens", "Adri\u00e0", ""], ["Vondrick", "Carl", ""], ["Khosla", "Aditya", ""], ["Torralba", "Antonio", ""]]}, {"id": "1612.03129", "submitter": "Zeeshan Hayder", "authors": "Zeeshan Hayder, Xuming He and Mathieu Salzmann", "title": "Boundary-aware Instance Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of instance-level semantic segmentation, which aims at\njointly detecting, segmenting and classifying every individual object in an\nimage. In this context, existing methods typically propose candidate objects,\nusually as bounding boxes, and directly predict a binary mask within each such\nproposal. As a consequence, they cannot recover from errors in the object\ncandidate generation process, such as too small or shifted boxes.\n  In this paper, we introduce a novel object segment representation based on\nthe distance transform of the object masks. We then design an object mask\nnetwork (OMN) with a new residual-deconvolution architecture that infers such a\nrepresentation and decodes it into the final binary object mask. This allows us\nto predict masks that go beyond the scope of the bounding boxes and are thus\nrobust to inaccurate object candidates. We integrate our OMN into a Multitask\nNetwork Cascade framework, and learn the resulting boundary-aware instance\nsegmentation (BAIS) network in an end-to-end manner. Our experiments on the\nPASCAL VOC 2012 and the Cityscapes datasets demonstrate the benefits of our\napproach, which outperforms the state-of-the-art in both object proposal\ngeneration and instance segmentation.\n", "versions": [{"version": "v1", "created": "Fri, 9 Dec 2016 18:57:33 GMT"}, {"version": "v2", "created": "Fri, 7 Apr 2017 01:43:57 GMT"}], "update_date": "2017-04-10", "authors_parsed": [["Hayder", "Zeeshan", ""], ["He", "Xuming", ""], ["Salzmann", "Mathieu", ""]]}, {"id": "1612.03142", "submitter": "Scott Workman", "authors": "Scott Workman, Richard Souvenir and Nathan Jacobs", "title": "Understanding and Mapping Natural Beauty", "comments": "International Conference on Computer Vision (ICCV) 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While natural beauty is often considered a subjective property of images, in\nthis paper, we take an objective approach and provide methods for quantifying\nand predicting the scenicness of an image. Using a dataset containing hundreds\nof thousands of outdoor images captured throughout Great Britain with\ncrowdsourced ratings of natural beauty, we propose an approach to predict\nscenicness which explicitly accounts for the variance of human ratings. We\ndemonstrate that quantitative measures of scenicness can benefit semantic image\nunderstanding, content-aware image processing, and a novel application of\ncross-view mapping, where the sparsity of ground-level images can be addressed\nby incorporating unlabeled overhead images in the training and prediction\nsteps. For each application, our methods for scenicness prediction result in\nquantitative and qualitative improvements over baseline approaches.\n", "versions": [{"version": "v1", "created": "Fri, 9 Dec 2016 19:48:15 GMT"}, {"version": "v2", "created": "Wed, 9 Aug 2017 17:37:29 GMT"}], "update_date": "2017-08-10", "authors_parsed": [["Workman", "Scott", ""], ["Souvenir", "Richard", ""], ["Jacobs", "Nathan", ""]]}, {"id": "1612.03144", "submitter": "Tsung-Yi Lin", "authors": "Tsung-Yi Lin, Piotr Doll\\'ar, Ross Girshick, Kaiming He, Bharath\n  Hariharan, Serge Belongie", "title": "Feature Pyramid Networks for Object Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Feature pyramids are a basic component in recognition systems for detecting\nobjects at different scales. But recent deep learning object detectors have\navoided pyramid representations, in part because they are compute and memory\nintensive. In this paper, we exploit the inherent multi-scale, pyramidal\nhierarchy of deep convolutional networks to construct feature pyramids with\nmarginal extra cost. A top-down architecture with lateral connections is\ndeveloped for building high-level semantic feature maps at all scales. This\narchitecture, called a Feature Pyramid Network (FPN), shows significant\nimprovement as a generic feature extractor in several applications. Using FPN\nin a basic Faster R-CNN system, our method achieves state-of-the-art\nsingle-model results on the COCO detection benchmark without bells and\nwhistles, surpassing all existing single-model entries including those from the\nCOCO 2016 challenge winners. In addition, our method can run at 5 FPS on a GPU\nand thus is a practical and accurate solution to multi-scale object detection.\nCode will be made publicly available.\n", "versions": [{"version": "v1", "created": "Fri, 9 Dec 2016 19:55:54 GMT"}, {"version": "v2", "created": "Wed, 19 Apr 2017 22:46:32 GMT"}], "update_date": "2017-04-21", "authors_parsed": [["Lin", "Tsung-Yi", ""], ["Doll\u00e1r", "Piotr", ""], ["Girshick", "Ross", ""], ["He", "Kaiming", ""], ["Hariharan", "Bharath", ""], ["Belongie", "Serge", ""]]}, {"id": "1612.03153", "submitter": "Hanbyul Joo", "authors": "Hanbyul Joo, Tomas Simon, Xulong Li, Hao Liu, Lei Tan, Lin Gui, Sean\n  Banerjee, Timothy Godisart, Bart Nabbe, Iain Matthews, Takeo Kanade, Shohei\n  Nobuhara, Yaser Sheikh", "title": "Panoptic Studio: A Massively Multiview System for Social Interaction\n  Capture", "comments": "Submitted to IEEE Transactions on Pattern Analysis and Machine\n  Intelligence", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an approach to capture the 3D motion of a group of people engaged\nin a social interaction. The core challenges in capturing social interactions\nare: (1) occlusion is functional and frequent; (2) subtle motion needs to be\nmeasured over a space large enough to host a social group; (3) human appearance\nand configuration variation is immense; and (4) attaching markers to the body\nmay prime the nature of interactions. The Panoptic Studio is a system organized\naround the thesis that social interactions should be measured through the\nintegration of perceptual analyses over a large variety of view points. We\npresent a modularized system designed around this principle, consisting of\nintegrated structural, hardware, and software innovations. The system takes, as\ninput, 480 synchronized video streams of multiple people engaged in social\nactivities, and produces, as output, the labeled time-varying 3D structure of\nanatomical landmarks on individuals in the space. Our algorithm is designed to\nfuse the \"weak\" perceptual processes in the large number of views by\nprogressively generating skeletal proposals from low-level appearance cues, and\na framework for temporal refinement is also presented by associating body parts\nto reconstructed dense 3D trajectory stream. Our system and method are the\nfirst in reconstructing full body motion of more than five people engaged in\nsocial interactions without using markers. We also empirically demonstrate the\nimpact of the number of views in achieving this goal.\n", "versions": [{"version": "v1", "created": "Fri, 9 Dec 2016 20:25:04 GMT"}], "update_date": "2016-12-12", "authors_parsed": [["Joo", "Hanbyul", ""], ["Simon", "Tomas", ""], ["Li", "Xulong", ""], ["Liu", "Hao", ""], ["Tan", "Lei", ""], ["Gui", "Lin", ""], ["Banerjee", "Sean", ""], ["Godisart", "Timothy", ""], ["Nabbe", "Bart", ""], ["Matthews", "Iain", ""], ["Kanade", "Takeo", ""], ["Nobuhara", "Shohei", ""], ["Sheikh", "Yaser", ""]]}, {"id": "1612.03217", "submitter": "Jianxu Chen", "authors": "Jianxu Chen, Chukka Srinivas", "title": "Automatic Lymphocyte Detection in H&E Images with Deep Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Automatic detection of lymphocyte in H&E images is a necessary first step in\nlots of tissue image analysis algorithms. An accurate and robust automated\nlymphocyte detection approach is of great importance in both computer science\nand clinical studies. Most of the existing approaches for lymphocyte detection\nare based on traditional image processing algorithms and/or classic machine\nlearning methods. In the recent years, deep learning techniques have\nfundamentally transformed the way that a computer interprets images and have\nbecome a matchless solution in various pattern recognition problems. In this\nwork, we design a new deep neural network model which extends the fully\nconvolutional network by combining the ideas in several recent techniques, such\nas shortcut links. Also, we design a new training scheme taking the prior\nknowledge about lymphocytes into consideration. The training scheme not only\nefficiently exploits the limited amount of free-form annotations from\npathologists, but also naturally supports efficient fine-tuning. As a\nconsequence, our model has the potential of self-improvement by leveraging the\nerrors collected during real applications. Our experiments show that our deep\nneural network model achieves good performance in the images of different\nstaining conditions or different types of tissues.\n", "versions": [{"version": "v1", "created": "Fri, 9 Dec 2016 23:31:35 GMT"}], "update_date": "2016-12-13", "authors_parsed": [["Chen", "Jianxu", ""], ["Srinivas", "Chukka", ""]]}, {"id": "1612.03236", "submitter": "Hieu Le", "authors": "Hieu Le, Chen-Ping Yu, Gregory Zelinsky, Dimitris Samaras", "title": "Co-localization with Category-Consistent Features and Geodesic Distance\n  Propagation", "comments": "IEEE International Conference on Computer Vision Workshops (ICCVW)", "journal-ref": null, "doi": "10.1109/ICCVW.2017.134", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Co-localization is the problem of localizing objects of the same class using\nonly the set of images that contain them. This is a challenging task because\nthe object detector must be built without negative examples that can lead to\nmore informative supervision signals. The main idea of our method is to cluster\nthe feature space of a generically pre-trained CNN, to find a set of CNN\nfeatures that are consistently and highly activated for an object category,\nwhich we call category-consistent CNN features. Then, we propagate their\ncombined activation map using superpixel geodesic distances for\nco-localization. In our first set of experiments, we show that the proposed\nmethod achieves state-of-the-art performance on three related benchmarks:\nPASCAL 2007, PASCAL-2012, and the Object Discovery dataset. We also show that\nour method is able to detect and localize truly unseen categories, on six\nheld-out ImageNet categories with accuracy that is significantly higher than\nprevious state-of-the-art. Our intuitive approach achieves this success without\nany region proposals or object detectors and can be based on a CNN that was\npre-trained purely on image classification tasks without further fine-tuning.\n", "versions": [{"version": "v1", "created": "Sat, 10 Dec 2016 01:43:01 GMT"}, {"version": "v2", "created": "Mon, 3 Apr 2017 17:09:56 GMT"}, {"version": "v3", "created": "Sun, 19 May 2019 08:47:42 GMT"}], "update_date": "2019-05-21", "authors_parsed": [["Le", "Hieu", ""], ["Yu", "Chen-Ping", ""], ["Zelinsky", "Gregory", ""], ["Samaras", "Dimitris", ""]]}, {"id": "1612.03242", "submitter": "Han Zhang", "authors": "Han Zhang, Tao Xu, Hongsheng Li, Shaoting Zhang, Xiaogang Wang,\n  Xiaolei Huang, Dimitris Metaxas", "title": "StackGAN: Text to Photo-realistic Image Synthesis with Stacked\n  Generative Adversarial Networks", "comments": "ICCV 2017 Oral Presentation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Synthesizing high-quality images from text descriptions is a challenging\nproblem in computer vision and has many practical applications. Samples\ngenerated by existing text-to-image approaches can roughly reflect the meaning\nof the given descriptions, but they fail to contain necessary details and vivid\nobject parts. In this paper, we propose Stacked Generative Adversarial Networks\n(StackGAN) to generate 256x256 photo-realistic images conditioned on text\ndescriptions. We decompose the hard problem into more manageable sub-problems\nthrough a sketch-refinement process. The Stage-I GAN sketches the primitive\nshape and colors of the object based on the given text description, yielding\nStage-I low-resolution images. The Stage-II GAN takes Stage-I results and text\ndescriptions as inputs, and generates high-resolution images with\nphoto-realistic details. It is able to rectify defects in Stage-I results and\nadd compelling details with the refinement process. To improve the diversity of\nthe synthesized images and stabilize the training of the conditional-GAN, we\nintroduce a novel Conditioning Augmentation technique that encourages\nsmoothness in the latent conditioning manifold. Extensive experiments and\ncomparisons with state-of-the-arts on benchmark datasets demonstrate that the\nproposed method achieves significant improvements on generating photo-realistic\nimages conditioned on text descriptions.\n", "versions": [{"version": "v1", "created": "Sat, 10 Dec 2016 03:11:37 GMT"}, {"version": "v2", "created": "Sat, 5 Aug 2017 02:18:21 GMT"}], "update_date": "2017-08-08", "authors_parsed": [["Zhang", "Han", ""], ["Xu", "Tao", ""], ["Li", "Hongsheng", ""], ["Zhang", "Shaoting", ""], ["Wang", "Xiaogang", ""], ["Huang", "Xiaolei", ""], ["Metaxas", "Dimitris", ""]]}, {"id": "1612.03268", "submitter": "Venkataraman Santhanam", "authors": "Venkataraman Santhanam, Vlad I. Morariu, Larry S. Davis", "title": "Generalized Deep Image to Image Regression", "comments": "Submitted to CVPR on November 15th, 2016. Code will be made available\n  soon", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a Deep Convolutional Neural Network architecture which serves as a\ngeneric image-to-image regressor that can be trained end-to-end without any\nfurther machinery. Our proposed architecture: the Recursively Branched\nDeconvolutional Network (RBDN) develops a cheap multi-context image\nrepresentation very early on using an efficient recursive branching scheme with\nextensive parameter sharing and learnable upsampling. This multi-context\nrepresentation is subjected to a highly non-linear locality preserving\ntransformation by the remainder of our network comprising of a series of\nconvolutions/deconvolutions without any spatial downsampling. The RBDN\narchitecture is fully convolutional and can handle variable sized images during\ninference. We provide qualitative/quantitative results on $3$ diverse tasks:\nrelighting, denoising and colorization and show that our proposed RBDN\narchitecture obtains comparable results to the state-of-the-art on each of\nthese tasks when used off-the-shelf without any post processing or\ntask-specific architectural modifications.\n", "versions": [{"version": "v1", "created": "Sat, 10 Dec 2016 08:22:27 GMT"}], "update_date": "2016-12-13", "authors_parsed": [["Santhanam", "Venkataraman", ""], ["Morariu", "Vlad I.", ""], ["Davis", "Larry S.", ""]]}, {"id": "1612.03273", "submitter": "Sankaraganesh Jonna", "authors": "Sankaraganesh Jonna and Krishna K. Nakka and Rajiv R. Sahay", "title": "Towards an Automated Image De-fencing Algorithm Using Sparsity", "comments": "The paper was accepted in VISAPP-2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conventional approaches to image de-fencing suffer from non-robust fence\ndetection and are limited to processing images of static scenes. In this\nposition paper, we propose an automatic de-fencing algorithm for images of\ndynamic scenes. We divide the problem of image de-fencing into the tasks of\nautomated fence detection, motion estimation and fusion of data from multiple\nframes of a captured video of the dynamic scene. Fences are detected\nautomatically using two approaches, namely, employing Gabor filter and a\nmachine learning method. We cast the fence removal problem in an optimization\nframework, by modeling the formation of the degraded observations. The inverse\nproblem is solved using split Bregman technique assuming total variation of the\nde-fenced image as the regularization constraint.\n", "versions": [{"version": "v1", "created": "Sat, 10 Dec 2016 10:07:24 GMT"}], "update_date": "2016-12-13", "authors_parsed": [["Jonna", "Sankaraganesh", ""], ["Nakka", "Krishna K.", ""], ["Sahay", "Rajiv R.", ""]]}, {"id": "1612.03284", "submitter": "Yongqing Liang", "authors": "Yongqing Liang", "title": "Salient Object Detection with Convex Hull Overlap", "comments": "Published in: 2018 IEEE International Conference on Big Data (Big\n  Data)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Salient object detection plays an important part in a vision system to detect\nimportant regions. Convolutional neural network (CNN) based methods directly\ntrain their models with large-scale datasets, but what is the crucial feature\nfor saliency is still a problem. In this paper, we establish a novel bottom-up\nfeature named convex hull overlap (CHO), combining with appearance contrast\nfeatures, to detect salient objects. CHO feature is a kind of enhanced Gestalt\ncue. Psychologists believe that surroundedness reflects objects overlap\nrelationship. An object which is on the top of the others is attractive. Our\nmethod significantly differs from other earlier works in (1) We set up a\nhand-crafted feature to detect salient object that our model does not need to\nbe trained by large-scale datasets; (2) Previous works only focus on appearance\nfeatures, while our CHO feature makes up the gap between the spatial object\ncovering and the object's saliency. Our experiments on a large number of public\ndatasets have obtained very positive results.\n", "versions": [{"version": "v1", "created": "Sat, 10 Dec 2016 12:42:10 GMT"}, {"version": "v2", "created": "Thu, 15 Oct 2020 18:14:11 GMT"}], "update_date": "2020-10-19", "authors_parsed": [["Liang", "Yongqing", ""]]}, {"id": "1612.03365", "submitter": "Marc-Andr\\'e Carbonneau", "authors": "Marc-Andr\\'e Carbonneau, Veronika Cheplygina, Eric Granger and\n  Ghyslain Gagnon", "title": "Multiple Instance Learning: A Survey of Problem Characteristics and\n  Applications", "comments": null, "journal-ref": null, "doi": "10.1016/j.patcog.2017.10.009", "report-no": null, "categories": "cs.CV cs.AI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multiple instance learning (MIL) is a form of weakly supervised learning\nwhere training instances are arranged in sets, called bags, and a label is\nprovided for the entire bag. This formulation is gaining interest because it\nnaturally fits various problems and allows to leverage weakly labeled data.\nConsequently, it has been used in diverse application fields such as computer\nvision and document classification. However, learning from bags raises\nimportant challenges that are unique to MIL. This paper provides a\ncomprehensive survey of the characteristics which define and differentiate the\ntypes of MIL problems. Until now, these problem characteristics have not been\nformally identified and described. As a result, the variations in performance\nof MIL algorithms from one data set to another are difficult to explain. In\nthis paper, MIL problem characteristics are grouped into four broad categories:\nthe composition of the bags, the types of data distribution, the ambiguity of\ninstance labels, and the task to be performed. Methods specialized to address\neach category are reviewed. Then, the extent to which these characteristics\nmanifest themselves in key MIL application areas are described. Finally,\nexperiments are conducted to compare the performance of 16 state-of-the-art MIL\nmethods on selected problem characteristics. This paper provides insight on how\nthe problem characteristics affect MIL algorithms, recommendations for future\nbenchmarking and promising avenues for research.\n", "versions": [{"version": "v1", "created": "Sun, 11 Dec 2016 02:19:22 GMT"}], "update_date": "2018-02-13", "authors_parsed": [["Carbonneau", "Marc-Andr\u00e9", ""], ["Cheplygina", "Veronika", ""], ["Granger", "Eric", ""], ["Gagnon", "Ghyslain", ""]]}, {"id": "1612.03373", "submitter": "Luyan Ji", "authors": "Jie Wang, Luyan Ji, Xiaomeng Huang, Haohuan Fu, Shiming Xu, Congcong\n  Li", "title": "A probabilistic graphical model approach in 30 m land cover mapping with\n  multiple data sources", "comments": "38 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is a trend to acquire high accuracy land-cover maps using multi-source\nclassification methods, most of which are based on data fusion, especially\npixel- or feature-level fusions. A probabilistic graphical model (PGM) approach\nis proposed in this research for 30 m resolution land-cover mapping with\nmulti-temporal Landsat and MODerate Resolution Imaging Spectroradiometer\n(MODIS) data. Independent classifiers were applied to two single-date Landsat 8\nscenes and the MODIS time-series data, respectively, for probability\nestimation. A PGM was created for each pixel in Landsat 8 data. Conditional\nprobability distributions were computed based on data quality and reliability\nby using information selectively. Using the administrative territory of Beijing\nCity (Area-1) and a coastal region of Shandong province, China (Area-2) as\nstudy areas, multiple land-cover maps were generated for comparison.\nQuantitative results show the effectiveness of the proposed method. Overall\naccuracies promoted from 74.0% (maps acquired from single-temporal Landsat\nimages) to 81.8% (output of the PGM) for Area-1. Improvements can also be seen\nwhen using MODIS data and only a single-temporal Landsat image as input\n(overall accuracy: 78.4% versus 74.0% for Area-1, and 86.8% versus 83.0% for\nArea-2). Information from MODIS data did not help much when the PGM was applied\nto cloud free regions of. One of the advantages of the proposed method is that\nit can be applied where multi-temporal data cannot be simply stacked as a\nmulti-layered image.\n", "versions": [{"version": "v1", "created": "Sun, 11 Dec 2016 06:02:41 GMT"}], "update_date": "2016-12-13", "authors_parsed": [["Wang", "Jie", ""], ["Ji", "Luyan", ""], ["Huang", "Xiaomeng", ""], ["Fu", "Haohuan", ""], ["Xu", "Shiming", ""], ["Li", "Congcong", ""]]}, {"id": "1612.03382", "submitter": "Sahar Yousefi", "authors": "Sahar Yousefi, M.T. Manzuri Shalmani, Jeremy Lin, Marius Staring", "title": "A Novel Motion Detection Method Resistant to Severe Illumination Changes", "comments": null, "journal-ref": null, "doi": "10.1109/TCSVT.2018.2885211", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Recently, there has been a considerable attention given to the motion\ndetection problem due to the explosive growth of its applications in video\nanalysis and surveillance systems. While the previous approaches can produce\ngood results, an accurate detection of motion remains a challenging task due to\nthe difficulties raised by illumination variations, occlusion, camouflage,\nburst physical motion, dynamic texture, and environmental changes such as those\non climate changes, sunlight changes during a day, etc. In this paper, we\npropose a novel per-pixel motion descriptor for both motion detection and\ndynamic texture segmentation which outperforms the current methods in the\nliterature particularly in severe scenarios. The proposed descriptor is based\non two complementary three-dimensional-discrete wavelet transform (3D-DWT) and\nthree-dimensional wavelet leader. In this approach, a feature vector is\nextracted for each pixel by applying a novel three dimensional wavelet-based\nmotion descriptor. Then, the extracted features are clustered by a clustering\nmethod such as well-known k-means algorithm or Gaussian Mixture Model (GMM).\nThe experimental results demonstrate the effectiveness of our proposed method\ncompared to the other motion detection approaches from the literature. The\napplication of the proposed method and additional experimental results for the\ndifferent datasets are available at\n(http://dspl.ce.sharif.edu/motiondetector.html).\n", "versions": [{"version": "v1", "created": "Sun, 11 Dec 2016 07:50:00 GMT"}, {"version": "v2", "created": "Wed, 1 Feb 2017 05:38:03 GMT"}, {"version": "v3", "created": "Mon, 8 May 2017 08:14:06 GMT"}, {"version": "v4", "created": "Wed, 10 May 2017 05:57:40 GMT"}, {"version": "v5", "created": "Thu, 5 Oct 2017 12:55:56 GMT"}, {"version": "v6", "created": "Thu, 15 Mar 2018 13:29:46 GMT"}], "update_date": "2021-03-29", "authors_parsed": [["Yousefi", "Sahar", ""], ["Shalmani", "M. T. Manzuri", ""], ["Lin", "Jeremy", ""], ["Staring", "Marius", ""]]}, {"id": "1612.03477", "submitter": "Dani\\\"el Reichman", "authors": "Dani\\\"el Reichman, Leslie M. Collins, and Jordan M. Malof", "title": "On Choosing Training and Testing Data for Supervised Algorithms in\n  Ground Penetrating Radar Data for Buried Threat Detection", "comments": "9 pages, 8 figures, journal paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ground penetrating radar (GPR) is one of the most popular and successful\nsensing modalities that has been investigated for landmine and subsurface\nthreat detection. Many of the detection algorithms applied to this task are\nsupervised and therefore require labeled examples of target and non-target data\nfor training. Training data most often consists of 2-dimensional images (or\npatches) of GPR data, from which features are extracted, and provided to the\nclassifier during training and testing. Identifying desirable training and\ntesting locations to extract patches, which we term \"keypoints\", is well\nestablished in the literature. In contrast however, a large variety of\nstrategies have been proposed regarding keypoint utilization (e.g., how many of\nthe identified keypoints should be used at targets, or non-target, locations).\nGiven the variety keypoint utilization strategies that are available, it is\nvery unclear (i) which strategies are best, or (ii) whether the choice of\nstrategy has a large impact on classifier performance. We address these\nquestions by presenting a taxonomy of existing utilization strategies, and then\nevaluating their effectiveness on a large dataset using many different\nclassifiers and features. We analyze the results and propose a new strategy,\ncalled PatchSelect, which outperforms other strategies across all experiments.\n", "versions": [{"version": "v1", "created": "Sun, 11 Dec 2016 21:05:18 GMT"}], "update_date": "2016-12-13", "authors_parsed": [["Reichman", "Dani\u00ebl", ""], ["Collins", "Leslie M.", ""], ["Malof", "Jordan M.", ""]]}, {"id": "1612.03530", "submitter": "Diqi Chen", "authors": "Diqi Chen, Yizhou Wang, Tianfu Wu, Wen Gao", "title": "An Attention-Driven Approach of No-Reference Image Quality Assessment", "comments": "9 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a novel method of no-reference image quality\nassessment (NR-IQA), which is to predict the perceptual quality score of a\ngiven image without using any reference image. The proposed method harnesses\nthree functions (i) the visual attention mechanism, which affects many aspects\nof visual perception including image quality assessment, however, is overlooked\nin the NR-IQA literature. The method assumes that the fixation areas on an\nimage contain key information to the process of IQA. (ii) the robust averaging\nstrategy, which is a means \\--- supported by psychology studies \\--- to\nintegrating multiple/step-wise evidence to make a final perceptual judgment.\n(iii) the multi-task learning, which is believed to be an effectual means to\nshape representation learning and could result in a more generalized model.\n  To exploit the synergy of the three, we consider the NR-IQA as a dynamic\nperception process, in which the model samples a sequence of \"informative\"\nareas and aggregates the information to learn a representation for the tasks of\njointly predicting the image quality score and the distortion type.\n  The model learning is implemented by a reinforcement strategy, in which the\nrewards of both tasks guide the learning of the optimal sampling policy to\nacquire the \"task-informative\" image regions so that the predictions can be\nmade accurately and efficiently (in terms of the sampling steps). The\nreinforcement learning is realized by a deep network with the policy gradient\nmethod and trained through back-propagation.\n  In experiments, the model is tested on the TID2008 dataset and it outperforms\nseveral state-of-the-art methods. Furthermore, the model is very efficient in\nthe sense that a small number of fixations are used in NR-IQA.\n", "versions": [{"version": "v1", "created": "Mon, 12 Dec 2016 03:25:35 GMT"}, {"version": "v2", "created": "Tue, 21 Mar 2017 01:46:45 GMT"}, {"version": "v3", "created": "Mon, 29 May 2017 02:42:28 GMT"}], "update_date": "2017-05-30", "authors_parsed": [["Chen", "Diqi", ""], ["Wang", "Yizhou", ""], ["Wu", "Tianfu", ""], ["Gao", "Wen", ""]]}, {"id": "1612.03550", "submitter": "Dongkuan Xu", "authors": "Dongkuan Xu, Jia Wu, Wei Zhang, Yingjie Tian", "title": "PIGMIL: Positive Instance Detection via Graph Updating for Multiple\n  Instance Learning", "comments": "11 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Positive instance detection, especially for these in positive bags (true\npositive instances, TPIs), plays a key role for multiple instance learning\n(MIL) arising from a specific classification problem only provided with bag (a\nset of instances) label information. However, most previous MIL methods on this\nissue ignore the global similarity among positive instances and that negative\ninstances are non-i.i.d., usually resulting in the detection of TPI not precise\nand sensitive to outliers. To the end, we propose a positive instance detection\nvia graph updating for multiple instance learning, called PIGMIL, to detect TPI\naccurately. PIGMIL selects instances from working sets (WSs) of some working\nbags (WBs) as positive candidate pool (PCP). The global similarity among\npositive instances and the robust discrimination of instances of PCP from\nnegative instances are measured to construct the consistent similarity and\ndiscrimination graph (CSDG). As a result, the primary goal (i.e. TPI detection)\nis transformed into PCP updating, which is approximated efficiently by updating\nCSDG with a random walk ranking algorithm and an instance updating strategy. At\nlast bags are transformed into feature representation vector based on the\nidentified TPIs to train a classifier. Extensive experiments demonstrate the\nhigh precision of PIGMIL's detection of TPIs and its excellent performance\ncompared to classic baseline MIL methods.\n", "versions": [{"version": "v1", "created": "Mon, 12 Dec 2016 06:12:19 GMT"}], "update_date": "2016-12-13", "authors_parsed": [["Xu", "Dongkuan", ""], ["Wu", "Jia", ""], ["Zhang", "Wei", ""], ["Tian", "Yingjie", ""]]}, {"id": "1612.03557", "submitter": "Jonghwan Mun", "authors": "Jonghwan Mun, Minsu Cho, Bohyung Han", "title": "Text-guided Attention Model for Image Captioning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual attention plays an important role to understand images and\ndemonstrates its effectiveness in generating natural language descriptions of\nimages. On the other hand, recent studies show that language associated with an\nimage can steer visual attention in the scene during our cognitive process.\nInspired by this, we introduce a text-guided attention model for image\ncaptioning, which learns to drive visual attention using associated captions.\nFor this model, we propose an exemplar-based learning approach that retrieves\nfrom training data associated captions with each image, and use them to learn\nattention on visual features. Our attention model enables to describe a\ndetailed state of scenes by distinguishing small or confusable objects\neffectively. We validate our model on MS-COCO Captioning benchmark and achieve\nthe state-of-the-art performance in standard metrics.\n", "versions": [{"version": "v1", "created": "Mon, 12 Dec 2016 06:52:36 GMT"}], "update_date": "2016-12-13", "authors_parsed": [["Mun", "Jonghwan", ""], ["Cho", "Minsu", ""], ["Han", "Bohyung", ""]]}, {"id": "1612.03590", "submitter": "Qiulei Dong", "authors": "Qiulei Dong and Zhanyi Hu", "title": "Statistics of Visual Responses to Object Stimuli from Primate AIT\n  Neurons to DNN Neurons", "comments": null, "journal-ref": "Neural Computation, 30, 447-476 (2018)", "doi": "10.1162/NECO_a_01039", "report-no": null, "categories": "cs.CV q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cadieu et al. (Cadieu,2014) reported that deep neural networks(DNNs) could\nrival the representation of primate inferotemporal cortex for object\nrecognition. Lehky et al. (Lehky,2011) provided a statistical analysis on\nneural responses to object stimuli in primate AIT cortex. They found the\nintrinsic dimensionality of object representations in AIT cortex is around 100\n(Lehky,2014). Considering the outstanding performance of DNNs in object\nrecognition, it is worthwhile investigating whether the responses of DNN\nneurons have similar response statistics to those of AIT neurons. Following\nLehky et al.'s works, we analyze the response statistics to image stimuli and\nthe intrinsic dimensionality of object representations of DNN neurons. Our\nfindings show in terms of kurtosis and Pareto tail index, the response\nstatistics on single-neuron selectivity and population sparseness of DNN\nneurons are fundamentally different from those of IT neurons except some\nspecial cases. By increasing the number of neurons and stimuli, the conclusions\ncould alter substantially. In addition, with the ascendancy of the\nconvolutional layers of DNNs, the single-neuron selectivity and population\nsparseness of DNN neurons increase, indicating the last convolutional layer is\nto learn features for object representations, while the following\nfully-connected layers are to learn categorization features. It is also found\nthat a sufficiently large number of stimuli and neurons are necessary for\nobtaining a stable dimensionality. To our knowledge, this is the first work to\nanalyze the response statistics of DNN neurons comparing with AIT neurons, and\nour results provide not only some insights into the discrepancy of DNN neurons\nwith respect to IT neurons in object representation, but also shed some light\non possible outcomes of IT neurons when the number of recorded neurons and\nstimuli is beyond the level in (Lehky,2011,2014).\n", "versions": [{"version": "v1", "created": "Mon, 12 Dec 2016 10:13:15 GMT"}, {"version": "v2", "created": "Fri, 20 Jan 2017 02:23:42 GMT"}], "update_date": "2019-06-07", "authors_parsed": [["Dong", "Qiulei", ""], ["Hu", "Zhanyi", ""]]}, {"id": "1612.03628", "submitter": "Marc Bola\\~nos", "authors": "Marc Bola\\~nos, \\'Alvaro Peris, Francisco Casacuberta, Petia Radeva", "title": "VIBIKNet: Visual Bidirectional Kernelized Network for Visual Question\n  Answering", "comments": "Submitted to IbPRIA'17, 8 pages, 3 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we address the problem of visual question answering by\nproposing a novel model, called VIBIKNet. Our model is based on integrating\nKernelized Convolutional Neural Networks and Long-Short Term Memory units to\ngenerate an answer given a question about an image. We prove that VIBIKNet is\nan optimal trade-off between accuracy and computational load, in terms of\nmemory and time consumption. We validate our method on the VQA challenge\ndataset and compare it to the top performing methods in order to illustrate its\nperformance and speed.\n", "versions": [{"version": "v1", "created": "Mon, 12 Dec 2016 11:41:46 GMT"}], "update_date": "2016-12-13", "authors_parsed": [["Bola\u00f1os", "Marc", ""], ["Peris", "\u00c1lvaro", ""], ["Casacuberta", "Francisco", ""], ["Radeva", "Petia", ""]]}, {"id": "1612.03630", "submitter": "Zichuan Liu", "authors": "Zichuan Liu, Yixing Li, Fengbo Ren, Hao Yu", "title": "A Binary Convolutional Encoder-decoder Network for Real-time Natural\n  Scene Text Processing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we develop a binary convolutional encoder-decoder network\n(B-CEDNet) for natural scene text processing (NSTP). It converts a text image\nto a class-distinguished salience map that reveals the categorical, spatial and\nmorphological information of characters. The existing solutions are either\nmemory consuming or run-time consuming that cannot be applied to real-time\napplications on resource-constrained devices such as advanced driver assistance\nsystems. The developed network can process multiple regions containing\ncharacters by one-off forward operation, and is trained to have binary weights\nand binary feature maps, which lead to both remarkable inference run-time\nspeedup and memory usage reduction. By training with over 200, 000 synthesis\nscene text images (size of $32\\times128$), it can achieve $90\\%$ and $91\\%$\npixel-wise accuracy on ICDAR-03 and ICDAR-13 datasets. It only consumes $4.59\\\nms$ inference run-time realized on GPU with a small network size of 2.14 MB,\nwhich is up to $8\\times$ faster and $96\\%$ smaller than it full-precision\nversion.\n", "versions": [{"version": "v1", "created": "Mon, 12 Dec 2016 11:48:00 GMT"}], "update_date": "2016-12-13", "authors_parsed": [["Liu", "Zichuan", ""], ["Li", "Yixing", ""], ["Ren", "Fengbo", ""], ["Yu", "Hao", ""]]}, {"id": "1612.03663", "submitter": "Maksim Lapin", "authors": "Maksim Lapin, Matthias Hein, and Bernt Schiele", "title": "Analysis and Optimization of Loss Functions for Multiclass, Top-k, and\n  Multilabel Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Top-k error is currently a popular performance measure on large scale image\nclassification benchmarks such as ImageNet and Places. Despite its wide\nacceptance, our understanding of this metric is limited as most of the previous\nresearch is focused on its special case, the top-1 error. In this work, we\nexplore two directions that shed more light on the top-k error. First, we\nprovide an in-depth analysis of established and recently proposed single-label\nmulticlass methods along with a detailed account of efficient optimization\nalgorithms for them. Our results indicate that the softmax loss and the smooth\nmulticlass SVM are surprisingly competitive in top-k error uniformly across all\nk, which can be explained by our analysis of multiclass top-k calibration.\nFurther improvements for a specific k are possible with a number of proposed\ntop-k loss functions. Second, we use the top-k methods to explore the\ntransition from multiclass to multilabel learning. In particular, we find that\nit is possible to obtain effective multilabel classifiers on Pascal VOC using a\nsingle label per image for training, while the gap between multiclass and\nmultilabel methods on MS COCO is more significant. Finally, our contribution of\nefficient algorithms for training with the considered top-k and multilabel loss\nfunctions is of independent interest.\n", "versions": [{"version": "v1", "created": "Mon, 12 Dec 2016 13:20:09 GMT"}], "update_date": "2016-12-19", "authors_parsed": [["Lapin", "Maksim", ""], ["Hein", "Matthias", ""], ["Schiele", "Bernt", ""]]}, {"id": "1612.03705", "submitter": "Francisco Aparecido Rodrigues", "authors": "Oscar A. C. Linares, Glenda Michele Botelho, Francisco Aparecido\n  Rodrigues, Jo\\~ao Batista Neto", "title": "Segmentation of large images based on super-pixels and community\n  detection in graphs", "comments": "20 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image segmentation has many applications which range from machine learning to\nmedical diagnosis. In this paper, we propose a framework for the segmentation\nof images based on super-pixels and algorithms for community identification in\ngraphs. The super-pixel pre-segmentation step reduces the number of nodes in\nthe graph, rendering the method the ability to process large images. Moreover,\ncommunity detection algorithms provide more accurate segmentation than\ntraditional approaches, such as those based on spectral graph partition. We\nalso compare our method with two algorithms: a) the graph-based approach by\nFelzenszwalb and Huttenlocher and b) the contour-based method by Arbelaez.\nResults have shown that our method provides more precise segmentation and is\nfaster than both of them.\n", "versions": [{"version": "v1", "created": "Mon, 12 Dec 2016 14:31:29 GMT"}], "update_date": "2016-12-13", "authors_parsed": [["Linares", "Oscar A. C.", ""], ["Botelho", "Glenda Michele", ""], ["Rodrigues", "Francisco Aparecido", ""], ["Neto", "Jo\u00e3o Batista", ""]]}, {"id": "1612.03716", "submitter": "Holger Caesar", "authors": "Holger Caesar, Jasper Uijlings, Vittorio Ferrari", "title": "COCO-Stuff: Thing and Stuff Classes in Context", "comments": "CVPR 2018 camera-ready", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semantic classes can be either things (objects with a well-defined shape,\ne.g. car, person) or stuff (amorphous background regions, e.g. grass, sky).\nWhile lots of classification and detection works focus on thing classes, less\nattention has been given to stuff classes. Nonetheless, stuff classes are\nimportant as they allow to explain important aspects of an image, including (1)\nscene type; (2) which thing classes are likely to be present and their location\n(through contextual reasoning); (3) physical attributes, material types and\ngeometric properties of the scene. To understand stuff and things in context we\nintroduce COCO-Stuff, which augments all 164K images of the COCO 2017 dataset\nwith pixel-wise annotations for 91 stuff classes. We introduce an efficient\nstuff annotation protocol based on superpixels, which leverages the original\nthing annotations. We quantify the speed versus quality trade-off of our\nprotocol and explore the relation between annotation time and boundary\ncomplexity. Furthermore, we use COCO-Stuff to analyze: (a) the importance of\nstuff and thing classes in terms of their surface cover and how frequently they\nare mentioned in image captions; (b) the spatial relations between stuff and\nthings, highlighting the rich contextual relations that make our dataset\nunique; (c) the performance of a modern semantic segmentation method on stuff\nand thing classes, and whether stuff is easier to segment than things.\n", "versions": [{"version": "v1", "created": "Mon, 12 Dec 2016 14:46:23 GMT"}, {"version": "v2", "created": "Wed, 8 Mar 2017 12:36:04 GMT"}, {"version": "v3", "created": "Tue, 27 Mar 2018 16:38:47 GMT"}, {"version": "v4", "created": "Wed, 28 Mar 2018 10:15:59 GMT"}], "update_date": "2018-03-29", "authors_parsed": [["Caesar", "Holger", ""], ["Uijlings", "Jasper", ""], ["Ferrari", "Vittorio", ""]]}, {"id": "1612.03777", "submitter": "Nima Sedaghat Alvar", "authors": "Nima Sedaghat, Mohammadreza Zolfaghari, Thomas Brox", "title": "Hybrid Learning of Optical Flow and Next Frame Prediction to Boost\n  Optical Flow in the Wild", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  CNN-based optical flow estimation has attracted attention recently, mainly\ndue to its impressively high frame rates. These networks perform well on\nsynthetic datasets, but they are still far behind the classical methods in\nreal-world videos. This is because there is no ground truth optical flow for\ntraining these networks on real data. In this paper, we boost CNN-based optical\nflow estimation in real scenes with the help of the freely available\nself-supervised task of next-frame prediction. To this end, we train the\nnetwork in a hybrid way, providing it with a mixture of synthetic and real\nvideos. With the help of a sample-variant multi-tasking architecture, the\nnetwork is trained on different tasks depending on the availability of\nground-truth. We also experiment with the prediction of \"next-flow\" instead of\nestimation of the current flow, which is intuitively closer to the task of\nnext-frame prediction and yields favorable results. We demonstrate the\nimprovement in optical flow estimation on the real-world KITTI benchmark.\nAdditionally, we test the optical flow indirectly in an action classification\nscenario. As a side product of this work, we report significant improvements\nover state-of-the-art in the task of next-frame prediction.\n", "versions": [{"version": "v1", "created": "Mon, 12 Dec 2016 16:45:08 GMT"}, {"version": "v2", "created": "Fri, 7 Apr 2017 13:01:12 GMT"}], "update_date": "2017-04-10", "authors_parsed": [["Sedaghat", "Nima", ""], ["Zolfaghari", "Mohammadreza", ""], ["Brox", "Thomas", ""]]}, {"id": "1612.03779", "submitter": "Alexander Krull", "authors": "Alexander Krull, Eric Brachmann, Sebastian Nowozin, Frank Michel,\n  Jamie Shotton, Carsten Rother", "title": "PoseAgent: Budget-Constrained 6D Object Pose Estimation via\n  Reinforcement Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  State-of-the-art computer vision algorithms often achieve efficiency by\nmaking discrete choices about which hypotheses to explore next. This allows\nallocation of computational resources to promising candidates, however, such\ndecisions are non-differentiable. As a result, these algorithms are hard to\ntrain in an end-to-end fashion. In this work we propose to learn an efficient\nalgorithm for the task of 6D object pose estimation. Our system optimizes the\nparameters of an existing state-of-the art pose estimation system using\nreinforcement learning, where the pose estimation system now becomes the\nstochastic policy, parametrized by a CNN. Additionally, we present an efficient\ntraining algorithm that dramatically reduces computation time. We show\nempirically that our learned pose estimation procedure makes better use of\nlimited resources and improves upon the state-of-the-art on a challenging\ndataset. Our approach enables differentiable end-to-end training of complex\nalgorithmic pipelines and learns to make optimal use of a given computational\nbudget.\n", "versions": [{"version": "v1", "created": "Mon, 12 Dec 2016 16:50:47 GMT"}, {"version": "v2", "created": "Tue, 11 Apr 2017 08:24:22 GMT"}], "update_date": "2017-04-12", "authors_parsed": [["Krull", "Alexander", ""], ["Brachmann", "Eric", ""], ["Nowozin", "Sebastian", ""], ["Michel", "Frank", ""], ["Shotton", "Jamie", ""], ["Rother", "Carsten", ""]]}, {"id": "1612.03809", "submitter": "Mehdi Mirza", "authors": "Mehdi Mirza, Aaron Courville, Yoshua Bengio", "title": "Generalizable Features From Unsupervised Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Humans learn a predictive model of the world and use this model to reason\nabout future events and the consequences of actions. In contrast to most\nmachine predictors, we exhibit an impressive ability to generalize to unseen\nscenarios and reason intelligently in these settings. One important aspect of\nthis ability is physical intuition(Lake et al., 2016). In this work, we explore\nthe potential of unsupervised learning to find features that promote better\ngeneralization to settings outside the supervised training distribution. Our\ntask is predicting the stability of towers of square blocks. We demonstrate\nthat an unsupervised model, trained to predict future frames of a video\nsequence of stable and unstable block configurations, can yield features that\nsupport extrapolating stability prediction to blocks configurations outside the\ntraining set distribution\n", "versions": [{"version": "v1", "created": "Mon, 12 Dec 2016 17:45:48 GMT"}], "update_date": "2016-12-13", "authors_parsed": [["Mirza", "Mehdi", ""], ["Courville", "Aaron", ""], ["Bengio", "Yoshua", ""]]}, {"id": "1612.03897", "submitter": "Chen-Hsuan Lin", "authors": "Chen-Hsuan Lin, Simon Lucey", "title": "Inverse Compositional Spatial Transformer Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we establish a theoretical connection between the classical\nLucas & Kanade (LK) algorithm and the emerging topic of Spatial Transformer\nNetworks (STNs). STNs are of interest to the vision and learning communities\ndue to their natural ability to combine alignment and classification within the\nsame theoretical framework. Inspired by the Inverse Compositional (IC) variant\nof the LK algorithm, we present Inverse Compositional Spatial Transformer\nNetworks (IC-STNs). We demonstrate that IC-STNs can achieve better performance\nthan conventional STNs with less model capacity; in particular, we show\nsuperior performance in pure image alignment tasks as well as joint\nalignment/classification problems on real-world problems.\n", "versions": [{"version": "v1", "created": "Mon, 12 Dec 2016 20:53:05 GMT"}], "update_date": "2016-12-13", "authors_parsed": [["Lin", "Chen-Hsuan", ""], ["Lucey", "Simon", ""]]}, {"id": "1612.03900", "submitter": "Xiaofang Wang", "authors": "Xiaofang Wang, Yi Shi and Kris M. Kitani", "title": "Deep Supervised Hashing with Triplet Labels", "comments": "Appear in ACCV 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hashing is one of the most popular and powerful approximate nearest neighbor\nsearch techniques for large-scale image retrieval. Most traditional hashing\nmethods first represent images as off-the-shelf visual features and then\nproduce hashing codes in a separate stage. However, off-the-shelf visual\nfeatures may not be optimally compatible with the hash code learning procedure,\nwhich may result in sub-optimal hash codes. Recently, deep hashing methods have\nbeen proposed to simultaneously learn image features and hash codes using deep\nneural networks and have shown superior performance over traditional hashing\nmethods. Most deep hashing methods are given supervised information in the form\nof pairwise labels or triplet labels. The current state-of-the-art deep hashing\nmethod DPSH~\\cite{li2015feature}, which is based on pairwise labels, performs\nimage feature learning and hash code learning simultaneously by maximizing the\nlikelihood of pairwise similarities. Inspired by DPSH~\\cite{li2015feature}, we\npropose a triplet label based deep hashing method which aims to maximize the\nlikelihood of the given triplet labels. Experimental results show that our\nmethod outperforms all the baselines on CIFAR-10 and NUS-WIDE datasets,\nincluding the state-of-the-art method DPSH~\\cite{li2015feature} and all the\nprevious triplet label based deep hashing methods.\n", "versions": [{"version": "v1", "created": "Mon, 12 Dec 2016 20:56:38 GMT"}], "update_date": "2016-12-13", "authors_parsed": [["Wang", "Xiaofang", ""], ["Shi", "Yi", ""], ["Kitani", "Kris M.", ""]]}, {"id": "1612.03925", "submitter": "Jose Dolz", "authors": "J. Dolz, C. Desrosiers, I. Ben Ayed", "title": "3D fully convolutional networks for subcortical segmentation in MRI: A\n  large-scale study", "comments": "Accepted in the special issue of Neuroimage: \"Brain Segmentation and\n  Parcellation\"", "journal-ref": null, "doi": "10.1016/j.neuroimage.2017.04.039", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This study investigates a 3D and fully convolutional neural network (CNN) for\nsubcortical brain structure segmentation in MRI. 3D CNN architectures have been\ngenerally avoided due to their computational and memory requirements during\ninference. We address the problem via small kernels, allowing deeper\narchitectures. We further model both local and global context by embedding\nintermediate-layer outputs in the final prediction, which encourages\nconsistency between features extracted at different scales and embeds\nfine-grained information directly in the segmentation process. Our model is\nefficiently trained end-to-end on a graphics processing unit (GPU), in a single\nstage, exploiting the dense inference capabilities of fully CNNs.\n  We performed comprehensive experiments over two publicly available datasets.\nFirst, we demonstrate a state-of-the-art performance on the ISBR dataset. Then,\nwe report a {\\em large-scale} multi-site evaluation over 1112 unregistered\nsubject datasets acquired from 17 different sites (ABIDE dataset), with ages\nranging from 7 to 64 years, showing that our method is robust to various\nacquisition protocols, demographics and clinical factors. Our method yielded\nsegmentations that are highly consistent with a standard atlas-based approach,\nwhile running in a fraction of the time needed by atlas-based methods and\navoiding registration/normalization steps. This makes it convenient for massive\nmulti-site neuroanatomical imaging studies. To the best of our knowledge, our\nwork is the first to study subcortical structure segmentation on such\nlarge-scale and heterogeneous data.\n", "versions": [{"version": "v1", "created": "Mon, 12 Dec 2016 21:09:06 GMT"}, {"version": "v2", "created": "Thu, 20 Apr 2017 02:03:35 GMT"}], "update_date": "2017-04-21", "authors_parsed": [["Dolz", "J.", ""], ["Desrosiers", "C.", ""], ["Ayed", "I. Ben", ""]]}, {"id": "1612.03928", "submitter": "Sergey Zagoruyko", "authors": "Sergey Zagoruyko and Nikos Komodakis", "title": "Paying More Attention to Attention: Improving the Performance of\n  Convolutional Neural Networks via Attention Transfer", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Attention plays a critical role in human visual experience. Furthermore, it\nhas recently been demonstrated that attention can also play an important role\nin the context of applying artificial neural networks to a variety of tasks\nfrom fields such as computer vision and NLP. In this work we show that, by\nproperly defining attention for convolutional neural networks, we can actually\nuse this type of information in order to significantly improve the performance\nof a student CNN network by forcing it to mimic the attention maps of a\npowerful teacher network. To that end, we propose several novel methods of\ntransferring attention, showing consistent improvement across a variety of\ndatasets and convolutional neural network architectures. Code and models for\nour experiments are available at\nhttps://github.com/szagoruyko/attention-transfer\n", "versions": [{"version": "v1", "created": "Mon, 12 Dec 2016 21:15:57 GMT"}, {"version": "v2", "created": "Tue, 24 Jan 2017 23:26:16 GMT"}, {"version": "v3", "created": "Sun, 12 Feb 2017 22:05:47 GMT"}], "update_date": "2017-02-14", "authors_parsed": [["Zagoruyko", "Sergey", ""], ["Komodakis", "Nikos", ""]]}, {"id": "1612.03959", "submitter": "Tomoyoshi Shimobaba Dr.", "authors": "Tomoyoshi Shimobaba, Yutaka Endo, Ryuji Hirayama, Yuki Nagahama,\n  Takayuki Takahashi, Takashi Nishitsuji, Takashi Kakue, Atsushi Shiraki, Naoki\n  Takada, Nobuyuki Masuda, Tomoyoshi Ito", "title": "Autoencoder-based holographic image restoration", "comments": null, "journal-ref": null, "doi": "10.1364/AO.56.000F27", "report-no": null, "categories": "cs.CV physics.optics", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a holographic image restoration method using an autoencoder, which\nis an artificial neural network. Because holographic reconstructed images are\noften contaminated by direct light, conjugate light, and speckle noise, the\ndiscrimination of reconstructed images may be difficult. In this paper, we\ndemonstrate the restoration of reconstructed images from holograms that record\npage data in holographic memory and QR codes by using the proposed method.\n", "versions": [{"version": "v1", "created": "Mon, 12 Dec 2016 22:49:03 GMT"}], "update_date": "2017-04-05", "authors_parsed": [["Shimobaba", "Tomoyoshi", ""], ["Endo", "Yutaka", ""], ["Hirayama", "Ryuji", ""], ["Nagahama", "Yuki", ""], ["Takahashi", "Takayuki", ""], ["Nishitsuji", "Takashi", ""], ["Kakue", "Takashi", ""], ["Shiraki", "Atsushi", ""], ["Takada", "Naoki", ""], ["Masuda", "Nobuyuki", ""], ["Ito", "Tomoyoshi", ""]]}, {"id": "1612.03961", "submitter": "Arjun Raj Rajanna", "authors": "Arjun Raj Rajanna, Kamelia Aryafar, Rajeev Ramchandran, Christye\n  Sisson, Ali Shokoufandeh, Raymond Ptucha", "title": "Neural Networks with Manifold Learning for Diabetic Retinopathy\n  Detection", "comments": "Published in Proceedings of \"IEEE Western NY Image & Signal\n  Processing Workshop\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Widespread outreach programs using remote retinal imaging have proven to\ndecrease the risk from diabetic retinopathy, the leading cause of blindness in\nthe US. However, this process still requires manual verification of image\nquality and grading of images for level of disease by a trained human grader\nand will continue to be limited by the lack of such scarce resources.\nComputer-aided diagnosis of retinal images have recently gained increasing\nattention in the machine learning community. In this paper, we introduce a set\nof neural networks for diabetic retinopathy classification of fundus retinal\nimages. We evaluate the efficiency of the proposed classifiers in combination\nwith preprocessing and augmentation steps on a sample dataset. Our experimental\nresults show that neural networks in combination with preprocessing on the\nimages can boost the classification accuracy on this dataset. Moreover the\nproposed models are scalable and can be used in large scale datasets for\ndiabetic retinopathy detection. The models introduced in this paper can be used\nto facilitate the diagnosis and speed up the detection process.\n", "versions": [{"version": "v1", "created": "Mon, 12 Dec 2016 22:51:17 GMT"}], "update_date": "2016-12-14", "authors_parsed": [["Rajanna", "Arjun Raj", ""], ["Aryafar", "Kamelia", ""], ["Ramchandran", "Rajeev", ""], ["Sisson", "Christye", ""], ["Shokoufandeh", "Ali", ""], ["Ptucha", "Raymond", ""]]}, {"id": "1612.03982", "submitter": "Marcel Sheeny De Moraes", "authors": "Marcel Sheeny de Moraes, Sankha Mukherjee, Neil M Robertson", "title": "Deep Convolutional Poses for Human Interaction Recognition in Monocular\n  Videos", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Human interaction recognition is a challenging problem in computer vision and\nhas been researched over the years due to its important applications. With the\ndevelopment of deep models for the human pose estimation problem, this work\naims to verify the effectiveness of using the human pose in order to recognize\nthe human interaction in monocular videos. This paper developed a method based\non 5 steps: detect each person in the scene, track them, retrieve the human\npose, extract features based on the pose and finally recognize the interaction\nusing a classifier. The Two-Person interaction dataset was used for the\ndevelopment of this methodology. Using a whole sequence evaluation approach it\nachieved 87.56% of average accuracy of all interaction. Yun, et at achieved\n91.10% using the same dataset, however their methodology used the depth sensor\nto recognize the interaction. The methodology developed in this paper shows\nthat an RGB camera can be as effective as depth cameras to recognize the\ninteraction between two persons using the recent development of deep models to\nestimate the human pose.\n", "versions": [{"version": "v1", "created": "Tue, 13 Dec 2016 00:22:58 GMT"}], "update_date": "2016-12-14", "authors_parsed": [["de Moraes", "Marcel Sheeny", ""], ["Mukherjee", "Sankha", ""], ["Robertson", "Neil M", ""]]}, {"id": "1612.03989", "submitter": "Sunil Kumar", "authors": "Sunil Kumar, J. V. Desai and Shaktidev Mukherjee", "title": "A Fast Keypoint Based Hybrid Method for Copy Move Forgery Detection", "comments": null, "journal-ref": null, "doi": "10.12785/ijcds/040203", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Copy move forgery detection in digital images has become a very popular\nresearch topic in the area of image forensics. Due to the availability of\nsophisticated image editing tools and ever increasing hardware capabilities, it\nhas become an easy task to manipulate the digital images. Passive forgery\ndetection techniques are more relevant as they can be applied without the prior\ninformation about the image in question. Block based techniques are used to\ndetect copy move forgery, but have limitations of large time complexity and\nsensitivity against affine operations like rotation and scaling. Keypoint based\napproaches are used to detect forgery in large images where the possibility of\nsignificant post processing operations like rotation and scaling is more. A\nhybrid approach is proposed using different methods for keypoint detection and\ndescription. Speeded Up Robust Features (SURF) are used to detect the keypoints\nin the image and Binary Robust Invariant Scalable Keypoints (BRISK) features\nare used to describe features at these keypoints. The proposed method has\nperformed better than the existing forgery detection method using SURF\nsignificantly in terms of detection speed and is invariant to post processing\noperations like rotation and scaling. The proposed method is also invariant to\nother commonly applied post processing operations like adding Gaussian noise\nand JPEG compression\n", "versions": [{"version": "v1", "created": "Sun, 11 Dec 2016 03:10:27 GMT"}], "update_date": "2017-02-16", "authors_parsed": [["Kumar", "Sunil", ""], ["Desai", "J. V.", ""], ["Mukherjee", "Shaktidev", ""]]}, {"id": "1612.04007", "submitter": "Ronnachai Jaroensri", "authors": "Ronnachai Jaroensri, Amy Zhao, Guha Balakrishnan, Derek Lo, Jeremy\n  Schmahmann, John Guttag, Fredo Durand", "title": "A Video-Based Method for Objectively Rating Ataxia", "comments": "MLHC 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  For many movement disorders, such as Parkinson's disease and ataxia, disease\nprogression is visually assessed by a clinician using a numerical disease\nrating scale. These tests are subjective, time-consuming, and must be\nadministered by a professional. This can be problematic where specialists are\nnot available, or when a patient is not consistently evaluated by the same\nclinician. We present an automated method for quantifying the severity of\nmotion impairment in patients with ataxia, using only video recordings. We\nconsider videos of the finger-to-nose test, a common movement task used as part\nof the assessment of ataxia progression during the course of routine clinical\ncheckups.\n  Our method uses neural network-based pose estimation and optical flow\ntechniques to track the motion of the patient's hand in a video recording. We\nextract features that describe qualities of the motion such as speed and\nvariation in performance. Using labels provided by an expert clinician, we\ntrain a supervised learning model that predicts severity according to the Brief\nAtaxia Rating Scale (BARS). The performance of our system is comparable to that\nof a group of ataxia specialists in terms of mean error and correlation, and\nour system's predictions were consistently within the range of inter-rater\nvariability. This work demonstrates the feasibility of using computer vision\nand machine learning to produce consistent and clinically useful measures of\nmotor impairment.\n", "versions": [{"version": "v1", "created": "Tue, 13 Dec 2016 03:21:57 GMT"}, {"version": "v2", "created": "Fri, 20 Jan 2017 02:06:27 GMT"}, {"version": "v3", "created": "Thu, 7 Sep 2017 16:43:21 GMT"}], "update_date": "2017-09-08", "authors_parsed": [["Jaroensri", "Ronnachai", ""], ["Zhao", "Amy", ""], ["Balakrishnan", "Guha", ""], ["Lo", "Derek", ""], ["Schmahmann", "Jeremy", ""], ["Guttag", "John", ""], ["Durand", "Fredo", ""]]}, {"id": "1612.04052", "submitter": "Bodo Rueckauer", "authors": "Bodo Rueckauer, Iulia-Alexandra Lungu, Yuhuang Hu, Michael Pfeiffer", "title": "Theory and Tools for the Conversion of Analog to Spiking Convolutional\n  Neural Networks", "comments": "9 pages, 2 figures, presented at the workshop \"Computing with Spikes\"\n  at NIPS 2016, Barcelona, Spain", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep convolutional neural networks (CNNs) have shown great potential for\nnumerous real-world machine learning applications, but performing inference in\nlarge CNNs in real-time remains a challenge. We have previously demonstrated\nthat traditional CNNs can be converted into deep spiking neural networks\n(SNNs), which exhibit similar accuracy while reducing both latency and\ncomputational load as a consequence of their data-driven, event-based style of\ncomputing. Here we provide a novel theory that explains why this conversion is\nsuccessful, and derive from it several new tools to convert a larger and more\npowerful class of deep networks into SNNs. We identify the main sources of\napproximation errors in previous conversion methods, and propose simple\nmechanisms to fix these issues. Furthermore, we develop spiking implementations\nof common CNN operations such as max-pooling, softmax, and batch-normalization,\nwhich allow almost loss-less conversion of arbitrary CNN architectures into the\nspiking domain. Empirical evaluation of different network architectures on the\nMNIST and CIFAR10 benchmarks leads to the best SNN results reported to date.\n", "versions": [{"version": "v1", "created": "Tue, 13 Dec 2016 07:58:34 GMT"}], "update_date": "2016-12-14", "authors_parsed": [["Rueckauer", "Bodo", ""], ["Lungu", "Iulia-Alexandra", ""], ["Hu", "Yuhuang", ""], ["Pfeiffer", "Michael", ""]]}, {"id": "1612.04061", "submitter": "Aditya Singh", "authors": "Aditya Singh, Saurabh Saini, Rajvi Shah, PJ Narayanan", "title": "Learning to Hash-tag Videos with Tag2Vec", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  User-given tags or labels are valuable resources for semantic understanding\nof visual media such as images and videos. Recently, a new type of labeling\nmechanism known as hash-tags have become increasingly popular on social media\nsites. In this paper, we study the problem of generating relevant and useful\nhash-tags for short video clips. Traditional data-driven approaches for tag\nenrichment and recommendation use direct visual similarity for label transfer\nand propagation. We attempt to learn a direct low-cost mapping from video to\nhash-tags using a two step training process. We first employ a natural language\nprocessing (NLP) technique, skip-gram models with neural network training to\nlearn a low-dimensional vector representation of hash-tags (Tag2Vec) using a\ncorpus of 10 million hash-tags. We then train an embedding function to map\nvideo features to the low-dimensional Tag2vec space. We learn this embedding\nfor 29 categories of short video clips with hash-tags. A query video without\nany tag-information can then be directly mapped to the vector space of tags\nusing the learned embedding and relevant tags can be found by performing a\nsimple nearest-neighbor retrieval in the Tag2Vec space. We validate the\nrelevance of the tags suggested by our system qualitatively and quantitatively\nwith a user study.\n", "versions": [{"version": "v1", "created": "Tue, 13 Dec 2016 08:32:02 GMT"}], "update_date": "2016-12-14", "authors_parsed": [["Singh", "Aditya", ""], ["Saini", "Saurabh", ""], ["Shah", "Rajvi", ""], ["Narayanan", "PJ", ""]]}, {"id": "1612.04062", "submitter": "Reza Fuad Rachmadi", "authors": "Reza Fuad Rachmadi, Keiichi Uchimura, and Gou Koutaki", "title": "Spatial Pyramid Convolutional Neural Network for Social Event Detection\n  in Static Image", "comments": "in Proceeding of 11th International Student Conference on Advanced\n  Science and Technology (ICAST) 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Social event detection in a static image is a very challenging problem and\nit's very useful for internet of things applications including automatic photo\norganization, ads recommender system, or image captioning. Several publications\nshow that variety of objects, scene, and people can be very ambiguous for the\nsystem to decide the event that occurs in the image. We proposed the spatial\npyramid configuration of convolutional neural network (CNN) classifier for\nsocial event detection in a static image. By applying the spatial pyramid\nconfiguration to the CNN classifier, the detail that occurs in the image can\nobserve more accurately by the classifier. USED dataset provided by Ahmad et\nal. is used to evaluate our proposed method, which consists of two different\nimage sets, EiMM, and SED dataset. As a result, the average accuracy of our\nsystem outperforms the baseline method by 15% and 2% respectively.\n", "versions": [{"version": "v1", "created": "Tue, 13 Dec 2016 08:32:56 GMT"}], "update_date": "2016-12-14", "authors_parsed": [["Rachmadi", "Reza Fuad", ""], ["Uchimura", "Keiichi", ""], ["Koutaki", "Gou", ""]]}, {"id": "1612.04110", "submitter": "Renata Rychtarikova", "authors": "Renata Rychtarikova and Dalibor Stys", "title": "Observation of dynamics inside an unlabeled live cell using bright-field\n  photon microscopy: Evaluation of organelles' trajectories", "comments": "12 pages, 5 figures, supplementary data", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM cs.CV q-bio.CB q-bio.SC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article presents an algorithm for the evaluation of organelles'\nmovements inside of an unmodified live cell. We used a time-lapse image series\nobtained using wide-field bright-field photon transmission microscopy as an\nalgorithm input. The benefit of the algorithm is the application of the R\\'enyi\ninformation entropy, namely a variable called a point information gain, which\nenables to highlight the borders of the intracellular organelles and to\nlocalize the organelles' centers of mass with the precision of one pixel.\n", "versions": [{"version": "v1", "created": "Tue, 13 Dec 2016 11:59:57 GMT"}], "update_date": "2016-12-14", "authors_parsed": [["Rychtarikova", "Renata", ""], ["Stys", "Dalibor", ""]]}, {"id": "1612.04158", "submitter": "Xi Zhang", "authors": "Xiaolin Wu, Xi Zhang, Chang Liu", "title": "Automated Inference on Sociopsychological Impressions of Attractive\n  Female Faces", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article is a sequel to our earlier work [25]. The main objective of our\nresearch is to explore the potential of supervised machine learning in\nface-induced social computing and cognition, riding on the momentum of much\nheralded successes of face processing, analysis and recognition on the tasks of\nbiometric-based identification. We present a case study of automated\nstatistical inference on sociopsychological perceptions of female faces\ncontrolled for race, attractiveness, age and nationality. Our empirical\nevidences point to the possibility of training machine learning algorithms,\nusing example face images characterized by internet users, to predict\nperceptions of personality traits and demeanors.\n", "versions": [{"version": "v1", "created": "Sun, 11 Dec 2016 06:31:10 GMT"}, {"version": "v2", "created": "Fri, 23 Dec 2016 14:36:52 GMT"}], "update_date": "2016-12-26", "authors_parsed": [["Wu", "Xiaolin", ""], ["Zhang", "Xi", ""], ["Liu", "Chang", ""]]}, {"id": "1612.04229", "submitter": "Anil Kumar Vadathya Mr", "authors": "Akshat Dave, Anil Kumar Vadathya, Kaushik Mitra", "title": "Compressive Image Recovery Using Recurrent Generative Model", "comments": "Submitted to ICIP 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reconstruction of signals from compressively sensed measurements is an\nill-posed problem. In this paper, we leverage the recurrent generative model,\nRIDE, as an image prior for compressive image reconstruction. Recurrent\nnetworks can model long-range dependencies in images and hence are suitable to\nhandle global multiplexing in reconstruction from compressive imaging. We\nperform MAP inference with RIDE using back-propagation to the inputs and\nprojected gradient method. We propose an entropy thresholding based approach\nfor preserving texture in images well. Our approach shows superior\nreconstructions compared to recent global reconstruction approaches like D-AMP\nand TVAL3 on both simulated and real data.\n", "versions": [{"version": "v1", "created": "Tue, 13 Dec 2016 15:21:41 GMT"}, {"version": "v2", "created": "Wed, 3 May 2017 19:48:30 GMT"}], "update_date": "2017-05-05", "authors_parsed": [["Dave", "Akshat", ""], ["Vadathya", "Anil Kumar", ""], ["Mitra", "Kaushik", ""]]}, {"id": "1612.04335", "submitter": "Vincent Sitzmann", "authors": "Vincent Sitzmann, Ana Serrano, Amy Pavel, Maneesh Agrawala, Diego\n  Gutierrez, Belen Masia, Gordon Wetzstein", "title": "How do people explore virtual environments?", "comments": "First two authors contributed equally", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding how people explore immersive virtual environments is crucial\nfor many applications, such as designing virtual reality (VR) content,\ndeveloping new compression algorithms, or learning computational models of\nsaliency or visual attention. Whereas a body of recent work has focused on\nmodeling saliency in desktop viewing conditions, VR is very different from\nthese conditions in that viewing behavior is governed by stereoscopic vision\nand by the complex interaction of head orientation, gaze, and other kinematic\nconstraints. To further our understanding of viewing behavior and saliency in\nVR, we capture and analyze gaze and head orientation data of 169 users\nexploring stereoscopic, static omni-directional panoramas, for a total of 1980\nhead and gaze trajectories for three different viewing conditions. We provide a\nthorough analysis of our data, which leads to several important insights, such\nas the existence of a particular fixation bias, which we then use to adapt\nexisting saliency predictors to immersive VR conditions. In addition, we\nexplore other applications of our data and analysis, including automatic\nalignment of VR video cuts, panorama thumbnails, panorama video synopsis, and\nsaliency-based compression.\n", "versions": [{"version": "v1", "created": "Tue, 13 Dec 2016 20:01:18 GMT"}, {"version": "v2", "created": "Tue, 19 Sep 2017 19:28:33 GMT"}], "update_date": "2017-09-21", "authors_parsed": [["Sitzmann", "Vincent", ""], ["Serrano", "Ana", ""], ["Pavel", "Amy", ""], ["Agrawala", "Maneesh", ""], ["Gutierrez", "Diego", ""], ["Masia", "Belen", ""], ["Wetzstein", "Gordon", ""]]}, {"id": "1612.04337", "submitter": "Tian Qi Chen", "authors": "Tian Qi Chen and Mark Schmidt", "title": "Fast Patch-based Style Transfer of Arbitrary Style", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Artistic style transfer is an image synthesis problem where the content of an\nimage is reproduced with the style of another. Recent works show that a\nvisually appealing style transfer can be achieved by using the hidden\nactivations of a pretrained convolutional neural network. However, existing\nmethods either apply (i) an optimization procedure that works for any style\nimage but is very expensive, or (ii) an efficient feedforward network that only\nallows a limited number of trained styles. In this work we propose a simpler\noptimization objective based on local matching that combines the content\nstructure and style textures in a single layer of the pretrained network. We\nshow that our objective has desirable properties such as a simpler optimization\nlandscape, intuitive parameter tuning, and consistent frame-by-frame\nperformance on video. Furthermore, we use 80,000 natural images and 80,000\npaintings to train an inverse network that approximates the result of the\noptimization. This results in a procedure for artistic style transfer that is\nefficient but also allows arbitrary content and style images.\n", "versions": [{"version": "v1", "created": "Tue, 13 Dec 2016 20:05:37 GMT"}], "update_date": "2016-12-14", "authors_parsed": [["Chen", "Tian Qi", ""], ["Schmidt", "Mark", ""]]}, {"id": "1612.04357", "submitter": "Xun Huang", "authors": "Xun Huang, Yixuan Li, Omid Poursaeed, John Hopcroft, Serge Belongie", "title": "Stacked Generative Adversarial Networks", "comments": "CVPR 2017, camera-ready version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a novel generative model named Stacked Generative\nAdversarial Networks (SGAN), which is trained to invert the hierarchical\nrepresentations of a bottom-up discriminative network. Our model consists of a\ntop-down stack of GANs, each learned to generate lower-level representations\nconditioned on higher-level representations. A representation discriminator is\nintroduced at each feature hierarchy to encourage the representation manifold\nof the generator to align with that of the bottom-up discriminative network,\nleveraging the powerful discriminative representations to guide the generative\nmodel. In addition, we introduce a conditional loss that encourages the use of\nconditional information from the layer above, and a novel entropy loss that\nmaximizes a variational lower bound on the conditional entropy of generator\noutputs. We first train each stack independently, and then train the whole\nmodel end-to-end. Unlike the original GAN that uses a single noise vector to\nrepresent all the variations, our SGAN decomposes variations into multiple\nlevels and gradually resolves uncertainties in the top-down generative process.\nBased on visual inspection, Inception scores and visual Turing test, we\ndemonstrate that SGAN is able to generate images of much higher quality than\nGANs without stacking.\n", "versions": [{"version": "v1", "created": "Tue, 13 Dec 2016 20:48:58 GMT"}, {"version": "v2", "created": "Tue, 31 Jan 2017 01:21:55 GMT"}, {"version": "v3", "created": "Tue, 7 Mar 2017 07:50:27 GMT"}, {"version": "v4", "created": "Wed, 12 Apr 2017 15:04:01 GMT"}], "update_date": "2017-04-13", "authors_parsed": [["Huang", "Xun", ""], ["Li", "Yixuan", ""], ["Poursaeed", "Omid", ""], ["Hopcroft", "John", ""], ["Belongie", "Serge", ""]]}, {"id": "1612.04402", "submitter": "Peiyun Hu", "authors": "Peiyun Hu, Deva Ramanan", "title": "Finding Tiny Faces", "comments": "CVPR 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Though tremendous strides have been made in object recognition, one of the\nremaining open challenges is detecting small objects. We explore three aspects\nof the problem in the context of finding small faces: the role of scale\ninvariance, image resolution, and contextual reasoning. While most recognition\napproaches aim to be scale-invariant, the cues for recognizing a 3px tall face\nare fundamentally different than those for recognizing a 300px tall face. We\ntake a different approach and train separate detectors for different scales. To\nmaintain efficiency, detectors are trained in a multi-task fashion: they make\nuse of features extracted from multiple layers of single (deep) feature\nhierarchy. While training detectors for large objects is straightforward, the\ncrucial challenge remains training detectors for small objects. We show that\ncontext is crucial, and define templates that make use of massively-large\nreceptive fields (where 99% of the template extends beyond the object of\ninterest). Finally, we explore the role of scale in pre-trained deep networks,\nproviding ways to extrapolate networks tuned for limited scales to rather\nextreme ranges. We demonstrate state-of-the-art results on\nmassively-benchmarked face datasets (FDDB and WIDER FACE). In particular, when\ncompared to prior art on WIDER FACE, our results reduce error by a factor of 2\n(our models produce an AP of 82% while prior art ranges from 29-64%).\n", "versions": [{"version": "v1", "created": "Tue, 13 Dec 2016 21:28:02 GMT"}, {"version": "v2", "created": "Sat, 15 Apr 2017 06:18:08 GMT"}], "update_date": "2017-04-18", "authors_parsed": [["Hu", "Peiyun", ""], ["Ramanan", "Deva", ""]]}, {"id": "1612.04440", "submitter": "Will Grathwohl", "authors": "Will Grathwohl, Aaron Wilson", "title": "Disentangling Space and Time in Video with Hierarchical Variational\n  Auto-encoders", "comments": "fixed typo in equation 16", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There are many forms of feature information present in video data. Principle\namong them are object identity information which is largely static across\nmultiple video frames, and object pose and style information which continuously\ntransforms from frame to frame. Most existing models confound these two types\nof representation by mapping them to a shared feature space. In this paper we\npropose a probabilistic approach for learning separable representations of\nobject identity and pose information using unsupervised video data. Our\napproach leverages a deep generative model with a factored prior distribution\nthat encodes properties of temporal invariances in the hidden feature set.\nLearning is achieved via variational inference. We present results of learning\nidentity and pose information on a dataset of moving characters as well as a\ndataset of rotating 3D objects. Our experimental results demonstrate our\nmodel's success in factoring its representation, and demonstrate that the model\nachieves improved performance in transfer learning tasks.\n", "versions": [{"version": "v1", "created": "Wed, 14 Dec 2016 00:20:46 GMT"}, {"version": "v2", "created": "Mon, 19 Dec 2016 17:17:26 GMT"}], "update_date": "2016-12-20", "authors_parsed": [["Grathwohl", "Will", ""], ["Wilson", "Aaron", ""]]}, {"id": "1612.04447", "submitter": "Uche Nnolim", "authors": "U. A. Nnolim", "title": "Analysis of proposed PDE-based underwater image enhancement algorithms", "comments": "57 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This report describes the experimental analysis of proposed underwater image\nenhancement algorithms based on partial differential equations (PDEs). The\nalgorithms perform simultaneous smoothing and enhancement due to the\ncombination of both processes within the PDE-formulation. The framework enables\nthe incorporation of suitable colour and contrast enhancement algorithms within\none unified functional. Additional modification of the formulation includes the\ncombination of the popular Contrast Limited Adaptive Histogram Equalization\n(CLAHE) with the proposed approach. This modification enables the hybrid\nalgorithm to provide both local enhancement (due to the CLAHE) and global\nenhancement (due to the proposed contrast term). Additionally, the CLAHE clip\nlimit parameter is computed dynamically in each iteration and used to gauge the\namount of local enhancement performed by the CLAHE within the formulation. This\nenables the algorithm to reduce or prevent the enhancement of noisy artifacts,\nwhich if present, are also smoothed out by the anisotropic diffusion term\nwithin the PDE formulation. In other words, the modified algorithm combines the\nstrength of the CLAHE, AD and the contrast term while minimizing their\nweaknesses. Ultimately, the system is optimized using image data metrics for\nautomated enhancement and compromise between visual and quantitative results.\nExperiments indicate that the proposed algorithms perform a series of functions\nsuch as illumination correction, colour enhancement correction and restoration,\ncontrast enhancement and noise suppression. Moreover, the proposed approaches\nsurpass most other conventional algorithms found in the literature.\n", "versions": [{"version": "v1", "created": "Wed, 14 Dec 2016 00:47:51 GMT"}], "update_date": "2016-12-15", "authors_parsed": [["Nnolim", "U. A.", ""]]}, {"id": "1612.04468", "submitter": "Jason J Corso", "authors": "Parker Koch and Jason J. Corso", "title": "Sparse Factorization Layers for Neural Networks with Limited Supervision", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Whereas CNNs have demonstrated immense progress in many vision problems, they\nsuffer from a dependence on monumental amounts of labeled training data. On the\nother hand, dictionary learning does not scale to the size of problems that\nCNNs can handle, despite being very effective at low-level vision tasks such as\ndenoising and inpainting. Recently, interest has grown in adapting dictionary\nlearning methods for supervised tasks such as classification and inverse\nproblems. We propose two new network layers that are based on dictionary\nlearning: a sparse factorization layer and a convolutional sparse factorization\nlayer, analogous to fully-connected and convolutional layers, respectively.\nUsing our derivations, these layers can be dropped in to existing CNNs, trained\ntogether in an end-to-end fashion with back-propagation, and leverage\nsemisupervision in ways classical CNNs cannot. We experimentally compare\nnetworks with these two new layers against a baseline CNN. Our results\ndemonstrate that networks with either of the sparse factorization layers are\nable to outperform classical CNNs when supervised data are few. They also show\nperformance improvements in certain tasks when compared to the CNN with no\nsparse factorization layers with the same exact number of parameters.\n", "versions": [{"version": "v1", "created": "Wed, 14 Dec 2016 03:13:29 GMT"}], "update_date": "2016-12-15", "authors_parsed": [["Koch", "Parker", ""], ["Corso", "Jason J.", ""]]}, {"id": "1612.04520", "submitter": "Zhichen Zhao", "authors": "Zhichen Zhao and Huimin Ma and Shaodi You", "title": "Single Image Action Recognition using Semantic Body Part Actions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a novel single image action recognition algorithm\nwhich is based on the idea of semantic body part actions. Unlike existing\nbottom up methods, we argue that the human action is a combination of\nmeaningful body part actions. In detail, we divide human body into five parts:\nhead, torso, arms, hands and legs. And for each of the body parts, we define\nseveral semantic body part actions, e.g., hand holding, hand waving. These\nsemantic body part actions are strongly related to the body actions, e.g.,\nwriting, and jogging. Based on the idea, we propose a deep neural network based\nsystem: first, body parts are localized by a Semi-FCN network. Second, for each\nbody parts, a Part Action Res-Net is used to predict semantic body part\nactions. And finally, we use SVM to fuse the body part actions and predict the\nentire body action. Experiments on two dataset: PASCAL VOC 2012 and Stanford-40\nreport mAP improvement from the state-of-the-art by 3.8% and 2.6% respectively.\n", "versions": [{"version": "v1", "created": "Wed, 14 Dec 2016 07:54:55 GMT"}], "update_date": "2016-12-15", "authors_parsed": [["Zhao", "Zhichen", ""], ["Ma", "Huimin", ""], ["You", "Shaodi", ""]]}, {"id": "1612.04526", "submitter": "Remi Flamary", "authors": "R\\'emi Flamary", "title": "Astronomical image reconstruction with convolutional neural networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV astro-ph.IM stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  State of the art methods in astronomical image reconstruction rely on the\nresolution of a regularized or constrained optimization problem. Solving this\nproblem can be computationally intensive and usually leads to a quadratic or at\nleast superlinear complexity w.r.t. the number of pixels in the image. We\ninvestigate in this work the use of convolutional neural networks for image\nreconstruction in astronomy. With neural networks, the computationally\nintensive tasks is the training step, but the prediction step has a fixed\ncomplexity per pixel, i.e. a linear complexity. Numerical experiments show that\nour approach is both computationally efficient and competitive with other state\nof the art methods in addition to being interpretable.\n", "versions": [{"version": "v1", "created": "Wed, 14 Dec 2016 08:17:07 GMT"}, {"version": "v2", "created": "Wed, 7 Jun 2017 08:25:27 GMT"}], "update_date": "2017-06-08", "authors_parsed": [["Flamary", "R\u00e9mi", ""]]}, {"id": "1612.04530", "submitter": "Nicholas Guttenberg", "authors": "Nicholas Guttenberg and Nathaniel Virgo and Olaf Witkowski and\n  Hidetoshi Aoki and Ryota Kanai", "title": "Permutation-equivariant neural networks applied to dynamics prediction", "comments": "7 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The introduction of convolutional layers greatly advanced the performance of\nneural networks on image tasks due to innately capturing a way of encoding and\nlearning translation-invariant operations, matching one of the underlying\nsymmetries of the image domain. In comparison, there are a number of problems\nin which there are a number of different inputs which are all 'of the same\ntype' --- multiple particles, multiple agents, multiple stock prices, etc. The\ncorresponding symmetry to this is permutation symmetry, in that the algorithm\nshould not depend on the specific ordering of the input data. We discuss a\npermutation-invariant neural network layer in analogy to convolutional layers,\nand show the ability of this architecture to learn to predict the motion of a\nvariable number of interacting hard discs in 2D. In the same way that\nconvolutional layers can generalize to different image sizes, the permutation\nlayer we describe generalizes to different numbers of objects.\n", "versions": [{"version": "v1", "created": "Wed, 14 Dec 2016 08:31:53 GMT"}], "update_date": "2016-12-15", "authors_parsed": [["Guttenberg", "Nicholas", ""], ["Virgo", "Nathaniel", ""], ["Witkowski", "Olaf", ""], ["Aoki", "Hidetoshi", ""], ["Kanai", "Ryota", ""]]}, {"id": "1612.04573", "submitter": "Reiner Lenz", "authors": "Reiner Lenz", "title": "The Mehler-Fock Transform and some Applications in Texture Analysis and\n  Color Processing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many stochastic processes are defined on special geometrical objects like\nspheres and cones. We describe how tools from harmonic analysis, i.e. Fourier\nanalysis on groups, can be used to investigate probability density functions\n(pdfs) on groups and homogeneous spaces. We consider the special case of the\nLorentz group SU(1,1) and the unit disk with its hyperbolic geometry, but the\nprocedure can be generalized to a much wider class of Lie-groups. We mainly\nconcentrate on the Mehler-Fock transform which is the radial part of the\nFourier transform on the disk. Some of the characteristic features of this\ntransform are the relation to group-convolutions, the isometry between signal\nand transform space, the relation to the Laplace-Beltrami operator and the\nrelation to group representation theory. We will give an overview over these\nproperties and their applications in signal processing. We will illustrate the\ntheory with two examples from low-level vision and color image processing.\n", "versions": [{"version": "v1", "created": "Wed, 14 Dec 2016 11:04:04 GMT"}], "update_date": "2016-12-15", "authors_parsed": [["Lenz", "Reiner", ""]]}, {"id": "1612.04631", "submitter": "Romain Bregier", "authors": "Romain Br\\'egier, Fr\\'ed\\'eric Devernay, Laetitia Leyrit, James\n  Crowley", "title": "Defining the Pose of any 3D Rigid Object and an Associated Distance", "comments": null, "journal-ref": "International Journal of Computer Vision, Springer Verlag, 2017", "doi": "10.1007/s11263-017-1052-4", "report-no": null, "categories": "cs.CV math.MG physics.class-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The pose of a rigid object is usually regarded as a rigid transformation,\ndescribed by a translation and a rotation. However, equating the pose space\nwith the space of rigid transformations is in general abusive, as it does not\naccount for objects with proper symmetries -- which are common among man-made\nobjects.In this article, we define pose as a distinguishable static state of an\nobject, and equate a pose with a set of rigid transformations. Based solely on\ngeometric considerations, we propose a frame-invariant metric on the space of\npossible poses, valid for any physical rigid object, and requiring no arbitrary\ntuning. This distance can be evaluated efficiently using a representation of\nposes within an Euclidean space of at most 12 dimensions depending on the\nobject's symmetries. This makes it possible to efficiently perform neighborhood\nqueries such as radius searches or k-nearest neighbor searches within a large\nset of poses using off-the-shelf methods. Pose averaging considering this\nmetric can similarly be performed easily, using a projection function from the\nEuclidean space onto the pose space. The practical value of those theoretical\ndevelopments is illustrated with an application of pose estimation of instances\nof a 3D rigid object given an input depth map, via a Mean Shift procedure.\n", "versions": [{"version": "v1", "created": "Wed, 14 Dec 2016 13:46:55 GMT"}, {"version": "v2", "created": "Wed, 16 Aug 2017 09:24:46 GMT"}, {"version": "v3", "created": "Wed, 29 Nov 2017 14:10:04 GMT"}], "update_date": "2017-11-30", "authors_parsed": [["Br\u00e9gier", "Romain", ""], ["Devernay", "Fr\u00e9d\u00e9ric", ""], ["Leyrit", "Laetitia", ""], ["Crowley", "James", ""]]}, {"id": "1612.04642", "submitter": "Daniel Worrall", "authors": "Daniel E. Worrall, Stephan J. Garbin, Daniyar Turmukhambetov and\n  Gabriel J. Brostow", "title": "Harmonic Networks: Deep Translation and Rotation Equivariance", "comments": "Submitted to CVPR 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Translating or rotating an input image should not affect the results of many\ncomputer vision tasks. Convolutional neural networks (CNNs) are already\ntranslation equivariant: input image translations produce proportionate feature\nmap translations. This is not the case for rotations. Global rotation\nequivariance is typically sought through data augmentation, but patch-wise\nequivariance is more difficult. We present Harmonic Networks or H-Nets, a CNN\nexhibiting equivariance to patch-wise translation and 360-rotation. We achieve\nthis by replacing regular CNN filters with circular harmonics, returning a\nmaximal response and orientation for every receptive field patch.\n  H-Nets use a rich, parameter-efficient and low computational complexity\nrepresentation, and we show that deep feature maps within the network encode\ncomplicated rotational invariants. We demonstrate that our layers are general\nenough to be used in conjunction with the latest architectures and techniques,\nsuch as deep supervision and batch normalization. We also achieve\nstate-of-the-art classification on rotated-MNIST, and competitive results on\nother benchmark challenges.\n", "versions": [{"version": "v1", "created": "Wed, 14 Dec 2016 14:01:11 GMT"}, {"version": "v2", "created": "Tue, 11 Apr 2017 13:34:17 GMT"}], "update_date": "2017-04-12", "authors_parsed": [["Worrall", "Daniel E.", ""], ["Garbin", "Stephan J.", ""], ["Turmukhambetov", "Daniyar", ""], ["Brostow", "Gabriel J.", ""]]}, {"id": "1612.04647", "submitter": "Yi Zhang", "authors": "Yi Zhang, Weichao Qiu, Qi Chen, Xiaolin Hu and Alan Yuille", "title": "UnrealStereo: Controlling Hazardous Factors to Analyze Stereo Vision", "comments": "3DV 2018 (oral)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A reliable stereo algorithm is critical for many robotics applications. But\ntextureless and specular regions can easily cause failure by making feature\nmatching difficult. Understanding whether an algorithm is robust to these\nhazardous regions is important. Although many stereo benchmarks have been\ndeveloped to evaluate performance, it is hard to quantify the effect of\nhazardous regions in real images because the location and severity of these\nregions are unknown. In this paper, we develop a synthetic image generation\ntool enabling to control hazardous factors, such as making objects more\nspecular or transparent, to produce hazardous regions at different degrees. The\ndensely controlled sampling strategy in virtual worlds enables to effectively\nstress test stereo algorithms by varying the types and degrees of the hazard.\nWe generate a large synthetic image dataset with automatically computed\nhazardous regions and analyze algorithms on these regions. The observations\nfrom synthetic images are further validated by annotating hazardous regions in\nreal-world datasets Middlebury and KITTI (which gives a sparse sampling of the\nhazards). Our synthetic image generation tool is based on a game engine Unreal\nEngine 4 and will be open-source along with the virtual scenes in our\nexperiments. Many publicly available realistic game contents can be used by our\ntool to provide an enormous resource for development and evaluation of\nalgorithms.\n", "versions": [{"version": "v1", "created": "Wed, 14 Dec 2016 14:13:59 GMT"}, {"version": "v2", "created": "Thu, 6 Sep 2018 12:08:22 GMT"}], "update_date": "2018-09-07", "authors_parsed": [["Zhang", "Yi", ""], ["Qiu", "Weichao", ""], ["Chen", "Qi", ""], ["Hu", "Xiaolin", ""], ["Yuille", "Alan", ""]]}, {"id": "1612.04733", "submitter": "Wen-Kai Yu", "authors": "Wen-Kai Yu, An-Dong Xiong, Xu-Ri Yao, Guang-Jie Zhai, and Qing Zhao", "title": "Efficient phase retrieval based on dark fringe recognition with an\n  ability of bypassing invalid fringes", "comments": "14 pages, 5 figures", "journal-ref": null, "doi": "10.1016/j.optcom.2017.06.058", "report-no": null, "categories": "cs.CV physics.optics", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper discusses the noisy phase retrieval problem: recovering a complex\nimage signal with independent noise from quadratic measurements. Inspired by\nthe dark fringes shown in the measured images of the array detector, a novel\nphase retrieval approach is proposed and demonstrated both theoretically and\nexperimentally to recognize the dark fringes and bypass the invalid fringes. A\nmore accurate relative phase ratio between arbitrary two pixels is achieved by\ncalculating the multiplicative ratios (or the sum of phase difference) on the\npath between them. Then the object phase image can be reconstructed precisely.\nOur approach is a good choice for retrieving high-quality phase images from\nnoisy signals and has many potential applications in the fields such as X-ray\ncrystallography, diffractive imaging, and so on.\n", "versions": [{"version": "v1", "created": "Wed, 14 Dec 2016 17:13:03 GMT"}], "update_date": "2017-06-27", "authors_parsed": [["Yu", "Wen-Kai", ""], ["Xiong", "An-Dong", ""], ["Yao", "Xu-Ri", ""], ["Zhai", "Guang-Jie", ""], ["Zhao", "Qing", ""]]}, {"id": "1612.04755", "submitter": "Zeling Wu", "authors": "Zeling Wu and Haoxiang Wang", "title": "Super-resolution Reconstruction of SAR Image based on Non-Local Means\n  Denoising Combined with BP Neural Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article, we propose a super-resolution method to resolve the problem\nof image low spatial because of the limitation of imaging devices. We make use\nof the strong non-linearity mapped ability of the back-propagation neural\nnetworks(BPNN). Training sample images are got by undersampled method. The\nelements chose as the inputs of the BPNN are pixels referred to Non-local\nmeans(NL-Means). Making use of the self-similarity of the images, those inputs\nare the pixels which are pixels gained from modified NL-means which is specific\nfor super-resolution. Besides, small change on core function of NL-means has\nbeen applied in the method we use in this article so that we can have a clearer\nedge in the shrunk image. Experimental results gained from the Peak Signal to\nNoise Ratio(PSNR) and the Equivalent Number of Look(ENL), indicate that adding\nthe similar pixels as inputs will increase the results than not taking them\ninto consideration.\n", "versions": [{"version": "v1", "created": "Wed, 14 Dec 2016 18:02:56 GMT"}], "update_date": "2016-12-15", "authors_parsed": [["Wu", "Zeling", ""], ["Wang", "Haoxiang", ""]]}, {"id": "1612.04757", "submitter": "Marcus Rohrbach", "authors": "Dong Huk Park, Lisa Anne Hendricks, Zeynep Akata, Bernt Schiele,\n  Trevor Darrell, Marcus Rohrbach", "title": "Attentive Explanations: Justifying Decisions and Pointing to the\n  Evidence", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep models are the defacto standard in visual decision models due to their\nimpressive performance on a wide array of visual tasks. However, they are\nfrequently seen as opaque and are unable to explain their decisions. In\ncontrast, humans can justify their decisions with natural language and point to\nthe evidence in the visual world which led to their decisions. We postulate\nthat deep models can do this as well and propose our Pointing and Justification\n(PJ-X) model which can justify its decision with a sentence and point to the\nevidence by introspecting its decision and explanation process using an\nattention mechanism. Unfortunately there is no dataset available with reference\nexplanations for visual decision making. We thus collect two datasets in two\ndomains where it is interesting and challenging to explain decisions. First, we\nextend the visual question answering task to not only provide an answer but\nalso a natural language explanation for the answer. Second, we focus on\nexplaining human activities which is traditionally more challenging than object\nclassification. We extensively evaluate our PJ-X model, both on the\njustification and pointing tasks, by comparing it to prior models and ablations\nusing both automatic and human evaluations.\n", "versions": [{"version": "v1", "created": "Wed, 14 Dec 2016 18:12:47 GMT"}, {"version": "v2", "created": "Tue, 25 Jul 2017 09:33:03 GMT"}], "update_date": "2017-07-26", "authors_parsed": [["Park", "Dong Huk", ""], ["Hendricks", "Lisa Anne", ""], ["Akata", "Zeynep", ""], ["Schiele", "Bernt", ""], ["Darrell", "Trevor", ""], ["Rohrbach", "Marcus", ""]]}, {"id": "1612.04770", "submitter": "Spyros Gidaris", "authors": "Spyros Gidaris, Nikos Komodakis", "title": "Detect, Replace, Refine: Deep Structured Prediction For Pixel Wise\n  Labeling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pixel wise image labeling is an interesting and challenging problem with\ngreat significance in the computer vision community. In order for a dense\nlabeling algorithm to be able to achieve accurate and precise results, it has\nto consider the dependencies that exist in the joint space of both the input\nand the output variables. An implicit approach for modeling those dependencies\nis by training a deep neural network that, given as input an initial estimate\nof the output labels and the input image, it will be able to predict a new\nrefined estimate for the labels. In this context, our work is concerned with\nwhat is the optimal architecture for performing the label improvement task. We\nargue that the prior approaches of either directly predicting new label\nestimates or predicting residual corrections w.r.t. the initial labels with\nfeed-forward deep network architectures are sub-optimal. Instead, we propose a\ngeneric architecture that decomposes the label improvement task to three steps:\n1) detecting the initial label estimates that are incorrect, 2) replacing the\nincorrect labels with new ones, and finally 3) refining the renewed labels by\npredicting residual corrections w.r.t. them. Furthermore, we explore and\ncompare various other alternative architectures that consist of the\naforementioned Detection, Replace, and Refine components. We extensively\nevaluate the examined architectures in the challenging task of dense disparity\nestimation (stereo matching) and we report both quantitative and qualitative\nresults on three different datasets. Finally, our dense disparity estimation\nnetwork that implements the proposed generic architecture, achieves\nstate-of-the-art results in the KITTI 2015 test surpassing prior approaches by\na significant margin.\n", "versions": [{"version": "v1", "created": "Wed, 14 Dec 2016 18:54:33 GMT"}], "update_date": "2016-12-15", "authors_parsed": [["Gidaris", "Spyros", ""], ["Komodakis", "Nikos", ""]]}, {"id": "1612.04774", "submitter": "Xu Xu", "authors": "Xu Xu, Sinisa Todorovic", "title": "Beam Search for Learning a Deep Convolutional Neural Network of 3D\n  Shapes", "comments": "ICPR 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses 3D shape recognition. Recent work typically represents a\n3D shape as a set of binary variables corresponding to 3D voxels of a uniform\n3D grid centered on the shape, and resorts to deep convolutional neural\nnetworks(CNNs) for modeling these binary variables. Robust learning of such\nCNNs is currently limited by the small datasets of 3D shapes available, an\norder of magnitude smaller than other common datasets in computer vision.\nRelated work typically deals with the small training datasets using a number of\nad hoc, hand-tuning strategies. To address this issue, we formulate CNN\nlearning as a beam search aimed at identifying an optimal CNN architecture,\nnamely, the number of layers, nodes, and their connectivity in the network, as\nwell as estimating parameters of such an optimal CNN. Each state of the beam\nsearch corresponds to a candidate CNN. Two types of actions are defined to add\nnew convolutional filters or new convolutional layers to a parent CNN, and thus\ntransition to children states. The utility function of each action is\nefficiently computed by transferring parameter values of the parent CNN to its\nchildren, thereby enabling an efficient beam search. Our experimental\nevaluation on the 3D ModelNet dataset demonstrates that our model pursuit using\nthe beam search yields a CNN with superior performance on 3D shape\nclassification than the state of the art.\n", "versions": [{"version": "v1", "created": "Wed, 14 Dec 2016 19:06:05 GMT"}], "update_date": "2016-12-15", "authors_parsed": [["Xu", "Xu", ""], ["Todorovic", "Sinisa", ""]]}, {"id": "1612.04787", "submitter": "Arthur Willis", "authors": "Arthur W. Wetzel, Jennifer Bakal, Markus Dittrich, David G. C.\n  Hildebrand, Josh L. Morgan, Jeff W. Lichtman", "title": "Registering large volume serial-section electron microscopy image sets\n  for neural circuit reconstruction using FFT signal whitening", "comments": "10 pages, 4 figures as submitted for the 2016 IEEE Applied Imagery\n  and Pattern Recognition Workshop proceedings, Oct 18-20, 2016", "journal-ref": null, "doi": "10.1109/AIPR.2016.8010595", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The detailed reconstruction of neural anatomy for connectomics studies\nrequires a combination of resolution and large three-dimensional data capture\nprovided by serial section electron microscopy (ssEM). The convergence of high\nthroughput ssEM imaging and improved tissue preparation methods now allows ssEM\ncapture of complete specimen volumes up to cubic millimeter scale. The\nresulting multi-terabyte image sets span thousands of serial sections and must\nbe precisely registered into coherent volumetric forms in which neural circuits\ncan be traced and segmented. This paper introduces a Signal Whitening Fourier\nTransform Image Registration approach (SWiFT-IR) under development at the\nPittsburgh Supercomputing Center and its use to align mouse and zebrafish brain\ndatasets acquired using the wafer mapper ssEM imaging technology recently\ndeveloped at Harvard University. Unlike other methods now used for ssEM\nregistration, SWiFT-IR modifies its spatial frequency response during image\nmatching to maximize a signal-to-noise measure used as its primary indicator of\nalignment quality. This alignment signal is more robust to rapid variations in\nbiological content and unavoidable data distortions than either phase-only or\nstandard Pearson correlation, thus allowing more precise alignment and\nstatistical confidence. These improvements in turn enable an iterative\nregistration procedure based on projections through multiple sections rather\nthan more typical adjacent-pair matching methods. This projection approach,\nwhen coupled with known anatomical constraints and iteratively applied in a\nmulti-resolution pyramid fashion, drives the alignment into a smooth form that\nproperly represents complex and widely varying anatomical content such as the\nfull cross-section zebrafish data.\n", "versions": [{"version": "v1", "created": "Wed, 14 Dec 2016 20:03:05 GMT"}], "update_date": "2017-11-21", "authors_parsed": [["Wetzel", "Arthur W.", ""], ["Bakal", "Jennifer", ""], ["Dittrich", "Markus", ""], ["Hildebrand", "David G. C.", ""], ["Morgan", "Josh L.", ""], ["Lichtman", "Jeff W.", ""]]}, {"id": "1612.04799", "submitter": "William Guss", "authors": "William H. Guss", "title": "Deep Function Machines: Generalized Neural Networks for Topological\n  Layer Expression", "comments": "23 pages, 9 figures, with experiments", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose a generalization of deep neural networks called deep\nfunction machines (DFMs). DFMs act on vector spaces of arbitrary (possibly\ninfinite) dimension and we show that a family of DFMs are invariant to the\ndimension of input data; that is, the parameterization of the model does not\ndirectly hinge on the quality of the input (eg. high resolution images). Using\nthis generalization we provide a new theory of universal approximation of\nbounded non-linear operators between function spaces. We then suggest that DFMs\nprovide an expressive framework for designing new neural network layer types\nwith topological considerations in mind. Finally, we introduce a novel\narchitecture, RippLeNet, for resolution invariant computer vision, which\nempirically achieves state of the art invariance.\n", "versions": [{"version": "v1", "created": "Wed, 14 Dec 2016 20:39:23 GMT"}, {"version": "v2", "created": "Mon, 6 Nov 2017 20:51:33 GMT"}], "update_date": "2017-11-08", "authors_parsed": [["Guss", "William H.", ""]]}, {"id": "1612.04809", "submitter": "Md Abul Hasnat", "authors": "Md. Abul Hasnat, Jussi Parkkinen and Markku Hauta-Kasari", "title": "Spectral video construction from RGB video: Application to Image Guided\n  Neurosurgery", "comments": "Experiments were conducted in 2011, Paper rewritten with recent\n  review in 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spectral imaging has received enormous interest in the field of medical\nimaging modalities. It provides a powerful tool for the analysis of different\norgans and non-invasive tissues. Therefore, significant amount of research has\nbeen conducted to explore the possibility of using spectral imaging in\nbiomedical applications. To observe spectral image information in real time\nduring surgery and monitor the temporal changes in the organs and tissues is a\ndemanding task. Available spectral imaging devices are not sufficient to\naccomplish this task with an acceptable spatial and spectral resolution. A\nsolution to this problem is to estimate the spectral video from RGB video and\nperform visualization with the most prominent spectral bands. In this research,\nwe propose a framework to generate neurosurgery spectral video from RGB video.\nA spectral estimation technique is applied on each RGB video frames. The RGB\nvideo is captured using a digital camera connected with an operational\nmicroscope dedicated to neurosurgery. A database of neurosurgery spectral\nimages is used to collect training data and evaluate the estimation accuracy. A\nsearching technique is used to identify the best training set. Five different\nspectrum estimation techniques are experimented to indentify the best method.\nAlthough this framework is established for neurosurgery spectral video\ngeneration, however, the methodology outlined here would also be applicable to\nother similar research.\n", "versions": [{"version": "v1", "created": "Wed, 14 Dec 2016 20:55:16 GMT"}], "update_date": "2016-12-15", "authors_parsed": [["Hasnat", "Md. Abul", ""], ["Parkkinen", "Jussi", ""], ["Hauta-Kasari", "Markku", ""]]}, {"id": "1612.04811", "submitter": "Seyed Abdulaziz Esmaeili", "authors": "Seyed A. Esmaeili, Bharat Singh, Larry S. Davis", "title": "Fast-AT: Fast Automatic Thumbnail Generation using Deep Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fast-AT is an automatic thumbnail generation system based on deep neural\nnetworks. It is a fully-convolutional deep neural network, which learns\nspecific filters for thumbnails of different sizes and aspect ratios. During\ninference, the appropriate filter is selected depending on the dimensions of\nthe target thumbnail. Unlike most previous work, Fast-AT does not utilize\nsaliency but addresses the problem directly. In addition, it eliminates the\nneed to conduct region search on the saliency map. The model generalizes to\nthumbnails of different sizes including those with extreme aspect ratios and\ncan generate thumbnails in real time. A data set of more than 70,000 thumbnail\nannotations was collected to train Fast-AT. We show competitive results in\ncomparison to existing techniques.\n", "versions": [{"version": "v1", "created": "Wed, 14 Dec 2016 20:59:19 GMT"}, {"version": "v2", "created": "Mon, 10 Apr 2017 23:17:19 GMT"}], "update_date": "2017-04-12", "authors_parsed": [["Esmaeili", "Seyed A.", ""], ["Singh", "Bharat", ""], ["Davis", "Larry S.", ""]]}, {"id": "1612.04844", "submitter": "Kenneth Marino", "authors": "Kenneth Marino, Ruslan Salakhutdinov, Abhinav Gupta", "title": "The More You Know: Using Knowledge Graphs for Image Classification", "comments": "CVPR 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One characteristic that sets humans apart from modern learning-based computer\nvision algorithms is the ability to acquire knowledge about the world and use\nthat knowledge to reason about the visual world. Humans can learn about the\ncharacteristics of objects and the relationships that occur between them to\nlearn a large variety of visual concepts, often with few examples. This paper\ninvestigates the use of structured prior knowledge in the form of knowledge\ngraphs and shows that using this knowledge improves performance on image\nclassification. We build on recent work on end-to-end learning on graphs,\nintroducing the Graph Search Neural Network as a way of efficiently\nincorporating large knowledge graphs into a vision classification pipeline. We\nshow in a number of experiments that our method outperforms standard neural\nnetwork baselines for multi-label classification.\n", "versions": [{"version": "v1", "created": "Wed, 14 Dec 2016 21:18:30 GMT"}, {"version": "v2", "created": "Sat, 22 Apr 2017 00:43:18 GMT"}], "update_date": "2017-04-25", "authors_parsed": [["Marino", "Kenneth", ""], ["Salakhutdinov", "Ruslan", ""], ["Gupta", "Abhinav", ""]]}, {"id": "1612.04854", "submitter": "Michal Yarom", "authors": "Michal Yarom and Michal Irani", "title": "Temporal-Needle: A view and appearance invariant video descriptor", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ability to detect similar actions across videos can be very useful for\nreal-world applications in many fields. However, this task is still challenging\nfor existing systems, since videos that present the same action, can be taken\nfrom significantly different viewing directions, performed by different actors\nand backgrounds and under various video qualities. Video descriptors play a\nsignificant role in these systems. In this work we propose the\n\"temporal-needle\" descriptor which captures the dynamic behavior, while being\ninvariant to viewpoint and appearance. The descriptor is computed using multi\ntemporal scales of the video and by computing self-similarity for every patch\nthrough time in every temporal scale. The descriptor is computed for every\npixel in the video. However, to find similar actions across videos, we consider\nonly a small subset of the descriptors - the statistical significant\ndescriptors. This allow us to find good correspondences across videos more\nefficiently. Using the descriptor, we were able to detect the same behavior\nacross videos in a variety of scenarios. We demonstrate the use of the\ndescriptor in tasks such as temporal and spatial alignment, action detection\nand even show its potential in unsupervised video clustering into categories.\nIn this work we handled only videos taken with stationary cameras, but the\ndescriptor can be extended to handle moving camera as well.\n", "versions": [{"version": "v1", "created": "Wed, 14 Dec 2016 21:46:09 GMT"}], "update_date": "2016-12-16", "authors_parsed": [["Yarom", "Michal", ""], ["Irani", "Michal", ""]]}, {"id": "1612.04862", "submitter": "Nadir Murru", "authors": "Giuseppe Air\\`o Farulla, Nadir Murru, Rosaria Rossini", "title": "A fuzzy approach for segmentation of touching characters", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of correctly segmenting touching characters is an hard task to\nsolve and it is of major relevance in pattern recognition. In the recent years,\nmany methods and algorithms have been proposed; still, a definitive solution is\nfar from being found. In this paper, we propose a novel method based on fuzzy\nlogic. The proposed method combines in a novel way three features for\nsegmenting touching characters that have been already proposed in other studies\nbut have been exploited only singularly so far. The proposed strategy is based\non a 3--input/1--output fuzzy inference system with fuzzy rules specifically\noptimized for segmenting touching characters in the case of Latin printed and\nhandwritten characters. The system performances are illustrated and supported\nby numerical examples showing that our approach can achieve a reasonable good\noverall accuracy in segmenting characters even on tricky conditions of touching\ncharacters. Moreover, numerical results suggest that the method can be applied\nto many different datasets of characters by means of a convenient tuning of the\nfuzzy sets and rules.\n", "versions": [{"version": "v1", "created": "Thu, 8 Dec 2016 14:44:31 GMT"}], "update_date": "2016-12-16", "authors_parsed": [["Farulla", "Giuseppe Air\u00f2", ""], ["Murru", "Nadir", ""], ["Rossini", "Rosaria", ""]]}, {"id": "1612.04869", "submitter": "Hadar Averbuch-Elor", "authors": "Hadar Averbuch-Elor, Nadav Bar, Daniel Cohen-Or", "title": "Border-Peeling Clustering", "comments": "9 pages, 9 figures, supplementary material added as ancillary file", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a novel non-parametric clustering technique. Our\ntechnique is based on the notion that each latent cluster is comprised of\nlayers that surround its core, where the external layers, or border points,\nimplicitly separate the clusters. Unlike previous techniques, such as DBSCAN,\nwhere the cores of the clusters are defined directly by their densities, here\nthe latent cores are revealed by a progressive peeling of the border points.\nAnalyzing the density of the local neighborhoods allows identifying the border\npoints and associating them with points of inner layers. We show that the\npeeling process adapts to the local densities and characteristics to\nsuccessfully separate adjacent clusters (of possibly different densities). We\nextensively tested our technique on large sets of labeled data, including\nhigh-dimensional datasets of deep features that were trained by a convolutional\nneural network. We show that our technique is competitive to other\nstate-of-the-art non-parametric methods using a fixed set of parameters\nthroughout the experiments.\n", "versions": [{"version": "v1", "created": "Wed, 14 Dec 2016 22:22:41 GMT"}, {"version": "v2", "created": "Sun, 30 Jun 2019 07:15:44 GMT"}], "update_date": "2019-07-02", "authors_parsed": [["Averbuch-Elor", "Hadar", ""], ["Bar", "Nadav", ""], ["Cohen-Or", "Daniel", ""]]}, {"id": "1612.04884", "submitter": "Rao Muhammad Anwer", "authors": "Fahad Shahbaz Khan, Joost van de Weijer, Rao Muhammad Anwer, Andrew D.\n  Bagdanov, Michael Felsberg, Jorma Laaksonen", "title": "Scale Coding Bag of Deep Features for Human Attribute and Action\n  Recognition", "comments": "To appear in Machine Vision and Applications", "journal-ref": null, "doi": "10.1007/s00138-017-0871-1", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most approaches to human attribute and action recognition in still images are\nbased on image representation in which multi-scale local features are pooled\nacross scale into a single, scale-invariant encoding. Both in bag-of-words and\nthe recently popular representations based on convolutional neural networks,\nlocal features are computed at multiple scales. However, these multi-scale\nconvolutional features are pooled into a single scale-invariant representation.\nWe argue that entirely scale-invariant image representations are sub-optimal\nand investigate approaches to scale coding within a Bag of Deep Features\nframework.\n  Our approach encodes multi-scale information explicitly during the image\nencoding stage. We propose two strategies to encode multi-scale information\nexplicitly in the final image representation. We validate our two scale coding\ntechniques on five datasets: Willow, PASCAL VOC 2010, PASCAL VOC 2012,\nStanford-40 and Human Attributes (HAT-27). On all datasets, the proposed scale\ncoding approaches outperform both the scale-invariant method and the standard\ndeep features of the same network. Further, combining our scale coding\napproaches with standard deep features leads to consistent improvement over the\nstate-of-the-art.\n", "versions": [{"version": "v1", "created": "Wed, 14 Dec 2016 23:44:23 GMT"}, {"version": "v2", "created": "Mon, 26 Mar 2018 12:11:49 GMT"}], "update_date": "2018-03-28", "authors_parsed": [["Khan", "Fahad Shahbaz", ""], ["van de Weijer", "Joost", ""], ["Anwer", "Rao Muhammad", ""], ["Bagdanov", "Andrew D.", ""], ["Felsberg", "Michael", ""], ["Laaksonen", "Jorma", ""]]}, {"id": "1612.04891", "submitter": "Aaron Lee", "authors": "Cecilia S. Lee, Doug M. Baughman, Aaron Y. Lee", "title": "Deep learning is effective for the classification of OCT images of\n  normal versus Age-related Macular Degeneration", "comments": "4 Figures, 1 Table", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Objective: The advent of Electronic Medical Records (EMR) with large\nelectronic imaging databases along with advances in deep neural networks with\nmachine learning has provided a unique opportunity to achieve milestones in\nautomated image analysis. Optical coherence tomography (OCT) is the most\ncommonly obtained imaging modality in ophthalmology and represents a dense and\nrich dataset when combined with labels derived from the EMR. We sought to\ndetermine if deep learning could be utilized to distinguish normal OCT images\nfrom images from patients with Age-related Macular Degeneration (AMD). Methods:\nAutomated extraction of an OCT imaging database was performed and linked to\nclinical endpoints from the EMR. OCT macula scans were obtained by Heidelberg\nSpectralis, and each OCT scan was linked to EMR clinical endpoints extracted\nfrom EPIC. The central 11 images were selected from each OCT scan of two\ncohorts of patients: normal and AMD. Cross-validation was performed using a\nrandom subset of patients. Area under receiver operator curves (auROC) were\nconstructed at an independent image level, macular OCT level, and patient\nlevel. Results: Of an extraction of 2.6 million OCT images linked to clinical\ndatapoints from the EMR, 52,690 normal and 48,312 AMD macular OCT images were\nselected. A deep neural network was trained to categorize images as either\nnormal or AMD. At the image level, we achieved an auROC of 92.78% with an\naccuracy of 87.63%. At the macula level, we achieved an auROC of 93.83% with an\naccuracy of 88.98%. At a patient level, we achieved an auROC of 97.45% with an\naccuracy of 93.45%. Peak sensitivity and specificity with optimal cutoffs were\n92.64% and 93.69% respectively. Conclusions: Deep learning techniques are\neffective for classifying OCT images. These findings have important\nimplications in utilizing OCT in automated screening and computer aided\ndiagnosis tools.\n", "versions": [{"version": "v1", "created": "Thu, 15 Dec 2016 00:23:43 GMT"}], "update_date": "2016-12-16", "authors_parsed": [["Lee", "Cecilia S.", ""], ["Baughman", "Doug M.", ""], ["Lee", "Aaron Y.", ""]]}, {"id": "1612.04901", "submitter": "Vivek Krishnan", "authors": "Vivek Krishnan and Deva Ramanan", "title": "Tinkering Under the Hood: Interactive Zero-Shot Learning with Net\n  Surgery", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the task of visual net surgery, in which a CNN can be\nreconfigured without extra data to recognize novel concepts that may be omitted\nfrom the training set. While most prior work make use of linguistic cues for\nsuch \"zero-shot\" learning, we do so by using a pictorial language\nrepresentation of the training set, implicitly learned by a CNN, to generalize\nto new classes. To this end, we introduce a set of visualization techniques\nthat better reveal the activation patterns and relations between groups of CNN\nfilters. We next demonstrate that knowledge of pictorial languages can be used\nto rewire certain CNN neurons into a part model, which we call a pictorial\nlanguage classifier. We demonstrate the robustness of simple PLCs by applying\nthem in a weakly supervised manner: labeling unlabeled concepts for visual\nclasses present in the training data. Specifically we show that a PLC built on\ntop of a CNN trained for ImageNet classification can localize humans in Graz-02\nand determine the pose of birds in PASCAL-VOC without extra labeled data or\nadditional training. We then apply PLCs in an interactive zero-shot manner,\ndemonstrating that pictorial languages are expressive enough to detect a set of\nvisual classes in MS-COCO that never appear in the ImageNet training set.\n", "versions": [{"version": "v1", "created": "Thu, 15 Dec 2016 01:27:10 GMT"}], "update_date": "2016-12-16", "authors_parsed": [["Krishnan", "Vivek", ""], ["Ramanan", "Deva", ""]]}, {"id": "1612.04904", "submitter": "Tal Hassner", "authors": "Anh Tuan Tran, Tal Hassner, Iacopo Masi, and Gerard Medioni", "title": "Regressing Robust and Discriminative 3D Morphable Models with a very\n  Deep Neural Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The 3D shapes of faces are well known to be discriminative. Yet despite this,\nthey are rarely used for face recognition and always under controlled viewing\nconditions. We claim that this is a symptom of a serious but often overlooked\nproblem with existing methods for single view 3D face reconstruction: when\napplied \"in the wild\", their 3D estimates are either unstable and change for\ndifferent photos of the same subject or they are over-regularized and generic.\nIn response, we describe a robust method for regressing discriminative 3D\nmorphable face models (3DMM). We use a convolutional neural network (CNN) to\nregress 3DMM shape and texture parameters directly from an input photo. We\novercome the shortage of training data required for this purpose by offering a\nmethod for generating huge numbers of labeled examples. The 3D estimates\nproduced by our CNN surpass state of the art accuracy on the MICC data set.\nCoupled with a 3D-3D face matching pipeline, we show the first competitive face\nrecognition results on the LFW, YTF and IJB-A benchmarks using 3D face shapes\nas representations, rather than the opaque deep feature vectors used by other\nmodern systems.\n", "versions": [{"version": "v1", "created": "Thu, 15 Dec 2016 02:02:28 GMT"}], "update_date": "2016-12-16", "authors_parsed": [["Tran", "Anh Tuan", ""], ["Hassner", "Tal", ""], ["Masi", "Iacopo", ""], ["Medioni", "Gerard", ""]]}, {"id": "1612.04949", "submitter": "Yang Yang", "authors": "Hao Liu, Yang Yang, Fumin Shen, Lixin Duan and Heng Tao Shen", "title": "Recurrent Image Captioner: Describing Images with Spatial-Invariant\n  Transformation and Attention Filtering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Along with the prosperity of recurrent neural network in modelling sequential\ndata and the power of attention mechanism in automatically identify salient\ninformation, image captioning, a.k.a., image description, has been remarkably\nadvanced in recent years. Nonetheless, most existing paradigms may suffer from\nthe deficiency of invariance to images with different scaling, rotation, etc.;\nand effective integration of standalone attention to form a holistic end-to-end\nsystem. In this paper, we propose a novel image captioning architecture, termed\nRecurrent Image Captioner (\\textbf{RIC}), which allows visual encoder and\nlanguage decoder to coherently cooperate in a recurrent manner. Specifically,\nwe first equip CNN-based visual encoder with a differentiable layer to enable\nspatially invariant transformation of visual signals. Moreover, we deploy an\nattention filter module (differentiable) between encoder and decoder to\ndynamically determine salient visual parts. We also employ bidirectional LSTM\nto preprocess sentences for generating better textual representations. Besides,\nwe propose to exploit variational inference to optimize the whole architecture.\nExtensive experimental results on three benchmark datasets (i.e., Flickr8k,\nFlickr30k and MS COCO) demonstrate the superiority of our proposed architecture\nas compared to most of the state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Thu, 15 Dec 2016 07:19:46 GMT"}], "update_date": "2016-12-16", "authors_parsed": [["Liu", "Hao", ""], ["Yang", "Yang", ""], ["Shen", "Fumin", ""], ["Duan", "Lixin", ""], ["Shen", "Heng Tao", ""]]}, {"id": "1612.04956", "submitter": "Or Litany", "authors": "Or Litany, Tal Remez, Alex Bronstein", "title": "Cloud Dictionary: Sparse Coding and Modeling for Point Clouds", "comments": "Signal Processing with Adaptive Sparse Structured Representations\n  (SPARS), 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the development of range sensors such as LIDAR and time-of-flight\ncameras, 3D point cloud scans have become ubiquitous in computer vision\napplications, the most prominent ones being gesture recognition and autonomous\ndriving. Parsimony-based algorithms have shown great success on images and\nvideos where data points are sampled on a regular Cartesian grid. We propose an\nadaptation of these techniques to irregularly sampled signals by using\ncontinuous dictionaries. We present an example application in the form of point\ncloud denoising.\n", "versions": [{"version": "v1", "created": "Thu, 15 Dec 2016 07:53:27 GMT"}, {"version": "v2", "created": "Mon, 20 Mar 2017 19:45:44 GMT"}], "update_date": "2017-03-22", "authors_parsed": [["Litany", "Or", ""], ["Remez", "Tal", ""], ["Bronstein", "Alex", ""]]}, {"id": "1612.04966", "submitter": "Naushad Ansari", "authors": "Naushad Ansari, Anubha Gupta, Rahul Duggal", "title": "Design of Image Matched Non-Separable Wavelet using Convolutional Neural\n  Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image-matched nonseparable wavelets can find potential use in many\napplications including image classification, segmen- tation, compressive\nsensing, etc. This paper proposes a novel design methodology that utilizes\nconvolutional neural net- work (CNN) to design two-channel non-separable\nwavelet matched to a given image. The design is proposed on quin- cunx lattice.\nThe loss function of the convolutional neural network is setup with total\nsquared error between the given input image to CNN and the reconstructed image\nat the output of CNN, leading to perfect reconstruction at the end of train-\ning. Simulation results have been shown on some standard images.\n", "versions": [{"version": "v1", "created": "Thu, 15 Dec 2016 08:28:16 GMT"}], "update_date": "2016-12-16", "authors_parsed": [["Ansari", "Naushad", ""], ["Gupta", "Anubha", ""], ["Duggal", "Rahul", ""]]}, {"id": "1612.05000", "submitter": "Tsubasa Hirakawa", "authors": "Tsubasa Hirakawa, Toru Tamaki, Bisser Raytchev, Kazufumi Kaneda,\n  Tetsushi Koide, Shigeto Yoshida, Hiroshi Mieno, Shinji Tanaka", "title": "Development of a Real-time Colorectal Tumor Classification System for\n  Narrow-band Imaging zoom-videoendoscopy", "comments": "9 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Colorectal endoscopy is important for the early detection and treatment of\ncolorectal cancer and is used worldwide. A computer-aided diagnosis (CAD)\nsystem that provides an objective measure to endoscopists during colorectal\nendoscopic examinations would be of great value. In this study, we describe a\nnewly developed CAD system that provides real-time objective measures. Our\nsystem captures the video stream from an endoscopic system and transfers it to\na desktop computer. The captured video stream is then classified by a\npretrained classifier and the results are displayed on a monitor. The\nexperimental results show that our developed system works efficiently in actual\nendoscopic examinations and is medically significant.\n", "versions": [{"version": "v1", "created": "Thu, 15 Dec 2016 09:59:23 GMT"}, {"version": "v2", "created": "Wed, 21 Dec 2016 03:48:01 GMT"}], "update_date": "2016-12-22", "authors_parsed": [["Hirakawa", "Tsubasa", ""], ["Tamaki", "Toru", ""], ["Raytchev", "Bisser", ""], ["Kaneda", "Kazufumi", ""], ["Koide", "Tetsushi", ""], ["Yoshida", "Shigeto", ""], ["Mieno", "Hiroshi", ""], ["Tanaka", "Shinji", ""]]}, {"id": "1612.05005", "submitter": "Ingmar Steiner", "authors": "Alexander Hewer, Stefanie Wuhrer, Ingmar Steiner, Korin Richmond", "title": "A Multilinear Tongue Model Derived from Speech Related MRI Data of the\n  Human Vocal Tract", "comments": null, "journal-ref": "Computer Speech & Language 51 (2018) 68-92", "doi": "10.1016/j.csl.2018.02.001", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present a multilinear statistical model of the human tongue that captures\nanatomical and tongue pose related shape variations separately. The model is\nderived from 3D magnetic resonance imaging data of 11 speakers sustaining\nspeech related vocal tract configurations. The extraction is performed by using\na minimally supervised method that uses as basis an image segmentation approach\nand a template fitting technique. Furthermore, it uses image denoising to deal\nwith possibly corrupt data, palate surface information reconstruction to handle\npalatal tongue contacts, and a bootstrap strategy to refine the obtained\nshapes. Our evaluation concludes that limiting the degrees of freedom for the\nanatomical and speech related variations to 5 and 4, respectively, produces a\nmodel that can reliably register unknown data while avoiding overfitting\neffects. Furthermore, we show that it can be used to generate a plausible\ntongue animation by tracking sparse motion capture data.\n", "versions": [{"version": "v1", "created": "Thu, 15 Dec 2016 10:31:40 GMT"}, {"version": "v2", "created": "Mon, 3 Apr 2017 08:51:42 GMT"}, {"version": "v3", "created": "Tue, 12 Dec 2017 16:00:02 GMT"}, {"version": "v4", "created": "Fri, 13 Apr 2018 09:27:33 GMT"}, {"version": "v5", "created": "Tue, 17 Apr 2018 08:16:54 GMT"}], "update_date": "2018-04-18", "authors_parsed": [["Hewer", "Alexander", ""], ["Wuhrer", "Stefanie", ""], ["Steiner", "Ingmar", ""], ["Richmond", "Korin", ""]]}, {"id": "1612.05038", "submitter": "Adrian Keith Davison", "authors": "Adrian K. Davison, Cliff Lansley, Choon Ching Ng, Kevin Tan, Moi Hoon\n  Yap", "title": "Objective Micro-Facial Movement Detection Using FACS-Based Regions and\n  Baseline Evaluation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Micro-facial expressions are regarded as an important human behavioural event\nthat can highlight emotional deception. Spotting these movements is difficult\nfor humans and machines, however research into using computer vision to detect\nsubtle facial expressions is growing in popularity. This paper proposes an\nindividualised baseline micro-movement detection method using 3D Histogram of\nOriented Gradients (3D HOG) temporal difference method. We define a face\ntemplate consisting of 26 regions based on the Facial Action Coding System\n(FACS). We extract the temporal features of each region using 3D HOG. Then, we\nuse Chi-square distance to find subtle facial motion in the local regions.\nFinally, an automatic peak detector is used to detect micro-movements above the\nnewly proposed adaptive baseline threshold. The performance is validated on two\nFACS coded datasets: SAMM and CASME II. This objective method focuses on the\nmovement of the 26 face regions. When comparing with the ground truth, the best\nresult was an AUC of 0.7512 and 0.7261 on SAMM and CASME II, respectively. The\nresults show that 3D HOG outperformed for micro-movement detection, compared to\nstate-of-the-art feature representations: Local Binary Patterns in Three\nOrthogonal Planes and Histograms of Oriented Optical Flow.\n", "versions": [{"version": "v1", "created": "Thu, 15 Dec 2016 12:15:36 GMT"}], "update_date": "2016-12-16", "authors_parsed": [["Davison", "Adrian K.", ""], ["Lansley", "Cliff", ""], ["Ng", "Choon Ching", ""], ["Tan", "Kevin", ""], ["Yap", "Moi Hoon", ""]]}, {"id": "1612.05050", "submitter": "Matthias Dorfer", "authors": "Matthias Dorfer, Andreas Arzt, Gerhard Widmer", "title": "Towards Score Following in Sheet Music Images", "comments": "Published In Proceedings of the 17th International Society for Music\n  Information Retrieval Conference (2016)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the matching of short music audio snippets to the\ncorresponding pixel location in images of sheet music. A system is presented\nthat simultaneously learns to read notes, listens to music and matches the\ncurrently played music to its corresponding notes in the sheet. It consists of\nan end-to-end multi-modal convolutional neural network that takes as input\nimages of sheet music and spectrograms of the respective audio snippets. It\nlearns to predict, for a given unseen audio snippet (covering approximately one\nbar of music), the corresponding position in the respective score line. Our\nresults suggest that with the use of (deep) neural networks -- which have\nproven to be powerful image processing models -- working with sheet music\nbecomes feasible and a promising future research direction.\n", "versions": [{"version": "v1", "created": "Thu, 15 Dec 2016 13:10:13 GMT"}], "update_date": "2016-12-16", "authors_parsed": [["Dorfer", "Matthias", ""], ["Arzt", "Andreas", ""], ["Widmer", "Gerhard", ""]]}, {"id": "1612.05062", "submitter": "Thomas Nestmeyer", "authors": "Thomas Nestmeyer, Peter V. Gehler", "title": "Reflectance Adaptive Filtering Improves Intrinsic Image Estimation", "comments": "CVPR 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Separating an image into reflectance and shading layers poses a challenge for\nlearning approaches because no large corpus of precise and realistic ground\ntruth decompositions exists. The Intrinsic Images in the Wild~(IIW) dataset\nprovides a sparse set of relative human reflectance judgments, which serves as\na standard benchmark for intrinsic images. A number of methods use IIW to learn\nstatistical dependencies between the images and their reflectance layer.\nAlthough learning plays an important role for high performance, we show that a\nstandard signal processing technique achieves performance on par with current\nstate-of-the-art. We propose a loss function for CNN learning of dense\nreflectance predictions. Our results show a simple pixel-wise decision, without\nany context or prior knowledge, is sufficient to provide a strong baseline on\nIIW. This sets a competitive baseline which only two other approaches surpass.\nWe then develop a joint bilateral filtering method that implements strong prior\nknowledge about reflectance constancy. This filtering operation can be applied\nto any intrinsic image algorithm and we improve several previous results\nachieving a new state-of-the-art on IIW. Our findings suggest that the effect\nof learning-based approaches may have been over-estimated so far. Explicit\nprior knowledge is still at least as important to obtain high performance in\nintrinsic image decompositions.\n", "versions": [{"version": "v1", "created": "Thu, 15 Dec 2016 13:42:54 GMT"}, {"version": "v2", "created": "Mon, 12 Jun 2017 12:39:49 GMT"}], "update_date": "2017-06-13", "authors_parsed": [["Nestmeyer", "Thomas", ""], ["Gehler", "Peter V.", ""]]}, {"id": "1612.05079", "submitter": "Ankur Handa", "authors": "John McCormac, Ankur Handa, Stefan Leutenegger, Andrew J. Davison", "title": "SceneNet RGB-D: 5M Photorealistic Images of Synthetic Indoor\n  Trajectories with Ground Truth", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce SceneNet RGB-D, expanding the previous work of SceneNet to\nenable large scale photorealistic rendering of indoor scene trajectories. It\nprovides pixel-perfect ground truth for scene understanding problems such as\nsemantic segmentation, instance segmentation, and object detection, and also\nfor geometric computer vision problems such as optical flow, depth estimation,\ncamera pose estimation, and 3D reconstruction. Random sampling permits\nvirtually unlimited scene configurations, and here we provide a set of 5M\nrendered RGB-D images from over 15K trajectories in synthetic layouts with\nrandom but physically simulated object poses. Each layout also has random\nlighting, camera trajectories, and textures. The scale of this dataset is well\nsuited for pre-training data-driven computer vision techniques from scratch\nwith RGB-D inputs, which previously has been limited by relatively small\nlabelled datasets in NYUv2 and SUN RGB-D. It also provides a basis for\ninvestigating 3D scene labelling tasks by providing perfect camera poses and\ndepth data as proxy for a SLAM system. We host the dataset at\nhttp://robotvault.bitbucket.io/scenenet-rgbd.html\n", "versions": [{"version": "v1", "created": "Thu, 15 Dec 2016 14:22:38 GMT"}, {"version": "v2", "created": "Fri, 16 Dec 2016 01:37:54 GMT"}, {"version": "v3", "created": "Mon, 30 Jan 2017 11:06:14 GMT"}], "update_date": "2017-01-31", "authors_parsed": [["McCormac", "John", ""], ["Handa", "Ankur", ""], ["Leutenegger", "Stefan", ""], ["Davison", "Andrew J.", ""]]}, {"id": "1612.05086", "submitter": "Lukas Balles", "authors": "Lukas Balles and Javier Romero and Philipp Hennig", "title": "Coupling Adaptive Batch Sizes with Learning Rates", "comments": "Thirty-Third Conference on Uncertainty in Artificial Intelligence\n  (UAI), 2017, (accepted)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mini-batch stochastic gradient descent and variants thereof have become\nstandard for large-scale empirical risk minimization like the training of\nneural networks. These methods are usually used with a constant batch size\nchosen by simple empirical inspection. The batch size significantly influences\nthe behavior of the stochastic optimization algorithm, though, since it\ndetermines the variance of the gradient estimates. This variance also changes\nover the optimization process; when using a constant batch size, stability and\nconvergence is thus often enforced by means of a (manually tuned) decreasing\nlearning rate schedule.\n  We propose a practical method for dynamic batch size adaptation. It estimates\nthe variance of the stochastic gradients and adapts the batch size to decrease\nthe variance proportionally to the value of the objective function, removing\nthe need for the aforementioned learning rate decrease. In contrast to recent\nrelated work, our algorithm couples the batch size to the learning rate,\ndirectly reflecting the known relationship between the two. On popular image\nclassification benchmarks, our batch size adaptation yields faster optimization\nconvergence, while simultaneously simplifying learning rate tuning. A\nTensorFlow implementation is available.\n", "versions": [{"version": "v1", "created": "Thu, 15 Dec 2016 14:42:45 GMT"}, {"version": "v2", "created": "Wed, 28 Jun 2017 12:07:02 GMT"}], "update_date": "2017-06-29", "authors_parsed": [["Balles", "Lukas", ""], ["Romero", "Javier", ""], ["Hennig", "Philipp", ""]]}, {"id": "1612.05203", "submitter": "Kai Xu", "authors": "Kai Xu, Fengbo Ren", "title": "CSVideoNet: A Real-time End-to-end Learning Framework for\n  High-frame-rate Video Compressive Sensing", "comments": "9 pages, 6 figures, 7 tables. Accepted by WACV 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the real-time encoding-decoding problem for\nhigh-frame-rate video compressive sensing (CS). Unlike prior works that perform\nreconstruction using iterative optimization-based approaches, we propose a\nnon-iterative model, named \"CSVideoNet\". CSVideoNet directly learns the inverse\nmapping of CS and reconstructs the original input in a single forward\npropagation. To overcome the limitations of existing CS cameras, we propose a\nmulti-rate CNN and a synthesizing RNN to improve the trade-off between\ncompression ratio (CR) and spatial-temporal resolution of the reconstructed\nvideos. The experiment results demonstrate that CSVideoNet significantly\noutperforms the state-of-the-art approaches. With no pre/post-processing, we\nachieve 25dB PSNR recovery quality at 100x CR, with a frame rate of 125 fps on\na Titan X GPU. Due to the feedforward and high-data-concurrency natures of\nCSVideoNet, it can take advantage of GPU acceleration to achieve three orders\nof magnitude speed-up over conventional iterative-based approaches. We share\nthe source code at https://github.com/PSCLab-ASU/CSVideoNet.\n", "versions": [{"version": "v1", "created": "Thu, 15 Dec 2016 19:28:38 GMT"}, {"version": "v2", "created": "Fri, 16 Dec 2016 20:19:46 GMT"}, {"version": "v3", "created": "Mon, 20 Mar 2017 19:40:08 GMT"}, {"version": "v4", "created": "Wed, 5 Apr 2017 05:13:49 GMT"}, {"version": "v5", "created": "Sun, 28 Jan 2018 04:32:43 GMT"}], "update_date": "2018-01-30", "authors_parsed": [["Xu", "Kai", ""], ["Ren", "Fengbo", ""]]}, {"id": "1612.05234", "submitter": "Xinshuo Weng", "authors": "Namhoon Lee, Xinshuo Weng, Vishnu Naresh Boddeti, Yu Zhang, Fares\n  Beainy, Kris Kitani, Takeo Kanade", "title": "Visual Compiler: Synthesizing a Scene-Specific Pedestrian Detector and\n  Pose Estimator", "comments": "submitted to CVPR 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce the concept of a Visual Compiler that generates a scene specific\npedestrian detector and pose estimator without any pedestrian observations.\nGiven a single image and auxiliary scene information in the form of camera\nparameters and geometric layout of the scene, the Visual Compiler first infers\ngeometrically and photometrically accurate images of humans in that scene\nthrough the use of computer graphics rendering. Using these renders we learn a\nscene-and-region specific spatially-varying fully convolutional neural network,\nfor simultaneous detection, pose estimation and segmentation of pedestrians. We\ndemonstrate that when real human annotated data is scarce or non-existent, our\ndata generation strategy can provide an excellent solution for bootstrapping\nhuman detection and pose estimation. Experimental results show that our\napproach outperforms off-the-shelf state-of-the-art pedestrian detectors and\npose estimators that are trained on real data.\n", "versions": [{"version": "v1", "created": "Thu, 15 Dec 2016 20:43:06 GMT"}], "update_date": "2016-12-16", "authors_parsed": [["Lee", "Namhoon", ""], ["Weng", "Xinshuo", ""], ["Boddeti", "Vishnu Naresh", ""], ["Zhang", "Yu", ""], ["Beainy", "Fares", ""], ["Kitani", "Kris", ""], ["Kanade", "Takeo", ""]]}, {"id": "1612.05322", "submitter": "Yutong Zheng", "authors": "Yutong Zheng, Chenchen Zhu, Khoa Luu, Chandrasekhar Bhagavatula, T.\n  Hoang Ngan Le, Marios Savvides", "title": "Towards a Deep Learning Framework for Unconstrained Face Detection", "comments": "Accepted by BTAS 2016. arXiv admin note: substantial text overlap\n  with arXiv:1606.05413", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Robust face detection is one of the most important pre-processing steps to\nsupport facial expression analysis, facial landmarking, face recognition, pose\nestimation, building of 3D facial models, etc. Although this topic has been\nintensely studied for decades, it is still challenging due to numerous variants\nof face images in real-world scenarios. In this paper, we present a novel\napproach named Multiple Scale Faster Region-based Convolutional Neural Network\n(MS-FRCNN) to robustly detect human facial regions from images collected under\nvarious challenging conditions, e.g. large occlusions, extremely low\nresolutions, facial expressions, strong illumination variations, etc. The\nproposed approach is benchmarked on two challenging face detection databases,\ni.e. the Wider Face database and the Face Detection Dataset and Benchmark\n(FDDB), and compared against recent other face detection methods, e.g.\nTwo-stage CNN, Multi-scale Cascade CNN, Faceness, Aggregate Chanel Features,\nHeadHunter, Multi-view Face Detection, Cascade CNN, etc. The experimental\nresults show that our proposed approach consistently achieves highly\ncompetitive results with the state-of-the-art performance against other recent\nface detection methods.\n", "versions": [{"version": "v1", "created": "Fri, 16 Dec 2016 00:34:06 GMT"}, {"version": "v2", "created": "Mon, 2 Jan 2017 18:06:49 GMT"}], "update_date": "2017-01-03", "authors_parsed": [["Zheng", "Yutong", ""], ["Zhu", "Chenchen", ""], ["Luu", "Khoa", ""], ["Bhagavatula", "Chandrasekhar", ""], ["Le", "T. Hoang Ngan", ""], ["Savvides", "Marios", ""]]}, {"id": "1612.05323", "submitter": "Alexis Arnaudon Mr", "authors": "Alexis Arnaudon, Darryl D. Holm, Akshay Pai, Stefan Sommer", "title": "A Stochastic Large Deformation Model for Computational Anatomy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the study of shapes of human organs using computational anatomy,\nvariations are found to arise from inter-subject anatomical differences,\ndisease-specific effects, and measurement noise. This paper introduces a\nstochastic model for incorporating random variations into the Large Deformation\nDiffeomorphic Metric Mapping (LDDMM) framework. By accounting for randomness in\na particular setup which is crafted to fit the geometrical properties of LDDMM,\nwe formulate the template estimation problem for landmarks with noise and give\ntwo methods for efficiently estimating the parameters of the noise fields from\na prescribed data set. One method directly approximates the time evolution of\nthe variance of each landmark by a finite set of differential equations, and\nthe other is based on an Expectation-Maximisation algorithm. In the second\nmethod, the evaluation of the data likelihood is achieved without registering\nthe landmarks, by applying bridge sampling using a stochastically perturbed\nversion of the large deformation gradient flow algorithm. The method and the\nestimation algorithms are experimentally validated on synthetic examples and\nshape data of human corpora callosa.\n", "versions": [{"version": "v1", "created": "Fri, 16 Dec 2016 00:45:46 GMT"}], "update_date": "2016-12-19", "authors_parsed": [["Arnaudon", "Alexis", ""], ["Holm", "Darryl D.", ""], ["Pai", "Akshay", ""], ["Sommer", "Stefan", ""]]}, {"id": "1612.05332", "submitter": "Ashton Fagg", "authors": "Ashton Fagg, Simon Lucey, Sridha Sridharan", "title": "Fast, Dense Feature SDM on an iPhone", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we present our method for enabling dense SDM to run at over 90\nFPS on a mobile device. Our contributions are two-fold. Drawing inspiration\nfrom the FFT, we propose a Sparse Compositional Regression (SCR) framework,\nwhich enables a significant speed up over classical dense regressors. Second,\nwe propose a binary approximation to SIFT features. Binary Approximated SIFT\n(BASIFT) features, which are a computationally efficient approximation to SIFT,\na commonly used feature with SDM. We demonstrate the performance of our\nalgorithm on an iPhone 7, and show that we achieve similar accuracy to SDM.\n", "versions": [{"version": "v1", "created": "Fri, 16 Dec 2016 01:47:31 GMT"}], "update_date": "2016-12-19", "authors_parsed": [["Fagg", "Ashton", ""], ["Lucey", "Simon", ""], ["Sridharan", "Sridha", ""]]}, {"id": "1612.05335", "submitter": "Dorian Tsai", "authors": "Dorian Tsai, Donald G. Dansereau, Steve Martin, Peter Corke", "title": "Mirrored Light Field Video Camera Adapter", "comments": "tech report, v0.5, 15 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes the design of a custom mirror-based light field camera\nadapter that is cheap, simple in construction, and accessible. Mirrors of\ndifferent shape and orientation reflect the scene into an upwards-facing camera\nto create an array of virtual cameras with overlapping field of view at\nspecified depths, and deliver video frame rate light fields. We describe the\ndesign, construction, decoding and calibration processes of our mirror-based\nlight field camera adapter in preparation for an open-source release to benefit\nthe robotic vision community.\n", "versions": [{"version": "v1", "created": "Fri, 16 Dec 2016 02:05:59 GMT"}], "update_date": "2016-12-19", "authors_parsed": [["Tsai", "Dorian", ""], ["Dansereau", "Donald G.", ""], ["Martin", "Steve", ""], ["Corke", "Peter", ""]]}, {"id": "1612.05360", "submitter": "Tran Minh Quan", "authors": "Tran Minh Quan, David G. C. Hildebrand and Won-Ki Jeong", "title": "FusionNet: A deep fully residual convolutional neural network for image\n  segmentation in connectomics", "comments": null, "journal-ref": null, "doi": "10.3389/fcomp.2021.613981", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Electron microscopic connectomics is an ambitious research direction with the\ngoal of studying comprehensive brain connectivity maps by using\nhigh-throughput, nano-scale microscopy. One of the main challenges in\nconnectomics research is developing scalable image analysis algorithms that\nrequire minimal user intervention. Recently, deep learning has drawn much\nattention in computer vision because of its exceptional performance in image\nclassification tasks. For this reason, its application to connectomic analyses\nholds great promise, as well. In this paper, we introduce a novel deep neural\nnetwork architecture, FusionNet, for the automatic segmentation of neuronal\nstructures in connectomics data. FusionNet leverages the latest advances in\nmachine learning, such as semantic segmentation and residual neural networks,\nwith the novel introduction of summation-based skip connections to allow a much\ndeeper network architecture for a more accurate segmentation. We demonstrate\nthe performance of the proposed method by comparing it with state-of-the-art\nelectron microscopy (EM) segmentation methods from the ISBI EM segmentation\nchallenge. We also show the segmentation results on two different tasks\nincluding cell membrane and cell body segmentation and a statistical analysis\nof cell morphology.\n", "versions": [{"version": "v1", "created": "Fri, 16 Dec 2016 04:08:43 GMT"}, {"version": "v2", "created": "Mon, 26 Dec 2016 11:42:56 GMT"}], "update_date": "2021-04-14", "authors_parsed": [["Quan", "Tran Minh", ""], ["Hildebrand", "David G. C.", ""], ["Jeong", "Won-Ki", ""]]}, {"id": "1612.05362", "submitter": "Roger Trullo", "authors": "Dong Nie, Roger Trullo, Caroline Petitjean, Su Ruan, Dinggang Shen", "title": "Medical Image Synthesis with Context-Aware Generative Adversarial\n  Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Computed tomography (CT) is critical for various clinical applications, e.g.,\nradiotherapy treatment planning and also PET attenuation correction. However,\nCT exposes radiation during acquisition, which may cause side effects to\npatients. Compared to CT, magnetic resonance imaging (MRI) is much safer and\ndoes not involve any radiations. Therefore, recently, researchers are greatly\nmotivated to estimate CT image from its corresponding MR image of the same\nsubject for the case of radiotherapy planning. In this paper, we propose a\ndata-driven approach to address this challenging problem. Specifically, we\ntrain a fully convolutional network to generate CT given an MR image. To better\nmodel the nonlinear relationship from MRI to CT and to produce more realistic\nimages, we propose to use the adversarial training strategy and an image\ngradient difference loss function. We further apply AutoContext Model to\nimplement a context-aware generative adversarial network. Experimental results\nshow that our method is accurate and robust for predicting CT images from MRI\nimages, and also outperforms three state-of-the-art methods under comparison.\n", "versions": [{"version": "v1", "created": "Fri, 16 Dec 2016 04:26:06 GMT"}], "update_date": "2016-12-19", "authors_parsed": [["Nie", "Dong", ""], ["Trullo", "Roger", ""], ["Petitjean", "Caroline", ""], ["Ruan", "Su", ""], ["Shen", "Dinggang", ""]]}, {"id": "1612.05363", "submitter": "Wei Shen", "authors": "Wei Shen and Rujie Liu", "title": "Learning Residual Images for Face Attribute Manipulation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Face attributes are interesting due to their detailed description of human\nfaces. Unlike prior researches working on attribute prediction, we address an\ninverse and more challenging problem called face attribute manipulation which\naims at modifying a face image according to a given attribute value. Instead of\nmanipulating the whole image, we propose to learn the corresponding residual\nimage defined as the difference between images before and after the\nmanipulation. In this way, the manipulation can be operated efficiently with\nmodest pixel modification. The framework of our approach is based on the\nGenerative Adversarial Network. It consists of two image transformation\nnetworks and a discriminative network. The transformation networks are\nresponsible for the attribute manipulation and its dual operation and the\ndiscriminative network is used to distinguish the generated images from real\nimages. We also apply dual learning to allow transformation networks to learn\nfrom each other. Experiments show that residual images can be effectively\nlearned and used for attribute manipulations. The generated images remain most\nof the details in attribute-irrelevant areas.\n", "versions": [{"version": "v1", "created": "Fri, 16 Dec 2016 04:28:26 GMT"}, {"version": "v2", "created": "Wed, 12 Apr 2017 06:00:19 GMT"}], "update_date": "2017-04-13", "authors_parsed": [["Shen", "Wei", ""], ["Liu", "Rujie", ""]]}, {"id": "1612.05365", "submitter": "Qixiang Ye", "authors": "Baochang Zhang, Zhigang Li, Xianbin Cao, Qixiang Ye, Chen Chen, Linlin\n  Shen, Alessandro Perina, Rongrong Ji", "title": "Output Constraint Transfer for Kernelized Correlation Filter in Tracking", "comments": "arXiv admin note: text overlap with arXiv:1404.7584 by other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Kernelized Correlation Filter (KCF) is one of the state-of-the-art object\ntrackers. However, it does not reasonably model the distribution of correlation\nresponse during tracking process, which might cause the drifting problem,\nespecially when targets undergo significant appearance changes due to\nocclusion, camera shaking, and/or deformation. In this paper, we propose an\nOutput Constraint Transfer (OCT) method that by modeling the distribution of\ncorrelation response in a Bayesian optimization framework is able to mitigate\nthe drifting problem. OCT builds upon the reasonable assumption that the\ncorrelation response to the target image follows a Gaussian distribution, which\nwe exploit to select training samples and reduce model uncertainty. OCT is\nrooted in a new theory which transfers data distribution to a constraint of the\noptimized variable, leading to an efficient framework to calculate correlation\nfilters. Extensive experiments on a commonly used tracking benchmark show that\nthe proposed method significantly improves KCF, and achieves better performance\nthan other state-of-the-art trackers. To encourage further developments, the\nsource code is made available https://github.com/bczhangbczhang/OCT-KCF.\n", "versions": [{"version": "v1", "created": "Fri, 16 Dec 2016 04:42:30 GMT"}], "update_date": "2016-12-19", "authors_parsed": [["Zhang", "Baochang", ""], ["Li", "Zhigang", ""], ["Cao", "Xianbin", ""], ["Ye", "Qixiang", ""], ["Chen", "Chen", ""], ["Shen", "Linlin", ""], ["Perina", "Alessandro", ""], ["Ji", "Rongrong", ""]]}, {"id": "1612.05386", "submitter": "Qi Wu", "authors": "Peng Wang, Qi Wu, Chunhua Shen, Anton van den Hengel", "title": "The VQA-Machine: Learning How to Use Existing Vision Algorithms to\n  Answer New Questions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the most intriguing features of the Visual Question Answering (VQA)\nchallenge is the unpredictability of the questions. Extracting the information\nrequired to answer them demands a variety of image operations from detection\nand counting, to segmentation and reconstruction. To train a method to perform\neven one of these operations accurately from {image,question,answer} tuples\nwould be challenging, but to aim to achieve them all with a limited set of such\ntraining data seems ambitious at best. We propose here instead a more general\nand scalable approach which exploits the fact that very good methods to achieve\nthese operations already exist, and thus do not need to be trained. Our method\nthus learns how to exploit a set of external off-the-shelf algorithms to\nachieve its goal, an approach that has something in common with the Neural\nTuring Machine. The core of our proposed method is a new co-attention model. In\naddition, the proposed approach generates human-readable reasons for its\ndecision, and can still be trained end-to-end without ground truth reasons\nbeing given. We demonstrate the effectiveness on two publicly available\ndatasets, Visual Genome and VQA, and show that it produces the state-of-the-art\nresults in both cases.\n", "versions": [{"version": "v1", "created": "Fri, 16 Dec 2016 07:07:25 GMT"}], "update_date": "2016-12-19", "authors_parsed": [["Wang", "Peng", ""], ["Wu", "Qi", ""], ["Shen", "Chunhua", ""], ["Hengel", "Anton van den", ""]]}, {"id": "1612.05400", "submitter": "Sailesh Conjeti", "authors": "Sailesh Conjeti, Abhijit Guha Roy, Amin Katouzian and Nassir Navab", "title": "Deep Residual Hashing", "comments": "Submitted to Information Processing in Medical Imaging, 2017 (Under\n  review)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hashing aims at generating highly compact similarity preserving code words\nwhich are well suited for large-scale image retrieval tasks.\n  Most existing hashing methods first encode the images as a vector of\nhand-crafted features followed by a separate binarization step to generate hash\ncodes. This two-stage process may produce sub-optimal encoding. In this paper,\nfor the first time, we propose a deep architecture for supervised hashing\nthrough residual learning, termed Deep Residual Hashing (DRH), for an\nend-to-end simultaneous representation learning and hash coding. The DRH model\nconstitutes four key elements: (1) a sub-network with multiple stacked residual\nblocks; (2) hashing layer for binarization; (3) supervised retrieval loss\nfunction based on neighbourhood component analysis for similarity preserving\nembedding; and (4) hashing related losses and regularisation to control the\nquantization error and improve the quality of hash coding. We present results\nof extensive experiments on a large public chest x-ray image database with\nco-morbidities and discuss the outcome showing substantial improvements over\nthe latest state-of-the art methods.\n", "versions": [{"version": "v1", "created": "Fri, 16 Dec 2016 09:31:17 GMT"}], "update_date": "2016-12-19", "authors_parsed": [["Conjeti", "Sailesh", ""], ["Roy", "Abhijit Guha", ""], ["Katouzian", "Amin", ""], ["Navab", "Nassir", ""]]}, {"id": "1612.05424", "submitter": "Konstantinos Bousmalis", "authors": "Konstantinos Bousmalis, Nathan Silberman, David Dohan, Dumitru Erhan\n  and Dilip Krishnan", "title": "Unsupervised Pixel-Level Domain Adaptation with Generative Adversarial\n  Networks", "comments": "Final CVPR 2017 paper and supplementary material", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Collecting well-annotated image datasets to train modern machine learning\nalgorithms is prohibitively expensive for many tasks. One appealing alternative\nis rendering synthetic data where ground-truth annotations are generated\nautomatically. Unfortunately, models trained purely on rendered images often\nfail to generalize to real images. To address this shortcoming, prior work\nintroduced unsupervised domain adaptation algorithms that attempt to map\nrepresentations between the two domains or learn to extract features that are\ndomain-invariant. In this work, we present a new approach that learns, in an\nunsupervised manner, a transformation in the pixel space from one domain to the\nother. Our generative adversarial network (GAN)-based method adapts\nsource-domain images to appear as if drawn from the target domain. Our approach\nnot only produces plausible samples, but also outperforms the state-of-the-art\non a number of unsupervised domain adaptation scenarios by large margins.\nFinally, we demonstrate that the adaptation process generalizes to object\nclasses unseen during training.\n", "versions": [{"version": "v1", "created": "Fri, 16 Dec 2016 10:50:36 GMT"}, {"version": "v2", "created": "Wed, 23 Aug 2017 12:35:56 GMT"}], "update_date": "2017-08-24", "authors_parsed": [["Bousmalis", "Konstantinos", ""], ["Silberman", "Nathan", ""], ["Dohan", "David", ""], ["Erhan", "Dumitru", ""], ["Krishnan", "Dilip", ""]]}, {"id": "1612.05441", "submitter": "Paul Swoboda", "authors": "Paul Swoboda and Bjoern Andres", "title": "A Message Passing Algorithm for the Minimum Cost Multicut Problem", "comments": "Added acknowledgments", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a dual decomposition and linear program relaxation of the NP -hard\nminimum cost multicut problem. Unlike other polyhedral relaxations of the\nmulticut polytope, it is amenable to efficient optimization by message passing.\nLike other polyhedral elaxations, it can be tightened efficiently by cutting\nplanes. We define an algorithm that alternates between message passing and\nefficient separation of cycle- and odd-wheel inequalities. This algorithm is\nmore efficient than state-of-the-art algorithms based on linear programming,\nincluding algorithms written in the framework of leading commercial software,\nas we show in experiments with large instances of the problem from applications\nin computer vision, biomedical image analysis and data mining.\n", "versions": [{"version": "v1", "created": "Fri, 16 Dec 2016 12:10:34 GMT"}, {"version": "v2", "created": "Thu, 12 Jan 2017 11:55:04 GMT"}], "update_date": "2017-01-13", "authors_parsed": [["Swoboda", "Paul", ""], ["Andres", "Bjoern", ""]]}, {"id": "1612.05460", "submitter": "Paul Swoboda", "authors": "Paul Swoboda and Jan Kuske and Bogdan Savchynskyy", "title": "A Dual Ascent Framework for Lagrangean Decomposition of Combinatorial\n  Problems", "comments": "Added acknowledgments", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a general dual ascent framework for Lagrangean decomposition of\ncombinatorial problems. Although methods of this type have shown their\nefficiency for a number of problems, so far there was no general algorithm\napplicable to multiple problem types. In his work, we propose such a general\nalgorithm. It depends on several parameters, which can be used to optimize its\nperformance in each particular setting. We demonstrate efficacy of our method\non graph matching and multicut problems, where it outperforms state-of-the-art\nsolvers including those based on subgradient optimization and off-the-shelf\nlinear programming solvers.\n", "versions": [{"version": "v1", "created": "Fri, 16 Dec 2016 13:32:18 GMT"}, {"version": "v2", "created": "Thu, 12 Jan 2017 11:53:40 GMT"}], "update_date": "2017-01-13", "authors_parsed": [["Swoboda", "Paul", ""], ["Kuske", "Jan", ""], ["Savchynskyy", "Bogdan", ""]]}, {"id": "1612.05476", "submitter": "Paul Swoboda", "authors": "Paul Swoboda, Carsten Rother, Hassan Abu Alhaija, Dagmar Kainmueller,\n  Bogdan Savchynskyy", "title": "A Study of Lagrangean Decompositions and Dual Ascent Solvers for Graph\n  Matching", "comments": "Added acknowledgments", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the quadratic assignment problem, in computer vision also known as\ngraph matching. Two leading solvers for this problem optimize the Lagrange\ndecomposition duals with sub-gradient and dual ascent (also known as message\npassing) updates. We explore s direction further and propose several additional\nLagrangean relaxations of the graph matching problem along with corresponding\nalgorithms, which are all based on a common dual ascent framework. Our\nextensive empirical evaluation gives several theoretical insights and suggests\na new state-of-the-art any-time solver for the considered problem. Our\nimprovement over state-of-the-art is particularly visible on a new dataset with\nlarge-scale sparse problem instances containing more than 500 graph nodes each.\n", "versions": [{"version": "v1", "created": "Fri, 16 Dec 2016 14:14:42 GMT"}, {"version": "v2", "created": "Thu, 12 Jan 2017 11:51:00 GMT"}], "update_date": "2017-01-13", "authors_parsed": [["Swoboda", "Paul", ""], ["Rother", "Carsten", ""], ["Alhaija", "Hassan Abu", ""], ["Kainmueller", "Dagmar", ""], ["Savchynskyy", "Bogdan", ""]]}, {"id": "1612.05478", "submitter": "Varun Jampani", "authors": "Varun Jampani and Raghudeep Gadde and Peter V. Gehler", "title": "Video Propagation Networks", "comments": "Appearing in Computer Vision and Pattern Recognition, 2017 (CVPR'17)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a technique that propagates information forward through video\ndata. The method is conceptually simple and can be applied to tasks that\nrequire the propagation of structured information, such as semantic labels,\nbased on video content. We propose a 'Video Propagation Network' that processes\nvideo frames in an adaptive manner. The model is applied online: it propagates\ninformation forward without the need to access future frames. In particular we\ncombine two components, a temporal bilateral network for dense and video\nadaptive filtering, followed by a spatial network to refine features and\nincreased flexibility. We present experiments on video object segmentation and\nsemantic video segmentation and show increased performance comparing to the\nbest previous task-specific methods, while having favorable runtime.\nAdditionally we demonstrate our approach on an example regression task of color\npropagation in a grayscale video.\n", "versions": [{"version": "v1", "created": "Fri, 16 Dec 2016 14:23:17 GMT"}, {"version": "v2", "created": "Thu, 5 Jan 2017 22:44:42 GMT"}, {"version": "v3", "created": "Tue, 11 Apr 2017 09:58:35 GMT"}], "update_date": "2017-04-12", "authors_parsed": [["Jampani", "Varun", ""], ["Gadde", "Raghudeep", ""], ["Gehler", "Peter V.", ""]]}, {"id": "1612.05515", "submitter": "Filippo Arcadu", "authors": "Filippo Arcadu, Marco Stampanoni and Federica Marone", "title": "On the crucial impact of the coupling projector-backprojector in\n  iterative tomographic reconstruction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The performance of an iterative reconstruction algorithm for X-ray tomography\nis strongly determined by the features of the used forward and backprojector.\nFor this reason, a large number of studies has focused on the to design of\nprojectors with increasingly higher accuracy and speed. To what extent the\naccuracy of an iterative algorithm is affected by the mathematical affinity and\nthe similarity between the actual implementation of the forward and\nbackprojection, referred here as \"coupling projector-backprojector\", has been\nan overlooked aspect so far. The experimental study presented here shows that\nthe reconstruction quality and the convergence of an iterative algorithm\ngreatly rely on a good matching between the implementation of the tomographic\noperators. In comparison, other aspects like the accuracy of the standalone\noperators, the usage of physical constraints or the choice of stopping criteria\nmay even play a less relevant role.\n", "versions": [{"version": "v1", "created": "Fri, 16 Dec 2016 15:48:11 GMT"}], "update_date": "2016-12-19", "authors_parsed": [["Arcadu", "Filippo", ""], ["Stampanoni", "Marco", ""], ["Marone", "Federica", ""]]}, {"id": "1612.05601", "submitter": "Christian Baumgartner", "authors": "Christian F. Baumgartner, Konstantinos Kamnitsas, Jacqueline Matthew,\n  Tara P. Fletcher, Sandra Smith, Lisa M. Koch, Bernhard Kainz and Daniel\n  Rueckert", "title": "SonoNet: Real-Time Detection and Localisation of Fetal Standard Scan\n  Planes in Freehand Ultrasound", "comments": "12 pages, 8 figures, published in IEEE Transactions in Medical\n  Imaging", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Identifying and interpreting fetal standard scan planes during 2D ultrasound\nmid-pregnancy examinations are highly complex tasks which require years of\ntraining. Apart from guiding the probe to the correct location, it can be\nequally difficult for a non-expert to identify relevant structures within the\nimage. Automatic image processing can provide tools to help experienced as well\nas inexperienced operators with these tasks. In this paper, we propose a novel\nmethod based on convolutional neural networks which can automatically detect 13\nfetal standard views in freehand 2D ultrasound data as well as provide a\nlocalisation of the fetal structures via a bounding box. An important\ncontribution is that the network learns to localise the target anatomy using\nweak supervision based on image-level labels only. The network architecture is\ndesigned to operate in real-time while providing optimal output for the\nlocalisation task. We present results for real-time annotation, retrospective\nframe retrieval from saved videos, and localisation on a very large and\nchallenging dataset consisting of images and video recordings of full clinical\nanomaly screenings. We found that the proposed method achieved an average\nF1-score of 0.798 in a realistic classification experiment modelling real-time\ndetection, and obtained a 90.09% accuracy for retrospective frame retrieval.\nMoreover, an accuracy of 77.8% was achieved on the localisation task.\n", "versions": [{"version": "v1", "created": "Fri, 16 Dec 2016 19:20:20 GMT"}, {"version": "v2", "created": "Tue, 25 Jul 2017 16:12:50 GMT"}], "update_date": "2017-07-26", "authors_parsed": [["Baumgartner", "Christian F.", ""], ["Kamnitsas", "Konstantinos", ""], ["Matthew", "Jacqueline", ""], ["Fletcher", "Tara P.", ""], ["Smith", "Sandra", ""], ["Koch", "Lisa M.", ""], ["Kainz", "Bernhard", ""], ["Rueckert", "Daniel", ""]]}, {"id": "1612.05712", "submitter": "Liao Ni", "authors": "Liao Ni, Yi Zhang, He Zheng, Shilei Liu, Houjun Huang, Wenxin Li", "title": "A Fusion Method Based on Decision Reliability Ratio for Finger Vein\n  Verification", "comments": "5 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Finger vein verification has developed a lot since its first proposal, but\nthere is still not a perfect algorithm. It is proved that algorithms with the\nsame overall accuracy may have different misclassified patterns. We could make\nuse of this complementation to fuse individual algorithms together for more\nprecise result. According to our observation, algorithm has different\nconfidence on its decisions but it is seldom considered in fusion methods. Our\nwork is first to define decision reliability ratio to quantify this confidence,\nand then propose the Maximum Decision Reliability Ratio (MDRR) fusion method\nincorporating Weighted Voting. Experiment conducted on a data set of 1000\nfingers and 5 images per finger proves the effectiveness of the method. The\nclassifier obtained by MDRR method gets an accuracy of 99.42% while the maximum\naccuracy of the original individual classifiers is 97.77%. The experiment\nresults also show the MDRR outperforms the traditional fusion methods as\nVoting, Weighted Voting, Sum and Weighted Sum.\n", "versions": [{"version": "v1", "created": "Sat, 17 Dec 2016 07:03:38 GMT"}], "update_date": "2016-12-20", "authors_parsed": [["Ni", "Liao", ""], ["Zhang", "Yi", ""], ["Zheng", "He", ""], ["Liu", "Shilei", ""], ["Huang", "Houjun", ""], ["Li", "Wenxin", ""]]}, {"id": "1612.05719", "submitter": "Xiangfei Kong", "authors": "Xiangfei Kong, Lin Yang", "title": "Microscopic Muscle Image Enhancement", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a robust image enhancement algorithm dedicated for muscle fiber\nspecimen images captured by optical microscopes. Blur or out of focus problems\nare prevalent in muscle images during the image acquisition stage. Traditional\nimage deconvolution methods do not work since they assume the blur kernels are\nknown and also produce ring artifacts. We provide a compact framework which\ninvolves a novel spatially non-uniform blind deblurring approach specialized to\nmuscle images which automatically detects and alleviates degraded regions. Ring\nartifacts problems are addressed and a kernel propagation strategy is proposed\nto speedup the algorithm and deals with the high non-uniformity of the blur\nkernels on muscle images. Experiments show that the proposed framework performs\nwell on muscle images taken with modern advanced optical microscopes. Our\nframework is free of laborious parameter settings and is computationally\nefficient.\n", "versions": [{"version": "v1", "created": "Sat, 17 Dec 2016 08:05:51 GMT"}], "update_date": "2016-12-20", "authors_parsed": [["Kong", "Xiangfei", ""], ["Yang", "Lin", ""]]}, {"id": "1612.05753", "submitter": "Seyed Sajad Mousavi", "authors": "Sajad Mousavi, Michael Schukat, Enda Howley, Ali Borji and Nasser\n  Mozayani", "title": "Learning to predict where to look in interactive environments using deep\n  recurrent q-learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bottom-Up (BU) saliency models do not perform well in complex interactive\nenvironments where humans are actively engaged in tasks (e.g., sandwich making\nand playing the video games). In this paper, we leverage Reinforcement Learning\n(RL) to highlight task-relevant locations of input frames. We propose a soft\nattention mechanism combined with the Deep Q-Network (DQN) model to teach an RL\nagent how to play a game and where to look by focusing on the most pertinent\nparts of its visual input. Our evaluations on several Atari 2600 games show\nthat the soft attention based model could predict fixation locations\nsignificantly better than bottom-up models such as Itti-Kochs saliency and\nGraph-Based Visual Saliency (GBVS) models.\n", "versions": [{"version": "v1", "created": "Sat, 17 Dec 2016 13:29:59 GMT"}, {"version": "v2", "created": "Sat, 18 Feb 2017 21:23:14 GMT"}], "update_date": "2017-02-21", "authors_parsed": [["Mousavi", "Sajad", ""], ["Schukat", "Michael", ""], ["Howley", "Enda", ""], ["Borji", "Ali", ""], ["Mozayani", "Nasser", ""]]}, {"id": "1612.05836", "submitter": "Shervin Ardeshir", "authors": "Shervin Ardeshir, Krishna Regmi, and Ali Borji", "title": "EgoTransfer: Transferring Motion Across Egocentric and Exocentric\n  Domains using Deep Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mirror neurons have been observed in the primary motor cortex of primate\nspecies, in particular in humans and monkeys. A mirror neuron fires when a\nperson performs a certain action, and also when he observes the same action\nbeing performed by another person. A crucial step towards building fully\nautonomous intelligent systems with human-like learning abilities is the\ncapability in modeling the mirror neuron. On one hand, the abundance of\negocentric cameras in the past few years has offered the opportunity to study a\nlot of vision problems from the first-person perspective. A great deal of\ninteresting research has been done during the past few years, trying to explore\nvarious computer vision tasks from the perspective of the self. On the other\nhand, videos recorded by traditional static cameras, capture humans performing\ndifferent actions from an exocentric third-person perspective. In this work, we\ntake the first step towards relating motion information across these two\nperspectives. We train models that predict motion in an egocentric view, by\nobserving it from an exocentric view, and vice versa. This allows models to\npredict how an egocentric motion would look like from outside. To do so, we\ntrain linear and nonlinear models and evaluate their performance in terms of\nretrieving the egocentric (exocentric) motion features, while having access to\nan exocentric (egocentric) motion feature. Our experimental results demonstrate\nthat motion information can be successfully transferred across the two views.\n", "versions": [{"version": "v1", "created": "Sat, 17 Dec 2016 23:33:37 GMT"}], "update_date": "2016-12-20", "authors_parsed": [["Ardeshir", "Shervin", ""], ["Regmi", "Krishna", ""], ["Borji", "Ali", ""]]}, {"id": "1612.05846", "submitter": "Evan Schwab", "authors": "Evan Schwab, Ren\\'e Vidal, Nicolas Charon", "title": "Joint Spatial-Angular Sparse Coding for dMRI with Separable Dictionaries", "comments": null, "journal-ref": "Evan Schwab, Rene Vidal, Nicolas Charon, Joint spatial-angular\n  sparse coding for dMRI with separable dictionaries, Medical Image Analysis,\n  Volume 48, August 2018, Pages 25-42, ISSN 1361-8415", "doi": "10.1016/j.media.2018.05.002.", "report-no": null, "categories": "stat.ML cs.CV q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Diffusion MRI (dMRI) provides the ability to reconstruct neuronal fibers in\nthe brain, $\\textit{in vivo}$, by measuring water diffusion along angular\ngradient directions in q-space. High angular resolution diffusion imaging\n(HARDI) can produce better estimates of fiber orientation than the popularly\nused diffusion tensor imaging, but the high number of samples needed to\nestimate diffusivity requires longer patient scan times. To accelerate dMRI,\ncompressed sensing (CS) has been utilized by exploiting a sparse dictionary\nrepresentation of the data, discovered through sparse coding. The sparser the\nrepresentation, the fewer samples are needed to reconstruct a high resolution\nsignal with limited information loss, and so an important area of research has\nfocused on finding the sparsest possible representation of dMRI. Current\nreconstruction methods however, rely on an angular representation $\\textit{per\nvoxel}$ with added spatial regularization, and so, for non-zero signals, one is\nrequired to have at least one non-zero coefficient per voxel. This means that\nthe global level of sparsity must be greater than the number of voxels. In\ncontrast, we propose a joint spatial-angular representation of dMRI that will\nallow us to achieve levels of global sparsity that are below the number of\nvoxels. A major challenge, however, is the computational complexity of solving\na global sparse coding problem over large-scale dMRI. In this work, we present\nnovel adaptations of popular sparse coding algorithms that become better suited\nfor solving large-scale problems by exploiting spatial-angular separability.\nOur experiments show that our method achieves significantly sparser\nrepresentations of HARDI than is possible by the state of the art.\n", "versions": [{"version": "v1", "created": "Sun, 18 Dec 2016 02:08:42 GMT"}, {"version": "v2", "created": "Tue, 25 Apr 2017 16:33:07 GMT"}, {"version": "v3", "created": "Tue, 29 May 2018 15:05:26 GMT"}], "update_date": "2018-05-30", "authors_parsed": [["Schwab", "Evan", ""], ["Vidal", "Ren\u00e9", ""], ["Charon", "Nicolas", ""]]}, {"id": "1612.05872", "submitter": "Matheus Gadelha", "authors": "Matheus Gadelha, Subhransu Maji and Rui Wang", "title": "3D Shape Induction from 2D Views of Multiple Objects", "comments": "Submitted to CVPR 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we investigate the problem of inducing a distribution over\nthree-dimensional structures given two-dimensional views of multiple objects\ntaken from unknown viewpoints. Our approach called \"projective generative\nadversarial networks\" (PrGANs) trains a deep generative model of 3D shapes\nwhose projections match the distributions of the input 2D views. The addition\nof a projection module allows us to infer the underlying 3D shape distribution\nwithout using any 3D, viewpoint information, or annotation during the learning\nphase. We show that our approach produces 3D shapes of comparable quality to\nGANs trained on 3D data for a number of shape categories including chairs,\nairplanes, and cars. Experiments also show that the disentangled representation\nof 2D shapes into geometry and viewpoint leads to a good generative model of 2D\nshapes. The key advantage is that our model allows us to predict 3D, viewpoint,\nand generate novel views from an input image in a completely unsupervised\nmanner.\n", "versions": [{"version": "v1", "created": "Sun, 18 Dec 2016 08:44:00 GMT"}], "update_date": "2016-12-20", "authors_parsed": [["Gadelha", "Matheus", ""], ["Maji", "Subhransu", ""], ["Wang", "Rui", ""]]}, {"id": "1612.05877", "submitter": "Zhiwu Huang", "authors": "Zhiwu Huang, Chengde Wan, Thomas Probst, Luc Van Gool", "title": "Deep Learning on Lie Groups for Skeleton-based Action Recognition", "comments": "Accepted to CVPR 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, skeleton-based action recognition has become a popular 3D\nclassification problem. State-of-the-art methods typically first represent each\nmotion sequence as a high-dimensional trajectory on a Lie group with an\nadditional dynamic time warping, and then shallowly learn favorable Lie group\nfeatures. In this paper we incorporate the Lie group structure into a deep\nnetwork architecture to learn more appropriate Lie group features for 3D action\nrecognition. Within the network structure, we design rotation mapping layers to\ntransform the input Lie group features into desirable ones, which are aligned\nbetter in the temporal domain. To reduce the high feature dimensionality, the\narchitecture is equipped with rotation pooling layers for the elements on the\nLie group. Furthermore, we propose a logarithm mapping layer to map the\nresulting manifold data into a tangent space that facilitates the application\nof regular output layers for the final classification. Evaluations of the\nproposed network for standard 3D human action recognition datasets clearly\ndemonstrate its superiority over existing shallow Lie group feature learning\nmethods as well as most conventional deep learning methods.\n", "versions": [{"version": "v1", "created": "Sun, 18 Dec 2016 09:08:29 GMT"}, {"version": "v2", "created": "Tue, 11 Apr 2017 08:47:00 GMT"}], "update_date": "2017-04-12", "authors_parsed": [["Huang", "Zhiwu", ""], ["Wan", "Chengde", ""], ["Probst", "Thomas", ""], ["Van Gool", "Luc", ""]]}, {"id": "1612.05890", "submitter": "Chao Ma", "authors": "Chao Ma, Chih-Yuan Yang, Xiaokang Yang, Ming-Hsuan Yang", "title": "Learning a No-Reference Quality Metric for Single-Image Super-Resolution", "comments": "Accepted by Computer Vision and Image Understanding", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Numerous single-image super-resolution algorithms have been proposed in the\nliterature, but few studies address the problem of performance evaluation based\non visual perception. While most super-resolution images are evaluated by\nfullreference metrics, the effectiveness is not clear and the required\nground-truth images are not always available in practice. To address these\nproblems, we conduct human subject studies using a large set of\nsuper-resolution images and propose a no-reference metric learned from visual\nperceptual scores. Specifically, we design three types of low-level statistical\nfeatures in both spatial and frequency domains to quantify super-resolved\nartifacts, and learn a two-stage regression model to predict the quality scores\nof super-resolution images without referring to ground-truth images. Extensive\nexperimental results show that the proposed metric is effective and efficient\nto assess the quality of super-resolution images based on human perception.\n", "versions": [{"version": "v1", "created": "Sun, 18 Dec 2016 10:30:58 GMT"}], "update_date": "2016-12-20", "authors_parsed": [["Ma", "Chao", ""], ["Yang", "Chih-Yuan", ""], ["Yang", "Xiaokang", ""], ["Yang", "Ming-Hsuan", ""]]}, {"id": "1612.05968", "submitter": "Wentao Zhu", "authors": "Wentao Zhu, Qi Lou, Yeeleng Scott Vang, Xiaohui Xie", "title": "Deep Multi-instance Networks with Sparse Label Assignment for Whole\n  Mammogram Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mammogram classification is directly related to computer-aided diagnosis of\nbreast cancer. Traditional methods requires great effort to annotate the\ntraining data by costly manual labeling and specialized computational models to\ndetect these annotations during test. Inspired by the success of using deep\nconvolutional features for natural image analysis and multi-instance learning\nfor labeling a set of instances/patches, we propose end-to-end trained deep\nmulti-instance networks for mass classification based on whole mammogram\nwithout the aforementioned costly need to annotate the training data. We\nexplore three different schemes to construct deep multi-instance networks for\nwhole mammogram classification. Experimental results on the INbreast dataset\ndemonstrate the robustness of proposed deep networks compared to previous work\nusing segmentation and detection annotations in the training.\n", "versions": [{"version": "v1", "created": "Sun, 18 Dec 2016 18:31:11 GMT"}], "update_date": "2016-12-20", "authors_parsed": [["Zhu", "Wentao", ""], ["Lou", "Qi", ""], ["Vang", "Yeeleng Scott", ""], ["Xie", "Xiaohui", ""]]}, {"id": "1612.05970", "submitter": "Wentao Zhu", "authors": "Wentao Zhu, Xiang Xiang, Trac D. Tran, Xiaohui Xie", "title": "Adversarial Deep Structural Networks for Mammographic Mass Segmentation", "comments": "First version on arXiv 2016, MICCAI 2017 Deep Learning in Medical\n  Image Analysis (DLMIA) workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mass segmentation is an important task in mammogram analysis, providing\neffective morphological features and regions of interest (ROI) for mass\ndetection and classification. Inspired by the success of using deep\nconvolutional features for natural image analysis and conditional random fields\n(CRF) for structural learning, we propose an end-to-end network for\nmammographic mass segmentation. The network employs a fully convolutional\nnetwork (FCN) to model potential function, followed by a CRF to perform\nstructural learning. Because the mass distribution varies greatly with pixel\nposition, the FCN is combined with position priori for the task. Due to the\nsmall size of mammogram datasets, we use adversarial training to control\nover-fitting. Four models with different convolutional kernels are further\nfused to improve the segmentation results. Experimental results on two public\ndatasets, INbreast and DDSM-BCRP, show that our end-to-end network combined\nwith adversarial training achieves the-state-of-the-art results.\n", "versions": [{"version": "v1", "created": "Sun, 18 Dec 2016 18:40:21 GMT"}, {"version": "v2", "created": "Fri, 9 Jun 2017 21:32:38 GMT"}], "update_date": "2017-06-13", "authors_parsed": [["Zhu", "Wentao", ""], ["Xiang", "Xiang", ""], ["Tran", "Trac D.", ""], ["Xie", "Xiaohui", ""]]}, {"id": "1612.06017", "submitter": "Victor Yurchenko", "authors": "Victor Yurchenko, Victor Lempitsky", "title": "Parsing Images of Overlapping Organisms with Deep Singling-Out Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work is motivated by the mostly unsolved task of parsing biological\nimages with multiple overlapping articulated model organisms (such as worms or\nlarvae). We present a general approach that separates the two main challenges\nassociated with such data, individual object shape estimation and object groups\ndisentangling. At the core of the approach is a deep feed-forward singling-out\nnetwork (SON) that is trained to map each local patch to a vectorial descriptor\nthat is sensitive to the characteristics (e.g. shape) of a central object,\nwhile being invariant to the variability of all other surrounding elements.\nGiven a SON, a local image patch can be matched to a gallery of isolated\nelements using their SON-descriptors, thus producing a hypothesis about the\nshape of the central element in that patch. The image-level optimization based\non integer programming can then pick a subset of the hypotheses to explain\n(parse) the whole image and disentangle groups of organisms.\n  While sharing many similarities with existing \"analysis-by-synthesis\"\napproaches, our method avoids the need for stochastic search in the\nhigh-dimensional configuration space and numerous rendering operations at\ntest-time. We show that our approach can parse microscopy images of three\npopular model organisms (the C.Elegans roundworms, the Drosophila larvae, and\nthe E.Coli bacteria) even under significant crowding and overlaps between\norganisms. We speculate that the overall approach is applicable to a wider\nclass of image parsing problems concerned with crowded articulated objects, for\nwhich rendering training images is possible.\n", "versions": [{"version": "v1", "created": "Mon, 19 Dec 2016 00:59:30 GMT"}], "update_date": "2016-12-20", "authors_parsed": [["Yurchenko", "Victor", ""], ["Lempitsky", "Victor", ""]]}, {"id": "1612.06053", "submitter": "Zhizhen Chi", "authors": "Zhizhen Chi and Hongyang Li and Huchuan Lu and Ming-Hsuan Yang", "title": "Dual Deep Network for Visual Tracking", "comments": null, "journal-ref": null, "doi": "10.1109/TIP.2017.2669880", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual tracking addresses the problem of identifying and localizing an\nunknown target in a video given the target specified by a bounding box in the\nfirst frame. In this paper, we propose a dual network to better utilize\nfeatures among layers for visual tracking. It is observed that features in\nhigher layers encode semantic context while its counterparts in lower layers\nare sensitive to discriminative appearance. Thus we exploit the hierarchical\nfeatures in different layers of a deep model and design a dual structure to\nobtain better feature representation from various streams, which is rarely\ninvestigated in previous work. To highlight geometric contours of the target,\nwe integrate the hierarchical feature maps with an edge detector as the coarse\nprior maps to further embed local details around the target. To leverage the\nrobustness of our dual network, we train it with random patches measuring the\nsimilarities between the network activation and target appearance, which serves\nas a regularization to enforce the dual network to focus on target object. The\nproposed dual network is updated online in a unique manner based on the\nobservation that the target being tracked in consecutive frames should share\nmore similar feature representations than those in the surrounding background.\nIt is also found that for a target object, the prior maps can help further\nenhance performance by passing message into the output maps of the dual\nnetwork. Therefore, an independent component analysis with reference algorithm\n(ICA-R) is employed to extract target context using prior maps as guidance.\nOnline tracking is conducted by maximizing the posterior estimate on the final\nmaps with stochastic and periodic update. Quantitative and qualitative\nevaluations on two large-scale benchmark data sets show that the proposed\nalgorithm performs favourably against the state-of-the-arts.\n", "versions": [{"version": "v1", "created": "Mon, 19 Dec 2016 06:01:30 GMT"}], "update_date": "2017-04-26", "authors_parsed": [["Chi", "Zhizhen", ""], ["Li", "Hongyang", ""], ["Lu", "Huchuan", ""], ["Yang", "Ming-Hsuan", ""]]}, {"id": "1612.06070", "submitter": "Mihir Mongia", "authors": "Mihir Mongia and Kundan Kumar and Akram Erraqabi and Yoshua Bengio", "title": "On Random Weights for Texture Generation in One Layer Neural Networks", "comments": "ICASSP 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent work in the literature has shown experimentally that one can use the\nlower layers of a trained convolutional neural network (CNN) to model natural\ntextures. More interestingly, it has also been experimentally shown that only\none layer with random filters can also model textures although with less\nvariability. In this paper we ask the question as to why one layer CNNs with\nrandom filters are so effective in generating textures? We theoretically show\nthat one layer convolutional architectures (without a non-linearity) paired\nwith the an energy function used in previous literature, can in fact preserve\nand modulate frequency coefficients in a manner so that random weights and\npretrained weights will generate the same type of images. Based on the results\nof this analysis we question whether similar properties hold in the case where\none uses one convolution layer with a non-linearity. We show that in the case\nof ReLu non-linearity there are situations where only one input will give the\nminimum possible energy whereas in the case of no nonlinearity, there are\nalways infinite solutions that will give the minimum possible energy. Thus we\ncan show that in certain situations adding a ReLu non-linearity generates less\nvariable images.\n", "versions": [{"version": "v1", "created": "Mon, 19 Dec 2016 08:21:04 GMT"}], "update_date": "2016-12-20", "authors_parsed": [["Mongia", "Mihir", ""], ["Kumar", "Kundan", ""], ["Erraqabi", "Akram", ""], ["Bengio", "Yoshua", ""]]}, {"id": "1612.06096", "submitter": "Shadi Albarqouni", "authors": "Shadi Albarqouni, Javad Fotouhi, Nassir Navab", "title": "X-ray In-Depth Decomposition: Revealing The Latent Structures", "comments": "Under review at MICCAI 2017", "journal-ref": "Medical Image Computing and Computer Assisted Intervention (MICCAI\n  2017)", "doi": "10.1007/978-3-319-66179-7_51", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  X-ray radiography is the most readily available imaging modality and has a\nbroad range of applications that spans from diagnosis to intra-operative\nguidance in cardiac, orthopedics, and trauma procedures. Proper interpretation\nof the hidden and obscured anatomy in X-ray images remains a challenge and\noften requires high radiation dose and imaging from several perspectives. In\nthis work, we aim at decomposing the conventional X-ray image into d X-ray\ncomponents of independent, non-overlapped, clipped sub-volumes using deep\nlearning approach. Despite the challenging aspects of modeling such a highly\nill-posed problem, exciting and encouraging results are obtained paving the\npath for further contributions in this direction.\n", "versions": [{"version": "v1", "created": "Mon, 19 Dec 2016 09:59:07 GMT"}, {"version": "v2", "created": "Wed, 22 Mar 2017 17:12:17 GMT"}], "update_date": "2019-04-19", "authors_parsed": [["Albarqouni", "Shadi", ""], ["Fotouhi", "Javad", ""], ["Navab", "Nassir", ""]]}, {"id": "1612.06098", "submitter": "Sailesh Conjeti", "authors": "Sailesh Conjeti, Anees Kazi, Nassir Navab and Amin Katouzian", "title": "Cross-Modal Manifold Learning for Cross-modal Retrieval", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a new scalable algorithm for cross-modal similarity\npreserving retrieval in a learnt manifold space. Unlike existing approaches\nthat compromise between preserving global and local geometries, the proposed\ntechnique respects both simultaneously during manifold alignment. The global\ntopologies are maintained by recovering underlying mapping functions in the\njoint manifold space by deploying partially corresponding instances. The\ninter-, and intra-modality affinity matrices are then computed to reinforce\noriginal data skeleton using perturbed minimum spanning tree (pMST), and\nmaximizing the affinity among similar cross-modal instances, respectively. The\nperformance of proposed algorithm is evaluated upon two multimodal image\ndatasets (coronary atherosclerosis histology and brain MRI) for two\napplications: classification, and regression. Our exhaustive validations and\nresults demonstrate the superiority of our technique over comparative methods\nand its feasibility for improving computer-assisted diagnosis systems, where\ndisease-specific complementary information shall be aggregated and interpreted\nacross modalities to form the final decision.\n", "versions": [{"version": "v1", "created": "Mon, 19 Dec 2016 10:03:58 GMT"}], "update_date": "2016-12-20", "authors_parsed": [["Conjeti", "Sailesh", ""], ["Kazi", "Anees", ""], ["Navab", "Nassir", ""], ["Katouzian", "Amin", ""]]}, {"id": "1612.06129", "submitter": "Christoph K\\\"ading", "authors": "Christoph K\\\"ading and Erik Rodner and Alexander Freytag and Joachim\n  Denzler", "title": "Active and Continuous Exploration with Deep Neural Networks and Expected\n  Model Output Changes", "comments": "accepted contribution at NIPS 2016 Workshop on Continual Learning and\n  Deep Networks", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The demands on visual recognition systems do not end with the complexity\noffered by current large-scale image datasets, such as ImageNet. In\nconsequence, we need curious and continuously learning algorithms that actively\nacquire knowledge about semantic concepts which are present in available\nunlabeled data. As a step towards this goal, we show how to perform continuous\nactive learning and exploration, where an algorithm actively selects relevant\nbatches of unlabeled examples for annotation. These examples could either\nbelong to already known or to yet undiscovered classes. Our algorithm is based\non a new generalization of the Expected Model Output Change principle for deep\narchitectures and is especially tailored to deep neural networks. Furthermore,\nwe show easy-to-implement approximations that yield efficient techniques for\nactive selection. Empirical experiments show that our method outperforms\ncurrently used heuristics.\n", "versions": [{"version": "v1", "created": "Mon, 19 Dec 2016 11:27:33 GMT"}], "update_date": "2016-12-20", "authors_parsed": [["K\u00e4ding", "Christoph", ""], ["Rodner", "Erik", ""], ["Freytag", "Alexander", ""], ["Denzler", "Joachim", ""]]}, {"id": "1612.06152", "submitter": "Zhongwen Xu", "authors": "Zhongwen Xu, Linchao Zhu, Yi Yang", "title": "Few-Shot Object Recognition from Machine-Labeled Web Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the tremendous advances of Convolutional Neural Networks (ConvNets) on\nobject recognition, we can now obtain reliable enough machine-labeled\nannotations easily by predictions from off-the-shelf ConvNets. In this work, we\npresent an abstraction memory based framework for few-shot learning, building\nupon machine-labeled image annotations. Our method takes some large-scale\nmachine-annotated datasets (e.g., OpenImages) as an external memory bank. In\nthe external memory bank, the information is stored in the memory slots with\nthe form of key-value, where image feature is regarded as key and label\nembedding serves as value. When queried by the few-shot examples, our model\nselects visually similar data from the external memory bank, and writes the\nuseful information obtained from related external data into another memory\nbank, i.e., abstraction memory. Long Short-Term Memory (LSTM) controllers and\nattention mechanisms are utilized to guarantee the data written to the\nabstraction memory is correlated to the query example. The abstraction memory\nconcentrates information from the external memory bank, so that it makes the\nfew-shot recognition effective. In the experiments, we firstly confirm that our\nmodel can learn to conduct few-shot object recognition on clean human-labeled\ndata from ImageNet dataset. Then, we demonstrate that with our model,\nmachine-labeled image annotations are very effective and abundant resources to\nperform object recognition on novel categories. Experimental results show that\nour proposed model with machine-labeled annotations achieves great performance,\nonly with a gap of 1% between of the one with human-labeled annotations.\n", "versions": [{"version": "v1", "created": "Mon, 19 Dec 2016 12:25:36 GMT"}], "update_date": "2016-12-20", "authors_parsed": [["Xu", "Zhongwen", ""], ["Zhu", "Linchao", ""], ["Yang", "Yi", ""]]}, {"id": "1612.06170", "submitter": "Weiya Ren", "authors": "Weiya Ren", "title": "Crowd collectiveness measure via graph-based node clique learning", "comments": "23 pages. 10 figures. 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Collectiveness motions of crowd systems have attracted a great deal of\nattentions in recently years. In this paper, we try to measure the\ncollectiveness of a crowd system by the proposed node clique learning method.\nThe proposed method is a graph based method, and investigates the influence\nfrom one node to other nodes. A node is represented by a set of nodes which\nnamed a clique, which is obtained by spreading information from this node to\nother nodes in graph. Then only nodes with sufficient information are selected\nas the clique of this node. The motion coherence between two nodes is defined\nby node cliques comparing. The collectiveness of a node and the collectiveness\nof the crowd system are defined by the nodes coherence. Self-driven particle\n(SDP) model and the crowd motion database are used to test the ability of the\nproposed method in measuring collectiveness.\n", "versions": [{"version": "v1", "created": "Mon, 19 Dec 2016 13:26:46 GMT"}], "update_date": "2016-12-20", "authors_parsed": [["Ren", "Weiya", ""]]}, {"id": "1612.06176", "submitter": "Dirk Lorenz", "authors": "Lars M. Mescheder, Dirk A. Lorenz", "title": "An extended Perona-Malik model based on probabilistic models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV math.NA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Perona-Malik model has been very successful at restoring images from\nnoisy input. In this paper, we reinterpret the Perona-Malik model in the\nlanguage of Gaussian scale mixtures and derive some extensions of the model.\nSpecifically, we show that the expectation-maximization (EM) algorithm applied\nto Gaussian scale mixtures leads to the lagged-diffusivity algorithm for\ncomputing stationary points of the Perona-Malik diffusion equations. Moreover,\nwe show how mean field approximations to these Gaussian scale mixtures lead to\na modification of the lagged-diffusivity algorithm that better captures the\nuncertainties in the restoration. Since this modification can be hard to\ncompute in practice we propose relaxations to the mean field objective to make\nthe algorithm computationally feasible. Our numerical experiments show that\nthis modified lagged-diffusivity algorithm often performs better at restoring\ntextured areas and fuzzy edges than the unmodified algorithm. As a second\napplication of the Gaussian scale mixture framework, we show how an efficient\nsampling procedure can be obtained for the probabilistic model, making the\ncomputation of the conditional mean and other expectations algorithmically\nfeasible. Again, the resulting algorithm has a strong resemblance to the\nlagged-diffusivity algorithm. Finally, we show that a probabilistic version of\nthe Mumford-Shah segementation model can be obtained in the same framework with\na discrete edge-prior.\n", "versions": [{"version": "v1", "created": "Mon, 19 Dec 2016 13:39:45 GMT"}], "update_date": "2016-12-20", "authors_parsed": [["Mescheder", "Lars M.", ""], ["Lorenz", "Dirk A.", ""]]}, {"id": "1612.06259", "submitter": "Dimitris Spathis", "authors": "Dimitris Spathis", "title": "Photo-Quality Evaluation based on Computational Aesthetics: Review of\n  Feature Extraction Techniques", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Researchers try to model the aesthetic quality of photographs into low and\nhigh- level features, drawing inspiration from art theory, psychology and\nmarketing. We attempt to describe every feature extraction measure employed in\nthe above process. The contribution of this literature review is the taxonomy\nof each feature by its implementation complexity, considering real-world\napplications and integration in mobile apps and digital cameras. Also, we\ndiscuss the machine learning results along with some unexplored research areas\nas future work.\n", "versions": [{"version": "v1", "created": "Mon, 19 Dec 2016 16:41:24 GMT"}], "update_date": "2016-12-20", "authors_parsed": [["Spathis", "Dimitris", ""]]}, {"id": "1612.06305", "submitter": "Ben Nassi", "authors": "Ben Nassi, Alona Levy, Yuval Elovici, Erez Shmueli", "title": "Handwritten Signature Verification Using Hand-Worn Devices", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.CV cs.CY", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Online signature verification technologies, such as those available in banks\nand post offices, rely on dedicated digital devices such as tablets or smart\npens to capture, analyze and verify signatures. In this paper, we suggest a\nnovel method for online signature verification that relies on the increasingly\navailable hand-worn devices, such as smartwatches or fitness trackers, instead\nof dedicated ad-hoc devices. Our method uses a set of known genuine and forged\nsignatures, recorded using the motion sensors of a hand-worn device, to train a\nmachine learning classifier. Then, given the recording of an unknown signature\nand a claimed identity, the classifier can determine whether the signature is\ngenuine or forged. In order to validate our method, it was applied on 1980\nrecordings of genuine and forged signatures that we collected from 66 subjects\nin our institution. Using our method, we were able to successfully distinguish\nbetween genuine and forged signatures with a high degree of accuracy (0.98 AUC\nand 0.05 EER).\n", "versions": [{"version": "v1", "created": "Mon, 19 Dec 2016 18:42:52 GMT"}], "update_date": "2016-12-20", "authors_parsed": [["Nassi", "Ben", ""], ["Levy", "Alona", ""], ["Elovici", "Yuval", ""], ["Shmueli", "Erez", ""]]}, {"id": "1612.06321", "submitter": "Andre Araujo", "authors": "Hyeonwoo Noh, Andre Araujo, Jack Sim, Tobias Weyand, Bohyung Han", "title": "Large-Scale Image Retrieval with Attentive Deep Local Features", "comments": "ICCV 2017. Code and dataset available:\n  https://github.com/tensorflow/models/tree/master/research/delf", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We propose an attentive local feature descriptor suitable for large-scale\nimage retrieval, referred to as DELF (DEep Local Feature). The new feature is\nbased on convolutional neural networks, which are trained only with image-level\nannotations on a landmark image dataset. To identify semantically useful local\nfeatures for image retrieval, we also propose an attention mechanism for\nkeypoint selection, which shares most network layers with the descriptor. This\nframework can be used for image retrieval as a drop-in replacement for other\nkeypoint detectors and descriptors, enabling more accurate feature matching and\ngeometric verification. Our system produces reliable confidence scores to\nreject false positives---in particular, it is robust against queries that have\nno correct match in the database. To evaluate the proposed descriptor, we\nintroduce a new large-scale dataset, referred to as Google-Landmarks dataset,\nwhich involves challenges in both database and query such as background\nclutter, partial occlusion, multiple landmarks, objects in variable scales,\netc. We show that DELF outperforms the state-of-the-art global and local\ndescriptors in the large-scale setting by significant margins. Code and dataset\ncan be found at the project webpage:\nhttps://github.com/tensorflow/models/tree/master/research/delf .\n", "versions": [{"version": "v1", "created": "Mon, 19 Dec 2016 19:35:56 GMT"}, {"version": "v2", "created": "Mon, 17 Apr 2017 16:24:04 GMT"}, {"version": "v3", "created": "Wed, 2 Aug 2017 17:27:45 GMT"}, {"version": "v4", "created": "Sat, 3 Feb 2018 02:19:16 GMT"}], "update_date": "2018-02-06", "authors_parsed": [["Noh", "Hyeonwoo", ""], ["Araujo", "Andre", ""], ["Sim", "Jack", ""], ["Weyand", "Tobias", ""], ["Han", "Bohyung", ""]]}, {"id": "1612.06341", "submitter": "Aron Yu", "authors": "Aron Yu, Kristen Grauman", "title": "Semantic Jitter: Dense Supervision for Visual Comparisons via Synthetic\n  Images", "comments": null, "journal-ref": null, "doi": "10.1109/ICCV.2017.594", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distinguishing subtle differences in attributes is valuable, yet learning to\nmake visual comparisons remains non-trivial. Not only is the number of possible\ncomparisons quadratic in the number of training images, but also access to\nimages adequately spanning the space of fine-grained visual differences is\nlimited. We propose to overcome the sparsity of supervision problem via\nsynthetically generated images. Building on a state-of-the-art image generation\nengine, we sample pairs of training images exhibiting slight modifications of\nindividual attributes. Augmenting real training image pairs with these\nexamples, we then train attribute ranking models to predict the relative\nstrength of an attribute in novel pairs of real images. Our results on datasets\nof faces and fashion images show the great promise of bootstrapping imperfect\nimage generators to counteract sample sparsity for learning to rank.\n", "versions": [{"version": "v1", "created": "Mon, 19 Dec 2016 20:42:43 GMT"}, {"version": "v2", "created": "Thu, 27 Apr 2017 17:31:55 GMT"}], "update_date": "2018-04-09", "authors_parsed": [["Yu", "Aron", ""], ["Grauman", "Kristen", ""]]}, {"id": "1612.06370", "submitter": "Deepak Pathak", "authors": "Deepak Pathak, Ross Girshick, Piotr Doll\\'ar, Trevor Darrell, Bharath\n  Hariharan", "title": "Learning Features by Watching Objects Move", "comments": "CVPR 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a novel yet intuitive approach to unsupervised feature\nlearning. Inspired by the human visual system, we explore whether low-level\nmotion-based grouping cues can be used to learn an effective visual\nrepresentation. Specifically, we use unsupervised motion-based segmentation on\nvideos to obtain segments, which we use as 'pseudo ground truth' to train a\nconvolutional network to segment objects from a single frame. Given the\nextensive evidence that motion plays a key role in the development of the human\nvisual system, we hope that this straightforward approach to unsupervised\nlearning will be more effective than cleverly designed 'pretext' tasks studied\nin the literature. Indeed, our extensive experiments show that this is the\ncase. When used for transfer learning on object detection, our representation\nsignificantly outperforms previous unsupervised approaches across multiple\nsettings, especially when training data for the target task is scarce.\n", "versions": [{"version": "v1", "created": "Mon, 19 Dec 2016 20:56:04 GMT"}, {"version": "v2", "created": "Wed, 12 Apr 2017 04:28:47 GMT"}], "update_date": "2017-04-13", "authors_parsed": [["Pathak", "Deepak", ""], ["Girshick", "Ross", ""], ["Doll\u00e1r", "Piotr", ""], ["Darrell", "Trevor", ""], ["Hariharan", "Bharath", ""]]}, {"id": "1612.06371", "submitter": "Gunnar Sigurdsson", "authors": "Gunnar A. Sigurdsson and Santosh Divvala and Ali Farhadi and Abhinav\n  Gupta", "title": "Asynchronous Temporal Fields for Action Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Actions are more than just movements and trajectories: we cook to eat and we\nhold a cup to drink from it. A thorough understanding of videos requires going\nbeyond appearance modeling and necessitates reasoning about the sequence of\nactivities, as well as the higher-level constructs such as intentions. But how\ndo we model and reason about these? We propose a fully-connected temporal CRF\nmodel for reasoning over various aspects of activities that includes objects,\nactions, and intentions, where the potentials are predicted by a deep network.\nEnd-to-end training of such structured models is a challenging endeavor: For\ninference and learning we need to construct mini-batches consisting of whole\nvideos, leading to mini-batches with only a few videos. This causes\nhigh-correlation between data points leading to breakdown of the backprop\nalgorithm. To address this challenge, we present an asynchronous variational\ninference method that allows efficient end-to-end training. Our method achieves\na classification mAP of 22.4% on the Charades benchmark, outperforming the\nstate-of-the-art (17.2% mAP), and offers equal gains on the task of temporal\nlocalization.\n", "versions": [{"version": "v1", "created": "Mon, 19 Dec 2016 20:56:40 GMT"}, {"version": "v2", "created": "Mon, 24 Jul 2017 09:58:14 GMT"}], "update_date": "2017-07-25", "authors_parsed": [["Sigurdsson", "Gunnar A.", ""], ["Divvala", "Santosh", ""], ["Farhadi", "Ali", ""], ["Gupta", "Abhinav", ""]]}, {"id": "1612.06423", "submitter": "Alireza Rahimpour", "authors": "Alireza Rahimpour, Ali Taalimi, Hairong Qi", "title": "Feature Encoding in Band-limited Distributed Surveillance Systems", "comments": "The 42th International Conference on Acoustics, Speech, and Signal\n  Processing (ICASSP 2017)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distributed surveillance systems have become popular in recent years due to\nsecurity concerns. However, transmitting high dimensional data in\nbandwidth-limited distributed systems becomes a major challenge. In this paper,\nwe address this issue by proposing a novel probabilistic algorithm based on the\ndivergence between the probability distributions of the visual features in\norder to reduce their dimensionality and thus save the network bandwidth in\ndistributed wireless smart camera networks. We demonstrate the effectiveness of\nthe proposed approach through extensive experiments on two surveillance\nrecognition tasks.\n", "versions": [{"version": "v1", "created": "Mon, 19 Dec 2016 21:38:53 GMT"}, {"version": "v2", "created": "Tue, 6 Jun 2017 20:37:10 GMT"}, {"version": "v3", "created": "Mon, 29 Apr 2019 04:09:41 GMT"}], "update_date": "2019-04-30", "authors_parsed": [["Rahimpour", "Alireza", ""], ["Taalimi", "Ali", ""], ["Qi", "Hairong", ""]]}, {"id": "1612.06435", "submitter": "Odemir Bruno PhD", "authors": "Jo\\~ao B. Florindo, Odemir M. Bruno", "title": "Fractal Descriptors of Texture Images Based on the Triangular Prism\n  Dimension", "comments": "21 pages, 14 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work presents a novel descriptor for texture images based on fractal\ngeometry and its application to image analysis. The descriptors are provided by\nestimating the triangular prism fractal dimension under different scales with a\nweight exponential parameter, followed by dimensionality reduction using\nKarhunen-Lo\\`{e}ve transform. The efficiency of the proposed descriptors is\ntested on two well-known texture data sets, that is, Brodatz and Vistex, both\nfor classification and image retrieval. The novel method is also tested\nconcerning invariances in situations when the textures are rotated or affected\nby Gaussian noise. The obtained results outperform other classical and\nstate-of-the-art descriptors in the literature and demonstrate the power of the\ntriangular descriptors in these tasks, suggesting their use in practical\napplications of image analysis based on texture features.\n", "versions": [{"version": "v1", "created": "Mon, 19 Dec 2016 22:01:59 GMT"}], "update_date": "2016-12-21", "authors_parsed": [["Florindo", "Jo\u00e3o B.", ""], ["Bruno", "Odemir M.", ""]]}, {"id": "1612.06443", "submitter": "Odemir Bruno PhD", "authors": "Mariane Barros Neiva, Antoine Manzanera, Odemir Martinez Bruno", "title": "Binary Distance Transform to Improve Feature Extraction", "comments": "9 pages, 4 figures, WVC 2016 proceedings", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To recognize textures many methods have been developed along the years.\nHowever, texture datasets may be hard to be classified due to artefacts such as\na variety of scale, illumination and noise. This paper proposes the application\nof binary distance transform on the original dataset to add information to\ntexture representation and consequently improve recognition. Texture images,\nusually in grayscale, suffers a binarization prior to distance transform and\none of the resulted images are combined with original texture to improve the\namount of information. Four datasets are used to evaluate our approach. For\nOutex dataset, for instance, the proposal outperforms all rates, improvements\nof an up to 10\\%, compared to traditional approach where descriptors are\napplied on the original dataset, showing the importance of this approach.\n", "versions": [{"version": "v1", "created": "Mon, 19 Dec 2016 22:19:19 GMT"}], "update_date": "2016-12-21", "authors_parsed": [["Neiva", "Mariane Barros", ""], ["Manzanera", "Antoine", ""], ["Bruno", "Odemir Martinez", ""]]}, {"id": "1612.06454", "submitter": "Henrique Morimitsu", "authors": "Henrique Morimitsu, Isabelle Bloch and Roberto M. Cesar-Jr", "title": "Exploring Structure for Long-Term Tracking of Multiple Objects in Sports\n  Videos", "comments": "This version corresponds to the preprint of the paper accepted for\n  CVIU", "journal-ref": null, "doi": "10.1016/j.cviu.2016.12.003", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a novel approach for exploiting structural\nrelations to track multiple objects that may undergo long-term occlusion and\nabrupt motion. We use a model-free approach that relies only on annotations\ngiven in the first frame of the video to track all the objects online, i.e.\nwithout knowledge from future frames. We initialize a probabilistic Attributed\nRelational Graph (ARG) from the first frame, which is incrementally updated\nalong the video. Instead of using the structural information only to evaluate\nthe scene, the proposed approach considers it to generate new tracking\nhypotheses. In this way, our method is capable of generating relevant object\ncandidates that are used to improve or recover the track of lost objects. The\nproposed method is evaluated on several videos of table tennis, volleyball, and\non the ACASVA dataset. The results show that our approach is very robust,\nflexible and able to outperform other state-of-the-art methods in sports videos\nthat present structural patterns.\n", "versions": [{"version": "v1", "created": "Mon, 19 Dec 2016 23:14:26 GMT"}], "update_date": "2016-12-21", "authors_parsed": [["Morimitsu", "Henrique", ""], ["Bloch", "Isabelle", ""], ["Cesar-Jr", "Roberto M.", ""]]}, {"id": "1612.06457", "submitter": "Corneliu Arsene Dr", "authors": "Corneliu T.C. Arsene, Stephen Church, Mark Dickinson", "title": "High Performance Software in Multidimensional Reduction Methods for\n  Image Processing with Application to Ancient Manuscripts", "comments": "25 pages; University of Manchester, UK. Paper submitted to Manuscript\n  Cultures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multispectral imaging is an important technique for improving the readability\nof written or printed text where the letters have faded, either due to\ndeliberate erasing or simply due to the ravages of time. Often the text can be\nread simply by looking at individual wavelengths, but in some cases the images\nneed further enhancement to maximise the chances of reading the text. There are\nmany possible enhancement techniques and this paper assesses and compares an\nextended set of dimensionality reduction methods for image processing. We\nassess 15 dimensionality reduction methods in two different manuscripts. This\nassessment was performed both subjectively by asking the opinions of scholars\nwho were experts in the languages used in the manuscripts which of the\ntechniques they preferred and also by using the Davies-Bouldin and Dunn indexes\nfor assessing the quality of the resulted image clusters. We found that the\nCanonical Variates Analysis (CVA) method which was using a Matlab\nimplementation and we have used previously to enhance multispectral images, it\nwas indeed superior to all the other tested methods. However it is very likely\nthat other approaches will be more suitable in specific circumstance so we\nwould still recommend that a range of these techniques are tried. In\nparticular, CVA is a supervised clustering technique so it requires\nconsiderably more user time and effort than a non-supervised technique such as\nthe much more commonly used Principle Component Analysis Approach (PCA). If the\nresults from PCA are adequate to allow a text to be read then the added effort\nrequired for CVA may not be justified. For the purposes of comparing the\ncomputational times and the image results, a CVA method is also implemented in\nC programming language and using the GNU (GNUs Not Unix) Scientific Library\n(GSL) and the OpenCV (OPEN source Computer Vision) computer vision programming\nlibrary.\n", "versions": [{"version": "v1", "created": "Mon, 19 Dec 2016 23:38:26 GMT"}, {"version": "v2", "created": "Sun, 25 Dec 2016 16:43:39 GMT"}, {"version": "v3", "created": "Tue, 1 Aug 2017 13:40:12 GMT"}, {"version": "v4", "created": "Fri, 29 Sep 2017 17:58:37 GMT"}, {"version": "v5", "created": "Wed, 18 Jul 2018 22:44:43 GMT"}], "update_date": "2018-07-20", "authors_parsed": [["Arsene", "Corneliu T. C.", ""], ["Church", "Stephen", ""], ["Dickinson", "Mark", ""]]}, {"id": "1612.06496", "submitter": "Nathan Cahill", "authors": "Renee T. Meinhold, Tyler L. Hayes, Nathan D. Cahill", "title": "Efficiently Computing Piecewise Flat Embeddings for Data Clustering and\n  Image Segmentation", "comments": "Presented at the 2016 IEEE MIT Undergraduate Research Technology\n  Conference (http://ieee.scripts.mit.edu/conference/index.php)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image segmentation is a popular area of research in computer vision that has\nmany applications in automated image processing. A recent technique called\npiecewise flat embeddings (PFE) has been proposed for use in image\nsegmentation; PFE transforms image pixel data into a lower dimensional\nrepresentation where similar pixels are pulled close together and dissimilar\npixels are pushed apart. This technique has shown promising results, but its\noriginal formulation is not computationally feasible for large images. We\npropose two improvements to the algorithm for computing PFE: first, we\nreformulate portions of the algorithm to enable various linear algebra\noperations to be performed in parallel; second, we propose utilizing an\niterative linear solver (preconditioned conjugate gradient) to quickly solve a\nlinear least-squares problem that occurs in the inner loop of a nested\niteration. With these two computational improvements, we show on a publicly\navailable image database that PFE can be sped up by an order of magnitude\nwithout sacrificing segmentation performance. Our results make this technique\nmore practical for use on large data sets, not only for image segmentation, but\nfor general data clustering problems.\n", "versions": [{"version": "v1", "created": "Tue, 20 Dec 2016 03:06:58 GMT"}], "update_date": "2016-12-21", "authors_parsed": [["Meinhold", "Renee T.", ""], ["Hayes", "Tyler L.", ""], ["Cahill", "Nathan D.", ""]]}, {"id": "1612.06508", "submitter": "Youngjung Kim", "authors": "Youngjung Kim, Hyungjoo Jung, Dongbo Min, Kwanghoon Sohn", "title": "Deeply Aggregated Alternating Minimization for Image Restoration", "comments": "9 PAGES", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Regularization-based image restoration has remained an active research topic\nin computer vision and image processing. It often leverages a guidance signal\ncaptured in different fields as an additional cue. In this work, we present a\ngeneral framework for image restoration, called deeply aggregated alternating\nminimization (DeepAM). We propose to train deep neural network to advance two\nof the steps in the conventional AM algorithm: proximal mapping and ?-\ncontinuation. Both steps are learned from a large dataset in an end-to-end\nmanner. The proposed framework enables the convolutional neural networks (CNNs)\nto operate as a prior or regularizer in the AM algorithm. We show that our\nlearned regularizer via deep aggregation outperforms the recent data-driven\napproaches as well as the nonlocalbased methods. The flexibility and\neffectiveness of our framework are demonstrated in several image restoration\ntasks, including single image denoising, RGB-NIR restoration, and depth\nsuper-resolution.\n", "versions": [{"version": "v1", "created": "Tue, 20 Dec 2016 04:56:56 GMT"}], "update_date": "2016-12-21", "authors_parsed": [["Kim", "Youngjung", ""], ["Jung", "Hyungjoo", ""], ["Min", "Dongbo", ""], ["Sohn", "Kwanghoon", ""]]}, {"id": "1612.06519", "submitter": "Forrest Iandola", "authors": "Forrest Iandola", "title": "Exploring the Design Space of Deep Convolutional Neural Networks at\n  Large Scale", "comments": "thesis, UC Berkeley (2016)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, the research community has discovered that deep neural\nnetworks (DNNs) and convolutional neural networks (CNNs) can yield higher\naccuracy than all previous solutions to a broad array of machine learning\nproblems. To our knowledge, there is no single CNN/DNN architecture that solves\nall problems optimally. Instead, the \"right\" CNN/DNN architecture varies\ndepending on the application at hand. CNN/DNNs comprise an enormous design\nspace. Quantitatively, we find that a small region of the CNN design space\ncontains 30 billion different CNN architectures.\n  In this dissertation, we develop a methodology that enables systematic\nexploration of the design space of CNNs. Our methodology is comprised of the\nfollowing four themes.\n  1. Judiciously choosing benchmarks and metrics.\n  2. Rapidly training CNN models.\n  3. Defining and describing the CNN design space.\n  4. Exploring the design space of CNN architectures.\n  Taken together, these four themes comprise an effective methodology for\ndiscovering the \"right\" CNN architectures to meet the needs of practical\napplications.\n", "versions": [{"version": "v1", "created": "Tue, 20 Dec 2016 06:20:43 GMT"}], "update_date": "2016-12-21", "authors_parsed": [["Iandola", "Forrest", ""]]}, {"id": "1612.06524", "submitter": "Ching-Hang Chen", "authors": "Ching-Hang Chen, Deva Ramanan", "title": "3D Human Pose Estimation = 2D Pose Estimation + Matching", "comments": "Demo code: https://github.com/flyawaychase/3DHumanPose", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We explore 3D human pose estimation from a single RGB image. While many\napproaches try to directly predict 3D pose from image measurements, we explore\na simple architecture that reasons through intermediate 2D pose predictions.\nOur approach is based on two key observations (1) Deep neural nets have\nrevolutionized 2D pose estimation, producing accurate 2D predictions even for\nposes with self occlusions. (2) Big-data sets of 3D mocap data are now readily\navailable, making it tempting to lift predicted 2D poses to 3D through simple\nmemorization (e.g., nearest neighbors). The resulting architecture is trivial\nto implement with off-the-shelf 2D pose estimation systems and 3D mocap\nlibraries. Importantly, we demonstrate that such methods outperform almost all\nstate-of-the-art 3D pose estimation systems, most of which directly try to\nregress 3D pose from 2D measurements.\n", "versions": [{"version": "v1", "created": "Tue, 20 Dec 2016 06:45:49 GMT"}, {"version": "v2", "created": "Tue, 11 Apr 2017 07:33:51 GMT"}], "update_date": "2017-04-12", "authors_parsed": [["Chen", "Ching-Hang", ""], ["Ramanan", "Deva", ""]]}, {"id": "1612.06530", "submitter": "Shaodi You", "authors": "Shijie Zhang, Lizhen Qu, Shaodi You, Zhenglu Yang, Jiawan Zhang", "title": "Automatic Generation of Grounded Visual Questions", "comments": "VQA", "journal-ref": "IJCAI 2017", "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose the first model to be able to generate visually\ngrounded questions with diverse types for a single image. Visual question\ngeneration is an emerging topic which aims to ask questions in natural language\nbased on visual input. To the best of our knowledge, it lacks automatic methods\nto generate meaningful questions with various types for the same visual input.\nTo circumvent the problem, we propose a model that automatically generates\nvisually grounded questions with varying types. Our model takes as input both\nimages and the captions generated by a dense caption model, samples the most\nprobable question types, and generates the questions in sequel. The\nexperimental results on two real world datasets show that our model outperforms\nthe strongest baseline in terms of both correctness and diversity with a wide\nmargin.\n", "versions": [{"version": "v1", "created": "Tue, 20 Dec 2016 07:20:16 GMT"}, {"version": "v2", "created": "Mon, 29 May 2017 12:54:35 GMT"}], "update_date": "2017-05-30", "authors_parsed": [["Zhang", "Shijie", ""], ["Qu", "Lizhen", ""], ["You", "Shaodi", ""], ["Yang", "Zhenglu", ""], ["Zhang", "Jiawan", ""]]}, {"id": "1612.06543", "submitter": "Niki Martinel", "authors": "Niki Martinel, Gian Luca Foresti and Christian Micheloni", "title": "Wide-Slice Residual Networks for Food Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Food diary applications represent a tantalizing market. Such applications,\nbased on image food recognition, opened to new challenges for computer vision\nand pattern recognition algorithms. Recent works in the field are focusing\neither on hand-crafted representations or on learning these by exploiting deep\nneural networks. Despite the success of such a last family of works, these\ngenerally exploit off-the shelf deep architectures to classify food dishes.\nThus, the architectures are not cast to the specific problem. We believe that\nbetter results can be obtained if the deep architecture is defined with respect\nto an analysis of the food composition. Following such an intuition, this work\nintroduces a new deep scheme that is designed to handle the food structure.\nSpecifically, inspired by the recent success of residual deep network, we\nexploit such a learning scheme and introduce a slice convolution block to\ncapture the vertical food layers. Outputs of the deep residual blocks are\ncombined with the sliced convolution to produce the classification score for\nspecific food categories. To evaluate our proposed architecture we have\nconducted experimental results on three benchmark datasets. Results demonstrate\nthat our solution shows better performance with respect to existing approaches\n(e.g., a top-1 accuracy of 90.27% on the Food-101 challenging dataset).\n", "versions": [{"version": "v1", "created": "Tue, 20 Dec 2016 08:19:52 GMT"}], "update_date": "2016-12-21", "authors_parsed": [["Martinel", "Niki", ""], ["Foresti", "Gian Luca", ""], ["Micheloni", "Christian", ""]]}, {"id": "1612.06558", "submitter": "Heechul Jung", "authors": "Heechul Jung, Min-Kook Choi, Kwon Soon, Woo Young Jung", "title": "End-to-End Pedestrian Collision Warning System based on a Convolutional\n  Neural Network with Semantic Segmentation", "comments": "6 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditional pedestrian collision warning systems sometimes raise alarms even\nwhen there is no danger (e.g., when all pedestrians are walking on the\nsidewalk). These false alarms can make it difficult for drivers to concentrate\non their driving. In this paper, we propose a novel framework for an end-to-end\npedestrian collision warning system based on a convolutional neural network.\nSemantic segmentation information is used to train the convolutional neural\nnetwork and two loss functions, such as cross entropy and Euclidean losses, are\nminimized. Finally, we demonstrate the effectiveness of our method in reducing\nfalse alarms and increasing warning accuracy compared to a traditional\nhistogram of oriented gradients (HoG)-based system.\n", "versions": [{"version": "v1", "created": "Tue, 20 Dec 2016 09:10:30 GMT"}], "update_date": "2016-12-21", "authors_parsed": [["Jung", "Heechul", ""], ["Choi", "Min-Kook", ""], ["Soon", "Kwon", ""], ["Jung", "Woo Young", ""]]}, {"id": "1612.06573", "submitter": "Sebastian Ramos", "authors": "Sebastian Ramos, Stefan Gehrig, Peter Pinggera, Uwe Franke, Carsten\n  Rother", "title": "Detecting Unexpected Obstacles for Self-Driving Cars: Fusing Deep\n  Learning and Geometric Modeling", "comments": "Submitted to the IEEE International Conference on Robotics and\n  Automation (ICRA) 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The detection of small road hazards, such as lost cargo, is a vital\ncapability for self-driving cars. We tackle this challenging and rarely\naddressed problem with a vision system that leverages appearance, contextual as\nwell as geometric cues. To utilize the appearance and contextual cues, we\npropose a new deep learning-based obstacle detection framework. Here a variant\nof a fully convolutional network is used to predict a pixel-wise semantic\nlabeling of (i) free-space, (ii) on-road unexpected obstacles, and (iii)\nbackground. The geometric cues are exploited using a state-of-the-art detection\napproach that predicts obstacles from stereo input images via model-based\nstatistical hypothesis tests. We present a principled Bayesian framework to\nfuse the semantic and stereo-based detection results. The mid-level Stixel\nrepresentation is used to describe obstacles in a flexible, compact and robust\nmanner. We evaluate our new obstacle detection system on the Lost and Found\ndataset, which includes very challenging scenes with obstacles of only 5 cm\nheight. Overall, we report a major improvement over the state-of-the-art, with\nrelative performance gains of up to 50%. In particular, we achieve a detection\nrate of over 90% for distances of up to 50 m. Our system operates at 22 Hz on\nour self-driving platform.\n", "versions": [{"version": "v1", "created": "Tue, 20 Dec 2016 09:55:00 GMT"}], "update_date": "2016-12-21", "authors_parsed": [["Ramos", "Sebastian", ""], ["Gehrig", "Stefan", ""], ["Pinggera", "Peter", ""], ["Franke", "Uwe", ""], ["Rother", "Carsten", ""]]}, {"id": "1612.06615", "submitter": "Martin Danelljan", "authors": "Susanna Gladh, Martin Danelljan, Fahad Shahbaz Khan, Michael Felsberg", "title": "Deep Motion Features for Visual Tracking", "comments": "ICPR 2016. Best paper award in the \"Computer Vision and Robot Vision\"\n  track", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Robust visual tracking is a challenging computer vision problem, with many\nreal-world applications. Most existing approaches employ hand-crafted\nappearance features, such as HOG or Color Names. Recently, deep RGB features\nextracted from convolutional neural networks have been successfully applied for\ntracking. Despite their success, these features only capture appearance\ninformation. On the other hand, motion cues provide discriminative and\ncomplementary information that can improve tracking performance. Contrary to\nvisual tracking, deep motion features have been successfully applied for action\nrecognition and video classification tasks. Typically, the motion features are\nlearned by training a CNN on optical flow images extracted from large amounts\nof labeled videos.\n  This paper presents an investigation of the impact of deep motion features in\na tracking-by-detection framework. We further show that hand-crafted, deep RGB,\nand deep motion features contain complementary information. To the best of our\nknowledge, we are the first to propose fusing appearance information with deep\nmotion features for visual tracking. Comprehensive experiments clearly suggest\nthat our fusion approach with deep motion features outperforms standard methods\nrelying on appearance information alone.\n", "versions": [{"version": "v1", "created": "Tue, 20 Dec 2016 11:33:31 GMT"}], "update_date": "2016-12-21", "authors_parsed": [["Gladh", "Susanna", ""], ["Danelljan", "Martin", ""], ["Khan", "Fahad Shahbaz", ""], ["Felsberg", "Michael", ""]]}, {"id": "1612.06699", "submitter": "Pierre Sermanet", "authors": "Pierre Sermanet, Kelvin Xu, Sergey Levine", "title": "Unsupervised Perceptual Rewards for Imitation Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reward function design and exploration time are arguably the biggest\nobstacles to the deployment of reinforcement learning (RL) agents in the real\nworld. In many real-world tasks, designing a reward function takes considerable\nhand engineering and often requires additional sensors to be installed just to\nmeasure whether the task has been executed successfully. Furthermore, many\ninteresting tasks consist of multiple implicit intermediate steps that must be\nexecuted in sequence. Even when the final outcome can be measured, it does not\nnecessarily provide feedback on these intermediate steps. To address these\nissues, we propose leveraging the abstraction power of intermediate visual\nrepresentations learned by deep models to quickly infer perceptual reward\nfunctions from small numbers of demonstrations. We present a method that is\nable to identify key intermediate steps of a task from only a handful of\ndemonstration sequences, and automatically identify the most discriminative\nfeatures for identifying these steps. This method makes use of the features in\na pre-trained deep model, but does not require any explicit specification of\nsub-goals. The resulting reward functions can then be used by an RL agent to\nlearn to perform the task in real-world settings. To evaluate the learned\nreward, we present qualitative results on two real-world tasks and a\nquantitative evaluation against a human-designed reward function. We also show\nthat our method can be used to learn a real-world door opening skill using a\nreal robot, even when the demonstration used for reward learning is provided by\na human using their own hand. To our knowledge, these are the first results\nshowing that complex robotic manipulation skills can be learned directly and\nwithout supervised labels from a video of a human performing the task.\nSupplementary material and data are available at\nhttps://sermanet.github.io/rewards\n", "versions": [{"version": "v1", "created": "Tue, 20 Dec 2016 15:04:38 GMT"}, {"version": "v2", "created": "Mon, 26 Dec 2016 13:47:34 GMT"}, {"version": "v3", "created": "Mon, 12 Jun 2017 21:38:17 GMT"}], "update_date": "2017-06-14", "authors_parsed": [["Sermanet", "Pierre", ""], ["Xu", "Kelvin", ""], ["Levine", "Sergey", ""]]}, {"id": "1612.06703", "submitter": "Harish Karunakaran", "authors": "Adhavan Jayabalan, Harish Karunakaran, Shravan Murlidharan, Tesia\n  Shizume", "title": "Dynamic Action Recognition: A convolutional neural network model for\n  temporally organized joint location data", "comments": "11 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivation: Recognizing human actions in a video is a challenging task which\nhas applications in various fields. Previous works in this area have either\nused images from a 2D or 3D camera. Few have used the idea that human actions\ncan be easily identified by the movement of the joints in the 3D space and\ninstead used a Recurrent Neural Network (RNN) for modeling. Convolutional\nneural networks (CNN) have the ability to recognise even the complex patterns\nin data which makes it suitable for detecting human actions. Thus, we modeled a\nCNN which can predict the human activity using the joint data. Furthermore,\nusing the joint data representation has the benefit of lower dimensionality\nthan image or video representations. This makes our model simpler and faster\nthan the RNN models. In this study, we have developed a six layer convolutional\nnetwork, which reduces each input feature vector of the form 15x1961x4 to an\none dimensional binary vector which gives us the predicted activity. Results:\nOur model is able to recognise an activity correctly upto 87% accuracy. Joint\ndata is taken from the Cornell Activity Datasets which have day to day\nactivities like talking, relaxing, eating, cooking etc.\n", "versions": [{"version": "v1", "created": "Tue, 20 Dec 2016 15:20:28 GMT"}], "update_date": "2016-12-21", "authors_parsed": [["Jayabalan", "Adhavan", ""], ["Karunakaran", "Harish", ""], ["Murlidharan", "Shravan", ""], ["Shizume", "Tesia", ""]]}, {"id": "1612.06704", "submitter": "Donggeun Yoo", "authors": "Donggeun Yoo, Sunggyun Park, Kyunghyun Paeng, Joon-Young Lee, In So\n  Kweon", "title": "Action-Driven Object Detection with Top-Down Visual Attentions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A dominant paradigm for deep learning based object detection relies on a\n\"bottom-up\" approach using \"passive\" scoring of class agnostic proposals. These\napproaches are efficient but lack of holistic analysis of scene-level context.\nIn this paper, we present an \"action-driven\" detection mechanism using our\n\"top-down\" visual attention model. We localize an object by taking sequential\nactions that the attention model provides. The attention model conditioned with\nan image region provides required actions to get closer toward a target object.\nAn action at each time step is weak itself but an ensemble of the sequential\nactions makes a bounding-box accurately converge to a target object boundary.\nThis attention model we call AttentionNet is composed of a convolutional neural\nnetwork. During our whole detection procedure, we only utilize the actions from\na single AttentionNet without any modules for object proposals nor post\nbounding-box regression. We evaluate our top-down detection mechanism over the\nPASCAL VOC series and ILSVRC CLS-LOC dataset, and achieve state-of-the-art\nperformances compared to the major bottom-up detection methods. In particular,\nour detection mechanism shows a strong advantage in elaborate localization by\noutperforming Faster R-CNN with a margin of +7.1% over PASCAL VOC 2007 when we\nincrease the IoU threshold for positive detection to 0.7.\n", "versions": [{"version": "v1", "created": "Tue, 20 Dec 2016 15:24:46 GMT"}], "update_date": "2016-12-21", "authors_parsed": [["Yoo", "Donggeun", ""], ["Park", "Sunggyun", ""], ["Paeng", "Kyunghyun", ""], ["Lee", "Joon-Young", ""], ["Kweon", "In So", ""]]}, {"id": "1612.06738", "submitter": "Sujit Kumar Sahoo Ph.D.", "authors": "Sujit Kumar Sahoo", "title": "Local Sparse Approximation for Image Restoration with Adaptive Block\n  Size Selection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper the problem of image restoration (denoising and inpainting) is\napproached using sparse approximation of local image blocks. The local image\nblocks are extracted by sliding square windows over the image. An adaptive\nblock size selection procedure for local sparse approximation is proposed,\nwhich affects the global recovery of underlying image. Ideally the adaptive\nlocal block selection yields the minimum mean square error (MMSE) in recovered\nimage. This framework gives us a clustered image based on the selected block\nsize, then each cluster is restored separately using sparse approximation. The\nresults obtained using the proposed framework are very much comparable with the\nrecently proposed image restoration techniques.\n", "versions": [{"version": "v1", "created": "Tue, 20 Dec 2016 16:28:48 GMT"}], "update_date": "2016-12-21", "authors_parsed": [["Sahoo", "Sujit Kumar", ""]]}, {"id": "1612.06795", "submitter": "Matti Pietik\\\"ainen", "authors": "Matti Pietik\\\"ainen and Guoying Zhao", "title": "Two decades of local binary patterns: A survey", "comments": "In Advances in Independent Component Analysis and Learning Machines,\n  Academic Press, 2015, Pages 175-210", "journal-ref": null, "doi": "10.1016/B978-0-12-802806-3.00009-9", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Texture is an important characteristic for many types of images. In recent\nyears very discriminative and computationally efficient local texture\ndescriptors based on local binary patterns (LBP) have been developed, which has\nled to significant progress in applying texture methods to different problems\nand applications. Due to this progress, the division between texture\ndescriptors and more generic image or video descriptors has been disappearing.\nA large number of different variants of LBP have been developed to improve its\nrobustness, and to increase its discriminative power and applicability to\ndifferent types of problems. In this chapter, the most recent and important\nvariants of LBP in 2D, spatiotemporal, 3D, and 4D domains are surveyed.\nInteresting new developments of LBP in 1D signal analysis are also considered.\nFinally, some future challenges for research are presented.\n", "versions": [{"version": "v1", "created": "Tue, 20 Dec 2016 18:33:28 GMT"}, {"version": "v2", "created": "Mon, 16 Jan 2017 02:49:16 GMT"}], "update_date": "2017-01-17", "authors_parsed": [["Pietik\u00e4inen", "Matti", ""], ["Zhao", "Guoying", ""]]}, {"id": "1612.06825", "submitter": "Le Hou", "authors": "Veda Murthy, Le Hou, Dimitris Samaras, Tahsin M. Kurc, Joel H. Saltz", "title": "Center-Focusing Multi-task CNN with Injected Features for Classification\n  of Glioma Nuclear Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Classifying the various shapes and attributes of a glioma cell nucleus is\ncrucial for diagnosis and understanding the disease. We investigate automated\nclassification of glioma nuclear shapes and visual attributes using\nConvolutional Neural Networks (CNNs) on pathology images of automatically\nsegmented nuclei. We propose three methods that improve the performance of a\npreviously-developed semi-supervised CNN. First, we propose a method that\nallows the CNN to focus on the most important part of an image- the image's\ncenter containing the nucleus. Second, we inject (concatenate) pre-extracted\nVGG features into an intermediate layer of our Semi-Supervised CNN so that\nduring training, the CNN can learn a set of complementary features. Third, we\nseparate the losses of the two groups of target classes (nuclear shapes and\nattributes) into a single-label loss and a multi-label loss so that the prior\nknowledge of inter-label exclusiveness can be incorporated. On a dataset of\n2078 images, the proposed methods combined reduce the error rate of attribute\nand shape classification by 21.54% and 15.07% respectively compared to the\nexisting state-of-the-art method on the same dataset.\n", "versions": [{"version": "v1", "created": "Tue, 20 Dec 2016 19:54:37 GMT"}, {"version": "v2", "created": "Tue, 10 Jan 2017 18:44:32 GMT"}], "update_date": "2017-01-11", "authors_parsed": [["Murthy", "Veda", ""], ["Hou", "Le", ""], ["Samaras", "Dimitris", ""], ["Kurc", "Tahsin M.", ""], ["Saltz", "Joel H.", ""]]}, {"id": "1612.06836", "submitter": "David Fouhey", "authors": "David F. Fouhey, Abhinav Gupta, Andrew Zisserman", "title": "From Images to 3D Shape Attributes", "comments": "Updated based on TPAMI reviews: title changed, sections reordered,\n  moderate modifications throughout text", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Our goal in this paper is to investigate properties of 3D shape that can be\ndetermined from a single image. We define 3D shape attributes -- generic\nproperties of the shape that capture curvature, contact and occupied space. Our\nfirst objective is to infer these 3D shape attributes from a single image. A\nsecond objective is to infer a 3D shape embedding -- a low dimensional vector\nrepresenting the 3D shape.\n  We study how the 3D shape attributes and embedding can be obtained from a\nsingle image by training a Convolutional Neural Network (CNN) for this task. We\nstart with synthetic images so that the contribution of various cues and\nnuisance parameters can be controlled. We then turn to real images and\nintroduce a large scale image dataset of sculptures containing 143K images\ncovering 2197 works from 242 artists.\n  For the CNN trained on the sculpture dataset we show the following: (i) which\nregions of the imaged sculpture are used by the CNN to infer the 3D shape\nattributes; (ii) that the shape embedding can be used to match previously\nunseen sculptures largely independent of viewpoint; and (iii) that the 3D\nattributes generalize to images of other (non-sculpture) object classes.\n", "versions": [{"version": "v1", "created": "Tue, 20 Dec 2016 20:24:57 GMT"}, {"version": "v2", "created": "Sun, 3 Dec 2017 22:58:22 GMT"}], "update_date": "2017-12-05", "authors_parsed": [["Fouhey", "David F.", ""], ["Gupta", "Abhinav", ""], ["Zisserman", "Andrew", ""]]}, {"id": "1612.06851", "submitter": "Abhinav Shrivastava", "authors": "Abhinav Shrivastava, Rahul Sukthankar, Jitendra Malik, Abhinav Gupta", "title": "Beyond Skip Connections: Top-Down Modulation for Object Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, we have seen tremendous progress in the field of object\ndetection. Most of the recent improvements have been achieved by targeting\ndeeper feedforward networks. However, many hard object categories such as\nbottle, remote, etc. require representation of fine details and not just\ncoarse, semantic representations. But most of these fine details are lost in\nthe early convolutional layers. What we need is a way to incorporate finer\ndetails from lower layers into the detection architecture. Skip connections\nhave been proposed to combine high-level and low-level features, but we argue\nthat selecting the right features from low-level requires top-down contextual\ninformation. Inspired by the human visual pathway, in this paper we propose\ntop-down modulations as a way to incorporate fine details into the detection\nframework. Our approach supplements the standard bottom-up, feedforward ConvNet\nwith a top-down modulation (TDM) network, connected using lateral connections.\nThese connections are responsible for the modulation of lower layer filters,\nand the top-down network handles the selection and integration of contextual\ninformation and low-level features. The proposed TDM architecture provides a\nsignificant boost on the COCO testdev benchmark, achieving 28.6 AP for VGG16,\n35.2 AP for ResNet101, and 37.3 for InceptionResNetv2 network, without any\nbells and whistles (e.g., multi-scale, iterative box refinement, etc.).\n", "versions": [{"version": "v1", "created": "Tue, 20 Dec 2016 20:57:59 GMT"}, {"version": "v2", "created": "Tue, 19 Sep 2017 22:37:40 GMT"}], "update_date": "2017-09-21", "authors_parsed": [["Shrivastava", "Abhinav", ""], ["Sukthankar", "Rahul", ""], ["Malik", "Jitendra", ""], ["Gupta", "Abhinav", ""]]}, {"id": "1612.06890", "submitter": "Justin Johnson", "authors": "Justin Johnson and Bharath Hariharan and Laurens van der Maaten and Li\n  Fei-Fei and C. Lawrence Zitnick and Ross Girshick", "title": "CLEVR: A Diagnostic Dataset for Compositional Language and Elementary\n  Visual Reasoning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When building artificial intelligence systems that can reason and answer\nquestions about visual data, we need diagnostic tests to analyze our progress\nand discover shortcomings. Existing benchmarks for visual question answering\ncan help, but have strong biases that models can exploit to correctly answer\nquestions without reasoning. They also conflate multiple sources of error,\nmaking it hard to pinpoint model weaknesses. We present a diagnostic dataset\nthat tests a range of visual reasoning abilities. It contains minimal biases\nand has detailed annotations describing the kind of reasoning each question\nrequires. We use this dataset to analyze a variety of modern visual reasoning\nsystems, providing novel insights into their abilities and limitations.\n", "versions": [{"version": "v1", "created": "Tue, 20 Dec 2016 21:40:40 GMT"}], "update_date": "2016-12-22", "authors_parsed": [["Johnson", "Justin", ""], ["Hariharan", "Bharath", ""], ["van der Maaten", "Laurens", ""], ["Fei-Fei", "Li", ""], ["Zitnick", "C. Lawrence", ""], ["Girshick", "Ross", ""]]}, {"id": "1612.06919", "submitter": "Subarna Tripathi", "authors": "Subarna Tripathi and Brian Guenter", "title": "A Statistical Approach to Continuous Self-Calibrating Eye Gaze Tracking\n  for Head-Mounted Virtual Reality Systems", "comments": "Accepted for publication in WACV 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel, automatic eye gaze tracking scheme inspired by smooth\npursuit eye motion while playing mobile games or watching virtual reality\ncontents. Our algorithm continuously calibrates an eye tracking system for a\nhead mounted display. This eliminates the need for an explicit calibration step\nand automatically compensates for small movements of the headset with respect\nto the head. The algorithm finds correspondences between corneal motion and\nscreen space motion, and uses these to generate Gaussian Process Regression\nmodels. A combination of those models provides a continuous mapping from\ncorneal position to screen space position. Accuracy is nearly as good as\nachieved with an explicit calibration step.\n", "versions": [{"version": "v1", "created": "Tue, 20 Dec 2016 23:25:27 GMT"}], "update_date": "2016-12-22", "authors_parsed": [["Tripathi", "Subarna", ""], ["Guenter", "Brian", ""]]}, {"id": "1612.06933", "submitter": "Kanji Tanaka", "authors": "Fei Xiaoxiao, Tanaka Kanji, Inamoto Kouya", "title": "Unsupervised Place Discovery for Visual Place Classification", "comments": "Technical Report, 5 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this study, we explore the use of deep convolutional neural networks\n(DCNNs) in visual place classification for robotic mapping and localization. An\nopen question is how to partition the robot's workspace into places to maximize\nthe performance (e.g., accuracy, precision, recall) of potential DCNN\nclassifiers. This is a chicken and egg problem: If we had a well-trained DCNN\nclassifier, it is rather easy to partition the robot's workspace into places,\nbut the training of a DCNN classifier requires a set of pre-defined place\nclasses. In this study, we address this problem and present several strategies\nfor unsupervised discovery of place classes (\"time cue,\" \"location cue,\"\n\"time-appearance cue,\" and \"location-appearance cue\"). We also evaluate the\nefficacy of the proposed methods using the publicly available University of\nMichigan North Campus Long-Term (NCLT) Dataset.\n", "versions": [{"version": "v1", "created": "Wed, 21 Dec 2016 00:53:18 GMT"}], "update_date": "2016-12-22", "authors_parsed": [["Xiaoxiao", "Fei", ""], ["Kanji", "Tanaka", ""], ["Kouya", "Inamoto", ""]]}, {"id": "1612.06950", "submitter": "Gil Levi", "authors": "Dotan Kaufman, Gil Levi, Tal Hassner and Lior Wolf", "title": "Temporal Tessellation: A Unified Approach for Video Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a general approach to video understanding, inspired by semantic\ntransfer techniques that have been successfully used for 2D image analysis. Our\nmethod considers a video to be a 1D sequence of clips, each one associated with\nits own semantics. The nature of these semantics -- natural language captions\nor other labels -- depends on the task at hand. A test video is processed by\nforming correspondences between its clips and the clips of reference videos\nwith known semantics, following which, reference semantics can be transferred\nto the test video. We describe two matching methods, both designed to ensure\nthat (a) reference clips appear similar to test clips and (b), taken together,\nthe semantics of the selected reference clips is consistent and maintains\ntemporal coherence. We use our method for video captioning on the LSMDC'16\nbenchmark, video summarization on the SumMe and TVSum benchmarks, Temporal\nAction Detection on the Thumos2014 benchmark, and sound prediction on the\nGreatest Hits benchmark. Our method not only surpasses the state of the art, in\nfour out of five benchmarks, but importantly, it is the only single method we\nknow of that was successfully applied to such a diverse range of tasks.\n", "versions": [{"version": "v1", "created": "Wed, 21 Dec 2016 02:29:53 GMT"}, {"version": "v2", "created": "Fri, 14 Apr 2017 19:20:10 GMT"}], "update_date": "2017-04-18", "authors_parsed": [["Kaufman", "Dotan", ""], ["Levi", "Gil", ""], ["Hassner", "Tal", ""], ["Wolf", "Lior", ""]]}, {"id": "1612.07003", "submitter": "Alex Zwanenburg", "authors": "Alex Zwanenburg, Stefan Leger, Martin Valli\\`eres and Steffen L\\\"ock\n  (for the Image Biomarker Standardisation Initiative)", "title": "Image biomarker standardisation initiative", "comments": "Added figures 2.5, 2.6. Replaced figure 2.7. Added missing section\n  header for the normalised dependence count non-uniformity feature. Fixed\n  layout issues with small font sizes that appeared in the last half of the\n  document", "journal-ref": "Radiology (2020)", "doi": "10.1148/radiol.2020191145", "report-no": null, "categories": "cs.CV eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The image biomarker standardisation initiative (IBSI) is an independent\ninternational collaboration which works towards standardising the extraction of\nimage biomarkers from acquired imaging for the purpose of high-throughput\nquantitative image analysis (radiomics). Lack of reproducibility and validation\nof high-throughput quantitative image analysis studies is considered to be a\nmajor challenge for the field. Part of this challenge lies in the scantiness of\nconsensus-based guidelines and definitions for the process of translating\nacquired imaging into high-throughput image biomarkers. The IBSI therefore\nseeks to provide image biomarker nomenclature and definitions, benchmark data\nsets, and benchmark values to verify image processing and image biomarker\ncalculations, as well as reporting guidelines, for high-throughput image\nanalysis.\n", "versions": [{"version": "v1", "created": "Wed, 21 Dec 2016 08:04:31 GMT"}, {"version": "v10", "created": "Wed, 23 Oct 2019 07:41:50 GMT"}, {"version": "v11", "created": "Tue, 17 Dec 2019 11:04:08 GMT"}, {"version": "v2", "created": "Tue, 31 Jan 2017 14:40:34 GMT"}, {"version": "v3", "created": "Thu, 9 Mar 2017 11:13:59 GMT"}, {"version": "v4", "created": "Wed, 19 Jul 2017 10:16:25 GMT"}, {"version": "v5", "created": "Thu, 16 Nov 2017 09:17:29 GMT"}, {"version": "v6", "created": "Tue, 17 Apr 2018 10:56:57 GMT"}, {"version": "v7", "created": "Mon, 17 Sep 2018 11:52:37 GMT"}, {"version": "v8", "created": "Thu, 28 Feb 2019 14:01:26 GMT"}, {"version": "v9", "created": "Thu, 16 May 2019 13:20:51 GMT"}], "update_date": "2020-03-11", "authors_parsed": [["Zwanenburg", "Alex", "", "for the Image Biomarker Standardisation Initiative"], ["Leger", "Stefan", "", "for the Image Biomarker Standardisation Initiative"], ["Valli\u00e8res", "Martin", "", "for the Image Biomarker Standardisation Initiative"], ["L\u00f6ck", "Steffen", "", "for the Image Biomarker Standardisation Initiative"]]}, {"id": "1612.07086", "submitter": "Jiuxiang Gu Mr", "authors": "Jiuxiang Gu, Gang Wang, Jianfei Cai, Tsuhan Chen", "title": "An Empirical Study of Language CNN for Image Captioning", "comments": "Comments: 10 pages, In proceedings of ICCV 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Language Models based on recurrent neural networks have dominated recent\nimage caption generation tasks. In this paper, we introduce a Language CNN\nmodel which is suitable for statistical language modeling tasks and shows\ncompetitive performance in image captioning. In contrast to previous models\nwhich predict next word based on one previous word and hidden state, our\nlanguage CNN is fed with all the previous words and can model the long-range\ndependencies of history words, which are critical for image captioning. The\neffectiveness of our approach is validated on two datasets MS COCO and\nFlickr30K. Our extensive experimental results show that our method outperforms\nthe vanilla recurrent neural network based language models and is competitive\nwith the state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Wed, 21 Dec 2016 13:04:18 GMT"}, {"version": "v2", "created": "Sat, 1 Apr 2017 07:02:31 GMT"}, {"version": "v3", "created": "Wed, 2 Aug 2017 12:33:50 GMT"}], "update_date": "2017-08-03", "authors_parsed": [["Gu", "Jiuxiang", ""], ["Wang", "Gang", ""], ["Cai", "Jianfei", ""], ["Chen", "Tsuhan", ""]]}, {"id": "1612.07089", "submitter": "Sandeep Kumar", "authors": "Ketan Rajawat and Sandeep Kumar", "title": "Stochastic Multidimensional Scaling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multidimensional scaling (MDS) is a popular dimensionality reduction\ntechniques that has been widely used for network visualization and cooperative\nlocalization. However, the traditional stress minimization formulation of MDS\nnecessitates the use of batch optimization algorithms that are not scalable to\nlarge-sized problems. This paper considers an alternative stochastic stress\nminimization framework that is amenable to incremental and distributed\nsolutions. A novel linear-complexity stochastic optimization algorithm is\nproposed that is provably convergent and simple to implement. The applicability\nof the proposed algorithm to localization and visualization tasks is also\nexpounded. Extensive tests on synthetic and real datasets demonstrate the\nefficacy of the proposed algorithm.\n", "versions": [{"version": "v1", "created": "Wed, 21 Dec 2016 13:08:35 GMT"}], "update_date": "2016-12-22", "authors_parsed": [["Rajawat", "Ketan", ""], ["Kumar", "Sandeep", ""]]}, {"id": "1612.07119", "submitter": "Yaman Umuroglu", "authors": "Yaman Umuroglu, Nicholas J. Fraser, Giulio Gambardella, Michaela\n  Blott, Philip Leong, Magnus Jahre, Kees Vissers", "title": "FINN: A Framework for Fast, Scalable Binarized Neural Network Inference", "comments": "To appear in the 25th International Symposium on Field-Programmable\n  Gate Arrays, February 2017", "journal-ref": null, "doi": "10.1145/3020078.3021744", "report-no": null, "categories": "cs.CV cs.AR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Research has shown that convolutional neural networks contain significant\nredundancy, and high classification accuracy can be obtained even when weights\nand activations are reduced from floating point to binary values. In this\npaper, we present FINN, a framework for building fast and flexible FPGA\naccelerators using a flexible heterogeneous streaming architecture. By\nutilizing a novel set of optimizations that enable efficient mapping of\nbinarized neural networks to hardware, we implement fully connected,\nconvolutional and pooling layers, with per-layer compute resources being\ntailored to user-provided throughput requirements. On a ZC706 embedded FPGA\nplatform drawing less than 25 W total system power, we demonstrate up to 12.3\nmillion image classifications per second with 0.31 {\\mu}s latency on the MNIST\ndataset with 95.8% accuracy, and 21906 image classifications per second with\n283 {\\mu}s latency on the CIFAR-10 and SVHN datasets with respectively 80.1%\nand 94.9% accuracy. To the best of our knowledge, ours are the fastest\nclassification rates reported to date on these benchmarks.\n", "versions": [{"version": "v1", "created": "Thu, 1 Dec 2016 22:19:47 GMT"}], "update_date": "2016-12-22", "authors_parsed": [["Umuroglu", "Yaman", ""], ["Fraser", "Nicholas J.", ""], ["Gambardella", "Giulio", ""], ["Blott", "Michaela", ""], ["Leong", "Philip", ""], ["Jahre", "Magnus", ""], ["Vissers", "Kees", ""]]}, {"id": "1612.07120", "submitter": "Bin Bai Dr.", "authors": "Bin Bai, Jianbin Liu, Yu Zhou, Songlin Zhang, Yuchen He, Zhuo Xu", "title": "Imaging around corners with single-pixel detector by computational ghost\n  imaging", "comments": null, "journal-ref": null, "doi": "10.1016/j.ijleo.2017.08.057", "report-no": null, "categories": "cs.CV physics.optics", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We have designed a single-pixel camera with imaging around corners based on\ncomputational ghost imaging. It can obtain the image of an object when the\ncamera cannot look at the object directly. Our imaging system explores the fact\nthat a bucket detector in a ghost imaging setup has no spatial resolution\ncapability. A series of experiments have been designed to confirm our\npredictions. This camera has potential applications for imaging around corner\nor other similar environments where the object cannot be observed directly.\n", "versions": [{"version": "v1", "created": "Thu, 8 Dec 2016 09:54:20 GMT"}], "update_date": "2017-10-11", "authors_parsed": [["Bai", "Bin", ""], ["Liu", "Jianbin", ""], ["Zhou", "Yu", ""], ["Zhang", "Songlin", ""], ["He", "Yuchen", ""], ["Xu", "Zhuo", ""]]}, {"id": "1612.07153", "submitter": "Kun Sun", "authors": "Kun Sun and Wenbing Tao", "title": "Trilaminar Multiway Reconstruction Tree for Efficient Large Scale\n  Structure from Motion", "comments": "this manuscript has been submitted to cvpr 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accuracy and efficiency are two key problems in large scale incremental\nStructure from Motion (SfM). In this paper, we propose a unified framework to\ndivide the image set into clusters suitable for reconstruction as well as find\nmultiple reliable and stable starting points. Image partitioning performs in\ntwo steps. First, some small image groups are selected at places with high\nimage density, and then all the images are clustered according to their optimal\nreconstruction paths to these image groups. This promises that the scene is\nalways reconstructed from dense places to sparse areas, which can reduce error\naccumulation when images have weak overlap. To enable faster speed, images\noutside the selected group in each cluster are further divided to achieve a\ngreater degree of parallelism. Experiments show that our method achieves\nsignificant speedup, higher accuracy and better completeness.\n", "versions": [{"version": "v1", "created": "Wed, 21 Dec 2016 14:50:36 GMT"}], "update_date": "2016-12-22", "authors_parsed": [["Sun", "Kun", ""], ["Tao", "Wenbing", ""]]}, {"id": "1612.07180", "submitter": "Kyunghyun Paeng", "authors": "Kyunghyun Paeng, Sangheum Hwang, Sunggyun Park, and Minsoo Kim", "title": "A Unified Framework for Tumor Proliferation Score Prediction in Breast\n  Histopathology", "comments": "Accepted to the 3rd Workshop on Deep Learning in Medical Image\n  Analysis (DLMIA 2017), MICCAI 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a unified framework to predict tumor proliferation scores from\nbreast histopathology whole slide images. Our system offers a fully automated\nsolution to predicting both a molecular data-based, and a mitosis\ncounting-based tumor proliferation score. The framework integrates three\nmodules, each fine-tuned to maximize the overall performance: An image\nprocessing component for handling whole slide images, a deep learning based\nmitosis detection network, and a proliferation scores prediction module. We\nhave achieved 0.567 quadratic weighted Cohen's kappa in mitosis counting-based\nscore prediction and 0.652 F1-score in mitosis detection. On Spearman's\ncorrelation coefficient, which evaluates predictive accuracy on the molecular\ndata based score, the system obtained 0.6171. Our approach won first place in\nall of the three tasks in Tumor Proliferation Assessment Challenge 2016 which\nis MICCAI grand challenge.\n", "versions": [{"version": "v1", "created": "Wed, 21 Dec 2016 15:24:34 GMT"}, {"version": "v2", "created": "Fri, 11 Aug 2017 06:59:20 GMT"}], "update_date": "2017-08-14", "authors_parsed": [["Paeng", "Kyunghyun", ""], ["Hwang", "Sangheum", ""], ["Park", "Sunggyun", ""], ["Kim", "Minsoo", ""]]}, {"id": "1612.07182", "submitter": "Angeliki  Lazaridou", "authors": "Angeliki Lazaridou, Alexander Peysakhovich, Marco Baroni", "title": "Multi-Agent Cooperation and the Emergence of (Natural) Language", "comments": "Accepted at ICLR 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV cs.GT cs.LG cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The current mainstream approach to train natural language systems is to\nexpose them to large amounts of text. This passive learning is problematic if\nwe are interested in developing interactive machines, such as conversational\nagents. We propose a framework for language learning that relies on multi-agent\ncommunication. We study this learning in the context of referential games. In\nthese games, a sender and a receiver see a pair of images. The sender is told\none of them is the target and is allowed to send a message from a fixed,\narbitrary vocabulary to the receiver. The receiver must rely on this message to\nidentify the target. Thus, the agents develop their own language interactively\nout of the need to communicate. We show that two networks with simple\nconfigurations are able to learn to coordinate in the referential game. We\nfurther explore how to make changes to the game environment to cause the \"word\nmeanings\" induced in the game to better reflect intuitive semantic properties\nof the images. In addition, we present a simple strategy for grounding the\nagents' code into natural language. Both of these are necessary steps towards\ndeveloping machines that are able to communicate with humans productively.\n", "versions": [{"version": "v1", "created": "Wed, 21 Dec 2016 15:27:06 GMT"}, {"version": "v2", "created": "Sun, 5 Mar 2017 21:40:51 GMT"}], "update_date": "2017-03-07", "authors_parsed": [["Lazaridou", "Angeliki", ""], ["Peysakhovich", "Alexander", ""], ["Baroni", "Marco", ""]]}, {"id": "1612.07217", "submitter": "Pavel Tokmakov", "authors": "Pavel Tokmakov, Karteek Alahari, Cordelia Schmid", "title": "Learning Motion Patterns in Videos", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of determining whether an object is in motion, irrespective of\ncamera motion, is far from being solved. We address this challenging task by\nlearning motion patterns in videos. The core of our approach is a fully\nconvolutional network, which is learned entirely from synthetic video\nsequences, and their ground-truth optical flow and motion segmentation. This\nencoder-decoder style architecture first learns a coarse representation of the\noptical flow field features, and then refines it iteratively to produce motion\nlabels at the original high-resolution. We further improve this labeling with\nan objectness map and a conditional random field, to account for errors in\noptical flow, and also to focus on moving \"things\" rather than \"stuff\". The\noutput label of each pixel denotes whether it has undergone independent motion,\ni.e., irrespective of camera motion. We demonstrate the benefits of this\nlearning framework on the moving object segmentation task, where the goal is to\nsegment all objects in motion. Our approach outperforms the top method on the\nrecently released DAVIS benchmark dataset, comprising real-world sequences, by\n5.6%. We also evaluate on the Berkeley motion segmentation database, achieving\nstate-of-the-art results.\n", "versions": [{"version": "v1", "created": "Wed, 21 Dec 2016 16:14:41 GMT"}, {"version": "v2", "created": "Mon, 10 Apr 2017 11:33:59 GMT"}], "update_date": "2017-04-11", "authors_parsed": [["Tokmakov", "Pavel", ""], ["Alahari", "Karteek", ""], ["Schmid", "Cordelia", ""]]}, {"id": "1612.07310", "submitter": "Cewu Lu", "authors": "Cewu Lu, Hao Su, Yongyi Lu, Li Yi, Chikeung Tang, Leonidas Guibas", "title": "Beyond Holistic Object Recognition: Enriching Image Understanding with\n  Part States", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": "23452523", "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Important high-level vision tasks such as human-object interaction, image\ncaptioning and robotic manipulation require rich semantic descriptions of\nobjects at part level. Based upon previous work on part localization, in this\npaper, we address the problem of inferring rich semantics imparted by an object\npart in still images. We propose to tokenize the semantic space as a discrete\nset of part states. Our modeling of part state is spatially localized,\ntherefore, we formulate the part state inference problem as a pixel-wise\nannotation problem. An iterative part-state inference neural network is\nspecifically designed for this task, which is efficient in time and accurate in\nperformance. Extensive experiments demonstrate that the proposed method can\neffectively predict the semantic states of parts and simultaneously correct\nlocalization errors, thus benefiting a few visual understanding applications.\nThe other contribution of this paper is our part state dataset which contains\nrich part-level semantic annotations.\n", "versions": [{"version": "v1", "created": "Thu, 15 Dec 2016 13:46:58 GMT"}], "update_date": "2016-12-22", "authors_parsed": [["Lu", "Cewu", ""], ["Su", "Hao", ""], ["Lu", "Yongyi", ""], ["Yi", "Li", ""], ["Tang", "Chikeung", ""], ["Guibas", "Leonidas", ""]]}, {"id": "1612.07360", "submitter": "Abir Das", "authors": "Vasili Ramanishka, Abir Das, Jianming Zhang, Kate Saenko", "title": "Top-down Visual Saliency Guided by Captions", "comments": "CVPR 2017 camera ready version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural image/video captioning models can generate accurate descriptions, but\ntheir internal process of mapping regions to words is a black box and therefore\ndifficult to explain. Top-down neural saliency methods can find important\nregions given a high-level semantic task such as object classification, but\ncannot use a natural language sentence as the top-down input for the task. In\nthis paper, we propose Caption-Guided Visual Saliency to expose the\nregion-to-word mapping in modern encoder-decoder networks and demonstrate that\nit is learned implicitly from caption training data, without any pixel-level\nannotations. Our approach can produce spatial or spatiotemporal heatmaps for\nboth predicted captions, and for arbitrary query sentences. It recovers\nsaliency without the overhead of introducing explicit attention layers, and can\nbe used to analyze a variety of existing model architectures and improve their\ndesign. Evaluation on large-scale video and image datasets demonstrates that\nour approach achieves comparable captioning performance with existing methods\nwhile providing more accurate saliency heatmaps. Our code is available at\nvisionlearninggroup.github.io/caption-guided-saliency/.\n", "versions": [{"version": "v1", "created": "Wed, 21 Dec 2016 22:02:34 GMT"}, {"version": "v2", "created": "Wed, 12 Apr 2017 22:49:47 GMT"}], "update_date": "2017-09-12", "authors_parsed": [["Ramanishka", "Vasili", ""], ["Das", "Abir", ""], ["Zhang", "Jianming", ""], ["Saenko", "Kate", ""]]}, {"id": "1612.07379", "submitter": "Jhony Heriberto Giraldo Zuluaga", "authors": "Jhony-Heriberto Giraldo-Zuluaga, Geman Diez, Alexander Gomez, Tatiana\n  Martinez, Mariana Pe\\~nuela Vasquez, Jesus Francisco Vargas Bonilla, Augusto\n  Salazar", "title": "Automatic Identification of Scenedesmus Polymorphic Microalgae from\n  Microscopic Images", "comments": "This is a pre-print of an article published in Pattern Analysis and\n  Applications. The final authenticated version is available online at:\n  https://doi.org/10.1007/s10044-017-0662-3, Pattern Anal Applic (2017)", "journal-ref": null, "doi": "10.1007/s10044-017-0662-3", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Microalgae counting is used to measure biomass quantity. Usually, it is\nperformed in a manual way using a Neubauer chamber and expert criterion, with\nthe risk of a high error rate. This paper addresses the methodology for\nautomatic identification of Scenedesmus microalgae (used in the methane\nproduction and food industry) and applies it to images captured by a digital\nmicroscope. The use of contrast adaptive histogram equalization for\npre-processing, and active contours for segmentation are presented. The\ncalculation of statistical features (Histogram of Oriented Gradients, Hu and\nZernike moments) with texture features (Haralick and Local Binary Patterns\ndescriptors) are proposed for algae characterization. Scenedesmus algae can\nbuild coenobia consisting of 1, 2, 4 and 8 cells. The amount of algae of each\ncoenobium helps to determine the amount of lipids, proteins, and other\nsubstances in a given sample of a algae crop. The knowledge of the quantity of\nthose elements improves the quality of bioprocess applications. Classification\nof coenobia achieves accuracies of 98.63% and 97.32% with Support Vector\nMachine (SVM) and Artificial Neural Network (ANN), respectively. According to\nthe results it is possible to consider the proposed methodology as an\nalternative to the traditional technique for algae counting. The database used\nin this paper is publicly available for download.\n", "versions": [{"version": "v1", "created": "Wed, 21 Dec 2016 23:09:01 GMT"}, {"version": "v2", "created": "Mon, 23 Oct 2017 16:13:18 GMT"}], "update_date": "2017-10-24", "authors_parsed": [["Giraldo-Zuluaga", "Jhony-Heriberto", ""], ["Diez", "Geman", ""], ["Gomez", "Alexander", ""], ["Martinez", "Tatiana", ""], ["Vasquez", "Mariana Pe\u00f1uela", ""], ["Bonilla", "Jesus Francisco Vargas", ""], ["Salazar", "Augusto", ""]]}, {"id": "1612.07403", "submitter": "Yi Zhu", "authors": "Yi Zhu and Shawn Newsam", "title": "Efficient Action Detection in Untrimmed Videos via Multi-Task Learning", "comments": "WACV 2017 camera ready, minor updates about test time efficiency", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies the joint learning of action recognition and temporal\nlocalization in long, untrimmed videos. We employ a multi-task learning\nframework that performs the three highly related steps of action proposal,\naction recognition, and action localization refinement in parallel instead of\nthe standard sequential pipeline that performs the steps in order. We develop a\nnovel temporal actionness regression module that estimates what proportion of a\nclip contains action. We use it for temporal localization but it could have\nother applications like video retrieval, surveillance, summarization, etc. We\nalso introduce random shear augmentation during training to simulate viewpoint\nchange. We evaluate our framework on three popular video benchmarks. Results\ndemonstrate that our joint model is efficient in terms of storage and\ncomputation in that we do not need to compute and cache dense trajectory\nfeatures, and that it is several times faster than its sequential ConvNets\ncounterpart. Yet, despite being more efficient, it outperforms state-of-the-art\nmethods with respect to accuracy.\n", "versions": [{"version": "v1", "created": "Thu, 22 Dec 2016 00:37:42 GMT"}, {"version": "v2", "created": "Tue, 4 Apr 2017 17:49:19 GMT"}], "update_date": "2017-04-05", "authors_parsed": [["Zhu", "Yi", ""], ["Newsam", "Shawn", ""]]}, {"id": "1612.07429", "submitter": "Yinda Zhang", "authors": "Yinda Zhang, Shuran Song, Ersin Yumer, Manolis Savva, Joon-Young Lee,\n  Hailin Jin, Thomas Funkhouser", "title": "Physically-Based Rendering for Indoor Scene Understanding Using\n  Convolutional Neural Networks", "comments": "Updates camera ready version. Accepted by CVPR 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Indoor scene understanding is central to applications such as robot\nnavigation and human companion assistance. Over the last years, data-driven\ndeep neural networks have outperformed many traditional approaches thanks to\ntheir representation learning capabilities. One of the bottlenecks in training\nfor better representations is the amount of available per-pixel ground truth\ndata that is required for core scene understanding tasks such as semantic\nsegmentation, normal prediction, and object edge detection. To address this\nproblem, a number of works proposed using synthetic data. However, a systematic\nstudy of how such synthetic data is generated is missing. In this work, we\nintroduce a large-scale synthetic dataset with 400K physically-based rendered\nimages from 45K realistic 3D indoor scenes. We study the effects of rendering\nmethods and scene lighting on training for three computer vision tasks: surface\nnormal prediction, semantic segmentation, and object boundary detection. This\nstudy provides insights into the best practices for training with synthetic\ndata (more realistic rendering is worth it) and shows that pretraining with our\nnew synthetic dataset can improve results beyond the current state of the art\non all three tasks.\n", "versions": [{"version": "v1", "created": "Thu, 22 Dec 2016 03:18:12 GMT"}, {"version": "v2", "created": "Fri, 30 Dec 2016 01:33:16 GMT"}, {"version": "v3", "created": "Sun, 2 Jul 2017 01:25:36 GMT"}], "update_date": "2017-07-04", "authors_parsed": [["Zhang", "Yinda", ""], ["Song", "Shuran", ""], ["Yumer", "Ersin", ""], ["Savva", "Manolis", ""], ["Lee", "Joon-Young", ""], ["Jin", "Hailin", ""], ["Funkhouser", "Thomas", ""]]}, {"id": "1612.07453", "submitter": "Angshul Majumdar Dr.", "authors": "Shikha Singh, Vanika Singhal and Angshul Majumdar", "title": "Deep Blind Compressed Sensing", "comments": "DCC 2017 Poster", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work addresses the problem of extracting deeply learned features\ndirectly from compressive measurements. There has been no work in this area.\nExisting deep learning tools only give good results when applied on the full\nsignal, that too usually after preprocessing. These techniques require the\nsignal to be reconstructed first. In this work we show that by learning\ndirectly from the compressed domain, considerably better results can be\nobtained. This work extends the recently proposed framework of deep matrix\nfactorization in combination with blind compressed sensing; hence the term deep\nblind compressed sensing. Simulation experiments have been carried out on\nimaging via single pixel camera, under-sampled biomedical signals, arising in\nwireless body area network and compressive hyperspectral imaging. In all cases,\nthe superiority of our proposed deep blind compressed sensing can be envisaged.\n", "versions": [{"version": "v1", "created": "Thu, 22 Dec 2016 06:12:43 GMT"}], "update_date": "2016-12-23", "authors_parsed": [["Singh", "Shikha", ""], ["Singhal", "Vanika", ""], ["Majumdar", "Angshul", ""]]}, {"id": "1612.07528", "submitter": "Bruno Stuner", "authors": "Bruno Stuner, Cl\\'ement Chatelain, Thierry Paquet", "title": "Handwriting recognition using Cohort of LSTM and lexicon verification\n  with extremely large lexicon", "comments": "31 pages, paper submitted to Pattern Recognition", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  State-of-the-art methods for handwriting recognition are based on Long Short\nTerm Memory (LSTM) recurrent neural networks (RNN), which now provides very\nimpressive character recognition performance. The character recognition is\ngenerally coupled with a lexicon driven decoding process which integrates\ndictionaries. Unfortunately these dictionaries are limited to hundred of\nthousands words for the best systems, which prevent from having a good language\ncoverage, and therefore limit the global recognition performance. In this\narticle, we propose an alternative to the lexicon driven decoding process based\non a lexicon verification process, coupled with an original cascade\narchitecture. The cascade is made of a large number of complementary networks\nextracted from a single training (called cohort), making the learning process\nvery light. The proposed method achieves new state-of-the art word recognition\nperformance on the Rimes and IAM databases. Dealing with gigantic lexicon of 3\nmillions words, the methods also demonstrates interesting performance with a\nfast decision stage.\n", "versions": [{"version": "v1", "created": "Thu, 22 Dec 2016 10:21:09 GMT"}, {"version": "v2", "created": "Fri, 23 Dec 2016 13:22:31 GMT"}, {"version": "v3", "created": "Wed, 22 Mar 2017 14:22:09 GMT"}, {"version": "v4", "created": "Mon, 25 Sep 2017 08:57:50 GMT"}], "update_date": "2017-09-26", "authors_parsed": [["Stuner", "Bruno", ""], ["Chatelain", "Cl\u00e9ment", ""], ["Paquet", "Thierry", ""]]}, {"id": "1612.07545", "submitter": "Deng Cai", "authors": "Deng Cai", "title": "A Revisit of Hashing Algorithms for Approximate Nearest Neighbor Search", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Approximate Nearest Neighbor Search (ANNS) is a fundamental problem in many\nareas of machine learning and data mining. During the past decade, numerous\nhashing algorithms are proposed to solve this problem. Every proposed algorithm\nclaims outperform other state-of-the-art hashing methods. However, the\nevaluation of these hashing papers was not thorough enough, and those claims\nshould be re-examined. The ultimate goal of an ANNS method is returning the\nmost accurate answers (nearest neighbors) in the shortest time. If implemented\ncorrectly, almost all the hashing methods will have their performance improved\nas the code length increases. However, many existing hashing papers only report\nthe performance with the code length shorter than 128. In this paper, we\ncarefully revisit the problem of search with a hash index, and analyze the pros\nand cons of two popular hash index search procedures. Then we proposed a very\nsimple but effective two level index structures and make a thorough comparison\nof eleven popular hashing algorithms. Surprisingly, the random-projection-based\nLocality Sensitive Hashing (LSH) is the best performed algorithm, which is in\ncontradiction to the claims in all the other ten hashing papers. Despite the\nextreme simplicity of random-projection-based LSH, our results show that the\ncapability of this algorithm has been far underestimated. For the sake of\nreproducibility, all the codes used in the paper are released on GitHub, which\ncan be used as a testing platform for a fair comparison between various hashing\nalgorithms.\n", "versions": [{"version": "v1", "created": "Thu, 22 Dec 2016 11:17:55 GMT"}, {"version": "v2", "created": "Tue, 27 Dec 2016 04:11:33 GMT"}, {"version": "v3", "created": "Wed, 31 Jan 2018 04:21:33 GMT"}, {"version": "v4", "created": "Sun, 18 Feb 2018 08:52:23 GMT"}, {"version": "v5", "created": "Tue, 15 May 2018 02:29:08 GMT"}, {"version": "v6", "created": "Wed, 19 Jun 2019 01:39:13 GMT"}], "update_date": "2019-06-20", "authors_parsed": [["Cai", "Deng", ""]]}, {"id": "1612.07600", "submitter": "Aykut Erdem", "authors": "Mert Kilickaya, Aykut Erdem, Nazli Ikizler-Cinbis, Erkut Erdem", "title": "Re-evaluating Automatic Metrics for Image Captioning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The task of generating natural language descriptions from images has received\na lot of attention in recent years. Consequently, it is becoming increasingly\nimportant to evaluate such image captioning approaches in an automatic manner.\nIn this paper, we provide an in-depth evaluation of the existing image\ncaptioning metrics through a series of carefully designed experiments.\nMoreover, we explore the utilization of the recently proposed Word Mover's\nDistance (WMD) document metric for the purpose of image captioning. Our\nfindings outline the differences and/or similarities between metrics and their\nrelative robustness by means of extensive correlation, accuracy and distraction\nbased evaluations. Our results also demonstrate that WMD provides strong\nadvantages over other metrics.\n", "versions": [{"version": "v1", "created": "Thu, 22 Dec 2016 14:00:28 GMT"}], "update_date": "2016-12-23", "authors_parsed": [["Kilickaya", "Mert", ""], ["Erdem", "Aykut", ""], ["Ikizler-Cinbis", "Nazli", ""], ["Erdem", "Erkut", ""]]}, {"id": "1612.07625", "submitter": "Vivienne Sze", "authors": "Vivienne Sze, Yu-Hsin Chen, Joel Emer, Amr Suleiman, Zhengdong Zhang", "title": "Hardware for Machine Learning: Challenges and Opportunities", "comments": "Published as an invited conference paper at CICC 2017", "journal-ref": null, "doi": "10.1109/CICC.2017.7993626", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning plays a critical role in extracting meaningful information\nout of the zetabytes of sensor data collected every day. For some applications,\nthe goal is to analyze and understand the data to identify trends (e.g.,\nsurveillance, portable/wearable electronics); in other applications, the goal\nis to take immediate action based the data (e.g., robotics/drones, self-driving\ncars, smart Internet of Things). For many of these applications, local embedded\nprocessing near the sensor is preferred over the cloud due to privacy or\nlatency concerns, or limitations in the communication bandwidth. However, at\nthe sensor there are often stringent constraints on energy consumption and cost\nin addition to throughput and accuracy requirements. Furthermore, flexibility\nis often required such that the processing can be adapted for different\napplications or environments (e.g., update the weights and model in the\nclassifier). In many applications, machine learning often involves transforming\nthe input data into a higher dimensional space, which, along with programmable\nweights, increases data movement and consequently energy consumption. In this\npaper, we will discuss how these challenges can be addressed at various levels\nof hardware design ranging from architecture, hardware-friendly algorithms,\nmixed-signal circuits, and advanced technologies (including memories and\nsensors).\n", "versions": [{"version": "v1", "created": "Thu, 22 Dec 2016 14:50:40 GMT"}, {"version": "v2", "created": "Fri, 17 Mar 2017 11:06:49 GMT"}, {"version": "v3", "created": "Sat, 8 Apr 2017 22:43:59 GMT"}, {"version": "v4", "created": "Tue, 1 Aug 2017 00:35:31 GMT"}, {"version": "v5", "created": "Tue, 17 Oct 2017 02:50:38 GMT"}], "update_date": "2017-10-18", "authors_parsed": [["Sze", "Vivienne", ""], ["Chen", "Yu-Hsin", ""], ["Emer", "Joel", ""], ["Suleiman", "Amr", ""], ["Zhang", "Zhengdong", ""]]}, {"id": "1612.07695", "submitter": "Marvin Teichmann", "authors": "Marvin Teichmann, Michael Weber, Marius Zoellner, Roberto Cipolla and\n  Raquel Urtasun", "title": "MultiNet: Real-time Joint Semantic Reasoning for Autonomous Driving", "comments": "9 pages, 7 tables and 9 figures; first place on Kitti Road\n  Segmentation; Code on GitHub (https://github.com/MarvinTeichmann/MultiNet)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While most approaches to semantic reasoning have focused on improving\nperformance, in this paper we argue that computational times are very important\nin order to enable real time applications such as autonomous driving. Towards\nthis goal, we present an approach to joint classification, detection and\nsemantic segmentation via a unified architecture where the encoder is shared\namongst the three tasks. Our approach is very simple, can be trained end-to-end\nand performs extremely well in the challenging KITTI dataset, outperforming the\nstate-of-the-art in the road segmentation task. Our approach is also very\nefficient, taking less than 100 ms to perform all tasks.\n", "versions": [{"version": "v1", "created": "Thu, 22 Dec 2016 16:55:02 GMT"}, {"version": "v2", "created": "Tue, 8 May 2018 18:36:33 GMT"}], "update_date": "2018-05-10", "authors_parsed": [["Teichmann", "Marvin", ""], ["Weber", "Michael", ""], ["Zoellner", "Marius", ""], ["Cipolla", "Roberto", ""], ["Urtasun", "Raquel", ""]]}, {"id": "1612.07697", "submitter": "Alexander Vakhitov", "authors": "A. Vakhitov, A. Kuzmin and V. Lempitsky", "title": "Set2Model Networks: Learning Discriminatively To Learn Generative Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new \"learning-to-learn\"-type approach that enables rapid\nlearning of concepts from small-to-medium sized training sets and is primarily\ndesigned for web-initialized image retrieval. At the core of our approach is a\ndeep architecture (a Set2Model network) that maps sets of examples to simple\ngenerative probabilistic models such as Gaussians or mixtures of Gaussians in\nthe space of high-dimensional descriptors. The parameters of the embedding into\nthe descriptor space are trained in the end-to-end fashion in the meta-learning\nstage using a set of training learning problems. The main technical novelty of\nour approach is the derivation of the backprop process through the mixture\nmodel fitting, which makes the likelihood of the resulting models\ndifferentiable with respect to the positions of the input descriptors.\n  While the meta-learning process for a Set2Model network is discriminative, a\ntrained Set2Model network performs generative learning of generative models in\nthe descriptor space, which facilitates learning in the cases when no negative\nexamples are available, and whenever the concept being learned is polysemous or\nrepresented by noisy training sets. Among other experiments, we demonstrate\nthat these properties allow Set2Model networks to pick visual concepts from the\nraw outputs of Internet image search engines better than a set of strong\nbaselines.\n", "versions": [{"version": "v1", "created": "Thu, 22 Dec 2016 17:01:01 GMT"}, {"version": "v2", "created": "Fri, 27 Oct 2017 12:29:53 GMT"}], "update_date": "2017-10-30", "authors_parsed": [["Vakhitov", "A.", ""], ["Kuzmin", "A.", ""], ["Lempitsky", "V.", ""]]}, {"id": "1612.07767", "submitter": "Fuxin Li", "authors": "Xin Li, Fuxin Li", "title": "Adversarial Examples Detection in Deep Networks with Convolutional\n  Filter Statistics", "comments": "Published in ICCV 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning has greatly improved visual recognition in recent years.\nHowever, recent research has shown that there exist many adversarial examples\nthat can negatively impact the performance of such an architecture. This paper\nfocuses on detecting those adversarial examples by analyzing whether they come\nfrom the same distribution as the normal examples. Instead of directly training\na deep neural network to detect adversarials, a much simpler approach was\nproposed based on statistics on outputs from convolutional layers. A cascade\nclassifier was designed to efficiently detect adversarials. Furthermore,\ntrained from one particular adversarial generating mechanism, the resulting\nclassifier can successfully detect adversarials from a completely different\nmechanism as well. The resulting classifier is non-subdifferentiable, hence\ncreates a difficulty for adversaries to attack by using the gradient of the\nclassifier. After detecting adversarial examples, we show that many of them can\nbe recovered by simply performing a small average filter on the image. Those\nfindings should lead to more insights about the classification mechanisms in\ndeep convolutional neural networks.\n", "versions": [{"version": "v1", "created": "Thu, 22 Dec 2016 19:45:31 GMT"}, {"version": "v2", "created": "Thu, 26 Oct 2017 18:42:57 GMT"}], "update_date": "2017-10-30", "authors_parsed": [["Li", "Xin", ""], ["Li", "Fuxin", ""]]}, {"id": "1612.07796", "submitter": "Nicholas Rhinehart", "authors": "Nicholas Rhinehart, Kris M. Kitani", "title": "First-Person Activity Forecasting with Online Inverse Reinforcement\n  Learning", "comments": "To appear at ICCV 2017 (Oral)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of incrementally modeling and forecasting long-term\ngoals of a first-person camera wearer: what the user will do, where they will\ngo, and what goal they seek. In contrast to prior work in trajectory\nforecasting, our algorithm, DARKO, goes further to reason about semantic states\n(will I pick up an object?), and future goal states that are far in terms of\nboth space and time. DARKO learns and forecasts from first-person visual\nobservations of the user's daily behaviors via an Online Inverse Reinforcement\nLearning (IRL) approach. Classical IRL discovers only the rewards in a batch\nsetting, whereas DARKO discovers the states, transitions, rewards, and goals of\na user from streaming data. Among other results, we show DARKO forecasts goals\nbetter than competing methods in both noisy and ideal settings, and our\napproach is theoretically and empirically no-regret.\n", "versions": [{"version": "v1", "created": "Thu, 22 Dec 2016 20:57:22 GMT"}, {"version": "v2", "created": "Thu, 13 Apr 2017 17:31:55 GMT"}, {"version": "v3", "created": "Sun, 6 Aug 2017 22:06:18 GMT"}], "update_date": "2017-08-08", "authors_parsed": [["Rhinehart", "Nicholas", ""], ["Kitani", "Kris M.", ""]]}, {"id": "1612.07801", "submitter": "Luyan Ji", "authors": "Luyan Ji, Jie Wang, Xiurui Geng, Peng Gong", "title": "Probabilistic graphical model based approach for water mapping using\n  GaoFen-2 (GF-2) high resolution imagery and Landsat 8 time series", "comments": "17 pages, 9 figures, 6 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The objective of this paper is to evaluate the potential of Gaofen-2 (GF-2)\nhigh resolution multispectral sensor (MS) and panchromatic (PAN) imagery on\nwater mapping. Difficulties of water mapping on high resolution data includes:\n1) misclassification between water and shadows or other low-reflectance ground\nobjects, which is mostly caused by the spectral similarity within the given\nband range; 2) small water bodies with size smaller than the spatial resolution\nof MS image. To solve the confusion between water and low-reflectance objects,\nthe Landsat 8 time series with two shortwave infrared (SWIR) bands is added\nbecause water has extremely strong absorption in SWIR. In order to integrate\nthe three multi-sensor, multi-resolution data sets, the probabilistic graphical\nmodel (PGM) is utilized here with conditional probability distribution defined\nmainly based on the size of each object. For comparison, results from the SVM\nclassifier on the PCA fused and MS data, thresholding method on the PAN image,\nand water index method on the Landsat data are computed. The confusion matrices\nare calculated for all the methods. The results demonstrate that the PGM method\ncan achieve the best performance with the highest overall accuracy. Moreover,\nsmall rivers can also be extracted by adding weight on the PAN result in PGM.\nFinally, the post-classification procedure is applied on the PGM result to\nfurther exclude misclassification in shadow and water-land boundary regions.\nAccordingly, the producer's, user's and overall accuracy are all increased,\nindicating the effectiveness of our method.\n", "versions": [{"version": "v1", "created": "Thu, 22 Dec 2016 02:59:54 GMT"}], "update_date": "2016-12-26", "authors_parsed": [["Ji", "Luyan", ""], ["Wang", "Jie", ""], ["Geng", "Xiurui", ""], ["Gong", "Peng", ""]]}, {"id": "1612.07828", "submitter": "Ashish Shrivastava", "authors": "Ashish Shrivastava, Tomas Pfister, Oncel Tuzel, Josh Susskind, Wenda\n  Wang, Russ Webb", "title": "Learning from Simulated and Unsupervised Images through Adversarial\n  Training", "comments": "Accepted at CVPR 2017 for oral presentation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With recent progress in graphics, it has become more tractable to train\nmodels on synthetic images, potentially avoiding the need for expensive\nannotations. However, learning from synthetic images may not achieve the\ndesired performance due to a gap between synthetic and real image\ndistributions. To reduce this gap, we propose Simulated+Unsupervised (S+U)\nlearning, where the task is to learn a model to improve the realism of a\nsimulator's output using unlabeled real data, while preserving the annotation\ninformation from the simulator. We develop a method for S+U learning that uses\nan adversarial network similar to Generative Adversarial Networks (GANs), but\nwith synthetic images as inputs instead of random vectors. We make several key\nmodifications to the standard GAN algorithm to preserve annotations, avoid\nartifacts, and stabilize training: (i) a 'self-regularization' term, (ii) a\nlocal adversarial loss, and (iii) updating the discriminator using a history of\nrefined images. We show that this enables generation of highly realistic\nimages, which we demonstrate both qualitatively and with a user study. We\nquantitatively evaluate the generated images by training models for gaze\nestimation and hand pose estimation. We show a significant improvement over\nusing synthetic images, and achieve state-of-the-art results on the MPIIGaze\ndataset without any labeled real data.\n", "versions": [{"version": "v1", "created": "Thu, 22 Dec 2016 22:10:51 GMT"}, {"version": "v2", "created": "Wed, 19 Jul 2017 21:24:52 GMT"}], "update_date": "2017-07-21", "authors_parsed": [["Shrivastava", "Ashish", ""], ["Pfister", "Tomas", ""], ["Tuzel", "Oncel", ""], ["Susskind", "Josh", ""], ["Wang", "Wenda", ""], ["Webb", "Russ", ""]]}, {"id": "1612.07833", "submitter": "Radu Soricut", "authors": "Nan Ding and Sebastian Goodman and Fei Sha and Radu Soricut", "title": "Understanding Image and Text Simultaneously: a Dual Vision-Language\n  Machine Comprehension Task", "comments": "11 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new multi-modal task for computer systems, posed as a combined\nvision-language comprehension challenge: identifying the most suitable text\ndescribing a scene, given several similar options. Accomplishing the task\nentails demonstrating comprehension beyond just recognizing \"keywords\" (or\nkey-phrases) and their corresponding visual concepts. Instead, it requires an\nalignment between the representations of the two modalities that achieves a\nvisually-grounded \"understanding\" of various linguistic elements and their\ndependencies. This new task also admits an easy-to-compute and well-studied\nmetric: the accuracy in detecting the true target among the decoys.\n  The paper makes several contributions: an effective and extensible mechanism\nfor generating decoys from (human-created) image captions; an instance of\napplying this mechanism, yielding a large-scale machine comprehension dataset\n(based on the COCO images and captions) that we make publicly available; human\nevaluation results on this dataset, informing a performance upper-bound; and\nseveral baseline and competitive learning approaches that illustrate the\nutility of the proposed task and dataset in advancing both image and language\ncomprehension. We also show that, in a multi-task learning setting, the\nperformance on the proposed task is positively correlated with the end-to-end\ntask of image captioning.\n", "versions": [{"version": "v1", "created": "Thu, 22 Dec 2016 22:44:17 GMT"}], "update_date": "2016-12-26", "authors_parsed": [["Ding", "Nan", ""], ["Goodman", "Sebastian", ""], ["Sha", "Fei", ""], ["Soricut", "Radu", ""]]}, {"id": "1612.07850", "submitter": "Manh Duong Phung", "authors": "M.D. Phung, C.H. Quach, D.T. Chu, N.Q. Nguyen, T.H. Dinh and Q.P. Ha", "title": "Automatic Interpretation of Unordered Point Cloud Data for UAV\n  Navigation in Construction", "comments": "In The 14th International Conference on Control, Automation, Robotics\n  and Vision, ICARCV 2016", "journal-ref": null, "doi": "10.1109/ICARCV.2016.7838683", "report-no": null, "categories": "cs.RO cs.CV cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The objective of this work is to develop a data processing system that can\nautomatically generate waypoints for navigation of an unmanned aerial vehicle\n(UAV) to inspect surfaces of structures like buildings and bridges. The input\nincludes data recorded by two 2D laser scanners, orthogonally mounted on the\nUAV, and an inertial measurement unit (IMU). To achieve the goal, algorithms\nare developed to process the data collected. They are separated into three\nmajor groups: (i) the data registration and filtering to generate a 3D model of\nthe structure and control the density of point clouds for data completeness\nenhancement; (ii) the surface and obstacle detection to assist the UAV in\nmonitoring tasks; and (iii) the waypoint generation to set the flight path.\nExperiments on different data sets show that the developed system is able to\nreconstruct a 3D point cloud of the structure, extract its surfaces and\nobjects, and generate waypoints for the UAV to accomplish inspection tasks.\n", "versions": [{"version": "v1", "created": "Fri, 23 Dec 2016 01:23:46 GMT"}, {"version": "v2", "created": "Sun, 12 Feb 2017 10:43:26 GMT"}], "update_date": "2017-02-14", "authors_parsed": [["Phung", "M. D.", ""], ["Quach", "C. H.", ""], ["Chu", "D. T.", ""], ["Nguyen", "N. Q.", ""], ["Dinh", "T. H.", ""], ["Ha", "Q. P.", ""]]}, {"id": "1612.07899", "submitter": "Louis Lettry", "authors": "Louis Lettry, Kenneth Vanhoey and Luc Van Gool", "title": "DARN: a Deep Adversial Residual Network for Intrinsic Image\n  Decomposition", "comments": "Published in Winter Conference on Applications of Computer Vision\n  2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new deep supervised learning method for intrinsic decomposition\nof a single image into its albedo and shading components. Our contributions are\nbased on a new fully convolutional neural network that estimates absolute\nalbedo and shading jointly. Our solution relies on a single end-to-end deep\nsequence of residual blocks and a perceptually-motivated metric formed by two\nadversarially trained discriminators. As opposed to classical intrinsic image\ndecomposition work, it is fully data-driven, hence does not require any\nphysical priors like shading smoothness or albedo sparsity, nor does it rely on\ngeometric information such as depth. Compared to recent deep learning\ntechniques, we simplify the architecture, making it easier to build and train,\nand constrain it to generate a valid and reversible decomposition. We rediscuss\nand augment the set of quantitative metrics so as to account for the more\nchallenging recovery of non scale-invariant quantities. We train and\ndemonstrate our architecture on the publicly available MPI Sintel dataset and\nits intrinsic image decomposition, show attenuated overfitting issues and\ndiscuss generalizability to other data. Results show that our work outperforms\nthe state of the art deep algorithms both on the qualitative and quantitative\naspect.\n", "versions": [{"version": "v1", "created": "Fri, 23 Dec 2016 08:20:10 GMT"}, {"version": "v2", "created": "Tue, 20 Mar 2018 11:05:57 GMT"}], "update_date": "2018-03-21", "authors_parsed": [["Lettry", "Louis", ""], ["Vanhoey", "Kenneth", ""], ["Van Gool", "Luc", ""]]}, {"id": "1612.07919", "submitter": "Mehdi S. M. Sajjadi", "authors": "Mehdi S. M. Sajjadi and Bernhard Sch\\\"olkopf and Michael Hirsch", "title": "EnhanceNet: Single Image Super-Resolution Through Automated Texture\n  Synthesis", "comments": "main paper and supplementary material", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Single image super-resolution is the task of inferring a high-resolution\nimage from a single low-resolution input. Traditionally, the performance of\nalgorithms for this task is measured using pixel-wise reconstruction measures\nsuch as peak signal-to-noise ratio (PSNR) which have been shown to correlate\npoorly with the human perception of image quality. As a result, algorithms\nminimizing these metrics tend to produce over-smoothed images that lack\nhigh-frequency textures and do not look natural despite yielding high PSNR\nvalues.\n  We propose a novel application of automated texture synthesis in combination\nwith a perceptual loss focusing on creating realistic textures rather than\noptimizing for a pixel-accurate reproduction of ground truth images during\ntraining. By using feed-forward fully convolutional neural networks in an\nadversarial training setting, we achieve a significant boost in image quality\nat high magnification ratios. Extensive experiments on a number of datasets\nshow the effectiveness of our approach, yielding state-of-the-art results in\nboth quantitative and qualitative benchmarks.\n", "versions": [{"version": "v1", "created": "Fri, 23 Dec 2016 10:16:26 GMT"}, {"version": "v2", "created": "Sun, 30 Jul 2017 21:52:23 GMT"}], "update_date": "2018-01-16", "authors_parsed": [["Sajjadi", "Mehdi S. M.", ""], ["Sch\u00f6lkopf", "Bernhard", ""], ["Hirsch", "Michael", ""]]}, {"id": "1612.07921", "submitter": "Amit Mishra", "authors": "Amit Kumar Mishra", "title": "Understanding Non-optical Remote-sensed Images: Needs, Challenges and\n  Ways Forward", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Non-optical remote-sensed images are going to be used more often in man-\naging disaster, crime and precision agriculture. With more small satellites and\nunmanned air vehicles planning to carry radar and hyperspectral image sensors\nthere is going to be an abundance of such data in the recent future.\nUnderstanding these data in real-time will be crucial in attaining some of the\nimportant sustain- able development goals. Processing non-optical images is, in\nmany ways, different from that of optical images. Most of the recent advances\nin the domain of image understanding has been using optical images. In this\narticle we shall explain the needs for image understanding in non-optical\ndomain and the typical challenges. Then we shall describe the existing\napproaches and how we can move from there to the desired goal of a reliable\nreal-time image understanding system.\n", "versions": [{"version": "v1", "created": "Fri, 23 Dec 2016 10:17:00 GMT"}], "update_date": "2016-12-26", "authors_parsed": [["Mishra", "Amit Kumar", ""]]}, {"id": "1612.07978", "submitter": "Hengkai Guo", "authors": "Hengkai Guo, Guijin Wang, Xinghao Chen", "title": "Two-stream convolutional neural network for accurate RGB-D fingertip\n  detection using depth and edge information", "comments": "Accepted by ICIP 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurate detection of fingertips in depth image is critical for\nhuman-computer interaction. In this paper, we present a novel two-stream\nconvolutional neural network (CNN) for RGB-D fingertip detection. Firstly edge\nimage is extracted from raw depth image using random forest. Then the edge\ninformation is combined with depth information in our CNN structure. We study\nseveral fusion approaches and suggest a slow fusion strategy as a promising way\nof fingertip detection. As shown in our experiments, our real-time algorithm\noutperforms state-of-the-art fingertip detection methods on the public dataset\nHandNet with an average 3D error of 9.9mm, and shows comparable accuracy of\nfingertip estimation on NYU hand dataset.\n", "versions": [{"version": "v1", "created": "Fri, 23 Dec 2016 14:17:31 GMT"}], "update_date": "2016-12-26", "authors_parsed": [["Guo", "Hengkai", ""], ["Wang", "Guijin", ""], ["Chen", "Xinghao", ""]]}, {"id": "1612.08012", "submitter": "Arnaud Arindra Adiyoso Setio", "authors": "Arnaud Arindra Adiyoso Setio, Alberto Traverso, Thomas de Bel, Moira\n  S.N. Berens, Cas van den Bogaard, Piergiorgio Cerello, Hao Chen, Qi Dou,\n  Maria Evelina Fantacci, Bram Geurts, Robbert van der Gugten, Pheng Ann Heng,\n  Bart Jansen, Michael M.J. de Kaste, Valentin Kotov, Jack Yu-Hung Lin, Jeroen\n  T.M.C. Manders, Alexander S\\'onora-Mengana, Juan Carlos Garc\\'ia-Naranjo,\n  Evgenia Papavasileiou, Mathias Prokop, Marco Saletta, Cornelia M\n  Schaefer-Prokop, Ernst T. Scholten, Luuk Scholten, Miranda M. Snoeren,\n  Ernesto Lopez Torres, Jef Vandemeulebroucke, Nicole Walasek, Guido C.A.\n  Zuidhof, Bram van Ginneken, Colin Jacobs", "title": "Validation, comparison, and combination of algorithms for automatic\n  detection of pulmonary nodules in computed tomography images: the LUNA16\n  challenge", "comments": null, "journal-ref": null, "doi": "10.1016/j.media.2017.06.015", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic detection of pulmonary nodules in thoracic computed tomography (CT)\nscans has been an active area of research for the last two decades. However,\nthere have only been few studies that provide a comparative performance\nevaluation of different systems on a common database. We have therefore set up\nthe LUNA16 challenge, an objective evaluation framework for automatic nodule\ndetection algorithms using the largest publicly available reference database of\nchest CT scans, the LIDC-IDRI data set. In LUNA16, participants develop their\nalgorithm and upload their predictions on 888 CT scans in one of the two\ntracks: 1) the complete nodule detection track where a complete CAD system\nshould be developed, or 2) the false positive reduction track where a provided\nset of nodule candidates should be classified. This paper describes the setup\nof LUNA16 and presents the results of the challenge so far. Moreover, the\nimpact of combining individual systems on the detection performance was also\ninvestigated. It was observed that the leading solutions employed convolutional\nnetworks and used the provided set of nodule candidates. The combination of\nthese solutions achieved an excellent sensitivity of over 95% at fewer than 1.0\nfalse positives per scan. This highlights the potential of combining algorithms\nto improve the detection performance. Our observer study with four expert\nreaders has shown that the best system detects nodules that were missed by\nexpert readers who originally annotated the LIDC-IDRI data. We released this\nset of additional nodules for further development of CAD systems.\n", "versions": [{"version": "v1", "created": "Fri, 23 Dec 2016 15:47:27 GMT"}, {"version": "v2", "created": "Thu, 5 Jan 2017 08:26:13 GMT"}, {"version": "v3", "created": "Fri, 30 Jun 2017 07:56:47 GMT"}, {"version": "v4", "created": "Sat, 15 Jul 2017 12:11:40 GMT"}], "update_date": "2017-07-18", "authors_parsed": [["Setio", "Arnaud Arindra Adiyoso", ""], ["Traverso", "Alberto", ""], ["de Bel", "Thomas", ""], ["Berens", "Moira S. N.", ""], ["Bogaard", "Cas van den", ""], ["Cerello", "Piergiorgio", ""], ["Chen", "Hao", ""], ["Dou", "Qi", ""], ["Fantacci", "Maria Evelina", ""], ["Geurts", "Bram", ""], ["van der Gugten", "Robbert", ""], ["Heng", "Pheng Ann", ""], ["Jansen", "Bart", ""], ["de Kaste", "Michael M. J.", ""], ["Kotov", "Valentin", ""], ["Lin", "Jack Yu-Hung", ""], ["Manders", "Jeroen T. M. C.", ""], ["S\u00f3nora-Mengana", "Alexander", ""], ["Garc\u00eda-Naranjo", "Juan Carlos", ""], ["Papavasileiou", "Evgenia", ""], ["Prokop", "Mathias", ""], ["Saletta", "Marco", ""], ["Schaefer-Prokop", "Cornelia M", ""], ["Scholten", "Ernst T.", ""], ["Scholten", "Luuk", ""], ["Snoeren", "Miranda M.", ""], ["Torres", "Ernesto Lopez", ""], ["Vandemeulebroucke", "Jef", ""], ["Walasek", "Nicole", ""], ["Zuidhof", "Guido C. A.", ""], ["van Ginneken", "Bram", ""], ["Jacobs", "Colin", ""]]}, {"id": "1612.08036", "submitter": "Agata Mosinska", "authors": "Agata Mosinska, Jakub Tarnawski, Pascal Fua", "title": "Active Learning and Proofreading for Delineation of Curvilinear\n  Structures", "comments": "extended version with fast reconstruction", "journal-ref": "Proc. of Medical Image Computing and Computer-Assisted\n  Intervention (MICCAI) 2017, pages 165-173", "doi": "10.1007/978-3-319-66185-8_19", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many state-of-the-art delineation methods rely on supervised machine learning\nalgorithms. As a result, they require manually annotated training data, which\nis tedious to obtain. Furthermore, even minor classification errors may\nsignificantly affect the topology of the final result. In this paper we propose\na generic approach to addressing both of these problems by taking into account\nthe influence of a potential misclassification on the resulting delineation. In\nan Active Learning context, we identify parts of linear structures that should\nbe annotated first in order to train a classifier effectively. In a\nproofreading context, we similarly find regions of the resulting reconstruction\nthat should be verified in priority to obtain a nearly-perfect result. In both\ncases, by focusing the attention of the human expert on potential\nclassification mistakes which are the most critical parts of the delineation,\nwe reduce the amount of required supervision. We demonstrate the effectiveness\nof our approach on microscopy images depicting blood vessels and neurons.\n", "versions": [{"version": "v1", "created": "Fri, 23 Dec 2016 17:01:31 GMT"}, {"version": "v2", "created": "Mon, 13 Mar 2017 11:06:03 GMT"}], "update_date": "2017-09-12", "authors_parsed": [["Mosinska", "Agata", ""], ["Tarnawski", "Jakub", ""], ["Fua", "Pascal", ""]]}, {"id": "1612.08037", "submitter": "Rui Chen", "authors": "Rui Chen, Huizhu Jia, Xiaodong Xie, Wen Gao", "title": "Blind restoration for non-uniform aerial images using non-local Retinex\n  model and shearlet-based higher-order regularization", "comments": "to be published in Journal of Electronic Imaging", "journal-ref": null, "doi": "10.1117/1.JEI.26.3.033016", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Aerial images are often degraded by space-varying motion blur and\nsimultaneous uneven illumination. To recover high-quality aerial image from its\nnon-uniform version, we propose a novel patch-wise restoration approach based\non a key observation that the degree of blurring is inevitably affected by the\nilluminated conditions. A non-local Retinex model is developed to accurately\nestimate the reflectance component from the degraded aerial image. Thereafter\nthe uneven illumination is corrected well. And then non-uniform coupled\nblurring in the enhanced reflectance image is alleviated and transformed\ntowards uniform distribution, which will facilitate the subsequent deblurring.\nFor constructing the multi-scale sparsified regularizer, the discrete shearlet\ntransform is improved to better represent anisotropic image features in term of\ndirectional sensitivity and selectivity. In addition, a new adaptive variant of\ntotal generalized variation is proposed for the structure-preserving\nregularizer. These complementary regularizers are elegantly integrated into an\nobjective function. The final deblurred image with uniform illumination can be\nextracted by applying the fast alternating direction scheme to solve the\nderived function. The experimental results demonstrate that our algorithm can\nnot only remove both the space-varying illumination and motion blur in the\naerial image effectively but also recover the abundant details of aerial scenes\nwith top-level objective and subjective quality, and outperforms other\nstate-of-the-art restoration methods.\n", "versions": [{"version": "v1", "created": "Fri, 23 Dec 2016 17:02:16 GMT"}], "update_date": "2017-06-28", "authors_parsed": [["Chen", "Rui", ""], ["Jia", "Huizhu", ""], ["Xie", "Xiaodong", ""], ["Gao", "Wen", ""]]}, {"id": "1612.08049", "submitter": "Rui Chen", "authors": "Rui Chen, Huizhu Jia, Xiaodong Xie, Wen Gao", "title": "Correlation Preserving Sparse Coding Over Multi-level Dictionaries for\n  Image Denoising", "comments": "to be published in IEEE Signal Processing Letters", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this letter, we propose a novel image denoising method based on\ncorrelation preserving sparse coding. Because the instable and unreliable\ncorrelations among basis set can limit the performance of the dictionary-driven\ndenoising methods, two effective regularized strategies are employed in the\ncoding process. Specifically, a graph-based regularizer is built for preserving\nthe global similarity correlations, which can adaptively capture both the\ngeometrical structures and discriminative features of textured patches. In\nparticular, edge weights in the graph are obtained by seeking a nonnegative\nlow-rank construction. Besides, a robust locality-constrained coding can\nautomatically preserve not only spatial neighborhood information but also\ninternal consistency present in noisy patches while learning overcomplete\ndictionary. Experimental results demonstrate that our proposed method achieves\nstate-of-the-art denoising performance in terms of both PSNR and subjective\nvisual quality.\n", "versions": [{"version": "v1", "created": "Fri, 23 Dec 2016 17:38:23 GMT"}], "update_date": "2016-12-26", "authors_parsed": [["Chen", "Rui", ""], ["Jia", "Huizhu", ""], ["Xie", "Xiaodong", ""], ["Gao", "Wen", ""]]}, {"id": "1612.08153", "submitter": "Shervin Ardeshir", "authors": "Shervin Ardeshir, Sandesh Sharma, Ali Broji", "title": "EgoReID: Cross-view Self-Identification and Human Re-identification in\n  Egocentric and Surveillance Videos", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human identification remains to be one of the challenging tasks in computer\nvision community due to drastic changes in visual features across different\nviewpoints, lighting conditions, occlusion, etc. Most of the literature has\nbeen focused on exploring human re-identification across viewpoints that are\nnot too drastically different in nature. Cameras usually capture oblique or\nside views of humans, leaving room for a lot of geometric and visual reasoning.\nGiven the recent popularity of egocentric and top-view vision,\nre-identification across these two drastically different views can now be\nexplored. Having an egocentric and a top view video, our goal is to identify\nthe cameraman in the content of the top-view video, and also re-identify the\npeople visible in the egocentric video, by matching them to the identities\npresent in the top-view video. We propose a CRF-based method to address the two\nproblems. Our experimental results demonstrates the efficiency of the proposed\napproach over a variety of video recorded from two views.\n", "versions": [{"version": "v1", "created": "Sat, 24 Dec 2016 09:00:37 GMT"}], "update_date": "2016-12-28", "authors_parsed": [["Ardeshir", "Shervin", ""], ["Sharma", "Sandesh", ""], ["Broji", "Ali", ""]]}, {"id": "1612.08169", "submitter": "Kaihua Zhang", "authors": "Kaihua Zhang and Xuejun Li and Qingshan Liu", "title": "Unsupervised Video Segmentation via Spatio-Temporally Nonlocal\n  Appearance Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video object segmentation is challenging due to the factors like rapidly fast\nmotion, cluttered backgrounds, arbitrary object appearance variation and shape\ndeformation. Most existing methods only explore appearance information between\ntwo consecutive frames, which do not make full use of the usefully long-term\nnonlocal information that is helpful to make the learned appearance stable, and\nhence they tend to fail when the targets suffer from large viewpoint changes\nand significant non-rigid deformations. In this paper, we propose a simple yet\neffective approach to mine the long-term sptatio-temporally nonlocal appearance\ninformation for unsupervised video segmentation. The motivation of our\nalgorithm comes from the spatio-temporal nonlocality of the region appearance\nreoccurrence in a video. Specifically, we first generate a set of superpixels\nto represent the foreground and background, and then update the appearance of\neach superpixel with its long-term sptatio-temporally nonlocal counterparts\ngenerated by the approximate nearest neighbor search method with the efficient\nKD-tree algorithm. Then, with the updated appearances, we formulate a\nspatio-temporal graphical model comprised of the superpixel label consistency\npotentials. Finally, we generate the segmentation by optimizing the graphical\nmodel via iteratively updating the appearance model and estimating the labels.\nExtensive evaluations on the SegTrack and Youtube-Objects datasets demonstrate\nthe effectiveness of the proposed method, which performs favorably against some\nstate-of-art methods.\n", "versions": [{"version": "v1", "created": "Sat, 24 Dec 2016 12:04:31 GMT"}], "update_date": "2016-12-28", "authors_parsed": [["Zhang", "Kaihua", ""], ["Li", "Xuejun", ""], ["Liu", "Qingshan", ""]]}, {"id": "1612.08170", "submitter": "Benjamin Berkels", "authors": "Benjamin Berkels and Benedikt Wirth", "title": "Joint denoising and distortion correction of atomic scale scanning\n  transmission electron microscopy images", "comments": null, "journal-ref": null, "doi": "10.1088/1361-6420/aa7b94", "report-no": null, "categories": "cs.CV physics.data-an", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nowadays, modern electron microscopes deliver images at atomic scale. The\nprecise atomic structure encodes information about material properties. Thus,\nan important ingredient in the image analysis is to locate the centers of the\natoms shown in micrographs as precisely as possible. Here, we consider scanning\ntransmission electron microscopy (STEM), which acquires data in a rastering\npattern, pixel by pixel. Due to this rastering combined with the magnification\nto atomic scale, movements of the specimen even at the nanometer scale lead to\nrandom image distortions that make precise atom localization difficult. Given a\nseries of STEM images, we derive a Bayesian method that jointly estimates the\ndistortion in each image and reconstructs the underlying atomic grid of the\nmaterial by fitting the atom bumps with suitable bump functions. The resulting\nhighly non-convex minimization problems are solved numerically with a trust\nregion approach. Well-posedness of the reconstruction method and the model\nbehavior for faster and faster rastering are investigated using variational\ntechniques. The performance of the method is finally evaluated on both\nsynthetic and real experimental data.\n", "versions": [{"version": "v1", "created": "Sat, 24 Dec 2016 12:37:17 GMT"}], "update_date": "2017-09-13", "authors_parsed": [["Berkels", "Benjamin", ""], ["Wirth", "Benedikt", ""]]}, {"id": "1612.08185", "submitter": "Alexander Kolesnikov", "authors": "Alexander Kolesnikov and Christoph H. Lampert", "title": "PixelCNN Models with Auxiliary Variables for Natural Image Modeling", "comments": "ICML 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study probabilistic models of natural images and extend the autoregressive\nfamily of PixelCNN architectures by incorporating auxiliary variables.\nSubsequently, we describe two new generative image models that exploit\ndifferent image transformations as auxiliary variables: a quantized grayscale\nview of the image or a multi-resolution image pyramid. The proposed models\ntackle two known shortcomings of existing PixelCNN models: 1) their tendency to\nfocus on low-level image details, while largely ignoring high-level image\ninformation, such as object shapes, and 2) their computationally costly\nprocedure for image sampling. We experimentally demonstrate benefits of the\nproposed models, in particular showing that they produce much more\nrealistically looking image samples than previous state-of-the-art\nprobabilistic models.\n", "versions": [{"version": "v1", "created": "Sat, 24 Dec 2016 14:20:05 GMT"}, {"version": "v2", "created": "Fri, 30 Dec 2016 13:14:23 GMT"}, {"version": "v3", "created": "Mon, 27 Feb 2017 12:34:24 GMT"}, {"version": "v4", "created": "Sat, 1 Jul 2017 13:24:08 GMT"}], "update_date": "2017-07-04", "authors_parsed": [["Kolesnikov", "Alexander", ""], ["Lampert", "Christoph H.", ""]]}, {"id": "1612.08230", "submitter": "Lingxi Xie", "authors": "Yuyin Zhou, Lingxi Xie, Wei Shen, Yan Wang, Elliot K. Fishman, Alan L.\n  Yuille", "title": "A Fixed-Point Model for Pancreas Segmentation in Abdominal CT Scans", "comments": "Accepted to MICCAI 2017 (8 pages, 3 figures)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks have been widely adopted for automatic organ\nsegmentation from abdominal CT scans. However, the segmentation accuracy of\nsome small organs (e.g., the pancreas) is sometimes below satisfaction,\narguably because deep networks are easily disrupted by the complex and variable\nbackground regions which occupies a large fraction of the input volume. In this\npaper, we formulate this problem into a fixed-point model which uses a\npredicted segmentation mask to shrink the input region. This is motivated by\nthe fact that a smaller input region often leads to more accurate segmentation.\nIn the training process, we use the ground-truth annotation to generate\naccurate input regions and optimize network weights. On the testing stage, we\nfix the network parameters and update the segmentation results in an iterative\nmanner. We evaluate our approach on the NIH pancreas segmentation dataset, and\noutperform the state-of-the-art by more than 4%, measured by the average\nDice-S{\\o}rensen Coefficient (DSC). In addition, we report 62.43% DSC in the\nworst case, which guarantees the reliability of our approach in clinical\napplications.\n", "versions": [{"version": "v1", "created": "Sun, 25 Dec 2016 02:15:50 GMT"}, {"version": "v2", "created": "Mon, 29 May 2017 07:41:05 GMT"}, {"version": "v3", "created": "Sun, 18 Jun 2017 02:52:24 GMT"}, {"version": "v4", "created": "Wed, 21 Jun 2017 04:00:59 GMT"}], "update_date": "2017-06-22", "authors_parsed": [["Zhou", "Yuyin", ""], ["Xie", "Lingxi", ""], ["Shen", "Wei", ""], ["Wang", "Yan", ""], ["Fishman", "Elliot K.", ""], ["Yuille", "Alan L.", ""]]}, {"id": "1612.08242", "submitter": "Joseph Redmon", "authors": "Joseph Redmon, Ali Farhadi", "title": "YOLO9000: Better, Faster, Stronger", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce YOLO9000, a state-of-the-art, real-time object detection system\nthat can detect over 9000 object categories. First we propose various\nimprovements to the YOLO detection method, both novel and drawn from prior\nwork. The improved model, YOLOv2, is state-of-the-art on standard detection\ntasks like PASCAL VOC and COCO. At 67 FPS, YOLOv2 gets 76.8 mAP on VOC 2007. At\n40 FPS, YOLOv2 gets 78.6 mAP, outperforming state-of-the-art methods like\nFaster RCNN with ResNet and SSD while still running significantly faster.\nFinally we propose a method to jointly train on object detection and\nclassification. Using this method we train YOLO9000 simultaneously on the COCO\ndetection dataset and the ImageNet classification dataset. Our joint training\nallows YOLO9000 to predict detections for object classes that don't have\nlabelled detection data. We validate our approach on the ImageNet detection\ntask. YOLO9000 gets 19.7 mAP on the ImageNet detection validation set despite\nonly having detection data for 44 of the 200 classes. On the 156 classes not in\nCOCO, YOLO9000 gets 16.0 mAP. But YOLO can detect more than just 200 classes;\nit predicts detections for more than 9000 different object categories. And it\nstill runs in real-time.\n", "versions": [{"version": "v1", "created": "Sun, 25 Dec 2016 07:21:38 GMT"}], "update_date": "2016-12-28", "authors_parsed": [["Redmon", "Joseph", ""], ["Farhadi", "Ali", ""]]}, {"id": "1612.08274", "submitter": "Jinho Lee", "authors": "Jinho Lee, Brian Kenji Iwana, Shouta Ide, Seiichi Uchida", "title": "Globally Optimal Object Tracking with Fully Convolutional Networks", "comments": "6pages, 8figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tracking is one of the most important but still difficult tasks in computer\nvision and pattern recognition. The main difficulties in the tracking field are\nappearance variation and occlusion. Most traditional tracking methods set the\nparameters or templates to track target objects in advance and should be\nmodified accordingly. Thus, we propose a new and robust tracking method using a\nFully Convolutional Network (FCN) to obtain an object probability map and\nDynamic Programming (DP) to seek the globally optimal path through all frames\nof video. Our proposed method solves the object appearance variation problem\nwith the use of a FCN and deals with occlusion by DP. We show that our method\nis effective in tracking various single objects through video frames.\n", "versions": [{"version": "v1", "created": "Sun, 25 Dec 2016 16:00:40 GMT"}], "update_date": "2016-12-28", "authors_parsed": [["Lee", "Jinho", ""], ["Iwana", "Brian Kenji", ""], ["Ide", "Shouta", ""], ["Uchida", "Seiichi", ""]]}, {"id": "1612.08354", "submitter": "Gwang Been Park", "authors": "Gwangbeen Park, Woobin Im", "title": "Image-Text Multi-Modal Representation Learning by Adversarial\n  Backpropagation", "comments": "8 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present novel method for image-text multi-modal representation learning.\nIn our knowledge, this work is the first approach of applying adversarial\nlearning concept to multi-modal learning and not exploiting image-text pair\ninformation to learn multi-modal feature. We only use category information in\ncontrast with most previous methods using image-text pair information for\nmulti-modal embedding. In this paper, we show that multi-modal feature can be\nachieved without image-text pair information and our method makes more similar\ndistribution with image and text in multi-modal feature space than other\nmethods which use image-text pair information. And we show our multi-modal\nfeature has universal semantic information, even though it was trained for\ncategory prediction. Our model is end-to-end backpropagation, intuitive and\neasily extended to other multi-modal learning work.\n", "versions": [{"version": "v1", "created": "Mon, 26 Dec 2016 09:51:18 GMT"}], "update_date": "2016-12-28", "authors_parsed": [["Park", "Gwangbeen", ""], ["Im", "Woobin", ""]]}, {"id": "1612.08359", "submitter": "Bahadir Gunturk", "authors": "Shah Rez Khan, Martin Feldman, Bahadir K. Gunturk", "title": "Extracting Sub-Exposure Images from a Single Capture Through\n  Fourier-based Optical Modulation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Through pixel-wise optical coding of images during exposure time, it is\npossible to extract sub-exposure images from a single capture. Such a\ncapability can be used for different purposes, including high-speed imaging,\nhigh-dynamic-range imaging and compressed sensing. In this paper, we\ndemonstrate a sub-exposure image extraction method, where the exposure coding\npattern is inspired from frequency division multiplexing idea of communication\nsystems. The coding masks modulate sub-exposure images in such a way that they\nare placed in non-overlapping regions in Fourier domain. The sub-exposure image\nextraction process involves digital filtering of the captured signal with\nproper band-pass filters. The prototype imaging system incorporates a Liquid\nCrystal over Silicon (LCoS) based spatial light modulator synchronized with a\ncamera for pixel-wise exposure coding.\n", "versions": [{"version": "v1", "created": "Mon, 26 Dec 2016 10:30:05 GMT"}, {"version": "v2", "created": "Tue, 13 Feb 2018 15:08:38 GMT"}], "update_date": "2018-02-14", "authors_parsed": [["Khan", "Shah Rez", ""], ["Feldman", "Martin", ""], ["Gunturk", "Bahadir K.", ""]]}, {"id": "1612.08408", "submitter": "Keke Tang", "authors": "Keke Tang, Peng Song and Xiaoping Chen", "title": "Signature of Geometric Centroids for 3D Local Shape Description and\n  Partial Shape Matching", "comments": "to be published in the Proceedings of the Asian Conference on\n  Computer Vision (ACCV'2016)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Depth scans acquired from different views may contain nuisances such as\nnoise, occlusion, and varying point density. We propose a novel Signature of\nGeometric Centroids descriptor, supporting direct shape matching on the scans,\nwithout requiring any preprocessing such as scan denoising or converting into a\nmesh. First, we construct the descriptor by voxelizing the local shape within a\nuniquely defined local reference frame and concatenating geometric centroid and\npoint density features extracted from each voxel. Second, we compare two\ndescriptors by employing only corresponding voxels that are both non-empty,\nthus supporting matching incomplete local shape such as those close to scan\nboundary. Third, we propose a descriptor saliency measure and compute it from a\ndescriptor-graph to improve shape matching performance. We demonstrate the\ndescriptor's robustness and effectiveness for shape matching by comparing it\nwith three state-of-the-art descriptors, and applying it to object/scene\nreconstruction and 3D object recognition.\n", "versions": [{"version": "v1", "created": "Mon, 26 Dec 2016 15:44:17 GMT"}], "update_date": "2016-12-28", "authors_parsed": [["Tang", "Keke", ""], ["Song", "Peng", ""], ["Chen", "Xiaoping", ""]]}, {"id": "1612.08484", "submitter": "Song Wang", "authors": "Song Wang, Li Sun, Wei Fan, Jun Sun, Satoshi Naoi, Koichi Shirahata,\n  Takuya Fukagai, Yasumoto Tomita and Atsushi Ike", "title": "An Automated CNN Recommendation System for Image Classification Tasks", "comments": "Submitted to ICME 2017 and all the methods in this paper are patented", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nowadays the CNN is widely used in practical applications for image\nclassification task. However the design of the CNN model is very professional\nwork and which is very difficult for ordinary users. Besides, even for experts\nof CNN, to select an optimal model for specific task may still need a lot of\ntime (to train many different models). In order to solve this problem, we\nproposed an automated CNN recommendation system for image classification task.\nOur system is able to evaluate the complexity of the classification task and\nthe classification ability of the CNN model precisely. By using the evaluation\nresults, the system can recommend the optimal CNN model and which can match the\ntask perfectly. The recommendation process of the system is very fast since we\ndon't need any model training. The experiment results proved that the\nevaluation methods are very accurate and reliable.\n", "versions": [{"version": "v1", "created": "Tue, 27 Dec 2016 03:18:28 GMT"}], "update_date": "2016-12-28", "authors_parsed": [["Wang", "Song", ""], ["Sun", "Li", ""], ["Fan", "Wei", ""], ["Sun", "Jun", ""], ["Naoi", "Satoshi", ""], ["Shirahata", "Koichi", ""], ["Fukagai", "Takuya", ""], ["Tomita", "Yasumoto", ""], ["Ike", "Atsushi", ""]]}, {"id": "1612.08499", "submitter": "Lilei Zheng", "authors": "Lilei Zheng, Ying Zhang, Stefan Duffner, Khalid Idrissi, Christophe\n  Garcia, Atilla Baskurt", "title": "End-to-End Data Visualization by Metric Learning and Coordinate\n  Transformation", "comments": "17 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a deep nonlinear metric learning framework for data\nvisualization on an image dataset. We propose the Triangular Similarity and\nprove its equivalence to the Cosine Similarity in measuring a data pair. Based\non this novel similarity, a geometrically motivated loss function - the\ntriangular loss - is then developed for optimizing a metric learning system\ncomprising two identical CNNs. It is shown that this deep nonlinear system can\nbe efficiently trained by a hybrid algorithm based on the conventional\nbackpropagation algorithm. More interestingly, benefiting from classical\nmanifold learning theories, the proposed system offers two different views to\nvisualize the outputs, the second of which provides better classification\nresults than the state-of-the-art methods in the visualizable spaces.\n", "versions": [{"version": "v1", "created": "Tue, 27 Dec 2016 05:03:09 GMT"}], "update_date": "2016-12-28", "authors_parsed": [["Zheng", "Lilei", ""], ["Zhang", "Ying", ""], ["Duffner", "Stefan", ""], ["Idrissi", "Khalid", ""], ["Garcia", "Christophe", ""], ["Baskurt", "Atilla", ""]]}, {"id": "1612.08510", "submitter": "Jian Shi", "authors": "Jian Shi, Yue Dong, Hao Su, Stella X. Yu", "title": "Learning Non-Lambertian Object Intrinsics across ShapeNet Categories", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the non-Lambertian object intrinsic problem of recovering diffuse\nalbedo, shading, and specular highlights from a single image of an object.\n  We build a large-scale object intrinsics database based on existing 3D models\nin the ShapeNet database. Rendered with realistic environment maps, millions of\nsynthetic images of objects and their corresponding albedo, shading, and\nspecular ground-truth images are used to train an encoder-decoder CNN. Once\ntrained, the network can decompose an image into the product of albedo and\nshading components, along with an additive specular component.\n  Our CNN delivers accurate and sharp results in this classical inverse problem\nof computer vision, sharp details attributed to skip layer connections at\ncorresponding resolutions from the encoder to the decoder. Benchmarked on our\nShapeNet and MIT intrinsics datasets, our model consistently outperforms the\nstate-of-the-art by a large margin.\n  We train and test our CNN on different object categories. Perhaps surprising\nespecially from the CNN classification perspective, our intrinsics CNN\ngeneralizes very well across categories. Our analysis shows that feature\nlearning at the encoder stage is more crucial for developing a universal\nrepresentation across categories.\n  We apply our synthetic data trained model to images and videos downloaded\nfrom the internet, and observe robust and realistic intrinsics results. Quality\nnon-Lambertian intrinsics could open up many interesting applications such as\nimage-based albedo and specular editing.\n", "versions": [{"version": "v1", "created": "Tue, 27 Dec 2016 06:38:43 GMT"}], "update_date": "2016-12-28", "authors_parsed": [["Shi", "Jian", ""], ["Dong", "Yue", ""], ["Su", "Hao", ""], ["Yu", "Stella X.", ""]]}, {"id": "1612.08534", "submitter": "Fang Zhao", "authors": "Fang Zhao, Jiashi Feng, Jian Zhao, Wenhan Yang, Shuicheng Yan", "title": "Robust LSTM-Autoencoders for Face De-Occlusion in the Wild", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Face recognition techniques have been developed significantly in recent\nyears. However, recognizing faces with partial occlusion is still challenging\nfor existing face recognizers which is heavily desired in real-world\napplications concerning surveillance and security. Although much research\neffort has been devoted to developing face de-occlusion methods, most of them\ncan only work well under constrained conditions, such as all the faces are from\na pre-defined closed set. In this paper, we propose a robust LSTM-Autoencoders\n(RLA) model to effectively restore partially occluded faces even in the wild.\nThe RLA model consists of two LSTM components, which aims at occlusion-robust\nface encoding and recurrent occlusion removal respectively. The first one,\nnamed multi-scale spatial LSTM encoder, reads facial patches of various scales\nsequentially to output a latent representation, and occlusion-robustness is\nachieved owing to the fact that the influence of occlusion is only upon some of\nthe patches. Receiving the representation learned by the encoder, the LSTM\ndecoder with a dual channel architecture reconstructs the overall face and\ndetects occlusion simultaneously, and by feat of LSTM, the decoder breaks down\nthe task of face de-occlusion into restoring the occluded part step by step.\nMoreover, to minimize identify information loss and guarantee face recognition\naccuracy over recovered faces, we introduce an identity-preserving adversarial\ntraining scheme to further improve RLA. Extensive experiments on both synthetic\nand real datasets of faces with occlusion clearly demonstrate the effectiveness\nof our proposed RLA in removing different types of facial occlusion at various\nlocations. The proposed method also provides significantly larger performance\ngain than other de-occlusion methods in promoting recognition performance over\npartially-occluded faces.\n", "versions": [{"version": "v1", "created": "Tue, 27 Dec 2016 08:36:48 GMT"}], "update_date": "2016-12-28", "authors_parsed": [["Zhao", "Fang", ""], ["Feng", "Jiashi", ""], ["Zhao", "Jian", ""], ["Yang", "Wenhan", ""], ["Yan", "Shuicheng", ""]]}, {"id": "1612.08642", "submitter": "Jaime Delgado Saa", "authors": "Jaime Fernando Delgado Saa, Mujdat Cetin", "title": "Bayesian Nonparametric Models for Synchronous Brain-Computer Interfaces", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV q-bio.NC", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  A brain-computer interface (BCI) is a system that aims for establishing a\nnon-muscular communication path for subjects who had suffer from a\nneurodegenerative disease. Many BCI systems make use of the phenomena of\nevent-related synchronization and de-synchronization of brain waves as a main\nfeature for classification of different cognitive tasks. However, the temporal\ndynamics of the electroencephalographic (EEG) signals contain additional\ninformation that can be incorporated into the inference engine in order to\nimprove the performance of the BCIs. This information about the dynamics of the\nsignals have been exploited previously in BCIs by means of generative and\ndiscriminative methods. In particular, hidden Markov models (HMMs) have been\nused in previous works. These methods have the disadvantage that the model\nparameters such as the number of hidden states and the number of Gaussian\nmixtures need to be fix \"a priori\". In this work, we propose a Bayesian\nnonparametric model for brain signal classification that does not require \"a\npriori\" selection of the number of hidden states and the number of Gaussian\nmixtures of a HMM. The results show that the proposed model outperform other\nmethods based on HMM as well as the winner algorithm of the BCI competition IV.\n", "versions": [{"version": "v1", "created": "Tue, 27 Dec 2016 14:17:20 GMT"}], "update_date": "2016-12-28", "authors_parsed": [["Saa", "Jaime Fernando Delgado", ""], ["Cetin", "Mujdat", ""]]}, {"id": "1612.08712", "submitter": "Aaditya Prakash", "authors": "Aaditya Prakash, Nick Moran, Solomon Garber, Antonella DiLillo and\n  James Storer", "title": "Semantic Perceptual Image Compression using Deep Convolution Networks", "comments": "Accepted to Data Compression Conference, 11 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It has long been considered a significant problem to improve the visual\nquality of lossy image and video compression. Recent advances in computing\npower together with the availability of large training data sets has increased\ninterest in the application of deep learning cnns to address image recognition\nand image processing tasks. Here, we present a powerful cnn tailored to the\nspecific task of semantic image understanding to achieve higher visual quality\nin lossy compression. A modest increase in complexity is incorporated to the\nencoder which allows a standard, off-the-shelf jpeg decoder to be used. While\njpeg encoding may be optimized for generic images, the process is ultimately\nunaware of the specific content of the image to be compressed. Our technique\nmakes jpeg content-aware by designing and training a model to identify multiple\nsemantic regions in a given image. Unlike object detection techniques, our\nmodel does not require labeling of object positions and is able to identify\nobjects in a single pass. We present a new cnn architecture directed\nspecifically to image compression, which generates a map that highlights\nsemantically-salient regions so that they can be encoded at higher quality as\ncompared to background regions. By adding a complete set of features for every\nclass, and then taking a threshold over the sum of all feature activations, we\ngenerate a map that highlights semantically-salient regions so that they can be\nencoded at a better quality compared to background regions. Experiments are\npresented on the Kodak PhotoCD dataset and the MIT Saliency Benchmark dataset,\nin which our algorithm achieves higher visual quality for the same compressed\nsize.\n", "versions": [{"version": "v1", "created": "Tue, 27 Dec 2016 19:21:18 GMT"}, {"version": "v2", "created": "Wed, 29 Mar 2017 16:29:54 GMT"}], "update_date": "2017-03-30", "authors_parsed": [["Prakash", "Aaditya", ""], ["Moran", "Nick", ""], ["Garber", "Solomon", ""], ["DiLillo", "Antonella", ""], ["Storer", "James", ""]]}, {"id": "1612.08780", "submitter": "Hosein M. Golshan", "authors": "Hosein M. Golshan, Adam O. Hebb, Sara J. Hanrahan, Joshua Nedrud,\n  Mohammad H. Mahoor", "title": "An FFT-based Synchronization Approach to Recognize Human Behaviors using\n  STN-LFP Signal", "comments": "IEEE Conf on ICASSP 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Classification of human behavior is key to developing closed-loop Deep Brain\nStimulation (DBS) systems, which may be able to decrease the power consumption\nand side effects of the existing systems. Recent studies have shown that the\nLocal Field Potential (LFP) signals from both Subthalamic Nuclei (STN) of the\nbrain can be used to recognize human behavior. Since the DBS leads implanted in\neach STN can collect three bipolar signals, the selection of a suitable pair of\nLFPs that achieves optimal recognition performance is still an open problem to\naddress. Considering the presence of synchronized aggregate activity in the\nbasal ganglia, this paper presents an FFT-based synchronization approach to\nautomatically select a relevant pair of LFPs and use the pair together with an\nSVM-based MKL classifier for behavior recognition purposes. Our experiments on\nfive subjects show the superiority of the proposed approach compared to other\nmethods used for behavior classification.\n", "versions": [{"version": "v1", "created": "Wed, 28 Dec 2016 01:08:56 GMT"}], "update_date": "2016-12-30", "authors_parsed": [["Golshan", "Hosein M.", ""], ["Hebb", "Adam O.", ""], ["Hanrahan", "Sara J.", ""], ["Nedrud", "Joshua", ""], ["Mahoor", "Mohammad H.", ""]]}, {"id": "1612.08792", "submitter": "Zhihua Ban", "authors": "Zhihua Ban, Jianguo Liu, Li Cao", "title": "Superpixel Segmentation Using Gaussian Mixture Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Superpixel segmentation algorithms are to partition an image into\nperceptually coherence atomic regions by assigning every pixel a superpixel\nlabel. Those algorithms have been wildly used as a preprocessing step in\ncomputer vision works, as they can enormously reduce the number of entries of\nsubsequent algorithms. In this work, we propose an alternative superpixel\nsegmentation method based on Gaussian mixture model (GMM) by assuming that each\nsuperpixel corresponds to a Gaussian distribution, and assuming that each pixel\nis generated by first randomly choosing one distribution from several Gaussian\ndistributions which are defined to be related to that pixel, and then the pixel\nis drawn from the selected distribution. Based on this assumption, each pixel\nis supposed to be drawn from a mixture of Gaussian distributions with unknown\nparameters (GMM). An algorithm based on expectation-maximization method is\napplied to estimate the unknown parameters. Once the unknown parameters are\nobtained, the superpixel label of a pixel is determined by a posterior\nprobability. The success of applying GMM to superpixel segmentation depends on\nthe two major differences between the traditional GMM-based clustering and the\nproposed one: data points in our model may be non-identically distributed, and\nwe present an approach to control the shape of the estimated Gaussian functions\nby adjusting their covariance matrices. Our method is of linear complexity with\nrespect to the number of pixels. The proposed algorithm is inherently parallel\nand can get faster speed by adding simple OpenMP directives to our\nimplementation. According to our experiments, our algorithm outperforms the\nstate-of-the-art superpixel algorithms in accuracy and presents a competitive\nperformance in computational efficiency.\n", "versions": [{"version": "v1", "created": "Wed, 28 Dec 2016 02:58:41 GMT"}, {"version": "v2", "created": "Tue, 21 Feb 2017 02:28:15 GMT"}], "update_date": "2017-02-22", "authors_parsed": [["Ban", "Zhihua", ""], ["Liu", "Jianguo", ""], ["Cao", "Li", ""]]}, {"id": "1612.08796", "submitter": "N Vinay Kumar", "authors": "D. S. Guru and N. Vinay Kumar", "title": "Symbolic Representation and Classification of Logos", "comments": "15 pages, 6 figures, 6 tables, Proceedings of International\n  Conference on Computer Vision and Image Processing (CVIP 2016). arXiv admin\n  note: text overlap with arXiv:1609.01414", "journal-ref": null, "doi": "10.1007/978-981-10-2104-6_50", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, a model for classification of logos based on symbolic\nrepresentation of features is presented. The proposed model makes use of global\nfeatures of logo images such as color, texture, and shape features for\nclassification. The logo images are broadly classified into three different\nclasses, viz., logo image containing only text, an image with only symbol, and\nan image with both text and a symbol. In each class, the similar looking logo\nimages are clustered using K-means clustering algorithm. The intra-cluster\nvariations present in each cluster corresponding to each class are then\npreserved using symbolic interval data. Thus referenced logo images are\nrepresented in the form of interval data. A sample logo image is then\nclassified using suitable symbolic classifier. For experimentation purpose,\nrelatively large amount of color logo images is created consisting of 5044 logo\nimages. The classification results are validated with the help of accuracy,\nprecision, recall, F-measure, and time. To check the efficacy of the proposed\nmodel, the comparative analyses are given against the other models. The results\nshow that the proposed model outperforms the other models with respect to time\nand F-measure.\n", "versions": [{"version": "v1", "created": "Wed, 28 Dec 2016 04:06:10 GMT"}], "update_date": "2016-12-30", "authors_parsed": [["Guru", "D. S.", ""], ["Kumar", "N. Vinay", ""]]}, {"id": "1612.08820", "submitter": "Xiahai Zhuang", "authors": "Xiahai Zhuang", "title": "Multivariate mixture model for myocardium segmentation combining\n  multi-source images", "comments": null, "journal-ref": "IEEE Transactions on Pattern Analysis and Machine Intelligence\n  2019", "doi": "10.1109/TPAMI.2018.2869576", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a method for simultaneous segmentation of multi-source\nimages, using the multivariate mixture model (MvMM) and maximum of\nlog-likelihood (LL) framework. The segmentation is a procedure of texture\nclassification, and the MvMM is used to model the joint intensity distribution\nof the images. Specifically, the method is applied to the myocardial\nsegmentation combining the complementary texture information from\nmulti-sequence (MS) cardiac magnetic resonance (CMR) images. Furthermore, there\nexist inter-image mis-registration and intra-image misalignment of slices in\nthe MS CMR images. Hence, the MvMM is formulated with transformations, which\nare embedded into the LL framework and optimized simultaneously with the\nsegmentation parameters. The proposed method is able to correct the inter- and\nintra-image misalignment by registering each slice of the MS CMR to a virtual\ncommon space, as well as to delineate the indistinguishable boundaries of\nmyocardium consisting of pathologies. Results have shown statistically\nsignificant improvement in the segmentation performance of the proposed method\nwith respect to the conventional approaches which can solely segment each image\nseparately. The proposed method has also demonstrated better robustness in the\nincongruent data, where some images may not fully cover the region of interest\nand the full coverage can only be reconstructed combining the images from\nmultiple sources.\n", "versions": [{"version": "v1", "created": "Wed, 28 Dec 2016 07:44:17 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Zhuang", "Xiahai", ""]]}, {"id": "1612.08825", "submitter": "Alexander Amini", "authors": "Alexander Amini, Berthold Horn, Alan Edelman", "title": "Accelerated Convolutions for Efficient Multi-Scale Time to Contact\n  Computation in Julia", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutions have long been regarded as fundamental to applied mathematics,\nphysics and engineering. Their mathematical elegance allows for common tasks\nsuch as numerical differentiation to be computed efficiently on large data\nsets. Efficient computation of convolutions is critical to artificial\nintelligence in real-time applications, like machine vision, where convolutions\nmust be continuously and efficiently computed on tens to hundreds of kilobytes\nper second. In this paper, we explore how convolutions are used in fundamental\nmachine vision applications. We present an accelerated n-dimensional\nconvolution package in the high performance computing language, Julia, and\ndemonstrate its efficacy in solving the time to contact problem for machine\nvision. Results are measured against synthetically generated videos and\nquantitatively assessed according to their mean squared error from the ground\ntruth. We achieve over an order of magnitude decrease in compute time and\nallocated memory for comparable machine vision applications. All code is\npackaged and integrated into the official Julia Package Manager to be used in\nvarious other scenarios.\n", "versions": [{"version": "v1", "created": "Wed, 28 Dec 2016 08:46:21 GMT"}], "update_date": "2016-12-30", "authors_parsed": [["Amini", "Alexander", ""], ["Horn", "Berthold", ""], ["Edelman", "Alan", ""]]}, {"id": "1612.08843", "submitter": "Hexiang Hu", "authors": "Hexiang Hu, Shiyi Lan, Yuning Jiang, Zhimin Cao, Fei Sha", "title": "FastMask: Segment Multi-scale Object Candidates in One Shot", "comments": "Accepted as CVPR 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Objects appear to scale differently in natural images. This fact requires\nmethods dealing with object-centric tasks (e.g. object proposal) to have robust\nperformance over variances in object scales. In the paper, we present a novel\nsegment proposal framework, namely FastMask, which takes advantage of\nhierarchical features in deep convolutional neural networks to segment\nmulti-scale objects in one shot. Innovatively, we adapt segment proposal\nnetwork into three different functional components (body, neck and head). We\nfurther propose a weight-shared residual neck module as well as a\nscale-tolerant attentional head module for efficient one-shot inference. On MS\nCOCO benchmark, the proposed FastMask outperforms all state-of-the-art segment\nproposal methods in average recall being 2~5 times faster. Moreover, with a\nslight trade-off in accuracy, FastMask can segment objects in near real time\n(~13 fps) with 800*600 resolution images, demonstrating its potential in\npractical applications. Our implementation is available on\nhttps://github.com/voidrank/FastMask.\n", "versions": [{"version": "v1", "created": "Wed, 28 Dec 2016 10:24:42 GMT"}, {"version": "v2", "created": "Fri, 30 Dec 2016 08:03:28 GMT"}, {"version": "v3", "created": "Mon, 16 Jan 2017 06:46:35 GMT"}, {"version": "v4", "created": "Tue, 11 Apr 2017 21:20:57 GMT"}], "update_date": "2017-04-13", "authors_parsed": [["Hu", "Hexiang", ""], ["Lan", "Shiyi", ""], ["Jiang", "Yuning", ""], ["Cao", "Zhimin", ""], ["Sha", "Fei", ""]]}, {"id": "1612.08871", "submitter": "David Nilsson", "authors": "David Nilsson, Cristian Sminchisescu", "title": "Semantic Video Segmentation by Gated Recurrent Flow Propagation", "comments": "The experiments section is extended compared to the previous version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semantic video segmentation is challenging due to the sheer amount of data\nthat needs to be processed and labeled in order to construct accurate models.\nIn this paper we present a deep, end-to-end trainable methodology to video\nsegmentation that is capable of leveraging information present in unlabeled\ndata in order to improve semantic estimates. Our model combines a convolutional\narchitecture and a spatio-temporal transformer recurrent layer that are able to\ntemporally propagate labeling information by means of optical flow, adaptively\ngated based on its locally estimated uncertainty. The flow, the recognition and\nthe gated temporal propagation modules can be trained jointly, end-to-end. The\ntemporal, gated recurrent flow propagation component of our model can be\nplugged into any static semantic segmentation architecture and turn it into a\nweakly supervised video processing one. Our extensive experiments in the\nchallenging CityScapes and Camvid datasets, and based on multiple deep\narchitectures, indicate that the resulting model can leverage unlabeled\ntemporal frames, next to a labeled one, in order to improve both the video\nsegmentation accuracy and the consistency of its temporal labeling, at no\nadditional annotation cost and with little extra computation.\n", "versions": [{"version": "v1", "created": "Wed, 28 Dec 2016 12:50:39 GMT"}, {"version": "v2", "created": "Mon, 2 Oct 2017 07:52:42 GMT"}], "update_date": "2017-10-03", "authors_parsed": [["Nilsson", "David", ""], ["Sminchisescu", "Cristian", ""]]}, {"id": "1612.08879", "submitter": "DaoYu Lin", "authors": "Daoyu Lin, Kun Fu, Yang Wang, Guangluan Xu, Xian Sun", "title": "MARTA GANs: Unsupervised Representation Learning for Remote Sensing\n  Image Classification", "comments": "IEEE GRSL", "journal-ref": "IEEE Geoscience and Remote Sensing Letters ( Volume: 14, Issue:\n  11, Nov. 2017 )", "doi": "10.1109/LGRS.2017.2752750", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the development of deep learning, supervised learning has frequently\nbeen adopted to classify remotely sensed images using convolutional networks\n(CNNs). However, due to the limited amount of labeled data available,\nsupervised learning is often difficult to carry out. Therefore, we proposed an\nunsupervised model called multiple-layer feature-matching generative\nadversarial networks (MARTA GANs) to learn a representation using only\nunlabeled data. MARTA GANs consists of both a generative model $G$ and a\ndiscriminative model $D$. We treat $D$ as a feature extractor. To fit the\ncomplex properties of remote sensing data, we use a fusion layer to merge the\nmid-level and global features. $G$ can produce numerous images that are similar\nto the training data; therefore, $D$ can learn better representations of\nremotely sensed images using the training data provided by $G$. The\nclassification results on two widely used remote sensing image databases show\nthat the proposed method significantly improves the classification performance\ncompared with other state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Wed, 28 Dec 2016 13:24:10 GMT"}, {"version": "v2", "created": "Thu, 2 Nov 2017 14:12:31 GMT"}, {"version": "v3", "created": "Tue, 21 Nov 2017 07:15:41 GMT"}], "update_date": "2017-11-22", "authors_parsed": [["Lin", "Daoyu", ""], ["Fu", "Kun", ""], ["Wang", "Yang", ""], ["Xu", "Guangluan", ""], ["Sun", "Xian", ""]]}, {"id": "1612.08894", "submitter": "Konstantinos Kamnitsas", "authors": "Konstantinos Kamnitsas, Christian Baumgartner, Christian Ledig,\n  Virginia F.J. Newcombe, Joanna P. Simpson, Andrew D. Kane, David K. Menon,\n  Aditya Nori, Antonio Criminisi, Daniel Rueckert, Ben Glocker", "title": "Unsupervised domain adaptation in brain lesion segmentation with\n  adversarial networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Significant advances have been made towards building accurate automatic\nsegmentation systems for a variety of biomedical applications using machine\nlearning. However, the performance of these systems often degrades when they\nare applied on new data that differ from the training data, for example, due to\nvariations in imaging protocols. Manually annotating new data for each test\ndomain is not a feasible solution. In this work we investigate unsupervised\ndomain adaptation using adversarial neural networks to train a segmentation\nmethod which is more invariant to differences in the input data, and which does\nnot require any annotations on the test domain. Specifically, we learn\ndomain-invariant features by learning to counter an adversarial network, which\nattempts to classify the domain of the input data by observing the activations\nof the segmentation network. Furthermore, we propose a multi-connected domain\ndiscriminator for improved adversarial training. Our system is evaluated using\ntwo MR databases of subjects with traumatic brain injuries, acquired using\ndifferent scanners and imaging protocols. Using our unsupervised approach, we\nobtain segmentation accuracies which are close to the upper bound of supervised\ndomain adaptation.\n", "versions": [{"version": "v1", "created": "Wed, 28 Dec 2016 14:23:34 GMT"}], "update_date": "2016-12-30", "authors_parsed": [["Kamnitsas", "Konstantinos", ""], ["Baumgartner", "Christian", ""], ["Ledig", "Christian", ""], ["Newcombe", "Virginia F. J.", ""], ["Simpson", "Joanna P.", ""], ["Kane", "Andrew D.", ""], ["Menon", "David K.", ""], ["Nori", "Aditya", ""], ["Criminisi", "Antonio", ""], ["Rueckert", "Daniel", ""], ["Glocker", "Ben", ""]]}, {"id": "1612.08927", "submitter": "Asad Khan Mr.", "authors": "Asad Khan, Luo Jiang, Wei Li, Ligang Liu", "title": "Fast color transfer from multiple images", "comments": "arXiv admin note: text overlap with arXiv:1610.04861", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Color transfer between images uses the statistics information of image\neffectively. We present a novel approach of local color transfer between images\nbased on the simple statistics and locally linear embedding. A sketching\ninterface is proposed for quickly and easily specifying the color\ncorrespondences between target and source image. The user can specify the\ncorrespondences of local region using scribes, which more accurately transfers\nthe target color to the source image while smoothly preserving the boundaries,\nand exhibits more natural output results. Our algorithm is not restricted to\none-to-one image color transfer and can make use of more than one target images\nto transfer the color in different regions in the source image. Moreover, our\nalgorithm does not require to choose the same color style and image size\nbetween source and target images. We propose the sub-sampling to reduce the\ncomputational load. Comparing with other approaches, our algorithm is much\nbetter in color blending in the input data. Our approach preserves the other\ncolor details in the source image. Various experimental results show that our\napproach specifies the correspondences of local color region in source and\ntarget images. And it expresses the intention of users and generates more\nactual and natural results of visual effect.\n", "versions": [{"version": "v1", "created": "Wed, 28 Dec 2016 16:50:55 GMT"}], "update_date": "2016-12-30", "authors_parsed": [["Khan", "Asad", ""], ["Jiang", "Luo", ""], ["Li", "Wei", ""], ["Liu", "Ligang", ""]]}, {"id": "1612.08936", "submitter": "Alina Zare", "authors": "Chao Chen, Alina Zare, Huy Trinh, Gbeng Omotara, J. Tory Cobb,\n  Timotius Lagaunne", "title": "Partial Membership Latent Dirichlet Allocation", "comments": "Version 1, Sent for Review. arXiv admin note: substantial text\n  overlap with arXiv:1511.02821", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Topic models (e.g., pLSA, LDA, sLDA) have been widely used for segmenting\nimagery. However, these models are confined to crisp segmentation, forcing a\nvisual word (i.e., an image patch) to belong to one and only one topic. Yet,\nthere are many images in which some regions cannot be assigned a crisp\ncategorical label (e.g., transition regions between a foggy sky and the ground\nor between sand and water at a beach). In these cases, a visual word is best\nrepresented with partial memberships across multiple topics. To address this,\nwe present a partial membership latent Dirichlet allocation (PM-LDA) model and\nan associated parameter estimation algorithm. This model can be useful for\nimagery where a visual word may be a mixture of multiple topics. Experimental\nresults on visual and sonar imagery show that PM-LDA can produce both crisp and\nsoft semantic image segmentations; a capability previous topic modeling methods\ndo not have.\n", "versions": [{"version": "v1", "created": "Wed, 28 Dec 2016 17:32:52 GMT"}], "update_date": "2016-12-30", "authors_parsed": [["Chen", "Chao", ""], ["Zare", "Alina", ""], ["Trinh", "Huy", ""], ["Omotara", "Gbeng", ""], ["Cobb", "J. Tory", ""], ["Lagaunne", "Timotius", ""]]}, {"id": "1612.09030", "submitter": "Vikas Garg", "authors": "Vikas K. Garg and Adam Tauman Kalai", "title": "Meta-Unsupervised-Learning: A supervised approach to unsupervised\n  learning", "comments": "22 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new paradigm to investigate unsupervised learning, reducing\nunsupervised learning to supervised learning. Specifically, we mitigate the\nsubjectivity in unsupervised decision-making by leveraging knowledge acquired\nfrom prior, possibly heterogeneous, supervised learning tasks. We demonstrate\nthe versatility of our framework via comprehensive expositions and detailed\nexperiments on several unsupervised problems such as (a) clustering, (b)\noutlier detection, and (c) similarity prediction under a common umbrella of\nmeta-unsupervised-learning. We also provide rigorous PAC-agnostic bounds to\nestablish the theoretical foundations of our framework, and show that our\nframing of meta-clustering circumvents Kleinberg's impossibility theorem for\nclustering.\n", "versions": [{"version": "v1", "created": "Thu, 29 Dec 2016 03:20:33 GMT"}, {"version": "v2", "created": "Tue, 3 Jan 2017 17:34:39 GMT"}], "update_date": "2017-01-04", "authors_parsed": [["Garg", "Vikas K.", ""], ["Kalai", "Adam Tauman", ""]]}, {"id": "1612.09134", "submitter": "Antonio Manuel Lopez Pe\\~na", "authors": "Antonio M. Lopez, Jiaolong Xu, Jose L. Gomez, David Vazquez, German\n  Ros", "title": "From Virtual to Real World Visual Perception using Domain Adaptation --\n  The DPM as Example", "comments": "Invited book chapter to appear in \"Domain Adaptation in Computer\n  Vision Applications\", Springer Series: Advances in Computer Vision and\n  Pattern Recognition, Edited by Gabriela Csurka", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Supervised learning tends to produce more accurate classifiers than\nunsupervised learning in general. This implies that training data is preferred\nwith annotations. When addressing visual perception challenges, such as\nlocalizing certain object classes within an image, the learning of the involved\nclassifiers turns out to be a practical bottleneck. The reason is that, at\nleast, we have to frame object examples with bounding boxes in thousands of\nimages. A priori, the more complex the model is regarding its number of\nparameters, the more annotated examples are required. This annotation task is\nperformed by human oracles, which ends up in inaccuracies and errors in the\nannotations (aka ground truth) since the task is inherently very cumbersome and\nsometimes ambiguous. As an alternative we have pioneered the use of virtual\nworlds for collecting such annotations automatically and with high precision.\nHowever, since the models learned with virtual data must operate in the real\nworld, we still need to perform domain adaptation (DA). In this chapter we\nrevisit the DA of a deformable part-based model (DPM) as an exemplifying case\nof virtual- to-real-world DA. As a use case, we address the challenge of\nvehicle detection for driver assistance, using different publicly available\nvirtual-world data. While doing so, we investigate questions such as: how does\nthe domain gap behave due to virtual-vs-real data with respect to dominant\nobject appearance per domain, as well as the role of photo-realism in the\nvirtual world.\n", "versions": [{"version": "v1", "created": "Thu, 29 Dec 2016 13:16:22 GMT"}], "update_date": "2016-12-30", "authors_parsed": [["Lopez", "Antonio M.", ""], ["Xu", "Jiaolong", ""], ["Gomez", "Jose L.", ""], ["Vazquez", "David", ""], ["Ros", "German", ""]]}, {"id": "1612.09161", "submitter": "Laurens van der Maaten", "authors": "Ang Li and Allan Jabri and Armand Joulin and Laurens van der Maaten", "title": "Learning Visual N-Grams from Web Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Real-world image recognition systems need to recognize tens of thousands of\nclasses that constitute a plethora of visual concepts. The traditional approach\nof annotating thousands of images per class for training is infeasible in such\na scenario, prompting the use of webly supervised data. This paper explores the\ntraining of image-recognition systems on large numbers of images and associated\nuser comments. In particular, we develop visual n-gram models that can predict\narbitrary phrases that are relevant to the content of an image. Our visual\nn-gram models are feed-forward convolutional networks trained using new loss\nfunctions that are inspired by n-gram models commonly used in language\nmodeling. We demonstrate the merits of our models in phrase prediction,\nphrase-based image retrieval, relating images and captions, and zero-shot\ntransfer.\n", "versions": [{"version": "v1", "created": "Thu, 29 Dec 2016 14:50:53 GMT"}, {"version": "v2", "created": "Sun, 6 Aug 2017 01:59:22 GMT"}], "update_date": "2017-08-08", "authors_parsed": [["Li", "Ang", ""], ["Jabri", "Allan", ""], ["Joulin", "Armand", ""], ["van der Maaten", "Laurens", ""]]}, {"id": "1612.09199", "submitter": "Mahajabin Rahman", "authors": "Mahajabin Rahman, Davi Geiger", "title": "Quantum Clustering and Gaussian Mixtures", "comments": "16 pages, 13 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The mixture of Gaussian distributions, a soft version of k-means , is\nconsidered a state-of-the-art clustering algorithm. It is widely used in\ncomputer vision for selecting classes, e.g., color, texture, and shapes. In\nthis algorithm, each class is described by a Gaussian distribution, defined by\nits mean and covariance. The data is described by a weighted sum of these\nGaussian distributions. We propose a new method, inspired by quantum\ninterference in physics. Instead of modeling each class distribution directly,\nwe model a class wave function such that its magnitude square is the class\nGaussian distribution. We then mix the class wave functions to create the\nmixture wave function. The final mixture distribution is then the magnitude\nsquare of the mixture wave function. As a result, we observe the quantum class\ninterference phenomena, not present in the Gaussian mixture model. We show that\nthe quantum method outperforms the Gaussian mixture method in every aspect of\nthe estimations. It provides more accurate estimations of all distribution\nparameters, with much less fluctuations, and it is also more robust to data\ndeformations from the Gaussian assumptions. We illustrate our method for color\nsegmentation as an example application.\n", "versions": [{"version": "v1", "created": "Thu, 29 Dec 2016 16:55:49 GMT"}], "update_date": "2016-12-30", "authors_parsed": [["Rahman", "Mahajabin", ""], ["Geiger", "Davi", ""]]}, {"id": "1612.09322", "submitter": "Hang Su", "authors": "Hang Su, Xiatian Zhu, Shaogang Gong", "title": "Deep Learning Logo Detection with Data Expansion by Synthesising Context", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Logo detection in unconstrained images is challenging, particularly when only\nvery sparse labelled training images are accessible due to high labelling\ncosts. In this work, we describe a model training image synthesising method\ncapable of improving significantly logo detection performance when only a\nhandful of (e.g., 10) labelled training images captured in realistic context\nare available, avoiding extensive manual labelling costs. Specifically, we\ndesign a novel algorithm for generating Synthetic Context Logo (SCL) training\nimages to increase model robustness against unknown background clutters,\nresulting in superior logo detection performance. For benchmarking model\nperformance, we introduce a new logo detection dataset TopLogo-10 collected\nfrom top 10 most popular clothing/wearable brandname logos captured in rich\nvisual context. Extensive comparisons show the advantages of our proposed SCL\nmodel over the state-of-the-art alternatives for logo detection using two\nreal-world logo benchmark datasets: FlickrLogo-32 and our new TopLogo-10.\n", "versions": [{"version": "v1", "created": "Thu, 29 Dec 2016 21:48:22 GMT"}, {"version": "v2", "created": "Wed, 4 Jan 2017 12:30:56 GMT"}, {"version": "v3", "created": "Fri, 16 Mar 2018 10:04:15 GMT"}], "update_date": "2018-03-19", "authors_parsed": [["Su", "Hang", ""], ["Zhu", "Xiatian", ""], ["Gong", "Shaogang", ""]]}, {"id": "1612.09346", "submitter": "Diego Marcos", "authors": "Diego Marcos, Michele Volpi, Nikos Komodakis and Devis Tuia", "title": "Rotation equivariant vector field networks", "comments": "10 pages, accepted at ICCV 2017", "journal-ref": null, "doi": "10.1109/ICCV.2017.540", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many computer vision tasks, we expect a particular behavior of the output\nwith respect to rotations of the input image. If this relationship is\nexplicitly encoded, instead of treated as any other variation, the complexity\nof the problem is decreased, leading to a reduction in the size of the required\nmodel. In this paper, we propose the Rotation Equivariant Vector Field Networks\n(RotEqNet), a Convolutional Neural Network (CNN) architecture encoding rotation\nequivariance, invariance and covariance. Each convolutional filter is applied\nat multiple orientations and returns a vector field representing magnitude and\nangle of the highest scoring orientation at every spatial location. We develop\na modified convolution operator relying on this representation to obtain deep\narchitectures. We test RotEqNet on several problems requiring different\nresponses with respect to the inputs' rotation: image classification,\nbiomedical image segmentation, orientation estimation and patch matching. In\nall cases, we show that RotEqNet offers extremely compact models in terms of\nnumber of parameters and provides results in line to those of networks orders\nof magnitude larger.\n", "versions": [{"version": "v1", "created": "Thu, 29 Dec 2016 23:42:49 GMT"}, {"version": "v2", "created": "Mon, 20 Mar 2017 17:30:01 GMT"}, {"version": "v3", "created": "Fri, 25 Aug 2017 12:26:43 GMT"}], "update_date": "2018-07-06", "authors_parsed": [["Marcos", "Diego", ""], ["Volpi", "Michele", ""], ["Komodakis", "Nikos", ""], ["Tuia", "Devis", ""]]}, {"id": "1612.09401", "submitter": "Pichao Wang", "authors": "Pichao Wang and Wanqing Li and Chuankun Li and Yonghong Hou", "title": "Action Recognition Based on Joint Trajectory Maps with Convolutional\n  Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional Neural Networks (ConvNets) have recently shown promising\nperformance in many computer vision tasks, especially image-based recognition.\nHow to effectively apply ConvNets to sequence-based data is still an open\nproblem. This paper proposes an effective yet simple method to represent\nspatio-temporal information carried in $3D$ skeleton sequences into three $2D$\nimages by encoding the joint trajectories and their dynamics into color\ndistribution in the images, referred to as Joint Trajectory Maps (JTM), and\nadopts ConvNets to learn the discriminative features for human action\nrecognition. Such an image-based representation enables us to fine-tune\nexisting ConvNets models for the classification of skeleton sequences without\ntraining the networks afresh. The three JTMs are generated in three orthogonal\nplanes and provide complimentary information to each other. The final\nrecognition is further improved through multiply score fusion of the three\nJTMs. The proposed method was evaluated on four public benchmark datasets, the\nlarge NTU RGB+D Dataset, MSRC-12 Kinect Gesture Dataset (MSRC-12), G3D Dataset\nand UTD Multimodal Human Action Dataset (UTD-MHAD) and achieved the\nstate-of-the-art results.\n", "versions": [{"version": "v1", "created": "Fri, 30 Dec 2016 06:32:38 GMT"}], "update_date": "2017-01-02", "authors_parsed": [["Wang", "Pichao", ""], ["Li", "Wanqing", ""], ["Li", "Chuankun", ""], ["Hou", "Yonghong", ""]]}, {"id": "1612.09411", "submitter": "Arnav Bhavsar", "authors": "Arnav Bhavsar", "title": "Shape Estimation from Defocus Cue for Microscopy Images via Belief\n  Propagation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, the usefulness of 3D shape estimation is being realized in\nmicroscopic or close-range imaging, as the 3D information can further be used\nin various applications. Due to limited depth of field at such small distances,\nthe defocus blur induced in images can provide information about the 3D shape\nof the object. The task of `shape from defocus' (SFD), involves the problem of\nestimating good quality 3D shape estimates from images with depth-dependent\ndefocus blur. While the research area of SFD is quite well-established, the\napproaches have largely demonstrated results on objects with bulk/coarse shape\nvariation. However, in many cases, objects studied under microscopes often\ninvolve fine/detailed structures, which have not been explicitly considered in\nmost methods. In addition, given that, in recent years, large data volumes are\ntypically associated with microscopy related applications, it is also important\nfor such SFD methods to be efficient. In this work, we provide an indication of\nthe usefulness of the Belief Propagation (BP) approach in addressing these\nconcerns for SFD. BP has been known to be an efficient combinatorial\noptimization approach, and has been empirically demonstrated to yield good\nquality solutions in low-level vision problems such as image restoration,\nstereo disparity estimation etc. For exploiting the efficiency of BP in SFD, we\nassume local space-invariance of the defocus blur, which enables the\napplication of BP in a straightforward manner. Even with such an assumption,\nthe ability of BP to provide good quality solutions while using non-convex\npriors, reflects in yielding plausible shape estimates in presence of fine\nstructures on the objects under microscopy imaging.\n", "versions": [{"version": "v1", "created": "Fri, 30 Dec 2016 07:41:25 GMT"}], "update_date": "2017-01-02", "authors_parsed": [["Bhavsar", "Arnav", ""]]}, {"id": "1612.09420", "submitter": "Fahime Sheikhzadeh", "authors": "Fahime Sheikhzadeh, Martial Guillaud, Rabab K. Ward", "title": "Automatic labeling of molecular biomarkers of whole slide\n  immunohistochemistry images using fully convolutional networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.TO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the problem of quantifying biomarkers in multi-stained\ntissues, based on color and spatial information. A deep learning based method\nthat can automatically localize and quantify the cells expressing biomarker(s)\nin a whole slide image is proposed. The deep learning network is a fully\nconvolutional network (FCN) whose input is the true RGB color image of a tissue\nand output is a map of the different biomarkers. The FCN relies on a\nconvolutional neural network (CNN) that classifies each cell separately\naccording to the biomarker it expresses. In this study, images of\nimmunohistochemistry (IHC) stained slides were collected and used. More than\n4,500 RGB images of cells were manually labeled based on the expressing\nbiomarkers. The labeled cell images were used to train the CNN (obtaining an\naccuracy of 92% in a test set). The trained CNN is then extended to an FCN that\ngenerates a map of all biomarkers in the whole slide image acquired by the\nscanner (instead of classifying every cell image). To evaluate our method, we\nmanually labeled all nuclei expressing different biomarkers in two whole slide\nimages and used theses as the ground truth. Our proposed method for\nimmunohistochemical analysis compares well with the manual labeling by humans\n(average F-score of 0.96).\n", "versions": [{"version": "v1", "created": "Fri, 30 Dec 2016 08:27:04 GMT"}], "update_date": "2017-01-02", "authors_parsed": [["Sheikhzadeh", "Fahime", ""], ["Guillaud", "Martial", ""], ["Ward", "Rabab K.", ""]]}, {"id": "1612.09506", "submitter": "Mundher Al-Shabi", "authors": "Tee Connie, Mundher Al-Shabi, Michael Goh", "title": "Smart Content Recognition from Images Using a Mixture of Convolutional\n  Neural Networks", "comments": "To be published in LNEE, Code: github.com/mundher/NSFW", "journal-ref": null, "doi": "10.1007/978-981-10-6451-7_2", "report-no": null, "categories": "stat.ML cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With rapid development of the Internet, web contents become huge. Most of the\nwebsites are publicly available, and anyone can access the contents from\nanywhere such as workplace, home and even schools. Nevertheless, not all the\nweb contents are appropriate for all users, especially children. An example of\nthese contents is pornography images which should be restricted to certain age\ngroup. Besides, these images are not safe for work (NSFW) in which employees\nshould not be seen accessing such contents during work. Recently, convolutional\nneural networks have been successfully applied to many computer vision\nproblems. Inspired by these successes, we propose a mixture of convolutional\nneural networks for adult content recognition. Unlike other works, our method\nis formulated on a weighted sum of multiple deep neural network models. The\nweights of each CNN models are expressed as a linear regression problem learned\nusing Ordinary Least Squares (OLS). Experimental results demonstrate that the\nproposed model outperforms both single CNN model and the average sum of CNN\nmodels in adult content recognition.\n", "versions": [{"version": "v1", "created": "Fri, 30 Dec 2016 15:18:39 GMT"}, {"version": "v2", "created": "Fri, 14 Jul 2017 09:03:57 GMT"}], "update_date": "2019-04-15", "authors_parsed": [["Connie", "Tee", ""], ["Al-Shabi", "Mundher", ""], ["Goh", "Michael", ""]]}, {"id": "1612.09508", "submitter": "Te-Lin Wu", "authors": "Amir R. Zamir, Te-Lin Wu, Lin Sun, William Shen, Jitendra Malik,\n  Silvio Savarese", "title": "Feedback Networks", "comments": "See a video describing the method at https://youtu.be/MY5Uhv38Ttg and\n  the website at http://feedbacknet.stanford.edu/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Currently, the most successful learning models in computer vision are based\non learning successive representations followed by a decision layer. This is\nusually actualized through feedforward multilayer neural networks, e.g.\nConvNets, where each layer forms one of such successive representations.\nHowever, an alternative that can achieve the same goal is a feedback based\napproach in which the representation is formed in an iterative manner based on\na feedback received from previous iteration's output.\n  We establish that a feedback based approach has several fundamental\nadvantages over feedforward: it enables making early predictions at the query\ntime, its output naturally conforms to a hierarchical structure in the label\nspace (e.g. a taxonomy), and it provides a new basis for Curriculum Learning.\nWe observe that feedback networks develop a considerably different\nrepresentation compared to feedforward counterparts, in line with the\naforementioned advantages. We put forth a general feedback based learning\narchitecture with the endpoint results on par or better than existing\nfeedforward networks with the addition of the above advantages. We also\ninvestigate several mechanisms in feedback architectures (e.g. skip connections\nin time) and design choices (e.g. feedback length). We hope this study offers\nnew perspectives in quest for more natural and practical learning models.\n", "versions": [{"version": "v1", "created": "Fri, 30 Dec 2016 15:39:45 GMT"}, {"version": "v2", "created": "Fri, 13 Jan 2017 04:26:49 GMT"}, {"version": "v3", "created": "Sun, 20 Aug 2017 07:15:55 GMT"}], "update_date": "2017-08-22", "authors_parsed": [["Zamir", "Amir R.", ""], ["Wu", "Te-Lin", ""], ["Sun", "Lin", ""], ["Shen", "William", ""], ["Malik", "Jitendra", ""], ["Savarese", "Silvio", ""]]}, {"id": "1612.09524", "submitter": "Hamza Bendaoudi", "authors": "Hamza Bendaoudi, Farida Cheriet and J. M. Pierre Langlois", "title": "Memory Efficient Multi-Scale Line Detector Architecture for Retinal\n  Blood Vessel Segmentation", "comments": "This paper was accepted and presented at Conference on Design and\n  Architectures for Signal and Image Processing - DASIP 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a memory efficient architecture that implements the\nMulti-Scale Line Detector (MSLD) algorithm for real-time retinal blood vessel\ndetection in fundus images on a Zynq FPGA. This implementation benefits from\nthe FPGA parallelism to drastically reduce the memory requirements of the MSLD\nfrom two images to a few values. The architecture is optimized in terms of\nresource utilization by reusing the computations and optimizing the bit-width.\nThe throughput is increased by designing fully pipelined functional units. The\narchitecture is capable of achieving a comparable accuracy to its software\nimplementation but 70x faster for low resolution images. For high resolution\nimages, it achieves an acceleration by a factor of 323x.\n", "versions": [{"version": "v1", "created": "Tue, 6 Dec 2016 19:06:28 GMT"}], "update_date": "2017-01-02", "authors_parsed": [["Bendaoudi", "Hamza", ""], ["Cheriet", "Farida", ""], ["Langlois", "J. M. Pierre", ""]]}, {"id": "1612.09542", "submitter": "Licheng Yu", "authors": "Licheng Yu, Hao Tan, Mohit Bansal, Tamara L. Berg", "title": "A Joint Speaker-Listener-Reinforcer Model for Referring Expressions", "comments": "Some typo fixed; comprehension results on refcocog updated; more\n  human evaluation results added", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Referring expressions are natural language constructions used to identify\nparticular objects within a scene. In this paper, we propose a unified\nframework for the tasks of referring expression comprehension and generation.\nOur model is composed of three modules: speaker, listener, and reinforcer. The\nspeaker generates referring expressions, the listener comprehends referring\nexpressions, and the reinforcer introduces a reward function to guide sampling\nof more discriminative expressions. The listener-speaker modules are trained\njointly in an end-to-end learning framework, allowing the modules to be aware\nof one another during learning while also benefiting from the discriminative\nreinforcer's feedback. We demonstrate that this unified framework and training\nachieves state-of-the-art results for both comprehension and generation on\nthree referring expression datasets. Project and demo page:\nhttps://vision.cs.unc.edu/refer\n", "versions": [{"version": "v1", "created": "Fri, 30 Dec 2016 17:39:19 GMT"}, {"version": "v2", "created": "Mon, 17 Apr 2017 20:13:49 GMT"}], "update_date": "2017-04-19", "authors_parsed": [["Yu", "Licheng", ""], ["Tan", "Hao", ""], ["Bansal", "Mohit", ""], ["Berg", "Tamara L.", ""]]}, {"id": "1612.09548", "submitter": "Zhenhua Feng", "authors": "Zhen-Hua Feng, Josef Kittler, William Christmas and Xiao-Jun Wu", "title": "A Unified Tensor-based Active Appearance Face Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Appearance variations result in many difficulties in face image analysis. To\ndeal with this challenge, we present a Unified Tensor-based Active Appearance\nModel (UT-AAM) for jointly modelling the geometry and texture information of 2D\nfaces. For each type of face information, namely shape and texture, we\nconstruct a unified tensor model capturing all relevant appearance variations.\nThis contrasts with the variation-specific models of the classical tensor AAM.\nTo achieve the unification across pose variations, a strategy for dealing with\nself-occluded faces is proposed to obtain consistent shape and texture\nrepresentations of pose-varied faces. In addition, our UT-AAM is capable of\nconstructing the model from an incomplete training dataset, using tensor\ncompletion methods. Last, we use an effective cascaded-regression-based method\nfor UT-AAM fitting. With these advancements, the utility of UT-AAM in practice\nis considerably enhanced. As an example, we demonstrate the improvements in\ntraining facial landmark detectors through the use of UT-AAM to synthesise a\nlarge number of virtual samples. Experimental results obtained using the\nMulti-PIE and 300-W face datasets demonstrate the merits of the proposed\napproach.\n", "versions": [{"version": "v1", "created": "Fri, 30 Dec 2016 18:08:16 GMT"}, {"version": "v2", "created": "Tue, 13 Jun 2017 16:33:25 GMT"}], "update_date": "2017-06-14", "authors_parsed": [["Feng", "Zhen-Hua", ""], ["Kittler", "Josef", ""], ["Christmas", "William", ""], ["Wu", "Xiao-Jun", ""]]}]