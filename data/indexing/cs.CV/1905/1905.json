[{"id": "1905.00006", "submitter": "Huibing Wang", "authors": "Jinjia Peng, Huibing Wang, Xianping Fu", "title": "Cross Domain Knowledge Learning with Dual-branch Adversarial Network for\n  Vehicle Re-identification", "comments": "arXiv admin note: substantial text overlap with arXiv:1903.07868", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The widespread popularization of vehicles has facilitated all people's life\nduring the last decades. However, the emergence of a large number of vehicles\nposes the critical but challenging problem of vehicle re-identification (reID).\nTill now, for most vehicle reID algorithms, both the training and testing\nprocesses are conducted on the same annotated datasets under supervision.\nHowever, even a well-trained model will still cause fateful performance drop\ndue to the severe domain bias between the trained dataset and the real-world\nscenes.\n  To address this problem, this paper proposes a domain adaptation framework\nfor vehicle reID (DAVR), which narrows the cross-domain bias by fully\nexploiting the labeled data from the source domain to adapt the target domain.\nDAVR develops an image-to-image translation network named Dual-branch\nAdversarial Network (DAN), which could promote the images from the source\ndomain (well-labeled) to learn the style of target domain (unlabeled) without\nany annotation and preserve identity information from source domain. Then the\ngenerated images are employed to train the vehicle reID model by a proposed\nattention-based feature learning model with more reasonable styles. Through the\nproposed framework, the well-trained reID model has better domain adaptation\nability for various scenes in real-world situations. Comprehensive experimental\nresults have demonstrated that our proposed DAVR can achieve excellent\nperformances on both VehicleID dataset and VeRi-776 dataset.\n", "versions": [{"version": "v1", "created": "Tue, 30 Apr 2019 05:17:57 GMT"}], "update_date": "2019-05-02", "authors_parsed": [["Peng", "Jinjia", ""], ["Wang", "Huibing", ""], ["Fu", "Xianping", ""]]}, {"id": "1905.00007", "submitter": "St\\'ephane Lathuili\\`ere", "authors": "Aliaksandr Siarohin, St\\'ephane Lathuili\\`ere, Enver Sangineto and\n  Nicu Sebe", "title": "Appearance and Pose-Conditioned Human Image Generation using Deformable\n  GANs", "comments": "To appear on IEEE TPAMI. arXiv admin note: substantial text overlap\n  with arXiv:1801.00055", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we address the problem of generating person images conditioned\non both pose and appearance information. Specifically, given an image xa of a\nperson and a target pose P(xb), extracted from a different image xb, we\nsynthesize a new image of that person in pose P(xb), while preserving the\nvisual details in xa. In order to deal with pixel-to-pixel misalignments caused\nby the pose differences between P(xa) and P(xb), we introduce deformable skip\nconnections in the generator of our Generative Adversarial Network. Moreover, a\nnearest-neighbour loss is proposed instead of the common L1 and L2 losses in\norder to match the details of the generated image with the target image.\nQuantitative and qualitative results, using common datasets and protocols\nrecently proposed for this task, show that our approach is competitive with\nrespect to the state of the art. Moreover, we conduct an extensive evaluation\nusing off-the-shell person re-identification (Re-ID) systems trained with\nperson-generation based augmented data, which is one of the main important\napplications for this task. Our experiments show that our Deformable GANs can\nsignificantly boost the Re-ID accuracy and are even better than\ndata-augmentation methods specifically trained using Re-ID losses.\n", "versions": [{"version": "v1", "created": "Tue, 30 Apr 2019 07:35:15 GMT"}, {"version": "v2", "created": "Mon, 14 Oct 2019 15:06:52 GMT"}], "update_date": "2019-10-15", "authors_parsed": [["Siarohin", "Aliaksandr", ""], ["Lathuili\u00e8re", "St\u00e9phane", ""], ["Sangineto", "Enver", ""], ["Sebe", "Nicu", ""]]}, {"id": "1905.00050", "submitter": "Sudhakar Kumawat", "authors": "Gagan Kanojia, Sudhakar Kumawat, Shanmuganathan Raman", "title": "Attentive Spatio-Temporal Representation Learning for Diving\n  Classification", "comments": "Accepted in CVPRW 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Competitive diving is a well recognized aquatic sport in which a person dives\nfrom a platform or a springboard into the water. Based on the acrobatics\nperformed during the dive, diving is classified into a finite set of action\nclasses which are standardized by FINA. In this work, we propose an attention\nguided LSTM-based neural network architecture for the task of diving\nclassification. The network takes the frames of a diving video as input and\ndetermines its class. We evaluate the performance of the proposed model on a\nrecently introduced competitive diving dataset, Diving48. It contains over\n18000 video clips which covers 48 classes of diving. The proposed model\noutperforms the classification accuracy of the state-of-the-art models in both\n2D and 3D frameworks by 11.54% and 4.24%, respectively. We show that the\nnetwork is able to localize the diver in the video frames during the dive\nwithout being trained with such a supervision.\n", "versions": [{"version": "v1", "created": "Tue, 30 Apr 2019 18:22:09 GMT"}], "update_date": "2019-05-02", "authors_parsed": [["Kanojia", "Gagan", ""], ["Kumawat", "Sudhakar", ""], ["Raman", "Shanmuganathan", ""]]}, {"id": "1905.00060", "submitter": "Yinan Zhao", "authors": "Danna Gurari, Yinan Zhao, Suyog Dutt Jain, Margrit Betke, Kristen\n  Grauman", "title": "Predicting How to Distribute Work Between Algorithms and Humans to\n  Segment an Image Batch", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Foreground object segmentation is a critical step for many image analysis\ntasks. While automated methods can produce high-quality results, their failures\ndisappoint users in need of practical solutions. We propose a resource\nallocation framework for predicting how best to allocate a fixed budget of\nhuman annotation effort in order to collect higher quality segmentations for a\ngiven batch of images and automated methods. The framework is based on a\nprediction module that estimates the quality of given algorithm-drawn\nsegmentations. We demonstrate the value of the framework for two novel tasks\nrelated to predicting how to distribute annotation efforts between algorithms\nand humans. Specifically, we develop two systems that automatically decide, for\na batch of images, when to recruit humans versus computers to create 1) coarse\nsegmentations required to initialize segmentation tools and 2) final,\nfine-grained segmentations. Experiments demonstrate the advantage of relying on\na mix of human and computer efforts over relying on either resource alone for\nsegmenting objects in images coming from three diverse modalities (visible,\nphase contrast microscopy, and fluorescence microscopy).\n", "versions": [{"version": "v1", "created": "Tue, 30 Apr 2019 18:56:24 GMT"}], "update_date": "2019-05-02", "authors_parsed": [["Gurari", "Danna", ""], ["Zhao", "Yinan", ""], ["Jain", "Suyog Dutt", ""], ["Betke", "Margrit", ""], ["Grauman", "Kristen", ""]]}, {"id": "1905.00122", "submitter": "Li Chen", "authors": "Li Chen, Carter Yagemann, Evan Downing", "title": "To believe or not to believe: Validating explanation fidelity for\n  dynamic malware analysis", "comments": "Accepted at the IEEE Computer Vision Pattern Recognition 2019\n  Explainable AI Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.AI cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Converting malware into images followed by vision-based deep learning\nalgorithms has shown superior threat detection efficacy compared with classical\nmachine learning algorithms. When malware are visualized as images,\nvisual-based interpretation schemes can also be applied to extract insights of\nwhy individual samples are classified as malicious. In this work, via two case\nstudies of dynamic malware classification, we extend the local interpretable\nmodel-agnostic explanation algorithm to explain image-based dynamic malware\nclassification and examine its interpretation fidelity. For both case studies,\nwe first train deep learning models via transfer learning on malware images,\ndemonstrate high classification effectiveness, apply an explanation method on\nthe images, and correlate the results back to the samples to validate whether\nthe algorithmic insights are consistent with security domain expertise. In our\nfirst case study, the interpretation framework identifies indirect calls that\nuniquely characterize the underlying exploit behavior of a malware family. In\nour second case study, the interpretation framework extracts insightful\ninformation such as cryptography-related APIs when applied on images created\nfrom API existence, but generate ambiguous interpretation on images created\nfrom API sequences and frequencies. Our findings indicate that current\nimage-based interpretation techniques are promising for explaining vision-based\nmalware classification. We continue to develop image-based interpretation\nschemes specifically for security applications.\n", "versions": [{"version": "v1", "created": "Tue, 30 Apr 2019 22:45:30 GMT"}], "update_date": "2019-05-02", "authors_parsed": [["Chen", "Li", ""], ["Yagemann", "Carter", ""], ["Downing", "Evan", ""]]}, {"id": "1905.00135", "submitter": "Matej Ulicny", "authors": "Matej Ulicny, Vladimir A. Krylov, Rozenn Dahyot", "title": "Harmonic Networks with Limited Training Samples", "comments": null, "journal-ref": "European Signal Processing Conference (EUSIPCO) 2019", "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional neural networks (CNNs) are very popular nowadays for image\nprocessing. CNNs allow one to learn optimal filters in a (mostly) supervised\nmachine learning context. However this typically requires abundant labelled\ntraining data to estimate the filter parameters. Alternative strategies have\nbeen deployed for reducing the number of parameters and / or filters to be\nlearned and thus decrease overfitting. In the context of reverting to preset\nfilters, we propose here a computationally efficient harmonic block that uses\nDiscrete Cosine Transform (DCT) filters in CNNs. In this work we examine the\nperformance of harmonic networks in limited training data scenario. We validate\nexperimentally that its performance compares well against scattering networks\nthat use wavelets as preset filters.\n", "versions": [{"version": "v1", "created": "Tue, 30 Apr 2019 23:35:30 GMT"}], "update_date": "2019-07-04", "authors_parsed": [["Ulicny", "Matej", ""], ["Krylov", "Vladimir A.", ""], ["Dahyot", "Rozenn", ""]]}, {"id": "1905.00136", "submitter": "Xiaolong Ma", "authors": "Xiaolong Ma, Geng Yuan, Sheng Lin, Zhengang Li, Hao Sun, Yanzhi Wang", "title": "ResNet Can Be Pruned 60x: Introducing Network Purification and Unused\n  Path Removal (P-RM) after Weight Pruning", "comments": "Submitted to ICML workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The state-of-art DNN structures involve high computation and great demand for\nmemory storage which pose intensive challenge on DNN framework resources. To\nmitigate the challenges, weight pruning techniques has been studied. However,\nhigh accuracy solution for extreme structured pruning that combines different\ntypes of structured sparsity still waiting for unraveling due to the extremely\nreduced weights in DNN networks. In this paper, we propose a DNN framework\nwhich combines two different types of structured weight pruning (filter and\ncolumn prune) by incorporating alternating direction method of multipliers\n(ADMM) algorithm for better prune performance. We are the first to find\nnon-optimality of ADMM process and unused weights in a structured pruned model,\nand further design an optimization framework which contains the first proposed\nNetwork Purification and Unused Path Removal algorithms which are dedicated to\npost-processing an structured pruned model after ADMM steps. Some high lights\nshows we achieve 232x compression on LeNet-5, 60x compression on ResNet-18\nCIFAR-10 and over 5x compression on AlexNet. We share our models at anonymous\nlink http://bit.ly/2VJ5ktv.\n", "versions": [{"version": "v1", "created": "Tue, 30 Apr 2019 23:40:51 GMT"}], "update_date": "2019-05-03", "authors_parsed": [["Ma", "Xiaolong", ""], ["Yuan", "Geng", ""], ["Lin", "Sheng", ""], ["Li", "Zhengang", ""], ["Sun", "Hao", ""], ["Wang", "Yanzhi", ""]]}, {"id": "1905.00149", "submitter": "Chun-Guang Li", "authors": "Junjian Zhang, Chun-Guang Li, Chong You, Xianbiao Qi, Honggang Zhang,\n  Jun Guo, and Zhouchen Lin", "title": "Self-Supervised Convolutional Subspace Clustering Network", "comments": "10 pages, 2 figures, and 5 tables. This paper has been accepted by\n  CVPR2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Subspace clustering methods based on data self-expression have become very\npopular for learning from data that lie in a union of low-dimensional linear\nsubspaces. However, the applicability of subspace clustering has been limited\nbecause practical visual data in raw form do not necessarily lie in such linear\nsubspaces. On the other hand, while Convolutional Neural Network (ConvNet) has\nbeen demonstrated to be a powerful tool for extracting discriminative features\nfrom visual data, training such a ConvNet usually requires a large amount of\nlabeled data, which are unavailable in subspace clustering applications. To\nachieve simultaneous feature learning and subspace clustering, we propose an\nend-to-end trainable framework, called Self-Supervised Convolutional Subspace\nClustering Network (S$^2$ConvSCN), that combines a ConvNet module (for feature\nlearning), a self-expression module (for subspace clustering) and a spectral\nclustering module (for self-supervision) into a joint optimization framework.\nParticularly, we introduce a dual self-supervision that exploits the output of\nspectral clustering to supervise the training of the feature learning module\n(via a classification loss) and the self-expression module (via a spectral\nclustering loss). Our experiments on four benchmark datasets show the\neffectiveness of the dual self-supervision and demonstrate superior performance\nof our proposed approach.\n", "versions": [{"version": "v1", "created": "Wed, 1 May 2019 01:05:25 GMT"}], "update_date": "2019-05-02", "authors_parsed": [["Zhang", "Junjian", ""], ["Li", "Chun-Guang", ""], ["You", "Chong", ""], ["Qi", "Xianbiao", ""], ["Zhang", "Honggang", ""], ["Guo", "Jun", ""], ["Lin", "Zhouchen", ""]]}, {"id": "1905.00160", "submitter": "Braden Hurl", "authors": "Braden Hurl, Krzysztof Czarnecki, Steven Waslander", "title": "Precise Synthetic Image and LiDAR (PreSIL) Dataset for Autonomous\n  Vehicle Perception", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce the Precise Synthetic Image and LiDAR (PreSIL) dataset for\nautonomous vehicle perception. Grand Theft Auto V (GTA V), a commercial video\ngame, has a large detailed world with realistic graphics, which provides a\ndiverse data collection environment. Existing works creating synthetic LiDAR\ndata for autonomous driving with GTA V have not released their datasets, rely\non an in-game raycasting function which represents people as cylinders, and can\nfail to capture vehicles past 30 metres. Our work creates a precise LiDAR\nsimulator within GTA V which collides with detailed models for all entities no\nmatter the type or position. The PreSIL dataset consists of over 50,000 frames\nand includes high-definition images with full resolution depth information,\nsemantic segmentation (images), point-wise segmentation (point clouds), and\ndetailed annotations for all vehicles and people. Collecting additional data\nwith our framework is entirely automatic and requires no human annotation of\nany kind. We demonstrate the effectiveness of our dataset by showing an\nimprovement of up to 5% average precision on the KITTI 3D Object Detection\nbenchmark challenge when state-of-the-art 3D object detection networks are\npre-trained with our data. The data and code are available at\nhttps://tinyurl.com/y3tb9sxy\n", "versions": [{"version": "v1", "created": "Wed, 1 May 2019 02:14:22 GMT"}, {"version": "v2", "created": "Tue, 7 May 2019 02:55:37 GMT"}], "update_date": "2019-05-08", "authors_parsed": [["Hurl", "Braden", ""], ["Czarnecki", "Krzysztof", ""], ["Waslander", "Steven", ""]]}, {"id": "1905.00174", "submitter": "Azadeh Mozafari", "authors": "Azadeh Sadat Mozafari, Hugo Siqueira Gomes, Wilson Le\\~ao and\n  Christian Gagn\\'e", "title": "Unsupervised Temperature Scaling: An Unsupervised Post-Processing\n  Calibration Method of Deep Networks", "comments": "arXiv admin note: text overlap with arXiv:1810.11586", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The great performances of deep learning are undeniable, with impressive\nresults over a wide range of tasks. However, the output confidence of these\nmodels is usually not well-calibrated, which can be an issue for applications\nwhere confidence on the decisions is central to providing trust and reliability\n(e.g., autonomous driving or medical diagnosis). For models using softmax at\nthe last layer, Temperature Scaling (TS) is a state-of-the-art calibration\nmethod, with low time and memory complexity as well as demonstrated\neffectiveness. TS relies on a T parameter to rescale and calibrate values of\nthe softmax layer, whose parameter value is computed from a labelled dataset.\nWe are proposing an Unsupervised Temperature Scaling (UTS) approach, which does\nnot depend on labelled samples to calibrate the model, which allows, for\nexample, the use of a part of a test samples to calibrate the pre-trained model\nbefore going into inference mode. We provide theoretical justifications for UTS\nand assess its effectiveness on a wide range of deep models and datasets. We\nalso demonstrate calibration results of UTS on skin lesion detection, a problem\nwhere a well-calibrated output can play an important role for accurate\ndecision-making.\n", "versions": [{"version": "v1", "created": "Wed, 1 May 2019 03:43:29 GMT"}, {"version": "v2", "created": "Wed, 8 May 2019 02:52:23 GMT"}, {"version": "v3", "created": "Mon, 10 Jun 2019 15:23:22 GMT"}], "update_date": "2019-06-11", "authors_parsed": [["Mozafari", "Azadeh Sadat", ""], ["Gomes", "Hugo Siqueira", ""], ["Le\u00e3o", "Wilson", ""], ["Gagn\u00e9", "Christian", ""]]}, {"id": "1905.00261", "submitter": "Marcelo Cabral Ghilardi", "authors": "Marcelo C. Ghilardi, Leandro Dihl, Estev\\~ao Testa, Pedro Braga,\n  Jo\\~ao P. Pianta, Isabel H. Manssour, Soraia R. Musse", "title": "Automatic Dataset Augmentation Using Virtual Human Simulation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Virtual Human Simulation has been widely used for different purposes, such as\ncomfort or accessibility analysis. In this paper, we investigate the\npossibility of using this type of technique to extend the training datasets of\npedestrians to be used with machine learning techniques. Our main goal is to\nverify if Computer Graphics (CG) images of virtual humans with a simplistic\nrendering can be efficient in order to augment datasets used for training\nmachine learning methods. In fact, from a machine learning point of view, there\nis a need to collect and label large datasets for ground truth, which sometimes\ndemands manual annotation. In addition, find out images and videos with real\npeople and also provide ground truth of people detection and counting is not\ntrivial. If CG images, which can have a ground truth automatically generated,\ncan also be used as training in machine learning techniques for pedestrian\ndetection and counting, it can certainly facilitate and optimize the whole\nprocess of event detection. In particular, we propose to parametrize virtual\nhumans using a data-driven approach. Results demonstrated that using the\nextended datasets with CG images outperforms the results when compared to only\nreal images sequences.\n", "versions": [{"version": "v1", "created": "Wed, 1 May 2019 11:01:39 GMT"}], "update_date": "2019-05-02", "authors_parsed": [["Ghilardi", "Marcelo C.", ""], ["Dihl", "Leandro", ""], ["Testa", "Estev\u00e3o", ""], ["Braga", "Pedro", ""], ["Pianta", "Jo\u00e3o P.", ""], ["Manssour", "Isabel H.", ""], ["Musse", "Soraia R.", ""]]}, {"id": "1905.00286", "submitter": "Behzad Bozorgtabar", "authors": "Behzad Bozorgtabar, Mohammad Saeed Rad, Haz{\\i}m Kemal Ekenel and\n  Jean-Philippe Thiran", "title": "Learn to synthesize and synthesize to learn", "comments": "Accepted to Computer Vision and Image Understanding (CVIU)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Attribute guided face image synthesis aims to manipulate attributes on a face\nimage. Most existing methods for image-to-image translation can either perform\na fixed translation between any two image domains using a single attribute or\nrequire training data with the attributes of interest for each subject.\nTherefore, these methods could only train one specific model for each pair of\nimage domains, which limits their ability in dealing with more than two\ndomains. Another disadvantage of these methods is that they often suffer from\nthe common problem of mode collapse that degrades the quality of the generated\nimages. To overcome these shortcomings, we propose attribute guided face image\ngeneration method using a single model, which is capable to synthesize multiple\nphoto-realistic face images conditioned on the attributes of interest. In\naddition, we adopt the proposed model to increase the realism of the simulated\nface images while preserving the face characteristics. Compared to existing\nmodels, synthetic face images generated by our method present a good\nphotorealistic quality on several face datasets. Finally, we demonstrate that\ngenerated facial images can be used for synthetic data augmentation, and\nimprove the performance of the classifier used for facial expression\nrecognition.\n", "versions": [{"version": "v1", "created": "Wed, 1 May 2019 12:45:31 GMT"}], "update_date": "2019-05-02", "authors_parsed": [["Bozorgtabar", "Behzad", ""], ["Rad", "Mohammad Saeed", ""], ["Ekenel", "Haz\u0131m Kemal", ""], ["Thiran", "Jean-Philippe", ""]]}, {"id": "1905.00292", "submitter": "Xiao Zhang", "authors": "Xiao Zhang, Rui Zhao, Yu Qiao, Xiaogang Wang, Hongsheng Li", "title": "AdaCos: Adaptively Scaling Cosine Logits for Effectively Learning Deep\n  Face Representations", "comments": "CVPR 2019 Oral", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The cosine-based softmax losses and their variants achieve great success in\ndeep learning based face recognition. However, hyperparameter settings in these\nlosses have significant influences on the optimization path as well as the\nfinal recognition performance. Manually tuning those hyperparameters heavily\nrelies on user experience and requires many training tricks. In this paper, we\ninvestigate in depth the effects of two important hyperparameters of\ncosine-based softmax losses, the scale parameter and angular margin parameter,\nby analyzing how they modulate the predicted classification probability. Based\non these analysis, we propose a novel cosine-based softmax loss, AdaCos, which\nis hyperparameter-free and leverages an adaptive scale parameter to\nautomatically strengthen the training supervisions during the training process.\nWe apply the proposed AdaCos loss to large-scale face verification and\nidentification datasets, including LFW, MegaFace, and IJB-C 1:1 Verification.\nOur results show that training deep neural networks with the AdaCos loss is\nstable and able to achieve high face recognition accuracy. Our method\noutperforms state-of-the-art softmax losses on all the three datasets.\n", "versions": [{"version": "v1", "created": "Wed, 1 May 2019 12:58:05 GMT"}, {"version": "v2", "created": "Tue, 7 May 2019 10:46:56 GMT"}], "update_date": "2019-05-08", "authors_parsed": [["Zhang", "Xiao", ""], ["Zhao", "Rui", ""], ["Qiao", "Yu", ""], ["Wang", "Xiaogang", ""], ["Li", "Hongsheng", ""]]}, {"id": "1905.00307", "submitter": "Stylianos Ploumpis", "authors": "Stylianos Moschoglou, Stylianos Ploumpis, Mihalis Nicolaou, Athanasios\n  Papaioannou, Stefanos Zafeiriou", "title": "3DFaceGAN: Adversarial Nets for 3D Face Representation, Generation, and\n  Translation", "comments": "15 pages, 12 figures. Submitted to International Journal of Computer\n  Vision (IJCV), special issue: Generative Adversarial Networks for Computer\n  Vision", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Over the past few years, Generative Adversarial Networks (GANs) have garnered\nincreased interest among researchers in Computer Vision, with applications\nincluding, but not limited to, image generation, translation, imputation, and\nsuper-resolution. Nevertheless, no GAN-based method has been proposed in the\nliterature that can successfully represent, generate or translate 3D facial\nshapes (meshes). This can be primarily attributed to two facts, namely that (a)\npublicly available 3D face databases are scarce as well as limited in terms of\nsample size and variability (e.g., few subjects, little diversity in race and\ngender), and (b) mesh convolutions for deep networks present several challenges\nthat are not entirely tackled in the literature, leading to operator\napproximations and model instability, often failing to preserve high-frequency\ncomponents of the distribution. As a result, linear methods such as Principal\nComponent Analysis (PCA) have been mainly utilized towards 3D shape analysis,\ndespite being unable to capture non-linearities and high frequency details of\nthe 3D face - such as eyelid and lip variations. In this work, we present\n3DFaceGAN, the first GAN tailored towards modeling the distribution of 3D\nfacial surfaces, while retaining the high frequency details of 3D face shapes.\nWe conduct an extensive series of both qualitative and quantitative\nexperiments, where the merits of 3DFaceGAN are clearly demonstrated against\nother, state-of-the-art methods in tasks such as 3D shape representation,\ngeneration, and translation.\n", "versions": [{"version": "v1", "created": "Wed, 1 May 2019 13:31:01 GMT"}, {"version": "v2", "created": "Thu, 9 May 2019 22:24:51 GMT"}], "update_date": "2019-05-13", "authors_parsed": [["Moschoglou", "Stylianos", ""], ["Ploumpis", "Stylianos", ""], ["Nicolaou", "Mihalis", ""], ["Papaioannou", "Athanasios", ""], ["Zafeiriou", "Stefanos", ""]]}, {"id": "1905.00310", "submitter": "Alexander Wong", "authors": "Kaylen J. Pfisterer, Robert Amelard, Braeden Syrnyk, and Alexander\n  Wong", "title": "Towards computer vision powered color-nutrient assessment of pureed food", "comments": "3 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.NE eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With one in four individuals afflicted with malnutrition, computer vision may\nprovide a way of introducing a new level of automation in the nutrition field\nto reliably monitor food and nutrient intake. In this study, we present a novel\napproach to modeling the link between color and vitamin A content using\ntransmittance imaging of a pureed foods dilution series in a computer vision\npowered nutrient sensing system via a fine-tuned deep autoencoder network,\nwhich in this case was trained to predict the relative concentration of sweet\npotato purees. Experimental results show the deep autoencoder network can\nachieve an accuracy of 80% across beginner (6 month) and intermediate (8 month)\ncommercially prepared pureed sweet potato samples. Prediction errors may be\nexplained by fundamental differences in optical properties which are further\ndiscussed.\n", "versions": [{"version": "v1", "created": "Wed, 1 May 2019 13:42:19 GMT"}], "update_date": "2019-05-02", "authors_parsed": [["Pfisterer", "Kaylen J.", ""], ["Amelard", "Robert", ""], ["Syrnyk", "Braeden", ""], ["Wong", "Alexander", ""]]}, {"id": "1905.00322", "submitter": "Indra Deep Mastan", "authors": "Indra Deep Mastan and Shanmuganathan Raman", "title": "Multi-level Encoder-Decoder Architectures for Image Restoration", "comments": "Accepted in the IEEE Conference on Computer Vision and Pattern\n  Recognition (CVPR) Workshop: \"New Trends in Image Restoration and Enhancement\n  workshop (NTIRE) 2019\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Many real-world solutions for image restoration are learning-free and based\non handcrafted image priors such as self-similarity. Recently, deep-learning\nmethods that use training data have achieved state-of-the-art results in\nvarious image restoration tasks (e.g., super-resolution and inpainting).\nUlyanov et al. bridge the gap between these two families of methods (CVPR 18).\nThey have shown that learning-free methods perform close to the\nstate-of-the-art learning-based methods (approximately 1 PSNR). Their approach\nbenefits from the encoder-decoder network. In this paper, we propose a\nframework based on the multi-level extensions of the encoder-decoder network,\nto investigate interesting aspects of the relationship between image\nrestoration and network construction independent of learning. Our framework\nallows various network structures by modifying the following network\ncomponents: skip links, cascading of the network input into intermediate\nlayers, a composition of the encoder-decoder subnetworks, and network depth.\nThese handcrafted network structures illustrate how the construction of\nuntrained networks influence the following image restoration tasks: denoising,\nsuper-resolution, and inpainting. We also demonstrate image reconstruction\nusing flash and no-flash image pairs. We provide performance comparisons with\nthe state-of-the-art methods for all the restoration tasks above.\n", "versions": [{"version": "v1", "created": "Wed, 1 May 2019 14:13:51 GMT"}, {"version": "v2", "created": "Thu, 2 May 2019 14:21:41 GMT"}, {"version": "v3", "created": "Mon, 6 May 2019 12:45:24 GMT"}], "update_date": "2019-05-07", "authors_parsed": [["Mastan", "Indra Deep", ""], ["Raman", "Shanmuganathan", ""]]}, {"id": "1905.00336", "submitter": "Yunfei Long", "authors": "Yunfei Long, Amber Bassett, Karen Cichy, Addie Thompson and Daniel\n  Morris", "title": "Bean Split Ratio for Dry Bean Canning Quality and Variety Analysis", "comments": "IEEE Conference on Computer Vision and Pattern Recognition Workshops,\n  2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Splits on canned beans appear in the process of preparation and canning.\nResearchers are studying how they are influenced by cooking environment and\ngenotype. However, there is no existing method to automatically quantify or to\ncharacterize the severity of splits. To solve this, we propose two measures:\nthe Bean Split Ratio (BSR) that quantifies the overall severity of splits, and\nthe Bean Split Histogram (BSH) that characterizes the size distribution of\nsplits. We create a pixel-wise segmentation method to automatically estimate\nthese measures from images. We also present a bean dataset of recombinant\ninbred lines of two genotypes, use the BSR and BSH to assess canning quality,\nand explore heritability of these properties.\n", "versions": [{"version": "v1", "created": "Wed, 1 May 2019 14:52:01 GMT"}], "update_date": "2019-05-02", "authors_parsed": [["Long", "Yunfei", ""], ["Bassett", "Amber", ""], ["Cichy", "Karen", ""], ["Thompson", "Addie", ""], ["Morris", "Daniel", ""]]}, {"id": "1905.00372", "submitter": "Juan Tapia Dr.", "authors": "Juan Tapia and Claudia Arellano", "title": "Gender Classification from Iris Texture Images Using a New Set of Binary\n  Statistical Image Features", "comments": "A pre-print version of the paper accepted at 12th IAPR International\n  Conference on Biometrics", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Soft biometric information such as gender can contribute to many applications\nlike as identification and security. This paper explores the use of a Binary\nStatistical Features (BSIF) algorithm for classifying gender from iris texture\nimages captured with NIR sensors. It uses the same pipeline for iris\nrecognition systems consisting of iris segmentation, normalisation and then\nclassification. Experiments show that applying BSIF is not straightforward\nsince it can create artificial textures causing misclassification. In order to\novercome this limitation, a new set of filters was trained from eye images and\ndifferent sized filters with padding bands were tested on a subject-disjoint\ndatabase. A Modified-BSIF (MBSIF) method was implemented. The latter achieved\nbetter gender classification results (94.6\\% and 91.33\\% for the left and right\neye respectively). These results are competitive with the state of the art in\ngender classification. In an additional contribution, a novel gender labelled\ndatabase was created and it will be available upon request.\n", "versions": [{"version": "v1", "created": "Wed, 1 May 2019 16:46:49 GMT"}], "update_date": "2019-05-02", "authors_parsed": [["Tapia", "Juan", ""], ["Arellano", "Claudia", ""]]}, {"id": "1905.00391", "submitter": "Qingbiao Li", "authors": "Qingbiao Li and Jianyu Lin and Neil T.Clancy and Daniel S. Elson", "title": "Estimation of Tissue Oxygen Saturation from RGB images and Sparse\n  Hyperspectral Signals based on Conditional Generative Adversarial Network", "comments": null, "journal-ref": "International journal of computer assisted radiology and surgery\n  (2019)", "doi": "10.1007/s11548-019-01940-2", "report-no": null, "categories": "cs.CV eess.IV", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Purpose: Intra-operative measurement of tissue oxygen saturation (StO2) is\nimportant in the detection of ischemia, monitoring perfusion and identifying\ndisease. Hyperspectral imaging (HSI) measures the optical reflectance spectrum\nof the tissue and uses this information to quantify its composition, including\nStO2. However, real-time monitoring is difficult due to the capture rate and\ndata processing time. Methods: An endoscopic system based on a multi-fiber\nprobe was previously developed to sparsely capture HSI data (sHSI). These were\ncombined with RGB images, via a deep neural network, to generate\nhigh-resolution hypercubes and calculate StO2. To improve accuracy and\nprocessing speed, we propose a dual-input conditional generative adversarial\nnetwork (cGAN), Dual2StO2, to directly estimate StO2 by fusing features from\nboth RGB and sHSI. Results: Validation experiments were carried out on in vivo\nporcine bowel data, where the ground truth StO2 was generated from the HSI\ncamera. The performance was also compared to our previous\nsuper-spectral-resolution network, SSRNet in terms of mean StO2 prediction\naccuracy and structural similarity metrics. Dual2StO2 was also tested using\nsimulated probe data with varying fiber number. Conclusions: StO2 estimation by\nDual2StO2 is visually closer to ground truth in general structure, achieves\nhigher prediction accuracy and faster processing speed than SSRNet. Simulations\nshowed that results improved when a greater number of fibers are used in the\nprobe. Future work will include refinement of the network architecture,\nhardware optimization based on simulation results, and evaluation of the\ntechnique in clinical applications beyond StO2 estimation.\n", "versions": [{"version": "v1", "created": "Wed, 1 May 2019 17:25:01 GMT"}, {"version": "v2", "created": "Thu, 16 May 2019 21:49:41 GMT"}], "update_date": "2019-05-20", "authors_parsed": [["Li", "Qingbiao", ""], ["Lin", "Jianyu", ""], ["Clancy", "Neil T.", ""], ["Elson", "Daniel S.", ""]]}, {"id": "1905.00396", "submitter": "Juan Tapia Dr.", "authors": "Juan Tapia, Christian Rathgeb and Christoph Busch", "title": "Sex-Prediction from Periocular Images across Multiple Sensors and\n  Spectra", "comments": "Pre-print version of Paper presented at Proc. International Workshop\n  on Ubiquitous implicit Biometrics and health signals monitoring for\n  person-centric applications (UBIO 18), 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we provide a comprehensive analysis of periocular-based\nsex-prediction (commonly referred to as gender classification) using\nstate-of-the-art machine learning techniques. In order to reflect a more\nchallenging scenario where periocular images are likely to be obtained from an\nunknown source, i.e. sensor, convolutional neural networks are trained on fused\nsets composed of several near-infrared (NIR) and visible wavelength (VW) image\ndatabases. In a cross-sensor scenario within each spectrum an average\nclassification accuracy of approximately 85\\% is achieved. When sex-prediction\nis performed across spectra an average classification accuracy of about 82\\% is\nobtained. Finally, a multi-spectral sex-prediction yields a classification\naccuracy of 83\\% on average. Compared to proposed works, obtained results\nprovide a more realistic estimation of the feasibility to predict a subject's\nsex from the periocular region.\n", "versions": [{"version": "v1", "created": "Wed, 1 May 2019 17:31:31 GMT"}], "update_date": "2019-05-02", "authors_parsed": [["Tapia", "Juan", ""], ["Rathgeb", "Christian", ""], ["Busch", "Christoph", ""]]}, {"id": "1905.00397", "submitter": "Sungbin Lim", "authors": "Sungbin Lim, Ildoo Kim, Taesup Kim, Chiheon Kim, Sungwoong Kim", "title": "Fast AutoAugment", "comments": "8 pages, 2 figure", "journal-ref": null, "doi": null, "report-no": "NeurIPS/2019/12", "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data augmentation is an essential technique for improving generalization\nability of deep learning models. Recently, AutoAugment has been proposed as an\nalgorithm to automatically search for augmentation policies from a dataset and\nhas significantly enhanced performances on many image recognition tasks.\nHowever, its search method requires thousands of GPU hours even for a\nrelatively small dataset. In this paper, we propose an algorithm called Fast\nAutoAugment that finds effective augmentation policies via a more efficient\nsearch strategy based on density matching. In comparison to AutoAugment, the\nproposed algorithm speeds up the search time by orders of magnitude while\nachieves comparable performances on image recognition tasks with various models\nand datasets including CIFAR-10, CIFAR-100, SVHN, and ImageNet.\n", "versions": [{"version": "v1", "created": "Wed, 1 May 2019 17:33:36 GMT"}, {"version": "v2", "created": "Sat, 25 May 2019 16:44:21 GMT"}], "update_date": "2020-01-09", "authors_parsed": [["Lim", "Sungbin", ""], ["Kim", "Ildoo", ""], ["Kim", "Taesup", ""], ["Kim", "Chiheon", ""], ["Kim", "Sungwoong", ""]]}, {"id": "1905.00401", "submitter": "Tal Hassner", "authors": "Matan Goldman, Tal Hassner, Shai Avidan", "title": "Learn Stereo, Infer Mono: Siamese Networks for Self-Supervised,\n  Monocular, Depth Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The field of self-supervised monocular depth estimation has seen huge\nadvancements in recent years. Most methods assume stereo data is available\nduring training but usually under-utilize it and only treat it as a reference\nsignal. We propose a novel self-supervised approach which uses both left and\nright images equally during training, but can still be used with a single input\nimage at test time, for monocular depth estimation. Our Siamese network\narchitecture consists of two, twin networks, each learns to predict a disparity\nmap from a single image. At test time, however, only one of these networks is\nused in order to infer depth. We show state-of-the-art results on the standard\nKITTI Eigen split benchmark as well as being the highest scoring\nself-supervised method on the new KITTI single view benchmark. To demonstrate\nthe ability of our method to generalize to new data sets, we further provide\nresults on the Make3D benchmark, which was not used during training.\n", "versions": [{"version": "v1", "created": "Wed, 1 May 2019 17:36:19 GMT"}], "update_date": "2019-05-02", "authors_parsed": [["Goldman", "Matan", ""], ["Hassner", "Tal", ""], ["Avidan", "Shai", ""]]}, {"id": "1905.00413", "submitter": "Pratul Srinivasan", "authors": "Pratul P. Srinivasan, Richard Tucker, Jonathan T. Barron, Ravi\n  Ramamoorthi, Ren Ng, Noah Snavely", "title": "Pushing the Boundaries of View Extrapolation with Multiplane Images", "comments": "Oral presentation at CVPR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We explore the problem of view synthesis from a narrow baseline pair of\nimages, and focus on generating high-quality view extrapolations with plausible\ndisocclusions. Our method builds upon prior work in predicting a multiplane\nimage (MPI), which represents scene content as a set of RGB$\\alpha$ planes\nwithin a reference view frustum and renders novel views by projecting this\ncontent into the target viewpoints. We present a theoretical analysis showing\nhow the range of views that can be rendered from an MPI increases linearly with\nthe MPI disparity sampling frequency, as well as a novel MPI prediction\nprocedure that theoretically enables view extrapolations of up to $4\\times$ the\nlateral viewpoint movement allowed by prior work. Our method ameliorates two\nspecific issues that limit the range of views renderable by prior methods: 1)\nWe expand the range of novel views that can be rendered without depth\ndiscretization artifacts by using a 3D convolutional network architecture along\nwith a randomized-resolution training procedure to allow our model to predict\nMPIs with increased disparity sampling frequency. 2) We reduce the repeated\ntexture artifacts seen in disocclusions by enforcing a constraint that the\nappearance of hidden content at any depth must be drawn from visible content at\nor behind that depth. Please see our results video at:\nhttps://www.youtube.com/watch?v=aJqAaMNL2m4.\n", "versions": [{"version": "v1", "created": "Wed, 1 May 2019 17:57:09 GMT"}], "update_date": "2019-05-02", "authors_parsed": [["Srinivasan", "Pratul P.", ""], ["Tucker", "Richard", ""], ["Barron", "Jonathan T.", ""], ["Ramamoorthi", "Ravi", ""], ["Ng", "Ren", ""], ["Snavely", "Noah", ""]]}, {"id": "1905.00441", "submitter": "Yandong Li", "authors": "Yandong Li, Lijun Li, Liqiang Wang, Tong Zhang, Boqing Gong", "title": "NATTACK: Learning the Distributions of Adversarial Examples for an\n  Improved Black-Box Attack on Deep Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Powerful adversarial attack methods are vital for understanding how to\nconstruct robust deep neural networks (DNNs) and for thoroughly testing defense\ntechniques. In this paper, we propose a black-box adversarial attack algorithm\nthat can defeat both vanilla DNNs and those generated by various defense\ntechniques developed recently. Instead of searching for an \"optimal\"\nadversarial example for a benign input to a targeted DNN, our algorithm finds a\nprobability density distribution over a small region centered around the input,\nsuch that a sample drawn from this distribution is likely an adversarial\nexample, without the need of accessing the DNN's internal layers or weights.\nOur approach is universal as it can successfully attack different neural\nnetworks by a single algorithm. It is also strong; according to the testing\nagainst 2 vanilla DNNs and 13 defended ones, it outperforms state-of-the-art\nblack-box or white-box attack methods for most test cases. Additionally, our\nresults reveal that adversarial training remains one of the best defense\ntechniques, and the adversarial examples are not as transferable across\ndefended DNNs as them across vanilla DNNs.\n", "versions": [{"version": "v1", "created": "Wed, 1 May 2019 18:20:09 GMT"}, {"version": "v2", "created": "Mon, 13 May 2019 18:26:21 GMT"}, {"version": "v3", "created": "Mon, 9 Dec 2019 19:18:49 GMT"}], "update_date": "2019-12-11", "authors_parsed": [["Li", "Yandong", ""], ["Li", "Lijun", ""], ["Wang", "Liqiang", ""], ["Zhang", "Tong", ""], ["Gong", "Boqing", ""]]}, {"id": "1905.00458", "submitter": "Ribana Roscher", "authors": "Laura Zabawa, Anna Kicherer, Lasse Klingbeil, Andres Milioto, Reinhard\n  T\\\"opfer, Heiner Kuhlmann, Ribana Roscher", "title": "Detection of Single Grapevine Berries in Images Using Fully\n  Convolutional Neural Networks", "comments": null, "journal-ref": "CVPR Workshop on Computer Vision Problems in Plant Phenotyping,\n  2019", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Yield estimation and forecasting are of special interest in the field of\ngrapevine breeding and viticulture. The number of harvested berries per plant\nis strongly correlated with the resulting quality. Therefore, early yield\nforecasting can enable a focused thinning of berries to ensure a high quality\nend product. Traditionally yield estimation is done by extrapolating from a\nsmall sample size and by utilizing historic data. Moreover, it needs to be\ncarried out by skilled experts with much experience in this field. Berry\ndetection in images offers a cheap, fast and non-invasive alternative to the\notherwise time-consuming and subjective on-site analysis by experts. We apply\nfully convolutional neural networks on images acquired with the Phenoliner, a\nfield phenotyping platform. We count single berries in images to avoid the\nerror-prone detection of grapevine clusters. Clusters are often overlapping and\ncan vary a lot in the size which makes the reliable detection of them\ndifficult. We address especially the detection of white grapes directly in the\nvineyard. The detection of single berries is formulated as a classification\ntask with three classes, namely 'berry', 'edge' and 'background'. A connected\ncomponent algorithm is applied to determine the number of berries in one image.\nWe compare the automatically counted number of berries with the manually\ndetected berries in 60 images showing Riesling plants in vertical shoot\npositioned trellis (VSP) and semi minimal pruned hedges (SMPH). We are able to\ndetect berries correctly within the VSP system with an accuracy of 94.0 \\% and\nfor the SMPH system with 85.6 \\%.\n", "versions": [{"version": "v1", "created": "Wed, 1 May 2019 19:19:55 GMT"}], "update_date": "2019-05-03", "authors_parsed": [["Zabawa", "Laura", ""], ["Kicherer", "Anna", ""], ["Klingbeil", "Lasse", ""], ["Milioto", "Andres", ""], ["T\u00f6pfer", "Reinhard", ""], ["Kuhlmann", "Heiner", ""], ["Roscher", "Ribana", ""]]}, {"id": "1905.00469", "submitter": "Tao Wang", "authors": "Tao Wang, Irene Cheng and Anup Basu", "title": "Fully Automatic Brain Tumor Segmentation using a Normalized Gaussian\n  Bayesian Classifier and 3D Fluid Vector Flow", "comments": "ICIP 2010", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG cs.MM stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Brain tumor segmentation from Magnetic Resonance Images (MRIs) is an\nimportant task to measure tumor responses to treatments. However, automatic\nsegmentation is very challenging. This paper presents an automatic brain tumor\nsegmentation method based on a Normalized Gaussian Bayesian classification and\na new 3D Fluid Vector Flow (FVF) algorithm. In our method, a Normalized\nGaussian Mixture Model (NGMM) is proposed and used to model the healthy brain\ntissues. Gaussian Bayesian Classifier is exploited to acquire a Gaussian\nBayesian Brain Map (GBBM) from the test brain MR images. GBBM is further\nprocessed to initialize the 3D FVF algorithm, which segments the brain tumor.\nThis algorithm has two major contributions. First, we present a NGMM to model\nhealthy brains. Second, we extend our 2D FVF algorithm to 3D space and use it\nfor brain tumor segmentation. The proposed method is validated on a publicly\navailable dataset.\n", "versions": [{"version": "v1", "created": "Wed, 1 May 2019 19:49:40 GMT"}], "update_date": "2019-05-03", "authors_parsed": [["Wang", "Tao", ""], ["Cheng", "Irene", ""], ["Basu", "Anup", ""]]}, {"id": "1905.00510", "submitter": "Nagesh Uba", "authors": "Nagesh Kumar Uba", "title": "Land Use and Land Cover Classification Using Deep Learning Techniques", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large datasets of sub-meter aerial imagery represented as orthophoto mosaics\nare widely available today, and these data sets may hold a great deal of\nuntapped information. This imagery has a potential to locate several types of\nfeatures; for example, forests, parking lots, airports, residential areas, or\nfreeways in the imagery. However, the appearances of these things vary based on\nmany things including the time that the image is captured, the sensor settings,\nprocessing done to rectify the image, and the geographical and cultural context\nof the region captured by the image. This thesis explores the use of deep\nconvolutional neural networks to classify land use from very high spatial\nresolution (VHR), orthorectified, visible band multispectral imagery. Recent\ntechnological and commercial applications have driven the collection a massive\namount of VHR images in the visible red, green, blue (RGB) spectral bands, this\nwork explores the potential for deep learning algorithms to exploit this\nimagery for automatic land use/ land cover (LULC) classification.\n", "versions": [{"version": "v1", "created": "Wed, 1 May 2019 21:38:54 GMT"}], "update_date": "2019-05-03", "authors_parsed": [["Uba", "Nagesh Kumar", ""]]}, {"id": "1905.00519", "submitter": "Daniel Barath", "authors": "Ivan Eichhardt, Daniel Barath", "title": "Optimal Multi-view Correction of Local Affine Frames", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The technique requires the epipolar geometry to be pre-estimated between each\nimage pair. It exploits the constraints which the camera movement implies, in\norder to apply a closed-form correction to the parameters of the input\naffinities. Also, it is shown that the rotations and scales obtained by\npartially affine-covariant detectors, e.g., AKAZE or SIFT, can be completed to\nbe full affine frames by the proposed algorithm. It is validated both in\nsynthetic experiments and on publicly available real-world datasets that the\nmethod always improves the output of the evaluated affine-covariant feature\ndetectors. As a by-product, these detectors are compared and the ones obtaining\nthe most accurate affine frames are reported. For demonstrating the\napplicability, we show that the proposed technique as a pre-processing step\nimproves the accuracy of pose estimation for a camera rig, surface normal and\nhomography estimation.\n", "versions": [{"version": "v1", "created": "Wed, 1 May 2019 22:17:59 GMT"}], "update_date": "2019-05-03", "authors_parsed": [["Eichhardt", "Ivan", ""], ["Barath", "Daniel", ""]]}, {"id": "1905.00525", "submitter": "Akshay Rangesh", "authors": "Walter Zimmer, Akshay Rangesh and Mohan Trivedi", "title": "3D BAT: A Semi-Automatic, Web-based 3D Annotation Toolbox for\n  Full-Surround, Multi-Modal Data Streams", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we focus on obtaining 2D and 3D labels, as well as track IDs\nfor objects on the road with the help of a novel 3D Bounding Box Annotation\nToolbox (3D BAT). Our open source, web-based 3D BAT incorporates several smart\nfeatures to improve usability and efficiency. For instance, this annotation\ntoolbox supports semi-automatic labeling of tracks using interpolation, which\nis vital for downstream tasks like tracking, motion planning and motion\nprediction. Moreover, annotations for all camera images are automatically\nobtained by projecting annotations from 3D space into the image domain. In\naddition to the raw image and point cloud feeds, a Masterview consisting of the\ntop view (bird's-eye-view), side view and front views is made available to\nobserve objects of interest from different perspectives. Comparisons of our\nmethod with other publicly available annotation tools reveal that 3D\nannotations can be obtained faster and more efficiently by using our toolbox.\n", "versions": [{"version": "v1", "created": "Wed, 1 May 2019 22:59:54 GMT"}], "update_date": "2019-05-03", "authors_parsed": [["Zimmer", "Walter", ""], ["Rangesh", "Akshay", ""], ["Trivedi", "Mohan", ""]]}, {"id": "1905.00526", "submitter": "Ramin Nabati", "authors": "Ramin Nabati, Hairong Qi", "title": "RRPN: Radar Region Proposal Network for Object Detection in Autonomous\n  Vehicles", "comments": "To appear in ICIP 2019", "journal-ref": null, "doi": "10.1109/ICIP.2019.8803392", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Region proposal algorithms play an important role in most state-of-the-art\ntwo-stage object detection networks by hypothesizing object locations in the\nimage. Nonetheless, region proposal algorithms are known to be the bottleneck\nin most two-stage object detection networks, increasing the processing time for\neach image and resulting in slow networks not suitable for real-time\napplications such as autonomous driving vehicles. In this paper we introduce\nRRPN, a Radar-based real-time region proposal algorithm for object detection in\nautonomous driving vehicles. RRPN generates object proposals by mapping Radar\ndetections to the image coordinate system and generating pre-defined anchor\nboxes for each mapped Radar detection point. These anchor boxes are then\ntransformed and scaled based on the object's distance from the vehicle, to\nprovide more accurate proposals for the detected objects. We evaluate our\nmethod on the newly released NuScenes dataset [1] using the Fast R-CNN object\ndetection network [2]. Compared to the Selective Search object proposal\nalgorithm [3], our model operates more than 100x faster while at the same time\nachieves higher detection precision and recall. Code has been made publicly\navailable at https://github.com/mrnabati/RRPN .\n", "versions": [{"version": "v1", "created": "Wed, 1 May 2019 23:03:22 GMT"}, {"version": "v2", "created": "Wed, 15 May 2019 14:45:23 GMT"}], "update_date": "2019-09-17", "authors_parsed": [["Nabati", "Ramin", ""], ["Qi", "Hairong", ""]]}, {"id": "1905.00538", "submitter": "Sunghoon Im", "authors": "Sunghoon Im, Hae-Gon Jeon, Stephen Lin, In So Kweon", "title": "DPSNet: End-to-end Deep Plane Sweep Stereo", "comments": "ICLR2019 accepted", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multiview stereo aims to reconstruct scene depth from images acquired by a\ncamera under arbitrary motion. Recent methods address this problem through deep\nlearning, which can utilize semantic cues to deal with challenges such as\ntextureless and reflective regions. In this paper, we present a convolutional\nneural network called DPSNet (Deep Plane Sweep Network) whose design is\ninspired by best practices of traditional geometry-based approaches for dense\ndepth reconstruction. Rather than directly estimating depth and/or optical flow\ncorrespondence from image pairs as done in many previous deep learning methods,\nDPSNet takes a plane sweep approach that involves building a cost volume from\ndeep features using the plane sweep algorithm, regularizing the cost volume via\na context-aware cost aggregation, and regressing the dense depth map from the\ncost volume. The cost volume is constructed using a differentiable warping\nprocess that allows for end-to-end training of the network. Through the\neffective incorporation of conventional multiview stereo concepts within a deep\nlearning framework, DPSNet achieves state-of-the-art reconstruction results on\na variety of challenging datasets.\n", "versions": [{"version": "v1", "created": "Thu, 2 May 2019 00:59:31 GMT"}], "update_date": "2019-05-03", "authors_parsed": [["Im", "Sunghoon", ""], ["Jeon", "Hae-Gon", ""], ["Lin", "Stephen", ""], ["Kweon", "In So", ""]]}, {"id": "1905.00546", "submitter": "I. Zeki Yalniz", "authors": "I. Zeki Yalniz, Herv\\'e J\\'egou, Kan Chen, Manohar Paluri, Dhruv\n  Mahajan", "title": "Billion-scale semi-supervised learning for image classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a study of semi-supervised learning with large\nconvolutional networks. We propose a pipeline, based on a teacher/student\nparadigm, that leverages a large collection of unlabelled images (up to 1\nbillion). Our main goal is to improve the performance for a given target\narchitecture, like ResNet-50 or ResNext. We provide an extensive analysis of\nthe success factors of our approach, which leads us to formulate some\nrecommendations to produce high-accuracy models for image classification with\nsemi-supervised learning. As a result, our approach brings important gains to\nstandard architectures for image, video and fine-grained classification. For\ninstance, by leveraging one billion unlabelled images, our learned vanilla\nResNet-50 achieves 81.2% top-1 accuracy on the ImageNet benchmark.\n", "versions": [{"version": "v1", "created": "Thu, 2 May 2019 02:08:18 GMT"}], "update_date": "2019-05-03", "authors_parsed": [["Yalniz", "I. Zeki", ""], ["J\u00e9gou", "Herv\u00e9", ""], ["Chen", "Kan", ""], ["Paluri", "Manohar", ""], ["Mahajan", "Dhruv", ""]]}, {"id": "1905.00561", "submitter": "Dhruv Mahajan", "authors": "Deepti Ghadiyaram, Matt Feiszli, Du Tran, Xueting Yan, Heng Wang,\n  Dhruv Mahajan", "title": "Large-scale weakly-supervised pre-training for video action recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current fully-supervised video datasets consist of only a few hundred\nthousand videos and fewer than a thousand domain-specific labels. This hinders\nthe progress towards advanced video architectures. This paper presents an\nin-depth study of using large volumes of web videos for pre-training video\nmodels for the task of action recognition. Our primary empirical finding is\nthat pre-training at a very large scale (over 65 million videos), despite on\nnoisy social-media videos and hashtags, substantially improves the\nstate-of-the-art on three challenging public action recognition datasets.\nFurther, we examine three questions in the construction of weakly-supervised\nvideo action datasets. First, given that actions involve interactions with\nobjects, how should one construct a verb-object pre-training label space to\nbenefit transfer learning the most? Second, frame-based models perform quite\nwell on action recognition; is pre-training for good image features sufficient\nor is pre-training for spatio-temporal features valuable for optimal transfer\nlearning? Finally, actions are generally less well-localized in long videos vs.\nshort videos; since action labels are provided at a video level, how should one\nchoose video clips for best performance, given some fixed budget of number or\nminutes of videos?\n", "versions": [{"version": "v1", "created": "Thu, 2 May 2019 03:05:43 GMT"}], "update_date": "2019-05-03", "authors_parsed": [["Ghadiyaram", "Deepti", ""], ["Feiszli", "Matt", ""], ["Tran", "Du", ""], ["Yan", "Xueting", ""], ["Wang", "Heng", ""], ["Mahajan", "Dhruv", ""]]}, {"id": "1905.00571", "submitter": "Xiaolong Ma", "authors": "Wei Niu, Xiaolong Ma, Yanzhi Wang, Bin Ren", "title": "26ms Inference Time for ResNet-50: Towards Real-Time Execution of all\n  DNNs on Smartphone", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the rapid emergence of a spectrum of high-end mobile devices, many\napplications that required desktop-level computation capability formerly can\nnow run on these devices without any problem. However, without a careful\noptimization, executing Deep Neural Networks (a key building block of the\nreal-time video stream processing that is the foundation of many popular\napplications) is still challenging, specifically, if an extremely low latency\nor high accuracy inference is needed. This work presents CADNN, a programming\nframework to efficiently execute DNN on mobile devices with the help of\nadvanced model compression (sparsity) and a set of thorough architecture-aware\noptimization. The evaluation result demonstrates that CADNN outperforms all the\nstate-of-the-art dense DNN execution frameworks like TensorFlow Lite and TVM.\n", "versions": [{"version": "v1", "created": "Thu, 2 May 2019 04:37:27 GMT"}], "update_date": "2019-05-03", "authors_parsed": [["Niu", "Wei", ""], ["Ma", "Xiaolong", ""], ["Wang", "Yanzhi", ""], ["Ren", "Bin", ""]]}, {"id": "1905.00582", "submitter": "Ekraam Sabir", "authors": "Ekraam Sabir, Jiaxin Cheng, Ayush Jaiswal, Wael AbdAlmageed, Iacopo\n  Masi, Prem Natarajan", "title": "Recurrent Convolutional Strategies for Face Manipulation Detection in\n  Videos", "comments": "To appear at Workshop on Applications of Computer Vision and Pattern\n  Recognition to Media Forensics at CVPR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The spread of misinformation through synthetically generated yet realistic\nimages and videos has become a significant problem, calling for robust\nmanipulation detection methods. Despite the predominant effort of detecting\nface manipulation in still images, less attention has been paid to the\nidentification of tampered faces in videos by taking advantage of the temporal\ninformation present in the stream. Recurrent convolutional models are a class\nof deep learning models which have proven effective at exploiting the temporal\ninformation from image streams across domains. We thereby distill the best\nstrategy for combining variations in these models along with domain specific\nface preprocessing techniques through extensive experimentation to obtain\nstate-of-the-art performance on publicly available video-based facial\nmanipulation benchmarks. Specifically, we attempt to detect Deepfake, Face2Face\nand FaceSwap tampered faces in video streams. Evaluation is performed on the\nrecently introduced FaceForensics++ dataset, improving the previous\nstate-of-the-art by up to 4.55% in accuracy.\n", "versions": [{"version": "v1", "created": "Thu, 2 May 2019 06:06:25 GMT"}, {"version": "v2", "created": "Tue, 7 May 2019 19:10:51 GMT"}, {"version": "v3", "created": "Thu, 16 May 2019 06:56:55 GMT"}], "update_date": "2019-05-17", "authors_parsed": [["Sabir", "Ekraam", ""], ["Cheng", "Jiaxin", ""], ["Jaiswal", "Ayush", ""], ["AbdAlmageed", "Wael", ""], ["Masi", "Iacopo", ""], ["Natarajan", "Prem", ""]]}, {"id": "1905.00593", "submitter": "Xi Yang", "authors": "Xi Yang, Bojian Wu, Issei Sato, Takeo Igarashi", "title": "Directing DNNs Attention for Facial Attribution Classification using\n  Gradient-weighted Class Activation Mapping", "comments": "CVPR-19 Workshop on Explainable AI", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks (DNNs) have a high accuracy on image classification\ntasks. However, DNNs trained by such dataset with co-occurrence bias may rely\non wrong features while making decisions for classification. It will greatly\naffect the transferability of pre-trained DNNs. In this paper, we propose an\ninteractive method to direct classifiers paying attentions to the regions that\nare manually specified by the users, in order to mitigate the influence of\nco-occurrence bias. We test on CelebA dataset, the pre-trained AlexNet is\nfine-tuned to focus on the specific facial attributes based on the results of\nGrad-CAM.\n", "versions": [{"version": "v1", "created": "Thu, 2 May 2019 07:05:33 GMT"}], "update_date": "2019-05-03", "authors_parsed": [["Yang", "Xi", ""], ["Wu", "Bojian", ""], ["Sato", "Issei", ""], ["Igarashi", "Takeo", ""]]}, {"id": "1905.00637", "submitter": "Chang-Hwan Son", "authors": "Chang-Hwan Son", "title": "Inverse Halftoning Through Structure-Aware Deep Convolutional Neural\n  Networks", "comments": null, "journal-ref": "Signal Processing, vol. 173, Aug. 2020", "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The primary issue in inverse halftoning is removing noisy dots on flat areas\nand restoring image structures (e.g., lines, patterns) on textured areas.\nHence, a new structure-aware deep convolutional neural network that\nincorporates two subnetworks is proposed in this paper. One subnetwork is for\nimage structure prediction while the other is for continuous-tone image\nreconstruction. First, to predict image structures, patch pairs comprising\ncontinuous-tone patches and the corresponding halftoned patches generated\nthrough digital halftoning are trained. Subsequently, gradient patches are\ngenerated by convolving gradient filters with the continuous-tone patches. The\nsubnetwork for the image structure prediction is trained using the mini-batch\ngradient descent algorithm given the halftoned patches and gradient patches,\nwhich are fed into the input and loss layers of the subnetwork, respectively.\nNext, the predicted map including the image structures is stacked on the top of\nthe input halftoned image through a fusion layer and fed into the image\nreconstruction subnetwork such that the entire network is trained adaptively to\nthe image structures. The experimental results confirm that the proposed\nstructure-aware network can remove noisy dot-patterns well on flat areas and\nrestore details clearly on textured areas. Furthermore, it is demonstrated that\nthe proposed method surpasses the conventional state-of-the-art methods based\non deep convolutional neural networks and locally learned dictionaries.\n", "versions": [{"version": "v1", "created": "Thu, 2 May 2019 09:39:54 GMT"}, {"version": "v2", "created": "Mon, 5 Aug 2019 06:49:11 GMT"}], "update_date": "2021-02-10", "authors_parsed": [["Son", "Chang-Hwan", ""]]}, {"id": "1905.00639", "submitter": "Giulia Orr\\`u", "authors": "Giulia Orr\\`u, Roberto Casula, Pierluigi Tuveri, Carlotta Bazzoni,\n  Giovanna Dessalvi, Marco Micheletto, Luca Ghiani, Gian Luca Marcialis", "title": "LivDet in Action - Fingerprint Liveness Detection Competition 2019", "comments": "Preprint version of a paper accepted at ICB 2019", "journal-ref": null, "doi": "10.1109/ICB45273.2019.8987281", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The International Fingerprint liveness Detection Competition (LivDet) is an\nopen and well-acknowledged meeting point of academies and private companies\nthat deal with the problem of distinguishing images coming from reproductions\nof fingerprints made of artificial materials and images relative to real\nfingerprints. In this edition of LivDet we invited the competitors to propose\nintegrated algorithms with matching systems. The goal was to investigate at\nwhich extent this integration impact on the whole performance. Twelve\nalgorithms were submitted to the competition, eight of which worked on\nintegrated systems.\n", "versions": [{"version": "v1", "created": "Thu, 2 May 2019 09:42:42 GMT"}], "update_date": "2020-07-08", "authors_parsed": [["Orr\u00f9", "Giulia", ""], ["Casula", "Roberto", ""], ["Tuveri", "Pierluigi", ""], ["Bazzoni", "Carlotta", ""], ["Dessalvi", "Giovanna", ""], ["Micheletto", "Marco", ""], ["Ghiani", "Luca", ""], ["Marcialis", "Gian Luca", ""]]}, {"id": "1905.00641", "submitter": "Jiankang Deng", "authors": "Jiankang Deng and Jia Guo and Yuxiang Zhou and Jinke Yu and Irene\n  Kotsia and Stefanos Zafeiriou", "title": "RetinaFace: Single-stage Dense Face Localisation in the Wild", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Though tremendous strides have been made in uncontrolled face detection,\naccurate and efficient face localisation in the wild remains an open challenge.\nThis paper presents a robust single-stage face detector, named RetinaFace,\nwhich performs pixel-wise face localisation on various scales of faces by\ntaking advantages of joint extra-supervised and self-supervised multi-task\nlearning. Specifically, We make contributions in the following five aspects:\n(1) We manually annotate five facial landmarks on the WIDER FACE dataset and\nobserve significant improvement in hard face detection with the assistance of\nthis extra supervision signal. (2) We further add a self-supervised mesh\ndecoder branch for predicting a pixel-wise 3D shape face information in\nparallel with the existing supervised branches. (3) On the WIDER FACE hard test\nset, RetinaFace outperforms the state of the art average precision (AP) by 1.1%\n(achieving AP equal to 91.4%). (4) On the IJB-C test set, RetinaFace enables\nstate of the art methods (ArcFace) to improve their results in face\nverification (TAR=89.59% for FAR=1e-6). (5) By employing light-weight backbone\nnetworks, RetinaFace can run real-time on a single CPU core for a\nVGA-resolution image. Extra annotations and code have been made available at:\nhttps://github.com/deepinsight/insightface/tree/master/RetinaFace.\n", "versions": [{"version": "v1", "created": "Thu, 2 May 2019 09:45:23 GMT"}, {"version": "v2", "created": "Sat, 4 May 2019 22:03:26 GMT"}], "update_date": "2019-05-07", "authors_parsed": [["Deng", "Jiankang", ""], ["Guo", "Jia", ""], ["Zhou", "Yuxiang", ""], ["Yu", "Jinke", ""], ["Kotsia", "Irene", ""], ["Zafeiriou", "Stefanos", ""]]}, {"id": "1905.00684", "submitter": "Xiaogang Xiong", "authors": "Xiaogang Xiong, Wenqing Chen, Zhichao Liu and Qiang Shen", "title": "DS-VIO: Robust and Efficient Stereo Visual Inertial Odometry based on\n  Dual Stage EKF", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a dual stage EKF (Extended Kalman Filter)-based algorithm\nfor the real-time and robust stereo VIO (visual inertial odometry). The first\nstage of this EKF-based algorithm performs the fusion of accelerometer and\ngyroscope while the second performs the fusion of stereo camera and IMU. Due to\nthe sufficient complementary characteristics between accelerometer and\ngyroscope as well as stereo camera and IMU, the dual stage EKF-based algorithm\ncan achieve a high precision of odometry estimations. At the same time, because\nof the low dimension of state vector in this algorithm, its computational\nefficiency is comparable to previous filter-based approaches. We call our\napproach DS-VIO (dual stage EKFbased stereo visual inertial odometry) and\nevaluate our DSVIO algorithm by comparing it with the state-of-art approaches\nincluding OKVIS, ROVIO, VINS-MONO and S-MSCKF on the EuRoC dataset. Results\nshow that our algorithm can achieve comparable or even better performances in\nterms of the RMS error\n", "versions": [{"version": "v1", "created": "Thu, 2 May 2019 11:59:25 GMT"}], "update_date": "2019-05-03", "authors_parsed": [["Xiong", "Xiaogang", ""], ["Chen", "Wenqing", ""], ["Liu", "Zhichao", ""], ["Shen", "Qiang", ""]]}, {"id": "1905.00693", "submitter": "Dakshina Ranjan Kisku", "authors": "Rinku Datta Rakshit, Dakshina Ranjan Kisku, Massimo Tistarelli,\n  Phalguni Gupta", "title": "Face Identification using Local Ternary Tree Pattern based Spatial\n  Structural Components", "comments": "13 pages, 5 figures, conference paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper reports a face identification system which makes use of a novel\nlocal descriptor called Local Ternary Tree Pattern (LTTP). Exploiting and\nextracting distinctive local descriptor from a face image plays a crucial role\nin face identification task in the presence of a variety of face images\nincluding constrained, unconstrained and plastic surgery images. LTTP has been\nused to extract robust and useful spatial features which use to describe the\nvarious structural components on a face. To extract the features, a ternary\ntree is formed for each pixel with its eight neighbors in each block. LTTP\npattern can be generated in four forms such as LTTP Left Depth (LTTP LD), LTTP\nLeft Breadth (LTTP LB), LTTP Right Depth (LTTP RD) and LTTP Right Breadth (LTTP\nRB). The encoding schemes of these patterns are very simple and efficient in\nterms of computational as well as time complexity. The proposed face\nidentification system is tested on six face databases, namely, the UMIST, the\nJAFFE, the extended Yale face B, the Plastic Surgery, the LFW and the UFI. The\nexperimental evaluation demonstrates the most promising results considering a\nvariety of faces captured under different environments. The proposed LTTP based\nsystem is also compared with some local descriptors under identical conditions.\n", "versions": [{"version": "v1", "created": "Thu, 2 May 2019 12:21:07 GMT"}, {"version": "v2", "created": "Thu, 16 Jul 2020 05:24:57 GMT"}], "update_date": "2020-07-17", "authors_parsed": [["Rakshit", "Rinku Datta", ""], ["Kisku", "Dakshina Ranjan", ""], ["Tistarelli", "Massimo", ""], ["Gupta", "Phalguni", ""]]}, {"id": "1905.00737", "submitter": "Sergi Caelles", "authors": "Sergi Caelles, Jordi Pont-Tuset, Federico Perazzi, Alberto Montes,\n  Kevis-Kokitsi Maninis, Luc Van Gool", "title": "The 2019 DAVIS Challenge on VOS: Unsupervised Multi-Object Segmentation", "comments": "CVPR 2019 Workshop/Challenge", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the 2019 DAVIS Challenge on Video Object Segmentation, the third\nedition of the DAVIS Challenge series, a public competition designed for the\ntask of Video Object Segmentation (VOS). In addition to the original\nsemi-supervised track and the interactive track introduced in the previous\nedition, a new unsupervised multi-object track will be featured this year. In\nthe newly introduced track, participants are asked to provide non-overlapping\nobject proposals on each image, along with an identifier linking them between\nframes (i.e. video object proposals), without any test-time human supervision\n(no scribbles or masks provided on the test video). In order to do so, we have\nre-annotated the train and val sets of DAVIS 2017 in a concise way that\nfacilitates the unsupervised track, and created new test-dev and test-challenge\nsets for the competition. Definitions, rules, and evaluation metrics for the\nunsupervised track are described in detail in this paper.\n", "versions": [{"version": "v1", "created": "Thu, 2 May 2019 13:40:43 GMT"}], "update_date": "2019-05-03", "authors_parsed": [["Caelles", "Sergi", ""], ["Pont-Tuset", "Jordi", ""], ["Perazzi", "Federico", ""], ["Montes", "Alberto", ""], ["Maninis", "Kevis-Kokitsi", ""], ["Van Gool", "Luc", ""]]}, {"id": "1905.00742", "submitter": "Georgios Kapidis", "authors": "Georgios Kapidis and Ronald Poppe and Elsbeth van Dam and Lucas P. J.\n  J. Noldus and Remco C. Veltkamp", "title": "Egocentric Hand Track and Object-based Human Action Recognition", "comments": "Accepted for publication at UIC 2019:Track 3, 8 pages, 5 figures,\n  Index terms: egocentric action recognition, hand detection, hand tracking,\n  hand identification, sequence classification, code available at:\n  https://github.com/georkap/hand_track_classification", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Egocentric vision is an emerging field of computer vision that is\ncharacterized by the acquisition of images and video from the first person\nperspective. In this paper we address the challenge of egocentric human action\nrecognition by utilizing the presence and position of detected regions of\ninterest in the scene explicitly, without further use of visual features.\n  Initially, we recognize that human hands are essential in the execution of\nactions and focus on obtaining their movements as the principal cues that\ndefine actions. We employ object detection and region tracking techniques to\nlocate hands and capture their movements. Prior knowledge about egocentric\nviews facilitates hand identification between left and right. With regard to\ndetection and tracking, we contribute a pipeline that successfully operates on\nunseen egocentric videos to find the camera wearer's hands and associate them\nthrough time. Moreover, we emphasize on the value of scene information for\naction recognition. We acknowledge that the presence of objects is significant\nfor the execution of actions by humans and in general for the description of a\nscene. To acquire this information, we utilize object detection for specific\nclasses that are relevant to the actions we want to recognize.\n  Our experiments are targeted on videos of kitchen activities from the\nEpic-Kitchens dataset. We model action recognition as a sequence learning\nproblem of the detected spatial positions in the frames. Our results show that\nexplicit hand and object detections with no other visual information can be\nrelied upon to classify hand-related human actions. Testing against methods\nfully dependent on visual features, signals that for actions where hand motions\nare conceptually important, a region-of-interest-based description of a video\ncontains equally expressive information with comparable classification\nperformance.\n", "versions": [{"version": "v1", "created": "Thu, 2 May 2019 13:43:28 GMT"}], "update_date": "2019-05-03", "authors_parsed": [["Kapidis", "Georgios", ""], ["Poppe", "Ronald", ""], ["van Dam", "Elsbeth", ""], ["Noldus", "Lucas P. J. J.", ""], ["Veltkamp", "Remco C.", ""]]}, {"id": "1905.00745", "submitter": "Hichem Sahbi", "authors": "Ahmed Mazari and Hichem Sahbi", "title": "Human Action Recognition with Deep Temporal Pyramids", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep convolutional neural networks (CNNs) are nowadays achieving significant\nleaps in different pattern recognition tasks including action recognition.\nCurrent CNNs are increasingly deeper, data-hungrier and this makes their\nsuccess tributary of the abundance of labeled training data. CNNs also rely on\nmax/average pooling which reduces dimensionality of output layers and hence\nattenuates their sensitivity to the availability of labeled data. However, this\nprocess may dilute the information of upstream convolutional layers and thereby\naffect the discrimination power of the trained representations, especially when\nthe learned categories are fine-grained.\n  In this paper, we introduce a novel hierarchical aggregation design, for\nfinal pooling, that controls granularity of the learned representations w.r.t\nthe actual granularity of action categories. Our solution is based on a\ntree-structured temporal pyramid that aggregates outputs of CNNs at different\nlevels. Top levels of this hierarchy are dedicated to coarse categories while\ndeep levels are more suitable to fine-grained ones. The design of our temporal\npyramid is based on solving a constrained minimization problem whose solution\ncorresponds to the distribution of weights of different representations in the\ntemporal pyramid. Experiments conducted using the challenging UCF101 database\nshow the relevance of our hierarchical design w.r.t other related methods.\n", "versions": [{"version": "v1", "created": "Thu, 2 May 2019 13:48:17 GMT"}], "update_date": "2019-05-03", "authors_parsed": [["Mazari", "Ahmed", ""], ["Sahbi", "Hichem", ""]]}, {"id": "1905.00773", "submitter": "Radu Tudor Ionescu", "authors": "Mariana-Iuliana Georgescu, Radu Tudor Ionescu", "title": "Clustering Images by Unmasking - A New Baseline", "comments": "Accepted at ICIP 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel agglomerative clustering method based on unmasking, a\ntechnique that was previously used for authorship verification of text\ndocuments and for abnormal event detection in videos. In order to join two\nclusters, we alternate between (i) training a binary classifier to distinguish\nbetween the samples from one cluster and the samples from the other cluster,\nand (ii) removing at each step the most discriminant features. The\nfaster-decreasing accuracy rates of the intermediately-obtained classifiers\nindicate that the two clusters should be joined. To the best of our knowledge,\nthis is the first work to apply unmasking in order to cluster images. We\ncompare our method with k-means as well as a recent state-of-the-art clustering\nmethod. The empirical results indicate that our approach is able to improve\nperformance for various (deep and shallow) feature representations and\ndifferent tasks, such as handwritten digit recognition, texture classification\nand fine-grained object recognition.\n", "versions": [{"version": "v1", "created": "Thu, 2 May 2019 14:35:29 GMT"}], "update_date": "2019-05-03", "authors_parsed": [["Georgescu", "Mariana-Iuliana", ""], ["Ionescu", "Radu Tudor", ""]]}, {"id": "1905.00780", "submitter": "Suraj Srinivas", "authors": "Suraj Srinivas, Francois Fleuret", "title": "Full-Gradient Representation for Neural Network Visualization", "comments": "NeurIPS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We introduce a new tool for interpreting neural net responses, namely\nfull-gradients, which decomposes the neural net response into input sensitivity\nand per-neuron sensitivity components. This is the first proposed\nrepresentation which satisfies two key properties: completeness and weak\ndependence, which provably cannot be satisfied by any saliency map-based\ninterpretability method. For convolutional nets, we also propose an approximate\nsaliency map representation, called FullGrad, obtained by aggregating the\nfull-gradient components.\n  We experimentally evaluate the usefulness of FullGrad in explaining model\nbehaviour with two quantitative tests: pixel perturbation and\nremove-and-retrain. Our experiments reveal that our method explains model\nbehaviour correctly, and more comprehensively than other methods in the\nliterature. Visual inspection also reveals that our saliency maps are sharper\nand more tightly confined to object regions than other methods.\n", "versions": [{"version": "v1", "created": "Thu, 2 May 2019 14:41:31 GMT"}, {"version": "v2", "created": "Wed, 24 Jul 2019 09:43:43 GMT"}, {"version": "v3", "created": "Thu, 5 Sep 2019 11:47:37 GMT"}, {"version": "v4", "created": "Tue, 3 Dec 2019 13:59:05 GMT"}], "update_date": "2019-12-04", "authors_parsed": [["Srinivas", "Suraj", ""], ["Fleuret", "Francois", ""]]}, {"id": "1905.00789", "submitter": "Sheng Lin", "authors": "Sheng Lin, Xiaolong Ma, Shaokai Ye, Geng Yuan, Kaisheng Ma, Yanzhi\n  Wang", "title": "Toward Extremely Low Bit and Lossless Accuracy in DNNs with Progressive\n  ADMM", "comments": "Accepted by ICML workshop (ODML-CDNNR2019). arXiv admin note:\n  substantial text overlap with arXiv:1903.09769", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Weight quantization is one of the most important techniques of Deep Neural\nNetworks (DNNs) model compression method. A recent work using systematic\nframework of DNN weight quantization with the advanced optimization algorithm\nADMM (Alternating Direction Methods of Multipliers) achieves one of\nstate-of-art results in weight quantization. In this work, we first extend such\nADMM-based framework to guarantee solution feasibility and we have further\ndeveloped a multi-step, progressive DNN weight quantization framework, with\ndual benefits of (i) achieving further weight quantization thanks to the\nspecial property of ADMM regularization, and (ii) reducing the search space\nwithin each step. Extensive experimental results demonstrate the superior\nperformance compared with prior work. Some highlights: we derive the first\nlossless and fully binarized (for all layers) LeNet-5 for MNIST; And we derive\nthe first fully binarized (for all layers) VGG-16 for CIFAR-10 and ResNet for\nImageNet with reasonable accuracy loss.\n", "versions": [{"version": "v1", "created": "Thu, 2 May 2019 14:53:24 GMT"}], "update_date": "2019-05-03", "authors_parsed": [["Lin", "Sheng", ""], ["Ma", "Xiaolong", ""], ["Ye", "Shaokai", ""], ["Yuan", "Geng", ""], ["Ma", "Kaisheng", ""], ["Wang", "Yanzhi", ""]]}, {"id": "1905.00824", "submitter": "Tiancheng Sun", "authors": "Tiancheng Sun, Jonathan T. Barron, Yun-Ta Tsai, Zexiang Xu, Xueming\n  Yu, Graham Fyffe, Christoph Rhemann, Jay Busch, Paul Debevec, and Ravi\n  Ramamoorthi", "title": "Single Image Portrait Relighting", "comments": "SIGGRAPH 2019 Technical Paper accepted", "journal-ref": "ACM Transactions on Graphics (SIGGRAPH 2019) 38 (4)", "doi": "10.1145/3306346.3323008", "report-no": null, "categories": "cs.GR cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Lighting plays a central role in conveying the essence and depth of the\nsubject in a portrait photograph. Professional photographers will carefully\ncontrol the lighting in their studio to manipulate the appearance of their\nsubject, while consumer photographers are usually constrained to the\nillumination of their environment. Though prior works have explored techniques\nfor relighting an image, their utility is usually limited due to requirements\nof specialized hardware, multiple images of the subject under controlled or\nknown illuminations, or accurate models of geometry and reflectance. To this\nend, we present a system for portrait relighting: a neural network that takes\nas input a single RGB image of a portrait taken with a standard cellphone\ncamera in an unconstrained environment, and from that image produces a relit\nimage of that subject as though it were illuminated according to any provided\nenvironment map. Our method is trained on a small database of 18 individuals\ncaptured under different directional light sources in a controlled light stage\nsetup consisting of a densely sampled sphere of lights. Our proposed technique\nproduces quantitatively superior results on our dataset's validation set\ncompared to prior works, and produces convincing qualitative relighting results\non a dataset of hundreds of real-world cellphone portraits. Because our\ntechnique can produce a 640 $\\times$ 640 image in only 160 milliseconds, it may\nenable interactive user-facing photographic applications in the future.\n", "versions": [{"version": "v1", "created": "Thu, 2 May 2019 15:56:15 GMT"}], "update_date": "2019-05-03", "authors_parsed": [["Sun", "Tiancheng", ""], ["Barron", "Jonathan T.", ""], ["Tsai", "Yun-Ta", ""], ["Xu", "Zexiang", ""], ["Yu", "Xueming", ""], ["Fyffe", "Graham", ""], ["Rhemann", "Christoph", ""], ["Busch", "Jay", ""], ["Debevec", "Paul", ""], ["Ramamoorthi", "Ravi", ""]]}, {"id": "1905.00851", "submitter": "Thomas M\\\"ollenhoff", "authors": "Thomas M\\\"ollenhoff, Daniel Cremers", "title": "Lifting Vectorial Variational Problems: A Natural Formulation based on\n  Geometric Measure Theory and Discrete Exterior Calculus", "comments": "Oral presentation at CVPR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Numerous tasks in imaging and vision can be formulated as variational\nproblems over vector-valued maps. We approach the relaxation and\nconvexification of such vectorial variational problems via a lifting to the\nspace of currents. To that end, we recall that functionals with polyconvex\nLagrangians can be reparametrized as convex one-homogeneous functionals on the\ngraph of the function. This leads to an equivalent shape optimization problem\nover oriented surfaces in the product space of domain and codomain. A convex\nformulation is then obtained by relaxing the search space from oriented\nsurfaces to more general currents. We propose a discretization of the resulting\ninfinite-dimensional optimization problem using Whitney forms, which also\ngeneralizes recent \"sublabel-accurate\" multilabeling approaches.\n", "versions": [{"version": "v1", "created": "Thu, 2 May 2019 16:54:58 GMT"}], "update_date": "2019-05-03", "authors_parsed": [["M\u00f6llenhoff", "Thomas", ""], ["Cremers", "Daniel", ""]]}, {"id": "1905.00875", "submitter": "Weidi Xie", "authors": "Zihang Lai and Weidi Xie", "title": "Self-supervised Learning for Video Correspondence Flow", "comments": "BMVC2019 (Oral Presentation)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The objective of this paper is self-supervised learning of feature embeddings\nthat are suitable for matching correspondences along the videos, which we term\ncorrespondence flow. By leveraging the natural spatial-temporal coherence in\nvideos, we propose to train a ``pointer'' that reconstructs a target frame by\ncopying pixels from a reference frame.\n  We make the following contributions: First, we introduce a simple information\nbottleneck that forces the model to learn robust features for correspondence\nmatching, and prevent it from learning trivial solutions, \\eg matching based on\nlow-level colour information. Second, to tackle the challenges from tracker\ndrifting, due to complex object deformations, illumination changes and\nocclusions, we propose to train a recursive model over long temporal windows\nwith scheduled sampling and cycle consistency. Third, we achieve\nstate-of-the-art performance on DAVIS 2017 video segmentation and JHMDB\nkeypoint tracking tasks, outperforming all previous self-supervised learning\napproaches by a significant margin. Fourth, in order to shed light on the\npotential of self-supervised learning on the task of video correspondence flow,\nwe probe the upper bound by training on additional data, \\ie more diverse\nvideos, further demonstrating significant improvements on video segmentation.\n", "versions": [{"version": "v1", "created": "Thu, 2 May 2019 17:45:16 GMT"}, {"version": "v2", "created": "Thu, 9 May 2019 21:55:38 GMT"}, {"version": "v3", "created": "Sat, 6 Jul 2019 11:43:28 GMT"}, {"version": "v4", "created": "Sat, 20 Jul 2019 21:59:59 GMT"}, {"version": "v5", "created": "Sat, 27 Jul 2019 22:59:37 GMT"}], "update_date": "2019-07-30", "authors_parsed": [["Lai", "Zihang", ""], ["Xie", "Weidi", ""]]}, {"id": "1905.00889", "submitter": "Ben Mildenhall", "authors": "Ben Mildenhall, Pratul P. Srinivasan, Rodrigo Ortiz-Cayon, Nima\n  Khademi Kalantari, Ravi Ramamoorthi, Ren Ng, Abhishek Kar", "title": "Local Light Field Fusion: Practical View Synthesis with Prescriptive\n  Sampling Guidelines", "comments": "SIGGRAPH 2019. Project page with video and code:\n  http://people.eecs.berkeley.edu/~bmild/llff/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a practical and robust deep learning solution for capturing and\nrendering novel views of complex real world scenes for virtual exploration.\nPrevious approaches either require intractably dense view sampling or provide\nlittle to no guidance for how users should sample views of a scene to reliably\nrender high-quality novel views. Instead, we propose an algorithm for view\nsynthesis from an irregular grid of sampled views that first expands each\nsampled view into a local light field via a multiplane image (MPI) scene\nrepresentation, then renders novel views by blending adjacent local light\nfields. We extend traditional plenoptic sampling theory to derive a bound that\nspecifies precisely how densely users should sample views of a given scene when\nusing our algorithm. In practice, we apply this bound to capture and render\nviews of real world scenes that achieve the perceptual quality of Nyquist rate\nview sampling while using up to 4000x fewer views. We demonstrate our\napproach's practicality with an augmented reality smartphone app that guides\nusers to capture input images of a scene and viewers that enable realtime\nvirtual exploration on desktop and mobile platforms.\n", "versions": [{"version": "v1", "created": "Thu, 2 May 2019 17:58:52 GMT"}], "update_date": "2019-05-03", "authors_parsed": [["Mildenhall", "Ben", ""], ["Srinivasan", "Pratul P.", ""], ["Ortiz-Cayon", "Rodrigo", ""], ["Kalantari", "Nima Khademi", ""], ["Ramamoorthi", "Ravi", ""], ["Ng", "Ren", ""], ["Kar", "Abhishek", ""]]}, {"id": "1905.00933", "submitter": "Jae Woong Soh", "authors": "Jae Woong Soh, Jae Sung Park, Nam Ik Cho", "title": "Joint High Dynamic Range Imaging and Super-Resolution from a Single\n  Image", "comments": "11 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a new framework for jointly enhancing the resolution and\nthe dynamic range of an image, i.e., simultaneous super-resolution (SR) and\nhigh dynamic range imaging (HDRI), based on a convolutional neural network\n(CNN). From the common trends of both tasks, we train a CNN for the joint HDRI\nand SR by focusing on the reconstruction of high-frequency details.\nSpecifically, the high-frequency component in our work is the reflectance\ncomponent according to the Retinex-based image decomposition, and only the\nreflectance component is manipulated by the CNN while another component\n(illumination) is processed in a conventional way. In training the CNN, we\ndevise an appropriate loss function that contributes to the naturalness quality\nof resulting images. Experiments show that our algorithm outperforms the\ncascade implementation of CNN-based SR and HDRI.\n", "versions": [{"version": "v1", "created": "Thu, 2 May 2019 18:57:18 GMT"}], "update_date": "2019-05-06", "authors_parsed": [["Soh", "Jae Woong", ""], ["Park", "Jae Sung", ""], ["Cho", "Nam Ik", ""]]}, {"id": "1905.00934", "submitter": "Fangda Li", "authors": "Fangda Li, Ankit Manerikar, Tanmay Prakash and Avinash Kak", "title": "A Splitting-Based Iterative Algorithm for GPU-Accelerated Statistical\n  Dual-Energy X-Ray CT Reconstruction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When dealing with material classification in baggage at airports, Dual-Energy\nComputed Tomography (DECT) allows characterization of any given material with\ncoefficients based on two attenuative effects: Compton scattering and\nphotoelectric absorption. However, straightforward projection-domain\ndecomposition methods for this characterization often yield poor\nreconstructions due to the high dynamic range of material properties\nencountered in an actual luggage scan. Hence, for better reconstruction quality\nunder a timing constraint, we propose a splitting-based, GPU-accelerated,\nstatistical DECT reconstruction algorithm. Compared to prior art, our main\ncontribution lies in the significant acceleration made possible by separating\nreconstruction and decomposition within an ADMM framework. Experimental\nresults, on both synthetic and real-world baggage phantoms, demonstrate a\nsignificant reduction in time required for convergence.\n", "versions": [{"version": "v1", "created": "Thu, 2 May 2019 18:58:42 GMT"}], "update_date": "2019-05-06", "authors_parsed": [["Li", "Fangda", ""], ["Manerikar", "Ankit", ""], ["Prakash", "Tanmay", ""], ["Kak", "Avinash", ""]]}, {"id": "1905.00941", "submitter": "Fabio Pizzati", "authors": "Fabio Pizzati and Fernando Garc\\'ia", "title": "Enhanced free space detection in multiple lanes based on single CNN with\n  scene identification", "comments": "Will appear in the 2019 IEEE Intelligent Vehicles Symposium (IV 2019)", "journal-ref": "2019 IEEE Intelligent Vehicles Symposium (IV)", "doi": "10.1109/IVS.2019.8814181", "report-no": null, "categories": "cs.CV cs.LG cs.RO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Many systems for autonomous vehicles' navigation rely on lane detection.\nTraditional algorithms usually estimate only the position of the lanes on the\nroad, but an autonomous control system may also need to know if a lane marking\ncan be crossed or not, and what portion of space inside the lane is free from\nobstacles, to make safer control decisions. On the other hand, free space\ndetection algorithms only detect navigable areas, without information about\nlanes. State-of-the-art algorithms use CNNs for both tasks, with significant\nconsumption of computing resources. We propose a novel approach that estimates\nthe free space inside each lane, with a single CNN. Additionally, adding only a\nsmall requirement concerning GPU RAM, we infer the road type, that will be\nuseful for path planning. To achieve this result, we train a multi-task CNN.\nThen, we further elaborate the output of the network, to extract polygons that\ncan be effectively used in navigation control. Finally, we provide a\ncomputationally efficient implementation, based on ROS, that can be executed in\nreal time. Our code and trained models are available online.\n", "versions": [{"version": "v1", "created": "Thu, 2 May 2019 19:26:07 GMT"}, {"version": "v2", "created": "Mon, 6 May 2019 07:00:33 GMT"}], "update_date": "2019-09-13", "authors_parsed": [["Pizzati", "Fabio", ""], ["Garc\u00eda", "Fernando", ""]]}, {"id": "1905.00953", "submitter": "Kaiyang Zhou", "authors": "Kaiyang Zhou, Yongxin Yang, Andrea Cavallaro, Tao Xiang", "title": "Omni-Scale Feature Learning for Person Re-Identification", "comments": "ICCV 2019; This version adds additional training recipes for\n  practitioners", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As an instance-level recognition problem, person re-identification (ReID)\nrelies on discriminative features, which not only capture different spatial\nscales but also encapsulate an arbitrary combination of multiple scales. We\ncall features of both homogeneous and heterogeneous scales omni-scale features.\nIn this paper, a novel deep ReID CNN is designed, termed Omni-Scale Network\n(OSNet), for omni-scale feature learning. This is achieved by designing a\nresidual block composed of multiple convolutional streams, each detecting\nfeatures at a certain scale. Importantly, a novel unified aggregation gate is\nintroduced to dynamically fuse multi-scale features with input-dependent\nchannel-wise weights. To efficiently learn spatial-channel correlations and\navoid overfitting, the building block uses pointwise and depthwise\nconvolutions. By stacking such block layer-by-layer, our OSNet is extremely\nlightweight and can be trained from scratch on existing ReID benchmarks.\nDespite its small model size, OSNet achieves state-of-the-art performance on\nsix person ReID datasets, outperforming most large-sized models, often by a\nclear margin. Code and models are available at:\n\\url{https://github.com/KaiyangZhou/deep-person-reid}.\n", "versions": [{"version": "v1", "created": "Thu, 2 May 2019 20:42:26 GMT"}, {"version": "v2", "created": "Mon, 1 Jul 2019 14:25:20 GMT"}, {"version": "v3", "created": "Thu, 25 Jul 2019 11:00:19 GMT"}, {"version": "v4", "created": "Mon, 5 Aug 2019 13:53:57 GMT"}, {"version": "v5", "created": "Wed, 11 Sep 2019 22:44:41 GMT"}, {"version": "v6", "created": "Wed, 18 Dec 2019 09:29:53 GMT"}], "update_date": "2019-12-19", "authors_parsed": [["Zhou", "Kaiyang", ""], ["Yang", "Yongxin", ""], ["Cavallaro", "Andrea", ""], ["Xiang", "Tao", ""]]}, {"id": "1905.00954", "submitter": "Zhongang Qi", "authors": "Zhongang Qi, Saeed Khorram, Li Fuxin", "title": "Visualizing Deep Networks by Optimizing with Integrated Gradients", "comments": null, "journal-ref": "AAAI 2020", "doi": "10.1609/aaai.v34i07.6863", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding and interpreting the decisions made by deep learning models is\nvaluable in many domains. In computer vision, computing heatmaps from a deep\nnetwork is a popular approach for visualizing and understanding deep networks.\nHowever, heatmaps that do not correlate with the network may mislead human,\nhence the performance of heatmaps in providing a faithful explanation to the\nunderlying deep network is crucial. In this paper, we propose I-GOS, which\noptimizes for a heatmap so that the classification scores on the masked image\nwould maximally decrease. The main novelty of the approach is to compute\ndescent directions based on the integrated gradients instead of the normal\ngradient, which avoids local optima and speeds up convergence. Compared with\nprevious approaches, our method can flexibly compute heatmaps at any resolution\nfor different user needs. Extensive experiments on several benchmark datasets\nshow that the heatmaps produced by our approach are more correlated with the\ndecision of the underlying deep network, in comparison with other\nstate-of-the-art approaches.\n", "versions": [{"version": "v1", "created": "Thu, 2 May 2019 20:45:37 GMT"}, {"version": "v2", "created": "Fri, 11 Dec 2020 10:55:28 GMT"}], "update_date": "2020-12-14", "authors_parsed": [["Qi", "Zhongang", ""], ["Khorram", "Saeed", ""], ["Fuxin", "Li", ""]]}, {"id": "1905.00966", "submitter": "Sahand Sharifzadeh", "authors": "Sahand Sharifzadeh, Sina Moayed Baharlou, Max Berrendorf, Rajat Koner,\n  Volker Tresp", "title": "Improving Visual Relation Detection using Depth Maps", "comments": "International Conference on Pattern Recognition 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual relation detection methods rely on object information extracted from\nRGB images such as 2D bounding boxes, feature maps, and predicted class\nprobabilities. We argue that depth maps can additionally provide valuable\ninformation on object relations, e.g. helping to detect not only spatial\nrelations, such as standing behind, but also non-spatial relations, such as\nholding. In this work, we study the effect of using different object features\nwith a focus on depth maps. To enable this study, we release a new synthetic\ndataset of depth maps, VG-Depth, as an extension to Visual Genome (VG). We also\nnote that given the highly imbalanced distribution of relations in VG, typical\nevaluation metrics for visual relation detection cannot reveal improvements of\nunder-represented relations. To address this problem, we propose using an\nadditional metric, calling it Macro Recall@K, and demonstrate its remarkable\nperformance on VG. Finally, our experiments confirm that by effective\nutilization of depth maps within a simple, yet competitive framework, the\nperformance of visual relation detection can be improved by a margin of up to\n8%.\n", "versions": [{"version": "v1", "created": "Thu, 2 May 2019 21:14:35 GMT"}, {"version": "v2", "created": "Mon, 7 Oct 2019 13:33:27 GMT"}, {"version": "v3", "created": "Fri, 24 Jan 2020 15:00:23 GMT"}, {"version": "v4", "created": "Sat, 17 Oct 2020 13:58:38 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Sharifzadeh", "Sahand", ""], ["Baharlou", "Sina Moayed", ""], ["Berrendorf", "Max", ""], ["Koner", "Rajat", ""], ["Tresp", "Volker", ""]]}, {"id": "1905.00989", "submitter": "Matthew Parno", "authors": "M. D. Parno, B. A. West, A. J. Song, T. S. Hodgdon, and D. T. O'Connor", "title": "Remote measurement of sea ice dynamics with regularized optimal\n  transport", "comments": null, "journal-ref": null, "doi": "10.1029/2019GL083037", "report-no": null, "categories": "cs.CV stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As Arctic conditions rapidly change, human activity in the Arctic will\ncontinue to increase and so will the need for high-resolution observations of\nsea ice. While satellite imagery can provide high spatial resolution, it is\ntemporally sparse and significant ice deformation can occur between\nobservations. This makes it difficult to apply feature tracking or image\ncorrelation techniques that require persistent features to exist between\nimages. With this in mind, we propose a technique based on optimal transport,\nwhich is commonly used to measure differences between probability\ndistributions. When little ice enters or leaves the image scene, we show that\nregularized optimal transport can be used to quantitatively estimate ice\ndeformation. We discuss the motivation for our approach and describe efficient\ncomputational implementations. Results are provided on a combination of\nsynthetic and MODIS imagery to demonstrate the ability of our approach to\nestimate dynamics properties at the original image resolution.\n", "versions": [{"version": "v1", "created": "Thu, 2 May 2019 22:47:22 GMT"}], "update_date": "2019-06-03", "authors_parsed": [["Parno", "M. D.", ""], ["West", "B. A.", ""], ["Song", "A. J.", ""], ["Hodgdon", "T. S.", ""], ["O'Connor", "D. T.", ""]]}, {"id": "1905.00996", "submitter": "Xuan Cao", "authors": "Xuan Cao, Yanhao Ge, Ying Tai, Wei Zhang, Jian Li, Chengjie Wang,\n  Jilin Li, Feiyue Huang", "title": "Anti-Confusing: Region-Aware Network for Human Pose Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we propose a novel framework named Region-Aware Network\n(RANet), which learns the ability of anti-confusing in case of heavy occlusion,\nnearby person and symmetric appearance, for human pose estimation.\nSpecifically, the proposed method addresses three key aspects, i.e., data\naugmentation, feature learning and prediction fusion, respectively. First, we\npropose Parsing-based Data Augmentation (PDA) to generate abundant data that\nsynthesizes confusing textures. Second, we not only propose a Feature Pyramid\nStem (FPS) to learn stronger low-level features in lower stage; but also\nincorporate an Effective Region Extraction (ERE) module to excavate better\ntarget-specific features. Third, we introduce Cascade Voting Fusion (CVF) to\nexplicitly exclude the inferior predictions and fuse the rest effective\npredictions for the final pose estimation. Extensive experimental results on\ntwo popular benchmarks, i.e. MPII and LSP, demonstrate the effectiveness of our\nmethod against the state-of-the-art competitors. Especially on\neasily-confusable joints, our method makes significant improvement.\n", "versions": [{"version": "v1", "created": "Fri, 3 May 2019 00:53:03 GMT"}, {"version": "v2", "created": "Mon, 27 May 2019 03:52:51 GMT"}], "update_date": "2019-05-28", "authors_parsed": [["Cao", "Xuan", ""], ["Ge", "Yanhao", ""], ["Tai", "Ying", ""], ["Zhang", "Wei", ""], ["Li", "Jian", ""], ["Wang", "Chengjie", ""], ["Li", "Jilin", ""], ["Huang", "Feiyue", ""]]}, {"id": "1905.01003", "submitter": "Trung Dung Do", "authors": "Trung Dung Do, Xuenan Cui, Thi Hai Binh Nguyen, Hakil Kim, and Van\n  Huan Nguyen", "title": "Blind Deconvolution Method using Omnidirectional Gabor Filter-based Edge\n  Information", "comments": "6 pages, 7 figures, 3 tables, conference paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the previous blind deconvolution methods, de-blurred images can be\nobtained by using the edge or pixel information. However, the existing\nedge-based methods did not take advantage of edge information in\nommi-directions, but only used horizontal and vertical edges when recovering\nthe de-blurred images. This limitation lowers the quality of the recovered\nimages. This paper proposes a method which utilizes edges in different\ndirections to recover the true sharp image. We also provide a statistical table\nscore to show how many directions are enough to recover a high quality true\nsharp image. In order to grade the quality of the deblurring image, we\nintroduce a measurement, namely Haar defocus score that takes advantage of the\nHaar-Wavelet transform. The experimental results prove that the proposed method\nobtains a high quality deblurred image with respect to both the Haar defocus\nscore and the Peak Signal to Noise Ratio.\n", "versions": [{"version": "v1", "created": "Fri, 3 May 2019 01:51:47 GMT"}], "update_date": "2019-05-06", "authors_parsed": [["Do", "Trung Dung", ""], ["Cui", "Xuenan", ""], ["Nguyen", "Thi Hai Binh", ""], ["Kim", "Hakil", ""], ["Nguyen", "Van Huan", ""]]}, {"id": "1905.01013", "submitter": "Trung Dung Do", "authors": "Trung Dung Do, Hakil Kim, and Van Huan Nguyen", "title": "Real-time and robust multiple-view gender classification using gait\n  features in video surveillance", "comments": "14 pages, 8 figures, 8 tables, journal paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is common to view people in real applications walking in arbitrary\ndirections, holding items, or wearing heavy coats. These factors are challenges\nin gait-based application methods because they significantly change a person's\nappearance. This paper proposes a novel method for classifying human gender in\nreal time using gait information. The use of an average gait image (AGI),\nrather than a gait energy image (GEI), allows this method to be computationally\nefficient and robust against view changes. A viewpoint (VP) model is created\nfor automatically determining the viewing angle during the testing phase. A\ndistance signal (DS) model is constructed to remove any areas with an\nattachment (carried items, worn coats) from a silhouette to reduce the\ninterference in the resulting classification. Finally, the human gender is\nclassified using multiple view-dependent classifiers trained using a support\nvector machine. Experiment results confirm that the proposed method achieves a\nhigh accuracy of 98.8% on the CASIA Dataset B and outperforms the recent\nstate-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Fri, 3 May 2019 02:50:41 GMT"}], "update_date": "2019-05-06", "authors_parsed": [["Do", "Trung Dung", ""], ["Kim", "Hakil", ""], ["Nguyen", "Van Huan", ""]]}, {"id": "1905.01025", "submitter": "Ming Lu", "authors": "Ming Lu, Ming Cheng, Yiling Xu, Shiliang Pu, Qiu Shen, and Zhan Ma", "title": "Learned Quality Enhancement via Multi-Frame Priors for HEVC Compliant\n  Low-Delay Applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Networked video applications, e.g., video conferencing, often suffer from\npoor visual quality due to unexpected network fluctuation and limited\nbandwidth. In this paper, we have developed a Quality Enhancement Network\n(QENet) to reduce the video compression artifacts, leveraging the spatial and\ntemporal priors generated by respective multi-scale convolutions spatially and\nwarped temporal predictions in a recurrent fashion temporally. We have\nintegrated this QENet as a standard-alone post-processing subsystem to the High\nEfficiency Video Coding (HEVC) compliant decoder. Experimental results show\nthat our QENet demonstrates the state-of-the-art performance against default\nin-loop filters in HEVC and other deep learning based methods with noticeable\nobjective gains in Peak-Signal-to-Noise Ratio (PSNR) and subjective gains\nvisually.\n", "versions": [{"version": "v1", "created": "Fri, 3 May 2019 03:05:09 GMT"}], "update_date": "2019-05-06", "authors_parsed": [["Lu", "Ming", ""], ["Cheng", "Ming", ""], ["Xu", "Yiling", ""], ["Pu", "Shiliang", ""], ["Shen", "Qiu", ""], ["Ma", "Zhan", ""]]}, {"id": "1905.01040", "submitter": "Zixu Zhao", "authors": "Zixu Zhao, Huangjing Lin, Hao Chen, Pheng-Ann Heng", "title": "PFA-ScanNet: Pyramidal Feature Aggregation with Synergistic Learning for\n  Breast Cancer Metastasis Analysis", "comments": "Accepted by MICCAI 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic detection of cancer metastasis from whole slide images (WSIs) is a\ncrucial step for following patient staging and prognosis. Recent convolutional\nneural network based approaches are struggling with the trade-off between\naccuracy and computational efficiency due to the difficulty in processing\nlarge-scale gigapixel WSIs. To meet this challenge, we propose a novel\nPyramidal Feature Aggregation ScanNet (PFA-ScanNet) for robust and fast\nanalysis of breast cancer metastasis. Our method mainly benefits from the\naggregation of extracted local-to-global features with diverse receptive\nfields, as well as the proposed synergistic learning for training the main\ndetector and extra decoder with semantic guidance. Furthermore, a\nhigh-efficiency inference mechanism is designed with dense pooling layers,\nwhich allows dense and fast scanning for gigapixel WSI analysis. As a result,\nthe proposed PFA-ScanNet achieved the state-of-the-art FROC of 90.2% on the\nCamelyon16 dataset, as well as competitive kappa score of 0.905 on the\nCamelyon17 leaderboard. In addition, our method shows leading speed advantage\nover other methods, about 7.2 min per WSI with a single GPU, making automatic\nanalysis of breast cancer metastasis more applicable in the clinical usage.\n", "versions": [{"version": "v1", "created": "Fri, 3 May 2019 06:31:38 GMT"}, {"version": "v2", "created": "Sat, 13 Jul 2019 02:45:07 GMT"}], "update_date": "2019-07-16", "authors_parsed": [["Zhao", "Zixu", ""], ["Lin", "Huangjing", ""], ["Chen", "Hao", ""], ["Heng", "Pheng-Ann", ""]]}, {"id": "1905.01047", "submitter": "Brojeshwar Bhowmick", "authors": "Sandika Biswas, Sanjana Sinha, Kavya Gupta and Brojeshwar Bhowmick", "title": "Lifting 2d Human Pose to 3d : A Weakly Supervised Approach", "comments": "Accepted in IJCNN 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimating 3d human pose from monocular images is a challenging problem due\nto the variety and complexity of human poses and the inherent ambiguity in\nrecovering depth from the single view. Recent deep learning based methods show\npromising results by using supervised learning on 3d pose annotated datasets.\nHowever, the lack of large-scale 3d annotated training data captured under\nin-the-wild settings makes the 3d pose estimation difficult for in-the-wild\nposes. Few approaches have utilized training images from both 3d and 2d pose\ndatasets in a weakly-supervised manner for learning 3d poses in unconstrained\nsettings. In this paper, we propose a method which can effectively predict 3d\nhuman pose from 2d pose using a deep neural network trained in a\nweakly-supervised manner on a combination of ground-truth 3d pose and\nground-truth 2d pose. Our method uses re-projection error minimization as a\nconstraint to predict the 3d locations of body joints, and this is crucial for\ntraining on data where the 3d ground-truth is not present. Since minimizing\nre-projection error alone may not guarantee an accurate 3d pose, we also use\nadditional geometric constraints on skeleton pose to regularize the pose in 3d.\nWe demonstrate the superior generalization ability of our method by\ncross-dataset validation on a challenging 3d benchmark dataset MPI-INF-3DHP\ncontaining in the wild 3d poses.\n", "versions": [{"version": "v1", "created": "Fri, 3 May 2019 06:51:11 GMT"}], "update_date": "2019-05-06", "authors_parsed": [["Biswas", "Sandika", ""], ["Sinha", "Sanjana", ""], ["Gupta", "Kavya", ""], ["Bhowmick", "Brojeshwar", ""]]}, {"id": "1905.01058", "submitter": "Andreas Pfeuffer", "authors": "Andreas Pfeuffer and Karina Schulz and Klaus Dietmayer", "title": "Semantic Segmentation of Video Sequences with Convolutional LSTMs", "comments": "This work has been submitted to the IEEE for possible publication.\n  Copyright may be transferred without notice, after which this version may no\n  longer be accessible", "journal-ref": "IEEE Intelligent Vehicles Symposium 2019 (IV'19)", "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most of the semantic segmentation approaches have been developed for single\nimage segmentation, and hence, video sequences are currently segmented by\nprocessing each frame of the video sequence separately. The disadvantage of\nthis is that temporal image information is not considered, which improves the\nperformance of the segmentation approach. One possibility to include temporal\ninformation is to use recurrent neural networks. However, there are only a few\napproaches using recurrent networks for video segmentation so far. These\napproaches extend the encoder-decoder network architecture of well-known\nsegmentation approaches and place convolutional LSTM layers between encoder and\ndecoder. However, in this paper it is shown that this position is not optimal,\nand that other positions in the network exhibit better performance. Nowadays,\nstate-of-the-art segmentation approaches rarely use the classical\nencoder-decoder structure, but use multi-branch architectures. These\narchitectures are more complex, and hence, it is more difficult to place the\nrecurrent units at a proper position. In this work, the multi-branch\narchitectures are extended by convolutional LSTM layers at different positions\nand evaluated on two different datasets in order to find the best one. It\nturned out that the proposed approach outperforms the pure CNN-based approach\nfor up to 1.6 percent.\n", "versions": [{"version": "v1", "created": "Fri, 3 May 2019 07:52:32 GMT"}], "update_date": "2019-05-06", "authors_parsed": [["Pfeuffer", "Andreas", ""], ["Schulz", "Karina", ""], ["Dietmayer", "Klaus", ""]]}, {"id": "1905.01067", "submitter": "Rosanne Liu", "authors": "Hattie Zhou, Janice Lan, Rosanne Liu, Jason Yosinski", "title": "Deconstructing Lottery Tickets: Zeros, Signs, and the Supermask", "comments": "NeurIPS 2019 camera ready version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The recent \"Lottery Ticket Hypothesis\" paper by Frankle & Carbin showed that\na simple approach to creating sparse networks (keeping the large weights)\nresults in models that are trainable from scratch, but only when starting from\nthe same initial weights. The performance of these networks often exceeds the\nperformance of the non-sparse base model, but for reasons that were not well\nunderstood. In this paper we study the three critical components of the Lottery\nTicket (LT) algorithm, showing that each may be varied significantly without\nimpacting the overall results. Ablating these factors leads to new insights for\nwhy LT networks perform as well as they do. We show why setting weights to zero\nis important, how signs are all you need to make the reinitialized network\ntrain, and why masking behaves like training. Finally, we discover the\nexistence of Supermasks, masks that can be applied to an untrained, randomly\ninitialized network to produce a model with performance far better than chance\n(86% on MNIST, 41% on CIFAR-10).\n", "versions": [{"version": "v1", "created": "Fri, 3 May 2019 08:21:07 GMT"}, {"version": "v2", "created": "Fri, 23 Aug 2019 01:12:35 GMT"}, {"version": "v3", "created": "Wed, 25 Sep 2019 22:14:24 GMT"}, {"version": "v4", "created": "Tue, 3 Mar 2020 05:40:51 GMT"}], "update_date": "2020-03-04", "authors_parsed": [["Zhou", "Hattie", ""], ["Lan", "Janice", ""], ["Liu", "Rosanne", ""], ["Yosinski", "Jason", ""]]}, {"id": "1905.01068", "submitter": "Qing Lian", "authors": "Qing Lian, Wen Li, Lin Chen, Lixin Duan", "title": "Known-class Aware Self-ensemble for Open Set Domain Adaptation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing domain adaptation methods generally assume different domains have\nthe identical label space, which is quite restrict for real-world applications.\nIn this paper, we focus on a more realistic and challenging case of open set\ndomain adaptation. Particularly, in open set domain adaptation, we allow the\nclasses from the source and target domains to be partially overlapped. In this\ncase, the assumption of conventional distribution alignment does not hold\nanymore, due to the different label spaces in two domains. To tackle this\nchallenge, we propose a new approach coined as Known-class Aware Self-Ensemble\n(KASE), which is built upon the recently developed self-ensemble model. In\nKASE, we first introduce a Known-class Aware Recognition (KAR) module to\nidentify the known and unknown classes from the target domain, which is\nachieved by encouraging a low cross-entropy for known classes and a high\nentropy based on the source data from the unknown class. Then, we develop a\nKnown-class Aware Adaptation (KAA) module to better adapt from the source\ndomain to the target by reweighing the adaptation loss based on the likeliness\nto belong to known classes of unlabeled target samples as predicted by KAR.\nExtensive experiments on multiple benchmark datasets demonstrate the\neffectiveness of our approach.\n", "versions": [{"version": "v1", "created": "Fri, 3 May 2019 08:25:03 GMT"}], "update_date": "2019-05-06", "authors_parsed": [["Lian", "Qing", ""], ["Li", "Wen", ""], ["Chen", "Lin", ""], ["Duan", "Lixin", ""]]}, {"id": "1905.01077", "submitter": "Ting Yao", "authors": "Jingwen Chen, Yingwei Pan, Yehao Li, Ting Yao, Hongyang Chao, Tao Mei", "title": "Temporal Deformable Convolutional Encoder-Decoder Networks for Video\n  Captioning", "comments": "AAAI 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is well believed that video captioning is a fundamental but challenging\ntask in both computer vision and artificial intelligence fields. The prevalent\napproach is to map an input video to a variable-length output sentence in a\nsequence to sequence manner via Recurrent Neural Network (RNN). Nevertheless,\nthe training of RNN still suffers to some degree from vanishing/exploding\ngradient problem, making the optimization difficult. Moreover, the inherently\nrecurrent dependency in RNN prevents parallelization within a sequence during\ntraining and therefore limits the computations. In this paper, we present a\nnovel design --- Temporal Deformable Convolutional Encoder-Decoder Networks\n(dubbed as TDConvED) that fully employ convolutions in both encoder and decoder\nnetworks for video captioning. Technically, we exploit convolutional block\nstructures that compute intermediate states of a fixed number of inputs and\nstack several blocks to capture long-term relationships. The structure in\nencoder is further equipped with temporal deformable convolution to enable\nfree-form deformation of temporal sampling. Our model also capitalizes on\ntemporal attention mechanism for sentence generation. Extensive experiments are\nconducted on both MSVD and MSR-VTT video captioning datasets, and superior\nresults are reported when comparing to conventional RNN-based encoder-decoder\ntechniques. More remarkably, TDConvED increases CIDEr-D performance from 58.8%\nto 67.2% on MSVD.\n", "versions": [{"version": "v1", "created": "Fri, 3 May 2019 08:59:10 GMT"}], "update_date": "2019-05-06", "authors_parsed": [["Chen", "Jingwen", ""], ["Pan", "Yingwei", ""], ["Li", "Yehao", ""], ["Yao", "Ting", ""], ["Chao", "Hongyang", ""], ["Mei", "Tao", ""]]}, {"id": "1905.01102", "submitter": "Spyros Gidaris", "authors": "Spyros Gidaris, Nikos Komodakis", "title": "Generating Classification Weights with GNN Denoising Autoencoders for\n  Few-Shot Learning", "comments": "Oral presentation at CVPR 2019. The code and models of our paper will\n  be published on: https://github.com/gidariss/wDAE_GNN_FewShot", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given an initial recognition model already trained on a set of base classes,\nthe goal of this work is to develop a meta-model for few-shot learning. The\nmeta-model, given as input some novel classes with few training examples per\nclass, must properly adapt the existing recognition model into a new model that\ncan correctly classify in a unified way both the novel and the base classes. To\naccomplish this goal it must learn to output the appropriate classification\nweight vectors for those two types of classes. To build our meta-model we make\nuse of two main innovations: we propose the use of a Denoising Autoencoder\nnetwork (DAE) that (during training) takes as input a set of classification\nweights corrupted with Gaussian noise and learns to reconstruct the\ntarget-discriminative classification weights. In this case, the injected noise\non the classification weights serves the role of regularizing the weight\ngenerating meta-model. Furthermore, in order to capture the co-dependencies\nbetween different classes in a given task instance of our meta-model, we\npropose to implement the DAE model as a Graph Neural Network (GNN). In order to\nverify the efficacy of our approach, we extensively evaluate it on ImageNet\nbased few-shot benchmarks and we report strong results that surpass prior\napproaches. The code and models of our paper will be published on:\nhttps://github.com/gidariss/wDAE_GNN_FewShot\n", "versions": [{"version": "v1", "created": "Fri, 3 May 2019 10:11:54 GMT"}], "update_date": "2019-05-06", "authors_parsed": [["Gidaris", "Spyros", ""], ["Komodakis", "Nikos", ""]]}, {"id": "1905.01118", "submitter": "Samanyou Garg", "authors": "Samanyou Garg", "title": "Group Emotion Recognition Using Machine Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic facial emotion recognition is a challenging task that has gained\nsignificant scientific interest over the past few years, but the problem of\nemotion recognition for a group of people has been less extensively studied.\nHowever, it is slowly gaining popularity due to the massive amount of data\navailable on social networking sites containing images of groups of people\nparticipating in various social events. Group emotion recognition is a\nchallenging problem due to obstructions like head and body pose variations,\nocclusions, variable lighting conditions, variance of actors, varied indoor and\noutdoor settings and image quality. The objective of this task is to classify a\ngroup's perceived emotion as Positive, Neutral or Negative. In this report, we\ndescribe our solution which is a hybrid machine learning system that\nincorporates deep neural networks and Bayesian classifiers. Deep Convolutional\nNeural Networks (CNNs) work from bottom to top, analysing facial expressions\nexpressed by individual faces extracted from the image. The Bayesian network\nworks from top to bottom, inferring the global emotion for the image, by\nintegrating the visual features of the contents of the image obtained through a\nscene descriptor. In the final pipeline, the group emotion category predicted\nby an ensemble of CNNs in the bottom-up module is passed as input to the\nBayesian Network in the top-down module and an overall prediction for the image\nis obtained. Experimental results show that the stated system achieves 65.27%\naccuracy on the validation set which is in line with state-of-the-art results.\nAs an outcome of this project, a Progressive Web Application and an\naccompanying Android app with a simple and intuitive user interface are\npresented, allowing users to test out the system with their own pictures.\n", "versions": [{"version": "v1", "created": "Fri, 3 May 2019 11:21:25 GMT"}], "update_date": "2019-05-06", "authors_parsed": [["Garg", "Samanyou", ""]]}, {"id": "1905.01164", "submitter": "Tamar Rott Shaham", "authors": "Tamar Rott Shaham, Tali Dekel and Tomer Michaeli", "title": "SinGAN: Learning a Generative Model from a Single Natural Image", "comments": "ICCV 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce SinGAN, an unconditional generative model that can be learned\nfrom a single natural image. Our model is trained to capture the internal\ndistribution of patches within the image, and is then able to generate high\nquality, diverse samples that carry the same visual content as the image.\nSinGAN contains a pyramid of fully convolutional GANs, each responsible for\nlearning the patch distribution at a different scale of the image. This allows\ngenerating new samples of arbitrary size and aspect ratio, that have\nsignificant variability, yet maintain both the global structure and the fine\ntextures of the training image. In contrast to previous single image GAN\nschemes, our approach is not limited to texture images, and is not conditional\n(i.e. it generates samples from noise). User studies confirm that the generated\nsamples are commonly confused to be real images. We illustrate the utility of\nSinGAN in a wide range of image manipulation tasks.\n", "versions": [{"version": "v1", "created": "Thu, 2 May 2019 16:15:38 GMT"}, {"version": "v2", "created": "Wed, 4 Sep 2019 17:08:49 GMT"}], "update_date": "2019-09-06", "authors_parsed": [["Shaham", "Tamar Rott", ""], ["Dekel", "Tali", ""], ["Michaeli", "Tomer", ""]]}, {"id": "1905.01168", "submitter": "Tapabrata Chakraborti", "authors": "Tapabrata Chakraborti, Brendan McCane, Steven Mills, Umapada Pal", "title": "Distance Metric Learned Collaborative Representation Classifier", "comments": "arXiv admin note: text overlap with arXiv:1903.09123", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Any generic deep machine learning algorithm is essentially a function fitting\nexercise, where the network tunes its weights and parameters to learn\ndiscriminatory features by minimizing some cost function. Though the network\ntries to learn the optimal feature space, it seldom tries to learn an optimal\ndistance metric in the cost function, and hence misses out on an additional\nlayer of abstraction. We present a simple effective way of achieving this by\nlearning a generic Mahalanabis distance in a collaborative loss function in an\nend-to-end fashion with any standard convolutional network as the feature\nlearner. The proposed method DML-CRC gives state-of-the-art performance on\nbenchmark fine-grained classification datasets CUB Birds, Oxford Flowers and\nOxford-IIIT Pets using the VGG-19 deep network. The method is network agnostic\nand can be used for any similar classification tasks.\n", "versions": [{"version": "v1", "created": "Fri, 3 May 2019 13:02:09 GMT"}, {"version": "v2", "created": "Wed, 11 Dec 2019 08:21:50 GMT"}], "update_date": "2019-12-13", "authors_parsed": [["Chakraborti", "Tapabrata", ""], ["McCane", "Brendan", ""], ["Mills", "Steven", ""], ["Pal", "Umapada", ""]]}, {"id": "1905.01173", "submitter": "Andrija Stajduhar", "authors": "Andrija \\v{S}tajduhar, Tomislav Lipi\\'c, Goran Sedmak, Sven\n  Lon\\v{c}ari\\'c, Milo\\v{s} Juda\\v{s}", "title": "Computational analysis of laminar structure of the human cortex based on\n  local neuron features", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG q-bio.NC q-bio.QM", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In this paper, we present a novel method for analysis and segmentation of\nlaminar structure of the cortex based on tissue characteristics whose change\nacross the gray matter underlies distinctive between cortical layers. We\ndevelop and analyze features of individual neurons to investigate changes in\ncytoarchitectonic differentiation and present a novel high-performance,\nautomated framework for neuron-level histological image analysis. Local tissue\nand cell descriptors such as density, neuron size and other measures are used\nfor development of more complex neuron features used in machine learning model\ntrained on data manually labeled by three human experts. Final neuron layer\nclassifications were obtained by training a separate model for each expert and\ncombining their probability outputs. Importances of developed neuron features\non both global model level and individual prediction level are presented and\ndiscussed.\n", "versions": [{"version": "v1", "created": "Fri, 3 May 2019 13:15:54 GMT"}, {"version": "v2", "created": "Fri, 13 Dec 2019 15:19:04 GMT"}], "update_date": "2019-12-16", "authors_parsed": [["\u0160tajduhar", "Andrija", ""], ["Lipi\u0107", "Tomislav", ""], ["Sedmak", "Goran", ""], ["Lon\u010dari\u0107", "Sven", ""], ["Juda\u0161", "Milo\u0161", ""]]}, {"id": "1905.01203", "submitter": "Bharti Munjal", "authors": "Bharti Munjal, Sikandar Amin, Federico Tombari, Fabio Galasso", "title": "Query-guided End-to-End Person Search", "comments": "Accepted as poster in CVPR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Person search has recently gained attention as the novel task of finding a\nperson, provided as a cropped sample, from a gallery of non-cropped images,\nwhereby several other people are also visible. We believe that i. person\ndetection and re-identification should be pursued in a joint optimization\nframework and that ii. the person search should leverage the query image\nextensively (e.g. emphasizing unique query patterns). However, so far, no prior\nart realizes this. We introduce a novel query-guided end-to-end person search\nnetwork (QEEPS) to address both aspects. We leverage a most recent joint\ndetector and re-identification work, OIM [37]. We extend this with i. a\nquery-guided Siamese squeeze-and-excitation network (QSSE-Net) that uses global\ncontext from both the query and gallery images, ii. a query-guided region\nproposal network (QRPN) to produce query-relevant proposals, and iii. a\nquery-guided similarity subnetwork (QSimNet), to learn a query-guided\nreidentification score. QEEPS is the first end-to-end query-guided detection\nand re-id network. On both the most recent CUHK-SYSU [37] and PRW [46]\ndatasets, we outperform the previous state-of-the-art by a large margin.\n", "versions": [{"version": "v1", "created": "Fri, 3 May 2019 14:31:23 GMT"}], "update_date": "2019-05-06", "authors_parsed": [["Munjal", "Bharti", ""], ["Amin", "Sikandar", ""], ["Tombari", "Federico", ""], ["Galasso", "Fabio", ""]]}, {"id": "1905.01207", "submitter": "Songxuan Lai", "authors": "Songxuan Lai and Lianwen Jin", "title": "Offline Writer Identification based on the Path Signature Feature", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a novel set of features for offline writer\nidentification based on the path signature approach, which provides a\nprincipled way to express information contained in a path. By extracting local\npathlets from handwriting contours, the path signature can also characterize\nthe offline handwriting style. A codebook method based on the log path\nsignature---a more compact way to express the path signature---is used in this\nwork and shows competitive results on several benchmark offline writer\nidentification datasets, namely the IAM, Firemaker, CVL and ICDAR2013 writer\nidentification contest dataset.\n", "versions": [{"version": "v1", "created": "Fri, 3 May 2019 14:41:55 GMT"}], "update_date": "2019-05-06", "authors_parsed": [["Lai", "Songxuan", ""], ["Jin", "Lianwen", ""]]}, {"id": "1905.01220", "submitter": "Samuel Rota Bul\\`o", "authors": "Lorenzo Porzi, Samuel Rota Bul\\`o, Aleksander Colovic, Peter\n  Kontschieder", "title": "Seamless Scene Segmentation", "comments": "extended version of the accepted CVPR 2019 paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we introduce a novel, CNN-based architecture that can be trained\nend-to-end to deliver seamless scene segmentation results. Our goal is to\npredict consistent semantic segmentation and detection results by means of a\npanoptic output format, going beyond the simple combination of independently\ntrained segmentation and detection models. The proposed architecture takes\nadvantage of a novel segmentation head that seamlessly integrates multi-scale\nfeatures generated by a Feature Pyramid Network with contextual information\nconveyed by a light-weight DeepLab-like module. As additional contribution we\nreview the panoptic metric and propose an alternative that overcomes its\nlimitations when evaluating non-instance categories. Our proposed network\narchitecture yields state-of-the-art results on three challenging street-level\ndatasets, i.e. Cityscapes, Indian Driving Dataset and Mapillary Vistas.\n", "versions": [{"version": "v1", "created": "Fri, 3 May 2019 15:21:25 GMT"}], "update_date": "2019-05-06", "authors_parsed": [["Porzi", "Lorenzo", ""], ["Bul\u00f2", "Samuel Rota", ""], ["Colovic", "Aleksander", ""], ["Kontschieder", "Peter", ""]]}, {"id": "1905.01235", "submitter": "Priya Goyal", "authors": "Priya Goyal, Dhruv Mahajan, Abhinav Gupta, Ishan Misra", "title": "Scaling and Benchmarking Self-Supervised Visual Representation Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Self-supervised learning aims to learn representations from the data itself\nwithout explicit manual supervision. Existing efforts ignore a crucial aspect\nof self-supervised learning - the ability to scale to large amount of data\nbecause self-supervision requires no manual labels. In this work, we revisit\nthis principle and scale two popular self-supervised approaches to 100 million\nimages. We show that by scaling on various axes (including data size and\nproblem 'hardness'), one can largely match or even exceed the performance of\nsupervised pre-training on a variety of tasks such as object detection, surface\nnormal estimation (3D) and visual navigation using reinforcement learning.\nScaling these methods also provides many interesting insights into the\nlimitations of current self-supervised techniques and evaluations. We conclude\nthat current self-supervised methods are not 'hard' enough to take full\nadvantage of large scale data and do not seem to learn effective high level\nsemantic representations. We also introduce an extensive benchmark across 9\ndifferent datasets and tasks. We believe that such a benchmark along with\ncomparable evaluation settings is necessary to make meaningful progress. Code\nis at: https://github.com/facebookresearch/fair_self_supervision_benchmark.\n", "versions": [{"version": "v1", "created": "Fri, 3 May 2019 15:50:51 GMT"}, {"version": "v2", "created": "Thu, 6 Jun 2019 13:08:29 GMT"}], "update_date": "2019-06-07", "authors_parsed": [["Goyal", "Priya", ""], ["Mahajan", "Dhruv", ""], ["Gupta", "Abhinav", ""], ["Misra", "Ishan", ""]]}, {"id": "1905.01270", "submitter": "Hsin-Ying Lee", "authors": "Hsin-Ying Lee, Hung-Yu Tseng, Qi Mao, Jia-Bin Huang, Yu-Ding Lu,\n  Maneesh Singh, Ming-Hsuan Yang", "title": "DRIT++: Diverse Image-to-Image Translation via Disentangled\n  Representations", "comments": "IJCV Journal extension for ECCV 2018 \"Diverse Image-to-Image\n  Translation via Disentangled Representations\" arXiv:1808.00948. Project Page:\n  http://vllab.ucmerced.edu/hylee/DRIT_pp/ Code:\n  https://github.com/HsinYingLee/DRIT", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image-to-image translation aims to learn the mapping between two visual\ndomains. There are two main challenges for this task: 1) lack of aligned\ntraining pairs and 2) multiple possible outputs from a single input image. In\nthis work, we present an approach based on disentangled representation for\ngenerating diverse outputs without paired training images. To synthesize\ndiverse outputs, we propose to embed images onto two spaces: a domain-invariant\ncontent space capturing shared information across domains and a domain-specific\nattribute space. Our model takes the encoded content features extracted from a\ngiven input and attribute vectors sampled from the attribute space to\nsynthesize diverse outputs at test time. To handle unpaired training data, we\nintroduce a cross-cycle consistency loss based on disentangled representations.\nQualitative results show that our model can generate diverse and realistic\nimages on a wide range of tasks without paired training data. For quantitative\nevaluations, we measure realism with user study and Fr\\'{e}chet inception\ndistance, and measure diversity with the perceptual distance metric,\nJensen-Shannon divergence, and number of statistically-different bins.\n", "versions": [{"version": "v1", "created": "Thu, 2 May 2019 17:49:30 GMT"}, {"version": "v2", "created": "Tue, 17 Dec 2019 22:57:28 GMT"}], "update_date": "2019-12-19", "authors_parsed": [["Lee", "Hsin-Ying", ""], ["Tseng", "Hung-Yu", ""], ["Mao", "Qi", ""], ["Huang", "Jia-Bin", ""], ["Lu", "Yu-Ding", ""], ["Singh", "Maneesh", ""], ["Yang", "Ming-Hsuan", ""]]}, {"id": "1905.01273", "submitter": "Hao Wang", "authors": "Hao Wang, Doyen Sahoo, Chenghao Liu, Ee-peng Lim, Steven C. H. Hoi", "title": "Learning Cross-Modal Embeddings with Adversarial Networks for Cooking\n  Recipes and Food Images", "comments": "Accepted at CVPR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Food computing is playing an increasingly important role in human daily life,\nand has found tremendous applications in guiding human behavior towards smart\nfood consumption and healthy lifestyle. An important task under the\nfood-computing umbrella is retrieval, which is particularly helpful for health\nrelated applications, where we are interested in retrieving important\ninformation about food (e.g., ingredients, nutrition, etc.). In this paper, we\ninvestigate an open research task of cross-modal retrieval between cooking\nrecipes and food images, and propose a novel framework Adversarial Cross-Modal\nEmbedding (ACME) to resolve the cross-modal retrieval task in food domains.\nSpecifically, the goal is to learn a common embedding feature space between the\ntwo modalities, in which our approach consists of several novel ideas: (i)\nlearning by using a new triplet loss scheme together with an effective sampling\nstrategy, (ii) imposing modality alignment using an adversarial learning\nstrategy, and (iii) imposing cross-modal translation consistency such that the\nembedding of one modality is able to recover some important information of\ncorresponding instances in the other modality. ACME achieves the\nstate-of-the-art performance on the benchmark Recipe1M dataset, validating the\nefficacy of the proposed technique.\n", "versions": [{"version": "v1", "created": "Fri, 3 May 2019 17:08:01 GMT"}], "update_date": "2020-03-10", "authors_parsed": [["Wang", "Hao", ""], ["Sahoo", "Doyen", ""], ["Liu", "Chenghao", ""], ["Lim", "Ee-peng", ""], ["Hoi", "Steven C. H.", ""]]}, {"id": "1905.01278", "submitter": "Mathilde Caron", "authors": "Mathilde Caron, Piotr Bojanowski, Julien Mairal and Armand Joulin", "title": "Unsupervised Pre-Training of Image Features on Non-Curated Data", "comments": "Accepted at ICCV 2019 (Oral)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pre-training general-purpose visual features with convolutional neural\nnetworks without relying on annotations is a challenging and important task.\nMost recent efforts in unsupervised feature learning have focused on either\nsmall or highly curated datasets like ImageNet, whereas using uncurated raw\ndatasets was found to decrease the feature quality when evaluated on a transfer\ntask. Our goal is to bridge the performance gap between unsupervised methods\ntrained on curated data, which are costly to obtain, and massive raw datasets\nthat are easily available. To that effect, we propose a new unsupervised\napproach which leverages self-supervision and clustering to capture\ncomplementary statistics from large-scale data. We validate our approach on 96\nmillion images from YFCC100M, achieving state-of-the-art results among\nunsupervised methods on standard benchmarks, which confirms the potential of\nunsupervised learning when only uncurated data are available. We also show that\npre-training a supervised VGG-16 with our method achieves 74.9% top-1\nclassification accuracy on the validation set of ImageNet, which is an\nimprovement of +0.8% over the same network trained from scratch. Our code is\navailable at https://github.com/facebookresearch/DeeperCluster.\n", "versions": [{"version": "v1", "created": "Fri, 3 May 2019 17:20:55 GMT"}, {"version": "v2", "created": "Wed, 7 Aug 2019 09:48:10 GMT"}, {"version": "v3", "created": "Tue, 13 Aug 2019 08:31:13 GMT"}], "update_date": "2019-08-14", "authors_parsed": [["Caron", "Mathilde", ""], ["Bojanowski", "Piotr", ""], ["Mairal", "Julien", ""], ["Joulin", "Armand", ""]]}, {"id": "1905.01296", "submitter": "Nicholas Rhinehart", "authors": "Nicholas Rhinehart, Rowan McAllister, Kris Kitani, Sergey Levine", "title": "PRECOG: PREdiction Conditioned On Goals in Visual Multi-Agent Settings", "comments": "To appear at the IEEE International Conference on Computer Vision\n  (ICCV 2019). Website: https://sites.google.com/view/precog", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG cs.RO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For autonomous vehicles (AVs) to behave appropriately on roads populated by\nhuman-driven vehicles, they must be able to reason about the uncertain\nintentions and decisions of other drivers from rich perceptual information.\nTowards these capabilities, we present a probabilistic forecasting model of\nfuture interactions between a variable number of agents. We perform both\nstandard forecasting and the novel task of conditional forecasting, which\nreasons about how all agents will likely respond to the goal of a controlled\nagent (here, the AV). We train models on real and simulated data to forecast\nvehicle trajectories given past positions and LIDAR. Our evaluation shows that\nour model is substantially more accurate in multi-agent driving scenarios\ncompared to existing state-of-the-art. Beyond its general ability to perform\nconditional forecasting queries, we show that our model's predictions of all\nagents improve when conditioned on knowledge of the AV's goal, further\nillustrating its capability to model agent interactions.\n", "versions": [{"version": "v1", "created": "Fri, 3 May 2019 17:54:09 GMT"}, {"version": "v2", "created": "Tue, 7 May 2019 19:51:38 GMT"}, {"version": "v3", "created": "Mon, 30 Sep 2019 16:45:21 GMT"}], "update_date": "2019-10-01", "authors_parsed": [["Rhinehart", "Nicholas", ""], ["McAllister", "Rowan", ""], ["Kitani", "Kris", ""], ["Levine", "Sergey", ""]]}, {"id": "1905.01298", "submitter": "Wei-Chih Hung", "authors": "Wei-Chih Hung, Varun Jampani, Sifei Liu, Pavlo Molchanov, Ming-Hsuan\n  Yang, Jan Kautz", "title": "SCOPS: Self-Supervised Co-Part Segmentation", "comments": "Accepted in CVPR 2019. Project page:\n  http://varunjampani.github.io/scops", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Parts provide a good intermediate representation of objects that is robust\nwith respect to the camera, pose and appearance variations. Existing works on\npart segmentation is dominated by supervised approaches that rely on large\namounts of manual annotations and can not generalize to unseen object\ncategories. We propose a self-supervised deep learning approach for part\nsegmentation, where we devise several loss functions that aids in predicting\npart segments that are geometrically concentrated, robust to object variations\nand are also semantically consistent across different object instances.\nExtensive experiments on different types of image collections demonstrate that\nour approach can produce part segments that adhere to object boundaries and\nalso more semantically consistent across object instances compared to existing\nself-supervised techniques.\n", "versions": [{"version": "v1", "created": "Fri, 3 May 2019 17:55:23 GMT"}], "update_date": "2019-05-06", "authors_parsed": [["Hung", "Wei-Chih", ""], ["Jampani", "Varun", ""], ["Liu", "Sifei", ""], ["Molchanov", "Pavlo", ""], ["Yang", "Ming-Hsuan", ""], ["Kautz", "Jan", ""]]}, {"id": "1905.01312", "submitter": "Masaya Kaneko", "authors": "Masaya Kaneko, Ken Sakurada, Kiyoharu Aizawa", "title": "TriDepth: Triangular Patch-based Deep Depth Prediction", "comments": "Project webpage: https://meshdepth.github.io/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel and efficient representation for single-view depth\nestimation using Convolutional Neural Networks (CNNs). Point-cloud is generally\nused for CNN-based 3D scene reconstruction; however it has some drawbacks: (1)\nit is redundant as a representation for planar surfaces, and (2) no spatial\nrelationships between points are available (e.g, texture and surface). As a\nmore efficient representation, we introduce a triangular-patch-cloud, which\nrepresents the surface of the 3D structure using a set of triangular patches,\nand propose a CNN framework for its 3D structure estimation. In our framework,\nwe create it by separating all the faces in a 2D mesh, which are determined\nadaptively from the input image, and estimate depths and normals of all the\nfaces. Using a common RGBD-dataset, we show that our representation has a\nbetter or comparable performance than the existing point-cloud-based methods,\nalthough it has much less parameters.\n", "versions": [{"version": "v1", "created": "Fri, 3 May 2019 18:00:01 GMT"}, {"version": "v2", "created": "Wed, 11 Mar 2020 07:49:05 GMT"}], "update_date": "2020-03-12", "authors_parsed": [["Kaneko", "Masaya", ""], ["Sakurada", "Ken", ""], ["Aizawa", "Kiyoharu", ""]]}, {"id": "1905.01326", "submitter": "Dominik Kulon", "authors": "Dominik Kulon, Haoyang Wang, Riza Alp G\\\"uler, Michael Bronstein,\n  Stefanos Zafeiriou", "title": "Single Image 3D Hand Reconstruction with Mesh Convolutions", "comments": "Proceedings of the British Machine Vision Conference (BMVC 2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Monocular 3D reconstruction of deformable objects, such as human body parts,\nhas been typically approached by predicting parameters of heavyweight linear\nmodels. In this paper, we demonstrate an alternative solution that is based on\nthe idea of encoding images into a latent non-linear representation of meshes.\nThe prior on 3D hand shapes is learned by training an autoencoder with\nintrinsic graph convolutions performed in the spectral domain. The pre-trained\ndecoder acts as a non-linear statistical deformable model. The latent\nparameters that reconstruct the shape and articulated pose of hands in the\nimage are predicted using an image encoder. We show that our system\nreconstructs plausible meshes and operates in real-time. We evaluate the\nquality of the mesh reconstructions produced by the decoder on a new dataset\nand show latent space interpolation results. Our code, data, and models will be\nmade publicly available.\n", "versions": [{"version": "v1", "created": "Sat, 4 May 2019 20:41:47 GMT"}, {"version": "v2", "created": "Mon, 13 May 2019 23:37:19 GMT"}, {"version": "v3", "created": "Mon, 5 Aug 2019 12:07:34 GMT"}], "update_date": "2019-08-06", "authors_parsed": [["Kulon", "Dominik", ""], ["Wang", "Haoyang", ""], ["G\u00fcler", "Riza Alp", ""], ["Bronstein", "Michael", ""], ["Zafeiriou", "Stefanos", ""]]}, {"id": "1905.01333", "submitter": "Davi Frossard", "authors": "Davi Frossard, Eric Kee, Raquel Urtasun", "title": "DeepSignals: Predicting Intent of Drivers Through Visual Signals", "comments": "To be presented at the IEEE International Conference on Robotics and\n  Automation (ICRA), 2019", "journal-ref": "In IEEE 2019 International Conference on Robotics and Automation\n  (ICRA) (pp. 9697-9703)", "doi": "10.1109/ICRA.2019.8794214", "report-no": null, "categories": "cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Detecting the intention of drivers is an essential task in self-driving,\nnecessary to anticipate sudden events like lane changes and stops. Turn signals\nand emergency flashers communicate such intentions, providing seconds of\npotentially critical reaction time. In this paper, we propose to detect these\nsignals in video sequences by using a deep neural network that reasons about\nboth spatial and temporal information. Our experiments on more than a million\nframes show high per-frame accuracy in very challenging scenarios.\n", "versions": [{"version": "v1", "created": "Fri, 3 May 2019 18:21:46 GMT"}], "update_date": "2020-11-13", "authors_parsed": [["Frossard", "Davi", ""], ["Kee", "Eric", ""], ["Urtasun", "Raquel", ""]]}, {"id": "1905.01354", "submitter": "Shuai Yang", "authors": "Shuai Yang, Zhangyang Wang, Zhaowen Wang, Ning Xu, Jiaying Liu,\n  Zongming Guo", "title": "Controllable Artistic Text Style Transfer via Shape-Matching GAN", "comments": "Accepted by ICCV 2019. Code is available at\n  https://williamyang1991.github.io/projects/ICCV2019/SMGAN.html", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Artistic text style transfer is the task of migrating the style from a source\nimage to the target text to create artistic typography. Recent style transfer\nmethods have considered texture control to enhance usability. However,\ncontrolling the stylistic degree in terms of shape deformation remains an\nimportant open challenge. In this paper, we present the first text style\ntransfer network that allows for real-time control of the crucial stylistic\ndegree of the glyph through an adjustable parameter. Our key contribution is a\nnovel bidirectional shape matching framework to establish an effective\nglyph-style mapping at various deformation levels without paired ground truth.\nBased on this idea, we propose a scale-controllable module to empower a single\nnetwork to continuously characterize the multi-scale shape features of the\nstyle image and transfer these features to the target text. The proposed method\ndemonstrates its superiority over previous state-of-the-arts in generating\ndiverse, controllable and high-quality stylized text.\n", "versions": [{"version": "v1", "created": "Fri, 3 May 2019 20:05:18 GMT"}, {"version": "v2", "created": "Sat, 10 Aug 2019 16:10:08 GMT"}], "update_date": "2019-08-13", "authors_parsed": [["Yang", "Shuai", ""], ["Wang", "Zhangyang", ""], ["Wang", "Zhaowen", ""], ["Xu", "Ning", ""], ["Liu", "Jiaying", ""], ["Guo", "Zongming", ""]]}, {"id": "1905.01382", "submitter": "Fuhao Shi", "authors": "Fuhao Shi, Sung-Fang Tsai, Youyou Wang, and Chia-Kai Liang", "title": "Steadiface: Real-Time Face-Centric Stabilization on Mobile Phones", "comments": "Accepted to be presented at ICIP 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present Steadiface, a new real-time face-centric video stabilization\nmethod that simultaneously removes hand shake and keeps subject's head stable.\nWe use a CNN to estimate the face landmarks and use them to optimize a\nstabilized head center. We then formulate an optimization problem to find a\nvirtual camera pose that locates the face to the stabilized head center while\nretains smooth rotation and translation transitions across frames. We test the\nproposed method on fieldtest videos and show it stabilizes both the head motion\nand background. It is robust to large head pose, occlusion, facial appearance\nvariations, and different kinds of camera motions. We show our method advances\nthe state of art in selfie video stabilization by comparing against alternative\nmethods. The whole process runs very efficiently on a modern mobile phone (8.1\nms/frame).\n", "versions": [{"version": "v1", "created": "Fri, 3 May 2019 22:57:37 GMT"}], "update_date": "2019-05-07", "authors_parsed": [["Shi", "Fuhao", ""], ["Tsai", "Sung-Fang", ""], ["Wang", "Youyou", ""], ["Liang", "Chia-Kai", ""]]}, {"id": "1905.01388", "submitter": "Vahid Mirjalili Dr", "authors": "Vahid Mirjalili, Sebastian Raschka and Arun Ross", "title": "FlowSAN: Privacy-enhancing Semi-Adversarial Networks to Confound\n  Arbitrary Face-based Gender Classifiers", "comments": "11 pages, 8 figures, 3 and 2 supplementary pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Privacy concerns in the modern digital age have prompted researchers to\ndevelop techniques that allow users to selectively suppress certain information\nin collected data while allowing for other information to be extracted. In this\nregard, Semi-Adversarial Networks (SAN) have recently emerged as a method for\nimparting soft-biometric privacy to face images. SAN enables modifications of\ninput face images so that the resulting face images can still be reliably used\nby arbitrary conventional face matchers for recognition purposes, while\nattribute classifiers, such as gender classifiers, are confounded. However, the\ngeneralizability of SANs across arbitrary gender classifiers has remained an\nopen concern. In this work, we propose a new method, FlowSAN, for allowing SANs\nto generalize to multiple unseen gender classifiers. We propose combining a\ndiverse set of SAN models to compensate each other's weaknesses, thereby,\nforming a robust model with improved generalization capability. Extensive\nexperiments using different unseen gender classifiers and face matchers\ndemonstrate the efficacy of the proposed paradigm in imparting gender privacy\nto face images.\n", "versions": [{"version": "v1", "created": "Fri, 3 May 2019 23:37:28 GMT"}], "update_date": "2019-05-07", "authors_parsed": [["Mirjalili", "Vahid", ""], ["Raschka", "Sebastian", ""], ["Ross", "Arun", ""]]}, {"id": "1905.01392", "submitter": "Martin Wistuba", "authors": "Martin Wistuba and Ambrish Rawat and Tejaswini Pedapati", "title": "A Survey on Neural Architecture Search", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The growing interest in both the automation of machine learning and deep\nlearning has inevitably led to the development of a wide variety of automated\nmethods for neural architecture search. The choice of the network architecture\nhas proven to be critical, and many advances in deep learning spring from its\nimmediate improvements. However, deep learning techniques are computationally\nintensive and their application requires a high level of domain knowledge.\nTherefore, even partial automation of this process helps to make deep learning\nmore accessible to both researchers and practitioners. With this survey, we\nprovide a formalism which unifies and categorizes the landscape of existing\nmethods along with a detailed analysis that compares and contrasts the\ndifferent approaches. We achieve this via a comprehensive discussion of the\ncommonly adopted architecture search spaces and architecture optimization\nalgorithms based on principles of reinforcement learning and evolutionary\nalgorithms along with approaches that incorporate surrogate and one-shot\nmodels. Additionally, we address the new research directions which include\nconstrained and multi-objective architecture search as well as automated data\naugmentation, optimizer and activation function search.\n", "versions": [{"version": "v1", "created": "Sat, 4 May 2019 00:08:49 GMT"}, {"version": "v2", "created": "Tue, 18 Jun 2019 09:32:21 GMT"}], "update_date": "2019-06-19", "authors_parsed": [["Wistuba", "Martin", ""], ["Rawat", "Ambrish", ""], ["Pedapati", "Tejaswini", ""]]}, {"id": "1905.01436", "submitter": "Jongmin Kim", "authors": "Jongmin Kim, Taesup Kim, Sungwoong Kim, Chang D. Yoo", "title": "Edge-labeling Graph Neural Network for Few-shot Learning", "comments": "accepted to CVPR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a novel edge-labeling graph neural network (EGNN),\nwhich adapts a deep neural network on the edge-labeling graph, for few-shot\nlearning. The previous graph neural network (GNN) approaches in few-shot\nlearning have been based on the node-labeling framework, which implicitly\nmodels the intra-cluster similarity and the inter-cluster dissimilarity. In\ncontrast, the proposed EGNN learns to predict the edge-labels rather than the\nnode-labels on the graph that enables the evolution of an explicit clustering\nby iteratively updating the edge-labels with direct exploitation of both\nintra-cluster similarity and the inter-cluster dissimilarity. It is also well\nsuited for performing on various numbers of classes without retraining, and can\nbe easily extended to perform a transductive inference. The parameters of the\nEGNN are learned by episodic training with an edge-labeling loss to obtain a\nwell-generalizable model for unseen low-data problem. On both of the supervised\nand semi-supervised few-shot image classification tasks with two benchmark\ndatasets, the proposed EGNN significantly improves the performances over the\nexisting GNNs.\n", "versions": [{"version": "v1", "created": "Sat, 4 May 2019 05:58:17 GMT"}], "update_date": "2019-05-07", "authors_parsed": [["Kim", "Jongmin", ""], ["Kim", "Taesup", ""], ["Kim", "Sungwoong", ""], ["Yoo", "Chang D.", ""]]}, {"id": "1905.01447", "submitter": "Tao Sun", "authors": "Tao Sun, Zonglin Di, Pengyu Che, Chun Liu and Yin Wang", "title": "Leveraging Crowdsourced GPS Data for Road Extraction from Aerial Imagery", "comments": "To be published in IEEE/CVF Conference on Computer Vision and Pattern\n  Recognition (CVPR) 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning is revolutionizing the mapping industry. Under lightweight\nhuman curation, computer has generated almost half of the roads in Thailand on\nOpenStreetMap (OSM) using high-resolution aerial imagery. Bing maps are\ndisplaying 125 million computer-generated building polygons in the U.S. While\ntremendously more efficient than manual mapping, one cannot map out everything\nfrom the air. Especially for roads, a small prediction gap by image occlusion\nrenders the entire road useless for routing. Misconnections can be more\ndangerous. Therefore computer-based mapping often requires local verifications,\nwhich is still labor intensive. In this paper, we propose to leverage\ncrowdsourced GPS data to improve and support road extraction from aerial\nimagery. Through novel data augmentation, GPS rendering, and 1D transpose\nconvolution techniques, we show almost 5% improvements over previous\ncompetition winning models, and much better robustness when predicting new\nareas without any new training data or domain adaptation.\n", "versions": [{"version": "v1", "created": "Sat, 4 May 2019 07:19:52 GMT"}], "update_date": "2019-05-07", "authors_parsed": [["Sun", "Tao", ""], ["Di", "Zonglin", ""], ["Che", "Pengyu", ""], ["Liu", "Chun", ""], ["Wang", "Yin", ""]]}, {"id": "1905.01489", "submitter": "Senthil Yogamani", "authors": "Senthil Yogamani, Ciaran Hughes, Jonathan Horgan, Ganesh Sistu,\n  Padraig Varley, Derek O'Dea, Michal Uricar, Stefan Milz, Martin Simon, Karl\n  Amende, Christian Witt, Hazem Rashed, Sumanth Chennupati, Sanjaya Nayak,\n  Saquib Mansoor, Xavier Perroton, Patrick Perez", "title": "WoodScape: A multi-task, multi-camera fisheye dataset for autonomous\n  driving", "comments": "Accepted for Oral Presentation at IEEE International Conference on\n  Computer Vision (ICCV) 2019. Please refer to our website\n  https://woodscape.valeo.com and https://github.com/valeoai/woodscape for\n  release status and updates", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG cs.RO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fisheye cameras are commonly employed for obtaining a large field of view in\nsurveillance, augmented reality and in particular automotive applications. In\nspite of their prevalence, there are few public datasets for detailed\nevaluation of computer vision algorithms on fisheye images. We release the\nfirst extensive fisheye automotive dataset, WoodScape, named after Robert Wood\nwho invented the fisheye camera in 1906. WoodScape comprises of four surround\nview cameras and nine tasks including segmentation, depth estimation, 3D\nbounding box detection and soiling detection. Semantic annotation of 40 classes\nat the instance level is provided for over 10,000 images and annotation for\nother tasks are provided for over 100,000 images. With WoodScape, we would like\nto encourage the community to adapt computer vision models for fisheye camera\ninstead of using naive rectification.\n", "versions": [{"version": "v1", "created": "Sat, 4 May 2019 13:14:12 GMT"}, {"version": "v2", "created": "Fri, 16 Aug 2019 20:40:58 GMT"}, {"version": "v3", "created": "Fri, 2 Jul 2021 22:16:13 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["Yogamani", "Senthil", ""], ["Hughes", "Ciaran", ""], ["Horgan", "Jonathan", ""], ["Sistu", "Ganesh", ""], ["Varley", "Padraig", ""], ["O'Dea", "Derek", ""], ["Uricar", "Michal", ""], ["Milz", "Stefan", ""], ["Simon", "Martin", ""], ["Amende", "Karl", ""], ["Witt", "Christian", ""], ["Rashed", "Hazem", ""], ["Chennupati", "Sumanth", ""], ["Nayak", "Sanjaya", ""], ["Mansoor", "Saquib", ""], ["Perroton", "Xavier", ""], ["Perez", "Patrick", ""]]}, {"id": "1905.01492", "submitter": "Senthil Yogamani", "authors": "Michal Uricar, Pavel Krizek, Ganesh Sistu and Senthil Yogamani", "title": "SoilingNet: Soiling Detection on Automotive Surround-View Cameras", "comments": "Accepted for Oral Presentation at IEEE Intelligent Transportation\n  Systems Conference (ITSC) 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG cs.RO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cameras are an essential part of sensor suite in autonomous driving.\nSurround-view cameras are directly exposed to external environment and are\nvulnerable to get soiled. Cameras have a much higher degradation in performance\ndue to soiling compared to other sensors. Thus it is critical to accurately\ndetect soiling on the cameras, particularly for higher levels of autonomous\ndriving. We created a new dataset having multiple types of soiling namely\nopaque and transparent. It will be released publicly as part of our WoodScape\ndataset \\cite{yogamani2019woodscape} to encourage further research. We\ndemonstrate high accuracy using a Convolutional Neural Network (CNN) based\narchitecture. We also show that it can be combined with the existing object\ndetection task in a multi-task learning framework. Finally, we make use of\nGenerative Adversarial Networks (GANs) to generate more images for data\naugmentation and show that it works successfully similar to the style transfer.\n", "versions": [{"version": "v1", "created": "Sat, 4 May 2019 13:39:48 GMT"}, {"version": "v2", "created": "Wed, 17 Jul 2019 16:34:40 GMT"}], "update_date": "2019-07-18", "authors_parsed": [["Uricar", "Michal", ""], ["Krizek", "Pavel", ""], ["Sistu", "Ganesh", ""], ["Yogamani", "Senthil", ""]]}, {"id": "1905.01509", "submitter": "Guanbin Li", "authors": "Yukai Shi, Guanbin Li, Qingxing Cao, Keze Wang, Liang Lin", "title": "Face Hallucination by Attentive Sequence Optimization with Reinforcement\n  Learning", "comments": "To be published in TPAMI", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Face hallucination is a domain-specific super-resolution problem that aims to\ngenerate a high-resolution (HR) face image from a low-resolution~(LR) input. In\ncontrast to the existing patch-wise super-resolution models that divide a face\nimage into regular patches and independently apply LR to HR mapping to each\npatch, we implement deep reinforcement learning and develop a novel\nattention-aware face hallucination (Attention-FH) framework, which recurrently\nlearns to attend a sequence of patches and performs facial part enhancement by\nfully exploiting the global interdependency of the image. Specifically, our\nproposed framework incorporates two components: a recurrent policy network for\ndynamically specifying a new attended region at each time step based on the\nstatus of the super-resolved image and the past attended region sequence, and a\nlocal enhancement network for selected patch hallucination and global state\nupdating. The Attention-FH model jointly learns the recurrent policy network\nand local enhancement network through maximizing a long-term reward that\nreflects the hallucination result with respect to the whole HR image. Extensive\nexperiments demonstrate that our Attention-FH significantly outperforms the\nstate-of-the-art methods on in-the-wild face images with large pose and\nillumination variations.\n", "versions": [{"version": "v1", "created": "Sat, 4 May 2019 15:01:57 GMT"}], "update_date": "2019-05-07", "authors_parsed": [["Shi", "Yukai", ""], ["Li", "Guanbin", ""], ["Cao", "Qingxing", ""], ["Wang", "Keze", ""], ["Lin", "Liang", ""]]}, {"id": "1905.01524", "submitter": "Ali Lenjani", "authors": "Ali Lenjani, Chul Min Yeum, Shirley Dyke, Ilias Bilionis", "title": "Automated building image extraction from 360{\\deg} panoramas for\n  postdisaster evaluation", "comments": null, "journal-ref": "Computer Aided Civil and Infrastructure Engineering (2019)", "doi": "10.1111/mice.12493", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  After a disaster, teams of structural engineers collect vast amounts of\nimages from damaged buildings to obtain new knowledge and extract lessons from\nthe event. However, in many cases, the images collected are captured without\nsufficient spatial context. When damage is severe, it may be quite difficult to\neven recognize the building. Accessing images of the pre-disaster condition of\nthose buildings is required to accurately identify the cause of the failure or\nthe actual loss in the building. Here, to address this issue, we develop a\nmethod to automatically extract pre-event building images from 360o panorama\nimages (panoramas). By providing a geotagged image collected near the target\nbuilding as the input, panoramas close to the input image location are\nautomatically downloaded through street view services (e.g., Google or Bing in\nthe United States). By computing the geometric relationship between the\npanoramas and the target building, the most suitable projection direction for\neach panorama is identified to generate high-quality 2D images of the building.\nRegion-based convolutional neural networks are exploited to recognize the\nbuilding within those 2D images. Several panoramas are used so that the\ndetected building images provide various viewpoints of the building. To\ndemonstrate the capability of the technique, we consider residential buildings\nin Holiday Beach, Texas, the United States which experienced significant\ndevastation in Hurricane Harvey in 2017. Using geotagged images gathered during\nactual post-disaster building reconnaissance missions, we verify the method by\nsuccessfully extracting residential building images from Google Street View\nimages, which were captured before the event.\n", "versions": [{"version": "v1", "created": "Sat, 4 May 2019 16:58:37 GMT"}, {"version": "v2", "created": "Tue, 5 Nov 2019 16:53:38 GMT"}], "update_date": "2019-11-06", "authors_parsed": [["Lenjani", "Ali", ""], ["Yeum", "Chul Min", ""], ["Dyke", "Shirley", ""], ["Bilionis", "Ilias", ""]]}, {"id": "1905.01555", "submitter": "Min Bai", "authors": "Min Bai, Gellert Mattyus, Namdar Homayounfar, Shenlong Wang, Shrinidhi\n  Kowshika Lakshmikanth, Raquel Urtasun", "title": "Deep Multi-Sensor Lane Detection", "comments": "IEEE International Conference on Intelligent Robots and Systems\n  (IROS) 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reliable and accurate lane detection has been a long-standing problem in the\nfield of autonomous driving. In recent years, many approaches have been\ndeveloped that use images (or videos) as input and reason in image space. In\nthis paper we argue that accurate image estimates do not translate to precise\n3D lane boundaries, which are the input required by modern motion planning\nalgorithms. To address this issue, we propose a novel deep neural network that\ntakes advantage of both LiDAR and camera sensors and produces very accurate\nestimates directly in 3D space. We demonstrate the performance of our approach\non both highways and in cities, and show very accurate estimates in complex\nscenarios such as heavy traffic (which produces occlusion), fork, merges and\nintersections.\n", "versions": [{"version": "v1", "created": "Sat, 4 May 2019 20:43:42 GMT"}], "update_date": "2019-05-07", "authors_parsed": [["Bai", "Min", ""], ["Mattyus", "Gellert", ""], ["Homayounfar", "Namdar", ""], ["Wang", "Shenlong", ""], ["Lakshmikanth", "Shrinidhi Kowshika", ""], ["Urtasun", "Raquel", ""]]}, {"id": "1905.01562", "submitter": "Manuel Lagunas", "authors": "Manuel Lagunas, Sandra Malpica, Ana Serrano, Elena Garces, Diego\n  Gutierrez, Belen Masia", "title": "A Similarity Measure for Material Appearance", "comments": "12 pages, 17 figures", "journal-ref": "ACM Transactions on Graphics (SIGGRAPH 2019)", "doi": "10.1145/3306346.3323036", "report-no": null, "categories": "cs.GR cs.AI cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a model to measure the similarity in appearance between different\nmaterials, which correlates with human similarity judgments. We first create a\ndatabase of 9,000 rendered images depicting objects with varying materials,\nshape and illumination. We then gather data on perceived similarity from\ncrowdsourced experiments; our analysis of over 114,840 answers suggests that\nindeed a shared perception of appearance similarity exists. We feed this data\nto a deep learning architecture with a novel loss function, which learns a\nfeature space for materials that correlates with such perceived appearance\nsimilarity. Our evaluation shows that our model outperforms existing metrics.\nLast, we demonstrate several applications enabled by our metric, including\nappearance-based search for material suggestions, database visualization,\nclustering and summarization, and gamut mapping.\n", "versions": [{"version": "v1", "created": "Sat, 4 May 2019 22:48:27 GMT"}], "update_date": "2020-03-18", "authors_parsed": [["Lagunas", "Manuel", ""], ["Malpica", "Sandra", ""], ["Serrano", "Ana", ""], ["Garces", "Elena", ""], ["Gutierrez", "Diego", ""], ["Masia", "Belen", ""]]}, {"id": "1905.01574", "submitter": "Qi Wang", "authors": "Qi Wang, Junyu Gao, Yuan Yuan", "title": "A Joint Convolutional Neural Networks and Context Transfer for Street\n  Scenes Labeling", "comments": "IEEE T-ITS 2018", "journal-ref": null, "doi": "10.1109/TITS.2017.2726546", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Street scene understanding is an essential task for autonomous driving. One\nimportant step towards this direction is scene labeling, which annotates each\npixel in the images with a correct class label. Although many approaches have\nbeen developed, there are still some weak points. Firstly, many methods are\nbased on the hand-crafted features whose image representation ability is\nlimited. Secondly, they can not label foreground objects accurately due to the\ndataset bias. Thirdly, in the refinement stage, the traditional Markov Random\nFiled (MRF) inference is prone to over smoothness. For improving the above\nproblems, this paper proposes a joint method of priori convolutional neural\nnetworks at superpixel level (called as ``priori s-CNNs'') and soft restricted\ncontext transfer. Our contributions are threefold: (1) A priori s-CNNs model\nthat learns priori location information at superpixel level is proposed to\ndescribe various objects discriminatingly; (2) A hierarchical data augmentation\nmethod is presented to alleviate dataset bias in the priori s-CNNs training\nstage, which improves foreground objects labeling significantly; (3) A soft\nrestricted MRF energy function is defined to improve the priori s-CNNs model's\nlabeling performance and reduce the over smoothness at the same time. The\nproposed approach is verified on CamVid dataset (11 classes) and SIFT Flow\nStreet dataset (16 classes) and achieves competitive performance.\n", "versions": [{"version": "v1", "created": "Sun, 5 May 2019 01:24:19 GMT"}], "update_date": "2019-05-07", "authors_parsed": [["Wang", "Qi", ""], ["Gao", "Junyu", ""], ["Yuan", "Yuan", ""]]}, {"id": "1905.01575", "submitter": "Qi Wang", "authors": "Qi Wang, Junyu Gao, Yuan Yuan", "title": "Embedding Structured Contour and Location Prior in Siamesed Fully\n  Convolutional Networks for Road Detection", "comments": "IEEE T-ITS 2018", "journal-ref": null, "doi": "10.1109/TITS.2017.2749964", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Road detection from the perspective of moving vehicles is a challenging issue\nin autonomous driving. Recently, many deep learning methods spring up for this\ntask because they can extract high-level local features to find road regions\nfrom raw RGB data, such as Convolutional Neural Networks (CNN) and Fully\nConvolutional Networks (FCN). However, how to detect the boundary of road\naccurately is still an intractable problem. In this paper, we propose a\nsiamesed fully convolutional networks (named as ``s-FCN-loc''), which is able\nto consider RGB-channel images, semantic contours and location priors\nsimultaneously to segment road region elaborately. To be specific, the\ns-FCN-loc has two streams to process the original RGB images and contour maps\nrespectively. At the same time, the location prior is directly appended to the\nsiamesed FCN to promote the final detection performance. Our contributions are\nthreefold: (1) An s-FCN-loc is proposed that learns more discriminative\nfeatures of road boundaries than the original FCN to detect more accurate road\nregions; (2) Location prior is viewed as a type of feature map and directly\nappended to the final feature map in s-FCN-loc to promote the detection\nperformance effectively, which is easier than other traditional methods, namely\ndifferent priors for different inputs (image patches); (3) The convergent speed\nof training s-FCN-loc model is 30\\% faster than the original FCN, because of\nthe guidance of highly structured contours. The proposed approach is evaluated\non KITTI Road Detection Benchmark and One-Class Road Detection Dataset, and\nachieves a competitive result with state of the arts.\n", "versions": [{"version": "v1", "created": "Sun, 5 May 2019 01:31:30 GMT"}], "update_date": "2019-05-07", "authors_parsed": [["Wang", "Qi", ""], ["Gao", "Junyu", ""], ["Yuan", "Yuan", ""]]}, {"id": "1905.01583", "submitter": "Qi Wang", "authors": "Yuan Yuan, Senior Member, IEEE, Zhitong Xiong, Student Member, IEEE,\n  and Qi Wang, Senior Member, IEEE", "title": "VSSA-NET: Vertical Spatial Sequence Attention Network for Traffic Sign\n  Detection", "comments": null, "journal-ref": null, "doi": "10.1109/TIP.2019.2896952", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although traffic sign detection has been studied for years and great progress\nhas been made with the rise of deep learning technique, there are still many\nproblems remaining to be addressed. For complicated real-world traffic scenes,\nthere are two main challenges. Firstly, traffic signs are usually small size\nobjects, which makes it more difficult to detect than large ones; Secondly, it\nis hard to distinguish false targets which resemble real traffic signs in\ncomplex street scenes without context information. To handle these problems, we\npropose a novel end-to-end deep learning method for traffic sign detection in\ncomplex environments. Our contributions are as follows: 1) We propose a\nmulti-resolution feature fusion network architecture which exploits densely\nconnected deconvolution layers with skip connections, and can learn more\neffective features for the small size object; 2) We frame the traffic sign\ndetection as a spatial sequence classification and regression task, and propose\na vertical spatial sequence attention (VSSA) module to gain more context\ninformation for better detection performance. To comprehensively evaluate the\nproposed method, we do experiments on several traffic sign datasets as well as\nthe general object detection dataset and the results have shown the\neffectiveness of our proposed method.\n", "versions": [{"version": "v1", "created": "Sun, 5 May 2019 02:16:43 GMT"}], "update_date": "2019-06-26", "authors_parsed": [["Yuan", "Yuan", ""], ["Member", "Senior", ""], ["IEEE", "", ""], ["Xiong", "Zhitong", ""], ["Member", "Student", ""], ["IEEE", "", ""], ["Wang", "Qi", ""], ["Member", "Senior", ""], ["IEEE", "", ""]]}, {"id": "1905.01585", "submitter": "Faen Zhang", "authors": "Faen Zhang, Xinyu Fan, Guo Ai, Jianfei Song, Yongqiang Qin, Jiahong Wu", "title": "Accurate Face Detection for High Performance", "comments": "9 pages, 3 figures, technical report", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Face detection has witnessed significant progress due to the advances of deep\nconvolutional neural networks (CNNs). Its central issue in recent years is how\nto improve the detection performance of tiny faces. To this end, many recent\nworks propose some specific strategies, redesign the architecture and introduce\nnew loss functions for tiny object detection. In this report, we start from the\npopular one-stage RetinaNet approach and apply some recent tricks to obtain a\nhigh performance face detector. Specifically, we apply the Intersection over\nUnion (IoU) loss function for regression, employ the two-step classification\nand regression for detection, revisit the data augmentation based on\ndata-anchor-sampling for training, utilize the max-out operation for\nclassification and use the multi-scale testing strategy for inference. As a\nconsequence, the proposed face detection method achieves state-of-the-art\nperformance on the most popular and challenging face detection benchmark WIDER\nFACE dataset.\n", "versions": [{"version": "v1", "created": "Sun, 5 May 2019 02:34:57 GMT"}, {"version": "v2", "created": "Sat, 18 May 2019 17:19:01 GMT"}, {"version": "v3", "created": "Fri, 24 May 2019 05:58:18 GMT"}], "update_date": "2019-05-27", "authors_parsed": [["Zhang", "Faen", ""], ["Fan", "Xinyu", ""], ["Ai", "Guo", ""], ["Song", "Jianfei", ""], ["Qin", "Yongqiang", ""], ["Wu", "Jiahong", ""]]}, {"id": "1905.01595", "submitter": "Yibing Zhan", "authors": "Yibing Zhan, Jun Yu, Ting Yu and Dacheng Tao", "title": "On Exploring Undetermined Relationships for Visual Relationship\n  Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In visual relationship detection, human-notated relationships can be regarded\nas determinate relationships. However, there are still large amount of\nunlabeled data, such as object pairs with less significant relationships or\neven with no relationships. We refer to these unlabeled but potentially useful\ndata as undetermined relationships. Although a vast body of literature exists,\nfew methods exploit these undetermined relationships for visual relationship\ndetection.\n  In this paper, we explore the beneficial effect of undetermined relationships\non visual relationship detection. We propose a novel multi-modal feature based\nundetermined relationship learning network (MF-URLN) and achieve great\nimprovements in relationship detection. In detail, our MF-URLN automatically\ngenerates undetermined relationships by comparing object pairs with\nhuman-notated data according to a designed criterion. Then, the MF-URLN\nextracts and fuses features of object pairs from three complementary modals:\nvisual, spatial, and linguistic modals. Further, the MF-URLN proposes two\ncorrelated subnetworks: one subnetwork decides the determinate confidence, and\nthe other predicts the relationships. We evaluate the MF-URLN on two datasets:\nthe Visual Relationship Detection (VRD) and the Visual Genome (VG) datasets.\nThe experimental results compared with state-of-the-art methods verify the\nsignificant improvements made by the undetermined relationships, e.g., the\ntop-50 relation detection recall improves from 19.5% to 23.9% on the VRD\ndataset.\n", "versions": [{"version": "v1", "created": "Sun, 5 May 2019 03:57:12 GMT"}], "update_date": "2019-05-07", "authors_parsed": [["Zhan", "Yibing", ""], ["Yu", "Jun", ""], ["Yu", "Ting", ""], ["Tao", "Dacheng", ""]]}, {"id": "1905.01608", "submitter": "Yikang Li", "authors": "Yikang Li, Tao Ma, Yeqi Bai, Nan Duan, Sining Wei, Xiaogang Wang", "title": "PasteGAN: A Semi-Parametric Method to Generate Image from Scene Graph", "comments": "10 pages, 6 figures; Accepted by NeurIPS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite some exciting progress on high-quality image generation from\nstructured(scene graphs) or free-form(sentences) descriptions, most of them\nonly guarantee the image-level semantical consistency, i.e. the generated image\nmatching the semantic meaning of the description. They still lack the\ninvestigations on synthesizing the images in a more controllable way, like\nfinely manipulating the visual appearance of every object. Therefore, to\ngenerate the images with preferred objects and rich interactions, we propose a\nsemi-parametric method, PasteGAN, for generating the image from the scene graph\nand the image crops, where spatial arrangements of the objects and their\npair-wise relationships are defined by the scene graph and the object\nappearances are determined by the given object crops. To enhance the\ninteractions of the objects in the output, we design a Crop Refining Network\nand an Object-Image Fuser to embed the objects as well as their relationships\ninto one map. Multiple losses work collaboratively to guarantee the generated\nimages highly respecting the crops and complying with the scene graphs while\nmaintaining excellent image quality. A crop selector is also proposed to pick\nthe most-compatible crops from our external object tank by encoding the\ninteractions around the objects in the scene graph if the crops are not\nprovided. Evaluated on Visual Genome and COCO-Stuff dataset, our proposed\nmethod significantly outperforms the SOTA methods on Inception Score, Diversity\nScore and Fr\\'echet Inception Distance. Extensive experiments also demonstrate\nour method's ability to generate complex and diverse images with given objects.\n", "versions": [{"version": "v1", "created": "Sun, 5 May 2019 05:16:09 GMT"}, {"version": "v2", "created": "Thu, 28 Nov 2019 03:19:19 GMT"}], "update_date": "2019-12-02", "authors_parsed": [["Li", "Yikang", ""], ["Ma", "Tao", ""], ["Bai", "Yeqi", ""], ["Duan", "Nan", ""], ["Wei", "Sining", ""], ["Wang", "Xiaogang", ""]]}, {"id": "1905.01614", "submitter": "Farhana Sultana", "authors": "F. Sultana, A. Sufian, P. Dutta", "title": "A Review of Object Detection Models based on Convolutional Neural\n  Network", "comments": "17 pages, 11 figures, 1 table", "journal-ref": "Intelligent Computing: Image Processing Based Applications.\n  Advances in Intelligent Systems and Computing, vol 1157, pages 1-16, 2020", "doi": "10.1007/978-981-15-4288-6_1", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional Neural Network (CNN) has become the state-of-the-art for object\ndetection in image task. In this chapter, we have explained different\nstate-of-the-art CNN based object detection models. We have made this review\nwith categorization those detection models according to two different\napproaches: two-stage approach and one-stage approach. Through this chapter, it\nhas shown advancements in object detection models from R-CNN to latest\nRefineDet. It has also discussed the model description and training details of\neach model. Here, we have also drawn a comparison among those models.\n", "versions": [{"version": "v1", "created": "Sun, 5 May 2019 05:45:21 GMT"}, {"version": "v2", "created": "Fri, 10 May 2019 15:50:21 GMT"}, {"version": "v3", "created": "Tue, 1 Oct 2019 10:27:07 GMT"}], "update_date": "2020-06-15", "authors_parsed": [["Sultana", "F.", ""], ["Sufian", "A.", ""], ["Dutta", "P.", ""]]}, {"id": "1905.01631", "submitter": "Jiachen Li", "authors": "Jiachen Li and Hengbo Ma and Masayoshi Tomizuka", "title": "Conditional Generative Neural System for Probabilistic Trajectory\n  Prediction", "comments": "Camera ready for IROS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG cs.RO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Effective understanding of the environment and accurate trajectory prediction\nof surrounding dynamic obstacles are critical for intelligent systems such as\nautonomous vehicles and wheeled mobile robotics navigating in complex scenarios\nto achieve safe and high-quality decision making, motion planning and control.\nDue to the uncertain nature of the future, it is desired to make inference from\na probability perspective instead of deterministic prediction. In this paper,\nwe propose a conditional generative neural system (CGNS) for probabilistic\ntrajectory prediction to approximate the data distribution, with which\nrealistic, feasible and diverse future trajectory hypotheses can be sampled.\nThe system combines the strengths of conditional latent space learning and\nvariational divergence minimization, and leverages both static context and\ninteraction information with soft attention mechanisms. We also propose a\nregularization method for incorporating soft constraints into deep neural\nnetworks with differentiable barrier functions, which can regulate and push the\ngenerated samples into the feasible regions. The proposed system is evaluated\non several public benchmark datasets for pedestrian trajectory prediction and a\nroundabout naturalistic driving dataset collected by ourselves. The\nexperimental results demonstrate that our model achieves better performance\nthan various baseline approaches in terms of prediction accuracy.\n", "versions": [{"version": "v1", "created": "Sun, 5 May 2019 08:19:50 GMT"}, {"version": "v2", "created": "Sun, 28 Jul 2019 08:26:20 GMT"}], "update_date": "2019-07-30", "authors_parsed": [["Li", "Jiachen", ""], ["Ma", "Hengbo", ""], ["Tomizuka", "Masayoshi", ""]]}, {"id": "1905.01634", "submitter": "Qi Wang", "authors": "Chengze Wang, Yuan Yuan, Qi Wang", "title": "Learning by Inertia: Self-supervised Monocular Visual Odometry for Road\n  Vehicles", "comments": "2019 IEEE. Personal use of this material is permitted. Permission\n  from IEEE must be obtained for all other uses, in any current or future\n  media, including reprinting/republishing this material for advertising or\n  promotional purposes, creating new collective works, for resale or\n  redistribution to servers or lists, or reuse of any copyrighted component of\n  this work in other works", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present iDVO (inertia-embedded deep visual odometry), a\nself-supervised learning based monocular visual odometry (VO) for road\nvehicles. When modelling the geometric consistency within adjacent frames, most\ndeep VO methods ignore the temporal continuity of the camera pose, which\nresults in a very severe jagged fluctuation in the velocity curves. With the\nobservation that road vehicles tend to perform smooth dynamic characteristics\nin most of the time, we design the inertia loss function to describe the\nabnormal motion variation, which assists the model to learn the consecutiveness\nfrom long-term camera ego-motion. Based on the recurrent convolutional neural\nnetwork (RCNN) architecture, our method implicitly models the dynamics of road\nvehicles and the temporal consecutiveness by the extended Long Short-Term\nMemory (LSTM) block. Furthermore, we develop the dynamic hard-edge mask to\nhandle the non-consistency in fast camera motion by blocking the boundary part\nand which generates more efficiency in the whole non-consistency mask. The\nproposed method is evaluated on the KITTI dataset, and the results demonstrate\nstate-of-the-art performance with respect to other monocular deep VO and SLAM\napproaches.\n", "versions": [{"version": "v1", "created": "Sun, 5 May 2019 08:58:35 GMT"}], "update_date": "2019-05-07", "authors_parsed": [["Wang", "Chengze", ""], ["Yuan", "Yuan", ""], ["Wang", "Qi", ""]]}, {"id": "1905.01635", "submitter": "Kai Li Lim", "authors": "Kai Li Lim and Thomas Br\\\"aunl", "title": "A Methodological Review of Visual Road Recognition Procedures for\n  Autonomous Driving Applications", "comments": "14 pages, 6 Figures, 2 Tables. Permission to reprint granted from\n  original figure authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The current research interest in autonomous driving is growing at a rapid\npace, attracting great investments from both the academic and corporate\nsectors. In order for vehicles to be fully autonomous, it is imperative that\nthe driver assistance system is adapt in road and lane keeping. In this paper,\nwe present a methodological review of techniques with a focus on visual road\ndetection and recognition. We adopt a pragmatic outlook in presenting this\nreview, whereby the procedures of road recognition is emphasised with respect\nto its practical implementations. The contribution of this review hence covers\nthe topic in two parts -- the first part describes the methodological approach\nto conventional road detection, which covers the algorithms and approaches\ninvolved to classify and segregate roads from non-road regions; and the other\npart focuses on recent state-of-the-art machine learning techniques that are\napplied to visual road recognition, with an emphasis on methods that\nincorporate convolutional neural networks and semantic segmentation. A\nsubsequent overview of recent implementations in the commercial sector is also\npresented, along with some recent research works pertaining to road detections.\n", "versions": [{"version": "v1", "created": "Sun, 5 May 2019 09:03:06 GMT"}], "update_date": "2019-05-07", "authors_parsed": [["Lim", "Kai Li", ""], ["Br\u00e4unl", "Thomas", ""]]}, {"id": "1905.01639", "submitter": "Dahun Kim", "authors": "Dahun Kim, Sanghyun Woo, Joon-Young Lee, In So Kweon", "title": "Deep Video Inpainting", "comments": "Accepted at CVPR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video inpainting aims to fill spatio-temporal holes with plausible content in\na video. Despite tremendous progress of deep neural networks for image\ninpainting, it is challenging to extend these methods to the video domain due\nto the additional time dimension. In this work, we propose a novel deep network\narchitecture for fast video inpainting. Built upon an image-based\nencoder-decoder model, our framework is designed to collect and refine\ninformation from neighbor frames and synthesize still-unknown regions. At the\nsame time, the output is enforced to be temporally consistent by a recurrent\nfeedback and a temporal memory module. Compared with the state-of-the-art image\ninpainting algorithm, our method produces videos that are much more\nsemantically correct and temporally smooth. In contrast to the prior video\ncompletion method which relies on time-consuming optimization, our method runs\nin near real-time while generating competitive video results. Finally, we\napplied our framework to video retargeting task, and obtain visually pleasing\nresults.\n", "versions": [{"version": "v1", "created": "Sun, 5 May 2019 09:23:35 GMT"}], "update_date": "2019-05-07", "authors_parsed": [["Kim", "Dahun", ""], ["Woo", "Sanghyun", ""], ["Lee", "Joon-Young", ""], ["Kweon", "In So", ""]]}, {"id": "1905.01641", "submitter": "Minjie Hua", "authors": "Minjie Hua, Fuyuan Shi, Yibing Nan, Kai Wang, Hao Chen, and Shiguo\n  Lian", "title": "Towards More Realistic Human-Robot Conversation: A Seq2Seq-based Body\n  Gesture Interaction System", "comments": "Accepted by IROS 2019. Slides: https://youtu.be/zOp6tT_etQE", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a novel system that enables intelligent robots to exhibit\nrealistic body gestures while communicating with humans. The proposed system\nconsists of a listening model and a speaking model used in corresponding\nconversational phases. Both models are adapted from the sequence-to-sequence\n(seq2seq) architecture to synthesize body gestures represented by the movements\nof twelve upper-body keypoints. All the extracted 2D keypoints are firstly\n3D-transformed, then rotated and normalized to discard irrelevant information.\nSubstantial videos of human conversations from Youtube are collected and\npreprocessed to train the listening and speaking models separately, after which\nthe two models are evaluated using metrics of mean squared error (MSE) and\ncosine similarity on the test dataset. The tuned system is implemented to drive\na virtual avatar as well as Pepper, a physical humanoid robot, to demonstrate\nthe improvement on conversational interaction abilities of our method in\npractice.\n", "versions": [{"version": "v1", "created": "Sun, 5 May 2019 09:53:29 GMT"}, {"version": "v2", "created": "Thu, 1 Aug 2019 07:38:41 GMT"}, {"version": "v3", "created": "Fri, 15 Nov 2019 06:36:28 GMT"}], "update_date": "2019-11-18", "authors_parsed": [["Hua", "Minjie", ""], ["Shi", "Fuyuan", ""], ["Nan", "Yibing", ""], ["Wang", "Kai", ""], ["Chen", "Hao", ""], ["Lian", "Shiguo", ""]]}, {"id": "1905.01657", "submitter": "Mahmoud Abdelgyd", "authors": "K.Amer, M.Samy, M.Shaker and M.ElHelw", "title": "Deep Convolutional Neural Network-Based Autonomous Drone Navigation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a novel approach for aerial drone autonomous navigation\nalong predetermined paths using only visual input form an onboard camera and\nwithout reliance on a Global Positioning System (GPS). It is based on using a\ndeep Convolutional Neural Network (CNN) combined with a regressor to output the\ndrone steering commands. Furthermore, multiple auxiliary navigation paths that\nform a navigation envelope are used for data augmentation to make the system\nadaptable to real-life deployment scenarios. The approach is suitable for\nautomating drone navigation in applications that exhibit regular trips or\nvisits to same locations such as environmental and desertification monitoring,\nparcel/aid delivery and drone-based wireless internet delivery. In this case,\nthe proposed algorithm replaces human operators, enhances accuracy of GPS-based\nmap navigation, alleviates problems related to GPS-spoofing and enables\nnavigation in GPS-denied environments. Our system is tested in two scenarios\nusing the Unreal Engine-based AirSim plugin for drone simulation with promising\nresults of average cross track distance less than 1.4 meters and mean waypoints\nminimum distance of less than 1 meter.\n", "versions": [{"version": "v1", "created": "Sun, 5 May 2019 11:25:24 GMT"}], "update_date": "2019-05-07", "authors_parsed": [["Amer", "K.", ""], ["Samy", "M.", ""], ["Shaker", "M.", ""], ["ElHelw", "M.", ""]]}, {"id": "1905.01658", "submitter": "Mahmoud Abdelgyd", "authors": "M. Samy, K. Amer, M. Shaker and M. ElHelw", "title": "Drone Path-Following in GPS-Denied Environments using Convolutional\n  Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  his paper presents a simple approach for drone navigation to follow a\npredetermined path using visual input only without reliance on a Global\nPositioning System (GPS). A Convolutional Neural Network (CNN) is used to\noutput the steering command of the drone in an end-to-end approach. We tested\nour approach in two simulated environments in the Unreal Engine using the\nAirSim plugin for drone simulation. Results show that the proposed approach,\ndespite its simplicity, has average cross track distance less than 2.9 meters\nin the simulated environment. We also investigate the significance of data\naugmentation in path following. Finally, we conclude by suggesting possible\nenhancements for extending our approach to more difficult paths in real life,\nin the hope that one day visual navigation will become the norm in GPS-denied\nzones.\n", "versions": [{"version": "v1", "created": "Sun, 5 May 2019 11:25:54 GMT"}], "update_date": "2019-05-07", "authors_parsed": [["Samy", "M.", ""], ["Amer", "K.", ""], ["Shaker", "M.", ""], ["ElHelw", "M.", ""]]}, {"id": "1905.01662", "submitter": "Qi Wang", "authors": "Qi Wang, Senior Member, IEEE, Zhenghang Yuan, Qian Du, Fellow, IEEE,\n  and Xuelong Li, Fellow, IEEE", "title": "GETNET: A General End-to-end Two-dimensional CNN Framework for\n  Hyperspectral Image Change Detection", "comments": null, "journal-ref": null, "doi": "10.1109/TGRS.2018.2849692", "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Change detection (CD) is an important application of remote sensing, which\nprovides timely change information about large-scale Earth surface. With the\nemergence of hyperspectral imagery, CD technology has been greatly promoted, as\nhyperspectral data with the highspectral resolution are capable of detecting\nfiner changes than using the traditional multispectral imagery. Nevertheless,\nthe high dimension of hyperspectral data makes it difficult to implement\ntraditional CD algorithms. Besides, endmember abundance information at subpixel\nlevel is often not fully utilized. In order to better handle high dimension\nproblem and explore abundance information, this paper presents a General\nEnd-to-end Two-dimensional CNN (GETNET) framework for hyperspectral image\nchange detection (HSI-CD). The main contributions of this work are threefold:\n1) Mixed-affinity matrix that integrates subpixel representation is introduced\nto mine more cross-channel gradient features and fuse multi-source information;\n2) 2-D CNN is designed to learn the discriminative features effectively from\nmulti-source data at a higher level and enhance the generalization ability of\nthe proposed CD algorithm; 3) A new HSI-CD data set is designed for the\nobjective comparison of different methods. Experimental results on real\nhyperspectral data sets demonstrate the proposed method outperforms most of the\nstate-of-the-arts.\n", "versions": [{"version": "v1", "created": "Sun, 5 May 2019 11:36:53 GMT"}], "update_date": "2019-05-15", "authors_parsed": [["Wang", "Qi", ""], ["Member", "Senior", ""], ["IEEE", "", ""], ["Yuan", "Zhenghang", ""], ["Du", "Qian", ""], ["Fellow", "", ""], ["IEEE", "", ""], ["Li", "Xuelong", ""], ["Fellow", "", ""], ["IEEE", "", ""]]}, {"id": "1905.01680", "submitter": "Kfir Aberman", "authors": "Kfir Aberman, Rundi Wu, Dani Lischinski, Baoquan Chen, Daniel Cohen-Or", "title": "Learning Character-Agnostic Motion for Motion Retargeting in 2D", "comments": "SIGGRAPH 2019. arXiv admin note: text overlap with arXiv:1804.05653\n  by other authors", "journal-ref": null, "doi": "10.1145/3306346.3322999", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Analyzing human motion is a challenging task with a wide variety of\napplications in computer vision and in graphics. One such application, of\nparticular importance in computer animation, is the retargeting of motion from\none performer to another. While humans move in three dimensions, the vast\nmajority of human motions are captured using video, requiring 2D-to-3D pose and\ncamera recovery, before existing retargeting approaches may be applied. In this\npaper, we present a new method for retargeting video-captured motion between\ndifferent human performers, without the need to explicitly reconstruct 3D poses\nand/or camera parameters. In order to achieve our goal, we learn to extract,\ndirectly from a video, a high-level latent motion representation, which is\ninvariant to the skeleton geometry and the camera view. Our key idea is to\ntrain a deep neural network to decompose temporal sequences of 2D poses into\nthree components: motion, skeleton, and camera view-angle. Having extracted\nsuch a representation, we are able to re-combine motion with novel skeletons\nand camera views, and decode a retargeted temporal sequence, which we compare\nto a ground truth from a synthetic dataset. We demonstrate that our framework\ncan be used to robustly extract human motion from videos, bypassing 3D\nreconstruction, and outperforming existing retargeting methods, when applied to\nvideos in-the-wild. It also enables additional applications, such as\nperformance cloning, video-driven cartoons, and motion retrieval.\n", "versions": [{"version": "v1", "created": "Sun, 5 May 2019 13:16:53 GMT"}], "update_date": "2019-05-13", "authors_parsed": [["Aberman", "Kfir", ""], ["Wu", "Rundi", ""], ["Lischinski", "Dani", ""], ["Chen", "Baoquan", ""], ["Cohen-Or", "Daniel", ""]]}, {"id": "1905.01681", "submitter": "Jianlong Chang", "authors": "Jianlong Chang, Yiwen Guo, Lingfeng Wang, Gaofeng Meng, Shiming Xiang,\n  Chunhong Pan", "title": "Deep Discriminative Clustering Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditional clustering methods often perform clustering with low-level\nindiscriminative representations and ignore relationships between patterns,\nresulting in slight achievements in the era of deep learning. To handle this\nproblem, we develop Deep Discriminative Clustering (DDC) that models the\nclustering task by investigating relationships between patterns with a deep\nneural network. Technically, a global constraint is introduced to adaptively\nestimate the relationships, and a local constraint is developed to endow the\nnetwork with the capability of learning high-level discriminative\nrepresentations. By iteratively training the network and estimating the\nrelationships in a mini-batch manner, DDC theoretically converges and the\ntrained network enables to generate a group of discriminative representations\nthat can be treated as clustering centers for straightway clustering. Extensive\nexperiments strongly demonstrate that DDC outperforms current methods on eight\nimage, text and audio datasets concurrently.\n", "versions": [{"version": "v1", "created": "Sun, 5 May 2019 13:22:03 GMT"}], "update_date": "2019-05-07", "authors_parsed": [["Chang", "Jianlong", ""], ["Guo", "Yiwen", ""], ["Wang", "Lingfeng", ""], ["Meng", "Gaofeng", ""], ["Xiang", "Shiming", ""], ["Pan", "Chunhong", ""]]}, {"id": "1905.01684", "submitter": "Lequan Yu", "authors": "Xianzhi Li, Lequan Yu, Chi-Wing Fu, Daniel Cohen-Or, Pheng-Ann Heng", "title": "Unsupervised Detection of Distinctive Regions on 3D Shapes", "comments": "Accepted by ACM TOG", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a novel approach to learn and detect distinctive regions\non 3D shapes. Unlike previous works, which require labeled data, our method is\nunsupervised. We conduct the analysis on point sets sampled from 3D shapes,\nthen formulate and train a deep neural network for an unsupervised shape\nclustering task to learn local and global features for distinguishing shapes\nwith respect to a given shape set. To drive the network to learn in an\nunsupervised manner, we design a clustering-based nonparametric softmax\nclassifier with an iterative re-clustering of shapes, and an adapted\ncontrastive loss for enhancing the feature embedding quality and stabilizing\nthe learning process. By then, we encourage the network to learn the point\ndistinctiveness on the input shapes. We extensively evaluate various aspects of\nour approach and present its applications for distinctiveness-guided shape\nretrieval, sampling, and view selection in 3D scenes.\n", "versions": [{"version": "v1", "created": "Sun, 5 May 2019 13:41:05 GMT"}, {"version": "v2", "created": "Mon, 20 Apr 2020 22:59:05 GMT"}], "update_date": "2020-04-22", "authors_parsed": [["Li", "Xianzhi", ""], ["Yu", "Lequan", ""], ["Fu", "Chi-Wing", ""], ["Cohen-Or", "Daniel", ""], ["Heng", "Pheng-Ann", ""]]}, {"id": "1905.01722", "submitter": "Takashi Isobe", "authors": "Takashi Isobe, Jian Han, Fang Zhu, Yali Li, Shengjin Wang", "title": "Intra-clip Aggregation for Video Person Re-identification", "comments": "Due to the privacy issue of person re-ID, we require to withdraw the\n  previous version of this paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video-based person re-identification has drawn massive attention in recent\nyears due to its extensive applications in video surveillance. While deep\nlearning-based methods have led to significant progress, these methods are\nlimited by ineffectively using complementary information, which is blamed on\nnecessary data augmentation in the training process. Data augmentation has been\nwidely used to mitigate the over-fitting trap and improve the ability of\nnetwork representation. However, the previous methods adopt image-based data\naugmentation scheme to individually process the input frames, which corrupts\nthe complementary information between consecutive frames and causes performance\ndegradation. Extensive experiments on three benchmark datasets demonstrate that\nour framework outperforms the most recent state-of-the-art methods. We also\nperform cross-dataset validation to prove the generality of our method.\n", "versions": [{"version": "v1", "created": "Sun, 5 May 2019 17:37:33 GMT"}, {"version": "v2", "created": "Tue, 21 Jul 2020 05:08:46 GMT"}, {"version": "v3", "created": "Sun, 14 Mar 2021 02:52:18 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Isobe", "Takashi", ""], ["Han", "Jian", ""], ["Zhu", "Fang", ""], ["Li", "Yali", ""], ["Wang", "Shengjin", ""]]}, {"id": "1905.01723", "submitter": "Ming-Yu Liu", "authors": "Ming-Yu Liu and Xun Huang and Arun Mallya and Tero Karras and Timo\n  Aila and Jaakko Lehtinen and Jan Kautz", "title": "Few-Shot Unsupervised Image-to-Image Translation", "comments": "The paper will be presented at the International Conference on\n  Computer Vision (ICCV) 2019", "journal-ref": "ICCV 2019", "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.GR cs.MM stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unsupervised image-to-image translation methods learn to map images in a\ngiven class to an analogous image in a different class, drawing on unstructured\n(non-registered) datasets of images. While remarkably successful, current\nmethods require access to many images in both source and destination classes at\ntraining time. We argue this greatly limits their use. Drawing inspiration from\nthe human capability of picking up the essence of a novel object from a small\nnumber of examples and generalizing from there, we seek a few-shot,\nunsupervised image-to-image translation algorithm that works on previously\nunseen target classes that are specified, at test time, only by a few example\nimages. Our model achieves this few-shot generation capability by coupling an\nadversarial training scheme with a novel network design. Through extensive\nexperimental validation and comparisons to several baseline methods on\nbenchmark datasets, we verify the effectiveness of the proposed framework. Our\nimplementation and datasets are available at https://github.com/NVlabs/FUNIT .\n", "versions": [{"version": "v1", "created": "Sun, 5 May 2019 17:41:31 GMT"}, {"version": "v2", "created": "Mon, 9 Sep 2019 06:11:56 GMT"}], "update_date": "2019-09-10", "authors_parsed": [["Liu", "Ming-Yu", ""], ["Huang", "Xun", ""], ["Mallya", "Arun", ""], ["Karras", "Tero", ""], ["Aila", "Timo", ""], ["Lehtinen", "Jaakko", ""], ["Kautz", "Jan", ""]]}, {"id": "1905.01726", "submitter": "Vikash Sehwag", "authors": "Vikash Sehwag, Arjun Nitin Bhagoji, Liwei Song, Chawin Sitawarin,\n  Daniel Cullina, Mung Chiang, Prateek Mittal", "title": "Better the Devil you Know: An Analysis of Evasion Attacks using\n  Out-of-Distribution Adversarial Examples", "comments": "18 pages, 5 figures, 9 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A large body of recent work has investigated the phenomenon of evasion\nattacks using adversarial examples for deep learning systems, where the\naddition of norm-bounded perturbations to the test inputs leads to incorrect\noutput classification. Previous work has investigated this phenomenon in\nclosed-world systems where training and test inputs follow a pre-specified\ndistribution. However, real-world implementations of deep learning\napplications, such as autonomous driving and content classification are likely\nto operate in the open-world environment. In this paper, we demonstrate the\nsuccess of open-world evasion attacks, where adversarial examples are generated\nfrom out-of-distribution inputs (OOD adversarial examples). In our study, we\nuse 11 state-of-the-art neural network models trained on 3 image datasets of\nvarying complexity. We first demonstrate that state-of-the-art detectors for\nout-of-distribution data are not robust against OOD adversarial examples. We\nthen consider 5 known defenses for adversarial examples, including\nstate-of-the-art robust training methods, and show that against these defenses,\nOOD adversarial examples can achieve up to 4$\\times$ higher target success\nrates compared to adversarial examples generated from in-distribution data. We\nalso take a quantitative look at how open-world evasion attacks may affect\nreal-world systems. Finally, we present the first steps towards a robust\nopen-world machine learning system.\n", "versions": [{"version": "v1", "created": "Sun, 5 May 2019 18:06:41 GMT"}], "update_date": "2019-05-07", "authors_parsed": [["Sehwag", "Vikash", ""], ["Bhagoji", "Arjun Nitin", ""], ["Song", "Liwei", ""], ["Sitawarin", "Chawin", ""], ["Cullina", "Daniel", ""], ["Chiang", "Mung", ""], ["Mittal", "Prateek", ""]]}, {"id": "1905.01743", "submitter": "Alexander Rakhlin", "authors": "Alexander Rakhlin, Aleksei Tiulpin, Alexey A. Shvets, Alexandr A.\n  Kalinin, Vladimir I. Iglovikov, Sergey Nikolenko", "title": "Breast Tumor Cellularity Assessment using Deep Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Breast cancer is one of the main causes of death worldwide. Histopathological\ncellularity assessment of residual tumors in post-surgical tissues is used to\nanalyze a tumor's response to a therapy. Correct cellularity assessment\nincreases the chances of getting an appropriate treatment and facilitates the\npatient's survival. In current clinical practice, tumor cellularity is manually\nestimated by pathologists; this process is tedious and prone to errors or low\nagreement rates between assessors. In this work, we evaluated three strong\nnovel Deep Learning-based approaches for automatic assessment of tumor\ncellularity from post-treated breast surgical specimens stained with\nhematoxylin and eosin. We validated the proposed methods on the BreastPathQ\nSPIE challenge dataset that consisted of 2395 image patches selected from whole\nslide images acquired from 64 patients. Compared to expert pathologist scoring,\nour best performing method yielded the Cohen's kappa coefficient of 0.70 (vs.\n0.42 previously known in literature) and the intra-class correlation\ncoefficient of 0.89 (vs. 0.83). Our results suggest that Deep Learning-based\nmethods have a significant potential to alleviate the burden on pathologists,\nenhance the diagnostic workflow, and, thereby, facilitate better clinical\noutcomes in breast cancer treatment.\n", "versions": [{"version": "v1", "created": "Sun, 5 May 2019 20:15:12 GMT"}, {"version": "v2", "created": "Thu, 29 Aug 2019 21:39:43 GMT"}, {"version": "v3", "created": "Tue, 3 Sep 2019 08:31:24 GMT"}], "update_date": "2019-09-04", "authors_parsed": [["Rakhlin", "Alexander", ""], ["Tiulpin", "Aleksei", ""], ["Shvets", "Alexey A.", ""], ["Kalinin", "Alexandr A.", ""], ["Iglovikov", "Vladimir I.", ""], ["Nikolenko", "Sergey", ""]]}, {"id": "1905.01744", "submitter": "Zhiqiang Shen", "authors": "Zhiqiang Shen and Mingyang Huang and Jianping Shi and Xiangyang Xue\n  and Thomas Huang", "title": "Towards Instance-level Image-to-Image Translation", "comments": "Accepted to CVPR 2019. Project page:\n  http://zhiqiangshen.com/projects/INIT/index.html", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unpaired Image-to-image Translation is a new rising and challenging vision\nproblem that aims to learn a mapping between unaligned image pairs in diverse\ndomains. Recent advances in this field like MUNIT and DRIT mainly focus on\ndisentangling content and style/attribute from a given image first, then\ndirectly adopting the global style to guide the model to synthesize new domain\nimages. However, this kind of approaches severely incurs contradiction if the\ntarget domain images are content-rich with multiple discrepant objects. In this\npaper, we present a simple yet effective instance-aware image-to-image\ntranslation approach (INIT), which employs the fine-grained local (instance)\nand global styles to the target image spatially. The proposed INIT exhibits\nthree import advantages: (1) the instance-level objective loss can help learn a\nmore accurate reconstruction and incorporate diverse attributes of objects; (2)\nthe styles used for target domain of local/global areas are from corresponding\nspatial regions in source domain, which intuitively is a more reasonable\nmapping; (3) the joint training process can benefit both fine and coarse\ngranularity and incorporates instance information to improve the quality of\nglobal translation. We also collect a large-scale benchmark for the new\ninstance-level translation task. We observe that our synthetic images can even\nbenefit real-world vision tasks like generic object detection.\n", "versions": [{"version": "v1", "created": "Sun, 5 May 2019 20:16:41 GMT"}], "update_date": "2019-05-07", "authors_parsed": [["Shen", "Zhiqiang", ""], ["Huang", "Mingyang", ""], ["Shi", "Jianping", ""], ["Xue", "Xiangyang", ""], ["Huang", "Thomas", ""]]}, {"id": "1905.01752", "submitter": "Shivangi Srivastava", "authors": "Shivangi Srivastava and John E. Vargas-Mu\\~noz and Devis Tuia", "title": "Understanding urban landuse from the above and ground perspectives: a\n  deep learning, multimodal solution", "comments": null, "journal-ref": "Remote Sensing of Environment, 228, pages 129 - 143, 2019", "doi": "10.1016/j.rse.2019.04.014", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Landuse characterization is important for urban planning. It is traditionally\nperformed with field surveys or manual photo interpretation, two practices that\nare time-consuming and labor-intensive. Therefore, we aim to automate landuse\nmapping at the urban-object level with a deep learning approach based on data\nfrom multiple sources (or modalities). We consider two image modalities:\noverhead imagery from Google Maps and ensembles of ground-based pictures\n(side-views) per urban-object from Google Street View (GSV). These modalities\nbring complementary visual information pertaining to the urban-objects. We\npropose an end-to-end trainable model, which uses OpenStreetMap annotations as\nlabels. The model can accommodate a variable number of GSV pictures for the\nground-based branch and can also function in the absence of ground pictures at\nprediction time. We test the effectiveness of our model over the area of\n\\^Ile-de-France, France, and test its generalization abilities on a set of\nurban-objects from the city of Nantes, France. Our proposed multimodal\nConvolutional Neural Network achieves considerably higher accuracies than\nmethods that use a single image modality, making it suitable for automatic\nlanduse map updates. Additionally, our approach could be easily scaled to\nmultiple cities, because it is based on data sources available for many cities\nworldwide.\n", "versions": [{"version": "v1", "created": "Sun, 5 May 2019 21:36:59 GMT"}], "update_date": "2019-05-07", "authors_parsed": [["Srivastava", "Shivangi", ""], ["Vargas-Mu\u00f1oz", "John E.", ""], ["Tuia", "Devis", ""]]}, {"id": "1905.01772", "submitter": "Amol Kapoor", "authors": "Amol Kapoor, Hunter Larco, Raimondas Kiveris", "title": "Nostalgin: Extracting 3D City Models from Historical Image Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CG cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  What did it feel like to walk through a city from the past? In this work, we\ndescribe Nostalgin (Nostalgia Engine), a method that can faithfully reconstruct\ncities from historical images. Unlike existing work in city reconstruction, we\nfocus on the task of reconstructing 3D cities from historical images. Working\nwith historical image data is substantially more difficult, as there are\nsignificantly fewer buildings available and the details of the camera\nparameters which captured the images are unknown. Nostalgin can generate a city\nmodel even if there is only a single image per facade, regardless of viewpoint\nor occlusions. To achieve this, our novel architecture combines image\nsegmentation, rectification, and inpainting. We motivate our design decisions\nwith experimental analysis of individual components of our pipeline, and show\nthat we can improve on baselines in both speed and visual realism. We\ndemonstrate the efficacy of our pipeline by recreating two 1940s Manhattan city\nblocks. We aim to deploy Nostalgin as an open source platform where users can\ngenerate immersive historical experiences from their own photos.\n", "versions": [{"version": "v1", "created": "Mon, 6 May 2019 00:18:15 GMT"}], "update_date": "2019-05-07", "authors_parsed": [["Kapoor", "Amol", ""], ["Larco", "Hunter", ""], ["Kiveris", "Raimondas", ""]]}, {"id": "1905.01786", "submitter": "Jianlong Chang", "authors": "Jianlong Chang, Xinbang Zhang, Yiwen Guo, Gaofeng Meng, Shiming Xiang,\n  Chunhong Pan", "title": "Differentiable Architecture Search with Ensemble Gumbel-Softmax", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For network architecture search (NAS), it is crucial but challenging to\nsimultaneously guarantee both effectiveness and efficiency. Towards achieving\nthis goal, we develop a differentiable NAS solution, where the search space\nincludes arbitrary feed-forward network consisting of the predefined number of\nconnections. Benefiting from a proposed ensemble Gumbel-Softmax estimator, our\nmethod optimizes both the architecture of a deep network and its parameters in\nthe same round of backward propagation, yielding an end-to-end mechanism of\nsearching network architectures. Extensive experiments on a variety of popular\ndatasets strongly evidence that our method is capable of discovering\nhigh-performance architectures, while guaranteeing the requisite efficiency\nduring searching.\n", "versions": [{"version": "v1", "created": "Mon, 6 May 2019 01:47:17 GMT"}], "update_date": "2019-05-07", "authors_parsed": [["Chang", "Jianlong", ""], ["Zhang", "Xinbang", ""], ["Guo", "Yiwen", ""], ["Meng", "Gaofeng", ""], ["Xiang", "Shiming", ""], ["Pan", "Chunhong", ""]]}, {"id": "1905.01787", "submitter": "Yiwu Yao", "authors": "Yiwu Yao, Weiqiang Yang, Haoqi Zhu", "title": "Creating Lightweight Object Detectors with Model Compression for\n  Deployment on Edge Devices", "comments": "lightweight detector, automatic channel pruning, fixed channel\n  deletion, knowledge distillation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To achieve lightweight object detectors for deployment on the edge devices,\nan effective model compression pipeline is proposed in this paper. The\ncompression pipeline consists of automatic channel pruning for the backbone,\nfixed channel deletion for the branch layers and knowledge distillation for the\nguidance learning. As results, the Resnet50-v1d is auto-pruned and fine-tuned\non ImageNet to attain a compact base model as the backbone of object detector.\nThen, lightweight object detectors are implemented with proposed compression\npipeline. For instance, the SSD-300 with model size=16.3MB, FLOPS=2.31G, and\nmAP=71.2 is created, revealing a better result than SSD-300-MobileNet.\n", "versions": [{"version": "v1", "created": "Mon, 6 May 2019 01:52:17 GMT"}], "update_date": "2019-05-07", "authors_parsed": [["Yao", "Yiwu", ""], ["Yang", "Weiqiang", ""], ["Zhu", "Haoqi", ""]]}, {"id": "1905.01796", "submitter": "Jinqiang Bai", "authors": "Zhaoxiang Liu, Huan Hu, Jinqiang Bai, Shaohua Li, Shiguo Lian", "title": "Feature Aggregation Network for Video Face Recognition", "comments": "9 pages, 4 figures, Accepted by ICCV 2019 workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper aims to learn a compact representation of a video for video face\nrecognition task. We make the following contributions: first, we propose a meta\nattention-based aggregation scheme which adaptively and fine-grained weighs the\nfeature along each feature dimension among all frames to form a compact and\ndiscriminative representation. It makes the best to exploit the valuable or\ndiscriminative part of each frame to promote the performance of face\nrecognition, without discarding or despising low quality frames as usual\nmethods do. Second, we build a feature aggregation network comprised of a\nfeature embedding module and a feature aggregation module. The embedding module\nis a convolutional neural network used to extract a feature vector from a face\nimage, while the aggregation module consists of cascaded two meta attention\nblocks which adaptively aggregate the feature vectors into a single\nfixed-length representation. The network can deal with arbitrary number of\nframes, and is insensitive to frame order. Third, we validate the performance\nof proposed aggregation scheme. Experiments on publicly available datasets,\nsuch as YouTube face dataset and IJB-A dataset, show the effectiveness of our\nmethod, and it achieves competitive performances on both the verification and\nidentification protocols.\n", "versions": [{"version": "v1", "created": "Mon, 6 May 2019 02:37:12 GMT"}, {"version": "v2", "created": "Thu, 12 Sep 2019 09:03:09 GMT"}], "update_date": "2019-09-13", "authors_parsed": [["Liu", "Zhaoxiang", ""], ["Hu", "Huan", ""], ["Bai", "Jinqiang", ""], ["Li", "Shaohua", ""], ["Lian", "Shiguo", ""]]}, {"id": "1905.01817", "submitter": "Song Gao", "authors": "Yuhao Kang, Qingyuan Jia, Song Gao, Xiaohuan Zeng, Yueyao Wang,\n  Stephan Angsuesser, Yu Liu, Xinyue Ye, Teng Fei", "title": "Extracting human emotions at different places based on facial\n  expressions and spatial clustering analysis", "comments": "40 pages; 9 figures", "journal-ref": "Transactions in GIS, Year 2019, Volume 23, Issue 3", "doi": "10.1111/tgis.12552", "report-no": null, "categories": "cs.CV cs.HC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The emergence of big data enables us to evaluate the various human emotions\nat places from a statistic perspective by applying affective computing. In this\nstudy, a novel framework for extracting human emotions from large-scale\ngeoreferenced photos at different places is proposed. After the construction of\nplaces based on spatial clustering of user generated footprints collected in\nsocial media websites, online cognitive services are utilized to extract human\nemotions from facial expressions using the state-of-the-art computer vision\ntechniques. And two happiness metrics are defined for measuring the human\nemotions at different places. To validate the feasibility of the framework, we\ntake 80 tourist attractions around the world as an example and a happiness\nranking list of places is generated based on human emotions calculated over 2\nmillion faces detected out from over 6 million photos. Different kinds of\ngeographical contexts are taken into consideration to find out the relationship\nbetween human emotions and environmental factors. Results show that much of the\nemotional variation at different places can be explained by a few factors such\nas openness. The research may offer insights on integrating human emotions to\nenrich the understanding of sense of place in geography and in place-based GIS.\n", "versions": [{"version": "v1", "created": "Mon, 6 May 2019 04:10:37 GMT"}], "update_date": "2019-05-07", "authors_parsed": [["Kang", "Yuhao", ""], ["Jia", "Qingyuan", ""], ["Gao", "Song", ""], ["Zeng", "Xiaohuan", ""], ["Wang", "Yueyao", ""], ["Angsuesser", "Stephan", ""], ["Liu", "Yu", ""], ["Ye", "Xinyue", ""], ["Fei", "Teng", ""]]}, {"id": "1905.01851", "submitter": "Yu Shu", "authors": "Yu Shu, Yemin Shi, Yaowei Wang, Tiejun Huang, Yonghong Tian", "title": "P-ODN: Prototype based Open Deep Network for Open Set Recognition", "comments": "15 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most of the existing recognition algorithms are proposed for closed set\nscenarios, where all categories are known beforehand. However, in practice,\nrecognition is essentially an open set problem. There are categories we know\ncalled \"knowns\", and there are more we do not know called \"unknowns\".\nEnumerating all categories beforehand is never possible, consequently it is\ninfeasible to prepare sufficient training samples for those unknowns. Applying\nclosed set recognition methods will naturally lead to unseen-category errors.\nTo address this problem, we propose the prototype based Open Deep Network\n(P-ODN) for open set recognition tasks. Specifically, we introduce prototype\nlearning into open set recognition. Prototypes and prototype radiuses are\ntrained jointly to guide a CNN network to derive more discriminative features.\nThen P-ODN detects the unknowns by applying a multi-class triplet thresholding\nmethod based on the distance metric between features and prototypes. Manual\nlabeling the unknowns which are detected in the previous process as new\ncategories. Predictors for new categories are added to the classification layer\nto \"open\" the deep neural networks to incorporate new categories dynamically.\nThe weights of new predictors are initialized exquisitely by applying a\ndistances based algorithm to transfer the learned knowledge. Consequently, this\ninitialization method speed up the fine-tuning process and reduce the samples\nneeded to train new predictors. Extensive experiments show that P-ODN can\neffectively detect unknowns and needs only few samples with human intervention\nto recognize a new category. In the real world scenarios, our method achieves\nstate-of-the-art performance on the UCF11, UCF50, UCF101 and HMDB51 datasets.\n", "versions": [{"version": "v1", "created": "Mon, 6 May 2019 07:30:59 GMT"}, {"version": "v2", "created": "Mon, 13 Jan 2020 07:51:14 GMT"}], "update_date": "2020-01-14", "authors_parsed": [["Shu", "Yu", ""], ["Shi", "Yemin", ""], ["Wang", "Yaowei", ""], ["Huang", "Tiejun", ""], ["Tian", "Yonghong", ""]]}, {"id": "1905.01861", "submitter": "Arnaud Dapogny", "authors": "Arnaud Dapogny, Matthieu Cord and Patrick Perez", "title": "The Missing Data Encoder: Cross-Channel Image Completion\\\\with\n  Hide-And-Seek Adversarial Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image completion is the problem of generating whole images from fragments\nonly. It encompasses inpainting (generating a patch given its surrounding),\nreverse inpainting/extrapolation (generating the periphery given the central\npatch) as well as colorization (generating one or several channels given other\nones). In this paper, we employ a deep network to perform image completion,\nwith adversarial training as well as perceptual and completion losses, and call\nit the ``missing data encoder'' (MDE). We consider several configurations based\non how the seed fragments are chosen. We show that training MDE for ``random\nextrapolation and colorization'' (MDE-REC), i.e. using random\nchannel-independent fragments, allows a better capture of the image semantics\nand geometry. MDE training makes use of a novel ``hide-and-seek'' adversarial\nloss, where the discriminator seeks the original non-masked regions, while the\ngenerator tries to hide them. We validate our models both qualitatively and\nquantitatively on several datasets, showing their interest for image\ncompletion, unsupervised representation learning as well as face occlusion\nhandling.\n", "versions": [{"version": "v1", "created": "Mon, 6 May 2019 07:51:04 GMT"}], "update_date": "2019-05-07", "authors_parsed": [["Dapogny", "Arnaud", ""], ["Cord", "Matthieu", ""], ["Perez", "Patrick", ""]]}, {"id": "1905.01892", "submitter": "Arnaud Dapogny", "authors": "Yifu Chen, Arnaud Dapogny and Matthieu Cord", "title": "SEMEDA: Enhancing Segmentation Precision with Semantic Edge Aware Loss", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While nowadays deep neural networks achieve impressive performances on\nsemantic segmentation tasks, they are usually trained by optimizing pixel-wise\nlosses such as cross-entropy. As a result, the predictions outputted by such\nnetworks usually struggle to accurately capture the object boundaries and\nexhibit holes inside the objects. In this paper, we propose a novel approach to\nimprove the structure of the predicted segmentation masks. We introduce a novel\nsemantic edge detection network, which allows to match the predicted and ground\ntruth segmentation masks. This Semantic Edge-Aware strategy (SEMEDA) can be\ncombined with any backbone deep network in an end-to-end training framework.\nThrough thorough experimental validation on Pascal VOC 2012 and Cityscapes\ndatasets, we show that the proposed SEMEDA approach enhances the structure of\nthe predicted segmentation masks by enforcing sharp boundaries and avoiding\ndiscontinuities inside objects, improving the segmentation performance. In\naddition, our semantic edge-aware loss can be integrated into any popular\nsegmentation network without requiring any additional annotation and with\nnegligible computational load, as compared to standard pixel-wise cross-entropy\nloss.\n", "versions": [{"version": "v1", "created": "Mon, 6 May 2019 09:14:01 GMT"}], "update_date": "2019-05-07", "authors_parsed": [["Chen", "Yifu", ""], ["Dapogny", "Arnaud", ""], ["Cord", "Matthieu", ""]]}, {"id": "1905.01902", "submitter": "Zheren Li", "authors": "Jie Xing, Zheren Li, Biyuan Wang, Yuji Qi, Bingbin Yu, Farhad G.\n  Zanjani, Aiwen Zheng, Remco Duits, Tao Tan", "title": "Lesion Segmentation in Ultrasound Using Semi-pixel-wise Cycle Generative\n  Adversarial Nets", "comments": null, "journal-ref": "IEEE/ACM Transactions on Computational Biology and Bioinformatics,\n  04 March 2020, pp.1-1", "doi": "10.1109/TCBB.2020.2978470", "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Breast cancer is the most common invasive cancer with the highest cancer\noccurrence in females. Handheld ultrasound is one of the most efficient ways to\nidentify and diagnose the breast cancer. The area and the shape information of\na lesion is very helpful for clinicians to make diagnostic decisions. In this\nstudy we propose a new deep-learning scheme, semi-pixel-wise cycle generative\nadversarial net (SPCGAN) for segmenting the lesion in 2D ultrasound. The method\ntakes the advantage of a fully convolutional neural network (FCN) and a\ngenerative adversarial net to segment a lesion by using prior knowledge. We\ncompared the proposed method to a fully connected neural network and the level\nset segmentation method on a test dataset consisting of 32 malignant lesions\nand 109 benign lesions. Our proposed method achieved a Dice similarity\ncoefficient (DSC) of 0.92 while FCN and the level set achieved 0.90 and 0.79\nrespectively. Particularly, for malignant lesions, our method increases the DSC\n(0.90) of the fully connected neural network to 0.93 significantly (p$<$0.001).\nThe results show that our SPCGAN can obtain robust segmentation results. The\nframework of SPCGAN is particularly effective when sufficient training samples\nare not available compared to FCN. Our proposed method may be used to relieve\nthe radiologists' burden for annotation.\n", "versions": [{"version": "v1", "created": "Mon, 6 May 2019 09:49:57 GMT"}, {"version": "v2", "created": "Sat, 18 May 2019 07:10:31 GMT"}, {"version": "v3", "created": "Mon, 27 May 2019 03:51:12 GMT"}, {"version": "v4", "created": "Sat, 17 Oct 2020 04:59:42 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Xing", "Jie", ""], ["Li", "Zheren", ""], ["Wang", "Biyuan", ""], ["Qi", "Yuji", ""], ["Yu", "Bingbin", ""], ["Zanjani", "Farhad G.", ""], ["Zheng", "Aiwen", ""], ["Duits", "Remco", ""], ["Tan", "Tao", ""]]}, {"id": "1905.01919", "submitter": "Philipp Harzig", "authors": "Philipp Harzig, Dan Zecha, Rainer Lienhart, Carolin Kaiser, Ren\\'e\n  Schallner", "title": "Image Captioning with Clause-Focused Metrics in a Multi-Modal Setting\n  for Marketing", "comments": "6 pages, accepted at MIPR 2019", "journal-ref": null, "doi": "10.1109/MIPR.2019.00085", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatically generating descriptive captions for images is a well-researched\narea in computer vision. However, existing evaluation approaches focus on\nmeasuring the similarity between two sentences disregarding fine-grained\nsemantics of the captions. In our setting of images depicting persons\ninteracting with branded products, the subject, predicate, object and the name\nof the branded product are important evaluation criteria of the generated\ncaptions. Generating image captions with these constraints is a new challenge,\nwhich we tackle in this work. By simultaneously predicting integer-valued\nratings that describe attributes of the human-product interaction, we optimize\na deep neural network architecture in a multi-task learning setting, which\nconsiderably improves the caption quality. Furthermore, we introduce a novel\nmetric that allows us to assess whether the generated captions meet our\nrequirements (i.e., subject, predicate, object, and product name) and describe\na series of experiments on caption quality and how to address annotator\ndisagreements for the image ratings with an approach called soft targets. We\nalso show that our novel clause-focused metrics are also applicable to other\nimage captioning datasets, such as the popular MSCOCO dataset.\n", "versions": [{"version": "v1", "created": "Mon, 6 May 2019 10:42:10 GMT"}], "update_date": "2019-08-07", "authors_parsed": [["Harzig", "Philipp", ""], ["Zecha", "Dan", ""], ["Lienhart", "Rainer", ""], ["Kaiser", "Carolin", ""], ["Schallner", "Ren\u00e9", ""]]}, {"id": "1905.01920", "submitter": "Haozhi Huang", "authors": "Sen-Zhe Xu, Hao-Zhi Huang, Shi-Min Hu, Wei Liu", "title": "FaceShapeGene: A Disentangled Shape Representation for Flexible Face\n  Image Editing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing methods for face image manipulation generally focus on editing the\nexpression, changing some predefined attributes, or applying different filters.\nHowever, users lack the flexibility of controlling the shapes of different\nsemantic facial parts in the generated face. In this paper, we propose an\napproach to compute a disentangled shape representation for a face image,\nnamely the FaceShapeGene. The proposed FaceShapeGene encodes the shape\ninformation of each semantic facial part separately into a 1D latent vector. On\nthe basis of the FaceShapeGene, a novel part-wise face image editing system is\ndeveloped, which contains a shape-remix network and a conditional label-to-face\ntransformer. The shape-remix network can freely recombine the part-wise latent\nvectors from different individuals, producing a remixed face shape in the form\nof a label map, which contains the facial characteristics of multiple subjects.\nThe conditional label-to-face transformer, which is trained in an unsupervised\ncyclic manner, performs part-wise face editing while preserving the original\nidentity of the subject. Experimental results on several tasks demonstrate that\nthe proposed FaceShapeGene representation correctly disentangles the shape\nfeatures of different semantic parts. %In addition, we test our system on\nseveral novel part-wise face editing tasks. Comparisons to existing methods\ndemonstrate the superiority of the proposed method on accomplishing novel face\nediting tasks.\n", "versions": [{"version": "v1", "created": "Mon, 6 May 2019 10:48:44 GMT"}], "update_date": "2019-05-07", "authors_parsed": [["Xu", "Sen-Zhe", ""], ["Huang", "Hao-Zhi", ""], ["Hu", "Shi-Min", ""], ["Liu", "Wei", ""]]}, {"id": "1905.01924", "submitter": "Lukas Hahn", "authors": "Lukas Hahn, Lutz Roese-Koerner, Klaus Friedrichs, Anton Kummert", "title": "Fast and Reliable Architecture Selection for Convolutional Neural\n  Networks", "comments": "As published in the proceedings of the 27th European Symposium on\n  Artificial Neural Networks, Computational Intelligence and Machine Learning\n  (ESANN), pages 179-184, Bruges 2019. 6 pages, 2 figures, 1 table", "journal-ref": "Proceedings of the 27th European Symposium on Artificial Neural\n  Networks, Computational Intelligence and Machine Learning (ESANN), pages\n  179-184, Bruges 2019", "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The performance of a Convolutional Neural Network (CNN) depends on its\nhyperparameters, like the number of layers, kernel sizes, or the learning rate\nfor example. Especially in smaller networks and applications with limited\ncomputational resources, optimisation is key. We present a fast and efficient\napproach for CNN architecture selection. Taking into account time consumption,\nprecision and robustness, we develop a heuristic to quickly and reliably assess\na network's performance. In combination with Bayesian optimisation (BO), to\neffectively cover the vast parameter space, our contribution offers a plain and\npowerful architecture search for this machine learning technique.\n", "versions": [{"version": "v1", "created": "Mon, 6 May 2019 10:58:23 GMT"}], "update_date": "2019-05-07", "authors_parsed": [["Hahn", "Lukas", ""], ["Roese-Koerner", "Lutz", ""], ["Friedrichs", "Klaus", ""], ["Kummert", "Anton", ""]]}, {"id": "1905.01932", "submitter": "Xiangwei Shi", "authors": "Xiangwei Shi, Seyran Khademi, Jan van Gemert", "title": "Deep Visual City Recognition Visualization", "comments": "CVPR-19 workshop on Explainable AI", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding how cities visually differ from each others is interesting for\nplanners, residents, and historians. We investigate the interpretation of deep\nfeatures learned by convolutional neural networks (CNNs) for city recognition.\nGiven a trained city recognition network, we first generate weighted masks\nusing the known Grad-CAM technique and to select the most discriminate regions\nin the image. Since the image classification label is the city name, it\ncontains no information of objects that are class-discriminate, we investigate\nthe interpretability of deep representations with two methods. (i) Unsupervised\nmethod is used to cluster the objects appearing in the visual explanations.\n(ii) A pretrained semantic segmentation model is used to label objects in pixel\nlevel, and then we introduce statistical measures to quantitatively evaluate\nthe interpretability of discriminate objects. The influence of network\narchitectures and random initializations in training, is studied on the\ninterpretability of CNN features for city recognition. The results suggest that\nnetwork architectures would affect the interpretability of learned visual\nrepresentations greater than different initializations.\n", "versions": [{"version": "v1", "created": "Mon, 6 May 2019 11:24:33 GMT"}], "update_date": "2019-05-07", "authors_parsed": [["Shi", "Xiangwei", ""], ["Khademi", "Seyran", ""], ["van Gemert", "Jan", ""]]}, {"id": "1905.01941", "submitter": "Seonwook Park", "authors": "Seonwook Park and Shalini De Mello and Pavlo Molchanov and Umar Iqbal\n  and Otmar Hilliges and Jan Kautz", "title": "Few-Shot Adaptive Gaze Estimation", "comments": "ICCV 2019 (Oral)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inter-personal anatomical differences limit the accuracy of\nperson-independent gaze estimation networks. Yet there is a need to lower gaze\nerrors further to enable applications requiring higher quality. Further gains\ncan be achieved by personalizing gaze networks, ideally with few calibration\nsamples. However, over-parameterized neural networks are not amenable to\nlearning from few examples as they can quickly over-fit. We embrace these\nchallenges and propose a novel framework for Few-shot Adaptive GaZE Estimation\n(FAZE) for learning person-specific gaze networks with very few (less than or\nequal to 9) calibration samples. FAZE learns a rotation-aware latent\nrepresentation of gaze via a disentangling encoder-decoder architecture along\nwith a highly adaptable gaze estimator trained using meta-learning. It is\ncapable of adapting to any new person to yield significant performance gains\nwith as few as 3 samples, yielding state-of-the-art performance of 3.18 degrees\non GazeCapture, a 19% improvement over prior art. We open-source our code at\nhttps://github.com/NVlabs/few_shot_gaze\n", "versions": [{"version": "v1", "created": "Mon, 6 May 2019 11:48:39 GMT"}, {"version": "v2", "created": "Mon, 14 Oct 2019 14:55:59 GMT"}], "update_date": "2019-10-15", "authors_parsed": [["Park", "Seonwook", ""], ["De Mello", "Shalini", ""], ["Molchanov", "Pavlo", ""], ["Iqbal", "Umar", ""], ["Hilliges", "Otmar", ""], ["Kautz", "Jan", ""]]}, {"id": "1905.01950", "submitter": "J{\\o}rgen Falck Erichsen M.Sc.", "authors": "Jorgen F. Erichsen, Heikki Sj\\\"oman, Martin Steinert, Torgeir Welo", "title": "Digitally Capturing Physical Prototypes During Early-Stage Engineering\n  Design Projects for Initial Analysis of Project Output and Progression", "comments": "27 pages, 2 tables, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Aiming to help researchers capture output from the early stages of\nengineering design projects, this article presents a new research tool for\ndigitally capturing physical prototypes. The motivation for this work is to\ncollect observations that can aid in understanding prototyping in the early\nstages of engineering design projects, and this article investigates if and how\ndigital capture of physical prototypes can be used for this purpose.\nEarly-stage prototypes are usually rough and of low-fidelity and are thus often\ndiscarded or substantially modified through the projects. Hence, retrospective\naccess to prototypes is a challenge when trying to gather accurate empirical\ndata. To capture the prototypes developed through the early stages of a\nproject, a new research tool has been developed for capturing prototypes\nthrough multi-view images, along with metadata describing by whom, why, when\nand where the prototypes were captured. Over the course of 17 months, this\nresearch tool has been used to capture more than 800 physical prototypes from\n76 individual users across many projects. In this article, one project is shown\nin detail to demonstrate how this capturing system can gather empirical data\nfor enriching engineering design project cases that focus on prototyping for\nconcept generation. The authors also analyse the metadata provided by the\nsystem to give understanding into prototyping patterns in the projects. Lastly,\nthrough enabling digital capture of large quantities of data, the research tool\npresents the foundations for training artificial intelligence-based predictors\nand classifiers that can be used for analysis in engineering design research.\n", "versions": [{"version": "v1", "created": "Fri, 26 Apr 2019 09:04:37 GMT"}, {"version": "v2", "created": "Thu, 19 Mar 2020 09:08:16 GMT"}], "update_date": "2020-03-20", "authors_parsed": [["Erichsen", "Jorgen F.", ""], ["Sj\u00f6man", "Heikki", ""], ["Steinert", "Martin", ""], ["Welo", "Torgeir", ""]]}, {"id": "1905.02025", "submitter": "Grigorios Kalliatakis", "authors": "Grigorios Kalliatakis, Shoaib Ehsan, Maria Fasli, Klaus McDonald-Maier", "title": "DisplaceNet: Recognising Displaced People from Images by Exploiting\n  Dominance Level", "comments": "To be published in CVPR Workshop on Computer Vision for Global\n  Challenges (CV4GC). arXiv admin note: substantial text overlap with\n  arXiv:1902.03817", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Every year millions of men, women and children are forced to leave their\nhomes and seek refuge from wars, human rights violations, persecution, and\nnatural disasters. The number of forcibly displaced people came at a record\nrate of 44,400 every day throughout 2017, raising the cumulative total to 68.5\nmillion at the years end, overtaken the total population of the United Kingdom.\nUp to 85% of the forcibly displaced find refuge in low- and middle-income\ncountries, calling for increased humanitarian assistance worldwide. To reduce\nthe amount of manual labour required for human-rights-related image analysis,\nwe introduce DisplaceNet, a novel model which infers potential displaced people\nfrom images by integrating the control level of the situation and conventional\nconvolutional neural network (CNN) classifier into one framework for image\nclassification. Experimental results show that DisplaceNet achieves up to 4%\ncoverage-the proportion of a data set for which a classifier is able to produce\na prediction-gain over the sole use of a CNN classifier. Our dataset, codes and\ntrained models will be available online at\nhttps://github.com/GKalliatakis/DisplaceNet.\n", "versions": [{"version": "v1", "created": "Fri, 3 May 2019 11:07:27 GMT"}], "update_date": "2019-05-07", "authors_parsed": [["Kalliatakis", "Grigorios", ""], ["Ehsan", "Shoaib", ""], ["Fasli", "Maria", ""], ["McDonald-Maier", "Klaus", ""]]}, {"id": "1905.02036", "submitter": "Sebastiano Vascon Mr", "authors": "Sebastiano Vascon, Sinem Aslan, Alessandro Torcinovich, Twan van\n  Laarhoven, Elena Marchiori and Marcello Pelillo", "title": "Unsupervised Domain Adaptation using Graph Transduction Games", "comments": "Oral IJCNN 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.GT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unsupervised domain adaptation (UDA) amounts to assigning class labels to the\nunlabeled instances of a dataset from a target domain, using labeled instances\nof a dataset from a related source domain. In this paper, we propose to cast\nthis problem in a game-theoretic setting as a non-cooperative game and\nintroduce a fully automatized iterative algorithm for UDA based on graph\ntransduction games (GTG). The main advantages of this approach are its\nprincipled foundation, guaranteed termination of the iterative algorithms to a\nNash equilibrium (which corresponds to a consistent labeling condition) and\nsoft labels quantifying the uncertainty of the label assignment process. We\nalso investigate the beneficial effect of using pseudo-labels from linear\nclassifiers to initialize the iterative process. The performance of the\nresulting methods is assessed on publicly available object recognition\nbenchmark datasets involving both shallow and deep features. Results of\nexperiments demonstrate the suitability of the proposed game-theoretic approach\nfor solving UDA tasks.\n", "versions": [{"version": "v1", "created": "Mon, 6 May 2019 13:34:04 GMT"}], "update_date": "2019-05-07", "authors_parsed": [["Vascon", "Sebastiano", ""], ["Aslan", "Sinem", ""], ["Torcinovich", "Alessandro", ""], ["van Laarhoven", "Twan", ""], ["Marchiori", "Elena", ""], ["Pelillo", "Marcello", ""]]}, {"id": "1905.02085", "submitter": "Xingyuan Zhang", "authors": "Xingyuan Zhang and Fuhai Zhang", "title": "Pixel-wise Regression: 3D Hand Pose Estimation via Spatial-form\n  Representation and Differentiable Decoder", "comments": "Update LaTeX version. Code coming soon", "journal-ref": null, "doi": "10.1109/TMM.2020.3047552", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  3D Hand pose estimation from a single depth image is an essential topic in\ncomputer vision and human-computer interaction. Although the rising of deep\nlearning method boosts the accuracy a lot, the problem is still hard to solve\ndue to the complex structure of the human hand. Existing methods with deep\nlearning either lose spatial information of hand structure or lack a direct\nsupervision of joint coordinates. In this paper, we propose a novel Pixel-wise\nRegression method, which use spatial-form representation (SFR) and\ndifferentiable decoder (DD) to solve the two problems. To use our method, we\nbuild a model, in which we design a particular SFR and its correlative DD which\ndivided the 3D joint coordinates into two parts, plane coordinates and depth\ncoordinates and use two modules named Plane Regression (PR) and Depth\nRegression (DR) to deal with them respectively. We conduct an ablation\nexperiment to show the method we proposed achieve better results than the\nformer methods. We also make an exploration on how different training\nstrategies influence the learned SFRs and results. The experiment on three\npublic datasets demonstrates that our model is comparable with the existing\nstate-of-the-art models and in one of them our model can reduce mean 3D joint\nerror by 25%.\n", "versions": [{"version": "v1", "created": "Mon, 6 May 2019 15:07:15 GMT"}, {"version": "v2", "created": "Mon, 27 May 2019 02:14:02 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["Zhang", "Xingyuan", ""], ["Zhang", "Fuhai", ""]]}, {"id": "1905.02102", "submitter": "Xiao Wang", "authors": "Xiao Wang, Ziliang Chen, Rui Yang, Bin Luo and Jin Tang", "title": "Improved Hard Example Mining by Discovering Attribute-based Hard Person\n  Identity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose Hard Person Identity Mining (HPIM) that attempts to\nrefine the hard example mining to improve the exploration efficacy in person\nre-identification. It is motivated by following observation: the more\nattributes some people share, the more difficult to separate their identities.\nBased on this observation, we develop HPIM via a transferred attribute\ndescriber, a deep multi-attribute classifier trained from the source noisy\nperson attribute datasets. We encode each image into the attribute\nprobabilistic description in the target person re-ID dataset. Afterwards in the\nattribute code space, we consider each person as a distribution to generate his\nview-specific attribute codes in different practical scenarios. Hence we\nestimate the person-specific statistical moments from zeroth to higher order,\nwhich are further used to calculate the central moment discrepancies between\npersons. Such discrepancy is a ground to choose hard identity to organize\nproper mini-batches, without concerning the person representation changing in\nmetric learning. It presents as a complementary tool of hard example mining,\nwhich helps to explore the global instead of the local hard example constraint\nin the mini-batch built by randomly sampled identities. Extensive experiments\non two person re-identification benchmarks validated the effectiveness of our\nproposed algorithm.\n", "versions": [{"version": "v1", "created": "Mon, 6 May 2019 15:38:36 GMT"}, {"version": "v2", "created": "Thu, 9 May 2019 06:43:06 GMT"}, {"version": "v3", "created": "Mon, 5 Aug 2019 23:23:10 GMT"}], "update_date": "2019-08-07", "authors_parsed": [["Wang", "Xiao", ""], ["Chen", "Ziliang", ""], ["Yang", "Rui", ""], ["Luo", "Bin", ""], ["Tang", "Jin", ""]]}, {"id": "1905.02106", "submitter": "Soumyabrata Dev", "authors": "Soumyabrata Dev, Murhaf Hossari, Matthew Nicholson, Killian McCabe,\n  Atul Nautiyal, Clare Conran, Jian Tang, Wei Xu, and Fran\\c{c}ois Piti\\'e", "title": "Localizing Adverts in Outdoor Scenes", "comments": "Published in 2019 IEEE International Conference on Multimedia & Expo\n  Workshops (ICMEW)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Online videos have witnessed an unprecedented growth over the last decade,\nowing to wide range of content creation. This provides the advertisement and\nmarketing agencies plethora of opportunities for targeted advertisements. Such\ntechniques involve replacing an existing advertisement in a video frame, with a\nnew advertisement. However, such post-processing of online videos is mostly\ndone manually by video editors. This is cumbersome and time-consuming. In this\npaper, we propose DeepAds -- a deep neural network, based on the simple\nencoder-decoder architecture, that can accurately localize the position of an\nadvert in a video frame. Our approach of localizing billboards in outdoor\nscenes using neural nets, is the first of its kind, and achieves the best\nperformance. We benchmark our proposed method with other semantic segmentation\nalgorithms, on a public dataset of outdoor scenes with manually annotated\nbillboard binary maps.\n", "versions": [{"version": "v1", "created": "Mon, 6 May 2019 15:44:31 GMT"}], "update_date": "2019-05-07", "authors_parsed": [["Dev", "Soumyabrata", ""], ["Hossari", "Murhaf", ""], ["Nicholson", "Matthew", ""], ["McCabe", "Killian", ""], ["Nautiyal", "Atul", ""], ["Conran", "Clare", ""], ["Tang", "Jian", ""], ["Xu", "Wei", ""], ["Piti\u00e9", "Fran\u00e7ois", ""]]}, {"id": "1905.02114", "submitter": "Lu Sheng", "authors": "Lu Sheng, Jianfei Cai, Tat-Jen Cham, Vladimir Pavlovic, King Ngi Ngan", "title": "Visibility Constrained Generative Model for Depth-based 3D Facial Pose\n  Tracking", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a generative framework that unifies depth-based 3D\nfacial pose tracking and face model adaptation on-the-fly, in the unconstrained\nscenarios with heavy occlusions and arbitrary facial expression variations.\nSpecifically, we introduce a statistical 3D morphable model that flexibly\ndescribes the distribution of points on the surface of the face model, with an\nefficient switchable online adaptation that gradually captures the identity of\nthe tracked subject and rapidly constructs a suitable face model when the\nsubject changes. Moreover, unlike prior art that employed ICP-based facial pose\nestimation, to improve robustness to occlusions, we propose a ray visibility\nconstraint that regularizes the pose based on the face model's visibility with\nrespect to the input point cloud. Ablation studies and experimental results on\nBiwi and ICT-3DHP datasets demonstrate that the proposed framework is effective\nand outperforms completing state-of-the-art depth-based methods.\n", "versions": [{"version": "v1", "created": "Mon, 6 May 2019 15:58:41 GMT"}], "update_date": "2019-05-07", "authors_parsed": [["Sheng", "Lu", ""], ["Cai", "Jianfei", ""], ["Cham", "Tat-Jen", ""], ["Pavlovic", "Vladimir", ""], ["Ngan", "King Ngi", ""]]}, {"id": "1905.02135", "submitter": "Junxi Feng", "authors": "Junxi Feng, Xiaohai He, Qizhi Teng, Chao Ren, Honggang Chen, Yang Li", "title": "Accurate and Fast reconstruction of Porous Media from Extremely Limited\n  Information Using Conditional Generative Adversarial Network", "comments": null, "journal-ref": "Phys. Rev. E 100, 033308 (2019)", "doi": "10.1103/PhysRevE.100.033308", "report-no": null, "categories": "eess.IV cs.CE cs.CV physics.data-an", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Porous media are ubiquitous in both nature and engineering applications, thus\ntheir modelling and understanding is of vital importance. In contrast to direct\nacquisition of three-dimensional (3D) images of such medium, obtaining its\nsub-region (s) like two-dimensional (2D) images or several small areas could be\nmuch feasible. Therefore, reconstructing whole images from the limited\ninformation is a primary technique in such cases. Specially, in practice the\ngiven data cannot generally be determined by users and may be incomplete or\npartially informed, thus making existing reconstruction methods inaccurate or\neven ineffective. To overcome this shortcoming, in this study we proposed a\ndeep learning-based framework for reconstructing full image from its much\nsmaller sub-area(s). Particularly, conditional generative adversarial network\n(CGAN) is utilized to learn the mapping between input (partial image) and\noutput (full image). To preserve the reconstruction accuracy, two simple but\neffective objective functions are proposed and then coupled with the other two\nfunctions to jointly constrain the training procedure. Due to the inherent\nessence of this ill-posed problem, a Gaussian noise is introduced for producing\nreconstruction diversity, thus allowing for providing multiple candidate\noutputs. Extensively tested on a variety of porous materials and demonstrated\nby both visual inspection and quantitative comparison, the method is shown to\nbe accurate, stable yet fast ($\\sim0.08s$ for a $128 \\times 128$ image\nreconstruction). We highlight that the proposed approach can be readily\nextended, such as incorporating any user-define conditional data and an\narbitrary number of object functions into reconstruction, and being coupled\nwith other reconstruction methods.\n", "versions": [{"version": "v1", "created": "Thu, 4 Apr 2019 09:08:28 GMT"}], "update_date": "2019-09-25", "authors_parsed": [["Feng", "Junxi", ""], ["He", "Xiaohai", ""], ["Teng", "Qizhi", ""], ["Ren", "Chao", ""], ["Chen", "Honggang", ""], ["Li", "Yang", ""]]}, {"id": "1905.02163", "submitter": "Olga Veksler", "authors": "Lena Gorelick and Olga Veksler", "title": "Simulating CRF with CNN for CNN", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Combining CNN with CRF for modeling dependencies between pixel labels is a\npopular research direction. This task is far from trivial, especially if\nend-to-end training is desired. In this paper, we propose a novel simple\napproach to CNN+CRF combination. In particular, we propose to simulate a CRF\nregularizer with a trainable module that has standard CNN architecture. We call\nthis module a CRF Simulator. We can automatically generate an unlimited amount\nof ground truth for training such CRF Simulator without any user interaction,\nprovided we have an efficient algorithm for optimization of the actual CRF\nregularizer. After our CRF Simulator is trained, it can be directly\nincorporated as part of any larger CNN architecture, enabling a seamless\nend-to-end training. In particular, the other modules can learn parameters that\nare more attuned to the performance of the CRF Simulator module. We demonstrate\nthe effectiveness of our approach on the task of salient object segmentation\nregularized with the standard binary CRF energy. In contrast to previous work\nwe do not need to develop and implement the complex mechanics of optimizing a\nspecific CRF as part of CNN. In fact, our approach can be easily extended to\nother CRF energies, including multi-label. To the best of our knowledge we are\nthe first to study the question of whether the output of CNNs can have\nregularization properties of CRFs.\n", "versions": [{"version": "v1", "created": "Mon, 6 May 2019 17:27:32 GMT"}], "update_date": "2019-05-07", "authors_parsed": [["Gorelick", "Lena", ""], ["Veksler", "Olga", ""]]}, {"id": "1905.02171", "submitter": "Kurt Degiorgio", "authors": "Kurt Degiorgio, Fabio Cuzzolin", "title": "Spatio-Temporal Action Localization in a Weakly Supervised Setting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Enabling computational systems with the ability to localize actions in\nvideo-based content has manifold applications. Traditionally, such a problem is\napproached in a fully-supervised setting where video-clips with complete\nframe-by-frame annotations around the actions of interest are provided for\ntraining. However, the data requirements needed to achieve adequate\ngeneralization in this setting is prohibitive. In this work, we circumvent this\nissue by casting the problem in a weakly supervised setting, i.e., by\nconsidering videos as labelled `sets' of unlabelled video segments. Firstly, we\napply unsupervised segmentation to take advantage of the elementary structure\nof each video. Subsequently, a convolutional neural network is used to extract\nRGB features from the resulting video segments. Finally, Multiple Instance\nLearning (MIL) is employed to predict labels at the video segment level, thus\ninherently performing spatio-temporal action detection. In contrast to previous\nwork, we make use of a different MIL formulation in which the label of each\nvideo segment is continuous rather then discrete, making the resulting\noptimization function tractable. Additionally, we utilize a set splitting\ntechnique for regularization. Experimental results considering multiple\nperformance indicators on the UCF-Sports data-set support the effectiveness of\nour approach.\n", "versions": [{"version": "v1", "created": "Mon, 6 May 2019 17:39:09 GMT"}], "update_date": "2019-05-07", "authors_parsed": [["Degiorgio", "Kurt", ""], ["Cuzzolin", "Fabio", ""]]}, {"id": "1905.02175", "submitter": "Dimitris Tsipras", "authors": "Andrew Ilyas, Shibani Santurkar, Dimitris Tsipras, Logan Engstrom,\n  Brandon Tran, Aleksander Madry", "title": "Adversarial Examples Are Not Bugs, They Are Features", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CR cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adversarial examples have attracted significant attention in machine\nlearning, but the reasons for their existence and pervasiveness remain unclear.\nWe demonstrate that adversarial examples can be directly attributed to the\npresence of non-robust features: features derived from patterns in the data\ndistribution that are highly predictive, yet brittle and incomprehensible to\nhumans. After capturing these features within a theoretical framework, we\nestablish their widespread existence in standard datasets. Finally, we present\na simple setting where we can rigorously tie the phenomena we observe in\npractice to a misalignment between the (human-specified) notion of robustness\nand the inherent geometry of the data.\n", "versions": [{"version": "v1", "created": "Mon, 6 May 2019 17:45:05 GMT"}, {"version": "v2", "created": "Tue, 7 May 2019 02:01:14 GMT"}, {"version": "v3", "created": "Wed, 19 Jun 2019 00:25:20 GMT"}, {"version": "v4", "created": "Mon, 12 Aug 2019 14:36:10 GMT"}], "update_date": "2019-08-13", "authors_parsed": [["Ilyas", "Andrew", ""], ["Santurkar", "Shibani", ""], ["Tsipras", "Dimitris", ""], ["Engstrom", "Logan", ""], ["Tran", "Brandon", ""], ["Madry", "Aleksander", ""]]}, {"id": "1905.02176", "submitter": "Jeff Calder", "authors": "Riley O'Neill, Pedro Angulo-Umana, Jeff Calder, Bo Hessburg, Peter J.\n  Olver, Chehrzad Shakiban, Katrina Yezzi-Woodley", "title": "Computation of Circular Area and Spherical Volume Invariants via\n  Boundary Integrals", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.CG cs.CV math.DG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show how to compute the circular area invariant of planar curves, and the\nspherical volume invariant of surfaces, in terms of line and surface integrals,\nrespectively. We use the Divergence Theorem to express the area and volume\nintegrals as line and surface integrals, respectively, against particular\nkernels; our results also extend to higher dimensional hypersurfaces. The\nresulting surface integrals are computable analytically on a triangulated mesh.\nThis gives a simple computational algorithm for computing the spherical volume\ninvariant for triangulated surfaces that does not involve discretizing the\nambient space. We discuss potential applications to feature detection on broken\nbone fragments of interest in anthropology.\n", "versions": [{"version": "v1", "created": "Mon, 6 May 2019 17:51:00 GMT"}], "update_date": "2019-05-07", "authors_parsed": [["O'Neill", "Riley", ""], ["Angulo-Umana", "Pedro", ""], ["Calder", "Jeff", ""], ["Hessburg", "Bo", ""], ["Olver", "Peter J.", ""], ["Shakiban", "Chehrzad", ""], ["Yezzi-Woodley", "Katrina", ""]]}, {"id": "1905.02185", "submitter": "Takuhiro Kaneko", "authors": "Takuhiro Kaneko, Tatsuya Harada", "title": "Label-Noise Robust Multi-Domain Image-to-Image Translation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-domain image-to-image translation is a problem where the goal is to\nlearn mappings among multiple domains. This problem is challenging in terms of\nscalability because it requires the learning of numerous mappings, the number\nof which increases proportional to the number of domains. However, generative\nadversarial networks (GANs) have emerged recently as a powerful framework for\nthis problem. In particular, label-conditional extensions (e.g., StarGAN) have\nbecome a promising solution owing to their ability to address this problem\nusing only a single unified model. Nonetheless, a limitation is that they rely\non the availability of large-scale clean-labeled data, which are often\nlaborious or impractical to collect in a real-world scenario. To overcome this\nlimitation, we propose a novel model called the label-noise robust\nimage-to-image translation model (RMIT) that can learn a clean label\nconditional generator even when noisy labeled data are only available. In\nparticular, we propose a novel loss called the virtual cycle consistency loss\nthat is able to regularize cyclic reconstruction independently of noisy labeled\ndata, as well as we introduce advanced techniques to boost the performance in\npractice. Our experimental results demonstrate that RMIT is useful for\nobtaining label-noise robustness in various settings including synthetic and\nreal-world noise.\n", "versions": [{"version": "v1", "created": "Mon, 6 May 2019 17:57:43 GMT"}], "update_date": "2019-05-07", "authors_parsed": [["Kaneko", "Takuhiro", ""], ["Harada", "Tatsuya", ""]]}, {"id": "1905.02188", "submitter": "Jiaqi Wang", "authors": "Jiaqi Wang, Kai Chen, Rui Xu, Ziwei Liu, Chen Change Loy, Dahua Lin", "title": "CARAFE: Content-Aware ReAssembly of FEatures", "comments": "ICCV 2019 Camera Ready (Oral)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Feature upsampling is a key operation in a number of modern convolutional\nnetwork architectures, e.g. feature pyramids. Its design is critical for dense\nprediction tasks such as object detection and semantic/instance segmentation.\nIn this work, we propose Content-Aware ReAssembly of FEatures (CARAFE), a\nuniversal, lightweight and highly effective operator to fulfill this goal.\nCARAFE has several appealing properties: (1) Large field of view. Unlike\nprevious works (e.g. bilinear interpolation) that only exploit sub-pixel\nneighborhood, CARAFE can aggregate contextual information within a large\nreceptive field. (2) Content-aware handling. Instead of using a fixed kernel\nfor all samples (e.g. deconvolution), CARAFE enables instance-specific\ncontent-aware handling, which generates adaptive kernels on-the-fly. (3)\nLightweight and fast to compute. CARAFE introduces little computational\noverhead and can be readily integrated into modern network architectures. We\nconduct comprehensive evaluations on standard benchmarks in object detection,\ninstance/semantic segmentation and inpainting. CARAFE shows consistent and\nsubstantial gains across all the tasks (1.2%, 1.3%, 1.8%, 1.1db respectively)\nwith negligible computational overhead. It has great potential to serve as a\nstrong building block for future research. It has great potential to serve as a\nstrong building block for future research. Code and models are available at\nhttps://github.com/open-mmlab/mmdetection.\n", "versions": [{"version": "v1", "created": "Mon, 6 May 2019 17:58:06 GMT"}, {"version": "v2", "created": "Tue, 7 May 2019 09:30:08 GMT"}, {"version": "v3", "created": "Tue, 29 Oct 2019 12:41:51 GMT"}], "update_date": "2019-10-30", "authors_parsed": [["Wang", "Jiaqi", ""], ["Chen", "Kai", ""], ["Xu", "Rui", ""], ["Liu", "Ziwei", ""], ["Loy", "Chen Change", ""], ["Lin", "Dahua", ""]]}, {"id": "1905.02196", "submitter": "Wenjie Hu", "authors": "Wenjie Hu, Jay Harshadbhai Patel, Zoe-Alanah Robert, Paul Novosad,\n  Samuel Asher, Zhongyi Tang, Marshall Burke, David Lobell, Stefano Ermon", "title": "Mapping Missing Population in Rural India: A Deep Learning Approach with\n  Satellite Imagery", "comments": "7 pages", "journal-ref": "AAAI/ACM Conference on AI, Ethics, and Society (AIES '19), January\n  27-28, 2019, Honolulu, HI, USA", "doi": "10.1145/3306618.3314263", "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Millions of people worldwide are absent from their country's census.\nAccurate, current, and granular population metrics are critical to improving\ngovernment allocation of resources, to measuring disease control, to responding\nto natural disasters, and to studying any aspect of human life in these\ncommunities. Satellite imagery can provide sufficient information to build a\npopulation map without the cost and time of a government census. We present two\nConvolutional Neural Network (CNN) architectures which efficiently and\neffectively combine satellite imagery inputs from multiple sources to\naccurately predict the population density of a region. In this paper, we use\nsatellite imagery from rural villages in India and population labels from the\n2011 SECC census. Our best model achieves better performance than previous\npapers as well as LandScan, a community standard for global population\ndistribution.\n", "versions": [{"version": "v1", "created": "Sat, 4 May 2019 18:33:22 GMT"}], "update_date": "2019-05-08", "authors_parsed": [["Hu", "Wenjie", ""], ["Patel", "Jay Harshadbhai", ""], ["Robert", "Zoe-Alanah", ""], ["Novosad", "Paul", ""], ["Asher", "Samuel", ""], ["Tang", "Zhongyi", ""], ["Burke", "Marshall", ""], ["Lobell", "David", ""], ["Ermon", "Stefano", ""]]}, {"id": "1905.02197", "submitter": "Mohammadreza Khajeh Hosseini", "authors": "Mohammadreza Khajeh-Hosseini and Alireza Talebpour", "title": "Back to the Future: Predicting Traffic Shockwave Formation and\n  Propagation Using a Convolutional Encoder-Decoder Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This study proposes a deep learning methodology to predict the propagation of\ntraffic shockwaves. The input to the deep neural network is time-space diagram\nof the study segment, and the output of the network is the predicted (future)\npropagation of the shockwave on the study segment in the form of time-space\ndiagram. The main feature of the proposed methodology is the ability to extract\nthe features embedded in the time-space diagram to predict the propagation of\ntraffic shockwaves.\n", "versions": [{"version": "v1", "created": "Sat, 4 May 2019 18:33:55 GMT"}], "update_date": "2019-05-08", "authors_parsed": [["Khajeh-Hosseini", "Mohammadreza", ""], ["Talebpour", "Alireza", ""]]}, {"id": "1905.02200", "submitter": "Song Gao", "authors": "Yuhao Kang, Song Gao, Robert E. Roth", "title": "Transferring Multiscale Map Styles Using Generative Adversarial Networks", "comments": "12 pages, 17 figure", "journal-ref": "International Journal of Cartography, 2019", "doi": "10.1080/23729333.2019.1615729", "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The advancement of the Artificial Intelligence (AI) technologies makes it\npossible to learn stylistic design criteria from existing maps or other visual\nart and transfer these styles to make new digital maps. In this paper, we\npropose a novel framework using AI for map style transfer applicable across\nmultiple map scales. Specifically, we identify and transfer the stylistic\nelements from a target group of visual examples, including Google Maps,\nOpenStreetMap, and artistic paintings, to unstylized GIS vector data through\ntwo generative adversarial network (GAN) models. We then train a binary\nclassifier based on a deep convolutional neural network to evaluate whether the\ntransfer styled map images preserve the original map design characteristics.\nOur experiment results show that GANs have a great potential for multiscale map\nstyle transferring, but many challenges remain requiring future research.\n", "versions": [{"version": "v1", "created": "Mon, 6 May 2019 04:52:42 GMT"}, {"version": "v2", "created": "Sat, 18 May 2019 05:47:15 GMT"}], "update_date": "2019-05-21", "authors_parsed": [["Kang", "Yuhao", ""], ["Gao", "Song", ""], ["Roth", "Robert E.", ""]]}, {"id": "1905.02229", "submitter": "Mikhail Mozerov Dr", "authors": "Mikhail G. Mozerov, and Fei Yang, and Joost van de Weijer", "title": "Sparse data interpolation using the geodesic distance affinity space", "comments": null, "journal-ref": null, "doi": "10.1109/LSP.2019.2914004", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we adapt the geodesic distance-based recursive filter to the\nsparse data interpolation problem. The proposed technique is general and can be\neasily applied to any kind of sparse data. We demonstrate the superiority over\nother interpolation techniques in three experiments for qualitative and\nquantitative evaluation.\n  In addition, we compare our method with the popular interpolation algorithm\npresented in the EpicFlow optical flow paper that is intuitively motivated by a\nsimilar geodesic distance principle. The comparison shows that our algorithm is\nmore accurate and considerably faster than the EpicFlow interpolation\ntechnique.\n", "versions": [{"version": "v1", "created": "Mon, 6 May 2019 18:24:11 GMT"}], "update_date": "2019-05-22", "authors_parsed": [["Mozerov", "Mikhail G.", ""], ["Yang", "Fei", ""], ["van de Weijer", "Joost", ""]]}, {"id": "1905.02231", "submitter": "Syed Ammar Abbas", "authors": "Ammar Abbas, Andrew Zisserman", "title": "A Geometric Approach to Obtain a Bird's Eye View from an Image", "comments": "ICCV Workshop \"Geometry Meets Deep Learning\" 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The objective of this paper is to rectify any monocular image by computing a\nhomography matrix that transforms it to a bird's eye (overhead) view.\n  We make the following contributions: (i) we show that the homography matrix\ncan be parameterised with only four parameters that specify the horizon line\nand the vertical vanishing point, or only two if the field of view or focal\nlength is known; (ii) We introduce a novel representation for the geometry of a\nline or point (which can be at infinity) that is suitable for regression with a\nconvolutional neural network (CNN); (iii) We introduce a large synthetic image\ndataset with ground truth for the orthogonal vanishing points, that can be used\nfor training a CNN to predict these geometric entities; and finally (iv) We\nachieve state-of-the-art results on horizon detection, with 74.52% AUC on the\nHorizon Lines in the Wild dataset. Our method is fast and robust, and can be\nused to remove perspective distortion from videos in real time.\n", "versions": [{"version": "v1", "created": "Mon, 6 May 2019 18:26:49 GMT"}, {"version": "v2", "created": "Sun, 27 Sep 2020 19:15:16 GMT"}], "update_date": "2020-09-29", "authors_parsed": [["Abbas", "Ammar", ""], ["Zisserman", "Andrew", ""]]}, {"id": "1905.02234", "submitter": "Shreyansh Gandhi", "authors": "Shreyansh Gandhi, Samrat Kokkula, Abon Chaudhuri, Alessandro Magnani,\n  Theban Stanley, Behzad Ahmadi, Venkatesh Kandaswamy, Omer Ovenc, Shie Mannor", "title": "Image Matters: Scalable Detection of Offensive and Non-Compliant Content\n  / Logo in Product Images", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In e-commerce, product content, especially product images have a significant\ninfluence on a customer's journey from product discovery to evaluation and\nfinally, purchase decision. Since many e-commerce retailers sell items from\nother third-party marketplace sellers besides their own, the content published\nby both internal and external content creators needs to be monitored and\nenriched, wherever possible. Despite guidelines and warnings, product listings\nthat contain offensive and non-compliant images continue to enter catalogs.\nOffensive and non-compliant content can include a wide range of objects, logos,\nand banners conveying violent, sexually explicit, racist, or promotional\nmessages. Such images can severely damage the customer experience, lead to\nlegal issues, and erode the company brand. In this paper, we present a computer\nvision driven offensive and non-compliant image detection system for extremely\nlarge image datasets. This paper delves into the unique challenges of applying\ndeep learning to real-world product image data from retail world. We\ndemonstrate how we resolve a number of technical challenges such as lack of\ntraining data, severe class imbalance, fine-grained class definitions etc.\nusing a number of practical yet unique technical strategies. Our system\ncombines state-of-the-art image classification and object detection techniques\nwith budgeted crowdsourcing to develop a solution customized for a massive,\ndiverse, and constantly evolving product catalog.\n", "versions": [{"version": "v1", "created": "Mon, 6 May 2019 18:35:28 GMT"}, {"version": "v2", "created": "Fri, 2 Aug 2019 07:38:26 GMT"}], "update_date": "2019-08-05", "authors_parsed": [["Gandhi", "Shreyansh", ""], ["Kokkula", "Samrat", ""], ["Chaudhuri", "Abon", ""], ["Magnani", "Alessandro", ""], ["Stanley", "Theban", ""], ["Ahmadi", "Behzad", ""], ["Kandaswamy", "Venkatesh", ""], ["Ovenc", "Omer", ""], ["Mannor", "Shie", ""]]}, {"id": "1905.02244", "submitter": "Mark Sandler", "authors": "Andrew Howard, Mark Sandler, Grace Chu, Liang-Chieh Chen, Bo Chen,\n  Mingxing Tan, Weijun Wang, Yukun Zhu, Ruoming Pang, Vijay Vasudevan, Quoc V.\n  Le, Hartwig Adam", "title": "Searching for MobileNetV3", "comments": "ICCV 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the next generation of MobileNets based on a combination of\ncomplementary search techniques as well as a novel architecture design.\nMobileNetV3 is tuned to mobile phone CPUs through a combination of\nhardware-aware network architecture search (NAS) complemented by the NetAdapt\nalgorithm and then subsequently improved through novel architecture advances.\nThis paper starts the exploration of how automated search algorithms and\nnetwork design can work together to harness complementary approaches improving\nthe overall state of the art. Through this process we create two new MobileNet\nmodels for release: MobileNetV3-Large and MobileNetV3-Small which are targeted\nfor high and low resource use cases. These models are then adapted and applied\nto the tasks of object detection and semantic segmentation. For the task of\nsemantic segmentation (or any dense pixel prediction), we propose a new\nefficient segmentation decoder Lite Reduced Atrous Spatial Pyramid Pooling\n(LR-ASPP). We achieve new state of the art results for mobile classification,\ndetection and segmentation. MobileNetV3-Large is 3.2\\% more accurate on\nImageNet classification while reducing latency by 15\\% compared to MobileNetV2.\nMobileNetV3-Small is 4.6\\% more accurate while reducing latency by 5\\% compared\nto MobileNetV2. MobileNetV3-Large detection is 25\\% faster at roughly the same\naccuracy as MobileNetV2 on COCO detection. MobileNetV3-Large LR-ASPP is 30\\%\nfaster than MobileNetV2 R-ASPP at similar accuracy for Cityscapes segmentation.\n", "versions": [{"version": "v1", "created": "Mon, 6 May 2019 19:38:31 GMT"}, {"version": "v2", "created": "Tue, 14 May 2019 16:27:20 GMT"}, {"version": "v3", "created": "Wed, 12 Jun 2019 20:26:06 GMT"}, {"version": "v4", "created": "Sat, 24 Aug 2019 08:17:43 GMT"}, {"version": "v5", "created": "Wed, 20 Nov 2019 17:26:40 GMT"}], "update_date": "2019-11-21", "authors_parsed": [["Howard", "Andrew", ""], ["Sandler", "Mark", ""], ["Chu", "Grace", ""], ["Chen", "Liang-Chieh", ""], ["Chen", "Bo", ""], ["Tan", "Mingxing", ""], ["Wang", "Weijun", ""], ["Zhu", "Yukun", ""], ["Pang", "Ruoming", ""], ["Vasudevan", "Vijay", ""], ["Le", "Quoc V.", ""], ["Adam", "Hartwig", ""]]}, {"id": "1905.02249", "submitter": "David Berthelot", "authors": "David Berthelot, Nicholas Carlini, Ian Goodfellow, Nicolas Papernot,\n  Avital Oliver, Colin Raffel", "title": "MixMatch: A Holistic Approach to Semi-Supervised Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semi-supervised learning has proven to be a powerful paradigm for leveraging\nunlabeled data to mitigate the reliance on large labeled datasets. In this\nwork, we unify the current dominant approaches for semi-supervised learning to\nproduce a new algorithm, MixMatch, that works by guessing low-entropy labels\nfor data-augmented unlabeled examples and mixing labeled and unlabeled data\nusing MixUp. We show that MixMatch obtains state-of-the-art results by a large\nmargin across many datasets and labeled data amounts. For example, on CIFAR-10\nwith 250 labels, we reduce error rate by a factor of 4 (from 38% to 11%) and by\na factor of 2 on STL-10. We also demonstrate how MixMatch can help achieve a\ndramatically better accuracy-privacy trade-off for differential privacy.\nFinally, we perform an ablation study to tease apart which components of\nMixMatch are most important for its success.\n", "versions": [{"version": "v1", "created": "Mon, 6 May 2019 19:56:03 GMT"}, {"version": "v2", "created": "Wed, 23 Oct 2019 18:47:34 GMT"}], "update_date": "2019-10-25", "authors_parsed": [["Berthelot", "David", ""], ["Carlini", "Nicholas", ""], ["Goodfellow", "Ian", ""], ["Papernot", "Nicolas", ""], ["Oliver", "Avital", ""], ["Raffel", "Colin", ""]]}, {"id": "1905.02259", "submitter": "Scott McCloskey", "authors": "Michael Albright and Scott McCloskey", "title": "Source Generator Attribution via Inversion", "comments": "Updated with new experiments", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With advances in Generative Adversarial Networks (GANs) leading to\ndramatically-improved synthetic images and video, there is an increased need\nfor algorithms which extend traditional forensics to this new category of\nimagery. While GANs have been shown to be helpful in a number of computer\nvision applications, there are other problematic uses such as `deep fakes'\nwhich necessitate such forensics. Source camera attribution algorithms using\nvarious cues have addressed this need for imagery captured by a camera, but\nthere are fewer options for synthetic imagery. We address the problem of\nattributing a synthetic image to a specific generator in a white box setting,\nby inverting the process of generation. This enables us to simultaneously\ndetermine whether the generator produced the image and recover an input which\nproduces a close match to the synthetic image.\n", "versions": [{"version": "v1", "created": "Mon, 6 May 2019 20:47:28 GMT"}, {"version": "v2", "created": "Thu, 13 Jun 2019 19:01:37 GMT"}], "update_date": "2019-06-17", "authors_parsed": [["Albright", "Michael", ""], ["McCloskey", "Scott", ""]]}, {"id": "1905.02283", "submitter": "Li Yao", "authors": "Tobi Olatunji, Li Yao, Ben Covington, Alexander Rhodes, Anthony Upton", "title": "Caveats in Generating Medical Imaging Labels from Radiology Reports", "comments": "Accepted workshop contribution for Medical Imaging with Deep Learning\n  (MIDL), 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Acquiring high-quality annotations in medical imaging is usually a costly\nprocess. Automatic label extraction with natural language processing (NLP) has\nemerged as a promising workaround to bypass the need of expert annotation.\nDespite the convenience, the limitation of such an approximation has not been\ncarefully examined and is not well understood. With a challenging set of 1,000\nchest X-ray studies and their corresponding radiology reports, we show that\nthere exists a surprisingly large discrepancy between what radiologists\nvisually perceive and what they clinically report. Furthermore, with inherently\nflawed report as ground truth, the state-of-the-art medical NLP fails to\nproduce high-fidelity labels.\n", "versions": [{"version": "v1", "created": "Mon, 6 May 2019 22:38:18 GMT"}], "update_date": "2019-05-08", "authors_parsed": [["Olatunji", "Tobi", ""], ["Yao", "Li", ""], ["Covington", "Ben", ""], ["Rhodes", "Alexander", ""], ["Upton", "Anthony", ""]]}, {"id": "1905.02285", "submitter": "Niels Ole Salscheider", "authors": "Niels Ole Salscheider", "title": "Simultaneous Object Detection and Semantic Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Both object detection in and semantic segmentation of camera images are\nimportant tasks for automated vehicles. Object detection is necessary so that\nthe planning and behavior modules can reason about other road users. Semantic\nsegmentation provides for example free space information and information about\nstatic and dynamic parts of the environment. There has been a lot of research\nto solve both tasks using Convolutional Neural Networks. These approaches give\ngood results but are computationally demanding. In practice, a compromise has\nto be found between detection performance, detection quality and the number of\ntasks. Otherwise it is not possible to meet the real-time requirements of\nautomated vehicles. In this work, we propose a neural network architecture to\nsolve both tasks simultaneously. This architecture was designed to run with\naround 10 Hz on 1 MP images on current hardware. Our approach achieves a mean\nIoU of 61.2% for the semantic segmentation task on the challenging Cityscapes\nbenchmark. It also achieves an average precision of 69.3% for cars and 67.7% on\nthe moderate difficulty level of the KITTI benchmark.\n", "versions": [{"version": "v1", "created": "Mon, 6 May 2019 22:45:22 GMT"}, {"version": "v2", "created": "Thu, 13 Feb 2020 13:47:18 GMT"}], "update_date": "2020-02-14", "authors_parsed": [["Salscheider", "Niels Ole", ""]]}, {"id": "1905.02292", "submitter": "Jimuyang Zhang", "authors": "Jimuyang Zhang, Sanping Zhou, Jinjun Wang, Dong Huang", "title": "Frame-wise Motion and Appearance for Real-time Multiple Object Tracking", "comments": "13 pages, 4 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The main challenge of Multiple Object Tracking (MOT) is the efficiency in\nassociating indefinite number of objects between video frames. Standard motion\nestimators used in tracking, e.g., Long Short Term Memory (LSTM), only deal\nwith single object, while Re-IDentification (Re-ID) based approaches\nexhaustively compare object appearances. Both approaches are computationally\ncostly when they are scaled to a large number of objects, making it very\ndifficult for real-time MOT. To address these problems, we propose a highly\nefficient Deep Neural Network (DNN) that simultaneously models association\namong indefinite number of objects. The inference computation of the DNN does\nnot increase with the number of objects. Our approach, Frame-wise Motion and\nAppearance (FMA), computes the Frame-wise Motion Fields (FMF) between two\nframes, which leads to very fast and reliable matching among a large number of\nobject bounding boxes. As auxiliary information is used to fix uncertain\nmatches, Frame-wise Appearance Features (FAF) are learned in parallel with\nFMFs. Extensive experiments on the MOT17 benchmark show that our method\nachieved real-time MOT with competitive results as the state-of-the-art\napproaches.\n", "versions": [{"version": "v1", "created": "Mon, 6 May 2019 23:37:05 GMT"}], "update_date": "2019-05-08", "authors_parsed": [["Zhang", "Jimuyang", ""], ["Zhou", "Sanping", ""], ["Wang", "Jinjun", ""], ["Huang", "Dong", ""]]}, {"id": "1905.02296", "submitter": "Leonardo Teixeira", "authors": "Leonardo Teixeira, Brian Jalaian and Bruno Ribeiro", "title": "Are Graph Neural Networks Miscalibrated?", "comments": "Presented at the ICML 2019 Workshop on Learning and Reasoning with\n  Graph-Structured Data", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph Neural Networks (GNNs) have proven to be successful in many\nclassification tasks, outperforming previous state-of-the-art methods in terms\nof accuracy. However, accuracy alone is not enough for high-stakes decision\nmaking. Decision makers want to know the likelihood that a specific GNN\nprediction is correct. For this purpose, obtaining calibrated models is\nessential. In this work, we perform an empirical evaluation of the calibration\nof state-of-the-art GNNs on multiple datasets. Our experiments show that GNNs\ncan be calibrated in some datasets but also badly miscalibrated in others, and\nthat state-of-the-art calibration methods are helpful but do not fix the\nproblem.\n", "versions": [{"version": "v1", "created": "Tue, 7 May 2019 00:05:31 GMT"}, {"version": "v2", "created": "Mon, 3 Jun 2019 14:10:50 GMT"}], "update_date": "2019-06-04", "authors_parsed": [["Teixeira", "Leonardo", ""], ["Jalaian", "Brian", ""], ["Ribeiro", "Bruno", ""]]}, {"id": "1905.02319", "submitter": "Muzammil Behzad", "authors": "Muzammil Behzad, Nhat Vo, Xiaobai Li, Guoying Zhao", "title": "Automatic 4D Facial Expression Recognition via Collaborative\n  Cross-domain Dynamic Image Network", "comments": "Published in the 30th British Machine Vision Conference (BMVC) 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a novel 4D Facial Expression Recognition (FER) method\nusing Collaborative Cross-domain Dynamic Image Network (CCDN). Given a 4D data\nof face scans, we first compute its geometrical images, and then combine their\ncorrelated information in the proposed cross-domain image representations. The\nacquired set is then used to generate cross-domain dynamic images (CDI) via\nrank pooling that encapsulates facial deformations over time in terms of a\nsingle image. For the training phase, these CDIs are fed into an end-to-end\ndeep learning model, and the resultant predictions collaborate over multi-views\nfor performance gain in expression classification. Furthermore, we propose a 4D\naugmentation scheme that not only expands the training data scale but also\nintroduces significant facial muscle movement patterns to improve the FER\nperformance. Results from extensive experiments on the commonly used BU-4DFE\ndataset under widely adopted settings show that our proposed method outperforms\nthe state-of-the-art 4D FER methods by achieving an accuracy of 96.5%\nindicating its effectiveness.\n", "versions": [{"version": "v1", "created": "Tue, 7 May 2019 01:44:26 GMT"}, {"version": "v2", "created": "Fri, 7 Feb 2020 09:32:31 GMT"}], "update_date": "2020-02-10", "authors_parsed": [["Behzad", "Muzammil", ""], ["Vo", "Nhat", ""], ["Li", "Xiaobai", ""], ["Zhao", "Guoying", ""]]}, {"id": "1905.02320", "submitter": "Songyao Jiang", "authors": "Songyao Jiang, Hongfu Liu, Yue Wu, Yun Fu", "title": "Spatially Constrained Generative Adversarial Networks for Conditional\n  Image Generation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image generation has raised tremendous attention in both academic and\nindustrial areas, especially for the conditional and target-oriented image\ngeneration, such as criminal portrait and fashion design. Although the current\nstudies have achieved preliminary results along this direction, they always\nfocus on class labels as the condition where spatial contents are randomly\ngenerated from latent vectors. Edge details are usually blurred since spatial\ninformation is difficult to preserve. In light of this, we propose a novel\nSpatially Constrained Generative Adversarial Network (SCGAN), which decouples\nthe spatial constraints from the latent vector and makes these constraints\nfeasible as additional controllable signals. To enhance the spatial\ncontrollability, a generator network is specially designed to take a semantic\nsegmentation, a latent vector and an attribute-level label as inputs step by\nstep. Besides, a segmentor network is constructed to impose spatial constraints\non the generator. Experimentally, we provide both visual and quantitative\nresults on CelebA and DeepFashion datasets, and demonstrate that the proposed\nSCGAN is very effective in controlling the spatial contents as well as\ngenerating high-quality images.\n", "versions": [{"version": "v1", "created": "Tue, 7 May 2019 02:00:03 GMT"}], "update_date": "2019-05-08", "authors_parsed": [["Jiang", "Songyao", ""], ["Liu", "Hongfu", ""], ["Wu", "Yue", ""], ["Fu", "Yun", ""]]}, {"id": "1905.02327", "submitter": "Xinxun Xu", "authors": "Xinxun Xu, Hao Wang, Leida Li and Cheng Deng", "title": "Semantic Adversarial Network for Zero-Shot Sketch-Based Image Retrieval", "comments": "There is a big problem with the paper and I hope it can be retracted", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Zero-shot sketch-based image retrieval (ZS-SBIR) is a specific cross-modal\nretrieval task for retrieving natural images with free-hand sketches under\nzero-shot scenario. Previous works mostly focus on modeling the correspondence\nbetween images and sketches or synthesizing image features with sketch\nfeatures. However, both of them ignore the large intra-class variance of\nsketches, thus resulting in unsatisfactory retrieval performance. In this\npaper, we propose a novel end-to-end semantic adversarial approach for ZS-SBIR.\nSpecifically, we devise a semantic adversarial module to maximize the\nconsistency between learned semantic features and category-level word vectors.\nMoreover, to preserve the discriminability of synthesized features within each\ntraining category, a triplet loss is employed for the generative module.\nAdditionally, the proposed model is trained in an end-to-end strategy to\nexploit better semantic features suitable for ZS-SBIR. Extensive experiments\nconducted on two large-scale popular datasets demonstrate that our proposed\napproach remarkably outperforms state-of-the-art approaches by more than 12\\%\non Sketchy dataset and about 3\\% on TU-Berlin dataset in the retrieval.\n", "versions": [{"version": "v1", "created": "Tue, 7 May 2019 02:20:42 GMT"}, {"version": "v2", "created": "Fri, 18 Oct 2019 02:49:59 GMT"}], "update_date": "2019-10-21", "authors_parsed": [["Xu", "Xinxun", ""], ["Wang", "Hao", ""], ["Li", "Leida", ""], ["Deng", "Cheng", ""]]}, {"id": "1905.02343", "submitter": "Saghir Ahmed Saghir Alfasly", "authors": "Saghir Ahmed Saghir Alfasly, Yongjian Hu, Tiancai Liang, Xiaofeng Jin,\n  Qingli Zhao, Beibei Liu", "title": "Variational Representation Learning for Vehicle Re-Identification", "comments": "5 pages, 1 figure, Accepted at ICIP 2019", "journal-ref": "ICIP (2019), pp. 3118-3122", "doi": "10.1109/ICIP.2019.8803366", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Vehicle Re-identification is attracting more and more attention in recent\nyears. One of the most challenging problems is to learn an efficient\nrepresentation for a vehicle from its multi-viewpoint images. Existing methods\ntend to derive features of dimensions ranging from thousands to tens of\nthousands. In this work we proposed a deep learning based framework that can\nlead to an efficient representation of vehicles. While the dimension of the\nlearned features can be as low as 256, experiments on different datasets show\nthat the Top-1 and Top-5 retrieval accuracies exceed multiple state-of-the-art\nmethods. The key to our framework is two-fold. Firstly, variational feature\nlearning is employed to generate variational features which are more\ndiscriminating. Secondly, long short-term memory (LSTM) is used to learn the\nrelationship among different viewpoints of a vehicle. The LSTM also plays as an\nencoder to downsize the features.\n", "versions": [{"version": "v1", "created": "Tue, 7 May 2019 03:42:31 GMT"}], "update_date": "2019-09-11", "authors_parsed": [["Alfasly", "Saghir Ahmed Saghir", ""], ["Hu", "Yongjian", ""], ["Liang", "Tiancai", ""], ["Jin", "Xiaofeng", ""], ["Zhao", "Qingli", ""], ["Liu", "Beibei", ""]]}, {"id": "1905.02378", "submitter": "Tejas Sudharshan Mathai", "authors": "Jiahong Ouyang, Tejas Sudharshan Mathai, Kira Lathrop, and John\n  Galeotti", "title": "Accurate Tissue Interface Segmentation via Adversarial Pre-Segmentation\n  of Anterior Segment OCT Images", "comments": "First two authors contributed equally. Biomedical Optics Express\n  journal submission. 27 pages, 15 figures. Submitted to the journal on May 6th\n  2019 at 11:38pm", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Optical Coherence Tomography (OCT) is an imaging modality that has been\nwidely adopted for visualizing corneal, retinal and limbal tissue structure\nwith micron resolution. It can be used to diagnose pathological conditions of\nthe eye, and for developing pre-operative surgical plans. In contrast to the\nposterior retina, imaging the anterior tissue structures, such as the limbus\nand cornea, results in B-scans that exhibit increased speckle noise patterns\nand imaging artifacts. These artifacts, such as shadowing and specularity, pose\na challenge during the analysis of the acquired volumes as they substantially\nobfuscate the location of tissue interfaces. To deal with the artifacts and\nspeckle noise patterns and accurately segment the shallowest tissue interface,\nwe propose a cascaded neural network framework, which comprises of a\nconditional Generative Adversarial Network (cGAN) and a Tissue Interface\nSegmentation Network (TISN). The cGAN pre-segments OCT B-scans by removing\nundesired specular artifacts and speckle noise patterns just above the\nshallowest tissue interface, and the TISN combines the original OCT image with\nthe pre-segmentation to segment the shallowest interface. We show the\napplicability of the cascaded framework to corneal datasets, demonstrate that\nit precisely segments the shallowest corneal interface, and also show its\ngeneralization capacity to limbal datasets. We also propose a hybrid framework,\nwherein the cGAN pre-segmentation is passed to a traditional image\nanalysis-based segmentation algorithm, and describe the improved segmentation\nperformance. To the best of our knowledge, this is the first approach to remove\nsevere specular artifacts and speckle noise patterns (prior to the shallowest\ninterface) that affects the interpretation of anterior segment OCT datasets,\nthereby resulting in the accurate segmentation of the shallowest tissue\ninterface.\n", "versions": [{"version": "v1", "created": "Tue, 7 May 2019 06:44:56 GMT"}], "update_date": "2019-05-08", "authors_parsed": [["Ouyang", "Jiahong", ""], ["Mathai", "Tejas Sudharshan", ""], ["Lathrop", "Kira", ""], ["Galeotti", "John", ""]]}, {"id": "1905.02419", "submitter": "Zitong Yu", "authors": "Zitong Yu, Xiaobai Li, Guoying Zhao", "title": "Remote Photoplethysmograph Signal Measurement from Facial Videos Using\n  Spatio-Temporal Networks", "comments": "Accepted by BMVC2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent studies demonstrated that the average heart rate (HR) can be measured\nfrom facial videos based on non-contact remote photoplethysmography (rPPG).\nHowever for many medical applications (e.g., atrial fibrillation (AF)\ndetection) knowing only the average HR is not sufficient, and measuring precise\nrPPG signals from face for heart rate variability (HRV) analysis is needed.\nHere we propose an rPPG measurement method, which is the first work to use deep\nspatio-temporal networks for reconstructing precise rPPG signals from raw\nfacial videos. With the constraint of trend-consistency with ground truth pulse\ncurves, our method is able to recover rPPG signals with accurate pulse peaks.\nComprehensive experiments are conducted on two benchmark datasets, and results\ndemonstrate that our method can achieve superior performance on both HR and HRV\nlevels comparing to the state-of-the-art methods. We also achieve promising\nresults of using reconstructed rPPG signals for AF detection and emotion\nrecognition.\n", "versions": [{"version": "v1", "created": "Tue, 7 May 2019 09:02:37 GMT"}, {"version": "v2", "created": "Wed, 31 Jul 2019 13:50:38 GMT"}], "update_date": "2019-08-01", "authors_parsed": [["Yu", "Zitong", ""], ["Li", "Xiaobai", ""], ["Zhao", "Guoying", ""]]}, {"id": "1905.02422", "submitter": "Chihye Han", "authors": "Chihye Han, Wonjun Yoon, Gihyun Kwon, Seungkyu Nam, Daeshik Kim", "title": "Representation of White- and Black-Box Adversarial Examples in Deep\n  Neural Networks and Humans: A Functional Magnetic Resonance Imaging Study", "comments": "Copyright 2019 IEEE. Personal use of this material is permitted.\n  Permission from IEEE must be obtained for all other uses, in any current or\n  future media, including reprinting/republishing this material for advertising\n  or promotional purposes, creating new collective works, for resale or\n  redistribution to servers or lists, or reuse of any copyrighted component of\n  this work in other works", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The recent success of brain-inspired deep neural networks (DNNs) in solving\ncomplex, high-level visual tasks has led to rising expectations for their\npotential to match the human visual system. However, DNNs exhibit\nidiosyncrasies that suggest their visual representation and processing might be\nsubstantially different from human vision. One limitation of DNNs is that they\nare vulnerable to adversarial examples, input images on which subtle, carefully\ndesigned noises are added to fool a machine classifier. The robustness of the\nhuman visual system against adversarial examples is potentially of great\nimportance as it could uncover a key mechanistic feature that machine vision is\nyet to incorporate. In this study, we compare the visual representations of\nwhite- and black-box adversarial examples in DNNs and humans by leveraging\nfunctional magnetic resonance imaging (fMRI). We find a small but significant\ndifference in representation patterns for different (i.e. white- versus black-\nbox) types of adversarial examples for both humans and DNNs. However, human\nperformance on categorical judgment is not degraded by noise regardless of the\ntype unlike DNN. These results suggest that adversarial examples may be\ndifferentially represented in the human visual system, but unable to affect the\nperceptual experience.\n", "versions": [{"version": "v1", "created": "Tue, 7 May 2019 09:10:48 GMT"}], "update_date": "2019-05-08", "authors_parsed": [["Han", "Chihye", ""], ["Yoon", "Wonjun", ""], ["Kwon", "Gihyun", ""], ["Nam", "Seungkyu", ""], ["Kim", "Daeshik", ""]]}, {"id": "1905.02423", "submitter": "Yu Wang", "authors": "Yu Wang, Quan Zhou, Jia Liu, Jian Xiong, Guangwei Gao, Xiaofu Wu,\n  Longin Jan Latecki", "title": "LEDNet: A Lightweight Encoder-Decoder Network for Real-Time Semantic\n  Segmentation", "comments": "5 pages,3 figures,3 tables,accepted in IEEE ICIP 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The extensive computational burden limits the usage of CNNs in mobile devices\nfor dense estimation tasks. In this paper, we present a lightweight network to\naddress this problem,namely LEDNet, which employs an asymmetric encoder-decoder\narchitecture for the task of real-time semantic segmentation.More specifically,\nthe encoder adopts a ResNet as backbone network, where two new operations,\nchannel split and shuffle, are utilized in each residual block to greatly\nreduce computation cost while maintaining higher segmentation accuracy. On the\nother hand, an attention pyramid network (APN) is employed in the decoder to\nfurther lighten the entire network complexity. Our model has less than 1M\nparameters,and is able to run at over 71 FPS in a single GTX 1080Ti GPU. The\ncomprehensive experiments demonstrate that our approach achieves\nstate-of-the-art results in terms of speed and accuracy trade-off on CityScapes\ndataset.\n", "versions": [{"version": "v1", "created": "Tue, 7 May 2019 09:12:18 GMT"}, {"version": "v2", "created": "Fri, 10 May 2019 04:43:32 GMT"}, {"version": "v3", "created": "Mon, 13 May 2019 07:57:09 GMT"}], "update_date": "2019-05-14", "authors_parsed": [["Wang", "Yu", ""], ["Zhou", "Quan", ""], ["Liu", "Jia", ""], ["Xiong", "Jian", ""], ["Gao", "Guangwei", ""], ["Wu", "Xiaofu", ""], ["Latecki", "Longin Jan", ""]]}, {"id": "1905.02442", "submitter": "Sho Maeoki", "authors": "Sho Maeoki, Kohei Uehara, Tatsuya Harada", "title": "Interactive Video Retrieval with Dialog", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Now that everyone can easily record videos, the quantity of which is\ncontinuously increasing, research on methods for improved video retrieval is\nimportant in the contemporary world. In cases where target videos are to be\nidentified within a large collection gathered by individuals, the appropriate\ninformation must be obtained to retrieve the correct video within a large\nnumber of similar items in the target database. The purpose of this research is\nto retrieve target videos in such cases by introducing an interaction, or a\ndialog, between the system and the user. We propose a system to retrieve videos\nby asking questions about the content of the videos and leveraging the user's\nresponses to the questions. Additionally, we confirmed the usefulness of the\nproposed system through experiments using the dataset called AVSD which\nincludes videos and dialogs about the videos.\n", "versions": [{"version": "v1", "created": "Tue, 7 May 2019 09:49:10 GMT"}], "update_date": "2019-05-08", "authors_parsed": [["Maeoki", "Sho", ""], ["Uehara", "Kohei", ""], ["Harada", "Tatsuya", ""]]}, {"id": "1905.02462", "submitter": "Chao Li", "authors": "Chao Li, Dongliang He, Xiao Liu, Yukang Ding, Shilei Wen", "title": "Adapting Image Super-Resolution State-of-the-arts and Learning\n  Multi-model Ensemble for Video Super-Resolution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, image super-resolution has been widely studied and achieved\nsignificant progress by leveraging the power of deep convolutional neural\nnetworks. However, there has been limited advancement in video super-resolution\n(VSR) due to the complex temporal patterns in videos. In this paper, we\ninvestigate how to adapt state-of-the-art methods of image super-resolution for\nvideo super-resolution. The proposed adapting method is straightforward. The\ninformation among successive frames is well exploited, while the overhead on\nthe original image super-resolution method is negligible. Furthermore, we\npropose a learning-based method to ensemble the outputs from multiple\nsuper-resolution models. Our methods show superior performance and rank second\nin the NTIRE2019 Video Super-Resolution Challenge Track 1.\n", "versions": [{"version": "v1", "created": "Tue, 7 May 2019 10:44:50 GMT"}], "update_date": "2019-05-08", "authors_parsed": [["Li", "Chao", ""], ["He", "Dongliang", ""], ["Liu", "Xiao", ""], ["Ding", "Yukang", ""], ["Wen", "Shilei", ""]]}, {"id": "1905.02463", "submitter": "Isaac Dunn", "authors": "Isaac Dunn, Hadrien Pouget, Tom Melham, Daniel Kroening", "title": "Adaptive Generation of Unrestricted Adversarial Inputs", "comments": "Updated to include new results", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural networks are vulnerable to adversarially-constructed perturbations of\ntheir inputs. Most research so far has considered perturbations of a fixed\nmagnitude under some $l_p$ norm. Although studying these attacks is valuable,\nthere has been increasing interest in the construction of (and robustness to)\nunrestricted attacks, which are not constrained to a small and rather\nartificial subset of all possible adversarial inputs. We introduce a novel\nalgorithm for generating such unrestricted adversarial inputs which, unlike\nprior work, is adaptive: it is able to tune its attacks to the classifier being\ntargeted. It also offers a 400-2,000x speedup over the existing state of the\nart. We demonstrate our approach by generating unrestricted adversarial inputs\nthat fool classifiers robust to perturbation-based attacks. We also show that,\nby virtue of being adaptive and unrestricted, our attack is able to defeat\nadversarial training against it.\n", "versions": [{"version": "v1", "created": "Tue, 7 May 2019 10:54:43 GMT"}, {"version": "v2", "created": "Tue, 1 Oct 2019 12:43:55 GMT"}], "update_date": "2019-10-02", "authors_parsed": [["Dunn", "Isaac", ""], ["Pouget", "Hadrien", ""], ["Melham", "Tom", ""], ["Kroening", "Daniel", ""]]}, {"id": "1905.02473", "submitter": "Gianluca Maguolo", "authors": "Gianluca Maguolo, Loris Nanni, Stefano Ghidoni", "title": "Ensemble of Convolutional Neural Networks Trained with Different\n  Activation Functions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Activation functions play a vital role in the training of Convolutional\nNeural Networks. For this reason, to develop efficient and performing functions\nis a crucial problem in the deep learning community. Key to these approaches is\nto permit a reliable parameter learning, avoiding vanishing gradient problems.\nThe goal of this work is to propose an ensemble of Convolutional Neural\nNetworks trained using several different activation functions. Moreover, a\nnovel activation function is here proposed for the first time. Our aim is to\nimprove the performance of Convolutional Neural Networks in small/medium size\nbiomedical datasets. Our results clearly show that the proposed ensemble\noutperforms Convolutional Neural Networks trained with standard ReLU as\nactivation function. The proposed ensemble outperforms with a p-value of 0.01\neach tested stand-alone activation function; for reliable performance\ncomparison we have tested our approach in more than 10 datasets, using two\nwell-known Convolutional Neural Network: Vgg16 and ResNet50. MATLAB code used\nhere will be available at https://github.com/LorisNanni.\n", "versions": [{"version": "v1", "created": "Tue, 7 May 2019 11:09:32 GMT"}, {"version": "v2", "created": "Wed, 15 May 2019 14:20:03 GMT"}, {"version": "v3", "created": "Fri, 31 May 2019 11:32:37 GMT"}, {"version": "v4", "created": "Tue, 29 Oct 2019 00:10:03 GMT"}, {"version": "v5", "created": "Mon, 21 Sep 2020 23:28:27 GMT"}], "update_date": "2020-09-23", "authors_parsed": [["Maguolo", "Gianluca", ""], ["Nanni", "Loris", ""], ["Ghidoni", "Stefano", ""]]}, {"id": "1905.02479", "submitter": "Xiao Zhang", "authors": "Xiao Zhang, Rui Zhao, Junjie Yan, Mengya Gao, Yu Qiao, Xiaogang Wang,\n  Hongsheng Li", "title": "P2SGrad: Refined Gradients for Optimizing Deep Face Models", "comments": "Accepted by CVPR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cosine-based softmax losses significantly improve the performance of deep\nface recognition networks. However, these losses always include sensitive\nhyper-parameters which can make training process unstable, and it is very\ntricky to set suitable hyper parameters for a specific dataset. This paper\naddresses this challenge by directly designing the gradients for adaptively\ntraining deep neural networks. We first investigate and unify previous cosine\nsoftmax losses by analyzing their gradients. This unified view inspires us to\npropose a novel gradient called P2SGrad (Probability-to-Similarity Gradient),\nwhich leverages a cosine similarity instead of classification probability to\ndirectly update the testing metrics for updating neural network parameters.\nP2SGrad is adaptive and hyper-parameter free, which makes the training process\nmore efficient and faster. We evaluate our P2SGrad on three face recognition\nbenchmarks, LFW, MegaFace, and IJB-C. The results show that P2SGrad is stable\nin training, robust to noise, and achieves state-of-the-art performance on all\nthe three benchmarks.\n", "versions": [{"version": "v1", "created": "Tue, 7 May 2019 11:38:29 GMT"}], "update_date": "2019-05-08", "authors_parsed": [["Zhang", "Xiao", ""], ["Zhao", "Rui", ""], ["Yan", "Junjie", ""], ["Gao", "Mengya", ""], ["Qiao", "Yu", ""], ["Wang", "Xiaogang", ""], ["Li", "Hongsheng", ""]]}, {"id": "1905.02488", "submitter": "Qi Wang", "authors": "Qi Wang, Xiange He, Xuelong Li", "title": "Locality and Structure Regularized Low Rank Representation for\n  Hyperspectral Image Classification", "comments": "14 pages, 7 figures, TGRS2019", "journal-ref": "IEEE Transactions on Geoscience and Remote Sensing ( Volume: 57 ,\n  Issue: 2 , Feb. 2019 )", "doi": "10.1109/TGRS.2018.2862899", "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hyperspectral image (HSI) classification, which aims to assign an accurate\nlabel for hyperspectral pixels, has drawn great interest in recent years.\nAlthough low rank representation (LRR) has been used to classify HSI, its\nability to segment each class from the whole HSI data has not been exploited\nfully yet. LRR has a good capacity to capture the underlying lowdimensional\nsubspaces embedded in original data. However, there are still two drawbacks for\nLRR. First, LRR does not consider the local geometric structure within data,\nwhich makes the local correlation among neighboring data easily ignored.\nSecond, the representation obtained by solving LRR is not discriminative enough\nto separate different data. In this paper, a novel locality and structure\nregularized low rank representation (LSLRR) model is proposed for HSI\nclassification. To overcome the above limitations, we present locality\nconstraint criterion (LCC) and structure preserving strategy (SPS) to improve\nthe classical LRR. Specifically, we introduce a new distance metric, which\ncombines both spatial and spectral features, to explore the local similarity of\npixels. Thus, the global and local structures of HSI data can be exploited\nsufficiently. Besides, we propose a structure constraint to make the\nrepresentation have a near block-diagonal structure. This helps to determine\nthe final classification labels directly. Extensive experiments have been\nconducted on three popular HSI datasets. And the experimental results\ndemonstrate that the proposed LSLRR outperforms other state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Tue, 7 May 2019 12:05:52 GMT"}], "update_date": "2019-05-08", "authors_parsed": [["Wang", "Qi", ""], ["He", "Xiange", ""], ["Li", "Xuelong", ""]]}, {"id": "1905.02506", "submitter": "Burak Uzkent", "authors": "Burak Uzkent, Evan Sheehan, Chenlin Meng, Zhongyi Tang, Marshall\n  Burke, David Lobell, Stefano Ermon", "title": "Learning to Interpret Satellite Images in Global Scale Using Wikipedia", "comments": "Accepted to IJCAI 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite recent progress in computer vision, finegrained interpretation of\nsatellite images remains challenging because of a lack of labeled training\ndata. To overcome this limitation, we construct a novel dataset called\nWikiSatNet by pairing georeferenced Wikipedia articles with satellite imagery\nof their corresponding locations. We then propose two strategies to learn\nrepresentations of satellite images by predicting properties of the\ncorresponding articles from the images. Leveraging this new multi-modal\ndataset, we can drastically reduce the quantity of human-annotated labels and\ntime required for downstream tasks. On the recently released fMoW dataset, our\npre-training strategies can boost the performance of a model pre-trained on\nImageNet by up to 4:5% in F1 score.\n", "versions": [{"version": "v1", "created": "Tue, 7 May 2019 12:47:39 GMT"}, {"version": "v2", "created": "Sat, 11 May 2019 08:02:01 GMT"}, {"version": "v3", "created": "Sun, 11 Aug 2019 21:56:27 GMT"}], "update_date": "2019-08-13", "authors_parsed": [["Uzkent", "Burak", ""], ["Sheehan", "Evan", ""], ["Meng", "Chenlin", ""], ["Tang", "Zhongyi", ""], ["Burke", "Marshall", ""], ["Lobell", "David", ""], ["Ermon", "Stefano", ""]]}, {"id": "1905.02507", "submitter": "Christopher Zach", "authors": "Christopher Zach, Virginia Estellers", "title": "Contrastive Learning for Lifted Networks", "comments": "9 pages, BMVC 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we address supervised learning of neural networks via lifted\nnetwork formulations. Lifted networks are interesting because they allow\ntraining on massively parallel hardware and assign energy models to\ndiscriminatively trained neural networks. We demonstrate that the training\nmethods for lifted networks proposed in the literature have significant\nlimitations and show how to use a contrastive loss to address those\nlimitations. We demonstrate that this contrastive training approximates\nback-propagation in theory and in practice and that it is superior to the\ntraining objective regularly used for lifted networks.\n", "versions": [{"version": "v1", "created": "Tue, 7 May 2019 12:49:03 GMT"}, {"version": "v2", "created": "Fri, 26 Jul 2019 15:40:16 GMT"}], "update_date": "2019-07-29", "authors_parsed": [["Zach", "Christopher", ""], ["Estellers", "Virginia", ""]]}, {"id": "1905.02538", "submitter": "Guocheng Qian", "authors": "Guocheng Qian and Yuanhao Wang and Chao Dong and Jimmy S. Ren and\n  Wolfgang Heidrich and Bernard Ghanem and Jinjin Gu", "title": "Rethinking the Pipeline of Demosaicing, Denoising and Super-Resolution", "comments": "Code is available at: https://github.com/guochengqian/TENet", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Incomplete color sampling, noise degradation, and limited resolution are the\nthree key problems that are unavoidable in modern camera systems. Demosaicing\n(DM), denoising (DN), and super-resolution (SR) are core components in a\ndigital image processing pipeline to overcome the three problems above,\nrespectively. Although each of these problems has been studied actively, the\nmixture problem of DM, DN, and SR, which is a higher practical value, lacks\nenough attention. Such a mixture problem is usually solved by a sequential\nsolution (applying each method independently in a fixed order: DM $\\to$ DN\n$\\to$ SR), or is simply tackled by an end-to-end network without enough\nanalysis into interactions among tasks, resulting in an undesired performance\ndrop in the final image quality. In this paper, we rethink the mixture problem\nfrom a holistic perspective and propose a new image processing pipeline: DN\n$\\to$ SR $\\to$ DM. Extensive experiments show that simply modifying the usual\nsequential solution by leveraging our proposed pipeline could enhance the image\nquality by a large margin. We further adopt the proposed pipeline into an\nend-to-end network, and present Trinity Enhancement Network (TENet).\nQuantitative and qualitative experiments demonstrate the superiority of our\nTENet to the state-of-the-art. Besides, we notice the literature lacks a full\ncolor sampled dataset. To this end, we contribute a new high-quality full color\nsampled real-world dataset, namely PixelShift200. Our experiments show the\nbenefit of the proposed PixelShift200 dataset for raw image processing.\n", "versions": [{"version": "v1", "created": "Tue, 7 May 2019 13:19:05 GMT"}, {"version": "v2", "created": "Mon, 29 Mar 2021 19:57:02 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Qian", "Guocheng", ""], ["Wang", "Yuanhao", ""], ["Dong", "Chao", ""], ["Ren", "Jimmy S.", ""], ["Heidrich", "Wolfgang", ""], ["Ghanem", "Bernard", ""], ["Gu", "Jinjin", ""]]}, {"id": "1905.02540", "submitter": "Xinshuo Weng", "authors": "Xinshuo Weng and Kris Kitani", "title": "Learning Spatio-Temporal Features with Two-Stream Deep 3D CNNs for\n  Lipreading", "comments": "camera ready version for BMVC 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We focus on the word-level visual lipreading, which requires recognizing the\nword being spoken, given only the video but not the audio. State-of-the-art\nmethods explore the use of end-to-end neural networks, including a shallow (up\nto three layers) 3D convolutional neural network (CNN) + a deep 2D CNN (e.g.,\nResNet) as the front-end to extract visual features, and a recurrent neural\nnetwork (e.g., bidirectional LSTM) as the back-end for classification. In this\nwork, we propose to replace the shallow 3D CNNs + deep 2D CNNs front-end with\nrecent successful deep 3D CNNs --- two-stream (i.e., grayscale video and\noptical flow streams) I3D. We evaluate different combinations of front-end and\nback-end modules with the grayscale video and optical flow inputs on the LRW\ndataset. The experiments show that, compared to the shallow 3D CNNs + deep 2D\nCNNs front-end, the deep 3D CNNs front-end with pre-training on the large-scale\nimage and video datasets (e.g., ImageNet and Kinetics) can improve the\nclassification accuracy. Also, we demonstrate that using the optical flow input\nalone can achieve comparable performance as using the grayscale video as input.\nMoreover, the two-stream network using both the grayscale video and optical\nflow inputs can further improve the performance. Overall, our two-stream I3D\nfront-end with a Bi-LSTM back-end results in an absolute improvement of 5.3%\nover the previous art on the LRW dataset.\n", "versions": [{"version": "v1", "created": "Sat, 4 May 2019 02:32:06 GMT"}, {"version": "v2", "created": "Fri, 19 Jul 2019 03:19:21 GMT"}], "update_date": "2019-07-22", "authors_parsed": [["Weng", "Xinshuo", ""], ["Kitani", "Kris", ""]]}, {"id": "1905.02544", "submitter": "Yaroub Elloumi", "authors": "Yaroub Elloumi (LIGM), Mohamed Akil (LIGM), Henda Boudegga", "title": "Ocular Diseases Diagnosis in Fundus Images using a Deep Learning:\n  Approaches, tools and Performance evaluation", "comments": null, "journal-ref": "SPIE Real-Time Image Processing and Deep Learning, Apr 2019,\n  Baltimore, Maryland, United States", "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ocular pathology detection from fundus images presents an important challenge\non health care. In fact, each pathology has different severity stages that may\nbe deduced by verifying the existence of specific lesions. Each lesion is\ncharacterized by morphological features. Moreover, several lesions of different\npathologies have similar features. We note that patient may be affected\nsimultaneously by several pathologies. Consequently, the ocular pathology\ndetection presents a multi-class classification with a complex resolution\nprinciple. Several detection methods of ocular pathologies from fundus images\nhave been proposed. The methods based on deep learning are distinguished by\nhigher performance detection, due to their capability to configure the network\nwith respect to the detection objective. This work proposes a survey of ocular\npathology detection methods based on deep learning. First, we study the\nexisting methods either for lesion segmentation or pathology classification.\nAfterwards, we extract the principle steps of processing and we analyze the\nproposed neural network structures. Subsequently, we identify the hardware and\nsoftware environment required to employ the deep learning architecture.\nThereafter, we investigate about the experimentation principles involved to\nevaluate the methods and the databases used either for training and testing\nphases. The detection performance ratios and execution times are also reported\nand discussed.\n", "versions": [{"version": "v1", "created": "Tue, 7 May 2019 13:21:01 GMT"}], "update_date": "2019-05-08", "authors_parsed": [["Elloumi", "Yaroub", "", "LIGM"], ["Akil", "Mohamed", "", "LIGM"], ["Boudegga", "Henda", ""]]}, {"id": "1905.02553", "submitter": "Bo Sun", "authors": "Bo Sun and Philippos Mordohai", "title": "Oriented Point Sampling for Plane Detection in Unorganized Point Clouds", "comments": "7 pages, 3 figures, 2019 IEEE International Conference on Robotics\n  and Automation (Accepted)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Plane detection in 3D point clouds is a crucial pre-processing step for\napplications such as point cloud segmentation, semantic mapping and SLAM. In\ncontrast to many recent plane detection methods that are only applicable on\norganized point clouds, our work is targeted to unorganized point clouds that\ndo not permit a 2D parametrization. We compare three methods for detecting\nplanes in point clouds efficiently. One is a novel method proposed in this\npaper that generates plane hypotheses by sampling from a set of points with\nestimated normals. We named this method Oriented Point Sampling (OPS) to\ncontrast with more conventional techniques that require the sampling of three\nunoriented points to generate plane hypotheses. We also implemented an\nefficient plane detection method based on local sampling of three unoriented\npoints and compared it with OPS and the 3D-KHT algorithm, which is based on\noctrees, on the detection of planes on 10,000 point clouds from the SUN RGB-D\ndataset.\n", "versions": [{"version": "v1", "created": "Sat, 4 May 2019 01:02:57 GMT"}], "update_date": "2019-05-08", "authors_parsed": [["Sun", "Bo", ""], ["Mordohai", "Philippos", ""]]}, {"id": "1905.02567", "submitter": "Weiwen Wu", "authors": "Weiwen Wu, Haijun Yu, Peijun Chen, Fulin Luo, Fenglin Liu, Qian Wang,\n  Yining Zhu, Yanbo Zhang, Jian Feng, Hengyong Yu", "title": "DLIMD: Dictionary Learning based Image-domain Material Decomposition for\n  spectral CT", "comments": null, "journal-ref": "Physics in Medicine & Biology, 2020", "doi": "10.1088/1361-6560/aba7ce", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The potential huge advantage of spectral computed tomography (CT) is its\ncapability to provide accuracy material identification and quantitative tissue\ninformation. This can benefit clinical applications, such as brain angiography,\nearly tumor recognition, etc. To achieve more accurate material components with\nhigher material image quality, we develop a dictionary learning based\nimage-domain material decomposition (DLIMD) for spectral CT in this paper.\nFirst, we reconstruct spectral CT image from projections and calculate material\ncoefficients matrix by selecting uniform regions of basis materials from image\nreconstruction results. Second, we employ the direct inversion (DI) method to\nobtain initial material decomposition results, and a set of image patches are\nextracted from the mode-1 unfolding of normalized material image tensor to\ntrain a united dictionary by the K-SVD technique. Third, the trained dictionary\nis employed to explore the similarities from decomposed material images by\nconstructing the DLIMD model. Fourth, more constraints (i.e., volume\nconservation and the bounds of each pixel within material maps) are further\nintegrated into the model to improve the accuracy of material decomposition.\nFinally, both physical phantom and preclinical experiments are employed to\nevaluate the performance of the proposed DLIMD in material decomposition\naccuracy, material image edge preservation and feature recovery.\n", "versions": [{"version": "v1", "created": "Mon, 6 May 2019 00:28:48 GMT"}, {"version": "v2", "created": "Fri, 24 May 2019 09:15:41 GMT"}], "update_date": "2020-08-18", "authors_parsed": [["Wu", "Weiwen", ""], ["Yu", "Haijun", ""], ["Chen", "Peijun", ""], ["Luo", "Fulin", ""], ["Liu", "Fenglin", ""], ["Wang", "Qian", ""], ["Zhu", "Yining", ""], ["Zhang", "Yanbo", ""], ["Feng", "Jian", ""], ["Yu", "Hengyong", ""]]}, {"id": "1905.02590", "submitter": "Nils Gessert", "authors": "Nils Gessert and Alexander Schlaefer", "title": "Efficient Neural Architecture Search on Low-Dimensional Data for OCT\n  Image Segmentation", "comments": "Accepted at MIDL 2019 [arXiv:1907.08612]", "journal-ref": null, "doi": null, "report-no": "MIDL/2019/ExtendedAbstract/Syg3FDjntN", "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Typically, deep learning architectures are handcrafted for their respective\nlearning problem. As an alternative, neural architecture search (NAS) has been\nproposed where the architecture's structure is learned in an additional\noptimization step. For the medical imaging domain, this approach is very\npromising as there are diverse problems and imaging modalities that require\narchitecture design. However, NAS is very time-consuming and medical learning\nproblems often involve high-dimensional data with high computational\nrequirements. We propose an efficient approach for NAS in the context of\nmedical, image-based deep learning problems by searching for architectures on\nlow-dimensional data which are subsequently transferred to high-dimensional\ndata. For OCT-based layer segmentation, we demonstrate that a search on 1D data\nreduces search time by 87.5% compared to a search on 2D data while the final 2D\nmodels achieve similar performance.\n", "versions": [{"version": "v1", "created": "Tue, 7 May 2019 14:01:56 GMT"}], "update_date": "2019-07-29", "authors_parsed": [["Gessert", "Nils", ""], ["Schlaefer", "Alexander", ""]]}, {"id": "1905.02610", "submitter": "Chunxu Zhang", "authors": "Chunxu Zhang and Jiaxu Cui and Bo Yang", "title": "Learning Optimal Data Augmentation Policies via Bayesian Optimization\n  for Image Classification Tasks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, deep learning has achieved remarkable achievements in many\nfields, including computer vision, natural language processing, speech\nrecognition and others. Adequate training data is the key to ensure the\neffectiveness of the deep models. However, obtaining valid data requires a lot\nof time and labor resources. Data augmentation (DA) is an effective alternative\napproach, which can generate new labeled data based on existing data using\nlabel-preserving transformations. Although we can benefit a lot from DA,\ndesigning appropriate DA policies requires a lot of expert experience and time\nconsumption, and the evaluation of searching the optimal policies is costly. So\nwe raise a new question in this paper: how to achieve automated data\naugmentation at as low cost as possible? We propose a method named BO-Aug for\nautomating the process by finding the optimal DA policies using the Bayesian\noptimization approach. Our method can find the optimal policies at a relatively\nlow search cost, and the searched policies based on a specific dataset are\ntransferable across different neural network architectures or even different\ndatasets. We validate the BO-Aug on three widely used image classification\ndatasets, including CIFAR-10, CIFAR-100 and SVHN. Experimental results show\nthat the proposed method can achieve state-of-the-art or near advanced\nclassification accuracy. Code to reproduce our experiments is available at\nhttps://github.com/zhangxiaozao/BO-Aug.\n", "versions": [{"version": "v1", "created": "Mon, 6 May 2019 15:49:02 GMT"}, {"version": "v2", "created": "Thu, 23 May 2019 04:50:16 GMT"}], "update_date": "2019-05-24", "authors_parsed": [["Zhang", "Chunxu", ""], ["Cui", "Jiaxu", ""], ["Yang", "Bo", ""]]}, {"id": "1905.02649", "submitter": "Bowen Cheng", "authors": "Bowen Cheng, Rong Xiao, Jianfeng Wang, Thomas Huang, Lei Zhang", "title": "High Frequency Residual Learning for Multi-Scale Image Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel high frequency residual learning framework, which leads to\na highly efficient multi-scale network (MSNet) architecture for mobile and\nembedded vision problems. The architecture utilizes two networks: a low\nresolution network to efficiently approximate low frequency components and a\nhigh resolution network to learn high frequency residuals by reusing the\nupsampled low resolution features. With a classifier calibration module, MSNet\ncan dynamically allocate computation resources during inference to achieve a\nbetter speed and accuracy trade-off. We evaluate our methods on the challenging\nImageNet-1k dataset and observe consistent improvements over different base\nnetworks. On ResNet-18 and MobileNet with alpha=1.0, MSNet gains 1.5% accuracy\nover both architectures without increasing computations. On the more efficient\nMobileNet with alpha=0.25, our method gains 3.8% accuracy with the same amount\nof computations.\n", "versions": [{"version": "v1", "created": "Tue, 7 May 2019 15:47:27 GMT"}], "update_date": "2019-05-08", "authors_parsed": [["Cheng", "Bowen", ""], ["Xiao", "Rong", ""], ["Wang", "Jianfeng", ""], ["Huang", "Thomas", ""], ["Zhang", "Lei", ""]]}, {"id": "1905.02655", "submitter": "St\\'ephane Lathuili\\`ere", "authors": "St\\'ephane Lathuili\\`ere, Enver Sangineto, Aliaksandr Siarohin and\n  Nicu Sebe", "title": "Attention-based Fusion for Multi-source Human Image Generation", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a generalization of the person-image generation task, in which a\nhuman image is generated conditioned on a target pose and a set X of source\nappearance images. In this way, we can exploit multiple, possibly complementary\nimages of the same person which are usually available at training and at\ntesting time. The solution we propose is mainly based on a local attention\nmechanism which selects relevant information from different source image\nregions, avoiding the necessity to build specific generators for each specific\ncardinality of X. The empirical evaluation of our method shows the practical\ninterest of addressing the person-image generation problem in a multi-source\nsetting.\n", "versions": [{"version": "v1", "created": "Tue, 7 May 2019 16:00:39 GMT"}], "update_date": "2019-05-08", "authors_parsed": [["Lathuili\u00e8re", "St\u00e9phane", ""], ["Sangineto", "Enver", ""], ["Siarohin", "Aliaksandr", ""], ["Sebe", "Nicu", ""]]}, {"id": "1905.02675", "submitter": "Todor Davchev", "authors": "Todor Davchev, Timos Korres, Stathi Fotiadis, Nick Antonopoulos,\n  Subramanian Ramamoorthy", "title": "An Empirical Evaluation of Adversarial Robustness under Transfer\n  Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we evaluate adversarial robustness in the context of transfer\nlearning from a source trained on CIFAR 100 to a target network trained on\nCIFAR 10. Specifically, we study the effects of using robust optimisation in\nthe source and target networks. This allows us to identify transfer learning\nstrategies under which adversarial defences are successfully retained, in\naddition to revealing potential vulnerabilities. We study the extent to which\nfeatures learnt by a fast gradient sign method (FGSM) and its iterative\nalternative (PGD) can preserve their defence properties against black and\nwhite-box attacks under three different transfer learning strategies. We find\nthat using PGD examples during training on the source task leads to more\ngeneral robust features that are easier to transfer. Furthermore, under\nsuccessful transfer, it achieves 5.2% more accuracy against white-box PGD\nattacks than suitable baselines. Overall, our empirical evaluations give\ninsights on how well adversarial robustness under transfer learning can\ngeneralise.\n", "versions": [{"version": "v1", "created": "Tue, 7 May 2019 16:26:26 GMT"}, {"version": "v2", "created": "Thu, 9 May 2019 00:34:28 GMT"}, {"version": "v3", "created": "Thu, 23 May 2019 09:37:44 GMT"}, {"version": "v4", "created": "Sat, 8 Jun 2019 22:25:52 GMT"}], "update_date": "2019-06-11", "authors_parsed": [["Davchev", "Todor", ""], ["Korres", "Timos", ""], ["Fotiadis", "Stathi", ""], ["Antonopoulos", "Nick", ""], ["Ramamoorthy", "Subramanian", ""]]}, {"id": "1905.02686", "submitter": "Yuemeng Li", "authors": "Yuemeng Li, Hangfan Liu, Hongming Li, Yong Fan", "title": "Feature-Fused Context-Encoding Network for Neuroanatomy Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic segmentation of fine-grained brain structures remains a challenging\ntask. Current segmentation methods mainly utilize 2D and 3D deep neural\nnetworks. The 2D networks take image slices as input to produce coarse\nsegmentation in less processing time, whereas the 3D networks take the whole\nimage volumes to generated fine-detailed segmentation with more computational\nburden. In order to obtain accurate fine-grained segmentation efficiently, in\nthis paper, we propose an end-to-end Feature-Fused Context-Encoding Network for\nbrain structure segmentation from MR (magnetic resonance) images. Our model is\nimplemented based on a 2D convolutional backbone, which integrates a 2D\nencoding module to acquire planar image features and a spatial encoding module\nto extract spatial context information. A global context encoding module is\nfurther introduced to capture global context semantics from the fused 2D\nencoding and spatial features. The proposed network aims to fully leverage the\nglobal anatomical prior knowledge learned from context semantics, which is\nrepresented by a structure-aware attention factor to recalibrate the outputs of\nthe network. In this way, the network is guaranteed to be aware of the\nclass-dependent feature maps to facilitate the segmentation. We evaluate our\nmodel on 2012 Brain Multi-Atlas Labelling Challenge dataset for 134\nfine-grained structure segmentation. Besides, we validate our network on 27\ncoarse structure segmentation tasks. Experimental results have demonstrated\nthat our model can achieve improved performance compared with the\nstate-of-the-art approaches.\n", "versions": [{"version": "v1", "created": "Tue, 7 May 2019 16:43:12 GMT"}], "update_date": "2019-05-08", "authors_parsed": [["Li", "Yuemeng", ""], ["Liu", "Hangfan", ""], ["Li", "Hongming", ""], ["Fan", "Yong", ""]]}, {"id": "1905.02693", "submitter": "Vitor Guizilini", "authors": "Vitor Guizilini, Rares Ambrus, Sudeep Pillai, Allan Raventos, Adrien\n  Gaidon", "title": "3D Packing for Self-Supervised Monocular Depth Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although cameras are ubiquitous, robotic platforms typically rely on active\nsensors like LiDAR for direct 3D perception. In this work, we propose a novel\nself-supervised monocular depth estimation method combining geometry with a new\ndeep network, PackNet, learned only from unlabeled monocular videos. Our\narchitecture leverages novel symmetrical packing and unpacking blocks to\njointly learn to compress and decompress detail-preserving representations\nusing 3D convolutions. Although self-supervised, our method outperforms other\nself, semi, and fully supervised methods on the KITTI benchmark. The 3D\ninductive bias in PackNet enables it to scale with input resolution and number\nof parameters without overfitting, generalizing better on out-of-domain data\nsuch as the NuScenes dataset. Furthermore, it does not require large-scale\nsupervised pretraining on ImageNet and can run in real-time. Finally, we\nrelease DDAD (Dense Depth for Automated Driving), a new urban driving dataset\nwith more challenging and accurate depth evaluation, thanks to longer-range and\ndenser ground-truth depth generated from high-density LiDARs mounted on a fleet\nof self-driving cars operating world-wide.\n", "versions": [{"version": "v1", "created": "Mon, 6 May 2019 17:09:52 GMT"}, {"version": "v2", "created": "Mon, 23 Sep 2019 21:34:18 GMT"}, {"version": "v3", "created": "Fri, 6 Dec 2019 03:21:21 GMT"}, {"version": "v4", "created": "Sat, 28 Mar 2020 18:49:27 GMT"}], "update_date": "2020-03-31", "authors_parsed": [["Guizilini", "Vitor", ""], ["Ambrus", "Rares", ""], ["Pillai", "Sudeep", ""], ["Raventos", "Allan", ""], ["Gaidon", "Adrien", ""]]}, {"id": "1905.02706", "submitter": "Tejas Khot", "authors": "Tejas Khot, Shubham Agrawal, Shubham Tulsiani, Christoph Mertz, Simon\n  Lucey, Martial Hebert", "title": "Learning Unsupervised Multi-View Stereopsis via Robust Photometric\n  Consistency", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a learning based approach for multi-view stereopsis (MVS). While\ncurrent deep MVS methods achieve impressive results, they crucially rely on\nground-truth 3D training data, and acquisition of such precise 3D geometry for\nsupervision is a major hurdle. Our framework instead leverages photometric\nconsistency between multiple views as supervisory signal for learning depth\nprediction in a wide baseline MVS setup. However, naively applying photo\nconsistency constraints is undesirable due to occlusion and lighting changes\nacross views. To overcome this, we propose a robust loss formulation that: a)\nenforces first order consistency and b) for each point, selectively enforces\nconsistency with some views, thus implicitly handling occlusions. We\ndemonstrate our ability to learn MVS without 3D supervision using a real\ndataset, and show that each component of our proposed robust loss results in a\nsignificant improvement. We qualitatively observe that our reconstructions are\noften more complete than the acquired ground truth, further showing the merits\nof this approach. Lastly, our learned model generalizes to novel settings, and\nour approach allows adaptation of existing CNNs to datasets without\nground-truth 3D by unsupervised finetuning. Project webpage:\nhttps://tejaskhot.github.io/unsup_mvs\n", "versions": [{"version": "v1", "created": "Tue, 7 May 2019 17:45:22 GMT"}, {"version": "v2", "created": "Thu, 6 Jun 2019 17:30:47 GMT"}], "update_date": "2019-06-07", "authors_parsed": [["Khot", "Tejas", ""], ["Agrawal", "Shubham", ""], ["Tulsiani", "Shubham", ""], ["Mertz", "Christoph", ""], ["Lucey", "Simon", ""], ["Hebert", "Martial", ""]]}, {"id": "1905.02710", "submitter": "Kumara Kahatapitiya", "authors": "Kumara Kahatapitiya, Dumindu Tissera and Ranga Rodrigo", "title": "Context-Aware Automatic Occlusion Removal", "comments": "Accepted to be published in Proceedings of IEEE International\n  Conference on Image Processing (ICIP), Taipei, Taiwan, September 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Occlusion removal is an interesting application of image enhancement, for\nwhich, existing work suggests manually-annotated or domain-specific occlusion\nremoval. No work tries to address automatic occlusion detection and removal as\na context-aware generic problem. In this paper, we present a novel methodology\nto identify objects that do not relate to the image context as occlusions and\nremove them, reconstructing the space occupied coherently. The proposed system\ndetects occlusions by considering the relation between foreground and\nbackground object classes represented as vector embeddings, and removes them\nthrough inpainting. We test our system on COCO-Stuff dataset and conduct a user\nstudy to establish a baseline in context-aware automatic occlusion removal.\n", "versions": [{"version": "v1", "created": "Tue, 7 May 2019 17:50:40 GMT"}], "update_date": "2019-05-08", "authors_parsed": [["Kahatapitiya", "Kumara", ""], ["Tissera", "Dumindu", ""], ["Rodrigo", "Ranga", ""]]}, {"id": "1905.02716", "submitter": "Xintao Wang", "authors": "Xintao Wang, Kelvin C.K. Chan, Ke Yu, Chao Dong, Chen Change Loy", "title": "EDVR: Video Restoration with Enhanced Deformable Convolutional Networks", "comments": "To appear in CVPR 2019 Workshop. The winners in all four tracks in\n  the NTIRE 2019 video restoration and enhancement challenges. Project page:\n  https://xinntao.github.io/projects/EDVR , Code:\n  https://github.com/xinntao/EDVR", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video restoration tasks, including super-resolution, deblurring, etc, are\ndrawing increasing attention in the computer vision community. A challenging\nbenchmark named REDS is released in the NTIRE19 Challenge. This new benchmark\nchallenges existing methods from two aspects: (1) how to align multiple frames\ngiven large motions, and (2) how to effectively fuse different frames with\ndiverse motion and blur. In this work, we propose a novel Video Restoration\nframework with Enhanced Deformable networks, termed EDVR, to address these\nchallenges. First, to handle large motions, we devise a Pyramid, Cascading and\nDeformable (PCD) alignment module, in which frame alignment is done at the\nfeature level using deformable convolutions in a coarse-to-fine manner. Second,\nwe propose a Temporal and Spatial Attention (TSA) fusion module, in which\nattention is applied both temporally and spatially, so as to emphasize\nimportant features for subsequent restoration. Thanks to these modules, our\nEDVR wins the champions and outperforms the second place by a large margin in\nall four tracks in the NTIRE19 video restoration and enhancement challenges.\nEDVR also demonstrates superior performance to state-of-the-art published\nmethods on video super-resolution and deblurring. The code is available at\nhttps://github.com/xinntao/EDVR.\n", "versions": [{"version": "v1", "created": "Tue, 7 May 2019 17:58:14 GMT"}], "update_date": "2019-05-08", "authors_parsed": [["Wang", "Xintao", ""], ["Chan", "Kelvin C. K.", ""], ["Yu", "Ke", ""], ["Dong", "Chao", ""], ["Loy", "Chen Change", ""]]}, {"id": "1905.02719", "submitter": "Masanari Kimura", "authors": "Masanari Kimura, Masayuki Tanaka", "title": "Intentional Attention Mask Transformation for Robust CNN Classification", "comments": "arXiv admin note: substantial text overlap with arXiv:1904.13078", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional Neural Networks have achieved impressive results in various\ntasks, but interpreting the internal mechanism is a challenging problem. To\ntackle this problem, we exploit a multi-channel attention mechanism in feature\nspace. Our network architecture allows us to obtain an attention mask for each\nfeature while existing CNN visualization methods provide only a common\nattention mask for all features. We apply the proposed multi-channel attention\nmechanism to multi-attribute recognition task. We can obtain different\nattention mask for each feature and for each attribute. Those analyses give us\ndeeper insight into the feature space of CNNs. Furthermore, our proposed\nattention mechanism naturally derives a method for improving the robustness of\nCNNs. From the observation of feature space based on the proposed attention\nmask, we demonstrate that we can obtain robust CNNs by intentionally\nemphasizing features that are important for attributes. The experimental\nresults for the benchmark dataset show that the proposed method gives high\nhuman interpretability while accurately grasping the attributes of the data,\nand improves network robustness.\n", "versions": [{"version": "v1", "created": "Tue, 7 May 2019 08:16:46 GMT"}, {"version": "v2", "created": "Mon, 20 May 2019 06:22:10 GMT"}], "update_date": "2019-05-22", "authors_parsed": [["Kimura", "Masanari", ""], ["Tanaka", "Masayuki", ""]]}, {"id": "1905.02722", "submitter": "Zhengqin Li", "authors": "Zhengqin Li, Mohammad Shafiei, Ravi Ramamoorthi, Kalyan Sunkavalli,\n  Manmohan Chandraker", "title": "Inverse Rendering for Complex Indoor Scenes: Shape, Spatially-Varying\n  Lighting and SVBRDF from a Single Image", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a deep inverse rendering framework for indoor scenes. From a\nsingle RGB image of an arbitrary indoor scene, we create a complete scene\nreconstruction, estimating shape, spatially-varying lighting, and\nspatially-varying, non-Lambertian surface reflectance. To train this network,\nwe augment the SUNCG indoor scene dataset with real-world materials and render\nthem with a fast, high-quality, physically-based GPU renderer to create a\nlarge-scale, photorealistic indoor dataset. Our inverse rendering network\nincorporates physical insights -- including a spatially-varying spherical\nGaussian lighting representation, a differentiable rendering layer to model\nscene appearance, a cascade structure to iteratively refine the predictions and\na bilateral solver for refinement -- allowing us to jointly reason about shape,\nlighting, and reflectance. Experiments show that our framework outperforms\nprevious methods for estimating individual scene components, which also enables\nvarious novel applications for augmented reality, such as photorealistic object\ninsertion and material editing. Code and data will be made publicly available.\n", "versions": [{"version": "v1", "created": "Tue, 7 May 2019 17:47:40 GMT"}], "update_date": "2019-05-09", "authors_parsed": [["Li", "Zhengqin", ""], ["Shafiei", "Mohammad", ""], ["Ramamoorthi", "Ravi", ""], ["Sunkavalli", "Kalyan", ""], ["Chandraker", "Manmohan", ""]]}, {"id": "1905.02744", "submitter": "Junming Zhang", "authors": "Junming Zhang, Manikandasriram Srinivasan Ramanagopal, Ram Vasudevan,\n  Matthew Johnson-Roberson", "title": "LiStereo: Generate Dense Depth Maps from LIDAR and Stereo Imagery", "comments": "14 pages, 3 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An accurate depth map of the environment is critical to the safe operation of\nautonomous robots and vehicles. Currently, either light detection and ranging\n(LIDAR) or stereo matching algorithms are used to acquire such depth\ninformation. However, a high-resolution LIDAR is expensive and produces sparse\ndepth map at large range; stereo matching algorithms are able to generate\ndenser depth maps but are typically less accurate than LIDAR at long range.\nThis paper combines these approaches together to generate high-quality dense\ndepth maps. Unlike previous approaches that are trained using ground-truth\nlabels, the proposed model adopts a self-supervised training process.\nExperiments show that the proposed method is able to generate high-quality\ndense depth maps and performs robustly even with low-resolution inputs. This\nshows the potential to reduce the cost by using LIDARs with lower resolution in\nconcert with stereo systems while maintaining high resolution.\n", "versions": [{"version": "v1", "created": "Tue, 7 May 2019 18:09:22 GMT"}, {"version": "v2", "created": "Sun, 21 Jun 2020 05:42:06 GMT"}, {"version": "v3", "created": "Thu, 25 Jun 2020 19:00:06 GMT"}], "update_date": "2020-06-29", "authors_parsed": [["Zhang", "Junming", ""], ["Ramanagopal", "Manikandasriram Srinivasan", ""], ["Vasudevan", "Ram", ""], ["Johnson-Roberson", "Matthew", ""]]}, {"id": "1905.02749", "submitter": "Litu Rout", "authors": "Litu Rout, Yatharath Bhateja, Ankur Garg, Indranil Mishra, S Manthira\n  Moorthi, and Debjyoti Dhar", "title": "DeepSWIR: A Deep Learning Based Approach for the Synthesis of Short-Wave\n  InfraRed Band using Multi-Sensor Concurrent Datasets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional Neural Network (CNN) is achieving remarkable progress in\nvarious computer vision tasks. In the past few years, the remote sensing\ncommunity has observed Deep Neural Network (DNN) finally taking off in several\nchallenging fields. In this study, we propose a DNN to generate a predefined\nHigh Resolution (HR) synthetic spectral band using an ensemble of concurrent\nLow Resolution (LR) bands and existing HR bands. Of particular interest, the\nproposed network, namely DeepSWIR, synthesizes Short-Wave InfraRed (SWIR) band\nat 5m Ground Sampling Distance (GSD) using Green (G), Red (R) and Near InfraRed\n(NIR) bands at both 24m and 5m GSD, and SWIR band at 24m GSD. To our knowledge,\nthe highest spatial resolution of commercially deliverable SWIR band is at 7.5m\nGSD. Also, we propose a Gaussian feathering based image stitching approach in\nlight of processing large satellite imagery. To experimentally validate the\nsynthesized HR SWIR band, we critically analyse the qualitative and\nquantitative results produced by DeepSWIR using state-of-the-art evaluation\nmetrics. Further, we convert the synthesized DN values to Top Of Atmosphere\n(TOA) reflectance and compare with the corresponding band of Sentinel-2B.\nFinally, we show one real world application of the synthesized band by using it\nto map wetland resources over our region of interest.\n", "versions": [{"version": "v1", "created": "Tue, 7 May 2019 18:11:24 GMT"}], "update_date": "2019-05-09", "authors_parsed": [["Rout", "Litu", ""], ["Bhateja", "Yatharath", ""], ["Garg", "Ankur", ""], ["Mishra", "Indranil", ""], ["Moorthi", "S Manthira", ""], ["Dhar", "Debjyoti", ""]]}, {"id": "1905.02756", "submitter": "Jingxiao Zheng", "authors": "Jingxiao Zheng, Ruichi Yu, Jun-Cheng Chen, Boyu Lu, Carlos D.\n  Castillo, Rama Chellappa", "title": "Uncertainty Modeling of Contextual-Connections between Tracklets for\n  Unconstrained Video-based Face Recognition", "comments": "To appear in ICCV 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unconstrained video-based face recognition is a challenging problem due to\nsignificant within-video variations caused by pose, occlusion and blur. To\ntackle this problem, an effective idea is to propagate the identity from\nhigh-quality faces to low-quality ones through contextual connections, which\nare constructed based on context such as body appearance. However, previous\nmethods have often propagated erroneous information due to lack of uncertainty\nmodeling of the noisy contextual connections. In this paper, we propose the\nUncertainty-Gated Graph (UGG), which conducts graph-based identity propagation\nbetween tracklets, which are represented by nodes in a graph. UGG explicitly\nmodels the uncertainty of the contextual connections by adaptively updating the\nweights of the edge gates according to the identity distributions of the nodes\nduring inference. UGG is a generic graphical model that can be applied at only\ninference time or with end-to-end training. We demonstrate the effectiveness of\nUGG with state-of-the-art results in the recently released challenging Cast\nSearch in Movies and IARPA Janus Surveillance Video Benchmark dataset.\n", "versions": [{"version": "v1", "created": "Tue, 7 May 2019 18:24:35 GMT"}, {"version": "v2", "created": "Wed, 21 Aug 2019 18:44:18 GMT"}], "update_date": "2019-08-23", "authors_parsed": [["Zheng", "Jingxiao", ""], ["Yu", "Ruichi", ""], ["Chen", "Jun-Cheng", ""], ["Lu", "Boyu", ""], ["Castillo", "Carlos D.", ""], ["Chellappa", "Rama", ""]]}, {"id": "1905.02758", "submitter": "Michael Teutsch", "authors": "Kevin Fritz, Daniel K\\\"onig, Ulrich Klauck, Michael Teutsch", "title": "Generalization ability of region proposal networks for multispectral\n  person detection", "comments": null, "journal-ref": "SPIE Proceedings Volume 10988, Automatic Target Recognition XXIX;\n  109880Y (2019)", "doi": "10.1117/12.2520705", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multispectral person detection aims at automatically localizing humans in\nimages that consist of multiple spectral bands. Usually, the visual-optical\n(VIS) and the thermal infrared (IR) spectra are combined to achieve higher\nrobustness for person detection especially in insufficiently illuminated\nscenes. This paper focuses on analyzing existing detection approaches for their\ngeneralization ability. Generalization is a key feature for machine learning\nbased detection algorithms that are supposed to perform well across different\ndatasets. Inspired by recent literature regarding person detection in the VIS\nspectrum, we perform a cross-validation study to empirically determine the most\npromising dataset to train a well-generalizing detector. Therefore, we pick one\nreference Deep Convolutional Neural Network (DCNN) architecture and three\ndifferent multispectral datasets. The Region Proposal Network (RPN) originally\nintroduced for object detection within the popular Faster R-CNN is chosen as a\nreference DCNN. The reason is that a stand-alone RPN is able to serve as a\ncompetitive detector for two-class problems such as person detection.\nFurthermore, current state-of-the-art approaches initially apply an RPN\nfollowed by individual classifiers. The three considered datasets are the KAIST\nMultispectral Pedestrian Benchmark including recently published improved\nannotations for training and testing, the Tokyo Multi-spectral Semantic\nSegmentation dataset, and the OSU Color-Thermal dataset including recently\nreleased annotations. The experimental results show that the KAIST\nMultispectral Pedestrian Benchmark with its improved annotations provides the\nbest basis to train a DCNN with good generalization ability compared to the\nother two multispectral datasets. On average, this detection model achieves a\nlog-average Miss Rate (MR) of 29.74 % evaluated on the reasonable test subsets\nof the three datasets.\n", "versions": [{"version": "v1", "created": "Tue, 7 May 2019 18:29:00 GMT"}], "update_date": "2019-05-21", "authors_parsed": [["Fritz", "Kevin", ""], ["K\u00f6nig", "Daniel", ""], ["Klauck", "Ulrich", ""], ["Teutsch", "Michael", ""]]}, {"id": "1905.02781", "submitter": "Ioan Andrei B\\^arsan", "authors": "Ioan Andrei B\\^arsan, Peidong Liu, Marc Pollefeys, Andreas Geiger", "title": "Robust Dense Mapping for Large-Scale Dynamic Environments", "comments": "Presented at IEEE International Conference on Robotics and Automation\n  (ICRA), 2018", "journal-ref": "Proceedings of the 2018 IEEE International Conference on Robotics\n  and Automation (ICRA), pages 7510--7517, 21-25 May 2018, Brisbane, QLD,\n  Australia", "doi": "10.1109/ICRA.2018.8462974", "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a stereo-based dense mapping algorithm for large-scale dynamic\nurban environments. In contrast to other existing methods, we simultaneously\nreconstruct the static background, the moving objects, and the potentially\nmoving but currently stationary objects separately, which is desirable for\nhigh-level mobile robotic tasks such as path planning in crowded environments.\nWe use both instance-aware semantic segmentation and sparse scene flow to\nclassify objects as either background, moving, or potentially moving, thereby\nensuring that the system is able to model objects with the potential to\ntransition from static to dynamic, such as parked cars. Given camera poses\nestimated from visual odometry, both the background and the (potentially)\nmoving objects are reconstructed separately by fusing the depth maps computed\nfrom the stereo input. In addition to visual odometry, sparse scene flow is\nalso used to estimate the 3D motions of the detected moving objects, in order\nto reconstruct them accurately. A map pruning technique is further developed to\nimprove reconstruction accuracy and reduce memory consumption, leading to\nincreased scalability. We evaluate our system thoroughly on the well-known\nKITTI dataset. Our system is capable of running on a PC at approximately 2.5Hz,\nwith the primary bottleneck being the instance-aware semantic segmentation,\nwhich is a limitation we hope to address in future work. The source code is\navailable from the project website (http://andreibarsan.github.io/dynslam).\n", "versions": [{"version": "v1", "created": "Tue, 7 May 2019 19:38:27 GMT"}], "update_date": "2019-05-09", "authors_parsed": [["B\u00e2rsan", "Ioan Andrei", ""], ["Liu", "Peidong", ""], ["Pollefeys", "Marc", ""], ["Geiger", "Andreas", ""]]}, {"id": "1905.02793", "submitter": "Nils Gessert", "authors": "Nils Gessert and Thilo Sentker and Frederic Madesta and R\\\"udiger\n  Schmitz and Helge Kniep and Ivo Baltruschat and Ren\\'e Werner and Alexander\n  Schlaefer", "title": "Skin Lesion Classification Using CNNs with Patch-Based Attention and\n  Diagnosis-Guided Loss Weighting", "comments": "Accepted for publication in IEEE Transactions on Biomedical\n  Engineering", "journal-ref": null, "doi": "10.1109/TBME.2019.2915839", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Objective: This work addresses two key problems of skin lesion\nclassification. The first problem is the effective use of high-resolution\nimages with pretrained standard architectures for image classification. The\nsecond problem is the high class imbalance encountered in real-world\nmulti-class datasets. Methods: To use high-resolution images, we propose a\nnovel patch-based attention architecture that provides global context between\nsmall, high-resolution patches. We modify three pretrained architectures and\nstudy the performance of patch-based attention. To counter class imbalance\nproblems, we compare oversampling, balanced batch sampling, and class-specific\nloss weighting. Additionally, we propose a novel diagnosis-guided loss\nweighting method which takes the method used for ground-truth annotation into\naccount. Results: Our patch-based attention mechanism outperforms previous\nmethods and improves the mean sensitivity by 7%. Class balancing significantly\nimproves the mean sensitivity and we show that our diagnosis-guided loss\nweighting method improves the mean sensitivity by 3% over normal loss\nbalancing. Conclusion: The novel patch-based attention mechanism can be\nintegrated into pretrained architectures and provides global context between\nlocal patches while outperforming other patch-based methods. Hence, pretrained\narchitectures can be readily used with high-resolution images without\ndownsampling. The new diagnosis-guided loss weighting method outperforms other\nmethods and allows for effective training when facing class imbalance.\nSignificance: The proposed methods improve automatic skin lesion\nclassification. They can be extended to other clinical applications where\nhigh-resolution image data and class imbalance are relevant.\n", "versions": [{"version": "v1", "created": "Tue, 7 May 2019 20:04:20 GMT"}, {"version": "v2", "created": "Thu, 9 May 2019 09:27:40 GMT"}], "update_date": "2019-05-10", "authors_parsed": [["Gessert", "Nils", ""], ["Sentker", "Thilo", ""], ["Madesta", "Frederic", ""], ["Schmitz", "R\u00fcdiger", ""], ["Kniep", "Helge", ""], ["Baltruschat", "Ivo", ""], ["Werner", "Ren\u00e9", ""], ["Schlaefer", "Alexander", ""]]}, {"id": "1905.02822", "submitter": "Guanghan Ning", "authors": "Guanghan Ning, Heng Huang", "title": "LightTrack: A Generic Framework for Online Top-Down Human Pose Tracking", "comments": "9 pages, 6 figures, 6 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a novel effective light-weight framework, called\nLightTrack, for online human pose tracking. The proposed framework is designed\nto be generic for top-down pose tracking and is faster than existing online and\noffline methods. Single-person Pose Tracking (SPT) and Visual Object Tracking\n(VOT) are incorporated into one unified functioning entity, easily implemented\nby a replaceable single-person pose estimation module. Our framework unifies\nsingle-person pose tracking with multi-person identity association and sheds\nfirst light upon bridging keypoint tracking with object tracking. We also\npropose a Siamese Graph Convolution Network (SGCN) for human pose matching as a\nRe-ID module in our pose tracking system. In contrary to other Re-ID modules,\nwe use a graphical representation of human joints for matching. The\nskeleton-based representation effectively captures human pose similarity and is\ncomputationally inexpensive. It is robust to sudden camera shift that\nintroduces human drifting. To the best of our knowledge, this is the first\npaper to propose an online human pose tracking framework in a top-down fashion.\nThe proposed framework is general enough to fit other pose estimators and\ncandidate matching mechanisms. Our method outperforms other online methods\nwhile maintaining a much higher frame rate, and is very competitive with our\noffline state-of-the-art. We make the code publicly available at:\nhttps://github.com/Guanghan/lighttrack.\n", "versions": [{"version": "v1", "created": "Tue, 7 May 2019 22:02:00 GMT"}], "update_date": "2019-05-09", "authors_parsed": [["Ning", "Guanghan", ""], ["Huang", "Heng", ""]]}, {"id": "1905.02843", "submitter": "Venkateshwaran Balasubramanian", "authors": "Erkan Baser, Venkateshwaran Balasubramanian, Prarthana Bhattacharyya,\n  Krzysztof Czarnecki", "title": "FANTrack: 3D Multi-Object Tracking with Feature Association Network", "comments": "8 pages, 10 figures, IEEE Intelligent Vehicles Symposium (IV 19)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a data-driven approach to online multi-object tracking (MOT) that\nuses a convolutional neural network (CNN) for data association in a\ntracking-by-detection framework. The problem of multi-target tracking aims to\nassign noisy detections to a-priori unknown and time-varying number of tracked\nobjects across a sequence of frames. A majority of the existing solutions focus\non either tediously designing cost functions or formulating the task of data\nassociation as a complex optimization problem that can be solved effectively.\nInstead, we exploit the power of deep learning to formulate the data\nassociation problem as inference in a CNN. To this end, we propose to learn a\nsimilarity function that combines cues from both image and spatial features of\nobjects. Our solution learns to perform global assignments in 3D purely from\ndata, handles noisy detections and a varying number of targets, and is easy to\ntrain. We evaluate our approach on the challenging KITTI dataset and show\ncompetitive results. Our code is available at\nhttps://git.uwaterloo.ca/wise-lab/fantrack.\n", "versions": [{"version": "v1", "created": "Tue, 7 May 2019 23:26:03 GMT"}], "update_date": "2019-05-09", "authors_parsed": [["Baser", "Erkan", ""], ["Balasubramanian", "Venkateshwaran", ""], ["Bhattacharyya", "Prarthana", ""], ["Czarnecki", "Krzysztof", ""]]}, {"id": "1905.02845", "submitter": "Benyamin Ghojogh", "authors": "Benyamin Ghojogh, Maria N. Samad, Sayema Asif Mashhadi, Tania Kapoor,\n  Wahab Ali, Fakhri Karray, Mark Crowley", "title": "Feature Selection and Feature Extraction in Pattern Analysis: A\n  Literature Review", "comments": "14 pages, 1 figure, 2 tables, survey (literature review) paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pattern analysis often requires a pre-processing stage for extracting or\nselecting features in order to help the classification, prediction, or\nclustering stage discriminate or represent the data in a better way. The reason\nfor this requirement is that the raw data are complex and difficult to process\nwithout extracting or selecting appropriate features beforehand. This paper\nreviews theory and motivation of different common methods of feature selection\nand extraction and introduces some of their applications. Some numerical\nimplementations are also shown for these methods. Finally, the methods in\nfeature selection and extraction are compared.\n", "versions": [{"version": "v1", "created": "Tue, 7 May 2019 23:41:34 GMT"}], "update_date": "2019-05-09", "authors_parsed": [["Ghojogh", "Benyamin", ""], ["Samad", "Maria N.", ""], ["Mashhadi", "Sayema Asif", ""], ["Kapoor", "Tania", ""], ["Ali", "Wahab", ""], ["Karray", "Fakhri", ""], ["Crowley", "Mark", ""]]}, {"id": "1905.02848", "submitter": "Ashish Tawari", "authors": "Mingfei Gao, Ashish Tawari and Sujitha Martin", "title": "Goal-oriented Object Importance Estimation in On-road Driving Videos", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We formulate a new problem as Object Importance Estimation (OIE) in on-road\ndriving videos, where the road users are considered as important objects if\nthey have influence on the control decision of the ego-vehicle's driver. The\nimportance of a road user depends on both its visual dynamics, e.g.,\nappearance, motion and location, in the driving scene and the driving goal,\n\\emph{e.g}., the planned path, of the ego vehicle. We propose a novel framework\nthat incorporates both visual model and goal representation to conduct OIE. To\nevaluate our framework, we collect an on-road driving dataset at traffic\nintersections in the real world and conduct human-labeled annotation of the\nimportant objects. Experimental results show that our goal-oriented method\noutperforms baselines and has much more improvement on the left-turn and\nright-turn scenarios. Furthermore, we explore the possibility of using object\nimportance for driving control prediction and demonstrate that binary brake\nprediction can be improved with the information of object importance.\n", "versions": [{"version": "v1", "created": "Wed, 8 May 2019 00:07:26 GMT"}], "update_date": "2019-05-09", "authors_parsed": [["Gao", "Mingfei", ""], ["Tawari", "Ashish", ""], ["Martin", "Sujitha", ""]]}, {"id": "1905.02857", "submitter": "Peng Gao", "authors": "Peng Gao, Yipeng Ma, Ruyue Yuan, Liyi Xiao, Fei Wang", "title": "Learning Cascaded Siamese Networks for High Performance Visual Tracking", "comments": "Accepted for IEEE 26th International Conference on Image Processing\n  (ICIP 2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual tracking is one of the most challenging computer vision problems. In\norder to achieve high performance visual tracking in various negative\nscenarios, a novel cascaded Siamese network is proposed and developed based on\ntwo different deep learning networks: a matching subnetwork and a\nclassification subnetwork. The matching subnetwork is a fully convolutional\nSiamese network. According to the similarity score between the exemplar image\nand the candidate image, it aims to search possible object positions and crop\nscaled candidate patches. The classification subnetwork is designed to further\nevaluate the cropped candidate patches and determine the optimal tracking\nresults based on the classification score. The matching subnetwork is trained\noffline and fixed online, while the classification subnetwork performs\nstochastic gradient descent online to learn more target-specific information.\nTo improve the tracking performance further, an effective classification\nsubnetwork update method based on both similarity and classification scores is\nutilized for updating the classification subnetwork. Extensive experimental\nresults demonstrate that our proposed approach achieves state-of-the-art\nperformance in recent benchmarks.\n", "versions": [{"version": "v1", "created": "Wed, 8 May 2019 01:06:23 GMT"}], "update_date": "2019-05-09", "authors_parsed": [["Gao", "Peng", ""], ["Ma", "Yipeng", ""], ["Yuan", "Ruyue", ""], ["Xiao", "Liyi", ""], ["Wang", "Fei", ""]]}, {"id": "1905.02876", "submitter": "Giorgos Bouritsas", "authors": "Giorgos Bouritsas, Sergiy Bokhnyak, Stylianos Ploumpis, Michael\n  Bronstein, Stefanos Zafeiriou", "title": "Neural 3D Morphable Models: Spiral Convolutional Networks for 3D Shape\n  Representation Learning and Generation", "comments": "to appear at ICCV 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative models for 3D geometric data arise in many important applications\nin 3D computer vision and graphics. In this paper, we focus on 3D deformable\nshapes that share a common topological structure, such as human faces and\nbodies. Morphable Models and their variants, despite their linear formulation,\nhave been widely used for shape representation, while most of the recently\nproposed nonlinear approaches resort to intermediate representations, such as\n3D voxel grids or 2D views. In this work, we introduce a novel graph\nconvolutional operator, acting directly on the 3D mesh, that explicitly models\nthe inductive bias of the fixed underlying graph. This is achieved by enforcing\nconsistent local orderings of the vertices of the graph, through the spiral\noperator, thus breaking the permutation invariance property that is adopted by\nall the prior work on Graph Neural Networks. Our operator comes by construction\nwith desirable properties (anisotropic, topology-aware, lightweight,\neasy-to-optimise), and by using it as a building block for traditional deep\ngenerative architectures, we demonstrate state-of-the-art results on a variety\nof 3D shape datasets compared to the linear Morphable Model and other graph\nconvolutional operators.\n", "versions": [{"version": "v1", "created": "Wed, 8 May 2019 02:37:27 GMT"}, {"version": "v2", "created": "Thu, 9 May 2019 21:14:27 GMT"}, {"version": "v3", "created": "Sat, 3 Aug 2019 00:14:45 GMT"}], "update_date": "2019-08-06", "authors_parsed": [["Bouritsas", "Giorgos", ""], ["Bokhnyak", "Sergiy", ""], ["Ploumpis", "Stylianos", ""], ["Bronstein", "Michael", ""], ["Zafeiriou", "Stefanos", ""]]}, {"id": "1905.02882", "submitter": "Haibin Huang", "authors": "Yifan Ding, Chuan Wang, Haibin Huang, Jiaming Liu, Jue Wang, Liqiang\n  Wang", "title": "Frame-Recurrent Video Inpainting by Robust Optical Flow Inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a new inpainting framework for recovering missing\nregions of video frames. Compared with image inpainting, performing this task\non video presents new challenges such as how to preserving temporal consistency\nand spatial details, as well as how to handle arbitrary input video size and\nlength fast and efficiently. Towards this end, we propose a novel deep learning\narchitecture which incorporates ConvLSTM and optical flow for modeling the\nspatial-temporal consistency in videos. It also saves much computational\nresource such that our method can handle videos with larger frame size and\narbitrary length streamingly in real-time. Furthermore, to generate an accurate\noptical flow from corrupted frames, we propose a robust flow generation module,\nwhere two sources of flows are fed and a flow blending network is trained to\nfuse them. We conduct extensive experiments to evaluate our method in various\nscenarios and different datasets, both qualitatively and quantitatively. The\nexperimental results demonstrate the superior of our method compared with the\nstate-of-the-art inpainting approaches.\n", "versions": [{"version": "v1", "created": "Wed, 8 May 2019 03:13:54 GMT"}], "update_date": "2019-05-09", "authors_parsed": [["Ding", "Yifan", ""], ["Wang", "Chuan", ""], ["Huang", "Haibin", ""], ["Liu", "Jiaming", ""], ["Wang", "Jue", ""], ["Wang", "Liqiang", ""]]}, {"id": "1905.02884", "submitter": "Rui Xu", "authors": "Rui Xu, Xiaoxiao Li, Bolei Zhou, Chen Change Loy", "title": "Deep Flow-Guided Video Inpainting", "comments": "cvpr'19", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video inpainting, which aims at filling in missing regions of a video,\nremains challenging due to the difficulty of preserving the precise spatial and\ntemporal coherence of video contents. In this work we propose a novel\nflow-guided video inpainting approach. Rather than filling in the RGB pixels of\neach frame directly, we consider video inpainting as a pixel propagation\nproblem. We first synthesize a spatially and temporally coherent optical flow\nfield across video frames using a newly designed Deep Flow Completion network.\nThen the synthesized flow field is used to guide the propagation of pixels to\nfill up the missing regions in the video. Specifically, the Deep Flow\nCompletion network follows a coarse-to-fine refinement to complete the flow\nfields, while their quality is further improved by hard flow example mining.\nFollowing the guide of the completed flow, the missing video regions can be\nfilled up precisely. Our method is evaluated on DAVIS and YouTube-VOS datasets\nqualitatively and quantitatively, achieving the state-of-the-art performance in\nterms of inpainting quality and speed.\n", "versions": [{"version": "v1", "created": "Wed, 8 May 2019 03:27:15 GMT"}], "update_date": "2019-05-09", "authors_parsed": [["Xu", "Rui", ""], ["Li", "Xiaoxiao", ""], ["Zhou", "Bolei", ""], ["Loy", "Chen Change", ""]]}, {"id": "1905.02899", "submitter": "Yuma Kinoshita", "authors": "Yuma Kinoshita and Hitoshi Kiya", "title": "Convolutional Neural Networks Considering Local and Global features for\n  Image Enhancement", "comments": "To appear in Proc. ICIP2019. arXiv admin note: text overlap with\n  arXiv:1901.05686", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a novel convolutional neural network (CNN)\narchitecture considering both local and global features for image enhancement.\nMost conventional image enhancement methods, including Retinex-based methods,\ncannot restore lost pixel values caused by clipping and quantizing. CNN-based\nmethods have recently been proposed to solve the problem, but they still have a\nlimited performance due to network architectures not handling global features.\nTo handle both local and global features, the proposed architecture consists of\nthree networks: a local encoder, a global encoder, and a decoder. In addition,\nhigh dynamic range (HDR) images are used for generating training data for our\nnetworks. The use of HDR images makes it possible to train CNNs with\nbetter-quality images than images directly captured with cameras. Experimental\nresults show that the proposed method can produce higher-quality images than\nconventional image enhancement methods including CNN-based methods, in terms of\nvarious objective quality metrics: TMQI, entropy, NIQE, and BRISQUE.\n", "versions": [{"version": "v1", "created": "Tue, 7 May 2019 08:20:30 GMT"}], "update_date": "2019-05-09", "authors_parsed": [["Kinoshita", "Yuma", ""], ["Kiya", "Hitoshi", ""]]}, {"id": "1905.02906", "submitter": "Jaehwan Lee", "authors": "Jaehwan Lee and Donggeon Yoo and Jung Yin Huh and Hyo-Eun Kim", "title": "Photometric Transformer Networks and Label Adjustment for Breast Density\n  Prediction", "comments": "miccai 2019 submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Grading breast density is highly sensitive to normalization settings of\ndigital mammogram as the density is tightly correlated with the distribution of\npixel intensity. Also, the grade varies with readers due to uncertain grading\ncriteria. These issues are inherent in the density assessment of digital\nmammography. They are problematic when designing a computer-aided prediction\nmodel for breast density and become worse if the data comes from multiple\nsites. In this paper, we proposed two novel deep learning techniques for breast\ndensity prediction: 1) photometric transformation which adaptively normalizes\nthe input mammograms, and 2) label distillation which adjusts the label by\nusing its output prediction. The photometric transformer network predicts\noptimal parameters for photometric transformation on the fly, learned jointly\nwith the main prediction network. The label distillation, a type of\npseudo-label techniques, is intended to mitigate the grading variation. We\nexperimentally showed that the proposed methods are beneficial in terms of\nbreast density prediction, resulting in significant performance improvement\ncompared to various previous approaches.\n", "versions": [{"version": "v1", "created": "Wed, 8 May 2019 04:32:34 GMT"}], "update_date": "2019-05-09", "authors_parsed": [["Lee", "Jaehwan", ""], ["Yoo", "Donggeon", ""], ["Huh", "Jung Yin", ""], ["Kim", "Hyo-Eun", ""]]}, {"id": "1905.02925", "submitter": "Panos Achlioptas", "authors": "Panos Achlioptas, Judy Fan, Robert X.D. Hawkins, Noah D. Goodman,\n  Leonidas J. Guibas", "title": "ShapeGlot: Learning Language for Shape Differentiation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we explore how fine-grained differences between the shapes of\ncommon objects are expressed in language, grounded on images and 3D models of\nthe objects. We first build a large scale, carefully controlled dataset of\nhuman utterances that each refers to a 2D rendering of a 3D CAD model so as to\ndistinguish it from a set of shape-wise similar alternatives. Using this\ndataset, we develop neural language understanding (listening) and production\n(speaking) models that vary in their grounding (pure 3D forms via point-clouds\nvs. rendered 2D images), the degree of pragmatic reasoning captured (e.g.\nspeakers that reason about a listener or not), and the neural architecture\n(e.g. with or without attention). We find models that perform well with both\nsynthetic and human partners, and with held out utterances and objects. We also\nfind that these models are amenable to zero-shot transfer learning to novel\nobject classes (e.g. transfer from training on chairs to testing on lamps), as\nwell as to real-world images drawn from furniture catalogs. Lesion studies\nindicate that the neural listeners depend heavily on part-related words and\nassociate these words correctly with visual parts of objects (without any\nexplicit network training on object parts), and that transfer to novel classes\nis most successful when known part-words are available. This work illustrates a\npractical approach to language grounding, and provides a case study in the\nrelationship between object shape and linguistic structure when it comes to\nobject differentiation.\n", "versions": [{"version": "v1", "created": "Wed, 8 May 2019 06:01:33 GMT"}], "update_date": "2019-05-09", "authors_parsed": [["Achlioptas", "Panos", ""], ["Fan", "Judy", ""], ["Hawkins", "Robert X. D.", ""], ["Goodman", "Noah D.", ""], ["Guibas", "Leonidas J.", ""]]}, {"id": "1905.02949", "submitter": "Dahun Kim", "authors": "Dahun Kim, Sanghyun Woo, Joon-Young Lee, In So Kweon", "title": "Deep Blind Video Decaptioning by Temporal Aggregation and Recurrence", "comments": "Accepted at CVPR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Blind video decaptioning is a problem of automatically removing text overlays\nand inpainting the occluded parts in videos without any input masks. While\nrecent deep learning based inpainting methods deal with a single image and\nmostly assume that the positions of the corrupted pixels are known, we aim at\nautomatic text removal in video sequences without mask information. In this\npaper, we propose a simple yet effective framework for fast blind video\ndecaptioning. We construct an encoder-decoder model, where the encoder takes\nmultiple source frames that can provide visible pixels revealed from the scene\ndynamics. These hints are aggregated and fed into the decoder. We apply a\nresidual connection from the input frame to the decoder output to enforce our\nnetwork to focus on the corrupted regions only. Our proposed model was ranked\nin the first place in the ECCV Chalearn 2018 LAP Inpainting Competition Track2:\nVideo decaptioning. In addition, we further improve this strong model by\napplying a recurrent feedback. The recurrent feedback not only enforces\ntemporal coherence but also provides strong clues on where the corrupted pixels\nare. Both qualitative and quantitative experiments demonstrate that our full\nmodel produces accurate and temporally consistent video results in real time\n(50+ fps).\n", "versions": [{"version": "v1", "created": "Wed, 8 May 2019 08:04:35 GMT"}], "update_date": "2019-05-09", "authors_parsed": [["Kim", "Dahun", ""], ["Woo", "Sanghyun", ""], ["Lee", "Joon-Young", ""], ["Kweon", "In So", ""]]}, {"id": "1905.02963", "submitter": "Liang Sun", "authors": "Liang Sun, Bing Li, Chunfeng Yuan, Zhengjun Zha, Weiming Hu", "title": "Multimodal Semantic Attention Network for Video Captioning", "comments": "6 pages, 4 figures, accepted by IEEE International Conference on\n  Multimedia and Expo (ICME) 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inspired by the fact that different modalities in videos carry complementary\ninformation, we propose a Multimodal Semantic Attention Network(MSAN), which is\na new encoder-decoder framework incorporating multimodal semantic attributes\nfor video captioning. In the encoding phase, we detect and generate multimodal\nsemantic attributes by formulating it as a multi-label classification problem.\nMoreover, we add auxiliary classification loss to our model that can obtain\nmore effective visual features and high-level multimodal semantic attribute\ndistributions for sufficient video encoding. In the decoding phase, we extend\neach weight matrix of the conventional LSTM to an ensemble of\nattribute-dependent weight matrices, and employ attention mechanism to pay\nattention to different attributes at each time of the captioning process. We\nevaluate algorithm on two popular public benchmarks: MSVD and MSR-VTT,\nachieving competitive results with current state-of-the-art across six\nevaluation metrics.\n", "versions": [{"version": "v1", "created": "Wed, 8 May 2019 08:51:12 GMT"}], "update_date": "2019-05-09", "authors_parsed": [["Sun", "Liang", ""], ["Li", "Bing", ""], ["Yuan", "Chunfeng", ""], ["Zha", "Zhengjun", ""], ["Hu", "Weiming", ""]]}, {"id": "1905.03003", "submitter": "Daniel Sanchez", "authors": "Daniel S\\'anchez, Marc Oliu, Meysam Madadi, Xavier Bar\\'o, Sergio\n  Escalera", "title": "Multi-task human analysis in still images: 2D/3D pose, depth map, and\n  multi-part segmentation", "comments": "8 pages, 4 Figures, 5 Tables, Conference Faces and Gestures 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While many individual tasks in the domain of human analysis have recently\nreceived an accuracy boost from deep learning approaches, multi-task learning\nhas mostly been ignored due to a lack of data. New synthetic datasets are being\nreleased, filling this gap with synthetic generated data. In this work, we\nanalyze four related human analysis tasks in still images in a multi-task\nscenario by leveraging such datasets. Specifically, we study the correlation of\n2D/3D pose estimation, body part segmentation and full-body depth estimation.\nThese tasks are learned via the well-known Stacked Hourglass module such that\neach of the task-specific streams shares information with the others. The main\ngoal is to analyze how training together these four related tasks can benefit\neach individual task for a better generalization. Results on the newly released\nSURREAL dataset show that all four tasks benefit from the multi-task approach,\nbut with different combinations of tasks: while combining all four tasks\nimproves 2D pose estimation the most, 2D pose improves neither 3D pose nor\nfull-body depth estimation. On the other hand 2D parts segmentation can benefit\nfrom 2D pose but not from 3D pose. In all cases, as expected, the maximum\nimprovement is achieved on those human body parts that show more variability in\nterms of spatial distribution, appearance and shape, e.g. wrists and ankles.\n", "versions": [{"version": "v1", "created": "Wed, 8 May 2019 10:55:02 GMT"}], "update_date": "2019-05-09", "authors_parsed": [["S\u00e1nchez", "Daniel", ""], ["Oliu", "Marc", ""], ["Madadi", "Meysam", ""], ["Bar\u00f3", "Xavier", ""], ["Escalera", "Sergio", ""]]}, {"id": "1905.03017", "submitter": "Magnus Gedda", "authors": "Magnus Gedda", "title": "Algorithms for Grey-Weighted Distance Computations", "comments": "16 pages, preprint submitted for journal publication, published in\n  printed phd thesis", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the increasing size of datasets and demand for real time response for\ninteractive applications, improving runtime for algorithms with excessive\ncomputational requirements has become increasingly important. Many different\nalgorithms combining efficient priority queues with various helper structures\nhave been proposed for computing grey-weighted distance transforms. Here we\ncompare the performance of popular competitive algorithms in different\nscenarios to form practical guidelines easy to adopt. The label-setting\ncategory of algorithms is shown to be the best choice for all scenarios. The\nhierarchical heap with a pointer array to keep track of nodes on the heap is\nshown to be the best choice as priority queue. However, if memory is a critical\nissue, then the best choice is the Dial priority queue for integer valued costs\nand the Untidy priority queue for real valued costs.\n", "versions": [{"version": "v1", "created": "Wed, 8 May 2019 11:53:45 GMT"}], "update_date": "2019-05-09", "authors_parsed": [["Gedda", "Magnus", ""]]}, {"id": "1905.03021", "submitter": "Xingbo Dong", "authors": "Xingbo Dong, Zhe Jin, Andrew Teoh Beng Jin", "title": "A Genetic Algorithm Enabled Similarity-Based Attack on Cancellable\n  Biometrics", "comments": "7 pages, 4 figures, for BTAS 2019", "journal-ref": "10th IEEE International Conference on Biometrics: Theory,\n  Applications and Systems (BTAS),23-26 September 2019, Tampa, Florida", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cancellable biometrics (CB) as a means for biometric template protection\napproach refers to an irreversible yet similarity preserving transformation on\nthe original template. With similarity preserving property, the matching\nbetween template and query instance can be performed in the transform domain\nwithout jeopardizing accuracy performance. Unfortunately, this trait invites a\nclass of attack, namely similarity-based attack (SA). SA produces a preimage,\nan inverse of transformed template, which can be exploited for impersonation\nand cross-matching. In this paper, we propose a Genetic Algorithm enabled\nsimilarity-based attack framework (GASAF) to demonstrate that CB schemes whose\npossess similarity preserving property are highly vulnerable to\nsimilarity-based attack. Besides that, a set of new metrics is designed to\nmeasure the effectiveness of the similarity-based attack. We conduct the\nexperiment on two representative CB schemes, i.e. BioHashing and Bloom-filter.\nThe experimental results attest the vulnerability under this type of attack.\n", "versions": [{"version": "v1", "created": "Wed, 8 May 2019 12:03:39 GMT"}, {"version": "v2", "created": "Sun, 15 Sep 2019 16:09:26 GMT"}], "update_date": "2019-09-17", "authors_parsed": [["Dong", "Xingbo", ""], ["Jin", "Zhe", ""], ["Jin", "Andrew Teoh Beng", ""]]}, {"id": "1905.03023", "submitter": "Panagiotis Kouzouglidis", "authors": "Panagiotis Kouzouglidis, Giorgos Sfikas, Christophoros Nikou", "title": "Automatic Video Colorization using 3D Conditional Generative Adversarial\n  Networks", "comments": "5 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this work, we present a method for automatic colorization of grayscale\nvideos. The core of the method is a Generative Adversarial Network that is\ntrained and tested on sequences of frames in a sliding window manner. Network\nconvolutional and deconvolutional layers are three-dimensional, with frame\nheight, width and time as the dimensions taken into account. Multiple\nchrominance estimates per frame are aggregated and combined with available\nluminance information to recreate a colored sequence. Colorization trials are\nrun succesfully on a dataset of old black-and-white films. The usefulness of\nour method is also validated with numerical results, computed with a newly\nproposed metric that measures colorization consistency over a frame sequence.\n", "versions": [{"version": "v1", "created": "Wed, 8 May 2019 12:06:29 GMT"}], "update_date": "2019-05-09", "authors_parsed": [["Kouzouglidis", "Panagiotis", ""], ["Sfikas", "Giorgos", ""], ["Nikou", "Christophoros", ""]]}, {"id": "1905.03026", "submitter": "Ivo Matteo Baltruschat", "authors": "Ivo Matteo Baltruschat and Patryk Szwargulski and Florian Griese and\n  Mirco Grosser and Ren\\'e Werner and Tobias Knopp", "title": "3d-SMRnet: Achieving a new quality of MPI system matrix recovery by deep\n  learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Magnetic particle imaging (MPI) data is commonly reconstructed using a system\nmatrix acquired in a time-consuming calibration measurement. The calibration\napproach has the important advantage over model-based reconstruction that it\ntakes the complex particle physics as well as system imperfections into\naccount. This benefit comes for the cost that the system matrix needs to be\nre-calibrated whenever the scan parameters, particle types or even the particle\nenvironment (e.g. viscosity or temperature) changes. One route for reducing the\ncalibration time is the sampling of the system matrix at a subset of the\nspatial positions of the intended field-of-view and employing system matrix\nrecovery. Recent approaches used compressed sensing (CS) and achieved\nsubsampling factors up to 28 that still allowed reconstructing MPI images of\nsufficient quality. In this work, we propose a novel framework with a 3d-System\nMatrix Recovery Network and demonstrate it to recover a 3d system matrix with a\nsubsampling factor of 64 in less than one minute and to outperform CS in terms\nof system matrix quality, reconstructed image quality, and processing time. The\nadvantage of our method is demonstrated by reconstructing open access MPI\ndatasets. The model is further shown to be capable of inferring system matrices\nfor different particle types.\n", "versions": [{"version": "v1", "created": "Wed, 8 May 2019 12:21:39 GMT"}], "update_date": "2019-05-09", "authors_parsed": [["Baltruschat", "Ivo Matteo", ""], ["Szwargulski", "Patryk", ""], ["Griese", "Florian", ""], ["Grosser", "Mirco", ""], ["Werner", "Ren\u00e9", ""], ["Knopp", "Tobias", ""]]}, {"id": "1905.03046", "submitter": "Peter Meltzer", "authors": "Peter Meltzer, Marcelo Daniel Gutierrez Mallea and Peter J. Bentley", "title": "PiNet: A Permutation Invariant Graph Neural Network for Graph\n  Classification", "comments": "7 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an end-to-end deep learning learning model for graph\nclassification and representation learning that is invariant to permutation of\nthe nodes of the input graphs. We address the challenge of learning a fixed\nsize graph representation for graphs of varying dimensions through a\ndifferentiable node attention pooling mechanism. In addition to a theoretical\nproof of its invariance to permutation, we provide empirical evidence\ndemonstrating the statistically significant gain in accuracy when faced with an\nisomorphic graph classification task given only a small number of training\nexamples. We analyse the effect of four different matrices to facilitate the\nlocal message passing mechanism by which graph convolutions are performed vs. a\nmatrix parametrised by a learned parameter pair able to transition smoothly\nbetween the former. Finally, we show that our model achieves competitive\nclassification performance with existing techniques on a set of molecule\ndatasets.\n", "versions": [{"version": "v1", "created": "Wed, 8 May 2019 12:51:52 GMT"}], "update_date": "2019-05-09", "authors_parsed": [["Meltzer", "Peter", ""], ["Mallea", "Marcelo Daniel Gutierrez", ""], ["Bentley", "Peter J.", ""]]}, {"id": "1905.03066", "submitter": "Manuel Herzog", "authors": "Manuel Herzog, Klaus Dietmayer", "title": "Training a Fast Object Detector for LiDAR Range Images Using Labeled\n  Data from Sensors with Higher Resolution", "comments": null, "journal-ref": "2019 IEEE Intelligent Transportation Systems Conference (ITSC),\n  Auckland, New Zealand, 2019, pp. 2707-2713", "doi": "10.1109/ITSC.2019.8917011", "report-no": null, "categories": "cs.CV cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we describe a strategy for training neural networks for object\ndetection in range images obtained from one type of LiDAR sensor using labeled\ndata from a different type of LiDAR sensor. Additionally, an efficient model\nfor object detection in range images for use in self-driving cars is presented.\nCurrently, the highest performing algorithms for object detection from LiDAR\nmeasurements are based on neural networks. Training these networks using\nsupervised learning requires large annotated datasets. Therefore, most research\nusing neural networks for object detection from LiDAR point clouds is conducted\non a very small number of publicly available datasets. Consequently, only a\nsmall number of sensor types are used. We use an existing annotated dataset to\ntrain a neural network that can be used with a LiDAR sensor that has a lower\nresolution than the one used for recording the annotated dataset. This is done\nby simulating data from the lower resolution LiDAR sensor based on the higher\nresolution dataset. Furthermore, improvements to models that use LiDAR range\nimages for object detection are presented. The results are validated using both\nsimulated sensor data and data from an actual lower resolution sensor mounted\nto a research vehicle. It is shown that the model can detect objects from\n360{\\deg} range images in real time.\n", "versions": [{"version": "v1", "created": "Wed, 8 May 2019 13:43:03 GMT"}, {"version": "v2", "created": "Mon, 22 Jul 2019 08:14:08 GMT"}, {"version": "v3", "created": "Thu, 5 Dec 2019 14:03:40 GMT"}], "update_date": "2019-12-06", "authors_parsed": [["Herzog", "Manuel", ""], ["Dietmayer", "Klaus", ""]]}, {"id": "1905.03079", "submitter": "Timo Bolkart", "authors": "Daniel Cudeiro, Timo Bolkart, Cassidy Laidlaw, Anurag Ranjan, Michael\n  J. Black", "title": "Capture, Learning, and Synthesis of 3D Speaking Styles", "comments": "To appear in CVPR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Audio-driven 3D facial animation has been widely explored, but achieving\nrealistic, human-like performance is still unsolved. This is due to the lack of\navailable 3D datasets, models, and standard evaluation metrics. To address\nthis, we introduce a unique 4D face dataset with about 29 minutes of 4D scans\ncaptured at 60 fps and synchronized audio from 12 speakers. We then train a\nneural network on our dataset that factors identity from facial motion. The\nlearned model, VOCA (Voice Operated Character Animation) takes any speech\nsignal as input - even speech in languages other than English - and\nrealistically animates a wide range of adult faces. Conditioning on subject\nlabels during training allows the model to learn a variety of realistic\nspeaking styles. VOCA also provides animator controls to alter speaking style,\nidentity-dependent facial shape, and pose (i.e. head, jaw, and eyeball\nrotations) during animation. To our knowledge, VOCA is the only realistic 3D\nfacial animation model that is readily applicable to unseen subjects without\nretargeting. This makes VOCA suitable for tasks like in-game video, virtual\nreality avatars, or any scenario in which the speaker, speech, or language is\nnot known in advance. We make the dataset and model available for research\npurposes at http://voca.is.tue.mpg.de.\n", "versions": [{"version": "v1", "created": "Wed, 8 May 2019 14:16:37 GMT"}], "update_date": "2019-05-09", "authors_parsed": [["Cudeiro", "Daniel", ""], ["Bolkart", "Timo", ""], ["Laidlaw", "Cassidy", ""], ["Ranjan", "Anurag", ""], ["Black", "Michael J.", ""]]}, {"id": "1905.03100", "submitter": "Per Rutquist", "authors": "Per Rutquist", "title": "Unsupervised Learning through Temporal Smoothing and Entropy\n  Maximization", "comments": "This paper has been submitted to the 58th IEEE Conference on Decision\n  and Control (CDC2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a method for machine learning from unlabeled data in the\nform of a time-series. The mapping that is learned is shown to extract slowly\nevolving information that would be useful for control applications, while\nefficiently filtering out unwanted, higher-frequency noise.\n  The method consists of training a feedforward artificial neural network with\nbackpropagation using two opposing objectives.\n  The first of these is to minimize the squared changes in activations between\ntime steps of each unit in the network. This \"temporal smoothing\" has the\neffect of correlating inputs that occur close in time with outputs that are\nclose in the L2-norm.\n  The second objective is to maximize the log determinant of the covariance\nmatrix of activations in each layer of the network. This objective ensures that\ninformation from each layer is passed through to the next. This second\nobjective acts as a balance to the first, which on its own would result in a\nnetwork with all input weights equal to zero.\n", "versions": [{"version": "v1", "created": "Wed, 8 May 2019 14:37:38 GMT"}], "update_date": "2019-05-09", "authors_parsed": [["Rutquist", "Per", ""]]}, {"id": "1905.03105", "submitter": "Henry Howard-Jenkins", "authors": "Henry Howard-Jenkins, Shuda Li, Victor Prisacariu", "title": "Thinking Outside the Box: Generation of Unconstrained 3D Room Layouts", "comments": "Asian Conference on Computer Vision (ACCV), 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a method for room layout estimation that does not rely on the\ntypical box approximation or Manhattan world assumption. Instead, we\nreformulate the geometry inference problem as an instance detection task, which\nwe solve by directly regressing 3D planes using an R-CNN. We then use a variant\nof probabilistic clustering to combine the 3D planes regressed at each frame in\na video sequence, with their respective camera poses, into a single global 3D\nroom layout estimate. Finally, we showcase results which make no assumptions\nabout perpendicular alignment, so can deal effectively with walls in any\nalignment.\n", "versions": [{"version": "v1", "created": "Wed, 8 May 2019 14:43:26 GMT"}], "update_date": "2019-05-09", "authors_parsed": [["Howard-Jenkins", "Henry", ""], ["Li", "Shuda", ""], ["Prisacariu", "Victor", ""]]}, {"id": "1905.03109", "submitter": "Amir Vajdi", "authors": "Amir Vajdi, Mohammad Reza Zaghian, Saman Farahmand, Elham Rastegar,\n  Kian Maroofi, Shaohua Jia, Marc Pomplun, Nurit Haspel, Akram Bayat", "title": "Human Gait Database for Normal Walk Collected by Smart Phone\n  Accelerometer", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.CV eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The goal of this study is to introduce a comprehensive gait database of 93\nhuman subjects who walked between two endpoints during two different sessions\nand record their gait data using two smartphones, one was attached to the right\nthigh and another one on the left side of the waist. This data is collected\nwith the intention to be utilized by a deep learning-based method which\nrequires enough time points. The metadata including age, gender, smoking, daily\nexercise time, height, and weight of an individual is recorded. this data set\nis publicly available.\n", "versions": [{"version": "v1", "created": "Sat, 4 May 2019 22:35:09 GMT"}, {"version": "v2", "created": "Tue, 18 Feb 2020 01:07:34 GMT"}], "update_date": "2020-02-19", "authors_parsed": [["Vajdi", "Amir", ""], ["Zaghian", "Mohammad Reza", ""], ["Farahmand", "Saman", ""], ["Rastegar", "Elham", ""], ["Maroofi", "Kian", ""], ["Jia", "Shaohua", ""], ["Pomplun", "Marc", ""], ["Haspel", "Nurit", ""], ["Bayat", "Akram", ""]]}, {"id": "1905.03198", "submitter": "Anis Koubaa", "authors": "Bilel Benjdira, Yakoub Bazi, Anis Koubaa, Kais Ouni", "title": "Unsupervised Domain Adaptation using Generative Adversarial Networks for\n  Semantic Segmentation of Aerial Images", "comments": "submitted to a journal", "journal-ref": "MDPI Remote Sensing, Volume 11, Issue 11, 2019", "doi": "10.3390/rs11111369", "report-no": "RIOTU-TR07", "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Segmenting aerial images is being of great potential in surveillance and\nscene understanding of urban areas. It provides a mean for automatic reporting\nof the different events that happen in inhabited areas. This remarkably\npromotes public safety and traffic management applications. After the wide\nadoption of convolutional neural networks methods, the accuracy of semantic\nsegmentation algorithms could easily surpass 80% if a robust dataset is\nprovided. Despite this success, the deployment of a pre-trained segmentation\nmodel to survey a new city that is not included in the training set\nsignificantly decreases the accuracy. This is due to the domain shift between\nthe source dataset on which the model is trained and the new target domain of\nthe new city images. In this paper, we address this issue and consider the\nchallenge of domain adaptation in semantic segmentation of aerial images. We\ndesign an algorithm that reduces the domain shift impact using Generative\nAdversarial Networks (GANs). In the experiments, we test the proposed\nmethodology on the International Society for Photogrammetry and Remote Sensing\n(ISPRS) semantic segmentation dataset and found that our method improves the\noverall accuracy from 35% to 52% when passing from Potsdam domain (considered\nas source domain) to Vaihingen domain (considered as target domain). In\naddition, the method allows recovering efficiently the inverted classes due to\nsensor variation. In particular, it improves the average segmentation accuracy\nof the inverted classes due to sensor variation from 14% to 61%.\n", "versions": [{"version": "v1", "created": "Wed, 8 May 2019 16:35:02 GMT"}], "update_date": "2019-06-10", "authors_parsed": [["Benjdira", "Bilel", ""], ["Bazi", "Yakoub", ""], ["Koubaa", "Anis", ""], ["Ouni", "Kais", ""]]}, {"id": "1905.03209", "submitter": "Sharib Ali Dr.", "authors": "Sharib Ali, Felix Zhou, Christian Daul, Barbara Braden, Adam Bailey,\n  Stefano Realdon, James East, Georges Wagni\\`eres, Victor Loschenov, Enrico\n  Grisan, Walter Blondel, Jens Rittscher", "title": "Endoscopy artifact detection (EAD 2019) challenge dataset", "comments": "12 pages, EAD2019 dataset description", "journal-ref": null, "doi": "10.17632/C7FJBXCGJ9.1", "report-no": null, "categories": "cs.CV cs.AI cs.LG eess.IV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Endoscopic artifacts are a core challenge in facilitating the diagnosis and\ntreatment of diseases in hollow organs. Precise detection of specific artifacts\nlike pixel saturations, motion blur, specular reflections, bubbles and debris\nis essential for high-quality frame restoration and is crucial for realizing\nreliable computer-assisted tools for improved patient care. At present most\nvideos in endoscopy are currently not analyzed due to the abundant presence of\nmulti-class artifacts in video frames. Through the endoscopic artifact\ndetection (EAD 2019) challenge, we address this key bottleneck problem by\nsolving the accurate identification and localization of endoscopic frame\nartifacts to enable further key quantitative analysis of unusable video frames\nsuch as mosaicking and 3D reconstruction which is crucial for delivering\nimproved patient care. This paper summarizes the challenge tasks and describes\nthe dataset and evaluation criteria established in the EAD 2019 challenge.\n", "versions": [{"version": "v1", "created": "Wed, 8 May 2019 16:53:14 GMT"}], "update_date": "2019-05-09", "authors_parsed": [["Ali", "Sharib", ""], ["Zhou", "Felix", ""], ["Daul", "Christian", ""], ["Braden", "Barbara", ""], ["Bailey", "Adam", ""], ["Realdon", "Stefano", ""], ["East", "James", ""], ["Wagni\u00e8res", "Georges", ""], ["Loschenov", "Victor", ""], ["Grisan", "Enrico", ""], ["Blondel", "Walter", ""], ["Rittscher", "Jens", ""]]}, {"id": "1905.03219", "submitter": "Priyadarshini Panda", "authors": "Priyadarshini Panda, Efstathia Soufleri, and Kaushik Roy", "title": "Evaluating the Stability of Recurrent Neural Models during Training with\n  Eigenvalue Spectra Analysis", "comments": "Accepted in IJCNN 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We analyze the stability of recurrent networks, specifically, reservoir\ncomputing models during training by evaluating the eigenvalue spectra of the\nreservoir dynamics. To circumvent the instability arising in examining a closed\nloop reservoir system with feedback, we propose to break the closed loop\nsystem. Essentially, we unroll the reservoir dynamics over time while\nincorporating the feedback effects that preserve the overall temporal integrity\nof the system. We evaluate our methodology for fixed point and time varying\ntargets with least squares regression and FORCE training, respectively. Our\nanalysis establishes eigenvalue spectra (which is, shrinking of spectral circle\nas training progresses) as a valid and effective metric to gauge the\nconvergence of training as well as the convergence of the chaotic activity of\nthe reservoir toward stable states.\n", "versions": [{"version": "v1", "created": "Wed, 8 May 2019 17:12:51 GMT"}], "update_date": "2019-05-09", "authors_parsed": [["Panda", "Priyadarshini", ""], ["Soufleri", "Efstathia", ""], ["Roy", "Kaushik", ""]]}, {"id": "1905.03229", "submitter": "Yu Li", "authors": "Yu Li, Hu Wang, Wenquan Shuai, Honghao Zhang, Yong Peng", "title": "Image-based reconstruction for the impact problems by using DPNNs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.OH cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the improvement of the pattern recognition and feature extraction of\nDeep Neural Networks (DPNNs), image-based design and optimization have been\nwidely used in multidisciplinary researches. Recently, a Reconstructive Neural\nNetwork (ReConNN) has been proposed to obtain an image-based model from an\nanalysis-based model [1, 2], and a steady-state heat transfer of a heat sink\nhas been successfully reconstructed. Commonly, this method is suitable to\nhandle stable-state problems. However, it has difficulties handling nonlinear\ntransient impact problems, due to the bottlenecks of the Deep Neural Network\n(DPNN). For example, nonlinear transient problems make it difficult for the\nGenerative Adversarial Network (GAN) to generate various reasonable images.\nTherefore, in this study, an improved ReConNN method is proposed to address the\nmentioned weaknesses. Time-dependent ordered images can be generated.\nFurthermore, the improved method is successfully applied in impact simulation\ncase and engineering experiment. Through the experiments, comparisons and\nanalyses, the improved method is demonstrated to outperform the former one in\nterms of its accuracy, efficiency and costs.\n", "versions": [{"version": "v1", "created": "Mon, 8 Apr 2019 04:48:36 GMT"}, {"version": "v2", "created": "Fri, 10 May 2019 01:18:58 GMT"}, {"version": "v3", "created": "Mon, 3 Jun 2019 05:43:12 GMT"}], "update_date": "2019-06-04", "authors_parsed": [["Li", "Yu", ""], ["Wang", "Hu", ""], ["Shuai", "Wenquan", ""], ["Zhang", "Honghao", ""], ["Peng", "Yong", ""]]}, {"id": "1905.03244", "submitter": "Nikos Kolotouros", "authors": "Nikos Kolotouros, Georgios Pavlakos, Kostas Daniilidis", "title": "Convolutional Mesh Regression for Single-Image Human Shape\n  Reconstruction", "comments": "To appear at CVPR 2019 (Oral Presentation). Project page:\n  https://www.seas.upenn.edu/~nkolot/projects/cmr/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the problem of 3D human pose and shape estimation from a\nsingle image. Previous approaches consider a parametric model of the human\nbody, SMPL, and attempt to regress the model parameters that give rise to a\nmesh consistent with image evidence. This parameter regression has been a very\nchallenging task, with model-based approaches underperforming compared to\nnonparametric solutions in terms of pose estimation. In our work, we propose to\nrelax this heavy reliance on the model's parameter space. We still retain the\ntopology of the SMPL template mesh, but instead of predicting model parameters,\nwe directly regress the 3D location of the mesh vertices. This is a heavy task\nfor a typical network, but our key insight is that the regression becomes\nsignificantly easier using a Graph-CNN. This architecture allows us to\nexplicitly encode the template mesh structure within the network and leverage\nthe spatial locality the mesh has to offer. Image-based features are attached\nto the mesh vertices and the Graph-CNN is responsible to process them on the\nmesh structure, while the regression target for each vertex is its 3D location.\nHaving recovered the complete 3D geometry of the mesh, if we still require a\nspecific model parametrization, this can be reliably regressed from the\nvertices locations. We demonstrate the flexibility and the effectiveness of our\nproposed graph-based mesh regression by attaching different types of features\non the mesh vertices. In all cases, we outperform the comparable baselines\nrelying on model parameter regression, while we also achieve state-of-the-art\nresults among model-based pose estimation approaches.\n", "versions": [{"version": "v1", "created": "Wed, 8 May 2019 17:59:06 GMT"}], "update_date": "2019-05-09", "authors_parsed": [["Kolotouros", "Nikos", ""], ["Pavlakos", "Georgios", ""], ["Daniilidis", "Kostas", ""]]}, {"id": "1905.03246", "submitter": "Yichao Zhou", "authors": "Yichao Zhou, Haozhi Qi, Yi Ma", "title": "End-to-End Wireframe Parsing", "comments": "ICCV 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a conceptually simple yet effective algorithm to detect wireframes\nin a given image. Compared to the previous methods which first predict an\nintermediate heat map and then extract straight lines with heuristic\nalgorithms, our method is end-to-end trainable and can directly output a\nvectorized wireframe that contains semantically meaningful and geometrically\nsalient junctions and lines. To better understand the quality of the outputs,\nwe propose a new metric for wireframe evaluation that penalizes overlapped line\nsegments and incorrect line connectivities. We conduct extensive experiments\nand show that our method significantly outperforms the previous\nstate-of-the-art wireframe and line extraction algorithms. We hope our simple\napproach can be served as a baseline for future wireframe parsing studies. Code\nhas been made publicly available at https://github.com/zhou13/lcnn.\n", "versions": [{"version": "v1", "created": "Wed, 8 May 2019 17:59:41 GMT"}, {"version": "v2", "created": "Sat, 8 Feb 2020 00:03:46 GMT"}, {"version": "v3", "created": "Tue, 4 May 2021 21:36:28 GMT"}], "update_date": "2021-05-06", "authors_parsed": [["Zhou", "Yichao", ""], ["Qi", "Haozhi", ""], ["Ma", "Yi", ""]]}, {"id": "1905.03277", "submitter": "Bartlomiej Wronski", "authors": "Bartlomiej Wronski, Ignacio Garcia-Dorado, Manfred Ernst, Damien\n  Kelly, Michael Krainin, Chia-Kai Liang, Marc Levoy, Peyman Milanfar", "title": "Handheld Multi-Frame Super-Resolution", "comments": "24 pages, accepted to Siggraph 2019 Technical Papers program", "journal-ref": null, "doi": "10.1145/3306346.3323024", "report-no": null, "categories": "cs.CV eess.IV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Compared to DSLR cameras, smartphone cameras have smaller sensors, which\nlimits their spatial resolution; smaller apertures, which limits their light\ngathering ability; and smaller pixels, which reduces their signal-to noise\nratio. The use of color filter arrays (CFAs) requires demosaicing, which\nfurther degrades resolution. In this paper, we supplant the use of traditional\ndemosaicing in single-frame and burst photography pipelines with a multiframe\nsuper-resolution algorithm that creates a complete RGB image directly from a\nburst of CFA raw images. We harness natural hand tremor, typical in handheld\nphotography, to acquire a burst of raw frames with small offsets. These frames\nare then aligned and merged to form a single image with red, green, and blue\nvalues at every pixel site. This approach, which includes no explicit\ndemosaicing step, serves to both increase image resolution and boost signal to\nnoise ratio. Our algorithm is robust to challenging scene conditions: local\nmotion, occlusion, or scene changes. It runs at 100 milliseconds per\n12-megapixel RAW input burst frame on mass-produced mobile phones.\nSpecifically, the algorithm is the basis of the Super-Res Zoom feature, as well\nas the default merge method in Night Sight mode (whether zooming or not) on\nGoogle's flagship phone.\n", "versions": [{"version": "v1", "created": "Wed, 8 May 2019 18:09:15 GMT"}, {"version": "v2", "created": "Tue, 16 Feb 2021 23:06:58 GMT"}], "update_date": "2021-02-18", "authors_parsed": [["Wronski", "Bartlomiej", ""], ["Garcia-Dorado", "Ignacio", ""], ["Ernst", "Manfred", ""], ["Kelly", "Damien", ""], ["Krainin", "Michael", ""], ["Liang", "Chia-Kai", ""], ["Levoy", "Marc", ""], ["Milanfar", "Peyman", ""]]}, {"id": "1905.03288", "submitter": "Abu Sufian", "authors": "Farhana Sultana, A. Sufian and Paramartha Dutta", "title": "Advancements in Image Classification using Convolutional Neural Network", "comments": "9 pages, 15 figures, 3 Tables. Submitted to 2018 Fourth International\n  Conference on Research in Computational Intelligence and Communication\n  Networks(ICRCICN 2018)", "journal-ref": "2018 Fourth International Conference on Research in Computational\n  Intelligence and Communication Networks (ICRCICN)", "doi": "10.1109/ICRCICN.2018.8718718", "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional Neural Network (CNN) is the state-of-the-art for image\nclassification task. Here we have briefly discussed different components of\nCNN. In this paper, We have explained different CNN architectures for image\nclassification. Through this paper, we have shown advancements in CNN from\nLeNet-5 to latest SENet model. We have discussed the model description and\ntraining details of each model. We have also drawn a comparison among those\nmodels.\n", "versions": [{"version": "v1", "created": "Wed, 8 May 2019 18:34:19 GMT"}], "update_date": "2019-05-27", "authors_parsed": [["Sultana", "Farhana", ""], ["Sufian", "A.", ""], ["Dutta", "Paramartha", ""]]}, {"id": "1905.03304", "submitter": "Yue Wang", "authors": "Yue Wang, Justin M. Solomon", "title": "Deep Closest Point: Learning Representations for Point Cloud\n  Registration", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Point cloud registration is a key problem for computer vision applied to\nrobotics, medical imaging, and other applications. This problem involves\nfinding a rigid transformation from one point cloud into another so that they\nalign. Iterative Closest Point (ICP) and its variants provide simple and\neasily-implemented iterative methods for this task, but these algorithms can\nconverge to spurious local optima. To address local optima and other\ndifficulties in the ICP pipeline, we propose a learning-based method, titled\nDeep Closest Point (DCP), inspired by recent techniques in computer vision and\nnatural language processing. Our model consists of three parts: a point cloud\nembedding network, an attention-based module combined with a pointer generation\nlayer, to approximate combinatorial matching, and a differentiable singular\nvalue decomposition (SVD) layer to extract the final rigid transformation. We\ntrain our model end-to-end on the ModelNet40 dataset and show in several\nsettings that it performs better than ICP, its variants (e.g., Go-ICP, FGR),\nand the recently-proposed learning-based method PointNetLK. Beyond providing a\nstate-of-the-art registration technique, we evaluate the suitability of our\nlearned features transferred to unseen objects. We also provide preliminary\nanalysis of our learned model to help understand whether domain-specific and/or\nglobal features facilitate rigid registration.\n", "versions": [{"version": "v1", "created": "Wed, 8 May 2019 19:10:01 GMT"}], "update_date": "2019-05-10", "authors_parsed": [["Wang", "Yue", ""], ["Solomon", "Justin M.", ""]]}, {"id": "1905.03313", "submitter": "Hieu Le", "authors": "Hieu Le, Bento Gon\\c{c}alves, Dimitris Samaras, and Heather Lynch", "title": "Weakly Labeling the Antarctic: The Penguin Colony Case", "comments": "CVPR19 - CV4GC workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Antarctic penguins are important ecological indicators -- especially in the\nface of climate change. In this work, we present a deep learning based model\nfor semantic segmentation of Ad\\'elie penguin colonies in high-resolution\nsatellite imagery. To train our segmentation models, we take advantage of the\nPenguin Colony Dataset: a unique dataset with 2044 georeferenced cropped images\nfrom 193 Ad\\'elie penguin colonies in Antarctica. In the face of a scarcity of\npixel-level annotation masks, we propose a weakly-supervised framework to\neffectively learn a segmentation model from weak labels. We use a\nclassification network to filter out data unsuitable for the segmentation\nnetwork. This segmentation network is trained with a specific loss function,\nbased on the average activation, to effectively learn from the data with the\nweakly-annotated labels. Our experiments show that adding weakly-annotated\ntraining examples significantly improves segmentation performance, increasing\nthe mean Intersection-over-Union from 42.3 to 60.0% on the Penguin Colony\nDataset.\n", "versions": [{"version": "v1", "created": "Wed, 8 May 2019 20:03:18 GMT"}, {"version": "v2", "created": "Sun, 19 May 2019 08:17:19 GMT"}], "update_date": "2019-05-21", "authors_parsed": [["Le", "Hieu", ""], ["Gon\u00e7alves", "Bento", ""], ["Samaras", "Dimitris", ""], ["Lynch", "Heather", ""]]}, {"id": "1905.03389", "submitter": "Vladimir Golkov", "authors": "Jan Schuchardt, Vladimir Golkov, Daniel Cremers", "title": "Learning to Evolve", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Evolution and learning are two of the fundamental mechanisms by which life\nadapts in order to survive and to transcend limitations. These biological\nphenomena inspired successful computational methods such as evolutionary\nalgorithms and deep learning. Evolution relies on random mutations and on\nrandom genetic recombination. Here we show that learning to evolve, i.e.\nlearning to mutate and recombine better than at random, improves the result of\nevolution in terms of fitness increase per generation and even in terms of\nattainable fitness. We use deep reinforcement learning to learn to dynamically\nadjust the strategy of evolutionary algorithms to varying circumstances. Our\nmethods outperform classical evolutionary algorithms on combinatorial and\ncontinuous optimization problems.\n", "versions": [{"version": "v1", "created": "Wed, 8 May 2019 23:35:02 GMT"}], "update_date": "2019-05-10", "authors_parsed": [["Schuchardt", "Jan", ""], ["Golkov", "Vladimir", ""], ["Cremers", "Daniel", ""]]}, {"id": "1905.03397", "submitter": "Pirazh Khorramshahi", "authors": "Pirazh Khorramshahi, Amit Kumar, Neehar Peri, Sai Saketh Rambhatla,\n  Jun-Cheng Chen and Rama Chellappa", "title": "A Dual-Path Model With Adaptive Attention For Vehicle Re-Identification", "comments": "This work has been accepted for oral presentation in ICCV 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In recent years, attention models have been extensively used for person and\nvehicle re-identification. Most re-identification methods are designed to focus\nattention on key-point locations. However, depending on the orientation, the\ncontribution of each key-point varies. In this paper, we present a novel\ndual-path adaptive attention model for vehicle re-identification (AAVER). The\nglobal appearance path captures macroscopic vehicle features while the\norientation conditioned part appearance path learns to capture localized\ndiscriminative features by focusing attention on the most informative\nkey-points. Through extensive experimentation, we show that the proposed AAVER\nmethod is able to accurately re-identify vehicles in unconstrained scenarios,\nyielding state of the art results on the challenging dataset VeRi-776. As a\nbyproduct, the proposed system is also able to accurately predict vehicle\nkey-points and shows an improvement of more than 7% over state of the art. The\ncode for key-point estimation model is available at\nhttps://github.com/Pirazh/Vehicle_Key_Point_Orientation_Estimation.\n", "versions": [{"version": "v1", "created": "Thu, 9 May 2019 00:52:19 GMT"}, {"version": "v2", "created": "Thu, 15 Aug 2019 22:30:12 GMT"}, {"version": "v3", "created": "Tue, 24 Sep 2019 16:16:35 GMT"}], "update_date": "2019-09-25", "authors_parsed": [["Khorramshahi", "Pirazh", ""], ["Kumar", "Amit", ""], ["Peri", "Neehar", ""], ["Rambhatla", "Sai Saketh", ""], ["Chen", "Jun-Cheng", ""], ["Chellappa", "Rama", ""]]}, {"id": "1905.03415", "submitter": "Zhengxin Li", "authors": "Ziheng Zhang, Zhengxin Li, Ning Bi, Jia Zheng, Jinlei Wang, Kun Huang,\n  Weixin Luo, Yanyu Xu, Shenghua Gao", "title": "PPGNet: Learning Point-Pair Graph for Line Segment Detection", "comments": "To appear in CVPR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a novel framework to detect line segments in\nman-made environments. Specifically, we propose to describe junctions, line\nsegments and relationships between them with a simple graph, which is more\nstructured and informative than end-point representation used in existing line\nsegment detection methods. In order to extract a line segment graph from an\nimage, we further introduce the PPGNet, a convolutional neural network that\ndirectly infers a graph from an image. We evaluate our method on published\nbenchmarks including York Urban and Wireframe datasets. The results demonstrate\nthat our method achieves satisfactory performance and generalizes well on all\nthe benchmarks. The source code of our work is available at\n\\url{https://github.com/svip-lab/PPGNet}.\n", "versions": [{"version": "v1", "created": "Thu, 9 May 2019 02:22:06 GMT"}, {"version": "v2", "created": "Thu, 16 May 2019 04:00:25 GMT"}], "update_date": "2019-05-17", "authors_parsed": [["Zhang", "Ziheng", ""], ["Li", "Zhengxin", ""], ["Bi", "Ning", ""], ["Zheng", "Jia", ""], ["Wang", "Jinlei", ""], ["Huang", "Kun", ""], ["Luo", "Weixin", ""], ["Xu", "Yanyu", ""], ["Gao", "Shenghua", ""]]}, {"id": "1905.03418", "submitter": "arXiv Admin", "authors": "Gael Kamdem De Teyou", "title": "Deep Learning Acceleration Techniques for Real Time Mobile Vision\n  Applications", "comments": "This submission has been withdrawn by arXiv administrators due to\n  inappropriate text reuse from external sources", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Learning (DL) has become a crucial technology for Artificial\nIntelligence (AI). It is a powerful technique to automatically extract\nhigh-level features from complex data which can be exploited for applications\nsuch as computer vision, natural language processing, cybersecurity,\ncommunications, and so on. For the particular case of computer vision, several\nalgorithms like object detection in real time videos have been proposed and\nthey work well on Desktop GPUs and distributed computing platforms. However\nthese algorithms are still heavy for mobile and embedded visual applications.\nThe rapid spreading of smart portable devices and the emerging 5G network are\nintroducing new smart multimedia applications in mobile environments. As a\nconsequence, the possibility of implementing deep neural networks to mobile\nenvironments has attracted a lot of researchers. This paper presents emerging\ndeep learning acceleration techniques that can enable the delivery of real time\nvisual recognition into the hands of end users, anytime and anywhere.\n", "versions": [{"version": "v1", "created": "Thu, 9 May 2019 02:39:37 GMT"}, {"version": "v2", "created": "Fri, 7 Jun 2019 15:31:48 GMT"}], "update_date": "2019-06-10", "authors_parsed": [["De Teyou", "Gael Kamdem", ""]]}, {"id": "1905.03421", "submitter": "Kazuya Kakizaki", "authors": "Kazuya Kakizaki, Kosuke Yoshida", "title": "Adversarial Image Translation: Unrestricted Adversarial Examples in Face\n  Recognition Systems", "comments": "Kazuya Kakizaki and Kosuke Yoshida share equal contributions.\n  Accepted at AAAI Workshop on Artificial Intelligence Safety (2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Thanks to recent advances in deep neural networks (DNNs), face recognition\nsystems have become highly accurate in classifying a large number of face\nimages. However, recent studies have found that DNNs could be vulnerable to\nadversarial examples, raising concerns about the robustness of such systems.\nAdversarial examples that are not restricted to small perturbations could be\nmore serious since conventional certified defenses might be ineffective against\nthem. To shed light on the vulnerability to such adversarial examples, we\npropose a flexible and efficient method for generating unrestricted adversarial\nexamples using image translation techniques. Our method enables us to translate\na source image into any desired facial appearance with large perturbations to\ndeceive target face recognition systems. Our experimental results indicate that\nour method achieved about $90$ and $80\\%$ attack success rates under white- and\nblack-box settings, respectively, and that the translated images are\nperceptually realistic and maintain the identifiability of the individual while\nthe perturbations are large enough to bypass certified defenses.\n", "versions": [{"version": "v1", "created": "Thu, 9 May 2019 02:58:45 GMT"}, {"version": "v2", "created": "Mon, 27 May 2019 04:05:43 GMT"}, {"version": "v3", "created": "Tue, 28 Jan 2020 06:36:40 GMT"}], "update_date": "2020-01-29", "authors_parsed": [["Kakizaki", "Kazuya", ""], ["Yoshida", "Kosuke", ""]]}, {"id": "1905.03422", "submitter": "Jieru Jia", "authors": "Jieru Jia, Qiuqi Ruan, Timothy M. Hospedales", "title": "Frustratingly Easy Person Re-Identification: Generalizing Person Re-ID\n  in Practice", "comments": "14 pages,2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Contemporary person re-identification (\\reid) methods usually require access\nto data from the deployment camera network during training in order to perform\nwell. This is because contemporary \\reid{} models trained on one dataset do not\ngeneralise to other camera networks due to the domain-shift between datasets.\nThis requirement is often the bottleneck for deploying \\reid{} systems in\npractical security or commercial applications, as it may be impossible to\ncollect this data in advance or prohibitively costly to annotate it. This paper\nalleviates this issue by proposing a simple baseline for domain\ngeneralizable~(DG) person re-identification. That is, to learn a \\reid{} model\nfrom a set of source domains that is suitable for application to unseen\ndatasets out-of-the-box, without any model updating. Specifically, we observe\nthat the domain discrepancy in \\reid{} is due to style and content variance\nacross datasets and demonstrate appropriate Instance and Feature Normalization\nalleviates much of the resulting domain-shift in Deep \\reid{} models. Instance\nNormalization~(IN) in early layers filters out style statistic variations and\nFeature Normalization~(FN) in deep layers is able to further eliminate\ndisparity in content statistics. Compared to contemporary alternatives, this\napproach is extremely simple to implement, while being faster to train and\ntest, thus making it an extremely valuable baseline for implementing \\reid{} in\npractice. With a few lines of code, it increases the rank 1 \\reid{} accuracy by\n{11.8\\%, 33.2\\%, 12.8\\% and 8.5\\%} on the VIPeR, PRID, GRID, and i-LIDS\nbenchmarks respectively. Source codes are available at\n\\url{https://github.com/BJTUJia/person_reID_DualNorm}.\n", "versions": [{"version": "v1", "created": "Thu, 9 May 2019 03:00:13 GMT"}, {"version": "v2", "created": "Fri, 19 Jul 2019 13:29:40 GMT"}, {"version": "v3", "created": "Mon, 22 Jul 2019 04:16:02 GMT"}], "update_date": "2019-07-23", "authors_parsed": [["Jia", "Jieru", ""], ["Ruan", "Qiuqi", ""], ["Hospedales", "Timothy M.", ""]]}, {"id": "1905.03433", "submitter": "Baoyuan Wu", "authors": "Baoyuan Wu, Li Shen, Tong Zhang, Bernard Ghanem", "title": "MAP Inference via L2-Sphere Linear Program Reformulation", "comments": "Accepted to International Journal of Computer Vision", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Maximum a posteriori (MAP) inference is an important task for graphical\nmodels. Due to complex dependencies among variables in realistic model, finding\nan exact solution for MAP inference is often intractable. Thus, many\napproximation methods have been developed, among which the linear programming\n(LP) relaxation based methods show promising performance. However, one major\ndrawback of LP relaxation is that it is possible to give fractional solutions.\nInstead of presenting a tighter relaxation, in this work we propose a\ncontinuous but equivalent reformulation of the original MAP inference problem,\ncalled LS-LP. We add the L2-sphere constraint onto the original LP relaxation,\nleading to an intersected space with the local marginal polytope that is\nequivalent to the space of all valid integer label configurations. Thus, LS-LP\nis equivalent to the original MAP inference problem. We propose a perturbed\nalternating direction method of multipliers (ADMM) algorithm to optimize the\nLS-LP problem, by adding a sufficiently small perturbation epsilon onto the\nobjective function and constraints. We prove that the perturbed ADMM algorithm\nglobally converges to the epsilon-Karush-Kuhn-Tucker (epsilon-KKT) point of the\nLS-LP problem. The convergence rate will also be analyzed. Experiments on\nseveral benchmark datasets from Probabilistic Inference Challenge (PIC 2011)\nand OpenGM 2 show competitive performance of our proposed method against\nstate-of-the-art MAP inference methods.\n", "versions": [{"version": "v1", "created": "Thu, 9 May 2019 03:47:15 GMT"}, {"version": "v2", "created": "Fri, 17 Jan 2020 08:52:11 GMT"}, {"version": "v3", "created": "Mon, 20 Jan 2020 03:09:50 GMT"}], "update_date": "2020-01-22", "authors_parsed": [["Wu", "Baoyuan", ""], ["Shen", "Li", ""], ["Zhang", "Tong", ""], ["Ghanem", "Bernard", ""]]}, {"id": "1905.03434", "submitter": "Guanbin Li", "authors": "Haofeng Li, Guanbin Li, Yizhou Yu", "title": "ROSA: Robust Salient Object Detection against Adversarial Attacks", "comments": "To be published in IEEE Transactions on Cybernetics", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently salient object detection has witnessed remarkable improvement owing\nto the deep convolutional neural networks which can harvest powerful features\nfor images. In particular, state-of-the-art salient object detection methods\nenjoy high accuracy and efficiency from fully convolutional network (FCN) based\nframeworks which are trained from end to end and predict pixel-wise labels.\nHowever, such framework suffers from adversarial attacks which confuse neural\nnetworks via adding quasi-imperceptible noises to input images without changing\nthe ground truth annotated by human subjects. To our knowledge, this paper is\nthe first one that mounts successful adversarial attacks on salient object\ndetection models and verifies that adversarial samples are effective on a wide\nrange of existing methods. Furthermore, this paper proposes a novel end-to-end\ntrainable framework to enhance the robustness for arbitrary FCN-based salient\nobject detection models against adversarial attacks. The proposed framework\nadopts a novel idea that first introduces some new generic noise to destroy\nadversarial perturbations, and then learns to predict saliency maps for input\nimages with the introduced noise. Specifically, our proposed method consists of\na segment-wise shielding component, which preserves boundaries and destroys\ndelicate adversarial noise patterns and a context-aware restoration component,\nwhich refines saliency maps through global contrast modeling. Experimental\nresults suggest that our proposed framework improves the performance\nsignificantly for state-of-the-art models on a series of datasets.\n", "versions": [{"version": "v1", "created": "Thu, 9 May 2019 03:56:32 GMT"}], "update_date": "2019-05-10", "authors_parsed": [["Li", "Haofeng", ""], ["Li", "Guanbin", ""], ["Yu", "Yizhou", ""]]}, {"id": "1905.03445", "submitter": "Cao Haichao", "authors": "Haichao Cao, Hong Liu, Enmin Song, Guangzhi Ma, Xiangyang Xu, Renchao\n  Jin, Tengying Liu, Chih-Cheng Hung", "title": "Two-Stage Convolutional Neural Network Architecture for Lung Nodule\n  Detection", "comments": "29 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Early detection of lung cancer is an effective way to improve the survival\nrate of patients. It is a critical step to have accurate detection of lung\nnodules in computed tomography (CT) images for the diagnosis of lung cancer.\nHowever, due to the heterogeneity of the lung nodules and the complexity of the\nsurrounding environment, robust nodule detection has been a challenging task.\nIn this study, we propose a two-stage convolutional neural network (TSCNN)\narchitecture for lung nodule detection. The CNN architecture in the first stage\nis based on the improved UNet segmentation network to establish an initial\ndetection of lung nodules. Simultaneously, in order to obtain a high recall\nrate without introducing excessive false positive nodules, we propose a novel\nsampling strategy, and use the offline hard mining idea for training and\nprediction according to the proposed cascaded prediction method. The CNN\narchitecture in the second stage is based on the proposed dual pooling\nstructure, which is built into three 3D CNN classification networks for false\npositive reduction. Since the network training requires a significant amount of\ntraining data, we adopt a data augmentation method based on random mask.\nFurthermore, we have improved the generalization ability of the false positive\nreduction model by means of ensemble learning. The proposed method has been\nexperimentally verified on the LUNA dataset. Experimental results show that the\nproposed TSCNN architecture can obtain competitive detection performance.\n", "versions": [{"version": "v1", "created": "Thu, 9 May 2019 05:21:40 GMT"}], "update_date": "2019-05-10", "authors_parsed": [["Cao", "Haichao", ""], ["Liu", "Hong", ""], ["Song", "Enmin", ""], ["Ma", "Guangzhi", ""], ["Xu", "Xiangyang", ""], ["Jin", "Renchao", ""], ["Liu", "Tengying", ""], ["Hung", "Chih-Cheng", ""]]}, {"id": "1905.03465", "submitter": "Erkun Yang", "authors": "Erkun Yang, Tongliang Liu, Cheng Deng, Wei Liu and Dacheng Tao", "title": "DistillHash: Unsupervised Deep Hashing by Distilling Data Pairs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to the high storage and search efficiency, hashing has become prevalent\nfor large-scale similarity search. Particularly, deep hashing methods have\ngreatly improved the search performance under supervised scenarios. In\ncontrast, unsupervised deep hashing models can hardly achieve satisfactory\nperformance due to the lack of reliable supervisory similarity signals. To\naddress this issue, we propose a novel deep unsupervised hashing model, dubbed\nDistillHash, which can learn a distilled data set consisted of data pairs,\nwhich have confidence similarity signals. Specifically, we investigate the\nrelationship between the initial noisy similarity signals learned from local\nstructures and the semantic similarity labels assigned by a Bayes optimal\nclassifier. We show that under a mild assumption, some data pairs, of which\nlabels are consistent with those assigned by the Bayes optimal classifier, can\nbe potentially distilled. Inspired by this fact, we design a simple yet\neffective strategy to distill data pairs automatically and further adopt a\nBayesian learning framework to learn hash functions from the distilled data\nset. Extensive experimental results on three widely used benchmark datasets\nshow that the proposed DistillHash consistently accomplishes the\nstate-of-the-art search performance.\n", "versions": [{"version": "v1", "created": "Thu, 9 May 2019 07:10:12 GMT"}], "update_date": "2019-05-10", "authors_parsed": [["Yang", "Erkun", ""], ["Liu", "Tongliang", ""], ["Deng", "Cheng", ""], ["Liu", "Wei", ""], ["Tao", "Dacheng", ""]]}, {"id": "1905.03466", "submitter": "Dongdong Yu", "authors": "Kai Su, Dongdong Yu, Zhenqi Xu, Xin Geng, Changhu Wang", "title": "Multi-Person Pose Estimation with Enhanced Channel-wise and Spatial\n  Information", "comments": "Accepted by CVPR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-person pose estimation is an important but challenging problem in\ncomputer vision. Although current approaches have achieved significant progress\nby fusing the multi-scale feature maps, they pay little attention to enhancing\nthe channel-wise and spatial information of the feature maps. In this paper, we\npropose two novel modules to perform the enhancement of the information for the\nmulti-person pose estimation. First, a Channel Shuffle Module (CSM) is proposed\nto adopt the channel shuffle operation on the feature maps with different\nlevels, promoting cross-channel information communication among the pyramid\nfeature maps. Second, a Spatial, Channel-wise Attention Residual Bottleneck\n(SCARB) is designed to boost the original residual unit with attention\nmechanism, adaptively highlighting the information of the feature maps both in\nthe spatial and channel-wise context. The effectiveness of our proposed modules\nis evaluated on the COCO keypoint benchmark, and experimental results show that\nour approach achieves the state-of-the-art results.\n", "versions": [{"version": "v1", "created": "Thu, 9 May 2019 07:12:40 GMT"}], "update_date": "2019-05-10", "authors_parsed": [["Su", "Kai", ""], ["Yu", "Dongdong", ""], ["Xu", "Zhenqi", ""], ["Geng", "Xin", ""], ["Wang", "Changhu", ""]]}, {"id": "1905.03469", "submitter": "Yinglu Liu", "authors": "Yinglu Liu, Hao Shen, Yue Si, Xiaobo Wang, Xiangyu Zhu, Hailin Shi,\n  Zhibin Hong, Hanqi Guo, Ziyuan Guo, Yanqin Chen, Bi Li, Teng Xi, Jun Yu,\n  Haonian Xie, Guochen Xie, Mengyan Li, Qing Lu, Zengfu Wang, Shenqi Lai,\n  Zhenhua Chai, and Xiaoming Wei", "title": "Grand Challenge of 106-Point Facial Landmark Localization", "comments": "This paper is accepted at ICME2019 Grand Challenge. The JD-landmark\n  dataset has been released and can be downloaded from\n  https://sites.google.com/view/hailin-shi", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Facial landmark localization is a very crucial step in numerous face related\napplications, such as face recognition, facial pose estimation, face image\nsynthesis, etc. However, previous competitions on facial landmark localization\n(i.e., the 300-W, 300-VW and Menpo challenges) aim to predict 68-point\nlandmarks, which are incompetent to depict the structure of facial components.\nIn order to overcome this problem, we construct a challenging dataset, named\nJD-landmark. Each image is manually annotated with 106-point landmarks. This\ndataset covers large variations on pose and expression, which brings a lot of\ndifficulties to predict accurate landmarks. We hold a 106-point facial landmark\nlocalization competition1 on this dataset in conjunction with IEEE\nInternational Conference on Multimedia and Expo (ICME) 2019. The purpose of\nthis competition is to discover effective and robust facial landmark\nlocalization approaches.\n", "versions": [{"version": "v1", "created": "Thu, 9 May 2019 07:18:57 GMT"}, {"version": "v2", "created": "Tue, 23 Jul 2019 03:59:51 GMT"}, {"version": "v3", "created": "Wed, 24 Jul 2019 04:49:28 GMT"}], "update_date": "2019-07-25", "authors_parsed": [["Liu", "Yinglu", ""], ["Shen", "Hao", ""], ["Si", "Yue", ""], ["Wang", "Xiaobo", ""], ["Zhu", "Xiangyu", ""], ["Shi", "Hailin", ""], ["Hong", "Zhibin", ""], ["Guo", "Hanqi", ""], ["Guo", "Ziyuan", ""], ["Chen", "Yanqin", ""], ["Li", "Bi", ""], ["Xi", "Teng", ""], ["Yu", "Jun", ""], ["Xie", "Haonian", ""], ["Xie", "Guochen", ""], ["Li", "Mengyan", ""], ["Lu", "Qing", ""], ["Wang", "Zengfu", ""], ["Lai", "Shenqi", ""], ["Chai", "Zhenhua", ""], ["Wei", "Xiaoming", ""]]}, {"id": "1905.03540", "submitter": "Masahiro Mitsuhara", "authors": "Masahiro Mitsuhara, Hiroshi Fukui, Yusuke Sakashita, Takanori Ogata,\n  Tsubasa Hirakawa, Takayoshi Yamashita, Hironobu Fujiyoshi", "title": "Embedding Human Knowledge into Deep Neural Network via Attention Map", "comments": "10 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we aim to realize a method for embedding human knowledge into\ndeep neural networks. While the conventional method to embed human knowledge\nhas been applied for non-deep machine learning, it is challenging to apply it\nfor deep learning models due to the enormous number of model parameters. To\ntackle this problem, we focus on the attention mechanism of an attention branch\nnetwork (ABN). In this paper, we propose a fine-tuning method that utilizes a\nsingle-channel attention map which is manually edited by a human expert. Our\nfine-tuning method can train a network so that the output attention map\ncorresponds to the edited ones. As a result, the fine-tuned network can output\nan attention map that takes into account human knowledge. Experimental results\nwith ImageNet, CUB-200-2010, and IDRiD demonstrate that it is possible to\nobtain a clear attention map for a visual explanation and improve the\nclassification performance. Our findings can be a novel framework for\noptimizing networks through human intuitive editing via a visual interface and\nsuggest new possibilities for human-machine cooperation in addition to the\nimprovement of visual explanations.\n", "versions": [{"version": "v1", "created": "Thu, 9 May 2019 11:32:44 GMT"}, {"version": "v2", "created": "Tue, 17 Dec 2019 15:17:57 GMT"}, {"version": "v3", "created": "Wed, 18 Dec 2019 06:28:48 GMT"}, {"version": "v4", "created": "Thu, 19 Dec 2019 06:16:51 GMT"}], "update_date": "2019-12-20", "authors_parsed": [["Mitsuhara", "Masahiro", ""], ["Fukui", "Hiroshi", ""], ["Sakashita", "Yusuke", ""], ["Ogata", "Takanori", ""], ["Hirakawa", "Tsubasa", ""], ["Yamashita", "Takayoshi", ""], ["Fujiyoshi", "Hironobu", ""]]}, {"id": "1905.03546", "submitter": "Shujaat Khan Engr", "authors": "Shujaat Khan, Imran Naseem, Roberto Togneri, and Mohammed Bennamoun", "title": "A Novel Adaptive Kernel for the RBF Neural Networks", "comments": null, "journal-ref": "Circuits, Systems, and Signal Processing, vol. 36, no. 4, pp.\n  1639-1653, 2017", "doi": "10.1007/s00034-016-0375-7", "report-no": null, "categories": "stat.ML cs.CV cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a novel adaptive kernel for the radial basis\nfunction (RBF) neural networks. The proposed kernel adaptively fuses the\nEuclidean and cosine distance measures to exploit the reciprocating properties\nof the two. The proposed framework dynamically adapts the weights of the\nparticipating kernels using the gradient descent method thereby alleviating the\nneed for predetermined weights. The proposed method is shown to outperform the\nmanual fusion of the kernels on three major problems of estimation namely\nnonlinear system identification, pattern classification and function\napproximation.\n", "versions": [{"version": "v1", "created": "Thu, 9 May 2019 11:38:57 GMT"}], "update_date": "2019-05-10", "authors_parsed": [["Khan", "Shujaat", ""], ["Naseem", "Imran", ""], ["Togneri", "Roberto", ""], ["Bennamoun", "Mohammed", ""]]}, {"id": "1905.03556", "submitter": "Weimin Tan", "authors": "Weimin Tan, Bo Yan, Chumin Lin, Xuejing Niu", "title": "Cycle-IR: Deep Cyclic Image Retargeting", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Supervised deep learning techniques have achieved great success in various\nfields due to getting rid of the limitation of handcrafted representations.\nHowever, most previous image retargeting algorithms still employ fixed design\nprinciples such as using gradient map or handcrafted features to compute\nsaliency map, which inevitably restricts its generality. Deep learning\ntechniques may help to address this issue, but the challenging problem is that\nwe need to build a large-scale image retargeting dataset for the training of\ndeep retargeting models. However, building such a dataset requires huge human\nefforts.\n  In this paper, we propose a novel deep cyclic image retargeting approach,\ncalled Cycle-IR, to firstly implement image retargeting with a single deep\nmodel, without relying on any explicit user annotations. Our idea is built on\nthe reverse mapping from the retargeted images to the given input images. If\nthe retargeted image has serious distortion or excessive loss of important\nvisual information, the reverse mapping is unlikely to restore the input image\nwell. We constrain this forward-reverse consistency by introducing a cyclic\nperception coherence loss. In addition, we propose a simple yet effective image\nretargeting network (IRNet) to implement the image retargeting process. Our\nIRNet contains a spatial and channel attention layer, which is able to\ndiscriminate visually important regions of input images effectively, especially\nin cluttered images. Given arbitrary sizes of input images and desired aspect\nratios, our Cycle-IR can produce visually pleasing target images directly.\nExtensive experiments on the standard RetargetMe dataset show the superiority\nof our Cycle-IR. In addition, our Cycle-IR outperforms the Multiop method and\nobtains the best result in the user study. Code is available at\nhttps://github.com/mintanwei/Cycle-IR.\n", "versions": [{"version": "v1", "created": "Thu, 9 May 2019 12:05:12 GMT"}], "update_date": "2019-05-10", "authors_parsed": [["Tan", "Weimin", ""], ["Yan", "Bo", ""], ["Lin", "Chumin", ""], ["Niu", "Xuejing", ""]]}, {"id": "1905.03561", "submitter": "Mihai Dusmanu", "authors": "Mihai Dusmanu, Ignacio Rocco, Tomas Pajdla, Marc Pollefeys, Josef\n  Sivic, Akihiko Torii, Torsten Sattler", "title": "D2-Net: A Trainable CNN for Joint Detection and Description of Local\n  Features", "comments": "Accepted at CVPR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we address the problem of finding reliable pixel-level\ncorrespondences under difficult imaging conditions. We propose an approach\nwhere a single convolutional neural network plays a dual role: It is\nsimultaneously a dense feature descriptor and a feature detector. By postponing\nthe detection to a later stage, the obtained keypoints are more stable than\ntheir traditional counterparts based on early detection of low-level\nstructures. We show that this model can be trained using pixel correspondences\nextracted from readily available large-scale SfM reconstructions, without any\nfurther annotations. The proposed method obtains state-of-the-art performance\non both the difficult Aachen Day-Night localization dataset and the InLoc\nindoor localization benchmark, as well as competitive performance on other\nbenchmarks for image matching and 3D reconstruction.\n", "versions": [{"version": "v1", "created": "Thu, 9 May 2019 12:12:14 GMT"}], "update_date": "2019-05-10", "authors_parsed": [["Dusmanu", "Mihai", ""], ["Rocco", "Ignacio", ""], ["Pajdla", "Tomas", ""], ["Pollefeys", "Marc", ""], ["Sivic", "Josef", ""], ["Torii", "Akihiko", ""], ["Sattler", "Torsten", ""]]}, {"id": "1905.03577", "submitter": "Wenshuai Hu", "authors": "Wen-Shuai Hu, Heng-Chao Li, Lei Pan, Wei Li, Ran Tao and Qian Du", "title": "Feature Extraction and Classification Based on Spatial-Spectral ConvLSTM\n  Neural Network for Hyperspectral Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, deep learning has presented a great advance in hyperspectral\nimage (HSI) classification. Particularly, Long Short-Term Memory (LSTM), as a\nspecial deep learning structure, has shown great ability in modeling long-term\ndependencies in the time dimension of video or the spectral dimension of HSIs.\nHowever, the loss of spatial information makes it quite difficult to obtain the\nbetter performance. In order to address this problem, two novel deep models are\nproposed to extract more discriminative spatial-spectral features by exploiting\nthe Convolutional LSTM (ConvLSTM) for the first time. By taking the data patch\nin a local sliding window as the input of each memory cell band by band, the\n2-D extended architecture of LSTM is considered for building the\nspatial-spectral ConvLSTM 2-D Neural Network (SSCL2DNN) to model long-range\ndependencies in the spectral domain. To take advantage of spatial and spectral\ninformation more effectively for extracting a more discriminative\nspatial-spectral feature representation, the spatial-spectral ConvLSTM 3-D\nNeural Network (SSCL3DNN) is further proposed by extending LSTM to 3-D version.\nThe experiments, conducted on three commonly used HSI data sets, demonstrate\nthat the proposed deep models have certain competitive advantages and can\nprovide better classification performance than other state-of-the-art\napproaches.\n", "versions": [{"version": "v1", "created": "Thu, 9 May 2019 12:43:11 GMT"}], "update_date": "2019-05-10", "authors_parsed": [["Hu", "Wen-Shuai", ""], ["Li", "Heng-Chao", ""], ["Pan", "Lei", ""], ["Li", "Wei", ""], ["Tao", "Ran", ""], ["Du", "Qian", ""]]}, {"id": "1905.03578", "submitter": "Mohammadreza Zolfaghari", "authors": "Mohammadreza Zolfaghari, \\\"Ozg\\\"un \\c{C}i\\c{c}ek, Syed Mohsin Ali,\n  Farzaneh Mahdisoltani, Can Zhang, Thomas Brox", "title": "Learning Representations for Predicting Future Activities", "comments": "14 pages, ICCV 2019 submission, Code and Models:\n  https://github.com/lmb-freiburg/PreFAct", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.IT cs.RO math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Foreseeing the future is one of the key factors of intelligence. It involves\nunderstanding of the past and current environment as well as decent experience\nof its possible dynamics. In this work, we address future prediction at the\nabstract level of activities. We propose a network module for learning\nembeddings of the environment's dynamics in a self-supervised way. To take the\nambiguities and high variances in the future activities into account, we use a\nmulti-hypotheses scheme that can represent multiple futures. We demonstrate the\napproach by classifying future activities on the Epic-Kitchens and Breakfast\ndatasets. Moreover, we generate captions that describe the future activities\n", "versions": [{"version": "v1", "created": "Thu, 9 May 2019 12:45:01 GMT"}], "update_date": "2019-05-16", "authors_parsed": [["Zolfaghari", "Mohammadreza", ""], ["\u00c7i\u00e7ek", "\u00d6zg\u00fcn", ""], ["Ali", "Syed Mohsin", ""], ["Mahdisoltani", "Farzaneh", ""], ["Zhang", "Can", ""], ["Brox", "Thomas", ""]]}, {"id": "1905.03590", "submitter": "Fayez Lahoud", "authors": "Fayez Lahoud and Sabine S\\\"usstrunk", "title": "Fast and Efficient Zero-Learning Image Fusion", "comments": "13 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a real-time image fusion method using pre-trained neural networks.\nOur method generates a single image containing features from multiple sources.\nWe first decompose images into a base layer representing large scale intensity\nvariations, and a detail layer containing small scale changes. We use visual\nsaliency to fuse the base layers, and deep feature maps extracted from a\npre-trained neural network to fuse the detail layers. We conduct ablation\nstudies to analyze our method's parameters such as decomposition filters,\nweight construction methods, and network depth and architecture. Then, we\nvalidate its effectiveness and speed on thermal, medical, and multi-focus\nfusion. We also apply it to multiple image inputs such as multi-exposure\nsequences. The experimental results demonstrate that our technique achieves\nstate-of-the-art performance in visual quality, objective assessment, and\nruntime efficiency.\n", "versions": [{"version": "v1", "created": "Thu, 9 May 2019 13:11:26 GMT"}], "update_date": "2019-05-10", "authors_parsed": [["Lahoud", "Fayez", ""], ["S\u00fcsstrunk", "Sabine", ""]]}, {"id": "1905.03633", "submitter": "Jan Kotera", "authors": "Jan Kotera, Denys Rozumnyi, Filip \\v{S}roubek, Ji\\v{r}\\'i Matas", "title": "Intra-frame Object Tracking by Deblatting", "comments": null, "journal-ref": "2019 IEEE/CVF International Conference on Computer Vision Workshop\n  (ICCVW)", "doi": "10.1109/ICCVW.2019.00283", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Objects moving at high speed along complex trajectories often appear in\nvideos, especially videos of sports. Such objects elapse non-negligible\ndistance during exposure time of a single frame and therefore their position in\nthe frame is not well defined. They appear as semi-transparent streaks due to\nthe motion blur and cannot be reliably tracked by standard trackers. We propose\na novel approach called Tracking by Deblatting based on the observation that\nmotion blur is directly related to the intra-frame trajectory of an object.\nBlur is estimated by solving two intertwined inverse problems, blind deblurring\nand image matting, which we call deblatting. The trajectory is then estimated\nby fitting a piecewise quadratic curve, which models physically justifiable\ntrajectories. As a result, tracked objects are precisely localized with higher\ntemporal resolution than by conventional trackers. The proposed TbD tracker was\nevaluated on a newly created dataset of videos with ground truth obtained by a\nhigh-speed camera using a novel Trajectory-IoU metric that generalizes the\ntraditional Intersection over Union and measures the accuracy of the\nintra-frame trajectory. The proposed method outperforms baseline both in recall\nand trajectory accuracy.\n", "versions": [{"version": "v1", "created": "Thu, 9 May 2019 13:48:01 GMT"}, {"version": "v2", "created": "Thu, 20 Jun 2019 11:17:14 GMT"}], "update_date": "2020-06-03", "authors_parsed": [["Kotera", "Jan", ""], ["Rozumnyi", "Denys", ""], ["\u0160roubek", "Filip", ""], ["Matas", "Ji\u0159\u00ed", ""]]}, {"id": "1905.03639", "submitter": "Karsten Roth", "authors": "Karsten Roth, Tomasz Konopczy\\'nski, J\\\"urgen Hesser", "title": "Liver Lesion Segmentation with slice-wise 2D Tiramisu and Tversky loss\n  function", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  At present, lesion segmentation is still performed manually (or\nsemi-automatically) by medical experts. To facilitate this process, we\ncontribute a fully-automatic lesion segmentation pipeline. This work proposes a\nmethod as a part of the LiTS (Liver Tumor Segmentation Challenge) competition\nfor ISBI 17 and MICCAI 17 comparing methods for automatics egmentation of liver\nlesions in CT scans. By utilizing cascaded, densely connected 2D U-Nets and a\nTversky-coefficient based loss function, our framework achieves very good shape\nextractions with high detection sensitivity, with competitive scores at time of\npublication. In addition, adjusting hyperparameters in our Tversky-loss allows\nto tune the network towards higher sensitivity or robustness.\n", "versions": [{"version": "v1", "created": "Thu, 9 May 2019 13:58:21 GMT"}], "update_date": "2019-05-10", "authors_parsed": [["Roth", "Karsten", ""], ["Konopczy\u0144ski", "Tomasz", ""], ["Hesser", "J\u00fcrgen", ""]]}, {"id": "1905.03646", "submitter": "Shuai Yang", "authors": "Shuai Yang, Wenjing Wang, Jiaying Liu", "title": "TE141K: Artistic Text Benchmark for Text Effect Transfer", "comments": "Accepted by TPAMI 2020. Project page:\n  https://daooshee.github.io/TE141K/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Text effects are combinations of visual elements such as outlines, colors and\ntextures of text, which can dramatically improve its artistry. Although text\neffects are extensively utilized in the design industry, they are usually\ncreated by human experts due to their extreme complexity; this is laborious and\nnot practical for normal users. In recent years, some efforts have been made\ntoward automatic text effect transfer; however, the lack of data limits the\ncapabilities of transfer models. To address this problem, we introduce a new\ntext effects dataset, TE141K, with 141,081 text effect/glyph pairs in total.\nOur dataset consists of 152 professionally designed text effects rendered on\nglyphs, including English letters, Chinese characters, and Arabic numerals. To\nthe best of our knowledge, this is the largest dataset for text effect transfer\nto date. Based on this dataset, we propose a baseline approach called text\neffect transfer GAN (TET-GAN), which supports the transfer of all 152 styles in\none model and can efficiently extend to new styles. Finally, we conduct a\ncomprehensive comparison in which 14 style transfer models are benchmarked.\nExperimental results demonstrate the superiority of TET-GAN both qualitatively\nand quantitatively and indicate that our dataset is effective and challenging.\n", "versions": [{"version": "v1", "created": "Wed, 8 May 2019 15:57:39 GMT"}, {"version": "v2", "created": "Fri, 10 May 2019 01:18:34 GMT"}, {"version": "v3", "created": "Tue, 24 Mar 2020 13:22:55 GMT"}], "update_date": "2020-03-25", "authors_parsed": [["Yang", "Shuai", ""], ["Wang", "Wenjing", ""], ["Liu", "Jiaying", ""]]}, {"id": "1905.03658", "submitter": "Jason Ramapuram", "authors": "Jason Ramapuram and Russ Webb", "title": "Improving Discrete Latent Representations With Differentiable\n  Approximation Bridges", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern neural network training relies on piece-wise (sub-)differentiable\nfunctions in order to use backpropagation to update model parameters. In this\nwork, we introduce a novel method to allow simple non-differentiable functions\nat intermediary layers of deep neural networks. We do so by training with a\ndifferentiable approximation bridge (DAB) neural network which approximates the\nnon-differentiable forward function and provides gradient updates during\nbackpropagation. We present strong empirical results (performing over 600\nexperiments) in four different domains: unsupervised (image) representation\nlearning, variational (image) density estimation, image classification, and\nsequence sorting to demonstrate that our proposed method improves state of the\nart performance. We demonstrate that training with DAB aided discrete\nnon-differentiable functions improves image reconstruction quality and\nposterior linear separability by 10% against the Gumbel-Softmax relaxed\nestimator [37, 26] as well as providing a 9% improvement in the test\nvariational lower bound in comparison to the state of the art RELAX [16]\ndiscrete estimator. We also observe an accuracy improvement of 77% in neural\nsequence sorting and a 25% improvement against the straight-through estimator\n[5] in an image classification setting. The DAB network is not used for\ninference and expands the class of functions that are usable in neural\nnetworks.\n", "versions": [{"version": "v1", "created": "Thu, 9 May 2019 14:31:59 GMT"}, {"version": "v2", "created": "Thu, 22 Aug 2019 13:46:02 GMT"}, {"version": "v3", "created": "Sat, 26 Oct 2019 01:41:50 GMT"}], "update_date": "2019-10-29", "authors_parsed": [["Ramapuram", "Jason", ""], ["Webb", "Russ", ""]]}, {"id": "1905.03670", "submitter": "Xiaohua Zhai", "authors": "Xiaohua Zhai, Avital Oliver, Alexander Kolesnikov, Lucas Beyer", "title": "S4L: Self-Supervised Semi-Supervised Learning", "comments": "All four authors contributed equally", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  This work tackles the problem of semi-supervised learning of image\nclassifiers. Our main insight is that the field of semi-supervised learning can\nbenefit from the quickly advancing field of self-supervised visual\nrepresentation learning. Unifying these two approaches, we propose the\nframework of self-supervised semi-supervised learning and use it to derive two\nnovel semi-supervised image classification methods. We demonstrate the\neffectiveness of these methods in comparison to both carefully tuned baselines,\nand existing semi-supervised learning methods. We then show that our approach\nand existing semi-supervised methods can be jointly trained, yielding a new\nstate-of-the-art result on semi-supervised ILSVRC-2012 with 10% of labels.\n", "versions": [{"version": "v1", "created": "Thu, 9 May 2019 14:55:56 GMT"}, {"version": "v2", "created": "Tue, 23 Jul 2019 13:31:21 GMT"}], "update_date": "2019-07-24", "authors_parsed": [["Zhai", "Xiaohua", ""], ["Oliver", "Avital", ""], ["Kolesnikov", "Alexander", ""], ["Beyer", "Lucas", ""]]}, {"id": "1905.03672", "submitter": "Jintao Zhang", "authors": "Jintao Zhang", "title": "Seesaw-Net: Convolution Neural Network With Uneven Group Convolution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we are interested in boosting the representation capability of\nconvolution neural networks which utilizing the inverted residual structure.\nBased on the success of Inverted Residual structure[Sandler et al. 2018] and\nInterleaved Low-Rank Group Convolutions[Sun et al. 2018], we rethink this two\npattern of neural network structure, rather than NAS(Neural architecture\nsearch) method[Zoph and Le 2017; Pham et al. 2018; Liu et al. 2018b], we\nintroduce uneven point-wise group convolution, which provide a novel search\nspace for designing basic blocks to obtain better trade-off between\nrepresentation capability and computational cost. Meanwhile, we propose two\nnovel information flow patterns that will enable cross-group information flow\nfor multiple group convolution layers with and without any channel\npermute/shuffle operation. Dense experiments on image classification task show\nthat our proposed model, named Seesaw-Net, achieves state-of-the-art(SOTA)\nperformance with limited computation and memory cost. Our code will be\nopen-source and available together with pre-trained models.\n", "versions": [{"version": "v1", "created": "Thu, 9 May 2019 14:56:59 GMT"}, {"version": "v2", "created": "Wed, 15 May 2019 13:10:09 GMT"}, {"version": "v3", "created": "Sun, 4 Aug 2019 04:05:03 GMT"}, {"version": "v4", "created": "Wed, 7 Aug 2019 03:21:21 GMT"}, {"version": "v5", "created": "Sun, 1 Dec 2019 12:48:35 GMT"}], "update_date": "2019-12-03", "authors_parsed": [["Zhang", "Jintao", ""]]}, {"id": "1905.03677", "submitter": "Donggeun Yoo", "authors": "Donggeun Yoo, In So Kweon", "title": "Learning Loss for Active Learning", "comments": "Accepted to CVPR 2019 (Oral)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The performance of deep neural networks improves with more annotated data.\nThe problem is that the budget for annotation is limited. One solution to this\nis active learning, where a model asks human to annotate data that it perceived\nas uncertain. A variety of recent methods have been proposed to apply active\nlearning to deep networks but most of them are either designed specific for\ntheir target tasks or computationally inefficient for large networks. In this\npaper, we propose a novel active learning method that is simple but\ntask-agnostic, and works efficiently with the deep networks. We attach a small\nparametric module, named \"loss prediction module,\" to a target network, and\nlearn it to predict target losses of unlabeled inputs. Then, this module can\nsuggest data that the target model is likely to produce a wrong prediction.\nThis method is task-agnostic as networks are learned from a single loss\nregardless of target tasks. We rigorously validate our method through image\nclassification, object detection, and human pose estimation, with the recent\nnetwork architectures. The results demonstrate that our method consistently\noutperforms the previous methods over the tasks.\n", "versions": [{"version": "v1", "created": "Thu, 9 May 2019 15:03:48 GMT"}], "update_date": "2019-05-10", "authors_parsed": [["Yoo", "Donggeun", ""], ["Kweon", "In So", ""]]}, {"id": "1905.03678", "submitter": "Maxim Tatarchenko", "authors": "Maxim Tatarchenko, Stephan R. Richter, Ren\\'e Ranftl, Zhuwen Li,\n  Vladlen Koltun, Thomas Brox", "title": "What Do Single-view 3D Reconstruction Networks Learn?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional networks for single-view object reconstruction have shown\nimpressive performance and have become a popular subject of research. All\nexisting techniques are united by the idea of having an encoder-decoder network\nthat performs non-trivial reasoning about the 3D structure of the output space.\nIn this work, we set up two alternative approaches that perform image\nclassification and retrieval respectively. These simple baselines yield better\nresults than state-of-the-art methods, both qualitatively and quantitatively.\nWe show that encoder-decoder methods are statistically indistinguishable from\nthese baselines, thus indicating that the current state of the art in\nsingle-view object reconstruction does not actually perform reconstruction but\nimage classification. We identify aspects of popular experimental procedures\nthat elicit this behavior and discuss ways to improve the current state of\nresearch.\n", "versions": [{"version": "v1", "created": "Thu, 9 May 2019 15:07:57 GMT"}], "update_date": "2019-05-10", "authors_parsed": [["Tatarchenko", "Maxim", ""], ["Richter", "Stephan R.", ""], ["Ranftl", "Ren\u00e9", ""], ["Li", "Zhuwen", ""], ["Koltun", "Vladlen", ""], ["Brox", "Thomas", ""]]}, {"id": "1905.03681", "submitter": "Olly Styles", "authors": "Olly Styles, Arun Ross, Victor Sanchez", "title": "Forecasting Pedestrian Trajectory with Machine-Annotated Training Data", "comments": "6 pages, 5 figures. To appear in the proceedings of the 2019 IEEE\n  Intelligent Vehicles Symposium (IV)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reliable anticipation of pedestrian trajectory is imperative for the\noperation of autonomous vehicles and can significantly enhance the\nfunctionality of advanced driver assistance systems. While significant progress\nhas been made in the field of pedestrian detection, forecasting pedestrian\ntrajectories remains a challenging problem due to the unpredictable nature of\npedestrians and the huge space of potentially useful features. In this work, we\npresent a deep learning approach for pedestrian trajectory forecasting using a\nsingle vehicle-mounted camera. Deep learning models that have revolutionized\nother areas in computer vision have seen limited application to trajectory\nforecasting, in part due to the lack of richly annotated training data. We\naddress the lack of training data by introducing a scalable machine annotation\nscheme that enables our model to be trained using a large dataset without human\nannotation. In addition, we propose Dynamic Trajectory Predictor (DTP), a model\nfor forecasting pedestrian trajectory up to one second into the future. DTP is\ntrained using both human and machine-annotated data, and anticipates dynamic\nmotion that is not captured by linear models. Experimental evaluation confirms\nthe benefits of the proposed model.\n", "versions": [{"version": "v1", "created": "Thu, 9 May 2019 15:13:36 GMT"}], "update_date": "2019-05-10", "authors_parsed": [["Styles", "Olly", ""], ["Ross", "Arun", ""], ["Sanchez", "Victor", ""]]}, {"id": "1905.03691", "submitter": "Wei Yan", "authors": "Wei Yan, Yiting shao, Shan Liu, Thomas H Li, Zhu Li, Ge Li", "title": "Deep AutoEncoder-based Lossy Geometry Compression for Point Clouds", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Point cloud is a fundamental 3D representation which is widely used in real\nworld applications such as autonomous driving. As a newly-developed media\nformat which is characterized by complexity and irregularity, point cloud\ncreates a need for compression algorithms which are more flexible than existing\ncodecs. Recently, autoencoders(AEs) have shown their effectiveness in many\nvisual analysis tasks as well as image compression, which inspires us to employ\nit in point cloud compression. In this paper, we propose a general\nautoencoder-based architecture for lossy geometry point cloud compression. To\nthe best of our knowledge, it is the first autoencoder-based geometry\ncompression codec that directly takes point clouds as input rather than voxel\ngrids or collections of images. Compared with handcrafted codecs, this approach\nadapts much more quickly to previously unseen media contents and media formats,\nmeanwhile achieving competitive performance. Our architecture consists of a\npointnet-based encoder, a uniform quantizer, an entropy estimation block and a\nnonlinear synthesis transformation module. In lossy geometry compression of\npoint cloud, results show that the proposed method outperforms the test model\nfor categories 1 and 3 (TMC13) published by MPEG-3DG group on the 125th\nmeeting, and on average a 73.15\\% BD-rate gain is achieved.\n", "versions": [{"version": "v1", "created": "Thu, 18 Apr 2019 02:44:50 GMT"}], "update_date": "2019-05-10", "authors_parsed": [["Yan", "Wei", ""], ["shao", "Yiting", ""], ["Liu", "Shan", ""], ["Li", "Thomas H", ""], ["Li", "Zhu", ""], ["Li", "Ge", ""]]}, {"id": "1905.03692", "submitter": "Isaac Ronald Ward", "authors": "Isaac Ronald Ward, M. A. Asim K. Jalwana and Mohammed Bennamoun", "title": "Improving Image-Based Localization with Deep Learning: The Impact of the\n  Loss Function", "comments": "Version 2", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work investigates the impact of the loss function on the performance of\nNeural Networks, in the context of a monocular, RGB-only, image localization\ntask. A common technique used when regressing a camera's pose from an image is\nto formulate the loss as a linear combination of positional and rotational mean\nsquared error (using tuned hyperparameters as coefficients). In this work we\nobserve that changes to rotation and position mutually affect the captured\nimage, and in order to improve performance, a pose regression network's loss\nfunction should include a term which combines the error of both of these\ncoupled quantities. Based on task specific observations and experimental\ntuning, we present said loss term, and create a new model by appending this\nloss term to the loss function of the pre-existing pose regression network\n`PoseNet'. We achieve improvements in the localization accuracy of the network\nfor indoor scenes; with decreases of up to 26.7% and 24.0% in the median\npositional and rotational error respectively, when compared to the default\nPoseNet.\n", "versions": [{"version": "v1", "created": "Sun, 28 Apr 2019 05:47:37 GMT"}, {"version": "v2", "created": "Sun, 28 Jul 2019 13:06:02 GMT"}], "update_date": "2019-07-30", "authors_parsed": [["Ward", "Isaac Ronald", ""], ["Jalwana", "M. A. Asim K.", ""], ["Bennamoun", "Mohammed", ""]]}, {"id": "1905.03694", "submitter": "Mingbao Lin", "authors": "Mingbao Lin, Rongrong Ji, Hong Liu, Yongjian Liu", "title": "Supervised Online Hashing via Hadamard Codebook Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, binary code learning, a.k.a hashing, has received extensive\nattention in large-scale multimedia retrieval. It aims to encode\nhigh-dimensional data points to binary codes, hence the original\nhigh-dimensional metric space can be efficiently approximated via Hamming\nspace. However, most existing hashing methods adopted offline batch learning,\nwhich is not suitable to handle incremental datasets with streaming data or new\ninstances. In contrast, the robustness of the existing online hashing remains\nas an open problem, while the embedding of supervised/semantic information\nhardly boosts the performance of the online hashing, mainly due to the defect\nof unknown category numbers in supervised learning. In this paper, we proposed\nan online hashing scheme, termed Hadamard Codebook based Online Hashing (HCOH),\nwhich aims to solve the above problems towards robust and supervised online\nhashing. In particular, we first assign an appropriate high-dimensional binary\ncodes to each class label, which is generated randomly by Hadamard codes to\neach class label, which is generated randomly by Hadamard codes. Subsequently,\nLSH is adopted to reduce the length of such Hadamard codes in accordance with\nthe hash bits, which can adapt the predefined binary codes online, and\ntheoretically guarantee the semantic similarity. Finally, we consider the\nsetting of stochastic data acquisition, which facilitates our method to\nefficiently learn the corresponding hashing functions via stochastic gradient\ndescend (SGD) online. Notably, the proposed HCOH can be embedded with\nsupervised labels and it not limited to a predefined category number. Extensive\nexperiments on three widely-used benchmarks demonstrate the merits of the\nproposed scheme over the state-of-the-art methods. The code is available at\nhttps://github.com/lmbxmu/mycode/tree/master/2018ACMMM_HCOH.\n", "versions": [{"version": "v1", "created": "Sun, 28 Apr 2019 02:33:53 GMT"}, {"version": "v2", "created": "Fri, 24 May 2019 18:02:53 GMT"}], "update_date": "2019-05-28", "authors_parsed": [["Lin", "Mingbao", ""], ["Ji", "Rongrong", ""], ["Liu", "Hong", ""], ["Liu", "Yongjian", ""]]}, {"id": "1905.03695", "submitter": "Kwang Woo Nam", "authors": "Wei Ding, KwangSoo Yang and Kwang Woo Nam", "title": "Measuring similarity between geo-tagged videos using largest common view", "comments": "2 pages", "journal-ref": "IET electronics letters, vol.55, no. 8, pp.450-452, 2019", "doi": "10.1049/el.2018.7499", "report-no": null, "categories": "cs.CV cs.DB cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a novel problem for discovering the similar trajectories\nbased on the field of view (FoV) of the video data. The problem is important\nfor many societal applications such as grouping moving objects, classifying\ngeo-images, and identifying the interesting trajectory patterns. Prior work\nconsider only either spatial locations or spatial relationship between two\nline-segments. However, these approaches show a limitation to find the similar\nmoving objects with common views. In this paper, we propose new algorithm that\ncan group both spatial locations and points of view to identify similar\ntrajectories. We also propose novel methods that reduce the computational cost\nfor the proposed work. Experimental results using real-world datasets\ndemonstrates that the proposed approach outperforms prior work and reduces the\ncomputational cost.\n", "versions": [{"version": "v1", "created": "Sun, 28 Apr 2019 14:46:06 GMT"}], "update_date": "2019-05-10", "authors_parsed": [["Ding", "Wei", ""], ["Yang", "KwangSoo", ""], ["Nam", "Kwang Woo", ""]]}, {"id": "1905.03696", "submitter": "Amir Gholami", "authors": "Zhen Dong, Zhewei Yao, Amir Gholami, Michael Mahoney, Kurt Keutzer", "title": "HAWQ: Hessian AWare Quantization of Neural Networks with Mixed-Precision", "comments": "ICCV 2019", "journal-ref": "ICCV 2019 paper", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Model size and inference speed/power have become a major challenge in the\ndeployment of Neural Networks for many applications. A promising approach to\naddress these problems is quantization. However, uniformly quantizing a model\nto ultra low precision leads to significant accuracy degradation. A novel\nsolution for this is to use mixed-precision quantization, as some parts of the\nnetwork may allow lower precision as compared to other layers. However, there\nis no systematic way to determine the precision of different layers. A brute\nforce approach is not feasible for deep networks, as the search space for\nmixed-precision is exponential in the number of layers. Another challenge is a\nsimilar factorial complexity for determining block-wise fine-tuning order when\nquantizing the model to a target precision. Here, we introduce Hessian AWare\nQuantization (HAWQ), a novel second-order quantization method to address these\nproblems. HAWQ allows for the automatic selection of the relative quantization\nprecision of each layer, based on the layer's Hessian spectrum. Moreover, HAWQ\nprovides a deterministic fine-tuning order for quantizing layers, based on\nsecond-order information. We show the results of our method on Cifar-10 using\nResNet20, and on ImageNet using Inception-V3, ResNet50 and SqueezeNext models.\nComparing HAWQ with state-of-the-art shows that we can achieve similar/better\naccuracy with $8\\times$ activation compression ratio on ResNet20, as compared\nto DNAS~\\cite{wu2018mixed}, and up to $1\\%$ higher accuracy with up to $14\\%$\nsmaller models on ResNet50 and Inception-V3, compared to recently proposed\nmethods of RVQuant~\\cite{park2018value} and HAQ~\\cite{wang2018haq}.\nFurthermore, we show that we can quantize SqueezeNext to just 1MB model size\nwhile achieving above $68\\%$ top1 accuracy on ImageNet.\n", "versions": [{"version": "v1", "created": "Mon, 29 Apr 2019 06:49:08 GMT"}], "update_date": "2020-03-29", "authors_parsed": [["Dong", "Zhen", ""], ["Yao", "Zhewei", ""], ["Gholami", "Amir", ""], ["Mahoney", "Michael", ""], ["Keutzer", "Kurt", ""]]}, {"id": "1905.03697", "submitter": "J{\\o}rgen Falck Erichsen M.Sc.", "authors": "Jorgen F. Erichsen, Sampsa Kohtala, Martin Steinert, Torgeir Welo", "title": "On Applying Machine Learning/Object Detection Models for Analysing\n  Digitally Captured Physical Prototypes from Engineering Design Projects", "comments": "13 pages, 4 tables, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While computer vision has received increasing attention in computer science\nover the last decade, there are few efforts in applying this to leverage\nengineering design research. Existing datasets and technologies allow\nresearchers to capture and access more observations and video files, hence\nanalysis is becoming a limiting factor. Therefore, this paper is investigating\nthe application of machine learning, namely object detection methods to aid in\nthe analysis of physical porotypes. With access to a large dataset of digitally\ncaptured physical prototypes from early-stage development projects (5950 images\nfrom 850 prototypes), the authors investigate applications that can be used for\nanalysing this dataset. The authors retrained two pre-trained object detection\nmodels from two known frameworks, the TensorFlow Object Detection API and\nDarknet, using custom image sets of images of physical prototypes. As a result,\na proof-of-concept of four trained models are presented; two models for\ndetecting samples of wood-based sheet materials and two models for detecting\nsamples containing microcontrollers. All models are evaluated using standard\nmetrics for object detection model performance and the applicability of using\nobject detection models in engineering design research is discussed. Results\nindicate that the models can successfully classify the type of material and\ntype of pre-made component, respectively. However, more work is needed to fully\nintegrate object detection models in the engineering design analysis workflow.\nThe authors also extrapolate that the use of object detection for analysing\nimages of physical prototypes will substantially reduce the effort required for\nanalysing large datasets in engineering design research.\n", "versions": [{"version": "v1", "created": "Tue, 7 May 2019 07:33:53 GMT"}], "update_date": "2019-05-16", "authors_parsed": [["Erichsen", "Jorgen F.", ""], ["Kohtala", "Sampsa", ""], ["Steinert", "Martin", ""], ["Welo", "Torgeir", ""]]}, {"id": "1905.03698", "submitter": "Yongshun Gong", "authors": "Yongshun Gong, Jinfeng Yi, Dongdong Chen, Jian Zhang, Jiayu Zhou,\n  Zhihua Zhou", "title": "Inferring the Importance of Product Appearance: A Step Towards the\n  Screenless Revolution", "comments": "We want to withdraw this paper. there are some insufficient\n  explanations and bad figure presentation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nowadays, almost all the online orders were placed through screened devices\nsuch as mobile phones, tablets, and computers. With the rapid development of\nthe Internet of Things (IoT) and smart appliances, more and more screenless\nsmart devices, e.g., smart speaker and smart refrigerator, appear in our daily\nlives. They open up new means of interaction and may provide an excellent\nopportunity to reach new customers and increase sales. However, not all the\nitems are suitable for screenless shopping, since some items' appearance play\nan important role in consumer decision making. Typical examples include\nclothes, dolls, bags, and shoes. In this paper, we aim to infer the\nsignificance of every item's appearance in consumer decision making and\nidentify the group of items that are suitable for screenless shopping.\nSpecifically, we formulate the problem as a classification task that predicts\nif an item's appearance has a significant impact on people's purchase behavior.\nTo solve this problem, we extract features from three different views, namely\nitems' intrinsic properties, items' images, and users' comments, and collect a\nset of necessary labels via crowdsourcing. We then propose an iterative\nsemi-supervised learning framework with three carefully designed loss\nfunctions. We conduct extensive experiments on a real-world transaction dataset\ncollected from the online retail giant JD.com. Experimental results verify the\neffectiveness of the proposed method.\n", "versions": [{"version": "v1", "created": "Wed, 1 May 2019 00:21:43 GMT"}, {"version": "v2", "created": "Sat, 18 May 2019 09:43:24 GMT"}], "update_date": "2019-05-21", "authors_parsed": [["Gong", "Yongshun", ""], ["Yi", "Jinfeng", ""], ["Chen", "Dongdong", ""], ["Zhang", "Jian", ""], ["Zhou", "Jiayu", ""], ["Zhou", "Zhihua", ""]]}, {"id": "1905.03699", "submitter": "Emad Ul Haq Qazi", "authors": "Helala AlShehri, Muhammad Hussain, Hatim AboAlSamh, Qazi Emad-ul-Haq,\n  and Aqil M. Azmi", "title": "Alignment-Free Cross-Sensor Fingerprint Matching based on the\n  Co-Occurrence of Ridge Orientations and Gabor-HoG Descriptor", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The existing automatic fingerprint verification methods are designed to work\nunder the assumption that the same sensor is installed for enrollment and\nauthentication (regular matching). There is a remarkable decrease in efficiency\nwhen one type of contact-based sensor is employed for enrolment and another\ntype of contact-based sensor is used for authentication (cross-matching or\nfingerprint sensor interoperability problem,). The ridge orientation patterns\nin a fingerprint are invariant to sensor type. Based on this observation, we\npropose a robust fingerprint descriptor called the co-occurrence of ridge\norientations (Co-Ror), which encodes the spatial distribution of ridge\norientations. Employing this descriptor, we introduce an efficient automatic\nfingerprint verification method for cross-matching problem. Further, to enhance\nthe robustness of the method, we incorporate scale based ridge orientation\ninformation through Gabor-HoG descriptor. The two descriptors are fused with\ncanonical correlation analysis (CCA), and the matching score between two\nfingerprints is calculated using city-block distance. The proposed method is\nalignment-free and can handle the matching process without the need for a\nregistration step. The intensive experiments on two benchmark databases\n(FingerPass and MOLF) show the effectiveness of the method and reveal its\nsignificant enhancement over the state-of-the-art methods such as VeriFinger (a\ncommercial SDK), minutia cylinder-code (MCC), MCC with scale, and the\nthin-plate spline (TPS) model. The proposed research will help security\nagencies, service providers and law-enforcement departments to overcome the\ninteroperability problem of contact sensors of different technology and\ninteraction types.\n", "versions": [{"version": "v1", "created": "Tue, 30 Apr 2019 13:26:04 GMT"}], "update_date": "2019-05-10", "authors_parsed": [["AlShehri", "Helala", ""], ["Hussain", "Muhammad", ""], ["AboAlSamh", "Hatim", ""], ["Emad-ul-Haq", "Qazi", ""], ["Azmi", "Aqil M.", ""]]}, {"id": "1905.03700", "submitter": "Birgitta Dresp-Langley", "authors": "John M. Wandeto, Birgitta Dresp-Langley", "title": "Unsupervised automatic classification of Scanning Electron Microscopy\n  (SEM) images of CD4+ cells with varying extent of HIV virion infection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Archiving large sets of medical or cell images in digital libraries may\nrequire ordering randomly scattered sets of image data according to specific\ncriteria, such as the spatial extent of a specific local color or contrast\ncontent that reveals different meaningful states of a physiological structure,\ntissue, or cell in a certain order, indicating progression or recession of a\npathology, or the progressive response of a cell structure to treatment. Here\nwe used a Self Organized Map (SOM)-based, fully automatic and unsupervised,\nclassification procedure described in our earlier work and applied it to sets\nof minimally processed grayscale and/or color processed Scanning Electron\nMicroscopy (SEM) images of CD4+ T-lymphocytes (so-called helper cells) with\nvarying extent of HIV virion infection. It is shown that the quantization error\nin the SOM output after training permits to scale the spatial magnitude and the\ndirection of change (+ or -) in local pixel contrast or color across images of\na series with a reliability that exceeds that of any human expert. The\nprocedure is easily implemented and fast, and represents a promising step\ntowards low-cost automatic digital image archiving with minimal intervention of\na human operator.\n", "versions": [{"version": "v1", "created": "Tue, 30 Apr 2019 15:15:21 GMT"}], "update_date": "2019-05-10", "authors_parsed": [["Wandeto", "John M.", ""], ["Dresp-Langley", "Birgitta", ""]]}, {"id": "1905.03702", "submitter": "Sachin Talathi", "authors": "Stephan J. Garbin, Yiru Shen, Immo Schuetz, Robert Cavin, Gregory\n  Hughes, and Sachin S. Talathi", "title": "OpenEDS: Open Eye Dataset", "comments": "11 pages; 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a large scale data set, OpenEDS: Open Eye Dataset, of eye-images\ncaptured using a virtual-reality (VR) head mounted display mounted with two\nsynchronized eyefacing cameras at a frame rate of 200 Hz under controlled\nillumination. This dataset is compiled from video capture of the eye-region\ncollected from 152 individual participants and is divided into four subsets:\n(i) 12,759 images with pixel-level annotations for key eye-regions: iris, pupil\nand sclera (ii) 252,690 unlabelled eye-images, (iii) 91,200 frames from\nrandomly selected video sequence of 1.5 seconds in duration and (iv) 143 pairs\nof left and right point cloud data compiled from corneal topography of eye\nregions collected from a subset, 143 out of 152, participants in the study. A\nbaseline experiment has been evaluated on OpenEDS for the task of semantic\nsegmentation of pupil, iris, sclera and background, with the mean\nintersectionover-union (mIoU) of 98.3 %. We anticipate that OpenEDS will create\nopportunities to researchers in the eye tracking community and the broader\nmachine learning and computer vision community to advance the state of\neye-tracking for VR applications. The dataset is available for download upon\nrequest at https://research.fb.com/programs/openeds-challenge\n", "versions": [{"version": "v1", "created": "Tue, 30 Apr 2019 17:47:53 GMT"}, {"version": "v2", "created": "Fri, 17 May 2019 16:18:02 GMT"}], "update_date": "2019-05-20", "authors_parsed": [["Garbin", "Stephan J.", ""], ["Shen", "Yiru", ""], ["Schuetz", "Immo", ""], ["Cavin", "Robert", ""], ["Hughes", "Gregory", ""], ["Talathi", "Sachin S.", ""]]}, {"id": "1905.03703", "submitter": "Luisa Polania", "authors": "Luisa F. Polania and Satyajit Gupte", "title": "Learning fashion compatibility across apparel categories for outfit\n  recommendation", "comments": "Accepted for publication at ICIP 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the problem of generating recommendations for completing\nthe outfit given that a user is interested in a particular apparel item. The\nproposed method is based on a siamese network used for feature extraction\nfollowed by a fully-connected network used for learning a fashion compatibility\nmetric. The embeddings generated by the siamese network are augmented with\ncolor histogram features motivated by the important role that color plays in\ndetermining fashion compatibility. The training of the network is formulated as\na maximum a posteriori (MAP) problem where Laplacian distributions are assumed\nfor the filters of the siamese network to promote sparsity and matrix-variate\nnormal distributions are assumed for the weights of the metric network to\nefficiently exploit correlations between the input units of each\nfully-connected layer.\n", "versions": [{"version": "v1", "created": "Wed, 1 May 2019 05:37:01 GMT"}], "update_date": "2019-05-10", "authors_parsed": [["Polania", "Luisa F.", ""], ["Gupte", "Satyajit", ""]]}, {"id": "1905.03704", "submitter": "Yuenan Hou", "authors": "Yuenan Hou", "title": "Agnostic Lane Detection", "comments": "6 pages, 2 figures, our codes are available at\n  https://github.com/cardwing/Codes-for-Lane-Detection", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Lane detection is an important yet challenging task in autonomous driving,\nwhich is affected by many factors, e.g., light conditions, occlusions caused by\nother vehicles, irrelevant markings on the road and the inherent long and thin\nproperty of lanes. Conventional methods typically treat lane detection as a\nsemantic segmentation task, which assigns a class label to each pixel of the\nimage. This formulation heavily depends on the assumption that the number of\nlanes is pre-defined and fixed and no lane changing occurs, which does not\nalways hold. To make the lane detection model applicable to an arbitrary number\nof lanes and lane changing scenarios, we adopt an instance segmentation\napproach, which first differentiates lanes and background and then classify\neach lane pixel into each lane instance. Besides, a multi-task learning\nparadigm is utilized to better exploit the structural information and the\nfeature pyramid architecture is used to detect extremely thin lanes. Three\npopular lane detection benchmarks, i.e., TuSimple, CULane and BDD100K, are used\nto validate the effectiveness of our proposed algorithm.\n", "versions": [{"version": "v1", "created": "Thu, 2 May 2019 05:58:17 GMT"}], "update_date": "2019-05-10", "authors_parsed": [["Hou", "Yuenan", ""]]}, {"id": "1905.03705", "submitter": "Tao Wang", "authors": "Tao Wang and Anup Basu", "title": "A note on 'A fully parallel 3D thinning algorithm and its applications'", "comments": null, "journal-ref": "Pattern Recognition Letters, 2007", "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A 3D thinning algorithm erodes a 3D binary image layer by layer to extract\nthe skeletons. This paper presents a correction to Ma and Sonka's thinning\nalgorithm, A fully parallel 3D thinning algorithm and its applications, which\nfails to preserve connectivity of 3D objects. We start with Ma and Sonka's\nalgorithm and examine its verification of connectivity preservation. Our\nanalysis leads to a group of different deleting templates, which can preserve\nconnectivity of 3D objects.\n", "versions": [{"version": "v1", "created": "Wed, 1 May 2019 19:57:49 GMT"}], "update_date": "2019-05-10", "authors_parsed": [["Wang", "Tao", ""], ["Basu", "Anup", ""]]}, {"id": "1905.03706", "submitter": "Elad Levi", "authors": "Eli Brosh, Matan Friedmann, Ilan Kadar, Lev Yitzhak Lavy, Elad Levi,\n  Shmuel Rippa, Yair Lempert, Bruno Fernandez-Ruiz, Roei Herzig, Trevor Darrell", "title": "Accurate Visual Localization for Automotive Applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurate vehicle localization is a crucial step towards building effective\nVehicle-to-Vehicle networks and automotive applications. Yet standard grade GPS\ndata, such as that provided by mobile phones, is often noisy and exhibits\nsignificant localization errors in many urban areas. Approaches for accurate\nlocalization from imagery often rely on structure-based techniques, and thus\nare limited in scale and are expensive to compute. In this paper, we present a\nscalable visual localization approach geared for real-time performance. We\npropose a hybrid coarse-to-fine approach that leverages visual and GPS location\ncues. Our solution uses a self-supervised approach to learn a compact road\nimage representation. This representation enables efficient visual retrieval\nand provides coarse localization cues, which are fused with vehicle ego-motion\nto obtain high accuracy location estimates. As a benchmark to evaluate the\nperformance of our visual localization approach, we introduce a new large-scale\ndriving dataset based on video and GPS data obtained from a large-scale network\nof connected dash-cams. Our experiments confirm that our approach is highly\neffective in challenging urban environments, reducing localization error by an\norder of magnitude.\n", "versions": [{"version": "v1", "created": "Wed, 1 May 2019 15:48:10 GMT"}], "update_date": "2019-05-10", "authors_parsed": [["Brosh", "Eli", ""], ["Friedmann", "Matan", ""], ["Kadar", "Ilan", ""], ["Lavy", "Lev Yitzhak", ""], ["Levi", "Elad", ""], ["Rippa", "Shmuel", ""], ["Lempert", "Yair", ""], ["Fernandez-Ruiz", "Bruno", ""], ["Herzig", "Roei", ""], ["Darrell", "Trevor", ""]]}, {"id": "1905.03707", "submitter": "Reza Malekian Ph.D.", "authors": "Schalk Wilhelm Pienaar, Reza Malekian", "title": "Human Activity Recognition Using Visual Object Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual Human Activity Recognition (HAR) and data fusion with other sensors\ncan help us at tracking the behavior and activity of underground miners with\nlittle obstruction. Existing models, such as Single Shot Detector (SSD),\ntrained on the Common Objects in Context (COCO) dataset is used in this paper\nto detect the current state of a miner, such as an injured miner vs a\nnon-injured miner. Tensorflow is used for the abstraction layer of implementing\nmachine learning algorithms, and although it uses Python to deal with nodes and\ntensors, the actual algorithms run on C++ libraries, providing a good balance\nbetween performance and speed of development. The paper further discusses\nevaluation methods for determining the accuracy of the machine-learning and an\napproach to increase the accuracy of the detected activity/state of people in a\nmining environment, by means of data fusion.\n", "versions": [{"version": "v1", "created": "Thu, 2 May 2019 07:17:51 GMT"}], "update_date": "2019-05-10", "authors_parsed": [["Pienaar", "Schalk Wilhelm", ""], ["Malekian", "Reza", ""]]}, {"id": "1905.03708", "submitter": "Javad Ghofrani", "authors": "Javad Ghofrani, Robert Kirschne, Daniel Rossburg, Dirk Reichelt, Tom\n  Dimter", "title": "Machine Vision in the Context of Robotics: A Systematic Literature\n  Review", "comments": "10 pages 5 figures, systematic literature study", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.RO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine vision is critical to robotics due to a wide range of applications\nwhich rely on input from visual sensors such as autonomous mobile robots and\nsmart production systems. To create the smart homes and systems of tomorrow, an\noverview about current challenges in the research field would be of use to\nidentify further possible directions, created in a systematic and reproducible\nmanner. In this work a systematic literature review was conducted covering\nresearch from the last 10 years. We screened 172 papers from four databases and\nselected 52 relevant papers. While robustness and computation time were\nimproved greatly, occlusion and lighting variance are still the biggest\nproblems faced. From the number of recent publications, we conclude that the\nobserved field is of relevance and interest to the research community. Further\nchallenges arise in many areas of the field.\n", "versions": [{"version": "v1", "created": "Fri, 3 May 2019 09:00:07 GMT"}], "update_date": "2019-05-10", "authors_parsed": [["Ghofrani", "Javad", ""], ["Kirschne", "Robert", ""], ["Rossburg", "Daniel", ""], ["Reichelt", "Dirk", ""], ["Dimter", "Tom", ""]]}, {"id": "1905.03709", "submitter": "Victor Schmidt", "authors": "Victor Schmidt, Alexandra Luccioni, S. Karthik Mukkavilli, Narmada\n  Balasooriya, Kris Sankaran, Jennifer Chayes, Yoshua Bengio", "title": "Visualizing the Consequences of Climate Change Using Cycle-Consistent\n  Adversarial Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present a project that aims to generate images that depict accurate,\nvivid, and personalized outcomes of climate change using Cycle-Consistent\nAdversarial Networks (CycleGANs). By training our CycleGAN model on street-view\nimages of houses before and after extreme weather events (e.g. floods, forest\nfires, etc.), we learn a mapping that can then be applied to images of\nlocations that have not yet experienced these events. This visual\ntransformation is paired with climate model predictions to assess likelihood\nand type of climate-related events in the long term (50 years) in order to\nbring the future closer in the viewers mind. The eventual goal of our project\nis to enable individuals to make more informed choices about their climate\nfuture by creating a more visceral understanding of the effects of climate\nchange, while maintaining scientific credibility by drawing on climate model\nprojections.\n", "versions": [{"version": "v1", "created": "Thu, 2 May 2019 15:34:53 GMT"}], "update_date": "2019-05-10", "authors_parsed": [["Schmidt", "Victor", ""], ["Luccioni", "Alexandra", ""], ["Mukkavilli", "S. Karthik", ""], ["Balasooriya", "Narmada", ""], ["Sankaran", "Kris", ""], ["Chayes", "Jennifer", ""], ["Bengio", "Yoshua", ""]]}, {"id": "1905.03710", "submitter": "Xiaorui Zhu", "authors": "Lijun Yan, Jun-Bao Li, Xiaorui Zhu, Jeng-Shyang Pan and Linlin Tang", "title": "Bilinear discriminant feature line analysis for image feature extraction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A novel bilinear discriminant feature line analysis (BDFLA) is proposed for\nimage feature extraction. The nearest feature line (NFL) is a powerful\nclassifier. Some NFL-based subspace algorithms were introduced recently. In\nmost of the classical NFL-based subspace learning approaches, the input samples\nare vectors. For image classification tasks, the image samples should be\ntransformed to vectors first. This process induces a high computational\ncomplexity and may also lead to loss of the geometric feature of samples. The\nproposed BDFLA is a matrix-based algorithm. It aims to minimise the\nwithin-class scatter and maximise the between-class scatter based on a\ntwo-dimensional (2D) NFL. Experimental results on two-image databases confirm\nthe effectiveness.\n", "versions": [{"version": "v1", "created": "Fri, 3 May 2019 04:54:43 GMT"}], "update_date": "2019-05-10", "authors_parsed": [["Yan", "Lijun", ""], ["Li", "Jun-Bao", ""], ["Zhu", "Xiaorui", ""], ["Pan", "Jeng-Shyang", ""], ["Tang", "Linlin", ""]]}, {"id": "1905.03711", "submitter": "Angelos Katharopoulos", "authors": "Angelos Katharopoulos and Fran\\c{c}ois Fleuret", "title": "Processing Megapixel Images with Deep Attention-Sampling Models", "comments": "Presented in ICML 2019. Code is available at\n  https://github.com/idiap/attention-sampling", "journal-ref": "Proceedings of the 36th International Conference on Machine\n  Learning, PMLR 97:3282-3291, 2019", "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing deep architectures cannot operate on very large signals such as\nmegapixel images due to computational and memory constraints. To tackle this\nlimitation, we propose a fully differentiable end-to-end trainable model that\nsamples and processes only a fraction of the full resolution input image. The\nlocations to process are sampled from an attention distribution computed from a\nlow resolution view of the input. We refer to our method as attention sampling\nand it can process images of several megapixels with a standard single GPU\nsetup. We show that sampling from the attention distribution results in an\nunbiased estimator of the full model with minimal variance, and we derive an\nunbiased estimator of the gradient that we use to train our model end-to-end\nwith a normal SGD procedure. This new method is evaluated on three\nclassification tasks, where we show that it allows to reduce computation and\nmemory footprint by an order of magnitude for the same accuracy as classical\narchitectures. We also show the consistency of the sampling that indeed focuses\non informative parts of the input images.\n", "versions": [{"version": "v1", "created": "Fri, 3 May 2019 16:27:46 GMT"}, {"version": "v2", "created": "Wed, 17 Jul 2019 15:20:50 GMT"}], "update_date": "2019-07-18", "authors_parsed": [["Katharopoulos", "Angelos", ""], ["Fleuret", "Fran\u00e7ois", ""]]}, {"id": "1905.03715", "submitter": "Kin Ng", "authors": "Kin Ng", "title": "Tuned Inception V3 for Recognizing States of Cooking Ingredients", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cooking is a task that must be performed in a daily basis, and thus it is an\nactivity that many people take for granted. For humans preparing a meal comes\nnaturally, but for robots even preparing a simple sandwich results in an\nextremely difficult task. In robotics, designing kitchen robots is complicated\nsince cooking relies on a variety of physical interactions that are dependent\non different conditions such as changes in the environment, proper execution of\nsequential instructions, along with motions, and detection of the different\nstates in which cooking-ingredients can be in for their correct grasping and\nmanipulation. In this paper, we focus on the challenge of state recognition and\npropose a fine tuned convolutional neural network that makes use of transfer\nlearning by reusing the Inception V3 pre-trained model. The model is trained\nand validated on a cooking dataset consisting of eleven states (e.g. peeled,\ndiced, whole, etc.). The work presented on this paper could provide insight\ninto finding a potential solution to the problem.\n", "versions": [{"version": "v1", "created": "Sun, 5 May 2019 16:38:52 GMT"}], "update_date": "2019-05-10", "authors_parsed": [["Ng", "Kin", ""]]}, {"id": "1905.03716", "submitter": "Yiwu Yao", "authors": "Yiwu Yao, Yuhua Cheng", "title": "Fully Parallel Architecture for Semi-global Stereo Matching with Refined\n  Rank Method", "comments": "stereo matching; SGM; Rank SAD; fully parallel architecture", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fully parallel architecture at disparity-level for efficient semi-global\nmatching (SGM) with refined rank method is presented. The improved SGM\nalgorithm is implemented with the non-parametric unified rank model which is\nthe combination of Rank filter/AD and Rank SAD. Rank SAD is a novel definition\nby introducing the constraints of local image structure into the rank method.\nAs a result, the unified rank model with Rank SAD can make up for the defects\nof Rank filter/AD. Experimental results show both excellent subjective quality\nand objective performance of the refined SGM algorithm. The fully parallel\nconstruction for hardware implementation of SGM is architected with reasonable\nstrategies at disparity-level. The parallelism of the data-stream allows proper\nthroughput for specific applications with acceptable maximum frequency. The\nresults of RTL emulation and synthesis ensure that the proposed parallel\narchitecture is suitable for VLSI implementation.\n", "versions": [{"version": "v1", "created": "Tue, 7 May 2019 02:33:37 GMT"}], "update_date": "2019-05-10", "authors_parsed": [["Yao", "Yiwu", ""], ["Cheng", "Yuhua", ""]]}, {"id": "1905.03721", "submitter": "Amin Parvaneh", "authors": "Amin Parvaneh, Ehsan Abbasnejad, Qi Wu, Javen Qinfeng Shi, Anton van\n  den Hengel", "title": "Show, Price and Negotiate: A Negotiator with Online Value Look-Ahead", "comments": "published in IEEE Transactions on Multimedia", "journal-ref": null, "doi": "10.1109/TMM.2021.3065169", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Negotiation, as an essential and complicated aspect of online shopping, is\nstill challenging for an intelligent agent. To that end, we propose the Price\nNegotiator, a modular deep neural network that addresses the unsolved problems\nin recent studies by (1) considering images of the items as a crucial, though\nneglected, source of information in a negotiation, (2) heuristically finding\nthe most similar items from an external online source to predict the potential\nvalue and an acceptable agreement price, (3) predicting a general price-based\naction at each turn which is fed into the language generator to output the\nsupporting natural language, and (4) adjusting the prices based on the\npredicted actions. Empirically, we show that our model, that is trained in both\nsupervised and reinforcement learning setting, significantly improves\nnegotiation on the CraigslistBargain dataset, in terms of the agreement price,\nprice consistency, and dialogue quality.\n", "versions": [{"version": "v1", "created": "Tue, 7 May 2019 06:20:59 GMT"}, {"version": "v2", "created": "Fri, 12 Mar 2021 07:10:54 GMT"}], "update_date": "2021-03-15", "authors_parsed": [["Parvaneh", "Amin", ""], ["Abbasnejad", "Ehsan", ""], ["Wu", "Qi", ""], ["Shi", "Javen Qinfeng", ""], ["Hengel", "Anton van den", ""]]}, {"id": "1905.03743", "submitter": "Gaurav Mittal", "authors": "Gaurav Mittal, Shubham Agrawal, Anuva Agarwal, Sushant Mehta, Tanya\n  Marwah", "title": "Interactive Image Generation Using Scene Graphs", "comments": "Published at ICLR 2019 Deep Generative Models for Highly Structured\n  Data Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent years have witnessed some exciting developments in the domain of\ngenerating images from scene-based text descriptions. These approaches have\nprimarily focused on generating images from a static text description and are\nlimited to generating images in a single pass. They are unable to generate an\nimage interactively based on an incrementally additive text description\n(something that is more intuitive and similar to the way we describe an image).\nWe propose a method to generate an image incrementally based on a sequence of\ngraphs of scene descriptions (scene-graphs). We propose a recurrent network\narchitecture that preserves the image content generated in previous steps and\nmodifies the cumulative image as per the newly provided scene information. Our\nmodel utilizes Graph Convolutional Networks (GCN) to cater to variable-sized\nscene graphs along with Generative Adversarial image translation networks to\ngenerate realistic multi-object images without needing any intermediate\nsupervision during training. We experiment with Coco-Stuff dataset which has\nmulti-object images along with annotations describing the visual scene and show\nthat our model significantly outperforms other approaches on the same dataset\nin generating visually consistent images for incrementally growing scene\ngraphs.\n", "versions": [{"version": "v1", "created": "Thu, 9 May 2019 16:39:31 GMT"}], "update_date": "2019-05-10", "authors_parsed": [["Mittal", "Gaurav", ""], ["Agrawal", "Shubham", ""], ["Agarwal", "Anuva", ""], ["Mehta", "Sushant", ""], ["Marwah", "Tanya", ""]]}, {"id": "1905.03767", "submitter": "Ashkan Khakzar", "authors": "Ashkan Khakzar, Shadi Albarqouni, Nassir Navab", "title": "Learning Interpretable Features via Adversarially Robust Optimization", "comments": "MICCAI 2019 (Medical Image Computing and Computer Assisted\n  Interventions)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural networks are proven to be remarkably successful for classification and\ndiagnosis in medical applications. However, the ambiguity in the\ndecision-making process and the interpretability of the learned features is a\nmatter of concern. In this work, we propose a method for improving the feature\ninterpretability of neural network classifiers. Initially, we propose a\nbaseline convolutional neural network with state of the art performance in\nterms of accuracy and weakly supervised localization. Subsequently, the loss is\nmodified to integrate robustness to adversarial examples into the training\nprocess. In this work, feature interpretability is quantified via evaluating\nthe weakly supervised localization using the ground truth bounding boxes.\nInterpretability is also visually assessed using class activation maps and\nsaliency maps. The method is applied to NIH ChestX-ray14, the largest publicly\navailable chest x-rays dataset. We demonstrate that the adversarially robust\noptimization paradigm improves feature interpretability both quantitatively and\nvisually.\n", "versions": [{"version": "v1", "created": "Thu, 9 May 2019 17:50:25 GMT"}, {"version": "v2", "created": "Mon, 19 Aug 2019 09:18:32 GMT"}], "update_date": "2019-08-20", "authors_parsed": [["Khakzar", "Ashkan", ""], ["Albarqouni", "Shadi", ""], ["Navab", "Nassir", ""]]}, {"id": "1905.03776", "submitter": "Daniel Park", "authors": "Daniel S. Park, Jascha Sohl-Dickstein, Quoc V. Le, Samuel L. Smith", "title": "The Effect of Network Width on Stochastic Gradient Descent and\n  Generalization: an Empirical Study", "comments": "17 pages, 3 tables, 17 figures; accepted to ICML 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate how the final parameters found by stochastic gradient descent\nare influenced by over-parameterization. We generate families of models by\nincreasing the number of channels in a base network, and then perform a large\nhyper-parameter search to study how the test error depends on learning rate,\nbatch size, and network width. We find that the optimal SGD hyper-parameters\nare determined by a \"normalized noise scale,\" which is a function of the batch\nsize, learning rate, and initialization conditions. In the absence of batch\nnormalization, the optimal normalized noise scale is directly proportional to\nwidth. Wider networks, with their higher optimal noise scale, also achieve\nhigher test accuracy. These observations hold for MLPs, ConvNets, and ResNets,\nand for two different parameterization schemes (\"Standard\" and \"NTK\"). We\nobserve a similar trend with batch normalization for ResNets. Surprisingly,\nsince the largest stable learning rate is bounded, the largest batch size\nconsistent with the optimal normalized noise scale decreases as the width\nincreases.\n", "versions": [{"version": "v1", "created": "Thu, 9 May 2019 17:58:13 GMT"}], "update_date": "2019-05-10", "authors_parsed": [["Park", "Daniel S.", ""], ["Sohl-Dickstein", "Jascha", ""], ["Le", "Quoc V.", ""], ["Smith", "Samuel L.", ""]]}, {"id": "1905.03820", "submitter": "Lele Chen", "authors": "Lele Chen, Ross K. Maddox, Zhiyao Duan, Chenliang Xu", "title": "Hierarchical Cross-Modal Talking Face Generationwith Dynamic Pixel-Wise\n  Loss", "comments": null, "journal-ref": "Published in CVPR 2019", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We devise a cascade GAN approach to generate talking face video, which is\nrobust to different face shapes, view angles, facial characteristics, and noisy\naudio conditions. Instead of learning a direct mapping from audio to video\nframes, we propose first to transfer audio to high-level structure, i.e., the\nfacial landmarks, and then to generate video frames conditioned on the\nlandmarks. Compared to a direct audio-to-image approach, our cascade approach\navoids fitting spurious correlations between audiovisual signals that are\nirrelevant to the speech content. We, humans, are sensitive to temporal\ndiscontinuities and subtle artifacts in video. To avoid those pixel jittering\nproblems and to enforce the network to focus on audiovisual-correlated regions,\nwe propose a novel dynamically adjustable pixel-wise loss with an attention\nmechanism. Furthermore, to generate a sharper image with well-synchronized\nfacial movements, we propose a novel regression-based discriminator structure,\nwhich considers sequence-level information along with frame-level information.\nThoughtful experiments on several datasets and real-world samples demonstrate\nsignificantly better results obtained by our method than the state-of-the-art\nmethods in both quantitative and qualitative comparisons.\n", "versions": [{"version": "v1", "created": "Thu, 9 May 2019 19:14:26 GMT"}], "update_date": "2019-05-13", "authors_parsed": [["Chen", "Lele", ""], ["Maddox", "Ross K.", ""], ["Duan", "Zhiyao", ""], ["Xu", "Chenliang", ""]]}, {"id": "1905.03890", "submitter": "Chaobing Zheng", "authors": "Chaobing Zheng, Zhengguo Li and Shiqian Wu", "title": "Exposure Interpolation by Combining Model-driven and Data-driven Methods", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning based methods have penetrated many image processing problems\nand become dominant solutions to these problems. A natural question raised here\nis \"Is there any space for conventional methods on these problems?\" In this\npaper, exposure interpolation is taken as an example to answer this question\nand the answer is \"Yes\". A framework on fusing conventional and deep learning\nmethod is introduced to generate an medium exposure image for two\nlarge-exposureratio images. Experimental results indicate that the quality of\nthe medium exposure image is increased significantly through using the deep\nlearning method to refine the interpolated image via the conventional method.\nThe conventional method can be adopted to improve the convergence speed of the\ndeep learning method and to reduce the number of samples which is required by\nthe deep learning method.\n", "versions": [{"version": "v1", "created": "Thu, 9 May 2019 23:36:55 GMT"}, {"version": "v2", "created": "Sun, 13 Oct 2019 04:40:04 GMT"}, {"version": "v3", "created": "Sun, 3 Nov 2019 12:39:11 GMT"}, {"version": "v4", "created": "Thu, 7 Nov 2019 09:02:22 GMT"}, {"version": "v5", "created": "Tue, 17 Nov 2020 06:02:17 GMT"}, {"version": "v6", "created": "Fri, 27 Nov 2020 12:48:29 GMT"}], "update_date": "2020-11-30", "authors_parsed": [["Zheng", "Chaobing", ""], ["Li", "Zhengguo", ""], ["Wu", "Shiqian", ""]]}, {"id": "1905.03892", "submitter": "Mateusz Kozi\\'nski", "authors": "Agata Mosinska, Mateusz Kozinski, Pascal Fua", "title": "Joint Segmentation and Path Classification of Curvilinear Structures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Detection of curvilinear structures in images has long been of interest. One\nof the most challenging aspects of this problem is inferring the graph\nrepresentation of the curvilinear network. Most existing delineation approaches\nfirst perform binary segmentation of the image and then refine it using either\na set of hand-designed heuristics or a separate classifier that assigns\nlikelihood to paths extracted from the pixel-wise prediction. In our work, we\nbridge the gap between segmentation and path classification by training a deep\nnetwork that performs those two tasks simultaneously. We show that this\napproach is beneficial because it enforces consistency across the whole\nprocessing pipeline. We apply our approach on roads and neurons datasets.\n", "versions": [{"version": "v1", "created": "Thu, 9 May 2019 23:50:22 GMT"}], "update_date": "2019-05-13", "authors_parsed": [["Mosinska", "Agata", ""], ["Kozinski", "Mateusz", ""], ["Fua", "Pascal", ""]]}, {"id": "1905.03894", "submitter": "Josh Harguess", "authors": "Chris M. Ward, Josh Harguess, Cameron Hilton", "title": "Ship classification from overhead imagery using synthetic data and\n  domain adaptation", "comments": "OCEANS 2018 MTS/IEEE Charleston", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  In this paper, we revisit the problem of classifying ships (maritime vessels)\ndetected from overhead imagery. Despite the last decade of research on this\nvery important and pertinent problem, it remains largely unsolved. One of the\nmajor issues with the detection and classification of ships and other objects\nin the maritime domain is the lack of substantial ground truth data needed to\ntrain state-of-the-art machine learning algorithms. We address this issue by\nbuilding a large (200k) synthetic image dataset using the Unity gaming engine\nand 3D ship models. We demonstrate that with the use of synthetic data,\nclassification performance increases dramatically, particularly when there are\nvery few annotated images used in training.\n", "versions": [{"version": "v1", "created": "Fri, 10 May 2019 00:01:52 GMT"}], "update_date": "2019-05-13", "authors_parsed": [["Ward", "Chris M.", ""], ["Harguess", "Josh", ""], ["Hilton", "Cameron", ""]]}, {"id": "1905.03897", "submitter": "Yannick Hold-Geoffroy", "authors": "Yannick Hold-Geoffroy, Akshaya Athawale, Jean-Fran\\c{c}ois Lalonde", "title": "Deep Sky Modeling for Single Image Outdoor Lighting Estimation", "comments": "CVPR'19 paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a data-driven learned sky model, which we use for outdoor lighting\nestimation from a single image. As no large-scale dataset of images and their\ncorresponding ground truth illumination is readily available, we use\ncomplementary datasets to train our approach, combining the vast diversity of\nillumination conditions of SUN360 with the radiometrically calibrated and\nphysically accurate Laval HDR sky database. Our key contribution is to provide\na holistic view of both lighting modeling and estimation, solving both problems\nend-to-end. From a test image, our method can directly estimate an HDR\nenvironment map of the lighting without relying on analytical lighting models.\nWe demonstrate the versatility and expressivity of our learned sky model and\nshow that it can be used to recover plausible illumination, leading to visually\npleasant virtual object insertions. To further evaluate our method, we capture\na dataset of HDR 360{\\deg} panoramas and show through extensive validation that\nwe significantly outperform previous state-of-the-art.\n", "versions": [{"version": "v1", "created": "Fri, 10 May 2019 00:12:20 GMT"}], "update_date": "2019-05-13", "authors_parsed": [["Hold-Geoffroy", "Yannick", ""], ["Athawale", "Akshaya", ""], ["Lalonde", "Jean-Fran\u00e7ois", ""]]}, {"id": "1905.03904", "submitter": "Chaobing Zheng", "authors": "Chaobing Zheng, Shiqian Wu, Wangming Xu, Shoulie Xie", "title": "Illumination Normalization via Merging Locally Enhanced Textures for\n  Robust Face Recognition", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In order to improve the accuracy of face recognition under varying\nillumination conditions, a local texture enhanced illumination normalization\nmethod based on fusion of differential filtering images (FDFI-LTEIN) is\nproposed to weaken the influence caused by illumination changes. Firstly, the\ndynamic range of the face image in dark or shadowed regions is expanded by\nlogarithmic transformation. Then, the global contrast enhanced face image is\nconvolved with difference of Gaussian filters and difference of bilateral\nfilters, and the filtered images are weighted and merged using a coefficient\nselection rule based on the standard deviation (SD) of image, which can enhance\nimage texture information while filtering out most noise. Finally, the local\ncontrast equalization (LCE) is performed on the fused face image to reduce the\ninfluence caused by over or under saturated pixel values in highlight or dark\nregions. Experimental results on the Extended Yale B face database and CMU PIE\nface database demonstrate that the proposed method is more robust to\nillumination changes and achieve higher recognition accuracy when compared with\nother illumination normalization methods and a deep CNNs based illumination\ninvariant face recognition method\n", "versions": [{"version": "v1", "created": "Fri, 10 May 2019 01:07:53 GMT"}], "update_date": "2019-05-13", "authors_parsed": [["Zheng", "Chaobing", ""], ["Wu", "Shiqian", ""], ["Xu", "Wangming", ""], ["Xie", "Shoulie", ""]]}, {"id": "1905.03907", "submitter": "Tucker Hermans", "authors": "Kanrun Huang and Tucker Hermans", "title": "Building 3D Object Models during Manipulation by Reconstruction-Aware\n  Trajectory Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Object shape provides important information for robotic manipulation; for\ninstance, selecting an effective grasp depends on both the global and local\nshape of the object of interest, while reaching into clutter requires accurate\nsurface geometry to avoid unintended contact with the environment. Model-based\n3D object manipulation is a widely studied problem; however, obtaining the\naccurate 3D object models for multiple objects often requires tedious work. In\nthis letter, we exploit Gaussian process implicit surfaces (GPIS) extracted\nfrom RGB-D sensor data to grasp an unknown object. We propose a\nreconstruction-aware trajectory optimization that makes use of the extracted\nGPIS model plan a motion to improve the ability to estimate the object's 3D\ngeometry, while performing a pick-and-place action. We present a probabilistic\napproach for a robot to autonomously learn and track the object, while achieve\nthe manipulation task.\n  We use a sampling-based trajectory generation method to explore the unseen\nparts of the object using the estimated conditional entropy of the GPIS model.\nWe validate our method with physical robot experiments across eleven different\nobjects of varying shape from the YCB object dataset. Our experiments show that\nour reconstruction-aware trajectory optimization provides higher-quality 3D\nobject reconstruction when compared with directly solving the manipulation task\nor using a heuristic to view unseen portions of the object.\n", "versions": [{"version": "v1", "created": "Fri, 10 May 2019 01:33:29 GMT"}], "update_date": "2019-05-13", "authors_parsed": [["Huang", "Kanrun", ""], ["Hermans", "Tucker", ""]]}, {"id": "1905.03912", "submitter": "Gyeongsik Moon", "authors": "Gyeongsik Moon, Ju Yong Chang, Kyoung Mu Lee", "title": "Multi-scale Aggregation R-CNN for 2D Multi-person Pose Estimation", "comments": "Published at CVPRW 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-person pose estimation from a 2D image is challenging because it\nrequires not only keypoint localization but also human detection. In\nstate-of-the-art top-down methods, multi-scale information is a crucial factor\nfor the accurate pose estimation because it contains both of local information\naround the keypoints and global information of the entire person. Although\nmulti-scale information allows these methods to achieve the state-of-the-art\nperformance, the top-down methods still require a huge amount of computation\nbecause they need to use an additional human detector to feed the cropped human\nimage to their pose estimation model. To effectively utilize multi-scale\ninformation with the smaller computation, we propose a multi-scale aggregation\nR-CNN (MSA R-CNN). It consists of multi-scale RoIAlign block (MS-RoIAlign) and\nmulti-scale keypoint head network (MS-KpsNet) which are designed to effectively\nutilize multi-scale information. Also, in contrast to previous top-down\nmethods, the MSA R-CNN performs human detection and keypoint localization in a\nsingle model, which results in reduced computation. The proposed model achieved\nthe best performance among single model-based methods and its results are\ncomparable to those of separated model-based methods with a smaller amount of\ncomputation on the publicly available 2D multi-person keypoint localization\ndataset.\n", "versions": [{"version": "v1", "created": "Fri, 10 May 2019 02:17:34 GMT"}], "update_date": "2019-05-13", "authors_parsed": [["Moon", "Gyeongsik", ""], ["Chang", "Ju Yong", ""], ["Lee", "Kyoung Mu", ""]]}, {"id": "1905.03922", "submitter": "Yang Feng", "authors": "Yang Feng, Lin Ma, Wei Liu, Jiebo Luo", "title": "Spatio-temporal Video Re-localization by Warp LSTM", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The need for efficiently finding the video content a user wants is increasing\nbecause of the erupting of user-generated videos on the Web. Existing\nkeyword-based or content-based video retrieval methods usually determine what\noccurs in a video but not when and where. In this paper, we make an answer to\nthe question of when and where by formulating a new task, namely\nspatio-temporal video re-localization. Specifically, given a query video and a\nreference video, spatio-temporal video re-localization aims to localize\ntubelets in the reference video such that the tubelets semantically correspond\nto the query. To accurately localize the desired tubelets in the reference\nvideo, we propose a novel warp LSTM network, which propagates the\nspatio-temporal information for a long period and thereby captures the\ncorresponding long-term dependencies. Another issue for spatio-temporal video\nre-localization is the lack of properly labeled video datasets. Therefore, we\nreorganize the videos in the AVA dataset to form a new dataset for\nspatio-temporal video re-localization research. Extensive experimental results\nshow that the proposed model achieves superior performances over the designed\nbaselines on the spatio-temporal video re-localization task.\n", "versions": [{"version": "v1", "created": "Fri, 10 May 2019 03:27:26 GMT"}], "update_date": "2019-05-13", "authors_parsed": [["Feng", "Yang", ""], ["Ma", "Lin", ""], ["Liu", "Wei", ""], ["Luo", "Jiebo", ""]]}, {"id": "1905.03966", "submitter": "Wenjie Pei", "authors": "Wenjie Pei, Jiyuan Zhang, Xiangrong Wang, Lei Ke, Xiaoyong Shen,\n  Yu-Wing Tai", "title": "Memory-Attended Recurrent Network for Video Captioning", "comments": "Accepted by CVPR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Typical techniques for video captioning follow the encoder-decoder framework,\nwhich can only focus on one source video being processed. A potential\ndisadvantage of such design is that it cannot capture the multiple visual\ncontext information of a word appearing in more than one relevant videos in\ntraining data. To tackle this limitation, we propose the Memory-Attended\nRecurrent Network (MARN) for video captioning, in which a memory structure is\ndesigned to explore the full-spectrum correspondence between a word and its\nvarious similar visual contexts across videos in training data. Thus, our model\nis able to achieve a more comprehensive understanding for each word and yield\nhigher captioning quality. Furthermore, the built memory structure enables our\nmethod to model the compatibility between adjacent words explicitly instead of\nasking the model to learn implicitly, as most existing models do. Extensive\nvalidation on two real-word datasets demonstrates that our MARN consistently\noutperforms state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Fri, 10 May 2019 06:47:57 GMT"}], "update_date": "2019-05-13", "authors_parsed": [["Pei", "Wenjie", ""], ["Zhang", "Jiyuan", ""], ["Wang", "Xiangrong", ""], ["Ke", "Lei", ""], ["Shen", "Xiaoyong", ""], ["Tai", "Yu-Wing", ""]]}, {"id": "1905.03968", "submitter": "Yaman Kumar", "authors": "Nilay Shrivastava, Astitwa Saxena, Yaman Kumar, Rajiv Ratn Shah,\n  Debanjan Mahata, Amanda Stent", "title": "MobiVSR: A Visual Speech Recognition Solution for Mobile Devices", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual speech recognition (VSR) is the task of recognizing spoken language\nfrom video input only, without any audio. VSR has many applications as an\nassistive technology, especially if it could be deployed in mobile devices and\nembedded systems. The need of intensive computational resources and large\nmemory footprint are two of the major obstacles in developing neural network\nmodels for VSR in a resource constrained environment. We propose a novel\nend-to-end deep neural network architecture for word level VSR called MobiVSR\nwith a design parameter that aids in balancing the model's accuracy and\nparameter count. We use depthwise-separable 3D convolution for the first time\nin the domain of VSR and show how it makes our model efficient. MobiVSR\nachieves an accuracy of 73\\% on a challenging Lip Reading in the Wild dataset\nwith 6 times fewer parameters and 20 times lesser memory footprint than the\ncurrent state of the art. MobiVSR can also be compressed to 6 MB by applying\npost training quantization.\n", "versions": [{"version": "v1", "created": "Fri, 10 May 2019 06:58:35 GMT"}, {"version": "v2", "created": "Sun, 26 May 2019 11:12:34 GMT"}, {"version": "v3", "created": "Wed, 5 Jun 2019 03:49:26 GMT"}], "update_date": "2019-06-06", "authors_parsed": [["Shrivastava", "Nilay", ""], ["Saxena", "Astitwa", ""], ["Kumar", "Yaman", ""], ["Shah", "Rajiv Ratn", ""], ["Mahata", "Debanjan", ""], ["Stent", "Amanda", ""]]}, {"id": "1905.04010", "submitter": "Huibing Wang", "authors": "Lin Feng, Caifeng Liu, Shenglan Liu, Huibing Wang", "title": "A fast online cascaded regression algorithm for face alignment", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditional face alignment based on machine learning usually tracks the\nlocalizations of facial landmarks employing a static model trained offline\nwhere all of the training data is available in advance. When new training\nsamples arrive, the static model must be retrained from scratch, which is\nexcessively time-consuming and memory-consuming. In many real-time\napplications, the training data is obtained one by one or batch by batch. It\nresults in that the static model limits its performance on sequential images\nwith extensive variations. Therefore, the most critical and challenging aspect\nin this field is dynamically updating the tracker's models to enhance\npredictive and generalization capabilities continuously. In order to address\nthis question, we develop a fast and accurate online learning algorithm for\nface alignment. Particularly, we incorporate on-line sequential extreme\nlearning machine into a parallel cascaded regression framework, coined\nincremental cascade regression(ICR). To the best of our knowledge, this is the\nfirst incremental cascaded framework with the non-linear regressor. One main\nadvantage of ICR is that the tracker model can be fast updated in an\nincremental way without the entire retraining process when a new input is\nincoming. Experimental results demonstrate that the proposed ICR is more\naccurate and efficient on still or sequential images compared with the recent\nstate-of-the-art cascade approaches. Furthermore, the incremental learning\nproposed in this paper can update the trained model in real time.\n", "versions": [{"version": "v1", "created": "Fri, 10 May 2019 08:37:33 GMT"}], "update_date": "2019-05-13", "authors_parsed": [["Feng", "Lin", ""], ["Liu", "Caifeng", ""], ["Liu", "Shenglan", ""], ["Wang", "Huibing", ""]]}, {"id": "1905.04014", "submitter": "Loic Landrieu", "authors": "Loic Landrieu and Mohamed Boussaha", "title": "Supervized Segmentation with Graph-Structured Deep Metric Learning", "comments": "arXiv admin note: substantial text overlap with arXiv:1904.02113", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a fully-supervized method for learning to segment data structured\nby an adjacency graph. We introduce the graph-structured contrastive loss, a\nloss function structured by a ground truth segmentation. It promotes learning\nvertex embeddings which are homogeneous within desired segments, and have high\ncontrast at their interface. Thus, computing a piecewise-constant approximation\nof such embeddings produces a graph-partition close to the objective\nsegmentation. This loss is fully backpropagable, which allows us to learn\nvertex embeddings with deep learning algorithms. We evaluate our methods on a\n3D point cloud oversegmentation task, defining a new state-of-the-art by a\nlarge margin. These results are based on the published work of Landrieu and\nBoussaha 2019.\n", "versions": [{"version": "v1", "created": "Fri, 10 May 2019 08:55:49 GMT"}, {"version": "v2", "created": "Mon, 24 Jun 2019 15:03:33 GMT"}], "update_date": "2019-06-26", "authors_parsed": [["Landrieu", "Loic", ""], ["Boussaha", "Mohamed", ""]]}, {"id": "1905.04016", "submitter": "Baoyuan Wu", "authors": "Yan Xu, Baoyuan Wu, Fumin Shen, Yanbo Fan, Yong Zhang, Heng Tao Shen,\n  Wei Liu", "title": "Exact Adversarial Attack to Image Captioning via Structured Output\n  Learning with Latent Variables", "comments": "Accepted to CVPR 2019. Yan Xu and Baoyuan Wu are co-first authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we study the robustness of a CNN+RNN based image captioning\nsystem being subjected to adversarial noises. We propose to fool an image\ncaptioning system to generate some targeted partial captions for an image\npolluted by adversarial noises, even the targeted captions are totally\nirrelevant to the image content. A partial caption indicates that the words at\nsome locations in this caption are observed, while words at other locations are\nnot restricted.It is the first work to study exact adversarial attacks of\ntargeted partial captions. Due to the sequential dependencies among words in a\ncaption, we formulate the generation of adversarial noises for targeted partial\ncaptions as a structured output learning problem with latent variables. Both\nthe generalized expectation maximization algorithm and structural SVMs with\nlatent variables are then adopted to optimize the problem. The proposed methods\ngenerate very successful at-tacks to three popular CNN+RNN based image\ncaptioning models. Furthermore, the proposed attack methods are used to\nunderstand the inner mechanism of image captioning systems, providing the\nguidance to further improve automatic image captioning systems towards human\ncaptioning.\n", "versions": [{"version": "v1", "created": "Fri, 10 May 2019 09:00:53 GMT"}], "update_date": "2019-05-13", "authors_parsed": [["Xu", "Yan", ""], ["Wu", "Baoyuan", ""], ["Shen", "Fumin", ""], ["Fan", "Yanbo", ""], ["Zhang", "Yong", ""], ["Shen", "Heng Tao", ""], ["Liu", "Wei", ""]]}, {"id": "1905.04042", "submitter": "Lu Liu", "authors": "Lu Liu, Tianyi Zhou, Guodong Long, Jing Jiang, Lina Yao, Chengqi Zhang", "title": "Prototype Propagation Networks (PPN) for Weakly-supervised Few-shot\n  Learning on Category Graph", "comments": "Accepted to IJCAI 2019, Code is publicly available at:\n  https://github.com/liulu112601/PPN", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A variety of machine learning applications expect to achieve rapid learning\nfrom a limited number of labeled data. However, the success of most current\nmodels is the result of heavy training on big data. Meta-learning addresses\nthis problem by extracting common knowledge across different tasks that can be\nquickly adapted to new tasks. However, they do not fully explore\nweakly-supervised information, which is usually free or cheap to collect. In\nthis paper, we show that weakly-labeled data can significantly improve the\nperformance of meta-learning on few-shot classification. We propose prototype\npropagation network (PPN) trained on few-shot tasks together with data\nannotated by coarse-label. Given a category graph of the targeted fine-classes\nand some weakly-labeled coarse-classes, PPN learns an attention mechanism which\npropagates the prototype of one class to another on the graph, so that the\nK-nearest neighbor (KNN) classifier defined on the propagated prototypes\nresults in high accuracy across different few-shot tasks. The training tasks\nare generated by subgraph sampling, and the training objective is obtained by\naccumulating the level-wise classification loss on the subgraph. The resulting\ngraph of prototypes can be continually re-used and updated for new tasks and\nclasses. We also introduce two practical test/inference settings which differ\naccording to whether the test task can leverage any weakly-supervised\ninformation as in training. On two benchmarks, PPN significantly outperforms\nmost recent few-shot learning methods in different settings, even when they are\nalso allowed to train on weakly-labeled data.\n", "versions": [{"version": "v1", "created": "Fri, 10 May 2019 09:57:23 GMT"}, {"version": "v2", "created": "Sun, 2 Jun 2019 12:40:13 GMT"}], "update_date": "2019-06-04", "authors_parsed": [["Liu", "Lu", ""], ["Zhou", "Tianyi", ""], ["Long", "Guodong", ""], ["Jiang", "Jing", ""], ["Yao", "Lina", ""], ["Zhang", "Chengqi", ""]]}, {"id": "1905.04073", "submitter": "Estefania Talavera", "authors": "Estefania Talavera, Alexandre Cola, Nicolai Petkov, Petia Radeva", "title": "Towards Egocentric Person Re-identification and Social Pattern Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Wearable cameras capture a first-person view of the daily activities of the\ncamera wearer, offering a visual diary of the user behaviour. Detection of the\nappearance of people the camera user interacts with for social interactions\nanalysis is of high interest. Generally speaking, social events, lifestyle and\nhealth are highly correlated, but there is a lack of tools to monitor and\nanalyse them. We consider that egocentric vision provides a tool to obtain\ninformation and understand users social interactions. We propose a model that\nenables us to evaluate and visualize social traits obtained by analysing social\ninteractions appearance within egocentric photostreams. Given sets of\negocentric images, we detect the appearance of faces within the days of the\ncamera wearer, and rely on clustering algorithms to group their feature\ndescriptors in order to re-identify persons. Recurrence of detected faces\nwithin photostreams allows us to shape an idea of the social pattern of\nbehaviour of the user. We validated our model over several weeks recorded by\ndifferent camera wearers. Our findings indicate that social profiles are\npotentially useful for social behaviour interpretation.\n", "versions": [{"version": "v1", "created": "Fri, 10 May 2019 11:17:47 GMT"}], "update_date": "2019-05-13", "authors_parsed": [["Talavera", "Estefania", ""], ["Cola", "Alexandre", ""], ["Petkov", "Nicolai", ""], ["Radeva", "Petia", ""]]}, {"id": "1905.04075", "submitter": "Kai Wang", "authors": "Kai Wang, Xiaojiang Peng, Jianfei Yang, Debin Meng and Yu Qiao", "title": "Region Attention Networks for Pose and Occlusion Robust Facial\n  Expression Recognition", "comments": "The test set and the code of this paper will be available at\n  https://github.com/kaiwang960112/Challenge-condition-FER-dataset", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Occlusion and pose variations, which can change facial appearance\nsignificantly, are two major obstacles for automatic Facial Expression\nRecognition (FER). Though automatic FER has made substantial progresses in the\npast few decades, occlusion-robust and pose-invariant issues of FER have\nreceived relatively less attention, especially in real-world scenarios. This\npaper addresses the real-world pose and occlusion robust FER problem with\nthree-fold contributions. First, to stimulate the research of FER under\nreal-world occlusions and variant poses, we build several in-the-wild facial\nexpression datasets with manual annotations for the community. Second, we\npropose a novel Region Attention Network (RAN), to adaptively capture the\nimportance of facial regions for occlusion and pose variant FER. The RAN\naggregates and embeds varied number of region features produced by a backbone\nconvolutional neural network into a compact fixed-length representation. Last,\ninspired by the fact that facial expressions are mainly defined by facial\naction units, we propose a region biased loss to encourage high attention\nweights for the most important regions. We validate our RAN and region biased\nloss on both our built test datasets and four popular datasets: FERPlus,\nAffectNet, RAF-DB, and SFEW. Extensive experiments show that our RAN and region\nbiased loss largely improve the performance of FER with occlusion and variant\npose. Our method also achieves state-of-the-art results on FERPlus, AffectNet,\nRAF-DB, and SFEW. Code and the collected test data will be publicly available.\n", "versions": [{"version": "v1", "created": "Fri, 10 May 2019 11:18:13 GMT"}, {"version": "v2", "created": "Thu, 5 Sep 2019 02:06:24 GMT"}], "update_date": "2019-09-06", "authors_parsed": [["Wang", "Kai", ""], ["Peng", "Xiaojiang", ""], ["Yang", "Jianfei", ""], ["Meng", "Debin", ""], ["Qiao", "Yu", ""]]}, {"id": "1905.04076", "submitter": "Estefania Talavera", "authors": "Estefania Talavera and Nicolai Petkov and Petia Radeva", "title": "Unsupervised routine discovery in egocentric photo-streams", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The routine of a person is defined by the occurrence of activities throughout\ndifferent days, and can directly affect the person's health. In this work, we\naddress the recognition of routine related days. To do so, we rely on\negocentric images, which are recorded by a wearable camera and allow to monitor\nthe life of the user from a first-person view perspective. We propose an\nunsupervised model that identifies routine related days, following an outlier\ndetection approach. We test the proposed framework over a total of 72 days in\nthe form of photo-streams covering around 2 weeks of the life of 5 different\ncamera wearers. Our model achieves an average of 76% Accuracy and 68% Weighted\nF-Score for all the users. Thus, we show that our framework is able to\nrecognise routine related days and opens the door to the understanding of the\nbehaviour of people.\n", "versions": [{"version": "v1", "created": "Fri, 10 May 2019 11:27:50 GMT"}], "update_date": "2019-05-13", "authors_parsed": [["Talavera", "Estefania", ""], ["Petkov", "Nicolai", ""], ["Radeva", "Petia", ""]]}, {"id": "1905.04084", "submitter": "Peng Zhang", "authors": "Peng Zhang, Xiaoyu Ge, Jochen Renz", "title": "Support Relation Analysis for Objects in Multiple View RGB-D Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding physical relations between objects, especially their support\nrelations, is crucial for robotic manipulation. There has been work on\nreasoning about support relations and structural stability of simple\nconfigurations in RGB-D images. In this paper, we propose a method for\nextracting more detailed physical knowledge from a set of RGB-D images taken\nfrom the same scene but from different views using qualitative reasoning and\nintuitive physical models. Rather than providing a simple contact relation\ngraph and approximating stability over convex shapes, our method is able to\nprovide a detailed supporting relation analysis based on a volumetric\nrepresentation. Specifically, true supporting relations between objects (e.g.,\nif an object supports another object by touching it on the side or if the\nobject above contributes to the stability of the object below) are identified.\nWe apply our method to real-world structures captured in warehouse scenarios\nand show our method works as desired.\n", "versions": [{"version": "v1", "created": "Fri, 10 May 2019 11:49:49 GMT"}], "update_date": "2019-05-13", "authors_parsed": [["Zhang", "Peng", ""], ["Ge", "Xiaoyu", ""], ["Renz", "Jochen", ""]]}, {"id": "1905.04088", "submitter": "Qian Zheng", "authors": "Qian Zheng, Yiming Jia, Boxin Shi, Xudong Jiang, Ling-Yu Duan, and\n  Alex C. Kot", "title": "SPLINE-Net: Sparse Photometric Stereo through Lighting Interpolation and\n  Normal Estimation Networks", "comments": "Accepted to ICCV 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper solves the Sparse Photometric stereo through Lighting\nInterpolation and Normal Estimation using a generative Network (SPLINE-Net).\nSPLINE-Net contains a lighting interpolation network to generate dense lighting\nobservations given a sparse set of lights as inputs followed by a normal\nestimation network to estimate surface normals. Both networks are jointly\nconstrained by the proposed symmetric and asymmetric loss functions to enforce\nisotropic constrain and perform outlier rejection of global illumination\neffects. SPLINE-Net is verified to outperform existing methods for photometric\nstereo of general BRDFs by using only ten images of different lights instead of\nusing nearly one hundred images.\n", "versions": [{"version": "v1", "created": "Fri, 10 May 2019 11:51:49 GMT"}, {"version": "v2", "created": "Wed, 9 Oct 2019 05:05:33 GMT"}], "update_date": "2019-10-10", "authors_parsed": [["Zheng", "Qian", ""], ["Jia", "Yiming", ""], ["Shi", "Boxin", ""], ["Jiang", "Xudong", ""], ["Duan", "Ling-Yu", ""], ["Kot", "Alex C.", ""]]}, {"id": "1905.04093", "submitter": "Estefania Talavera", "authors": "Estefania Talavera, Nicolai Petkov and Petia Radeva", "title": "Towards Unsupervised Familiar Scene Recognition in Egocentric Videos", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nowadays, there is an upsurge of interest in using lifelogging devices. Such\ndevices generate huge amounts of image data; consequently, the need for\nautomatic methods for analyzing and summarizing these data is drastically\nincreasing. We present a new method for familiar scene recognition in\negocentric videos, based on background pattern detection through automatically\nconfigurable COSFIRE filters. We present some experiments over egocentric data\nacquired with the Narrative Clip.\n", "versions": [{"version": "v1", "created": "Fri, 10 May 2019 11:58:10 GMT"}], "update_date": "2019-05-13", "authors_parsed": [["Talavera", "Estefania", ""], ["Petkov", "Nicolai", ""], ["Radeva", "Petia", ""]]}, {"id": "1905.04094", "submitter": "Jin Chen", "authors": "Jin Chen, Xinxiao Wu, Lixin Duan and Shenghua Gao", "title": "Domain Adversarial Reinforcement Learning for Partial Domain Adaptation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Partial domain adaptation aims to transfer knowledge from a label-rich source\ndomain to a label-scarce target domain which relaxes the fully shared label\nspace assumption across different domains. In this more general and practical\nscenario, a major challenge is how to select source instances in the shared\nclasses across different domains for positive transfer. To address this issue,\nwe propose a Domain Adversarial Reinforcement Learning (DARL) framework to\nautomatically select source instances in the shared classes for circumventing\nnegative transfer as well as to simultaneously learn transferable features\nbetween domains by reducing the domain shift. Specifically, in this framework,\nwe employ deep Q-learning to learn policies for an agent to make selection\ndecisions by approximating the action-value function. Moreover, domain\nadversarial learning is introduced to learn domain-invariant features for the\nselected source instances by the agent and the target instances, and also to\ndetermine rewards for the agent based on how relevant the selected source\ninstances are to the target domain. Experiments on several benchmark datasets\ndemonstrate that the superior performance of our DARL method over existing\nstate of the arts for partial domain adaptation.\n", "versions": [{"version": "v1", "created": "Fri, 10 May 2019 12:02:32 GMT"}], "update_date": "2019-05-13", "authors_parsed": [["Chen", "Jin", ""], ["Wu", "Xinxiao", ""], ["Duan", "Lixin", ""], ["Gao", "Shenghua", ""]]}, {"id": "1905.04097", "submitter": "Estefania Talavera", "authors": "Estefania Talavera, Maria Leyva-Vallina, Md. Mostafa Kamal Sarker,\n  Domenec Puig, Nicolai Petkov and Petia Radeva", "title": "Hierarchical approach to classify food scenes in egocentric\n  photo-streams", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent studies have shown that the environment where people eat can affect\ntheir nutritional behaviour. In this work, we provide automatic tools for a\npersonalised analysis of a person's health habits by the examination of daily\nrecorded egocentric photo-streams. Specifically, we propose a new automatic\napproach for the classification of food-related environments, that is able to\nclassify up to 15 such scenes. In this way, people can monitor the context\naround their food intake in order to get an objective insight into their daily\neating routine. We propose a model that classifies food-related scenes\norganized in a semantic hierarchy. Additionally, we present and make available\na new egocentric dataset composed of more than 33000 images recorded by a\nwearable camera, over which our proposed model has been tested. Our approach\nobtains an accuracy and F-score of 56\\% and 65\\%, respectively, clearly\noutperforming the baseline methods.\n", "versions": [{"version": "v1", "created": "Fri, 10 May 2019 12:07:28 GMT"}], "update_date": "2019-05-13", "authors_parsed": [["Talavera", "Estefania", ""], ["Leyva-Vallina", "Maria", ""], ["Sarker", "Md. Mostafa Kamal", ""], ["Puig", "Domenec", ""], ["Petkov", "Nicolai", ""], ["Radeva", "Petia", ""]]}, {"id": "1905.04105", "submitter": "Jong Chul Ye", "authors": "Dongwook Lee, Won-Jin Moon, Jong Chul Ye", "title": "Which Contrast Does Matter? Towards a Deep Understanding of MR Contrast\n  using Collaborative GAN", "comments": "32 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Thanks to the recent success of generative adversarial network (GAN) for\nimage synthesis, there are many exciting GAN approaches that successfully\nsynthesize MR image contrast from other images with different contrasts. These\napproaches are potentially important for image imputation problems, where\ncomplete set of data is often difficult to obtain and image synthesis is one of\nthe key solutions for handling the missing data problem. Unfortunately, the\nlack of the scalability of the existing GAN-based image translation approaches\nposes a fundamental challenge to understand the nature of the MR contrast\nimputation problem: which contrast does matter? Here, we present a systematic\napproach using Collaborative Generative Adversarial Networks (CollaGAN), which\nenable the learning of the joint image manifold of multiple MR contrasts to\ninvestigate which contrasts are essential. Our experimental results showed that\nthe exogenous contrast from contrast agents is not replaceable, but other\nendogenous contrast such as T1, T2, etc can be synthesized from other contrast.\nThese findings may give important guidance to the acquisition protocol design\nfor MR in real clinical environment.\n", "versions": [{"version": "v1", "created": "Fri, 10 May 2019 12:18:19 GMT"}], "update_date": "2019-05-13", "authors_parsed": [["Lee", "Dongwook", ""], ["Moon", "Won-Jin", ""], ["Ye", "Jong Chul", ""]]}, {"id": "1905.04107", "submitter": "Estefania Talavera", "authors": "Estefania Talavera, Petia Radeva, Nicolai Petkov", "title": "Towards Emotion Retrieval in Egocentric PhotoStream", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The availability and use of egocentric data are rapidly increasing due to the\ngrowing use of wearable cameras. Our aim is to study the effect (positive,\nneutral or negative) of egocentric images or events on an observer. Given\negocentric photostreams capturing the wearer's days, we propose a method that\naims to assign sentiment to events extracted from egocentric photostreams. Such\nmoments can be candidates to retrieve according to their possibility of\nrepresenting a positive experience for the camera's wearer. The proposed\napproach obtained a classification accuracy of 75% on the test set, with a\ndeviation of 8%. Our model makes a step forward opening the door to sentiment\nrecognition in egocentric photostreams.\n", "versions": [{"version": "v1", "created": "Fri, 10 May 2019 12:24:00 GMT"}], "update_date": "2019-05-13", "authors_parsed": [["Talavera", "Estefania", ""], ["Radeva", "Petia", ""], ["Petkov", "Nicolai", ""]]}, {"id": "1905.04129", "submitter": "Hiroyuki Kobayashi", "authors": "Hiroyuki Kobayashi and Osamu Watanabe and Hitoshi Kiya", "title": "Two-layer Near-lossless HDR Coding with Backward Compatibility to JPEG", "comments": "To appear in IEEE International Conference on Image Processing 2019,\n  Taipei, Taiwan", "journal-ref": null, "doi": "10.1587/transfun.E102.A.1842", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an efficient two-layer near-lossless coding method using an\nextended histogram packing technique with backward compatibility to the legacy\nJPEG standard. The JPEG XT, which is the international standard to compress HDR\nimages, adopts a two-layer coding method for backward compatibility to the\nlegacy JPEG standard. However, there are two problems with this two-layer\ncoding method. One is that it does not exhibit better near-lossless performance\nthan other methods for HDR image compression with single-layer structure. The\nother problem is that the determining the appropriate values of the coding\nparameters may be required for each input image to achieve good compression\nperformance of near-lossless compression with the two-layer coding method of\nthe JPEG XT. To solve these problems, we focus on a histogram-packing technique\nthat takes into account the histogram sparseness of HDR images. We used\nzero-skip quantization, which is an extension of the histogram-packing\ntechnique proposed for lossless coding, for implementing the proposed\nnear-lossless coding method. The experimental results indicate that the\nproposed method exhibits not only a better near-lossless compression\nperformance than that of the two-layer coding method of the JPEG XT, but also\nthere are no issue regarding the combination of parameter values without losing\nbackward compatibility to the JPEG standard.\n", "versions": [{"version": "v1", "created": "Thu, 9 May 2019 09:12:41 GMT"}], "update_date": "2020-01-08", "authors_parsed": [["Kobayashi", "Hiroyuki", ""], ["Watanabe", "Osamu", ""], ["Kiya", "Hitoshi", ""]]}, {"id": "1905.04132", "submitter": "Eric Brachmann", "authors": "Eric Brachmann, Carsten Rother", "title": "Neural-Guided RANSAC: Learning Where to Sample Model Hypotheses", "comments": "ICCV 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present Neural-Guided RANSAC (NG-RANSAC), an extension to the classic\nRANSAC algorithm from robust optimization. NG-RANSAC uses prior information to\nimprove model hypothesis search, increasing the chance of finding outlier-free\nminimal sets. Previous works use heuristic side-information like hand-crafted\ndescriptor distance to guide hypothesis search. In contrast, we learn\nhypothesis search in a principled fashion that lets us optimize an arbitrary\ntask loss during training, leading to large improvements on classic computer\nvision tasks. We present two further extensions to NG-RANSAC. Firstly, using\nthe inlier count itself as training signal allows us to train neural guidance\nin a self-supervised fashion. Secondly, we combine neural guidance with\ndifferentiable RANSAC to build neural networks which focus on certain parts of\nthe input data and make the output predictions as good as possible. We evaluate\nNG-RANSAC on a wide array of computer vision tasks, namely estimation of\nepipolar geometry, horizon line estimation and camera re-localization. We\nachieve superior or competitive results compared to state-of-the-art robust\nestimators, including very recent, learned ones.\n", "versions": [{"version": "v1", "created": "Fri, 10 May 2019 12:50:27 GMT"}, {"version": "v2", "created": "Wed, 31 Jul 2019 08:47:41 GMT"}], "update_date": "2019-08-01", "authors_parsed": [["Brachmann", "Eric", ""], ["Rother", "Carsten", ""]]}, {"id": "1905.04153", "submitter": "Shiyu Song", "authors": "Weixin Lu, Guowei Wan, Yao Zhou, Xiangyu Fu, Pengfei Yuan, Shiyu Song", "title": "DeepICP: An End-to-End Deep Neural Network for 3D Point Cloud\n  Registration", "comments": "10 pages, 6 figures, 3 tables, typos corrected, experimental results\n  updated, accepted by ICCV 2019", "journal-ref": "The IEEE International Conference on Computer Vision (ICCV), 2019,\n  pp. 12-21", "doi": "10.1109/ICCV.2019.00010", "report-no": null, "categories": "cs.CV cs.CG cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present DeepICP - a novel end-to-end learning-based 3D point cloud\nregistration framework that achieves comparable registration accuracy to prior\nstate-of-the-art geometric methods. Different from other keypoint based methods\nwhere a RANSAC procedure is usually needed, we implement the use of various\ndeep neural network structures to establish an end-to-end trainable network.\nOur keypoint detector is trained through this end-to-end structure and enables\nthe system to avoid the inference of dynamic objects, leverages the help of\nsufficiently salient features on stationary objects, and as a result, achieves\nhigh robustness. Rather than searching the corresponding points among existing\npoints, the key contribution is that we innovatively generate them based on\nlearned matching probabilities among a group of candidates, which can boost the\nregistration accuracy. Our loss function incorporates both the local similarity\nand the global geometric constraints to ensure all above network designs can\nconverge towards the right direction. We comprehensively validate the\neffectiveness of our approach using both the KITTI dataset and the\nApollo-SouthBay dataset. Results demonstrate that our method achieves\ncomparable or better performance than the state-of-the-art geometry-based\nmethods. Detailed ablation and visualization analysis are included to further\nillustrate the behavior and insights of our network. The low registration error\nand high robustness of our method makes it attractive for substantial\napplications relying on the point cloud registration task.\n", "versions": [{"version": "v1", "created": "Fri, 10 May 2019 13:08:28 GMT"}, {"version": "v2", "created": "Mon, 16 Sep 2019 08:49:52 GMT"}], "update_date": "2020-07-14", "authors_parsed": [["Lu", "Weixin", ""], ["Wan", "Guowei", ""], ["Zhou", "Yao", ""], ["Fu", "Xiangyu", ""], ["Yuan", "Pengfei", ""], ["Song", "Shiyu", ""]]}, {"id": "1905.04159", "submitter": "Dimitrios Stamoulis", "authors": "Dimitrios Stamoulis, Ruizhou Ding, Di Wang, Dimitrios Lymberopoulos,\n  Bodhi Priyantha, Jie Liu, Diana Marculescu", "title": "Single-Path NAS: Device-Aware Efficient ConvNet Design", "comments": "ODML-CDNNR 2019 (ICML'19 workshop) oral presentation (extended\n  abstract, required non-archival version). Full paper: arXiv:1904.02877", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Can we automatically design a Convolutional Network (ConvNet) with the\nhighest image classification accuracy under the latency constraint of a mobile\ndevice? Neural Architecture Search (NAS) for ConvNet design is a challenging\nproblem due to the combinatorially large design space and search time (at least\n200 GPU-hours). To alleviate this complexity, we propose Single-Path NAS, a\nnovel differentiable NAS method for designing device-efficient ConvNets in less\nthan 4 hours. 1. Novel NAS formulation: our method introduces a single-path,\nover-parameterized ConvNet to encode all architectural decisions with shared\nconvolutional kernel parameters. 2. NAS efficiency: Our method decreases the\nNAS search cost down to 8 epochs (30 TPU-hours), i.e., up to 5,000x faster\ncompared to prior work. 3. On-device image classification: Single-Path NAS\nachieves 74.96% top-1 accuracy on ImageNet with 79ms inference latency on a\nPixel 1 phone, which is state-of-the-art accuracy compared to NAS methods with\nsimilar latency (<80ms).\n", "versions": [{"version": "v1", "created": "Fri, 10 May 2019 13:23:48 GMT"}], "update_date": "2019-05-13", "authors_parsed": [["Stamoulis", "Dimitrios", ""], ["Ding", "Ruizhou", ""], ["Wang", "Di", ""], ["Lymberopoulos", "Dimitrios", ""], ["Priyantha", "Bodhi", ""], ["Liu", "Jie", ""], ["Marculescu", "Diana", ""]]}, {"id": "1905.04161", "submitter": "Xiaojie Guo", "authors": "Yonghua Zhang, Jiawan Zhang, and Xiaojie Guo", "title": "Kindling the Darkness: A Practical Low-light Image Enhancer", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Images captured under low-light conditions often suffer from (partially) poor\nvisibility. Besides unsatisfactory lightings, multiple types of degradations,\nsuch as noise and color distortion due to the limited quality of cameras, hide\nin the dark. In other words, solely turning up the brightness of dark regions\nwill inevitably amplify hidden artifacts. This work builds a simple yet\neffective network for \\textbf{Kin}dling the \\textbf{D}arkness (denoted as\nKinD), which, inspired by Retinex theory, decomposes images into two\ncomponents. One component (illumination) is responsible for light adjustment,\nwhile the other (reflectance) for degradation removal. In such a way, the\noriginal space is decoupled into two smaller subspaces, expecting to be better\nregularized/learned. It is worth to note that our network is trained with\npaired images shot under different exposure conditions, instead of using any\nground-truth reflectance and illumination information. Extensive experiments\nare conducted to demonstrate the efficacy of our design and its superiority\nover state-of-the-art alternatives. Our KinD is robust against severe visual\ndefects, and user-friendly to arbitrarily adjust light levels. In addition, our\nmodel spends less than 50ms to process an image in VGA resolution on a 2080Ti\nGPU. All the above merits make our KinD attractive for practical use.\n", "versions": [{"version": "v1", "created": "Sat, 4 May 2019 11:05:20 GMT"}], "update_date": "2019-05-13", "authors_parsed": [["Zhang", "Yonghua", ""], ["Zhang", "Jiawan", ""], ["Guo", "Xiaojie", ""]]}, {"id": "1905.04172", "submitter": "Christian Etmann", "authors": "Christian Etmann, Sebastian Lunz, Peter Maass, Carola-Bibiane\n  Sch\\\"onlieb", "title": "On the Connection Between Adversarial Robustness and Saliency Map\n  Interpretability", "comments": "12 pages, accepted for publication at the 36th International\n  Conference on Machine Learning 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent studies on the adversarial vulnerability of neural networks have shown\nthat models trained to be more robust to adversarial attacks exhibit more\ninterpretable saliency maps than their non-robust counterparts. We aim to\nquantify this behavior by considering the alignment between input image and\nsaliency map. We hypothesize that as the distance to the decision boundary\ngrows,so does the alignment. This connection is strictly true in the case of\nlinear models. We confirm these theoretical findings with experiments based on\nmodels trained with a local Lipschitz regularization and identify where the\nnon-linear nature of neural networks weakens the relation.\n", "versions": [{"version": "v1", "created": "Fri, 10 May 2019 13:45:21 GMT"}], "update_date": "2019-05-13", "authors_parsed": [["Etmann", "Christian", ""], ["Lunz", "Sebastian", ""], ["Maass", "Peter", ""], ["Sch\u00f6nlieb", "Carola-Bibiane", ""]]}, {"id": "1905.04175", "submitter": "Fabien Lotte", "authors": "Giuseppe Amato (CNR PISA), Malte Behrmann, Fr\\'ed\\'eric Bimbot\n  (PANAMA), Baptiste Caramiaux (LRI, EX-SITU), Fabrizio Falchi (CNR PISA),\n  Ander Garcia, Joost Geurts (Inria), Jaume Gibert, Guillaume Gravier\n  (LinkMedia), Hadmut Holken, Hartmut Koenitz (HKU), Sylvain Lefebvre (MFX),\n  Antoine Liutkus (LORIA, ZENITH), Fabien Lotte (Potioc, LaBRI), Andrew Perkis\n  (NTNU), Rafael Redondo, Enrico Turrin (FEP), Thierry Vieville (Mnemosyne),\n  Emmanuel Vincent (MULTISPEECH)", "title": "AI in the media and creative industries", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Thanks to the Big Data revolution and increasing computing capacities,\nArtificial Intelligence (AI) has made an impressive revival over the past few\nyears and is now omnipresent in both research and industry. The creative\nsectors have always been early adopters of AI technologies and this continues\nto be the case. As a matter of fact, recent technological developments keep\npushing the boundaries of intelligent systems in creative applications: the\ncritically acclaimed movie \"Sunspring\", released in 2016, was entirely written\nby AI technology, and the first-ever Music Album, called \"Hello World\",\nproduced using AI has been released this year. Simultaneously, the exploratory\nnature of the creative process is raising important technical challenges for AI\nsuch as the ability for AI-powered techniques to be accurate under limited data\nresources, as opposed to the conventional \"Big Data\" approach, or the ability\nto process, analyse and match data from multiple modalities (text, sound,\nimages, etc.) at the same time. The purpose of this white paper is to\nunderstand future technological advances in AI and their growing impact on\ncreative industries. This paper addresses the following questions: Where does\nAI operate in creative Industries? What is its operative role? How will AI\ntransform creative industries in the next ten years? This white paper aims to\nprovide a realistic perspective of the scope of AI actions in creative\nindustries, proposes a vision of how this technology could contribute to\nresearch and development works in such context, and identifies research and\ndevelopment challenges.\n", "versions": [{"version": "v1", "created": "Fri, 10 May 2019 13:56:52 GMT"}], "update_date": "2019-05-13", "authors_parsed": [["Amato", "Giuseppe", "", "CNR PISA"], ["Behrmann", "Malte", "", "PANAMA"], ["Bimbot", "Fr\u00e9d\u00e9ric", "", "PANAMA"], ["Caramiaux", "Baptiste", "", "LRI, EX-SITU"], ["Falchi", "Fabrizio", "", "CNR PISA"], ["Garcia", "Ander", "", "Inria"], ["Geurts", "Joost", "", "Inria"], ["Gibert", "Jaume", "", "LinkMedia"], ["Gravier", "Guillaume", "", "LinkMedia"], ["Holken", "Hadmut", "", "HKU"], ["Koenitz", "Hartmut", "", "HKU"], ["Lefebvre", "Sylvain", "", "MFX"], ["Liutkus", "Antoine", "", "LORIA, ZENITH"], ["Lotte", "Fabien", "", "Potioc, LaBRI"], ["Perkis", "Andrew", "", "NTNU"], ["Redondo", "Rafael", "", "FEP"], ["Turrin", "Enrico", "", "FEP"], ["Vieville", "Thierry", "", "Mnemosyne"], ["Vincent", "Emmanuel", "", "MULTISPEECH"]]}, {"id": "1905.04197", "submitter": "Tae Joon Jun", "authors": "Tae Joon Jun, Jihoon Kweon, Young-Hak Kim, Daeyoung Kim", "title": "T-Net: Nested encoder-decoder architecture for the main vessel\n  segmentation in coronary angiography", "comments": "Neural Networks, Accepted", "journal-ref": null, "doi": "10.1016/j.neunet.2020.05.002", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we proposed T-Net containing a small encoder-decoder inside\nthe encoder-decoder structure (EDiED). T-Net overcomes the limitation that\nU-Net can only have a single set of the concatenate layer between encoder and\ndecoder block. To be more precise, the U-Net symmetrically forms the\nconcatenate layers, so the low-level feature of the encoder is connected to the\nlatter part of the decoder, and the high-level feature is connected to the\nbeginning of the decoder. T-Net arranges the pooling and up-sampling\nappropriately during the encoder process, and likewise during the decoding\nprocess so that feature-maps of various sizes are obtained in a single block.\nAs a result, all features from the low-level to the high-level extracted from\nthe encoder are delivered from the beginning of the decoder to predict a more\naccurate mask. We evaluated T-Net for the problem of segmenting three main\nvessels in coronary angiography images. The experiment consisted of a\ncomparison of U-Net and T-Nets under the same conditions, and an optimized\nT-Net for the main vessel segmentation. As a result, T-Net recorded a Dice\nSimilarity Coefficient score (DSC) of 0.815, 0.095 higher than that of U-Net,\nand the optimized T-Net recorded a DSC of 0.890 which was 0.170 higher than\nthat of U-Net. In addition, we visualized the weight activation of the\nconvolutional layer of T-Net and U-Net to show that T-Net actually predicts the\nmask from earlier decoders. Therefore, we expect that T-Net can be effectively\napplied to other similar medical image segmentation problems.\n", "versions": [{"version": "v1", "created": "Fri, 10 May 2019 14:38:24 GMT"}, {"version": "v2", "created": "Wed, 20 May 2020 04:47:32 GMT"}], "update_date": "2020-05-22", "authors_parsed": [["Jun", "Tae Joon", ""], ["Kweon", "Jihoon", ""], ["Kim", "Young-Hak", ""], ["Kim", "Daeyoung", ""]]}, {"id": "1905.04215", "submitter": "Xudong Mao", "authors": "Xudong Mao, Yun Ma, Zhenguo Yang, Yangbin Chen, Qing Li", "title": "Virtual Mixup Training for Unsupervised Domain Adaptation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of unsupervised domain adaptation which aims to adapt\nmodels trained on a labeled source domain to a completely unlabeled target\ndomain. Recently, the cluster assumption has been applied to unsupervised\ndomain adaptation and achieved strong performance. One critical factor in\nsuccessful training of the cluster assumption is to impose the\nlocally-Lipschitz constraint to the model. Existing methods only impose the\nlocally-Lipschitz constraint around the training points while miss the other\nareas, such as the points in-between training data. In this paper, we address\nthis issue by encouraging the model to behave linearly in-between training\npoints. We propose a new regularization method called Virtual Mixup Training\n(VMT), which is able to incorporate the locally-Lipschitz constraint to the\nareas in-between training data. Unlike the traditional mixup model, our method\nconstructs the combination samples without using the label information,\nallowing it to apply to unsupervised domain adaptation. The proposed method is\ngeneric and can be combined with most existing models such as the recent\nstate-of-the-art model called VADA. Extensive experiments demonstrate that VMT\nsignificantly improves the performance of VADA on six domain adaptation\nbenchmark datasets. For the challenging task of adapting MNIST to SVHN, VMT can\nimprove the accuracy of VADA by over 30\\%. Code is available at\n\\url{https://github.com/xudonmao/VMT}.\n", "versions": [{"version": "v1", "created": "Fri, 10 May 2019 15:24:17 GMT"}, {"version": "v2", "created": "Fri, 24 May 2019 06:45:42 GMT"}, {"version": "v3", "created": "Sat, 6 Jul 2019 13:25:00 GMT"}, {"version": "v4", "created": "Fri, 6 Sep 2019 17:32:55 GMT"}], "update_date": "2019-09-09", "authors_parsed": [["Mao", "Xudong", ""], ["Ma", "Yun", ""], ["Yang", "Zhenguo", ""], ["Chen", "Yangbin", ""], ["Li", "Qing", ""]]}, {"id": "1905.04222", "submitter": "Alexander Wong", "authors": "Zhong Qiu Lin, Brendan Chwyl, and Alexander Wong", "title": "EdgeSegNet: A Compact Network for Semantic Segmentation", "comments": "5 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this study, we introduce EdgeSegNet, a compact deep convolutional neural\nnetwork for the task of semantic segmentation. A human-machine collaborative\ndesign strategy is leveraged to create EdgeSegNet, where principled network\ndesign prototyping is coupled with machine-driven design exploration to create\nnetworks with customized module-level macroarchitecture and microarchitecture\ndesigns tailored for the task. Experimental results showed that EdgeSegNet can\nachieve semantic segmentation accuracy comparable with much larger and\ncomputationally complex networks (>20x} smaller model size than RefineNet) as\nwell as achieving an inference speed of ~38.5 FPS on an NVidia Jetson AGX\nXavier. As such, the proposed EdgeSegNet is well-suited for low-power edge\nscenarios.\n", "versions": [{"version": "v1", "created": "Fri, 10 May 2019 15:43:23 GMT"}], "update_date": "2019-05-13", "authors_parsed": [["Lin", "Zhong Qiu", ""], ["Chwyl", "Brendan", ""], ["Wong", "Alexander", ""]]}, {"id": "1905.04225", "submitter": "Okan K\\\"op\\\"ukl\\\"u", "authors": "Okan K\\\"op\\\"ukl\\\"u, Yao Rong, Gerhard Rigoll", "title": "Talking With Your Hands: Scaling Hand Gestures and Recognition With CNNs", "comments": "Accepted to ICCV 2019 workshop - Observing and Understanding Hands in\n  Action (HANDS 2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The use of hand gestures provides a natural alternative to cumbersome\ninterface devices for Human-Computer Interaction (HCI) systems. As the\ntechnology advances and communication between humans and machines becomes more\ncomplex, HCI systems should also be scaled accordingly in order to accommodate\nthe introduced complexities. In this paper, we propose a methodology to scale\nhand gestures by forming them with predefined gesture-phonemes, and a\nconvolutional neural network (CNN) based framework to recognize hand gestures\nby learning only their constituents of gesture-phonemes. The total number of\npossible hand gestures can be increased exponentially by increasing the number\nof used gesture-phonemes. For this objective, we introduce a new benchmark\ndataset named Scaled Hand Gestures Dataset (SHGD) with only gesture-phonemes in\nits training set and 3-tuples gestures in the test set. In our experimental\nanalysis, we achieve to recognize hand gestures containing one and three\ngesture-phonemes with an accuracy of 98.47% (in 15 classes) and 94.69% (in 810\nclasses), respectively. Our dataset, code and pretrained models are publicly\navailable.\n", "versions": [{"version": "v1", "created": "Fri, 10 May 2019 15:49:16 GMT"}, {"version": "v2", "created": "Fri, 30 Aug 2019 08:28:07 GMT"}], "update_date": "2019-09-02", "authors_parsed": [["K\u00f6p\u00fckl\u00fc", "Okan", ""], ["Rong", "Yao", ""], ["Rigoll", "Gerhard", ""]]}, {"id": "1905.04243", "submitter": "Zhengwei Wang", "authors": "Zhengwei Wang, Qi She, Alan F. Smeaton, Tomas E. Ward, Graham Healy", "title": "Synthetic-Neuroscore: Using A Neuro-AI Interface for Evaluating\n  Generative Adversarial Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative adversarial networks (GANs) are increasingly attracting attention\nin the computer vision, natural language processing, speech synthesis and\nsimilar domains. Arguably the most striking results have been in the area of\nimage synthesis. However, evaluating the performance of GANs is still an open\nand challenging problem. Existing evaluation metrics primarily measure the\ndissimilarity between real and generated images using automated statistical\nmethods. They often require large sample sizes for evaluation and do not\ndirectly reflect human perception of image quality. In this work, we describe\nan evaluation metric we call Neuroscore, for evaluating the performance of\nGANs, that more directly reflects psychoperceptual image quality through the\nutilization of brain signals. Our results show that Neuroscore has superior\nperformance to the current evaluation metrics in that: (1) It is more\nconsistent with human judgment; (2) The evaluation process needs much smaller\nnumbers of samples; and (3) It is able to rank the quality of images on a per\nGAN basis. A convolutional neural network (CNN) based neuro-AI interface is\nproposed to predict Neuroscore from GAN-generated images directly without the\nneed for neural responses. Importantly, we show that including neural responses\nduring the training phase of the network can significantly improve the\nprediction capability of the proposed model. Materials related to this work are\nprovided at https://github.com/villawang/Neuro-AI-Interface.\n", "versions": [{"version": "v1", "created": "Fri, 10 May 2019 16:25:07 GMT"}, {"version": "v2", "created": "Mon, 3 Feb 2020 00:55:46 GMT"}], "update_date": "2020-02-04", "authors_parsed": [["Wang", "Zhengwei", ""], ["She", "Qi", ""], ["Smeaton", "Alan F.", ""], ["Ward", "Tomas E.", ""], ["Healy", "Graham", ""]]}, {"id": "1905.04247", "submitter": "Parvin Yousefikamal", "authors": "Parvin Yousefikamal", "title": "Breast Tumor Classification and Segmentation using Convolutional Neural\n  Networks", "comments": "12 Pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Breast cancer is considered as the most fatal type of cancer among women\nworldwide and it is crucially important to be diagnosed at its early stages. In\nthe current study, we aim to represent a fast and efficient framework which\nconsists of two main parts:1- image classification, and 2- tumor region\nsegmentation. At the initial stage, the images are classified into the two\ncategories of normal and abnormal. Since the Deep Neural Networks have\nperformed successfully in machine vision task, we would employ the\nconvolutional neural networks for the classification of images. In the second\nstage, the suggested framework is to diagnose and segment the tumor in the\nmammography images. First, the mammography images are pre-processed by removing\nnoise and artifacts, and then, segment the image using the level-set algorithm\nbased on the spatial fuzzy c-means clustering. The proper initialization and\noptimal configuration have strong effects on the performance of the level-set\nsegmentation. Thus, in our suggested framework, we have improved the level-set\nalgorithm by utilizing the spatial fuzzy c-means clustering which ultimately\nresults in a more precise segmentation. In order to evaluate the proposed\napproach, we conducted experiments using the Mammographic Image Analysis (MIAS)\ndataset. The tests have shown that the convolutional neural networks could\nachieve high accuracy in classification of images. Moreover, the improved\nlevel-set segmentation method, along with the fuzzy c-means clustering, could\nperfectly do the segmentation on the tumor area. The suggested method has\nclassified the images with the accuracy of 78% and the AUC of 69%, which, as\ncompared to the previous methods, is 2% more accurate and 6% better AUC; and\nhas been able to extract the tumor area in a more precise way.\n", "versions": [{"version": "v1", "created": "Fri, 10 May 2019 16:33:23 GMT"}], "update_date": "2019-05-13", "authors_parsed": [["Yousefikamal", "Parvin", ""]]}, {"id": "1905.04266", "submitter": "Anurag Arnab", "authors": "Anurag Arnab, Carl Doersch, Andrew Zisserman", "title": "Exploiting temporal context for 3D human pose estimation in the wild", "comments": "CVPR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a bundle-adjustment-based algorithm for recovering accurate 3D\nhuman pose and meshes from monocular videos. Unlike previous algorithms which\noperate on single frames, we show that reconstructing a person over an entire\nsequence gives extra constraints that can resolve ambiguities. This is because\nvideos often give multiple views of a person, yet the overall body shape does\nnot change and 3D positions vary slowly. Our method improves not only on\nstandard mocap-based datasets like Human 3.6M -- where we show quantitative\nimprovements -- but also on challenging in-the-wild datasets such as Kinetics.\nBuilding upon our algorithm, we present a new dataset of more than 3 million\nframes of YouTube videos from Kinetics with automatically generated 3D poses\nand meshes. We show that retraining a single-frame 3D pose estimator on this\ndata improves accuracy on both real-world and mocap data by evaluating on the\n3DPW and HumanEVA datasets.\n", "versions": [{"version": "v1", "created": "Fri, 10 May 2019 17:12:27 GMT"}], "update_date": "2019-05-13", "authors_parsed": [["Arnab", "Anurag", ""], ["Doersch", "Carl", ""], ["Zisserman", "Andrew", ""]]}, {"id": "1905.04270", "submitter": "Fuxun Yu", "authors": "Fuxun Yu, Zhuwei Qin, Chenchen Liu, Liang Zhao, Yanzhi Wang, Xiang\n  Chen", "title": "Interpreting and Evaluating Neural Network Robustness", "comments": "Accepted in IJCAI'19", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, adversarial deception becomes one of the most considerable threats\nto deep neural networks. However, compared to extensive research in new designs\nof various adversarial attacks and defenses, the neural networks' intrinsic\nrobustness property is still lack of thorough investigation. This work aims to\nqualitatively interpret the adversarial attack and defense mechanism through\nloss visualization, and establish a quantitative metric to evaluate the neural\nnetwork model's intrinsic robustness. The proposed robustness metric identifies\nthe upper bound of a model's prediction divergence in the given domain and thus\nindicates whether the model can maintain a stable prediction. With extensive\nexperiments, our metric demonstrates several advantages over conventional\nadversarial testing accuracy based robustness estimation: (1) it provides a\nuniformed evaluation to models with different structures and parameter scales;\n(2) it over-performs conventional accuracy based robustness estimation and\nprovides a more reliable evaluation that is invariant to different test\nsettings; (3) it can be fast generated without considerable testing cost.\n", "versions": [{"version": "v1", "created": "Fri, 10 May 2019 17:21:15 GMT"}], "update_date": "2019-05-13", "authors_parsed": [["Yu", "Fuxun", ""], ["Qin", "Zhuwei", ""], ["Liu", "Chenchen", ""], ["Zhao", "Liang", ""], ["Wang", "Yanzhi", ""], ["Chen", "Xiang", ""]]}, {"id": "1905.04293", "submitter": "Naifan Zhuang", "authors": "Naifan Zhuang, Guo-Jun Qi, The Duc Kieu, and Kien A. Hua", "title": "Differential Recurrent Neural Network and its Application for Human\n  Activity Recognition", "comments": "arXiv admin note: substantial text overlap with arXiv:1504.06678,\n  arXiv:1804.04192", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Long Short-Term Memory (LSTM) recurrent neural network is capable of\nprocessing complex sequential information since it utilizes special gating\nschemes for learning representations from long input sequences. It has the\npotential to model any sequential time-series data, where the current hidden\nstate has to be considered in the context of the past hidden states. This\nproperty makes LSTM an ideal choice to learn the complex dynamics present in\nlong sequences. Unfortunately, the conventional LSTMs do not consider the\nimpact of spatio-temporal dynamics corresponding to the given salient motion\npatterns, when they gate the information that ought to be memorized through\ntime. To address this problem, we propose a differential gating scheme for the\nLSTM neural network, which emphasizes on the change in information gain caused\nby the salient motions between the successive video frames. This change in\ninformation gain is quantified by Derivative of States (DoS), and thus the\nproposed LSTM model is termed as differential Recurrent Neural Network (dRNN).\nIn addition, the original work used the hidden state at the last time-step to\nmodel the entire video sequence. Based on the energy profiling of DoS, we\nfurther propose to employ the State Energy Profile (SEP) to search for salient\ndRNN states and construct more informative representations. The effectiveness\nof the proposed model was demonstrated by automatically recognizing human\nactions from the real-world 2D and 3D single-person action datasets. We point\nout that LSTM is a special form of dRNN. As a result, we have introduced a new\nfamily of LSTMs. Our study is one of the first works towards demonstrating the\npotential of learning complex time-series representations via high-order\nderivatives of states.\n", "versions": [{"version": "v1", "created": "Thu, 9 May 2019 18:31:54 GMT"}], "update_date": "2019-05-14", "authors_parsed": [["Zhuang", "Naifan", ""], ["Qi", "Guo-Jun", ""], ["Kieu", "The Duc", ""], ["Hua", "Kien A.", ""]]}, {"id": "1905.04302", "submitter": "Uche Nnolim", "authors": "U. A. Nnolim", "title": "Analysis of Probabilistic multi-scale fractional order fusion-based\n  de-hazing algorithm", "comments": "22 pages, 8 figures, journal preprint", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this report, a de-hazing algorithm based on probability and multi-scale\nfractional order-based fusion is proposed. The proposed scheme improves on a\npreviously implemented multiscale fraction order-based fusion by augmenting its\nlocal contrast and edge sharpening features. It also brightens de-hazed images,\nwhile avoiding sky region over-enhancement. The results of the proposed\nalgorithm are analyzed and compared with existing methods from the literature\nand indicate better performance in most cases.\n", "versions": [{"version": "v1", "created": "Fri, 10 May 2019 10:33:54 GMT"}], "update_date": "2019-05-14", "authors_parsed": [["Nnolim", "U. A.", ""]]}, {"id": "1905.04354", "submitter": "Jonah Philion", "authors": "Jonah Philion", "title": "FastDraw: Addressing the Long Tail of Lane Detection by Adapting a\n  Sequential Prediction Network", "comments": "CVPR 2019 poster", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The search for predictive models that generalize to the long tail of sensor\ninputs is the central difficulty when developing data-driven models for\nautonomous vehicles. In this paper, we use lane detection to study modeling and\ntraining techniques that yield better performance on real world test drives. On\nthe modeling side, we introduce a novel fully convolutional model of lane\ndetection that learns to decode lane structures instead of delegating structure\ninference to post-processing. In contrast to previous works, our convolutional\ndecoder is able to represent an arbitrary number of lanes per image, preserves\nthe polyline representation of lanes without reducing lanes to polynomials, and\ndraws lanes iteratively without requiring the computational and temporal\ncomplexity of recurrent neural networks. Because our model includes an estimate\nof the joint distribution of neighboring pixels belonging to the same lane, our\nformulation includes a natural and computationally cheap definition of\nuncertainty. On the training side, we demonstrate a simple yet effective\napproach to adapt the model to new environments using unsupervised style\ntransfer. By training FastDraw to make predictions of lane structure that are\ninvariant to low-level stylistic differences between images, we achieve strong\nperformance at test time in weather and lighting conditions that deviate\nsubstantially from those of the annotated datasets that are publicly available.\nWe quantitatively evaluate our approach on the CVPR 2017 Tusimple lane marking\nchallenge, difficult CULane datasets, and a small labeled dataset of our own\nand achieve competitive accuracy while running at 90 FPS.\n", "versions": [{"version": "v1", "created": "Fri, 10 May 2019 19:39:32 GMT"}, {"version": "v2", "created": "Tue, 14 May 2019 12:59:26 GMT"}], "update_date": "2019-05-15", "authors_parsed": [["Philion", "Jonah", ""]]}, {"id": "1905.04368", "submitter": "Chee Seng Chan", "authors": "Lixin Fan and KamWoh Ng and Chee Seng Chan", "title": "Digital Passport: A Novel Technological Strategy for Intellectual\n  Property Protection of Convolutional Neural Networks", "comments": "This paper proposes a new timely IPR solution that embed digital\n  passports into CNN models to prevent the unauthorized network usage (i.e.\n  infringement) by paralyzing the networks while maintaining its functionality\n  for verified users", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In order to prevent deep neural networks from being infringed by unauthorized\nparties, we propose a generic solution which embeds a designated digital\npassport into a network, and subsequently, either paralyzes the network\nfunctionalities for unauthorized usages or maintain its functionalities in the\npresence of a verified passport. Such a desired network behavior is\nsuccessfully demonstrated in a number of implementation schemes, which provide\nreliable, preventive and timely protections against tens of thousands of\nfake-passport deceptions. Extensive experiments also show that the deep neural\nnetwork performance under unauthorized usages deteriorate significantly (e.g.\nwith 33% to 82% reductions of CIFAR10 classification accuracies), while\nnetworks endorsed with valid passports remain intact.\n", "versions": [{"version": "v1", "created": "Fri, 10 May 2019 20:13:38 GMT"}], "update_date": "2019-05-14", "authors_parsed": [["Fan", "Lixin", ""], ["Ng", "KamWoh", ""], ["Chan", "Chee Seng", ""]]}, {"id": "1905.04384", "submitter": "Sharib Ali Dr.", "authors": "Sharib Ali and Jens Rittscher", "title": "Efficient video indexing for monitoring disease activity and progression\n  in the upper gastrointestinal tract", "comments": "Accepted at IEEE International Symposium on Biomedical Imaging\n  (ISBI), 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Endoscopy is a routine imaging technique used for both diagnosis and\nminimally invasive surgical treatment. While the endoscopy video contains a\nwealth of information, tools to capture this information for the purpose of\nclinical reporting are rather poor. In date, endoscopists do not have any\naccess to tools that enable them to browse the video data in an efficient and\nuser friendly manner. Fast and reliable video retrieval methods could for\nexample, allow them to review data from previous exams and therefore improve\ntheir ability to monitor disease progression. Deep learning provides new\navenues of compressing and indexing video in an extremely efficient manner. In\nthis study, we propose to use an autoencoder for efficient video compression\nand fast retrieval of video images. To boost the accuracy of video image\nretrieval and to address data variability like multi-modality and view-point\nchanges, we propose the integration of a Siamese network. We demonstrate that\nour approach is competitive in retrieving images from 3 large scale videos of 3\ndifferent patients obtained against the query samples of their previous\ndiagnosis. Quantitative validation shows that the combined approach yield an\noverall improvement of 5% and 8% over classical and variational autoencoders,\nrespectively.\n", "versions": [{"version": "v1", "created": "Fri, 10 May 2019 21:31:11 GMT"}], "update_date": "2019-05-14", "authors_parsed": [["Ali", "Sharib", ""], ["Rittscher", "Jens", ""]]}, {"id": "1905.04385", "submitter": "Sharib Ali Dr.", "authors": "Sharib Ali, Nasullah Khalid Alham, Clare Verrill, Jens Rittscher", "title": "Ink removal from histopathology whole slide images by combining\n  classification, detection and image generation models", "comments": "Accepted paper at IEEE International Symposium on Biomedical Imaging\n  (ISBI) 2019, Venice, Italy", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Histopathology slides are routinely marked by pathologists using permanent\nink markers that should not be removed as they form part of the medical record.\nOften tumour regions are marked up for the purpose of highlighting features or\nother downstream processing such an gene sequencing. Once digitised there is no\nestablished method for removing this information from the whole slide images\nlimiting its usability in research and study. Removal of marker ink from these\nhigh-resolution whole slide images is non-trivial and complex problem as they\ncontaminate different regions and in an inconsistent manner. We propose an\nefficient pipeline using convolution neural networks that results in ink-free\nimages without compromising information and image resolution. Our pipeline\nincludes a sequential classical convolution neural network for accurate\nclassification of contaminated image tiles, a fast region detector and a domain\nadaptive cycle consistent adversarial generative model for restoration of\nforeground pixels. Both quantitative and qualitative results on four different\nwhole slide images show that our approach yields visually coherent ink-free\nwhole slide images.\n", "versions": [{"version": "v1", "created": "Fri, 10 May 2019 21:33:12 GMT"}], "update_date": "2019-05-14", "authors_parsed": [["Ali", "Sharib", ""], ["Alham", "Nasullah Khalid", ""], ["Verrill", "Clare", ""], ["Rittscher", "Jens", ""]]}, {"id": "1905.04392", "submitter": "Mohsen Joneidi", "authors": "Mohsen Joneidi, Ismail Alkhouri, Nazanin Rahnavard", "title": "Large-Scale Spectrum Occupancy Learning via Tensor Decomposition and\n  LSTM Networks", "comments": "Submitted to the 2019 IEEE Global Communications Conference\n  (GLOBECOM)", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A new paradigm for large-scale spectrum occupancy learning based on long\nshort-term memory (LSTM) recurrent neural networks is proposed. Studies have\nshown that spectrum usage is a highly correlated time series. Moreover, there\nis a correlation for occupancy of spectrum between different frequency\nchannels. Therefore, revealing all these correlations using learning and\nprediction of one-dimensional time series is not a trivial task. In this paper,\nwe introduce a new framework for representing the spectrum measurements in a\ntensor format. Next, a time-series prediction method based on CANDECOMP/PARFAC\n(CP) tensor decomposition and LSTM recurrent neural networks is proposed. The\nproposed method is computationally efficient and is able to capture different\ntypes of correlation within the measured spectrum. Moreover, it is robust\nagainst noise and missing entries of sensed spectrum. The superiority of the\nproposed method is evaluated over a large-scale synthetic dataset in terms of\nprediction accuracy and computational efficiency.\n", "versions": [{"version": "v1", "created": "Fri, 10 May 2019 22:22:04 GMT"}], "update_date": "2019-05-14", "authors_parsed": [["Joneidi", "Mohsen", ""], ["Alkhouri", "Ismail", ""], ["Rahnavard", "Nazanin", ""]]}, {"id": "1905.04398", "submitter": "Avinash Ravichandran", "authors": "Avinash Ravichandran, Rahul Bhotika, Stefano Soatto", "title": "Few-Shot Learning with Embedded Class Models and Shot-Free Meta Training", "comments": "Accepted to ICCV 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a method for learning embeddings for few-shot learning that is\nsuitable for use with any number of ways and any number of shots (shot-free).\nRather than fixing the class prototypes to be the Euclidean average of sample\nembeddings, we allow them to live in a higher-dimensional space (embedded class\nmodels) and learn the prototypes along with the model parameters. The class\nrepresentation function is defined implicitly, which allows us to deal with a\nvariable number of shots per each class with a simple constant-size\narchitecture. The class embedding encompasses metric learning, that facilitates\nadding new classes without crowding the class representation space. Despite\nbeing general and not tuned to the benchmark, our approach achieves\nstate-of-the-art performance on the standard few-shot benchmark datasets.\n", "versions": [{"version": "v1", "created": "Fri, 10 May 2019 23:21:31 GMT"}, {"version": "v2", "created": "Tue, 21 Apr 2020 18:15:58 GMT"}], "update_date": "2020-04-23", "authors_parsed": [["Ravichandran", "Avinash", ""], ["Bhotika", "Rahul", ""], ["Soatto", "Stefano", ""]]}, {"id": "1905.04405", "submitter": "Ronghang Hu", "authors": "Ronghang Hu, Anna Rohrbach, Trevor Darrell, Kate Saenko", "title": "Language-Conditioned Graph Networks for Relational Reasoning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Solving grounded language tasks often requires reasoning about relationships\nbetween objects in the context of a given task. For example, to answer the\nquestion \"What color is the mug on the plate?\" we must check the color of the\nspecific mug that satisfies the \"on\" relationship with respect to the plate.\nRecent work has proposed various methods capable of complex relational\nreasoning. However, most of their power is in the inference structure, while\nthe scene is represented with simple local appearance features. In this paper,\nwe take an alternate approach and build contextualized representations for\nobjects in a visual scene to support relational reasoning. We propose a general\nframework of Language-Conditioned Graph Networks (LCGN), where each node\nrepresents an object, and is described by a context-aware representation from\nrelated objects through iterative message passing conditioned on the textual\ninput. E.g., conditioning on the \"on\" relationship to the plate, the object\n\"mug\" gathers messages from the object \"plate\" to update its representation to\n\"mug on the plate\", which can be easily consumed by a simple classifier for\nanswer prediction. We experimentally show that our LCGN approach effectively\nsupports relational reasoning and improves performance across several tasks and\ndatasets. Our code is available at http://ronghanghu.com/lcgn.\n", "versions": [{"version": "v1", "created": "Fri, 10 May 2019 23:47:05 GMT"}, {"version": "v2", "created": "Fri, 16 Aug 2019 18:55:14 GMT"}], "update_date": "2019-08-20", "authors_parsed": [["Hu", "Ronghang", ""], ["Rohrbach", "Anna", ""], ["Darrell", "Trevor", ""], ["Saenko", "Kate", ""]]}, {"id": "1905.04411", "submitter": "Angelina Wang", "authors": "Angelina Wang, Thanard Kurutach, Kara Liu, Pieter Abbeel, Aviv Tamar", "title": "Learning Robotic Manipulation through Visual Planning and Acting", "comments": "RSS 2019. Website at https://sites.google.com/berkeley.edu/vpa/home", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Planning for robotic manipulation requires reasoning about the changes a\nrobot can affect on objects. When such interactions can be modelled\nanalytically, as in domains with rigid objects, efficient planning algorithms\nexist. However, in both domestic and industrial domains, the objects of\ninterest can be soft, or deformable, and hard to model analytically. For such\ncases, we posit that a data-driven modelling approach is more suitable. In\nrecent years, progress in deep generative models has produced methods that\nlearn to `imagine' plausible images from data. Building on the recent Causal\nInfoGAN generative model, in this work we learn to imagine goal-directed object\nmanipulation directly from raw image data of self-supervised interaction of the\nrobot with the object. After learning, given a goal observation of the system,\nour model can generate an imagined plan -- a sequence of images that transition\nthe object into the desired goal. To execute the plan, we use it as a reference\ntrajectory to track with a visual servoing controller, which we also learn from\nthe data as an inverse dynamics model. In a simulated manipulation task, we\nshow that separating the problem into visual planning and visual tracking\ncontrol is more sample efficient and more interpretable than alternative\ndata-driven approaches. We further demonstrate our approach on learning to\nimagine and execute in 3 environments, the final of which is deformable rope\nmanipulation on a PR2 robot.\n", "versions": [{"version": "v1", "created": "Sat, 11 May 2019 00:30:21 GMT"}], "update_date": "2019-05-14", "authors_parsed": [["Wang", "Angelina", ""], ["Kurutach", "Thanard", ""], ["Liu", "Kara", ""], ["Abbeel", "Pieter", ""], ["Tamar", "Aviv", ""]]}, {"id": "1905.04421", "submitter": "Alireza Sepas-Moghaddam", "authors": "Alireza Sepas-Moghaddam, Ali Etemad, Fernando Pereira, and Paulo\n  Lobato Correia", "title": "Long Short-Term Memory with Gate and State Level Fusion for Light\n  Field-Based Face Recognition", "comments": "Submitted to IEEE TIFS", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Long Short-Term Memory (LSTM) is a prominent recurrent neural network for\nextracting dependencies from sequential data such as time-series and multi-view\ndata, having achieved impressive results for different visual recognition\ntasks. A conventional LSTM network can learn a model to posteriorly extract\ninformation from one input sequence. However, if two or more dependent\nsequences of data are simultaneously acquired, the conventional LSTM networks\nmay only process those sequences consecutively, not taking benefit of the\ninformation carried out by their mutual dependencies. In this context, this\npaper proposes two novel LSTM cell architectures that are able to jointly learn\nfrom multiple sequences simultaneously acquired, targeting to create richer and\nmore effective models for recognition tasks. The efficacy of the novel LSTM\ncell architectures is assessed by integrating them into deep learning-based\nmethods for face recognition with multi-view, light field images. The new cell\narchitectures jointly learn the scene horizontal and vertical parallaxes\navailable in a light field image, to capture richer spatio-angular information\nfrom both directions. A comprehensive evaluation, with the IST-EURECOM LFFD\ndataset using three challenging evaluation protocols, shows the advantage of\nusing the novel LSTM cell architectures for face recognition over the\nstate-of-the-art light field-based methods. These results highlight the added\nvalue of the novel cell architectures when learning from correlated input\nsequences.\n", "versions": [{"version": "v1", "created": "Sat, 11 May 2019 01:38:50 GMT"}, {"version": "v2", "created": "Mon, 1 Jun 2020 20:59:56 GMT"}], "update_date": "2020-06-03", "authors_parsed": [["Sepas-Moghaddam", "Alireza", ""], ["Etemad", "Ali", ""], ["Pereira", "Fernando", ""], ["Correia", "Paulo Lobato", ""]]}, {"id": "1905.04424", "submitter": "Songsong Wu", "authors": "Songsong Wu, Yan Yan, Hao Tang, Jianjun Qian, Jian Zhang, Xiao-Yuan\n  Jing", "title": "Structured Discriminative Tensor Dictionary Learning for Unsupervised\n  Domain Adaptation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unsupervised Domain Adaptation (UDA) addresses the problem of performance\ndegradation due to domain shift between training and testing sets, which is\ncommon in computer vision applications. Most existing UDA approaches are based\non vector-form data although the typical format of data or features in visual\napplications is multi-dimensional tensor. Besides, current methods, including\nthe deep network approaches, assume that abundant labeled source samples are\nprovided for training. However, the number of labeled source samples are always\nlimited due to expensive annotation cost in practice, making sub-optimal\nperformance been observed. In this paper, we propose to seek discriminative\nrepresentation for multi-dimensional data by learning a structured dictionary\nin tensor space. The dictionary separates domain-specific information and\nclass-specific information to guarantee the representation robust to domains.\nIn addition, a pseudo-label estimation scheme is developed to combine with\ndiscriminant analysis in the algorithm iteration for avoiding the external\nclassifier design. We perform extensive results on different datasets with\nlimited source samples. Experimental results demonstrates that the proposed\nmethod outperforms the state-of-the-art approaches.\n", "versions": [{"version": "v1", "created": "Sat, 11 May 2019 02:28:04 GMT"}], "update_date": "2019-05-14", "authors_parsed": [["Wu", "Songsong", ""], ["Yan", "Yan", ""], ["Tang", "Hao", ""], ["Qian", "Jianjun", ""], ["Zhang", "Jian", ""], ["Jing", "Xiao-Yuan", ""]]}, {"id": "1905.04425", "submitter": "Haitao Yang", "authors": "Yajing Xu, Haitao Yang, Mingfei Cheng, Si Li", "title": "Cyclone intensity estimate with context-aware cyclegan", "comments": "5 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning approaches to cyclone intensity estimationhave recently shown\npromising results. However, sufferingfrom the extreme scarcity of cyclone data\non specific in-tensity, most existing deep learning methods fail to\nachievesatisfactory performance on cyclone intensity estimation,especially on\nclasses with few instances. To avoid the degra-dation of recognition\nperformance caused by scarce samples,we propose a context-aware CycleGAN which\nlearns the la-tent evolution features from adjacent cyclone intensity\nandsynthesizes CNN features of classes lacking samples fromunpaired source\nclasses. Specifically, our approach synthe-sizes features conditioned on the\nlearned evolution features,while the extra information is not required.\nExperimentalresults of several evaluation methods show the effectivenessof our\napproach, even can predicting unseen classes.\n", "versions": [{"version": "v1", "created": "Sat, 11 May 2019 02:37:34 GMT"}], "update_date": "2019-05-14", "authors_parsed": [["Xu", "Yajing", ""], ["Yang", "Haitao", ""], ["Cheng", "Mingfei", ""], ["Li", "Si", ""]]}, {"id": "1905.04430", "submitter": "Mohammad Mahdi Kazemi Moghaddam", "authors": "Mohammad Mahdi Kazemi Moghaddam, Ehsan Abbasnejad and Javen Shi", "title": "Follow the Attention: Combining Partial Pose and Object Motion for\n  Fine-Grained Action Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Retailers have long been searching for ways to effectively understand their\ncustomers' behaviour in order to provide a smooth and pleasant shopping\nexperience that attracts more customers everyday and maximises their revenue,\nconsequently. Humans can flawlessly understand others' behaviour by combining\ndifferent visual cues from activity to gestures and facial expressions.\nEmpowering the computer vision systems to do so, however, is still an open\nproblem due to its intrinsic challenges as well as extrinsic enforced\ndifficulties like lack of publicly available data and unique environment\nconditions (wild). In this work, We emphasise on detecting the first and by far\nthe most crucial cue in behaviour analysis; that is human activity detection in\ncomputer vision. To do so, we introduce a framework for integrating human pose\nand object motion to both temporally detect and classify the activities in a\nfine-grained manner (very short and similar activities). We incorporate partial\nhuman pose and interaction with the objects in a multi-stream neural network\narchitecture to guide the spatiotemporal attention mechanism for more efficient\nactivity recognition. To this end, in the absence of pose supervision, we\npropose to use the Generative Adversarial Network (GAN) to generate exact joint\nlocations from noisy probability heat maps. Additionally, based on the\nintuition that complex actions demand more than one source of information to be\nidentified even by humans, we integrate the second stream of object motion to\nour network as a prior knowledge that we quantitatively show improves the\nrecognition results. We empirically show the capability of our approach by\nachieving state-of-the-art results on MERL shopping dataset. We further\ninvestigate the effectiveness of this approach on a new shopping dataset that\nwe have collected to address existing shortcomings.\n", "versions": [{"version": "v1", "created": "Sat, 11 May 2019 02:54:01 GMT"}, {"version": "v2", "created": "Wed, 26 Jun 2019 04:44:58 GMT"}], "update_date": "2019-06-27", "authors_parsed": [["Moghaddam", "Mohammad Mahdi Kazemi", ""], ["Abbasnejad", "Ehsan", ""], ["Shi", "Javen", ""]]}, {"id": "1905.04432", "submitter": "Songsong Wu", "authors": "Songsong Wu, Zhiqiang Lu, Hao Tang, Yan Yan, Songhao Zhu, Xiao-Yuan\n  Jing, Zuoyong Li", "title": "Joint Learning of Self-Representation and Indicator for Multi-View Image\n  Clustering", "comments": "Accepted by ICIP 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-view subspace clustering aims to divide a set of multisource data into\nseveral groups according to their underlying subspace structure. Although the\nspectral clustering based methods achieve promotion in multi-view clustering,\ntheir utility is limited by the separate learning manner in which affinity\nmatrix construction and cluster indicator estimation are isolated. In this\npaper, we propose to jointly learn the self-representation, continue and\ndiscrete cluster indicators in an unified model. Our model can explore the\nsubspace structure of each view and fusion them to facilitate clustering\nsimultaneously. Experimental results on two benchmark datasets demonstrate that\nour method outperforms other existing competitive multi-view clustering\nmethods.\n", "versions": [{"version": "v1", "created": "Sat, 11 May 2019 02:57:00 GMT"}], "update_date": "2019-05-14", "authors_parsed": [["Wu", "Songsong", ""], ["Lu", "Zhiqiang", ""], ["Tang", "Hao", ""], ["Yan", "Yan", ""], ["Zhu", "Songhao", ""], ["Jing", "Xiao-Yuan", ""], ["Li", "Zuoyong", ""]]}, {"id": "1905.04446", "submitter": "Pravendra Singh", "authors": "Pravendra Singh, Vinay Kumar Verma, Piyush Rai, Vinay P. Namboodiri", "title": "Play and Prune: Adaptive Filter Pruning for Deep Model Compression", "comments": "International Joint Conference on Artificial Intelligence\n  (IJCAI-2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While convolutional neural networks (CNN) have achieved impressive\nperformance on various classification/recognition tasks, they typically consist\nof a massive number of parameters. This results in significant memory\nrequirement as well as computational overheads. Consequently, there is a\ngrowing need for filter-level pruning approaches for compressing CNN based\nmodels that not only reduce the total number of parameters but reduce the\noverall computation as well. We present a new min-max framework for\nfilter-level pruning of CNNs. Our framework, called Play and Prune (PP),\njointly prunes and fine-tunes CNN model parameters, with an adaptive pruning\nrate, while maintaining the model's predictive performance. Our framework\nconsists of two modules: (1) An adaptive filter pruning (AFP) module, which\nminimizes the number of filters in the model; and (2) A pruning rate controller\n(PRC) module, which maximizes the accuracy during pruning. Moreover, unlike\nmost previous approaches, our approach allows directly specifying the desired\nerror tolerance instead of pruning level. Our compressed models can be deployed\nat run-time, without requiring any special libraries or hardware. Our approach\nreduces the number of parameters of VGG-16 by an impressive factor of 17.5X,\nand number of FLOPS by 6.43X, with no loss of accuracy, significantly\noutperforming other state-of-the-art filter pruning methods.\n", "versions": [{"version": "v1", "created": "Sat, 11 May 2019 04:37:10 GMT"}], "update_date": "2019-05-14", "authors_parsed": [["Singh", "Pravendra", ""], ["Verma", "Vinay Kumar", ""], ["Rai", "Piyush", ""], ["Namboodiri", "Vinay P.", ""]]}, {"id": "1905.04449", "submitter": "Tai-Xiang Jiang", "authors": "Xi-Le Zhao, Wen-Hao Xu, Tai-Xiang Jiang, Yao Wang and Michael Ng", "title": "Deep Plug-and-play Prior for Low-rank Tensor Completion", "comments": "Accepted by Neurocoputing. Code availiable at\n  https://github.com/TaiXiangJiang/Deep-Plug-and-Play-Prior-for-Low-Rank-Tensor-Completion", "journal-ref": null, "doi": "10.1016/j.neucom.2020.03.018", "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-dimensional images, such as color images and multi-spectral images, are\nhighly correlated and contain abundant spatial and spectral information.\nHowever, real-world multi-dimensional images are usually corrupted by missing\nentries. By integrating deterministic low-rankness prior to the data-driven\ndeep prior, we suggest a novel regularized tensor completion model for\nmulti-dimensional image completion. In the objective function, we adopt the\nnewly emerged tensor nuclear norm (TNN) to characterize the global low-rankness\nprior of the multi-dimensional images. We also formulate an implicit\nregularizer by plugging into a denoising neural network (termed as deep\ndenoiser), which is convinced to express the deep image prior learned from a\nlarge number of natural images. The resulting model can be solved by the\nalternating directional method of multipliers algorithm under the plug-and-play\n(PnP) framework. Experimental results on color images, videos, and\nmulti-spectral images demonstrate that the proposed method can recover both the\nglobal structure and fine details very well and achieve superior performance\nover competing methods in terms of quality metrics and visual effects.\n", "versions": [{"version": "v1", "created": "Sat, 11 May 2019 05:10:45 GMT"}, {"version": "v2", "created": "Sat, 14 Sep 2019 17:22:22 GMT"}, {"version": "v3", "created": "Mon, 4 May 2020 04:49:19 GMT"}], "update_date": "2020-05-05", "authors_parsed": [["Zhao", "Xi-Le", ""], ["Xu", "Wen-Hao", ""], ["Jiang", "Tai-Xiang", ""], ["Wang", "Yao", ""], ["Ng", "Michael", ""]]}, {"id": "1905.04451", "submitter": "Zhaokang Chen", "authors": "Zhaokang Chen and Bertram E. Shi", "title": "Offset Calibration for Appearance-Based Gaze Estimation via Gaze\n  Decomposition", "comments": "Accepted by WACV2020. This is not the camera-ready version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Appearance-based gaze estimation provides relatively unconstrained gaze\ntracking. However, subject-independent models achieve limited accuracy partly\ndue to individual variations. To improve estimation, we propose a novel gaze\ndecomposition method and a single gaze point calibration method, motivated by\nour finding that the inter-subject squared bias exceeds the intra-subject\nvariance for a subject-independent estimator. We decompose the gaze angle into\na subject-dependent bias term and a subject-independent term between the gaze\nangle and the bias. The subject-independent term is estimated by a deep\nconvolutional network. For calibration-free tracking, we set the\nsubject-dependent bias term to zero. For single gaze point calibration, we\nestimate the bias from a few images taken as the subject gazes at a point.\nExperiments on three datasets indicate that as a calibration-free estimator,\nthe proposed method outperforms the state-of-the-art methods by up to $10.0\\%$.\nThe proposed calibration method is robust and reduces estimation error\nsignificantly (up to $35.6\\%$), achieving state-of-the-art performance for\nappearance-based eye trackers with calibration.\n", "versions": [{"version": "v1", "created": "Sat, 11 May 2019 05:24:48 GMT"}, {"version": "v2", "created": "Thu, 9 Jan 2020 07:10:19 GMT"}], "update_date": "2020-01-10", "authors_parsed": [["Chen", "Zhaokang", ""], ["Shi", "Bertram E.", ""]]}, {"id": "1905.04453", "submitter": "Sudeep Pillai", "authors": "Sudeep Pillai and John Leonard", "title": "Self-Supervised Visual Place Recognition Learning in Mobile Robots", "comments": "Presented at Learning for Localization and Mapping Workshop at IROS\n  2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Place recognition is a critical component in robot navigation that enables it\nto re-establish previously visited locations, and simultaneously use this\ninformation to correct the drift incurred in its dead-reckoned estimate. In\nthis work, we develop a self-supervised approach to place recognition in\nrobots. The task of visual loop-closure identification is cast as a metric\nlearning problem, where the labels for positive and negative examples of\nloop-closures can be bootstrapped using a GPS-aided navigation solution that\nthe robot already uses. By leveraging the synchronization between sensors, we\nshow that we are able to learn an appropriate distance metric for arbitrary\nreal-valued image descriptors (including state-of-the-art CNN models), that is\nspecifically geared for visual place recognition in mobile robots. Furthermore,\nwe show that the newly learned embedding can be particularly powerful in\ndisambiguating visual scenes for the task of vision-based loop-closure\nidentification in mobile robots.\n", "versions": [{"version": "v1", "created": "Sat, 11 May 2019 05:49:36 GMT"}], "update_date": "2019-05-14", "authors_parsed": [["Pillai", "Sudeep", ""], ["Leonard", "John", ""]]}, {"id": "1905.04457", "submitter": "Yushu Feng", "authors": "Yushu Feng, Huan Wang, Daniel T. Yi, Roland Hu", "title": "Triplet Distillation for Deep Face Recognition", "comments": "5 pages, 2 tables, accpeted by ICML 2019 ODML-CDNNR Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional neural networks (CNNs) have achieved a great success in face\nrecognition, which unfortunately comes at the cost of massive computation and\nstorage consumption. Many compact face recognition networks are thus proposed\nto resolve this problem. Triplet loss is effective to further improve the\nperformance of those compact models. However, it normally employs a fixed\nmargin to all the samples, which neglects the informative similarity structures\nbetween different identities. In this paper, we propose an enhanced version of\ntriplet loss, named triplet distillation, which exploits the capability of a\nteacher model to transfer the similarity information to a small model by\nadaptively varying the margin between positive and negative pairs. Experiments\non LFW, AgeDB, and CPLFW datasets show the merits of our method compared to the\noriginal triplet loss.\n", "versions": [{"version": "v1", "created": "Sat, 11 May 2019 06:23:51 GMT"}, {"version": "v2", "created": "Sun, 19 May 2019 14:13:37 GMT"}], "update_date": "2019-05-21", "authors_parsed": [["Feng", "Yushu", ""], ["Wang", "Huan", ""], ["Yi", "Daniel T.", ""], ["Hu", "Roland", ""]]}, {"id": "1905.04467", "submitter": "Alfred Sch\\\"ottl", "authors": "Fabian Truetsch, Alfred Sch\\\"ottl", "title": "Monocular Depth Estimation with Directional Consistency by Deep Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As processing power has become more available, more human-like artificial\nintelligences are created to solve image processing tasks that we are\ninherently good at. As such we propose a model that estimates depth from a\nmonocular image. Our approach utilizes a combination of structure from motion\nand stereo disparity. We estimate a pose between the source image and a\ndifferent viewpoint and a dense depth map and use a simple transformation to\nreconstruct the image seen from said viewpoint. We can then use the real image\nat that viewpoint to act as supervision to train out model. The metric chosen\nfor image comparison employs standard L1 and structural similarity and a\nconsistency constraint between depth maps as well as smoothness constraint. We\nshow that similar to human perception utilizing the correlation within the\nprovided data by two different approaches increases the accuracy and\noutperforms the individual components.\n", "versions": [{"version": "v1", "created": "Sat, 11 May 2019 07:13:16 GMT"}], "update_date": "2019-05-14", "authors_parsed": [["Truetsch", "Fabian", ""], ["Sch\u00f6ttl", "Alfred", ""]]}, {"id": "1905.04509", "submitter": "Jongheon Jeong", "authors": "Jongheon Jeong and Jinwoo Shin", "title": "Training CNNs with Selective Allocation of Channels", "comments": "15 pages; Accepted to ICML 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent progress in deep convolutional neural networks (CNNs) have enabled a\nsimple paradigm of architecture design: larger models typically achieve better\naccuracy. Due to this, in modern CNN architectures, it becomes more important\nto design models that generalize well under certain resource constraints, e.g.\nthe number of parameters. In this paper, we propose a simple way to improve the\ncapacity of any CNN model having large-scale features, without adding more\nparameters. In particular, we modify a standard convolutional layer to have a\nnew functionality of channel-selectivity, so that the layer is trained to\nselect important channels to re-distribute their parameters. Our experimental\nresults under various CNN architectures and datasets demonstrate that the\nproposed new convolutional layer allows new optima that generalize better via\nefficient resource utilization, compared to the baseline.\n", "versions": [{"version": "v1", "created": "Sat, 11 May 2019 12:00:55 GMT"}], "update_date": "2019-05-14", "authors_parsed": [["Jeong", "Jongheon", ""], ["Shin", "Jinwoo", ""]]}, {"id": "1905.04510", "submitter": "Yao Xie", "authors": "Yao Xie and Peng Xu and Zhanyu Ma", "title": "Deep Zero-Shot Learning for Scene Sketch", "comments": "5 pages, 3 figures, IEEE International Conference on Image Processing\n  (ICIP)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a novel problem of scene sketch zero-shot learning (SSZSL),\nwhich is a challenging task, since (i) different from photo, the gap between\ncommon semantic domain (e.g., word vector) and sketch is too huge to exploit\ncommon semantic knowledge as the bridge for knowledge transfer, and (ii)\ncompared with single-object sketch, more expressive feature representation for\nscene sketch is required to accommodate its high-level of abstraction and\ncomplexity. To overcome these challenges, we propose a deep embedding model for\nscene sketch zero-shot learning. In particular, we propose the augmented\nsemantic vector to conduct domain alignment by fusing multi-modal semantic\nknowledge (e.g., cartoon image, natural image, text description), and adopt\nattention-based network for scene sketch feature learning. Moreover, we propose\na novel distance metric to improve the similarity measure during testing.\nExtensive experiments and ablation studies demonstrate the benefit of our\nsketch-specific design.\n", "versions": [{"version": "v1", "created": "Sat, 11 May 2019 12:07:16 GMT"}], "update_date": "2019-05-14", "authors_parsed": [["Xie", "Yao", ""], ["Xu", "Peng", ""], ["Ma", "Zhanyu", ""]]}, {"id": "1905.04511", "submitter": "Ayyappa Pambala", "authors": "Ayyappa Kumar Pambala, Titir Dutta, Soma Biswas", "title": "Unified Generator-Classifier for Efficient Zero-Shot Learning", "comments": "4 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Generative models have achieved state-of-the-art performance for the\nzero-shot learning problem, but they require re-training the classifier every\ntime a new object category is encountered. The traditional semantic embedding\napproaches, though very elegant, usually do not perform at par with their\ngenerative counterparts. In this work, we propose an unified framework termed\nGenClass, which integrates the generator with the classifier for efficient\nzero-shot learning, thus combining the representative power of the generative\napproaches and the elegance of the embedding approaches. End-to-end training of\nthe unified framework not only eliminates the requirement of additional\nclassifier for new object categories as in the generative approaches, but also\nfacilitates the generation of more discriminative and useful features.\nExtensive evaluation on three standard zero-shot object classification\ndatasets, namely AWA, CUB and SUN shows the effectiveness of the proposed\napproach. The approach without any modification, also gives state-of-the-art\nperformance for zero-shot action classification, thus showing its\ngeneralizability to other domains.\n", "versions": [{"version": "v1", "created": "Sat, 11 May 2019 12:11:42 GMT"}], "update_date": "2019-05-14", "authors_parsed": [["Pambala", "Ayyappa Kumar", ""], ["Dutta", "Titir", ""], ["Biswas", "Soma", ""]]}, {"id": "1905.04523", "submitter": "Devraj Mandal", "authors": "Supritam Bhattacharjee, Devraj Mandal, Soma Biswas", "title": "Multi-class Novelty Detection Using Mix-up Technique", "comments": "Updated version has been accepted in WACV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Multi-class novelty detection is increasingly becoming an important area of\nresearch due to the continuous increase in the number of object categories. It\ntries to answer the pertinent question: given a test sample, should we even try\nto classify it? We propose a novel solution using the concept of mixup\ntechnique for novelty detection, termed as Segregation Network. During\ntraining, a pair of examples are selected from the training data and an\ninterpolated data point using their convex combination is constructed. We\ndevelop a suitable loss function to train our model to predict its constituent\nclasses. During testing, each input query is combined with the known class\nprototypes to generate mixed samples which are then passed through the trained\nnetwork. Our model which is trained to reveal the constituent classes can then\nbe used to determine whether the sample is novel or not. The intuition is that\nif a query comes from a known class and is mixed with the set of known class\nprototypes, then the prediction of the trained model for the correct class\nshould be high. In contrast, for a query from a novel class, the predictions\nfor all the known classes should be low. The proposed model is trained using\nonly the available known class data and does not need access to any auxiliary\ndataset or attributes. Extensive experiments on two benchmark datasets, namely\nCaltech 256 and Stanford Dogs and comparisons with the state-of-the-art\nalgorithms justifies the usefulness of our approach.\n", "versions": [{"version": "v1", "created": "Sat, 11 May 2019 13:10:39 GMT"}, {"version": "v2", "created": "Tue, 15 Oct 2019 04:29:58 GMT"}, {"version": "v3", "created": "Wed, 1 Jan 2020 07:30:10 GMT"}], "update_date": "2020-01-03", "authors_parsed": [["Bhattacharjee", "Supritam", ""], ["Mandal", "Devraj", ""], ["Biswas", "Soma", ""]]}, {"id": "1905.04525", "submitter": "Zhixiang Wang", "authors": "Zelong Zeng, Zhixiang Wang, Zheng Wang, Yinqiang Zheng, Yung-Yu\n  Chuang, Shin'ichi Satoh", "title": "Illumination-Adaptive Person Re-identification", "comments": "Accepted by TMM", "journal-ref": null, "doi": "10.1109/tmm.2020.2969782", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most person re-identification (ReID) approaches assume that person images are\ncaptured under relatively similar illumination conditions. In reality,\nlong-term person retrieval is common, and person images are often captured\nunder different illumination conditions at different times across a day. In\nthis situation, the performances of existing ReID models often degrade\ndramatically. This paper addresses the ReID problem with illumination\nvariations and names it as {\\em Illumination-Adaptive Person Re-identification\n(IA-ReID)}. We propose an Illumination-Identity Disentanglement (IID) network\nto dispel different scales of illuminations away while preserving individuals'\nidentity information. To demonstrate the illumination issue and to evaluate our\nmodel, we construct two large-scale simulated datasets with a wide range of\nillumination variations. Experimental results on the simulated datasets and\nreal-world images demonstrate the effectiveness of the proposed framework.\n", "versions": [{"version": "v1", "created": "Sat, 11 May 2019 13:28:25 GMT"}, {"version": "v2", "created": "Thu, 23 Apr 2020 13:31:19 GMT"}], "update_date": "2020-04-24", "authors_parsed": [["Zeng", "Zelong", ""], ["Wang", "Zhixiang", ""], ["Wang", "Zheng", ""], ["Zheng", "Yinqiang", ""], ["Chuang", "Yung-Yu", ""], ["Satoh", "Shin'ichi", ""]]}, {"id": "1905.04535", "submitter": "Shengjie Liu", "authors": "Shengjie Liu, and Qian Shi", "title": "Multitask Deep Learning with Spectral Knowledge for Hyperspectral Image\n  Classification", "comments": "Accepted by IEEE GRSL", "journal-ref": null, "doi": "10.1109/LGRS.2019.2962768", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this letter, we propose a multitask deep learning method for\nclassification of multiple hyperspectral data in a single training. Deep\nlearning models have achieved promising results on hyperspectral image\nclassification, but their performance highly rely on sufficient labeled\nsamples, which are scarce on hyperspectral images. However, samples from\nmultiple data sets might be sufficient to train one deep learning model,\nthereby improving its performance. To do so, we trained an identical feature\nextractor for all data, and the extracted features were fed into corresponding\nSoftmax classifiers. Spectral knowledge was introduced to ensure that the\nshared features were similar across domains. Four hyperspectral data sets were\nused in the experiments. We achieved higher classification accuracies on three\ndata sets (Pavia University, Pavia Center, and Indian Pines) and competitive\nresults on the Salinas Valley data compared with the baseline. Spectral\nknowledge was useful to prevent the deep network from overfitting when the data\nshared similar spectral response. The proposed method tested on two deep CNNs\nsuccessfully shows its ability to utilize samples from multiple data sets and\nenhance networks' performance.\n", "versions": [{"version": "v1", "created": "Sat, 11 May 2019 14:50:34 GMT"}, {"version": "v2", "created": "Wed, 3 Jul 2019 17:27:08 GMT"}, {"version": "v3", "created": "Tue, 9 Jul 2019 15:26:38 GMT"}, {"version": "v4", "created": "Thu, 26 Dec 2019 00:22:12 GMT"}], "update_date": "2020-12-30", "authors_parsed": [["Liu", "Shengjie", ""], ["Shi", "Qian", ""]]}, {"id": "1905.04538", "submitter": "Wayne Wu", "authors": "Wayne Wu, Kaidi Cao, Cheng Li, Chen Qian, Chen Change Loy", "title": "Disentangling Content and Style via Unsupervised Geometry Distillation", "comments": "Accepted to ICLR 2019 Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is challenging to disentangle an object into two orthogonal spaces of\ncontent and style since each can influence the visual observation differently\nand unpredictably. It is rare for one to have access to a large number of data\nto help separate the influences. In this paper, we present a novel framework to\nlearn this disentangled representation in a completely unsupervised manner. We\naddress this problem in a two-branch Autoencoder framework. For the structural\ncontent branch, we project the latent factor into a soft structured point\ntensor and constrain it with losses derived from prior knowledge. This\nconstraint encourages the branch to distill geometry information. Another\nbranch learns the complementary style information. The two branches form an\neffective framework that can disentangle object's content-style representation\nwithout any human annotation. We evaluate our approach on four image datasets,\non which we demonstrate the superior disentanglement and visual analogy quality\nboth in synthesized and real-world data. We are able to generate\nphoto-realistic images with 256*256 resolution that are clearly disentangled in\ncontent and style.\n", "versions": [{"version": "v1", "created": "Sat, 11 May 2019 15:12:52 GMT"}], "update_date": "2019-05-14", "authors_parsed": [["Wu", "Wayne", ""], ["Cao", "Kaidi", ""], ["Li", "Cheng", ""], ["Qian", "Chen", ""], ["Loy", "Chen Change", ""]]}, {"id": "1905.04571", "submitter": "Siheng Chen", "authors": "Siheng Chen and Chaojing Duan and Yaoqing Yang and Duanshun Li and\n  Chen Feng and Dong Tian", "title": "Deep Unsupervised Learning of 3D Point Clouds via Graph Topology\n  Inference and Filtering", "comments": "To appear in IEEE Transactions on Image Processing", "journal-ref": null, "doi": "10.1109/TIP.2019.2957935", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a deep autoencoder with graph topology inference and filtering to\nachieve compact representations of unorganized 3D point clouds in an\nunsupervised manner. Many previous works discretize 3D points to voxels and\nthen use lattice-based methods to process and learn 3D spatial information;\nhowever, this leads to inevitable discretization errors. In this work, we\nhandle raw 3D points without such compromise. The proposed networks follow the\nautoencoder framework with a focus on designing the decoder. The encoder adopts\nsimilar architectures as in PointNet. The decoder involves three novel modules.\nThe folding module folds a canonical 2D lattice to the underlying surface of a\n3D point cloud, achieving coarse reconstruction; the graph-topology-inference\nmodule learns a graph topology to represent pairwise relationships between 3D\npoints, pushing the latent code to preserve both coordinates and pairwise\nrelationships of points in 3D point clouds; and the graph-filtering module\ncouples the above two modules, refining the coarse reconstruction through a\nlearnt graph topology to obtain the final reconstruction. The proposed decoder\nleverages a learnable graph topology to push the codeword to preserve\nrepresentative features and further improve the unsupervised-learning\nperformance. We further provide theoretical analyses of the proposed\narchitecture. In the experiments, we validate the proposed networks in three\ntasks, including 3D point cloud reconstruction, visualization, and transfer\nclassification. The experimental results show that (1) the proposed networks\noutperform the state-of-the-art methods in various tasks; (2) a graph topology\ncan be inferred as auxiliary information without specific supervision on graph\ntopology inference; and (3) graph filtering refines the reconstruction, leading\nto better performances.\n", "versions": [{"version": "v1", "created": "Sat, 11 May 2019 18:25:58 GMT"}, {"version": "v2", "created": "Mon, 25 Nov 2019 05:20:46 GMT"}], "update_date": "2020-02-19", "authors_parsed": [["Chen", "Siheng", ""], ["Duan", "Chaojing", ""], ["Yang", "Yaoqing", ""], ["Li", "Duanshun", ""], ["Feng", "Chen", ""], ["Tian", "Dong", ""]]}, {"id": "1905.04598", "submitter": "Hongru Zhu", "authors": "Hongru Zhu, Peng Tang, Jeongho Park, Soojin Park, Alan Yuille", "title": "Robustness of Object Recognition under Extreme Occlusion in Humans and\n  Computational Models", "comments": "To be presented at the 41st Annual Meeting of the Cognitive Science\n  Society", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most objects in the visual world are partially occluded, but humans can\nrecognize them without difficulty. However, it remains unknown whether object\nrecognition models like convolutional neural networks (CNNs) can handle\nreal-world occlusion. It is also a question whether efforts to make these\nmodels robust to constant mask occlusion are effective for real-world\nocclusion. We test both humans and the above-mentioned computational models in\na challenging task of object recognition under extreme occlusion, where target\nobjects are heavily occluded by irrelevant real objects in real backgrounds.\nOur results show that human vision is very robust to extreme occlusion while\nCNNs are not, even with modifications to handle constant mask occlusion. This\nimplies that the ability to handle constant mask occlusion does not entail\nrobustness to real-world occlusion. As a comparison, we propose another\ncomputational model that utilizes object parts/subparts in a compositional\nmanner to build robustness to occlusion. This performs significantly better\nthan CNN-based models on our task with error patterns similar to humans. These\nfindings suggest that testing under extreme occlusion can better reveal the\nrobustness of visual recognition, and that the principle of composition can\nencourage such robustness.\n", "versions": [{"version": "v1", "created": "Sat, 11 May 2019 22:01:04 GMT"}, {"version": "v2", "created": "Tue, 4 Jun 2019 06:56:11 GMT"}], "update_date": "2019-06-05", "authors_parsed": [["Zhu", "Hongru", ""], ["Tang", "Peng", ""], ["Park", "Jeongho", ""], ["Park", "Soojin", ""], ["Yuille", "Alan", ""]]}, {"id": "1905.04621", "submitter": "Zilong Zhong", "authors": "Zilong Zhong, Jonathan Li, David A. Clausi, Alexander Wong", "title": "Generative Adversarial Networks and Conditional Random Fields for\n  Hyperspectral Image Classification", "comments": "Accepted by IEEE T-CYB", "journal-ref": null, "doi": "10.1109/TCYB.2019.2915094", "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we address the hyperspectral image (HSI) classification task\nwith a generative adversarial network and conditional random field (GAN-CRF)\n-based framework, which integrates a semi-supervised deep learning and a\nprobabilistic graphical model, and make three contributions. First, we design\nfour types of convolutional and transposed convolutional layers that consider\nthe characteristics of HSIs to help with extracting discriminative features\nfrom limited numbers of labeled HSI samples. Second, we construct\nsemi-supervised GANs to alleviate the shortage of training samples by adding\nlabels to them and implicitly reconstructing real HSI data distribution through\nadversarial training. Third, we build dense conditional random fields (CRFs) on\ntop of the random variables that are initialized to the softmax predictions of\nthe trained GANs and are conditioned on HSIs to refine classification maps.\nThis semi-supervised framework leverages the merits of discriminative and\ngenerative models through a game-theoretical approach. Moreover, even though we\nused very small numbers of labeled training HSI samples from the two most\nchallenging and extensively studied datasets, the experimental results\ndemonstrated that spectral-spatial GAN-CRF (SS-GAN-CRF) models achieved\ntop-ranking accuracy for semi-supervised HSI classification.\n", "versions": [{"version": "v1", "created": "Sun, 12 May 2019 01:13:35 GMT"}], "update_date": "2019-05-14", "authors_parsed": [["Zhong", "Zilong", ""], ["Li", "Jonathan", ""], ["Clausi", "David A.", ""], ["Wong", "Alexander", ""]]}, {"id": "1905.04641", "submitter": "Danlu Chen", "authors": "Danlu Chen, Xu-Yao Zhang, Wei Zhang, Yao Lu, Xiuli Li, Tao Mei", "title": "Predictive Ensemble Learning with Application to Scene Text Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning based approaches have achieved significant progresses in\ndifferent tasks like classification, detection, segmentation, and so on.\nEnsemble learning is widely known to further improve performance by combining\nmultiple complementary models. It is easy to apply ensemble learning for\nclassification tasks, for example, based on averaging, voting, or other\nmethods. However, for other tasks (like object detection) where the outputs are\nvarying in quantity and unable to be simply compared, the ensemble of multiple\nmodels become difficult. In this paper, we propose a new method called\nPredictive Ensemble Learning (PEL), based on powerful predictive ability of\ndeep neural networks, to directly predict the best performing model among a\npool of base models for each test example, thus transforming ensemble learning\nto a traditional classification task. Taking scene text detection as the\napplication, where no suitable ensemble learning strategy exists, PEL can\nsignificantly improve the performance, compared to either individual\nstate-of-the-art models, or the fusion of multiple models by non-maximum\nsuppression. Experimental results show the possibility and potential of PEL in\npredicting different models' performance based only on a query example, which\ncan be extended for ensemble learning in many other complex tasks.\n", "versions": [{"version": "v1", "created": "Sun, 12 May 2019 03:49:29 GMT"}, {"version": "v2", "created": "Thu, 16 May 2019 02:09:27 GMT"}], "update_date": "2019-05-17", "authors_parsed": [["Chen", "Danlu", ""], ["Zhang", "Xu-Yao", ""], ["Zhang", "Wei", ""], ["Lu", "Yao", ""], ["Li", "Xiuli", ""], ["Mei", "Tao", ""]]}, {"id": "1905.04663", "submitter": "Daniel Worrall", "authors": "Nichita Diaconu, Daniel E Worrall", "title": "Learning to Convolve: A Generalized Weight-Tying Approach", "comments": "Accepted to ICML 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent work (Cohen & Welling, 2016) has shown that generalizations of\nconvolutions, based on group theory, provide powerful inductive biases for\nlearning. In these generalizations, filters are not only translated but can\nalso be rotated, flipped, etc. However, coming up with exact models of how to\nrotate a 3 x 3 filter on a square pixel-grid is difficult. In this paper, we\nlearn how to transform filters for use in the group convolution, focussing on\nroto-translation. For this, we learn a filter basis and all rotated versions of\nthat filter basis. Filters are then encoded by a set of rotation invariant\ncoefficients. To rotate a filter, we switch the basis. We demonstrate we can\nproduce feature maps with low sensitivity to input rotations, while achieving\nhigh performance on MNIST and CIFAR-10.\n", "versions": [{"version": "v1", "created": "Sun, 12 May 2019 07:55:59 GMT"}], "update_date": "2019-05-14", "authors_parsed": [["Diaconu", "Nichita", ""], ["Worrall", "Daniel E", ""]]}, {"id": "1905.04668", "submitter": "Mohammadreza Babaee", "authors": "Mohammadreza Babaee, David Full, Gerhard Rigoll", "title": "On Flow Profile Image for Video Representation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video representation is a key challenge in many computer vision applications\nsuch as video classification, video captioning, and video surveillance. In this\npaper, we propose a novel approach for video representation that captures\nmeaningful information including motion and appearance from a sequence of video\nframes and compacts it into a single image. To this end, we compute the optical\nflow and use it in a least squares optimization to find a new image, the\nso-called Flow Profile Image (FPI). This image encodes motions as well as\nforeground appearance information while background information is removed. The\nquality of this image is validated in activity recognition experiments and the\nresults are compared with other video representation techniques such as dynamic\nimages [1] and eigen images [2]. The experimental results as well as visual\nquality confirm that FPIs can be successfully used in video processing\napplications.\n", "versions": [{"version": "v1", "created": "Sun, 12 May 2019 08:48:06 GMT"}], "update_date": "2019-05-14", "authors_parsed": [["Babaee", "Mohammadreza", ""], ["Full", "David", ""], ["Rigoll", "Gerhard", ""]]}, {"id": "1905.04693", "submitter": "Fangneng Zhan", "authors": "Fangneng Zhan, Jiaxing Huang and Shijian Lu", "title": "Hierarchy Composition GAN for High-fidelity Image Synthesis", "comments": "11 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the rapid progress of generative adversarial networks (GANs) in image\nsynthesis in recent years, the existing image synthesis approaches work in\neither geometry domain or appearance domain alone which often introduces\nvarious synthesis artifacts. This paper presents an innovative Hierarchical\nComposition GAN (HIC-GAN) that incorporates image synthesis in geometry and\nappearance domains into an end-to-end trainable network and achieves superior\nsynthesis realism in both domains simultaneously. We design an innovative\nhierarchical composition mechanism that is capable of learning realistic\ncomposition geometry and handling occlusions while multiple foreground objects\nare involved in image composition. In addition, we introduce a novel attention\nmask mechanism that guides to adapt the appearance of foreground objects which\nalso helps to provide better training reference for learning in geometry\ndomain. Extensive experiments on scene text image synthesis, portrait editing\nand indoor rendering tasks show that the proposed HIC-GAN achieves superior\nsynthesis performance qualitatively and quantitatively.\n", "versions": [{"version": "v1", "created": "Sun, 12 May 2019 11:11:24 GMT"}, {"version": "v2", "created": "Tue, 14 May 2019 06:58:27 GMT"}, {"version": "v3", "created": "Sat, 9 Jan 2021 15:32:17 GMT"}], "update_date": "2021-01-12", "authors_parsed": [["Zhan", "Fangneng", ""], ["Huang", "Jiaxing", ""], ["Lu", "Shijian", ""]]}, {"id": "1905.04696", "submitter": "Junjun Jiang", "authors": "Junjun Jiang, Yi Yu, Zheng Wang, Suhua Tang, Ruimin Hu, and Jiayi Ma", "title": "Ensemble Super-Resolution with A Reference Dataset", "comments": "14 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  By developing sophisticated image priors or designing deep(er) architectures,\na variety of image Super-Resolution (SR) approaches have been proposed recently\nand achieved very promising performance. A natural question that arises is\nwhether these methods can be reformulated into a unifying framework and whether\nthis framework assists in SR reconstruction? In this paper, we present a simple\nbut effective single image SR method based on ensemble learning, which can\nproduce a better performance than that could be obtained from any of SR methods\nto be ensembled (or called component super-resolvers). Based on the assumption\nthat better component super-resolver should have larger ensemble weight when\nperforming SR reconstruction, we present a Maximum A Posteriori (MAP)\nestimation framework for the inference of optimal ensemble weights. Specially,\nwe introduce a reference dataset, which is composed of High-Resolution (HR) and\nLow-Resolution (LR) image pairs, to measure the super-resolution abilities\n(prior knowledge) of different component super-resolvers. To obtain the optimal\nensemble weights, we propose to incorporate the reconstruction constraint,\nwhich states that the degenerated HR image should be equal to the LR\nobservation one, as well as the prior knowledge of ensemble weights into the\nMAP estimation framework. Moreover, the proposed optimization problem can be\nsolved by an analytical solution. We study the performance of the proposed\nmethod by comparing with different competitive approaches, including four\nstate-of-the-art non-deep learning based methods, four latest deep learning\nbased methods and one ensemble learning based method, and prove its\neffectiveness and superiority on three public datasets.\n", "versions": [{"version": "v1", "created": "Sun, 12 May 2019 11:22:18 GMT"}], "update_date": "2019-05-14", "authors_parsed": [["Jiang", "Junjun", ""], ["Yu", "Yi", ""], ["Wang", "Zheng", ""], ["Tang", "Suhua", ""], ["Hu", "Ruimin", ""], ["Ma", "Jiayi", ""]]}, {"id": "1905.04711", "submitter": "Boyuan Ma", "authors": "Boyuan Ma, Xiaoyan Wei, Chuni Liu, Xiaojuan Ban, Haiyou Huang, Hao\n  Wang, Weihua Xue, Stephen Wu, Mingfei Gao, Qing Shen, Adnan Omer Abuassba,\n  Haokai Shen, Yanjing Su", "title": "Data augmentation in microscopic images for material data mining", "comments": "17 pages, technical report", "journal-ref": "npj computational materials 2020", "doi": "10.1038/s41524-020-00392-6", "report-no": null, "categories": "cond-mat.mtrl-sci cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent progress in material data mining has been driven by high-capacity\nmodels trained on large datasets. However, collecting experimental data (real\ndata) has been extremely costly since the amount of human effort and expertise\nrequired. Here, we develop a novel transfer learning strategy to address small\nor insufficient data problem. This strategy realizes the fusion of real and\nsimulated data, and the augmentation of training data in data mining procedure.\nFor a specific task of image segmentation, this strategy can generate synthetic\nimages by fusing physical mechanism of simulated images and \"image style\" of\nreal images. The result shows that the model trained with the acquired\nsynthetic images and 35% of the real images outperforms the model trained on\nall real images. As the time required to generate synthetic data is almost\nnegligible, this strategy is able to reduce the time cost of real data\npreparation by roughly 65%.\n", "versions": [{"version": "v1", "created": "Sun, 12 May 2019 12:37:57 GMT"}, {"version": "v2", "created": "Fri, 24 May 2019 09:54:37 GMT"}, {"version": "v3", "created": "Mon, 28 Oct 2019 04:12:43 GMT"}], "update_date": "2020-10-01", "authors_parsed": [["Ma", "Boyuan", ""], ["Wei", "Xiaoyan", ""], ["Liu", "Chuni", ""], ["Ban", "Xiaojuan", ""], ["Huang", "Haiyou", ""], ["Wang", "Hao", ""], ["Xue", "Weihua", ""], ["Wu", "Stephen", ""], ["Gao", "Mingfei", ""], ["Shen", "Qing", ""], ["Abuassba", "Adnan Omer", ""], ["Shen", "Haokai", ""], ["Su", "Yanjing", ""]]}, {"id": "1905.04717", "submitter": "Vahid Mirjalili Dr", "authors": "Arun Ross, Sudipta Banerjee, Cunjian Chen, Anurag Chowdhury, Vahid\n  Mirjalili, Renu Sharma, Thomas Swearingen, Shivangi Yadav", "title": "Some Research Problems in Biometrics: The Future Beckons", "comments": "8 pages, 12 figures, ICB-2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The need for reliably determining the identity of a person is critical in a\nnumber of different domains ranging from personal smartphones to border\nsecurity; from autonomous vehicles to e-voting; from tracking child\nvaccinations to preventing human trafficking; from crime scene investigation to\npersonalization of customer service. Biometrics, which entails the use of\nbiological attributes such as face, fingerprints and voice for recognizing a\nperson, is being increasingly used in several such applications. While\nbiometric technology has made rapid strides over the past decade, there are\nseveral fundamental issues that are yet to be satisfactorily resolved. In this\narticle, we will discuss some of these issues and enumerate some of the\nexciting challenges in this field.\n", "versions": [{"version": "v1", "created": "Sun, 12 May 2019 13:06:17 GMT"}], "update_date": "2019-05-14", "authors_parsed": [["Ross", "Arun", ""], ["Banerjee", "Sudipta", ""], ["Chen", "Cunjian", ""], ["Chowdhury", "Anurag", ""], ["Mirjalili", "Vahid", ""], ["Sharma", "Renu", ""], ["Swearingen", "Thomas", ""], ["Yadav", "Shivangi", ""]]}, {"id": "1905.04729", "submitter": "Ziqiang Zheng", "authors": "Ziqiang Zheng, Zhibin Yu, Haiyong Zheng, Yang Yang, Heng Tao Shen", "title": "One-Shot Image-to-Image Translation via Part-Global Learning with a\n  Multi-adversarial Framework", "comments": "9 pages, 13 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is well known that humans can learn and recognize objects effectively from\nseveral limited image samples. However, learning from just a few images is\nstill a tremendous challenge for existing main-stream deep neural networks.\nInspired by analogical reasoning in the human mind, a feasible strategy is to\ntranslate the abundant images of a rich source domain to enrich the relevant\nyet different target domain with insufficient image data. To achieve this goal,\nwe propose a novel, effective multi-adversarial framework (MA) based on\npart-global learning, which accomplishes one-shot cross-domain image-to-image\ntranslation. In specific, we first devise a part-global adversarial training\nscheme to provide an efficient way for feature extraction and prevent\ndiscriminators being over-fitted. Then, a multi-adversarial mechanism is\nemployed to enhance the image-to-image translation ability to unearth the\nhigh-level semantic representation. Moreover, a balanced adversarial loss\nfunction is presented, which aims to balance the training data and stabilize\nthe training process. Extensive experiments demonstrate that the proposed\napproach can obtain impressive results on various datasets between two\nextremely imbalanced image domains and outperform state-of-the-art methods on\none-shot image-to-image translation.\n", "versions": [{"version": "v1", "created": "Sun, 12 May 2019 14:29:57 GMT"}], "update_date": "2019-05-14", "authors_parsed": [["Zheng", "Ziqiang", ""], ["Yu", "Zhibin", ""], ["Zheng", "Haiyong", ""], ["Yang", "Yang", ""], ["Shen", "Heng Tao", ""]]}, {"id": "1905.04730", "submitter": "Thomas M\\\"ollenhoff", "authors": "Thomas M\\\"ollenhoff, Daniel Cremers", "title": "Flat Metric Minimization with Applications in Generative Modeling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We take the novel perspective to view data not as a probability distribution\nbut rather as a current. Primarily studied in the field of geometric measure\ntheory, $k$-currents are continuous linear functionals acting on compactly\nsupported smooth differential forms and can be understood as a generalized\nnotion of oriented $k$-dimensional manifold. By moving from distributions\n(which are $0$-currents) to $k$-currents, we can explicitly orient the data by\nattaching a $k$-dimensional tangent plane to each sample point. Based on the\nflat metric which is a fundamental distance between currents, we derive\nFlatGAN, a formulation in the spirit of generative adversarial networks but\ngeneralized to $k$-currents. In our theoretical contribution we prove that the\nflat metric between a parametrized current and a reference current is Lipschitz\ncontinuous in the parameters. In experiments, we show that the proposed shift\nto $k>0$ leads to interpretable and disentangled latent representations which\nbehave equivariantly to the specified oriented tangent planes.\n", "versions": [{"version": "v1", "created": "Sun, 12 May 2019 14:37:07 GMT"}], "update_date": "2019-05-14", "authors_parsed": [["M\u00f6llenhoff", "Thomas", ""], ["Cremers", "Daniel", ""]]}, {"id": "1905.04734", "submitter": "Emanuel S\\'anchez Aimar", "authors": "Emanuel Sanchez Aimar, Petia Radeva, Mariella Dimiccoli", "title": "Social Relation Recognition in Egocentric Photostreams", "comments": "Accepted at ICIP 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes an approach to automatically categorize the social\ninteractions of a user wearing a photo-camera 2fpm, by relying solely on what\nthe camera is seeing. The problem is challenging due to the overwhelming\ncomplexity of social life and the extreme intra-class variability of social\ninteractions captured under unconstrained conditions. We adopt the\nformalization proposed in Bugental's social theory, that groups human relations\ninto five social domains with related categories. Our method is a new deep\nlearning architecture that exploits the hierarchical structure of the label\nspace and relies on a set of social attributes estimated at frame level to\nprovide a semantic representation of social interactions. Experimental results\non the new EgoSocialRelation dataset demonstrate the effectiveness of our\nproposal.\n", "versions": [{"version": "v1", "created": "Sun, 12 May 2019 15:20:26 GMT"}], "update_date": "2019-05-14", "authors_parsed": [["Aimar", "Emanuel Sanchez", ""], ["Radeva", "Petia", ""], ["Dimiccoli", "Mariella", ""]]}, {"id": "1905.04740", "submitter": "Shouyu Wang", "authors": "Shouyu Wang, Weitao Tang", "title": "Object Detection in Specific Traffic Scenes using YOLOv2", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  object detection framework plays crucial role in autonomous driving. In this\npaper, we introduce the real-time object detection framework called You Only\nLook Once (YOLOv1) and the related improvements of YOLOv2. We further explore\nthe capability of YOLOv2 by implementing its pre-trained model to do the object\ndetecting tasks in some specific traffic scenes. The four artificially designed\ntraffic scenes include single-car, single-person, frontperson-rearcar and\nfrontcar-rearperson.\n", "versions": [{"version": "v1", "created": "Sun, 12 May 2019 16:38:19 GMT"}], "update_date": "2019-05-14", "authors_parsed": [["Wang", "Shouyu", ""], ["Tang", "Weitao", ""]]}, {"id": "1905.04748", "submitter": "Xiaohan Ding", "authors": "Xiaohan Ding, Guiguang Ding, Yuchen Guo, Jungong Han, Chenggang Yan", "title": "Approximated Oracle Filter Pruning for Destructive CNN Width\n  Optimization", "comments": "ICML 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is not easy to design and run Convolutional Neural Networks (CNNs) due to:\n1) finding the optimal number of filters (i.e., the width) at each layer is\ntricky, given an architecture; and 2) the computational intensity of CNNs\nimpedes the deployment on computationally limited devices. Oracle Pruning is\ndesigned to remove the unimportant filters from a well-trained CNN, which\nestimates the filters' importance by ablating them in turn and evaluating the\nmodel, thus delivers high accuracy but suffers from intolerable time\ncomplexity, and requires a given resulting width but cannot automatically find\nit. To address these problems, we propose Approximated Oracle Filter Pruning\n(AOFP), which keeps searching for the least important filters in a binary\nsearch manner, makes pruning attempts by masking out filters randomly,\naccumulates the resulting errors, and finetunes the model via a multi-path\nframework. As AOFP enables simultaneous pruning on multiple layers, we can\nprune an existing very deep CNN with acceptable time cost, negligible accuracy\ndrop, and no heuristic knowledge, or re-design a model which exerts higher\naccuracy and faster inference.\n", "versions": [{"version": "v1", "created": "Sun, 12 May 2019 17:14:19 GMT"}], "update_date": "2019-05-14", "authors_parsed": [["Ding", "Xiaohan", ""], ["Ding", "Guiguang", ""], ["Guo", "Yuchen", ""], ["Han", "Jungong", ""], ["Yan", "Chenggang", ""]]}, {"id": "1905.04753", "submitter": "Mengtian Li", "authors": "Mengtian Li, Ersin Yumer and Deva Ramanan", "title": "Budgeted Training: Rethinking Deep Neural Network Training Under\n  Resource Constraints", "comments": "ICLR 2020. Project page with code is at\n  http://www.cs.cmu.edu/~mengtial/proj/budgetnn/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In most practical settings and theoretical analyses, one assumes that a model\ncan be trained until convergence. However, the growing complexity of machine\nlearning datasets and models may violate such assumptions. Indeed, current\napproaches for hyper-parameter tuning and neural architecture search tend to be\nlimited by practical resource constraints. Therefore, we introduce a formal\nsetting for studying training under the non-asymptotic, resource-constrained\nregime, i.e., budgeted training. We analyze the following problem: \"given a\ndataset, algorithm, and fixed resource budget, what is the best achievable\nperformance?\" We focus on the number of optimization iterations as the\nrepresentative resource. Under such a setting, we show that it is critical to\nadjust the learning rate schedule according to the given budget. Among\nbudget-aware learning schedules, we find simple linear decay to be both robust\nand high-performing. We support our claim through extensive experiments with\nstate-of-the-art models on ImageNet (image classification), Kinetics (video\nclassification), MS COCO (object detection and instance segmentation), and\nCityscapes (semantic segmentation). We also analyze our results and find that\nthe key to a good schedule is budgeted convergence, a phenomenon whereby the\ngradient vanishes at the end of each allowed budget. We also revisit existing\napproaches for fast convergence and show that budget-aware learning schedules\nreadily outperform such approaches under (the practical but under-explored)\nbudgeted training setting.\n", "versions": [{"version": "v1", "created": "Sun, 12 May 2019 17:49:49 GMT"}, {"version": "v2", "created": "Thu, 18 Jul 2019 20:49:09 GMT"}, {"version": "v3", "created": "Tue, 18 Feb 2020 21:06:30 GMT"}, {"version": "v4", "created": "Tue, 30 Jun 2020 00:45:59 GMT"}], "update_date": "2020-07-01", "authors_parsed": [["Li", "Mengtian", ""], ["Yumer", "Ersin", ""], ["Ramanan", "Deva", ""]]}, {"id": "1905.04757", "submitter": "Jun Liu", "authors": "Jun Liu, Amir Shahroudy, Mauricio Perez, Gang Wang, Ling-Yu Duan, Alex\n  C. Kot", "title": "NTU RGB+D 120: A Large-Scale Benchmark for 3D Human Activity\n  Understanding", "comments": "IEEE Transactions on Pattern Analysis and Machine Intelligence\n  (TPAMI)", "journal-ref": null, "doi": "10.1109/TPAMI.2019.2916873", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Research on depth-based human activity analysis achieved outstanding\nperformance and demonstrated the effectiveness of 3D representation for action\nrecognition. The existing depth-based and RGB+D-based action recognition\nbenchmarks have a number of limitations, including the lack of large-scale\ntraining samples, realistic number of distinct class categories, diversity in\ncamera views, varied environmental conditions, and variety of human subjects.\nIn this work, we introduce a large-scale dataset for RGB+D human action\nrecognition, which is collected from 106 distinct subjects and contains more\nthan 114 thousand video samples and 8 million frames. This dataset contains 120\ndifferent action classes including daily, mutual, and health-related\nactivities. We evaluate the performance of a series of existing 3D activity\nanalysis methods on this dataset, and show the advantage of applying deep\nlearning methods for 3D-based human action recognition. Furthermore, we\ninvestigate a novel one-shot 3D activity recognition problem on our dataset,\nand a simple yet effective Action-Part Semantic Relevance-aware (APSR)\nframework is proposed for this task, which yields promising results for\nrecognition of the novel action classes. We believe the introduction of this\nlarge-scale dataset will enable the community to apply, adapt, and develop\nvarious data-hungry learning techniques for depth-based and RGB+D-based human\nactivity understanding. [The dataset is available at:\nhttp://rose1.ntu.edu.sg/Datasets/actionRecognition.asp]\n", "versions": [{"version": "v1", "created": "Sun, 12 May 2019 17:58:55 GMT"}, {"version": "v2", "created": "Mon, 10 Jun 2019 07:04:29 GMT"}], "update_date": "2019-06-11", "authors_parsed": [["Liu", "Jun", ""], ["Shahroudy", "Amir", ""], ["Perez", "Mauricio", ""], ["Wang", "Gang", ""], ["Duan", "Ling-Yu", ""], ["Kot", "Alex C.", ""]]}, {"id": "1905.04789", "submitter": "Onorina Kovalenko", "authors": "Onorina Kovalenko and Vladislav Golyanik and Jameel Malik and Ahmed\n  Elhayek and Didier Stricker", "title": "Structure from Articulated Motion: Accurate and Stable Monocular 3D\n  Reconstruction without Training Data", "comments": "21 pages, 8 figures, 2 tables", "journal-ref": "Sensors 2019, 19(20), 4603", "doi": "10.3390/s19204603", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recovery of articulated 3D structure from 2D observations is a challenging\ncomputer vision problem with many applications. Current learning-based\napproaches achieve state-of-the-art accuracy on public benchmarks but are\nrestricted to specific types of objects and motions covered by the training\ndatasets. Model-based approaches do not rely on training data but show lower\naccuracy on these datasets. In this paper, we introduce a model-based method\ncalled Structure from Articulated Motion (SfAM), which can recover multiple\nobject and motion types without training on extensive data collections. At the\nsame time, it performs on par with learning-based state-of-the-art approaches\non public benchmarks and outperforms previous non-rigid structure from motion\n(NRSfM) methods. SfAM is built upon a general-purpose NRSfM technique while\nintegrating a soft spatio-temporal constraint on the bone lengths. We use\nalternating optimization strategy to recover optimal geometry (i.e., bone\nproportions) together with 3D joint positions by enforcing the bone lengths\nconsistency over a series of frames. SfAM is highly robust to noisy 2D\nannotations, generalizes to arbitrary objects and does not rely on training\ndata, which is shown in extensive experiments on public benchmarks and real\nvideo sequences. We believe that it brings a new perspective on the domain of\nmonocular 3D recovery of articulated structures, including human motion\ncapture.\n", "versions": [{"version": "v1", "created": "Sun, 12 May 2019 20:33:49 GMT"}, {"version": "v2", "created": "Tue, 12 Nov 2019 13:08:08 GMT"}], "update_date": "2019-11-13", "authors_parsed": [["Kovalenko", "Onorina", ""], ["Golyanik", "Vladislav", ""], ["Malik", "Jameel", ""], ["Elhayek", "Ahmed", ""], ["Stricker", "Didier", ""]]}, {"id": "1905.04791", "submitter": "Jun Zhang", "authors": "Jun Zhang, Tong Zheng, Shengping Zhang, Meng Wang", "title": "DeepIlluminance: Contextual Illuminance Estimation via Deep Neural\n  Networks", "comments": "12 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computational color constancy refers to the estimation of the scene\nillumination and makes the perceived color relatively stable under varying\nillumination. In the past few years, deep Convolutional Neural Networks (CNNs)\nhave delivered superior performance in illuminant estimation. Several\nrepresentative methods formulate it as a multi-label prediction problem by\nlearning the local appearance of image patches using CNNs. However, these\napproaches inevitably make incorrect estimations for the ambiguous patches\naffected by their neighborhood contexts. Inaccurate local estimates are likely\nto bring in degraded performance when combining into a global prediction. To\naddress the above issues, we propose a contextual deep network for patch-based\nilluminant estimation equipped with refinement. First, the contextual net with\na center-surround architecture extracts local contextual features from image\npatches, and generates initial illuminant estimates and the corresponding color\ncorrected patches. The patches are sampled based on the observation that pixels\nwith large color differences describe the illumination well. Then, the\nrefinement net integrates the input patches with the corrected patches in\nconjunction with the use of intermediate features to improve the performance.\nTo train such a network with numerous parameters, we propose a stage-wise\ntraining strategy, in which the features and the predicted illuminant from\nprevious stages are provided to the next learning stage with more finer\nestimates recovered. Experiments show that our approach obtains competitive\nperformance on two illuminant estimation benchmarks.\n", "versions": [{"version": "v1", "created": "Sun, 12 May 2019 20:43:59 GMT"}, {"version": "v2", "created": "Thu, 11 Jul 2019 07:34:03 GMT"}], "update_date": "2019-07-12", "authors_parsed": [["Zhang", "Jun", ""], ["Zheng", "Tong", ""], ["Zhang", "Shengping", ""], ["Wang", "Meng", ""]]}, {"id": "1905.04804", "submitter": "Linjie Yang", "authors": "Linjie Yang, Yuchen Fan, Ning Xu", "title": "Video Instance Segmentation", "comments": "Accepted to ICCV 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present a new computer vision task, named video instance\nsegmentation. The goal of this new task is simultaneous detection, segmentation\nand tracking of instances in videos. In words, it is the first time that the\nimage instance segmentation problem is extended to the video domain. To\nfacilitate research on this new task, we propose a large-scale benchmark called\nYouTube-VIS, which consists of 2883 high-resolution YouTube videos, a\n40-category label set and 131k high-quality instance masks. In addition, we\npropose a novel algorithm called MaskTrack R-CNN for this task. Our new method\nintroduces a new tracking branch to Mask R-CNN to jointly perform the\ndetection, segmentation and tracking tasks simultaneously. Finally, we evaluate\nthe proposed method and several strong baselines on our new dataset.\nExperimental results clearly demonstrate the advantages of the proposed\nalgorithm and reveal insight for future improvement. We believe the video\ninstance segmentation task will motivate the community along the line of\nresearch for video understanding.\n", "versions": [{"version": "v1", "created": "Sun, 12 May 2019 22:42:17 GMT"}, {"version": "v2", "created": "Sun, 2 Jun 2019 22:56:50 GMT"}, {"version": "v3", "created": "Thu, 8 Aug 2019 00:36:39 GMT"}, {"version": "v4", "created": "Fri, 16 Aug 2019 17:49:13 GMT"}], "update_date": "2019-08-19", "authors_parsed": [["Yang", "Linjie", ""], ["Fan", "Yuchen", ""], ["Xu", "Ning", ""]]}, {"id": "1905.04815", "submitter": "Vishwanath Saragadam Raja Venkata", "authors": "Vishwanath Saragadam and Aswin C. Sankaranarayanan", "title": "Programmable Spectrometry -- Per-pixel Classification of Materials using\n  Learned Spectral Filters", "comments": null, "journal-ref": null, "doi": "10.1109/ICCP48838.2020.9105281", "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many materials have distinct spectral profiles. This facilitates estimation\nof the material composition of a scene at each pixel by first acquiring its\nhyperspectral image, and subsequently filtering it using a bank of spectral\nprofiles. This process is inherently wasteful since only a set of linear\nprojections of the acquired measurements contribute to the classification task.\nWe propose a novel programmable camera that is capable of producing images of a\nscene with an arbitrary spectral filter. We use this camera to optically\nimplement the spectral filtering of the scene's hyperspectral image with the\nbank of spectral profiles needed to perform per-pixel material classification.\nThis provides gains both in terms of acquisition speed --- since only the\nrelevant measurements are acquired --- and in signal-to-noise ratio --- since\nwe invariably avoid narrowband filters that are light inefficient. Given\ntraining data, we use a range of classical and modern techniques including SVMs\nand neural networks to identify the bank of spectral profiles that facilitate\nmaterial classification. We verify the method in simulations on standard\ndatasets as well as real data using a lab prototype of the camera.\n", "versions": [{"version": "v1", "created": "Mon, 13 May 2019 00:20:31 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["Saragadam", "Vishwanath", ""], ["Sankaranarayanan", "Aswin C.", ""]]}, {"id": "1905.04828", "submitter": "Josh Harguess", "authors": "Chris M. Ward, Josh Harguess, Alexander G. Corelli", "title": "Leveraging synthetic imagery for collision-at-sea avoidance", "comments": null, "journal-ref": "Proc. SPIE 10645, Geospatial Informatics, Motion Imagery, and\n  Network Analytics VIII, 1064507 (4 May 2018)", "doi": "10.1117/12.2306113", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Maritime collisions involving multiple ships are considered rare, but in 2017\nseveral United States Navy vessels were involved in fatal at-sea collisions\nthat resulted in the death of seventeen American Servicemembers. The\nexperimentation introduced in this paper is a direct response to these\nincidents. We propose a shipboard Collision-At-Sea avoidance system, based on\nvideo image processing, that will help ensure the safe stationing and\nnavigation of maritime vessels. Our system leverages a convolutional neural\nnetwork trained on synthetic maritime imagery in order to detect nearby vessels\nwithin a scene, perform heading analysis of detected vessels, and provide an\nalert in the presence of an inbound vessel. Additionally, we present the\nNavigational Hazards - Synthetic (NAVHAZ-Synthetic) dataset. This dataset, is\ncomprised of one million annotated images of ten vessel classes observed from\nvirtual vessel-mounted cameras, as well as a human \"Topside Lookout\"\nperspective. NAVHAZ-Synthetic includes imagery displaying varying sea-states,\nlighting conditions, and optical degradations such as fog, sea-spray, and\nsalt-accumulation. We present our results on the use of synthetic imagery in a\ncomputer vision based collision-at-sea warning system with promising\nperformance.\n", "versions": [{"version": "v1", "created": "Mon, 13 May 2019 02:01:26 GMT"}], "update_date": "2019-05-14", "authors_parsed": [["Ward", "Chris M.", ""], ["Harguess", "Josh", ""], ["Corelli", "Alexander G.", ""]]}, {"id": "1905.04830", "submitter": "Yinglu Liu", "authors": "Yinglu Liu, Hailin Shi, Yue Si, Hao Shen, Xiaobo Wang, Tao Mei", "title": "A High-Efficiency Framework for Constructing Large-Scale Face Parsing\n  Benchmark", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Face parsing, which is to assign a semantic label to each pixel in face\nimages, has recently attracted increasing interest due to its huge application\npotentials. Although many face related fields (e.g., face recognition and face\ndetection) have been well studied for many years, the existing datasets for\nface parsing are still severely limited in terms of the scale and quality,\ne.g., the widely used Helen dataset only contains 2,330 images. This is mainly\nbecause pixel-level annotation is a high cost and time-consuming work,\nespecially for the facial parts without clear boundaries. The lack of accurate\nannotated datasets becomes a major obstacle in the progress of face parsing\ntask. It is a feasible way to utilize dense facial landmarks to guide the\nparsing annotation. However, annotating dense landmarks on human face\nencounters the same issues as the parsing annotation. To overcome the above\nproblems, in this paper, we develop a high-efficiency framework for face\nparsing annotation, which considerably simplifies and speeds up the parsing\nannotation by two consecutive modules. Benefit from the proposed framework, we\nconstruct a new Dense Landmark Guided Face Parsing (LaPa) benchmark. It\nconsists of 22,000 face images with large variations in expression, pose,\nocclusion, etc. Each image is provided with accurate annotation of a\n11-category pixel-level label map along with coordinates of 106-point\nlandmarks. To the best of our knowledge, it is currently the largest public\ndataset for face parsing. To make full use of our LaPa dataset with abundant\nface shape and boundary priors, we propose a simple yet effective\nBoundary-Sensitive Parsing Network (BSPNet). Our network is taken as a baseline\nmodel on the proposed LaPa dataset, and meanwhile, it achieves the\nstate-of-the-art performance on the Helen dataset without resorting to extra\nface alignment.\n", "versions": [{"version": "v1", "created": "Mon, 13 May 2019 02:17:56 GMT"}], "update_date": "2019-05-14", "authors_parsed": [["Liu", "Yinglu", ""], ["Shi", "Hailin", ""], ["Si", "Yue", ""], ["Shen", "Hao", ""], ["Wang", "Xiaobo", ""], ["Mei", "Tao", ""]]}, {"id": "1905.04835", "submitter": "Hossein K. Mousavi", "authors": "Hossein K. Mousavi, Mohammadreza Nazari, Martin Tak\\'a\\v{c}, Nader\n  Motee", "title": "Multi-Agent Image Classification via Reinforcement Learning", "comments": "Preprint of the paper to be published in IROS'19 proceedings", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.MA cs.RO cs.SY stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate a classification problem using multiple mobile agents capable\nof collecting (partial) pose-dependent observations of an unknown environment.\nThe objective is to classify an image over a finite time horizon. We propose a\nnetwork architecture on how agents should form a local belief, take local\nactions, and extract relevant features from their raw partial observations.\nAgents are allowed to exchange information with their neighboring agents to\nupdate their own beliefs. It is shown how reinforcement learning techniques can\nbe utilized to achieve decentralized implementation of the classification\nproblem by running a decentralized consensus protocol. Our experimental results\non the MNIST handwritten digit dataset demonstrates the effectiveness of our\nproposed framework.\n", "versions": [{"version": "v1", "created": "Mon, 13 May 2019 02:24:19 GMT"}, {"version": "v2", "created": "Tue, 6 Aug 2019 17:16:22 GMT"}], "update_date": "2019-08-07", "authors_parsed": [["Mousavi", "Hossein K.", ""], ["Nazari", "Mohammadreza", ""], ["Tak\u00e1\u010d", "Martin", ""], ["Motee", "Nader", ""]]}, {"id": "1905.04849", "submitter": "Cai Shaofeng", "authors": "Shaofeng Cai, Yao Shu, Wei Wang, Beng Chin Ooi", "title": "Dynamic Routing Networks", "comments": "10 pages, 3 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The deployment of deep neural networks in real-world applications is mostly\nrestricted by their high inference costs. Extensive efforts have been made to\nimprove the accuracy with expert-designed or algorithm-searched architectures.\nHowever, the incremental improvement is typically achieved with increasingly\nmore expensive models that only a small portion of input instances really need.\nInference with a static architecture that processes all input instances via the\nsame transformation would thus incur unnecessary computational costs.\nTherefore, customizing the model capacity in an instance-aware manner is much\nneeded for higher inference efficiency. In this paper, we propose Dynamic\nRouting Networks (DRNets), which support efficient instance-aware inference by\nrouting the input instance to only necessary transformation branches selected\nfrom a candidate set of branches for each connection between transformation\nnodes. The branch selection is dynamically determined via the corresponding\nbranch importance weights, which are first generated from lightweight\nhypernetworks (RouterNets) and then recalibrated with Gumbel-Softmax before the\nselection. Extensive experiments show that DRNets can reduce a substantial\namount of parameter size and FLOPs during inference with prediction performance\ncomparable to state-of-the-art architectures.\n", "versions": [{"version": "v1", "created": "Mon, 13 May 2019 03:45:42 GMT"}, {"version": "v2", "created": "Thu, 23 May 2019 16:45:18 GMT"}, {"version": "v3", "created": "Thu, 24 Oct 2019 04:47:36 GMT"}, {"version": "v4", "created": "Tue, 28 Jul 2020 16:26:29 GMT"}, {"version": "v5", "created": "Sun, 8 Nov 2020 13:11:45 GMT"}], "update_date": "2020-11-10", "authors_parsed": [["Cai", "Shaofeng", ""], ["Shu", "Yao", ""], ["Wang", "Wei", ""], ["Ooi", "Beng Chin", ""]]}, {"id": "1905.04854", "submitter": "Ziling Huang", "authors": "Ziling Huang, Zheng Wang, Chung-Chi Tsai, Shin'ichi Satoh, Chia-Wen\n  Lin", "title": "DotSCN: Group Re-identification via Domain-Transferred Single and Couple\n  Representation Learning", "comments": "accepted in IEEE Transctions on Circuits and Systems for Video\n  Technology", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Group re-identification (G-ReID) is an important yet less-studied task. Its\nchallenges not only lie in appearance changes of individuals which have been\nwell-investigated in general person re-identification (ReID), but also derive\nfrom group layout and membership changes. So the key task of G-ReID is to learn\nrepresentations robust to such changes. To address this issue, we propose a\nTransferred Single and Couple Representation Learning Network (TSCN). Its\nmerits are two aspects: 1) Due to the lack of labelled training samples,\nexisting G-ReID methods mainly rely on unsatisfactory hand-crafted features. To\ngain the superiority of deep learning models, we treat a group as multiple\npersons and transfer the domain of a labeled ReID dataset to a G-ReID target\ndataset style to learn single representations. 2) Taking into account the\nneighborhood relationship in a group, we further propose learning a novel\ncouple representation between two group members, that achieves more\ndiscriminative power in G-ReID tasks. In addition, an unsupervised weight\nlearning method is exploited to adaptively fuse the results of different views\ntogether according to result patterns. Extensive experimental results\ndemonstrate the effectiveness of our approach that significantly outperforms\nstate-of-the-art methods by 11.7\\% CMC-1 on the Road Group dataset and by\n39.0\\% CMC-1 on the DukeMCMT dataset.\n", "versions": [{"version": "v1", "created": "Mon, 13 May 2019 04:19:10 GMT"}, {"version": "v2", "created": "Tue, 13 Oct 2020 06:34:26 GMT"}], "update_date": "2020-10-14", "authors_parsed": [["Huang", "Ziling", ""], ["Wang", "Zheng", ""], ["Tsai", "Chung-Chi", ""], ["Satoh", "Shin'ichi", ""], ["Lin", "Chia-Wen", ""]]}, {"id": "1905.04877", "submitter": "Yangyang Guo", "authors": "Yangyang Guo and Zhiyong Cheng and Liqiang Nie and Yibing Liu and\n  Yinglong Wang and Mohan Kankanhalli", "title": "Quantifying and Alleviating the Language Prior Problem in Visual\n  Question Answering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Benefiting from the advancement of computer vision, natural language\nprocessing and information retrieval techniques, visual question answering\n(VQA), which aims to answer questions about an image or a video, has received\nlots of attentions over the past few years. Although some progress has been\nachieved so far, several studies have pointed out that current VQA models are\nheavily affected by the \\emph{language prior problem}, which means they tend to\nanswer questions based on the co-occurrence patterns of question keywords\n(e.g., how many) and answers (e.g., 2) instead of understanding images and\nquestions. Existing methods attempt to solve this problem by either balancing\nthe biased datasets or forcing models to better understand images. However,\nonly marginal effects and even performance deterioration are observed for the\nfirst and second solution, respectively. In addition, another important issue\nis the lack of measurement to quantitatively measure the extent of the language\nprior effect, which severely hinders the advancement of related techniques.\n  In this paper, we make contributions to solve the above problems from two\nperspectives. Firstly, we design a metric to quantitatively measure the\nlanguage prior effect of VQA models. The proposed metric has been demonstrated\nto be effective in our empirical studies. Secondly, we propose a regularization\nmethod (i.e., score regularization module) to enhance current VQA models by\nalleviating the language prior problem as well as boosting the backbone model\nperformance. The proposed score regularization module adopts a pair-wise\nlearning strategy, which makes the VQA models answer the question based on the\nreasoning of the image (upon this question) instead of basing on\nquestion-answer patterns observed in the biased training set. The score\nregularization module is flexible to be integrated into various VQA models.\n", "versions": [{"version": "v1", "created": "Mon, 13 May 2019 06:31:33 GMT"}], "update_date": "2019-05-14", "authors_parsed": [["Guo", "Yangyang", ""], ["Cheng", "Zhiyong", ""], ["Nie", "Liqiang", ""], ["Liu", "Yibing", ""], ["Wang", "Yinglong", ""], ["Kankanhalli", "Mohan", ""]]}, {"id": "1905.04890", "submitter": "Peng Gao", "authors": "Qi Ni, Fei Wang, Ziwei Zhao, Peng Gao", "title": "FPGA-based Binocular Image Feature Extraction and Matching System", "comments": "Accepted for the 4th International Conference on Multimedia Systems\n  and Signal Processing (ICMSSP 2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AR cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image feature extraction and matching is a fundamental but computation\nintensive task in machine vision. This paper proposes a novel FPGA-based\nembedded system to accelerate feature extraction and matching. It implements\nSURF feature point detection and BRIEF feature descriptor construction and\nmatching. For binocular stereo vision, feature matching includes both tracking\nmatching and stereo matching, which simultaneously provide feature point\ncorrespondences and parallax information. Our system is evaluated on a ZYNQ\nXC7Z045 FPGA. The result demonstrates that it can process binocular video data\nat a high frame rate (640$\\times$480 @ 162fps). Moreover, an extensive test\nproves our system has robustness for image compression, blurring and\nillumination.\n", "versions": [{"version": "v1", "created": "Mon, 13 May 2019 07:31:26 GMT"}, {"version": "v2", "created": "Tue, 14 May 2019 01:03:34 GMT"}], "update_date": "2019-05-15", "authors_parsed": [["Ni", "Qi", ""], ["Wang", "Fei", ""], ["Zhao", "Ziwei", ""], ["Gao", "Peng", ""]]}, {"id": "1905.04899", "submitter": "Sangdoo Yun", "authors": "Sangdoo Yun, Dongyoon Han, Seong Joon Oh, Sanghyuk Chun, Junsuk Choe,\n  Youngjoon Yoo", "title": "CutMix: Regularization Strategy to Train Strong Classifiers with\n  Localizable Features", "comments": "Accepted at ICCV 2019 (oral talk). 14 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Regional dropout strategies have been proposed to enhance the performance of\nconvolutional neural network classifiers. They have proved to be effective for\nguiding the model to attend on less discriminative parts of objects (e.g. leg\nas opposed to head of a person), thereby letting the network generalize better\nand have better object localization capabilities. On the other hand, current\nmethods for regional dropout remove informative pixels on training images by\noverlaying a patch of either black pixels or random noise. Such removal is not\ndesirable because it leads to information loss and inefficiency during\ntraining. We therefore propose the CutMix augmentation strategy: patches are\ncut and pasted among training images where the ground truth labels are also\nmixed proportionally to the area of the patches. By making efficient use of\ntraining pixels and retaining the regularization effect of regional dropout,\nCutMix consistently outperforms the state-of-the-art augmentation strategies on\nCIFAR and ImageNet classification tasks, as well as on the ImageNet\nweakly-supervised localization task. Moreover, unlike previous augmentation\nmethods, our CutMix-trained ImageNet classifier, when used as a pretrained\nmodel, results in consistent performance gains in Pascal detection and MS-COCO\nimage captioning benchmarks. We also show that CutMix improves the model\nrobustness against input corruptions and its out-of-distribution detection\nperformances. Source code and pretrained models are available at\nhttps://github.com/clovaai/CutMix-PyTorch .\n", "versions": [{"version": "v1", "created": "Mon, 13 May 2019 08:10:22 GMT"}, {"version": "v2", "created": "Wed, 7 Aug 2019 07:15:29 GMT"}], "update_date": "2019-08-11", "authors_parsed": [["Yun", "Sangdoo", ""], ["Han", "Dongyoon", ""], ["Oh", "Seong Joon", ""], ["Chun", "Sanghyuk", ""], ["Choe", "Junsuk", ""], ["Yoo", "Youngjoon", ""]]}, {"id": "1905.04912", "submitter": "Jianhao Jiao", "authors": "Jianhao Jiao, Yang Yu, Qinghai Liao, Haoyang Ye, Ming Liu", "title": "Automatic Calibration of Multiple 3D LiDARs in Urban Environments", "comments": "7 pages, 10 figures, submitted to IROS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multiple LiDARs have progressively emerged on autonomous vehicles for\nrendering a wide field of view and dense measurements. However, the lack of\nprecise calibration negatively affects their potential applications in\nlocalization and perception systems. In this paper, we propose a novel system\nthat enables automatic multi-LiDAR calibration without any calibration target,\nprior environmental information, and initial values of the extrinsic\nparameters. Our approach starts with a hand-eye calibration for automatic\ninitialization by aligning the estimated motions of each sensor. The resulting\nparameters are then refined with an appearance-based method by minimizing a\ncost function constructed from point-plane correspondences. Experimental\nresults on simulated and real-world data sets demonstrate the reliability and\naccuracy of our calibration approach. The proposed approach can calibrate a\nmulti-LiDAR system with the rotation and translation errors less than 0.04\n[rad] and 0.1 [m] respectively for a mobile platform.\n", "versions": [{"version": "v1", "created": "Mon, 13 May 2019 08:51:30 GMT"}], "update_date": "2019-05-14", "authors_parsed": [["Jiao", "Jianhao", ""], ["Yu", "Yang", ""], ["Liao", "Qinghai", ""], ["Ye", "Haoyang", ""], ["Liu", "Ming", ""]]}, {"id": "1905.04957", "submitter": "Hung-Yu Tseng", "authors": "Hung-Yu Tseng, Shalini De Mello, Jonathan Tremblay, Sifei Liu, Stan\n  Birchfield, Ming-Hsuan Yang, Jan Kautz", "title": "Few-Shot Viewpoint Estimation", "comments": "BMVC 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Viewpoint estimation for known categories of objects has been improved\nsignificantly thanks to deep networks and large datasets, but generalization to\nunknown categories is still very challenging. With an aim towards improving\nperformance on unknown categories, we introduce the problem of category-level\nfew-shot viewpoint estimation. We design a novel framework to successfully\ntrain viewpoint networks for new categories with few examples (10 or less). We\nformulate the problem as one of learning to estimate category-specific 3D\ncanonical shapes, their associated depth estimates, and semantic 2D keypoints.\nWe apply meta-learning to learn weights for our network that are amenable to\ncategory-specific few-shot fine-tuning. Furthermore, we design a flexible\nmeta-Siamese network that maximizes information sharing during meta-learning.\nThrough extensive experimentation on the ObjectNet3D and Pascal3D+ benchmark\ndatasets, we demonstrate that our framework, which we call MetaView,\nsignificantly outperforms fine-tuning the state-of-the-art models with few\nexamples, and that the specific architectural innovations of our method are\ncrucial to achieving good performance.\n", "versions": [{"version": "v1", "created": "Mon, 13 May 2019 10:54:28 GMT"}, {"version": "v2", "created": "Wed, 31 Jul 2019 22:10:01 GMT"}], "update_date": "2019-08-02", "authors_parsed": [["Tseng", "Hung-Yu", ""], ["De Mello", "Shalini", ""], ["Tremblay", "Jonathan", ""], ["Liu", "Sifei", ""], ["Birchfield", "Stan", ""], ["Yang", "Ming-Hsuan", ""], ["Kautz", "Jan", ""]]}, {"id": "1905.04967", "submitter": "Dushyant Mehta", "authors": "Dushyant Mehta, Kwang In Kim, Christian Theobalt", "title": "Implicit Filter Sparsification In Convolutional Neural Networks", "comments": "ODML-CDNNR 2019 (ICML'19 workshop) extended abstract of the CVPR 2019\n  paper \"On Implicit Filter Level Sparsity in Convolutional Neural Networks,\n  Mehta et al.\" (arXiv:1811.12495)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show implicit filter level sparsity manifests in convolutional neural\nnetworks (CNNs) which employ Batch Normalization and ReLU activation, and are\ntrained with adaptive gradient descent techniques and L2 regularization or\nweight decay. Through an extensive empirical study (Mehta et al., 2019) we\nhypothesize the mechanism behind the sparsification process, and find\nsurprising links to certain filter sparsification heuristics proposed in\nliterature. Emergence of, and the subsequent pruning of selective features is\nobserved to be one of the contributing mechanisms, leading to feature sparsity\nat par or better than certain explicit sparsification / pruning approaches. In\nthis workshop article we summarize our findings, and point out corollaries of\nselective-featurepenalization which could also be employed as heuristics for\nfilter pruning\n", "versions": [{"version": "v1", "created": "Mon, 13 May 2019 11:10:14 GMT"}], "update_date": "2019-05-14", "authors_parsed": [["Mehta", "Dushyant", ""], ["Kim", "Kwang In", ""], ["Theobalt", "Christian", ""]]}, {"id": "1905.05010", "submitter": "Oleksii Sidorov", "authors": "Oleksii Sidorov and Jon Yngve Hardeberg", "title": "Craquelure as a Graph: Application of Image Processing and Graph Neural\n  Networks to the Description of Fracture Patterns", "comments": "Published in ICCV 2019 Workshops", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Cracks on a painting is not a defect but an inimitable signature of an\nartwork which can be used for origin examination, aging monitoring, damage\nidentification, and even forgery detection. This work presents the development\nof a new methodology and corresponding toolbox for the extraction and\ncharacterization of information from an image of a craquelure pattern.\n  The proposed approach processes craquelure network as a graph. The graph\nrepresentation captures the network structure via mutual organization of\njunctions and fractures. Furthermore, it is invariant to any geometrical\ndistortions. At the same time, our tool extracts the properties of each node\nand edge individually, which allows to characterize the pattern statistically.\n  We illustrate benefits from the graph representation and statistical features\nindividually using novel Graph Neural Network and hand-crafted descriptors\ncorrespondingly. However, we also show that the best performance is achieved\nwhen both techniques are merged into one framework. We perform experiments on\nthe dataset for paintings' origin classification and demonstrate that our\napproach outperforms existing techniques by a large margin.\n", "versions": [{"version": "v1", "created": "Mon, 13 May 2019 12:39:01 GMT"}, {"version": "v2", "created": "Wed, 4 Dec 2019 06:44:09 GMT"}], "update_date": "2019-12-05", "authors_parsed": [["Sidorov", "Oleksii", ""], ["Hardeberg", "Jon Yngve", ""]]}, {"id": "1905.05037", "submitter": "Alexander Bihlo", "authors": "Alexander Bihlo", "title": "Precipitation nowcasting using a stochastic variational frame predictor\n  with learned prior distribution", "comments": "7 pages, 3 figures, release version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG physics.ao-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose the use of a stochastic variational frame prediction deep neural\nnetwork with a learned prior distribution trained on two-dimensional rain radar\nreflectivity maps for precipitation nowcasting with lead times of up to 2 1/2\nhours. We present a comparison to a standard convolutional LSTM network and\nassess the evolution of the structural similarity index for both methods. Case\nstudies are presented that illustrate that the novel methodology can yield\nmeaningful forecasts without excessive blur for the time horizons of interest.\n", "versions": [{"version": "v1", "created": "Mon, 13 May 2019 13:51:51 GMT"}], "update_date": "2019-05-14", "authors_parsed": [["Bihlo", "Alexander", ""]]}, {"id": "1905.05055", "submitter": "Zhengxia Zou", "authors": "Zhengxia Zou (1), Zhenwei Shi (2), Yuhong Guo (3 and 4), Jieping Ye (1\n  and 4) ((1) University of Michigan, (2) Beihang University, (3) Carleton\n  University, (4) DiDi Chuxing)", "title": "Object Detection in 20 Years: A Survey", "comments": "This work has been submitted to the IEEE TPAMI for possible\n  publication", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Object detection, as of one the most fundamental and challenging problems in\ncomputer vision, has received great attention in recent years. Its development\nin the past two decades can be regarded as an epitome of computer vision\nhistory. If we think of today's object detection as a technical aesthetics\nunder the power of deep learning, then turning back the clock 20 years we would\nwitness the wisdom of cold weapon era. This paper extensively reviews 400+\npapers of object detection in the light of its technical evolution, spanning\nover a quarter-century's time (from the 1990s to 2019). A number of topics have\nbeen covered in this paper, including the milestone detectors in history,\ndetection datasets, metrics, fundamental building blocks of the detection\nsystem, speed up techniques, and the recent state of the art detection methods.\nThis paper also reviews some important detection applications, such as\npedestrian detection, face detection, text detection, etc, and makes an in-deep\nanalysis of their challenges as well as technical improvements in recent years.\n", "versions": [{"version": "v1", "created": "Mon, 13 May 2019 14:26:50 GMT"}, {"version": "v2", "created": "Thu, 16 May 2019 00:52:27 GMT"}], "update_date": "2019-05-17", "authors_parsed": [["Zou", "Zhengxia", "", "3 and 4"], ["Shi", "Zhenwei", "", "3 and 4"], ["Guo", "Yuhong", "", "3 and 4"], ["Ye", "Jieping", "", "1\n  and 4"]]}, {"id": "1905.05084", "submitter": "Yuan Ma", "authors": "Kewen Liu, Yuan Ma, Hongxia Xiong, Zejun Yan, Zhijun Zhou, Panpan\n  Fang, Chaoyang Liu", "title": "Medical image super-resolution method based on dense blended attention\n  network", "comments": "12 pages, 4 figures, 32 references", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV physics.med-ph", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In order to address the issue that medical image would suffer from severe\nblurring caused by the lack of high-frequency details in the process of image\nsuper-resolution reconstruction, a novel medical image super-resolution method\nbased on dense neural network and blended attention mechanism is proposed. The\nproposed method adds blended attention blocks to dense neural\nnetwork(DenseNet), so that the neural network can concentrate more attention to\nthe regions and channels with sufficient high-frequency details. Batch\nnormalization layers are removed to avoid loss of high-frequency texture\ndetails. Final obtained high resolution medical image are obtained using\ndeconvolutional layers at the very end of the network as up-sampling operators.\nExperimental results show that the proposed method has an improvement of 0.05db\nto 11.25dB and 0.6% to 14.04% on the peak signal-to-noise ratio(PSNR) metric\nand structural similarity index(SSIM) metric, respectively, compared with the\nmainstream image super-resolution methods. This work provides a new idea for\ntheoretical studies of medical image super-resolution reconstruction.\n", "versions": [{"version": "v1", "created": "Mon, 13 May 2019 15:25:37 GMT"}], "update_date": "2019-05-14", "authors_parsed": [["Liu", "Kewen", ""], ["Ma", "Yuan", ""], ["Xiong", "Hongxia", ""], ["Yan", "Zejun", ""], ["Zhou", "Zhijun", ""], ["Fang", "Panpan", ""], ["Liu", "Chaoyang", ""]]}, {"id": "1905.05087", "submitter": "Zhiqiang Gong", "authors": "Zhiqiang Gong and Ping Zhong and Weidong Hu and Zixuan Xiao and Xuping\n  Yin", "title": "A novel statistical metric learning for hyperspectral image\n  classification", "comments": "Submitted to Whispers2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, a novel statistical metric learning is developed for\nspectral-spatial classification of the hyperspectral image. First, the standard\nvariance of the samples of each class in each batch is used to decrease the\nintra-class variance within each class. Then, the distances between the means\nof different classes are used to penalize the inter-class variance of the\ntraining samples. Finally, the standard variance between the means of different\nclasses is added as an additional diversity term to repulse different classes\nfrom each other. Experiments have conducted over two real-world hyperspectral\nimage datasets and the experimental results have shown the effectiveness of the\nproposed statistical metric learning.\n", "versions": [{"version": "v1", "created": "Mon, 13 May 2019 15:28:15 GMT"}], "update_date": "2019-05-14", "authors_parsed": [["Gong", "Zhiqiang", ""], ["Zhong", "Ping", ""], ["Hu", "Weidong", ""], ["Xiao", "Zixuan", ""], ["Yin", "Xuping", ""]]}, {"id": "1905.05091", "submitter": "Wenqing Chu", "authors": "Wenqing Chu, Wei-Chih Hung, Yi-Hsuan Tsai, Deng Cai, Ming-Hsuan Yang", "title": "Weakly-supervised Caricature Face Parsing through Domain Adaptation", "comments": "Accepted in ICIP 2019, code and model are available at\n  https://github.com/ZJULearning/CariFaceParsing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A caricature is an artistic form of a person's picture in which certain\nstriking characteristics are abstracted or exaggerated in order to create a\nhumor or sarcasm effect. For numerous caricature related applications such as\nattribute recognition and caricature editing, face parsing is an essential\npre-processing step that provides a complete facial structure understanding.\nHowever, current state-of-the-art face parsing methods require large amounts of\nlabeled data on the pixel-level and such process for caricature is tedious and\nlabor-intensive. For real photos, there are numerous labeled datasets for face\nparsing. Thus, we formulate caricature face parsing as a domain adaptation\nproblem, where real photos play the role of the source domain, adapting to the\ntarget caricatures. Specifically, we first leverage a spatial transformer based\nnetwork to enable shape domain shifts. A feed-forward style transfer network is\nthen utilized to capture texture-level domain gaps. With these two steps, we\nsynthesize face caricatures from real photos, and thus we can use parsing\nground truths of the original photos to learn the parsing model. Experimental\nresults on the synthetic and real caricatures demonstrate the effectiveness of\nthe proposed domain adaptation algorithm. Code is available at:\nhttps://github.com/ZJULearning/CariFaceParsing .\n", "versions": [{"version": "v1", "created": "Mon, 13 May 2019 15:36:41 GMT"}], "update_date": "2019-05-14", "authors_parsed": [["Chu", "Wenqing", ""], ["Hung", "Wei-Chih", ""], ["Tsai", "Yi-Hsuan", ""], ["Cai", "Deng", ""], ["Yang", "Ming-Hsuan", ""]]}, {"id": "1905.05092", "submitter": "Thibaud Ehret", "authors": "Thibaud Ehret, Axel Davy, Pablo Arias, Gabriele Facciolo", "title": "Joint Demosaicking and Denoising by Fine-Tuning of Bursts of Raw Images", "comments": "ICCV 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Demosaicking and denoising are the first steps of any camera image processing\npipeline and are key for obtaining high quality RGB images. A promising current\nresearch trend aims at solving these two problems jointly using convolutional\nneural networks. Due to the unavailability of ground truth data these networks\ncannot be currently trained using real RAW images. Instead, they resort to\nsimulated data. In this paper we present a method to learn demosaicking\ndirectly from mosaicked images, without requiring ground truth RGB data. We\napply this to learn joint demosaicking and denoising only from RAW images, thus\nenabling the use of real data. In addition we show that for this application\nfine-tuning a network to a specific burst improves the quality of restoration\nfor both demosaicking and denoising.\n", "versions": [{"version": "v1", "created": "Mon, 13 May 2019 15:36:50 GMT"}, {"version": "v2", "created": "Tue, 10 Sep 2019 09:21:54 GMT"}], "update_date": "2019-09-11", "authors_parsed": [["Ehret", "Thibaud", ""], ["Davy", "Axel", ""], ["Arias", "Pablo", ""], ["Facciolo", "Gabriele", ""]]}, {"id": "1905.05113", "submitter": "Ulugbek Kamilov", "authors": "Yu Sun, Jiaming Liu, and Ulugbek S. Kamilov", "title": "Block Coordinate Regularization by Denoising", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of estimating a vector from its noisy measurements\nusing a prior specified only through a denoising function. Recent work on\nplug-and-play priors (PnP) and regularization-by-denoising (RED) has shown the\nstate-of-the-art performance of estimators under such priors in a range of\nimaging tasks. In this work, we develop a new block coordinate RED algorithm\nthat decomposes a large-scale estimation problem into a sequence of updates\nover a small subset of the unknown variables. We theoretically analyze the\nconvergence of the algorithm and discuss its relationship to the traditional\nproximal optimization. Our analysis complements and extends recent theoretical\nresults for RED-based estimation methods. We numerically validate our method\nusing several denoiser priors, including those based on convolutional neural\nnetwork (CNN) denoisers.\n", "versions": [{"version": "v1", "created": "Mon, 13 May 2019 16:04:38 GMT"}, {"version": "v2", "created": "Thu, 19 Sep 2019 23:47:05 GMT"}], "update_date": "2019-09-23", "authors_parsed": [["Sun", "Yu", ""], ["Liu", "Jiaming", ""], ["Kamilov", "Ulugbek S.", ""]]}, {"id": "1905.05143", "submitter": "Noureldien Hussein", "authors": "Noureldien Hussein, Efstratios Gavves, Arnold W.M. Smeulders", "title": "VideoGraph: Recognizing Minutes-Long Human Activities in Videos", "comments": null, "journal-ref": "ICCV 2019, Workshop on Scene Graph Representation and Learning", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many human activities take minutes to unfold. To represent them, related\nworks opt for statistical pooling, which neglects the temporal structure.\nOthers opt for convolutional methods, as CNN and Non-Local. While successful in\nlearning temporal concepts, they are short of modeling minutes-long temporal\ndependencies. We propose VideoGraph, a method to achieve the best of two\nworlds: represent minutes-long human activities and learn their underlying\ntemporal structure. VideoGraph learns a graph-based representation for human\nactivities. The graph, its nodes and edges are learned entirely from video\ndatasets, making VideoGraph applicable to problems without node-level\nannotation. The result is improvements over related works on benchmarks:\nEpic-Kitchen and Breakfast. Besides, we demonstrate that VideoGraph is able to\nlearn the temporal structure of human activities in minutes-long videos.\n", "versions": [{"version": "v1", "created": "Mon, 13 May 2019 16:57:40 GMT"}, {"version": "v2", "created": "Sun, 13 Oct 2019 09:44:11 GMT"}], "update_date": "2019-10-15", "authors_parsed": [["Hussein", "Noureldien", ""], ["Gavves", "Efstratios", ""], ["Smeulders", "Arnold W. M.", ""]]}, {"id": "1905.05169", "submitter": "Xuaner (Cecilia) Zhang", "authors": "Xuaner Cecilia Zhang, Qifeng Chen, Ren Ng, Vladlen Koltun", "title": "Zoom To Learn, Learn To Zoom", "comments": "CVPR 2019,\n  https://ceciliavision.github.io/project-pages/project-zoom.html (paper,\n  video, supp, code, dataset)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper shows that when applying machine learning to digital zoom for\nphotography, it is beneficial to use real, RAW sensor data for training.\nExisting learning-based super-resolution methods do not use real sensor data,\ninstead operating on RGB images. In practice, these approaches result in loss\nof detail and accuracy in their digitally zoomed output when zooming in on\ndistant image regions. We also show that synthesizing sensor data by resampling\nhigh-resolution RGB images is an oversimplified approximation of real sensor\ndata and noise, resulting in worse image quality. The key barrier to using real\nsensor data for training is that ground truth high-resolution imagery is\nmissing. We show how to obtain the ground-truth data with optically zoomed\nimages and contribute a dataset, SR-RAW, for real-world computational zoom. We\nuse SR-RAW to train a deep network with a novel contextual bilateral loss\n(CoBi) that delivers critical robustness to mild misalignment in input-output\nimage pairs. The trained network achieves state-of-the-art performance in 4X\nand 8X computational zoom.\n", "versions": [{"version": "v1", "created": "Mon, 13 May 2019 17:56:15 GMT"}], "update_date": "2019-05-14", "authors_parsed": [["Zhang", "Xuaner Cecilia", ""], ["Chen", "Qifeng", ""], ["Ng", "Ren", ""], ["Koltun", "Vladlen", ""]]}, {"id": "1905.05172", "submitter": "Shunsuke Saito", "authors": "Shunsuke Saito, Zeng Huang, Ryota Natsume, Shigeo Morishima, Angjoo\n  Kanazawa, Hao Li", "title": "PIFu: Pixel-Aligned Implicit Function for High-Resolution Clothed Human\n  Digitization", "comments": "project page: https://shunsukesaito.github.io/PIFu", "journal-ref": "The IEEE International Conference on Computer Vision (ICCV), 2019,\n  pp. 2304-2314", "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce Pixel-aligned Implicit Function (PIFu), a highly effective\nimplicit representation that locally aligns pixels of 2D images with the global\ncontext of their corresponding 3D object. Using PIFu, we propose an end-to-end\ndeep learning method for digitizing highly detailed clothed humans that can\ninfer both 3D surface and texture from a single image, and optionally, multiple\ninput images. Highly intricate shapes, such as hairstyles, clothing, as well as\ntheir variations and deformations can be digitized in a unified way. Compared\nto existing representations used for 3D deep learning, PIFu can produce\nhigh-resolution surfaces including largely unseen regions such as the back of a\nperson. In particular, it is memory efficient unlike the voxel representation,\ncan handle arbitrary topology, and the resulting surface is spatially aligned\nwith the input image. Furthermore, while previous techniques are designed to\nprocess either a single image or multiple views, PIFu extends naturally to\narbitrary number of views. We demonstrate high-resolution and robust\nreconstructions on real world images from the DeepFashion dataset, which\ncontains a variety of challenging clothing types. Our method achieves\nstate-of-the-art performance on a public benchmark and outperforms the prior\nwork for clothed human digitization from a single image.\n", "versions": [{"version": "v1", "created": "Mon, 13 May 2019 17:59:56 GMT"}, {"version": "v2", "created": "Wed, 15 May 2019 21:51:04 GMT"}, {"version": "v3", "created": "Tue, 3 Dec 2019 19:00:16 GMT"}], "update_date": "2019-12-05", "authors_parsed": [["Saito", "Shunsuke", ""], ["Huang", "Zeng", ""], ["Natsume", "Ryota", ""], ["Morishima", "Shigeo", ""], ["Kanazawa", "Angjoo", ""], ["Li", "Hao", ""]]}, {"id": "1905.05186", "submitter": "Nupur Kumari", "authors": "Mayank Singh, Abhishek Sinha, Nupur Kumari, Harshitha Machiraju,\n  Balaji Krishnamurthy, Vineeth N Balasubramanian", "title": "Harnessing the Vulnerability of Latent Layers in Adversarially Trained\n  Models", "comments": "Accepted at IJCAI 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural networks are vulnerable to adversarial attacks -- small visually\nimperceptible crafted noise which when added to the input drastically changes\nthe output. The most effective method of defending against these adversarial\nattacks is to use the methodology of adversarial training. We analyze the\nadversarially trained robust models to study their vulnerability against\nadversarial attacks at the level of the latent layers. Our analysis reveals\nthat contrary to the input layer which is robust to adversarial attack, the\nlatent layer of these robust models are highly susceptible to adversarial\nperturbations of small magnitude. Leveraging this information, we introduce a\nnew technique Latent Adversarial Training (LAT) which comprises of fine-tuning\nthe adversarially trained models to ensure the robustness at the feature\nlayers. We also propose Latent Attack (LA), a novel algorithm for construction\nof adversarial examples. LAT results in minor improvement in test accuracy and\nleads to a state-of-the-art adversarial accuracy against the universal\nfirst-order adversarial PGD attack which is shown for the MNIST, CIFAR-10,\nCIFAR-100 datasets.\n", "versions": [{"version": "v1", "created": "Mon, 13 May 2019 16:44:03 GMT"}, {"version": "v2", "created": "Tue, 25 Jun 2019 19:38:57 GMT"}], "update_date": "2019-06-27", "authors_parsed": [["Singh", "Mayank", ""], ["Sinha", "Abhishek", ""], ["Kumari", "Nupur", ""], ["Machiraju", "Harshitha", ""], ["Krishnamurthy", "Balaji", ""], ["Balasubramanian", "Vineeth N", ""]]}, {"id": "1905.05212", "submitter": "Sara Elkerdawy", "authors": "Sara Elkerdawy, Hong Zhang, Nilanjan Ray", "title": "Lightweight Monocular Depth Estimation Model by Joint End-to-End Filter\n  pruning", "comments": null, "journal-ref": null, "doi": "10.1109/ICIP.2019.8803544", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional neural networks (CNNs) have emerged as the state-of-the-art in\nmultiple vision tasks including depth estimation. However, memory and computing\npower requirements remain as challenges to be tackled in these models.\nMonocular depth estimation has significant use in robotics and virtual reality\nthat requires deployment on low-end devices. Training a small model from\nscratch results in a significant drop in accuracy and it does not benefit from\npre-trained large models. Motivated by the literature of model pruning, we\npropose a lightweight monocular depth model obtained from a large trained\nmodel. This is achieved by removing the least important features with a novel\njoint end-to-end filter pruning. We propose to learn a binary mask for each\nfilter to decide whether to drop the filter or not. These masks are trained\njointly to exploit relations between filters at different layers as well as\nredundancy within the same layer. We show that we can achieve around 5x\ncompression rate with small drop in accuracy on the KITTI driving dataset. We\nalso show that masking can improve accuracy over the baseline with fewer\nparameters, even without enforcing compression loss.\n", "versions": [{"version": "v1", "created": "Mon, 13 May 2019 18:01:01 GMT"}], "update_date": "2020-05-19", "authors_parsed": [["Elkerdawy", "Sara", ""], ["Zhang", "Hong", ""], ["Ray", "Nilanjan", ""]]}, {"id": "1905.05241", "submitter": "Matias Valdenegro-Toro", "authors": "Matias Valdenegro-Toro", "title": "Deep Neural Networks for Marine Debris Detection in Sonar Images", "comments": "PhD Thesis submitted to Heriot-Watt University", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Garbage and waste disposal is one of the biggest challenges currently faced\nby mankind. Proper waste disposal and recycling is a must in any sustainable\ncommunity, and in many coastal areas there is significant water pollution in\nthe form of floating or submerged garbage. This is called marine debris.\nSubmerged marine debris threatens marine life, and for shallow coastal areas,\nit can also threaten fishing vessels [I\\~niguez et al. 2016, Renewable and\nSustainable Energy Reviews]. Submerged marine debris typically stays in the\nenvironment for a long time (20+ years), and consists of materials that can be\nrecycled, such as metals, plastics, glass, etc. Many of these items should not\nbe disposed in water bodies as this has a negative effect in the environment\nand human health. This thesis performs a comprehensive evaluation on the use of\nDNNs for the problem of marine debris detection in FLS images, as well as\nrelated problems such as image classification, matching, and detection\nproposals. We do this in a dataset of 2069 FLS images that we captured with an\nARIS Explorer 3000 sensor on marine debris objects lying in the floor of a\nsmall water tank. The objects we used to produce this dataset contain typical\nhousehold marine debris and distractor marine objects (tires, hooks, valves,\netc), divided in 10 classes plus a background class. Our results show that for\nthe evaluated tasks, DNNs are a superior technique than the corresponding state\nof the art. There are large gains particularly for the matching and detection\nproposal tasks. We also study the effect of sample complexity and object size\nin many tasks, which is valuable information for practitioners. We expect that\nour results will advance the objective of using Autonomous Underwater Vehicles\nto automatically survey, detect and collect marine debris from underwater\nenvironments.\n", "versions": [{"version": "v1", "created": "Mon, 13 May 2019 18:51:40 GMT"}], "update_date": "2019-05-15", "authors_parsed": [["Valdenegro-Toro", "Matias", ""]]}, {"id": "1905.05243", "submitter": "Hanxiang Hao", "authors": "Hanxiang Hao, David G\\\"uera, J\\'anos Horv\\'ath, Amy R. Reibman and\n  Edward J. Delp", "title": "Robustness Analysis of Face Obscuration", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Face obscuration is needed by law enforcement and mass media outlets to\nguarantee privacy. Sharing sensitive content where obscuration or redaction\ntechniques have failed to completely remove all identifiable traces can lead to\nmany legal and social issues. Hence, we need to be able to systematically\nmeasure the face obscuration performance of a given technique. In this paper we\npropose to measure the effectiveness of eight obscuration techniques. We do so\nby attacking the redacted faces in three scenarios: obscured face\nidentification, verification, and reconstruction. Threat modeling is also\nconsidered to provide a vulnerability analysis for each studied obscuration\ntechnique. Based on our evaluation, we show that the k-same based methods are\nthe most effective.\n", "versions": [{"version": "v1", "created": "Mon, 13 May 2019 19:03:35 GMT"}, {"version": "v2", "created": "Tue, 15 Oct 2019 20:30:48 GMT"}], "update_date": "2019-10-17", "authors_parsed": [["Hao", "Hanxiang", ""], ["G\u00fcera", "David", ""], ["Horv\u00e1th", "J\u00e1nos", ""], ["Reibman", "Amy R.", ""], ["Delp", "Edward J.", ""]]}, {"id": "1905.05265", "submitter": "Qi Chen", "authors": "Qi Chen, Sihai Tang, Qing Yang, Song Fu", "title": "Cooper: Cooperative Perception for Connected Autonomous Vehicles based\n  on 3D Point Clouds", "comments": "Accepted by the 39th IEEE International Conference on Distributed\n  Computing Systems (ICDCS 2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Autonomous vehicles may make wrong decisions due to inaccurate detection and\nrecognition. Therefore, an intelligent vehicle can combine its own data with\nthat of other vehicles to enhance perceptive ability, and thus improve\ndetection accuracy and driving safety. However, multi-vehicle cooperative\nperception requires the integration of real world scenes and the traffic of raw\nsensor data exchange far exceeds the bandwidth of existing vehicular networks.\nTo the best our knowledge, we are the first to conduct a study on raw-data\nlevel cooperative perception for enhancing the detection ability of\nself-driving systems. In this work, relying on LiDAR 3D point clouds, we fuse\nthe sensor data collected from different positions and angles of connected\nvehicles. A point cloud based 3D object detection method is proposed to work on\na diversity of aligned point clouds. Experimental results on KITTI and our\ncollected dataset show that the proposed system outperforms perception by\nextending sensing area, improving detection accuracy and promoting augmented\nresults. Most importantly, we demonstrate it is possible to transmit point\nclouds data for cooperative perception via existing vehicular network\ntechnologies.\n", "versions": [{"version": "v1", "created": "Mon, 13 May 2019 19:57:12 GMT"}], "update_date": "2019-05-15", "authors_parsed": [["Chen", "Qi", ""], ["Tang", "Sihai", ""], ["Yang", "Qing", ""], ["Fu", "Song", ""]]}, {"id": "1905.05300", "submitter": "Alexander Wong", "authors": "Rene Bidart and Alexander Wong", "title": "Affine Variational Autoencoders: An Efficient Approach for Improving\n  Generalization and Robustness to Distribution Shift", "comments": "6 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this study, we propose the Affine Variational Autoencoder (AVAE), a\nvariant of Variational Autoencoder (VAE) designed to improve robustness by\novercoming the inability of VAEs to generalize to distributional shifts in the\nform of affine perturbations. By optimizing an affine transform to maximize\nELBO, the proposed AVAE transforms an input to the training distribution\nwithout the need to increase model complexity to model the full distribution of\naffine transforms. In addition, we introduce a training procedure to create an\nefficient model by learning a subset of the training distribution, and using\nthe AVAE to improve generalization and robustness to distributional shift at\ntest time. Experiments on affine perturbations demonstrate that the proposed\nAVAE significantly improves generalization and robustness to distributional\nshift in the form of affine perturbations without an increase in model\ncomplexity.\n", "versions": [{"version": "v1", "created": "Mon, 13 May 2019 21:56:27 GMT"}], "update_date": "2019-05-15", "authors_parsed": [["Bidart", "Rene", ""], ["Wong", "Alexander", ""]]}, {"id": "1905.05336", "submitter": "Dian Sheng", "authors": "Dian Sheng, Yiheng Wei, Yuquan Chen, Yong Wang", "title": "Convolutional neural networks with fractional order gradient method", "comments": null, "journal-ref": null, "doi": "10.1016/j.neucom.2019.10.017", "report-no": null, "categories": "math.OC cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a fractional order gradient method for the backward\npropagation of convolutional neural networks. To overcome the problem that\nfractional order gradient method cannot converge to real extreme point, a\nsimplified fractional order gradient method is designed based on Caputo's\ndefinition. The parameters within layers are updated by the designed gradient\nmethod, but the propagations between layers still use integer order gradients,\nand thus the complicated derivatives of composite functions are avoided and the\nchain rule will be kept. By connecting every layers in series and adding loss\nfunctions, the proposed convolutional neural networks can be trained smoothly\naccording to various tasks. Some practical experiments are carried out in order\nto demonstrate fast convergence, high accuracy and ability to escape local\noptimal point at last.\n", "versions": [{"version": "v1", "created": "Tue, 14 May 2019 01:20:33 GMT"}, {"version": "v2", "created": "Mon, 16 Sep 2019 09:11:53 GMT"}], "update_date": "2020-01-07", "authors_parsed": [["Sheng", "Dian", ""], ["Wei", "Yiheng", ""], ["Chen", "Yuquan", ""], ["Wang", "Yong", ""]]}, {"id": "1905.05344", "submitter": "Pejman Habashi", "authors": "Pejman Habashi, Boubakeur Boufama, Imran Shafiq Ahmad", "title": "Disparity-Augmented Trajectories for Human Activity Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Numerous methods for human activity recognition have been proposed in the\npast two decades. Many of these methods are based on sparse representation,\nwhich describes the whole video content by a set of local features.\nTrajectories, being mid-level sparse features, are capable of describing the\nmotion of an interest-point in 2D space. 2D trajectories might be affected by\nviewpoint changes, potentially decreasing their accuracy. In this paper, we\ninitially propose and compare different 2D trajectory-based algorithms for\nhuman activity recognition. Moreover, we propose a new way of fusing disparity\ninformation with 2D trajectory information, without the calculation of 3D\nreconstruction. The obtained results show a 2.76\\% improvement when using\ndisparity-augmented trajectories, compared to using the classical 2D trajectory\ninformation only. Furthermore, we have also tested our method on the\nchallenging Hollywood 3D dataset, and we have obtained competitive results, at\na faster speed.\n", "versions": [{"version": "v1", "created": "Tue, 14 May 2019 02:06:40 GMT"}], "update_date": "2019-05-15", "authors_parsed": [["Habashi", "Pejman", ""], ["Boufama", "Boubakeur", ""], ["Ahmad", "Imran Shafiq", ""]]}, {"id": "1905.05350", "submitter": "Nachiket Deo", "authors": "Daniela A. Ridel, Nachiket Deo, Denis Wolf and Mohan M. Trivedi", "title": "Understanding Pedestrian-Vehicle Interactions with Vehicle Mounted\n  Vision: An LSTM Model and Empirical Analysis", "comments": "IV 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pedestrians and vehicles often share the road in complex inner city traffic.\nThis leads to interactions between the vehicle and pedestrians, with each\naffecting the other's motion. In order to create robust methods to reason about\npedestrian behavior and to design interfaces of communication between\nself-driving cars and pedestrians we need to better understand such\ninteractions. In this paper, we present a data-driven approach to implicitly\nmodel pedestrians' interactions with vehicles, to better predict pedestrian\nbehavior. We propose a LSTM model that takes as input the past trajectories of\nthe pedestrian and ego-vehicle, and pedestrian head orientation, and predicts\nthe future positions of the pedestrian. Our experiments based on a real-world,\ninner city dataset captured with vehicle mounted cameras, show that the usage\nof such cues improve pedestrian prediction when compared to a baseline that\npurely uses the past trajectory of the pedestrian.\n", "versions": [{"version": "v1", "created": "Tue, 14 May 2019 02:20:05 GMT"}], "update_date": "2019-05-15", "authors_parsed": [["Ridel", "Daniela A.", ""], ["Deo", "Nachiket", ""], ["Wolf", "Denis", ""], ["Trivedi", "Mohan M.", ""]]}, {"id": "1905.05352", "submitter": "Weirui Lu", "authors": "Weirui Lu, Xiaofen Xing, Bolun Cai, Xiangmin Xu", "title": "Listwise View Ranking for Image Cropping", "comments": null, "journal-ref": null, "doi": "10.1109/ACCESS.2019.2925430", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Rank-based Learning with deep neural network has been widely used for image\ncropping. However, the performance of ranking-based methods is often poor and\nthis is mainly due to two reasons: 1) image cropping is a listwise ranking task\nrather than pairwise comparison; 2) the rescaling caused by pooling layer and\nthe deformation in view generation damage the performance of composition\nlearning. In this paper, we develop a novel model to overcome these problems.\nTo address the first problem, we formulate the image cropping as a listwise\nranking problem to find the best view composition. For the second problem, a\nrefined view sampling (called RoIRefine) is proposed to extract refined feature\nmaps for candidate view generation. Given a series of candidate views, the\nproposed model learns the Top-1 probability distribution of views and picks up\nthe best one. By integrating refined sampling and listwise ranking, the\nproposed network called LVRN achieves the state-of-the-art performance both in\naccuracy and speed.\n", "versions": [{"version": "v1", "created": "Tue, 14 May 2019 02:21:59 GMT"}], "update_date": "2019-08-28", "authors_parsed": [["Lu", "Weirui", ""], ["Xing", "Xiaofen", ""], ["Cai", "Bolun", ""], ["Xu", "Xiangmin", ""]]}, {"id": "1905.05355", "submitter": "Dongdong Yu", "authors": "Dongdong Yu, Kai Su, Xin Geng, Changhu Wang", "title": "A Context-and-Spatial Aware Network for Multi-Person Pose Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-person pose estimation is a fundamental yet challenging task in\ncomputer vision. Both rich context information and spatial information are\nrequired to precisely locate the keypoints for all persons in an image. In this\npaper, a novel Context-and-Spatial Aware Network (CSANet), which integrates\nboth a Context Aware Path and Spatial Aware Path, is proposed to obtain\neffective features involving both context information and spatial information.\nSpecifically, we design a Context Aware Path with structure supervision\nstrategy and spatial pyramid pooling strategy to enhance the context\ninformation. Meanwhile, a Spatial Aware Path is proposed to preserve the\nspatial information, which also shortens the information propagation path from\nlow-level features to high-level features. On top of these two paths, we employ\na Heavy Head Path to further combine and enhance the features effectively.\nExperimentally, our proposed network outperforms state-of-the-art methods on\nthe COCO keypoint benchmark, which verifies the effectiveness of our method and\nfurther corroborates the above proposition.\n", "versions": [{"version": "v1", "created": "Tue, 14 May 2019 02:25:40 GMT"}], "update_date": "2019-05-15", "authors_parsed": [["Yu", "Dongdong", ""], ["Su", "Kai", ""], ["Geng", "Xin", ""], ["Wang", "Changhu", ""]]}, {"id": "1905.05373", "submitter": "Josh Harguess", "authors": "Chris M. Ward, Josh Harguess, Brendan Crabb, Shibin Parameswaran", "title": "Image quality assessment for determining efficacy and limitations of\n  Super-Resolution Convolutional Neural Network (SRCNN)", "comments": null, "journal-ref": "Proceedings Volume 10396, Applications of Digital Image Processing\n  XL; 1039605 (2017)", "doi": "10.1117/12.2275157", "report-no": null, "categories": "cs.CV eess.IV", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Traditional metrics for evaluating the efficacy of image processing\ntechniques do not lend themselves to understanding the capabilities and\nlimitations of modern image processing methods - particularly those enabled by\ndeep learning. When applying image processing in engineering solutions, a\nscientist or engineer has a need to justify their design decisions with clear\nmetrics. By applying blind/referenceless image spatial quality (BRISQUE),\nStructural SIMilarity (SSIM) index scores, and Peak signal-to-noise ratio\n(PSNR) to images before and after image processing, we can quantify quality\nimprovements in a meaningful way and determine the lowest recoverable image\nquality for a given method.\n", "versions": [{"version": "v1", "created": "Tue, 14 May 2019 03:19:40 GMT"}], "update_date": "2019-05-15", "authors_parsed": [["Ward", "Chris M.", ""], ["Harguess", "Josh", ""], ["Crabb", "Brendan", ""], ["Parameswaran", "Shibin", ""]]}, {"id": "1905.05375", "submitter": "Yu-Ding Lu", "authors": "Yu-Ding Lu, Hsin-Ying Lee, Hung-Yu Tseng, Ming-Hsuan Yang", "title": "Self-supervised Audio Spatialization with Correspondence Classifier", "comments": "ICIP 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spatial audio is an essential medium to audiences for 3D visual and auditory\nexperience. However, the recording devices and techniques are expensive or\ninaccessible to the general public. In this work, we propose a self-supervised\naudio spatialization network that can generate spatial audio given the\ncorresponding video and monaural audio. To enhance spatialization performance,\nwe use an auxiliary classifier to classify ground-truth videos and those with\naudio where the left and right channels are swapped. We collect a large-scale\nvideo dataset with spatial audio to validate the proposed method. Experimental\nresults demonstrate the effectiveness of the proposed model on the audio\nspatialization task.\n", "versions": [{"version": "v1", "created": "Tue, 14 May 2019 03:20:49 GMT"}], "update_date": "2019-05-15", "authors_parsed": [["Lu", "Yu-Ding", ""], ["Lee", "Hsin-Ying", ""], ["Tseng", "Hung-Yu", ""], ["Yang", "Ming-Hsuan", ""]]}, {"id": "1905.05377", "submitter": "Anh Duc Le Dr.", "authors": "Anh Duc Le, Tarin Clanuwat, Asanobu Kitamoto", "title": "A human-inspired recognition system for premodern Japanese historical\n  documents", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recognition of historical documents is a challenging problem due to the\nnoised, damaged characters and background. However, in Japanese historical\ndocuments, not only contains the mentioned problems, pre-modern Japanese\ncharacters were written in cursive and are connected. Therefore, character\nsegmentation based methods do not work well. This leads to the idea of creating\na new recognition system. In this paper, we propose a human-inspired document\nreading system to recognize multiple lines of premodern Japanese historical\ndocuments. During the reading, people employ eyes movement to determine the\nstart of a text line. Then, they move the eyes from the current character/word\nto the next character/word. They can also determine the end of a line or skip a\nfigure to move to the next line. The eyes movement integrates with visual\nprocessing to operate the reading process in the brain. We employ\nattention-based encoder-decoder to implement this recognition system. First,\nthe recognition system detects where to start a text line. Second, the system\nscans and recognize character by character until the text line is completed.\nThen, the system continues to detect the start of the next text line. This\nprocess is repeated until reading the whole document. We tested our\nhuman-inspired recognition system on the pre-modern Japanese historical\ndocument provide by the PRMU Kuzushiji competition. The results of the\nexperiments demonstrate the superiority and effectiveness of our proposed\nsystem by achieving Sequence Error Rate of 9.87% and 53.81% on level 2 and\nlevel 3 of the dataset, respectively. These results outperform to any other\nsystems participated in the PRMU Kuzushiji competition.\n", "versions": [{"version": "v1", "created": "Tue, 14 May 2019 03:26:25 GMT"}], "update_date": "2019-05-15", "authors_parsed": [["Le", "Anh Duc", ""], ["Clanuwat", "Tarin", ""], ["Kitamoto", "Asanobu", ""]]}, {"id": "1905.05381", "submitter": "Anh Duc Le Dr.", "authors": "Anh Duc Le, Hung Tuan Nguyen and Masaki Nakagawa", "title": "End to End Recognition System for Recognizing Offline Unconstrained\n  Vietnamese Handwriting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inspired by recent successes in neural machine translation and image caption\ngeneration, we present an attention based encoder decoder model (AED) to\nrecognize Vietnamese Handwritten Text. The model composes of two parts: a\nDenseNet for extracting invariant features, and a Long Short-Term Memory\nnetwork (LSTM) with an attention model incorporated for generating output text\n(LSTM decoder), which are connected from the CNN part to the attention model.\nThe input of the CNN part is a handwritten text image and the target of the\nLSTM decoder is the corresponding text of the input image. Our model is trained\nend-to-end to predict the text from a given input image since all the parts are\ndifferential components. In the experiment section, we evaluate our proposed\nAED model on the VNOnDB-Word and VNOnDB-Line datasets to verify its efficiency.\nThe experiential results show that our model achieves 12.30% of word error rate\nwithout using any language model. This result is competitive with the\nhandwriting recognition system provided by Google in the Vietnamese Online\nHandwritten Text Recognition competition.\n", "versions": [{"version": "v1", "created": "Tue, 14 May 2019 03:59:46 GMT"}], "update_date": "2019-05-15", "authors_parsed": [["Le", "Anh Duc", ""], ["Nguyen", "Hung Tuan", ""], ["Nakagawa", "Masaki", ""]]}, {"id": "1905.05382", "submitter": "Chuan-Xian Ren", "authors": "Chuan-Xian Ren, Bo-Hua Liang, Zhen Lei", "title": "Domain Adaptive Person Re-Identification via Camera Style Generation and\n  Label Propagation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unsupervised domain adaptation in person re-identification resorts to labeled\nsource data to promote the model training on target domain, facing the dilemmas\ncaused by large domain shift and large camera variations. The non-overlapping\nlabels challenge that source domain and target domain have entirely different\npersons further increases the re-identification difficulty. In this paper, we\npropose a novel algorithm to narrow such domain gaps. We derive a camera style\nadaptation framework to learn the style-based mappings between different camera\nviews, from the target domain to the source domain, and then we can transfer\nthe identity-based distribution from the source domain to the target domain on\nthe camera level. To overcome the non-overlapping labels challenge and guide\nthe person re-identification model to narrow the gap further, an efficient and\neffective soft-labeling method is proposed to mine the intrinsic local\nstructure of the target domain through building the connection between\nGAN-translated source domain and the target domain. Experiment results\nconducted on real benchmark datasets indicate that our method gets\nstate-of-the-art results.\n", "versions": [{"version": "v1", "created": "Tue, 14 May 2019 04:07:57 GMT"}], "update_date": "2019-05-15", "authors_parsed": [["Ren", "Chuan-Xian", ""], ["Liang", "Bo-Hua", ""], ["Lei", "Zhen", ""]]}, {"id": "1905.05393", "submitter": "Daniel Ho", "authors": "Daniel Ho, Eric Liang, Ion Stoica, Pieter Abbeel, Xi Chen", "title": "Population Based Augmentation: Efficient Learning of Augmentation Policy\n  Schedules", "comments": "ICML 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A key challenge in leveraging data augmentation for neural network training\nis choosing an effective augmentation policy from a large search space of\ncandidate operations. Properly chosen augmentation policies can lead to\nsignificant generalization improvements; however, state-of-the-art approaches\nsuch as AutoAugment are computationally infeasible to run for the ordinary\nuser. In this paper, we introduce a new data augmentation algorithm, Population\nBased Augmentation (PBA), which generates nonstationary augmentation policy\nschedules instead of a fixed augmentation policy. We show that PBA can match\nthe performance of AutoAugment on CIFAR-10, CIFAR-100, and SVHN, with three\norders of magnitude less overall compute. On CIFAR-10 we achieve a mean test\nerror of 1.46%, which is a slight improvement upon the current\nstate-of-the-art. The code for PBA is open source and is available at\nhttps://github.com/arcelien/pba.\n", "versions": [{"version": "v1", "created": "Tue, 14 May 2019 05:01:43 GMT"}], "update_date": "2019-05-15", "authors_parsed": [["Ho", "Daniel", ""], ["Liang", "Eric", ""], ["Stoica", "Ion", ""], ["Abbeel", "Pieter", ""], ["Chen", "Xi", ""]]}, {"id": "1905.05396", "submitter": "Taekyung Kim", "authors": "Taekyung Kim, Minki Jeong, Seunghyeon Kim, Seokeon Choi, Changick Kim", "title": "Diversify and Match: A Domain Adaptive Representation Learning Paradigm\n  for Object Detection", "comments": "Accepted to CVPR 2019. Source code will be uploaded soon", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a novel unsupervised domain adaptation approach for object\ndetection. We aim to alleviate the imperfect translation problem of pixel-level\nadaptations, and the source-biased discriminativity problem of feature-level\nadaptations simultaneously. Our approach is composed of two stages, i.e.,\nDomain Diversification (DD) and Multi-domain-invariant Representation Learning\n(MRL). At the DD stage, we diversify the distribution of the labeled data by\ngenerating various distinctive shifted domains from the source domain. At the\nMRL stage, we apply adversarial learning with a multi-domain discriminator to\nencourage feature to be indistinguishable among the domains. DD addresses the\nsource-biased discriminativity, while MRL mitigates the imperfect image\ntranslation. We construct a structured domain adaptation framework for our\nlearning paradigm and introduce a practical way of DD for implementation. Our\nmethod outperforms the state-of-the-art methods by a large margin of 3%~11% in\nterms of mean average precision (mAP) on various datasets.\n", "versions": [{"version": "v1", "created": "Tue, 14 May 2019 05:23:23 GMT"}], "update_date": "2019-05-15", "authors_parsed": [["Kim", "Taekyung", ""], ["Jeong", "Minki", ""], ["Kim", "Seunghyeon", ""], ["Choi", "Seokeon", ""], ["Kim", "Changick", ""]]}, {"id": "1905.05404", "submitter": "Yinglong Wang", "authors": "Yinglong Wang and Dong Gong and Jie Yang and Qinfeng Shi and Anton van\n  den Hengel and Dehua Xie and Bing Zeng", "title": "An Effective Two-Branch Model-Based Deep Network for Single Image\n  Deraining", "comments": "10 pages, 9 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Removing rain effects from an image is of importance for various applications\nsuch as autonomous driving, drone piloting, and photo editing. Conventional\nmethods rely on some heuristics to handcraft various priors to remove or\nseparate the rain effects from an image. Recent deep learning models are\nproposed to learn end-to-end methods to complete this task. However, they often\nfail to obtain satisfactory results in many realistic scenarios, especially\nwhen the observed images suffer from heavy rain. Heavy rain brings not only\nrain streaks but also haze-like effect caused by the accumulation of tiny\nraindrops. Different from the existing deep learning deraining methods that\nmainly focus on handling the rain streaks, we design a deep neural network by\nincorporating a physical raining image model. Specifically, in the proposed\nmodel, two branches are designed to handle both the rain streaks and haze-like\neffects. An additional submodule is jointly trained to finally refine the\nresults, which give the model flexibility to control the strength of removing\nthe mist. Extensive experiments on several datasets show that our method\noutperforms the state-of-the-art in both objective assessments and visual\nquality.\n", "versions": [{"version": "v1", "created": "Tue, 14 May 2019 06:04:39 GMT"}, {"version": "v2", "created": "Tue, 17 Sep 2019 12:27:06 GMT"}], "update_date": "2019-09-18", "authors_parsed": [["Wang", "Yinglong", ""], ["Gong", "Dong", ""], ["Yang", "Jie", ""], ["Shi", "Qinfeng", ""], ["Hengel", "Anton van den", ""], ["Xie", "Dehua", ""], ["Zeng", "Bing", ""]]}, {"id": "1905.05406", "submitter": "Ernest Ryu", "authors": "Ernest K. Ryu and Jialin Liu and Sicheng Wang and Xiaohan Chen and\n  Zhangyang Wang and Wotao Yin", "title": "Plug-and-Play Methods Provably Converge with Properly Trained Denoisers", "comments": "Published in the International Conference on Machine Learning, 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Plug-and-play (PnP) is a non-convex framework that integrates modern\ndenoising priors, such as BM3D or deep learning-based denoisers, into ADMM or\nother proximal algorithms. An advantage of PnP is that one can use pre-trained\ndenoisers when there is not sufficient data for end-to-end training. Although\nPnP has been recently studied extensively with great empirical success,\ntheoretical analysis addressing even the most basic question of convergence has\nbeen insufficient. In this paper, we theoretically establish convergence of\nPnP-FBS and PnP-ADMM, without using diminishing stepsizes, under a certain\nLipschitz condition on the denoisers. We then propose real spectral\nnormalization, a technique for training deep learning-based denoisers to\nsatisfy the proposed Lipschitz condition. Finally, we present experimental\nresults validating the theory.\n", "versions": [{"version": "v1", "created": "Tue, 14 May 2019 06:12:42 GMT"}], "update_date": "2019-05-15", "authors_parsed": [["Ryu", "Ernest K.", ""], ["Liu", "Jialin", ""], ["Wang", "Sicheng", ""], ["Chen", "Xiaohan", ""], ["Wang", "Zhangyang", ""], ["Yin", "Wotao", ""]]}, {"id": "1905.05416", "submitter": "Hao Tang", "authors": "Hao Tang, Wei Wang, Songsong Wu, Xinya Chen, Dan Xu, Nicu Sebe, Yan\n  Yan", "title": "Expression Conditional GAN for Facial Expression-to-Expression\n  Translation", "comments": "5 pages, 5 figures, accepted to ICIP 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we focus on the facial expression translation task and propose\na novel Expression Conditional GAN (ECGAN) which can learn the mapping from one\nimage domain to another one based on an additional expression attribute. The\nproposed ECGAN is a generic framework and is applicable to different expression\ngeneration tasks where specific facial expression can be easily controlled by\nthe conditional attribute label. Besides, we introduce a novel face mask loss\nto reduce the influence of background changing. Moreover, we propose an entire\nframework for facial expression generation and recognition in the wild, which\nconsists of two modules, i.e., generation and recognition. Finally, we evaluate\nour framework on several public face datasets in which the subjects have\ndifferent races, illumination, occlusion, pose, color, content and background\nconditions. Even though these datasets are very diverse, both the qualitative\nand quantitative results demonstrate that our approach is able to generate\nfacial expressions accurately and robustly.\n", "versions": [{"version": "v1", "created": "Tue, 14 May 2019 06:52:03 GMT"}], "update_date": "2019-05-15", "authors_parsed": [["Tang", "Hao", ""], ["Wang", "Wei", ""], ["Wu", "Songsong", ""], ["Chen", "Xinya", ""], ["Xu", "Dan", ""], ["Sebe", "Nicu", ""], ["Yan", "Yan", ""]]}, {"id": "1905.05420", "submitter": "Cagatay Odabasi", "authors": "Cagatay Odabasi and Jewel Jose", "title": "Towards a Skeleton-Based Action Recognition For Realistic Scenarios", "comments": null, "journal-ref": null, "doi": "10.13140/RG.2.2.23016.52485", "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding human actions is a crucial problem for service robots. However,\nthe general trend in Action Recognition is developing and testing these systems\non structured datasets. That's why this work presents a practical\nSkeleton-based Action Recognition framework which can be used in realistic\nscenarios. Our results show that although non-augmented and non-normalized data\nmay yield comparable results on the test split of the dataset, it is far from\nbeing useful on another dataset which is a manually collected data.\n", "versions": [{"version": "v1", "created": "Tue, 14 May 2019 07:05:18 GMT"}], "update_date": "2019-05-15", "authors_parsed": [["Odabasi", "Cagatay", ""], ["Jose", "Jewel", ""]]}, {"id": "1905.05425", "submitter": "Ruiqi Cheng", "authors": "Ruiqi Cheng, Kaiwei Wang, Shufei Lin, Weijian Hu, Kailun Yang, Xiao\n  Huang, Huabing Li, Dongming Sun, Jian Bai", "title": "Panoramic Annular Localizer: Tackling the Variation Challenges of\n  Outdoor Localization Using Panoramic Annular Images and Active Deep\n  Descriptors", "comments": "Accepted by ITSC 2019", "journal-ref": null, "doi": "10.1109/ITSC.2019.8917508", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual localization is an attractive problem that estimates the camera\nlocalization from database images based on the query image. It is a crucial\ntask for various applications, such as autonomous vehicles, assistive\nnavigation and augmented reality. The challenging issues of the task lie in\nvarious appearance variations between query and database images, including\nillumination variations, dynamic object variations and viewpoint variations. In\norder to tackle those challenges, Panoramic Annular Localizer into which\npanoramic annular lens and robust deep image descriptors are incorporated is\nproposed in this paper. The panoramic annular images captured by the single\ncamera are processed and fed into the NetVLAD network to form the active deep\ndescriptor, and sequential matching is utilized to generate the localization\nresult. The experiments carried on the public datasets and in the field\nillustrate the validation of the proposed system.\n", "versions": [{"version": "v1", "created": "Tue, 14 May 2019 07:26:48 GMT"}, {"version": "v2", "created": "Mon, 15 Jul 2019 16:24:41 GMT"}], "update_date": "2019-12-04", "authors_parsed": [["Cheng", "Ruiqi", ""], ["Wang", "Kaiwei", ""], ["Lin", "Shufei", ""], ["Hu", "Weijian", ""], ["Yang", "Kailun", ""], ["Huang", "Xiao", ""], ["Li", "Huabing", ""], ["Sun", "Dongming", ""], ["Bai", "Jian", ""]]}, {"id": "1905.05441", "submitter": "Julien Rabin", "authors": "Lo\\\"ic Simon, Ryan Webster and Julien Rabin", "title": "Revisiting Precision and Recall Definition for Generative Model\n  Evaluation", "comments": "ICML 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article we revisit the definition of Precision-Recall (PR) curves for\ngenerative models proposed by Sajjadi et al. (arXiv:1806.00035). Rather than\nproviding a scalar for generative quality, PR curves distinguish mode-collapse\n(poor recall) and bad quality (poor precision). We first generalize their\nformulation to arbitrary measures, hence removing any restriction to finite\nsupport. We also expose a bridge between PR curves and type I and type II error\nrates of likelihood ratio classifiers on the task of discriminating between\nsamples of the two distributions. Building upon this new perspective, we\npropose a novel algorithm to approximate precision-recall curves, that shares\nsome interesting methodological properties with the hypothesis testing\ntechnique from Lopez-Paz et al (arXiv:1610.06545). We demonstrate the interest\nof the proposed formulation over the original approach on controlled\nmulti-modal datasets.\n", "versions": [{"version": "v1", "created": "Tue, 14 May 2019 08:15:35 GMT"}], "update_date": "2019-05-15", "authors_parsed": [["Simon", "Lo\u00efc", ""], ["Webster", "Ryan", ""], ["Rabin", "Julien", ""]]}, {"id": "1905.05442", "submitter": "Lin-Zhuo Chen", "authors": "Lin-Zhuo Chen, Xuan-Yi Li, Deng-Ping Fan, Kai Wang, Shao-Ping Lu,\n  Ming-Ming Cheng", "title": "LSANet: Feature Learning on Point Sets by Local Spatial Aware Layer", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Directly learning features from the point cloud has become an active research\ndirection in 3D understanding. Existing learning-based methods usually\nconstruct local regions from the point cloud and extract the corresponding\nfeatures. However, most of these processes do not adequately take the spatial\ndistribution of the point cloud into account, limiting the ability to perceive\nfine-grained patterns. We design a novel Local Spatial Aware (LSA) layer, which\ncan learn to generate Spatial Distribution Weights (SDWs) hierarchically based\non the spatial relationship in local region for spatial independent operations,\nto establish the relationship between these operations and spatial\ndistribution, thus capturing the local geometric structure sensitively.We\nfurther propose the LSANet, which is based on LSA layer, aggregating the\nspatial information with associated features in each layer of the network\nbetter in network design.The experiments show that our LSANet can achieve on\npar or better performance than the state-of-the-art methods when evaluating on\nthe challenging benchmark datasets. For example, our LSANet can achieve 93.2%\naccuracy on ModelNet40 dataset using only 1024 points, significantly higher\nthan other methods under the same conditions. The source code is available at\nhttps://github.com/LinZhuoChen/LSANet.\n", "versions": [{"version": "v1", "created": "Tue, 14 May 2019 08:17:50 GMT"}, {"version": "v2", "created": "Wed, 19 Jun 2019 14:21:02 GMT"}, {"version": "v3", "created": "Thu, 20 Jun 2019 12:07:08 GMT"}], "update_date": "2019-06-21", "authors_parsed": [["Chen", "Lin-Zhuo", ""], ["Li", "Xuan-Yi", ""], ["Fan", "Deng-Ping", ""], ["Wang", "Kai", ""], ["Lu", "Shao-Ping", ""], ["Cheng", "Ming-Ming", ""]]}, {"id": "1905.05445", "submitter": "Zhe Chen", "authors": "Zhe Chen, Xiao-Jun Wu, and Josef Kittler", "title": "Transition Subspace Learning based Least Squares Regression for Image\n  Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Only learning one projection matrix from original samples to the\ncorresponding binary labels is too strict and will consequentlly lose some\nintrinsic geometric structures of data. In this paper, we propose a novel\ntransition subspace learning based least squares regression (TSL-LSR) model for\nmulticategory image classification. The main idea of TSL-LSR is to learn a\ntransition subspace between the original samples and binary labels to alleviate\nthe problem of overfitting caused by strict projection learning. Moreover, in\norder to reflect the underlying low-rank structure of transition matrix and\nlearn more discriminative projection matrix, a low-rank constraint is added to\nthe transition subspace. Experimental results on several image datasets\ndemonstrate the effectiveness of the proposed TSL-LSR model in comparison with\nstate-of-the-art algorithms\n", "versions": [{"version": "v1", "created": "Tue, 14 May 2019 08:23:07 GMT"}, {"version": "v2", "created": "Fri, 14 Jun 2019 07:10:54 GMT"}], "update_date": "2019-06-17", "authors_parsed": [["Chen", "Zhe", ""], ["Wu", "Xiao-Jun", ""], ["Kittler", "Josef", ""]]}, {"id": "1905.05469", "submitter": "Ngoc-Trung Tran", "authors": "Ngoc-Trung Tran, Viet-Hung Tran, Ngoc-Bao Nguyen, Ngai-Man Cheung", "title": "An Improved Self-supervised GAN via Adversarial Training", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose to improve unconditional Generative Adversarial Networks (GAN) by\ntraining the self-supervised learning with the adversarial process. In\nparticular, we apply self-supervised learning via the geometric transformation\non input images and assign the pseudo-labels to these transformed images. (i)\nIn addition to the GAN task, which distinguishes data (real) versus generated\n(fake) samples, we train the discriminator to predict the correct pseudo-labels\nof real transformed samples (classification task). Importantly, we find out\nthat simultaneously training the discriminator to classify the fake class from\nthe pseudo-classes of real samples for the classification task will improve the\ndiscriminator and subsequently lead better guides to train generator. (ii) The\ngenerator is trained by attempting to confuse the discriminator for not only\nthe GAN task but also the classification task. For the classification task, the\ngenerator tries to confuse the discriminator recognizing the transformation of\nits output as one of the real transformed classes. Especially, we exploit that\nwhen the generator creates samples that result in a similar loss (via\ncross-entropy) as that of the real ones, the training is more stable and the\ngenerator distribution tends to match better the data distribution. When\nintegrating our techniques into a state-of-the-art Auto-Encoder (AE) based-GAN\nmodel, they help to significantly boost the model's performance and also\nestablish new state-of-the-art Fr\\'echet Inception Distance (FID) scores in the\nliterature of unconditional GAN for CIFAR-10 and STL-10 datasets.\n", "versions": [{"version": "v1", "created": "Tue, 14 May 2019 09:05:35 GMT"}], "update_date": "2019-05-15", "authors_parsed": [["Tran", "Ngoc-Trung", ""], ["Tran", "Viet-Hung", ""], ["Nguyen", "Ngoc-Bao", ""], ["Cheung", "Ngai-Man", ""]]}, {"id": "1905.05487", "submitter": "Brij Rokad", "authors": "Nikhil Kasukurthi, Brij Rokad, Shiv Bidani and Dr. Aju Dennisan", "title": "American Sign Language Alphabet Recognition using Deep Learning", "comments": "6 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tremendous headway has been made in the field of 3D hand pose estimation but\nthe 3D depth cameras are usually inaccessible. We propose a model to recognize\nAmerican Sign Language alphabet from RGB images. Images for the training were\nresized and pre-processed before training the Deep Neural Network. The model\nwas trained on a squeezenet architecture to make it capable of running on\nmobile devices with an accuracy of 83.29%.\n", "versions": [{"version": "v1", "created": "Tue, 14 May 2019 09:51:58 GMT"}], "update_date": "2019-05-15", "authors_parsed": [["Kasukurthi", "Nikhil", ""], ["Rokad", "Brij", ""], ["Bidani", "Shiv", ""], ["Dennisan", "Dr. Aju", ""]]}, {"id": "1905.05618", "submitter": "Ivan Barabanau", "authors": "Ivan Barabanau, Alexey Artemov, Evgeny Burnaev, Vyacheslav Murashkin", "title": "Monocular 3D Object Detection via Geometric Reasoning on Keypoints", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Monocular 3D object detection is well-known to be a challenging vision task\ndue to the loss of depth information; attempts to recover depth using separate\nimage-only approaches lead to unstable and noisy depth estimates, harming 3D\ndetections. In this paper, we propose a novel keypoint-based approach for 3D\nobject detection and localization from a single RGB image. We build our\nmulti-branch model around 2D keypoint detection in images and complement it\nwith a conceptually simple geometric reasoning method. Our network performs in\nan end-to-end manner, simultaneously and interdependently estimating 2D\ncharacteristics, such as 2D bounding boxes, keypoints, and orientation, along\nwith full 3D pose in the scene. We fuse the outputs of distinct branches,\napplying a reprojection consistency loss during training. The experimental\nevaluation on the challenging KITTI dataset benchmark demonstrates that our\nnetwork achieves state-of-the-art results among other monocular 3D detectors.\n", "versions": [{"version": "v1", "created": "Tue, 14 May 2019 14:00:19 GMT"}], "update_date": "2019-05-15", "authors_parsed": [["Barabanau", "Ivan", ""], ["Artemov", "Alexey", ""], ["Burnaev", "Evgeny", ""], ["Murashkin", "Vyacheslav", ""]]}, {"id": "1905.05622", "submitter": "Boyi Jiang", "authors": "Boyi Jiang, Juyong Zhang, Jianfei Cai, Jianmin Zheng", "title": "Disentangled Human Body Embedding Based on Deep Hierarchical Neural\n  Network", "comments": "This manuscript is accepted for publication in the IEEE Transactions\n  on Visualization and Computer Graphics Journal (IEEE TVCG). The Code is\n  available at https://github.com/Juyong/DHNN_BodyRepresentation", "journal-ref": null, "doi": "10.1109/TVCG.2020.2988476", "report-no": null, "categories": "cs.CV cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human bodies exhibit various shapes for different identities or poses, but\nthe body shape has certain similarities in structure and thus can be embedded\nin a low-dimensional space. This paper presents an autoencoder-like network\narchitecture to learn disentangled shape and pose embedding specifically for\nthe 3D human body. This is inspired by recent progress of deformation-based\nlatent representation learning. To improve the reconstruction accuracy, we\npropose a hierarchical reconstruction pipeline for the disentangling process\nand construct a large dataset of human body models with consistent connectivity\nfor the learning of the neural network. Our learned embedding can not only\nachieve superior reconstruction accuracy but also provide great flexibility in\n3D human body generation via interpolation, bilinear interpolation, and latent\nspace sampling. The results from extensive experiments demonstrate the\npowerfulness of our learned 3D human body embedding in various applications.\n", "versions": [{"version": "v1", "created": "Tue, 14 May 2019 14:06:54 GMT"}, {"version": "v2", "created": "Fri, 17 Apr 2020 10:09:32 GMT"}], "update_date": "2020-04-20", "authors_parsed": [["Jiang", "Boyi", ""], ["Zhang", "Juyong", ""], ["Cai", "Jianfei", ""], ["Zheng", "Jianmin", ""]]}, {"id": "1905.05652", "submitter": "Xingqian Li Mr", "authors": "Xingqian Li, Chenwei Lou, Jian Zhao, HuaPeng Wei, Hongwei Zhao", "title": "\"Tom\" pet robot applied to urban autism", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the fast development of network information technology, more and more\npeople are immersed in the virtual community environment brought by the\nnetwork, ignoring the social interaction in real life. The consequent urban\nautism problem has become more and more serious. Promoting offline\ncommunication between people \" and \"eliminating loneliness through emotional\ncommunication between pet robots and breeders\" to solve this problem, and has\ndeveloped a design called \"Tom\". \"Tom\" is a smart pet robot with a pet\nrobot-based social mechanism Called \"Tom-Talker\". The main contribution of this\npaper is to propose a social mechanism called \"Tom-Talker\" that encourages\nusers to socialize offline. And \"Tom-Talker\" also has a corresponding reward\nmechanism and a friend recommendation algorithm. It also proposes a pet robot\nnamed \"Tom\" with an emotional interaction algorithm to recognize users'\nemotions, simulate animal emotions and communicate emotionally with use s. This\npaper designs experiments and analyzes the results. The results show that our\npet robots have a good effect on solving urban autism problems.\n", "versions": [{"version": "v1", "created": "Tue, 14 May 2019 14:53:38 GMT"}], "update_date": "2019-05-15", "authors_parsed": [["Li", "Xingqian", ""], ["Lou", "Chenwei", ""], ["Zhao", "Jian", ""], ["Wei", "HuaPeng", ""], ["Zhao", "Hongwei", ""]]}, {"id": "1905.05661", "submitter": "Ivan Kre\\v{s}o", "authors": "Ivan Kre\\v{s}o, Josip Krapac, Sini\\v{s}a \\v{S}egvi\\'c", "title": "Efficient Ladder-style DenseNets for Semantic Segmentation of Large\n  Images", "comments": "12 pages, 6 figures, under review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent progress of deep image classification models has provided great\npotential to improve state-of-the-art performance in related computer vision\ntasks. However, the transition to semantic segmentation is hampered by strict\nmemory limitations of contemporary GPUs. The extent of feature map caching\nrequired by convolutional backprop poses significant challenges even for\nmoderately sized Pascal images, while requiring careful architectural\nconsiderations when the source resolution is in the megapixel range. To address\nthese concerns, we propose a novel DenseNet-based ladder-style architecture\nwhich features high modelling power and a very lean upsampling datapath. We\nalso propose to substantially reduce the extent of feature map caching by\nexploiting inherent spatial efficiency of the DenseNet feature extractor. The\nresulting models deliver high performance with fewer parameters than\ncompetitive approaches, and allow training at megapixel resolution on commodity\nhardware. The presented experimental results outperform the state-of-the-art in\nterms of prediction accuracy and execution speed on Cityscapes, Pascal VOC\n2012, CamVid and ROB 2018 datasets. Source code will be released upon\npublication.\n", "versions": [{"version": "v1", "created": "Tue, 14 May 2019 15:14:51 GMT"}], "update_date": "2019-05-15", "authors_parsed": [["Kre\u0161o", "Ivan", ""], ["Krapac", "Josip", ""], ["\u0160egvi\u0107", "Sini\u0161a", ""]]}, {"id": "1905.05675", "submitter": "Radoslaw Martin Cichy", "authors": "Radoslaw Martin Cichy, Gemma Roig, Alex Andonian, Kshitij Dwivedi,\n  Benjamin Lahner, Alex Lascelles, Yalda Mohsenzadeh, Kandan Ramakrishnan, Aude\n  Oliva", "title": "The Algonauts Project: A Platform for Communication between the Sciences\n  of Biological and Artificial Intelligence", "comments": "4 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI q-bio.NC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In the last decade, artificial intelligence (AI) models inspired by the brain\nhave made unprecedented progress in performing real-world perceptual tasks like\nobject classification and speech recognition. Recently, researchers of natural\nintelligence have begun using those AI models to explore how the brain performs\nsuch tasks. These developments suggest that future progress will benefit from\nincreased interaction between disciplines. Here we introduce the Algonauts\nProject as a structured and quantitative communication channel for\ninterdisciplinary interaction between natural and artificial intelligence\nresearchers. The project's core is an open challenge with a quantitative\nbenchmark whose goal is to account for brain data through computational models.\nThis project has the potential to provide better models of natural intelligence\nand to gather findings that advance AI. The 2019 Algonauts Project focuses on\nbenchmarking computational models predicting human brain activity when people\nlook at pictures of objects. The 2019 edition of the Algonauts Project is\navailable online: http://algonauts.csail.mit.edu/.\n", "versions": [{"version": "v1", "created": "Tue, 14 May 2019 15:37:22 GMT"}], "update_date": "2019-05-15", "authors_parsed": [["Cichy", "Radoslaw Martin", ""], ["Roig", "Gemma", ""], ["Andonian", "Alex", ""], ["Dwivedi", "Kshitij", ""], ["Lahner", "Benjamin", ""], ["Lascelles", "Alex", ""], ["Mohsenzadeh", "Yalda", ""], ["Ramakrishnan", "Kandan", ""], ["Oliva", "Aude", ""]]}, {"id": "1905.05739", "submitter": "Ben Glocker", "authors": "Ian Walker and Ben Glocker", "title": "Graph Convolutional Gaussian Processes", "comments": "Accepted at ICML 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel Bayesian nonparametric method to learn\ntranslation-invariant relationships on non-Euclidean domains. The resulting\ngraph convolutional Gaussian processes can be applied to problems in machine\nlearning for which the input observations are functions with domains on general\ngraphs. The structure of these models allows for high dimensional inputs while\nretaining expressibility, as is the case with convolutional neural networks. We\npresent applications of graph convolutional Gaussian processes to images and\ntriangular meshes, demonstrating their versatility and effectiveness, comparing\nfavorably to existing methods, despite being relatively simple models.\n", "versions": [{"version": "v1", "created": "Tue, 14 May 2019 17:32:29 GMT"}], "update_date": "2019-05-15", "authors_parsed": [["Walker", "Ian", ""], ["Glocker", "Ben", ""]]}, {"id": "1905.05749", "submitter": "Lukas Mosser", "authors": "Lukas Mosser, Olivier Dubrule, Martin J. Blunt", "title": "DeepFlow: History Matching in the Space of Deep Generative Models", "comments": "25 pages, 15 figures, fixed typos", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV physics.comp-ph physics.geo-ph stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The calibration of a reservoir model with observed transient data of fluid\npressures and rates is a key task in obtaining a predictive model of the flow\nand transport behaviour of the earth's subsurface. The model calibration task,\ncommonly referred to as \"history matching\", can be formalised as an ill-posed\ninverse problem where we aim to find the underlying spatial distribution of\npetrophysical properties that explain the observed dynamic data. We use a\ngenerative adversarial network pretrained on geostatistical object-based models\nto represent the distribution of rock properties for a synthetic model of a\nhydrocarbon reservoir. The dynamic behaviour of the reservoir fluids is\nmodelled using a transient two-phase incompressible Darcy formulation. We\ninvert for the underlying reservoir properties by first modeling property\ndistributions using the pre-trained generative model then using the adjoint\nequations of the forward problem to perform gradient descent on the latent\nvariables that control the output of the generative model. In addition to the\ndynamic observation data, we include well rock-type constraints by introducing\nan additional objective function. Our contribution shows that for a synthetic\ntest case, we are able to obtain solutions to the inverse problem by optimising\nin the latent variable space of a deep generative model, given a set of\ntransient observations of a non-linear forward problem.\n", "versions": [{"version": "v1", "created": "Tue, 14 May 2019 17:52:52 GMT"}, {"version": "v2", "created": "Wed, 12 Jun 2019 12:49:02 GMT"}], "update_date": "2019-06-13", "authors_parsed": [["Mosser", "Lukas", ""], ["Dubrule", "Olivier", ""], ["Blunt", "Martin J.", ""]]}, {"id": "1905.05754", "submitter": "Karim Iskakov", "authors": "Karim Iskakov, Egor Burkov, Victor Lempitsky, Yury Malkov", "title": "Learnable Triangulation of Human Pose", "comments": "Project page: https://saic-violet.github.io/learnable-triangulation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present two novel solutions for multi-view 3D human pose estimation based\non new learnable triangulation methods that combine 3D information from\nmultiple 2D views. The first (baseline) solution is a basic differentiable\nalgebraic triangulation with an addition of confidence weights estimated from\nthe input images. The second solution is based on a novel method of volumetric\naggregation from intermediate 2D backbone feature maps. The aggregated volume\nis then refined via 3D convolutions that produce final 3D joint heatmaps and\nallow modelling a human pose prior. Crucially, both approaches are end-to-end\ndifferentiable, which allows us to directly optimize the target metric. We\ndemonstrate transferability of the solutions across datasets and considerably\nimprove the multi-view state of the art on the Human3.6M dataset. Video\ndemonstration, annotations and additional materials will be posted on our\nproject page (https://saic-violet.github.io/learnable-triangulation).\n", "versions": [{"version": "v1", "created": "Tue, 14 May 2019 17:59:20 GMT"}], "update_date": "2019-05-15", "authors_parsed": [["Iskakov", "Karim", ""], ["Burkov", "Egor", ""], ["Lempitsky", "Victor", ""], ["Malkov", "Yury", ""]]}, {"id": "1905.05774", "submitter": "Samarth Tripathi", "authors": "Samarth Tripathi, Jiayi Liu, Unmesh Kurup, Mohak Shah, Sauptik Dhar", "title": "Improving Model Training by Periodic Sampling over Weight Distributions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we explore techniques centered around periodic sampling of\nmodel weights that provide convergence improvements on gradient update methods\n(vanilla \\acs{SGD}, Momentum, Adam) for a variety of vision problems\n(classification, detection, segmentation). Importantly, our algorithms provide\nbetter, faster and more robust convergence and training performance with only a\nslight increase in computation time. Our techniques are independent of the\nneural network model, gradient optimization methods or existing optimal\ntraining policies and converge in a less volatile fashion with performance\nimprovements that are approximately monotonic. We conduct a variety of\nexperiments to quantify these improvements and identify scenarios where these\ntechniques could be more useful.\n", "versions": [{"version": "v1", "created": "Tue, 14 May 2019 18:00:23 GMT"}, {"version": "v2", "created": "Thu, 19 Mar 2020 21:37:12 GMT"}], "update_date": "2020-03-23", "authors_parsed": [["Tripathi", "Samarth", ""], ["Liu", "Jiayi", ""], ["Kurup", "Unmesh", ""], ["Shah", "Mohak", ""], ["Dhar", "Sauptik", ""]]}, {"id": "1905.05820", "submitter": "Yujia Chen", "authors": "Yujia Chen, Yang Lou, Kun Wang, Matthew A. Kupinski, Mark A. Anastasio", "title": "Reconstruction-Aware Imaging System Ranking by use of a Sparsity-Driven\n  Numerical Observer Enabled by Variational Bayesian Inference", "comments": "IEEE transactions on medical imaging (2018)", "journal-ref": null, "doi": "10.1109/TMI.2018.2880870", "report-no": null, "categories": "cs.CV stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is widely accepted that optimization of imaging system performance should\nbe guided by task-based measures of image quality (IQ). It has been advocated\nthat imaging hardware or data-acquisition designs should be optimized by use of\nan ideal observer (IO) that exploits full statistical knowledge of the\nmeasurement noise and class of objects to be imaged, without consideration of\nthe reconstruction method. In practice, accurate and tractable models of the\ncomplete object statistics are often difficult to determine. Moreover, in\nimaging systems that employ compressive sensing concepts, imaging hardware and\nsparse image reconstruction are innately coupled technologies. In this work, a\nsparsity-driven observer (SDO) that can be employed to optimize hardware by use\nof a stochastic object model describing object sparsity is described and\ninvestigated. The SDO and sparse reconstruction method can therefore be\n\"matched\" in the sense that they both utilize the same statistical information\nregarding the class of objects to be imaged. To efficiently compute the SDO\ntest statistic, computational tools developed recently for variational Bayesian\ninference with sparse linear models are adopted. The use of the SDO to rank\ndata-acquisition designs in a stylized example as motivated by magnetic\nresonance imaging (MRI) is demonstrated. This study reveals that the SDO can\nproduce rankings that are consistent with visual assessments of the\nreconstructed images but different from those produced by use of the\ntraditionally employed Hotelling observer (HO).\n", "versions": [{"version": "v1", "created": "Tue, 14 May 2019 20:12:49 GMT"}], "update_date": "2019-05-16", "authors_parsed": [["Chen", "Yujia", ""], ["Lou", "Yang", ""], ["Wang", "Kun", ""], ["Kupinski", "Matthew A.", ""], ["Anastasio", "Mark A.", ""]]}, {"id": "1905.05833", "submitter": "Juan Irving Vasquez-Gomez", "authors": "Miguel Mendoza, J. Irving Vasquez-Gomez, Hind Taud, Luis Enrique\n  Sucar, Carolina Reta", "title": "Supervised Learning of the Next-Best-View for 3D Object Reconstruction", "comments": "Under review in Pattern Recognition Letters", "journal-ref": "Pattern Recognition Letters, Volume 133, 2020, Pages 224-231, ISSN\n  0167-8655", "doi": "10.1016/j.patrec.2020.02.024", "report-no": null, "categories": "cs.CV cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by the advances in 3D sensing technology and the spreading of\nlow-cost robotic platforms, 3D object reconstruction has become a common task\nin many areas. Nevertheless, the selection of the optimal sensor pose that\nmaximizes the reconstructed surface is a problem that remains open. It is known\nin the literature as the next-best-view planning problem. In this paper, we\npropose a novel next-best-view planning scheme based on supervised deep\nlearning. The scheme contains an algorithm for automatic generation of datasets\nand an original three-dimensional convolutional neural network (3D-CNN) used to\nlearn the next-best-view. Unlike previous work where the problem is addressed\nas a search, the trained 3D-CNN directly predicts the sensor pose. We present a\ncomparison of the proposed network against a similar net, and we present\nseveral experiments of the reconstruction of unknown objects validating the\neffectiveness of the proposed scheme.\n", "versions": [{"version": "v1", "created": "Tue, 14 May 2019 20:39:38 GMT"}], "update_date": "2021-01-27", "authors_parsed": [["Mendoza", "Miguel", ""], ["Vasquez-Gomez", "J. Irving", ""], ["Taud", "Hind", ""], ["Sucar", "Luis Enrique", ""], ["Reta", "Carolina", ""]]}, {"id": "1905.05840", "submitter": "Yan-Li Liu", "authors": "Yan-li Liu and Chu-min Li and Hua Jiang and Kun He", "title": "A Learning based Branch and Bound for Maximum Common Subgraph Problems", "comments": "6 pages, 4 figures, uses ijcai19.sty", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Branch-and-bound (BnB) algorithms are widely used to solve combinatorial\nproblems, and the performance crucially depends on its branching heuristic.In\nthis work, we consider a typical problem of maximum common subgraph (MCS), and\npropose a branching heuristic inspired from reinforcement learning with a goal\nof reaching a tree leaf as early as possible to greatly reduce the search tree\nsize.Extensive experiments show that our method is beneficial and outperforms\ncurrent best BnB algorithm for the MCS.\n", "versions": [{"version": "v1", "created": "Wed, 15 May 2019 01:37:02 GMT"}, {"version": "v2", "created": "Wed, 22 May 2019 01:40:04 GMT"}], "update_date": "2019-05-23", "authors_parsed": [["Liu", "Yan-li", ""], ["Li", "Chu-min", ""], ["Jiang", "Hua", ""], ["He", "Kun", ""]]}, {"id": "1905.05849", "submitter": "Shaeke Salman", "authors": "Shaeke Salman, Seyedeh Neelufar Payrovnaziri, Xiuwen Liu, Pablo\n  Rengifo-Moreno, Zhe He", "title": "Consensus-based Interpretable Deep Neural Networks with Application to\n  Mortality Prediction", "comments": "8 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks have achieved remarkable success in various challenging\ntasks. However, the black-box nature of such networks is not acceptable to\ncritical applications, such as healthcare. In particular, the existence of\nadversarial examples and their overgeneralization to irrelevant,\nout-of-distribution inputs with high confidence makes it difficult, if not\nimpossible, to explain decisions by such networks. In this paper, we analyze\nthe underlying mechanism of generalization of deep neural networks and propose\nan ($n$, $k$) consensus algorithm which is insensitive to adversarial examples\nand can reliably reject out-of-distribution samples. Furthermore, the consensus\nalgorithm is able to improve classification accuracy by using multiple trained\ndeep neural networks. To handle the complexity of deep neural networks, we\ncluster linear approximations of individual models and identify highly\ncorrelated clusters among different models to capture feature importance\nrobustly, resulting in improved interpretability. Motivated by the importance\nof building accurate and interpretable prediction models for healthcare, our\nexperimental results on an ICU dataset show the effectiveness of our algorithm\nin enhancing both the prediction accuracy and the interpretability of deep\nneural network models on one-year patient mortality prediction. In particular,\nwhile the proposed method maintains similar interpretability as conventional\nshallow models such as logistic regression, it improves the prediction accuracy\nsignificantly.\n", "versions": [{"version": "v1", "created": "Tue, 14 May 2019 21:26:56 GMT"}, {"version": "v2", "created": "Wed, 11 Sep 2019 05:32:07 GMT"}], "update_date": "2019-09-12", "authors_parsed": [["Salman", "Shaeke", ""], ["Payrovnaziri", "Seyedeh Neelufar", ""], ["Liu", "Xiuwen", ""], ["Rengifo-Moreno", "Pablo", ""], ["He", "Zhe", ""]]}, {"id": "1905.05880", "submitter": "Miriam Bellver Bueno", "authors": "Miriam Bellver, Amaia Salvador, Jordi Torres and Xavier Giro-i-Nieto", "title": "Budget-aware Semi-Supervised Semantic and Instance Segmentation", "comments": "To appear in CVPR-W 2019 (DeepVision workshop)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Methods that move towards less supervised scenarios are key for image\nsegmentation, as dense labels demand significant human intervention. Generally,\nthe annotation burden is mitigated by labeling datasets with weaker forms of\nsupervision, e.g. image-level labels or bounding boxes. Another option are\nsemi-supervised settings, that commonly leverage a few strong annotations and a\nhuge number of unlabeled/weakly-labeled data. In this paper, we revisit\nsemi-supervised segmentation schemes and narrow down significantly the\nannotation budget (in terms of total labeling time of the training set)\ncompared to previous approaches. With a very simple pipeline, we demonstrate\nthat at low annotation budgets, semi-supervised methods outperform by a wide\nmargin weakly-supervised ones for both semantic and instance segmentation. Our\napproach also outperforms previous semi-supervised works at a much reduced\nlabeling cost. We present results for the Pascal VOC benchmark and unify weakly\nand semi-supervised approaches by considering the total annotation budget, thus\nallowing a fairer comparison between methods.\n", "versions": [{"version": "v1", "created": "Tue, 14 May 2019 23:19:41 GMT"}, {"version": "v2", "created": "Thu, 23 May 2019 21:24:46 GMT"}], "update_date": "2019-05-27", "authors_parsed": [["Bellver", "Miriam", ""], ["Salvador", "Amaia", ""], ["Torres", "Jordi", ""], ["Giro-i-Nieto", "Xavier", ""]]}, {"id": "1905.05882", "submitter": "Patsorn Sangkloy", "authors": "Wittawat Jitkrittum, Patsorn Sangkloy, Muhammad Waleed Gondal, Amit\n  Raj, James Hays, Bernhard Sch\\\"olkopf", "title": "Kernel Mean Matching for Content Addressability of GANs", "comments": "Wittawat Jitkrittum and Patsorn Sangkloy contributed equally to this\n  work", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel procedure which adds \"content-addressability\" to any given\nunconditional implicit model e.g., a generative adversarial network (GAN). The\nprocedure allows users to control the generative process by specifying a set\n(arbitrary size) of desired examples based on which similar samples are\ngenerated from the model. The proposed approach, based on kernel mean matching,\nis applicable to any generative models which transform latent vectors to\nsamples, and does not require retraining of the model. Experiments on various\nhigh-dimensional image generation problems (CelebA-HQ, LSUN bedroom, bridge,\ntower) show that our approach is able to generate images which are consistent\nwith the input set, while retaining the image quality of the original model. To\nour knowledge, this is the first work that attempts to construct, at test time,\na content-addressable generative model from a trained marginal model.\n", "versions": [{"version": "v1", "created": "Tue, 14 May 2019 23:32:53 GMT"}], "update_date": "2019-05-16", "authors_parsed": [["Jitkrittum", "Wittawat", ""], ["Sangkloy", "Patsorn", ""], ["Gondal", "Muhammad Waleed", ""], ["Raj", "Amit", ""], ["Hays", "James", ""], ["Sch\u00f6lkopf", "Bernhard", ""]]}, {"id": "1905.05889", "submitter": "Dominic Cheng", "authors": "Dominic Cheng, Renjie Liao, Sanja Fidler, Raquel Urtasun", "title": "DARNet: Deep Active Ray Network for Building Segmentation", "comments": "CVPR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a Deep Active Ray Network (DARNet) for automatic\nbuilding segmentation. Taking an image as input, it first exploits a deep\nconvolutional neural network (CNN) as the backbone to predict energy maps,\nwhich are further utilized to construct an energy function. A polygon-based\ncontour is then evolved via minimizing the energy function, of which the\nminimum defines the final segmentation. Instead of parameterizing the contour\nusing Euclidean coordinates, we adopt polar coordinates, i.e., rays, which not\nonly prevents self-intersection but also simplifies the design of the energy\nfunction. Moreover, we propose a loss function that directly encourages the\ncontours to match building boundaries. Our DARNet is trained end-to-end by\nback-propagating through the energy minimization and the backbone CNN, which\nmakes the CNN adapt to the dynamics of the contour evolution. Experiments on\nthree building instance segmentation datasets demonstrate our DARNet achieves\neither state-of-the-art or comparable performances to other competitors.\n", "versions": [{"version": "v1", "created": "Wed, 15 May 2019 00:00:21 GMT"}], "update_date": "2019-05-16", "authors_parsed": [["Cheng", "Dominic", ""], ["Liao", "Renjie", ""], ["Fidler", "Sanja", ""], ["Urtasun", "Raquel", ""]]}, {"id": "1905.05891", "submitter": "Muhammad Bilal", "authors": "Adwan Alownie Alanazi and Muhammad Bilal", "title": "Crowd Density Estimation using Novel Feature Descriptor", "comments": null, "journal-ref": "International Research Journal of Engineering and Technology,\n  Volume 05, Issue 10, Oct 2018", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Crowd density estimation is an important task for crowd monitoring. Many\nefforts have been done to automate the process of estimating crowd density from\nimages and videos. Despite series of efforts, it remains a challenging task. In\nthis paper, we proposes a new texture feature-based approach for the estimation\nof crowd density based on Completed Local Binary Pattern (CLBP). We first\ndivide the image into blocks and then re-divide the blocks into cells. For each\ncell, we compute CLBP and then concatenate them to describe the texture of the\ncorresponding block. We then train a multi-class Support Vector Machine (SVM)\nclassifier, which classifies each block of image into one of four categories,\ni.e. Very Low, Low, Medium, and High. We evaluate our technique on the PETS\n2009 dataset, and from the experiments, we show to achieve 95% accuracy for the\nproposed descriptor. We also compare other state-of-the-art texture descriptors\nand from the experimental results, we show that our proposed method outperforms\nother state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Wed, 15 May 2019 00:00:59 GMT"}], "update_date": "2019-05-16", "authors_parsed": [["Alanazi", "Adwan Alownie", ""], ["Bilal", "Muhammad", ""]]}, {"id": "1905.05895", "submitter": "Chen Huang", "authors": "Chen Huang, Shuangfei Zhai, Walter Talbott, Miguel Angel Bautista,\n  Shih-Yu Sun, Carlos Guestrin, Josh Susskind", "title": "Addressing the Loss-Metric Mismatch with Adaptive Loss Alignment", "comments": "Accepted to ICML 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In most machine learning training paradigms a fixed, often handcrafted, loss\nfunction is assumed to be a good proxy for an underlying evaluation metric. In\nthis work we assess this assumption by meta-learning an adaptive loss function\nto directly optimize the evaluation metric. We propose a sample efficient\nreinforcement learning approach for adapting the loss dynamically during\ntraining. We empirically show how this formulation improves performance by\nsimultaneously optimizing the evaluation metric and smoothing the loss\nlandscape. We verify our method in metric learning and classification\nscenarios, showing considerable improvements over the state-of-the-art on a\ndiverse set of tasks. Importantly, our method is applicable to a wide range of\nloss functions and evaluation metrics. Furthermore, the learned policies are\ntransferable across tasks and data, demonstrating the versatility of the\nmethod.\n", "versions": [{"version": "v1", "created": "Wed, 15 May 2019 00:12:07 GMT"}], "update_date": "2019-05-16", "authors_parsed": [["Huang", "Chen", ""], ["Zhai", "Shuangfei", ""], ["Talbott", "Walter", ""], ["Bautista", "Miguel Angel", ""], ["Sun", "Shih-Yu", ""], ["Guestrin", "Carlos", ""], ["Susskind", "Josh", ""]]}, {"id": "1905.05908", "submitter": "Senthil Purushwalkam", "authors": "Senthil Purushwalkam, Maximilian Nickel, Abhinav Gupta, Marc'Aurelio\n  Ranzato", "title": "Task-Driven Modular Networks for Zero-Shot Compositional Learning", "comments": "http://www.cs.cmu.edu/~spurushw/projects/compositional.html", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the hallmarks of human intelligence is the ability to compose learned\nknowledge into novel concepts which can be recognized without a single training\nexample. In contrast, current state-of-the-art methods require hundreds of\ntraining examples for each possible category to build reliable and accurate\nclassifiers. To alleviate this striking difference in efficiency, we propose a\ntask-driven modular architecture for compositional reasoning and sample\nefficient learning. Our architecture consists of a set of neural network\nmodules, which are small fully connected layers operating in semantic concept\nspace. These modules are configured through a gating function conditioned on\nthe task to produce features representing the compatibility between the input\nimage and the concept under consideration. This enables us to express tasks as\na combination of sub-tasks and to generalize to unseen categories by\nreweighting a set of small modules. Furthermore, the network can be trained\nefficiently as it is fully differentiable and its modules operate on small\nsub-spaces. We focus our study on the problem of compositional zero-shot\nclassification of object-attribute categories. We show in our experiments that\ncurrent evaluation metrics are flawed as they only consider unseen\nobject-attribute pairs. When extending the evaluation to the generalized\nsetting which accounts also for pairs seen during training, we discover that\nnaive baseline methods perform similarly or better than current approaches.\nHowever, our modular network is able to outperform all existing approaches on\ntwo widely-used benchmark datasets.\n", "versions": [{"version": "v1", "created": "Wed, 15 May 2019 01:29:08 GMT"}], "update_date": "2019-05-16", "authors_parsed": [["Purushwalkam", "Senthil", ""], ["Nickel", "Maximilian", ""], ["Gupta", "Abhinav", ""], ["Ranzato", "Marc'Aurelio", ""]]}, {"id": "1905.05916", "submitter": "Yong-Goo Shin", "authors": "Yong-Goo Shin, Seung Park, Yoon-Jae Yeo, Min-Jae Yoo, Sung-Jea Ko", "title": "Unsupervised Deep Contrast Enhancement with Power Constraint for OLED\n  Displays", "comments": "Accepted to IEEE transactions on Image Processing. To be published", "journal-ref": null, "doi": "10.1109/TIP.2019.2953352", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Various power-constrained contrast enhancement (PCCE) techniques have been\napplied to an organic light emitting diode (OLED) display for reducing the\npower demands of the display while preserving the image quality. In this paper,\nwe propose a new deep learning-based PCCE scheme that constrains the power\nconsumption of the OLED displays while enhancing the contrast of the displayed\nimage. In the proposed method, the power consumption is constrained by simply\nreducing the brightness a certain ratio, whereas the perceived visual quality\nis preserved as much as possible by enhancing the contrast of the image using a\nconvolutional neural network (CNN). Furthermore, our CNN can learn the PCCE\ntechnique without a reference image by unsupervised learning. Experimental\nresults show that the proposed method is superior to conventional ones in terms\nof image quality assessment metrics such as a visual saliency-induced index\n(VSI) and a measure of enhancement (EME).\n", "versions": [{"version": "v1", "created": "Wed, 15 May 2019 02:15:35 GMT"}, {"version": "v2", "created": "Sun, 11 Aug 2019 23:29:43 GMT"}, {"version": "v3", "created": "Fri, 1 Nov 2019 02:06:20 GMT"}, {"version": "v4", "created": "Sat, 9 Nov 2019 05:16:14 GMT"}, {"version": "v5", "created": "Tue, 10 Dec 2019 01:52:59 GMT"}], "update_date": "2019-12-11", "authors_parsed": [["Shin", "Yong-Goo", ""], ["Park", "Seung", ""], ["Yeo", "Yoon-Jae", ""], ["Yoo", "Min-Jae", ""], ["Ko", "Sung-Jea", ""]]}, {"id": "1905.05927", "submitter": "Anoop Cherian", "authors": "Arvind U. Raghunathan, Anoop Cherian, Devesh K. Jha", "title": "Game Theoretic Optimization via Gradient-based Nikaido-Isoda Function", "comments": "Accepted at International Conference on Machine Learning (ICML), 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computing Nash equilibrium (NE) of multi-player games has witnessed renewed\ninterest due to recent advances in generative adversarial networks. However,\ncomputing equilibrium efficiently is challenging. To this end, we introduce the\nGradient-based Nikaido-Isoda (GNI) function which serves: (i) as a merit\nfunction, vanishing only at the first-order stationary points of each player's\noptimization problem, and (ii) provides error bounds to a stationary Nash\npoint. Gradient descent is shown to converge sublinearly to a first-order\nstationary point of the GNI function. For the particular case of bilinear\nmin-max games and multi-player quadratic games, the GNI function is convex.\nHence, the application of gradient descent in this case yields linear\nconvergence to an NE (when one exists). In our numerical experiments, we\nobserve that the GNI formulation always converges to the first-order stationary\npoint of each player's optimization problem.\n", "versions": [{"version": "v1", "created": "Wed, 15 May 2019 03:20:45 GMT"}], "update_date": "2019-05-16", "authors_parsed": [["Raghunathan", "Arvind U.", ""], ["Cherian", "Anoop", ""], ["Jha", "Devesh K.", ""]]}, {"id": "1905.05929", "submitter": "Kui Jia", "authors": "Kui Jia, Shuai Li, Yuxin Wen, Tongliang Liu, and Dacheng Tao", "title": "Orthogonal Deep Neural Networks", "comments": "To Appear in IEEE Transactions on Pattern Analysis and Machine\n  Intelligence", "journal-ref": "IEEE Transactions on Pattern Analysis and Machine Intelligence,\n  2019", "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce the algorithms of Orthogonal Deep Neural Networks\n(OrthDNNs) to connect with recent interest of spectrally regularized deep\nlearning methods. OrthDNNs are theoretically motivated by generalization\nanalysis of modern DNNs, with the aim to find solution properties of network\nweights that guarantee better generalization. To this end, we first prove that\nDNNs are of local isometry on data distributions of practical interest; by\nusing a new covering of the sample space and introducing the local isometry\nproperty of DNNs into generalization analysis, we establish a new\ngeneralization error bound that is both scale- and range-sensitive to singular\nvalue spectrum of each of networks' weight matrices. We prove that the optimal\nbound w.r.t. the degree of isometry is attained when each weight matrix has a\nspectrum of equal singular values, among which orthogonal weight matrix or a\nnon-square one with orthonormal rows or columns is the most straightforward\nchoice, suggesting the algorithms of OrthDNNs. We present both algorithms of\nstrict and approximate OrthDNNs, and for the later ones we propose a simple yet\neffective algorithm called Singular Value Bounding (SVB), which performs as\nwell as strict OrthDNNs, but at a much lower computational cost. We also\npropose Bounded Batch Normalization (BBN) to make compatible use of batch\nnormalization with OrthDNNs. We conduct extensive comparative studies by using\nmodern architectures on benchmark image classification. Experiments show the\nefficacy of OrthDNNs.\n", "versions": [{"version": "v1", "created": "Wed, 15 May 2019 03:34:10 GMT"}, {"version": "v2", "created": "Tue, 15 Oct 2019 13:14:20 GMT"}], "update_date": "2019-10-16", "authors_parsed": [["Jia", "Kui", ""], ["Li", "Shuai", ""], ["Wen", "Yuxin", ""], ["Liu", "Tongliang", ""], ["Tao", "Dacheng", ""]]}, {"id": "1905.05941", "submitter": "Tai-Xiang Jiang", "authors": "Hao Zhang, Xi-Le Zhao, Tai-Xiang Jiang and Michael Kwok-Po Ng", "title": "Constrained low-tubal-rank tensor recovery for hyperspectral images\n  mixed noise removal by bilateral random projections", "comments": "Accepted by IGARSS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a novel low-tubal-rank tensor recovery model, which\ndirectly constrains the tubal rank prior for effectively removing the mixed\nGaussian and sparse noise in hyperspectral images. The constraints of\ntubal-rank and sparsity can govern the solution of the denoised tensor in the\nrecovery procedure. To solve the constrained low-tubal-rank model, we develop\nan iterative algorithm based on bilateral random projections to efficiently\nsolve the proposed model. The advantage of random projections is that the\napproximation of the low-tubal-rank tensor can be obtained quite accurately in\nan inexpensive manner. Experimental examples for hyperspectral image denoising\nare presented to demonstrate the effectiveness and efficiency of the proposed\nmethod.\n", "versions": [{"version": "v1", "created": "Wed, 15 May 2019 04:20:12 GMT"}], "update_date": "2019-05-16", "authors_parsed": [["Zhang", "Hao", ""], ["Zhao", "Xi-Le", ""], ["Jiang", "Tai-Xiang", ""], ["Ng", "Michael Kwok-Po", ""]]}, {"id": "1905.05946", "submitter": "Gerardo Flores Gera", "authors": "Sergio Trejo and Karla Martinez and Gerardo Flores", "title": "Depth map estimation methodology for detecting free-obstacle navigation\n  areas", "comments": null, "journal-ref": "ICUAS'19 The 2019 International Conference on Unmanned Aircraft\n  Systems", "doi": null, "report-no": null, "categories": "cs.RO cs.CV cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a vision-based methodology which makes use of a stereo\ncamera rig and a one dimension LiDAR to estimate free obstacle areas for\nquadrotor navigation. The presented approach fuses information provided by a\ndepth map from a stereo camera rig, and the sensing distance of the 1D-LiDAR.\nOnce the depth map is filtered with a Weighted Least Squares filter (WLS), the\ninformation is fused through a Kalman filter algorithm. To determine if there\nis a free space large enough for the quadrotor to pass through, our approach\nmarks an area inside the disparity map by using the Kalman Filter output\ninformation. The whole process is implemented in an embedded computer Jetson\nTX2 and coded in the Robotic Operating System (ROS). Experiments demonstrate\nthe effectiveness of our approach.\n", "versions": [{"version": "v1", "created": "Wed, 15 May 2019 04:57:53 GMT"}], "update_date": "2019-05-16", "authors_parsed": [["Trejo", "Sergio", ""], ["Martinez", "Karla", ""], ["Flores", "Gerardo", ""]]}, {"id": "1905.05947", "submitter": "Chi Zhang", "authors": "Zongliang Li, Chi Zhang, Gaofeng Meng, Yuehu Liu", "title": "Joint haze image synthesis and dehazing with mmd-vae losses", "comments": "Preprinted version on arxiv, May-05-2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fog and haze are weathers with low visibility which are adversarial to the\ndriving safety of intelligent vehicles equipped with optical sensors like\ncameras and LiDARs. Therefore image dehazing for perception enhancement and\nhaze image synthesis for testing perception abilities are equivalently\nimportant in the development of such autonomous driving systems. From the view\nof image translation, these two problems are essentially dual with each other,\nwhich have the potentiality to be solved jointly. In this paper, we propose an\nunsupervised Image-to-Image Translation framework based on Variational\nAutoencoders (VAE) and Generative Adversarial Nets (GAN) to handle haze image\nsynthesis and haze removal simultaneously. Since the KL divergence in the VAE\nobjectives could not guarantee the optimal mapping under imbalanced and\nunpaired training samples with limited size, Maximum mean discrepancy (MMD)\nbased VAE is utilized to ensure the translating consistency in both directions.\nThe comprehensive analysis on both synthesis and dehazing performance of our\nmethod demonstrate the feasibility and practicability of the proposed method.\n", "versions": [{"version": "v1", "created": "Wed, 15 May 2019 05:12:21 GMT"}], "update_date": "2019-05-16", "authors_parsed": [["Li", "Zongliang", ""], ["Zhang", "Chi", ""], ["Meng", "Gaofeng", ""], ["Liu", "Yuehu", ""]]}, {"id": "1905.05961", "submitter": "Zijian Wang", "authors": "Zijian Wang, Scott A. Hale, David Adelani, Przemyslaw A. Grabowicz,\n  Timo Hartmann, Fabian Fl\\\"ock, David Jurgens", "title": "Demographic Inference and Representative Population Estimates from\n  Multilingual Social Media Data", "comments": "12 pages, 10 figures, Proceedings of the 2019 World Wide Web\n  Conference (WWW '19)", "journal-ref": "Proceedings of the 2019 World Wide Web Conference (WWW '19), May\n  13--17, 2019, San Francisco, CA, USA", "doi": "10.1145/3308558.3313684", "report-no": null, "categories": "cs.CY cs.CL cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Social media provide access to behavioural data at an unprecedented scale and\ngranularity. However, using these data to understand phenomena in a broader\npopulation is difficult due to their non-representativeness and the bias of\nstatistical inference tools towards dominant languages and groups. While\ndemographic attribute inference could be used to mitigate such bias, current\ntechniques are almost entirely monolingual and fail to work in a global\nenvironment. We address these challenges by combining multilingual demographic\ninference with post-stratification to create a more representative population\nsample. To learn demographic attributes, we create a new multimodal deep neural\narchitecture for joint classification of age, gender, and organization-status\nof social media users that operates in 32 languages. This method substantially\noutperforms current state of the art while also reducing algorithmic bias. To\ncorrect for sampling biases, we propose fully interpretable multilevel\nregression methods that estimate inclusion probabilities from inferred joint\npopulation counts and ground-truth population counts. In a large experiment\nover multilingual heterogeneous European regions, we show that our demographic\ninference and bias correction together allow for more accurate estimates of\npopulations and make a significant step towards representative social sensing\nin downstream applications with multilingual social media.\n", "versions": [{"version": "v1", "created": "Wed, 15 May 2019 06:15:16 GMT"}], "update_date": "2019-05-16", "authors_parsed": [["Wang", "Zijian", ""], ["Hale", "Scott A.", ""], ["Adelani", "David", ""], ["Grabowicz", "Przemyslaw A.", ""], ["Hartmann", "Timo", ""], ["Fl\u00f6ck", "Fabian", ""], ["Jurgens", "David", ""]]}, {"id": "1905.05964", "submitter": "Heming Zhang", "authors": "Heming Zhang, Xiaolong Wang, C.-C. Jay Kuo", "title": "Deep Kinship Verification via Appearance-shape Joint Prediction and\n  Adaptation-based Approach", "comments": "ICIP 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Kinship verification aims to identify the kin relation between two given face\nimages. It is a very challenging problem due to the lack of training data and\nfacial similarity variations between kinship pairs. In this work, we build a\nnovel appearance and shape based deep learning pipeline. First we adopt the\nknowledge learned from general face recognition network to learn general facial\nfeatures. Afterwards, we learn kinship oriented appearance and shape features\nfrom kinship pairs and combine them for the final prediction. We have evaluated\nthe model performance on a widely used popular benchmark and demonstrated the\nsuperiority over the state-of-the-art.\n", "versions": [{"version": "v1", "created": "Wed, 15 May 2019 06:17:09 GMT"}], "update_date": "2019-05-16", "authors_parsed": [["Zhang", "Heming", ""], ["Wang", "Xiaolong", ""], ["Kuo", "C. -C. Jay", ""]]}, {"id": "1905.05980", "submitter": "Yingying Jiang", "authors": "Xiaobing Wang, Yingying Jiang, Zhenbo Luo, Cheng-Lin Liu, Hyunsoo\n  Choi, Sungjin Kim", "title": "Arbitrary Shape Scene Text Detection with Adaptive Text Region\n  Representation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scene text detection attracts much attention in computer vision, because it\ncan be widely used in many applications such as real-time text translation,\nautomatic information entry, blind person assistance, robot sensing and so on.\nThough many methods have been proposed for horizontal and oriented texts,\ndetecting irregular shape texts such as curved texts is still a challenging\nproblem. To solve the problem, we propose a robust scene text detection method\nwith adaptive text region representation. Given an input image, a text region\nproposal network is first used for extracting text proposals. Then, these\nproposals are verified and refined with a refinement network. Here, recurrent\nneural network based adaptive text region representation is proposed for text\nregion refinement, where a pair of boundary points are predicted each time step\nuntil no new points are found. In this way, text regions of arbitrary shapes\nare detected and represented with adaptive number of boundary points. This\ngives more accurate description of text regions. Experimental results on five\nbenchmarks, namely, CTW1500, TotalText, ICDAR2013, ICDAR2015 and MSRATD500,\nshow that the proposed method achieves state-of-the-art in scene text\ndetection.\n", "versions": [{"version": "v1", "created": "Wed, 15 May 2019 07:06:27 GMT"}], "update_date": "2019-05-16", "authors_parsed": [["Wang", "Xiaobing", ""], ["Jiang", "Yingying", ""], ["Luo", "Zhenbo", ""], ["Liu", "Cheng-Lin", ""], ["Choi", "Hyunsoo", ""], ["Kim", "Sungjin", ""]]}, {"id": "1905.06081", "submitter": "Timur Sokhin", "authors": "Timur Sokhin, Nikolay Butakov, Denis Nasonov", "title": "User profiles matching for different social networks based on faces\n  embeddings", "comments": "Submitted to HAIS 2019 conference", "journal-ref": null, "doi": "10.1007/978-3-030-29859-3_47", "report-no": null, "categories": "cs.CV cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is common practice nowadays to use multiple social networks for different\nsocial roles. Although this, these networks assume differences in content type,\ncommunications and style of speech. If we intend to understand human behaviour\nas a key-feature for recommender systems, banking risk assessments or\nsociological researches, this is better to achieve using a combination of the\ndata from different social media. In this paper, we propose a new approach for\nuser profiles matching across social media based on embeddings of publicly\navailable users' face photos and conduct an experimental study of its\nefficiency. Our approach is stable to changes in content and style for certain\nsocial media.\n", "versions": [{"version": "v1", "created": "Wed, 15 May 2019 10:50:51 GMT"}], "update_date": "2020-05-07", "authors_parsed": [["Sokhin", "Timur", ""], ["Butakov", "Nikolay", ""], ["Nasonov", "Denis", ""]]}, {"id": "1905.06113", "submitter": "Andrey Rudenko", "authors": "Andrey Rudenko, Luigi Palmieri, Michael Herman, Kris M. Kitani, Dariu\n  M. Gavrila and Kai O. Arras", "title": "Human Motion Trajectory Prediction: A Survey", "comments": "Submitted to the International Journal of Robotics Research (IJRR),\n  37 pages", "journal-ref": null, "doi": "10.1177/0278364920917446", "report-no": null, "categories": "cs.RO cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With growing numbers of intelligent autonomous systems in human environments,\nthe ability of such systems to perceive, understand and anticipate human\nbehavior becomes increasingly important. Specifically, predicting future\npositions of dynamic agents and planning considering such predictions are key\ntasks for self-driving vehicles, service robots and advanced surveillance\nsystems. This paper provides a survey of human motion trajectory prediction. We\nreview, analyze and structure a large selection of work from different\ncommunities and propose a taxonomy that categorizes existing methods based on\nthe motion modeling approach and level of contextual information used. We\nprovide an overview of the existing datasets and performance metrics. We\ndiscuss limitations of the state of the art and outline directions for further\nresearch.\n", "versions": [{"version": "v1", "created": "Wed, 15 May 2019 12:09:55 GMT"}, {"version": "v2", "created": "Thu, 13 Jun 2019 11:09:46 GMT"}, {"version": "v3", "created": "Tue, 17 Dec 2019 09:27:25 GMT"}], "update_date": "2020-07-27", "authors_parsed": [["Rudenko", "Andrey", ""], ["Palmieri", "Luigi", ""], ["Herman", "Michael", ""], ["Kitani", "Kris M.", ""], ["Gavrila", "Dariu M.", ""], ["Arras", "Kai O.", ""]]}, {"id": "1905.06139", "submitter": "Fenglin Liu", "authors": "Fenglin Liu, Yuanxin Liu, Xuancheng Ren, Xiaodong He, Xu Sun", "title": "Aligning Visual Regions and Textual Concepts for Semantic-Grounded Image\n  Representations", "comments": "Accepted by NeurIPS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In vision-and-language grounding problems, fine-grained representations of\nthe image are considered to be of paramount importance. Most of the current\nsystems incorporate visual features and textual concepts as a sketch of an\nimage. However, plainly inferred representations are usually undesirable in\nthat they are composed of separate components, the relations of which are\nelusive. In this work, we aim at representing an image with a set of integrated\nvisual regions and corresponding textual concepts, reflecting certain\nsemantics. To this end, we build the Mutual Iterative Attention (MIA) module,\nwhich integrates correlated visual features and textual concepts, respectively,\nby aligning the two modalities. We evaluate the proposed approach on two\nrepresentative vision-and-language grounding tasks, i.e., image captioning and\nvisual question answering. In both tasks, the semantic-grounded image\nrepresentations consistently boost the performance of the baseline models under\nall metrics across the board. The results demonstrate that our approach is\neffective and generalizes well to a wide range of models for image-related\napplications. (The code is available at https://github.com/fenglinliu98/MIA)\n", "versions": [{"version": "v1", "created": "Wed, 15 May 2019 12:39:49 GMT"}, {"version": "v2", "created": "Sun, 26 May 2019 08:10:43 GMT"}, {"version": "v3", "created": "Mon, 4 Nov 2019 17:10:36 GMT"}], "update_date": "2019-11-05", "authors_parsed": [["Liu", "Fenglin", ""], ["Liu", "Yuanxin", ""], ["Ren", "Xuancheng", ""], ["He", "Xiaodong", ""], ["Sun", "Xu", ""]]}, {"id": "1905.06203", "submitter": "Mohammad Mahdi Dehshibi Dr.", "authors": "Mohammad Mahdi Dehshibi, Gerard Pons, Bita Baiani, David Masip", "title": "VICSOM: VIsual Clues from SOcial Media for psychological assessment", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CY cs.SI", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Sharing multimodal information (typically images, videos or text) in Social\nNetwork Sites (SNS) occupies a relevant part of our time. The particular way\nhow users expose themselves in SNS can provide useful information to infer\nhuman behaviors. This paper proposes to use multimodal data gathered from\nInstagram accounts to predict the perceived prototypical needs described in\nGlasser's choice theory. The contribution is two-fold: (i) we provide a large\nmultimodal database from Instagram public profiles (more than 30,000 images and\ntext captions) annotated by expert Psychologists on each perceived behavior\naccording to Glasser's theory, and (ii) we propose to automate the recognition\nof the (unconsciously) perceived needs by the users. Particularly, we propose a\nbaseline using three different feature sets: visual descriptors based on pixel\nimages (SURF and Visual Bag of Words), a high-level descriptor based on the\nautomated scene description using Convolutional Neural Networks, and a\ntext-based descriptor (Word2vec) obtained from processing the captions provided\nby the users. Finally, we propose a multimodal fusion of these descriptors\nobtaining promising results in the multi-label classification problem.\n", "versions": [{"version": "v1", "created": "Wed, 15 May 2019 14:18:01 GMT"}], "update_date": "2019-05-16", "authors_parsed": [["Dehshibi", "Mohammad Mahdi", ""], ["Pons", "Gerard", ""], ["Baiani", "Bita", ""], ["Masip", "David", ""]]}, {"id": "1905.06228", "submitter": "Andreas Thoma", "authors": "Andreas Thoma and Sridhar Ravi", "title": "Significance of parallel computing on the performance of Digital Image\n  Correlation algorithms in MATLAB", "comments": "17 pages, 5 figures, 6 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Digital Image Correlation (DIC) is a powerful tool used to evaluate\ndisplacements and deformations in a non-intrusive manner. By comparing two\nimages, one of the undeformed reference state of a specimen and another of the\ndeformed target state, the relative displacement between those two states is\ndetermined. DIC is well known and often used for post-processing analysis of\nin-plane displacements and deformation of specimen. Increasing the analysis\nspeed to enable real-time DIC analysis will be beneficial and extend the field\nof use of this technique. Here we tested several combinations of the most\ncommon DIC methods in combination with different parallelization approaches in\nMATLAB and evaluated their performance to determine whether real-time analysis\nis possible with these methods. To reflect improvements in computing technology\ndifferent hardware settings were also analysed. We found that implementation\nproblems can reduce the efficiency of a theoretically superior algorithm such\nthat it becomes practically slower than a sub-optimal algorithm. The\nNewton-Raphson algorithm in combination with a modified Particle Swarm\nalgorithm in parallel image computation was found to be most effective. This is\ncontrary to theory, suggesting that the inverse-compositional Gauss-Newton\nalgorithm is superior. As expected, the Brute Force Search algorithm is the\nleast effective method. We also found that the correct choice of\nparallelization tasks is crucial to achieve improvements in computing speed. A\npoorly chosen parallelisation approach with high parallel overhead leads to\ninferior performance. Finally, irrespective of the computing mode the correct\nchoice of combinations of integer-pixel and sub-pixel search algorithms is\ndecisive for an efficient analysis. Using currently available hardware\nreal-time analysis at high framerates remains an aspiration.\n", "versions": [{"version": "v1", "created": "Wed, 15 May 2019 15:05:15 GMT"}], "update_date": "2019-05-16", "authors_parsed": [["Thoma", "Andreas", ""], ["Ravi", "Sridhar", ""]]}, {"id": "1905.06229", "submitter": "Josef Spjut", "authors": "Josef Spjut and Ben Boudaoud and Jonghyun Kim and Trey Greer and\n  Rachel Albert and Michael Stengel and Kaan Aksit and David Luebke", "title": "Toward Standardized Classification of Foveated Displays", "comments": "9 pages, 8 figures, presented at IEEE VR 2020", "journal-ref": "in IEEE Transactions on Visualization and Computer Graphics, vol.\n  26, no. 5, pp. 2126-2134, May 2020", "doi": "10.1109/TVCG.2020.2973053", "report-no": null, "categories": "cs.GR cs.CV cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Emergent in the field of head mounted display design is a desire to leverage\nthe limitations of the human visual system to reduce the computation,\ncommunication, and display workload in power and form-factor constrained\nsystems. Fundamental to this reduced workload is the ability to match display\nresolution to the acuity of the human visual system, along with a resulting\nneed to follow the gaze of the eye as it moves, a process referred to as\nfoveation. A display that moves its content along with the eye may be called a\nFoveated Display, though this term is also commonly used to describe displays\nwith non-uniform resolution that attempt to mimic human visual acuity. We\ntherefore recommend a definition for the term Foveated Display that accepts\nboth of these interpretations. Furthermore, we include a simplified model for\nhuman visual Acuity Distribution Functions (ADFs) at various levels of visual\nacuity, across wide fields of view and propose comparison of this ADF with the\nResolution Distribution Function of a foveated display for evaluation of its\nresolution at a particular gaze direction. We also provide a taxonomy to allow\nthe field to meaningfully compare and contrast various aspects of foveated\ndisplays in a display and optical technology-agnostic manner.\n", "versions": [{"version": "v1", "created": "Fri, 3 May 2019 20:45:05 GMT"}, {"version": "v2", "created": "Thu, 2 Jul 2020 16:14:09 GMT"}], "update_date": "2020-07-03", "authors_parsed": [["Spjut", "Josef", ""], ["Boudaoud", "Ben", ""], ["Kim", "Jonghyun", ""], ["Greer", "Trey", ""], ["Albert", "Rachel", ""], ["Stengel", "Michael", ""], ["Aksit", "Kaan", ""], ["Luebke", "David", ""]]}, {"id": "1905.06231", "submitter": "Martin Garbade", "authors": "Yueh-Tung Chen, Martin Garbade, Juergen Gall", "title": "3D Semantic Scene Completion from a Single Depth Image using Adversarial\n  Training", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the task of 3D semantic scene completion, i.e. , given a single\ndepth image, we predict the semantic labels and occupancy of voxels in a 3D\ngrid representing the scene. In light of the recently introduced generative\nadversarial networks (GAN), our goal is to explore the potential of this model\nand the efficiency of various important design choices. Our results show that\nusing conditional GANs outperforms the vanilla GAN setup. We evaluate these\narchitecture designs on several datasets. Based on our experiments, we\ndemonstrate that GANs are able to outperform the performance of a baseline 3D\nCNN in case of clean annotations, but they suffer from poorly aligned\nannotations.\n", "versions": [{"version": "v1", "created": "Wed, 15 May 2019 15:12:41 GMT"}], "update_date": "2019-05-16", "authors_parsed": [["Chen", "Yueh-Tung", ""], ["Garbade", "Martin", ""], ["Gall", "Juergen", ""]]}, {"id": "1905.06242", "submitter": "Rodrigo Berriel", "authors": "Rodrigo Berriel, St\\'ephane Lathuili\\`ere, Moin Nabi, Tassilo Klein,\n  Thiago Oliveira-Santos, Nicu Sebe, Elisa Ricci", "title": "Budget-Aware Adapters for Multi-Domain Learning", "comments": "ICCV 2019", "journal-ref": null, "doi": "10.1109/ICCV.2019.00047", "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-Domain Learning (MDL) refers to the problem of learning a set of models\nderived from a common deep architecture, each one specialized to perform a task\nin a certain domain (e.g., photos, sketches, paintings). This paper tackles MDL\nwith a particular interest in obtaining domain-specific models with an\nadjustable budget in terms of the number of network parameters and\ncomputational complexity. Our intuition is that, as in real applications the\nnumber of domains and tasks can be very large, an effective MDL approach should\nnot only focus on accuracy but also on having as few parameters as possible. To\nimplement this idea we derive specialized deep models for each domain by\nadapting a pre-trained architecture but, differently from other methods, we\npropose a novel strategy to automatically adjust the computational complexity\nof the network. To this aim, we introduce Budget-Aware Adapters that select the\nmost relevant feature channels to better handle data from a novel domain. Some\nconstraints on the number of active switches are imposed in order to obtain a\nnetwork respecting the desired complexity budget. Experimentally, we show that\nour approach leads to recognition accuracy competitive with state-of-the-art\napproaches but with much lighter networks both in terms of storage and\ncomputation.\n", "versions": [{"version": "v1", "created": "Wed, 15 May 2019 15:25:04 GMT"}, {"version": "v2", "created": "Tue, 13 Aug 2019 12:38:44 GMT"}, {"version": "v3", "created": "Tue, 8 Dec 2020 14:18:06 GMT"}], "update_date": "2020-12-09", "authors_parsed": [["Berriel", "Rodrigo", ""], ["Lathuili\u00e8re", "St\u00e9phane", ""], ["Nabi", "Moin", ""], ["Klein", "Tassilo", ""], ["Oliveira-Santos", "Thiago", ""], ["Sebe", "Nicu", ""], ["Ricci", "Elisa", ""]]}, {"id": "1905.06252", "submitter": "Giovanni Iacca Dr.", "authors": "Cristiano Saltori, Subhankar Roy, Nicu Sebe, Giovanni Iacca", "title": "Regularized Evolutionary Algorithm for Dynamic Neural Topology Search", "comments": null, "journal-ref": null, "doi": "10.1007/978-3-030-30642-7_20", "report-no": null, "categories": "cs.NE cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Designing neural networks for object recognition requires considerable\narchitecture engineering. As a remedy, neuro-evolutionary network architecture\nsearch, which automatically searches for optimal network architectures using\nevolutionary algorithms, has recently become very popular. Although very\neffective, evolutionary algorithms rely heavily on having a large population of\nindividuals (i.e., network architectures) and is therefore memory expensive. In\nthis work, we propose a Regularized Evolutionary Algorithm with low memory\nfootprint to evolve a dynamic image classifier. In details, we introduce novel\ncustom operators that regularize the evolutionary process of a micro-population\nof 10 individuals. We conduct experiments on three different digits datasets\n(MNIST, USPS, SVHN) and show that our evolutionary method obtains competitive\nresults with the current state-of-the-art.\n", "versions": [{"version": "v1", "created": "Wed, 15 May 2019 15:36:56 GMT"}, {"version": "v2", "created": "Mon, 19 Aug 2019 08:49:44 GMT"}], "update_date": "2020-12-21", "authors_parsed": [["Saltori", "Cristiano", ""], ["Roy", "Subhankar", ""], ["Sebe", "Nicu", ""], ["Iacca", "Giovanni", ""]]}, {"id": "1905.06292", "submitter": "Junseok Kwon", "authors": "Dong Wook Shu and Sung Woo Park and Junseok Kwon", "title": "3D Point Cloud Generative Adversarial Network Based on Tree Structured\n  Graph Convolutions", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a novel generative adversarial network (GAN) for 3D\npoint clouds generation, which is called tree-GAN. To achieve state-of-the-art\nperformance for multi-class 3D point cloud generation, a tree-structured graph\nconvolution network (TreeGCN) is introduced as a generator for tree-GAN.\nBecause TreeGCN performs graph convolutions within a tree, it can use ancestor\ninformation to boost the representation power for features. To evaluate GANs\nfor 3D point clouds accurately, we develop a novel evaluation metric called\nFrechet point cloud distance (FPD). Experimental results demonstrate that the\nproposed tree-GAN outperforms state-of-the-art GANs in terms of both\nconventional metrics and FPD, and can generate point clouds for different\nsemantic parts without prior knowledge.\n", "versions": [{"version": "v1", "created": "Wed, 15 May 2019 16:51:18 GMT"}, {"version": "v2", "created": "Thu, 16 May 2019 02:26:59 GMT"}], "update_date": "2019-05-17", "authors_parsed": [["Shu", "Dong Wook", ""], ["Park", "Sung Woo", ""], ["Kwon", "Junseok", ""]]}, {"id": "1905.06312", "submitter": "Ziyuan Zhao", "authors": "Ziyuan Zhao, Kerui Zhang, Xuejie Hao, Jing Tian, Matthew Chin Heng\n  Chua, Li Chen, Xin Xu", "title": "BiRA-Net: Bilinear Attention Net for Diabetic Retinopathy Grading", "comments": "Accepted at ICIP 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Diabetic retinopathy (DR) is a common retinal disease that leads to\nblindness. For diagnosis purposes, DR image grading aims to provide automatic\nDR grade classification, which is not addressed in conventional research\nmethods of binary DR image classification. Small objects in the eye images,\nlike lesions and microaneurysms, are essential to DR grading in medical\nimaging, but they could easily be influenced by other objects. To address these\nchallenges, we propose a new deep learning architecture, called BiRA-Net, which\ncombines the attention model for feature extraction and bilinear model for\nfine-grained classification. Furthermore, in considering the distance between\ndifferent grades of different DR categories, we propose a new loss function,\ncalled grading loss, which leads to improved training convergence of the\nproposed approach. Experimental results are provided to demonstrate the\nsuperior performance of the proposed approach.\n", "versions": [{"version": "v1", "created": "Wed, 15 May 2019 17:38:52 GMT"}, {"version": "v2", "created": "Mon, 1 Jul 2019 04:48:50 GMT"}], "update_date": "2019-07-02", "authors_parsed": [["Zhao", "Ziyuan", ""], ["Zhang", "Kerui", ""], ["Hao", "Xuejie", ""], ["Tian", "Jing", ""], ["Chua", "Matthew Chin Heng", ""], ["Chen", "Li", ""], ["Xu", "Xin", ""]]}, {"id": "1905.06326", "submitter": "Xuaner Cecilia Zhang", "authors": "Xuaner Zhang, Kevin Matzen, Vivien Nguyen, Dillon Yao, You Zhang, Ren\n  Ng", "title": "Synthetic Defocus and Look-Ahead Autofocus for Casual Videography", "comments": "(V2 author name corrected) SIGGRAPH 2019; project website:\n  https://ceciliavision.github.io/vid-auto-focus/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In cinema, large camera lenses create beautiful shallow depth of field (DOF),\nbut make focusing difficult and expensive. Accurate cinema focus usually relies\non a script and a person to control focus in realtime. Casual videographers\noften crave cinematic focus, but fail to achieve it. We either sacrifice\nshallow DOF, as in smartphone videos; or we struggle to deliver accurate focus,\nas in videos from larger cameras. This paper is about a new approach in the\npursuit of cinematic focus for casual videography. We present a system that\nsynthetically renders refocusable video from a deep DOF video shot with a\nsmartphone, and analyzes future video frames to deliver context-aware autofocus\nfor the current frame. To create refocusable video, we extend recent machine\nlearning methods designed for still photography, contributing a new dataset for\nmachine training, a rendering model better suited to cinema focus, and a\nfiltering solution for temporal coherence. To choose focus accurately for each\nframe, we demonstrate autofocus that looks at upcoming video frames and applies\nAI-assist modules such as motion, face, audio and saliency detection. We also\nshow that autofocus benefits from machine learning and a large-scale video\ndataset with focus annotation, where we use our RVR-LAAF GUI to create this\nsizable dataset efficiently. We deliver, for example, a shallow DOF video where\nthe autofocus transitions onto each person before she begins to speak. This is\nimpossible for conventional camera autofocus because it would require seeing\ninto the future.\n", "versions": [{"version": "v1", "created": "Wed, 15 May 2019 17:59:05 GMT"}, {"version": "v2", "created": "Fri, 17 May 2019 08:09:48 GMT"}, {"version": "v3", "created": "Tue, 21 May 2019 13:26:02 GMT"}], "update_date": "2019-05-22", "authors_parsed": [["Zhang", "Xuaner", ""], ["Matzen", "Kevin", ""], ["Nguyen", "Vivien", ""], ["Yao", "Dillon", ""], ["Zhang", "You", ""], ["Ng", "Ren", ""]]}, {"id": "1905.06330", "submitter": "Weimin Zhou", "authors": "Weimin Zhou, Hua Li, Mark A. Anastasio", "title": "Approximating the Ideal Observer and Hotelling Observer for binary\n  signal detection tasks by use of supervised learning methods", "comments": "IEEE Transactions on Medical Imaging (Early Access), 2019", "journal-ref": null, "doi": "10.1109/TMI.2019.2911211", "report-no": null, "categories": "eess.SP cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is widely accepted that optimization of medical imaging system performance\nshould be guided by task-based measures of image quality (IQ). Task-based\nmeasures of IQ quantify the ability of an observer to perform a specific task\nsuch as detection or estimation of a signal (e.g., a tumor). For binary signal\ndetection tasks, the Bayesian Ideal Observer (IO) sets an upper limit of\nobserver performance and has been advocated for use in optimizing medical\nimaging systems and data-acquisition designs. Except in special cases,\ndetermination of the IO test statistic is analytically intractable.\nMarkov-chain Monte Carlo (MCMC) techniques can be employed to approximate IO\ndetection performance, but their reported applications have been limited to\nrelatively simple object models. In cases where the IO test statistic is\ndifficult to compute, the Hotelling Observer (HO) can be employed. To compute\nthe HO test statistic, potentially large covariance matrices must be accurately\nestimated and subsequently inverted, which can present computational\nchallenges. This work investigates supervised learning-based methodologies for\napproximating the IO and HO test statistics. Convolutional neural networks\n(CNNs) and single-layer neural networks (SLNNs) are employed to approximate the\nIO and HO test statistics, respectively. Numerical simulations were conducted\nfor both signal-known-exactly (SKE) and signal-known-statistically (SKS) signal\ndetection tasks. The performances of the supervised learning methods are\nassessed via receiver operating characteristic (ROC) analysis and the results\nare compared to those produced by use of traditional numerical methods or\nanalytical calculations when feasible. The potential advantages of the proposed\nsupervised learning approaches for approximating the IO and HO test statistics\nare discussed.\n", "versions": [{"version": "v1", "created": "Wed, 15 May 2019 03:19:29 GMT"}], "update_date": "2019-05-17", "authors_parsed": [["Zhou", "Weimin", ""], ["Li", "Hua", ""], ["Anastasio", "Mark A.", ""]]}, {"id": "1905.06358", "submitter": "Oriane Sim\\'eoni", "authors": "Oriane Sim\\'eoni, Yannis Avrithis, Ondrej Chum", "title": "Local Features and Visual Words Emerge in Activations", "comments": null, "journal-ref": "CVPR 2019", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel method of deep spatial matching (DSM) for image retrieval.\nInitial ranking is based on image descriptors extracted from convolutional\nneural network activations by global pooling, as in recent state-of-the-art\nwork. However, the same sparse 3D activation tensor is also approximated by a\ncollection of local features. These local features are then robustly matched to\napproximate the optimal alignment of the tensors. This happens without any\nnetwork modification, additional layers or training. No local feature detection\nhappens on the original image. No local feature descriptors and no visual\nvocabulary are needed throughout the whole process.\n  We experimentally show that the proposed method achieves the state-of-the-art\nperformance on standard benchmarks across different network architectures and\ndifferent global pooling methods. The highest gain in performance is achieved\nwhen diffusion on the nearest-neighbor graph of global descriptors is initiated\nfrom spatially verified images.\n", "versions": [{"version": "v1", "created": "Wed, 15 May 2019 18:04:10 GMT"}], "update_date": "2019-11-19", "authors_parsed": [["Sim\u00e9oni", "Oriane", ""], ["Avrithis", "Yannis", ""], ["Chum", "Ondrej", ""]]}, {"id": "1905.06362", "submitter": "Sebastian Guendel", "authors": "Sebastian Guendel, Florin C. Ghesu, Sasa Grbic, Eli Gibson, Bogdan\n  Georgescu, Andreas Maier, Dorin Comaniciu", "title": "Multi-task Learning for Chest X-ray Abnormality Classification on Noisy\n  Labels", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Chest X-ray (CXR) is the most common X-ray examination performed in daily\nclinical practice for the diagnosis of various heart and lung abnormalities.\nThe large amount of data to be read and reported, with 100+ studies per day for\na single radiologist, poses a challenge in maintaining consistently high\ninterpretation accuracy. In this work, we propose a method for the\nclassification of different abnormalities based on CXR scans of the human body.\nThe system is based on a novel multi-task deep learning architecture that in\naddition to the abnormality classification, supports the segmentation of the\nlungs and heart and classification of regions where the abnormality is located.\nWe demonstrate that by training these tasks concurrently, one can increase the\nclassification performance of the model. Experiments were performed on an\nextensive collection of 297,541 chest X-ray images from 86,876 patients,\nleading to a state-of-the-art performance level of 0.883 AUC on average for 12\ndifferent abnormalities. We also conducted a detailed performance analysis and\ncompared the accuracy of our system with 3 board-certified radiologists. In\nthis context, we highlight the high level of label noise inherent to this\nproblem. On a reduced subset containing only cases with high confidence\nreference labels based on the consensus of the 3 radiologists, our system\nreached an average AUC of 0.945.\n", "versions": [{"version": "v1", "created": "Wed, 15 May 2019 18:09:40 GMT"}], "update_date": "2019-05-17", "authors_parsed": [["Guendel", "Sebastian", ""], ["Ghesu", "Florin C.", ""], ["Grbic", "Sasa", ""], ["Gibson", "Eli", ""], ["Georgescu", "Bogdan", ""], ["Maier", "Andreas", ""], ["Comaniciu", "Dorin", ""]]}, {"id": "1905.06368", "submitter": "Wuyang Chen", "authors": "Wuyang Chen, Ziyu Jiang, Zhangyang Wang, Kexin Cui, Xiaoning Qian", "title": "Collaborative Global-Local Networks for Memory-Efficient Segmentation of\n  Ultra-High Resolution Images", "comments": "CVPR2019 oral", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Segmentation of ultra-high resolution images is increasingly demanded, yet\nposes significant challenges for algorithm efficiency, in particular\nconsidering the (GPU) memory limits. Current approaches either downsample an\nultra-high resolution image or crop it into small patches for separate\nprocessing. In either way, the loss of local fine details or global contextual\ninformation results in limited segmentation accuracy. We propose collaborative\nGlobal-Local Networks (GLNet) to effectively preserve both global and local\ninformation in a highly memory-efficient manner. GLNet is composed of a global\nbranch and a local branch, taking the downsampled entire image and its cropped\nlocal patches as respective inputs. For segmentation, GLNet deeply fuses\nfeature maps from two branches, capturing both the high-resolution fine\nstructures from zoomed-in local patches and the contextual dependency from the\ndownsampled input. To further resolve the potential class imbalance problem\nbetween background and foreground regions, we present a coarse-to-fine variant\nof GLNet, also being memory-efficient. Extensive experiments and analyses have\nbeen performed on three real-world ultra-high aerial and medical image datasets\n(resolution up to 30 million pixels). With only one single 1080Ti GPU and less\nthan 2GB memory used, our GLNet yields high-quality segmentation results and\nachieves much more competitive accuracy-memory usage trade-offs compared to\nstate-of-the-arts.\n", "versions": [{"version": "v1", "created": "Wed, 15 May 2019 18:22:06 GMT"}, {"version": "v2", "created": "Thu, 24 Oct 2019 05:20:57 GMT"}, {"version": "v3", "created": "Wed, 3 Mar 2021 17:35:25 GMT"}], "update_date": "2021-03-04", "authors_parsed": [["Chen", "Wuyang", ""], ["Jiang", "Ziyu", ""], ["Wang", "Zhangyang", ""], ["Cui", "Kexin", ""], ["Qian", "Xiaoning", ""]]}, {"id": "1905.06372", "submitter": "Hila Barel", "authors": "Shay Maymon, Hila Barel", "title": "Contrast Optimization And Local Adaptation (COALA) for HDR Compression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper develops a novel approach for high dynamic-range compression. It\nrelies on the widely accepted assumption that the human visual system is not\nvery sensitive to absolute luminance reaching the retina, but rather responds\nto relative luminance ratios. Dynamic-range compression is then formulated as a\nregularized optimization in which the image dynamic range is reduced while the\nlocal contrast of the original scene is preserved. Our method is shown to be\ncapable of drastic dynamic-range compression, while preserving fine details and\navoiding common artifacts such as halos, gradient reversals, or loss of local\ncontrast.\n", "versions": [{"version": "v1", "created": "Wed, 15 May 2019 18:22:54 GMT"}], "update_date": "2019-05-17", "authors_parsed": [["Maymon", "Shay", ""], ["Barel", "Hila", ""]]}, {"id": "1905.06381", "submitter": "Hui-Lee Ooi", "authors": "Hui-Lee Ooi, Guillaume-Alexandre Bilodeau, and Nicolas Saunier", "title": "Tracking in Urban Traffic Scenes from Background Subtraction and Object\n  Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose to combine detections from background subtraction\nand from a multiclass object detector for multiple object tracking (MOT) in\nurban traffic scenes. These objects are associated across frames using spatial,\ncolour and class label information, and trajectory prediction is evaluated to\nyield the final MOT outputs. The proposed method was tested on the Urban\ntracker dataset and shows competitive performances compared to state-of-the-art\napproaches. Results show that the integration of different detection inputs\nremains a challenging task that greatly affects the MOT performance.\n", "versions": [{"version": "v1", "created": "Wed, 15 May 2019 18:46:01 GMT"}], "update_date": "2019-05-17", "authors_parsed": [["Ooi", "Hui-Lee", ""], ["Bilodeau", "Guillaume-Alexandre", ""], ["Saunier", "Nicolas", ""]]}, {"id": "1905.06435", "submitter": "Simeon Spasov Mr", "authors": "Simeon E. Spasov and Pietro Lio", "title": "Dynamic Neural Network Channel Execution for Efficient Training", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing methods for reducing the computational burden of neural networks at\nrun-time, such as parameter pruning or dynamic computational path selection,\nfocus solely on improving computational efficiency during inference. On the\nother hand, in this work, we propose a novel method which reduces the memory\nfootprint and number of computing operations required for training and\ninference. Our framework efficiently integrates pruning as part of the training\nprocedure by exploring and tracking the relative importance of convolutional\nchannels. At each training step, we select only a subset of highly salient\nchannels to execute according to the combinatorial upper confidence bound\nalgorithm, and run a forward and backward pass only on these activated\nchannels, hence learning their parameters. Consequently, we enable the\nefficient discovery of compact models. We validate our approach empirically on\nstate-of-the-art CNNs - VGGNet, ResNet and DenseNet, and on several image\nclassification datasets. Results demonstrate our framework for dynamic channel\nexecution reduces computational cost up to 4x and parameter count up to 9x,\nthus reducing the memory and computational demands for discovering and training\ncompact neural network models.\n", "versions": [{"version": "v1", "created": "Wed, 15 May 2019 21:10:28 GMT"}], "update_date": "2019-05-17", "authors_parsed": [["Spasov", "Simeon E.", ""], ["Lio", "Pietro", ""]]}, {"id": "1905.06442", "submitter": "Mohammadhassan Izadyyazdanabadi", "authors": "Mohammadhassan Izadyyazdanabadi, Evgenii Belykh, Xiaochun Zhao,\n  Leandro Borba Moreira, Sirin Gandhi, Claudio Cavallo, Jennifer Eschbacher,\n  Peter Nakaji, Mark C. Preul, and Yezhou Yang", "title": "Fluorescence Image Histology Pattern Transformation using Image Style\n  Transfer", "comments": "Submitted to Frontiers in Oncology", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Confocal laser endomicroscopy (CLE) allow on-the-fly in vivo intraoperative\nimaging in a discreet field of view, especially for brain tumors, rather than\nextracting tissue for examination ex vivo with conventional light microscopy.\nFluorescein sodium-driven CLE imaging is more interactive, rapid, and portable\nthan conventional hematoxylin and eosin (H&E)-staining. However, it has several\nlimitations: CLE images may be contaminated with artifacts (motion, red blood\ncells, noise), and neuropathologists are mainly trained on colorful stained\nhistology slides like H&E while the CLE images are gray. To improve the\ndiagnostic quality of CLE, we used a micrograph of an H&E slide from a glioma\ntumor biopsy and image style transfer, a neural network method for integrating\nthe content and style of two images. This was done through minimizing the\ndeviation of the target image from both the content (CLE) and style (H&E)\nimages. The style transferred images were assessed and compared to conventional\nH&E histology by neurosurgeons and a neuropathologist who then validated the\nquality enhancement in 100 pairs of original and transformed images. Average\nreviewers' score on test images showed 84 out of 100 transformed images had\nfewer artifacts and more noticeable critical structures compared to their\noriginal CLE form. By providing images that are more interpretable than the\noriginal CLE images and more rapidly acquired than H&E slides, the style\ntransfer method allows a real-time, cellular-level tissue examination using CLE\ntechnology that closely resembles the conventional appearance of H&E staining\nand may yield better diagnostic recognition than original CLE grayscale images.\n", "versions": [{"version": "v1", "created": "Wed, 15 May 2019 21:33:07 GMT"}], "update_date": "2019-05-17", "authors_parsed": [["Izadyyazdanabadi", "Mohammadhassan", ""], ["Belykh", "Evgenii", ""], ["Zhao", "Xiaochun", ""], ["Moreira", "Leandro Borba", ""], ["Gandhi", "Sirin", ""], ["Cavallo", "Claudio", ""], ["Eschbacher", "Jennifer", ""], ["Nakaji", "Peter", ""], ["Preul", "Mark C.", ""], ["Yang", "Yezhou", ""]]}, {"id": "1905.06457", "submitter": "Emily Evans", "authors": "Emily Evans and Marissa Graham", "title": "An interdisciplinary survey of network similarity methods", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.CV q-bio.MN", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Comparative graph and network analysis play an important role in both systems\nbiology and pattern recognition, but existing surveys on the topic have\nhistorically ignored or underserved one or the other of these fields. We\npresent an integrative introduction to the key objectives and methods of graph\nand network comparison in each field, with the intent of remaining accessible\nto relative novices in order to mitigate the barrier to interdisciplinary idea\ncrossover.\n  To guide our investigation, and to quantitatively justify our assertions\nabout what the key objectives and methods of each field are, we have\nconstructed a citation network containing 5,793 vertices from the full\nreference lists of over two hundred relevant papers, which we collected by\nsearching Google Scholar for ten different network comparison-related search\nterms. We investigate its basic statistics and community structure, and frame\nour presentation around the papers found to have high importance according to\nfive different standard centrality measures.\n", "versions": [{"version": "v1", "created": "Wed, 15 May 2019 22:16:48 GMT"}], "update_date": "2019-05-17", "authors_parsed": [["Evans", "Emily", ""], ["Graham", "Marissa", ""]]}, {"id": "1905.06458", "submitter": "Zhigang Jia", "authors": "Xiao Chen, Zhi-Gang Jia, Yunfeng Cai, Mei-Xiang Zhao", "title": "Relaxed 2-D Principal Component Analysis by $L_p$ Norm for Face\n  Recognition", "comments": "19 pages, 11 figures", "journal-ref": "In: Huang DS., Bevilacqua V., Premaratne P. (eds) Intelligent\n  Computing Theories and Application. ICIC 2019. Lecture Notes in Computer\n  Science, vol 11643. Springer, Cham", "doi": "10.1007/978-3-030-26763-6_19", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A relaxed two dimensional principal component analysis (R2DPCA) approach is\nproposed for face recognition. Different to the 2DPCA, 2DPCA-$L_1$ and G2DPCA,\nthe R2DPCA utilizes the label information (if known) of training samples to\ncalculate a relaxation vector and presents a weight to each subset of training\ndata. A new relaxed scatter matrix is defined and the computed projection axes\nare able to increase the accuracy of face recognition. The optimal $L_p$-norms\nare selected in a reasonable range. Numerical experiments on practical face\ndatabased indicate that the R2DPCA has high generalization ability and can\nachieve a higher recognition rate than state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Wed, 15 May 2019 22:23:09 GMT"}], "update_date": "2020-10-06", "authors_parsed": [["Chen", "Xiao", ""], ["Jia", "Zhi-Gang", ""], ["Cai", "Yunfeng", ""], ["Zhao", "Mei-Xiang", ""]]}, {"id": "1905.06464", "submitter": "Jasper Sebastiaan Wijnands", "authors": "Jasper S. Wijnands, Kerry A. Nice, Jason Thompson, Haifeng Zhao, Mark\n  Stevenson", "title": "Streetscape augmentation using generative adversarial networks: insights\n  related to health and wellbeing", "comments": "20 pages, 8 figures. Preprint accepted for publication in Sustainable\n  Cities and Society", "journal-ref": null, "doi": "10.1016/j.scs.2019.101602", "report-no": null, "categories": "cs.CY cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning using neural networks has provided advances in image style\ntransfer, merging the content of one image (e.g., a photo) with the style of\nanother (e.g., a painting). Our research shows this concept can be extended to\nanalyse the design of streetscapes in relation to health and wellbeing\noutcomes. An Australian population health survey (n=34,000) was used to\nidentify the spatial distribution of health and wellbeing outcomes, including\ngeneral health and social capital. For each outcome, the most and least\ndesirable locations formed two domains. Streetscape design was sampled using\naround 80,000 Google Street View images per domain. Generative adversarial\nnetworks translated these images from one domain to the other, preserving the\nmain structure of the input image, but transforming the `style' from locations\nwhere self-reported health was bad to locations where it was good. These\ntranslations indicate that areas in Melbourne with good general health are\ncharacterised by sufficient green space and compactness of the urban\nenvironment, whilst streetscape imagery related to high social capital\ncontained more and wider footpaths, fewer fences and more grass. Beyond\nidentifying relationships, the method is a first step towards\ncomputer-generated design interventions that have the potential to improve\npopulation health and wellbeing.\n", "versions": [{"version": "v1", "created": "Tue, 14 May 2019 03:13:15 GMT"}], "update_date": "2019-05-17", "authors_parsed": [["Wijnands", "Jasper S.", ""], ["Nice", "Kerry A.", ""], ["Thompson", "Jason", ""], ["Zhao", "Haifeng", ""], ["Stevenson", "Mark", ""]]}, {"id": "1905.06484", "submitter": "Wenyuan Li", "authors": "Wenyuan Li, Zichen Wang, Jiayun Li, Jennifer Polson, William Speier,\n  Corey Arnold", "title": "Semi-supervised learning based on generative adversarial network: a\n  comparison between good GAN and bad GAN approach", "comments": "This paper appears at CVPR 2019 Weakly Supervised Learning for\n  Real-World Computer Vision Applications (LID) Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, semi-supervised learning methods based on generative adversarial\nnetworks (GANs) have received much attention. Among them, two distinct\napproaches have achieved competitive results on a variety of benchmark\ndatasets. Bad GAN learns a classifier with unrealistic samples distributed on\nthe complement of the support of the input data. Conversely, Triple GAN\nconsists of a three-player game that tries to leverage good generated samples\nto boost classification results. In this paper, we perform a comprehensive\ncomparison of these two approaches on different benchmark datasets. We\ndemonstrate their different properties on image generation, and sensitivity to\nthe amount of labeled data provided. By comprehensively comparing these two\nmethods, we hope to shed light on the future of GAN-based semi-supervised\nlearning.\n", "versions": [{"version": "v1", "created": "Thu, 16 May 2019 00:43:47 GMT"}, {"version": "v2", "created": "Fri, 17 May 2019 01:53:45 GMT"}], "update_date": "2019-05-20", "authors_parsed": [["Li", "Wenyuan", ""], ["Wang", "Zichen", ""], ["Li", "Jiayun", ""], ["Polson", "Jennifer", ""], ["Speier", "William", ""], ["Arnold", "Corey", ""]]}, {"id": "1905.06498", "submitter": "Chengcheng Li", "authors": "Chengcheng Li, Zi Wang, Dali Wang, Xiangyang Wang, Hairong Qi", "title": "Investigating Channel Pruning through Structural Redundancy Reduction --\n  A Statistical Study", "comments": "2019 ICML Workshop, Joint Workshop on On-Device Machine Learning &\n  Compact Deep Neural Network Representations (ODML-CDNNR)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most existing channel pruning methods formulate the pruning task from a\nperspective of inefficiency reduction which iteratively rank and remove the\nleast important filters, or find the set of filters that minimizes some\nreconstruction errors after pruning. In this work, we investigate the channel\npruning from a new perspective with statistical modeling. We hypothesize that\nthe number of filters at a certain layer reflects the level of 'redundancy' in\nthat layer and thus formulate the pruning problem from the aspect of redundancy\nreduction. Based on both theoretic analysis and empirical studies, we make an\nimportant discovery: randomly pruning filters from layers of high redundancy\noutperforms pruning the least important filters across all layers based on the\nstate-of-the-art ranking criterion. These results advance our understanding of\npruning and further testify to the recent findings that the structure of the\npruned model plays a key role in the network efficiency as compared to\ninherited weights.\n", "versions": [{"version": "v1", "created": "Thu, 16 May 2019 02:10:05 GMT"}, {"version": "v2", "created": "Sun, 19 May 2019 17:29:43 GMT"}, {"version": "v3", "created": "Tue, 16 Jul 2019 16:41:15 GMT"}], "update_date": "2019-07-17", "authors_parsed": [["Li", "Chengcheng", ""], ["Wang", "Zi", ""], ["Wang", "Dali", ""], ["Wang", "Xiangyang", ""], ["Qi", "Hairong", ""]]}, {"id": "1905.06499", "submitter": "Chi Zhang", "authors": "Chi Zhang, Yuehu Liu, Ying Wu, Qilin Zhang, Le Wang", "title": "Bimodal Stereo: Joint Shape and Pose Estimation from Color-Depth Image\n  Pair", "comments": "Preprinted version on May 15, 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mutual calibration between color and depth cameras is a challenging topic in\nmulti-modal data registration. In this paper, we are confronted with a \"Bimodal\nStereo\" problem, which aims to solve camera pose from a pair of an uncalibrated\ncolor image and a depth map from different views automatically. To address this\nproblem, an iterative Shape-from-Shading (SfS) based framework is proposed to\nestimate shape and pose simultaneously. In the pipeline, the estimated shape is\nrefined by the shape prior from the given depth map under the estimated pose.\nMeanwhile, the estimated pose is improved by the registration of estimated\nshape and shape from given depth map. We also introduce a shading based\nrefinement in the pipeline to address noisy depth map with holes. Extensive\nexperiments showed that through our method, both the depth map, the recovered\nshape as well as its pose can be desirably refined and recovered.\n", "versions": [{"version": "v1", "created": "Thu, 16 May 2019 02:15:01 GMT"}], "update_date": "2019-05-17", "authors_parsed": [["Zhang", "Chi", ""], ["Liu", "Yuehu", ""], ["Wu", "Ying", ""], ["Zhang", "Qilin", ""], ["Wang", "Le", ""]]}, {"id": "1905.06505", "submitter": "Yao Luo", "authors": "Yao Luo, Xiaoguang Tu, Mei Xie", "title": "Learning Robust 3D Face Reconstruction and Discriminative Identity\n  Representation", "comments": "5 pages, 6 figures, IEEE International Conference on Information\n  Communication and Signal Processing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  3D face reconstruction from a single 2D image is a very important topic in\ncomputer vision. However, the current reconstruction methods are usually\nnon-sensitive to face identities and over-sensitive to facial poses, which may\nresult in similar 3D geometries for faces of different identities, or obtain\ndifferent shapes for the same identity with different poses. When such methods\nare applied practically, their 3D estimates are either changeable for different\nphotos of the same subject or over-regularized and generic to distinguish face\nidentities. In this paper, we propose a robust solution to solve this problem\nby carefully designing a novel Siamese Convolutional Neural Network (SCNN).\nSpecifically, regarding the 3D Morphable face Model (3DMM) parameters of the\nsame individual as the same class, we employ the contrastive loss to enlarge\nthe inter-class distance and meanwhile reduce the intra-class distance for the\noutput 3DMM parameters. We also propose an identity loss to preserve the\nidentity information for the same individual in the feature space. Training\nwith these two losses, our SCNN could learn representations that are more\ndiscriminative for face identity and generalizable for pose variants.\nExperiments on the challenging database 300W-LP and AFLW2000-3D have shown the\neffectiveness of our method by comparing with state-of-the-arts.\n", "versions": [{"version": "v1", "created": "Thu, 16 May 2019 02:31:34 GMT"}], "update_date": "2019-05-17", "authors_parsed": [["Luo", "Yao", ""], ["Tu", "Xiaoguang", ""], ["Xie", "Mei", ""]]}, {"id": "1905.06509", "submitter": "Tae Joon Jun", "authors": "Tae Joon Jun, Youngsub Eom, Dohyeun Kim, Cherry Kim, Ji-Hye Park,\n  Hoang Minh Nguyen, Daeyoung Kim", "title": "TRk-CNN: Transferable Ranking-CNN for image classification of glaucoma,\n  glaucoma suspect, and normal eyes", "comments": "49 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we proposed Transferable Ranking Convolutional Neural Network\n(TRk-CNN) that can be effectively applied when the classes of images to be\nclassified show a high correlation with each other. The multi-class\nclassification method based on the softmax function, which is generally used,\nis not effective in this case because the inter-class relationship is ignored.\nAlthough there is a Ranking-CNN that takes into account the ordinal classes, it\ncannot reflect the inter-class relationship to the final prediction. TRk-CNN,\non the other hand, combines the weights of the primitive classification model\nto reflect the inter-class information to the final classification phase. We\nevaluated TRk-CNN in glaucoma image dataset that was labeled into three\nclasses: normal, glaucoma suspect, and glaucoma eyes. Based on the literature\nwe surveyed, this study is the first to classify three status of glaucoma\nfundus image dataset into three different classes. We compared the evaluation\nresults of TRk-CNN with Ranking-CNN (Rk-CNN) and multi-class CNN (MC-CNN) using\nthe DenseNet as the backbone CNN model. As a result, TRk-CNN achieved an\naverage accuracy of 92.96%, specificity of 93.33%, sensitivity for glaucoma\nsuspect of 95.12% and sensitivity for glaucoma of 93.98%. Based on average\naccuracy, TRk-CNN is 8.04% and 9.54% higher than Rk-CNN and MC-CNN and\nsurprisingly 26.83% higher for sensitivity for suspicious than multi-class CNN.\nOur TRk-CNN is expected to be effectively applied to the medical image\nclassification problem where the disease state is continuous and increases in\nthe positive class direction.\n", "versions": [{"version": "v1", "created": "Thu, 16 May 2019 02:45:04 GMT"}], "update_date": "2019-05-17", "authors_parsed": [["Jun", "Tae Joon", ""], ["Eom", "Youngsub", ""], ["Kim", "Dohyeun", ""], ["Kim", "Cherry", ""], ["Park", "Ji-Hye", ""], ["Nguyen", "Hoang Minh", ""], ["Kim", "Daeyoung", ""]]}, {"id": "1905.06514", "submitter": "Ziqiang Zheng", "authors": "Ziqiang Zheng, Yang Wu, Zhibin Yu, Yang Yang, Haiyong Zheng, Takeo\n  Kanade", "title": "ReshapeGAN: Object Reshaping by Providing A Single Reference Image", "comments": "25 pages, 23 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The aim of this work is learning to reshape the object in an input image to\nan arbitrary new shape, by just simply providing a single reference image with\nan object instance in the desired shape. We propose a new Generative\nAdversarial Network (GAN) architecture for such an object reshaping problem,\nnamed ReshapeGAN. The network can be tailored for handling all kinds of problem\nsettings, including both within-domain (or single-dataset) reshaping and\ncross-domain (typically across mutiple datasets) reshaping, with paired or\nunpaired training data. The appearance of the input object is preserved in all\ncases, and thus it is still identifiable after reshaping, which has never been\nachieved as far as we are aware. We present the tailored models of the proposed\nReshapeGAN for all the problem settings, and have them tested on 8 kinds of\nreshaping tasks with 13 different datasets, demonstrating the ability of\nReshapeGAN on generating convincing and superior results for object reshaping.\nTo the best of our knowledge, we are the first to be able to make one GAN\nframework work on all such object reshaping tasks, especially the cross-domain\ntasks on handling multiple diverse datasets. We present here both ablation\nstudies on our proposed ReshapeGAN models and comparisons with the\nstate-of-the-art models when they are made comparable, using all kinds of\napplicable metrics that we are aware of.\n", "versions": [{"version": "v1", "created": "Thu, 16 May 2019 03:54:12 GMT"}], "update_date": "2019-05-17", "authors_parsed": [["Zheng", "Ziqiang", ""], ["Wu", "Yang", ""], ["Yu", "Zhibin", ""], ["Yang", "Yang", ""], ["Zheng", "Haiyong", ""], ["Kanade", "Takeo", ""]]}, {"id": "1905.06526", "submitter": "Zaiwei Zhang", "authors": "Zaiwei Zhang, Xiangru Huang, Qixing Huang, Xiao Zhang, Yuan Li", "title": "Joint Learning of Neural Networks via Iterative Reweighted Least Squares", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce the problem of jointly learning feed-forward\nneural networks across a set of relevant but diverse datasets. Compared to\nlearning a separate network from each dataset in isolation, joint learning\nenables us to extract correlated information across multiple datasets to\nsignificantly improve the quality of learned networks. We formulate this\nproblem as joint learning of multiple copies of the same network architecture\nand enforce the network weights to be shared across these networks. Instead of\nhand-encoding the shared network layers, we solve an optimization problem to\nautomatically determine how layers should be shared between each pair of\ndatasets. Experimental results show that our approach outperforms baselines\nwithout joint learning and those using pretraining-and-fine-tuning. We show the\neffectiveness of our approach on three tasks: image classification, learning\nauto-encoders, and image generation.\n", "versions": [{"version": "v1", "created": "Thu, 16 May 2019 04:38:55 GMT"}, {"version": "v2", "created": "Tue, 11 Jun 2019 04:41:46 GMT"}], "update_date": "2019-06-12", "authors_parsed": [["Zhang", "Zaiwei", ""], ["Huang", "Xiangru", ""], ["Huang", "Qixing", ""], ["Zhang", "Xiao", ""], ["Li", "Yuan", ""]]}, {"id": "1905.06537", "submitter": "Bayram Bayramli", "authors": "Bayram Bayramli, Usman Ali, Te Qi, Hongtao Lu", "title": "FH-GAN: Face Hallucination and Recognition using Generative Adversarial\n  Network", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There are many factors affecting visual face recognition, such as low\nresolution images, aging, illumination and pose variance, etc. One of the most\nimportant problem is low resolution face images which can result in bad\nperformance on face recognition. Most of the general face recognition\nalgorithms usually assume a sufficient resolution for the face images. However,\nin practice many applications often do not have sufficient image resolutions.\nThe modern face hallucination models demonstrate reasonable performance to\nreconstruct high-resolution images from its corresponding low resolution\nimages. However, they do not consider identity level information during\nhallucination which directly affects results of the recognition of low\nresolution faces. To address this issue, we propose a Face Hallucination\nGenerative Adversarial Network (FH-GAN) which improves the quality of low\nresolution face images and accurately recognize those low quality images.\nConcretely, we make the following contributions: 1) we propose FH-GAN network,\nan end-to-end system, that improves both face hallucination and face\nrecognition simultaneously. The novelty of this proposed network depends on\nincorporating identity information in a GAN-based face hallucination algorithm\nvia combining a face recognition network for identity preserving. 2) We also\npropose a new face hallucination network, namely Dense Sparse Network (DSNet),\nwhich improves upon the state-of-art in face hallucination. 3) We demonstrate\nbenefits of training the face recognition and GAN-based DSNet jointly by\nreporting good result on face hallucination and recognition.\n", "versions": [{"version": "v1", "created": "Thu, 16 May 2019 05:49:01 GMT"}], "update_date": "2019-05-17", "authors_parsed": [["Bayramli", "Bayram", ""], ["Ali", "Usman", ""], ["Qi", "Te", ""], ["Lu", "Hongtao", ""]]}, {"id": "1905.06540", "submitter": "Shivang Bharadwaj", "authors": "Shivang Bharadwaj, Bhupendra Niranjan, Anant Kumar", "title": "A Non-Intrusive Method of Face Liveness Detection\u00e2\u0080\u00a6 [Redacted]", "comments": "This paper has been withdrawn as it is the proprietary property of an\n  organization. A revision might or might not be uploaded in the future after\n  further internal reviews and revisions. arXiv admin note: Title redacted", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  arXiv admin note: This version removed by arXiv administrators as the\nsubmitter did not have the right to agree to the license at the time of\nsubmission\n", "versions": [{"version": "v1", "created": "Thu, 16 May 2019 05:55:38 GMT"}, {"version": "v2", "created": "Mon, 27 May 2019 07:11:10 GMT"}], "update_date": "2019-08-13", "authors_parsed": [["Bharadwaj", "Shivang", ""], ["Niranjan", "Bhupendra", ""], ["Kumar", "Anant", ""]]}, {"id": "1905.06567", "submitter": "Sifeng Xia", "authors": "Jiaying Liu, Sifeng Xia, Wenhan Yang", "title": "Deep Reference Generation with Multi-Domain Hierarchical Constraints for\n  Inter Prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inter prediction is an important module in video coding for temporal\nredundancy removal, where similar reference blocks are searched from previously\ncoded frames and employed to predict the block to be coded. Although\ntraditional video codecs can estimate and compensate for block-level motions,\ntheir inter prediction performance is still heavily affected by the remaining\ninconsistent pixel-wise displacement caused by irregular rotation and\ndeformation. In this paper, we address the problem by proposing a deep frame\ninterpolation network to generate additional reference frames in coding\nscenarios. First, we summarize the previous adaptive convolutions used for\nframe interpolation and propose a factorized kernel convolutional network to\nimprove the modeling capacity and simultaneously keep its compact form. Second,\nto better train this network, multi-domain hierarchical constraints are\nintroduced to regularize the training of our factorized kernel convolutional\nnetwork. For spatial domain, we use a gradually down-sampled and up-sampled\nauto-encoder to generate the factorized kernels for frame interpolation at\ndifferent scales. For quality domain, considering the inconsistent quality of\nthe input frames, the factorized kernel convolution is modulated with\nquality-related features to learn to exploit more information from high quality\nframes. For frequency domain, a sum of absolute transformed difference loss\nthat performs frequency transformation is utilized to facilitate network\noptimization from the view of coding performance. With the well-designed frame\ninterpolation network regularized by multi-domain hierarchical constraints, our\nmethod surpasses HEVC on average 6.1% BD-rate saving and up to 11.0% BD-rate\nsaving for the luma component under the random access configuration.\n", "versions": [{"version": "v1", "created": "Thu, 16 May 2019 07:23:20 GMT"}], "update_date": "2019-05-17", "authors_parsed": [["Liu", "Jiaying", ""], ["Xia", "Sifeng", ""], ["Yang", "Wenhan", ""]]}, {"id": "1905.06568", "submitter": "Javier Hernandez-Ortega", "authors": "Javier Hernandez-Ortega, Shigenori Nagae, Julian Fierrez, Aythami\n  Morales", "title": "Quality-based Pulse Estimation from NIR Face Video with Application to\n  Driver Monitoring", "comments": "Preprint of the paper presented to IbPRIA 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we develop a robust for heart rate (HR) estimation method using\nface video for challenging scenarios with high variability sources such as head\nmovement, illumination changes, vibration, blur, etc. Our method employs a\nquality measure Q to extract a remote Plethysmography (rPPG) signal as clean as\npossible from a specific face video segment. Our main motivation is developing\nrobust technology for driver monitoring. Therefore, for our experiments we use\na self-collected dataset consisting of Near Infrared (NIR) videos acquired with\na camera mounted in the dashboard of a real moving car. We compare the\nperformance of a classic rPPG algorithm, and the performance of the same\nmethod, but using Q for selecting which video segments present a lower amount\nof variability. Our results show that using the video segments with the highest\nquality in a realistic driving setup improves the HR estimation with a relative\naccuracy improvement larger than 20%.\n", "versions": [{"version": "v1", "created": "Thu, 16 May 2019 07:25:51 GMT"}, {"version": "v2", "created": "Mon, 20 May 2019 06:22:20 GMT"}], "update_date": "2019-05-21", "authors_parsed": [["Hernandez-Ortega", "Javier", ""], ["Nagae", "Shigenori", ""], ["Fierrez", "Julian", ""], ["Morales", "Aythami", ""]]}, {"id": "1905.06576", "submitter": "Hongnian Wang", "authors": "Hongnian Wang and Han Su", "title": "STAR: A Concise Deep Learning Framework for Citywide Human Mobility\n  Prediction", "comments": "Accepted by MDM 2019", "journal-ref": "2019 20th IEEE International Conference on Mobile Data Management\n  (MDM), Hong Kong, 2019, pp. 304-309", "doi": "10.1109/MDM.2019.00-44", "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human mobility forecasting in a city is of utmost importance to\ntransportation and public safety, but with the process of urbanization and the\ngeneration of big data, intensive computing and determination of mobility\npattern have become challenging. This study focuses on how to improve the\naccuracy and efficiency of predicting citywide human mobility via a simpler\nsolution. A spatio-temporal mobility event prediction framework based on a\nsingle fully-convolutional residual network (STAR) is proposed. STAR is a\nhighly simple, general and effective method for learning a single tensor\nrepresenting the mobility event. Residual learning is utilized for training the\ndeep network to derive the detailed result for scenarios of citywide\nprediction. Extensive benchmark evaluation results on real-world data\ndemonstrate that STAR outperforms state-of-the-art approaches in single- and\nmulti-step prediction while utilizing fewer parameters and achieving higher\nefficiency.\n", "versions": [{"version": "v1", "created": "Thu, 16 May 2019 07:42:17 GMT"}], "update_date": "2019-08-16", "authors_parsed": [["Wang", "Hongnian", ""], ["Su", "Han", ""]]}, {"id": "1905.06635", "submitter": "Seungyong Moon", "authors": "Seungyong Moon, Gaon An, Hyun Oh Song", "title": "Parsimonious Black-Box Adversarial Attacks via Efficient Combinatorial\n  Optimization", "comments": "Accepted and to appear at ICML 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Solving for adversarial examples with projected gradient descent has been\ndemonstrated to be highly effective in fooling the neural network based\nclassifiers. However, in the black-box setting, the attacker is limited only to\nthe query access to the network and solving for a successful adversarial\nexample becomes much more difficult. To this end, recent methods aim at\nestimating the true gradient signal based on the input queries but at the cost\nof excessive queries. We propose an efficient discrete surrogate to the\noptimization problem which does not require estimating the gradient and\nconsequently becomes free of the first order update hyperparameters to tune.\nOur experiments on Cifar-10 and ImageNet show the state of the art black-box\nattack performance with significant reduction in the required queries compared\nto a number of recently proposed methods. The source code is available at\nhttps://github.com/snu-mllab/parsimonious-blackbox-attack.\n", "versions": [{"version": "v1", "created": "Thu, 16 May 2019 10:14:20 GMT"}], "update_date": "2019-05-17", "authors_parsed": [["Moon", "Seungyong", ""], ["An", "Gaon", ""], ["Song", "Hyun Oh", ""]]}, {"id": "1905.06648", "submitter": "Feng Li", "authors": "Feng Li, Xiaohe Wu, Wangmeng Zuo, David Zhang, Lei Zhang", "title": "Remove Cosine Window from Correlation Filter-based Visual Trackers: When\n  and How", "comments": "13 pages, 7 figures, submitted to IEEE Transactions on Image\n  Processing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Correlation filters (CFs) have been continuously advancing the\nstate-of-the-art tracking performance and have been extensively studied in the\nrecent few years. Most of the existing CF trackers adopt a cosine window to\nspatially reweight base image to alleviate boundary discontinuity. However,\ncosine window emphasizes more on the central region of base image and has the\nrisk of contaminating negative training samples during model learning. On the\nother hand, spatial regularization deployed in many recent CF trackers plays a\nsimilar role as cosine window by enforcing spatial penalty on CF coefficients.\nTherefore, we in this paper investigate the feasibility to remove cosine window\nfrom CF trackers with spatial regularization. When simply removing cosine\nwindow, CF with spatial regularization still suffers from small degree of\nboundary discontinuity. To tackle this issue, binary and Gaussian shaped mask\nfunctions are further introduced for eliminating boundary discontinuity while\nreweighting the estimation error of each training sample, and can be\nincorporated with multiple CF trackers with spatial regularization. In\ncomparison to the counterparts with cosine window, our methods are effective in\nhandling boundary discontinuity and sample contamination, thereby benefiting\ntracking performance. Extensive experiments on three benchmarks show that our\nmethods perform favorably against the state-of-the-art trackers using either\nhandcrafted or deep CNN features. The code is publicly available at\nhttps://github.com/lifeng9472/Removing_cosine_window_from_CF_trackers.\n", "versions": [{"version": "v1", "created": "Thu, 16 May 2019 10:34:57 GMT"}], "update_date": "2019-05-17", "authors_parsed": [["Li", "Feng", ""], ["Wu", "Xiaohe", ""], ["Zuo", "Wangmeng", ""], ["Zhang", "David", ""], ["Zhang", "Lei", ""]]}, {"id": "1905.06653", "submitter": "Muhammad Afifi", "authors": "Mohamed Afifi, Yara Ali, Karim Amer, Mahmoud Shaker, Mohamed ElHelw", "title": "Robust Real-time Pedestrian Detection in Aerial Imagery on Jetson TX2", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Detection of pedestrians in aerial imagery captured by drones has many\napplications including intersection monitoring, patrolling, and surveillance,\nto name a few. However, the problem is involved due to continuouslychanging\ncamera viewpoint and object appearance as well as the need for lightweight\nalgorithms to run on on-board embedded systems. To address this issue, the\npaper proposes a framework for pedestrian detection in videos based on the YOLO\nobject detection network [6] while having a high throughput of more than 5 FPS\non the Jetson TX2 embedded board. The framework exploits deep learning for\nrobust operation and uses a pre-trained model without the need for any\nadditional training which makes it flexible to apply on different setups with\nminimum amount of tuning. The method achieves ~81 mAP when applied on a sample\nvideo from the Embedded Real-Time Inference (ERTI) Challenge where pedestrians\nare monitored by a UAV.\n", "versions": [{"version": "v1", "created": "Thu, 16 May 2019 10:54:07 GMT"}], "update_date": "2019-05-17", "authors_parsed": [["Afifi", "Mohamed", ""], ["Ali", "Yara", ""], ["Amer", "Karim", ""], ["Shaker", "Mahmoud", ""], ["ElHelw", "Mohamed", ""]]}, {"id": "1905.06656", "submitter": "Kai Zhu", "authors": "Kai Zhu, Wei Zhai, Zheng-Jun Zha, Yang Cao", "title": "One-Shot Texture Retrieval with Global Context Metric", "comments": "ijcai2019-lastest", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we tackle one-shot texture retrieval: given an example of a\nnew reference texture, detect and segment all the pixels of the same texture\ncategory within an arbitrary image. To address this problem, we present an\nOS-TR network to encode both reference and query image, leading to achieve\ntexture segmentation towards the reference category. Unlike the existing\ntexture encoding methods that integrate CNN with orderless pooling, we propose\na directionality-aware module to capture the texture variations at each\ndirection, resulting in spatially invariant representation. To segment new\ncategories given only few examples, we incorporate a self-gating mechanism into\nrelation network to exploit global context information for adjusting\nper-channel modulation weights of local relation features. Extensive\nexperiments on benchmark texture datasets and real scenarios demonstrate the\nabove-par segmentation performance and robust generalization across domains of\nour proposed method.\n", "versions": [{"version": "v1", "created": "Thu, 16 May 2019 11:00:49 GMT"}, {"version": "v2", "created": "Sun, 12 Apr 2020 03:25:11 GMT"}], "update_date": "2020-04-14", "authors_parsed": [["Zhu", "Kai", ""], ["Zhai", "Wei", ""], ["Zha", "Zheng-Jun", ""], ["Cao", "Yang", ""]]}, {"id": "1905.06658", "submitter": "Guoguang Du", "authors": "Guoguang Du, Kai Wang, Shiguo Lian and Kaiyong Zhao", "title": "Vision-based Robotic Grasping From Object Localization, Object Pose\n  Estimation to Grasp Estimation for Parallel Grippers: A Review", "comments": "This is a pre-print of an article published in Artificial\n  Intelligence Review. The final authenticated version is available online at:\n  https://doi.org/10.1007/s10462-020-09888-5. Related refs are summarized at:\n  https://github.com/GeorgeDu/vision-based-robotic-grasping", "journal-ref": null, "doi": "10.1007/s10462-020-09888-5", "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a comprehensive survey on vision-based robotic grasping.\nWe conclude three key tasks during vision-based robotic grasping, which are\nobject localization, object pose estimation and grasp estimation. In detail,\nthe object localization task contains object localization without\nclassification, object detection and object instance segmentation. This task\nprovides the regions of the target object in the input data. The object pose\nestimation task mainly refers to estimating the 6D object pose and includes\ncorrespondence-based methods, template-based methods and voting-based methods,\nwhich affords the generation of grasp poses for known objects. The grasp\nestimation task includes 2D planar grasp methods and 6DoF grasp methods, where\nthe former is constrained to grasp from one direction. These three tasks could\naccomplish the robotic grasping with different combinations. Lots of object\npose estimation methods need not object localization, and they conduct object\nlocalization and object pose estimation jointly. Lots of grasp estimation\nmethods need not object localization and object pose estimation, and they\nconduct grasp estimation in an end-to-end manner. Both traditional methods and\nlatest deep learning-based methods based on the RGB-D image inputs are reviewed\nelaborately in this survey. Related datasets and comparisons between\nstate-of-the-art methods are summarized as well. In addition, challenges about\nvision-based robotic grasping and future directions in addressing these\nchallenges are also pointed out.\n", "versions": [{"version": "v1", "created": "Thu, 16 May 2019 11:12:01 GMT"}, {"version": "v2", "created": "Mon, 4 May 2020 12:22:19 GMT"}, {"version": "v3", "created": "Wed, 19 Aug 2020 11:12:30 GMT"}, {"version": "v4", "created": "Mon, 26 Oct 2020 03:03:20 GMT"}], "update_date": "2020-12-24", "authors_parsed": [["Du", "Guoguang", ""], ["Wang", "Kai", ""], ["Lian", "Shiguo", ""], ["Zhao", "Kaiyong", ""]]}, {"id": "1905.06683", "submitter": "Hao Wu", "authors": "Hao Wu, Xiangrong Xu, Wenbin Gao", "title": "Uneven illumination surface defects inspection based on convolutional\n  neural network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Surface defect inspection based on machine vision is often affected by uneven\nillumination. In order to improve the inspection rate of surface defects\ninspection under uneven illumination condition, this paper proposes a method\nfor detecting surface image defects based on convolutional neural network,\nwhich is based on the adjustment of convolutional neural networks, training\nparameters, changing the structure of the network, to achieve the purpose of\naccurately identifying various defects. Experimental on defect inspection of\ncopper strip and steel images shows that the convolutional neural network can\nautomatically learn features without preprocessing the image, and correct\nidentification of various types of image defects affected by uneven\nillumination, thus overcoming the drawbacks of traditional machine vision\ninspection methods under uneven illumination.\n", "versions": [{"version": "v1", "created": "Thu, 16 May 2019 12:18:42 GMT"}, {"version": "v2", "created": "Fri, 6 Sep 2019 02:00:46 GMT"}], "update_date": "2019-09-09", "authors_parsed": [["Wu", "Hao", ""], ["Xu", "Xiangrong", ""], ["Gao", "Wenbin", ""]]}, {"id": "1905.06741", "submitter": "Chenglong Li", "authors": "Zhengzheng Tu, Tian Xia, Chenglong Li, Xiaoxiao Wang, Yan Ma and Jin\n  Tang", "title": "RGB-T Image Saliency Detection via Collaborative Graph Learning", "comments": "14 pages, 14 figures, 7 tables, accepted by IEEE Transactions on\n  Multimedia with minor revisions", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image saliency detection is an active research topic in the community of\ncomputer vision and multimedia. Fusing complementary RGB and thermal infrared\ndata has been proven to be effective for image saliency detection. In this\npaper, we propose an effective approach for RGB-T image saliency detection. Our\napproach relies on a novel collaborative graph learning algorithm. In\nparticular, we take superpixels as graph nodes, and collaboratively use\nhierarchical deep features to jointly learn graph affinity and node saliency in\na unified optimization framework. Moreover, we contribute a more challenging\ndataset for the purpose of RGB-T image saliency detection, which contains 1000\nspatially aligned RGB-T image pairs and their ground truth annotations.\nExtensive experiments on the public dataset and the newly created dataset\nsuggest that the proposed approach performs favorably against the\nstate-of-the-art RGB-T saliency detection methods.\n", "versions": [{"version": "v1", "created": "Thu, 16 May 2019 13:35:21 GMT"}], "update_date": "2019-05-17", "authors_parsed": [["Tu", "Zhengzheng", ""], ["Xia", "Tian", ""], ["Li", "Chenglong", ""], ["Wang", "Xiaoxiao", ""], ["Ma", "Yan", ""], ["Tang", "Jin", ""]]}, {"id": "1905.06747", "submitter": "Yaoyi Li", "authors": "Yaoyi Li, Jianfu Zhang, Weijie Zhao, Hongtao Lu", "title": "Inductive Guided Filter: Real-time Deep Image Matting with Weakly\n  Annotated Masks on Mobile Devices", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, significant progress has been achieved in deep image matting. Most\nof the classical image matting methods are time-consuming and require an ideal\ntrimap which is difficult to attain in practice. A high efficient image matting\nmethod based on a weakly annotated mask is in demand for mobile applications.\nIn this paper, we propose a novel method based on Deep Learning and Guided\nFilter, called Inductive Guided Filter, which can tackle the real-time general\nimage matting task on mobile devices. We design a lightweight hourglass network\nto parameterize the original Guided Filter method that takes an image and a\nweakly annotated mask as input. Further, the use of Gabor loss is proposed for\ntraining networks for complicated textures in image matting. Moreover, we\ncreate an image matting dataset MAT-2793 with a variety of foreground objects.\nExperimental results demonstrate that our proposed method massively reduces\nrunning time with robust accuracy.\n", "versions": [{"version": "v1", "created": "Thu, 16 May 2019 13:39:41 GMT"}], "update_date": "2019-05-17", "authors_parsed": [["Li", "Yaoyi", ""], ["Zhang", "Jianfu", ""], ["Zhao", "Weijie", ""], ["Lu", "Hongtao", ""]]}, {"id": "1905.06749", "submitter": "Chungkwong Chan", "authors": "Chungkwong Chan", "title": "Stroke extraction for offline handwritten mathematical expression\n  recognition", "comments": "22 pages, 7 figures", "journal-ref": "IEEE Access, vol. 8, pp. 61565-61575, 2020", "doi": "10.1109/ACCESS.2020.2984627", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Offline handwritten mathematical expression recognition is often considered\nmuch harder than its online counterpart due to the absence of temporal\ninformation. In order to take advantage of the more mature methods for online\nrecognition and save resources, an oversegmentation approach is proposed to\nrecover strokes from textual bitmap images automatically. The proposed\nalgorithm first breaks down the skeleton of a binarized image into junctions\nand segments, then segments are merged to form strokes, finally stroke order is\nnormalized by using recursive projection and topological sort. Good offline\naccuracy was obtained in combination with ordinary online recognizers, which\nare not specially designed for extracted strokes. Given a ready-made\nstate-of-the-art online handwritten mathematical expression recognizer, the\nproposed procedure correctly recognized 58.22%, 65.65%, and 65.22% of the\noffline formulas rendered from the datasets of the Competitions on Recognition\nof Online Handwritten Mathematical Expressions(CROHME) in 2014, 2016, and 2019\nrespectively. Furthermore, given a trainable online recognition system,\nretraining it with extracted strokes resulted in an offline recognizer with the\nsame level of accuracy. On the other hand, the speed of the entire pipeline was\nfast enough to facilitate on-device recognition on mobile phones with limited\nresources. To conclude, stroke extraction provides an attractive way to build\noptical character recognition software.\n", "versions": [{"version": "v1", "created": "Thu, 16 May 2019 13:40:43 GMT"}, {"version": "v2", "created": "Thu, 16 Jan 2020 14:08:58 GMT"}], "update_date": "2020-04-14", "authors_parsed": [["Chan", "Chungkwong", ""]]}, {"id": "1905.06764", "submitter": "Berkan Demirel", "authors": "Berkan Demirel, Ramazan Gokberk Cinbis, Nazli Ikizler-Cinbis", "title": "Learning Visually Consistent Label Embeddings for Zero-Shot Learning", "comments": "To appear at IEEE Int. Conference on Image Processing (ICIP) 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we propose a zero-shot learning method to effectively model\nknowledge transfer between classes via jointly learning visually consistent\nword vectors and label embedding model in an end-to-end manner. The main idea\nis to project the vector space word vectors of attributes and classes into the\nvisual space such that word representations of semantically related classes\nbecome more closer, and use the projected vectors in the proposed embedding\nmodel to identify unseen classes. We evaluate the proposed approach on two\nbenchmark datasets and the experimental results show that our method yields\nsignificant improvements in recognition accuracy.\n", "versions": [{"version": "v1", "created": "Thu, 16 May 2019 14:10:20 GMT"}], "update_date": "2019-05-17", "authors_parsed": [["Demirel", "Berkan", ""], ["Cinbis", "Ramazan Gokberk", ""], ["Ikizler-Cinbis", "Nazli", ""]]}, {"id": "1905.06774", "submitter": "Yi-Fan Song", "authors": "Yi-Fan Song, Zhang Zhang and Liang Wang", "title": "Richly Activated Graph Convolutional Network for Action Recognition with\n  Incomplete Skeletons", "comments": "Accepted by ICIP 2019, 5 pages, 3 figures, 3 tables", "journal-ref": null, "doi": "10.1109/ICIP.2019.8802917", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current methods for skeleton-based human action recognition usually work with\ncompletely observed skeletons. However, in real scenarios, it is prone to\ncapture incomplete and noisy skeletons, which will deteriorate the performance\nof traditional models. To enhance the robustness of action recognition models\nto incomplete skeletons, we propose a multi-stream graph convolutional network\n(GCN) for exploring sufficient discriminative features distributed over all\nskeleton joints. Here, each stream of the network is only responsible for\nlearning features from currently unactivated joints, which are distinguished by\nthe class activation maps (CAM) obtained by preceding streams, so that the\nactivated joints of the proposed method are obviously more than traditional\nmethods. Thus, the proposed method is termed richly activated GCN (RA-GCN),\nwhere the richly discovered features will improve the robustness of the model.\nCompared to the state-of-the-art methods, the RA-GCN achieves comparable\nperformance on the NTU RGB+D dataset. Moreover, on a synthetic occlusion\ndataset, the performance deterioration can be alleviated by the RA-GCN\nsignificantly.\n", "versions": [{"version": "v1", "created": "Thu, 16 May 2019 14:22:07 GMT"}, {"version": "v2", "created": "Fri, 17 May 2019 01:31:03 GMT"}], "update_date": "2020-01-08", "authors_parsed": [["Song", "Yi-Fan", ""], ["Zhang", "Zhang", ""], ["Wang", "Liang", ""]]}, {"id": "1905.06784", "submitter": "Johann Sawatzky", "authors": "Johann Sawatzky, Debayan Banerjee, Juergen Gall", "title": "Harvesting Information from Captions for Weakly Supervised Semantic\n  Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Since acquiring pixel-wise annotations for training convolutional neural\nnetworks for semantic image segmentation is time-consuming, weakly supervised\napproaches that only require class tags have been proposed. In this work, we\npropose another form of supervision, namely image captions as they can be found\non the Internet. These captions have two advantages. They do not require\nadditional curation as it is the case for the clean class tags used by current\nweakly supervised approaches and they provide textual context for the classes\npresent in an image. To leverage such textual context, we deploy a multi-modal\nnetwork that learns a joint embedding of the visual representation of the image\nand the textual representation of the caption. The network estimates text\nactivation maps (TAMs) for class names as well as compound concepts, i.e.\ncombinations of nouns and their attributes. The TAMs of compound concepts\ndescribing classes of interest substantially improve the quality of the\nestimated class activation maps which are then used to train a network for\nsemantic segmentation. We evaluate our method on the COCO dataset where it\nachieves state of the art results for weakly supervised image segmentation.\n", "versions": [{"version": "v1", "created": "Thu, 16 May 2019 14:35:09 GMT"}], "update_date": "2019-05-17", "authors_parsed": [["Sawatzky", "Johann", ""], ["Banerjee", "Debayan", ""], ["Gall", "Juergen", ""]]}, {"id": "1905.06803", "submitter": "Zhaohui Che", "authors": "Zhaohui Che and Ali Borji and Guangtao Zhai and Xiongkuo Min and\n  Guodong Guo and Patrick Le Callet", "title": "How is Gaze Influenced by Image Transformations? Dataset and Model", "comments": null, "journal-ref": null, "doi": "10.1109/TIP.2019.2945857", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data size is the bottleneck for developing deep saliency models, because\ncollecting eye-movement data is very time consuming and expensive. Most of\ncurrent studies on human attention and saliency modeling have used high quality\nstereotype stimuli. In real world, however, captured images undergo various\ntypes of transformations. Can we use these transformations to augment existing\nsaliency datasets? Here, we first create a novel saliency dataset including\nfixations of 10 observers over 1900 images degraded by 19 types of\ntransformations. Second, by analyzing eye movements, we find that observers\nlook at different locations over transformed versus original images. Third, we\nutilize the new data over transformed images, called data augmentation\ntransformation (DAT), to train deep saliency models. We find that label\npreserving DATs with negligible impact on human gaze boost saliency prediction,\nwhereas some other DATs that severely impact human gaze degrade the\nperformance. These label preserving valid augmentation transformations provide\na solution to enlarge existing saliency datasets. Finally, we introduce a novel\nsaliency model based on generative adversarial network (dubbed GazeGAN). A\nmodified UNet is proposed as the generator of the GazeGAN, which combines\nclassic skip connections with a novel center-surround connection (CSC), in\norder to leverage multi level features. We also propose a histogram loss based\non Alternative Chi Square Distance (ACS HistLoss) to refine the saliency map in\nterms of luminance distribution. Extensive experiments and comparisons over 3\ndatasets indicate that GazeGAN achieves the best performance in terms of\npopular saliency evaluation metrics, and is more robust to various\nperturbations. Our code and data are available at:\nhttps://github.com/CZHQuality/Sal-CFS-GAN.\n", "versions": [{"version": "v1", "created": "Thu, 16 May 2019 14:48:29 GMT"}, {"version": "v2", "created": "Sat, 18 May 2019 13:24:50 GMT"}, {"version": "v3", "created": "Sat, 25 May 2019 12:21:09 GMT"}, {"version": "v4", "created": "Thu, 3 Oct 2019 16:10:46 GMT"}], "update_date": "2019-11-20", "authors_parsed": [["Che", "Zhaohui", ""], ["Borji", "Ali", ""], ["Zhai", "Guangtao", ""], ["Min", "Xiongkuo", ""], ["Guo", "Guodong", ""], ["Callet", "Patrick Le", ""]]}, {"id": "1905.06817", "submitter": "Timo Bolkart", "authors": "Soubhik Sanyal, Timo Bolkart, Haiwen Feng, Michael J. Black", "title": "Learning to Regress 3D Face Shape and Expression from an Image without\n  3D Supervision", "comments": "To appear in CVPR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The estimation of 3D face shape from a single image must be robust to\nvariations in lighting, head pose, expression, facial hair, makeup, and\nocclusions. Robustness requires a large training set of in-the-wild images,\nwhich by construction, lack ground truth 3D shape. To train a network without\nany 2D-to-3D supervision, we present RingNet, which learns to compute 3D face\nshape from a single image. Our key observation is that an individual's face\nshape is constant across images, regardless of expression, pose, lighting, etc.\nRingNet leverages multiple images of a person and automatically detected 2D\nface features. It uses a novel loss that encourages the face shape to be\nsimilar when the identity is the same and different for different people. We\nachieve invariance to expression by representing the face using the FLAME\nmodel. Once trained, our method takes a single image and outputs the parameters\nof FLAME, which can be readily animated. Additionally we create a new database\nof faces `not quite in-the-wild' (NoW) with 3D head scans and high-resolution\nimages of the subjects in a wide variety of conditions. We evaluate publicly\navailable methods and find that RingNet is more accurate than methods that use\n3D supervision. The dataset, model, and results are available for research\npurposes at http://ringnet.is.tuebingen.mpg.de.\n", "versions": [{"version": "v1", "created": "Thu, 16 May 2019 14:57:45 GMT"}], "update_date": "2019-05-17", "authors_parsed": [["Sanyal", "Soubhik", ""], ["Bolkart", "Timo", ""], ["Feng", "Haiwen", ""], ["Black", "Michael J.", ""]]}, {"id": "1905.06820", "submitter": "Koen Dercksen", "authors": "Koen Dercksen, Wouter Bulten, Geert Litjens", "title": "Dealing with Label Scarcity in Computational Pathology: A Use Case in\n  Prostate Cancer Classification", "comments": "4 pages, 3 figures,MIDL 2019 [arXiv:1907.08612] extended abstract", "journal-ref": null, "doi": null, "report-no": "MIDL/2019/ExtendedAbstract/SJlq_10N94", "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large amounts of unlabelled data are commonplace for many applications in\ncomputational pathology, whereas labelled data is often expensive, both in time\nand cost, to acquire. We investigate the performance of unsupervised and\nsupervised deep learning methods when few labelled data are available. Three\nmethods are compared: clustering autoencoder latent vectors (unsupervised), a\nsingle layer classifier combined with a pre-trained autoencoder\n(semi-supervised), and a supervised CNN. We apply these methods on hematoxylin\nand eosin (H&E) stained prostatectomy images to classify tumour versus\nnon-tumour tissue. Results show that semi-/unsupervised methods have an\nadvantage over supervised learning when few labels are available. Additionally,\nwe show that incorporating immunohistochemistry (IHC) stained data provides an\nincrease in performance over only using H&E.\n", "versions": [{"version": "v1", "created": "Thu, 16 May 2019 14:59:22 GMT"}], "update_date": "2019-07-27", "authors_parsed": [["Dercksen", "Koen", ""], ["Bulten", "Wouter", ""], ["Litjens", "Geert", ""]]}, {"id": "1905.06900", "submitter": "Nicolas Le Scouarnec", "authors": "Fabien Andr\\'e, Anne-Marie Kermarrec, Nicolas Le Scouarnec", "title": "Derived Codebooks for High-Accuracy Nearest Neighbor Search", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High-dimensional Nearest Neighbor (NN) search is central in multimedia search\nsystems. Product Quantization (PQ) is a widespread NN search technique which\nhas a high performance and good scalability. PQ compresses high-dimensional\nvectors into compact codes thanks to a combination of quantizers. Large\ndatabases can, therefore, be stored entirely in RAM, enabling fast responses to\nNN queries. In almost all cases, PQ uses 8-bit quantizers as they offer low\nresponse times. In this paper, we advocate the use of 16-bit quantizers.\nCompared to 8-bit quantizers, 16-bit quantizers boost accuracy but they\nincrease response time by a factor of 3 to 10. We propose a novel approach that\nallows 16-bit quantizers to offer the same response time as 8-bit quantizers,\nwhile still providing a boost of accuracy. Our approach builds on two key\nideas: (i) the construction of derived codebooks that allow a fast and\napproximate distance evaluation, and (ii) a two-pass NN search procedure which\nbuilds a candidate set using the derived codebooks, and then refines it using\n16-bit quantizers. On 1 billion SIFT vectors, with an inverted index, our\napproach offers a Recall@100 of 0.85 in 5.2 ms. By contrast, 16-bit quantizers\nalone offer a Recall@100 of 0.85 in 39 ms, and 8-bit quantizers a Recall@100 of\n0.82 in 3.8 ms.\n", "versions": [{"version": "v1", "created": "Thu, 16 May 2019 16:47:30 GMT"}], "update_date": "2019-05-17", "authors_parsed": [["Andr\u00e9", "Fabien", ""], ["Kermarrec", "Anne-Marie", ""], ["Scouarnec", "Nicolas Le", ""]]}, {"id": "1905.06902", "submitter": "Xingde Ying", "authors": "Xingde Ying, Heng Guo, Kai Ma, Jian Wu, Zhengxin Weng and Yefeng Zheng", "title": "X2CT-GAN: Reconstructing CT from Biplanar X-Rays with Generative\n  Adversarial Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computed tomography (CT) can provide a 3D view of the patient's internal\norgans, facilitating disease diagnosis, but it incurs more radiation dose to a\npatient and a CT scanner is much more cost prohibitive than an X-ray machine\ntoo. Traditional CT reconstruction methods require hundreds of X-ray\nprojections through a full rotational scan of the body, which cannot be\nperformed on a typical X-ray machine. In this work, we propose to reconstruct\nCT from two orthogonal X-rays using the generative adversarial network (GAN)\nframework. A specially designed generator network is exploited to increase data\ndimension from 2D (X-rays) to 3D (CT), which is not addressed in previous\nresearch of GAN. A novel feature fusion method is proposed to combine\ninformation from two X-rays.The mean squared error (MSE) loss and adversarial\nloss are combined to train the generator, resulting in a high-quality CT volume\nboth visually and quantitatively. Extensive experiments on a publicly available\nchest CT dataset demonstrate the effectiveness of the proposed method. It could\nbe a nice enhancement of a low-cost X-ray machine to provide physicians a\nCT-like 3D volume in several niche applications.\n", "versions": [{"version": "v1", "created": "Thu, 16 May 2019 16:50:26 GMT"}], "update_date": "2019-05-17", "authors_parsed": [["Ying", "Xingde", ""], ["Guo", "Heng", ""], ["Ma", "Kai", ""], ["Wu", "Jian", ""], ["Weng", "Zhengxin", ""], ["Zheng", "Yefeng", ""]]}, {"id": "1905.06916", "submitter": "Owen Levin", "authors": "Owen Levin, Zihang Meng, Vikas Singh, Xiaojin Zhu", "title": "Fooling Computer Vision into Inferring the Wrong Body Mass Index", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently it's been shown that neural networks can use images of human faces\nto accurately predict Body Mass Index (BMI), a widely used health indicator. In\nthis paper we demonstrate that a neural network performing BMI inference is\nindeed vulnerable to test-time adversarial attacks. This extends test-time\nadversarial attacks from classification tasks to regression. The application we\nhighlight is BMI inference in the insurance industry, where such adversarial\nattacks imply a danger of insurance fraud.\n", "versions": [{"version": "v1", "created": "Thu, 16 May 2019 17:29:08 GMT"}], "update_date": "2019-05-18", "authors_parsed": [["Levin", "Owen", ""], ["Meng", "Zihang", ""], ["Singh", "Vikas", ""], ["Zhu", "Xiaojin", ""]]}, {"id": "1905.06937", "submitter": "Dequan Wang", "authors": "Dequan Wang, Coline Devin, Qi-Zhi Cai, Philipp Kr\\\"ahenb\\\"uhl, Trevor\n  Darrell", "title": "Monocular Plan View Networks for Autonomous Driving", "comments": "8 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutions on monocular dash cam videos capture spatial invariances in the\nimage plane but do not explicitly reason about distances and depth. We propose\na simple transformation of observations into a bird's eye view, also known as\nplan view, for end-to-end control. We detect vehicles and pedestrians in the\nfirst person view and project them into an overhead plan view. This\nrepresentation provides an abstraction of the environment from which a deep\nnetwork can easily deduce the positions and directions of entities.\nAdditionally, the plan view enables us to leverage advances in 3D object\ndetection in conjunction with deep policy learning. We evaluate our monocular\nplan view network on the photo-realistic Grand Theft Auto V simulator. A\nnetwork using both a plan view and front view causes less than half as many\ncollisions as previous detection-based methods and an order of magnitude fewer\ncollisions than pure pixel-based policies.\n", "versions": [{"version": "v1", "created": "Thu, 16 May 2019 17:56:33 GMT"}], "update_date": "2019-05-17", "authors_parsed": [["Wang", "Dequan", ""], ["Devin", "Coline", ""], ["Cai", "Qi-Zhi", ""], ["Kr\u00e4henb\u00fchl", "Philipp", ""], ["Darrell", "Trevor", ""]]}, {"id": "1905.06981", "submitter": "Vijay Anand", "authors": "Vijay Anand and Vivek Kanhangad", "title": "PoreNet: CNN-based Pore Descriptor for High-resolution Fingerprint\n  Recognition", "comments": "7 pages, 4 figures, 6tables", "journal-ref": null, "doi": "10.1109/JSEN.2020.2987287", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the development of high-resolution fingerprint scanners, high-resolution\nfingerprint-based biometric recognition has received increasing attention in\nrecent years. This paper presents a pore feature-based approach for biometric\nrecognition. Our approach employs a convolutional neural network (CNN) model,\nDeepResPore, to detect pores in the input fingerprint image. Thereafter, a\nCNN-based descriptor is computed for a patch around each detected pore.\nSpecifically, we have designed a residual learning-based CNN, referred to as\nPoreNet that learns distinctive feature representation from pore patches. For\nverification, the match score is generated by comparing pore descriptors\nobtained from a pair of fingerprint images in bi-directional manner using the\nEuclidean distance. The proposed approach for high-resolution fingerprint\nrecognition achieves 2.91% and 0.57% equal error rates (EERs) on partial (DBI)\nand complete (DBII) fingerprints of the benchmark PolyU HRF dataset. Most\nimportantly, it achieves lower FMR1000 and FMR10000 values than the current\nstate-of-the-art approach on both the datasets.\n", "versions": [{"version": "v1", "created": "Thu, 16 May 2019 18:23:30 GMT"}, {"version": "v2", "created": "Thu, 29 Aug 2019 06:55:07 GMT"}], "update_date": "2020-06-11", "authors_parsed": [["Anand", "Vijay", ""], ["Kanhangad", "Vivek", ""]]}, {"id": "1905.07005", "submitter": "Guido de Croon", "authors": "Tom van Dijk, Guido C.H.E. de Croon", "title": "How do neural networks see depth in single images?", "comments": "Submitted", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks have lead to a breakthrough in depth estimation from\nsingle images. Recent work often focuses on the accuracy of the depth map,\nwhere an evaluation on a publicly available test set such as the KITTI vision\nbenchmark is often the main result of the article. While such an evaluation\nshows how well neural networks can estimate depth, it does not show how they do\nthis. To the best of our knowledge, no work currently exists that analyzes what\nthese networks have learned.\n  In this work we take the MonoDepth network by Godard et al. and investigate\nwhat visual cues it exploits for depth estimation. We find that the network\nignores the apparent size of known obstacles in favor of their vertical\nposition in the image. Using the vertical position requires the camera pose to\nbe known; however we find that MonoDepth only partially corrects for changes in\ncamera pitch and roll and that these influence the estimated depth towards\nobstacles. We further show that MonoDepth's use of the vertical image position\nallows it to estimate the distance towards arbitrary obstacles, even those not\nappearing in the training set, but that it requires a strong edge at the ground\ncontact point of the object to do so. In future work we will investigate\nwhether these observations also apply to other neural networks for monocular\ndepth estimation.\n", "versions": [{"version": "v1", "created": "Thu, 16 May 2019 19:19:13 GMT"}], "update_date": "2019-05-20", "authors_parsed": [["van Dijk", "Tom", ""], ["de Croon", "Guido C. H. E.", ""]]}, {"id": "1905.07058", "submitter": "Nasrin Sadeghzadehyazdi", "authors": "Nasrin Sadeghzadehyazdi, Tamal Batabyal, Nibir K. Dhar, B. O.\n  Familoni, K. M. Iftekharuddin, Scott T. Acton", "title": "GlidarCo: gait recognition by 3D skeleton estimation and biometric\n  feature correction of flash lidar data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Gait recognition using noninvasively acquired data has been attracting an\nincreasing interest in the last decade. Among various modalities of data\nsources, it is experimentally found that the data involving skeletal\nrepresentation are amenable for reliable feature compaction and fast\nprocessing. Model-based gait recognition methods that exploit features from a\nfitted model, like skeleton, are recognized for their view and scale-invariant\nproperties. We propose a model-based gait recognition method, using sequences\nrecorded by a single flash lidar. Existing state-of-the-art model-based\napproaches that exploit features from high quality skeletal data collected by\nKinect and Mocap are limited to controlled laboratory environments. The\nperformance of conventional research efforts is negatively affected by poor\ndata quality. We address the problem of gait recognition under challenging\nscenarios, such as lower quality and noisy imaging process of lidar, that\ndegrades the performance of state-of-the-art skeleton-based systems. We present\nGlidarCo to attain high accuracy on gait recognition under the described\nconditions. A filtering mechanism corrects faulty skeleton joint measurements,\nand robust statistics are integrated to conventional feature moments to encode\nthe dynamic of the motion. As a comparison, length-based and vector-based\nfeatures extracted from the noisy skeletons are investigated for outlier\nremoval. Experimental results illustrate the efficacy of the proposed\nmethodology in improving gait recognition given noisy low resolution lidar\ndata.\n", "versions": [{"version": "v1", "created": "Thu, 16 May 2019 23:22:16 GMT"}, {"version": "v2", "created": "Mon, 20 May 2019 14:18:48 GMT"}], "update_date": "2019-05-21", "authors_parsed": [["Sadeghzadehyazdi", "Nasrin", ""], ["Batabyal", "Tamal", ""], ["Dhar", "Nibir K.", ""], ["Familoni", "B. O.", ""], ["Iftekharuddin", "K. M.", ""], ["Acton", "Scott T.", ""]]}, {"id": "1905.07061", "submitter": "Rajhans Singh", "authors": "Rajhans Singh (1), Pavan Turaga (1), Suren Jayasuriya (1), Ravi Garg\n  (2), Martin W. Braun (2) ((1) Arizona State University, (2) Intel\n  Corporation)", "title": "Non-Parametric Priors For Generative Adversarial Networks", "comments": null, "journal-ref": "International Conference on Machine Learning (2019)", "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The advent of generative adversarial networks (GAN) has enabled new\ncapabilities in synthesis, interpolation, and data augmentation heretofore\nconsidered very challenging. However, one of the common assumptions in most GAN\narchitectures is the assumption of simple parametric latent-space\ndistributions. While easy to implement, a simple latent-space distribution can\nbe problematic for uses such as interpolation. This is due to distributional\nmismatches when samples are interpolated in the latent space. We present a\nstraightforward formalization of this problem; using basic results from\nprobability theory and off-the-shelf-optimization tools, we develop ways to\narrive at appropriate non-parametric priors. The obtained prior exhibits\nunusual qualitative properties in terms of its shape, and quantitative benefits\nin terms of lower divergence with its mid-point distribution. We demonstrate\nthat our designed prior helps improve image generation along any Euclidean\nstraight line during interpolation, both qualitatively and quantitatively,\nwithout any additional training or architectural modifications. The proposed\nformulation is quite flexible, paving the way to impose newer constraints on\nthe latent-space statistics.\n", "versions": [{"version": "v1", "created": "Thu, 16 May 2019 23:31:20 GMT"}], "update_date": "2019-05-20", "authors_parsed": [["Singh", "Rajhans", ""], ["Turaga", "Pavan", ""], ["Jayasuriya", "Suren", ""], ["Garg", "Ravi", ""], ["Braun", "Martin W.", ""]]}, {"id": "1905.07072", "submitter": "Kartikeya Bhardwaj", "authors": "Kartikeya Bhardwaj, Naveen Suda, Radu Marculescu", "title": "Dream Distillation: A Data-Independent Model Compression Framework", "comments": "Presented at the ICML 2019 Joint Workshop on On-Device Machine\n  Learning & Compact Deep Neural Network Representations (ODML-CDNNR)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Model compression is eminently suited for deploying deep learning on\nIoT-devices. However, existing model compression techniques rely on access to\nthe original or some alternate dataset. In this paper, we address the model\ncompression problem when no real data is available, e.g., when data is private.\nTo this end, we propose Dream Distillation, a data-independent model\ncompression framework. Our experiments show that Dream Distillation can achieve\n88.5% accuracy on the CIFAR-10 test set without actually training on the\noriginal data!\n", "versions": [{"version": "v1", "created": "Fri, 17 May 2019 00:28:49 GMT"}], "update_date": "2019-05-20", "authors_parsed": [["Bhardwaj", "Kartikeya", ""], ["Suda", "Naveen", ""], ["Marculescu", "Radu", ""]]}, {"id": "1905.07075", "submitter": "Karan Sikka", "authors": "Karan Sikka and Lucas Van Bramer and Ajay Divakaran", "title": "Deep Unified Multimodal Embeddings for Understanding both Content and\n  Users in Social Media Networks", "comments": "Preprint submitted to IJCV", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL cs.CV cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There has been an explosion of multimodal content generated on social media\nnetworks in the last few years, which has necessitated a deeper understanding\nof social media content and user behavior. We present a novel\ncontent-independent content-user-reaction model for social multimedia content\nanalysis. Compared to prior works that generally tackle semantic content\nunderstanding and user behavior modeling in isolation, we propose a generalized\nsolution to these problems within a unified framework. We embed users, images\nand text drawn from open social media in a common multimodal geometric space,\nusing a novel loss function designed to cope with distant and disparate\nmodalities, and thereby enable seamless three-way retrieval. Our model not only\noutperforms unimodal embedding based methods on cross-modal retrieval tasks but\nalso shows improvements stemming from jointly solving the two tasks on Twitter\ndata. We also show that the user embeddings learned within our joint multimodal\nembedding model are better at predicting user interests compared to those\nlearned with unimodal content on Instagram data. Our framework thus goes beyond\nthe prior practice of using explicit leader-follower link information to\nestablish affiliations by extracting implicit content-centric affiliations from\nisolated users. We provide qualitative results to show that the user clusters\nemerging from learned embeddings have consistent semantics and the ability of\nour model to discover fine-grained semantics from noisy and unstructured data.\nOur work reveals that social multimodal content is inherently multimodal and\npossesses a consistent structure because in social networks meaning is created\nthrough interactions between users and content.\n", "versions": [{"version": "v1", "created": "Fri, 17 May 2019 01:16:15 GMT"}, {"version": "v2", "created": "Tue, 21 May 2019 14:48:32 GMT"}, {"version": "v3", "created": "Tue, 11 Jun 2019 03:17:53 GMT"}], "update_date": "2019-06-12", "authors_parsed": [["Sikka", "Karan", ""], ["Van Bramer", "Lucas", ""], ["Divakaran", "Ajay", ""]]}, {"id": "1905.07108", "submitter": "Yuxi Li", "authors": "Weiyao Lin, Yuxi Li, Hao Xiao, John See, Junni Zou, Hongkai Xiong,\n  Jingdong Wang and Tao Mei", "title": "Group Re-Identification with Multi-grained Matching and Integration", "comments": "14 pages, 10 figures, to appear in IEEE transaction on Cybernetics", "journal-ref": null, "doi": "10.1109/TCYB.2019.2917713", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The task of re-identifying groups of people underdifferent camera views is an\nimportant yet less-studied problem.Group re-identification (Re-ID) is a very\nchallenging task sinceit is not only adversely affected by common issues in\ntraditionalsingle object Re-ID problems such as viewpoint and human\nposevariations, but it also suffers from changes in group layout andgroup\nmembership. In this paper, we propose a novel conceptof group granularity by\ncharacterizing a group image by multi-grained objects: individual persons and\nsub-groups of two andthree people within a group. To achieve robust group\nRe-ID,we first introduce multi-grained representations which can beextracted\nvia the development of two separate schemes, i.e. onewith hand-crafted\ndescriptors and another with deep neuralnetworks. The proposed representation\nseeks to characterize bothappearance and spatial relations of multi-grained\nobjects, and isfurther equipped with importance weights which capture\nvaria-tions in intra-group dynamics. Optimal group-wise matching isfacilitated\nby a multi-order matching process which in turn,dynamically updates the\nimportance weights in iterative fashion.We evaluated on three multi-camera\ngroup datasets containingcomplex scenarios and large dynamics, with\nexperimental resultsdemonstrating the effectiveness of our approach. The\npublished dataset can be found in\n\\url{http://min.sjtu.edu.cn/lwydemo/GroupReID.html}\n", "versions": [{"version": "v1", "created": "Fri, 17 May 2019 04:04:47 GMT"}, {"version": "v2", "created": "Sun, 26 May 2019 11:52:25 GMT"}], "update_date": "2019-05-28", "authors_parsed": [["Lin", "Weiyao", ""], ["Li", "Yuxi", ""], ["Xiao", "Hao", ""], ["See", "John", ""], ["Zou", "Junni", ""], ["Xiong", "Hongkai", ""], ["Wang", "Jingdong", ""], ["Mei", "Tao", ""]]}, {"id": "1905.07177", "submitter": "Hui Yin", "authors": "Hui Yin and Yuanhao Gong and Guoping Qiu", "title": "Side Window Filtering", "comments": "2019 CVPR", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Local windows are routinely used in computer vision and almost without\nexception the center of the window is aligned with the pixels being processed.\nWe show that this conventional wisdom is not universally applicable. When a\npixel is on an edge, placing the center of the window on the pixel is one of\nthe fundamental reasons that cause many filtering algorithms to blur the edges.\nBased on this insight, we propose a new Side Window Filtering (SWF) technique\nwhich aligns the window's side or corner with the pixel being processed. The\nSWF technique is surprisingly simple yet theoretically rooted and very\neffective in practice. We show that many traditional linear and nonlinear\nfilters can be easily implemented under the SWF framework. Extensive analysis\nand experiments show that implementing the SWF principle can significantly\nimprove their edge preserving capabilities and achieve state of the art\nperformances in applications such as image smoothing, denoising, enhancement,\nstructure-preserving texture-removing, mutual-structure extraction, and HDR\ntone mapping. In addition to image filtering, we further show that the SWF\nprinciple can be extended to other applications involving the use of a local\nwindow. Using colorization by optimization as an example, we demonstrate that\nimplementing the SWF principle can effectively prevent artifacts such as color\nleakage associated with the conventional implementation. Given the ubiquity of\nwindow based operations in computer vision, the new SWF technique is likely to\nbenefit many more applications.\n", "versions": [{"version": "v1", "created": "Fri, 17 May 2019 09:39:53 GMT"}], "update_date": "2019-05-20", "authors_parsed": [["Yin", "Hui", ""], ["Gong", "Yuanhao", ""], ["Qiu", "Guoping", ""]]}, {"id": "1905.07198", "submitter": "Alberto Gomez", "authors": "Alberto Gomez, Cornelia Schmitz, Markus Henningsson, James Housden,\n  Yohan Noh, Veronika A. Zimmer, James R. Clough, Ilkay Oksuz, Nicolas\n  Toussaint, Andrew P. King and Julia A. Schnabel", "title": "Mechanically Powered Motion Imaging Phantoms: Proof of Concept", "comments": "Accepted for publication at IEEE EMBC (41st International Engineering\n  in Medicine and Biology Conference) 2019", "journal-ref": null, "doi": "10.1109/EMBC.2019.8856577", "report-no": null, "categories": "eess.IV cs.CV physics.med-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motion imaging phantoms are expensive, bulky and difficult to transport and\nset-up. The purpose of this paper is to demonstrate a simple approach to the\ndesign of multi-modality motion imaging phantoms that use mechanically stored\nenergy to produce motion. We propose two phantom designs that use mainsprings\nand elastic bands to store energy. A rectangular piece was attached to an axle\nat the end of the transmission chain of each phantom, and underwent a rotary\nmotion upon release of the mechanical motor. The phantoms were imaged with MRI\nand US, and the image sequences were embedded in a 1D non linear manifold\n(Laplacian Eigenmap) and the spectrogram of the embedding was used to derive\nthe angular velocity over time. The derived velocities were consistent and\nreproducible within a small error. The proposed motion phantom concept showed\ngreat potential for the construction of simple and affordable motion phantoms\n", "versions": [{"version": "v1", "created": "Fri, 17 May 2019 11:14:03 GMT"}], "update_date": "2020-05-15", "authors_parsed": [["Gomez", "Alberto", ""], ["Schmitz", "Cornelia", ""], ["Henningsson", "Markus", ""], ["Housden", "James", ""], ["Noh", "Yohan", ""], ["Zimmer", "Veronika A.", ""], ["Clough", "James R.", ""], ["Oksuz", "Ilkay", ""], ["Toussaint", "Nicolas", ""], ["King", "Andrew P.", ""], ["Schnabel", "Julia A.", ""]]}, {"id": "1905.07202", "submitter": "Simon Chadwick", "authors": "Simon Chadwick and Paul Newman", "title": "Training Object Detectors With Noisy Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The availability of a large quantity of labelled training data is crucial for\nthe training of modern object detectors. Hand labelling training data is time\nconsuming and expensive while automatic labelling methods inevitably add\nunwanted noise to the labels. We examine the effect of different types of label\nnoise on the performance of an object detector. We then show how co-teaching, a\nmethod developed for handling noisy labels and previously demonstrated on a\nclassification problem, can be improved to mitigate the effects of label noise\nin an object detection setting. We illustrate our results using simulated noise\non the KITTI dataset and on a vehicle detection task using automatically\nlabelled data.\n", "versions": [{"version": "v1", "created": "Fri, 17 May 2019 11:20:33 GMT"}], "update_date": "2019-05-20", "authors_parsed": [["Chadwick", "Simon", ""], ["Newman", "Paul", ""]]}, {"id": "1905.07203", "submitter": "Misgina Tsighe Hagos", "authors": "Misgina Tsighe Hagos, Shri Kant", "title": "Transfer Learning based Detection of Diabetic Retinopathy from Small\n  Dataset", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Annotated training data insufficiency remains to be one of the challenges of\napplying deep learning in medical data classification problems. Transfer\nlearning from an already trained deep convolutional network can be used to\nreduce the cost of training from scratch and to train with small training data\nfor deep learning. This raises the question of whether we can use transfer\nlearning to overcome the training data insufficiency problem in deep learning\nbased medical data classifications. Deep convolutional networks have been\nachieving high performance results on the ImageNet Large Scale Visual\nRecognition Competition (ILSVRC) image classification challenge. One example is\nthe Inception-V3 model that was the first runner up on the ILSVRC 2015\nchallenge. Inception modules that help to extract different sized features of\ninput images in one level of convolution are the unique features of the\nInception-V3. In this work, we have used a pretrained Inception-V3 model to\ntake advantage of its Inception modules for Diabetic Retinopathy detection. In\norder to tackle the labelled data insufficiency problem, we sub-sampled a\nsmaller version of the Kaggle Diabetic Retinopathy classification challenge\ndataset for model training, and tested the model's accuracy on a previously\nunseen data subset. Our technique could be used in other deep learning based\nmedical image classification problems facing the challenge of labeled training\ndata insufficiency.\n", "versions": [{"version": "v1", "created": "Fri, 17 May 2019 11:21:36 GMT"}, {"version": "v2", "created": "Wed, 22 May 2019 07:18:21 GMT"}], "update_date": "2019-05-23", "authors_parsed": [["Hagos", "Misgina Tsighe", ""], ["Kant", "Shri", ""]]}, {"id": "1905.07220", "submitter": "Maryam Abdolali", "authors": "Maryam Abdolali and Mohammad Rahmati", "title": "Neither Global Nor Local: A Hierarchical Robust Subspace Clustering For\n  Image Data", "comments": null, "journal-ref": "Information Sciences 514, pp. 333-353, 2020", "doi": "10.1016/j.ins.2019.11.031", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider the problem of subspace clustering in presence of\ncontiguous noise, occlusion and disguise. We argue that self-expressive\nrepresentation of data in current state-of-the-art approaches is severely\nsensitive to occlusions and complex real-world noises. To alleviate this\nproblem, we propose a hierarchical framework that brings robustness of local\npatches-based representations and discriminant property of global\nrepresentations together. This approach consists of 1) a top-down stage, in\nwhich the input data is subject to repeated division to smaller patches and 2)\na bottom-up stage, in which the low rank embedding of local patches in field of\nview of a corresponding patch in upper level are merged on a Grassmann\nmanifold. This summarized information provides two key information for the\ncorresponding patch on the upper level: cannot-links and recommended-links.\nThis information is employed for computing a self-expressive representation of\neach patch at upper levels using a weighted sparse group lasso optimization\nproblem. Numerical results on several real data sets confirm the efficiency of\nour approach.\n", "versions": [{"version": "v1", "created": "Fri, 17 May 2019 12:07:01 GMT"}], "update_date": "2020-01-07", "authors_parsed": [["Abdolali", "Maryam", ""], ["Rahmati", "Mohammad", ""]]}, {"id": "1905.07259", "submitter": "Michael Oechsle", "authors": "Michael Oechsle, Lars Mescheder, Michael Niemeyer, Thilo Strauss,\n  Andreas Geiger", "title": "Texture Fields: Learning Texture Representations in Function Space", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, substantial progress has been achieved in learning-based\nreconstruction of 3D objects. At the same time, generative models were proposed\nthat can generate highly realistic images. However, despite this success in\nthese closely related tasks, texture reconstruction of 3D objects has received\nlittle attention from the research community and state-of-the-art methods are\neither limited to comparably low resolution or constrained experimental setups.\nA major reason for these limitations is that common representations of texture\nare inefficient or hard to interface for modern deep learning techniques. In\nthis paper, we propose Texture Fields, a novel texture representation which is\nbased on regressing a continuous 3D function parameterized with a neural\nnetwork. Our approach circumvents limiting factors like shape discretization\nand parameterization, as the proposed texture representation is independent of\nthe shape representation of the 3D object. We show that Texture Fields are able\nto represent high frequency texture and naturally blend with modern deep\nlearning techniques. Experimentally, we find that Texture Fields compare\nfavorably to state-of-the-art methods for conditional texture reconstruction of\n3D objects and enable learning of probabilistic generative models for texturing\nunseen 3D models. We believe that Texture Fields will become an important\nbuilding block for the next generation of generative 3D models.\n", "versions": [{"version": "v1", "created": "Fri, 17 May 2019 13:39:18 GMT"}], "update_date": "2019-05-20", "authors_parsed": [["Oechsle", "Michael", ""], ["Mescheder", "Lars", ""], ["Niemeyer", "Michael", ""], ["Strauss", "Thilo", ""], ["Geiger", "Andreas", ""]]}, {"id": "1905.07273", "submitter": "Aditya Kuppa", "authors": "Aditya Kuppa, Slawomir Grzonkowski, Muhammad Rizwan Asghar and\n  Nhien-An Le-Khac", "title": "Finding Rats in Cats: Detecting Stealthy Attacks using Group Anomaly\n  Detection", "comments": "Preprint: Modified, Extended Version will be presented at TrustCom\n  2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Advanced attack campaigns span across multiple stages and stay stealthy for\nlong time periods. There is a growing trend of attackers using off-the-shelf\ntools and pre-installed system applications (such as \\emph{powershell} and\n\\emph{wmic}) to evade the detection because the same tools are also used by\nsystem administrators and security analysts for legitimate purposes for their\nroutine tasks. To start investigations, event logs can be collected from\noperational systems; however, these logs are generic enough and it often\nbecomes impossible to attribute a potential attack to a specific attack group.\nRecent approaches in the literature have used anomaly detection techniques,\nwhich aim at distinguishing between malicious and normal behavior of computers\nor network systems. Unfortunately, anomaly detection systems based on point\nanomalies are too rigid in a sense that they could miss the malicious activity\nand classify the attack, not an outlier. Therefore, there is a research\nchallenge to make better detection of malicious activities. To address this\nchallenge, in this paper, we leverage Group Anomaly Detection (GAD), which\ndetects anomalous collections of individual data points.\n  Our approach is to build a neural network model utilizing Adversarial\nAutoencoder (AAE-$\\alpha$) in order to detect the activity of an attacker who\nleverages off-the-shelf tools and system applications. In addition, we also\nbuild \\textit{Behavior2Vec} and \\textit{Command2Vec} sentence embedding deep\nlearning models specific for feature extraction tasks. We conduct extensive\nexperiments to evaluate our models on real-world datasets collected for a\nperiod of two months. The empirical results demonstrate that our approach is\neffective and robust in discovering targeted attacks, pen-tests, and attack\ncampaigns leveraging custom tools.\n", "versions": [{"version": "v1", "created": "Thu, 16 May 2019 17:16:52 GMT"}, {"version": "v2", "created": "Mon, 20 May 2019 13:22:04 GMT"}], "update_date": "2019-05-21", "authors_parsed": [["Kuppa", "Aditya", ""], ["Grzonkowski", "Slawomir", ""], ["Asghar", "Muhammad Rizwan", ""], ["Le-Khac", "Nhien-An", ""]]}, {"id": "1905.07286", "submitter": "Nantheera Anantrasirichai", "authors": "Nantheera Anantrasirichai and Juliet Biggs and Fabien Albino and David\n  Bull", "title": "A deep learning approach to detecting volcano deformation from satellite\n  imagery using synthetic datasets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Satellites enable widespread, regional or global surveillance of volcanoes\nand can provide the first indication of volcanic unrest or eruption. Here we\nconsider Interferometric Synthetic Aperture Radar (InSAR), which can be\nemployed to detect surface deformation with a strong statistical link to\neruption. The ability of machine learning to automatically identify signals of\ninterest in these large InSAR datasets has already been demonstrated, but\ndata-driven techniques, such as convolutional neutral networks (CNN) require\nbalanced training datasets of positive and negative signals to effectively\ndifferentiate between real deformation and noise. As only a small proportion of\nvolcanoes are deforming and atmospheric noise is ubiquitous, the use of machine\nlearning for detecting volcanic unrest is more challenging. In this paper, we\naddress this problem using synthetic interferograms to train the AlexNet. The\nsynthetic interferograms are composed of 3 parts: 1) deformation patterns based\non a Monte Carlo selection of parameters for analytic forward models, 2)\nstratified atmospheric effects derived from weather models and 3) turbulent\natmospheric effects based on statistical simulations of correlated noise. The\nAlexNet architecture trained with synthetic data outperforms that trained using\nreal interferograms alone, based on classification accuracy and positive\npredictive value (PPV). However, the models used to generate the synthetic\nsignals are a simplification of the natural processes, so we retrain the CNN\nwith a combined dataset consisting of synthetic models and selected real\nexamples, achieving a final PPV of 82%. Although applying atmospheric\ncorrections to the entire dataset is computationally expensive, it is\nrelatively simple to apply them to the small subset of positive results. This\nfurther improves the detection performance without a significant increase in\ncomputational burden.\n", "versions": [{"version": "v1", "created": "Fri, 17 May 2019 14:19:30 GMT"}], "update_date": "2019-05-20", "authors_parsed": [["Anantrasirichai", "Nantheera", ""], ["Biggs", "Juliet", ""], ["Albino", "Fabien", ""], ["Bull", "David", ""]]}, {"id": "1905.07287", "submitter": "Max Mehltretter", "authors": "Max Mehltretter, Christian Heipke", "title": "CNN-based Cost Volume Analysis as Confidence Measure for Dense Matching", "comments": "The IEEE International Conference on Computer Vision (ICCV) Workshops\n  (2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to its capability to identify erroneous disparity assignments in dense\nstereo matching, confidence estimation is beneficial for a wide range of\napplications, e.g. autonomous driving, which needs a high degree of confidence\nas mandatory prerequisite. Especially, the introduction of deep learning based\nmethods resulted in an increasing popularity of this field in recent years,\ncaused by a significantly improved accuracy. Despite this remarkable\ndevelopment, most of these methods rely on features learned from disparity maps\nonly, not taking into account the corresponding 3-dimensional cost volumes.\nHowever, it was already demonstrated that with conventional methods based on\nhand-crafted features this additional information can be used to further\nincrease the accuracy. In order to combine the advantages of deep learning and\ncost volume based features, in this paper, we propose a novel Convolutional\nNeural Network (CNN) architecture to directly learn features for confidence\nestimation from volumetric 3D data. An extensive evaluation on three datasets\nusing three common dense stereo matching techniques demonstrates the generality\nand state-of-the-art accuracy of the proposed method.\n", "versions": [{"version": "v1", "created": "Fri, 17 May 2019 14:24:43 GMT"}, {"version": "v2", "created": "Tue, 22 Oct 2019 12:04:48 GMT"}], "update_date": "2019-11-06", "authors_parsed": [["Mehltretter", "Max", ""], ["Heipke", "Christian", ""]]}, {"id": "1905.07290", "submitter": "Mohamed Zahran", "authors": "Ahmad El Sallab, Ibrahim Sobh, Mohamed Zahran and Nader Essam", "title": "LiDAR Sensor modeling and Data augmentation with GANs for Autonomous\n  driving", "comments": "Accepted at ICML Workshop on AI for Autonomous Driving", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.RO eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the autonomous driving domain, data collection and annotation from real\nvehicles are expensive and sometimes unsafe. Simulators are often used for data\naugmentation, which requires realistic sensor models that are hard to formulate\nand model in closed forms. Instead, sensors models can be learned from real\ndata. The main challenge is the absence of paired data set, which makes\ntraditional supervised learning techniques not suitable. In this work, we\nformulate the problem as image translation from unpaired data and employ\nCycleGANs to solve the sensor modeling problem for LiDAR, to produce realistic\nLiDAR from simulated LiDAR (sim2real). Further, we generate high-resolution,\nrealistic LiDAR from lower resolution one (real2real). The LiDAR 3D point cloud\nis processed in Bird-eye View and Polar 2D representations. The experimental\nresults show a high potential of the proposed approach.\n", "versions": [{"version": "v1", "created": "Fri, 17 May 2019 14:30:07 GMT"}], "update_date": "2019-05-20", "authors_parsed": [["Sallab", "Ahmad El", ""], ["Sobh", "Ibrahim", ""], ["Zahran", "Mohamed", ""], ["Essam", "Nader", ""]]}, {"id": "1905.07320", "submitter": "Hui Zhu", "authors": "Hui Zhu, Zhulin An, Chuanguang Yang, Kaiqiang Xu, Erhu Zhao, Yongjun\n  Xu", "title": "EENA: Efficient Evolution of Neural Architecture", "comments": "Accepted by ICCV2019 Neural Architects Workshop (ICCVW)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Latest algorithms for automatic neural architecture search perform remarkable\nbut are basically directionless in search space and computational expensive in\ntraining of every intermediate architecture. In this paper, we propose a method\nfor efficient architecture search called EENA (Efficient Evolution of Neural\nArchitecture). Due to the elaborately designed mutation and crossover\noperations, the evolution process can be guided by the information have already\nbeen learned. Therefore, less computational effort will be required while the\nsearching and training time can be reduced significantly. On CIFAR-10\nclassification, EENA using minimal computational resources (0.65 GPU-days) can\ndesign highly effective neural architecture which achieves 2.56% test error\nwith 8.47M parameters. Furthermore, the best architecture discovered is also\ntransferable for CIFAR-100.\n", "versions": [{"version": "v1", "created": "Fri, 10 May 2019 02:34:23 GMT"}, {"version": "v2", "created": "Mon, 20 May 2019 02:25:44 GMT"}, {"version": "v3", "created": "Tue, 27 Aug 2019 03:32:59 GMT"}], "update_date": "2019-08-28", "authors_parsed": [["Zhu", "Hui", ""], ["An", "Zhulin", ""], ["Yang", "Chuanguang", ""], ["Xu", "Kaiqiang", ""], ["Zhao", "Erhu", ""], ["Xu", "Yongjun", ""]]}, {"id": "1905.07332", "submitter": "Jeffrey Liu", "authors": "Jeffrey Liu, Andrew Weinert, Saurabh Amin", "title": "Semantic Analysis of Traffic Camera Data: Topic Signal Extraction and\n  Anomalous Event Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traffic Management Centers (TMCs) routinely use traffic cameras to provide\nsituational awareness regarding traffic, road, and weather conditions. Camera\nfootage is quite useful for a variety of diagnostic purposes; yet, most footage\nis kept for only a few days, if at all. This is largely due to the fact that\ncurrently, identification of notable footage is done via manual review by human\noperators---a laborious and inefficient process. In this article, we propose a\nsemantics-oriented approach to analyzing sequential image data, and demonstrate\nits application for automatic detection of real-world, anomalous events in\nweather and traffic conditions. Our approach constructs semantic vector\nrepresentations of image contents from textual labels which can be easily\nobtained from off-the-shelf, pretrained image labeling software. These semantic\nlabel vectors are used to construct semantic topic signals---time series\nrepresentations of physical processes---using the Latent Dirichlet Allocation\n(LDA) topic model. By detecting anomalies in the topic signals, we identify\nnotable footage corresponding to winter storms and anomalous traffic\ncongestion. In validation against real-world events, anomaly detection using\nsemantic topic signals significantly outperforms detection using any individual\nlabel signal.\n", "versions": [{"version": "v1", "created": "Fri, 17 May 2019 15:35:54 GMT"}], "update_date": "2019-05-20", "authors_parsed": [["Liu", "Jeffrey", ""], ["Weinert", "Andrew", ""], ["Amin", "Saurabh", ""]]}, {"id": "1905.07373", "submitter": "Chen Lin", "authors": "Chen Lin, Minghao Guo, Chuming Li, Yuan Xin, Wei Wu, Dahua Lin, Wanli\n  Ouyang, Junjie Yan", "title": "Online Hyper-parameter Learning for Auto-Augmentation Strategy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data augmentation is critical to the success of modern deep learning\ntechniques. In this paper, we propose Online Hyper-parameter Learning for\nAuto-Augmentation (OHL-Auto-Aug), an economical solution that learns the\naugmentation policy distribution along with network training. Unlike previous\nmethods on auto-augmentation that search augmentation strategies in an offline\nmanner, our method formulates the augmentation policy as a parameterized\nprobability distribution, thus allowing its parameters to be optimized jointly\nwith network parameters. Our proposed OHL-Auto-Aug eliminates the need of\nre-training and dramatically reduces the cost of the overall search process,\nwhile establishes significantly accuracy improvements over baseline models. On\nboth CIFAR-10 and ImageNet, our method achieves remarkable on search accuracy,\n60x faster on CIFAR-10 and 24x faster on ImageNet, while maintaining\ncompetitive accuracies.\n", "versions": [{"version": "v1", "created": "Fri, 17 May 2019 16:59:31 GMT"}, {"version": "v2", "created": "Wed, 14 Aug 2019 05:58:46 GMT"}], "update_date": "2019-08-15", "authors_parsed": [["Lin", "Chen", ""], ["Guo", "Minghao", ""], ["Li", "Chuming", ""], ["Xin", "Yuan", ""], ["Wu", "Wei", ""], ["Lin", "Dahua", ""], ["Ouyang", "Wanli", ""], ["Yan", "Junjie", ""]]}, {"id": "1905.07375", "submitter": "Chen Lin", "authors": "Chuming Li, Yuan Xin, Chen Lin, Minghao Guo, Wei Wu, Wanli Ouyang,\n  Junjie Yan", "title": "AM-LFS: AutoML for Loss Function Search", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Designing an effective loss function plays an important role in visual\nanalysis. Most existing loss function designs rely on hand-crafted heuristics\nthat require domain experts to explore the large design space, which is usually\nsub-optimal and time-consuming. In this paper, we propose AutoML for Loss\nFunction Search (AM-LFS) which leverages REINFORCE to search loss functions\nduring the training process. The key contribution of this work is the design of\nsearch space which can guarantee the generalization and transferability on\ndifferent vision tasks by including a bunch of existing prevailing loss\nfunctions in a unified formulation. We also propose an efficient optimization\nframework which can dynamically optimize the parameters of loss function's\ndistribution during training. Extensive experimental results on four benchmark\ndatasets show that, without any tricks, our method outperforms existing\nhand-crafted loss functions in various computer vision tasks.\n", "versions": [{"version": "v1", "created": "Fri, 17 May 2019 17:06:49 GMT"}, {"version": "v2", "created": "Wed, 14 Aug 2019 05:58:25 GMT"}], "update_date": "2019-08-15", "authors_parsed": [["Li", "Chuming", ""], ["Xin", "Yuan", ""], ["Lin", "Chen", ""], ["Guo", "Minghao", ""], ["Wu", "Wei", ""], ["Ouyang", "Wanli", ""], ["Yan", "Junjie", ""]]}, {"id": "1905.07376", "submitter": "Emiel Hoogeboom", "authors": "Emiel Hoogeboom, Jorn W.T. Peters, Rianne van den Berg, Max Welling", "title": "Integer Discrete Flows and Lossless Compression", "comments": "Accepted as a conference paper at Neural Information Processing\n  Systems (NeurIPS) 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Lossless compression methods shorten the expected representation size of data\nwithout loss of information, using a statistical model. Flow-based models are\nattractive in this setting because they admit exact likelihood optimization,\nwhich is equivalent to minimizing the expected number of bits per message.\nHowever, conventional flows assume continuous data, which may lead to\nreconstruction errors when quantized for compression. For that reason, we\nintroduce a flow-based generative model for ordinal discrete data called\nInteger Discrete Flow (IDF): a bijective integer map that can learn rich\ntransformations on high-dimensional data. As building blocks for IDFs, we\nintroduce a flexible transformation layer called integer discrete coupling. Our\nexperiments show that IDFs are competitive with other flow-based generative\nmodels. Furthermore, we demonstrate that IDF based compression achieves\nstate-of-the-art lossless compression rates on CIFAR10, ImageNet32, and\nImageNet64. To the best of our knowledge, this is the first lossless\ncompression method that uses invertible neural networks.\n", "versions": [{"version": "v1", "created": "Fri, 17 May 2019 17:07:58 GMT"}, {"version": "v2", "created": "Thu, 23 May 2019 18:00:10 GMT"}, {"version": "v3", "created": "Mon, 25 Nov 2019 16:55:38 GMT"}, {"version": "v4", "created": "Fri, 6 Dec 2019 10:15:54 GMT"}], "update_date": "2019-12-09", "authors_parsed": [["Hoogeboom", "Emiel", ""], ["Peters", "Jorn W. T.", ""], ["Berg", "Rianne van den", ""], ["Welling", "Max", ""]]}, {"id": "1905.07385", "submitter": "Effrosyni Mavroudi", "authors": "Effrosyni Mavroudi, Benjam\\'in B\\'ejar Haro, Ren\\'e Vidal", "title": "Representation Learning on Visual-Symbolic Graphs for Video\n  Understanding", "comments": "ECCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Events in natural videos typically arise from spatio-temporal interactions\nbetween actors and objects and involve multiple co-occurring activities and\nobject classes. To capture this rich visual and semantic context, we propose\nusing two graphs: (1) an attributed spatio-temporal visual graph whose nodes\ncorrespond to actors and objects and whose edges encode different types of\ninteractions, and (2) a symbolic graph that models semantic relationships. We\nfurther propose a graph neural network for refining the representations of\nactors, objects and their interactions on the resulting hybrid graph. Our model\ngoes beyond current approaches that assume nodes and edges are of the same\ntype, operate on graphs with fixed edge weights and do not use a symbolic\ngraph. In particular, our framework: a) has specialized attention-based message\nfunctions for different node and edge types; b) uses visual edge features; c)\nintegrates visual evidence with label relationships; and d) performs global\nreasoning in the semantic space. Experiments on challenging video understanding\ntasks, such as temporal action localization on the Charades dataset, show that\nthe proposed method leads to state-of-the-art performance.\n", "versions": [{"version": "v1", "created": "Fri, 17 May 2019 17:33:48 GMT"}, {"version": "v2", "created": "Wed, 30 Sep 2020 15:55:51 GMT"}], "update_date": "2020-10-01", "authors_parsed": [["Mavroudi", "Effrosyni", ""], ["Haro", "Benjam\u00edn B\u00e9jar", ""], ["Vidal", "Ren\u00e9", ""]]}, {"id": "1905.07387", "submitter": "Ching-Yun Ko", "authors": "Ching-Yun Ko, Zhaoyang Lyu, Tsui-Wei Weng, Luca Daniel, Ngai Wong,\n  Dahua Lin", "title": "POPQORN: Quantifying Robustness of Recurrent Neural Networks", "comments": "10 pages, Ching-Yun Ko and Zhaoyang Lyu contributed equally, accepted\n  to ICML 2019. Please see arXiv source codes for the appendix by clicking\n  [Other formats]", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The vulnerability to adversarial attacks has been a critical issue for deep\nneural networks. Addressing this issue requires a reliable way to evaluate the\nrobustness of a network. Recently, several methods have been developed to\ncompute $\\textit{robustness quantification}$ for neural networks, namely,\ncertified lower bounds of the minimum adversarial perturbation. Such methods,\nhowever, were devised for feed-forward networks, e.g. multi-layer perceptron or\nconvolutional networks. It remains an open problem to quantify robustness for\nrecurrent networks, especially LSTM and GRU. For such networks, there exist\nadditional challenges in computing the robustness quantification, such as\nhandling the inputs at multiple steps and the interaction between gates and\nstates. In this work, we propose $\\textit{POPQORN}$\n($\\textbf{P}$ropagated-$\\textbf{o}$ut$\\textbf{p}$ut $\\textbf{Q}$uantified\nR$\\textbf{o}$bustness for $\\textbf{RN}$Ns), a general algorithm to quantify\nrobustness of RNNs, including vanilla RNNs, LSTMs, and GRUs. We demonstrate its\neffectiveness on different network architectures and show that the robustness\nquantification on individual steps can lead to new insights.\n", "versions": [{"version": "v1", "created": "Fri, 17 May 2019 17:35:04 GMT"}], "update_date": "2019-05-20", "authors_parsed": [["Ko", "Ching-Yun", ""], ["Lyu", "Zhaoyang", ""], ["Weng", "Tsui-Wei", ""], ["Daniel", "Luca", ""], ["Wong", "Ngai", ""], ["Lin", "Dahua", ""]]}, {"id": "1905.07419", "submitter": "Alejandro Linares-Barranco A. Linares-Barranco", "authors": "Alejandro Linares-Barranco, Antonio Rios-Navarro, Ricardo\n  Tapiador-Morales, Tobi Delbruck", "title": "Dynamic Vision Sensor integration on FPGA-based CNN accelerators for\n  high-speed visual classification", "comments": "7 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep-learning is a cutting edge theory that is being applied to many fields.\nFor vision applications the Convolutional Neural Networks (CNN) are demanding\nsignificant accuracy for classification tasks. Numerous hardware accelerators\nhave populated during the last years to improve CPU or GPU based solutions.\nThis technology is commonly prototyped and tested over FPGAs before being\nconsidered for ASIC fabrication for mass production. The use of commercial\ntypical cameras (30fps) limits the capabilities of these systems for high speed\napplications. The use of dynamic vision sensors (DVS) that emulate the behavior\nof a biological retina is taking an incremental importance to improve this\napplications due to its nature, where the information is represented by a\ncontinuous stream of spikes and the frames to be processed by the CNN are\nconstructed collecting a fixed number of these spikes (called events). The\nfaster an object is, the more events are produced by DVS, so the higher is the\nequivalent frame rate. Therefore, these DVS utilization allows to compute a\nframe at the maximum speed a CNN accelerator can offer. In this paper we\npresent a VHDL/HLS description of a pipelined design for FPGA able to collect\nevents from an Address-Event-Representation (AER) DVS retina to obtain a\nnormalized histogram to be used by a particular CNN accelerator, called\nNullHop. VHDL is used to describe the circuit, and HLS for computation blocks,\nwhich are used to perform the normalization of a frame needed for the CNN.\nResults outperform previous implementations of frames collection and\nnormalization using ARM processors running at 800MHz on a Zynq7100 in both\nlatency and power consumption. A measured 67% speedup factor is presented for a\nRoshambo CNN real-time experiment running at 160fps peak rate.\n", "versions": [{"version": "v1", "created": "Fri, 17 May 2019 18:06:12 GMT"}], "update_date": "2019-05-21", "authors_parsed": [["Linares-Barranco", "Alejandro", ""], ["Rios-Navarro", "Antonio", ""], ["Tapiador-Morales", "Ricardo", ""], ["Delbruck", "Tobi", ""]]}, {"id": "1905.07424", "submitter": "Mike Walmsley", "authors": "Mike Walmsley, Lewis Smith, Chris Lintott, Yarin Gal, Steven Bamford,\n  Hugh Dickinson, Lucy Fortson, Sandor Kruk, Karen Masters, Claudia Scarlata,\n  Brooke Simmons, Rebecca Smethurst, Darryl Wright", "title": "Galaxy Zoo: Probabilistic Morphology through Bayesian CNNs and Active\n  Learning", "comments": "Accepted by MNRAS. 21 pages, including appendices", "journal-ref": null, "doi": "10.1093/mnras/stz2816", "report-no": null, "categories": "astro-ph.GA cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We use Bayesian convolutional neural networks and a novel generative model of\nGalaxy Zoo volunteer responses to infer posteriors for the visual morphology of\ngalaxies. Bayesian CNN can learn from galaxy images with uncertain labels and\nthen, for previously unlabelled galaxies, predict the probability of each\npossible label. Our posteriors are well-calibrated (e.g. for predicting bars,\nwe achieve coverage errors of 11.8% within a vote fraction deviation of 0.2)\nand hence are reliable for practical use. Further, using our posteriors, we\napply the active learning strategy BALD to request volunteer responses for the\nsubset of galaxies which, if labelled, would be most informative for training\nour network. We show that training our Bayesian CNNs using active learning\nrequires up to 35-60% fewer labelled galaxies, depending on the morphological\nfeature being classified. By combining human and machine intelligence, Galaxy\nZoo will be able to classify surveys of any conceivable scale on a timescale of\nweeks, providing massive and detailed morphology catalogues to support research\ninto galaxy evolution.\n", "versions": [{"version": "v1", "created": "Fri, 17 May 2019 18:20:35 GMT"}, {"version": "v2", "created": "Fri, 4 Oct 2019 16:38:44 GMT"}], "update_date": "2019-10-07", "authors_parsed": [["Walmsley", "Mike", ""], ["Smith", "Lewis", ""], ["Lintott", "Chris", ""], ["Gal", "Yarin", ""], ["Bamford", "Steven", ""], ["Dickinson", "Hugh", ""], ["Fortson", "Lucy", ""], ["Kruk", "Sandor", ""], ["Masters", "Karen", ""], ["Scarlata", "Claudia", ""], ["Simmons", "Brooke", ""], ["Smethurst", "Rebecca", ""], ["Wright", "Darryl", ""]]}, {"id": "1905.07443", "submitter": "Tonmoy Saikia", "authors": "Tonmoy Saikia, Yassine Marrakchi, Arber Zela, Frank Hutter, Thomas\n  Brox", "title": "AutoDispNet: Improving Disparity Estimation With AutoML", "comments": "In Proceedings of the 2019 IEEE International Conference on Computer\n  Vision (ICCV)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Much research work in computer vision is being spent on optimizing existing\nnetwork architectures to obtain a few more percentage points on benchmarks.\nRecent AutoML approaches promise to relieve us from this effort. However, they\nare mainly designed for comparatively small-scale classification tasks. In this\nwork, we show how to use and extend existing AutoML techniques to efficiently\noptimize large-scale U-Net-like encoder-decoder architectures. In particular,\nwe leverage gradient-based neural architecture search and Bayesian optimization\nfor hyperparameter search. The resulting optimization does not require a\nlarge-scale compute cluster. We show results on disparity estimation that\nclearly outperform the manually optimized baseline and reach state-of-the-art\nperformance.\n", "versions": [{"version": "v1", "created": "Fri, 17 May 2019 19:05:25 GMT"}, {"version": "v2", "created": "Sun, 6 Oct 2019 20:19:32 GMT"}], "update_date": "2019-10-08", "authors_parsed": [["Saikia", "Tonmoy", ""], ["Marrakchi", "Yassine", ""], ["Zela", "Arber", ""], ["Hutter", "Frank", ""], ["Brox", "Thomas", ""]]}, {"id": "1905.07446", "submitter": "Azin Asgarian", "authors": "Azin Asgarian, Shun Zhao, Ahmed B. Ashraf, M. Erin Browne, Kenneth M.\n  Prkachin, Alex Mihailidis, Thomas Hadjistavropoulos, and Babak Taati", "title": "Limitations and Biases in Facial Landmark Detection -- An Empirical\n  Study on Older Adults with Dementia", "comments": "Face and Gesture Analysis for Health Informatics (FGAHI) Workshop at\n  CVPR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurate facial expression analysis is an essential step in various clinical\napplications that involve physical and mental health assessments of older\nadults (e.g. diagnosis of pain or depression). Although remarkable progress has\nbeen achieved toward developing robust facial landmark detection methods,\nstate-of-the-art methods still face many challenges when encountering\nuncontrolled environments, different ranges of facial expressions, and\ndifferent demographics of the population. A recent study has revealed that the\nhealth status of individuals can also affect the performance of facial landmark\ndetection methods on front views of faces. In this work, we investigate this\nmatter in a much greater context using seven facial landmark detection methods.\nWe perform our evaluation not only on frontal faces but also on profile faces\nand in various regions of the face. Our results shed light on limitations of\nthe existing methods and challenges of applying these methods in clinical\nsettings by indicating: 1) a significant difference between the performance of\nstate-of-the-art when tested on the profile or frontal faces of individuals\nwith vs. without dementia; 2) insights on the existing bias for all regions of\nthe face; and 3) the presence of this bias despite re-training/fine-tuning with\nvarious configurations of six datasets.\n", "versions": [{"version": "v1", "created": "Fri, 17 May 2019 19:15:15 GMT"}], "update_date": "2019-05-21", "authors_parsed": [["Asgarian", "Azin", ""], ["Zhao", "Shun", ""], ["Ashraf", "Ahmed B.", ""], ["Browne", "M. Erin", ""], ["Prkachin", "Kenneth M.", ""], ["Mihailidis", "Alex", ""], ["Hadjistavropoulos", "Thomas", ""], ["Taati", "Babak", ""]]}, {"id": "1905.07447", "submitter": "Dinesh Jayaraman", "authors": "Brian Yang, Jesse Zhang, Vitchyr Pong, Sergey Levine, and Dinesh\n  Jayaraman", "title": "REPLAB: A Reproducible Low-Cost Arm Benchmark Platform for Robotic\n  Learning", "comments": "Extended version of paper accepted to ICRA 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Standardized evaluation measures have aided in the progress of machine\nlearning approaches in disciplines such as computer vision and machine\ntranslation. In this paper, we make the case that robotic learning would also\nbenefit from benchmarking, and present the \"REPLAB\" platform for benchmarking\nvision-based manipulation tasks. REPLAB is a reproducible and self-contained\nhardware stack (robot arm, camera, and workspace) that costs about 2000 USD,\noccupies a cuboid of size 70x40x60 cm, and permits full assembly within a few\nhours. Through this low-cost, compact design, REPLAB aims to drive wide\nparticipation by lowering the barrier to entry into robotics and to enable easy\nscaling to many robots. We envision REPLAB as a framework for reproducible\nresearch across manipulation tasks, and as a step in this direction, we define\na template for a grasping benchmark consisting of a task definition, evaluation\nprotocol, performance measures, and a dataset of 92k grasp attempts. We\nimplement, evaluate, and analyze several previously proposed grasping\napproaches to establish baselines for this benchmark. Finally, we also\nimplement and evaluate a deep reinforcement learning approach for 3D reaching\ntasks on our REPLAB platform. Project page with assembly instructions, code,\nand videos: https://goo.gl/5F9dP4.\n", "versions": [{"version": "v1", "created": "Fri, 17 May 2019 19:16:54 GMT"}], "update_date": "2019-05-21", "authors_parsed": [["Yang", "Brian", ""], ["Zhang", "Jesse", ""], ["Pong", "Vitchyr", ""], ["Levine", "Sergey", ""], ["Jayaraman", "Dinesh", ""]]}, {"id": "1905.07475", "submitter": "Rongjun Qin", "authors": "Rongjun Qin", "title": "Automated 3D recovery from very high resolution multi-view satellite\n  images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents an automated pipeline for processing multi-view satellite\nimages to 3D digital surface models (DSM). The proposed pipeline performs\nautomated geo-referencing and generates high-quality densely matched point\nclouds. In particular, a novel approach is developed that fuses multiple depth\nmaps derived by stereo matching to generate high-quality 3D maps. By learning\ncritical configurations of stereo pairs from sample LiDAR data, we rank the\nimage pairs based on the proximity of the results to the sample data. Multiple\ndepth maps derived from individual image pairs are fused with an adaptive 3D\nmedian filter that considers the image spectral similarities. We demonstrate\nthat the proposed adaptive median filter generally delivers better results in\ngeneral as compared to normal median filter, and achieved an accuracy of\nimprovement of 0.36 meters RMSE in the best case. Results and analysis are\nintroduced in detail.\n", "versions": [{"version": "v1", "created": "Fri, 17 May 2019 20:56:52 GMT"}, {"version": "v2", "created": "Sat, 5 Oct 2019 01:46:03 GMT"}], "update_date": "2019-10-08", "authors_parsed": [["Qin", "Rongjun", ""]]}, {"id": "1905.07476", "submitter": "Rongjun Qin", "authors": "Rongjun Qin", "title": "Analysis of critical parameters of satellite stereo image for 3D\n  reconstruction and mapping", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although nowadays advanced dense image matching (DIM) algorithms are able to\nproduce LiDAR (Light Detection And Ranging) comparable dense point clouds from\nsatellite stereo images, the accuracy and completeness of such point clouds\nheavily depend on the geometric parameters of the satellite stereo images. The\nintersection angle between two images are normally seen as the most important\none in stereo data acquisition, as the state-of-the-art DIM algorithms work\nbest on narrow baseline (smaller intersection angle) stereos (E.g. Semi-Global\nMatching regards 15-25 degrees as good intersection angle). This factor is in\nline with the traditional aerial photogrammetry configuration, as the\nintersection angle directly relates to the base-high ratio and texture\ndistortion in the parallax direction, thus both affecting the horizontal and\nvertical accuracy. However, our experiments found that even with very similar\n(and good) intersection angles, the same DIM algorithm applied on different\nstereo pairs (of the same area) produced point clouds with dramatically\ndifferent accuracy as compared to the ground truth LiDAR data. This raises a\nvery practical question that is often asked by practitioners: what factors\nconstitute a good satellite stereo pair, such that it produces accurate and\noptimal results for mapping purpose? In this work, we provide a comprehensive\nanalysis on this matter by performing stereo matching over 1,000 satellite\nstereo pairs with different acquisition parameters including their intersection\nangles, off-nadir angles, sun elevation & azimuth angles, as well as time\ndifferences, thus to offer a thorough answer to this question. This work will\npotentially provide a valuable reference to researchers working on multi-view\nsatellite image reconstruction, as well as industrial practitioners minimizing\ncosts for high-quality large-scale mapping.\n", "versions": [{"version": "v1", "created": "Fri, 17 May 2019 20:58:57 GMT"}], "update_date": "2019-05-21", "authors_parsed": [["Qin", "Rongjun", ""]]}, {"id": "1905.07481", "submitter": "Dat Thanh Tran", "authors": "Dat Thanh Tran, Mehmet Yamac, Aysen Degerli, Moncef Gabbouj,\n  Alexandros Iosifidis", "title": "Multilinear Compressive Learning", "comments": "accepted in IEEE Transactions on Neural Networks and Learning Systems\n  2020", "journal-ref": null, "doi": "10.1109/TNNLS.2020.2984831", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Compressive Learning is an emerging topic that combines signal acquisition\nvia compressive sensing and machine learning to perform inference tasks\ndirectly on a small number of measurements. Many data modalities naturally have\na multi-dimensional or tensorial format, with each dimension or tensor mode\nrepresenting different features such as the spatial and temporal information in\nvideo sequences or the spatial and spectral information in hyperspectral\nimages. However, in existing compressive learning frameworks, the compressive\nsensing component utilizes either random or learned linear projection on the\nvectorized signal to perform signal acquisition, thus discarding the\nmulti-dimensional structure of the signals. In this paper, we propose\nMultilinear Compressive Learning, a framework that takes into account the\ntensorial nature of multi-dimensional signals in the acquisition step and\nbuilds the subsequent inference model on the structurally sensed measurements.\nOur theoretical complexity analysis shows that the proposed framework is more\nefficient compared to its vector-based counterpart in both memory and\ncomputation requirement. With extensive experiments, we also empirically show\nthat our Multilinear Compressive Learning framework outperforms the\nvector-based framework in object classification and face recognition tasks, and\nscales favorably when the dimensionalities of the original signals increase,\nmaking it highly efficient for high-dimensional multi-dimensional signals.\n", "versions": [{"version": "v1", "created": "Fri, 17 May 2019 21:04:44 GMT"}, {"version": "v2", "created": "Wed, 5 Jun 2019 07:45:51 GMT"}, {"version": "v3", "created": "Wed, 21 Oct 2020 09:01:42 GMT"}], "update_date": "2020-10-22", "authors_parsed": [["Tran", "Dat Thanh", ""], ["Yamac", "Mehmet", ""], ["Degerli", "Aysen", ""], ["Gabbouj", "Moncef", ""], ["Iosifidis", "Alexandros", ""]]}, {"id": "1905.07482", "submitter": "Yichao Zhou", "authors": "Yichao Zhou, Haozhi Qi, Yuexiang Zhai, Qi Sun, Zhili Chen, Li-Yi Wei,\n  Yi Ma", "title": "Learning to Reconstruct 3D Manhattan Wireframes from a Single Image", "comments": "ICCV 2019. A video demonstration can be found in\n  https://youtu.be/l3sUtddPJPY", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a method to obtain a compact and accurate 3D\nwireframe representation from a single image by effectively exploiting global\nstructural regularities. Our method trains a convolutional neural network to\nsimultaneously detect salient junctions and straight lines, as well as predict\ntheir 3D depth and vanishing points. Compared with the state-of-the-art\nlearning-based wireframe detection methods, our network is simpler and more\nunified, leading to better 2D wireframe detection. With global structural\npriors from parallelism, our method further reconstructs a full 3D wireframe\nmodel, a compact vector representation suitable for a variety of high-level\nvision tasks such as AR and CAD. We conduct extensive evaluations on a large\nsynthetic dataset of urban scenes as well as real images. Our code and datasets\nhave been made public at https://github.com/zhou13/shapeunity.\n", "versions": [{"version": "v1", "created": "Fri, 17 May 2019 21:09:27 GMT"}, {"version": "v2", "created": "Fri, 16 Apr 2021 04:54:06 GMT"}], "update_date": "2021-04-19", "authors_parsed": [["Zhou", "Yichao", ""], ["Qi", "Haozhi", ""], ["Zhai", "Yuexiang", ""], ["Sun", "Qi", ""], ["Chen", "Zhili", ""], ["Wei", "Li-Yi", ""], ["Ma", "Yi", ""]]}, {"id": "1905.07498", "submitter": "Stanley Chan", "authors": "Nicholas Chimitt, Zhiyuan Mao, Guanzhe Hong, Stanley H. Chan", "title": "Rethinking Atmospheric Turbulence Mitigation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  State-of-the-art atmospheric turbulence image restoration methods utilize\nstandard image processing tools such as optical flow, lucky region and blind\ndeconvolution to restore the images. While promising results have been reported\nover the past decade, many of the methods are agnostic to the physical model\nthat generates the distortion. In this paper, we revisit the turbulence\nrestoration problem by analyzing the reference frame generation and the blind\ndeconvolution steps in a typical restoration pipeline. By leveraging tools in\nlarge deviation theory, we rigorously prove the minimum number of frames\nrequired to generate a reliable reference for both static and dynamic scenes.\nWe discuss how a turbulence agnostic model can lead to potential flaws, and how\nto configure a simple spatial-temporal non-local weighted averaging method to\ngenerate references. For blind deconvolution, we present a new data-driven\nprior by analyzing the distributions of the point spread functions. We\ndemonstrate how a simple prior can outperform state-of-the-art blind\ndeconvolution methods.\n", "versions": [{"version": "v1", "created": "Fri, 17 May 2019 22:59:11 GMT"}], "update_date": "2019-05-21", "authors_parsed": [["Chimitt", "Nicholas", ""], ["Mao", "Zhiyuan", ""], ["Hong", "Guanzhe", ""], ["Chan", "Stanley H.", ""]]}, {"id": "1905.07503", "submitter": "Zhizhong Han", "authors": "Zhizhong Han and Xiyang Wang and Chi-Man Vong and Yu-Shen Liu and\n  Matthias Zwicker and C.L. Philip Chen", "title": "3DViewGraph: Learning Global Features for 3D Shapes from A Graph of\n  Unordered Views with Attention", "comments": "To appear at IJCAI2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning global features by aggregating information over multiple views has\nbeen shown to be effective for 3D shape analysis. For view aggregation in deep\nlearning models, pooling has been applied extensively. However, pooling leads\nto a loss of the content within views, and the spatial relationship among\nviews, which limits the discriminability of learned features. We propose\n3DViewGraph to resolve this issue, which learns 3D global features by more\neffectively aggregating unordered views with attention. Specifically, unordered\nviews taken around a shape are regarded as view nodes on a view graph.\n3DViewGraph first learns a novel latent semantic mapping to project low-level\nview features into meaningful latent semantic embeddings in a lower dimensional\nspace, which is spanned by latent semantic patterns. Then, the content and\nspatial information of each pair of view nodes are encoded by a novel spatial\npattern correlation, where the correlation is computed among latent semantic\npatterns. Finally, all spatial pattern correlations are integrated with\nattention weights learned by a novel attention mechanism. This further\nincreases the discriminability of learned features by highlighting the\nunordered view nodes with distinctive characteristics and depressing the ones\nwith appearance ambiguity. We show that 3DViewGraph outperforms\nstate-of-the-art methods under three large-scale benchmarks.\n", "versions": [{"version": "v1", "created": "Fri, 17 May 2019 23:52:05 GMT"}], "update_date": "2019-05-21", "authors_parsed": [["Han", "Zhizhong", ""], ["Wang", "Xiyang", ""], ["Vong", "Chi-Man", ""], ["Liu", "Yu-Shen", ""], ["Zwicker", "Matthias", ""], ["Chen", "C. L. Philip", ""]]}, {"id": "1905.07506", "submitter": "Zhizhong Han", "authors": "Zhizhong Han and Xinhai Liu and Yu-Shen Liu and Matthias Zwicker", "title": "Parts4Feature: Learning 3D Global Features from Generally Semantic Parts\n  in Multiple Views", "comments": "To appear at IJCAI2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning has achieved remarkable results in 3D shape analysis by\nlearning global shape features from the pixel-level over multiple views.\nPrevious methods, however, compute low-level features for entire views without\nconsidering part-level information. In contrast, we propose a deep neural\nnetwork, called Parts4Feature, to learn 3D global features from part-level\ninformation in multiple views. We introduce a novel definition of generally\nsemantic parts, which Parts4Feature learns to detect in multiple views from\ndifferent 3D shape segmentation benchmarks. A key idea of our architecture is\nthat it transfers the ability to detect semantically meaningful parts in\nmultiple views to learn 3D global features. Parts4Feature achieves this by\ncombining a local part detection branch and a global feature learning branch\nwith a shared region proposal module. The global feature learning branch\naggregates the detected parts in terms of learned part patterns with a novel\nmulti-attention mechanism, while the region proposal module enables locally and\nglobally discriminative information to be promoted by each other. We\ndemonstrate that Parts4Feature outperforms the state-of-the-art under three\nlarge-scale 3D shape benchmarks.\n", "versions": [{"version": "v1", "created": "Sat, 18 May 2019 00:04:11 GMT"}], "update_date": "2019-05-21", "authors_parsed": [["Han", "Zhizhong", ""], ["Liu", "Xinhai", ""], ["Liu", "Yu-Shen", ""], ["Zwicker", "Matthias", ""]]}, {"id": "1905.07512", "submitter": "Daniel Gordon", "authors": "Daniel Gordon, Abhishek Kadian, Devi Parikh, Judy Hoffman, Dhruv Batra", "title": "SplitNet: Sim2Sim and Task2Task Transfer for Embodied Visual Navigation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose SplitNet, a method for decoupling visual perception and policy\nlearning. By incorporating auxiliary tasks and selective learning of portions\nof the model, we explicitly decompose the learning objectives for visual\nnavigation into perceiving the world and acting on that perception. We show\ndramatic improvements over baseline models on transferring between simulators,\nan encouraging step towards Sim2Real. Additionally, SplitNet generalizes better\nto unseen environments from the same simulator and transfers faster and more\neffectively to novel embodied navigation tasks. Further, given only a small\nsample from a target domain, SplitNet can match the performance of traditional\nend-to-end pipelines which receive the entire dataset. Code is available\nhttps://github.com/facebookresearch/splitnet\n", "versions": [{"version": "v1", "created": "Sat, 18 May 2019 00:57:19 GMT"}, {"version": "v2", "created": "Tue, 21 May 2019 00:50:54 GMT"}, {"version": "v3", "created": "Wed, 23 Oct 2019 19:07:16 GMT"}], "update_date": "2019-10-25", "authors_parsed": [["Gordon", "Daniel", ""], ["Kadian", "Abhishek", ""], ["Parikh", "Devi", ""], ["Hoffman", "Judy", ""], ["Batra", "Dhruv", ""]]}, {"id": "1905.07515", "submitter": "Yajie Zhao", "authors": "Yajie Zhao, Zeng Huang, Tianye Li, Weikai Chen, Chloe LeGendre,\n  Xinglei Ren, Jun Xing, Ari Shapiro, and Hao Li", "title": "Learning Perspective Undistortion of Portraits", "comments": "13 pages, 15 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Near-range portrait photographs often contain perspective distortion\nartifacts that bias human perception and challenge both facial recognition and\nreconstruction techniques. We present the first deep learning based approach to\nremove such artifacts from unconstrained portraits. In contrast to the previous\nstate-of-the-art approach, our method handles even portraits with extreme\nperspective distortion, as we avoid the inaccurate and error-prone step of\nfirst fitting a 3D face model. Instead, we predict a distortion correction flow\nmap that encodes a per-pixel displacement that removes distortion artifacts\nwhen applied to the input image. Our method also automatically infers missing\nfacial features, i.e. occluded ears caused by strong perspective distortion,\nwith coherent details. We demonstrate that our approach significantly\noutperforms the previous state-of-the-art both qualitatively and\nquantitatively, particularly for portraits with extreme perspective distortion\nor facial expressions. We further show that our technique benefits a number of\nfundamental tasks, significantly improving the accuracy of both face\nrecognition and 3D reconstruction and enables a novel camera calibration\ntechnique from a single portrait. Moreover, we also build the first perspective\nportrait database with a large diversity in identities, expression and poses,\nwhich will benefit the related research in this area.\n", "versions": [{"version": "v1", "created": "Sat, 18 May 2019 01:08:47 GMT"}], "update_date": "2019-05-21", "authors_parsed": [["Zhao", "Yajie", ""], ["Huang", "Zeng", ""], ["Li", "Tianye", ""], ["Chen", "Weikai", ""], ["LeGendre", "Chloe", ""], ["Ren", "Xinglei", ""], ["Xing", "Jun", ""], ["Shapiro", "Ari", ""], ["Li", "Hao", ""]]}, {"id": "1905.07529", "submitter": "Xiawu Zheng", "authors": "Xiawu Zheng, Rongrong Ji, Lang Tang, Baochang Zhang, Jianzhuang Liu,\n  Qi Tian", "title": "Multinomial Distribution Learning for Effective Neural Architecture\n  Search", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Architectures obtained by Neural Architecture Search (NAS) have achieved\nhighly competitive performance in various computer vision tasks. However, the\nprohibitive computation demand of forward-backward propagation in deep neural\nnetworks and searching algorithms makes it difficult to apply NAS in practice.\nIn this paper, we propose a Multinomial Distribution Learning for extremely\neffective NAS,which considers the search space as a joint multinomial\ndistribution, i.e., the operation between two nodes is sampled from this\ndistribution, and the optimal network structure is obtained by the operations\nwith the most likely probability in this distribution. Therefore, NAS can be\ntransformed to a multinomial distribution learning problem, i.e., the\ndistribution is optimized to have a high expectation of the performance.\nBesides, a hypothesis that the performance ranking is consistent in every\ntraining epoch is proposed and demonstrated to further accelerate the learning\nprocess. Experiments on CIFAR10 and ImageNet demonstrate the effectiveness of\nour method. On CIFAR-10, the structure searched by our method achieves 2.55%\ntest error, while being 6.0x (only 4 GPU hours on GTX1080Ti) faster compared\nwith state-of-the-art NAS algorithms. On ImageNet, our model achieves 75.2%\ntop1 accuracy under MobileNet settings (MobileNet V1/V2), while being 1.2x\nfaster with measured GPU latency. Test code with pre-trained models are\navailable at https://github.com/tanglang96/MDENAS\n", "versions": [{"version": "v1", "created": "Sat, 18 May 2019 03:30:47 GMT"}, {"version": "v2", "created": "Thu, 23 May 2019 07:18:24 GMT"}, {"version": "v3", "created": "Wed, 14 Aug 2019 02:41:43 GMT"}], "update_date": "2019-08-15", "authors_parsed": [["Zheng", "Xiawu", ""], ["Ji", "Rongrong", ""], ["Tang", "Lang", ""], ["Zhang", "Baochang", ""], ["Liu", "Jianzhuang", ""], ["Tian", "Qi", ""]]}, {"id": "1905.07542", "submitter": "Ali Jahani Amiri", "authors": "Ali Jahani Amiri, Shing Yan Loo, Hong Zhang", "title": "Semi-Supervised Monocular Depth Estimation with Left-Right Consistency\n  Using Deep Neural Network", "comments": "Submitted to IROS2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There has been tremendous research progress in estimating the depth of a\nscene from a monocular camera image. Existing methods for single-image depth\nprediction are exclusively based on deep neural networks, and their training\ncan be unsupervised using stereo image pairs, supervised using LiDAR point\nclouds, or semi-supervised using both stereo and LiDAR. In general,\nsemi-supervised training is preferred as it does not suffer from the weaknesses\nof either supervised training, resulting from the difference in the cameras and\nthe LiDARs field of view, or unsupervised training, resulting from the poor\ndepth accuracy that can be recovered from a stereo pair. In this paper, we\npresent our research in single image depth prediction using semi-supervised\ntraining that outperforms the state-of-the-art. We achieve this through a loss\nfunction that explicitly exploits left-right consistency in a stereo\nreconstruction, which has not been adopted in previous semi-supervised\ntraining. In addition, we describe the correct use of ground truth depth\nderived from LiDAR that can significantly reduce prediction error. The\nperformance of our depth prediction model is evaluated on popular datasets, and\nthe importance of each aspect of our semi-supervised training approach is\ndemonstrated through experimental results. Our deep neural network model has\nbeen made publicly available.\n", "versions": [{"version": "v1", "created": "Sat, 18 May 2019 06:07:56 GMT"}], "update_date": "2019-05-21", "authors_parsed": [["Amiri", "Ali Jahani", ""], ["Loo", "Shing Yan", ""], ["Zhang", "Hong", ""]]}, {"id": "1905.07553", "submitter": "Trevor Standley", "authors": "Trevor Standley, Amir R. Zamir, Dawn Chen, Leonidas Guibas, Jitendra\n  Malik and Silvio Savarese", "title": "Which Tasks Should Be Learned Together in Multi-task Learning?", "comments": "Presented to ICML 2020 See project website at\n  http://taskgrouping.stanford.edu/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many computer vision applications require solving multiple tasks in\nreal-time. A neural network can be trained to solve multiple tasks\nsimultaneously using multi-task learning. This can save computation at\ninference time as only a single network needs to be evaluated. Unfortunately,\nthis often leads to inferior overall performance as task objectives can\ncompete, which consequently poses the question: which tasks should and should\nnot be learned together in one network when employing multi-task learning? We\nstudy task cooperation and competition in several different learning settings\nand propose a framework for assigning tasks to a few neural networks such that\ncooperating tasks are computed by the same neural network, while competing\ntasks are computed by different networks. Our framework offers a time-accuracy\ntrade-off and can produce better accuracy using less inference time than not\nonly a single large multi-task neural network but also many single-task\nnetworks.\n", "versions": [{"version": "v1", "created": "Sat, 18 May 2019 08:20:14 GMT"}, {"version": "v2", "created": "Tue, 21 May 2019 06:42:04 GMT"}, {"version": "v3", "created": "Thu, 9 Jul 2020 06:07:41 GMT"}, {"version": "v4", "created": "Thu, 3 Sep 2020 00:03:26 GMT"}], "update_date": "2020-09-04", "authors_parsed": [["Standley", "Trevor", ""], ["Zamir", "Amir R.", ""], ["Chen", "Dawn", ""], ["Guibas", "Leonidas", ""], ["Malik", "Jitendra", ""], ["Savarese", "Silvio", ""]]}, {"id": "1905.07650", "submitter": "Chaitanya Kaul", "authors": "Chaitanya Kaul, Nick Pears, Suresh Manandhar", "title": "SAWNet: A Spatially Aware Deep Neural Network for 3D Point Cloud\n  Processing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks have established themselves as the state-of-the-art\nmethodology in almost all computer vision tasks to date. But their application\nto processing data lying on non-Euclidean domains is still a very active area\nof research. One such area is the analysis of point cloud data which poses a\nchallenge due to its lack of order. Many recent techniques have been proposed,\nspearheaded by the PointNet architecture. These techniques use either global or\nlocal information from the point clouds to extract a latent representation for\nthe points, which is then used for the task at hand\n(classification/segmentation). In our work, we introduce a neural network layer\nthat combines both global and local information to produce better embeddings of\nthese points. We enhance our architecture with residual connections, to pass\ninformation between the layers, which also makes the network easier to train.\nWe achieve state-of-the-art results on the ModelNet40 dataset with our\narchitecture, and our results are also highly competitive with the\nstate-of-the-art on the ShapeNet part segmentation dataset and the indoor scene\nsegmentation dataset. We plan to open source our pre-trained models on github\nto encourage the research community to test our networks on their data, or\nsimply use them for benchmarking purposes.\n", "versions": [{"version": "v1", "created": "Sat, 18 May 2019 22:04:40 GMT"}], "update_date": "2019-05-21", "authors_parsed": [["Kaul", "Chaitanya", ""], ["Pears", "Nick", ""], ["Manandhar", "Suresh", ""]]}, {"id": "1905.07666", "submitter": "Takahiro Itazuri", "authors": "Takahiro Itazuri, Yoshihiro Fukuhara, Hirokatsu Kataoka, Shigeo\n  Morishima", "title": "What Do Adversarially Robust Models Look At?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we address the open question: \"What do adversarially robust\nmodels look at?\" Recently, it has been reported in many works that there exists\nthe trade-off between standard accuracy and adversarial robustness. According\nto prior works, this trade-off is rooted in the fact that adversarially robust\nand standard accurate models might depend on very different sets of features.\nHowever, it has not been well studied what kind of difference actually exists.\nIn this paper, we analyze this difference through various experiments visually\nand quantitatively. Experimental results show that adversarially robust models\nlook at things at a larger scale than standard models and pay less attention to\nfine textures. Furthermore, although it has been claimed that adversarially\nrobust features are not compatible with standard accuracy, there is even a\npositive effect by using them as pre-trained models particularly in low\nresolution datasets.\n", "versions": [{"version": "v1", "created": "Sun, 19 May 2019 00:06:08 GMT"}], "update_date": "2019-05-21", "authors_parsed": [["Itazuri", "Takahiro", ""], ["Fukuhara", "Yoshihiro", ""], ["Kataoka", "Hirokatsu", ""], ["Morishima", "Shigeo", ""]]}, {"id": "1905.07700", "submitter": "Chao Tan", "authors": "Chao Tan, Xin Feng, Jianwu Long, Li Geng", "title": "FORECAST-CLSTM: A New Convolutional LSTM Network for Cloudage Nowcasting", "comments": null, "journal-ref": "IEEE Conference of Visual Communications and Image Processing 2018", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the highly demand of large-scale and real-time weather service for\npublic, a refinement of short-time cloudage prediction has become an essential\npart of the weather forecast productions. To provide a\nweather-service-compliant cloudage nowcasting, in this paper, we propose a\nnovel hierarchical Convolutional Long-Short-Term Memory network based deep\nlearning model, which we term as FORECAST-CLSTM, with a new Forecaster loss\nfunction to predict the future satellite cloud images. The model is designed to\nfuse multi-scale features in the hierarchical network structure to predict the\npixel value and the morphological movement of the cloudage simultaneously. We\nalso collect about 40K infrared satellite nephograms and create a large-scale\nSatellite Cloudage Map Dataset(SCMD). The proposed FORECAST-CLSTM model is\nshown to achieve better prediction performance compared with the\nstate-of-the-art ConvLSTM model and the proposed Forecaster Loss Function is\nalso demonstrated to retain the uncertainty of the real atmosphere condition\nbetter than conventional loss function.\n", "versions": [{"version": "v1", "created": "Sun, 19 May 2019 07:34:57 GMT"}], "update_date": "2019-05-21", "authors_parsed": [["Tan", "Chao", ""], ["Feng", "Xin", ""], ["Long", "Jianwu", ""], ["Geng", "Li", ""]]}, {"id": "1905.07709", "submitter": "Ali Akbar Kiaei Khoshroudbari", "authors": "Ali A. Kiaei, Hassan Khotanlou, Mahdi Abbasi, Paniz Kiaei, Yasin\n  Bhrouzi", "title": "An Objective Evaluation Metric for image fusion based on Del Operator", "comments": "22 pages, 14 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, a novel objective evaluation metric for image fusion is\npresented. Remarkable and attractive points of the proposed metric are that it\nhas no parameter, the result is probability in the range of [0, 1] and it is\nfree from illumination dependence. This metric is easy to implement and the\nresult is computed in four steps: (1) Smoothing the images using Gaussian\nfilter. (2) Transforming images to a vector field using Del operator. (3)\nComputing the normal distribution function ({\\mu},{\\sigma}) for each\ncorresponding pixel, and converting to the standard normal distribution\nfunction. (4) Computing the probability of being well-behaved fusion method as\nthe result. To judge the quality of the proposed metric, it is compared to\nthirteen well-known non-reference objective evaluation metrics, where eight\nfusion methods are employed on seven experiments of multimodal medical images.\nThe experimental results and statistical comparisons show that in contrast to\nthe previously objective evaluation metrics the proposed one performs better in\nterms of both agreeing with human visual perception and evaluating fusion\nmethods that are not performed at the same level.\n", "versions": [{"version": "v1", "created": "Sun, 19 May 2019 08:41:51 GMT"}, {"version": "v2", "created": "Sat, 25 May 2019 14:36:12 GMT"}], "update_date": "2019-05-28", "authors_parsed": [["Kiaei", "Ali A.", ""], ["Khotanlou", "Hassan", ""], ["Abbasi", "Mahdi", ""], ["Kiaei", "Paniz", ""], ["Bhrouzi", "Yasin", ""]]}, {"id": "1905.07710", "submitter": "Sulaiman Vesal", "authors": "Sulaiman Vesal, Nishant Ravikumar, Andreas Maier", "title": "A 2D dilated residual U-Net for multi-organ segmentation in thoracic CT", "comments": "ISBI-SegTHOR 2019 Challenge", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic segmentation of organs-at-risk (OAR) in computed tomography (CT) is\nan essential part of planning effective treatment strategies to combat lung and\nesophageal cancer. Accurate segmentation of organs surrounding tumours helps\naccount for the variation in position and morphology inherent across patients,\nthereby facilitating adaptive and computer-assisted radiotherapy. Although\nmanual delineation of OARs is still highly prevalent, it is prone to errors due\nto complex variations in the shape and position of organs across patients, and\nlow soft tissue contrast between neighbouring organs in CT images. Recently,\ndeep convolutional neural networks (CNNs) have gained tremendous traction and\nachieved state-of-the-art results in medical image segmentation. In this paper,\nwe propose a deep learning framework to segment OARs in thoracic CT images,\nspecifically for the: heart, esophagus, trachea and aorta. Our approach employs\ndilated convolutions and aggregated residual connections in the bottleneck of a\nU-Net styled network, which incorporates global context and dense information.\nOur method achieved an overall Dice score of 91.57% on 20 unseen test samples\nfrom the ISBI 2019 SegTHOR challenge.\n", "versions": [{"version": "v1", "created": "Sun, 19 May 2019 08:53:13 GMT"}], "update_date": "2019-05-21", "authors_parsed": [["Vesal", "Sulaiman", ""], ["Ravikumar", "Nishant", ""], ["Maier", "Andreas", ""]]}, {"id": "1905.07718", "submitter": "Zhe Wang", "authors": "Zhe Wang, Liyan Chen, Shaurya Rathore, Daeyun Shin, Charless Fowlkes", "title": "Geometric Pose Affordance: 3D Human Pose with Scene Constraints", "comments": "$\\href{https://wangzheallen.github.io/GPA.html}{Project Page}$", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Full 3D estimation of human pose from a single image remains a challenging\ntask despite many recent advances. In this paper, we explore the hypothesis\nthat strong prior information about scene geometry can be used to improve pose\nestimation accuracy. To tackle this question empirically, we have assembled a\nnovel $\\textbf{Geometric Pose Affordance}$ dataset, consisting of multi-view\nimagery of people interacting with a variety of rich 3D environments. We\nutilized a commercial motion capture system to collect gold-standard estimates\nof pose and construct accurate geometric 3D CAD models of the scene itself.\n  To inject prior knowledge of scene constraints into existing frameworks for\npose estimation from images, we introduce a novel, view-based representation of\nscene geometry, a $\\textbf{multi-layer depth map}$, which employs multi-hit ray\ntracing to concisely encode multiple surface entry and exit points along each\ncamera view ray direction. We propose two different mechanisms for integrating\nmulti-layer depth information pose estimation: input as encoded ray features\nused in lifting 2D pose to full 3D, and secondly as a differentiable loss that\nencourages learned models to favor geometrically consistent pose estimates. We\nshow experimentally that these techniques can improve the accuracy of 3D pose\nestimates, particularly in the presence of occlusion and complex scene\ngeometry.\n", "versions": [{"version": "v1", "created": "Sun, 19 May 2019 10:04:52 GMT"}], "update_date": "2019-05-21", "authors_parsed": [["Wang", "Zhe", ""], ["Chen", "Liyan", ""], ["Rathore", "Shaurya", ""], ["Shin", "Daeyun", ""], ["Fowlkes", "Charless", ""]]}, {"id": "1905.07733", "submitter": "Thomas Brunner", "authors": "Thomas Brunner, Frederik Diehl, Michael Truong Le, Alois Knoll", "title": "Leveraging Semantic Embeddings for Safety-Critical Applications", "comments": "Accepted at CVPR 2019 Workshop: Safe Artificial Intelligence for\n  Automated Driving", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semantic Embeddings are a popular way to represent knowledge in the field of\nzero-shot learning. We observe their interpretability and discuss their\npotential utility in a safety-critical context. Concretely, we propose to use\nthem to add introspection and error detection capabilities to neural network\nclassifiers. First, we show how to create embeddings from symbolic domain\nknowledge. We discuss how to use them for interpreting mispredictions and\npropose a simple error detection scheme. We then introduce the concept of\nsemantic distance: a real-valued score that measures confidence in the semantic\nspace. We evaluate this score on a traffic sign classifier and find that it\nachieves near state-of-the-art performance, while being significantly faster to\ncompute than other confidence scores. Our approach requires no changes to the\noriginal network and is thus applicable to any task for which domain knowledge\nis available.\n", "versions": [{"version": "v1", "created": "Sun, 19 May 2019 12:42:41 GMT"}], "update_date": "2019-05-21", "authors_parsed": [["Brunner", "Thomas", ""], ["Diehl", "Frederik", ""], ["Le", "Michael Truong", ""], ["Knoll", "Alois", ""]]}, {"id": "1905.07754", "submitter": "Fredrik Vatsendvik", "authors": "{\\AA}smund Brekke, Fredrik Vatsendvik, Frank Lindseth", "title": "Multimodal 3D Object Detection from Simulated Pretraining", "comments": "12 pages, part of proceedings for the NAIS 2019 symposium", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The need for simulated data in autonomous driving applications has become\nincreasingly important, both for validation of pretrained models and for\ntraining new models. In order for these models to generalize to real-world\napplications, it is critical that the underlying dataset contains a variety of\ndriving scenarios and that simulated sensor readings closely mimics real-world\nsensors. We present the Carla Automated Dataset Extraction Tool (CADET), a\nnovel tool for generating training data from the CARLA simulator to be used in\nautonomous driving research. The tool is able to export high-quality,\nsynchronized LIDAR and camera data with object annotations, and offers\nconfiguration to accurately reflect a real-life sensor array. Furthermore, we\nuse this tool to generate a dataset consisting of 10 000 samples and use this\ndataset in order to train the 3D object detection network AVOD-FPN, with\nfinetuning on the KITTI dataset in order to evaluate the potential for\neffective pretraining. We also present two novel LIDAR feature map\nconfigurations in Bird's Eye View for use with AVOD-FPN that can be easily\nmodified. These configurations are tested on the KITTI and CADET datasets in\norder to evaluate their performance as well as the usability of the simulated\ndataset for pretraining. Although insufficient to fully replace the use of real\nworld data, and generally not able to exceed the performance of systems fully\ntrained on real data, our results indicate that simulated data can considerably\nreduce the amount of training on real data required to achieve satisfactory\nlevels of accuracy.\n", "versions": [{"version": "v1", "created": "Sun, 19 May 2019 15:13:41 GMT"}], "update_date": "2019-05-21", "authors_parsed": [["Brekke", "\u00c5smund", ""], ["Vatsendvik", "Fredrik", ""], ["Lindseth", "Frank", ""]]}, {"id": "1905.07797", "submitter": "Yipu Zhao", "authors": "Yipu Zhao, Wenkai Ye, Patricio A. Vela", "title": "Low-latency Visual SLAM with Appearance-Enhanced Local Map Building", "comments": "7 pages, 6 figures, accepted at ICRA 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A local map module is often implemented in modern VO/VSLAM systems to improve\ndata association and pose estimation. Conventionally, the local map contents\nare determined by co-visibility. While co-visibility is cheap to establish, it\nutilizes the relatively-weak temporal prior (i.e. seen before, likely to be\nseen now), therefore admitting more features into the local map than necessary.\nThis paper describes an enhancement to co-visibility local map building by\nincorporating a strong appearance prior, which leads to a more compact local\nmap and latency reduction in downstream data association. The appearance prior\ncollected from the current image influences the local map contents: only the\nmap features visually similar to the current measurements are potentially\nuseful for data association. To that end, mapped features are indexed and\nqueried with Multi-index Hashing (MIH). An online hash table selection\nalgorithm is developed to further reduce the query overhead of MIH and the\nlocal map size. The proposed appearance-based local map building method is\nintegrated into a state-of-the-art VO/VSLAM system. When evaluated on two\npublic benchmarks, the size of the local map, as well as the latency of\nreal-time pose tracking in VO/VSLAM are significantly reduced. Meanwhile, the\nVO/VSLAM mean performance is preserved or improves.\n", "versions": [{"version": "v1", "created": "Sun, 19 May 2019 19:34:03 GMT"}], "update_date": "2019-05-21", "authors_parsed": [["Zhao", "Yipu", ""], ["Ye", "Wenkai", ""], ["Vela", "Patricio A.", ""]]}, {"id": "1905.07808", "submitter": "Yipu Zhao", "authors": "Wenkai Ye, Yipu Zhao, Patricio A. Vela", "title": "Characterizing SLAM Benchmarks and Methods for the Robust Perception Age", "comments": "7 pages, 5 figures, accepted at ICRA 2019 Workshop on Dataset\n  Generation and Benchmarking of SLAM Algorithms for Robotics and VR/AR", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The diversity of SLAM benchmarks affords extensive testing of SLAM algorithms\nto understand their performance, individually or in relative terms. The ad-hoc\ncreation of these benchmarks does not necessarily illuminate the particular\nweak points of a SLAM algorithm when performance is evaluated. In this paper,\nwe propose to use a decision tree to identify challenging benchmark properties\nfor state-of-the-art SLAM algorithms and important components within the SLAM\npipeline regarding their ability to handle these challenges. Establishing what\nfactors of a particular sequence lead to track failure or degradation relative\nto these characteristics is important if we are to arrive at a strong\nunderstanding for the core computational needs of a robust SLAM algorithm.\nLikewise, we argue that it is important to profile the computational\nperformance of the individual SLAM components for use when benchmarking. In\nparticular, we advocate the use of time-dilation during ROS bag playback, or\nwhat we refer to as slo-mo playback. Using slo-mo to benchmark SLAM\ninstantiations can provide clues to how SLAM implementations should be improved\nat the computational component level. Three prevalent VO/SLAM algorithms and\ntwo low-latency algorithms of our own are tested on selected typical sequences,\nwhich are generated from benchmark characterization, to further demonstrate the\nbenefits achieved from computationally efficient components.\n", "versions": [{"version": "v1", "created": "Sun, 19 May 2019 20:51:18 GMT"}], "update_date": "2019-05-21", "authors_parsed": [["Ye", "Wenkai", ""], ["Zhao", "Yipu", ""], ["Vela", "Patricio A.", ""]]}, {"id": "1905.07817", "submitter": "Shehroz Khan", "authors": "Shehroz S. Khan, Jacob Nogas, Alex Mihailidis", "title": "Spatio-Temporal Adversarial Learning for Detecting Unseen Falls", "comments": "17 pages, 10 figures, 4 tables, 39 references", "journal-ref": "Pattern Analysis and Applications, 2020", "doi": "10.1007/s10044-020-00901-9", "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fall detection is an important problem from both the health and machine\nlearning perspective. A fall can lead to severe injuries, long term impairments\nor even death in some cases. In terms of machine learning, it presents a\nseverely class imbalance problem with very few or no training data for falls\nowing to the fact that falls occur rarely. In this paper, we take an alternate\nphilosophy to detect falls in the absence of their training data, by training\nthe classifier on only the normal activities (that are available in abundance)\nand identifying a fall as an anomaly. To realize such a classifier, we use an\nadversarial learning framework, which comprises of a spatio-temporal\nautoencoder for reconstructing input video frames and a spatio-temporal\nconvolution network to discriminate them against original video frames. 3D\nconvolutions are used to learn spatial and temporal features from the input\nvideo frames. The adversarial learning of the spatio-temporal autoencoder will\nenable reconstructing the normal activities of daily living efficiently; thus,\nrendering detecting unseen falls plausible within this framework. We tested the\nperformance of the proposed framework on camera sensing modalities that may\npreserve an individual's privacy (fully or partially), such as thermal and\ndepth camera. Our results on three publicly available datasets show that the\nproposed spatio-temporal adversarial framework performed better than other\nbaseline frame based (or spatial) adversarial learning methods.\n", "versions": [{"version": "v1", "created": "Sun, 19 May 2019 22:19:17 GMT"}, {"version": "v2", "created": "Mon, 2 Mar 2020 17:57:23 GMT"}], "update_date": "2020-07-24", "authors_parsed": [["Khan", "Shehroz S.", ""], ["Nogas", "Jacob", ""], ["Mihailidis", "Alex", ""]]}, {"id": "1905.07826", "submitter": "Jingle Jiang", "authors": "Heguang Liu and Jingle Jiang", "title": "U-Net Based Multi-instance Video Object Segmentation", "comments": "Stanford cs231n class project", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-instance video object segmentation is to segment specific instances\nthroughout a video sequence in pixel level, given only an annotated first\nframe. In this paper, we implement an effective fully convolutional networks\nwith U-Net similar structure built on top of OSVOS fine-tuned layer. We use\ninstance isolation to transform this multi-instance segmentation problem into\nbinary labeling problem, and use weighted cross entropy loss and dice\ncoefficient loss as our loss function. Our best model achieves F mean of 0.467\nand J mean of 0.424 on DAVIS dataset, which is a comparable performance with\nthe State-of-the-Art approach. But case analysis shows this model can achieve a\nsmoother contour and better instance coverage, meaning it better for recall\nfocused segmentation scenario. We also did experiments on other convolutional\nneural networks, including Seg-Net, Mask R-CNN, and provide insightful\ncomparison and discussion.\n", "versions": [{"version": "v1", "created": "Sun, 19 May 2019 23:22:49 GMT"}], "update_date": "2019-05-21", "authors_parsed": [["Liu", "Heguang", ""], ["Jiang", "Jingle", ""]]}, {"id": "1905.07831", "submitter": "Yuchi Tian", "authors": "Yuchi Tian, Ziyuan Zhong, Vicente Ordonez, Gail Kaiser, Baishakhi Ray", "title": "Testing DNN Image Classifiers for Confusion & Bias Errors", "comments": null, "journal-ref": null, "doi": "10.1145/3377811.3380400", "report-no": null, "categories": "cs.SE cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image classifiers are an important component of today's software, from\nconsumer and business applications to safety-critical domains. The advent of\nDeep Neural Networks (DNNs) is the key catalyst behind such wide-spread\nsuccess. However, wide adoption comes with serious concerns about the\nrobustness of software systems dependent on DNNs for image classification, as\nseveral severe erroneous behaviors have been reported under sensitive and\ncritical circumstances. We argue that developers need to rigorously test their\nsoftware's image classifiers and delay deployment until acceptable. We present\nan approach to testing image classifier robustness based on class property\nviolations.\n  We found that many of the reported erroneous cases in popular DNN image\nclassifiers occur because the trained models confuse one class with another or\nshow biases towards some classes over others. These bugs usually violate some\nclass properties of one or more of those classes. Most DNN testing techniques\nfocus on per-image violations, so fail to detect class-level confusions or\nbiases.\n  We developed a testing technique to automatically detect class-based\nconfusion and bias errors in DNN-driven image classification software. We\nevaluated our implementation, DeepInspect, on several popular image classifiers\nwith precision up to 100% (avg.~72.6%) for confusion errors, and up to 84.3%\n(avg.~66.8%) for bias errors. DeepInspect found hundreds of classification\nmistakes in widely-used models, many exposing errors indicating confusion or\nbias.\n", "versions": [{"version": "v1", "created": "Mon, 20 May 2019 00:00:24 GMT"}, {"version": "v2", "created": "Sat, 21 Dec 2019 23:02:13 GMT"}, {"version": "v3", "created": "Tue, 11 Feb 2020 19:32:09 GMT"}], "update_date": "2020-02-13", "authors_parsed": [["Tian", "Yuchi", ""], ["Zhong", "Ziyuan", ""], ["Ordonez", "Vicente", ""], ["Kaiser", "Gail", ""], ["Ray", "Baishakhi", ""]]}, {"id": "1905.07836", "submitter": "Alexander Wong", "authors": "Linda Wang and Alexander Wong", "title": "Enabling Computer Vision Driven Assistive Devices for the Visually\n  Impaired via Micro-architecture Design Exploration", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent improvements in object detection have shown potential to aid in tasks\nwhere previous solutions were not able to achieve. A particular area is\nassistive devices for individuals with visual impairment. While\nstate-of-the-art deep neural networks have been shown to achieve superior\nobject detection performance, their high computational and memory requirements\nmake them cost prohibitive for on-device operation. Alternatively, cloud-based\noperation leads to privacy concerns, both not attractive to potential users. To\naddress these challenges, this study investigates creating an efficient object\ndetection network specifically for OLIV, an AI-powered assistant for object\nlocalization for the visually impaired, via micro-architecture design\nexploration. In particular, we formulate the problem of finding an optimal\nnetwork micro-architecture as an numerical optimization problem, where we find\nthe set of hyperparameters controlling the MobileNetV2-SSD network\nmicro-architecture that maximizes a modified NetScore objective function for\nthe MSCOCO-OLIV dataset of indoor objects. Experimental results show that such\na micro-architecture design exploration strategy leads to a compact deep neural\nnetwork with a balanced trade-off between accuracy, size, and speed, making it\nwell-suited for enabling on-device computer vision driven assistive devices for\nthe visually impaired.\n", "versions": [{"version": "v1", "created": "Mon, 20 May 2019 01:30:15 GMT"}], "update_date": "2019-05-21", "authors_parsed": [["Wang", "Linda", ""], ["Wong", "Alexander", ""]]}, {"id": "1905.07841", "submitter": "Zhou Yu", "authors": "Jun Yu, Jing Li, Zhou Yu, Qingming Huang", "title": "Multimodal Transformer with Multi-View Visual Representation for Image\n  Captioning", "comments": "submitted to a journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image captioning aims to automatically generate a natural language\ndescription of a given image, and most state-of-the-art models have adopted an\nencoder-decoder framework. The framework consists of a convolution neural\nnetwork (CNN)-based image encoder that extracts region-based visual features\nfrom the input image, and an recurrent neural network (RNN)-based caption\ndecoder that generates the output caption words based on the visual features\nwith the attention mechanism. Despite the success of existing studies, current\nmethods only model the co-attention that characterizes the inter-modal\ninteractions while neglecting the self-attention that characterizes the\nintra-modal interactions. Inspired by the success of the Transformer model in\nmachine translation, here we extend it to a Multimodal Transformer (MT) model\nfor image captioning. Compared to existing image captioning approaches, the MT\nmodel simultaneously captures intra- and inter-modal interactions in a unified\nattention block. Due to the in-depth modular composition of such attention\nblocks, the MT model can perform complex multimodal reasoning and output\naccurate captions. Moreover, to further improve the image captioning\nperformance, multi-view visual features are seamlessly introduced into the MT\nmodel. We quantitatively and qualitatively evaluate our approach using the\nbenchmark MSCOCO image captioning dataset and conduct extensive ablation\nstudies to investigate the reasons behind its effectiveness. The experimental\nresults show that our method significantly outperforms the previous\nstate-of-the-art methods. With an ensemble of seven models, our solution ranks\nthe 1st place on the real-time leaderboard of the MSCOCO image captioning\nchallenge at the time of the writing of this paper.\n", "versions": [{"version": "v1", "created": "Mon, 20 May 2019 01:56:06 GMT"}], "update_date": "2019-05-21", "authors_parsed": [["Yu", "Jun", ""], ["Li", "Jing", ""], ["Yu", "Zhou", ""], ["Huang", "Qingming", ""]]}, {"id": "1905.07844", "submitter": "Alexander Wong", "authors": "Linda Wang and Alexander Wong", "title": "Implications of Computer Vision Driven Assistive Technologies Towards\n  Individuals with Visual Impairment", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computer vision based technology is becoming ubiquitous in society. One\napplication area that has seen an increase in computer vision is assistive\ntechnologies, specifically for those with visual impairment. Research has shown\nthe ability of computer vision models to achieve tasks such provide scene\ncaptions, detect objects and recognize faces. Although assisting individuals\nwith visual impairment with these tasks increases their independence and\nautonomy, concerns over bias, privacy and potential usefulness arise. This\npaper addresses the positive and negative implications computer vision based\nassistive technologies have on individuals with visual impairment, as well as\nconsiderations for computer vision researchers and developers in order to\nmitigate the amount of negative implications.\n", "versions": [{"version": "v1", "created": "Mon, 20 May 2019 02:00:56 GMT"}], "update_date": "2019-05-21", "authors_parsed": [["Wang", "Linda", ""], ["Wong", "Alexander", ""]]}, {"id": "1905.07852", "submitter": "Evgeny Burnaev", "authors": "Alexey Bokhovkin and Evgeny Burnaev", "title": "Boundary Loss for Remote Sensing Imagery Semantic Segmentation", "comments": "14 pages, 10 figures", "journal-ref": "Proceedings of 16th International Symposium on Neural Networks,\n  2019", "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In response to the growing importance of geospatial data, its analysis\nincluding semantic segmentation becomes an increasingly popular task in\ncomputer vision today. Convolutional neural networks are powerful visual models\nthat yield hierarchies of features and practitioners widely use them to process\nremote sensing data. When performing remote sensing image segmentation,\nmultiple instances of one class with precisely defined boundaries are often the\ncase, and it is crucial to extract those boundaries accurately. The accuracy of\nsegments boundaries delineation influences the quality of the whole segmented\nareas explicitly. However, widely-used segmentation loss functions such as BCE,\nIoU loss or Dice loss do not penalize misalignment of boundaries sufficiently.\nIn this paper, we propose a novel loss function, namely a differentiable\nsurrogate of a metric accounting accuracy of boundary detection. We can use the\nloss function with any neural network for binary segmentation. We performed\nvalidation of our loss function with various modifications of UNet on a\nsynthetic dataset, as well as using real-world data (ISPRS Potsdam, INRIA AIL).\nTrained with the proposed loss function, models outperform baseline methods in\nterms of IoU score.\n", "versions": [{"version": "v1", "created": "Mon, 20 May 2019 03:02:44 GMT"}], "update_date": "2019-05-21", "authors_parsed": [["Bokhovkin", "Alexey", ""], ["Burnaev", "Evgeny", ""]]}, {"id": "1905.07853", "submitter": "Xingyu Liu", "authors": "Xingyu Liu, Joon-Young Lee, Hailin Jin", "title": "Learning Video Representations from Correspondence Proposals", "comments": "CVPR 2019 (Oral)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Correspondences between frames encode rich information about dynamic content\nin videos. However, it is challenging to effectively capture and learn those\ndue to their irregular structure and complex dynamics. In this paper, we\npropose a novel neural network that learns video representations by aggregating\ninformation from potential correspondences. This network, named $CPNet$, can\nlearn evolving 2D fields with temporal consistency. In particular, it can\neffectively learn representations for videos by mixing appearance and\nlong-range motion with an RGB-only input. We provide extensive ablation\nexperiments to validate our model. CPNet shows stronger performance than\nexisting methods on Kinetics and achieves the state-of-the-art performance on\nSomething-Something and Jester. We provide analysis towards the behavior of our\nmodel and show its robustness to errors in proposals.\n", "versions": [{"version": "v1", "created": "Mon, 20 May 2019 03:07:51 GMT"}], "update_date": "2019-05-21", "authors_parsed": [["Liu", "Xingyu", ""], ["Lee", "Joon-Young", ""], ["Jin", "Hailin", ""]]}, {"id": "1905.07862", "submitter": "Jue Wang", "authors": "Jue Wang, Shaoli Huang, Xinchao Wang, Dacheng Tao", "title": "Not All Parts Are Created Equal: 3D Pose Estimation by Modelling\n  Bi-directional Dependencies of Body Parts", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Not all the human body parts have the same~degree of freedom~(DOF) due to the\nphysiological structure. For example, the limbs may move more flexibly and\nfreely than the torso does. Most of the existing 3D pose estimation methods,\ndespite the very promising results achieved, treat the body joints equally and\nconsequently often lead to larger reconstruction errors on the limbs. In this\npaper, we propose a progressive approach that explicitly accounts for the\ndistinct DOFs among the body parts. We model parts with higher DOFs like the\nelbows, as dependent components of the corresponding parts with lower DOFs like\nthe torso, of which the 3D locations can be more reliably estimated. Meanwhile,\nthe high-DOF parts may, in turn, impose a constraint on where the low-DOF ones\nlie. As a result, parts with different DOFs supervise one another, yielding\nphysically constrained and plausible pose-estimation results. To further\nfacilitate the prediction of the high-DOF parts, we introduce a pose-attribute\nestimation, where the relative location of a limb joint with respect to the\ntorso, which has the least DOF of a human body, is explicitly estimated and\nfurther fed to the joint-estimation module. The proposed approach achieves very\npromising results, outperforming the state of the art on several benchmarks.\n", "versions": [{"version": "v1", "created": "Mon, 20 May 2019 04:01:28 GMT"}], "update_date": "2019-05-21", "authors_parsed": [["Wang", "Jue", ""], ["Huang", "Shaoli", ""], ["Wang", "Xinchao", ""], ["Tao", "Dacheng", ""]]}, {"id": "1905.07877", "submitter": "Evgeny Burnaev", "authors": "Maria Kolos and Anton Marin and Alexey Artemov and Evgeny Burnaev", "title": "Procedural Synthesis of Remote Sensing Images for Robust Change\n  Detection with Neural Networks", "comments": "17 pages, 11 figures", "journal-ref": "16th International Symposium on Neural Networks, ISNN 2019", "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data-driven methods such as convolutional neural networks (CNNs) are known to\ndeliver state-of-the-art performance on image recognition tasks when the\ntraining data are abundant. However, in some instances, such as change\ndetection in remote sensing images, annotated data cannot be obtained in\nsufficient quantities. In this work, we propose a simple and efficient method\nfor creating realistic targeted synthetic datasets in the remote sensing\ndomain, leveraging the opportunities offered by game development engines. We\nprovide a description of the pipeline for procedural geometry generation and\nrendering as well as an evaluation of the efficiency of produced datasets in a\nchange detection scenario. Our evaluations demonstrate that our pipeline helps\nto improve the performance and convergence of deep learning models when the\namount of real-world data is severely limited.\n", "versions": [{"version": "v1", "created": "Mon, 20 May 2019 05:24:33 GMT"}], "update_date": "2019-05-21", "authors_parsed": [["Kolos", "Maria", ""], ["Marin", "Anton", ""], ["Artemov", "Alexey", ""], ["Burnaev", "Evgeny", ""]]}, {"id": "1905.07898", "submitter": "Jianfeng Wang", "authors": "Jianfeng Wang, Rong Xiao, Yandong Guo, Lei Zhang", "title": "Learning to Count Objects with Few Exemplar Annotations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study the problem of object counting with incomplete\nannotations. Based on the observation that in many object counting problems the\ntarget objects are normally repeated and highly similar to each other, we are\nparticularly interested in the setting when only a few exemplar annotations are\nprovided. Directly applying object detection with incomplete annotations will\nresult in severe accuracy degradation due to its improper handling of unlabeled\nobject instances. To address the problem, we propose a positiveness-focused\nobject detector (PFOD) to progressively propagate the incomplete labels before\napplying the general object detection algorithm. The PFOD focuses on the\npositive samples and ignore the negative instances at most of the learning\ntime. This strategy, though simple, dramatically boosts the object counting\naccuracy. On the CARPK dataset for parking lot car counting, we improved\nmAP@0.5 from 4.58% to 72.44% using only 5 training images each with 5 bounding\nboxes. On the Drink35 dataset for shelf product counting, the mAP@0.5 is\nimproved from 14.16% to 53.73% using 10 training images each with 5 bounding\nboxes.\n", "versions": [{"version": "v1", "created": "Mon, 20 May 2019 06:28:44 GMT"}], "update_date": "2019-05-21", "authors_parsed": [["Wang", "Jianfeng", ""], ["Xiao", "Rong", ""], ["Guo", "Yandong", ""], ["Zhang", "Lei", ""]]}, {"id": "1905.07917", "submitter": "Luc Brun", "authors": "Xuan Nguyen, Luc Brun, Olivier Lezoray, S\\'ebastien Bougleux", "title": "Skeleton-Based Hand Gesture Recognition by Learning SPD Matrices with\n  Neural Networks", "comments": null, "journal-ref": "14th IEEE International Conference on Automatic Face and Gesture\n  Recognition, May 2019, Lille, France", "doi": null, "report-no": null, "categories": "cs.CV cs.HC cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a new hand gesture recognition method based on\nskeletal data by learning SPD matrices with neural networks. We model the hand\nskeleton as a graph and introduce a neural network for SPD matrix learning,\ntaking as input the 3D coordinates of hand joints. The proposed network is\nbased on two newly designed layers that transform a set of SPD matrices into a\nSPD matrix. For gesture recognition, we train a linear SVM classifier using\nfeatures extracted from our network. Experimental results on a challenging\ndataset (Dynamic Hand Gesture dataset from the SHREC 2017 3D Shape Retrieval\nContest) show that the proposed method outperforms state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Mon, 20 May 2019 07:10:49 GMT"}], "update_date": "2019-05-21", "authors_parsed": [["Nguyen", "Xuan", ""], ["Brun", "Luc", ""], ["Lezoray", "Olivier", ""], ["Bougleux", "S\u00e9bastien", ""]]}, {"id": "1905.07918", "submitter": "Celine Loscos", "authors": "Jennifer Bonnard (CRESTIC), Gilles Valette (CRESTIC), C\\'eline Loscos\n  (CRESTIC)", "title": "Disparity-based HDR imaging", "comments": "Digital Image & Signal Processing, Apr 2019, St Hugh's College,\n  Oxford University, United Kingdom, United Kingdom", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High-dynamic range imaging permits to extend the dynamic range of intensity\nvalues to get close to what the human eye is able to perceive. Although there\nhas been a huge progress in the digital camera sensor range capacity, the need\nof capturing several exposures in order to reconstruct high-dynamic range\nvalues persist. In this paper, we present a study on how to acquire\nhigh-dynamic range values for multi-stereo images. In many papers, disparity\nhas been used to register pixels of different images and guide the\nreconstruction. In this paper, we show the limitations of such approaches and\npropose heuristics as solutions to identified problematic cases.\n", "versions": [{"version": "v1", "created": "Mon, 20 May 2019 07:13:08 GMT"}], "update_date": "2019-05-21", "authors_parsed": [["Bonnard", "Jennifer", "", "CRESTIC"], ["Valette", "Gilles", "", "CRESTIC"], ["Loscos", "C\u00e9line", "", "CRESTIC"]]}, {"id": "1905.07922", "submitter": "Yangbin Lin", "authors": "Yangbin Lin, Jialian Li, Cheng Wang, Zhonggui Chen, Zongyue Wang, and\n  Jonathan Li", "title": "Fast Regularity-Constrained Plane Reconstruction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Man-made environments typically comprise planar structures that exhibit\nnumerous geometric relationships, such as parallelism, coplanarity, and\northogonality. Making full use of these relationships can considerably improve\nthe robustness of algorithmic plane reconstruction of complex scenes. This\nresearch leverages a constraint model requiring minimal prior knowledge to\nimplicitly establish relationships among planes. We introduce a method based on\nenergy minimization to reconstruct the planes consistent with our constraint\nmodel. The proposed algorithm is efficient, easily to understand, and simple to\nimplement. The experimental results show that our algorithm successfully\nreconstructs planes under high percentages of noise and outliers. This is\nsuperior to other state-of-the-art regularity-constrained plane reconstruction\nmethods in terms of speed and robustness.\n", "versions": [{"version": "v1", "created": "Mon, 20 May 2019 07:32:02 GMT"}], "update_date": "2019-05-21", "authors_parsed": [["Lin", "Yangbin", ""], ["Li", "Jialian", ""], ["Wang", "Cheng", ""], ["Chen", "Zhonggui", ""], ["Wang", "Zongyue", ""], ["Li", "Jonathan", ""]]}, {"id": "1905.07933", "submitter": "Xiaofeng Xu", "authors": "Xiaofeng Xu, Ivor W. Tsang, Xiaofeng Cao, Ruiheng Zhang and Chuancai\n  Liu", "title": "Learning Image-Specific Attributes by Hyperbolic Neighborhood Graph\n  Propagation", "comments": "Accepted for IJCAI 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As a kind of semantic representation of visual object descriptions,\nattributes are widely used in various computer vision tasks. In most of\nexisting attribute-based research, class-specific attributes (CSA), which are\nclass-level annotations, are usually adopted due to its low annotation cost for\neach class instead of each individual image. However, class-specific attributes\nare usually noisy because of annotation errors and diversity of individual\nimages. Therefore, it is desirable to obtain image-specific attributes (ISA),\nwhich are image-level annotations, from the original class-specific attributes.\nIn this paper, we propose to learn image-specific attributes by graph-based\nattribute propagation. Considering the intrinsic property of hyperbolic\ngeometry that its distance expands exponentially, hyperbolic neighborhood graph\n(HNG) is constructed to characterize the relationship between samples. Based on\nHNG, we define neighborhood consistency for each sample to identify\ninconsistent samples. Subsequently, inconsistent samples are refined based on\ntheir neighbors in HNG. Extensive experiments on five benchmark datasets\ndemonstrate the significant superiority of the learned image-specific\nattributes over the original class-specific attributes in the zero-shot object\nclassification task.\n", "versions": [{"version": "v1", "created": "Mon, 20 May 2019 08:05:47 GMT"}, {"version": "v2", "created": "Sat, 25 May 2019 02:19:27 GMT"}], "update_date": "2019-05-28", "authors_parsed": [["Xu", "Xiaofeng", ""], ["Tsang", "Ivor W.", ""], ["Cao", "Xiaofeng", ""], ["Zhang", "Ruiheng", ""], ["Liu", "Chuancai", ""]]}, {"id": "1905.07967", "submitter": "Jonas Tebbe", "authors": "Jonas Tebbe, Lukas Klamt, Yapeng Gao, and Andreas Zell", "title": "Spin Detection in Robotic Table Tennis", "comments": "submitted to ICRA 2020", "journal-ref": "2020 IEEE International Conference on Robotics and Automation\n  (ICRA), Paris, France, 2020, pp. 9694-9700", "doi": "10.1109/ICRA40945.2020.9196536", "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In table tennis, the rotation (spin) of the ball plays a crucial role. A\ntable tennis match will feature a variety of strokes. Each generates different\namounts and types of spin. To develop a robot that can compete with a human\nplayer, the robot needs to detect spin, so it can plan an appropriate return\nstroke. In this paper we compare three methods to estimate spin. The first two\napproaches use a high-speed camera that captures the ball in flight at a frame\nrate of 380 Hz. This camera allows the movement of the circular brand logo\nprinted on the ball to be seen. The first approach uses background difference\nto determine the position of the logo. In a second alternative, we train a CNN\nto predict the orientation of the logo. The third method evaluates the\ntrajectory of the ball and derives the rotation from the effect of the Magnus\nforce. This method gives the highest accuracy and is used for a demonstration.\nOur robot successfully copes with different spin types in a real table tennis\nrally against a human opponent.\n", "versions": [{"version": "v1", "created": "Mon, 20 May 2019 10:01:32 GMT"}, {"version": "v2", "created": "Wed, 16 Oct 2019 16:25:33 GMT"}], "update_date": "2020-11-09", "authors_parsed": [["Tebbe", "Jonas", ""], ["Klamt", "Lukas", ""], ["Gao", "Yapeng", ""], ["Zell", "Andreas", ""]]}, {"id": "1905.07991", "submitter": "Nils Gessert", "authors": "Nils Gessert, Marcel Bengs, Lukas Wittig, Daniel Dr\\\"omann, Tobias\n  Keck, Alexander Schlaefer, David B. Ellebrecht", "title": "Deep Transfer Learning Methods for Colon Cancer Classification in\n  Confocal Laser Microscopy Images", "comments": "Accepted for publication in the International Journal of Computer\n  Assisted Radiology and Surgery (IJCARS)", "journal-ref": null, "doi": "10.1007/s11548-019-02004-1", "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Purpose: The gold standard for colorectal cancer metastases detection in the\nperitoneum is histological evaluation of a removed tissue sample. For feedback\nduring interventions, real-time in-vivo imaging with confocal laser microscopy\nhas been proposed for differentiation of benign and malignant tissue by manual\nexpert evaluation. Automatic image classification could improve the surgical\nworkflow further by providing immediate feedback.\n  Methods: We analyze the feasibility of classifying tissue from confocal laser\nmicroscopy in the colon and peritoneum. For this purpose, we adopt both\nclassical and state-of-the-art convolutional neural networks to directly learn\nfrom the images. As the available dataset is small, we investigate several\ntransfer learning strategies including partial freezing variants and full\nfine-tuning. We address the distinction of different tissue types, as well as\nbenign and malignant tissue.\n  Results: We present a thorough analysis of transfer learning strategies for\ncolorectal cancer with confocal laser microscopy. In the peritoneum, metastases\nare classified with an AUC of 97.1 and in the colon, the primarius is\nclassified with an AUC of 73.1. In general, transfer learning substantially\nimproves performance over training from scratch. We find that the optimal\ntransfer learning strategy differs for models and classification tasks.\n  Conclusions: We demonstrate that convolutional neural networks and transfer\nlearning can be used to identify cancer tissue with confocal laser microscopy.\nWe show that there is no generally optimal transfer learning strategy and model\nas well as task-specific engineering is required. Given the high performance\nfor the peritoneum, even with a small dataset, application for intraoperative\ndecision support could be feasible.\n", "versions": [{"version": "v1", "created": "Mon, 20 May 2019 11:02:21 GMT"}], "update_date": "2019-05-28", "authors_parsed": [["Gessert", "Nils", ""], ["Bengs", "Marcel", ""], ["Wittig", "Lukas", ""], ["Dr\u00f6mann", "Daniel", ""], ["Keck", "Tobias", ""], ["Schlaefer", "Alexander", ""], ["Ellebrecht", "David B.", ""]]}, {"id": "1905.08008", "submitter": "Wang Zheng", "authors": "Zheng Wang, Jianwu Li, Ge Song, Tieling Li", "title": "Less Memory, Faster Speed: Refining Self-Attention Module for Image\n  Reconstruction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Self-attention (SA) mechanisms can capture effectively global dependencies in\ndeep neural networks, and have been applied to natural language processing and\nimage processing successfully. However, SA modules for image reconstruction\nhave high time and space complexity, which restrict their applications to\nhigher-resolution images. In this paper, we refine the SA module in\nself-attention generative adversarial networks (SAGAN) via adapting a non-local\noperation, revising the connectivity among the units in SA module and\nre-implementing its computational pattern, such that its time and space\ncomplexity is reduced from $\\text{O}(n^2)$ to $\\text{O}(n)$, but it is still\nequivalent to the original SA module. Further, we explore the principles behind\nthe module and discover that our module is a special kind of channel attention\nmechanisms. Experimental results based on two benchmark datasets of image\nreconstruction, verify that under the same computational environment, two\nmodels can achieve comparable effectiveness for image reconstruction, but the\nproposed one runs faster and takes up less memory space.\n", "versions": [{"version": "v1", "created": "Mon, 20 May 2019 11:43:37 GMT"}], "update_date": "2019-05-21", "authors_parsed": [["Wang", "Zheng", ""], ["Li", "Jianwu", ""], ["Song", "Ge", ""], ["Li", "Tieling", ""]]}, {"id": "1905.08032", "submitter": "Roozbeh Rajabi", "authors": "Sara Khoshsokhan, Roozbeh Rajabi, Hadi Zayyani", "title": "Clustered Multitask Nonnegative Matrix Factorization for Spectral\n  Unmixing of Hyperspectral Data", "comments": "one column, 22 pages, 12 figures, journal. arXiv admin note:\n  substantial text overlap with arXiv:1902.07593, arXiv:1812.10788", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, the new algorithm based on clustered multitask network is\nproposed to solve spectral unmixing problem in hyperspectral imagery. In the\nproposed algorithm, the clustered network is employed. Each pixel in the\nhyperspectral image considered as a node in this network. The nodes in the\nnetwork are clustered using the fuzzy c-means clustering method. Diffusion\nleast mean square strategy has been used to optimize the proposed cost\nfunction. To evaluate the proposed method, experiments are conducted on\nsynthetic and real datasets. Simulation results based on spectral angle\ndistance, abundance angle distance and reconstruction error metrics illustrate\nthe advantage of the proposed algorithm compared with other methods.\n", "versions": [{"version": "v1", "created": "Thu, 16 May 2019 20:22:37 GMT"}], "update_date": "2019-05-21", "authors_parsed": [["Khoshsokhan", "Sara", ""], ["Rajabi", "Roozbeh", ""], ["Zayyani", "Hadi", ""]]}, {"id": "1905.08077", "submitter": "Benedikt Pf\\\"ulb", "authors": "B. Pf\\\"ulb, A. Gepperth, S. Abdullah and A. Kilian", "title": "Catastrophic forgetting: still a problem for DNNs", "comments": "10 pages, 11 figures, Artificial Neural Networks and Machine Learning\n  - ICANN 2018", "journal-ref": null, "doi": "10.1007/978-3-030-01418-6_48", "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the performance of DNNs when trained on class-incremental\nvisual problems consisting of initial training, followed by retraining with\nadded visual classes. Catastrophic forgetting (CF) behavior is measured using a\nnew evaluation procedure that aims at an application-oriented view of\nincremental learning. In particular, it imposes that model selection must be\nperformed on the initial dataset alone, as well as demanding that retraining\ncontrol be performed only using the retraining dataset, as initial dataset is\nusually too large to be kept. Experiments are conducted on class-incremental\nproblems derived from MNIST, using a variety of different DNN models, some of\nthem recently proposed to avoid catastrophic forgetting. When comparing our new\nevaluation procedure to previous approaches for assessing CF, we find their\nfindings are completely negated, and that none of the tested methods can avoid\nCF in all experiments. This stresses the importance of a realistic empirical\nmeasurement procedure for catastrophic forgetting, and the need for further\nresearch in incremental learning for DNNs.\n", "versions": [{"version": "v1", "created": "Mon, 20 May 2019 13:06:30 GMT"}], "update_date": "2019-05-21", "authors_parsed": [["Pf\u00fclb", "B.", ""], ["Gepperth", "A.", ""], ["Abdullah", "S.", ""], ["Kilian", "A.", ""]]}, {"id": "1905.08090", "submitter": "Behzad Bozorgtabar", "authors": "Behzad Bozorgtabar, Mohammad Saeed Rad, Hazim Kemal Ekenel and\n  Jean-Philippe Thiran", "title": "Using Photorealistic Face Synthesis and Domain Adaptation to Improve\n  Facial Expression Analysis", "comments": "8 pages, 8 figures, 5 tables, accepted by FG 2019. arXiv admin note:\n  substantial text overlap with arXiv:1905.00286", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cross-domain synthesizing realistic faces to learn deep models has attracted\nincreasing attention for facial expression analysis as it helps to improve the\nperformance of expression recognition accuracy despite having small number of\nreal training images. However, learning from synthetic face images can be\nproblematic due to the distribution discrepancy between low-quality synthetic\nimages and real face images and may not achieve the desired performance when\nthe learned model applies to real world scenarios. To this end, we propose a\nnew attribute guided face image synthesis to perform a translation between\nmultiple image domains using a single model. In addition, we adopt the proposed\nmodel to learn from synthetic faces by matching the feature distributions\nbetween different domains while preserving each domain's characteristics. We\nevaluate the effectiveness of the proposed approach on several face datasets on\ngenerating realistic face images. We demonstrate that the expression\nrecognition performance can be enhanced by benefiting from our face synthesis\nmodel. Moreover, we also conduct experiments on a near-infrared dataset\ncontaining facial expression videos of drivers to assess the performance using\nin-the-wild data for driver emotion recognition.\n", "versions": [{"version": "v1", "created": "Fri, 17 May 2019 12:36:52 GMT"}], "update_date": "2019-05-21", "authors_parsed": [["Bozorgtabar", "Behzad", ""], ["Rad", "Mohammad Saeed", ""], ["Ekenel", "Hazim Kemal", ""], ["Thiran", "Jean-Philippe", ""]]}, {"id": "1905.08110", "submitter": "Yiyu Wang", "authors": "Yiyu Wang, Jungang Xu, Yingfei Sun, Ben He", "title": "Image Captioning based on Deep Learning Methods: A Survey", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image captioning is a challenging task and attracting more and more attention\nin the field of Artificial Intelligence, and which can be applied to efficient\nimage retrieval, intelligent blind guidance and human-computer interaction,\netc. In this paper, we present a survey on advances in image captioning based\non Deep Learning methods, including Encoder-Decoder structure, improved methods\nin Encoder, improved methods in Decoder, and other improvements. Furthermore,\nwe discussed future research directions.\n", "versions": [{"version": "v1", "created": "Mon, 20 May 2019 13:43:52 GMT"}], "update_date": "2019-05-21", "authors_parsed": [["Wang", "Yiyu", ""], ["Xu", "Jungang", ""], ["Sun", "Yingfei", ""], ["He", "Ben", ""]]}, {"id": "1905.08114", "submitter": "Konda Reddy Mopuri", "authors": "Gaurav Kumar Nayak, Konda Reddy Mopuri, Vaisakh Shaj, R. Venkatesh\n  Babu, Anirban Chakraborty", "title": "Zero-Shot Knowledge Distillation in Deep Networks", "comments": "Accepted in ICML 2019, codes will be available at\n  https://github.com/vcl-iisc/ZSKD", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Knowledge distillation deals with the problem of training a smaller model\n(Student) from a high capacity source model (Teacher) so as to retain most of\nits performance. Existing approaches use either the training data or meta-data\nextracted from it in order to train the Student. However, accessing the dataset\non which the Teacher has been trained may not always be feasible if the dataset\nis very large or it poses privacy or safety concerns (e.g., bio-metric or\nmedical data). Hence, in this paper, we propose a novel data-free method to\ntrain the Student from the Teacher. Without even using any meta-data, we\nsynthesize the Data Impressions from the complex Teacher model and utilize\nthese as surrogates for the original training data samples to transfer its\nlearning to Student via knowledge distillation. We, therefore, dub our method\n\"Zero-Shot Knowledge Distillation\" and demonstrate that our framework results\nin competitive generalization performance as achieved by distillation using the\nactual training data samples on multiple benchmark datasets.\n", "versions": [{"version": "v1", "created": "Mon, 20 May 2019 13:49:28 GMT"}], "update_date": "2019-05-21", "authors_parsed": [["Nayak", "Gaurav Kumar", ""], ["Mopuri", "Konda Reddy", ""], ["Shaj", "Vaisakh", ""], ["Babu", "R. Venkatesh", ""], ["Chakraborty", "Anirban", ""]]}, {"id": "1905.08139", "submitter": "Eldad Klaiman", "authors": "Jacob Gildenblat and Eldad Klaiman", "title": "Self-Supervised Similarity Learning for Digital Pathology", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Using features extracted from networks pretrained on ImageNet is a common\npractice in applications of deep learning for digital pathology. However it\npresents the downside of missing domain specific image information. In digital\npathology, supervised training data is expensive and difficult to collect. We\npropose a self-supervised method for feature extraction by similarity learning\non whole slide images (WSI) that is simple to implement and allows creation of\nrobust and compact image descriptors. We train a siamese network, exploiting\nimage spatial continuity and assuming spatially adjacent tiles in the image are\nmore similar to each other than distant tiles. Our network outputs feature\nvectors of length 128, which allows dramatically lower memory storage and\nfaster processing than networks pretrained on ImageNet. We apply the method on\ndigital pathology WSIs from the Camelyon16 train set and assess and compare our\nmethod by measuring image retrieval of tumor tiles and descriptor pair distance\nratio for distant/near tiles in the Camelyon16 test set. We show that our\nmethod yields better retrieval task results than existing ImageNet based and\ngeneric self-supervised feature extraction methods. To the best of our\nknowledge, this is also the first published method for self-supervised learning\ntailored for digital pathology.\n", "versions": [{"version": "v1", "created": "Mon, 20 May 2019 14:31:26 GMT"}, {"version": "v2", "created": "Thu, 11 Jul 2019 13:41:27 GMT"}, {"version": "v3", "created": "Mon, 13 Jan 2020 11:20:38 GMT"}], "update_date": "2020-01-14", "authors_parsed": [["Gildenblat", "Jacob", ""], ["Klaiman", "Eldad", ""]]}, {"id": "1905.08170", "submitter": "Shashank Singh", "authors": "Shashank Singh, Ashish Khetan, Zohar Karnin", "title": "DARC: Differentiable ARchitecture Compression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many learning situations, resources at inference time are significantly\nmore constrained than resources at training time. This paper studies a general\nparadigm, called Differentiable ARchitecture Compression (DARC), that combines\nmodel compression and architecture search to learn models that are\nresource-efficient at inference time. Given a resource-intensive base\narchitecture, DARC utilizes the training data to learn which sub-components can\nbe replaced by cheaper alternatives. The high-level technique can be applied to\nany neural architecture, and we report experiments on state-of-the-art\nconvolutional neural networks for image classification. For a WideResNet with\n$97.2\\%$ accuracy on CIFAR-10, we improve single-sample inference speed by\n$2.28\\times$ and memory footprint by $5.64\\times$, with no accuracy loss. For a\nResNet with $79.15\\%$ Top1 accuracy on ImageNet, we improve batch inference\nspeed by $1.29\\times$ and memory footprint by $3.57\\times$ with $1\\%$ accuracy\nloss. We also give theoretical Rademacher complexity bounds in simplified\ncases, showing how DARC avoids overfitting despite over-parameterization.\n", "versions": [{"version": "v1", "created": "Mon, 20 May 2019 15:30:06 GMT"}], "update_date": "2019-05-21", "authors_parsed": [["Singh", "Shashank", ""], ["Khetan", "Ashish", ""], ["Karnin", "Zohar", ""]]}, {"id": "1905.08171", "submitter": "Qin Wang", "authors": "Qin Wang, Wen Li, Luc Van Gool", "title": "Semi-Supervised Learning by Augmented Distribution Alignment", "comments": "To appear in ICCV 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we propose a simple yet effective semi-supervised learning\napproach called Augmented Distribution Alignment. We reveal that an essential\nsampling bias exists in semi-supervised learning due to the limited number of\nlabeled samples, which often leads to a considerable empirical distribution\nmismatch between labeled data and unlabeled data. To this end, we propose to\nalign the empirical distributions of labeled and unlabeled data to alleviate\nthe bias. On one hand, we adopt an adversarial training strategy to minimize\nthe distribution distance between labeled and unlabeled data as inspired by\ndomain adaptation works. On the other hand, to deal with the small sample size\nissue of labeled data, we also propose a simple interpolation strategy to\ngenerate pseudo training samples. Those two strategies can be easily\nimplemented into existing deep neural networks. We demonstrate the\neffectiveness of our proposed approach on the benchmark SVHN and CIFAR10\ndatasets. Our code is available at \\url{https://github.com/qinenergy/adanet}.\n", "versions": [{"version": "v1", "created": "Mon, 20 May 2019 15:30:10 GMT"}, {"version": "v2", "created": "Sun, 18 Aug 2019 12:45:33 GMT"}], "update_date": "2019-08-20", "authors_parsed": [["Wang", "Qin", ""], ["Li", "Wen", ""], ["Van Gool", "Luc", ""]]}, {"id": "1905.08214", "submitter": "Majed El Helou", "authors": "Xiaoyan Zou, Ruofan Zhou, Majed El Helou, Sabine S\\\"usstrunk", "title": "Drone Shadow Tracking", "comments": "5 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Aerial videos taken by a drone not too far above the surface may contain the\ndrone's shadow projected on the scene. This deteriorates the aesthetic quality\nof videos. With the presence of other shadows, shadow removal cannot be\ndirectly applied, and the shadow of the drone must be tracked. Tracking a\ndrone's shadow in a video is, however, challenging. The varying size, shape,\nchange of orientation and drone altitude pose difficulties. The shadow can also\neasily disappear over dark areas. However, a shadow has specific properties\nthat can be leveraged, besides its geometric shape. In this paper, we\nincorporate knowledge of the shadow's physical properties, in the form of\nshadow detection masks, into a correlation-based tracking algorithm. We capture\na test set of aerial videos taken with different settings and compare our\nresults to those of a state-of-the-art tracking algorithm.\n", "versions": [{"version": "v1", "created": "Mon, 20 May 2019 16:57:21 GMT"}], "update_date": "2019-05-21", "authors_parsed": [["Zou", "Xiaoyan", ""], ["Zhou", "Ruofan", ""], ["Helou", "Majed El", ""], ["S\u00fcsstrunk", "Sabine", ""]]}, {"id": "1905.08231", "submitter": "Qingfu Wan", "authors": "Qingfu Wan, Weichao Qiu, Alan L. Yuille", "title": "Patch-based 3D Human Pose Refinement", "comments": "Accepted by CVPR 2019 Augmented Human: Human-centric Understanding\n  and 2D/3D Synthesis, and the third Look Into Person (LIP) Challenge Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  State-of-the-art 3D human pose estimation approaches typically estimate pose\nfrom the entire RGB image in a single forward run. In this paper, we develop a\npost-processing step to refine 3D human pose estimation from body part patches.\nUsing local patches as input has two advantages. First, the fine details around\nbody parts are zoomed in to high resolution for preciser 3D pose prediction.\nSecond, it enables the part appearance to be shared between poses to benefit\nrare poses. In order to acquire informative representation of patches, we\nexplore different input modalities and validate the superiority of fusing\npredicted segmentation with RGB. We show that our method consistently boosts\nthe accuracy of state-of-the-art 3D human pose methods.\n", "versions": [{"version": "v1", "created": "Mon, 20 May 2019 17:51:41 GMT"}], "update_date": "2019-05-21", "authors_parsed": [["Wan", "Qingfu", ""], ["Qiu", "Weichao", ""], ["Yuille", "Alan L.", ""]]}, {"id": "1905.08232", "submitter": "Parsa Saadatpanah", "authors": "Ali Shafahi, Parsa Saadatpanah, Chen Zhu, Amin Ghiasi, Christoph\n  Studer, David Jacobs, Tom Goldstein", "title": "Adversarially robust transfer learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transfer learning, in which a network is trained on one task and re-purposed\non another, is often used to produce neural network classifiers when data is\nscarce or full-scale training is too costly. When the goal is to produce a\nmodel that is not only accurate but also adversarially robust, data scarcity\nand computational limitations become even more cumbersome. We consider robust\ntransfer learning, in which we transfer not only performance but also\nrobustness from a source model to a target domain. We start by observing that\nrobust networks contain robust feature extractors. By training classifiers on\ntop of these feature extractors, we produce new models that inherit the\nrobustness of their parent networks. We then consider the case of fine tuning a\nnetwork by re-training end-to-end in the target domain. When using lifelong\nlearning strategies, this process preserves the robustness of the source\nnetwork while achieving high accuracy. By using such strategies, it is possible\nto produce accurate and robust models with little data, and without the cost of\nadversarial training. Additionally, we can improve the generalization of\nadversarially trained models, while maintaining their robustness.\n", "versions": [{"version": "v1", "created": "Mon, 20 May 2019 17:57:57 GMT"}, {"version": "v2", "created": "Fri, 21 Feb 2020 15:51:23 GMT"}], "update_date": "2020-02-24", "authors_parsed": [["Shafahi", "Ali", ""], ["Saadatpanah", "Parsa", ""], ["Zhu", "Chen", ""], ["Ghiasi", "Amin", ""], ["Studer", "Christoph", ""], ["Jacobs", "David", ""], ["Goldstein", "Tom", ""]]}, {"id": "1905.08233", "submitter": "Egor Zakharov", "authors": "Egor Zakharov, Aliaksandra Shysheya, Egor Burkov, Victor Lempitsky", "title": "Few-Shot Adversarial Learning of Realistic Neural Talking Head Models", "comments": "UPDATE: the data we used for evaluation is available for download!\n  See https://drive.google.com/open?id=1PeGG6zO3ZjrHk2GAXItB8khwMhPPyDHe and\n  refer to the README for description", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Several recent works have shown how highly realistic human head images can be\nobtained by training convolutional neural networks to generate them. In order\nto create a personalized talking head model, these works require training on a\nlarge dataset of images of a single person. However, in many practical\nscenarios, such personalized talking head models need to be learned from a few\nimage views of a person, potentially even a single image. Here, we present a\nsystem with such few-shot capability. It performs lengthy meta-learning on a\nlarge dataset of videos, and after that is able to frame few- and one-shot\nlearning of neural talking head models of previously unseen people as\nadversarial training problems with high capacity generators and discriminators.\nCrucially, the system is able to initialize the parameters of both the\ngenerator and the discriminator in a person-specific way, so that training can\nbe based on just a few images and done quickly, despite the need to tune tens\nof millions of parameters. We show that such an approach is able to learn\nhighly realistic and personalized talking head models of new people and even\nportrait paintings.\n", "versions": [{"version": "v1", "created": "Mon, 20 May 2019 17:58:04 GMT"}, {"version": "v2", "created": "Wed, 25 Sep 2019 11:16:01 GMT"}], "update_date": "2019-12-17", "authors_parsed": [["Zakharov", "Egor", ""], ["Shysheya", "Aliaksandra", ""], ["Burkov", "Egor", ""], ["Lempitsky", "Victor", ""]]}, {"id": "1905.08315", "submitter": "Shanka Subhra Mondal", "authors": "Shanka Subhra Mondal, Rachana Sathish, Debdoot Sheet", "title": "Multitask Learning of Temporal Connectionism in Convolutional Networks\n  using a Joint Distribution Loss Function to Simultaneously Identify Tools and\n  Phase in Surgical Videos", "comments": "15 pages, 8 figures, 5th MedImage Workshop of 11th Indian Conference\n  on Computer Vision, Graphics and Image Processing, Hyderabad, India, 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Surgical workflow analysis is of importance for understanding onset and\npersistence of surgical phases and individual tool usage across surgery and in\neach phase. It is beneficial for clinical quality control and to hospital\nadministrators for understanding surgery planning. Video acquired during\nsurgery typically can be leveraged for this task. Currently, a combination of\nconvolutional neural network (CNN) and recurrent neural networks (RNN) are\npopularly used for video analysis in general, not only being restricted to\nsurgical videos. In this paper, we propose a multi-task learning framework\nusing CNN followed by a bi-directional long short term memory (Bi-LSTM) to\nlearn to encapsulate both forward and backward temporal dependencies. Further,\nthe joint distribution indicating set of tools associated with a phase is used\nas an additional loss during learning to correct for their co-occurrence in any\npredictions. Experimental evaluation is performed using the Cholec80 dataset.\nWe report a mean average precision (mAP) score of 0.99 and 0.86 for tool and\nphase identification respectively which are higher compared to prior-art in the\nfield.\n", "versions": [{"version": "v1", "created": "Mon, 20 May 2019 19:42:40 GMT"}, {"version": "v2", "created": "Sat, 25 May 2019 16:38:08 GMT"}], "update_date": "2019-05-28", "authors_parsed": [["Mondal", "Shanka Subhra", ""], ["Sathish", "Rachana", ""], ["Sheet", "Debdoot", ""]]}, {"id": "1905.08369", "submitter": "Xiaofan Zhang", "authors": "Xiaofan Zhang, Cong Hao, Yuhong Li, Yao Chen, Jinjun Xiong, Wen-mei\n  Hwu, Deming Chen", "title": "A Bi-Directional Co-Design Approach to Enable Deep Learning on IoT\n  Devices", "comments": "Accepted by the ICML 2019 Workshop on On-Device Machine Learning &\n  Compact Deep Neural Network Representations (ODML-CDNNR)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Developing deep learning models for resource-constrained Internet-of-Things\n(IoT) devices is challenging, as it is difficult to achieve both good quality\nof results (QoR), such as DNN model inference accuracy, and quality of service\n(QoS), such as inference latency, throughput, and power consumption. Existing\napproaches typically separate the DNN model development step from its\ndeployment on IoT devices, resulting in suboptimal solutions. In this paper, we\nfirst introduce a few interesting but counterintuitive observations about such\na separate design approach, and empirically show why it may lead to suboptimal\ndesigns. Motivated by these observations, we then propose a novel and practical\nbi-directional co-design approach: a bottom-up DNN model design strategy\ntogether with a top-down flow for DNN accelerator design. It enables a joint\noptimization of both DNN models and their deployment configurations on IoT\ndevices as represented as FPGAs. We demonstrate the effectiveness of the\nproposed co-design approach on a real-life object detection application using\nPynq-Z1 embedded FPGA. Our method obtains the state-of-the-art results on both\nQoR with high accuracy (IoU) and QoS with high throughput (FPS) and high energy\nefficiency.\n", "versions": [{"version": "v1", "created": "Mon, 20 May 2019 22:36:38 GMT"}], "update_date": "2019-05-22", "authors_parsed": [["Zhang", "Xiaofan", ""], ["Hao", "Cong", ""], ["Li", "Yuhong", ""], ["Chen", "Yao", ""], ["Xiong", "Jinjun", ""], ["Hwu", "Wen-mei", ""], ["Chen", "Deming", ""]]}, {"id": "1905.08409", "submitter": "Marc Eder", "authors": "Marc Eder and Jan-Michael Frahm", "title": "Convolutions on Spherical Images", "comments": "Oral presentation at the 2019 SUMO Workshop on 360{\\deg} Indoor Scene\n  Understanding and Modeling at CVPR2019, in Proceedings of the IEEE Conference\n  on Computer Vision and Pattern Recognition Workshops. 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Applying convolutional neural networks to spherical images requires\nparticular considerations. We look to the millennia of work on cartographic map\nprojections to provide the tools to define an optimal representation of\nspherical images for the convolution operation. We propose a representation for\ndeep spherical image inference based on the icosahedral Snyder equal-area\n(ISEA) projection, a projection onto a geodesic grid, and show that it vastly\nexceeds the state-of-the-art for convolution on spherical images, improving\nsemantic segmentation results by 12.6%.\n", "versions": [{"version": "v1", "created": "Tue, 21 May 2019 02:38:36 GMT"}], "update_date": "2020-02-24", "authors_parsed": [["Eder", "Marc", ""], ["Frahm", "Jan-Michael", ""]]}, {"id": "1905.08413", "submitter": "Cao Haichao", "authors": "Haichao Cao, Hong Liu, Enmin Song, Chih-Cheng Hung, Guangzhi Ma,\n  Xiangyang Xu, Renchao Jin and Jianguo Lu", "title": "Dual-branch residual network for lung nodule segmentation", "comments": "24 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An accurate segmentation of lung nodules in computed tomography (CT) images\nis critical to lung cancer analysis and diagnosis. However, due to the variety\nof lung nodules and the similarity of visual characteristics between nodules\nand their surroundings, a robust segmentation of nodules becomes a challenging\nproblem. In this study, we propose the Dual-branch Residual Network (DB-ResNet)\nwhich is a data-driven model. Our approach integrates two new schemes to\nimprove the generalization capability of the model: 1) the proposed model can\nsimultaneously capture multi-view and multi-scale features of different nodules\nin CT images; 2) we combine the features of the intensity and the convolution\nneural networks (CNN). We propose a pooling method, called the central\nintensity-pooling layer (CIP), to extract the intensity features of the center\nvoxel of the block, and then use the CNN to obtain the convolutional features\nof the center voxel of the block. In addition, we designed a weighted sampling\nstrategy based on the boundary of nodules for the selection of those voxels\nusing the weighting score, to increase the accuracy of the model. The proposed\nmethod has been extensively evaluated on the LIDC dataset containing 986\nnodules. Experimental results show that the DB-ResNet achieves superior\nsegmentation performance with an average dice score of 82.74% on the dataset.\nMoreover, we compared our results with those of four radiologists on the same\ndataset. The comparison showed that our average dice score was 0.49% higher\nthan that of human experts. This proves that our proposed method is as good as\nthe experienced radiologist.\n", "versions": [{"version": "v1", "created": "Tue, 21 May 2019 02:57:56 GMT"}], "update_date": "2019-05-22", "authors_parsed": [["Cao", "Haichao", ""], ["Liu", "Hong", ""], ["Song", "Enmin", ""], ["Hung", "Chih-Cheng", ""], ["Ma", "Guangzhi", ""], ["Xu", "Xiangyang", ""], ["Jin", "Renchao", ""], ["Lu", "Jianguo", ""]]}, {"id": "1905.08416", "submitter": "Cao Haichao", "authors": "Haichao Cao, Hong Liu and Enmin Song", "title": "A novel algorithm for segmentation of leukocytes in peripheral blood", "comments": "25 pages, 10 figures", "journal-ref": "Biomedical Signal Processing and Control, 2018, 45:10-21", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the detection of anemia, leukemia and other blood diseases, the number and\ntype of leukocytes are essential evaluation parameters. However, the\nconventional leukocyte counting method is not only quite time-consuming but\nalso error-prone. Consequently, many automation methods are introduced for the\ndiagnosis of medical images. It remains difficult to accurately extract related\nfeatures and count the number of cells under the variable conditions such as\nbackground, staining method, staining degree, light conditions and so on.\nTherefore, in order to adapt to various complex situations, we consider RGB\ncolor space, HSI color space, and the linear combination of G, H and S\ncomponents, and propose a fast and accurate algorithm for the segmentation of\nperipheral blood leukocytes in this paper. First, the nucleus of leukocyte was\nseparated by using the stepwise averaging method. Then based on the\ninterval-valued fuzzy sets, the cytoplasm of leukocyte was segmented by\nminimizing the fuzzy divergence. Next, post-processing was carried out by using\nthe concave-convex iterative repair algorithm and the decision mechanism of\ncandidate mask sets. Experimental results show that the proposed method\noutperforms the existing non-fuzzy sets methods. Among the methods based on\nfuzzy sets, the interval-valued fuzzy sets perform slightly better than\ninterval-valued intuitionistic fuzzy sets and intuitionistic fuzzy sets.\n", "versions": [{"version": "v1", "created": "Tue, 21 May 2019 03:04:14 GMT"}], "update_date": "2019-05-22", "authors_parsed": [["Cao", "Haichao", ""], ["Liu", "Hong", ""], ["Song", "Enmin", ""]]}, {"id": "1905.08419", "submitter": "Zhao Kang", "authors": "Zhao Kang, Honghui Xu, Boyu Wang, Hongyuan Zhu, Zenglin Xu", "title": "Clustering with Similarity Preserving", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph-based clustering has shown promising performance in many tasks. A key\nstep of graph-based approach is the similarity graph construction. In general,\nlearning graph in kernel space can enhance clustering accuracy due to the\nincorporation of nonlinearity. However, most existing kernel-based graph\nlearning mechanisms is not similarity-preserving, hence leads to sub-optimal\nperformance. To overcome this drawback, we propose a more discriminative graph\nlearning method which can preserve the pairwise similarities between samples in\nan adaptive manner for the first time. Specifically, we require the learned\ngraph be close to a kernel matrix, which serves as a measure of similarity in\nraw data. Moreover, the structure is adaptively tuned so that the number of\nconnected components of the graph is exactly equal to the number of clusters.\nFinally, our method unifies clustering and graph learning which can directly\nobtain cluster indicators from the graph itself without performing further\nclustering step. The effectiveness of this approach is examined on both single\nand multiple kernel learning scenarios in several datasets.\n", "versions": [{"version": "v1", "created": "Tue, 21 May 2019 03:11:30 GMT"}], "update_date": "2019-05-22", "authors_parsed": [["Kang", "Zhao", ""], ["Xu", "Honghui", ""], ["Wang", "Boyu", ""], ["Zhu", "Hongyuan", ""], ["Xu", "Zenglin", ""]]}, {"id": "1905.08474", "submitter": "Yakov Miron", "authors": "Yakov Miron and Yona Coscas", "title": "S-Flow GAN", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Our work offers a new method for domain translation from semantic label maps\nand Computer Graphic (CG) simulation edge map images to photo-realistic images.\nWe train a Generative Adversarial Network (GAN) in a conditional way to\ngenerate a photo-realistic version of a given CG scene. Existing architectures\nof GANs still lack the photo-realism capabilities needed to train DNNs for\ncomputer vision tasks, we address this issue by embedding edge maps, and\ntraining it in an adversarial mode. We also offer an extension to our model\nthat uses our GAN architecture to create visually appealing and temporally\ncoherent videos.\n", "versions": [{"version": "v1", "created": "Tue, 21 May 2019 07:55:46 GMT"}, {"version": "v2", "created": "Wed, 25 Sep 2019 08:15:13 GMT"}], "update_date": "2019-09-26", "authors_parsed": [["Miron", "Yakov", ""], ["Coscas", "Yona", ""]]}, {"id": "1905.08501", "submitter": "Yosuke Kaga", "authors": "Yosuke Kaga, Masakazu Fujio, Kenta Takahashi, Tetsushi Ohki, Masakatsu\n  Nishigaki", "title": "PDH : Probabilistic deep hashing based on MAP estimation of Hamming\n  distance", "comments": "Accepted by the 26th IEEE International Conference on Image\n  Processing(ICIP2019)", "journal-ref": "2019 26th IEEE International Conference on Image Processing (ICIP)", "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the growth of image on the web, research on hashing which enables\nhigh-speed image retrieval has been actively studied. In recent years, various\nhashing methods based on deep neural networks have been proposed and achieved\nhigher precision than the other hashing methods. In these methods, multiple\nlosses for hash codes and the parameters of neural networks are defined. They\ngenerate hash codes that minimize the weighted sum of the losses. Therefore, an\nexpert has to tune the weights for the losses heuristically, and the\nprobabilistic optimality of the loss function cannot be explained. In order to\ngenerate explainable hash codes without weight tuning, we theoretically derive\na single loss function with no hyperparameters for the hash code from the\nprobability distribution of the images. By generating hash codes that minimize\nthis loss function, highly accurate image retrieval with probabilistic\noptimality is performed. We evaluate the performance of hashing using MNIST,\nCIFAR-10, SVHN and show that the proposed method outperforms the\nstate-of-the-art hashing methods.\n", "versions": [{"version": "v1", "created": "Tue, 21 May 2019 08:51:02 GMT"}], "update_date": "2019-05-23", "authors_parsed": [["Kaga", "Yosuke", ""], ["Fujio", "Masakazu", ""], ["Takahashi", "Kenta", ""], ["Ohki", "Tetsushi", ""], ["Nishigaki", "Masakatsu", ""]]}, {"id": "1905.08502", "submitter": "Andrea Romanoni", "authors": "Andrea Romanoni, Matteo Matteucci", "title": "Mesh-based Camera Pairs Selection and Occlusion-Aware Masking for Mesh\n  Refinement", "comments": "Accepted for publication in Pattern Recognition Letters", "journal-ref": null, "doi": "10.1016/j.patrec.2019.05.006", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many Multi-View-Stereo algorithms extract a 3D mesh model of a scene, after\nfusing depth maps into a volumetric representation of the space. Due to the\nlimited scalability of such representations, the estimated model does not\ncapture fine details of the scene. Therefore a mesh refinement algorithm is\nusually applied; it improves the mesh resolution and accuracy by minimizing the\nphotometric error induced by the 3D model into pairs of cameras. The choice of\nthese pairs significantly affects the quality of the refinement and usually\nrelies on sparse 3D points belonging to the surface. Instead, in this paper, to\nincrease the quality of pairs selection, we exploit the 3D model (before the\nrefinement) to compute five metrics: scene coverage, mutual image overlap,\nimage resolution, camera parallax, and a new symmetry term. To improve the\nrefinement robustness, we also propose an explicit method to manage occlusions,\nwhich may negatively affect the computation of the photometric error. The\nproposed method takes into account the depth of the model while computing the\nsimilarity measure and its gradient. We quantitatively and qualitatively\nvalidated our approach on publicly available datasets against state of the art\nreconstruction methods.\n", "versions": [{"version": "v1", "created": "Tue, 21 May 2019 08:51:15 GMT"}], "update_date": "2019-05-23", "authors_parsed": [["Romanoni", "Andrea", ""], ["Matteucci", "Matteo", ""]]}, {"id": "1905.08509", "submitter": "Pengqian Yu", "authors": "Xinhan Di, Pengqian Yu, Rui Bu, Mingchao Sun", "title": "Mutual Information Maximization in Graph Neural Networks", "comments": "Accepted for presentation at IJCNN 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A variety of graph neural networks (GNNs) frameworks for representation\nlearning on graphs have been recently developed. These frameworks rely on\naggregation and iteration scheme to learn the representation of nodes. However,\ninformation between nodes is inevitably lost in the scheme during learning. In\norder to reduce the loss, we extend the GNNs frameworks by exploring the\naggregation and iteration scheme in the methodology of mutual information. We\npropose a new approach of enlarging the normal neighborhood in the aggregation\nof GNNs, which aims at maximizing mutual information. Based on a series of\nexperiments conducted on several benchmark datasets, we show that the proposed\napproach improves the state-of-the-art performance for four types of graph\ntasks, including supervised and semi-supervised graph classification, graph\nlink prediction and graph edge generation and classification.\n", "versions": [{"version": "v1", "created": "Tue, 21 May 2019 09:15:10 GMT"}, {"version": "v2", "created": "Tue, 10 Sep 2019 07:42:49 GMT"}, {"version": "v3", "created": "Mon, 23 Mar 2020 04:42:31 GMT"}, {"version": "v4", "created": "Tue, 24 Mar 2020 02:54:08 GMT"}], "update_date": "2020-03-25", "authors_parsed": [["Di", "Xinhan", ""], ["Yu", "Pengqian", ""], ["Bu", "Rui", ""], ["Sun", "Mingchao", ""]]}, {"id": "1905.08538", "submitter": "Xiaohao Cai", "authors": "Xiaohao Cai, Raymond Chan, Xiaoyu Xie, Tieyong Zeng", "title": "A Two-stage Classification Method for High-dimensional Data and Point\n  Clouds", "comments": "21 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High-dimensional data classification is a fundamental task in machine\nlearning and imaging science. In this paper, we propose a two-stage multiphase\nsemi-supervised classification method for classifying high-dimensional data and\nunstructured point clouds. To begin with, a fuzzy classification method such as\nthe standard support vector machine is used to generate a warm initialization.\nWe then apply a two-stage approach named SaT (smoothing and thresholding) to\nimprove the classification. In the first stage, an unconstraint convex\nvariational model is implemented to purify and smooth the initialization,\nfollowed by the second stage which is to project the smoothed partition\nobtained at stage one to a binary partition. These two stages can be repeated,\nwith the latest result as a new initialization, to keep improving the\nclassification quality. We show that the convex model of the smoothing stage\nhas a unique solution and can be solved by a specifically designed primal-dual\nalgorithm whose convergence is guaranteed. We test our method and compare it\nwith the state-of-the-art methods on several benchmark data sets. The\nexperimental results demonstrate clearly that our method is superior in both\nthe classification accuracy and computation speed for high-dimensional data and\npoint clouds.\n", "versions": [{"version": "v1", "created": "Tue, 21 May 2019 10:45:59 GMT"}], "update_date": "2019-05-22", "authors_parsed": [["Cai", "Xiaohao", ""], ["Chan", "Raymond", ""], ["Xie", "Xiaoyu", ""], ["Zeng", "Tieyong", ""]]}, {"id": "1905.08545", "submitter": "Rafsanjany Kushol", "authors": "Rafsanjany Kushol, Md. Nishat Raihan, Md Sirajus Salekin and A. B. M.\n  Ashikur Rahman", "title": "Contrast Enhancement of Medical X-Ray Image Using Morphological\n  Operators with Optimal Structuring Element", "comments": "5 pages, 4 figures, conference paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To guide surgical and medical treatment X-ray images have been used by\nphysicians in every modern healthcare organization and hospitals. Doctor's\nevaluation process and disease identification in the area of skeletal system\ncan be performed in a faster and efficient way with the help of X-ray imaging\ntechnique as they can depict bone structure painlessly. This paper presents an\nefficient contrast enhancement technique using morphological operators which\nwill help to visualize important bone segments and soft tissues more clearly.\nTop-hat and Bottom-hat transform are utilized to enhance the image where\ngradient magnitude value is calculated for automatically selecting the\nstructuring element (SE) size. Experimental evaluation on different x-ray\nimaging databases shows the effectiveness of our method which also produces\ncomparatively better output against some existing image enhancement techniques.\n", "versions": [{"version": "v1", "created": "Tue, 21 May 2019 11:01:27 GMT"}], "update_date": "2019-05-22", "authors_parsed": [["Kushol", "Rafsanjany", ""], ["Raihan", "Md. Nishat", ""], ["Salekin", "Md Sirajus", ""], ["Rahman", "A. B. M. Ashikur", ""]]}, {"id": "1905.08574", "submitter": "Prerana Mukherjee", "authors": "Chandra Sekhar V, Prerana Mukherjee, D.S. Guru, Viswanath Pulabaigari", "title": "Online Signature Verification Based on Writer Specific Feature Selection\n  and Fuzzy Similarity Measure", "comments": "accepted in Applications of Computer Vision and Pattern Recognition\n  to Media Forensics, CVPRW, 2019, Long Beach, California", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Online Signature Verification (OSV) is a widely used biometric attribute for\nuser behavioral characteristic verification in digital forensics. In this\nmanuscript, owing to large intra-individual variability, a novel method for OSV\nbased on an interval symbolic representation and a fuzzy similarity measure\ngrounded on writer specific parameter selection is proposed. The two\nparameters, namely, writer specific acceptance threshold and optimal feature\nset to be used for authenticating the writer are selected based on minimum\nequal error rate (EER) attained during parameter fixation phase using the\ntraining signature samples. This is in variation to current techniques for OSV,\nwhich are primarily writer independent, in which a common set of features and\nacceptance threshold are chosen. To prove the robustness of our system, we have\nexhaustively assessed our system with four standard datasets i.e. MCYT-100\n(DB1), MCYT-330 (DB2), SUSIG-Visual corpus and SVC-2004- Task2. Experimental\noutcome confirms the effectiveness of fuzzy similarity metric-based writer\ndependent parameter selection for OSV by achieving a lower error rate as\ncompared to many recent and state-of-the art OSV models.\n", "versions": [{"version": "v1", "created": "Tue, 21 May 2019 12:19:12 GMT"}], "update_date": "2019-05-22", "authors_parsed": [["Sekhar", "Chandra", "V"], ["Mukherjee", "Prerana", ""], ["Guru", "D. S.", ""], ["Pulabaigari", "Viswanath", ""]]}, {"id": "1905.08586", "submitter": "Xi Shen", "authors": "Yuan Yuan, Yueming Lyu, Xi Shen, Ivor W. Tsang, Dit-Yan Yeung", "title": "Marginalized Average Attentional Network for Weakly-Supervised Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In weakly-supervised temporal action localization, previous works have failed\nto locate dense and integral regions for each entire action due to the\noverestimation of the most salient regions. To alleviate this issue, we propose\na marginalized average attentional network (MAAN) to suppress the dominant\nresponse of the most salient regions in a principled manner. The MAAN employs a\nnovel marginalized average aggregation (MAA) module and learns a set of latent\ndiscriminative probabilities in an end-to-end fashion. MAA samples multiple\nsubsets from the video snippet features according to a set of latent\ndiscriminative probabilities and takes the expectation over all the averaged\nsubset features. Theoretically, we prove that the MAA module with learned\nlatent discriminative probabilities successfully reduces the difference in\nresponses between the most salient regions and the others. Therefore, MAAN is\nable to generate better class activation sequences and identify dense and\nintegral action regions in the videos. Moreover, we propose a fast algorithm to\nreduce the complexity of constructing MAA from O($2^T$) to O($T^2$). Extensive\nexperiments on two large-scale video datasets show that our MAAN achieves\nsuperior performance on weakly-supervised temporal action localization\n", "versions": [{"version": "v1", "created": "Tue, 21 May 2019 12:49:22 GMT"}], "update_date": "2019-05-22", "authors_parsed": [["Yuan", "Yuan", ""], ["Lyu", "Yueming", ""], ["Shen", "Xi", ""], ["Tsang", "Ivor W.", ""], ["Yeung", "Dit-Yan", ""]]}, {"id": "1905.08598", "submitter": "Micha\\\"el Ramamonjisoa", "authors": "Micha\\\"el Ramamonjisoa, Vincent Lepetit", "title": "SharpNet: Fast and Accurate Recovery of Occluding Contours in Monocular\n  Depth Estimation", "comments": "Accepted at ICCV \"3D Reconstruction in the Wild\" workshop", "journal-ref": "2019 International Conference on Computer Vision (ICCV) Workshops", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce SharpNet, a method that predicts an accurate depth map for an\ninput color image, with a particular attention to the reconstruction of\noccluding contours: Occluding contours are an important cue for object\nrecognition, and for realistic integration of virtual objects in Augmented\nReality, but they are also notoriously difficult to reconstruct accurately. For\nexample, they are a challenge for stereo-based reconstruction methods, as\npoints around an occluding contour are visible in only one image. Inspired by\nrecent methods that introduce normal estimation to improve depth prediction, we\nintroduce a novel term that constrains depth and occluding contours\npredictions. Since ground truth depth is difficult to obtain with pixel-perfect\naccuracy along occluding contours, we use synthetic images for training,\nfollowed by fine-tuning on real data. We demonstrate our approach on the\nchallenging NYUv2-Depth dataset, and show that our method outperforms the\nstate-of-the-art along occluding contours, while performing on par with the\nbest recent methods for the rest of the images. Its accuracy along the\noccluding contours is actually better than the `ground truth' acquired by a\ndepth camera based on structured light. We show this by introducing a new\nbenchmark based on NYUv2-Depth for evaluating occluding contours in monocular\nreconstruction, which is our second contribution.\n", "versions": [{"version": "v1", "created": "Tue, 21 May 2019 13:08:52 GMT"}, {"version": "v2", "created": "Wed, 7 Aug 2019 13:27:53 GMT"}, {"version": "v3", "created": "Mon, 11 Nov 2019 23:32:10 GMT"}], "update_date": "2019-11-13", "authors_parsed": [["Ramamonjisoa", "Micha\u00ebl", ""], ["Lepetit", "Vincent", ""]]}, {"id": "1905.08606", "submitter": "Juan Wilches", "authors": "Juan Wilches", "title": "VGG Fine-tuning for Cooking State Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An important task that domestic robots need to achieve is the recognition of\nstates of food ingredients so they can continue their cooking actions. This\nproject focuses on a fine-tuning algorithm for the VGG (Visual Geometry Group)\narchitecture of deep convolutional neural networks (CNN) for object\nrecognition. The algorithm aims to identify eleven different ingredient cooking\nstates for an image dataset. The original VGG model was adjusted and trained to\nproperly classify the food states. The model was initialized with Imagenet\nweights. Different experiments were carried out in order to find the model\nparameters that provided the best performance. The accuracy achieved for the\nvalidation set was 76.7% and for the test set 76.6% after changing several\nparameters of the VGG model.\n", "versions": [{"version": "v1", "created": "Mon, 13 May 2019 20:49:14 GMT"}], "update_date": "2019-05-22", "authors_parsed": [["Wilches", "Juan", ""]]}, {"id": "1905.08607", "submitter": "Yu-Min Chung", "authors": "Yu-Min Chung, Chuan-Shen Hu, Austin Lawson, Clifford Smyth", "title": "TopoResNet: A hybrid deep learning architecture and its application to\n  skin lesion classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Skin cancer is one of the most common cancers in the United States. As\ntechnological advancements are made, algorithmic diagnosis of skin lesions is\nbecoming more important. In this paper, we develop algorithms for segmenting\nthe actual diseased area of skin in a given image of a skin lesion, and for\nclassifying different types of skin lesions pictured in a given image. The\ncores of the algorithms used were based in persistent homology, an algebraic\ntopology technique that is part of the rising field of Topological Data\nAnalysis (TDA). The segmentation algorithm utilizes a similar concept to\npersistent homology that captures the robustness of segmented regions. For\nclassification, we design two families of topological features from persistence\ndiagrams---which we refer to as {\\em persistence statistics} (PS) and {\\em\npersistence curves} (PC), and use linear support vector machine as classifiers.\nWe also combined those topological features, PS and PC, into ResNet-101 model,\nwhich we call {\\em TopoResNet-101}, the results show that PS and PC are\neffective in two folds---improving classification performances and stabilizing\nthe training process. Although convolutional features are the most important\nlearning targets in CNN models, global information of images may be lost in the\ntraining process. Because topological features were extracted globally, our\nresults show that the global property of topological features provide\nadditional information to machine learning models.\n", "versions": [{"version": "v1", "created": "Mon, 13 May 2019 21:16:39 GMT"}], "update_date": "2019-05-22", "authors_parsed": [["Chung", "Yu-Min", ""], ["Hu", "Chuan-Shen", ""], ["Lawson", "Austin", ""], ["Smyth", "Clifford", ""]]}, {"id": "1905.08608", "submitter": "Changfeng Wu", "authors": "Lei Qu and Changfeng Wu and Liang Zou", "title": "3D Dense Separated Convolution Module for Volumetric Image Analysis", "comments": "7 pages,5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the thriving of deep learning, 3D Convolutional Neural Networks have\nbecome a popular choice in volumetric image analysis due to their impressive 3D\ncontexts mining ability. However, the 3D convolutional kernels will introduce a\nsignificant increase in the amount of trainable parameters. Considering the\ntraining data is often limited in biomedical tasks, a tradeoff has to be made\nbetween model size and its representational power. To address this concern, in\nthis paper, we propose a novel 3D Dense Separated Convolution (3D-DSC) module\nto replace the original 3D convolutional kernels. The 3D-DSC module is\nconstructed by a series of densely connected 1D filters. The decomposition of\n3D kernel into 1D filters reduces the risk of over-fitting by removing the\nredundancy of 3D kernels in a topologically constrained manner, while providing\nthe infrastructure for deepening the network. By further introducing nonlinear\nlayers and dense connections between 1D filters, the network's representational\npower can be significantly improved while maintaining a compact architecture.\nWe demonstrate the superiority of 3D-DSC on volumetric image classification and\nsegmentation, which are two challenging tasks often encountered in biomedical\nimage computing.\n", "versions": [{"version": "v1", "created": "Tue, 14 May 2019 03:26:27 GMT"}], "update_date": "2019-05-22", "authors_parsed": [["Qu", "Lei", ""], ["Wu", "Changfeng", ""], ["Zou", "Liang", ""]]}, {"id": "1905.08609", "submitter": "Mingzhen Shao", "authors": "Mingzhen Shao, Zhun Sun, Mete Ozay, Takayuki Okatani", "title": "Improving Head Pose Estimation with a Combined Loss and Bounding Box\n  Margin Adjustment", "comments": "IEEE International Conference on Automatic Face & Gesture Recognition\n  (FG2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address a problem of estimating pose of a person's head from its RGB\nimage. The employment of CNNs for the problem has contributed to significant\nimprovement in accuracy in recent works. However, we show that the following\ntwo methods, despite their simplicity, can attain further improvement: (i)\nproper adjustment of the margin of bounding box of a detected face, and (ii)\nchoice of loss functions. We show that the integration of these two methods\nachieve the new state-of-the-art on standard benchmark datasets for in-the-wild\nhead pose estimation.\n", "versions": [{"version": "v1", "created": "Tue, 14 May 2019 21:58:35 GMT"}], "update_date": "2019-05-22", "authors_parsed": [["Shao", "Mingzhen", ""], ["Sun", "Zhun", ""], ["Ozay", "Mete", ""], ["Okatani", "Takayuki", ""]]}, {"id": "1905.08610", "submitter": "Brij Rokad", "authors": "Brij Rokad and Dr. Sureshkumar Nagarajan", "title": "Skin Cancer Recognition using Deep Residual Network", "comments": "6 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The advances in technology have enabled people to access internet from every\npart of the world. But to date, access to healthcare in remote areas is sparse.\nThis proposed solution aims to bridge the gap between specialist doctors and\npatients. This prototype will be able to detect skin cancer from an image\ncaptured by the phone or any other camera. The network is deployed on cloud\nserver-side processing for an even more accurate result. The Deep Residual\nlearning model has been used for predicting the probability of cancer for\nserver side The ResNet has three parametric layers. Each layer has\nConvolutional Neural Network, Batch Normalization, Maxpool and ReLU. Currently\nthe model achieves an accuracy of 77% on the ISIC - 2017 challenge.\n", "versions": [{"version": "v1", "created": "Tue, 14 May 2019 10:04:38 GMT"}], "update_date": "2019-05-22", "authors_parsed": [["Rokad", "Brij", ""], ["Nagarajan", "Dr. Sureshkumar", ""]]}, {"id": "1905.08611", "submitter": "Soumick Chatterjee", "authors": "Rupali Khatun and Soumick Chatterjee", "title": "Machine learning approach for segmenting glands in colon histology\n  images using local intensity and texture features", "comments": null, "journal-ref": "8th International Advance Computing Conference (IACC), 2018", "doi": "10.1109/IADCC.2018.8692135", "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Colon Cancer is one of the most common types of cancer. The treatment is\nplanned to depend on the grade or stage of cancer. One of the preconditions for\ngrading of colon cancer is to segment the glandular structures of tissues.\nManual segmentation method is very time-consuming, and it leads to life risk\nfor the patients. The principal objective of this project is to assist the\npathologist to accurate detection of colon cancer. In this paper, the authors\nhave proposed an algorithm for an automatic segmentation of glands in colon\nhistology using local intensity and texture features. Here the dataset images\nare cropped into patches with different window sizes and taken the intensity of\nthose patches, and also calculated texture-based features. Random forest\nclassifier has been used to classify this patch into different labels. A\nmultilevel random forest technique in a hierarchical way is proposed. This\nsolution is fast, accurate and it is very much applicable in a clinical setup.\n", "versions": [{"version": "v1", "created": "Wed, 15 May 2019 16:40:14 GMT"}], "update_date": "2019-05-22", "authors_parsed": [["Khatun", "Rupali", ""], ["Chatterjee", "Soumick", ""]]}, {"id": "1905.08612", "submitter": "Mohamed Nafzi", "authors": "Mohamed Nafzi and Michael Brauckmann and Tobias Glasmachers", "title": "Vehicle Shape and Color Classification Using Convolutional Neural\n  Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  This paper presents a module of vehicle reidentification based on make/model\nand color classification. It could be used by the Automated Vehicular\nSurveillance (AVS) or by the fast analysis of video data. Many of problems,\nthat are related to this topic, had to be addressed. In order to facilitate and\naccelerate the progress in this subject, we will present our way to collect and\nto label a large scale data set. We used deeper neural networks in our\ntraining. They showed a good classification accuracy. We show the results of\nmake/model and color classification on controlled and video data set. We\ndemonstrate with the help of a developed application the re-identification of\nvehicles on video images based on make/model and color classification. This\nwork was partially funded under the grant.\n", "versions": [{"version": "v1", "created": "Wed, 15 May 2019 11:10:18 GMT"}], "update_date": "2019-05-22", "authors_parsed": [["Nafzi", "Mohamed", ""], ["Brauckmann", "Michael", ""], ["Glasmachers", "Tobias", ""]]}, {"id": "1905.08613", "submitter": "Cyprien Ruffino", "authors": "Cyprien Ruffino (LITIS, INSA Rouen Normandie, NU), Romain H\\'erault\n  (DocApp - LITIS), Eric Laloy (SCK-CEN), Gilles Gasso (LITIS)", "title": "Dilated Spatial Generative Adversarial Networks for Ergodic Image\n  Generation", "comments": null, "journal-ref": "Conf{\\'e}rence sur l'Apprentissage Automatique, Jun 2018, Rouen,\n  France", "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative models have recently received renewed attention as a result of\nadversarial learning. Generative adversarial networks consist of samples\ngeneration model and a discrimination model able to distinguish between genuine\nand synthetic samples. In combination with convolutional (for the\ndiscriminator) and de-convolutional (for the generator) layers, they are\nparticularly suitable for image generation, especially of natural scenes.\nHowever, the presence of fully connected layers adds global dependencies in the\ngenerated images. This may lead to high and global variations in the generated\nsample for small local variations in the input noise. In this work we propose\nto use architec-tures based on fully convolutional networks (including among\nothers dilated layers), architectures specifically designed to generate\nglobally ergodic images, that is images without global dependencies. Conducted\nexperiments reveal that these architectures are well suited for generating\nnatural textures such as geologic structures .\n", "versions": [{"version": "v1", "created": "Wed, 15 May 2019 08:12:53 GMT"}], "update_date": "2019-05-22", "authors_parsed": [["Ruffino", "Cyprien", "", "LITIS, INSA Rouen Normandie, NU"], ["H\u00e9rault", "Romain", "", "DocApp - LITIS"], ["Laloy", "Eric", "", "SCK-CEN"], ["Gasso", "Gilles", "", "LITIS"]]}, {"id": "1905.08614", "submitter": "Zhaoxia Yin", "authors": "Hua Wang, Jie Wang, Zhaoxia Yin", "title": "An Efficient Pre-processing Method to Eliminate Adversarial Effects", "comments": "in Chinese", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Neural Networks (DNNs) are vulnerable to adversarial examples generated\nby imposing subtle perturbations to inputs that lead a model to predict\nincorrect outputs. Currently, a large number of researches on defending\nadversarial examples pay little attention to the real-world applications,\neither with high computational complexity or poor defensive effects. Motivated\nby this observation, we develop an efficient preprocessing method to defend\nadversarial images. Specifically, before an adversarial example is fed into the\nmodel, we perform two image transformations: WebP compression, which is\nutilized to remove the small adversarial noises. Flip operation, which flips\nthe image once along one side of the image to destroy the specific structure of\nadversarial perturbations. Finally, a de-perturbed sample is obtained and can\nbe correctly classified by DNNs. Experimental results on ImageNet show that our\nmethod outperforms the state-of-the-art defense methods. It can effectively\ndefend adversarial attacks while ensure only very small accuracy drop on normal\nimages.\n", "versions": [{"version": "v1", "created": "Wed, 15 May 2019 05:38:29 GMT"}, {"version": "v2", "created": "Mon, 30 Dec 2019 12:53:27 GMT"}], "update_date": "2020-01-01", "authors_parsed": [["Wang", "Hua", ""], ["Wang", "Jie", ""], ["Yin", "Zhaoxia", ""]]}, {"id": "1905.08615", "submitter": "Hiroshi Kaizuka", "authors": "Hiroshi Kaizuka, Yasuhiro Nagasaki, Ryo Sako", "title": "ROI Regularization for Semi-supervised and Supervised Learning", "comments": "14 pages, 7 tables, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose ROI regularization (ROIreg) as a semi-supervised learning method\nfor image classification. ROIreg focuses on the maximum probability of a\nposterior probability distribution g(x) obtained when inputting an unlabeled\ndata sample x into a convolutional neural network (CNN). ROIreg divides the\npixel set of x into multiple blocks and evaluates, for each block, its\ncontribution to the maximum probability. A masked data sample x_ROI is\ngenerated by replacing blocks with relatively small degrees of contribution\nwith random images. Then, ROIreg trains CNN so that g(x_ROI ) does not change\nas much as possible from g(x). Therefore, ROIreg can be said to refine the\nclassification ability of CNN more. On the other hand, Virtual Adverserial\nTraining (VAT), which is an excellent semi-supervised learning method,\ngenerates data sample x_VAT by perturbing x in the direction in which g(x)\nchanges most. Then, VAT trains CNN so that g(x_VAT ) does not change from g(x)\nas much as possible. Therefore, VAT can be said to be a method to improve CNN's\nweakness. Thus, ROIreg and VAT have complementary training effects. In fact,\nthe combination of VAT and ROIreg improves the results obtained when using VAT\nor ROIreg alone. This combination also improves the state-of-the-art on \"SVHN\nwith and without data augmentation\" and \"CIFAR-10 without data augmentation\".\nWe also propose a method called ROI augmentation (ROIaug) as a method to apply\nROIreg to data augmentation in supervised learning. However, the evaluation\nfunction used there is different from the standard cross-entropy. ROIaug\nimproves the performance of supervised learning for both SVHN and CIFAR-10.\nFinally, we investigate the performance degradation of VAT and VAT+ROIreg when\ndata samples not belonging to classification classes are included in unlabeled\ndata.\n", "versions": [{"version": "v1", "created": "Wed, 15 May 2019 03:54:24 GMT"}], "update_date": "2019-05-22", "authors_parsed": [["Kaizuka", "Hiroshi", ""], ["Nagasaki", "Yasuhiro", ""], ["Sako", "Ryo", ""]]}, {"id": "1905.08616", "submitter": "Alex Wong", "authors": "Alex Wong, Xiaohan Fei, Stephanie Tsuei, Stefano Soatto", "title": "Unsupervised Depth Completion from Visual Inertial Odometry", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  We describe a method to infer dense depth from camera motion and sparse depth\nas estimated using a visual-inertial odometry system. Unlike other scenarios\nusing point clouds from lidar or structured light sensors, we have few hundreds\nto few thousand points, insufficient to inform the topology of the scene. Our\nmethod first constructs a piecewise planar scaffolding of the scene, and then\nuses it to infer dense depth using the image along with the sparse points. We\nuse a predictive cross-modal criterion, akin to `self-supervision,' measuring\nphotometric consistency across time, forward-backward pose consistency, and\ngeometric compatibility with the sparse point cloud. We also launch the first\nvisual-inertial + depth dataset, which we hope will foster additional\nexploration into combining the complementary strengths of visual and inertial\nsensors. To compare our method to prior work, we adopt the unsupervised KITTI\ndepth completion benchmark, and show state-of-the-art performance on it. Code\navailable at:\nhttps://github.com/alexklwong/unsupervised-depth-completion-visual-inertial-odometry.\n", "versions": [{"version": "v1", "created": "Wed, 15 May 2019 03:47:18 GMT"}, {"version": "v2", "created": "Fri, 7 Feb 2020 23:20:01 GMT"}, {"version": "v3", "created": "Tue, 11 Feb 2020 03:51:28 GMT"}, {"version": "v4", "created": "Wed, 21 Jul 2021 11:21:08 GMT"}], "update_date": "2021-07-22", "authors_parsed": [["Wong", "Alex", ""], ["Fei", "Xiaohan", ""], ["Tsuei", "Stephanie", ""], ["Soatto", "Stefano", ""]]}, {"id": "1905.08617", "submitter": "Chongyang Bai", "authors": "Chongyang Bai, Maksim Bolonkin, Judee Burgoon, Chao Chen, Norah\n  Dunbar, Bharat Singh, V. S. Subrahmanian and Zhe Wu", "title": "Automatic Long-Term Deception Detection in Group Interaction Videos", "comments": "ICME 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most work on automated deception detection (ADD) in video has two\nrestrictions: (i) it focuses on a video of one person, and (ii) it focuses on a\nsingle act of deception in a one or two minute video. In this paper, we propose\na new ADD framework which captures long term deception in a group setting. We\nstudy deception in the well-known Resistance game (like Mafia and Werewolf)\nwhich consists of 5-8 players of whom 2-3 are spies. Spies are deceptive\nthroughout the game (typically 30-65 minutes) to keep their identity hidden. We\ndevelop an ensemble predictive model to identify spies in Resistance videos. We\nshow that features from low-level and high-level video analysis are\ninsufficient, but when combined with a new class of features that we call\nLiarRank, produce the best results. We achieve AUCs of over 0.70 in a fully\nautomated setting. Our demo can be found at\nhttp://home.cs.dartmouth.edu/~mbolonkin/scan/demo/\n", "versions": [{"version": "v1", "created": "Wed, 15 May 2019 15:45:02 GMT"}, {"version": "v2", "created": "Sat, 15 Jun 2019 16:36:53 GMT"}], "update_date": "2019-06-18", "authors_parsed": [["Bai", "Chongyang", ""], ["Bolonkin", "Maksim", ""], ["Burgoon", "Judee", ""], ["Chen", "Chao", ""], ["Dunbar", "Norah", ""], ["Singh", "Bharat", ""], ["Subrahmanian", "V. S.", ""], ["Wu", "Zhe", ""]]}, {"id": "1905.08618", "submitter": "Longwei Wang", "authors": "Longwei Wang, Peijie Chen", "title": "Neurons Activation Visualization and Information Theoretic Analysis", "comments": "the paper is not so well written and need to be revised", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding the inner working mechanism of deep neural networks (DNNs) is\nessential and important for researchers to design and improve the performance\nof DNNs. In this work, the entropy analysis is leveraged to study the neurons\nactivation behavior of the fully connected layers of DNNs. The entropy of the\nactivation patterns of each layer can provide a performance metric for the\nevaluation of the network model accuracy. The study is conducted based on a\nwell trained network model. The activation patterns of shallow and deep layers\nof the fully connected layers are analyzed by inputting the images of a single\nclass. It is found that for the well trained deep neural networks model, the\nentropy of the neuron activation pattern is monotonically reduced with the\ndepth of the layers. That is, the neuron activation patterns become more and\nmore stable with the depth of the fully connected layers. The entropy pattern\nof the fully connected layers can also provide guidelines as to how many fully\nconnected layers are needed to guarantee the accuracy of the model. The study\nin this work provides a new perspective on the analysis of DNN, which shows\nsome interesting results.\n", "versions": [{"version": "v1", "created": "Tue, 14 May 2019 15:33:15 GMT"}, {"version": "v2", "created": "Sat, 2 Nov 2019 14:52:59 GMT"}, {"version": "v3", "created": "Tue, 5 Nov 2019 16:01:46 GMT"}], "update_date": "2019-11-06", "authors_parsed": [["Wang", "Longwei", ""], ["Chen", "Peijie", ""]]}, {"id": "1905.08622", "submitter": "Mingyuan Zhou", "authors": "Hao Zhang, Bo Chen, Long Tian, Zhengjue Wang, Mingyuan Zhou", "title": "Variational Hetero-Encoder Randomized GANs for Joint Image-Text Modeling", "comments": "ICLR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For bidirectional joint image-text modeling, we develop variational\nhetero-encoder (VHE) randomized generative adversarial network (GAN), a\nversatile deep generative model that integrates a probabilistic text decoder,\nprobabilistic image encoder, and GAN into a coherent end-to-end multi-modality\nlearning framework. VHE randomized GAN (VHE-GAN) encodes an image to decode its\nassociated text, and feeds the variational posterior as the source of\nrandomness into the GAN image generator. We plug three off-the-shelf modules,\nincluding a deep topic model, a ladder-structured image encoder, and\nStackGAN++, into VHE-GAN, which already achieves competitive performance. This\nfurther motivates the development of VHE-raster-scan-GAN that generates\nphoto-realistic images in not only a multi-scale low-to-high-resolution manner,\nbut also a hierarchical-semantic coarse-to-fine fashion. By capturing and\nrelating hierarchical semantic and visual concepts with end-to-end training,\nVHE-raster-scan-GAN achieves state-of-the-art performance in a wide variety of\nimage-text multi-modality learning and generation tasks.\n", "versions": [{"version": "v1", "created": "Sat, 18 May 2019 13:58:12 GMT"}, {"version": "v2", "created": "Wed, 25 Sep 2019 16:43:14 GMT"}, {"version": "v3", "created": "Tue, 7 Jan 2020 20:51:34 GMT"}], "update_date": "2020-01-09", "authors_parsed": [["Zhang", "Hao", ""], ["Chen", "Bo", ""], ["Tian", "Long", ""], ["Wang", "Zhengjue", ""], ["Zhou", "Mingyuan", ""]]}, {"id": "1905.08633", "submitter": "Vinay Prabhu", "authors": "Vinay Uday Prabhu, Sanghyun Han, Dian Ang Yap, Mihail Douhaniaris,\n  Preethi Seshadri and John Whaley", "title": "Fonts-2-Handwriting: A Seed-Augment-Train framework for universal digit\n  classification", "comments": "Published as a workshop paper at ICLR 2019 (DeepGenStruct-2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a Seed-Augment-Train/Transfer (SAT) framework that\ncontains a synthetic seed image dataset generation procedure for languages with\ndifferent numeral systems using freely available open font file datasets. This\nseed dataset of images is then augmented to create a purely synthetic training\ndataset, which is in turn used to train a deep neural network and test on\nheld-out real world handwritten digits dataset spanning five Indic scripts,\nKannada, Tamil, Gujarati, Malayalam, and Devanagari. We showcase the efficacy\nof this approach both qualitatively, by training a Boundary-seeking GAN (BGAN)\nthat generates realistic digit images in the five languages, and also\nquantitatively by testing a CNN trained on the synthetic data on the real-world\ndatasets. This establishes not only an interesting nexus between the\nfont-datasets-world and transfer learning but also provides a recipe for\nuniversal-digit classification in any script.\n", "versions": [{"version": "v1", "created": "Thu, 16 May 2019 20:38:05 GMT"}], "update_date": "2019-05-22", "authors_parsed": [["Prabhu", "Vinay Uday", ""], ["Han", "Sanghyun", ""], ["Yap", "Dian Ang", ""], ["Douhaniaris", "Mihail", ""], ["Seshadri", "Preethi", ""], ["Whaley", "John", ""]]}, {"id": "1905.08654", "submitter": "Fl\\'avia Dias Casagrande", "authors": "Flavia Dias Casagrande and Evi Zouganeli", "title": "Activity Recognition and Prediction in Real Homes", "comments": "12 pages, Symposium of the Norwegian AI Society NAIS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present work in progress on activity recognition and\nprediction in real homes using either binary sensor data or depth video data.\nWe present our field trial and set-up for collecting and storing the data, our\nmethods, and our current results. We compare the accuracy of predicting the\nnext binary sensor event using probabilistic methods and Long Short-Term Memory\n(LSTM) networks, include the time information to improve prediction accuracy,\nas well as predict both the next sensor event and its mean time of occurrence\nusing one LSTM model. We investigate transfer learning between apartments and\nshow that it is possible to pre-train the model with data from other apartments\nand achieve good accuracy in a new apartment straight away. In addition, we\npresent preliminary results from activity recognition using low-resolution\ndepth video data from seven apartments, and classify four activities - no\nmovement, standing up, sitting down, and TV interaction - by using a relatively\nsimple processing method where we apply an Infinite Impulse Response (IIR)\nfilter to extract movements from the frames prior to feeding them to a\nconvolutional LSTM network for the classification.\n", "versions": [{"version": "v1", "created": "Mon, 20 May 2019 13:14:55 GMT"}], "update_date": "2019-05-22", "authors_parsed": [["Casagrande", "Flavia Dias", ""], ["Zouganeli", "Evi", ""]]}, {"id": "1905.08663", "submitter": "Zhenliang Ni", "authors": "Zhen-Liang Ni, Gui-Bin Bian, Xiao-Liang Xie, Zeng-Guang Hou, Xiao-Hu\n  Zhou and Yan-Jie Zhou", "title": "RASNet: Segmentation for Tracking Surgical Instruments in Surgical\n  Videos Using Refined Attention Segmentation Network", "comments": "This paper has been accepted by 2019 41st Annual International\n  Conference of the IEEE Engineering in Medicine &Biology Society (EMBC)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Segmentation for tracking surgical instruments plays an important role in\nrobot-assisted surgery. Segmentation of surgical instruments contributes to\ncapturing accurate spatial information for tracking. In this paper, a novel\nnetwork, Refined Attention Segmentation Network, is proposed to simultaneously\nsegment surgical instruments and identify their categories. The U-shape network\nwhich is popular in segmentation is used. Different from previous work, an\nattention module is adopted to help the network focus on key regions, which can\nimprove the segmentation accuracy. To solve the class imbalance problem, the\nweighted sum of the cross entropy loss and the logarithm of the Jaccard index\nis used as loss function. Furthermore, transfer learning is adopted in our\nnetwork. The encoder is pre-trained on ImageNet. The dataset from the MICCAI\nEndoVis Challenge 2017 is used to evaluate our network. Based on this dataset,\nour network achieves state-of-the-art performance 94.65% mean Dice and 90.33%\nmean IOU.\n", "versions": [{"version": "v1", "created": "Tue, 21 May 2019 14:25:36 GMT"}, {"version": "v2", "created": "Thu, 19 Sep 2019 11:34:23 GMT"}], "update_date": "2019-09-20", "authors_parsed": [["Ni", "Zhen-Liang", ""], ["Bian", "Gui-Bin", ""], ["Xie", "Xiao-Liang", ""], ["Hou", "Zeng-Guang", ""], ["Zhou", "Xiao-Hu", ""], ["Zhou", "Yan-Jie", ""]]}, {"id": "1905.08685", "submitter": "Jen-Yen Chang", "authors": "Jen-Yen Chang, Antonio Tejero-de-Pablos, Tatsuya Harada", "title": "Improved Optical Flow for Gesture-based Human-robot Interaction", "comments": "Accepted by ICRA 2019 on Jan 31 2019", "journal-ref": null, "doi": "10.1109/ICRA.2019.8793825", "report-no": null, "categories": "cs.HC cs.CV cs.RO", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Gesture interaction is a natural way of communicating with a robot as an\nalternative to speech. Gesture recognition methods leverage optical flow in\norder to understand human motion. However, while accurate optical flow\nestimation (i.e., traditional) methods are costly in terms of runtime, fast\nestimation (i.e., deep learning) methods' accuracy can be improved. In this\npaper, we present a pipeline for gesture-based human-robot interaction that\nuses a novel optical flow estimation method in order to achieve an improved\nspeed-accuracy trade-off. Our optical flow estimation method introduces four\nimprovements to previous deep learning-based methods: strong feature\nextractors, attention to contours, midway features, and a combination of these\nthree. This results in a better understanding of motion, and a finer\nrepresentation of silhouettes. In order to evaluate our pipeline, we generated\nour own dataset, MIBURI, which contains gestures to command a house service\nrobot. In our experiments, we show how our method improves not only optical\nflow estimation, but also gesture recognition, offering a speed-accuracy\ntrade-off more realistic for practical robot applications.\n", "versions": [{"version": "v1", "created": "Tue, 21 May 2019 15:03:31 GMT"}], "update_date": "2019-10-01", "authors_parsed": [["Chang", "Jen-Yen", ""], ["Tejero-de-Pablos", "Antonio", ""], ["Harada", "Tatsuya", ""]]}, {"id": "1905.08705", "submitter": "Can Chen", "authors": "Can Chen, Luca Zanotti Fragonara, Antonios Tsourdos", "title": "GAPNet: Graph Attention based Point Neural Network for Exploiting Local\n  Feature of Point Cloud", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Exploiting fine-grained semantic features on point cloud is still challenging\ndue to its irregular and sparse structure in a non-Euclidean space. Among\nexisting studies, PointNet provides an efficient and promising approach to\nlearn shape features directly on unordered 3D point cloud and has achieved\ncompetitive performance. However, local feature that is helpful towards better\ncontextual learning is not considered. Meanwhile, attention mechanism shows\nefficiency in capturing node representation on graph-based data by attending\nover neighboring nodes. In this paper, we propose a novel neural network for\npoint cloud, dubbed GAPNet, to learn local geometric representations by\nembedding graph attention mechanism within stacked Multi-Layer-Perceptron (MLP)\nlayers. Firstly, we introduce a GAPLayer to learn attention features for each\npoint by highlighting different attention weights on neighborhood. Secondly, in\norder to exploit sufficient features, a multi-head mechanism is employed to\nallow GAPLayer to aggregate different features from independent heads. Thirdly,\nwe propose an attention pooling layer over neighbors to capture local signature\naimed at enhancing network robustness. Finally, GAPNet applies stacked MLP\nlayers to attention features and local signature to fully extract local\ngeometric structures. The proposed GAPNet architecture is tested on the\nModelNet40 and ShapeNet part datasets, and achieves state-of-the-art\nperformance in both shape classification and part segmentation tasks.\n", "versions": [{"version": "v1", "created": "Tue, 21 May 2019 15:44:31 GMT"}], "update_date": "2019-05-22", "authors_parsed": [["Chen", "Can", ""], ["Fragonara", "Luca Zanotti", ""], ["Tsourdos", "Antonios", ""]]}, {"id": "1905.08711", "submitter": "Alexander Kozlov", "authors": "Alexander Kozlov, Vadim Andronov, Yana Gritsenko", "title": "Lightweight Network Architecture for Real-Time Action Recognition", "comments": "8 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this work we present a new efficient approach to Human Action Recognition\ncalled Video Transformer Network (VTN). It leverages the latest advances in\nComputer Vision and Natural Language Processing and applies them to video\nunderstanding. The proposed method allows us to create lightweight CNN models\nthat achieve high accuracy and real-time speed using just an RGB mono camera\nand general purpose CPU. Furthermore, we explain how to improve accuracy by\ndistilling from multiple models with different modalities into a single model.\nWe conduct a comparison with state-of-the-art methods and show that our\napproach performs on par with most of them on famous Action Recognition\ndatasets. We benchmark the inference time of the models using the modern\ninference framework and argue that our approach compares favorably with other\nmethods in terms of speed/accuracy trade-off, running at 56 FPS on CPU. The\nmodels and the training code are available.\n", "versions": [{"version": "v1", "created": "Tue, 21 May 2019 15:50:24 GMT"}], "update_date": "2019-05-22", "authors_parsed": [["Kozlov", "Alexander", ""], ["Andronov", "Vadim", ""], ["Gritsenko", "Yana", ""]]}, {"id": "1905.08720", "submitter": "Xuhua Ren", "authors": "Xuhua Ren, Lichi Zhang, Sahar Ahmad, Dong Nie, Fan Yang, Lei Xiang,\n  Qian Wang and Dinggang Shen", "title": "Task Decomposition and Synchronization for Semantic Biomedical Image\n  Segmentation", "comments": "IEEE Transactions on Medical Imaging", "journal-ref": null, "doi": "10.1109/TIP.2020.3003735", "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semantic segmentation is essentially important to biomedical image analysis.\nMany recent works mainly focus on integrating the Fully Convolutional Network\n(FCN) architecture with sophisticated convolution implementation and deep\nsupervision. In this paper, we propose to decompose the single segmentation\ntask into three subsequent sub-tasks, including (1) pixel-wise image\nsegmentation, (2) prediction of the class labels of the objects within the\nimage, and (3) classification of the scene the image belonging to. While these\nthree sub-tasks are trained to optimize their individual loss functions of\ndifferent perceptual levels, we propose to let them interact by the task-task\ncontext ensemble. Moreover, we propose a novel sync-regularization to penalize\nthe deviation between the outputs of the pixel-wise segmentation and the class\nprediction tasks. These effective regularizations help FCN utilize context\ninformation comprehensively and attain accurate semantic segmentation, even\nthough the number of the images for training may be limited in many biomedical\napplications. We have successfully applied our framework to three diverse 2D/3D\nmedical image datasets, including Robotic Scene Segmentation Challenge 18\n(ROBOT18), Brain Tumor Segmentation Challenge 18 (BRATS18), and Retinal Fundus\nGlaucoma Challenge (REFUGE18). We have achieved top-tier performance in all\nthree challenges.\n", "versions": [{"version": "v1", "created": "Tue, 21 May 2019 15:59:04 GMT"}, {"version": "v2", "created": "Sat, 22 Jun 2019 09:01:52 GMT"}], "update_date": "2020-08-26", "authors_parsed": [["Ren", "Xuhua", ""], ["Zhang", "Lichi", ""], ["Ahmad", "Sahar", ""], ["Nie", "Dong", ""], ["Yang", "Fan", ""], ["Xiang", "Lei", ""], ["Wang", "Qian", ""], ["Shen", "Dinggang", ""]]}, {"id": "1905.08748", "submitter": "Pierre Biasutti", "authors": "Pierre Biasutti and Aur\\'elie Bugeau and Jean-Fran\\c{c}ois Aujol and\n  Mathieu Br\\'edif", "title": "RIU-Net: Embarrassingly simple semantic segmentation of 3D LiDAR point\n  cloud", "comments": "This version of the article contains revised scores to match the\n  evaluation process presented in SqueezeSegV2", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes RIU-Net (for Range-Image U-Net), the adaptation of a\npopular semantic segmentation network for the semantic segmentation of a 3D\nLiDAR point cloud. The point cloud is turned into a 2D range-image by\nexploiting the topology of the sensor. This image is then used as input to a\nU-net. This architecture has already proved its efficiency for the task of\nsemantic segmentation of medical images. We demonstrate how it can also be used\nfor the accurate semantic segmentation of a 3D LiDAR point cloud and how it\nrepresents a valid bridge between image processing and 3D point cloud\nprocessing. Our model is trained on range-images built from KITTI 3D object\ndetection dataset. Experiments show that RIU-Net, despite being very simple,\noffers results that are comparable to the state-of-the-art of range-image based\nmethods. Finally, we demonstrate that this architecture is able to operate at\n90fps on a single GPU, which enables deployment for real-time segmentation.\n", "versions": [{"version": "v1", "created": "Tue, 21 May 2019 16:51:35 GMT"}, {"version": "v2", "created": "Thu, 6 Jun 2019 13:16:27 GMT"}, {"version": "v3", "created": "Mon, 17 Jun 2019 08:15:03 GMT"}], "update_date": "2019-06-18", "authors_parsed": [["Biasutti", "Pierre", ""], ["Bugeau", "Aur\u00e9lie", ""], ["Aujol", "Jean-Fran\u00e7ois", ""], ["Br\u00e9dif", "Mathieu", ""]]}, {"id": "1905.08766", "submitter": "Wenju Xu", "authors": "Wenju Xu and Shawn Keshmiri and Guanghui Wang", "title": "Toward Learning a Unified Many-to-Many Mapping for Diverse Image\n  Translation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image-to-image translation, which translates input images to a different\ndomain with a learned one-to-one mapping, has achieved impressive success in\nrecent years. The success of translation mainly relies on the network\narchitecture to reserve the structural information while modify the appearance\nslightly at the pixel level through adversarial training. Although these\nnetworks are able to learn the mapping, the translated images are predictable\nwithout exclusion. It is more desirable to diversify them using image-to-image\ntranslation by introducing uncertainties, i.e., the generated images hold\npotential for variations in colors and textures in addition to the general\nsimilarity to the input images, and this happens in both the target and source\ndomains. To this end, we propose a novel generative adversarial network (GAN)\nbased model, InjectionGAN, to learn a many-to-many mapping. In this model, the\ninput image is combined with latent variables, which comprise of\ndomain-specific attribute and unspecific random variations. The domain-specific\nattribute indicates the target domain of the translation, while the unspecific\nrandom variations introduce uncertainty into the model. A unified framework is\nproposed to regroup these two parts and obtain diverse generations in each\ndomain. Extensive experiments demonstrate that the diverse generations have\nhigh quality for the challenging image-to-image translation tasks where no\npairing information of the training dataset exits. Both quantitative and\nqualitative results prove the superior performance of InjectionGAN over the\nstate-of-the-art approaches.\n", "versions": [{"version": "v1", "created": "Tue, 21 May 2019 17:35:57 GMT"}], "update_date": "2019-05-22", "authors_parsed": [["Xu", "Wenju", ""], ["Keshmiri", "Shawn", ""], ["Wang", "Guanghui", ""]]}, {"id": "1905.08776", "submitter": "Aliaksandra Shysheya Ms", "authors": "Aliaksandra Shysheya (Samsung AI Center, Skolkovo Institute of Science\n  and Technology), Egor Zakharov (Samsung AI Center, Skolkovo Institute of\n  Science and Technology), Kara-Ali Aliev (Samsung AI Center), Renat Bashirov\n  (Samsung AI Center), Egor Burkov (Samsung AI Center, Skolkovo Institute of\n  Science and Technology), Karim Iskakov (Samsung AI Center), Aleksei\n  Ivakhnenko (Samsung AI Center), Yury Malkov (Samsung AI Center), Igor\n  Pasechnik (Samsung AI Center), Dmitry Ulyanov (Samsung AI Center, Skolkovo\n  Institute of Science and Technology), Alexander Vakhitov (Samsung AI Center,\n  Skolkovo Institute of Science and Technology) and Victor Lempitsky (Samsung\n  AI Center, Skolkovo Institute of Science and Technology)", "title": "Textured Neural Avatars", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a system for learning full-body neural avatars, i.e. deep networks\nthat produce full-body renderings of a person for varying body pose and camera\nposition. Our system takes the middle path between the classical graphics\npipeline and the recent deep learning approaches that generate images of humans\nusing image-to-image translation. In particular, our system estimates an\nexplicit two-dimensional texture map of the model surface. At the same time, it\nabstains from explicit shape modeling in 3D. Instead, at test time, the system\nuses a fully-convolutional network to directly map the configuration of body\nfeature points w.r.t. the camera to the 2D texture coordinates of individual\npixels in the image frame. We show that such a system is capable of learning to\ngenerate realistic renderings while being trained on videos annotated with 3D\nposes and foreground masks. We also demonstrate that maintaining an explicit\ntexture representation helps our system to achieve better generalization\ncompared to systems that use direct image-to-image translation.\n", "versions": [{"version": "v1", "created": "Tue, 21 May 2019 17:46:16 GMT"}], "update_date": "2019-05-22", "authors_parsed": [["Shysheya", "Aliaksandra", "", "Samsung AI Center, Skolkovo Institute of Science\n  and Technology"], ["Zakharov", "Egor", "", "Samsung AI Center, Skolkovo Institute of\n  Science and Technology"], ["Aliev", "Kara-Ali", "", "Samsung AI Center"], ["Bashirov", "Renat", "", "Samsung AI Center"], ["Burkov", "Egor", "", "Samsung AI Center, Skolkovo Institute of\n  Science and Technology"], ["Iskakov", "Karim", "", "Samsung AI Center"], ["Ivakhnenko", "Aleksei", "", "Samsung AI Center"], ["Malkov", "Yury", "", "Samsung AI Center"], ["Pasechnik", "Igor", "", "Samsung AI Center"], ["Ulyanov", "Dmitry", "", "Samsung AI Center, Skolkovo\n  Institute of Science and Technology"], ["Vakhitov", "Alexander", "", "Samsung AI Center,\n  Skolkovo Institute of Science and Technology"], ["Lempitsky", "Victor", "", "Samsung\n  AI Center, Skolkovo Institute of Science and Technology"]]}, {"id": "1905.08789", "submitter": "Chaitanya Devaguptapu", "authors": "Chaitanya Devaguptapu, Ninad Akolekar, Manuj M Sharma, Vineeth N\n  Balasubramanian", "title": "Borrow from Anywhere: Pseudo Multi-modal Object Detection in Thermal\n  Imagery", "comments": "Accepted at Perception Beyond Visible Spectrum Workshop, CVPR 2019", "journal-ref": null, "doi": "10.1109/CVPRW.2019.00135", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Can we improve detection in the thermal domain by borrowing features from\nrich domains like visual RGB? In this paper, we propose a pseudo-multimodal\nobject detector trained on natural image domain data to help improve the\nperformance of object detection in thermal images. We assume access to a\nlarge-scale dataset in the visual RGB domain and relatively smaller dataset (in\nterms of instances) in the thermal domain, as is common today. We propose the\nuse of well-known image-to-image translation frameworks to generate pseudo-RGB\nequivalents of a given thermal image and then use a multi-modal architecture\nfor object detection in the thermal image. We show that our framework\noutperforms existing benchmarks without the explicit need for paired training\nexamples from the two domains. We also show that our framework has the ability\nto learn with less data from thermal domain when using our approach. Our code\nand pre-trained models are made available at\nhttps://github.com/tdchaitanya/MMTOD\n", "versions": [{"version": "v1", "created": "Tue, 21 May 2019 12:24:40 GMT"}, {"version": "v2", "created": "Wed, 15 Jul 2020 15:50:35 GMT"}], "update_date": "2020-07-16", "authors_parsed": [["Devaguptapu", "Chaitanya", ""], ["Akolekar", "Ninad", ""], ["Sharma", "Manuj M", ""], ["Balasubramanian", "Vineeth N", ""]]}, {"id": "1905.08790", "submitter": "Zirui Xu", "authors": "Zirui Xu, Fuxun Yu, Xiang Chen", "title": "DoPa: A Comprehensive CNN Detection Methodology against Physical\n  Adversarial Attacks", "comments": "5 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, Convolutional Neural Networks (CNNs) demonstrate a considerable\nvulnerability to adversarial attacks, which can be easily misled by adversarial\nperturbations. With more aggressive methods proposed, adversarial attacks can\nbe also applied to the physical world, causing practical issues to various CNN\npowered applications. To secure CNNs, adversarial attack detection is\nconsidered as the most critical approach. However, most existing works focus on\nsuperficial patterns and merely search a particular method to differentiate the\nadversarial inputs and natural inputs, ignoring the analysis of CNN inner\nvulnerability. Therefore, they can only target to specific physical adversarial\nattacks, lacking expected versatility to different attacks. To address this\nissue, we propose DoPa -- a comprehensive CNN detection methodology for various\nphysical adversarial attacks. By interpreting the CNN's vulnerability, we find\nthat non-semantic adversarial perturbations can activate CNN with significantly\nabnormal activations and even overwhelm other semantic input patterns'\nactivations. Therefore, we add a self-verification stage to analyze the\nsemantics of distinguished activation patterns, which improves the CNN\nrecognition process. We apply such a detection methodology into both image and\naudio CNN recognition scenarios. Experiments show that DoPa can achieve an\naverage rate of 90% success for image attack detection and 92% success for\naudio attack detection.\n  Announcement:[The original DoPa draft on arXiv was modified and submitted to\na conference already, while this short abstract was submitted only for a\npresentation at the KDD 2019 AIoT Workshop.]\n", "versions": [{"version": "v1", "created": "Tue, 21 May 2019 19:53:38 GMT"}, {"version": "v2", "created": "Fri, 19 Jul 2019 18:56:50 GMT"}, {"version": "v3", "created": "Fri, 23 Aug 2019 20:38:44 GMT"}, {"version": "v4", "created": "Wed, 28 Aug 2019 15:07:07 GMT"}], "update_date": "2019-08-29", "authors_parsed": [["Xu", "Zirui", ""], ["Yu", "Fuxun", ""], ["Chen", "Xiang", ""]]}, {"id": "1905.08843", "submitter": "Ahmad Babaeian Jelodar", "authors": "Ahmad Babaeian Jelodar, Yu Sun", "title": "Joint Object and State Recognition using Language Knowledge", "comments": "5 pages, 4 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The state of an object is an important piece of knowledge in robotics\napplications. States and objects are intertwined together, meaning that object\ninformation can help recognize the state of an image and vice versa. This paper\naddresses the state identification problem in cooking related images and uses\nstate and object predictions together to improve the classification accuracy of\nobjects and their states from a single image. The pipeline presented in this\npaper includes a CNN with a double classification layer and the Concept-Net\nlanguage knowledge graph on top. The language knowledge creates a semantic\nlikelihood between objects and states. The resulting object and state\nconfidences from the deep architecture are used together with object and state\nrelatedness estimates from a language knowledge graph to produce marginal\nprobabilities for objects and states. The marginal probabilities and\nconfidences of objects (or states) are fused together to improve the final\nobject (or state) classification results. Experiments on a dataset of cooking\nobjects show that using a language knowledge graph on top of a deep neural\nnetwork effectively enhances object and state classification.\n", "versions": [{"version": "v1", "created": "Mon, 13 May 2019 13:26:17 GMT"}], "update_date": "2019-05-23", "authors_parsed": [["Jelodar", "Ahmad Babaeian", ""], ["Sun", "Yu", ""]]}, {"id": "1905.08845", "submitter": "Sylvestre-Alvise Rebuffi", "authors": "Sylvestre-Alvise Rebuffi, Sebastien Ehrhardt, Kai Han, Andrea Vedaldi,\n  Andrew Zisserman", "title": "Semi-Supervised Learning with Scarce Annotations", "comments": "Workshop on Deep Vision, CVPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While semi-supervised learning (SSL) algorithms provide an efficient way to\nmake use of both labelled and unlabelled data, they generally struggle when the\nnumber of annotated samples is very small. In this work, we consider the\nproblem of SSL multi-class classification with very few labelled instances. We\nintroduce two key ideas. The first is a simple but effective one: we leverage\nthe power of transfer learning among different tasks and self-supervision to\ninitialize a good representation of the data without making use of any label.\nThe second idea is a new algorithm for SSL that can exploit well such a\npre-trained representation.\n  The algorithm works by alternating two phases, one fitting the labelled\npoints and one fitting the unlabelled ones, with carefully-controlled\ninformation flow between them. The benefits are greatly reducing overfitting of\nthe labelled data and avoiding issue with balancing labelled and unlabelled\nlosses during training. We show empirically that this method can successfully\ntrain competitive models with as few as 10 labelled data points per class. More\nin general, we show that the idea of bootstrapping features using\nself-supervised learning always improves SSL on standard benchmarks. We show\nthat our algorithm works increasingly well compared to other methods when\nrefining from other tasks or datasets.\n", "versions": [{"version": "v1", "created": "Tue, 21 May 2019 19:41:42 GMT"}, {"version": "v2", "created": "Tue, 21 Apr 2020 21:20:30 GMT"}], "update_date": "2020-04-23", "authors_parsed": [["Rebuffi", "Sylvestre-Alvise", ""], ["Ehrhardt", "Sebastien", ""], ["Han", "Kai", ""], ["Vedaldi", "Andrea", ""], ["Zisserman", "Andrew", ""]]}, {"id": "1905.08853", "submitter": "Chao Wang", "authors": "Chao Wang and Xiaohu Guo", "title": "Efficient Plane-Based Optimization of Geometry and Texture for Indoor\n  RGB-D Reconstruction", "comments": "In the SUMO Workshop of CVPR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel approach to reconstruct RGB-D indoor scene based on plane\nprimitives. Our approach takes as input a RGB-D sequence and a dense coarse\nmesh reconstructed from it, and generates a lightweight, low-polygonal mesh\nwith clear face textures and sharp features without losing geometry details\nfrom the original scene. Compared to existing methods which only cover large\nplanar regions in the scene, our method builds the entire scene by adaptive\nplanes without losing geometry details and also preserves sharp features in the\nmesh. Experiments show that our method is more efficient to generate textured\nmesh from RGB-D data than state-of-the-arts.\n", "versions": [{"version": "v1", "created": "Tue, 21 May 2019 20:11:03 GMT"}], "update_date": "2019-05-23", "authors_parsed": [["Wang", "Chao", ""], ["Guo", "Xiaohu", ""]]}, {"id": "1905.08855", "submitter": "Chiho Choi", "authors": "Chiho Choi and Behzad Dariush", "title": "Looking to Relations for Future Trajectory Forecast", "comments": "ICCV 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inferring relational behavior between road users as well as road users and\ntheir surrounding physical space is an important step toward effective modeling\nand prediction of navigation strategies adopted by participants in road scenes.\nTo this end, we propose a relation-aware framework for future trajectory\nforecast. Our system aims to infer relational information from the interactions\nof road users with each other and with the environment. The first module\ninvolves visual encoding of spatio-temporal features, which captures\nhuman-human and human-space interactions over time. The following module\nexplicitly constructs pair-wise relations from spatio-temporal interactions and\nidentifies more descriptive relations that highly influence future motion of\nthe target road user by considering its past trajectory. The resulting\nrelational features are used to forecast future locations of the target, in the\nform of heatmaps with an additional guidance of spatial dependencies and\nconsideration of the uncertainty. Extensive evaluations on the public benchmark\ndatasets demonstrate the robustness and efficacy of the proposed framework as\nobserved by performances higher than the state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Tue, 21 May 2019 20:12:42 GMT"}, {"version": "v2", "created": "Sat, 27 Jul 2019 00:58:28 GMT"}, {"version": "v3", "created": "Wed, 7 Aug 2019 16:05:50 GMT"}, {"version": "v4", "created": "Tue, 27 Aug 2019 17:48:18 GMT"}], "update_date": "2019-08-28", "authors_parsed": [["Choi", "Chiho", ""], ["Dariush", "Behzad", ""]]}, {"id": "1905.08886", "submitter": "Dogancan Temel", "authors": "Dogancan Temel and Melvin J. Mathew and Ghassan AlRegib and Yousuf M.\n  Khalifa", "title": "Automated Pupillary Light Reflex Test on a Portable Platform", "comments": "7 pages, 11 figures, 3 tables", "journal-ref": "International Symposium on Medical Robotics (ISMR), Atlanta, GA,\n  USA, 2019, pp. 1-7", "doi": "10.1109/ISMR.2019.8710182", "report-no": null, "categories": "eess.IV cs.CV eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce a portable eye imaging device denoted as\nlab-on-a-headset, which can automatically perform a swinging flashlight test.\nWe utilized this device in a clinical study to obtain high-resolution\nrecordings of eyes while they are exposed to a varying light stimuli. Half of\nthe participants had relative afferent pupillary defect (RAPD) while the other\nhalf was a control group. In case of positive RAPD, patients pupils constrict\nless or do not constrict when light stimuli swings from the unaffected eye to\nthe affected eye. To automatically diagnose RAPD, we propose an algorithm based\non pupil localization, pupil size measurement, and pupil size comparison of\nright and left eye during the light reflex test. We validate the algorithmic\nperformance over a dataset obtained from 22 subjects and show that proposed\nalgorithm can achieve a sensitivity of 93.8% and a specificity of 87.5%.\n", "versions": [{"version": "v1", "created": "Tue, 21 May 2019 22:15:21 GMT"}], "update_date": "2019-05-23", "authors_parsed": [["Temel", "Dogancan", ""], ["Mathew", "Melvin J.", ""], ["AlRegib", "Ghassan", ""], ["Khalifa", "Yousuf M.", ""]]}, {"id": "1905.08910", "submitter": "Michael Kissner", "authors": "Michael Kissner, Helmut Mayer", "title": "A Neural-Symbolic Architecture for Inverse Graphics Improved by Lifelong\n  Meta-Learning", "comments": "German Conference on Pattern Recognition (GCPR) 2019", "journal-ref": "41st German Conference, GCPR 2019, Proceedings", "doi": "10.1007/978-3-030-33676-9", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We follow the idea of formulating vision as inverse graphics and propose a\nnew type of element for this task, a neural-symbolic capsule. It is capable of\nde-rendering a scene into semantic information feed-forward, as well as\nrendering it feed-backward. An initial set of capsules for graphical primitives\nis obtained from a generative grammar and connected into a full capsule\nnetwork. Lifelong meta-learning continuously improves this network's detection\ncapabilities by adding capsules for new and more complex objects it detects in\na scene using few-shot learning. Preliminary results demonstrate the potential\nof our novel approach.\n", "versions": [{"version": "v1", "created": "Wed, 22 May 2019 00:48:12 GMT"}, {"version": "v2", "created": "Thu, 5 Sep 2019 00:55:13 GMT"}], "update_date": "2019-09-26", "authors_parsed": [["Kissner", "Michael", ""], ["Mayer", "Helmut", ""]]}, {"id": "1905.08924", "submitter": "Peng Xu", "authors": "Peng Xu, Zhaohong Deng, Kup-Sze Choi, Jun Wang, Shitong Wang", "title": "Joint Information Preservation for Heterogeneous Domain Adaptation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Domain adaptation aims to assist the modeling tasks of the target domain with\nknowledge of the source domain. The two domains often lie in different feature\nspaces due to diverse data collection methods, which leads to the more\nchallenging task of heterogeneous domain adaptation (HDA). A core issue of HDA\nis how to preserve the information of the original data during adaptation. In\nthis paper, we propose a joint information preservation method to deal with the\nproblem. The method preserves the information of the original data from two\naspects. On the one hand, although paired samples often exist between the two\ndomains of the HDA, current algorithms do not utilize such information\nsufficiently. The proposed method preserves the paired information by\nmaximizing the correlation of the paired samples in the shared subspace. On the\nother hand, the proposed method improves the strategy of preserving the\nstructural information of the original data, where the local and global\nstructural information are preserved simultaneously. Finally, the joint\ninformation preservation is integrated by distribution matching. Experimental\nresults show the superiority of the proposed method over the state-of-the-art\nHDA algorithms.\n", "versions": [{"version": "v1", "created": "Wed, 22 May 2019 02:15:18 GMT"}], "update_date": "2019-05-23", "authors_parsed": [["Xu", "Peng", ""], ["Deng", "Zhaohong", ""], ["Choi", "Kup-Sze", ""], ["Wang", "Jun", ""], ["Wang", "Shitong", ""]]}, {"id": "1905.08929", "submitter": "Zhen Mingmin", "authors": "Mingmin Zhen, Jinglu Wang, Lei Zhou, Tian Fang, Long Quan", "title": "Learning Fully Dense Neural Networks for Image Semantic Segmentation", "comments": null, "journal-ref": "AAAI 2019", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semantic segmentation is pixel-wise classification which retains critical\nspatial information. The \"feature map reuse\" has been commonly adopted in CNN\nbased approaches to take advantage of feature maps in the early layers for the\nlater spatial reconstruction. Along this direction, we go a step further by\nproposing a fully dense neural network with an encoder-decoder structure that\nwe abbreviate as FDNet. For each stage in the decoder module, feature maps of\nall the previous blocks are adaptively aggregated to feed-forward as input. On\nthe one hand, it reconstructs the spatial boundaries accurately. On the other\nhand, it learns more efficiently with the more efficient gradient\nbackpropagation. In addition, we propose the boundary-aware loss function to\nfocus more attention on the pixels near the boundary, which boosts the \"hard\nexamples\" labeling. We have demonstrated the best performance of the FDNet on\nthe two benchmark datasets: PASCAL VOC 2012, NYUDv2 over previous works when\nnot considering training on other datasets.\n", "versions": [{"version": "v1", "created": "Wed, 22 May 2019 02:41:53 GMT"}], "update_date": "2019-05-23", "authors_parsed": [["Zhen", "Mingmin", ""], ["Wang", "Jinglu", ""], ["Zhou", "Lei", ""], ["Fang", "Tian", ""], ["Quan", "Long", ""]]}, {"id": "1905.08955", "submitter": "Khaled Saleh", "authors": "Khaled Saleh, Ahmed Abobakr, Mohammed Attia, Julie Iskander, Darius\n  Nahavandi, Mohammed Hossny", "title": "Domain Adaptation for Vehicle Detection from Bird's Eye View LiDAR Point\n  Cloud Data", "comments": "Under review for IEEE SMC 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Point cloud data from 3D LiDAR sensors are one of the most crucial sensor\nmodalities for versatile safety-critical applications such as self-driving\nvehicles. Since the annotations of point cloud data is an expensive and\ntime-consuming process, therefore recently the utilisation of simulated\nenvironments and 3D LiDAR sensors for this task started to get some popularity.\nWith simulated sensors and environments, the process for obtaining an annotated\nsynthetic point cloud data became much easier. However, the generated synthetic\npoint cloud data are still missing the artefacts usually exist in point cloud\ndata from real 3D LiDAR sensors. As a result, the performance of the trained\nmodels on this data for perception tasks when tested on real point cloud data\nis degraded due to the domain shift between simulated and real environments.\nThus, in this work, we are proposing a domain adaptation framework for bridging\nthis gap between synthetic and real point cloud data. Our proposed framework is\nbased on the deep cycle-consistent generative adversarial networks (CycleGAN)\narchitecture. We have evaluated the performance of our proposed framework on\nthe task of vehicle detection from a bird's eye view (BEV) point cloud images\ncoming from real 3D LiDAR sensors. The framework has shown competitive results\nwith an improvement of more than 7% in average precision score over other\nbaseline approaches when tested on real BEV point cloud images.\n", "versions": [{"version": "v1", "created": "Wed, 22 May 2019 05:24:26 GMT"}], "update_date": "2019-05-23", "authors_parsed": [["Saleh", "Khaled", ""], ["Abobakr", "Ahmed", ""], ["Attia", "Mohammed", ""], ["Iskander", "Julie", ""], ["Nahavandi", "Darius", ""], ["Hossny", "Mohammed", ""]]}, {"id": "1905.08965", "submitter": "Sicheng Wang", "authors": "Sicheng Wang, Bihan Wen, Junru Wu, Dacheng Tao, Zhangyang Wang", "title": "Segmentation-Aware Image Denoising without Knowing True Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Several recent works discussed application-driven image restoration neural\nnetworks, which are capable of not only removing noise in images but also\npreserving their semantic-aware details, making them suitable for various\nhigh-level computer vision tasks as the pre-processing step. However, such\napproaches require extra annotations for their high-level vision tasks, in\norder to train the joint pipeline using hybrid losses. The availability of\nthose annotations is yet often limited to a few image sets, potentially\nrestricting the general applicability of these methods to denoising more unseen\nand unannotated images. Motivated by that, we propose a segmentation-aware\nimage denoising model dubbed U-SAID, based on a novel unsupervised approach\nwith a pixel-wise uncertainty loss. U-SAID does not need any ground-truth\nsegmentation map, and thus can be applied to any image dataset. It generates\ndenoised images with comparable or even better quality, and the denoised\nresults show stronger robustness for subsequent semantic segmentation tasks,\nwhen compared to either its supervised counterpart or classical\n\"application-agnostic\" denoisers. Moreover, we demonstrate the superior\ngeneralizability of U-SAID in three-folds, by plugging its \"universal\" denoiser\nwithout fine-tuning: (1) denoising unseen types of images; (2) denoising as\npre-processing for segmenting unseen noisy images; and (3) denoising for unseen\nhigh-level tasks. Extensive experiments demonstrate the effectiveness,\nrobustness and generalizability of the proposed U-SAID over various popular\nimage sets.\n", "versions": [{"version": "v1", "created": "Wed, 22 May 2019 06:05:36 GMT"}], "update_date": "2019-05-23", "authors_parsed": [["Wang", "Sicheng", ""], ["Wen", "Bihan", ""], ["Wu", "Junru", ""], ["Tao", "Dacheng", ""], ["Wang", "Zhangyang", ""]]}, {"id": "1905.08983", "submitter": "Babak Namazi", "authors": "Babak Namazi, Ganesh Sankaranarayanan, Venkat Devarajan", "title": "LapTool-Net: A Contextual Detector of Surgical Tools in Laparoscopic\n  Videos Based on Recurrent Convolutional Neural Networks", "comments": "18 pages, 4 figures, Submitted to Medical Image Analysis", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new multilabel classifier, called LapTool-Net to detect the\npresence of surgical tools in each frame of a laparoscopic video. The novelty\nof LapTool-Net is the exploitation of the correlation among the usage of\ndifferent tools and, the tools and tasks - namely, the context of the tools'\nusage. Towards this goal, the pattern in the co-occurrence of the tools is\nutilized for designing a decision policy for a multilabel classifier based on a\nRecurrent Convolutional Neural Network (RCNN) architecture to simultaneously\nextract the spatio-temporal features. In contrast to the previous multilabel\nclassification methods, the RCNN and the decision model are trained in an\nend-to-end manner using a multitask learning scheme. To overcome the high\nimbalance and avoid overfitting caused by the lack of variety in the training\ndata, a high down-sampling rate is chosen based on the more frequent\ncombinations. Furthermore, at the post-processing step, the prediction for all\nthe frames of a video are corrected by designing a bi-directional RNN to model\nthe long-term task's order. LapTool-net was trained using a publicly available\ndataset of laparoscopic cholecystectomy. The results show LapTool-Net\noutperforms existing methods significantly, even while using fewer training\nsamples and a shallower architecture.\n", "versions": [{"version": "v1", "created": "Wed, 22 May 2019 06:55:41 GMT"}], "update_date": "2019-05-23", "authors_parsed": [["Namazi", "Babak", ""], ["Sankaranarayanan", "Ganesh", ""], ["Devarajan", "Venkat", ""]]}, {"id": "1905.08997", "submitter": "Xianmin Lin", "authors": "Aihua Zheng, Xianmin Lin, Chenglong Li, Ran He, and Jin Tang", "title": "Attributes Guided Feature Learning for Vehicle Re-identification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Vehicle Re-ID has recently attracted enthusiastic attention due to its\npotential applications in smart city and urban surveillance. However, it\nsuffers from large intra-class variation caused by view variations and\nillumination changes, and inter-class similarity especially for different\nidentities with the similar appearance. To handle these issues, in this paper,\nwe propose a novel deep network architecture, which guided by meaningful\nattributes including camera views, vehicle types and colors for vehicle Re-ID.\nIn particular, our network is end-to-end trained and contains three subnetworks\nof deep features embedded by the corresponding attributes (i.e., camera view,\nvehicle type and vehicle color). Moreover, to overcome the shortcomings of\nlimited vehicle images of different views, we design a view-specified\ngenerative adversarial network to generate the multi-view vehicle images. For\nnetwork training, we annotate the view labels on the VeRi-776 dataset. Note\nthat one can directly adopt the pre-trained view (as well as type and color)\nsubnetwork on the other datasets with only ID information, which demonstrates\nthe generalization of our model. Extensive experiments on the benchmark\ndatasets VeRi-776 and VehicleID suggest that the proposed approach achieves the\npromising performance and yields to a new state-of-the-art for vehicle Re-ID.\n", "versions": [{"version": "v1", "created": "Wed, 22 May 2019 07:42:02 GMT"}], "update_date": "2019-05-23", "authors_parsed": [["Zheng", "Aihua", ""], ["Lin", "Xianmin", ""], ["Li", "Chenglong", ""], ["He", "Ran", ""], ["Tang", "Jin", ""]]}, {"id": "1905.09000", "submitter": "Yousif Hashisho", "authors": "Yousif Hashisho, Mohamad Albadawi, Tom Krause, and Uwe Freiherr von\n  Lukas", "title": "Underwater Color Restoration Using U-Net Denoising Autoencoder", "comments": "6 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Visual inspection of underwater structures by vehicles, e.g. remotely\noperated vehicles (ROVs), plays an important role in scientific, military, and\ncommercial sectors. However, the automatic extraction of information using\nsoftware tools is hindered by the characteristics of water which degrade the\nquality of captured videos. As a contribution for restoring the color of\nunderwater images, Underwater Denoising Autoencoder (UDAE) model is developed\nusing a denoising autoencoder with U-Net architecture. The proposed network\ntakes into consideration the accuracy and the computation cost to enable\nreal-time implementation on underwater visual tasks using end-to-end\nautoencoder network. Underwater vehicles perception is improved by\nreconstructing captured frames; hence obtaining better performance in\nunderwater tasks. Related learning methods use generative adversarial networks\n(GANs) to generate color corrected underwater images, and to our knowledge this\npaper is the first to deal with a single autoencoder capable of producing same\nor better results. Moreover, image pairs are constructed for training the\nproposed network, where it is hard to obtain such dataset from underwater\nscenery. At the end, the proposed model is compared to a state-of-the-art\nmethod.\n", "versions": [{"version": "v1", "created": "Wed, 22 May 2019 07:49:45 GMT"}], "update_date": "2020-02-18", "authors_parsed": [["Hashisho", "Yousif", ""], ["Albadawi", "Mohamad", ""], ["Krause", "Tom", ""], ["von Lukas", "Uwe Freiherr", ""]]}, {"id": "1905.09010", "submitter": "Yong-Goo Shin", "authors": "Yong-Goo Shin, Min-Cheol Sagong, Yoon-Jae Yeo, Seung-Wook Kim,\n  Sung-Jea Ko", "title": "PEPSI++: Fast and Lightweight Network for Image Inpainting", "comments": "Accepted to IEEE transactions on Neural Networks and Learning\n  Systems. To be published", "journal-ref": null, "doi": "10.1109/TNNLS.2020.2978501", "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Among the various generative adversarial network (GAN)-based image inpainting\nmethods, a coarse-to-fine network with a contextual attention module (CAM) has\nshown remarkable performance. However, owing to two stacked generative\nnetworks, the coarse-to-fine network needs numerous computational resources\nsuch as convolution operations and network parameters, which result in low\nspeed. To address this problem, we propose a novel network architecture called\nPEPSI: parallel extended-decoder path for semantic inpainting network, which\naims at reducing the hardware costs and improving the inpainting performance.\nPEPSI consists of a single shared encoding network and parallel decoding\nnetworks called coarse and inpainting paths. The coarse path produces a\npreliminary inpainting result to train the encoding network for the prediction\nof features for the CAM. Simultaneously, the inpainting path generates higher\ninpainting quality using the refined features reconstructed via the CAM. In\naddition, we propose Diet-PEPSI that significantly reduces the network\nparameters while maintaining the performance. In Diet-PEPSI, to capture the\nglobal contextual information with low hardware costs, we propose novel\nrate-adaptive dilated convolutional layers, which employ the common weights but\nproduce dynamic features depending on the given dilation rates. Extensive\nexperiments comparing the performance with state-of-the-art image inpainting\nmethods demonstrate that both PEPSI and Diet-PEPSI improve the qualitative\nscores, i.e. the peak signal-to-noise ratio (PSNR) and structural similarity\n(SSIM), as well as significantly reduce hardware costs such as computational\ntime and the number of network parameters.\n", "versions": [{"version": "v1", "created": "Wed, 22 May 2019 08:18:48 GMT"}, {"version": "v2", "created": "Wed, 9 Oct 2019 05:45:43 GMT"}, {"version": "v3", "created": "Thu, 10 Oct 2019 03:23:20 GMT"}, {"version": "v4", "created": "Tue, 21 Jan 2020 08:25:53 GMT"}, {"version": "v5", "created": "Fri, 6 Mar 2020 23:33:43 GMT"}], "update_date": "2020-03-20", "authors_parsed": [["Shin", "Yong-Goo", ""], ["Sagong", "Min-Cheol", ""], ["Yeo", "Yoon-Jae", ""], ["Kim", "Seung-Wook", ""], ["Ko", "Sung-Jea", ""]]}, {"id": "1905.09033", "submitter": "Davide Mazzini", "authors": "Davide Mazzini, Raimondo Schettini", "title": "Spatial Sampling Network for Fast Scene Understanding", "comments": "Accepted at CVPR2019 Workshop on Autonomous Driving", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a network architecture to perform efficient scene understanding.\nThis work presents three main novelties: the first is an Improved Guided\nUpsampling Module that can replace in toto the decoder part in common semantic\nsegmentation networks. Our second contribution is the introduction of a new\nmodule based on spatial sampling to perform Instance Segmentation. It provides\na very fast instance segmentation, needing only thresholding as post-processing\nstep at inference time. Finally, we propose a novel efficient network design\nthat includes the new modules and test it against different datasets for\noutdoor scene understanding. To our knowledge, our network is one of the\nthemost efficient architectures for scene understanding published to date,\nfurthermore being 8.6% more accurate than the fastest competitor on semantic\nsegmentation and almost five times faster than the most efficient network for\ninstance segmentation.\n", "versions": [{"version": "v1", "created": "Wed, 22 May 2019 09:24:17 GMT"}], "update_date": "2019-05-23", "authors_parsed": [["Mazzini", "Davide", ""], ["Schettini", "Raimondo", ""]]}, {"id": "1905.09035", "submitter": "Antonino Furnari", "authors": "Antonino Furnari and Giovanni Maria Farinella", "title": "What Would You Expect? Anticipating Egocentric Actions with\n  Rolling-Unrolling LSTMs and Modality Attention", "comments": "Accepted as oral to ICCV [International Conference on Computer\n  Vision] 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Egocentric action anticipation consists in understanding which objects the\ncamera wearer will interact with in the near future and which actions they will\nperform. We tackle the problem proposing an architecture able to anticipate\nactions at multiple temporal scales using two LSTMs to 1) summarize the past,\nand 2) formulate predictions about the future. The input video is processed\nconsidering three complimentary modalities: appearance (RGB), motion (optical\nflow) and objects (object-based features). Modality-specific predictions are\nfused using a novel Modality ATTention (MATT) mechanism which learns to weigh\nmodalities in an adaptive fashion. Extensive evaluations on two large-scale\nbenchmark datasets show that our method outperforms prior art by up to +7% on\nthe challenging EPIC-Kitchens dataset including more than 2500 actions, and\ngeneralizes to EGTEA Gaze+. Our approach is also shown to generalize to the\ntasks of early action recognition and action recognition. Our method is ranked\nfirst in the public leaderboard of the EPIC-Kitchens egocentric action\nanticipation challenge 2019. Please see our web pages for code and examples:\nhttp://iplab.dmi.unict.it/rulstm - https://github.com/fpv-iplab/rulstm.\n", "versions": [{"version": "v1", "created": "Wed, 22 May 2019 09:25:25 GMT"}, {"version": "v2", "created": "Mon, 5 Aug 2019 21:09:08 GMT"}], "update_date": "2019-08-07", "authors_parsed": [["Furnari", "Antonino", ""], ["Farinella", "Giovanni Maria", ""]]}, {"id": "1905.09043", "submitter": "Federica Arrigoni", "authors": "Federica Arrigoni and Tomas Pajdla", "title": "Robust Motion Segmentation from Pairwise Matches", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we address a classification problem that has not been\nconsidered before, namely motion segmentation given pairwise matches only. Our\ncontribution to this unexplored task is a novel formulation of motion\nsegmentation as a two-step process. First, motion segmentation is performed on\nimage pairs independently. Secondly, we combine independent pairwise\nsegmentation results in a robust way into the final globally consistent\nsegmentation. Our approach is inspired by the success of averaging methods. We\ndemonstrate in simulated as well as in real experiments that our method is very\neffective in reducing the errors in the pairwise motion segmentation and can\ncope with large number of mismatches.\n", "versions": [{"version": "v1", "created": "Wed, 22 May 2019 09:52:22 GMT"}], "update_date": "2019-05-23", "authors_parsed": [["Arrigoni", "Federica", ""], ["Pajdla", "Tomas", ""]]}, {"id": "1905.09045", "submitter": "Lorenzo Cerrone", "authors": "Lorenzo Cerrone, Alexander Zeilmann, Fred A. Hamprecht", "title": "End-to-End Learned Random Walker for Seeded Image Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present an end-to-end learned algorithm for seeded segmentation. Our\nmethod is based on the Random Walker algorithm, where we predict the edge\nweights of the underlying graph using a convolutional neural network. This can\nbe interpreted as learning context-dependent diffusivities for a linear\ndiffusion process. Besides calculating the exact gradient for optimizing these\ndiffusivities, we also propose simplifications that sparsely sample the\ngradient and still yield competitive results. The proposed method achieves the\ncurrently best results on a seeded version of the CREMI neuron segmentation\nchallenge.\n", "versions": [{"version": "v1", "created": "Wed, 22 May 2019 09:56:04 GMT"}], "update_date": "2019-05-23", "authors_parsed": [["Cerrone", "Lorenzo", ""], ["Zeilmann", "Alexander", ""], ["Hamprecht", "Fred A.", ""]]}, {"id": "1905.09049", "submitter": "Jia You", "authors": "Jia You, Philip L.H. Yu, Anderson C.O. Tsang, Eva L.H. Tsui, Pauline\n  P.S. Woo and Gilberto K.K. Leung", "title": "Automated Segmentation for Hyperdense Middle Cerebral Artery Sign of\n  Acute Ischemic Stroke on Non-Contrast CT Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The hyperdense middle cerebral artery (MCA) dot sign has been reported as an\nimportant factor in the diagnosis of acute ischemic stroke due to large vessel\nocclusion. Interpreting the initial CT brain scan in these patients requires\nhigh level of expertise, and has high inter-observer variability. An automated\ncomputerized interpretation of the urgent CT brain image, with an emphasis to\npick up early signs of ischemic stroke will facilitate early patient diagnosis,\ntriage, and shorten the door-to-revascularization time for these group of\npatients. In this paper, we present an automated detection method of segmenting\nthe MCA dot sign on non-contrast CT brain image scans based on powerful deep\nlearning technique.\n", "versions": [{"version": "v1", "created": "Wed, 22 May 2019 10:07:26 GMT"}], "update_date": "2019-05-23", "authors_parsed": [["You", "Jia", ""], ["Yu", "Philip L. H.", ""], ["Tsang", "Anderson C. O.", ""], ["Tsui", "Eva L. H.", ""], ["Woo", "Pauline P. S.", ""], ["Leung", "Gilberto K. K.", ""]]}, {"id": "1905.09050", "submitter": "Mahesh Chandra Mukkamala", "authors": "Mahesh Chandra Mukkamala, Peter Ochs", "title": "Beyond Alternating Updates for Matrix Factorization with Inertial\n  Bregman Proximal Gradient Algorithms", "comments": "Accepted at NeuRIPS 2019. Paper url:\n  http://papers.nips.cc/paper/8679-beyond-alternating-updates-for-matrix-factorization-with-inertial-bregman-proximal-gradient-algorithms", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.CV cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Matrix Factorization is a popular non-convex optimization problem, for which\nalternating minimization schemes are mostly used. They usually suffer from the\nmajor drawback that the solution is biased towards one of the optimization\nvariables. A remedy is non-alternating schemes. However, due to a lack of\nLipschitz continuity of the gradient in matrix factorization problems,\nconvergence cannot be guaranteed. A recently developed approach relies on the\nconcept of Bregman distances, which generalizes the standard Euclidean\ndistance. We exploit this theory by proposing a novel Bregman distance for\nmatrix factorization problems, which, at the same time, allows for\nsimple/closed form update steps. Therefore, for non-alternating schemes, such\nas the recently introduced Bregman Proximal Gradient (BPG) method and an\ninertial variant Convex--Concave Inertial BPG (CoCaIn BPG), convergence of the\nwhole sequence to a stationary point is proved for Matrix Factorization. In\nseveral experiments, we observe a superior performance of our non-alternating\nschemes in terms of speed and objective value at the limit point.\n", "versions": [{"version": "v1", "created": "Wed, 22 May 2019 10:09:54 GMT"}, {"version": "v2", "created": "Fri, 6 Dec 2019 12:33:13 GMT"}], "update_date": "2019-12-09", "authors_parsed": [["Mukkamala", "Mahesh Chandra", ""], ["Ochs", "Peter", ""]]}, {"id": "1905.09054", "submitter": "Mete Ozay", "authors": "Mete Ozay", "title": "Fine-grained Optimization of Deep Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent studies, several asymptotic upper bounds on generalization errors\non deep neural networks (DNNs) are theoretically derived. These bounds are\nfunctions of several norms of weights of the DNNs, such as the Frobenius and\nspectral norms, and they are computed for weights grouped according to either\ninput and output channels of the DNNs. In this work, we conjecture that if we\ncan impose multiple constraints on weights of DNNs to upper bound the norms of\nthe weights, and train the DNNs with these weights, then we can attain\nempirical generalization errors closer to the derived theoretical bounds, and\nimprove accuracy of the DNNs.\n  To this end, we pose two problems. First, we aim to obtain weights whose\ndifferent norms are all upper bounded by a constant number, e.g. 1.0. To\nachieve these bounds, we propose a two-stage renormalization procedure; (i)\nnormalization of weights according to different norms used in the bounds, and\n(ii) reparameterization of the normalized weights to set a constant and finite\nupper bound of their norms. In the second problem, we consider training DNNs\nwith these renormalized weights. To this end, we first propose a strategy to\nconstruct joint spaces (manifolds) of weights according to different\nconstraints in DNNs. Next, we propose a fine-grained SGD algorithm (FG-SGD) for\noptimization on the weight manifolds to train DNNs with assurance of\nconvergence to minima. Experimental results show that image classification\naccuracy of baseline DNNs can be boosted using FG-SGD on collections of\nmanifolds identified by multiple constraints.\n", "versions": [{"version": "v1", "created": "Wed, 22 May 2019 10:18:58 GMT"}], "update_date": "2019-05-23", "authors_parsed": [["Ozay", "Mete", ""]]}, {"id": "1905.09113", "submitter": "Linda Studer", "authors": "Linda Studer, Michele Alberti, Vinaychandran Pondenkandath, Pinar\n  Goktepe, Thomas Kolonko, Andreas Fischer, Marcus Liwicki, Rolf Ingold", "title": "A Comprehensive Study of ImageNet Pre-Training for Historical Document\n  Image Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic analysis of scanned historical documents comprises a wide range of\nimage analysis tasks, which are often challenging for machine learning due to a\nlack of human-annotated learning samples. With the advent of deep neural\nnetworks, a promising way to cope with the lack of training data is to\npre-train models on images from a different domain and then fine-tune them on\nhistorical documents. In the current research, a typical example of such\ncross-domain transfer learning is the use of neural networks that have been\npre-trained on the ImageNet database for object recognition. It remains a\nmostly open question whether or not this pre-training helps to analyse\nhistorical documents, which have fundamentally different image properties when\ncompared with ImageNet. In this paper, we present a comprehensive empirical\nsurvey on the effect of ImageNet pre-training for diverse historical document\nanalysis tasks, including character recognition, style classification,\nmanuscript dating, semantic segmentation, and content-based retrieval. While we\nobtain mixed results for semantic segmentation at pixel-level, we observe a\nclear trend across different network architectures that ImageNet pre-training\nhas a positive effect on classification as well as content-based retrieval.\n", "versions": [{"version": "v1", "created": "Wed, 22 May 2019 13:07:00 GMT"}], "update_date": "2019-05-23", "authors_parsed": [["Studer", "Linda", ""], ["Alberti", "Michele", ""], ["Pondenkandath", "Vinaychandran", ""], ["Goktepe", "Pinar", ""], ["Kolonko", "Thomas", ""], ["Fischer", "Andreas", ""], ["Liwicki", "Marcus", ""], ["Ingold", "Rolf", ""]]}, {"id": "1905.09147", "submitter": "Rongjun Qin", "authors": "Bihe Chen, Rongjun Qin, Xu Huang, Shuang Song and Xiaohu Lu", "title": "A Comparison of Stereo-Matching Cost between Convolutional Neural\n  Network and Census for Satellite Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stereo dense image matching can be categorized to low-level feature based\nmatching and deep feature based matching according to their matching cost\nmetrics. Census has been proofed to be one of the most efficient low-level\nfeature based matching methods, while fast Convolutional Neural Network\n(fst-CNN), as a deep feature based method, has small computing time and is\nrobust for satellite images. Thus, a comparison between fst-CNN and census is\ncritical for further studies in stereo dense image matching. This paper used\ncost function of fst-CNN and census to do stereo matching, then utilized\nsemi-global matching method to obtain optimized disparity images. Those images\nare used to produce digital surface model to compare with ground truth points.\nIt addresses that fstCNN performs better than census in the aspect of absolute\nmatching accuracy, histogram of error distribution and matching completeness,\nbut these two algorithms still performs in the same order of magnitude.\n", "versions": [{"version": "v1", "created": "Wed, 22 May 2019 14:00:11 GMT"}], "update_date": "2019-05-23", "authors_parsed": [["Chen", "Bihe", ""], ["Qin", "Rongjun", ""], ["Huang", "Xu", ""], ["Song", "Shuang", ""], ["Lu", "Xiaohu", ""]]}, {"id": "1905.09150", "submitter": "Rongjun Qin", "authors": "Xiaohu Lu, Rongjun Qin and Xu Huang", "title": "Using Orthophoto for Building Boundary Sharpening in the Digital Surface\n  Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nowadays dense stereo matching has become one of the dominant tools in 3D\nreconstruction of urban regions for its low cost and high flexibility in\ngenerating dense 3D points. However, state-of-the-art stereo matching\nalgorithms usually apply a semi-global matching (SGM) strategy. This strategy\nnormally assumes the surface geometry pieceswise planar, where a smooth penalty\nis imposed to deal with non-texture or repeating-texture areas. This on one\nhand, generates much smooth surface models, while on the other hand, may\npartially leads to smoothing on depth discontinuities, particularly for\nfence-shaped regions or densely built areas with narrow streets. To solve this\nproblem, in this work, we propose to use the line segment information extracted\nfrom the corresponding orthophoto as a pose-processing tool to sharpen the\nbuilding boundary of the Digital Surface Model (DSM) generated by SGM. Two\nmethods which are based on graph-cut and plane fitting are proposed and\ncompared. Experimental results on several satellite datasets with ground truth\nshow the robustness and effectiveness of the proposed DSM sharpening method.\n", "versions": [{"version": "v1", "created": "Wed, 22 May 2019 14:02:58 GMT"}], "update_date": "2019-05-23", "authors_parsed": [["Lu", "Xiaohu", ""], ["Qin", "Rongjun", ""], ["Huang", "Xu", ""]]}, {"id": "1905.09152", "submitter": "Rongjun Qin", "authors": "Xu Huang and Rongjun Qin", "title": "Multi-View Large-Scale Bundle Adjustment Method for High-Resolution\n  Satellite Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given enough multi-view image corresponding points (also called tie points)\nand ground control points (GCP), bundle adjustment for high-resolution\nsatellite images is used to refine the orientations or most often used\ngeometric parameters Rational Polynomial Coefficients (RPC) of each satellite\nimage in a unified geodetic framework, which is very critical in many\nphotogrammetry and computer vision applications. However, the growing number of\nhigh resolution spaceborne optical sensors has brought two challenges to the\nbundle adjustment: 1) images come from different satellite cameras may have\ndifferent imaging dates, viewing angles, resolutions, etc., thus resulting in\ngeometric and radiometric distortions in the bundle adjustment; 2) The\nlarge-scale mapping area always corresponds to vast number of bundle adjustment\ncorrections (including RPC bias and object space point coordinates). Due to the\nlimitation of computer memory, it is hard to refine all corrections at the same\ntime. Hence, how to efficiently realize the bundle adjustment in large-scale\nregions is very important. This paper particularly addresses the multi-view\nlarge-scale bundle adjustment problem by two steps: 1) to get robust tie points\namong different satellite images, we design a multi-view, multi-source tie\npoint matching algorithm based on plane rectification and epipolar constraints,\nwhich is able to compensate geometric and local nonlinear radiometric\ndistortions among satellite datasets, and 2) to solve dozens of thousands or\neven millions of variables bundle adjustment corrections in the large scale\nbundle adjustment, we use an efficient solution with only a little computer\nmemory. Experiments on in-track and off-track satellite datasets show that the\nproposed method is capable of computing sub-pixel accuracy bundle adjustment\nresults.\n", "versions": [{"version": "v1", "created": "Wed, 22 May 2019 14:06:09 GMT"}], "update_date": "2019-05-23", "authors_parsed": [["Huang", "Xu", ""], ["Qin", "Rongjun", ""]]}, {"id": "1905.09211", "submitter": "Berkan Demirel", "authors": "Berkan Demirel, Omer Ozdil, Yunus Emre Esin, Safak Ozturk", "title": "Segmentation-Aware Hyperspectral Image Classification", "comments": "To appear at International Geoscience and Remote Sensing Symposium\n  (IGARSS) 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose an unified hyperspectral image classification\nmethod which takes three-dimensional hyperspectral data cube as an input and\nproduces a classification map. In the proposed method, a deep neural network\nwhich uses spectral and spatial information together with residual connections,\nand pixel affinity network based segmentation-aware superpixels are used\ntogether. In the architecture, segmentation-aware superpixels run on the\ninitial classification map of deep residual network, and apply majority voting\non obtained results. Experimental results show that our propoped method yields\nstate-of-the-art results in two benchmark datasets. Moreover, we also show that\nthe segmentation-aware superpixels have great contribution to the success of\nhyperspectral image classification methods in cases where training data is\ninsufficient.\n", "versions": [{"version": "v1", "created": "Wed, 22 May 2019 16:03:01 GMT"}], "update_date": "2019-05-23", "authors_parsed": [["Demirel", "Berkan", ""], ["Ozdil", "Omer", ""], ["Esin", "Yunus Emre", ""], ["Ozturk", "Safak", ""]]}, {"id": "1905.09226", "submitter": "Boyuan Ma", "authors": "Boyuan Ma, Chuni Liu, Xiaojuan Ban, Hao Wang, Weihua Xue, Haiyou Huang", "title": "WPU-Net: Boundary Learning by Using Weighted Propagation in Convolution\n  Network", "comments": "technical report", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning has driven a great progress in natural and biological image\nprocessing. However, in material science and engineering, there are often some\nflaws and indistinctions in material microscopic images induced from complex\nsample preparation, even due to the material itself, hindering the detection of\ntarget objects. In this work, we propose WPU-net that redesigns the\narchitecture and weighted loss of U-Net, which forces the network to integrate\ninformation from adjacent slices and pays more attention to the topology in\nboundary detection task. Then, the WPU-net is applied into a typical material\nexample, i.e., the grain boundary detection of polycrystalline material.\nExperiments demonstrate that the proposed method achieves promising performance\nand outperforms state-of-the-art methods. Besides, we propose a new method for\nobject tracking between adjacent slices, which can effectively reconstruct 3D\nstructure of the whole material. Finally, we present a material microscopic\nimage dataset with the goal of advancing the state-of-the-art in image\nprocessing for material science.\n", "versions": [{"version": "v1", "created": "Wed, 22 May 2019 16:23:23 GMT"}, {"version": "v2", "created": "Fri, 30 Aug 2019 15:52:09 GMT"}], "update_date": "2019-09-02", "authors_parsed": [["Ma", "Boyuan", ""], ["Liu", "Chuni", ""], ["Ban", "Xiaojuan", ""], ["Wang", "Hao", ""], ["Xue", "Weihua", ""], ["Huang", "Haiyou", ""]]}, {"id": "1905.09231", "submitter": "Zahra Montazeri", "authors": "Zahra Montazeri, Gopi M", "title": "Separating Overlapping Tissue Layers from Microscopy Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Manual preparation of tissue slices for microscopy imaging can introduce\ntissue tears and overlaps. Typically, further digital processing algorithms\nsuch as registration and 3D reconstruction from tissue image stacks cannot\nhandle images with tissue tear/overlap artifacts, and so such images are\nusually discarded. In this paper, we propose an imaging model and an algorithm\nto digitally separate overlapping tissue data of mouse brain images into two\nlayers. We show the correctness of our model and the algorithm by comparing our\nresults with the ground truth.\n", "versions": [{"version": "v1", "created": "Wed, 22 May 2019 16:31:17 GMT"}], "update_date": "2019-05-23", "authors_parsed": [["Montazeri", "Zahra", ""], ["M", "Gopi", ""]]}, {"id": "1905.09240", "submitter": "Elmar Langholz", "authors": "Elmar Langholz", "title": "Oculum afficit: Ocular Affect Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recognizing human affect and emotions is a problem that has a wide range of\napplications within both academia and industry. Affect and emotion recognition\nwithin computer vision primarily relies on images of faces. With the prevalence\nof portable devices (e.g. smart phones and/or smart glasses),acquiring user\nfacial images requires focus, time, and precision. While existing systems work\ngreat for full frontal faces, they tend to not work so well with partial faces\nlike those of the operator of the device when under use. Due to this, we\npropose a methodology in which we can accurately infer the overall affect of a\nperson by looking at the ocular region of an individual.\n", "versions": [{"version": "v1", "created": "Wed, 22 May 2019 16:58:12 GMT"}], "update_date": "2019-05-23", "authors_parsed": [["Langholz", "Elmar", ""]]}, {"id": "1905.09247", "submitter": "Johan Phan", "authors": "Johan Phan, Massimiliano Ruocco, and Francesco Scibilia", "title": "Dual Active Sampling on Batch-Incremental Active Learning", "comments": "6 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, Convolutional Neural Networks (CNNs) have shown unprecedented\nsuccess in the field of computer vision, especially on challenging image\nclassification tasks by relying on a universal approach, i.e., training a deep\nmodel on a massive dataset of supervised examples. While unlabeled data are\noften an abundant resource, collecting a large set of labeled data, on the\nother hand, are very expensive, which often require considerable human efforts.\nOne way to ease out this is to effectively select and label highly informative\ninstances from a pool of unlabeled data (i.e., active learning). This paper\nproposed a new method of batch-mode active learning, Dual Active Sampling(DAS),\nwhich is based on a simple assumption, if two deep neural networks (DNNs) of\nthe same structure and trained on the same dataset give significantly different\noutput for a given sample, then that particular sample should be picked for\nadditional training. While other state of the art methods in this field usually\nrequire intensive computational power or relying on a complicated structure,\nDAS is simpler to implement and, managed to get improved results on Cifar-10\nwith preferable computational time compared to the core-set method.\n", "versions": [{"version": "v1", "created": "Wed, 22 May 2019 17:11:57 GMT"}], "update_date": "2019-05-23", "authors_parsed": [["Phan", "Johan", ""], ["Ruocco", "Massimiliano", ""], ["Scibilia", "Francesco", ""]]}, {"id": "1905.09265", "submitter": "Hsueh-Ying Lai", "authors": "Hsueh-Ying Lai, Yi-Hsuan Tsai, Wei-Chen Chiu", "title": "Bridging Stereo Matching and Optical Flow via Spatiotemporal\n  Correspondence", "comments": "Accepted in CVPR'19. Code and model are available at:\n  https://github.com/lelimite4444/BridgeDepthFlow", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stereo matching and flow estimation are two essential tasks for scene\nunderstanding, spatially in 3D and temporally in motion. Existing approaches\nhave been focused on the unsupervised setting due to the limited resource to\nobtain the large-scale ground truth data. To construct a self-learnable\nobjective, co-related tasks are often linked together to form a joint\nframework. However, the prior work usually utilizes independent networks for\neach task, thus not allowing to learn shared feature representations across\nmodels. In this paper, we propose a single and principled network to jointly\nlearn spatiotemporal correspondence for stereo matching and flow estimation,\nwith a newly designed geometric connection as the unsupervised signal for\ntemporally adjacent stereo pairs. We show that our method performs favorably\nagainst several state-of-the-art baselines for both unsupervised depth and flow\nestimation on the KITTI benchmark dataset.\n", "versions": [{"version": "v1", "created": "Wed, 22 May 2019 17:51:00 GMT"}], "update_date": "2019-05-23", "authors_parsed": [["Lai", "Hsueh-Ying", ""], ["Tsai", "Yi-Hsuan", ""], ["Chiu", "Wei-Chen", ""]]}, {"id": "1905.09272", "submitter": "Olivier H\\'enaff", "authors": "Olivier J. H\\'enaff, Aravind Srinivas, Jeffrey De Fauw, Ali Razavi,\n  Carl Doersch, S. M. Ali Eslami, Aaron van den Oord", "title": "Data-Efficient Image Recognition with Contrastive Predictive Coding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human observers can learn to recognize new categories of images from a\nhandful of examples, yet doing so with artificial ones remains an open\nchallenge. We hypothesize that data-efficient recognition is enabled by\nrepresentations which make the variability in natural signals more predictable.\nWe therefore revisit and improve Contrastive Predictive Coding, an unsupervised\nobjective for learning such representations. This new implementation produces\nfeatures which support state-of-the-art linear classification accuracy on the\nImageNet dataset. When used as input for non-linear classification with deep\nneural networks, this representation allows us to use 2-5x less labels than\nclassifiers trained directly on image pixels. Finally, this unsupervised\nrepresentation substantially improves transfer learning to object detection on\nthe PASCAL VOC dataset, surpassing fully supervised pre-trained ImageNet\nclassifiers.\n", "versions": [{"version": "v1", "created": "Wed, 22 May 2019 17:57:49 GMT"}, {"version": "v2", "created": "Fri, 6 Dec 2019 18:35:23 GMT"}, {"version": "v3", "created": "Wed, 1 Jul 2020 11:22:05 GMT"}], "update_date": "2020-07-02", "authors_parsed": [["H\u00e9naff", "Olivier J.", ""], ["Srinivas", "Aravind", ""], ["De Fauw", "Jeffrey", ""], ["Razavi", "Ali", ""], ["Doersch", "Carl", ""], ["Eslami", "S. M. Ali", ""], ["Oord", "Aaron van den", ""]]}, {"id": "1905.09282", "submitter": "Nils Gessert", "authors": "Nils Gessert, Torben Priegnitz, Thore Saathoff, Sven-Thomas Antoni,\n  David Meyer, Moritz Franz Hamann, Klaus-Peter J\\\"unemann, Christoph Otte,\n  Alexander Schlaefer", "title": "Spatio-Temporal Deep Learning Models for Tip Force Estimation During\n  Needle Insertion", "comments": "Accepted for publication in the International Journal of Computer\n  Assisted Radiology and Surgery", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Purpose. Precise placement of needles is a challenge in a number of clinical\napplications such as brachytherapy or biopsy. Forces acting at the needle cause\ntissue deformation and needle deflection which in turn may lead to misplacement\nor injury. Hence, a number of approaches to estimate the forces at the needle\nhave been proposed. Yet, integrating sensors into the needle tip is challenging\nand a careful calibration is required to obtain good force estimates.\n  Methods. We describe a fiber-optical needle tip force sensor design using a\nsingle OCT fiber for measurement. The fiber images the deformation of an epoxy\nlayer placed below the needle tip which results in a stream of 1D depth\nprofiles. We study different deep learning approaches to facilitate calibration\nbetween this spatio-temporal image data and the related forces. In particular,\nwe propose a novel convGRU-CNN architecture for simultaneous spatial and\ntemporal data processing.\n  Results. The needle can be adapted to different operating ranges by changing\nthe stiffness of the epoxy layer. Likewise, calibration can be adapted by\ntraining the deep learning models. Our novel convGRU-CNN architecture results\nin the lowest mean absolute error of 1.59 +- 1.3 mN and a cross-correlation\ncoefficient of 0.9997, and clearly outperforms the other methods. Ex vivo\nexperiments in human prostate tissue demonstrate the needle's application.\n  Conclusions. Our OCT-based fiber-optical sensor presents a viable alternative\nfor needle tip force estimation. The results indicate that the rich\nspatio-temporal information included in the stream of images showing the\ndeformation throughout the epoxy layer can be effectively used by deep learning\nmodels. Particularly, we demonstrate that the convGRU-CNN architecture performs\nfavorably, making it a promising approach for other spatio-temporal learning\nproblems.\n", "versions": [{"version": "v1", "created": "Wed, 22 May 2019 15:45:50 GMT"}], "update_date": "2019-05-24", "authors_parsed": [["Gessert", "Nils", ""], ["Priegnitz", "Torben", ""], ["Saathoff", "Thore", ""], ["Antoni", "Sven-Thomas", ""], ["Meyer", "David", ""], ["Hamann", "Moritz Franz", ""], ["J\u00fcnemann", "Klaus-Peter", ""], ["Otte", "Christoph", ""], ["Schlaefer", "Alexander", ""]]}, {"id": "1905.09304", "submitter": "Xinke Deng", "authors": "Xinke Deng, Arsalan Mousavian, Yu Xiang, Fei Xia, Timothy Bretl,\n  Dieter Fox", "title": "PoseRBPF: A Rao-Blackwellized Particle Filter for 6D Object Pose\n  Tracking", "comments": "Accepted to RSS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tracking 6D poses of objects from videos provides rich information to a robot\nin performing different tasks such as manipulation and navigation. In this\nwork, we formulate the 6D object pose tracking problem in the Rao-Blackwellized\nparticle filtering framework, where the 3D rotation and the 3D translation of\nan object are decoupled. This factorization allows our approach, called\nPoseRBPF, to efficiently estimate the 3D translation of an object along with\nthe full distribution over the 3D rotation. This is achieved by discretizing\nthe rotation space in a fine-grained manner, and training an auto-encoder\nnetwork to construct a codebook of feature embeddings for the discretized\nrotations. As a result, PoseRBPF can track objects with arbitrary symmetries\nwhile still maintaining adequate posterior distributions. Our approach achieves\nstate-of-the-art results on two 6D pose estimation benchmarks. A video showing\nthe experiments can be found at https://youtu.be/lE5gjzRKWuA\n", "versions": [{"version": "v1", "created": "Wed, 22 May 2019 18:01:55 GMT"}], "update_date": "2019-05-24", "authors_parsed": [["Deng", "Xinke", ""], ["Mousavian", "Arsalan", ""], ["Xiang", "Yu", ""], ["Xia", "Fei", ""], ["Bretl", "Timothy", ""], ["Fox", "Dieter", ""]]}, {"id": "1905.09324", "submitter": "Sanketh Vedula", "authors": "Tomer Weiss, Sanketh Vedula, Ortal Senouf, Oleg Michailovich, Michael\n  Zibulevsky, Alex Bronstein", "title": "Joint learning of cartesian undersampling and reconstruction for\n  accelerated MRI", "comments": "ICASSP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV eess.SP physics.med-ph", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Magnetic Resonance Imaging (MRI) is considered today the golden-standard\nmodality for soft tissues. The long acquisition times, however, make it more\nprone to motion artifacts as well as contribute to the relatively high costs of\nthis examination. Over the years, multiple studies concentrated on designing\nreduced measurement schemes and image reconstruction schemes for MRI, however,\nthese problems have been so far addressed separately. On the other hand, recent\nworks in optical computational imaging have demonstrated growing success of the\nsimultaneous learning-based design of the acquisition and reconstruction\nschemes manifesting significant improvement in the reconstruction quality with\na constrained time budget. Inspired by these successes, in this work, we\npropose to learn accelerated MR acquisition schemes (in the form of Cartesian\ntrajectories) jointly with the image reconstruction operator. To this end, we\npropose an algorithm for training the combined acquisition-reconstruction\npipeline end-to-end in a differentiable way. We demonstrate the significance of\nusing the learned Cartesian trajectories at different speed up rates.\n", "versions": [{"version": "v1", "created": "Wed, 22 May 2019 18:55:18 GMT"}, {"version": "v2", "created": "Sun, 5 Apr 2020 06:56:04 GMT"}], "update_date": "2020-04-07", "authors_parsed": [["Weiss", "Tomer", ""], ["Vedula", "Sanketh", ""], ["Senouf", "Ortal", ""], ["Michailovich", "Oleg", ""], ["Zibulevsky", "Michael", ""], ["Bronstein", "Alex", ""]]}, {"id": "1905.09325", "submitter": "Sanketh Vedula", "authors": "Ortal Senouf, Sanketh Vedula, Tomer Weiss, Alex Bronstein, Oleg\n  Michailovich, Michael Zibulevsky", "title": "Self-supervised learning of inverse problem solvers in medical imaging", "comments": "preprint", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV eess.SP", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In the past few years, deep learning-based methods have demonstrated enormous\nsuccess for solving inverse problems in medical imaging. In this work, we\naddress the following question:\\textit{Given a set of measurements obtained\nfrom real imaging experiments, what is the best way to use a learnable model\nand the physics of the modality to solve the inverse problem and reconstruct\nthe latent image?} Standard supervised learning based methods approach this\nproblem by collecting data sets of known latent images and their corresponding\nmeasurements. However, these methods are often impractical due to the lack of\navailability of appropriately sized training sets, and, more generally, due to\nthe inherent difficulty in measuring the \"groundtruth\" latent image. In light\nof this, we propose a self-supervised approach to training inverse models in\nmedical imaging in the absence of aligned data. Our method only requiring\naccess to the measurements and the forward model at training. We showcase its\neffectiveness on inverse problems arising in accelerated magnetic resonance\nimaging (MRI).\n", "versions": [{"version": "v1", "created": "Wed, 22 May 2019 18:56:26 GMT"}], "update_date": "2019-05-24", "authors_parsed": [["Senouf", "Ortal", ""], ["Vedula", "Sanketh", ""], ["Weiss", "Tomer", ""], ["Bronstein", "Alex", ""], ["Michailovich", "Oleg", ""], ["Zibulevsky", "Michael", ""]]}, {"id": "1905.09339", "submitter": "Maryana Alegro", "authors": "Maryana Alegro, Eduardo J. L. Alho, Maria da Graca Morais Martin, Lea\n  Teneholz Grinberg, Helmut Heinsen, Roseli de Deus Lopes, Edson Amaro-Jr,\n  Lilla Z\\\"ollei", "title": "Automating Whole Brain Histology to MRI Registration: Implementation of\n  a Computational Pipeline", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV q-bio.TO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although the latest advances in MRI technology have allowed the acquisition\nof higher resolution images, reliable delineation of cytoarchitectural or\nsubcortical nuclei boundaries is not possible. As a result, histological images\nare still required to identify the exact limits of neuroanatomical structures.\nHowever, histological processing is associated with tissue distortion and\nfixation artifacts, which prevent a direct comparison between the two\nmodalities. Our group has previously proposed a histological procedure based on\ncelloidin embedding that reduces the amount of artifacts and yields high\nquality whole brain histological slices. Celloidin embedded tissue,\nnevertheless, still bears distortions that must be corrected. We propose a\ncomputational pipeline designed to semi-automatically process the celloidin\nembedded histology and register them to their MRI counterparts. In this paper\nwe report the accuracy of our pipeline in two whole brain volumes from the\nBrain Bank of the Brazilian Aging Brain Study Group (BBBABSG). Results were\nassessed by comparison of manual segmentations from two experts in both MRIs\nand the registered histological volumes. The two whole brain histology/MRI\ndatasets were successfully registered using minimal user interaction. We also\npoint to possible improvements based on recent implementations that could be\nadded to this pipeline, potentially allowing for higher precision and further\nperformance gains.\n", "versions": [{"version": "v1", "created": "Wed, 22 May 2019 19:27:49 GMT"}], "update_date": "2019-05-24", "authors_parsed": [["Alegro", "Maryana", ""], ["Alho", "Eduardo J. L.", ""], ["Martin", "Maria da Graca Morais", ""], ["Grinberg", "Lea Teneholz", ""], ["Heinsen", "Helmut", ""], ["Lopes", "Roseli de Deus", ""], ["Amaro-Jr", "Edson", ""], ["Z\u00f6llei", "Lilla", ""]]}, {"id": "1905.09400", "submitter": "Siddhesh Khandelwal", "authors": "Siddhesh Khandelwal and Leonid Sigal", "title": "AttentionRNN: A Structured Spatial Attention Mechanism", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual attention mechanisms have proven to be integrally important\nconstituent components of many modern deep neural architectures. They provide\nan efficient and effective way to utilize visual information selectively, which\nhas shown to be especially valuable in multi-modal learning tasks. However, all\nprior attention frameworks lack the ability to explicitly model structural\ndependencies among attention variables, making it difficult to predict\nconsistent attention masks. In this paper we develop a novel structured spatial\nattention mechanism which is end-to-end trainable and can be integrated with\nany feed-forward convolutional neural network. This proposed AttentionRNN layer\nexplicitly enforces structure over the spatial attention variables by\nsequentially predicting attention values in the spatial mask in a\nbi-directional raster-scan and inverse raster-scan order. As a result, each\nattention value depends not only on local image or contextual information, but\nalso on the previously predicted attention values. Our experiments show\nconsistent quantitative and qualitative improvements on a variety of\nrecognition tasks and datasets; including image categorization, question\nanswering and image generation.\n", "versions": [{"version": "v1", "created": "Wed, 22 May 2019 23:13:05 GMT"}], "update_date": "2019-05-24", "authors_parsed": [["Khandelwal", "Siddhesh", ""], ["Sigal", "Leonid", ""]]}, {"id": "1905.09441", "submitter": "Qingwen Xu", "authors": "Haofei Kuang, Qingwen Xu, S\\\"oren Schwertfeger", "title": "Depth Estimation on Underwater Omni-directional Images Using a Deep\n  Neural Network", "comments": "7 pages, 8 figures, 1 table, accepted by 2019 ICRA workshop\n  \"Underwater Robotics Perception\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we exploit a depth estimation Fully Convolutional Residual\nNeural Network (FCRN) for in-air perspective images to estimate the depth of\nunderwater perspective and omni-directional images. We train one conventional\nand one spherical FCRN for underwater perspective and omni-directional images,\nrespectively. The spherical FCRN is derived from the perspective FCRN via a\nspherical longitude-latitude mapping. For that, the omni-directional camera is\nmodeled as a sphere, while images captured by it are displayed in the\nlongitude-latitude form. Due to the lack of underwater datasets, we synthesize\nimages in both data-driven and theoretical ways, which are used in training and\ntesting. Finally, experiments are conducted on these synthetic images and\nresults are displayed in both qualitative and quantitative way. The comparison\nbetween ground truth and the estimated depth map indicates the effectiveness of\nour method.\n", "versions": [{"version": "v1", "created": "Thu, 23 May 2019 02:58:32 GMT"}], "update_date": "2019-05-24", "authors_parsed": [["Kuang", "Haofei", ""], ["Xu", "Qingwen", ""], ["Schwertfeger", "S\u00f6ren", ""]]}, {"id": "1905.09447", "submitter": "Mengmi Zhang", "authors": "Mengmi Zhang, Tao Wang, Joo Hwee Lim, Gabriel Kreiman, Jiashi Feng", "title": "Variational Prototype Replays for Continual Learning", "comments": "under submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Continual learning refers to the ability to acquire and transfer knowledge\nwithout catastrophically forgetting what was previously learned. In this work,\nwe consider \\emph{few-shot} continual learning in classification tasks, and we\npropose a novel method, Variational Prototype Replays, that efficiently\nconsolidates and recalls previous knowledge to avoid catastrophic forgetting.\nIn each classification task, our method learns a set of variational prototypes\nwith their means and variances, where embedding of the samples from the same\nclass can be represented in a prototypical distribution and\nclass-representative prototypes are separated apart. To alleviate catastrophic\nforgetting, our method replays one sample per class from previous tasks, and\ncorrespondingly matches newly predicted embeddings to their nearest\nclass-representative prototypes stored from previous tasks. Compared with\nrecent continual learning approaches, our method can readily adapt to new tasks\nwith more classes without requiring the addition of new units. Furthermore, our\nmethod is more memory efficient since only class-representative prototypes with\ntheir means and variances, as well as only one sample per class from previous\ntasks need to be stored. Without tampering with the performance on initial\ntasks, our method learns novel concepts given a few training examples of each\nclass in new tasks.\n", "versions": [{"version": "v1", "created": "Thu, 23 May 2019 03:25:33 GMT"}, {"version": "v2", "created": "Tue, 24 Sep 2019 19:20:05 GMT"}, {"version": "v3", "created": "Sat, 15 Feb 2020 05:54:07 GMT"}], "update_date": "2020-02-18", "authors_parsed": [["Zhang", "Mengmi", ""], ["Wang", "Tao", ""], ["Lim", "Joo Hwee", ""], ["Kreiman", "Gabriel", ""], ["Feng", "Jiashi", ""]]}, {"id": "1905.09481", "submitter": "Kien Nguyen Thanh", "authors": "Kien Nguyen and Clinton Fookes and Sridha Sridharan", "title": "Constrained Design of Deep Iris Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Despite the promise of recent deep neural networks in the iris recognition\nsetting, there are vital properties of the classic IrisCode which are almost\nunable to be achieved with current deep iris networks: the compactness of model\nand the small number of computing operations (FLOPs). This paper re-models the\niris network design process as a constrained optimization problem which takes\nmodel size and computation into account as learning criteria. On one hand, this\nallows us to fully automate the network design process to search for the best\niris network confined to the computation and model compactness constraints. On\nthe other hand, it allows us to investigate the optimality of the classic\nIrisCode and recent iris networks. It also allows us to learn an optimal iris\nnetwork and demonstrate state-of-the-art performance with less computation and\nmemory requirements.\n", "versions": [{"version": "v1", "created": "Thu, 23 May 2019 05:24:53 GMT"}], "update_date": "2019-05-24", "authors_parsed": [["Nguyen", "Kien", ""], ["Fookes", "Clinton", ""], ["Sridharan", "Sridha", ""]]}, {"id": "1905.09500", "submitter": "Jihye Hwang", "authors": "Jihye Hwang, Jieun Lee, Sungheon Park and Nojun Kwak", "title": "Pose estimator and tracker using temporal flow maps for limbs", "comments": "Won the Honorable Mention Award in the 18'ECCV PoseTrack challenge.\n  Accepted in the 19'IJCNN conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For human pose estimation in videos, it is significant how to use temporal\ninformation between frames. In this paper, we propose temporal flow maps for\nlimbs (TML) and a multi-stride method to estimate and track human poses. The\nproposed temporal flow maps are unit vectors describing the limbs' movements.\nWe constructed a network to learn both spatial information and temporal\ninformation end-to-end. Spatial information such as joint heatmaps and part\naffinity fields is regressed in the spatial network part, and the TML is\nregressed in the temporal network part. We also propose a data augmentation\nmethod to learn various types of TML better. The proposed multi-stride method\nexpands the data by randomly selecting two frames within a defined range. We\ndemonstrate that the proposed method efficiently estimates and tracks human\nposes on the PoseTrack 2017 and 2018 datasets.\n", "versions": [{"version": "v1", "created": "Thu, 23 May 2019 07:08:44 GMT"}], "update_date": "2019-05-24", "authors_parsed": [["Hwang", "Jihye", ""], ["Lee", "Jieun", ""], ["Park", "Sungheon", ""], ["Kwak", "Nojun", ""]]}, {"id": "1905.09523", "submitter": "Niels Hellinga", "authors": "Niels Hellinga, Vlado Menkovski", "title": "Hierarchical Annotation of Images with Two-Alternative-Forced-Choice\n  Metric Learning", "comments": "presented at 2019 ICML Workshop on Human in the Loop Learning (HILL\n  2019), Long Beach, USA", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many tasks such as retrieval and recommendations can significantly benefit\nfrom structuring the data, commonly in a hierarchical way. To achieve this\nthrough annotations of high dimensional data such as images or natural text can\nbe significantly labor intensive. We propose an approach for uncovering the\nhierarchical structure of data based on efficient discriminative testing rather\nthan annotations of individual datapoints. Using two-alternative-forced-choice\n(2AFC) testing and deep metric learning we achieve embedding of the data in\nsemantic space where we are able to successfully hierarchically cluster. We\nactively select triplets for the 2AFC test such that the modeling process is\nhighly efficient with respect to the number of tests presented to the\nannotator. We empirically demonstrate the feasibility of the method by\nconfirming the shape bias on synthetic data and extract hierarchical structure\non the Fashion-MNIST dataset to a finer granularity than the original labels.\n", "versions": [{"version": "v1", "created": "Thu, 23 May 2019 08:11:15 GMT"}, {"version": "v2", "created": "Wed, 5 Jun 2019 13:55:18 GMT"}], "update_date": "2019-06-06", "authors_parsed": [["Hellinga", "Niels", ""], ["Menkovski", "Vlado", ""]]}, {"id": "1905.09533", "submitter": "Jilin Mei", "authors": "Jilin Mei and Huijing Zhao", "title": "Incorporating Human Domain Knowledge in 3D LiDAR-based Semantic\n  Segmentation", "comments": "8 Pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work studies semantic segmentation using 3D LiDAR data. Popular deep\nlearning methods applied for this task require a large number of manual\nannotations to train the parameters. We propose a new method that makes full\nuse of the advantages of traditional methods and deep learning methods via\nincorporating human domain knowledge into the neural network model to reduce\nthe demand for large numbers of manual annotations and improve the training\nefficiency. We first pretrain a model with autogenerated samples from a\nrule-based classifier so that human knowledge can be propagated into the\nnetwork. Based on the pretrained model, only a small set of annotations is\nrequired for further fine-tuning. Quantitative experiments show that the\npretrained model achieves better performance than random initialization in\nalmost all cases; furthermore, our method can achieve similar performance with\nfewer manual annotations.\n", "versions": [{"version": "v1", "created": "Thu, 23 May 2019 08:41:35 GMT"}], "update_date": "2019-05-24", "authors_parsed": [["Mei", "Jilin", ""], ["Zhao", "Huijing", ""]]}, {"id": "1905.09588", "submitter": "Kazuto Ichimaru", "authors": "Kazuto Ichimaru and Hiroshi Kawasaki", "title": "Underwater Stereo using Refraction-free Image Synthesized from Light\n  Field Camera", "comments": "Accepted in 2019 IEEE International Conference on Image Processing\n  (ICIP)", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is a strong demand on capturing underwater scenes without distortions\ncaused by refraction. Since a light field camera can capture several light rays\nat each point of an image plane from various directions, if geometrically\ncorrect rays are chosen, it is possible to synthesize a refraction-free image.\nIn this paper, we propose a novel technique to efficiently select such rays to\nsynthesize a refraction-free image from an underwater image captured by a light\nfield camera. In addition, we propose a stereo technique to reconstruct 3D\nshapes using a pair of our refraction-free images, which are central\nprojection. In the experiment, we captured several underwater scenes by two\nlight field cameras, synthesized refraction free images and applied stereo\ntechnique to reconstruct 3D shapes. The results are compared with previous\ntechniques which are based on approximation, showing the strength of our\nmethod.\n", "versions": [{"version": "v1", "created": "Thu, 23 May 2019 11:12:12 GMT"}], "update_date": "2019-05-24", "authors_parsed": [["Ichimaru", "Kazuto", ""], ["Kawasaki", "Hiroshi", ""]]}, {"id": "1905.09591", "submitter": "Huaxia Wang", "authors": "Huaxia Wang and Chun-Nam Yu", "title": "A Direct Approach to Robust Deep Learning Using Adversarial Networks", "comments": "15 pages", "journal-ref": "ICLR 2019", "doi": null, "report-no": null, "categories": "cs.CV cs.CR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks have been shown to perform well in many classical\nmachine learning problems, especially in image classification tasks. However,\nresearchers have found that neural networks can be easily fooled, and they are\nsurprisingly sensitive to small perturbations imperceptible to humans.\nCarefully crafted input images (adversarial examples) can force a well-trained\nneural network to provide arbitrary outputs. Including adversarial examples\nduring training is a popular defense mechanism against adversarial attacks. In\nthis paper we propose a new defensive mechanism under the generative\nadversarial network (GAN) framework. We model the adversarial noise using a\ngenerative network, trained jointly with a classification discriminative\nnetwork as a minimax game. We show empirically that our adversarial network\napproach works well against black box attacks, with performance on par with\nstate-of-art methods such as ensemble adversarial training and adversarial\ntraining with projected gradient descent.\n", "versions": [{"version": "v1", "created": "Thu, 23 May 2019 11:32:28 GMT"}], "update_date": "2019-05-24", "authors_parsed": [["Wang", "Huaxia", ""], ["Yu", "Chun-Nam", ""]]}, {"id": "1905.09598", "submitter": "Ravi Vadlamani", "authors": "Rohit Gavval, Vadlamani Ravi, Kalavala Revanth Harshal, Akhilesh\n  Gangwar and Kumar Ravi", "title": "CUDA-Self-Organizing feature map based visual sentiment analysis of bank\n  customer complaints for Analytical CRM", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  With the widespread use of social media, companies now have access to a\nwealth of customer feedback data which has valuable applications to Customer\nRelationship Management (CRM). Analyzing customer grievances data, is paramount\nas their speedy non-redressal would lead to customer churn resulting in lower\nprofitability. In this paper, we propose a descriptive analytics framework\nusing Self-organizing feature map (SOM), for Visual Sentiment Analysis of\ncustomer complaints. The network learns the inherent grouping of the complaints\nautomatically which can then be visualized too using various techniques.\nAnalytical Customer Relationship Management (ACRM) executives can draw useful\nbusiness insights from the maps and take timely remedial action. We also\npropose a high-performance version of the algorithm CUDASOM (CUDA based Self\nOrganizing feature Map) implemented using NVIDIA parallel computing platform,\nCUDA, which speeds up the processing of high-dimensional text data and\ngenerates fast results. The efficacy of the proposed model has been\ndemonstrated on the customer complaints data regarding the products and\nservices of four leading Indian banks. CUDASOM achieved an average speed up of\n44 times. Our approach can expand research into intelligent grievance redressal\nsystem to provide rapid solutions to the complaining customers.\n", "versions": [{"version": "v1", "created": "Thu, 23 May 2019 11:49:48 GMT"}], "update_date": "2019-05-24", "authors_parsed": [["Gavval", "Rohit", ""], ["Ravi", "Vadlamani", ""], ["Harshal", "Kalavala Revanth", ""], ["Gangwar", "Akhilesh", ""], ["Ravi", "Kumar", ""]]}, {"id": "1905.09634", "submitter": "Ziquan Lan", "authors": "Ziquan Lan, Zi Jian Yew, Gim Hee Lee", "title": "Robust Point Cloud Based Reconstruction of Large-Scale Outdoor Scenes", "comments": "CVPR 2019, 8 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Outlier feature matches and loop-closures that survived front-end data\nassociation can lead to catastrophic failures in the back-end optimization of\nlarge-scale point cloud based 3D reconstruction. To alleviate this problem, we\npropose a probabilistic approach for robust back-end optimization in the\npresence of outliers. More specifically, we model the problem as a Bayesian\nnetwork and solve it using the Expectation-Maximization algorithm. Our approach\nleverages on a long-tail Cauchy distribution to suppress outlier feature\nmatches in the odometry constraints, and a Cauchy-Uniform mixture model with a\nset of binary latent variables to simultaneously suppress outlier loop-closure\nconstraints and outlier feature matches in the inlier loop-closure constraints.\nFurthermore, we show that by using a Gaussian-Uniform mixture model, our\napproach degenerates to the formulation of a state-of-the-art approach for\nrobust indoor reconstruction. Experimental results demonstrate that our\napproach has comparable performance with the state-of-the-art on a benchmark\nindoor dataset, and outperforms it on a large-scale outdoor dataset. Our source\ncode can be found on the project website.\n", "versions": [{"version": "v1", "created": "Thu, 23 May 2019 13:10:21 GMT"}], "update_date": "2019-05-24", "authors_parsed": [["Lan", "Ziquan", ""], ["Yew", "Zi Jian", ""], ["Lee", "Gim Hee", ""]]}, {"id": "1905.09645", "submitter": "Nantheera Anantrasirichai", "authors": "Nantheera Anantrasirichai, Rencheng Zheng, Ivan Selesnick, Alin Achim", "title": "Image Fusion via Sparse Regularization with Non-Convex Penalties", "comments": null, "journal-ref": null, "doi": "10.1016/j.patrec.2020.01.020", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The L1 norm regularized least squares method is often used for finding sparse\napproximate solutions and is widely used in 1-D signal restoration. Basis\npursuit denoising (BPD) performs noise reduction in this way. However, the\nshortcoming of using L1 norm regularization is the underestimation of the true\nsolution. Recently, a class of non-convex penalties have been proposed to\nimprove this situation. This kind of penalty function is non-convex itself, but\npreserves the convexity property of the whole cost function. This approach has\nbeen confirmed to offer good performance in 1-D signal denoising. This paper\ndemonstrates the aforementioned method to 2-D signals (images) and applies it\nto multisensor image fusion. The problem is posed as an inverse one and a\ncorresponding cost function is judiciously designed to include two data\nattachment terms. The whole cost function is proved to be convex upon suitably\nchoosing the non-convex penalty, so that the cost function minimization can be\ntackled by convex optimization approaches, which comprise simple computations.\nThe performance of the proposed method is benchmarked against a number of\nstate-of-the-art image fusion techniques and superior performance is\ndemonstrated both visually and in terms of various assessment measures.\n", "versions": [{"version": "v1", "created": "Thu, 23 May 2019 13:33:26 GMT"}, {"version": "v2", "created": "Tue, 17 Dec 2019 16:43:43 GMT"}, {"version": "v3", "created": "Wed, 29 Jan 2020 18:58:12 GMT"}], "update_date": "2020-01-30", "authors_parsed": [["Anantrasirichai", "Nantheera", ""], ["Zheng", "Rencheng", ""], ["Selesnick", "Ivan", ""], ["Achim", "Alin", ""]]}, {"id": "1905.09646", "submitter": "Xiang Li", "authors": "Xiang Li, Xiaolin Hu and Jian Yang", "title": "Spatial Group-wise Enhance: Improving Semantic Feature Learning in\n  Convolutional Networks", "comments": "Code available at: https://github.com/implus/PytorchInsight", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Convolutional Neural Networks (CNNs) generate the feature representation\nof complex objects by collecting hierarchical and different parts of semantic\nsub-features. These sub-features can usually be distributed in grouped form in\nthe feature vector of each layer, representing various semantic entities.\nHowever, the activation of these sub-features is often spatially affected by\nsimilar patterns and noisy backgrounds, resulting in erroneous localization and\nidentification. We propose a Spatial Group-wise Enhance (SGE) module that can\nadjust the importance of each sub-feature by generating an attention factor for\neach spatial location in each semantic group, so that every individual group\ncan autonomously enhance its learnt expression and suppress possible noise. The\nattention factors are only guided by the similarities between the global and\nlocal feature descriptors inside each group, thus the design of SGE module is\nextremely lightweight with \\emph{almost no extra parameters and calculations}.\nDespite being trained with only category supervisions, the SGE component is\nextremely effective in highlighting multiple active areas with various\nhigh-order semantics (such as the dog's eyes, nose, etc.). When integrated with\npopular CNN backbones, SGE can significantly boost the performance of image\nrecognition tasks. Specifically, based on ResNet50 backbones, SGE achieves\n1.2\\% Top-1 accuracy improvement on the ImageNet benchmark and 1.0$\\sim$2.0\\%\nAP gain on the COCO benchmark across a wide range of detectors\n(Faster/Mask/Cascade RCNN and RetinaNet). Codes and pretrained models are\navailable at https://github.com/implus/PytorchInsight.\n", "versions": [{"version": "v1", "created": "Thu, 23 May 2019 13:36:54 GMT"}, {"version": "v2", "created": "Sat, 25 May 2019 08:07:40 GMT"}], "update_date": "2019-05-28", "authors_parsed": [["Li", "Xiang", ""], ["Hu", "Xiaolin", ""], ["Yang", "Jian", ""]]}, {"id": "1905.09661", "submitter": "Javad Amirian", "authors": "Javad Amirian, Wouter van Toll, Jean-Bernard Hayet, Julien Pettr\\'e", "title": "Data-Driven Crowd Simulation with Generative Adversarial Networks", "comments": "Accepted in CASA '19 (Computer Animation and Social Agents)", "journal-ref": null, "doi": "10.1145/3328756.3328769", "report-no": null, "categories": "cs.GR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a novel data-driven crowd simulation method that can\nmimic the observed traffic of pedestrians in a given environment. Given a set\nof observed trajectories, we use a recent form of neural networks, Generative\nAdversarial Networks (GANs), to learn the properties of this set and generate\nnew trajectories with similar properties. We define a way for simulated\npedestrians (agents) to follow such a trajectory while handling local collision\navoidance. As such, the system can generate a crowd that behaves similarly to\nobservations, while still enabling real-time interactions between agents. Via\nexperiments with real-world data, we show that our simulated trajectories\npreserve the statistical properties of their input. Our method simulates crowds\nin real time that resemble existing crowds, while also allowing insertion of\nextra agents, combination with other simulation methods, and user interaction.\n", "versions": [{"version": "v1", "created": "Thu, 23 May 2019 13:53:31 GMT"}], "update_date": "2019-05-24", "authors_parsed": [["Amirian", "Javad", ""], ["van Toll", "Wouter", ""], ["Hayet", "Jean-Bernard", ""], ["Pettr\u00e9", "Julien", ""]]}, {"id": "1905.09706", "submitter": "Ioannis Ivrissimtzis", "authors": "Xin Zhang, Ning Jia and Ioannis Ivrissimtzis", "title": "Watermark retrieval from 3D printed objects via synthetic data training", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a deep neural network based method for the retrieval of watermarks\nfrom images of 3D printed objects. To deal with the variability of all possible\n3D printing and image acquisition settings we train the network with synthetic\ndata. The main simulator parameters such as texture, illumination and camera\nposition are dynamically randomized in non-realistic ways, forcing the neural\nnetwork to learn the intrinsic features of the 3D printed watermarks. At the\nend of the pipeline, the watermark, in the form of a two-dimensional bit array,\nis retrieved through a series of simple image processing and statistical\noperations applied on the confidence map generated by the neural network. The\nresults demonstrate that the inclusion of synthetic DR data in the training set\nincreases the generalization power of the network, which performs better on\nimages from previously unseen 3D printed objects. We conclude that in our\napplication domain of information retrieval from 3D printed objects, where\naccess to the exact CAD files of the printed objects can be assumed, one can\nuse inexpensive synthetic data to enhance neural network training, reducing the\nneed for the labour intensive process of creating large amounts of hand\nlabelled real data or the need to generate photorealistic synthetic data.\n", "versions": [{"version": "v1", "created": "Thu, 23 May 2019 15:12:01 GMT"}], "update_date": "2019-05-24", "authors_parsed": [["Zhang", "Xin", ""], ["Jia", "Ning", ""], ["Ivrissimtzis", "Ioannis", ""]]}, {"id": "1905.09716", "submitter": "Xiao Liang", "authors": "Seyed Omid Sajedi and Xiao Liang", "title": "A Convolutional Cost-Sensitive Crack Localization Algorithm for\n  Automated and Reliable RC Bridge Inspection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bridges are an essential part of the transportation infrastructure and need\nto be monitored periodically. Visual inspections by dedicated teams have been\none of the primary tools in structural health monitoring (SHM) of bridge\nstructures. However, such conventional methods have certain shortcomings.\nManual inspections may be challenging in harsh environments and are commonly\nbiased in nature. In the last decade, camera-equipped unmanned aerial vehicles\n(UAVs) have been widely used for visual inspections; however, the task of\nautomatically extracting useful information from raw images is still\nchallenging. In this paper, a deep learning semantic segmentation framework is\nproposed to automatically localize surface cracks. Due to the high imbalance of\ncrack and background classes in images, different strategies are investigated\nto improve performance and reliability. The trained models are tested on\nreal-world crack images showing impressive robustness in terms of the metrics\ndefined by the concepts of precision and recall. These techniques can be used\nin SHM of bridges to extract useful information from the unprocessed images\ntaken from UAVs.\n", "versions": [{"version": "v1", "created": "Thu, 23 May 2019 15:22:16 GMT"}], "update_date": "2019-05-24", "authors_parsed": [["Sajedi", "Seyed Omid", ""], ["Liang", "Xiao", ""]]}, {"id": "1905.09717", "submitter": "Xuanyi Dong", "authors": "Xuanyi Dong, Yi Yang", "title": "Network Pruning via Transformable Architecture Search", "comments": "Published in the 33rd Conference on Neural Information Processing\n  Systems (NeurIPS 2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Network pruning reduces the computation costs of an over-parameterized\nnetwork without performance damage. Prevailing pruning algorithms pre-define\nthe width and depth of the pruned networks, and then transfer parameters from\nthe unpruned network to pruned networks. To break the structure limitation of\nthe pruned networks, we propose to apply neural architecture search to search\ndirectly for a network with flexible channel and layer sizes. The number of the\nchannels/layers is learned by minimizing the loss of the pruned networks. The\nfeature map of the pruned network is an aggregation of K feature map fragments\n(generated by K networks of different sizes), which are sampled based on the\nprobability distribution.The loss can be back-propagated not only to the\nnetwork weights, but also to the parameterized distribution to explicitly tune\nthe size of the channels/layers. Specifically, we apply channel-wise\ninterpolation to keep the feature map with different channel sizes aligned in\nthe aggregation procedure. The maximum probability for the size in each\ndistribution serves as the width and depth of the pruned network, whose\nparameters are learned by knowledge transfer, e.g., knowledge distillation,\nfrom the original networks. Experiments on CIFAR-10, CIFAR-100 and ImageNet\ndemonstrate the effectiveness of our new perspective of network pruning\ncompared to traditional network pruning algorithms. Various searching and\nknowledge transfer approaches are conducted to show the effectiveness of the\ntwo components. Code is at: https://github.com/D-X-Y/NAS-Projects.\n", "versions": [{"version": "v1", "created": "Thu, 23 May 2019 15:22:25 GMT"}, {"version": "v2", "created": "Wed, 19 Jun 2019 18:14:31 GMT"}, {"version": "v3", "created": "Mon, 14 Oct 2019 09:49:00 GMT"}, {"version": "v4", "created": "Tue, 15 Oct 2019 11:50:20 GMT"}, {"version": "v5", "created": "Wed, 16 Oct 2019 05:11:01 GMT"}], "update_date": "2019-10-17", "authors_parsed": [["Dong", "Xuanyi", ""], ["Yang", "Yi", ""]]}, {"id": "1905.09747", "submitter": "Micah Goldblum", "authors": "Micah Goldblum, Liam Fowl, Soheil Feizi, Tom Goldstein", "title": "Adversarially Robust Distillation", "comments": "Accepted to AAAI Conference on Artificial Intelligence, 2020", "journal-ref": null, "doi": "10.1609/aaai.v34i04.5816", "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Knowledge distillation is effective for producing small, high-performance\nneural networks for classification, but these small networks are vulnerable to\nadversarial attacks. This paper studies how adversarial robustness transfers\nfrom teacher to student during knowledge distillation. We find that a large\namount of robustness may be inherited by the student even when distilled on\nonly clean images. Second, we introduce Adversarially Robust Distillation (ARD)\nfor distilling robustness onto student networks. In addition to producing small\nmodels with high test accuracy like conventional distillation, ARD also passes\nthe superior robustness of large networks onto the student. In our experiments,\nwe find that ARD student models decisively outperform adversarially trained\nnetworks of identical architecture in terms of robust accuracy, surpassing\nstate-of-the-art methods on standard robustness benchmarks. Finally, we adapt\nrecent fast adversarial training methods to ARD for accelerated robust\ndistillation.\n", "versions": [{"version": "v1", "created": "Thu, 23 May 2019 16:09:20 GMT"}, {"version": "v2", "created": "Mon, 2 Dec 2019 22:37:09 GMT"}], "update_date": "2020-07-02", "authors_parsed": [["Goldblum", "Micah", ""], ["Fowl", "Liam", ""], ["Feizi", "Soheil", ""], ["Goldstein", "Tom", ""]]}, {"id": "1905.09773", "submitter": "Tae-Hyun Oh", "authors": "Tae-Hyun Oh, Tali Dekel, Changil Kim, Inbar Mosseri, William T.\n  Freeman, Michael Rubinstein, Wojciech Matusik", "title": "Speech2Face: Learning the Face Behind a Voice", "comments": "To appear in CVPR2019. Project page: http://speech2face.github.io", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  How much can we infer about a person's looks from the way they speak? In this\npaper, we study the task of reconstructing a facial image of a person from a\nshort audio recording of that person speaking. We design and train a deep\nneural network to perform this task using millions of natural Internet/YouTube\nvideos of people speaking. During training, our model learns voice-face\ncorrelations that allow it to produce images that capture various physical\nattributes of the speakers such as age, gender and ethnicity. This is done in a\nself-supervised manner, by utilizing the natural co-occurrence of faces and\nspeech in Internet videos, without the need to model attributes explicitly. We\nevaluate and numerically quantify how--and in what manner--our Speech2Face\nreconstructions, obtained directly from audio, resemble the true face images of\nthe speakers.\n", "versions": [{"version": "v1", "created": "Thu, 23 May 2019 16:54:17 GMT"}], "update_date": "2019-05-24", "authors_parsed": [["Oh", "Tae-Hyun", ""], ["Dekel", "Tali", ""], ["Kim", "Changil", ""], ["Mosseri", "Inbar", ""], ["Freeman", "William T.", ""], ["Rubinstein", "Michael", ""], ["Matusik", "Wojciech", ""]]}, {"id": "1905.09788", "submitter": "Hiroshi Inoue", "authors": "Hiroshi Inoue", "title": "Multi-Sample Dropout for Accelerated Training and Better Generalization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dropout is a simple but efficient regularization technique for achieving\nbetter generalization of deep neural networks (DNNs); hence it is widely used\nin tasks based on DNNs. During training, dropout randomly discards a portion of\nthe neurons to avoid overfitting. This paper presents an enhanced dropout\ntechnique, which we call multi-sample dropout, for both accelerating training\nand improving generalization over the original dropout. The original dropout\ncreates a randomly selected subset (called a dropout sample) from the input in\neach training iteration while the multi-sample dropout creates multiple dropout\nsamples. The loss is calculated for each sample, and then the sample losses are\naveraged to obtain the final loss. This technique can be easily implemented by\nduplicating a part of the network after the dropout layer while sharing the\nweights among the duplicated fully connected layers. Experimental results using\nimage classification tasks including ImageNet, CIFAR-10, and CIFAR-100 showed\nthat multi-sample dropout accelerates training. Moreover, the networks trained\nusing multi-sample dropout achieved lower error rates compared to networks\ntrained with the original dropout. The additional computation cost due to the\nduplicated operations is not significant for deep convolutional networks\nbecause most of the computation time is consumed in the convolution layers\nbefore the dropout layer, which are not duplicated.\n", "versions": [{"version": "v1", "created": "Thu, 23 May 2019 17:22:57 GMT"}, {"version": "v2", "created": "Tue, 28 May 2019 06:25:23 GMT"}, {"version": "v3", "created": "Wed, 21 Oct 2020 02:39:55 GMT"}], "update_date": "2020-10-22", "authors_parsed": [["Inoue", "Hiroshi", ""]]}, {"id": "1905.09797", "submitter": "Tianyuan Zhang", "authors": "Tianyuan Zhang, Zhanxing Zhu", "title": "Interpreting Adversarially Trained Convolutional Neural Networks", "comments": "To apper in ICML19", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We attempt to interpret how adversarially trained convolutional neural\nnetworks (AT-CNNs) recognize objects. We design systematic approaches to\ninterpret AT-CNNs in both qualitative and quantitative ways and compare them\nwith normally trained models. Surprisingly, we find that adversarial training\nalleviates the texture bias of standard CNNs when trained on object recognition\ntasks, and helps CNNs learn a more shape-biased representation. We validate our\nhypothesis from two aspects. First, we compare the salience maps of AT-CNNs and\nstandard CNNs on clean images and images under different transformations. The\ncomparison could visually show that the prediction of the two types of CNNs is\nsensitive to dramatically different types of features. Second, to achieve\nquantitative verification, we construct additional test datasets that destroy\neither textures or shapes, such as style-transferred version of clean data,\nsaturated images and patch-shuffled ones, and then evaluate the classification\naccuracy of AT-CNNs and normal CNNs on these datasets. Our findings shed some\nlight on why AT-CNNs are more robust than those normally trained ones and\ncontribute to a better understanding of adversarial training over CNNs from an\ninterpretation perspective.\n", "versions": [{"version": "v1", "created": "Thu, 23 May 2019 17:40:41 GMT"}], "update_date": "2019-05-24", "authors_parsed": [["Zhang", "Tianyuan", ""], ["Zhu", "Zhanxing", ""]]}, {"id": "1905.09826", "submitter": "Raphael Sivera", "authors": "Rapha\\\"el Sivera (EPIONE), Herv\\'e Delingette (EPIONE), Marco Lorenzi\n  (EPIONE), Xavier Pennec (EPIONE), Nicholas Ayache (EPIONE)", "title": "A model of brain morphological changes related to aging and Alzheimer's\n  disease from cross-sectional assessments", "comments": "NeuroImage, Elsevier, In press", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.CV eess.IV q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this study we propose a deformation-based framework to jointly model the\ninfluence of aging and Alzheimer's disease (AD) on the brain morphological\nevolution. Our approach combines a spatio-temporal description of both\nprocesses into a generative model. A reference morphology is deformed along\nspecific trajectories to match subject specific morphologies. It is used to\ndefine two imaging progression markers: 1) a morphological age and 2) a disease\nscore. These markers can be computed locally in any brain region. The approach\nis evaluated on brain structural magnetic resonance images (MRI) from the ADNI\ndatabase. The generative model is first estimated on a control population,\nthen, for each subject, the markers are computed for each acquisition. The\nlongitudinal evolution of these markers is then studied in relation with the\nclinical diagnosis of the subjects and used to generate possible morphological\nevolution. In the model, the morphological changes associated with normal aging\nare mainly found around the ventricles, while the Alzheimer's disease specific\nchanges are more located in the temporal lobe and the hippocampal area. The\nstatistical analysis of these markers highlights differences between clinical\nconditions even though the inter-subject variability is quiet high. In this\ncontext, the model can be used to generate plausible morphological trajectories\nassociated with the disease. Our method gives two interpretable scalar imaging\nbiomarkers assessing the effects of aging and disease on brain morphology at\nthe individual and population level. These markers confirm an acceleration of\napparent aging for Alzheimer's subjects and can help discriminate clinical\nconditions even in prodromal stages. More generally, the joint modeling of\nnormal and pathological evolutions shows promising results to describe\nage-related brain diseases over long time scales.\n", "versions": [{"version": "v1", "created": "Thu, 23 May 2019 12:26:30 GMT"}], "update_date": "2019-05-27", "authors_parsed": [["Sivera", "Rapha\u00ebl", "", "EPIONE"], ["Delingette", "Herv\u00e9", "", "EPIONE"], ["Lorenzi", "Marco", "", "EPIONE"], ["Pennec", "Xavier", "", "EPIONE"], ["Ayache", "Nicholas", "", "EPIONE"]]}, {"id": "1905.09829", "submitter": "Chao Wang", "authors": "Chao Wang and Xiaohu Guo", "title": "Plane-Based Optimization of Geometry and Texture for RGB-D\n  Reconstruction of Indoor Scenes", "comments": "in International Conference on 3D Vision 2018; Models and Code: see\n  https://github.com/chaowang15/plane-opt-rgbd. arXiv admin note: text overlap\n  with arXiv:1905.08853", "journal-ref": "International Conference on 3D Vision (2018) 533--541", "doi": "10.1109/3DV.2018.00067", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel approach to reconstruct RGB-D indoor scene with plane\nprimitives. Our approach takes as input a RGB-D sequence and a dense coarse\nmesh reconstructed by some 3D reconstruction method on the sequence, and\ngenerate a lightweight, low-polygonal mesh with clear face textures and sharp\nfeatures without losing geometry details from the original scene. To achieve\nthis, we firstly partition the input mesh with plane primitives, simplify it\ninto a lightweight mesh next, then optimize plane parameters, camera poses and\ntexture colors to maximize the photometric consistency across frames, and\nfinally optimize mesh geometry to maximize consistency between geometry and\nplanes. Compared to existing planar reconstruction methods which only cover\nlarge planar regions in the scene, our method builds the entire scene by\nadaptive planes without losing geometry details and preserves sharp features in\nthe final mesh. We demonstrate the effectiveness of our approach by applying it\nonto several RGB-D scans and comparing it to other state-of-the-art\nreconstruction methods.\n", "versions": [{"version": "v1", "created": "Thu, 23 May 2019 17:03:52 GMT"}], "update_date": "2019-05-27", "authors_parsed": [["Wang", "Chao", ""], ["Guo", "Xiaohu", ""]]}, {"id": "1905.09871", "submitter": "Haidar Khan", "authors": "Haidar Khan, Daniel Park, Azer Khan, B\\\"ulent Yener", "title": "Thwarting finite difference adversarial attacks with output\n  randomization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adversarial examples pose a threat to deep neural network models in a variety\nof scenarios, from settings where the adversary has complete knowledge of the\nmodel and to the opposite \"black box\" setting. Black box attacks are\nparticularly threatening as the adversary only needs access to the input and\noutput of the model. Defending against black box adversarial example generation\nattacks is paramount as currently proposed defenses are not effective. Since\nthese types of attacks rely on repeated queries to the model to estimate\ngradients over input dimensions, we investigate the use of randomization to\nthwart such adversaries from successfully creating adversarial examples.\nRandomization applied to the output of the deep neural network model has the\npotential to confuse potential attackers, however this introduces a tradeoff\nbetween accuracy and robustness. We show that for certain types of\nrandomization, we can bound the probability of introducing errors by carefully\nsetting distributional parameters. For the particular case of finite difference\nblack box attacks, we quantify the error introduced by the defense in the\nfinite difference estimate of the gradient. Lastly, we show empirically that\nthe defense can thwart two adaptive black box adversarial attack algorithms.\n", "versions": [{"version": "v1", "created": "Thu, 23 May 2019 18:58:39 GMT"}], "update_date": "2019-05-27", "authors_parsed": [["Khan", "Haidar", ""], ["Park", "Daniel", ""], ["Khan", "Azer", ""], ["Yener", "B\u00fclent", ""]]}, {"id": "1905.09888", "submitter": "Farzad Khalvati", "authors": "Yucheng Zhang, Edrise M. Lobo-Mueller, Paul Karanicolas, Steven\n  Gallinger, Masoom A. Haider, Farzad Khalvati", "title": "Prognostic Value of Transfer Learning Based Features in Resectable\n  Pancreatic Ductal Adenocarcinoma", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pancreatic Ductal Adenocarcinoma (PDAC) is one of the most aggressive cancers\nwith an extremely poor prognosis. Radiomics has shown prognostic ability in\nmultiple types of cancer including PDAC. However, the prognostic value of\ntraditional radiomics pipelines, which are based on hand-crafted radiomic\nfeatures alone is limited. Convolutional neural networks (CNNs) have been shown\nto outperform these feature-based models in computer vision tasks. However,\ntraining a CNN from scratch needs a large sample size which is not feasible in\nmost medical imaging studies. As an alternative solution, CNN-based transfer\nlearning has shown potential for achieving reasonable performance using small\ndatasets. In this work, we developed and validated a CNN-based transfer\nlearning approach for prognostication of PDAC patients for overall survival\nusing two independent resectable PDAC cohorts. The proposed deep transfer\nlearning model for prognostication of PDAC achieved the area under the receiver\noperating characteristic curve of 0.74, which was significantly higher than\nthat of the traditional radiomics model (0.56) as well as a CNN model trained\nfrom scratch (0.50). These results suggest that deep transfer learning may\nsignificantly improve prognosis performance using small datasets in medical\nimaging.\n", "versions": [{"version": "v1", "created": "Thu, 23 May 2019 19:35:41 GMT"}, {"version": "v2", "created": "Wed, 21 Aug 2019 15:16:00 GMT"}], "update_date": "2019-08-22", "authors_parsed": [["Zhang", "Yucheng", ""], ["Lobo-Mueller", "Edrise M.", ""], ["Karanicolas", "Paul", ""], ["Gallinger", "Steven", ""], ["Haider", "Masoom A.", ""], ["Khalvati", "Farzad", ""]]}, {"id": "1905.09891", "submitter": "Michael Kissner", "authors": "Michael Kissner, Helmut Mayer", "title": "Adding Intuitive Physics to Neural-Symbolic Capsules Using Interaction\n  Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Many current methods to learn intuitive physics are based on interaction\nnetworks and similar approaches. However, they rely on information that has\nproven difficult to estimate directly from image data in the past. We aim to\nnarrow this gap by inferring all the semantic information needed from raw pixel\ndata in the form of a scene-graph. Our approach is based on neural-symbolic\ncapsules, which identify which objects in the scene are static, dynamic,\nelastic or rigid, possible joints between them, as well as their collision\ninformation. By integrating all this with interaction networks, we demonstrate\nhow our method is able to learn intuitive physics directly from image sequences\nand apply its knowledge to new scenes and objects, resulting in an\ninverse-simulation pipeline.\n", "versions": [{"version": "v1", "created": "Thu, 23 May 2019 19:39:07 GMT"}, {"version": "v2", "created": "Sat, 22 Jun 2019 15:26:00 GMT"}], "update_date": "2019-06-25", "authors_parsed": [["Kissner", "Michael", ""], ["Mayer", "Helmut", ""]]}, {"id": "1905.09907", "submitter": "Zhiling Long", "authors": "Yuting Hu, Zhiling Long, and Ghassan AlRegib", "title": "Multi-level Texture Encoding and Representation (MuLTER) based on Deep\n  Neural Networks", "comments": "Proceedings of IEEE International Conference on Image Processing\n  (ICIP), Sep. 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a multi-level texture encoding and representation\nnetwork (MuLTER) for texture-related applications. Based on a multi-level\npooling architecture, the MuLTER network simultaneously leverages low- and\nhigh-level features to maintain both texture details and spatial information.\nSuch a pooling architecture involves few extra parameters and keeps feature\ndimensions fixed despite of the changes of image sizes. In comparison with\nstate-of-the-art texture descriptors, the MuLTER network yields higher\nrecognition accuracy on typical texture datasets such as MINC-2500 and\nGTOS-mobile with a discriminative and compact representation. In addition, we\nanalyze the impact of combining features from different levels, which supports\nour claim that the fusion of multi-level features efficiently enhances\nrecognition performance. Our source code will be published on GitHub\n(https://github.com/olivesgatech).\n", "versions": [{"version": "v1", "created": "Thu, 23 May 2019 20:25:37 GMT"}], "update_date": "2019-05-27", "authors_parsed": [["Hu", "Yuting", ""], ["Long", "Zhiling", ""], ["AlRegib", "Ghassan", ""]]}, {"id": "1905.09932", "submitter": "Vladimir Ivashkin", "authors": "Vadim Lebedev, Vladimir Ivashkin, Irina Rudenko, Alexander Ganshin,\n  Alexander Molchanov, Sergey Ovcharenko, Ruslan Grokhovetskiy, Ivan\n  Bushmarinov, Dmitry Solomentsev", "title": "Precipitation Nowcasting with Satellite Imagery", "comments": "Final KDD 2019 version", "journal-ref": "in Proceedings of The 25th ACM SIGKDD Conference on Knowledge\n  Discovery and Data Mining, KDD 2019", "doi": "10.1145/3292500.3330762", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Precipitation nowcasting is a short-range forecast of rain/snow (up to 2\nhours), often displayed on top of the geographical map by the weather service.\nModern precipitation nowcasting algorithms rely on the extrapolation of\nobservations by ground-based radars via optical flow techniques or neural\nnetwork models. Dependent on these radars, typical nowcasting is limited to the\nregions around their locations. We have developed a method for precipitation\nnowcasting based on geostationary satellite imagery and incorporated the\nresulting data into the Yandex.Weather precipitation map (including an alerting\nservice with push notifications for products in the Yandex ecosystem), thus\nexpanding its coverage and paving the way to a truly global nowcasting service.\n", "versions": [{"version": "v1", "created": "Thu, 23 May 2019 21:11:16 GMT"}], "update_date": "2020-03-17", "authors_parsed": [["Lebedev", "Vadim", ""], ["Ivashkin", "Vladimir", ""], ["Rudenko", "Irina", ""], ["Ganshin", "Alexander", ""], ["Molchanov", "Alexander", ""], ["Ovcharenko", "Sergey", ""], ["Grokhovetskiy", "Ruslan", ""], ["Bushmarinov", "Ivan", ""], ["Solomentsev", "Dmitry", ""]]}, {"id": "1905.09939", "submitter": "Djamila Aouada", "authors": "Hassan Afzal, Djamila Aouada, Michel Antunes, David Fofi, Bruno\n  Mirbach, Bj\\\"orn Ottersten", "title": "Bi-objective Framework for Sensor Fusion in RGB-D Multi-View Systems:\n  Applications in Calibration", "comments": "23 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Complete and textured 3D reconstruction of dynamic scenes has been\nfacilitated by mapped RGB and depth information acquired by RGB-D cameras based\nmulti-view systems. One of the most critical steps in such multi-view systems\nis to determine the relative poses of all cameras via a process known as\nextrinsic calibration. In this work, we propose a sensor fusion framework based\non a weighted bi-objective optimization for refinement of extrinsic calibration\ntailored for RGB-D multi-view systems. The weighted bi-objective cost function,\nwhich makes use of 2D information from RGB images and 3D information from depth\nimages, is analytically derived via the Maximum Likelihood (ML) method. The\nweighting factor appears as a function of noise in 2D and 3D measurements and\ntakes into account the affect of residual errors on the optimization. We\npropose an iterative scheme to estimate noise variances in 2D and 3D\nmeasurements, for simultaneously computing the weighting factor together with\nthe camera poses. An extensive quantitative and qualitative evaluation of the\nproposed approach shows improved calibration performance as compared to\nrefinement schemes which use only 2D or 3D measurement information.\n", "versions": [{"version": "v1", "created": "Thu, 23 May 2019 21:25:42 GMT"}], "update_date": "2019-05-27", "authors_parsed": [["Afzal", "Hassan", ""], ["Aouada", "Djamila", ""], ["Antunes", "Michel", ""], ["Fofi", "David", ""], ["Mirbach", "Bruno", ""], ["Ottersten", "Bj\u00f6rn", ""]]}, {"id": "1905.09949", "submitter": "Nachiket Deo", "authors": "Nachiket Deo and Mohan M. Trivedi", "title": "Scene Induced Multi-Modal Trajectory Forecasting via Planning", "comments": "ICRA Workshop on Long Term Human Motion Prediction (extended\n  abstract)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address multi-modal trajectory forecasting of agents in unknown scenes by\nformulating it as a planning problem. We present an approach consisting of\nthree models; a goal prediction model to identify potential goals of the agent,\nan inverse reinforcement learning model to plan optimal paths to each goal, and\na trajectory generator to obtain future trajectories along the planned paths.\nAnalysis of predictions on the Stanford drone dataset, shows generalizability\nof our approach to novel scenes.\n", "versions": [{"version": "v1", "created": "Thu, 23 May 2019 22:00:17 GMT"}], "update_date": "2019-05-30", "authors_parsed": [["Deo", "Nachiket", ""], ["Trivedi", "Mohan M.", ""]]}, {"id": "1905.09970", "submitter": "Vlad Paunescu", "authors": "Andretti Naiden, Vlad Paunescu, Gyeongmo Kim, ByeongMoon Jeon, Marius\n  Leordeanu", "title": "Shift R-CNN: Deep Monocular 3D Object Detection with Closed-Form\n  Geometric Constraints", "comments": "v1: Accepted to be published in 2019 IEEE International Conference on\n  Image Processing, Sep 22-25, 2019, Taipei. IEEE Copyright notice added. Minor\n  changes for camera-ready version. (updated May. 15, 2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose Shift R-CNN, a hybrid model for monocular 3D object detection,\nwhich combines deep learning with the power of geometry. We adapt a Faster\nR-CNN network for regressing initial 2D and 3D object properties and combine it\nwith a least squares solution for the inverse 2D to 3D geometric mapping\nproblem, using the camera projection matrix. The closed-form solution of the\nmathematical system, along with the initial output of the adapted Faster R-CNN\nare then passed through a final ShiftNet network that refines the result using\nour newly proposed Volume Displacement Loss. Our novel, geometrically\nconstrained deep learning approach to monocular 3D object detection obtains top\nresults on KITTI 3D Object Detection Benchmark, being the best among all\nmonocular methods that do not use any pre-trained network for depth estimation.\n", "versions": [{"version": "v1", "created": "Thu, 23 May 2019 23:41:07 GMT"}], "update_date": "2019-05-27", "authors_parsed": [["Naiden", "Andretti", ""], ["Paunescu", "Vlad", ""], ["Kim", "Gyeongmo", ""], ["Jeon", "ByeongMoon", ""], ["Leordeanu", "Marius", ""]]}, {"id": "1905.09976", "submitter": "Hasan Al-Marzouqi", "authors": "Hasan Al-Marzouqi, Yuting Hu, Ghassan AlRegib", "title": "Texture retrieval using periodically extended and adaptive curvelets", "comments": null, "journal-ref": "Signal Processing: Image Communication, Volume 76, 2019, Pages\n  252-260, ISSN 0923-5965", "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image retrieval is an important problem in the area of multimedia processing.\nThis paper presents two new curvelet-based algorithms for texture retrieval\nwhich are suitable for use in constrained-memory devices. The developed\nalgorithms are tested on three publicly available texture datasets: CUReT,\nMondial-Marmi, and STex-fabric. Our experiments confirm the effectiveness of\nthe proposed system. Furthermore, a weighted version of the proposed retrieval\nalgorithm is proposed, which is shown to achieve promising results in the\nclassification of seismic activities.\n", "versions": [{"version": "v1", "created": "Fri, 24 May 2019 00:15:19 GMT"}], "update_date": "2019-05-27", "authors_parsed": [["Al-Marzouqi", "Hasan", ""], ["Hu", "Yuting", ""], ["AlRegib", "Ghassan", ""]]}, {"id": "1905.09979", "submitter": "Hanhan Li", "authors": "Hanhan Li, Joe Yue-Hei Ng, Paul Natsev", "title": "EnsembleNet: End-to-End Optimization of Multi-headed Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ensembling is a universally useful approach to boost the performance of\nmachine learning models. However, individual models in an ensemble were\ntraditionally trained independently in separate stages without information\naccess about the overall ensemble. Many co-distillation approaches were\nproposed in order to treat model ensembling as first-class citizens. In this\npaper, we reveal a deeper connection between ensembling and distillation, and\ncome up with a simpler yet more effective co-distillation architecture. On\nlarge-scale datasets including ImageNet, YouTube-8M, and Kinetics, we\ndemonstrate a general procedure that can convert a single deep neural network\nto a multi-headed model that has not only a smaller size but also better\nperformance. The model can be optimized end-to-end with our proposed\nco-distillation loss in a single stage without human intervention.\n", "versions": [{"version": "v1", "created": "Fri, 24 May 2019 00:31:11 GMT"}, {"version": "v2", "created": "Thu, 26 Sep 2019 04:00:03 GMT"}], "update_date": "2019-09-27", "authors_parsed": [["Li", "Hanhan", ""], ["Ng", "Joe Yue-Hei", ""], ["Natsev", "Paul", ""]]}, {"id": "1905.09980", "submitter": "Jia Peng", "authors": "Yi Huang and Peng Jia and Dongmei Cai and Bojun Cai", "title": "Perception Evaluation -- A new solar image quality metric based on the\n  multi-fractal property of texture features", "comments": "15 pages, 13 figures, accepted by Solar Physics", "journal-ref": null, "doi": "10.1007/s11207-019-1524-5", "report-no": null, "categories": "astro-ph.IM astro-ph.SR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Next-generation ground-based solar observations require good image quality\nmetrics for post-facto processing techniques. Based on the assumption that\ntexture features in solar images are multi-fractal which can be extracted by a\ntrained deep neural network as feature maps, a new reduced-reference objective\nimage quality metric, the perception evaluation is proposed. The perception\nevaluation is defined as cosine distance of Gram matrix between feature maps\nextracted from high resolution reference image and that from blurred images. We\nevaluate performance of the perception evaluation with simulated and real\nobservation images. The results show that with a high resolution image as\nreference, the perception evaluation can give robust estimate of image quality\nfor solar images of different scenes.\n", "versions": [{"version": "v1", "created": "Fri, 24 May 2019 00:33:17 GMT"}, {"version": "v2", "created": "Wed, 26 Jun 2019 08:31:02 GMT"}], "update_date": "2019-10-09", "authors_parsed": [["Huang", "Yi", ""], ["Jia", "Peng", ""], ["Cai", "Dongmei", ""], ["Cai", "Bojun", ""]]}, {"id": "1905.09994", "submitter": "Xiaoye Sun", "authors": "Xiaoye Sun, Gongyan Li, Shaoyun Xu", "title": "FSD: Feature Skyscraper Detector for Stem End and Blossom End of Navel\n  Orange", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To accurately and efficiently distinguish the stem end and the blossom end of\nnavel orange from its black spots, we propose a feature skyscraper detector\n(FSD) with low computational cost, compact architecture and high detection\naccuracy. The main part of the detector is inspired from small object that stem\n(blossom) end is complex and black spot is densely distributed, so we design\nthe feature skyscraper networks (FSN) based on dense connectivity. In\nparticular, FSN is distinguished from regular feature pyramids, and which\nprovides more intensive detection of high-level features. Then we design the\nbackbone of the FSD based on attention mechanism and dense block for better\nfeature extraction to the FSN. In addition, the architecture of the detector is\nalso added Swish to further improve the accuracy. And we create a dataset in\nPascal VOC format annotated three types of detection targets the stem end, the\nblossom end and the black spot. Experimental results on our orange data set\nconfirm that FSD has competitive results to the state-of-the-art one-stage\ndetectors like SSD, DSOD, YOLOv2, YOLOv3, RFB and FSSD, and it achieves\n87.479%mAP at 131 FPS with only 5.812M parameters.\n", "versions": [{"version": "v1", "created": "Fri, 24 May 2019 01:44:57 GMT"}, {"version": "v2", "created": "Mon, 11 Nov 2019 16:30:15 GMT"}], "update_date": "2019-11-12", "authors_parsed": [["Sun", "Xiaoye", ""], ["Li", "Gongyan", ""], ["Xu", "Shaoyun", ""]]}, {"id": "1905.09998", "submitter": "Jialin Wu", "authors": "Jialin Wu and Raymond J. Mooney", "title": "Self-Critical Reasoning for Robust Visual Question Answering", "comments": "In NeurIPS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual Question Answering (VQA) deep-learning systems tend to capture\nsuperficial statistical correlations in the training data because of strong\nlanguage priors and fail to generalize to test data with a significantly\ndifferent question-answer (QA) distribution. To address this issue, we\nintroduce a self-critical training objective that ensures that visual\nexplanations of correct answers match the most influential image regions more\nthan other competitive answer candidates. The influential regions are either\ndetermined from human visual/textual explanations or automatically from just\nsignificant words in the question and answer. We evaluate our approach on the\nVQA generalization task using the VQA-CP dataset, achieving a new\nstate-of-the-art i.e., 49.5% using textual explanations and 48.5% using\nautomatically annotated regions.\n", "versions": [{"version": "v1", "created": "Fri, 24 May 2019 01:52:31 GMT"}, {"version": "v2", "created": "Tue, 29 Oct 2019 17:47:23 GMT"}, {"version": "v3", "created": "Mon, 30 Dec 2019 06:33:21 GMT"}], "update_date": "2020-01-01", "authors_parsed": [["Wu", "Jialin", ""], ["Mooney", "Raymond J.", ""]]}, {"id": "1905.10000", "submitter": "Yongxi Lu", "authors": "Yongxi Lu, Ziyao Tang and Tara Javidi", "title": "Implicit Label Augmentation on Partially Annotated Clips via\n  Temporally-Adaptive Features Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Partially annotated clips contain rich temporal contexts that can complement\nthe sparse key frame annotations in providing supervision for model training.\nWe present a novel paradigm called Temporally-Adaptive Features (TAF) learning\nthat can utilize such data to learn better single frame models. By imposing\ndistinct temporal change rate constraints on different factors in the model,\nTAF enables learning from unlabeled frames using context to enhance model\naccuracy. TAF generalizes \"slow feature\" learning and we present much stronger\nempirical evidence than prior works, showing convincing gains for the\nchallenging semantic segmentation task over a variety of architecture designs\nand on two popular datasets. TAF can be interpreted as an implicit label\naugmentation method but is a more principled formulation compared to existing\nexplicit augmentation techniques. Our work thus connects two promising methods\nthat utilize partially annotated clips for single frame model training and can\ninspire future explorations in this direction.\n", "versions": [{"version": "v1", "created": "Fri, 24 May 2019 02:02:35 GMT"}], "update_date": "2019-05-27", "authors_parsed": [["Lu", "Yongxi", ""], ["Tang", "Ziyao", ""], ["Javidi", "Tara", ""]]}, {"id": "1905.10011", "submitter": "Yixing Li", "authors": "Yixing Li and Fengbo Ren", "title": "Light-Weight RetinaNet for Object Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Object detection has gained great progress driven by the development of deep\nlearning. Compared with a widely studied task -- classification, generally\nspeaking, object detection even need one or two orders of magnitude more FLOPs\n(floating point operations) in processing the inference task. To enable a\npractical application, it is essential to explore effective runtime and\naccuracy trade-off scheme. Recently, a growing number of studies are intended\nfor object detection on resource constraint devices, such as YOLOv1, YOLOv2,\nSSD, MobileNetv2-SSDLite, whose accuracy on COCO test-dev detection results are\nyield to mAP around 22-25% (mAP-20-tier). On the contrary, very few studies\ndiscuss the computation and accuracy trade-off scheme for mAP-30-tier detection\nnetworks. In this paper, we illustrate the insights of why RetinaNet gives\neffective computation and accuracy trade-off for object detection and how to\nbuild a light-weight RetinaNet. We propose to only reduce FLOPs in\ncomputational intensive layers and keep other layer the same. Compared with\nmost common way -- input image scaling for FLOPs-accuracy trade-off, the\nproposed solution shows a constantly better FLOPs-mAP trade-off line.\nQuantitatively, the proposed method result in 0.1% mAP improvement at 1.15x\nFLOPs reduction and 0.3% mAP improvement at 1.8x FLOPs reduction.\n", "versions": [{"version": "v1", "created": "Fri, 24 May 2019 02:56:28 GMT"}], "update_date": "2019-05-27", "authors_parsed": [["Li", "Yixing", ""], ["Ren", "Fengbo", ""]]}, {"id": "1905.10037", "submitter": "Satoshi Nishida", "authors": "Satoshi Nishida, Yusuke Nakano, Antoine Blanc, Naoya Maeda, Masataka\n  Kado, and Shinji Nishimoto", "title": "Brain-mediated Transfer Learning of Convolutional Neural Networks", "comments": null, "journal-ref": "Proc. Thirty-Fourth AAAI Conf. Artif. Intell. (2020) 5281-5288", "doi": "10.1609/aaai.v34i04.5974", "report-no": null, "categories": "cs.CV q-bio.NC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The human brain can effectively learn a new task from a small number of\nsamples, which indicate that the brain can transfer its prior knowledge to\nsolve tasks in different domains. This function is analogous to transfer\nlearning (TL) in the field of machine learning. TL uses a well-trained feature\nspace in a specific task domain to improve performance in new tasks with\ninsufficient training data. TL with rich feature representations, such as\nfeatures of convolutional neural networks (CNNs), shows high generalization\nability across different task domains. However, such TL is still insufficient\nin making machine learning attain generalization ability comparable to that of\nthe human brain. To examine if the internal representation of the brain could\nbe used to achieve more efficient TL, we introduce a method for TL mediated by\nhuman brains. Our method transforms feature representations of audiovisual\ninputs in CNNs into those in activation patterns of individual brains via their\nassociation learned ahead using measured brain responses. Then, to estimate\nlabels reflecting human cognition and behavior induced by the audiovisual\ninputs, the transformed representations are used for TL. We demonstrate that\nour brain-mediated TL (BTL) shows higher performance in the label estimation\nthan the standard TL. In addition, we illustrate that the estimations mediated\nby different brains vary from brain to brain, and the variability reflects the\nindividual variability in perception. Thus, our BTL provides a framework to\nimprove the generalization ability of machine-learning feature representations\nand enable machine learning to estimate human-like cognition and behavior,\nincluding individual variability.\n", "versions": [{"version": "v1", "created": "Fri, 24 May 2019 05:15:17 GMT"}, {"version": "v2", "created": "Tue, 10 Sep 2019 10:09:35 GMT"}, {"version": "v3", "created": "Wed, 9 Oct 2019 02:20:53 GMT"}], "update_date": "2020-07-06", "authors_parsed": [["Nishida", "Satoshi", ""], ["Nakano", "Yusuke", ""], ["Blanc", "Antoine", ""], ["Maeda", "Naoya", ""], ["Kado", "Masataka", ""], ["Nishimoto", "Shinji", ""]]}, {"id": "1905.10048", "submitter": "Zhixiang Wang", "authors": "Zheng Wang, Zhixiang Wang, Yinqiang Zheng, Yang Wu, Wenjun Zeng,\n  Shin'ichi Satoh", "title": "Beyond Intra-modality: A Survey of Heterogeneous Person\n  Re-identification", "comments": "Accepted by IJCAI 2020. Project url:\n  https://github.com/lightChaserX/Awesome-Hetero-reID", "journal-ref": "IJCAI 2020", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An efficient and effective person re-identification (ReID) system relieves\nthe users from painful and boring video watching and accelerates the process of\nvideo analysis. Recently, with the explosive demands of practical applications,\na lot of research efforts have been dedicated to heterogeneous person\nre-identification (Hetero-ReID). In this paper, we provide a comprehensive\nreview of state-of-the-art Hetero-ReID methods that address the challenge of\ninter-modality discrepancies. According to the application scenario, we\nclassify the methods into four categories -- low-resolution, infrared, sketch,\nand text. We begin with an introduction of ReID, and make a comparison between\nHomogeneous ReID (Homo-ReID) and Hetero-ReID tasks. Then, we describe and\ncompare existing datasets for performing evaluations, and survey the models\nthat have been widely employed in Hetero-ReID. We also summarize and compare\nthe representative approaches from two perspectives, i.e., the application\nscenario and the learning pipeline. We conclude by a discussion of some future\nresearch directions. Follow-up updates are avaible at:\nhttps://github.com/lightChaserX/Awesome-Hetero-reID\n", "versions": [{"version": "v1", "created": "Fri, 24 May 2019 06:27:32 GMT"}, {"version": "v2", "created": "Mon, 22 Jul 2019 06:37:30 GMT"}, {"version": "v3", "created": "Mon, 25 Nov 2019 13:39:39 GMT"}, {"version": "v4", "created": "Mon, 27 Apr 2020 15:35:46 GMT"}], "update_date": "2020-04-28", "authors_parsed": [["Wang", "Zheng", ""], ["Wang", "Zhixiang", ""], ["Zheng", "Yinqiang", ""], ["Wu", "Yang", ""], ["Zeng", "Wenjun", ""], ["Satoh", "Shin'ichi", ""]]}, {"id": "1905.10054", "submitter": "Sathyaprakash Narayanan", "authors": "Sathyaprakash Narayanan, Yeshwanth Bethi, Chetan Singh Thakur", "title": "A Compressive Sensing Video dataset using Pixel-wise coded exposure", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Manifold amount of video data gets generated every minute as we read this\ndocument, ranging from surveillance to broadcasting purposes. There are two\nroadblocks that restrain us from using this data as such, first being the\nstorage which restricts us from only storing the information based on the\nhardware constraints. Secondly, the computation required to process this data\nis highly expensive which makes it infeasible to work on them. Compressive\nsensing(CS)[2] is a signal process technique[11], through optimization, the\nsparsity of a signal can be exploited to recover it from far fewer samples than\nrequired by the Shannon-Nyquist sampling theorem. There are two conditions\nunder which recovery is possible. The first one is sparsity which requires the\nsignal to be sparse in some domain. The second one is incoherence which is\napplied through the isometric property which is sufficient for sparse\nsignals[9][10]. To sustain these characteristics, preserving all attributes in\nthe uncompressed domain would help any kind of in this field. However, existing\ndataset fallback in terms of continuous tracking of all the object present in\nthe scene, very few video datasets have comprehensive continuous tracking of\nobjects. To address these problems collectively, in this work we propose a new\ncomprehensive video dataset, where the data is compressed using pixel-wise\ncoded exposure [3] that resolves various other impediments.\n", "versions": [{"version": "v1", "created": "Fri, 24 May 2019 06:39:12 GMT"}, {"version": "v2", "created": "Tue, 9 Jul 2019 02:59:37 GMT"}], "update_date": "2019-07-10", "authors_parsed": [["Narayanan", "Sathyaprakash", ""], ["Bethi", "Yeshwanth", ""], ["Thakur", "Chetan Singh", ""]]}, {"id": "1905.10059", "submitter": "Yuanyuan Liu", "authors": "Yuanyuan Liu, Jiyao Peng, Jiabei Zeng, and Shiguang Shan", "title": "Pose-adaptive Hierarchical Attention Network for Facial Expression\n  Recognition", "comments": "12 pages, 15 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-view facial expression recognition (FER) is a challenging task because\nthe appearance of an expression varies in poses. To alleviate the influences of\nposes, recent methods either perform pose normalization or learn separate FER\nclassifiers for each pose. However, these methods usually have two stages and\nrely on good performance of pose estimators. Different from existing methods,\nwe propose a pose-adaptive hierarchical attention network (PhaNet) that can\njointly recognize the facial expressions and poses in unconstrained\nenvironment. Specifically, PhaNet discovers the most relevant regions to the\nfacial expression by an attention mechanism in hierarchical scales, and the\nmost informative scales are then selected to learn the pose-invariant and\nexpression-discriminative representations. PhaNet is end-to-end trainable by\nminimizing the hierarchical attention losses, the FER loss and pose loss with\ndynamically learned loss weights. We validate the effectiveness of the proposed\nPhaNet on three multi-view datasets (BU-3DFE, Multi-pie, and KDEF) and two\nin-the-wild FER datasets (AffectNet and SFEW). Extensive experiments\ndemonstrate that our framework outperforms the state-of-the-arts under both\nwithin-dataset and cross-dataset settings, achieving the average accuracies of\n84.92\\%, 93.53\\%, 88.5\\%, 54.82\\% and 31.25\\% respectively.\n", "versions": [{"version": "v1", "created": "Fri, 24 May 2019 06:54:14 GMT"}], "update_date": "2019-05-27", "authors_parsed": [["Liu", "Yuanyuan", ""], ["Peng", "Jiyao", ""], ["Zeng", "Jiabei", ""], ["Shan", "Shiguang", ""]]}, {"id": "1905.10064", "submitter": "Peng Sun", "authors": "Peng Sun, Peiwen Lin, Guangliang Cheng, Jianping Shi, Jiawan Zhang, Xi\n  Li", "title": "OVSNet : Towards One-Pass Real-Time Video Object Segmentation", "comments": "10 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video object segmentation aims at accurately segmenting the target object\nregions across consecutive frames. It is technically challenging for coping\nwith complicated factors (e.g., shape deformations, occlusion and out of the\nlens). Recent approaches have largely solved them by using backforth\nre-identification and bi-directional mask propagation. However, their methods\nare extremely slow and only support offline inference, which in principle\ncannot be applied in real time. Motivated by this observation, we propose a\nefficient detection-based paradigm for video object segmentation. We propose an\nunified One-Pass Video Segmentation framework (OVS-Net) for modeling\nspatial-temporal representation in a unified pipeline, which seamlessly\nintegrates object detection, object segmentation, and object re-identification.\nThe proposed framework lends itself to one-pass inference that effectively and\nefficiently performs video object segmentation. Moreover, we propose a\nmaskguided attention module for modeling the multi-scale object boundary and\nmulti-level feature fusion. Experiments on the challenging DAVIS 2017\ndemonstrate the effectiveness of the proposed framework with comparable\nperformance to the state-of-the-art, and the great efficiency about 11.5 FPS\ntowards pioneering real-time work to our knowledge, more than 5 times faster\nthan other state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Fri, 24 May 2019 07:12:48 GMT"}, {"version": "v2", "created": "Tue, 2 Jul 2019 11:37:33 GMT"}], "update_date": "2019-07-03", "authors_parsed": [["Sun", "Peng", ""], ["Lin", "Peiwen", ""], ["Cheng", "Guangliang", ""], ["Shi", "Jianping", ""], ["Zhang", "Jiawan", ""], ["Li", "Xi", ""]]}, {"id": "1905.10071", "submitter": "HsuanKung Yang", "authors": "Hsuan-Kung Yang, Po-Han Chiang, Min-Fong Hong, and Chun-Yi Lee", "title": "Flow-based Intrinsic Curiosity Module", "comments": "The SOLE copyright holder is IJCAI (International Joint Conferences\n  on Artificial Intelligence), all rights reserved. The link is provided as\n  follows: https://www.ijcai.org/Proceedings/2020/286", "journal-ref": "Proceedings of the Twenty-Ninth International Joint Conference on\n  Artificial Intelligence Main track. Pages 2065-2072", "doi": "10.24963/ijcai.2020/286", "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we focus on a prediction-based novelty estimation strategy\nupon the deep reinforcement learning (DRL) framework, and present a flow-based\nintrinsic curiosity module (FICM) to exploit the prediction errors from optical\nflow estimation as exploration bonuses. We propose the concept of leveraging\nmotion features captured between consecutive observations to evaluate the\nnovelty of observations in an environment. FICM encourages a DRL agent to\nexplore observations with unfamiliar motion features, and requires only two\nconsecutive frames to obtain sufficient information when estimating the\nnovelty. We evaluate our method and compare it with a number of existing\nmethods on multiple benchmark environments, including Atari games, Super Mario\nBros., and ViZDoom. We demonstrate that FICM is favorable to tasks or\nenvironments featuring moving objects, which allow FICM to utilize the motion\nfeatures between consecutive observations. We further ablatively analyze the\nencoding efficiency of FICM, and discuss its applicable domains\ncomprehensively.\n", "versions": [{"version": "v1", "created": "Fri, 24 May 2019 07:32:30 GMT"}, {"version": "v2", "created": "Wed, 25 Sep 2019 15:06:51 GMT"}, {"version": "v3", "created": "Wed, 15 Jul 2020 04:39:13 GMT"}], "update_date": "2020-07-28", "authors_parsed": [["Yang", "Hsuan-Kung", ""], ["Chiang", "Po-Han", ""], ["Hong", "Min-Fong", ""], ["Lee", "Chun-Yi", ""]]}, {"id": "1905.10085", "submitter": "Junyu Gao", "authors": "Junyu Gao, Qi Wang, Xuelong Li", "title": "PCC Net: Perspective Crowd Counting via Spatial Convolutional Network", "comments": "accepted by IEEE T-CSVT", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Crowd counting from a single image is a challenging task due to high\nappearance similarity, perspective changes and severe congestion. Many methods\nonly focus on the local appearance features and they cannot handle the\naforementioned challenges. In order to tackle them, we propose a Perspective\nCrowd Counting Network (PCC Net), which consists of three parts: 1) Density Map\nEstimation (DME) focuses on learning very local features for density map\nestimation; 2) Random High-level Density Classification (R-HDC) extracts global\nfeatures to predict the coarse density labels of random patches in images; 3)\nFore-/Background Segmentation (FBS) encodes mid-level features to segments the\nforeground and background. Besides, the DULR module is embedded in PCC Net to\nencode the perspective changes on four directions (Down, Up, Left and Right).\nThe proposed PCC Net is verified on five mainstream datasets, which achieves\nthe state-of-the-art performance on the one and attains the competitive results\non the other four datasets. The source code is available at\nhttps://github.com/gjy3035/PCC-Net.\n", "versions": [{"version": "v1", "created": "Fri, 24 May 2019 08:23:13 GMT"}], "update_date": "2019-05-27", "authors_parsed": [["Gao", "Junyu", ""], ["Wang", "Qi", ""], ["Li", "Xuelong", ""]]}, {"id": "1905.10089", "submitter": "Xinxin Hu", "authors": "Xinxin Hu, Kailun Yang, Lei Fei and Kaiwei Wang", "title": "ACNet: Attention Based Network to Exploit Complementary Features for\n  RGBD Semantic Segmentation", "comments": "Accepted to be published in 2019 IEEE International Conference on\n  Image Processing (ICIP 2019), Sep 22-25, 2019, Taipei. IEEE Copyright notice\n  added, 5 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Compared to RGB semantic segmentation, RGBD semantic segmentation can achieve\nbetter performance by taking depth information into consideration. However, it\nis still problematic for contemporary segmenters to effectively exploit RGBD\ninformation since the feature distributions of RGB and depth (D) images vary\nsignificantly in different scenes. In this paper, we propose an Attention\nComplementary Network (ACNet) that selectively gathers features from RGB and\ndepth branches. The main contributions lie in the Attention Complementary\nModule (ACM) and the architecture with three parallel branches. More precisely,\nACM is a channel attention-based module that extracts weighted features from\nRGB and depth branches. The architecture preserves the inference of the\noriginal RGB and depth branches, and enables the fusion branch at the same\ntime. Based on the above structures, ACNet is capable of exploiting more\nhigh-quality features from different channels. We evaluate our model on\nSUN-RGBD and NYUDv2 datasets, and prove that our model outperforms\nstate-of-the-art methods. In particular, a mIoU score of 48.3\\% on NYUDv2 test\nset is achieved with ResNet50. We will release our source code based on PyTorch\nand the trained segmentation model at https://github.com/anheidelonghu/ACNet.\n", "versions": [{"version": "v1", "created": "Fri, 24 May 2019 08:44:41 GMT"}], "update_date": "2019-05-27", "authors_parsed": [["Hu", "Xinxin", ""], ["Yang", "Kailun", ""], ["Fei", "Lei", ""], ["Wang", "Kaiwei", ""]]}, {"id": "1905.10100", "submitter": "Yang Lu", "authors": "Yang Lu, Xiaohui Liang and Frederick W. B. Li", "title": "Multi-Scale Dual-Branch Fully Convolutional Network for Hand Parsing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, fully convolutional neural networks (FCNs) have shown significant\nperformance in image parsing, including scene parsing and object parsing.\nDifferent from generic object parsing tasks, hand parsing is more challenging\ndue to small size, complex structure, heavy self-occlusion and ambiguous\ntexture problems. In this paper, we propose a novel parsing framework,\nMulti-Scale Dual-Branch Fully Convolutional Network (MSDB-FCN), for hand\nparsing tasks. Our network employs a Dual-Branch architecture to extract\nfeatures of hand area, paying attention on the hand itself. These features are\nused to generate multi-scale features with pyramid pooling strategy. In order\nto better encode multi-scale features, we design a Deconvolution and Bilinear\nInterpolation Block (DB-Block) for upsampling and merging the features of\ndifferent scales. To address data imbalance, which is a common problem in many\ncomputer vision tasks as well as hand parsing tasks, we propose a\ngeneralization of Focal Loss, namely Multi-Class Balanced Focal Loss, to tackle\ndata imbalance in multi-class classification. Extensive experiments on\nRHD-PARSING dataset demonstrate that our MSDB-FCN has achieved the\nstate-of-the-art performance for hand parsing.\n", "versions": [{"version": "v1", "created": "Fri, 24 May 2019 09:09:37 GMT"}], "update_date": "2019-05-27", "authors_parsed": [["Lu", "Yang", ""], ["Liang", "Xiaohui", ""], ["Li", "Frederick W. B.", ""]]}, {"id": "1905.10107", "submitter": "Matteo Poggi", "authors": "Matteo Poggi, Davide Pallotti, Fabio Tosi, Stefano Mattoccia", "title": "Guided Stereo Matching", "comments": "CVPR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stereo is a prominent technique to infer dense depth maps from images, and\ndeep learning further pushed forward the state-of-the-art, making end-to-end\narchitectures unrivaled when enough data is available for training. However,\ndeep networks suffer from significant drops in accuracy when dealing with new\nenvironments. Therefore, in this paper, we introduce Guided Stereo Matching, a\nnovel paradigm leveraging a small amount of sparse, yet reliable depth\nmeasurements retrieved from an external source enabling to ameliorate this\nweakness. The additional sparse cues required by our method can be obtained\nwith any strategy (e.g., a LiDAR) and used to enhance features linked to\ncorresponding disparity hypotheses. Our formulation is general and fully\ndifferentiable, thus enabling to exploit the additional sparse inputs in\npre-trained deep stereo networks as well as for training a new instance from\nscratch. Extensive experiments on three standard datasets and two\nstate-of-the-art deep architectures show that even with a small set of sparse\ninput cues, i) the proposed paradigm enables significant improvements to\npre-trained networks. Moreover, ii) training from scratch notably increases\naccuracy and robustness to domain shifts. Finally, iii) it is suited and\neffective even with traditional stereo algorithms such as SGM.\n", "versions": [{"version": "v1", "created": "Fri, 24 May 2019 09:32:34 GMT"}], "update_date": "2019-05-27", "authors_parsed": [["Poggi", "Matteo", ""], ["Pallotti", "Davide", ""], ["Tosi", "Fabio", ""], ["Mattoccia", "Stefano", ""]]}, {"id": "1905.10112", "submitter": "Vincenzo Lomonaco PhD", "authors": "Vincenzo Lomonaco, Karan Desai, Eugenio Culurciello, Davide Maltoni", "title": "Continual Reinforcement Learning in 3D Non-stationary Environments", "comments": "Accepted in the CLVision Workshop at CVPR2020: 13 pages, 4 figures, 5\n  tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High-dimensional always-changing environments constitute a hard challenge for\ncurrent reinforcement learning techniques. Artificial agents, nowadays, are\noften trained off-line in very static and controlled conditions in simulation\nsuch that training observations can be thought as sampled i.i.d. from the\nentire observations space. However, in real world settings, the environment is\noften non-stationary and subject to unpredictable, frequent changes. In this\npaper we propose and openly release CRLMaze, a new benchmark for learning\ncontinually through reinforcement in a complex 3D non-stationary task based on\nViZDoom and subject to several environmental changes. Then, we introduce an\nend-to-end model-free continual reinforcement learning strategy showing\ncompetitive results with respect to four different baselines and not requiring\nany access to additional supervised signals, previously encountered\nenvironmental conditions or observations.\n", "versions": [{"version": "v1", "created": "Fri, 24 May 2019 09:38:42 GMT"}, {"version": "v2", "created": "Tue, 21 Apr 2020 14:57:48 GMT"}], "update_date": "2020-04-22", "authors_parsed": [["Lomonaco", "Vincenzo", ""], ["Desai", "Karan", ""], ["Culurciello", "Eugenio", ""], ["Maltoni", "Davide", ""]]}, {"id": "1905.10117", "submitter": "Andreas Pfeuffer", "authors": "Andreas Pfeuffer and Klaus Dietmayer", "title": "Robust Semantic Segmentation in Adverse Weather Conditions by means of\n  Sensor Data Fusion", "comments": null, "journal-ref": "22st International Conference on Information Fusion (FUSION)\n  (2019)", "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A robust and reliable semantic segmentation in adverse weather conditions is\nvery important for autonomous cars, but most state-of-the-art approaches only\nachieve high accuracy rates in optimal weather conditions. The reason is that\nthey are only optimized for good weather conditions and given noise models.\nHowever, most of them fail, if data with unknown disturbances occur, and their\nperformance decrease enormously. One possibility to still obtain reliable\nresults is to observe the environment with different sensor types, such as\ncamera and lidar, and to fuse the sensor data by means of neural networks,\nsince different sensors behave differently in diverse weather conditions.\nHence, the sensors can complement each other by means of an appropriate sensor\ndata fusion. Nevertheless, the fusion-based approaches are still susceptible to\ndisturbances and fail to classify disturbed image areas correctly. This problem\ncan be solved by means of a special training method, the so called Robust\nLearning Method (RLM), a method by which the neural network learns to handle\nunknown noise. In this work, two different sensor fusion architectures for\nsemantic segmentation are compared and evaluated on several datasets.\nFurthermore, it is shown that the RLM increases the robustness in adverse\nweather conditions enormously, and achieve good results although no disturbance\nmodel has been learned by the neural network.\n", "versions": [{"version": "v1", "created": "Fri, 24 May 2019 09:55:57 GMT"}], "update_date": "2019-05-27", "authors_parsed": [["Pfeuffer", "Andreas", ""], ["Dietmayer", "Klaus", ""]]}, {"id": "1905.10150", "submitter": "Yang Li", "authors": "Yang Li and Xuanqin Mou", "title": "Saliency detection based on structural dissimilarity induced by image\n  quality assessment model", "comments": "For associated source code, see https://github.com/yangli-xjtu/SDS", "journal-ref": "J. Electron. Imag. 28(2) 023025 (3 April 2019)", "doi": "10.1117/1.JEI.28.2.023025", "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The distinctiveness of image regions is widely used as the cue of saliency.\nGenerally, the distinctiveness is computed according to the absolute difference\nof features. However, according to the image quality assessment (IQA) studies,\nthe human visual system is highly sensitive to structural changes rather than\nabsolute difference. Accordingly, we propose the computation of the structural\ndissimilarity between image patches as the distinctiveness measure for saliency\ndetection. Similar to IQA models, the structural dissimilarity is computed\nbased on the correlation of the structural features. The global structural\ndissimilarity of a patch to all the other patches represents saliency of the\npatch. We adopt two widely used structural features, namely the local contrast\nand gradient magnitude, into the structural dissimilarity computation in the\nproposed model. Without any postprocessing, the proposed model based on the\ncorrelation of either of the two structural features outperforms 11\nstate-of-the-art saliency models on three saliency databases.\n", "versions": [{"version": "v1", "created": "Fri, 24 May 2019 11:13:14 GMT"}], "update_date": "2019-05-27", "authors_parsed": [["Li", "Yang", ""], ["Mou", "Xuanqin", ""]]}, {"id": "1905.10161", "submitter": "George Moustakides", "authors": "Kalliopi Basioti and George V. Moustakides", "title": "Optimizing Shallow Networks for Binary Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data driven classification that relies on neural networks is based on\noptimization criteria that involve some form of distance between the output of\nthe network and the desired label. Using the same mathematical analysis, for a\nmultitude of such measures, we can show that their optimum solution matches the\nideal likelihood ratio test classifier. In this work we introduce a different\nfamily of optimization problems which is not covered by the existing approaches\nand, therefore, opens possibilities for new training algorithms for neural\nnetwork based classification. We give examples that lead to algorithms that are\nsimple in implementation, exhibit stable convergence characteristics and are\nantagonistic to the most popular existing techniques.\n", "versions": [{"version": "v1", "created": "Fri, 24 May 2019 11:40:24 GMT"}, {"version": "v2", "created": "Mon, 24 Jun 2019 00:42:01 GMT"}], "update_date": "2019-06-25", "authors_parsed": [["Basioti", "Kalliopi", ""], ["Moustakides", "George V.", ""]]}, {"id": "1905.10170", "submitter": "Thanh-Dat Truong", "authors": "Thanh-Dat Truong, Khoa Luu, Chi Nhan Duong, Ngan Le and Minh-Triet\n  Tran", "title": "Generative Flow via Invertible nxn Convolution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Flow-based generative models have recently become one of the most efficient\napproaches to model data generation. Indeed, they are constructed with a\nsequence of invertible and tractable transformations. Glow first introduced a\nsimple type of generative flow using an invertible $1 \\times 1$ convolution.\nHowever, the $1 \\times 1$ convolution suffers from limited flexibility compared\nto the standard convolutions. In this paper, we propose a novel invertible $n\n\\times n$ convolution approach that overcomes the limitations of the invertible\n$1 \\times 1$ convolution. In addition, our proposed network is not only\ntractable and invertible but also uses fewer parameters than standard\nconvolutions. The experiments on CIFAR-10, ImageNet and Celeb-HQ datasets, have\nshown that our invertible $n \\times n$ convolution helps to improve the\nperformance of generative models significantly.\n", "versions": [{"version": "v1", "created": "Fri, 24 May 2019 11:58:05 GMT"}, {"version": "v2", "created": "Fri, 16 Jul 2021 06:44:53 GMT"}], "update_date": "2021-07-19", "authors_parsed": [["Truong", "Thanh-Dat", ""], ["Luu", "Khoa", ""], ["Duong", "Chi Nhan", ""], ["Le", "Ngan", ""], ["Tran", "Minh-Triet", ""]]}, {"id": "1905.10218", "submitter": "Santosh Tirunagari", "authors": "Santosh Tirunagari, Norman Poh, Kevin Wells, Miroslaw Bober, Isky\n  Gorden, David Windridge", "title": "Functional Segmentation through Dynamic Mode Decomposition: Automatic\n  Quantification of Kidney Function in DCE-MRI Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quantification of kidney function in Dynamic Contrast-Enhanced Magnetic\nResonance Imaging (DCE-MRI) requires careful segmentation of the renal region\nof interest (ROI). Traditionally, human experts are required to manually\ndelineate the kidney ROI across multiple images in the dynamic sequence. This\napproach is costly, time-consuming and labour intensive, and therefore acts to\nlimit patient throughout and acts as one of the factors limiting the wider\nadoption of DCR-MRI in clinical practice. Therefore, to address this issue, we\npresent the first use of Dynamic Mode Decomposition (DMD) as a basis for\nautomatic segmentation of a dynamic sequence, in this case, kidney ROIs in\nDCE-MRI. Using DMD coupled combined with thresholding and connected component\nanalysis is first validated on synthetically generated data with known\nground-truth, and then applied to ten healthy volunteers' DCE-MRI datasets. We\nfind that the segmentation result obtained from our proposed DMD framework is\ncomparable to that of expert observers and very significantly better than that\nof an a-priori bounding box segmentation. Our result gives a mean Jaccard\ncoefficient of 0.87, compared to mean scores of 0.85, 0.88 and 0.87 produced\nfrom three independent manual annotations. This represents the first use of DMD\nas a robust automatic data-driven segmentation approach without requiring any\nhuman intervention. This is a viable, efficient alternative approach to current\nmanual methods of isolation of kidney function in DCE-MRI.\n", "versions": [{"version": "v1", "created": "Fri, 24 May 2019 13:12:23 GMT"}], "update_date": "2019-05-27", "authors_parsed": [["Tirunagari", "Santosh", ""], ["Poh", "Norman", ""], ["Wells", "Kevin", ""], ["Bober", "Miroslaw", ""], ["Gorden", "Isky", ""], ["Windridge", "David", ""]]}, {"id": "1905.10226", "submitter": "Chenfei Wu", "authors": "Chenfei Wu, Yanzhao Zhou, Gen Li, Nan Duan, Duyu Tang, Xiaojie Wang", "title": "Deep Reason: A Strong Baseline for Real-World Visual Reasoning", "comments": "CVPR 2019 Visual Question Answering and Dialog Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a strong baseline for real-world visual reasoning (GQA),\nwhich achieves 60.93% in GQA 2019 challenge and won the sixth place. GQA is a\nlarge dataset with 22M questions involving spatial understanding and multi-step\ninference. To help further research in this area, we identified three crucial\nparts that improve the performance, namely: multi-source features, fine-grained\nencoder, and score-weighted ensemble. We provide a series of analysis on their\nimpact on performance.\n", "versions": [{"version": "v1", "created": "Fri, 24 May 2019 13:34:21 GMT"}, {"version": "v2", "created": "Mon, 17 Jun 2019 15:26:58 GMT"}], "update_date": "2019-06-18", "authors_parsed": [["Wu", "Chenfei", ""], ["Zhou", "Yanzhao", ""], ["Li", "Gen", ""], ["Duan", "Nan", ""], ["Tang", "Duyu", ""], ["Wang", "Xiaojie", ""]]}, {"id": "1905.10231", "submitter": "Junxing Hu", "authors": "Junxing Hu, Ling Li, Yijun Lin, Fengge Wu, Junsuo Zhao", "title": "A Comparison and Strategy of Semantic Segmentation on Remote Sensing\n  Images", "comments": "8 pages, 3 figures, ICNC-FSKD 2019", "journal-ref": null, "doi": "10.1007/978-3-030-32456-8_3", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, with the development of aerospace technology, we use more\nand more images captured by satellites to obtain information. But a large\nnumber of useless raw images, limited data storage resource and poor\ntransmission capability on satellites hinder our use of valuable images.\nTherefore, it is necessary to deploy an on-orbit semantic segmentation model to\nfilter out useless images before data transmission. In this paper, we present a\ndetailed comparison on the recent deep learning models. Considering the\ncomputing environment of satellites, we compare methods from accuracy,\nparameters and resource consumption on the same public dataset. And we also\nanalyze the relation between them. Based on experimental results, we further\npropose a viable on-orbit semantic segmentation strategy. It will be deployed\non the TianZhi-2 satellite which supports deep learning methods and will be\nlunched soon.\n", "versions": [{"version": "v1", "created": "Fri, 24 May 2019 13:39:12 GMT"}], "update_date": "2019-12-11", "authors_parsed": [["Hu", "Junxing", ""], ["Li", "Ling", ""], ["Lin", "Yijun", ""], ["Wu", "Fengge", ""], ["Zhao", "Junsuo", ""]]}, {"id": "1905.10236", "submitter": "Junxing Hu", "authors": "Ling Li, Junxing Hu, Fengge Wu, Junsuo Zhao", "title": "A Research and Strategy of Remote Sensing Image Denoising Algorithms", "comments": "9 pages, 4 figures, ICNC-FSKD 2019", "journal-ref": null, "doi": "10.1007/978-3-030-32591-6_75", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most raw data download from satellites are useless, resulting in transmission\nwaste, one solution is to process data directly on satellites, then only\ntransmit the processed results to the ground. Image processing is the main data\nprocessing on satellites, in this paper, we focus on image denoising which is\nthe basic image processing. There are many high-performance denoising\napproaches at present, however, most of them rely on advanced computing\nresources or rich images on the ground. Considering the limited computing\nresources of satellites and the characteristics of remote sensing images, we do\nsome research on these high-performance ground image denoising approaches and\ncompare them in simulation experiments to analyze whether they are suitable for\nsatellites. According to the analysis results, we propose two feasible image\ndenoising strategies for satellites based on satellite TianZhi-1.\n", "versions": [{"version": "v1", "created": "Fri, 24 May 2019 13:47:19 GMT"}], "update_date": "2019-12-11", "authors_parsed": [["Li", "Ling", ""], ["Hu", "Junxing", ""], ["Wu", "Fengge", ""], ["Zhao", "Junsuo", ""]]}, {"id": "1905.10240", "submitter": "Yunpeng Li", "authors": "Yunpeng Li, Dominik Roblek, Marco Tagliasacchi", "title": "From Here to There: Video Inbetweening Using Direct 3D Convolutions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of generating plausible and diverse video sequences,\nwhen we are only given a start and an end frame. This task is also known as\ninbetweening, and it belongs to the broader area of stochastic video\ngeneration, which is generally approached by means of recurrent neural networks\n(RNN). In this paper, we propose instead a fully convolutional model to\ngenerate video sequences directly in the pixel domain. We first obtain a latent\nvideo representation using a stochastic fusion mechanism that learns how to\nincorporate information from the start and end frames. Our model learns to\nproduce such latent representation by progressively increasing the temporal\nresolution, and then decode in the spatiotemporal domain using 3D convolutions.\nThe model is trained end-to-end by minimizing an adversarial loss. Experiments\non several widely-used benchmark datasets show that it is able to generate\nmeaningful and diverse in-between video sequences, according to both\nquantitative and qualitative evaluations.\n", "versions": [{"version": "v1", "created": "Fri, 24 May 2019 14:01:08 GMT"}, {"version": "v2", "created": "Mon, 27 May 2019 07:55:23 GMT"}, {"version": "v3", "created": "Tue, 4 Jun 2019 07:54:06 GMT"}], "update_date": "2019-06-05", "authors_parsed": [["Li", "Yunpeng", ""], ["Roblek", "Dominik", ""], ["Tagliasacchi", "Marco", ""]]}, {"id": "1905.10257", "submitter": "Yassir Saquil", "authors": "Yassir Saquil, Qun-Ce Xu, Yong-Liang Yang, Peter Hall", "title": "Rank3DGAN: Semantic mesh generation using relative attributes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we investigate a novel problem of using generative adversarial\nnetworks in the task of 3D shape generation according to semantic attributes.\nRecent works map 3D shapes into 2D parameter domain, which enables training\nGenerative Adversarial Networks (GANs) for 3D shape generation task. We extend\nthese architectures to the conditional setting, where we generate 3D shapes\nwith respect to subjective attributes defined by the user. Given pairwise\ncomparisons of 3D shapes, our model performs two tasks: it learns a generative\nmodel with a controlled latent space, and a ranking function for the 3D shapes\nbased on their multi-chart representation in 2D. The capability of the model is\ndemonstrated with experiments on HumanShape, Basel Face Model and reconstructed\n3D CUB datasets. We also present various applications that benefit from our\nmodel, such as multi-attribute exploration, mesh editing, and mesh attribute\ntransfer.\n", "versions": [{"version": "v1", "created": "Fri, 24 May 2019 14:30:58 GMT"}, {"version": "v2", "created": "Tue, 28 May 2019 14:06:24 GMT"}], "update_date": "2019-05-29", "authors_parsed": [["Saquil", "Yassir", ""], ["Xu", "Qun-Ce", ""], ["Yang", "Yong-Liang", ""], ["Hall", "Peter", ""]]}, {"id": "1905.10290", "submitter": "Edgar Tretschk", "authors": "Edgar Tretschk, Ayush Tewari, Michael Zollh\\\"ofer, Vladislav Golyanik,\n  Christian Theobalt", "title": "DEMEA: Deep Mesh Autoencoders for Non-Rigidly Deforming Objects", "comments": "27 pages, including supplementary material", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mesh autoencoders are commonly used for dimensionality reduction, sampling\nand mesh modeling. We propose a general-purpose DEep MEsh Autoencoder (DEMEA)\nwhich adds a novel embedded deformation layer to a graph-convolutional mesh\nautoencoder. The embedded deformation layer (EDL) is a differentiable\ndeformable geometric proxy which explicitly models point displacements of\nnon-rigid deformations in a lower dimensional space and serves as a local\nrigidity regularizer. DEMEA decouples the parameterization of the deformation\nfrom the final mesh resolution since the deformation is defined over a lower\ndimensional embedded deformation graph. We perform a large-scale study on four\ndifferent datasets of deformable objects. Reasoning about the local rigidity of\nmeshes using EDL allows us to achieve higher-quality results for highly\ndeformable objects, compared to directly regressing vertex positions. We\ndemonstrate multiple applications of DEMEA, including non-rigid 3D\nreconstruction from depth and shading cues, non-rigid surface tracking, as well\nas the transfer of deformations over different meshes.\n", "versions": [{"version": "v1", "created": "Fri, 24 May 2019 15:35:37 GMT"}, {"version": "v2", "created": "Tue, 4 Aug 2020 15:35:07 GMT"}], "update_date": "2020-08-05", "authors_parsed": [["Tretschk", "Edgar", ""], ["Tewari", "Ayush", ""], ["Zollh\u00f6fer", "Michael", ""], ["Golyanik", "Vladislav", ""], ["Theobalt", "Christian", ""]]}, {"id": "1905.10296", "submitter": "Florian Kraus", "authors": "Florian Kraus and Klaus Dietmayer", "title": "Uncertainty Estimation in One-Stage Object Detection", "comments": null, "journal-ref": "IEEE Intelligent Transportation Systems Conference (ITSC), 2019,\n  pp. 53-60", "doi": "10.1109/ITSC.2019.8917494", "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Environment perception is the task for intelligent vehicles on which all\nsubsequent steps rely. A key part of perception is to safely detect other road\nusers such as vehicles, pedestrians, and cyclists. With modern deep learning\ntechniques huge progress was made over the last years in this field. However\nsuch deep learning based object detection models cannot predict how certain\nthey are in their predictions, potentially hampering the performance of later\nsteps such as tracking or sensor fusion. We present a viable approaches to\nestimate uncertainty in an one-stage object detector, while improving the\ndetection performance of the baseline approach. The proposed model is evaluated\non a large scale automotive pedestrian dataset. Experimental results show that\nthe uncertainty outputted by our system is coupled with detection accuracy and\nthe occlusion level of pedestrians.\n", "versions": [{"version": "v1", "created": "Fri, 24 May 2019 15:51:59 GMT"}, {"version": "v2", "created": "Fri, 10 Jul 2020 11:13:58 GMT"}], "update_date": "2020-07-13", "authors_parsed": [["Kraus", "Florian", ""], ["Dietmayer", "Klaus", ""]]}, {"id": "1905.10308", "submitter": "Dell Zhang", "authors": "Dan A. Calian, Peter Roelants, Jacques Cali, Ben Carr, Krishna Dubba,\n  John E. Reid, Dell Zhang", "title": "SCRAM: Spatially Coherent Randomized Attention Maps", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Attention mechanisms and non-local mean operations in general are key\ningredients in many state-of-the-art deep learning techniques. In particular,\nthe Transformer model based on multi-head self-attention has recently achieved\ngreat success in natural language processing and computer vision. However, the\nvanilla algorithm computing the Transformer of an image with n pixels has\nO(n^2) complexity, which is often painfully slow and sometimes prohibitively\nexpensive for large-scale image data. In this paper, we propose a fast\nrandomized algorithm --- SCRAM --- that only requires O(n log(n)) time to\nproduce an image attention map. Such a dramatic acceleration is attributed to\nour insight that attention maps on real-world images usually exhibit (1)\nspatial coherence and (2) sparse structure. The central idea of SCRAM is to\nemploy PatchMatch, a randomized correspondence algorithm, to quickly pinpoint\nthe most compatible key (argmax) for each query first, and then exploit that\nknowledge to design a sparse approximation to non-local mean operations. Using\nthe argmax (mode) to dynamically construct the sparse approximation\ndistinguishes our algorithm from all of the existing sparse approximate methods\nand makes it very efficient. Moreover, SCRAM is a broadly applicable\napproximation to any non-local mean layer in contrast to some other sparse\napproximations that can only approximate self-attention. Our preliminary\nexperimental results suggest that SCRAM is indeed promising for speeding up or\nscaling up the computation of attention maps in the Transformer.\n", "versions": [{"version": "v1", "created": "Fri, 24 May 2019 16:03:44 GMT"}], "update_date": "2019-05-27", "authors_parsed": [["Calian", "Dan A.", ""], ["Roelants", "Peter", ""], ["Cali", "Jacques", ""], ["Carr", "Ben", ""], ["Dubba", "Krishna", ""], ["Reid", "John E.", ""], ["Zhang", "Dell", ""]]}, {"id": "1905.10346", "submitter": "Shuyang Gu", "authors": "Shuyang Gu, Jianmin Bao, Hao Yang, Dong Chen, Fang Wen, Lu Yuan", "title": "Mask-Guided Portrait Editing with Conditional GANs", "comments": "To appear in CVPR2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Portrait editing is a popular subject in photo manipulation. The Generative\nAdversarial Network (GAN) advances the generating of realistic faces and allows\nmore face editing. In this paper, we argue about three issues in existing\ntechniques: diversity, quality, and controllability for portrait synthesis and\nediting. To address these issues, we propose a novel end-to-end learning\nframework that leverages conditional GANs guided by provided face masks for\ngenerating faces. The framework learns feature embeddings for every face\ncomponent (e.g., mouth, hair, eye), separately, contributing to better\ncorrespondences for image translation, and local face editing. With the mask,\nour network is available to many applications, like face synthesis driven by\nmask, face Swap+ (including hair in swapping), and local manipulation. It can\nalso boost the performance of face parsing a bit as an option of data\naugmentation.\n", "versions": [{"version": "v1", "created": "Fri, 24 May 2019 17:27:42 GMT"}], "update_date": "2019-05-27", "authors_parsed": [["Gu", "Shuyang", ""], ["Bao", "Jianmin", ""], ["Yang", "Hao", ""], ["Chen", "Dong", ""], ["Wen", "Fang", ""], ["Yuan", "Lu", ""]]}, {"id": "1905.10357", "submitter": "Tauseef Ali", "authors": "Tauseef Ali and Eissa Jaber Alreshidi", "title": "Deep Trajectory for Recognition of Human Behaviours", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Identifying human actions in complex scenes is widely considered as a\nchallenging research problem due to the unpredictable behaviors and variation\nof appearances and postures. For extracting variations in motion and postures,\ntrajectories provide meaningful way. However, simple trajectories are normally\nrepresented by vector of spatial coordinates. In order to identify human\nactions, we must exploit structural relationship between different\ntrajectories. In this paper, we propose a method that divides the video into N\nnumber of segments and then for each segment we extract trajectories. We then\ncompute trajectory descriptor for each segment which capture the structural\nrelationship among different trajectories in the video segment. For trajectory\ndescriptor, we project all extracted trajectories on the canvas. This will\nresult in texture image which can store the relative motion and structural\nrelationship among the trajectories. We then train Convolution Neural Network\n(CNN) to capture and learn the representation from dense trajectories. .\nExperimental results shows that our proposed method out performs state of the\nart methods by 90.01% on benchmark data set.\n", "versions": [{"version": "v1", "created": "Fri, 24 May 2019 17:42:37 GMT"}], "update_date": "2019-05-27", "authors_parsed": [["Ali", "Tauseef", ""], ["Alreshidi", "Eissa Jaber", ""]]}, {"id": "1905.10448", "submitter": "Matthew Hirn", "authors": "Michael Perlmutter and Feng Gao and Guy Wolf and Matthew Hirn", "title": "Geometric Wavelet Scattering Networks on Compact Riemannian Manifolds", "comments": "35 pages; 3 figures; 2 tables; v3: Revisions based on reviewer\n  comments", "journal-ref": "Proceedings of The First Mathematical and Scientific Machine\n  Learning Conference, PMLR 107:570-604, 2020", "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG math.FA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Euclidean scattering transform was introduced nearly a decade ago to\nimprove the mathematical understanding of convolutional neural networks.\nInspired by recent interest in geometric deep learning, which aims to\ngeneralize convolutional neural networks to manifold and graph-structured\ndomains, we define a geometric scattering transform on manifolds. Similar to\nthe Euclidean scattering transform, the geometric scattering transform is based\non a cascade of wavelet filters and pointwise nonlinearities. It is invariant\nto local isometries and stable to certain types of diffeomorphisms. Empirical\nresults demonstrate its utility on several geometric learning tasks. Our\nresults generalize the deformation stability and local translation invariance\nof Euclidean scattering, and demonstrate the importance of linking the used\nfilter structures to the underlying geometry of the data.\n", "versions": [{"version": "v1", "created": "Fri, 24 May 2019 21:19:04 GMT"}, {"version": "v2", "created": "Thu, 5 Dec 2019 21:44:23 GMT"}, {"version": "v3", "created": "Fri, 17 Jul 2020 01:30:40 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Perlmutter", "Michael", ""], ["Gao", "Feng", ""], ["Wolf", "Guy", ""], ["Hirn", "Matthew", ""]]}, {"id": "1905.10452", "submitter": "Matteo Spallanzani", "authors": "Matteo Spallanzani, Lukas Cavigelli, Gian Paolo Leonardi, Marko\n  Bertogna and Luca Benini", "title": "Additive Noise Annealing and Approximation Properties of Quantized\n  Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a theoretical and experimental investigation of the quantization\nproblem for artificial neural networks. We provide a mathematical definition of\nquantized neural networks and analyze their approximation capabilities, showing\nin particular that any Lipschitz-continuous map defined on a hypercube can be\nuniformly approximated by a quantized neural network. We then focus on the\nregularization effect of additive noise on the arguments of multi-step\nfunctions inherent to the quantization of continuous variables. In particular,\nwhen the expectation operator is applied to a non-differentiable multi-step\nrandom function, and if the underlying probability density is differentiable\n(in either classical or weak sense), then a differentiable function is\nretrieved, with explicit bounds on its Lipschitz constant. Based on these\nresults, we propose a novel gradient-based training algorithm for quantized\nneural networks that generalizes the straight-through estimator, acting on\nnoise applied to the network's parameters. We evaluate our algorithm on the\nCIFAR-10 and ImageNet image classification benchmarks, showing state-of-the-art\nperformance on AlexNet and MobileNetV2 for ternary networks.\n", "versions": [{"version": "v1", "created": "Fri, 24 May 2019 21:30:54 GMT"}], "update_date": "2019-05-29", "authors_parsed": [["Spallanzani", "Matteo", ""], ["Cavigelli", "Lukas", ""], ["Leonardi", "Gian Paolo", ""], ["Bertogna", "Marko", ""], ["Benini", "Luca", ""]]}, {"id": "1905.10484", "submitter": "Keegan Lensink", "authors": "Keegan Lensink, Bas Peters, Eldad Haber", "title": "Fully Hyperbolic Convolutional Neural Networks", "comments": "21 pages, 9 figures, Updated work to include additional numerical\n  experiments, a section about VAEs and learnable wavelets", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional Neural Networks (CNN) have recently seen tremendous success in\nvarious computer vision tasks. However, their application to problems with high\ndimensional input and output, such as high-resolution image and video\nsegmentation or 3D medical imaging, has been limited by various factors.\nPrimarily, in the training stage, it is necessary to store network activations\nfor back propagation. In these settings, the memory requirements associated\nwith storing activations can exceed what is feasible with current hardware,\nespecially for problems in 3D. Motivated by the propagation of signals over\nphysical networks, that are governed by the hyperbolic Telegraph equation, in\nthis work we introduce a fully conservative hyperbolic network for problems\nwith high dimensional input and output. We introduce a coarsening operation\nthat allows completely reversible CNNs by using a learnable Discrete Wavelet\nTransform and its inverse to both coarsen and interpolate the network state and\nchange the number of channels. We show that fully reversible networks are able\nto achieve results comparable to the state of the art in 4D time-lapse hyper\nspectral image segmentation and full 3D video segmentation, with a much lower\nmemory footprint that is a constant independent of the network depth. We also\nextend the use of such networks to Variational Auto Encoders with high\nresolution input and output.\n", "versions": [{"version": "v1", "created": "Fri, 24 May 2019 23:43:36 GMT"}, {"version": "v2", "created": "Tue, 17 Sep 2019 20:45:03 GMT"}, {"version": "v3", "created": "Tue, 7 Jul 2020 18:02:05 GMT"}], "update_date": "2020-07-09", "authors_parsed": [["Lensink", "Keegan", ""], ["Peters", "Bas", ""], ["Haber", "Eldad", ""]]}, {"id": "1905.10485", "submitter": "Qing Yan", "authors": "Zhisheng Xiao, Qing Yan, Yali Amit", "title": "Generative Latent Flow", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we propose the Generative Latent Flow (GLF), an algorithm for\ngenerative modeling of the data distribution. GLF uses an Auto-encoder (AE) to\nlearn latent representations of the data, and a normalizing flow to map the\ndistribution of the latent variables to that of simple i.i.d noise. In contrast\nto some other Auto-encoder based generative models, which use various\nregularizers that encourage the encoded latent distribution to match the prior\ndistribution, our model explicitly constructs a mapping between these two\ndistributions, leading to better density matching while avoiding over\nregularizing the latent variables. We compare our model with several related\ntechniques, and show that it has many relative advantages including fast\nconvergence, single stage training and minimal reconstruction trade-off. We\nalso study the relationship between our model and its stochastic counterpart,\nand show that our model can be viewed as a vanishing noise limit of VAEs with\nflow prior. Quantitatively, under standardized evaluations, our method achieves\nstate-of-the-art sample quality among AE based models on commonly used\ndatasets, and is competitive with GANs' benchmarks.\n", "versions": [{"version": "v1", "created": "Fri, 24 May 2019 23:44:50 GMT"}, {"version": "v2", "created": "Sun, 22 Sep 2019 22:57:15 GMT"}], "update_date": "2019-09-24", "authors_parsed": [["Xiao", "Zhisheng", ""], ["Yan", "Qing", ""], ["Amit", "Yali", ""]]}, {"id": "1905.10488", "submitter": "Sungmin Cha", "authors": "Sungmin Cha, Taeeon Park, Byeongjoon Kim, Jongduk Baek and Taesup Moon", "title": "GAN2GAN: Generative Noise Learning for Blind Denoising with Single Noisy\n  Images", "comments": "ICLR 2021 camera ready version", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We tackle a challenging blind image denoising problem, in which only single\ndistinct noisy images are available for training a denoiser, and no information\nabout noise is known, except for it being zero-mean, additive, and independent\nof the clean image. In such a setting, which often occurs in practice, it is\nnot possible to train a denoiser with the standard discriminative training or\nwith the recently developed Noise2Noise (N2N) training; the former requires the\nunderlying clean image for the given noisy image, and the latter requires two\nindependently realized noisy image pair for a clean image. To that end, we\npropose GAN2GAN (Generated-Artificial-Noise to Generated-Artificial-Noise)\nmethod that first learns a generative model that can 1) simulate the noise in\nthe given noisy images and 2) generate a rough, noisy estimates of the clean\nimages, then 3) iteratively trains a denoiser with subsequently synthesized\nnoisy image pairs (as in N2N), obtained from the generative model. In results,\nwe show the denoiser trained with our GAN2GAN achieves an impressive denoising\nperformance on both synthetic and real-world datasets for the blind denoising\nsetting; it almost approaches the performance of the standard\ndiscriminatively-trained or N2N-trained models that have more information than\nours, and it significantly outperforms the recent baseline for the same\nsetting, \\textit{e.g.}, Noise2Void, and a more conventional yet strong one,\nBM3D. The official code of our method is available at\nhttps://github.com/csm9493/GAN2GAN.\n", "versions": [{"version": "v1", "created": "Sat, 25 May 2019 00:16:09 GMT"}, {"version": "v2", "created": "Thu, 19 Dec 2019 17:23:24 GMT"}, {"version": "v3", "created": "Fri, 12 Jun 2020 02:58:45 GMT"}, {"version": "v4", "created": "Mon, 19 Apr 2021 09:01:11 GMT"}, {"version": "v5", "created": "Sun, 4 Jul 2021 09:16:19 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["Cha", "Sungmin", ""], ["Park", "Taeeon", ""], ["Kim", "Byeongjoon", ""], ["Baek", "Jongduk", ""], ["Moon", "Taesup", ""]]}, {"id": "1905.10498", "submitter": "L\\'eon Bottou", "authors": "Chhavi Yadav and L\\'eon Bottou", "title": "Cold Case: The Lost MNIST Digits", "comments": "Final NeurIPS version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although the popular MNIST dataset [LeCun et al., 1994] is derived from the\nNIST database [Grother and Hanaoka, 1995], the precise processing steps for\nthis derivation have been lost to time. We propose a reconstruction that is\naccurate enough to serve as a replacement for the MNIST dataset, with\ninsignificant changes in accuracy. We trace each MNIST digit to its NIST source\nand its rich metadata such as writer identifier, partition identifier, etc. We\nalso reconstruct the complete MNIST test set with 60,000 samples instead of the\nusual 10,000. Since the balance 50,000 were never distributed, they enable us\nto investigate the impact of twenty-five years of MNIST experiments on the\nreported testing performances. Our results unambiguously confirm the trends\nobserved by Recht et al. [2018, 2019]: although the misclassification rates are\nslightly off, classifier ordering and model selection remain broadly reliable.\nWe attribute this phenomenon to the pairing benefits of comparing classifiers\non the same digits.\n", "versions": [{"version": "v1", "created": "Sat, 25 May 2019 01:50:51 GMT"}, {"version": "v2", "created": "Mon, 4 Nov 2019 21:05:26 GMT"}], "update_date": "2019-11-06", "authors_parsed": [["Yadav", "Chhavi", ""], ["Bottou", "L\u00e9on", ""]]}, {"id": "1905.10520", "submitter": "Arsalan Mousavian", "authors": "Arsalan Mousavian, Clemens Eppner, Dieter Fox", "title": "6-DOF GraspNet: Variational Grasp Generation for Object Manipulation", "comments": "Accepted to ICCV 2019. Extended camera ready version with additional\n  experiments", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generating grasp poses is a crucial component for any robot object\nmanipulation task. In this work, we formulate the problem of grasp generation\nas sampling a set of grasps using a variational autoencoder and assess and\nrefine the sampled grasps using a grasp evaluator model. Both Grasp Sampler and\nGrasp Refinement networks take 3D point clouds observed by a depth camera as\ninput. We evaluate our approach in simulation and real-world robot experiments.\nOur approach achieves 88\\% success rate on various commonly used objects with\ndiverse appearances, scales, and weights. Our model is trained purely in\nsimulation and works in the real world without any extra steps. The video of\nour experiments can be found at:\nhttps://research.nvidia.com/publication/2019-10_6-DOF-GraspNet\\%3A-Variational\n", "versions": [{"version": "v1", "created": "Sat, 25 May 2019 04:59:52 GMT"}, {"version": "v2", "created": "Sat, 17 Aug 2019 00:23:16 GMT"}], "update_date": "2019-08-20", "authors_parsed": [["Mousavian", "Arsalan", ""], ["Eppner", "Clemens", ""], ["Fox", "Dieter", ""]]}, {"id": "1905.10529", "submitter": "Yangru Huang", "authors": "Yangru Huang, Peixi Peng, Yi Jin, Junliang Xing, Congyan Lang, Songhe\n  Feng", "title": "Domain Adaptive Attention Model for Unsupervised Cross-Domain Person\n  Re-Identification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Person re-identification (Re-ID) across multiple datasets is a challenging\nyet important task due to the possibly large distinctions between different\ndatasets and the lack of training samples in practical applications. This work\nproposes a novel unsupervised domain adaption framework which transfers\ndiscriminative representations from the labeled source domain (dataset) to the\nunlabeled target domain (dataset). We propose to formulate the domain adaption\ntask as an one-class classification problem with a novel domain similarity\nloss. Given the feature map of any image from a backbone network, a novel\ndomain adaptive attention model (DAAM) first automatically learns to separate\nthe feature map of an image to a domain-shared feature (DSH) map and a\ndomain-specific feature (DSP) map simultaneously. Specially, the residual\nattention mechanism is designed to model DSP feature map for avoiding negative\ntransfer. Then, a DSH branch and a DSP branch are introduced to learn DSH and\nDSP feature maps respectively. To reduce domain divergence caused by that the\nsource and target datasets are collected from different environments, we force\nto project the DSH feature maps from different domains to a new nominal domain,\nand a novel domain similarity loss is proposed based on one-class\nclassification. In addition, a novel unsupervised person Re-ID loss is proposed\nto take full use of unlabeled target data. Extensive experiments on the\nMarket-1501 and DukeMTMC-reID benchmarks demonstrate state-of-the-art\nperformance of the proposed method. Code will be released to facilitate further\nstudies on the cross-domain person re-identification task.\n", "versions": [{"version": "v1", "created": "Sat, 25 May 2019 06:05:49 GMT"}], "update_date": "2019-05-28", "authors_parsed": [["Huang", "Yangru", ""], ["Peng", "Peixi", ""], ["Jin", "Yi", ""], ["Xing", "Junliang", ""], ["Lang", "Congyan", ""], ["Feng", "Songhe", ""]]}, {"id": "1905.10535", "submitter": "Constantin Pape", "authors": "Constantin Pape, Alex Matskevych, Adrian Wolny, Julian Hennies, Giula\n  Mizzon, Marion Louveaux, Jacob Musser, Alexis Maizel, Detlev Arendt, Anna\n  Kreshuk", "title": "Leveraging Domain Knowledge to Improve Microscopy Image Segmentation\n  with Lifted Multicuts", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The throughput of electron microscopes has increased significantly in recent\nyears, enabling detailed analysis of cell morphology and ultrastructure.\nAnalysis of neural circuits at single-synapse resolution remains the flagship\ntarget of this technique, but applications to cell and developmental biology\nare also starting to emerge at scale. The amount of data acquired in such\nstudies makes manual instance segmentation, a fundamental step in many analysis\npipelines, impossible. While automatic segmentation approaches have improved\nsignificantly thanks to the adoption of convolutional neural networks, their\naccuracy still lags behind human annotations and requires additional manual\nproof-reading. A major hindrance to further improvements is the limited field\nof view of the segmentation networks preventing them from exploiting the\nexpected cell morphology or other prior biological knowledge which humans use\nto inform their segmentation decisions. In this contribution, we show how such\ndomain-specific information can be leveraged by expressing it as long-range\ninteractions in a graph partitioning problem known as the lifted multicut\nproblem. Using this formulation, we demonstrate significant improvement in\nsegmentation accuracy for three challenging EM segmentation problems from\nneuroscience and cell biology.\n", "versions": [{"version": "v1", "created": "Sat, 25 May 2019 06:54:18 GMT"}, {"version": "v2", "created": "Mon, 5 Aug 2019 21:56:22 GMT"}], "update_date": "2019-08-07", "authors_parsed": [["Pape", "Constantin", ""], ["Matskevych", "Alex", ""], ["Wolny", "Adrian", ""], ["Hennies", "Julian", ""], ["Mizzon", "Giula", ""], ["Louveaux", "Marion", ""], ["Musser", "Jacob", ""], ["Maizel", "Alexis", ""], ["Arendt", "Detlev", ""], ["Kreshuk", "Anna", ""]]}, {"id": "1905.10548", "submitter": "Zhenzhou Wang", "authors": "Zhenzhou Wang", "title": "A New Clustering Method Based on Morphological Operations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the booming development of data science, many clustering methods have\nbeen proposed. All clustering methods have inherent merits and deficiencies.\nTherefore, they are only capable of clustering some specific types of data\nrobustly. In addition, the accuracies of the clustering methods rely heavily on\nthe characteristics of the data. In this paper, we propose a new clustering\nmethod based on the morphological operations. The morphological dilation is\nused to connect the data points based on their adjacency and form different\nconnected domains. The iteration of the morphological dilation process stops\nwhen the number of connected domains equals the number of the clusters or when\nthe maximum number of iteration is reached. The morphological dilation is then\nused to label the connected domains. The Euclidean distance between each data\npoint and the points in each labeled connected domain is calculated. For each\ndata point, there is a labeled connected domain that contains a point that\nyields the smallest Euclidean distance. The data point is assigned with the\nsame labeling number as the labeled connected domain. We evaluate and compare\nthe proposed method with state of the art clustering methods with different\ntypes of data. Experimental results show that the proposed method is more\nrobust and generic for clustering two-dimensional or three-dimensional data.\n", "versions": [{"version": "v1", "created": "Sat, 25 May 2019 07:40:41 GMT"}], "update_date": "2019-05-28", "authors_parsed": [["Wang", "Zhenzhou", ""]]}, {"id": "1905.10550", "submitter": "Evgeny Burnaev", "authors": "Marina Pominova and Anna Kuzina and Ekaterina Kondrateva and Svetlana\n  Sushchinskaya and Maxim Sharaev and Evgeny Burnaev and and Vyacheslav Yarkin", "title": "Ensemble of 3D CNN regressors with data fusion for fluid intelligence\n  prediction", "comments": "10 pages, 1 figure, 2 tables", "journal-ref": "ABCD Neurocognitive Prediction Challenge, Springer LNCS, 2019", "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we aim at predicting children's fluid intelligence scores based\non structural T1-weighted MR images from the largest long-term study of brain\ndevelopment and child health. The target variable was regressed on a data\ncollection site, socio-demographic variables and brain volume, thus being\nindependent to the potentially informative factors, which are not directly\nrelated to the brain functioning. We investigate both feature extraction and\ndeep learning approaches as well as different deep CNN architectures and their\nensembles. We propose an advanced architecture of VoxCNNs ensemble, which yield\nMSE (92.838) on blind test.\n", "versions": [{"version": "v1", "created": "Sat, 25 May 2019 07:54:56 GMT"}], "update_date": "2019-05-28", "authors_parsed": [["Pominova", "Marina", ""], ["Kuzina", "Anna", ""], ["Kondrateva", "Ekaterina", ""], ["Sushchinskaya", "Svetlana", ""], ["Sharaev", "Maxim", ""], ["Burnaev", "Evgeny", ""], ["Yarkin", "and Vyacheslav", ""]]}, {"id": "1905.10564", "submitter": "Zhao Zhang", "authors": "Zhao Zhang, Yan Zhang, Sheng Li, Guangcan Liu, Meng Wang, Shuicheng\n  Yan", "title": "Robust Unsupervised Flexible Auto-weighted Local-Coordinate Concept\n  Factorization for Image Clustering", "comments": "Accepted at the 44th IEEE International Conference on Acoustics,\n  Speech, and Signal Processing(ICASSP 2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the high-dimensional data clustering problem by proposing a\nnovel and unsupervised representation learning model called Robust Flexible\nAuto-weighted Local-coordinate Concept Factorization (RFA-LCF). RFA-LCF\nintegrates the robust flexible CF, robust sparse local-coordinate coding and\nthe adaptive reconstruction weighting learning into a unified model. The\nadaptive weighting is driven by including the joint manifold preserving\nconstraints on the recovered clean data, basis concepts and new representation.\nSpecifically, our RFA-LCF uses a L2,1-norm based flexible residue to encode the\nmismatch between clean data and its reconstruction, and also applies the robust\nadaptive sparse local-coordinate coding to represent the data using a few\nnearby basis concepts, which can make the factorization more accurate and\nrobust to noise. The robust flexible factorization is also performed in the\nrecovered clean data space for enhancing representations. RFA-LCF also\nconsiders preserving the local manifold structures of clean data space, basis\nconcept space and the new coordinate space jointly in an adaptive manner way.\nExtensive comparisons show that RFA-LCF can deliver enhanced clustering\nresults.\n", "versions": [{"version": "v1", "created": "Sat, 25 May 2019 10:02:08 GMT"}], "update_date": "2019-05-28", "authors_parsed": [["Zhang", "Zhao", ""], ["Zhang", "Yan", ""], ["Li", "Sheng", ""], ["Liu", "Guangcan", ""], ["Wang", "Meng", ""], ["Yan", "Shuicheng", ""]]}, {"id": "1905.10568", "submitter": "Zhao Zhang", "authors": "Zhao Zhang, Weiming Jiang, Zheng Zhang, Sheng Li, Guangcan Liu and Jie\n  Qin", "title": "Scalable Block-Diagonal Locality-Constrained Projective Dictionary\n  Learning", "comments": "Accepted at the 28th International Joint Conference on Artificial\n  Intelligence(IJCAI 2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel structured discriminative block-diagonal dictionary\nlearning method, referred to as scalable Locality-Constrained Projective\nDictionary Learning (LC-PDL), for efficient representation and classification.\nTo improve the scalability by saving both training and testing time, our LC-PDL\naims at learning a structured discriminative dictionary and a block-diagonal\nrepresentation without using costly l0/l1-norm. Besides, it avoids extra\ntime-consuming sparse reconstruction process with the well-trained dictionary\nfor new sample as many existing models. More importantly, LC-PDL avoids using\nthe complementary data matrix to learn the sub-dictionary over each class. To\nenhance the performance, we incorporate a locality constraint of atoms into the\nDL procedures to keep local information and obtain the codes of samples over\neach class separately. A block-diagonal discriminative approximation term is\nalso derived to learn a discriminative projection to bridge data with their\ncodes by extracting the special block-diagonal features from data, which can\nensure the approximate coefficients to associate with its label information\nclearly. Then, a robust multiclass classifier is trained over extracted\nblock-diagonal codes for accurate label predictions. Experimental results\nverify the effectiveness of our algorithm.\n", "versions": [{"version": "v1", "created": "Sat, 25 May 2019 10:36:53 GMT"}], "update_date": "2019-05-28", "authors_parsed": [["Zhang", "Zhao", ""], ["Jiang", "Weiming", ""], ["Zhang", "Zheng", ""], ["Li", "Sheng", ""], ["Liu", "Guangcan", ""], ["Qin", "Jie", ""]]}, {"id": "1905.10572", "submitter": "Zhao Zhang", "authors": "Zhao Zhang, Yan Zhang, Guangcan Liu, Jinhui Tang, Shuicheng Yan and\n  Meng Wang", "title": "Joint Label Prediction based Semi-Supervised Adaptive Concept\n  Factorization for Robust Data Representation", "comments": "Accepted at IEEE TKDE", "journal-ref": "DOI: 10.1109/TKDE.2019.2893956", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Constrained Concept Factorization (CCF) yields the enhanced representation\nability over CF by incorporating label information as additional constraints,\nbut it cannot classify and group unlabeled data appropriately. Minimizing the\ndifference between the original data and its reconstruction directly can enable\nCCF to model a small noisy perturbation, but is not robust to gross sparse\nerrors. Besides, CCF cannot preserve the manifold structures in new\nrepresentation space explicitly, especially in an adaptive manner. In this\npaper, we propose a joint label prediction based Robust Semi-Supervised\nAdaptive Concept Factorization (RS2ACF) framework. To obtain robust\nrepresentation, RS2ACF relaxes the factorization to make it simultaneously\nstable to small entrywise noise and robust to sparse errors. To enrich prior\nknowledge to enhance the discrimination, RS2ACF clearly uses class information\nof labeled data and more importantly propagates it to unlabeled data by jointly\nlearning an explicit label indicator for unlabeled data. By the label\nindicator, RS2ACF can ensure the unlabeled data of the same predicted label to\nbe mapped into the same class in feature space. Besides, RS2ACF incorporates\nthe joint neighborhood reconstruction error over the new representations and\npredicted labels of both labeled and unlabeled data, so the manifold structures\ncan be preserved explicitly and adaptively in the representation space and\nlabel space at the same time. Owing to the adaptive manner, the tricky process\nof determining the neighborhood size or kernel width can be avoided. Extensive\nresults on public databases verify that our RS2ACF can deliver state-of-the-art\ndata representation, compared with other related methods.\n", "versions": [{"version": "v1", "created": "Sat, 25 May 2019 11:18:45 GMT"}], "update_date": "2019-05-29", "authors_parsed": [["Zhang", "Zhao", ""], ["Zhang", "Yan", ""], ["Liu", "Guangcan", ""], ["Tang", "Jinhui", ""], ["Yan", "Shuicheng", ""], ["Wang", "Meng", ""]]}, {"id": "1905.10575", "submitter": "Zhaohong Deng", "authors": "Xiang Ma, Zhaohong Deng, Peng Xu, Kup-Sze Choi, Dongrui Wu, Shitong\n  Wang", "title": "Deep Image Feature Learning with Fuzzy Rules", "comments": "Submitted to IEEE TETCI", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The methods of extracting image features are the key to many image processing\ntasks. At present, the most popular method is the deep neural network which can\nautomatically extract robust features through end-to-end training instead of\nhand-crafted feature extraction. However, the deep neural network currently\nfaces many challenges: 1) its effectiveness is heavily dependent on large\ndatasets, so the computational complexity is very high; 2) it is usually\nregarded as a black box model with poor interpretability. To meet the above\nchallenges, a more interpretable and scalable feature learning method, i.e.,\ndeep image feature learning with fuzzy rules (DIFL-FR), is proposed in the\npaper, which combines the rule-based fuzzy modeling technique and the deep\nstacked learning strategy. The method progressively learns image features\nthrough a layer-by-layer manner based on fuzzy rules, so the feature learning\nprocess can be better explained by the generated rules. More importantly, the\nlearning process of the method is only based on forward propagation without\nback propagation and iterative learning, which results in the high learning\nefficiency. In addition, the method is under the settings of unsupervised\nlearning and can be easily extended to scenes of supervised and semi-supervised\nlearning. Extensive experiments are conducted on image datasets of different\nscales. The results obviously show the effectiveness of the proposed method.\n", "versions": [{"version": "v1", "created": "Sat, 25 May 2019 11:33:02 GMT"}, {"version": "v2", "created": "Fri, 1 Nov 2019 00:03:26 GMT"}], "update_date": "2019-11-04", "authors_parsed": [["Ma", "Xiang", ""], ["Deng", "Zhaohong", ""], ["Xu", "Peng", ""], ["Choi", "Kup-Sze", ""], ["Wu", "Dongrui", ""], ["Wang", "Shitong", ""]]}, {"id": "1905.10576", "submitter": "Michael Gygli", "authors": "Michael Gygli and Vittorio Ferrari", "title": "Efficient Object Annotation via Speaking and Pointing", "comments": "this article is an extension of arXiv:1811.09461, which was published\n  at CVPR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks deliver state-of-the-art visual recognition, but they\nrely on large datasets, which are time-consuming to annotate. These datasets\nare typically annotated in two stages: (1) determining the presence of object\nclasses at the image level and (2) marking the spatial extent for all objects\nof these classes. In this work we use speech, together with mouse inputs, to\nspeed up this process. We first improve stage one, by letting annotators\nindicate object class presence via speech. We then combine the two stages:\nannotators draw an object bounding box via the mouse and simultaneously provide\nits class label via speech. Using speech has distinct advantages over relying\non mouse inputs alone. First, it is fast and allows for direct access to the\nclass name, by simply saying it. Second, annotators can simultaneously speak\nand mark an object location. Finally, speech-based interfaces can be kept\nextremely simple, hence using them requires less mouse movement compared to\nexisting approaches. Through extensive experiments on the COCO and ILSVRC\ndatasets we show that our approach yields high-quality annotations at\nsignificant speed gains. Stage one takes 2.3x - 14.9x less annotation time than\nexisting methods based on a hierarchical organization of the classes to be\nannotated. Moreover, when combining the two stages, we find that object class\nlabels come for free: annotating them at the same time as bounding boxes has\nzero additional cost. On COCO, this makes the overall process 1.9x faster than\nthe two-stage approach.\n", "versions": [{"version": "v1", "created": "Sat, 25 May 2019 11:36:16 GMT"}, {"version": "v2", "created": "Mon, 14 Oct 2019 12:44:38 GMT"}, {"version": "v3", "created": "Thu, 31 Oct 2019 16:50:25 GMT"}, {"version": "v4", "created": "Thu, 19 Dec 2019 12:57:30 GMT"}], "update_date": "2019-12-20", "authors_parsed": [["Gygli", "Michael", ""], ["Ferrari", "Vittorio", ""]]}, {"id": "1905.10595", "submitter": "Honey Gupta", "authors": "Honey Gupta and Kaushik Mitra", "title": "Unsupervised Single Image Underwater Depth Estimation", "comments": "Accepted for publication at IEEE International Conference on Image\n  Processing (ICIP), 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Depth estimation from a single underwater image is one of the most\nchallenging problems and is highly ill-posed. Due to the absence of large\ngeneralized underwater depth datasets and the difficulty in obtaining ground\ntruth depth-maps, supervised learning techniques such as direct depth\nregression cannot be used. In this paper, we propose an unsupervised method for\ndepth estimation from a single underwater image taken `in the wild' by using\nhaze as a cue for depth. Our approach is based on indirect depth-map estimation\nwhere we learn the mapping functions between unpaired RGB-D terrestrial images\nand arbitrary underwater images to estimate the required depth-map. We propose\na method which is based on the principles of cycle-consistent learning and uses\ndense-block based auto-encoders as generator networks. We evaluate and compare\nour method both quantitatively and qualitatively on various underwater images\nwith diverse attenuation and scattering conditions and show that our method\nproduces state-of-the-art results for unsupervised depth estimation from a\nsingle underwater image.\n", "versions": [{"version": "v1", "created": "Sat, 25 May 2019 13:26:44 GMT"}, {"version": "v2", "created": "Tue, 28 May 2019 07:41:52 GMT"}], "update_date": "2019-05-29", "authors_parsed": [["Gupta", "Honey", ""], ["Mitra", "Kaushik", ""]]}, {"id": "1905.10598", "submitter": "Aritra Dutta", "authors": "Aritra Dutta, Filip Hanzely, Jingwei Liang, Peter Richt\\'arik", "title": "Best Pair Formulation & Accelerated Scheme for Non-convex Principal\n  Component Pursuit", "comments": null, "journal-ref": null, "doi": "10.1109/TSP.2020.3011024", "report-no": null, "categories": "math.OC cs.CV cs.NA math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The best pair problem aims to find a pair of points that minimize the\ndistance between two disjoint sets. In this paper, we formulate the classical\nrobust principal component analysis (RPCA) as the best pair; which was not\nconsidered before. We design an accelerated proximal gradient scheme to solve\nit, for which we show global convergence, as well as the local linear rate. Our\nextensive numerical experiments on both real and synthetic data suggest that\nthe algorithm outperforms relevant baseline algorithms in the literature.\n", "versions": [{"version": "v1", "created": "Sat, 25 May 2019 13:59:03 GMT"}, {"version": "v2", "created": "Tue, 28 May 2019 09:24:16 GMT"}], "update_date": "2020-12-02", "authors_parsed": [["Dutta", "Aritra", ""], ["Hanzely", "Filip", ""], ["Liang", "Jingwei", ""], ["Richt\u00e1rik", "Peter", ""]]}, {"id": "1905.10604", "submitter": "Rita Singh", "authors": "Yandong Wen and Rita Singh and Bhiksha Raj", "title": "Reconstructing faces from voices", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.CV cs.LG eess.AS eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Voice profiling aims at inferring various human parameters from their speech,\ne.g. gender, age, etc. In this paper, we address the challenge posed by a\nsubtask of voice profiling - reconstructing someone's face from their voice.\nThe task is designed to answer the question: given an audio clip spoken by an\nunseen person, can we picture a face that has as many common elements, or\nassociations as possible with the speaker, in terms of identity? To address\nthis problem, we propose a simple but effective computational framework based\non generative adversarial networks (GANs). The network learns to generate faces\nfrom voices by matching the identities of generated faces to those of the\nspeakers, on a training set. We evaluate the performance of the network by\nleveraging a closely related task - cross-modal matching. The results show that\nour model is able to generate faces that match several biometric\ncharacteristics of the speaker, and results in matching accuracies that are\nmuch better than chance.\n", "versions": [{"version": "v1", "created": "Sat, 25 May 2019 14:33:59 GMT"}, {"version": "v2", "created": "Fri, 31 May 2019 19:53:01 GMT"}], "update_date": "2019-06-04", "authors_parsed": [["Wen", "Yandong", ""], ["Singh", "Rita", ""], ["Raj", "Bhiksha", ""]]}, {"id": "1905.10608", "submitter": "Tingting Xie", "authors": "Tingting Xie, Xiaoshan Yang, Tianzhu Zhang, Changsheng Xu, Ioannis\n  Patras", "title": "Exploring Feature Representation and Training strategies in Temporal\n  Action Localization", "comments": "ICIP19 Camera Ready", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Temporal action localization has recently attracted significant interest in\nthe Computer Vision community. However, despite the great progress, it is hard\nto identify which aspects of the proposed methods contribute most to the\nincrease in localization performance. To address this issue, we conduct\nablative experiments on feature extraction methods, fixed-size feature\nrepresentation methods and training strategies, and report how each influences\nthe overall performance. Based on our findings, we propose a two-stage detector\nthat outperforms the state of the art in THUMOS14, achieving a mAP@tIoU=0.5\nequal to 44.2%.\n", "versions": [{"version": "v1", "created": "Sat, 25 May 2019 14:55:48 GMT"}, {"version": "v2", "created": "Wed, 29 May 2019 17:13:46 GMT"}], "update_date": "2019-05-30", "authors_parsed": [["Xie", "Tingting", ""], ["Yang", "Xiaoshan", ""], ["Zhang", "Tianzhu", ""], ["Xu", "Changsheng", ""], ["Patras", "Ioannis", ""]]}, {"id": "1905.10620", "submitter": "Chi Nhan Duong", "authors": "Chi Nhan Duong, Khoa Luu, Kha Gia Quach, Ngan Le", "title": "ShrinkTeaNet: Million-scale Lightweight Face Recognition via Shrinking\n  Teacher-Student Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large-scale face recognition in-the-wild has been recently achieved matured\nperformance in many real work applications. However, such systems are built on\nGPU platforms and mostly deploy heavy deep network architectures. Given a\nhigh-performance heavy network as a teacher, this work presents a simple and\nelegant teacher-student learning paradigm, namely ShrinkTeaNet, to train a\nportable student network that has significantly fewer parameters and\ncompetitive accuracy against the teacher network. Far apart from prior\nteacher-student frameworks mainly focusing on accuracy and compression ratios\nin closed-set problems, our proposed teacher-student network is proved to be\nmore robust against open-set problem, i.e. large-scale face recognition. In\naddition, this work introduces a novel Angular Distillation Loss for distilling\nthe feature direction and the sample distributions of the teacher's hypersphere\nto its student. Then ShrinkTeaNet framework can efficiently guide the student's\nlearning process with the teacher's knowledge presented in both intermediate\nand last stages of the feature embedding. Evaluations on LFW, CFP-FP, AgeDB,\nIJB-B and IJB-C Janus, and MegaFace with one million distractors have\ndemonstrated the efficiency of the proposed approach to learn robust student\nnetworks which have satisfying accuracy and compact sizes. Our ShrinkTeaNet is\nable to support the light-weight architecture achieving high performance with\n99.77% on LFW and 95.64% on large-scale Megaface protocols.\n", "versions": [{"version": "v1", "created": "Sat, 25 May 2019 15:44:40 GMT"}], "update_date": "2019-05-28", "authors_parsed": [["Duong", "Chi Nhan", ""], ["Luu", "Khoa", ""], ["Quach", "Kha Gia", ""], ["Le", "Ngan", ""]]}, {"id": "1905.10622", "submitter": "Suman Ghosh", "authors": "Arka Ujjal Dey, Suman Kumar Ghosh, Ernest Valveny, Gaurav Harit", "title": "Beyond Visual Semantics: Exploring the Role of Scene Text in Image\n  Understanding", "comments": "The paper is under consideration at Pattern Recognition Letters", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Images with visual and scene text content are ubiquitous in everyday life.\nHowever, current image interpretation systems are mostly limited to using only\nthe visual features, neglecting to leverage the scene text content. In this\npaper, we propose to jointly use scene text and visual channels for robust\nsemantic interpretation of images. We do not only extract and encode visual and\nscene text cues, but also model their interplay to generate a contextual joint\nembedding with richer semantics. The contextual embedding thus generated is\napplied to retrieval and classification tasks on multimedia images, with scene\ntext content, to demonstrate its effectiveness. In the retrieval framework, we\naugment our learned text-visual semantic representation with scene text cues,\nto mitigate vocabulary misses that may have occurred during the semantic\nembedding. To deal with irrelevant or erroneous recognition of scene text, we\nalso apply query-based attention to our text channel. We show how the\nmulti-channel approach, involving visual semantics and scene text, improves\nupon state of the art.\n", "versions": [{"version": "v1", "created": "Sat, 25 May 2019 15:53:14 GMT"}, {"version": "v2", "created": "Mon, 24 Jun 2019 09:47:57 GMT"}, {"version": "v3", "created": "Wed, 4 Dec 2019 11:17:25 GMT"}], "update_date": "2019-12-05", "authors_parsed": [["Dey", "Arka Ujjal", ""], ["Ghosh", "Suman Kumar", ""], ["Valveny", "Ernest", ""], ["Harit", "Gaurav", ""]]}, {"id": "1905.10628", "submitter": "Engkarat Techapanurak", "authors": "Engkarat Techapanurak and Masanori Suganuma and Takayuki Okatani", "title": "Hyperparameter-Free Out-of-Distribution Detection Using Softmax of\n  Scaled Cosine Similarity", "comments": "Extend the supplementary material", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ability to detect out-of-distribution (OOD) samples is vital to secure\nthe reliability of deep neural networks in real-world applications. Considering\nthe nature of OOD samples, detection methods should not have hyperparameters\nthat need to be tuned depending on incoming OOD samples. However, most of the\nrecently proposed methods do not meet this requirement, leading to compromised\nperformance in real-world applications. In this paper, we propose a simple,\nhyperparameter-free method based on softmax of scaled cosine similarity. It\nresembles the approach employed by modern metric learning methods, but it\ndiffers in details; the differences are essential to achieve high detection\nperformance. We show through experiments that our method outperforms the\nexisting methods on the evaluation test recently proposed by Shafaei et al.,\nwhich takes the above issue of hyperparameter dependency into account. We also\nshow that it achieves at least comparable performance to other methods on the\nconventional test, where their hyperparameters are chosen using explicit OOD\nsamples. Furthermore, it is computationally more efficient than most of the\nprevious methods, since it needs only a single forward pass.\n", "versions": [{"version": "v1", "created": "Sat, 25 May 2019 16:33:38 GMT"}, {"version": "v2", "created": "Wed, 20 Nov 2019 08:18:11 GMT"}, {"version": "v3", "created": "Tue, 26 Nov 2019 03:55:52 GMT"}], "update_date": "2019-11-27", "authors_parsed": [["Techapanurak", "Engkarat", ""], ["Suganuma", "Masanori", ""], ["Okatani", "Takayuki", ""]]}, {"id": "1905.10629", "submitter": "Jonathan Vacher", "authors": "Jonathan Vacher, Claire Launay and Ruben Coen-Cagli", "title": "Flexibly Regularized Mixture Models and Application to Image\n  Segmentation", "comments": "33 pages ( 30 + 3 for appendix). 11 figures + 1 in appendix", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG q-bio.NC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Probabilistic finite mixture models are widely used for unsupervised\nclustering. These models can often be improved by adapting them to the topology\nof the data. For instance, in order to classify spatially adjacent data points\nsimilarly, it is common to introduce a Laplacian constraint on the posterior\nprobability that each data point belongs to a class. Alternatively, the mixing\nprobabilities can be treated as free parameters, while assuming Gauss-Markov or\nmore complex priors to regularize those mixing probabilities. However, these\napproaches are constrained by the shape of the prior and often lead to\ncomplicated or intractable inference. Here, we propose a new parametrization of\nthe Dirichlet distribution to flexibly regularize the mixing probabilities of\nover-parametrized mixture distributions. Using the Expectation-Maximization\nalgorithm, we show that our approach allows us to define any linear update rule\nfor the mixing probabilities, including spatial smoothing regularization as a\nspecial case. We then show that this flexible design can be extended to share\nclass information between multiple mixture models. We apply our algorithm to\nartificial and natural image segmentation tasks, and we provide quantitative\nand qualitative comparison of the performance of Gaussian and Student-t\nmixtures on the Berkeley Segmentation Dataset. We also demonstrate how to\npropagate class information across the layers of deep convolutional neural\nnetworks in a probabilistically optimal way, suggesting a new interpretation\nfor feedback signals in biological visual systems. Our flexible approach can be\neasily generalized to adapt probabilistic mixture models to arbitrary data\ntopologies.\n", "versions": [{"version": "v1", "created": "Sat, 25 May 2019 16:55:19 GMT"}, {"version": "v2", "created": "Thu, 8 Jul 2021 08:48:58 GMT"}], "update_date": "2021-07-09", "authors_parsed": [["Vacher", "Jonathan", ""], ["Launay", "Claire", ""], ["Coen-Cagli", "Ruben", ""]]}, {"id": "1905.10654", "submitter": "Yi Zhu", "authors": "Yi Zhu", "title": "Exploring Temporal Information for Improved Video Understanding", "comments": "PhD dissertation. Posting here for easier access and update. Chapter\n  4, 6 and 7 are collaborative work", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this dissertation, I present my work towards exploring temporal\ninformation for better video understanding. Specifically, I have worked on two\nproblems: action recognition and semantic segmentation. For action recognition,\nI have proposed a framework, termed hidden two-stream networks, to learn an\noptimal motion representation that does not require the computation of optical\nflow. My framework alleviates several challenges faced in video classification,\nsuch as learning motion representations, real-time inference, multi-framerate\nhandling, generalizability to unseen actions, etc. For semantic segmentation, I\nhave introduced a general framework that uses video prediction models to\nsynthesize new training samples. By scaling up the training dataset, my trained\nmodels are more accurate and robust than previous models even without\nmodifications to the network architectures or objective functions. I believe\nvideos have much more potential to be mined, and temporal information is one of\nthe most important cues for machines to perceive the visual world better.\n", "versions": [{"version": "v1", "created": "Sat, 25 May 2019 18:53:19 GMT"}], "update_date": "2019-05-28", "authors_parsed": [["Zhu", "Yi", ""]]}, {"id": "1905.10671", "submitter": "Zhongzhan Huang", "authors": "Zhongzhan Huang, Senwei Liang, Mingfu Liang, Haizhao Yang", "title": "DIANet: Dense-and-Implicit Attention Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Attention networks have successfully boosted the performance in various\nvision problems. Previous works lay emphasis on designing a new attention\nmodule and individually plug them into the networks. Our paper proposes a\nnovel-and-simple framework that shares an attention module throughout different\nnetwork layers to encourage the integration of layer-wise information and this\nparameter-sharing module is referred as Dense-and-Implicit-Attention (DIA)\nunit. Many choices of modules can be used in the DIA unit. Since Long Short\nTerm Memory (LSTM) has a capacity of capturing long-distance dependency, we\nfocus on the case when the DIA unit is the modified LSTM (refer as DIA-LSTM).\nExperiments on benchmark datasets show that the DIA-LSTM unit is capable of\nemphasizing layer-wise feature interrelation and leads to significant\nimprovement of image classification accuracy. We further empirically show that\nthe DIA-LSTM has a strong regularization ability on stabilizing the training of\ndeep networks by the experiments with the removal of skip connections or Batch\nNormalization in the whole residual network. The code is released at\nhttps://github.com/gbup-group/DIANet.\n", "versions": [{"version": "v1", "created": "Sat, 25 May 2019 20:51:07 GMT"}, {"version": "v2", "created": "Mon, 23 Sep 2019 08:23:50 GMT"}], "update_date": "2019-09-24", "authors_parsed": [["Huang", "Zhongzhan", ""], ["Liang", "Senwei", ""], ["Liang", "Mingfu", ""], ["Yang", "Haizhao", ""]]}, {"id": "1905.10675", "submitter": "Artzai Picon", "authors": "Alfonso Medela and Artzai Picon", "title": "Constellation Loss: Improving the efficiency of deep metric learning\n  loss functions for optimal embedding", "comments": "Submitted to NeurIPS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Metric learning has become an attractive field for research on the latest\nyears. Loss functions like contrastive loss, triplet loss or multi-class N-pair\nloss have made possible generating models capable of tackling complex scenarios\nwith the presence of many classes and scarcity on the number of images per\nclass not only work to build classifiers, but to many other applications where\nmeasuring similarity is the key. Deep Neural Networks trained via metric\nlearning also offer the possibility to solve few-shot learning problems.\nCurrently used state of the art loss functions such as triplet and contrastive\nloss functions, still suffer from slow convergence due to the selection of\neffective training samples that has been partially solved by the multi-class\nN-pair loss by simultaneously adding additional samples from the different\nclasses. In this work, we extend triplet and multiclass-N-pair loss function by\nproposing the constellation loss metric where the distances among all class\ncombinations are simultaneously learned. We have compared our constellation\nloss for visual class embedding showing that our loss function over-performs\nthe other methods by obtaining more compact clusters while achieving better\nclassification results.\n", "versions": [{"version": "v1", "created": "Sat, 25 May 2019 21:16:06 GMT"}], "update_date": "2019-05-28", "authors_parsed": [["Medela", "Alfonso", ""], ["Picon", "Artzai", ""]]}, {"id": "1905.10679", "submitter": "Callie Federer", "authors": "Callie Federer and Haoyan Xu and Alona Fyshe and Joel Zylberberg", "title": "Improved object recognition using neural networks trained to mimic the\n  brain's statistical properties", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The current state-of-the-art object recognition algorithms, deep\nconvolutional neural networks (DCNNs), are inspired by the architecture of the\nmammalian visual system, and are capable of human-level performance on many\ntasks. However, even these algorithms make errors. As they are trained for\nobject recognition tasks, it has been shown that DCNNs develop hidden\nrepresentations that resemble those observed in the mammalian visual system.\nMoreover, DCNNs trained on object recognition tasks are currently among the\nbest models we have of the mammalian visual system. This led us to hypothesize\nthat teaching DCNNs to achieve even more brain-like representations could\nimprove their performance. To test this, we trained DCNNs on a composite task,\nwherein networks were trained to: a) classify images of objects; while b)\nhaving intermediate representations that resemble those observed in neural\nrecordings from monkey visual cortex. Compared with DCNNs trained purely for\nobject categorization, DCNNs trained on the composite task had better object\nrecognition performance and are more robust to label corruption. Interestingly,\nwe also found that neural data was not required, but randomized data with the\nsame statistics as neural data also boosted performance. Our results outline a\nnew way to train object recognition networks, using strategies in which the\nbrain - or at least the statistical properties of its activation patterns -\nserves as a teacher signal for training DCNNs.\n", "versions": [{"version": "v1", "created": "Sat, 25 May 2019 21:35:58 GMT"}, {"version": "v2", "created": "Fri, 22 Nov 2019 23:39:03 GMT"}, {"version": "v3", "created": "Mon, 6 Apr 2020 18:55:56 GMT"}, {"version": "v4", "created": "Wed, 15 Jul 2020 18:25:16 GMT"}], "update_date": "2020-07-17", "authors_parsed": [["Federer", "Callie", ""], ["Xu", "Haoyan", ""], ["Fyshe", "Alona", ""], ["Zylberberg", "Joel", ""]]}, {"id": "1905.10693", "submitter": "Hamed R. Tavakoli", "authors": "Hamed R. Tavakoli, Ali Borji, Esa Rahtu, Juho Kannala", "title": "DAVE: A Deep Audio-Visual Embedding for Dynamic Saliency Prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies audio-visual deep saliency prediction. It introduces a\nconceptually simple and effective Deep Audio-Visual Embedding for dynamic\nsaliency prediction dubbed ``DAVE\" in conjunction with our efforts towards\nbuilding an Audio-Visual Eye-tracking corpus named ``AVE\". Despite existing a\nstrong relation between auditory and visual cues for guiding gaze during\nperception, video saliency models only consider visual cues and neglect the\nauditory information that is ubiquitous in dynamic scenes. Here, we investigate\nthe applicability of audio cues in conjunction with visual ones in predicting\nsaliency maps using deep neural networks. To this end, the proposed model is\nintentionally designed to be simple. Two baseline models are developed on the\nsame architecture which consists of an encoder-decoder. The encoder projects\nthe input into a feature space followed by a decoder that infers saliency. We\nconduct an extensive analysis on different modalities and various aspects of\nmulti-model dynamic saliency prediction. Our results suggest that (1) audio is\na strong contributing cue for saliency prediction, (2) salient visible\nsound-source is the natural cause of the superiority of our Audio-Visual model,\n(3) richer feature representations for the input space leads to more powerful\npredictions even in absence of more sophisticated saliency decoders, and (4)\nAudio-Visual model improves over 53.54\\% of the frames predicted by the best\nVisual model (our baseline). Our endeavour demonstrates that audio is an\nimportant cue that boosts dynamic video saliency prediction and helps models to\napproach human performance. The code is available at\nhttps://github.com/hrtavakoli/DAVE\n", "versions": [{"version": "v1", "created": "Sat, 25 May 2019 23:16:57 GMT"}, {"version": "v2", "created": "Tue, 7 Jan 2020 21:52:07 GMT"}], "update_date": "2020-01-09", "authors_parsed": [["Tavakoli", "Hamed R.", ""], ["Borji", "Ali", ""], ["Rahtu", "Esa", ""], ["Kannala", "Juho", ""]]}, {"id": "1905.10695", "submitter": "Tianfu Wu", "authors": "Zekun Zhang and Tianfu Wu", "title": "Adversarial Distillation for Ordered Top-k Attacks", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Neural Networks (DNNs) are vulnerable to adversarial attacks, especially\nwhite-box targeted attacks. One scheme of learning attacks is to design a\nproper adversarial objective function that leads to the imperceptible\nperturbation for any test image (e.g., the Carlini-Wagner (C&W) method). Most\nmethods address targeted attacks in the Top-1 manner. In this paper, we propose\nto learn ordered Top-k attacks (k>= 1) for image classification tasks, that is\nto enforce the Top-k predicted labels of an adversarial example to be the k\n(randomly) selected and ordered labels (the ground-truth label is exclusive).\nTo this end, we present an adversarial distillation framework: First, we\ncompute an adversarial probability distribution for any given ordered Top-k\ntargeted labels with respect to the ground-truth of a test image. Then, we\nlearn adversarial examples by minimizing the Kullback-Leibler (KL) divergence\ntogether with the perturbation energy penalty, similar in spirit to the network\ndistillation method. We explore how to leverage label semantic similarities in\ncomputing the targeted distributions, leading to knowledge-oriented attacks. In\nexperiments, we thoroughly test Top-1 and Top-5 attacks in the ImageNet-1000\nvalidation dataset using two popular DNNs trained with clean ImageNet-1000\ntrain dataset, ResNet-50 and DenseNet-121. For both models, our proposed\nadversarial distillation approach outperforms the C&W method in the Top-1\nsetting, as well as other baseline methods. Our approach shows significant\nimprovement in the Top-5 setting against a strong modified C&W method.\n", "versions": [{"version": "v1", "created": "Sat, 25 May 2019 23:24:15 GMT"}], "update_date": "2019-05-28", "authors_parsed": [["Zhang", "Zekun", ""], ["Wu", "Tianfu", ""]]}, {"id": "1905.10698", "submitter": "Farshid Varno", "authors": "Farshid Varno, Behrouz Haji Soleimani, Marzie Saghayi, Lisa Di Jorio\n  and Stan Matwin", "title": "Efficient Neural Task Adaptation by Maximum Entropy Initialization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transferring knowledge from one neural network to another has been shown to\nbe helpful for learning tasks with few training examples. Prevailing\nfine-tuning methods could potentially contaminate pre-trained features by\ncomparably high energy random noise. This noise is mainly delivered from a\ncareless replacement of task-specific parameters. We analyze theoretically such\nknowledge contamination for classification tasks and propose a practical and\neasy to apply method to trap and minimize the contaminant. In our approach, the\nentropy of the output estimates gets maximized initially and the first\nback-propagated error is stalled at the output of the last layer. Our proposed\nmethod not only outperforms the traditional fine-tuning, but also significantly\nspeeds up the convergence of the learner. It is robust to randomness and\nindependent of the choice of architecture. Overall, our experiments show that\nthe power of transfer learning has been substantially underestimated so far.\n", "versions": [{"version": "v1", "created": "Sat, 25 May 2019 23:37:34 GMT"}, {"version": "v2", "created": "Fri, 12 Jul 2019 02:32:53 GMT"}], "update_date": "2019-07-15", "authors_parsed": [["Varno", "Farshid", ""], ["Soleimani", "Behrouz Haji", ""], ["Saghayi", "Marzie", ""], ["Di Jorio", "Lisa", ""], ["Matwin", "Stan", ""]]}, {"id": "1905.10701", "submitter": "Piyush Shrivastava Mr.", "authors": "Aditya Narayanaswamy, Yichuan Philip Ma, Piyush Shrivastava", "title": "Image Detection and Digit Recognition to solve Sudoku as a Constraint\n  Satisfaction Problem", "comments": "Pages: 9", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Sudoku is a puzzle well-known to the scientific community with simple rules\nof completion, which may require a com-plex line of reasoning. This paper\naddresses the problem of partitioning the Sudoku image into a 1-D array,\nrecognizing digits from the array and representing it as a Constraint\nSat-isfaction Problem (CSP). In this paper, we introduce new fea-ture\nextraction techniques for recognizing digits, which are used with our benchmark\nclassifiers in conjunction with the CSP algorithms to provide performance\nassessment. Experi-mental results show that application of CSP techniques can\ndecrease the solution's search time by eliminating incon-sistent values from\nthe search space.\n", "versions": [{"version": "v1", "created": "Sat, 25 May 2019 23:47:08 GMT"}], "update_date": "2019-05-28", "authors_parsed": [["Narayanaswamy", "Aditya", ""], ["Ma", "Yichuan Philip", ""], ["Shrivastava", "Piyush", ""]]}, {"id": "1905.10708", "submitter": "Dmitry Konovalov", "authors": "Dmitry A. Konovalov, Alzayat Saleh, Michael Bradley, Mangalam\n  Sankupellay, Simone Marini, Marcus Sheaves", "title": "Underwater Fish Detection with Weak Multi-Domain Supervision", "comments": "Published in the 2019 International Joint Conference on Neural\n  Networks (IJCNN-2019), Budapest, Hungary, July 14-19, 2019,\n  https://www.ijcnn.org/ , https://ieeexplore.ieee.org/document/8851907", "journal-ref": "2019 International Joint Conference on Neural Networks (IJCNN),\n  Budapest, Hungary, 2019, pp. 1-8", "doi": "10.1109/IJCNN.2019.8851907", "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a sufficiently large training dataset, it is relatively easy to train a\nmodern convolution neural network (CNN) as a required image classifier.\nHowever, for the task of fish classification and/or fish detection, if a CNN\nwas trained to detect or classify particular fish species in particular\nbackground habitats, the same CNN exhibits much lower accuracy when applied to\nnew/unseen fish species and/or fish habitats. Therefore, in practice, the CNN\nneeds to be continuously fine-tuned to improve its classification accuracy to\nhandle new project-specific fish species or habitats. In this work we present a\nlabelling-efficient method of training a CNN-based fish-detector (the Xception\nCNN was used as the base) on relatively small numbers (4,000) of project-domain\nunderwater fish/no-fish images from 20 different habitats. Additionally, 17,000\nof known negative (that is, missing fish) general-domain (VOC2012) above-water\nimages were used. Two publicly available fish-domain datasets supplied\nadditional 27,000 of above-water and underwater positive/fish images. By using\nthis multi-domain collection of images, the trained Xception-based binary\n(fish/not-fish) classifier achieved 0.17% false-positives and 0.61%\nfalse-negatives on the project's 20,000 negative and 16,000 positive holdout\ntest images, respectively. The area under the ROC curve (AUC) was 99.94%.\n", "versions": [{"version": "v1", "created": "Sun, 26 May 2019 01:43:58 GMT"}, {"version": "v2", "created": "Sat, 2 Nov 2019 02:29:12 GMT"}], "update_date": "2019-11-05", "authors_parsed": [["Konovalov", "Dmitry A.", ""], ["Saleh", "Alzayat", ""], ["Bradley", "Michael", ""], ["Sankupellay", "Mangalam", ""], ["Marini", "Simone", ""], ["Sheaves", "Marcus", ""]]}, {"id": "1905.10710", "submitter": "Alexander Tong", "authors": "Alexander Tong, Guy Wolf, Smita Krishnaswamy", "title": "Fixing Bias in Reconstruction-based Anomaly Detection with Lipschitz\n  Discriminators", "comments": "6 pages, 4 figures, 2 tables, presented at IEEE MLSP", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Anomaly detection is of great interest in fields where abnormalities need to\nbe identified and corrected (e.g., medicine and finance). Deep learning methods\nfor this task often rely on autoencoder reconstruction error, sometimes in\nconjunction with other errors. We show that this approach exhibits intrinsic\nbiases that lead to undesirable results. Reconstruction-based methods are\nsensitive to training-data outliers and simple-to-reconstruct points. Instead,\nwe introduce a new unsupervised Lipschitz anomaly discriminator that does not\nsuffer from these biases. Our anomaly discriminator is trained, similar to the\nones used in GANs, to detect the difference between the training data and\ncorruptions of the training data. We show that this procedure successfully\ndetects unseen anomalies with guarantees on those that have a certain\nWasserstein distance from the data or corrupted training set. These additions\nallow us to show improved performance on MNIST, CIFAR10, and health record\ndata.\n", "versions": [{"version": "v1", "created": "Sun, 26 May 2019 01:57:42 GMT"}, {"version": "v2", "created": "Sun, 9 Feb 2020 21:20:55 GMT"}, {"version": "v3", "created": "Sun, 26 Jul 2020 13:49:41 GMT"}], "update_date": "2020-07-28", "authors_parsed": [["Tong", "Alexander", ""], ["Wolf", "Guy", ""], ["Krishnaswamy", "Smita", ""]]}, {"id": "1905.10711", "submitter": "Qiangeng Xu", "authors": "Qiangeng Xu, Weiyue Wang, Duygu Ceylan, Radomir Mech, Ulrich Neumann", "title": "DISN: Deep Implicit Surface Network for High-quality Single-view 3D\n  Reconstruction", "comments": null, "journal-ref": "33rd Annual Conference on Neural Information Processing Systems\n  (NeurIPS 2019)", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reconstructing 3D shapes from single-view images has been a long-standing\nresearch problem. In this paper, we present DISN, a Deep Implicit Surface\nNetwork which can generate a high-quality detail-rich 3D mesh from an 2D image\nby predicting the underlying signed distance fields. In addition to utilizing\nglobal image features, DISN predicts the projected location for each 3D point\non the 2D image, and extracts local features from the image feature maps.\nCombining global and local features significantly improves the accuracy of the\nsigned distance field prediction, especially for the detail-rich areas. To the\nbest of our knowledge, DISN is the first method that constantly captures\ndetails such as holes and thin structures present in 3D shapes from single-view\nimages. DISN achieves the state-of-the-art single-view reconstruction\nperformance on a variety of shape categories reconstructed from both synthetic\nand real images. Code is available at https://github.com/xharlie/DISN The\nsupplementary can be found at\nhttps://xharlie.github.io/images/neurips_2019_supp.pdf\n", "versions": [{"version": "v1", "created": "Sun, 26 May 2019 01:58:28 GMT"}, {"version": "v2", "created": "Thu, 3 Oct 2019 03:32:26 GMT"}], "update_date": "2019-10-04", "authors_parsed": [["Xu", "Qiangeng", ""], ["Wang", "Weiyue", ""], ["Ceylan", "Duygu", ""], ["Mech", "Radomir", ""], ["Neumann", "Ulrich", ""]]}, {"id": "1905.10725", "submitter": "Yueqi Cao", "authors": "Yueqi Cao, Didong Li, Huafei Sun, Amir H Assadi, Shiqiang Zhang", "title": "Efficient Weingarten Map and Curvature Estimation on Manifolds", "comments": "23 pages, 8 figures", "journal-ref": "Machine Learning (2021)", "doi": "10.1007/s10994-021-05953-4", "report-no": null, "categories": "stat.ML cs.CV cs.LG math.DG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose an efficient method to estimate the Weingarten map\nfor point cloud data sampled from manifold embedded in Euclidean space. A\nstatistical model is established to analyze the asymptotic property of the\nestimator. In particular, we show the convergence rate as the sample size tends\nto infinity. We verify the convergence rate through simulated data and apply\nthe estimated Weingarten map to curvature estimation and point cloud\nsimplification to multiple real data sets.\n", "versions": [{"version": "v1", "created": "Sun, 26 May 2019 04:48:32 GMT"}, {"version": "v2", "created": "Mon, 7 Sep 2020 03:51:21 GMT"}], "update_date": "2021-05-17", "authors_parsed": [["Cao", "Yueqi", ""], ["Li", "Didong", ""], ["Sun", "Huafei", ""], ["Assadi", "Amir H", ""], ["Zhang", "Shiqiang", ""]]}, {"id": "1905.10729", "submitter": "Hebi Li", "authors": "Hebi Li and Qi Xiao and Shixin Tian and Jin Tian", "title": "Purifying Adversarial Perturbation with Adversarially Trained\n  Auto-encoders", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning models are vulnerable to adversarial examples. Iterative\nadversarial training has shown promising results against strong white-box\nattacks. However, adversarial training is very expensive, and every time a\nmodel needs to be protected, such expensive training scheme needs to be\nperformed. In this paper, we propose to apply iterative adversarial training\nscheme to an external auto-encoder, which once trained can be used to protect\nother models directly. We empirically show that our model outperforms other\npurifying-based methods against white-box attacks, and transfers well to\ndirectly protect other base models with different architectures.\n", "versions": [{"version": "v1", "created": "Sun, 26 May 2019 04:57:55 GMT"}], "update_date": "2019-05-28", "authors_parsed": [["Li", "Hebi", ""], ["Xiao", "Qi", ""], ["Tian", "Shixin", ""], ["Tian", "Jin", ""]]}, {"id": "1905.10742", "submitter": "Sitao Xiang", "authors": "Sitao Xiang, Hao Li", "title": "Disentangling Style and Content in Anime Illustrations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing methods for AI-generated artworks still struggle with generating\nhigh-quality stylized content, where high-level semantics are preserved, or\nseparating fine-grained styles from various artists. We propose a novel\nGenerative Adversarial Disentanglement Network which can disentangle two\ncomplementary factors of variations when only one of them is labelled in\ngeneral, and fully decompose complex anime illustrations into style and content\nin particular. Training such model is challenging, since given a style, various\ncontent data may exist but not the other way round. Our approach is divided\ninto two stages, one that encodes an input image into a style independent\ncontent, and one based on a dual-conditional generator. We demonstrate the\nability to generate high-fidelity anime portraits with a fixed content and a\nlarge variety of styles from over a thousand artists, and vice versa, using a\nsingle end-to-end network and with applications in style transfer. We show this\nunique capability as well as superior output to the current state-of-the-art.\n", "versions": [{"version": "v1", "created": "Sun, 26 May 2019 06:17:09 GMT"}, {"version": "v2", "created": "Tue, 28 May 2019 04:17:01 GMT"}, {"version": "v3", "created": "Fri, 20 Dec 2019 14:52:26 GMT"}], "update_date": "2019-12-23", "authors_parsed": [["Xiang", "Sitao", ""], ["Li", "Hao", ""]]}, {"id": "1905.10748", "submitter": "Guanyu Cai", "authors": "Guanyu Cai, Yuqin Wang, Lianghua He", "title": "Learning Smooth Representation for Unsupervised Domain Adaptation", "comments": "Code is available at https://github.com/CuthbertCai/SRDA", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In unsupervised domain adaptation, existing methods have achieved remarkable\nperformance, but few pay attention to the Lipschitz constraint. It has been\nstudied that not just reducing the divergence between distributions, but the\nsatisfaction of Lipschitz continuity guarantees an error bound for the target\ndistribution. In this paper, we adopt this principle and extend it to a deep\nend-to-end model. We define a formula named local smooth discrepancy to measure\nthe Lipschitzness for target distribution in a pointwise way. Further, several\ncritical factors affecting the error bound are taken into account in our\nproposed optimization strategy to ensure the effectiveness and stability.\nEmpirical evidence shows that the proposed method is comparable or superior to\nthe state-of-the-art methods and our modifications are important for the\nvalidity.\n", "versions": [{"version": "v1", "created": "Sun, 26 May 2019 06:55:30 GMT"}, {"version": "v2", "created": "Wed, 3 Jul 2019 07:26:38 GMT"}, {"version": "v3", "created": "Tue, 5 Nov 2019 07:57:48 GMT"}], "update_date": "2019-11-06", "authors_parsed": [["Cai", "Guanyu", ""], ["Wang", "Yuqin", ""], ["He", "Lianghua", ""]]}, {"id": "1905.10756", "submitter": "Zhihong Chen", "authors": "Zhihong Chen, Chao Chen, Zhaowei Cheng, Boyuan Jiang, Ke Fang, Xinyu\n  Jin", "title": "Selective Transfer with Reinforced Transfer Network for Partial Domain\n  Adaptation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One crucial aspect of partial domain adaptation (PDA) is how to select the\nrelevant source samples in the shared classes for knowledge transfer. Previous\nPDA methods tackle this problem by re-weighting the source samples based on\ntheir high-level information (deep features). However, since the domain shift\nbetween source and target domains, only using the deep features for sample\nselection is defective. We argue that it is more reasonable to additionally\nexploit the pixel-level information for PDA problem, as the appearance\ndifference between outlier source classes and target classes is significantly\nlarge. In this paper, we propose a reinforced transfer network (RTNet), which\nutilizes both high-level and pixel-level information for PDA problem. Our RTNet\nis composed of a reinforced data selector (RDS) based on reinforcement learning\n(RL), which filters out the outlier source samples, and a domain adaptation\nmodel which minimizes the domain discrepancy in the shared label space.\nSpecifically, in the RDS, we design a novel reward based on the reconstruct\nerrors of selected source samples on the target generator, which introduces the\npixel-level information to guide the learning of RDS. Besides, we develope a\nstate containing high-level information, which used by the RDS for sample\nselection. The proposed RDS is a general module, which can be easily integrated\ninto existing DA models to make them fit the PDA situation. Extensive\nexperiments indicate that RTNet can achieve state-of-the-art performance for\nPDA tasks on several benchmark datasets.\n", "versions": [{"version": "v1", "created": "Sun, 26 May 2019 07:59:36 GMT"}, {"version": "v2", "created": "Sun, 1 Sep 2019 07:38:52 GMT"}, {"version": "v3", "created": "Thu, 27 Feb 2020 04:14:54 GMT"}, {"version": "v4", "created": "Fri, 28 Feb 2020 01:59:52 GMT"}], "update_date": "2020-03-02", "authors_parsed": [["Chen", "Zhihong", ""], ["Chen", "Chao", ""], ["Cheng", "Zhaowei", ""], ["Jiang", "Boyuan", ""], ["Fang", "Ke", ""], ["Jin", "Xinyu", ""]]}, {"id": "1905.10759", "submitter": "Yash Akhauri", "authors": "Yash Akhauri", "title": "HadaNets: Flexible Quantization Strategies for Neural Networks", "comments": "Accepted in CVPR 2019, UAVision 2019", "journal-ref": null, "doi": "10.1109/CVPRW.2019.00078", "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  On-board processing elements on UAVs are currently inadequate for training\nand inference of Deep Neural Networks. This is largely due to the energy\nconsumption of memory accesses in such a network. HadaNets introduce a flexible\ntrain-from-scratch tensor quantization scheme by pairing a full precision\ntensor to a binary tensor in the form of a Hadamard product. Unlike wider\nreduced precision neural network models, we preserve the train-time parameter\ncount, thus out-performing XNOR-Nets without a train-time memory penalty. Such\ntraining routines could see great utility in semi-supervised online learning\ntasks. Our method also offers advantages in model compression, as we reduce the\nmodel size of ResNet-18 by 7.43 times with respect to a full precision model\nwithout utilizing any other compression techniques. We also demonstrate a\n'Hadamard Binary Matrix Multiply' kernel, which delivers a 10-fold increase in\nperformance over full precision matrix multiplication with a similarly\noptimized kernel.\n", "versions": [{"version": "v1", "created": "Sun, 26 May 2019 08:17:54 GMT"}], "update_date": "2020-04-15", "authors_parsed": [["Akhauri", "Yash", ""]]}, {"id": "1905.10777", "submitter": "Hanyang Kong", "authors": "Hanyang Kong, Jian Zhao, Xiaoguang Tu, Junliang Xing, Shengmei Shen,\n  Jiashi Feng", "title": "Cross-Resolution Face Recognition via Prior-Aided Face Hallucination and\n  Residual Knowledge Distillation", "comments": "10 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent deep learning based face recognition methods have achieved great\nperformance, but it still remains challenging to recognize very low-resolution\nquery face like 28x28 pixels when CCTV camera is far from the captured subject.\nSuch face with very low-resolution is totally out of detail information of the\nface identity compared to normal resolution in a gallery and hard to find\ncorresponding faces therein. To this end, we propose a Resolution Invariant\nModel (RIM) for addressing such cross-resolution face recognition problems,\nwith three distinct novelties. First, RIM is a novel and unified deep\narchitecture, containing a Face Hallucination sub-Net (FHN) and a Heterogeneous\nRecognition sub-Net (HRN), which are jointly learned end to end. Second, FHN is\na well-designed tri-path Generative Adversarial Network (GAN) which\nsimultaneously perceives facial structure and geometry prior information, i.e.\nlandmark heatmaps and parsing maps, incorporated with an unsupervised\ncross-domain adversarial training strategy to super-resolve very low-resolution\nquery image to its 8x larger ones without requiring them to be well aligned.\nThird, HRN is a generic Convolutional Neural Network (CNN) for heterogeneous\nface recognition with our proposed residual knowledge distillation strategy for\nlearning discriminative yet generalized feature representation. Quantitative\nand qualitative experiments on several benchmarks demonstrate the superiority\nof the proposed model over the state-of-the-arts. Codes and models will be\nreleased upon acceptance.\n", "versions": [{"version": "v1", "created": "Sun, 26 May 2019 10:08:17 GMT"}], "update_date": "2019-05-28", "authors_parsed": [["Kong", "Hanyang", ""], ["Zhao", "Jian", ""], ["Tu", "Xiaoguang", ""], ["Xing", "Junliang", ""], ["Shen", "Shengmei", ""], ["Feng", "Jiashi", ""]]}, {"id": "1905.10784", "submitter": "Ioan Marius Bilasco PhD", "authors": "Romain Belmonte, Benjamin Allaert, Pierre Tirilly, Ioan Marius\n  Bilasco, Chaabane Djeraba, Nicu Sebe", "title": "Impact of facial landmark localization on facial expression recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although facial landmark localization (FLL) approaches are becoming\nincreasingly accurate for characterizing facial regions, one question remains\nunanswered: what is the impact of these approaches on subsequent related tasks?\nIn this paper, the focus is put on facial expression recognition (FER), where\nfacial landmarks are used for face registration, which is a common usage. Since\nthe most used datasets for facial landmark localization do not allow for a\nproper measurement of performance according to the different difficulties\n(e.g., pose, expression, illumination, occlusion, motion blur), we also\nquantify the performance of recent approaches in the presence of head pose\nvariations and facial expressions. Finally, a study of the impact of these\napproaches on FER is conducted. We show that the landmark accuracy achieved so\nfar optimizing the conventional Euclidean distance does not necessarily\nguarantee a gain in performance for FER. To deal with this issue, we propose a\nnew evaluation metric for FLL adapted to FER.\n", "versions": [{"version": "v1", "created": "Sun, 26 May 2019 10:50:46 GMT"}, {"version": "v2", "created": "Sun, 20 Dec 2020 19:17:40 GMT"}, {"version": "v3", "created": "Mon, 19 Jul 2021 07:11:14 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Belmonte", "Romain", ""], ["Allaert", "Benjamin", ""], ["Tirilly", "Pierre", ""], ["Bilasco", "Ioan Marius", ""], ["Djeraba", "Chaabane", ""], ["Sebe", "Nicu", ""]]}, {"id": "1905.10793", "submitter": "Aron Monszpart", "authors": "S\\'ebastien Ehrhardt, Aron Monszpart, Niloy J. Mitra, Andrea Vedaldi", "title": "Unsupervised Intuitive Physics from Past Experiences", "comments": "Under review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We are interested in learning models of intuitive physics similar to the ones\nthat animals use for navigation, manipulation and planning. In addition to\nlearning general physical principles, however, we are also interested in\nlearning ``on the fly'', from a few experiences, physical properties specific\nto new environments. We do all this in an unsupervised manner, using a\nmeta-learning formulation where the goal is to predict videos containing\ndemonstrations of physical phenomena, such as objects moving and colliding with\na complex background. We introduce the idea of summarizing past experiences in\na very compact manner, in our case using dynamic images, and show that this can\nbe used to solve the problem well and efficiently. Empirically, we show via\nextensive experiments and ablation studies, that our model learns to perform\nphysical predictions that generalize well in time and space, as well as to a\nvariable number of interacting physical objects.\n", "versions": [{"version": "v1", "created": "Sun, 26 May 2019 12:22:56 GMT"}], "update_date": "2019-05-28", "authors_parsed": [["Ehrhardt", "S\u00e9bastien", ""], ["Monszpart", "Aron", ""], ["Mitra", "Niloy J.", ""], ["Vedaldi", "Andrea", ""]]}, {"id": "1905.10797", "submitter": "Bryan Plummer", "authors": "Bryan A. Plummer, Mariya I. Vasileva, Vitali Petsiuk, Kate Saenko,\n  David Forsyth", "title": "Why do These Match? Explaining the Behavior of Image Similarity Models", "comments": "Accepted at ECCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Explaining a deep learning model can help users understand its behavior and\nallow researchers to discern its shortcomings. Recent work has primarily\nfocused on explaining models for tasks like image classification or visual\nquestion answering. In this paper, we introduce Salient Attributes for Network\nExplanation (SANE) to explain image similarity models, where a model's output\nis a score measuring the similarity of two inputs rather than a classification\nscore. In this task, an explanation depends on both of the input images, so\nstandard methods do not apply. Our SANE explanations pairs a saliency map\nidentifying important image regions with an attribute that best explains the\nmatch. We find that our explanations provide additional information not\ntypically captured by saliency maps alone, and can also improve performance on\nthe classic task of attribute recognition. Our approach's ability to generalize\nis demonstrated on two datasets from diverse domains, Polyvore Outfits and\nAnimals with Attributes 2. Code available at:\nhttps://github.com/VisionLearningGroup/SANE\n", "versions": [{"version": "v1", "created": "Sun, 26 May 2019 12:48:23 GMT"}, {"version": "v2", "created": "Mon, 24 Aug 2020 16:14:37 GMT"}], "update_date": "2020-08-25", "authors_parsed": [["Plummer", "Bryan A.", ""], ["Vasileva", "Mariya I.", ""], ["Petsiuk", "Vitali", ""], ["Saenko", "Kate", ""], ["Forsyth", "David", ""]]}, {"id": "1905.10822", "submitter": "Mohamed Elgharib", "authors": "Mohamed Elgharib, Mallikarjun BR, Ayush Tewari, Hyeongwoo Kim, Wentao\n  Liu, Hans-Peter Seidel, Christian Theobalt", "title": "EgoFace: Egocentric Face Performance Capture and Videorealistic\n  Reenactment", "comments": "Project Page: http://gvv.mpi-inf.mpg.de/projects/EgoFace/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Face performance capture and reenactment techniques use multiple cameras and\nsensors, positioned at a distance from the face or mounted on heavy wearable\ndevices. This limits their applications in mobile and outdoor environments. We\npresent EgoFace, a radically new lightweight setup for face performance capture\nand front-view videorealistic reenactment using a single egocentric RGB camera.\nOur lightweight setup allows operations in uncontrolled environments, and lends\nitself to telepresence applications such as video-conferencing from dynamic\nenvironments. The input image is projected into a low dimensional latent space\nof the facial expression parameters. Through careful adversarial training of\nthe parameter-space synthetic rendering, a videorealistic animation is\nproduced. Our problem is challenging as the human visual system is sensitive to\nthe smallest face irregularities that could occur in the final results. This\nsensitivity is even stronger for video results. Our solution is trained in a\npre-processing stage, through a supervised manner without manual annotations.\nEgoFace captures a wide variety of facial expressions, including mouth\nmovements and asymmetrical expressions. It works under varying illuminations,\nbackground, movements, handles people from different ethnicities and can\noperate in real time.\n", "versions": [{"version": "v1", "created": "Sun, 26 May 2019 15:57:47 GMT"}], "update_date": "2019-05-28", "authors_parsed": [["Elgharib", "Mohamed", ""], ["BR", "Mallikarjun", ""], ["Tewari", "Ayush", ""], ["Kim", "Hyeongwoo", ""], ["Liu", "Wentao", ""], ["Seidel", "Hans-Peter", ""], ["Theobalt", "Christian", ""]]}, {"id": "1905.10830", "submitter": "Evgenii Zheltonozhskii", "authors": "Brian Chmiel, Chaim Baskin, Ron Banner, Evgenii Zheltonozhskii,\n  Yevgeny Yermolin, Alex Karbachevsky, Alex M. Bronstein and Avi Mendelson", "title": "Feature Map Transform Coding for Energy-Efficient CNN Inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Convolutional neural networks (CNNs) achieve state-of-the-art accuracy in a\nvariety of tasks in computer vision and beyond. One of the major obstacles\nhindering the ubiquitous use of CNNs for inference on low-power edge devices is\ntheir high computational complexity and memory bandwidth requirements. The\nlatter often dominates the energy footprint on modern hardware. In this paper,\nwe introduce a lossy transform coding approach, inspired by image and video\ncompression, designed to reduce the memory bandwidth due to the storage of\nintermediate activation calculation results. Our method does not require\nfine-tuning the network weights and halves the data transfer volumes to the\nmain memory by compressing feature maps, which are highly correlated, with\nvariable length coding. Our method outperform previous approach in term of the\nnumber of bits per value with minor accuracy degradation on ResNet-34 and\nMobileNetV2. We analyze the performance of our approach on a variety of CNN\narchitectures and demonstrate that FPGA implementation of ResNet-18 with our\napproach results in a reduction of around 40% in the memory energy footprint,\ncompared to quantized network, with negligible impact on accuracy. When\nallowing accuracy degradation of up to 2%, the reduction of 60% is achieved. A\nreference implementation is available at\nhttps://github.com/CompressTeam/TransformCodingInference\n", "versions": [{"version": "v1", "created": "Sun, 26 May 2019 16:29:47 GMT"}, {"version": "v2", "created": "Wed, 25 Sep 2019 14:02:16 GMT"}, {"version": "v3", "created": "Thu, 26 Sep 2019 14:43:47 GMT"}], "update_date": "2019-09-27", "authors_parsed": [["Chmiel", "Brian", ""], ["Baskin", "Chaim", ""], ["Banner", "Ron", ""], ["Zheltonozhskii", "Evgenii", ""], ["Yermolin", "Yevgeny", ""], ["Karbachevsky", "Alex", ""], ["Bronstein", "Alex M.", ""], ["Mendelson", "Avi", ""]]}, {"id": "1905.10835", "submitter": "Usman Roshan", "authors": "Yunzhe Xue, Fadi G. Farhat, Olga Boukrina, A .M. Barrett, Jeffrey R.\n  Binder, Usman W. Roshan, William W. Graves", "title": "A multi-path 2.5 dimensional convolutional neural network system for\n  segmenting stroke lesions in brain MRI images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic identification of brain lesions from magnetic resonance imaging\n(MRI) scans of stroke survivors would be a useful aid in patient diagnosis and\ntreatment planning. We propose a multi-modal multi-path convolutional neural\nnetwork system for automating stroke lesion segmentation. Our system has nine\nend-to-end UNets that take as input 2-dimensional (2D) slices and examines all\nthree planes with three different normalizations. Outputs from these nine total\npaths are concatenated into a 3D volume that is then passed to a 3D\nconvolutional neural network to output a final lesion mask. We trained and\ntested our method on datasets from three sources: Medical College of Wisconsin\n(MCW), Kessler Foundation (KF), and the publicly available Anatomical Tracings\nof Lesions After Stroke (ATLAS) dataset. Cross-study validation results (with\nindependent training and validation datasets) were obtained to compare with\nprevious methods based on naive Bayes, random forests, and three recently\npublished convolutional neural networks. Model performance was quantified in\nterms of the Dice coefficient. Training on the KF and MCW images and testing on\nthe ATLAS images yielded a mean Dice coefficient of 0.54. This was reliably\nbetter than the next best previous model, UNet, at 0.47. Reversing the train\nand test datasets yields a mean Dice of 0.47 on KF and MCW images, whereas the\nnext best UNet reaches 0.45. With all three datasets combined, the current\nsystem compared to previous methods also attained a reliably higher\ncross-validation accuracy. It also achieved high Dice values for many smaller\nlesions that existing methods have difficulty identifying. Overall, our system\nis a clear improvement over previous methods for automating stroke lesion\nsegmentation, bringing us an important step closer to the inter-rater accuracy\nlevel of human experts.\n", "versions": [{"version": "v1", "created": "Sun, 26 May 2019 16:39:04 GMT"}], "update_date": "2019-05-28", "authors_parsed": [["Xue", "Yunzhe", ""], ["Farhat", "Fadi G.", ""], ["Boukrina", "Olga", ""], ["Barrett", "A . M.", ""], ["Binder", "Jeffrey R.", ""], ["Roshan", "Usman W.", ""], ["Graves", "William W.", ""]]}, {"id": "1905.10836", "submitter": "Bingchen Liu", "authors": "Bingchen Liu, Yizhe Zhu, Zuohui Fu, Gerard de Melo, Ahmed Elgammal", "title": "OOGAN: Disentangling GAN with One-Hot Sampling and Orthogonal\n  Regularization", "comments": "AAAI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Exploring the potential of GANs for unsupervised disentanglement learning,\nthis paper proposes a novel GAN-based disentanglement framework with One-Hot\nSampling and Orthogonal Regularization (OOGAN). While previous works mostly\nattempt to tackle disentanglement learning through VAE and seek to implicitly\nminimize the Total Correlation (TC) objective with various sorts of\napproximation methods, we show that GANs have a natural advantage in\ndisentangling with an alternating latent variable (noise) sampling method that\nis straightforward and robust. Furthermore, we provide a brand-new perspective\non designing the structure of the generator and discriminator, demonstrating\nthat a minor structural change and an orthogonal regularization on model\nweights entails an improved disentanglement. Instead of experimenting on simple\ntoy datasets, we conduct experiments on higher-resolution images and show that\nOOGAN greatly pushes the boundary of unsupervised disentanglement.\n", "versions": [{"version": "v1", "created": "Sun, 26 May 2019 16:42:56 GMT"}, {"version": "v2", "created": "Mon, 16 Sep 2019 02:53:54 GMT"}, {"version": "v3", "created": "Tue, 19 Nov 2019 21:35:39 GMT"}, {"version": "v4", "created": "Sat, 29 Feb 2020 02:28:59 GMT"}, {"version": "v5", "created": "Tue, 10 Mar 2020 21:14:52 GMT"}], "update_date": "2020-03-12", "authors_parsed": [["Liu", "Bingchen", ""], ["Zhu", "Yizhe", ""], ["Fu", "Zuohui", ""], ["de Melo", "Gerard", ""], ["Elgammal", "Ahmed", ""]]}, {"id": "1905.10841", "submitter": "Han Le", "authors": "Han Le, Rajarsi Gupta, Le Hou, Shahira Abousamra, Danielle Fassler,\n  Tahsin Kurc, Dimitris Samaras, Rebecca Batiste, Tianhao Zhao, Arvind Rao,\n  Alison L. Van Dyke, Ashish Sharma, Erich Bremer, Jonas S.Almeida, Joel Saltz", "title": "Utilizing Automated Breast Cancer Detection to Identify Spatial\n  Distributions of Tumor Infiltrating Lymphocytes in Invasive Breast Cancer", "comments": "The American Journal of Pathology", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Quantitative assessment of Tumor-TIL spatial relationships is increasingly\nimportant in both basic science and clinical aspects of breast cancer research.\nWe have developed and evaluated convolutional neural network (CNN) analysis\npipelines to generate combined maps of cancer regions and tumor infiltrating\nlymphocytes (TILs) in routine diagnostic breast cancer whole slide tissue\nimages (WSIs). We produce interactive whole slide maps that provide 1) insight\nabout the structural patterns and spatial distribution of lymphocytic\ninfiltrates and 2) facilitate improved quantification of TILs. We evaluated\nboth tumor and TIL analyses using three CNN networks - Resnet-34, VGG16 and\nInception v4, and demonstrated that the results compared favorably to those\nobtained by what believe are the best published methods. We have produced\nopen-source tools and generated a public dataset consisting of tumor/TIL maps\nfor 1,015 TCGA breast cancer images. We also present a customized web-based\ninterface that enables easy visualization and interactive exploration of\nhigh-resolution combined Tumor-TIL maps for 1,015TCGA invasive breast cancer\ncases that can be downloaded for further downstream analyses.\n", "versions": [{"version": "v1", "created": "Sun, 26 May 2019 17:25:18 GMT"}, {"version": "v2", "created": "Wed, 29 May 2019 15:49:06 GMT"}, {"version": "v3", "created": "Mon, 13 Jan 2020 16:32:45 GMT"}], "update_date": "2020-01-14", "authors_parsed": [["Le", "Han", ""], ["Gupta", "Rajarsi", ""], ["Hou", "Le", ""], ["Abousamra", "Shahira", ""], ["Fassler", "Danielle", ""], ["Kurc", "Tahsin", ""], ["Samaras", "Dimitris", ""], ["Batiste", "Rebecca", ""], ["Zhao", "Tianhao", ""], ["Rao", "Arvind", ""], ["Van Dyke", "Alison L.", ""], ["Sharma", "Ashish", ""], ["Bremer", "Erich", ""], ["Almeida", "Jonas S.", ""], ["Saltz", "Joel", ""]]}, {"id": "1905.10858", "submitter": "Roberto Arroyo", "authors": "Roberto Arroyo, Javier Tovar, Francisco J. Delgado, Emilio J.\n  Almaz\\'an, Diego G. Serrador, Antonio Hurtado", "title": "Integration of Text-maps in Convolutional Neural Networks for Region\n  Detection among Different Textual Categories", "comments": "Conference on Computer Vision and Pattern Recognition (CVPR).\n  Language and Vision Workshop 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In this work, we propose a new technique that combines appearance and text in\na Convolutional Neural Network (CNN), with the aim of detecting regions of\ndifferent textual categories. We define a novel visual representation of the\nsemantic meaning of text that allows a seamless integration in a standard CNN\narchitecture. This representation, referred to as text-map, is integrated with\nthe actual image to provide a much richer input to the network. Text-maps are\ncolored with different intensities depending on the relevance of the words\nrecognized over the image. Concretely, these words are previously extracted\nusing Optical Character Recognition (OCR) and they are colored according to the\nprobability of belonging to a textual category of interest. In this sense, this\nsolution is especially relevant in the context of item coding for supermarket\nproducts, where different types of textual categories must be identified, such\nas ingredients or nutritional facts. We evaluated our solution in the\nproprietary item coding dataset of Nielsen Brandbank, which contains more than\n10,000 images for train and 2,000 images for test. The reported results\ndemonstrate that our approach focused on visual and textual data outperforms\nstate-of-the-art algorithms only based on appearance, such as standard Faster\nR-CNN. These enhancements are reflected in precision and recall, which are\nimproved in 42 and 33 points respectively.\n", "versions": [{"version": "v1", "created": "Sun, 26 May 2019 18:59:32 GMT"}], "update_date": "2019-05-28", "authors_parsed": [["Arroyo", "Roberto", ""], ["Tovar", "Javier", ""], ["Delgado", "Francisco J.", ""], ["Almaz\u00e1n", "Emilio J.", ""], ["Serrador", "Diego G.", ""], ["Hurtado", "Antonio", ""]]}, {"id": "1905.10861", "submitter": "Min-Hung Chen", "authors": "Min-Hung Chen, Zsolt Kira, Ghassan AlRegib", "title": "Temporal Attentive Alignment for Video Domain Adaptation", "comments": "CVPR2019 Workshop (Learning from Unlabeled Videos). Source code:\n  http://github.com/cmhungsteve/TA3N", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although various image-based domain adaptation (DA) techniques have been\nproposed in recent years, domain shift in videos is still not well-explored.\nMost previous works only evaluate performance on small-scale datasets which are\nsaturated. Therefore, we first propose a larger-scale dataset with larger\ndomain discrepancy: UCF-HMDB_full. Second, we investigate different DA\nintegration methods for videos, and show that simultaneously aligning and\nlearning temporal dynamics achieves effective alignment even without\nsophisticated DA methods. Finally, we propose Temporal Attentive Adversarial\nAdaptation Network (TA3N), which explicitly attends to the temporal dynamics\nusing domain discrepancy for more effective domain alignment, achieving\nstate-of-the-art performance on three video DA datasets. The code and data are\nreleased at http://github.com/cmhungsteve/TA3N.\n", "versions": [{"version": "v1", "created": "Sun, 26 May 2019 19:15:23 GMT"}, {"version": "v2", "created": "Tue, 28 May 2019 01:24:44 GMT"}, {"version": "v3", "created": "Wed, 29 May 2019 17:17:38 GMT"}, {"version": "v4", "created": "Tue, 4 Jun 2019 05:24:54 GMT"}, {"version": "v5", "created": "Fri, 7 Jun 2019 03:45:29 GMT"}], "update_date": "2019-06-10", "authors_parsed": [["Chen", "Min-Hung", ""], ["Kira", "Zsolt", ""], ["AlRegib", "Ghassan", ""]]}, {"id": "1905.10885", "submitter": "Safa Cicek", "authors": "Safa Cicek, Stefano Soatto", "title": "Unsupervised Domain Adaptation via Regularized Conditional Alignment", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a method for unsupervised domain adaptation that trains a shared\nembedding to align the joint distributions of inputs (domain) and outputs\n(classes), making any classifier agnostic to the domain. Joint alignment\nensures that not only the marginal distributions of the domain are aligned, but\nthe labels as well. We propose a novel objective function that encourages the\nclass-conditional distributions to have disjoint support in feature space. We\nfurther exploit adversarial regularization to improve the performance of the\nclassifier on the domain for which no annotated data is available.\n", "versions": [{"version": "v1", "created": "Sun, 26 May 2019 21:25:45 GMT"}], "update_date": "2019-05-28", "authors_parsed": [["Cicek", "Safa", ""], ["Soatto", "Stefano", ""]]}, {"id": "1905.10901", "submitter": "Alexander Wong", "authors": "Andrew Hryniowski and Alexander Wong", "title": "Seeing Convolution Through the Eyes of Finite Transformation Semigroup\n  Theory: An Abstract Algebraic Interpretation of Convolutional Neural Networks", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Researchers are actively trying to gain better insights into the\nrepresentational properties of convolutional neural networks for guiding better\nnetwork designs and for interpreting a network's computational nature. Gaining\nsuch insights can be an arduous task due to the number of parameters in a\nnetwork and the complexity of a network's architecture. Current approaches of\nneural network interpretation include Bayesian probabilistic interpretations\nand information theoretic interpretations. In this study, we take a different\napproach to studying convolutional neural networks by proposing an abstract\nalgebraic interpretation using finite transformation semigroup theory.\nSpecifically, convolutional layers are broken up and mapped to a finite space.\nThe state space of the proposed finite transformation semigroup is then defined\nas a single element within the convolutional layer, with the acting elements\ndefined by surrounding state elements combined with convolution kernel\nelements. Generators of the finite transformation semigroup are defined to\ncomplete the interpretation. We leverage this approach to analyze the basic\nproperties of the resulting finite transformation semigroup to gain insights on\nthe representational properties of convolutional neural networks, including\ninsights into quantized network representation. Such a finite transformation\nsemigroup interpretation can also enable better understanding outside of the\nconfines of fixed lattice data structures, thus useful for handling data that\nlie on irregular lattices. Furthermore, the proposed abstract algebraic\ninterpretation is shown to be viable for interpreting convolutional operations\nwithin a variety of convolutional neural network architectures.\n", "versions": [{"version": "v1", "created": "Sun, 26 May 2019 23:11:18 GMT"}], "update_date": "2019-05-28", "authors_parsed": [["Hryniowski", "Andrew", ""], ["Wong", "Alexander", ""]]}, {"id": "1905.10915", "submitter": "Bochen Guan", "authors": "Bochen Guan, Jinnian Zhang, William A. Sethares, Richard Kijowski and\n  Fang Liu", "title": "SpecNet: Spectral Domain Convolutional Neural Network", "comments": "Accepted by ICASSP 21. Contact author: Bochen Guan\n  (bochen.guan@gmail.com)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  The memory consumption of most Convolutional Neural Network (CNN)\narchitectures grows rapidly with increasing depth of the network, which is a\nmajor constraint for efficient network training on modern GPUs with limited\nmemory, embedded systems, and mobile devices. Several studies show that the\nfeature maps (as generated after the convolutional layers) are the main\nbottleneck in this memory problem. Often, these feature maps mimic natural\nphotographs in the sense that their energy is concentrated in the spectral\ndomain. Although embedding CNN architectures in the spectral domain is widely\nexploited to accelerate the training process, we demonstrate that it is also\npossible to use the spectral domain to reduce the memory footprint, a method we\ncall Spectral Domain Convolutional Neural Network (SpecNet) that performs both\nthe convolution and the activation operations in the spectral domain. The\nperformance of SpecNet is evaluated on three competitive object recognition\nbenchmark tasks (CIFAR-10, SVHN, and ImageNet), and compared with several\nstate-of-the-art implementations. Overall, SpecNet is able to reduce memory\nconsumption by about 60% without significant loss of performance for all tested\nnetworks.\n", "versions": [{"version": "v1", "created": "Mon, 27 May 2019 00:44:14 GMT"}, {"version": "v2", "created": "Tue, 28 May 2019 06:10:23 GMT"}, {"version": "v3", "created": "Thu, 25 Jul 2019 18:51:50 GMT"}, {"version": "v4", "created": "Wed, 20 Nov 2019 02:15:26 GMT"}, {"version": "v5", "created": "Tue, 20 Oct 2020 04:49:48 GMT"}, {"version": "v6", "created": "Mon, 8 Feb 2021 23:36:39 GMT"}], "update_date": "2021-02-10", "authors_parsed": [["Guan", "Bochen", ""], ["Zhang", "Jinnian", ""], ["Sethares", "William A.", ""], ["Kijowski", "Richard", ""], ["Liu", "Fang", ""]]}, {"id": "1905.10920", "submitter": "Hamideh Kerdegari", "authors": "Hamideh Kerdegari, Manzoor Razaak, Vasileios Argyriou, Paolo Remagnino", "title": "Semi-supervised GAN for Classification of Multispectral Imagery Acquired\n  by UAVs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unmanned aerial vehicles (UAV) are used in precision agriculture (PA) to\nenable aerial monitoring of farmlands. Intelligent methods are required to\npinpoint weed infestations and make optimal choice of pesticide. UAV can fly a\nmultispectral camera and collect data. However, the classification of\nmultispectral images using supervised machine learning algorithms such as\nconvolutional neural networks (CNN) requires large amount of training data.\nThis is a common drawback in deep learning we try to circumvent making use of a\nsemi-supervised generative adversarial networks (GAN), providing a pixel-wise\nclassification for all the acquired multispectral images. Our algorithm\nconsists of a generator network that provides photo-realistic images as extra\ntraining data to a multi-class classifier, acting as a discriminator and\ntrained on small amounts of labeled data. The performance of the proposed\nmethod is evaluated on the weedNet dataset consisting of multispectral crop and\nweed images collected by a micro aerial vehicle (MAV). The results by the\nproposed semi-supervised GAN achieves high classification accuracy and\ndemonstrates the potential of GAN-based methods for the challenging task of\nmultispectral image classification.\n", "versions": [{"version": "v1", "created": "Fri, 24 May 2019 14:43:21 GMT"}], "update_date": "2019-05-28", "authors_parsed": [["Kerdegari", "Hamideh", ""], ["Razaak", "Manzoor", ""], ["Argyriou", "Vasileios", ""], ["Remagnino", "Paolo", ""]]}, {"id": "1905.10939", "submitter": "Masanari Kimura", "authors": "Masanari Kimura", "title": "PNUNet: Anomaly Detection using Positive-and-Negative Noise based on\n  Self-Training Procedure", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose the novel framework for anomaly detection in images. Our new\nframework, PNUNet, is based on many normal data and few anomalous data. We\nassume that some noises are added to the input images and learn to remove the\nnoise. In addition, the proposed method achieves significant performance\nimprovement by updating the noise assumed in the inputs using a self-training\nframework. The experimental results for the benchmark datasets show the\nusefulness of our new anomaly detection framework.\n", "versions": [{"version": "v1", "created": "Mon, 27 May 2019 02:22:16 GMT"}], "update_date": "2019-05-28", "authors_parsed": [["Kimura", "Masanari", ""]]}, {"id": "1905.10944", "submitter": "Shuzhi Yu", "authors": "Shuzhi Yu and Carlo Tomasi", "title": "Identity Connections in Residual Nets Improve Noise Stability", "comments": "ICML 2019 Workshop on Understanding and Improving Generalization in\n  Deep Learning, additional analysis on a property called Dominant Gradient\n  Flow of Residual Nets in Appendix D", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Residual Neural Networks (ResNets) achieve state-of-the-art performance in\nmany computer vision problems. Compared to plain networks without residual\nconnections (PlnNets), ResNets train faster, generalize better, and suffer less\nfrom the so-called degradation problem. We introduce simplified (but still\nnonlinear) versions of ResNets and PlnNets for which these discrepancies still\nhold, although to a lesser degree. We establish a 1-1 mapping between\nsimplified ResNets and simplified PlnNets, and show that they are exactly\nequivalent to each other in expressive power for the same computational\ncomplexity. We conjecture that ResNets generalize better because they have\nbetter noise stability, and empirically support it for both simplified and\nfully-fledged networks.\n", "versions": [{"version": "v1", "created": "Mon, 27 May 2019 02:52:46 GMT"}], "update_date": "2019-05-28", "authors_parsed": [["Yu", "Shuzhi", ""], ["Tomasi", "Carlo", ""]]}, {"id": "1905.10954", "submitter": "Yu Yin", "authors": "Yu Yin, Zhenya Huang, Enhong Chen, Qi Liu, Fuzheng Zhang, Xing Xie and\n  Guoping Hu", "title": "Transcribing Content from Structural Images with Spotlight Mechanism", "comments": "Accepted by KDD2018 Research Track. In proceedings of the 24th ACM\n  SIGKDD International Conference on Knowledge Discovery and Data Mining\n  (KDD'18)", "journal-ref": null, "doi": "10.1145/3219819.3219962", "report-no": null, "categories": "cs.LG cs.CV cs.SD stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Transcribing content from structural images, e.g., writing notes from music\nscores, is a challenging task as not only the content objects should be\nrecognized, but the internal structure should also be preserved. Existing image\nrecognition methods mainly work on images with simple content (e.g., text lines\nwith characters), but are not capable to identify ones with more complex\ncontent (e.g., structured symbols), which often follow a fine-grained grammar.\nTo this end, in this paper, we propose a hierarchical Spotlight Transcribing\nNetwork (STN) framework followed by a two-stage \"where-to-what\" solution.\nSpecifically, we first decide \"where-to-look\" through a novel spotlight\nmechanism to focus on different areas of the original image following its\nstructure. Then, we decide \"what-to-write\" by developing a GRU based network\nwith the spotlight areas for transcribing the content accordingly. Moreover, we\npropose two implementations on the basis of STN, i.e., STNM and STNR, where the\nspotlight movement follows the Markov property and Recurrent modeling,\nrespectively. We also design a reinforcement method to refine the framework by\nself-improving the spotlight mechanism. We conduct extensive experiments on\nmany structural image datasets, where the results clearly demonstrate the\neffectiveness of STN framework.\n", "versions": [{"version": "v1", "created": "Mon, 27 May 2019 03:25:29 GMT"}], "update_date": "2019-05-28", "authors_parsed": [["Yin", "Yu", ""], ["Huang", "Zhenya", ""], ["Chen", "Enhong", ""], ["Liu", "Qi", ""], ["Zhang", "Fuzheng", ""], ["Xie", "Xing", ""], ["Hu", "Guoping", ""]]}, {"id": "1905.10959", "submitter": "Wang Wei", "authors": "Ye Tian, Li Yang, Wei Wang, Jing Zhang, Qing Tang, Mili Ji, Yang Yu,\n  Yu Li, Hong Yang, Airong Qian", "title": "Computer-aided Detection of Squamous Carcinoma of the Cervix in Whole\n  Slide Images", "comments": "8 pages, 5figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Goal: Squamous cell carcinoma of cervix is one of the most prevalent cancer\nworldwide in females. Traditionally, the most indispensable diagnosis of cervix\nsquamous carcinoma is histopathological assessment which is achieved under\nmicroscope by pathologist. However, human evaluation of pathology slide is\nhighly depending on the experience of pathologist, thus big inter- and\nintra-observer variability exists. Digital pathology, in combination with deep\nlearning provides an opportunity to improve the objectivity and efficiency of\nhistopathologic slide analysis. Methods: In this study, we obtained 800\nhaematoxylin and eosin stained slides from 300 patients suffered from cervix\nsquamous carcinoma. Based on information from morphological heterogeneity in\nthe tumor and its adjacent area, we established deep learning models using\npopular convolution neural network architectures (inception-v3,\nInceptionResnet-v2 and Resnet50). Then random forest was introduced to feature\nextractions and slide-based classification. Results: The overall performance of\nour proposed models on slide-based tumor discrimination were outstanding with\nan AUC scores > 0.94. While, location identifications of lesions in whole slide\nimages were mediocre (FROC scores > 0.52) duo to the extreme complexity of\ntumor tissues. Conclusion: For the first time, our analysis workflow\nhighlighted a quantitative visual-based slide analysis of cervix squamous\ncarcinoma. Significance: This study demonstrates a pathway to assist\npathologist and accelerate the diagnosis of patients by utilizing new\ncomputational approaches.\n", "versions": [{"version": "v1", "created": "Mon, 27 May 2019 03:53:18 GMT"}], "update_date": "2019-05-28", "authors_parsed": [["Tian", "Ye", ""], ["Yang", "Li", ""], ["Wang", "Wei", ""], ["Zhang", "Jing", ""], ["Tang", "Qing", ""], ["Ji", "Mili", ""], ["Yu", "Yang", ""], ["Li", "Yu", ""], ["Yang", "Hong", ""], ["Qian", "Airong", ""]]}, {"id": "1905.10974", "submitter": "Agnieszka Miko{\\l}ajczyk", "authors": "Agnieszka Miko{\\l}ajczyk, Micha{\\l} Grochowski", "title": "Style transfer-based image synthesis as an efficient regularization\n  technique in deep learning", "comments": "6 pages, 4 figures, accepted to the 24th International Conference on\n  Methods and Models in Automation and Robotics (MMAR 2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  These days deep learning is the fastest-growing area in the field of Machine\nLearning. Convolutional Neural Networks are currently the main tool used for\nimage analysis and classification purposes. Although great achievements and\nperspectives, deep neural networks and accompanying learning algorithms have\nsome relevant challenges to tackle. In this paper, we have focused on the most\nfrequently mentioned problem in the field of machine learning, that is\nrelatively poor generalization abilities. Partial remedies for this are\nregularization techniques e.g. dropout, batch normalization, weight decay,\ntransfer learning, early stopping and data augmentation. In this paper, we have\nfocused on data augmentation. We propose to use a method based on a neural\nstyle transfer, which allows generating new unlabeled images of a high\nperceptual quality that combine the content of a base image with the appearance\nof another one. In a proposed approach, the newly created images are described\nwith pseudo-labels, and then used as a training dataset. Real, labeled images\nare divided into the validation and test set. We validated the proposed method\non a challenging skin lesion classification case study. Four representative\nneural architectures are examined. Obtained results show the strong potential\nof the proposed approach.\n", "versions": [{"version": "v1", "created": "Mon, 27 May 2019 04:56:39 GMT"}], "update_date": "2019-05-28", "authors_parsed": [["Miko\u0142ajczyk", "Agnieszka", ""], ["Grochowski", "Micha\u0142", ""]]}, {"id": "1905.10980", "submitter": "Ming-Wei Li", "authors": "Ming-Wei Li, Qing-Yuan Jiang and Wu-Jun Li", "title": "Deep Multi-Index Hashing for Person Re-Identification", "comments": "10 pages, 6 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditional person re-identification (ReID) methods typically represent\nperson images as real-valued features, which makes ReID inefficient when the\ngallery set is extremely large. Recently, some hashing methods have been\nproposed to make ReID more efficient. However, these hashing methods will\ndeteriorate the accuracy in general, and the efficiency of them is still not\nhigh enough. In this paper, we propose a novel hashing method, called deep\nmulti-index hashing (DMIH), to improve both efficiency and accuracy for ReID.\nDMIH seamlessly integrates multi-index hashing and multi-branch based networks\ninto the same framework. Furthermore, a novel block-wise multi-index hashing\ntable construction approach and a search-aware multi-index (SAMI) loss are\nproposed in DMIH to improve the search efficiency. Experiments on three widely\nused datasets show that DMIH can outperform other state-of-the-art baselines,\nincluding both hashing methods and real-valued methods, in terms of both\nefficiency and accuracy.\n", "versions": [{"version": "v1", "created": "Mon, 27 May 2019 05:26:59 GMT"}], "update_date": "2019-05-28", "authors_parsed": [["Li", "Ming-Wei", ""], ["Jiang", "Qing-Yuan", ""], ["Li", "Wu-Jun", ""]]}, {"id": "1905.10982", "submitter": "Hazrat Ali", "authors": "Sulaiman Khan, Hazrat Ali, Zia Ullah, Mohammad Farhad Bulbul", "title": "An Intelligent Monitoring System of Vehicles on Highway Traffic", "comments": "5 pages", "journal-ref": "2018 12th International Conference on Open Source Systems and\n  Technologies (ICOSST), Lahore, Pakistan, 2018, pp. 71-75", "doi": "10.1109/ICOSST.2018.8632192", "report-no": null, "categories": "cs.CV cs.AI cs.LG eess.SP", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Vehicle speed monitoring and management of highways is the critical problem\nof the road in this modern age of growing technology and population. A poor\nmanagement results in frequent traffic jam, traffic rules violation and fatal\nroad accidents. Using traditional techniques of RADAR, LIDAR and LASAR to\naddress this problem is time-consuming, expensive and tedious. This paper\npresents an efficient framework to produce a simple, cost efficient and\nintelligent system for vehicle speed monitoring. The proposed method uses an HD\n(High Definition) camera mounted on the road side either on a pole or on a\ntraffic signal for recording video frames. On the basis of these frames, a\nvehicle can be tracked by using radius growing method, and its speed can be\ncalculated by calculating vehicle mask and its displacement in consecutive\nframes. The method uses pattern recognition, digital image processing and\nmathematical techniques for vehicle detection, tracking and speed calculation.\nThe validity of the proposed model is proved by testing it on different\nhighways.\n", "versions": [{"version": "v1", "created": "Mon, 27 May 2019 05:45:56 GMT"}], "update_date": "2019-05-28", "authors_parsed": [["Khan", "Sulaiman", ""], ["Ali", "Hazrat", ""], ["Ullah", "Zia", ""], ["Bulbul", "Mohammad Farhad", ""]]}, {"id": "1905.11005", "submitter": "Haiping Zhu", "authors": "Haiping Zhu, Yuheng Zhang, Guohao Li, Junping Zhang, and Hongming Shan", "title": "Ordinal Distribution Regression for Gait-based Age Estimation", "comments": "Accepted by the journal of \"SCIENCE CHINA Information Sciences\"", "journal-ref": "Haiping Z, Yuheng Z, Guohao L, et al. Ordinal distribution\n  regression for gait-based age estimation[J]. SCIENCE CHINA Information\n  Sciences, 2020", "doi": "10.1007/s11432-019-2733-4", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computer vision researchers prefer to estimate age from face images because\nfacial features provide useful information. However, estimating age from face\nimages becomes challenging when people are distant from the camera or occluded.\nA person's gait is a unique biometric feature that can be perceived efficiently\neven at a distance. Thus, gait can be used to predict age when face images are\nnot available. However, existing gait-based classification or regression\nmethods ignore the ordinal relationship of different ages, which is an\nimportant clue for age estimation. This paper proposes an ordinal distribution\nregression with a global and local convolutional neural network for gait-based\nage estimation. Specifically, we decompose gait-based age regression into a\nseries of binary classifications to incorporate the ordinal age information.\nThen, an ordinal distribution loss is proposed to consider the inner\nrelationships among these classifications by penalizing the distribution\ndiscrepancy between the estimated value and the ground truth. In addition, our\nneural network comprises a global and three local sub-networks, and thus, is\ncapable of learning the global structure and local details from the head, body,\nand feet. Experimental results indicate that the proposed approach outperforms\nstate-of-the-art gait-based age estimation methods on the OULP-Age dataset.\n", "versions": [{"version": "v1", "created": "Mon, 27 May 2019 07:07:56 GMT"}, {"version": "v2", "created": "Thu, 18 Jul 2019 07:04:08 GMT"}, {"version": "v3", "created": "Sun, 12 Jan 2020 23:18:29 GMT"}, {"version": "v4", "created": "Fri, 31 Jan 2020 04:20:07 GMT"}], "update_date": "2020-02-03", "authors_parsed": [["Zhu", "Haiping", ""], ["Zhang", "Yuheng", ""], ["Li", "Guohao", ""], ["Zhang", "Junping", ""], ["Shan", "Hongming", ""]]}, {"id": "1905.11026", "submitter": "Yunhan Jia", "authors": "Yunhan Jia, Yantao Lu, Junjie Shen, Qi Alfred Chen, Zhenyu Zhong, Tao\n  Wei", "title": "Fooling Detection Alone is Not Enough: First Adversarial Attack against\n  Multiple Object Tracking", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recent work in adversarial machine learning started to focus on the visual\nperception in autonomous driving and studied Adversarial Examples (AEs) for\nobject detection models. However, in such visual perception pipeline the\ndetected objects must also be tracked, in a process called Multiple Object\nTracking (MOT), to build the moving trajectories of surrounding obstacles.\nSince MOT is designed to be robust against errors in object detection, it poses\na general challenge to existing attack techniques that blindly target objection\ndetection: we find that a success rate of over 98% is needed for them to\nactually affect the tracking results, a requirement that no existing attack\ntechnique can satisfy. In this paper, we are the first to study adversarial\nmachine learning attacks against the complete visual perception pipeline in\nautonomous driving, and discover a novel attack technique, tracker hijacking,\nthat can effectively fool MOT using AEs on object detection. Using our\ntechnique, successful AEs on as few as one single frame can move an existing\nobject in to or out of the headway of an autonomous vehicle to cause potential\nsafety hazards. We perform evaluation using the Berkeley Deep Drive dataset and\nfind that on average when 3 frames are attacked, our attack can have a nearly\n100% success rate while attacks that blindly target object detection only have\nup to 25%.\n", "versions": [{"version": "v1", "created": "Mon, 27 May 2019 07:55:05 GMT"}, {"version": "v2", "created": "Thu, 30 May 2019 06:29:59 GMT"}], "update_date": "2019-05-31", "authors_parsed": [["Jia", "Yunhan", ""], ["Lu", "Yantao", ""], ["Shen", "Junjie", ""], ["Chen", "Qi Alfred", ""], ["Zhong", "Zhenyu", ""], ["Wei", "Tao", ""]]}, {"id": "1905.11034", "submitter": "Amanda Berg", "authors": "Amanda Berg and J\\\"orgen Ahlberg and Michael Felsberg", "title": "Unsupervised Learning of Anomaly Detection from Contaminated Image Data\n  using Simultaneous Encoder Training", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unsupervised learning of anomaly detection in high-dimensional data, such as\nimages, is a challenging problem recently subject to intense research. Through\ncareful modelling of the data distribution of normal samples, it is possible to\ndetect deviant samples, so called anomalies. Generative Adversarial Networks\n(GANs) can model the highly complex, high-dimensional data distribution of\nnormal image samples, and have shown to be a suitable approach to the problem.\nPreviously published GAN-based anomaly detection methods often assume that\nanomaly-free data is available for training. However, this assumption is not\nvalid in most real-life scenarios, a.k.a. in the wild. In this work, we\nevaluate the effects of anomaly contaminations in the training data on\nstate-of-the-art GAN-based anomaly detection methods. As expected, detection\nperformance deteriorates. To address this performance drop, we propose to add\nan additional encoder network already at training time and show that joint\ngenerator-encoder training stratifies the latent space, mitigating the problem\nwith contaminated data. We show experimentally that the norm of a query image\nin this stratified latent space becomes a highly significant cue to\ndiscriminate anomalies from normal data. The proposed method achieves\nstate-of-the-art performance on CIFAR-10 as well as on a large, previously\nuntested dataset with cell images.\n", "versions": [{"version": "v1", "created": "Mon, 27 May 2019 08:22:55 GMT"}, {"version": "v2", "created": "Wed, 20 Nov 2019 12:49:09 GMT"}], "update_date": "2019-11-21", "authors_parsed": [["Berg", "Amanda", ""], ["Ahlberg", "J\u00f6rgen", ""], ["Felsberg", "Michael", ""]]}, {"id": "1905.11045", "submitter": "Yuyang Xue", "authors": "Yuyang Xue, Jiannan Su", "title": "Attention Based Image Compression Post-Processing Convolutional Neural\n  Network", "comments": "4 pages, 2 figures, CVPR Compression Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The traditional image compressors, e.g., BPG and H.266, have achieved great\nimage and video compression quality. Recently, Convolutional Neural Network has\nbeen used widely in image compression. We proposed an attention-based\nconvolutional neural network for low bit-rate compression to post-process the\noutput of traditional image compression decoder. Across the experimental\nresults on validation sets, the post-processing module trained by MAE and\nMS-SSIM losses yields the highest PSNR of 32.10 on average at the bit-rate of\n0.15.\n", "versions": [{"version": "v1", "created": "Mon, 27 May 2019 08:49:10 GMT"}], "update_date": "2019-05-28", "authors_parsed": [["Xue", "Yuyang", ""], ["Su", "Jiannan", ""]]}, {"id": "1905.11058", "submitter": "Yichao Wu", "authors": "Zhenmao Li, Yichao Wu, Ken Chen, Yudong Wu, Shunfeng Zhou, Jiaheng\n  Liu, Junjie Yan", "title": "Learning to Auto Weight: Entirely Data-driven and Highly Efficient\n  Weighting Framework", "comments": "Accepted by AAAI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Example weighting algorithm is an effective solution to the training bias\nproblem, however, most previous typical methods are usually limited to human\nknowledge and require laborious tuning of hyperparameters. In this paper, we\npropose a novel example weighting framework called Learning to Auto Weight\n(LAW). The proposed framework finds step-dependent weighting policies\nadaptively, and can be jointly trained with target networks without any\nassumptions or prior knowledge about the dataset. It consists of three key\ncomponents: Stage-based Searching Strategy (3SM) is adopted to shrink the huge\nsearching space in a complete training process; Duplicate Network Reward (DNR)\ngives more accurate supervision by removing randomness during the searching\nprocess; Full Data Update (FDU) further improves the updating efficiency.\nExperimental results demonstrate the superiority of weighting policy explored\nby LAW over standard training pipeline. Compared with baselines, LAW can find a\nbetter weighting schedule which achieves much more superior accuracy on both\nbiased CIFAR and ImageNet.\n", "versions": [{"version": "v1", "created": "Mon, 27 May 2019 09:05:28 GMT"}, {"version": "v2", "created": "Mon, 25 Nov 2019 06:43:14 GMT"}, {"version": "v3", "created": "Tue, 26 Nov 2019 05:39:17 GMT"}], "update_date": "2019-11-27", "authors_parsed": [["Li", "Zhenmao", ""], ["Wu", "Yichao", ""], ["Chen", "Ken", ""], ["Wu", "Yudong", ""], ["Zhou", "Shunfeng", ""], ["Liu", "Jiaheng", ""], ["Yan", "Junjie", ""]]}, {"id": "1905.11074", "submitter": "Danfeng Hong", "authors": "Xin Wu, Danfeng Hong, Jocelyn Chanussot, Yang Xu, Ran Tao, Yue Wang", "title": "Fourier-based Rotation-invariant Feature Boosting: An Efficient\n  Framework for Geospatial Object Detection", "comments": null, "journal-ref": null, "doi": "10.1109/LGRS.2019.2919755", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Geospatial object detection of remote sensing imagery has been attracting an\nincreasing interest in recent years, due to the rapid development in spaceborne\nimaging. Most of previously proposed object detectors are very sensitive to\nobject deformations, such as scaling and rotation. To this end, we propose a\nnovel and efficient framework for geospatial object detection in this letter,\ncalled Fourier-based rotation-invariant feature boosting (FRIFB). A\nFourier-based rotation-invariant feature is first generated in polar\ncoordinate. Then, the extracted features can be further structurally refined\nusing aggregate channel features. This leads to a faster feature computation\nand more robust feature representation, which is good fitting for the coming\nboosting learning. Finally, in the test phase, we achieve a fast pyramid\nfeature extraction by estimating a scale factor instead of directly collecting\nall features from image pyramid. Extensive experiments are conducted on two\nsubsets of NWPU VHR-10 dataset, demonstrating the superiority and effectiveness\nof the FRIFB compared to previous state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Mon, 27 May 2019 09:25:47 GMT"}], "update_date": "2020-02-19", "authors_parsed": [["Wu", "Xin", ""], ["Hong", "Danfeng", ""], ["Chanussot", "Jocelyn", ""], ["Xu", "Yang", ""], ["Tao", "Ran", ""], ["Wang", "Yue", ""]]}, {"id": "1905.11116", "submitter": "Hongyang Li", "authors": "Hongyang Li and David Eigen and Samuel Dodge and Matthew Zeiler and\n  Xiaogang Wang", "title": "Finding Task-Relevant Features for Few-Shot Learning by Category\n  Traversal", "comments": "CVPR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Few-shot learning is an important area of research. Conceptually, humans are\nreadily able to understand new concepts given just a few examples, while in\nmore pragmatic terms, limited-example training situations are common in\npractice. Recent effective approaches to few-shot learning employ a\nmetric-learning framework to learn a feature similarity comparison between a\nquery (test) example, and the few support (training) examples. However, these\napproaches treat each support class independently from one another, never\nlooking at the entire task as a whole. Because of this, they are constrained to\nuse a single set of features for all possible test-time tasks, which hinders\nthe ability to distinguish the most relevant dimensions for the task at hand.\nIn this work, we introduce a Category Traversal Module that can be inserted as\na plug-and-play module into most metric-learning based few-shot learners. This\ncomponent traverses across the entire support set at once, identifying\ntask-relevant features based on both intra-class commonality and inter-class\nuniqueness in the feature space. Incorporating our module improves performance\nconsiderably (5%-10% relative) over baseline systems on both mini-ImageNet and\ntieredImageNet benchmarks, with overall performance competitive with recent\nstate-of-the-art systems.\n", "versions": [{"version": "v1", "created": "Mon, 27 May 2019 10:55:51 GMT"}], "update_date": "2019-05-28", "authors_parsed": [["Li", "Hongyang", ""], ["Eigen", "David", ""], ["Dodge", "Samuel", ""], ["Zeiler", "Matthew", ""], ["Wang", "Xiaogang", ""]]}, {"id": "1905.11137", "submitter": "Elad Amrani", "authors": "Elad Amrani, Rami Ben-Ari, Tal Hakim, Alex Bronstein", "title": "Learning to Detect and Retrieve Objects from Unlabeled Videos", "comments": "ICCV 2019 Workshop on Multi-modal Video Analysis and Moments in Time\n  Challenge", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning an object detector or retrieval requires a large data set with\nmanual annotations. Such data sets are expensive and time consuming to create\nand therefore difficult to obtain on a large scale. In this work, we propose to\nexploit the natural correlation in narrations and the visual presence of\nobjects in video, to learn an object detector and retrieval without any manual\nlabeling involved. We pose the problem as weakly supervised learning with noisy\nlabels, and propose a novel object detection paradigm under these constraints.\nWe handle the background rejection by using contrastive samples and confront\nthe high level of label noise with a new clustering score. Our evaluation is\nbased on a set of 11 manually annotated objects in over 5000 frames. We show\ncomparison to a weakly-supervised approach as baseline and provide a strongly\nlabeled upper bound.\n", "versions": [{"version": "v1", "created": "Mon, 27 May 2019 11:36:32 GMT"}, {"version": "v2", "created": "Sat, 19 Oct 2019 07:26:57 GMT"}], "update_date": "2019-10-22", "authors_parsed": [["Amrani", "Elad", ""], ["Ben-Ari", "Rami", ""], ["Hakim", "Tal", ""], ["Bronstein", "Alex", ""]]}, {"id": "1905.11139", "submitter": "Devraj Mandal", "authors": "Devraj Mandal, Pramod Rao, Soma Biswas", "title": "Label Prediction Framework for Semi-Supervised Cross-Modal Retrieval", "comments": "12 pages, 3 tables, 2 figures, 1 algorithm flowchart", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Cross-modal data matching refers to retrieval of data from one modality, when\ngiven a query from another modality. In general, supervised algorithms achieve\nbetter retrieval performance compared to their unsupervised counterpart, as\nthey can learn better representative features by leveraging the available label\ninformation. However, this comes at the cost of requiring huge amount of\nlabeled examples, which may not always be available. In this work, we propose a\nnovel framework in a semi-supervised setting, which can predict the labels of\nthe unlabeled data using complementary information from different modalities.\nThe proposed framework can be used as an add-on with any baseline crossmodal\nalgorithm to give significant performance improvement, even in case of limited\nlabeled data. Finally, we analyze the challenging scenario where the unlabeled\nexamples can even come from classes not in the training data and evaluate the\nperformance of our algorithm under such setting. Extensive evaluation using\nseveral baseline algorithms across three different datasets shows the\neffectiveness of our label prediction framework.\n", "versions": [{"version": "v1", "created": "Mon, 27 May 2019 11:39:15 GMT"}], "update_date": "2019-05-28", "authors_parsed": [["Mandal", "Devraj", ""], ["Rao", "Pramod", ""], ["Biswas", "Soma", ""]]}, {"id": "1905.11142", "submitter": "Guanzhong Tian", "authors": "Guanzhong Tian, Yi Yuan, Yong liu", "title": "Audio2Face: Generating Speech/Face Animation from Single Audio with\n  Attention-Based Bidirectional LSTM Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.SD eess.AS eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an end to end deep learning approach for generating real-time\nfacial animation from just audio. Specifically, our deep architecture employs\ndeep bidirectional long short-term memory network and attention mechanism to\ndiscover the latent representations of time-varying contextual information\nwithin the speech and recognize the significance of different information\ncontributed to certain face status. Therefore, our model is able to drive\ndifferent levels of facial movements at inference and automatically keep up\nwith the corresponding pitch and latent speaking style in the input audio, with\nno assumption or further human intervention. Evaluation results show that our\nmethod could not only generate accurate lip movements from audio, but also\nsuccessfully regress the speaker's time-varying facial movements.\n", "versions": [{"version": "v1", "created": "Mon, 27 May 2019 11:40:21 GMT"}], "update_date": "2019-05-28", "authors_parsed": [["Tian", "Guanzhong", ""], ["Yuan", "Yi", ""], ["liu", "Yong", ""]]}, {"id": "1905.11159", "submitter": "Micha{\\l} Byra", "authors": "Michal Byra, Michael Andre", "title": "Breast mass classification in ultrasound based on Kendall's shape\n  manifold", "comments": "4 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV physics.med-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Morphological features play an important role in breast mass classification\nin sonography. While benign breast masses tend to have a well-defined\nellipsoidal contour, shape of malignant breast masses is commonly ill-defined\nand highly variable. Various handcrafted morphological features have been\ndeveloped over the years to assess this phenomenon and help the radiologists\ndifferentiate benign and malignant masses. In this paper we propose an\nautomatic approach to morphology analysis, we express shapes of breast masses\nas points on the Kendall's shape manifold. Next, we use the full Procrustes\ndistance to develop support vector machine classifiers for breast mass\ndifferentiation. The usefulness of our method is demonstrated using a dataset\nof B-mode images collected from 163 breast masses. Our method achieved area\nunder the receiver operating characteristic curve of 0.81. The proposed method\ncan be used to assess shapes of breast masses in ultrasound without any feature\nengineering.\n", "versions": [{"version": "v1", "created": "Mon, 27 May 2019 12:15:43 GMT"}], "update_date": "2019-05-28", "authors_parsed": [["Byra", "Michal", ""], ["Andre", "Michael", ""]]}, {"id": "1905.11163", "submitter": "Wojciech Michal Matkowski", "authors": "Wojciech Michal Matkowski, Adams Wai Kin Kong, Han Su, Peng Chen, Rong\n  Hou, and Zhihe Zhang", "title": "Giant Panda Face Recognition Using Small Dataset", "comments": "Accepted in the IEEE 2019 International Conference on Image\n  Processing (ICIP 2019), scheduled for 22-25 September 2019 in Taipei, Taiwan", "journal-ref": "2019 IEEE International Conference on Image Processing (ICIP)", "doi": "10.1109/ICIP.2019.8803125", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Giant panda (panda) is a highly endangered animal. Significant efforts and\nresources have been put on panda conservation. To measure effectiveness of\nconservation schemes, estimating its population size in wild is an important\ntask. The current population estimation approaches, including\ncapture-recapture, human visual identification and collection of DNA from hair\nor feces, are invasive, subjective, costly or even dangerous to the workers who\nperform these tasks in wild. Cameras have been widely installed in the regions\nwhere pandas live. It opens a new possibility for non-invasive image based\npanda recognition. Panda face recognition is naturally a small dataset problem,\nbecause of the number of pandas in the world and the number of qualified images\ncaptured by the cameras in each encounter. In this paper, a panda face\nrecognition algorithm, which includes alignment, large feature set extraction\nand matching is proposed and evaluated on a dataset consisting of 163 images.\nThe experimental results are encouraging.\n", "versions": [{"version": "v1", "created": "Mon, 27 May 2019 12:22:31 GMT"}], "update_date": "2019-10-01", "authors_parsed": [["Matkowski", "Wojciech Michal", ""], ["Kong", "Adams Wai Kin", ""], ["Su", "Han", ""], ["Chen", "Peng", ""], ["Hou", "Rong", ""], ["Zhang", "Zhihe", ""]]}, {"id": "1905.11169", "submitter": "Miguel Jaques", "authors": "Miguel Jaques, Michael Burke, Timothy Hospedales", "title": "Physics-as-Inverse-Graphics: Unsupervised Physical Parameter Estimation\n  from Video", "comments": "11 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a model that is able to perform unsupervised physical parameter\nestimation of systems from video, where the differential equations governing\nthe scene dynamics are known, but labeled states or objects are not available.\nExisting physical scene understanding methods require either object state\nsupervision, or do not integrate with differentiable physics to learn\ninterpretable system parameters and states. We address this problem through a\nphysics-as-inverse-graphics approach that brings together\nvision-as-inverse-graphics and differentiable physics engines, enabling objects\nand explicit state and velocity representations to be discovered. This\nframework allows us to perform long term extrapolative video prediction, as\nwell as vision-based model-predictive control. Our approach significantly\noutperforms related unsupervised methods in long-term future frame prediction\nof systems with interacting objects (such as ball-spring or 3-body\ngravitational systems), due to its ability to build dynamics into the model as\nan inductive bias. We further show the value of this tight vision-physics\nintegration by demonstrating data-efficient learning of vision-actuated\nmodel-based control for a pendulum system. We also show that the controller's\ninterpretability provides unique capabilities in goal-driven control and\nphysical reasoning for zero-data adaptation.\n", "versions": [{"version": "v1", "created": "Mon, 27 May 2019 12:37:14 GMT"}, {"version": "v2", "created": "Tue, 21 Apr 2020 12:14:26 GMT"}], "update_date": "2020-04-22", "authors_parsed": [["Jaques", "Miguel", ""], ["Burke", "Michael", ""], ["Hospedales", "Timothy", ""]]}, {"id": "1905.11172", "submitter": "Dong-Wook Kim", "authors": "Dong-Wook Kim, Jae Ryun Chung, Seung-Won Jung", "title": "GRDN:Grouped Residual Dense Network for Real Image Denoising and\n  GAN-based Real-world Noise Modeling", "comments": "To appear in CVPR 2019 workshop. The winners of the NTIRE2019\n  Challenge on Image Denoising Challenge: Track 2 sRGB", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent research on image denoising has progressed with the development of\ndeep learning architectures, especially convolutional neural networks. However,\nreal-world image denoising is still very challenging because it is not possible\nto obtain ideal pairs of ground-truth images and real-world noisy images. Owing\nto the recent release of benchmark datasets, the interest of the image\ndenoising community is now moving toward the real-world denoising problem. In\nthis paper, we propose a grouped residual dense network (GRDN), which is an\nextended and generalized architecture of the state-of-the-art residual dense\nnetwork (RDN). The core part of RDN is defined as grouped residual dense block\n(GRDB) and used as a building module of GRDN. We experimentally show that the\nimage denoising performance can be significantly improved by cascading GRDBs.\nIn addition to the network architecture design, we also develop a new\ngenerative adversarial network-based real-world noise modeling method. We\ndemonstrate the superiority of the proposed methods by achieving the highest\nscore in terms of both the peak signal-to-noise ratio and the structural\nsimilarity in the NTIRE2019 Real Image Denoising Challenge - Track 2:sRGB.\n", "versions": [{"version": "v1", "created": "Mon, 27 May 2019 12:39:32 GMT"}], "update_date": "2019-05-28", "authors_parsed": [["Kim", "Dong-Wook", ""], ["Chung", "Jae Ryun", ""], ["Jung", "Seung-Won", ""]]}, {"id": "1905.11192", "submitter": "Jintao Song", "authors": "Jintao Song, Huizhu Pan, Wuanquan Liu, Zisen Xu, Zhenkuan Pan", "title": "The Chan-Vese Model with Elastica and Landmark Constraints for Image\n  Segmentation", "comments": "17pages,9figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In order to completely separate objects with large sections of occluded\nboundaries in an image, we devise a new variational level set model for image\nsegmentation combining the Chan-Vese model with elastica and landmark\nconstraints. For computational efficiency, we design its Augmented Lagrangian\nMethod (ALM) or Alternating Direction Method of Multiplier (ADMM) method by\nintroducing some auxiliary variables, Lagrange multipliers, and penalty\nparameters. In each loop of alternating iterative optimization, the\nsub-problems of minimization can be easily solved via the Gauss-Seidel\niterative method and generalized soft thresholding formulas with projection,\nrespectively. Numerical experiments show that the proposed model can not only\nrecover larger broken boundaries but can also improve segmentation efficiency,\nas well as decrease the dependence of segmentation on parameter tuning and\ninitialization.\n", "versions": [{"version": "v1", "created": "Mon, 27 May 2019 13:29:59 GMT"}, {"version": "v2", "created": "Wed, 7 Aug 2019 05:43:31 GMT"}], "update_date": "2019-08-08", "authors_parsed": [["Song", "Jintao", ""], ["Pan", "Huizhu", ""], ["Liu", "Wuanquan", ""], ["Xu", "Zisen", ""], ["Pan", "Zhenkuan", ""]]}, {"id": "1905.11233", "submitter": "Xinshao Wang Dr", "authors": "Xinshao Wang, Elyor Kodirov, Yang Hua, Neil M. Robertson", "title": "Derivative Manipulation for General Example Weighting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Real-world large-scale datasets usually contain noisy labels and are\nimbalanced. Therefore, we propose derivative manipulation (DM), a novel and\ngeneral example weighting approach for training robust deep models under these\nadverse conditions.\n  DM has two main merits. First, loss function and example weighting are common\ntechniques in the literature. DM reveals their connection (a loss function does\nexample weighting) and is a replacement of both. Second, despite that a loss\ndefines an example weighting scheme by its derivative, in the loss design, we\nneed to consider whether it is differentiable. Instead, DM is more flexible by\ndirectly modifying the derivative so that a loss can be a non-elementary format\ntoo. Technically, DM defines an emphasis density function by a derivative\nmagnitude function. DM is generic in that diverse weighting schemes can be\nderived.\n  Extensive experiments on both vision and language tasks prove DM's\neffectiveness.\n", "versions": [{"version": "v1", "created": "Mon, 27 May 2019 13:59:08 GMT"}, {"version": "v10", "created": "Sat, 3 Oct 2020 19:00:18 GMT"}, {"version": "v2", "created": "Tue, 13 Aug 2019 00:19:09 GMT"}, {"version": "v3", "created": "Fri, 22 Nov 2019 10:49:32 GMT"}, {"version": "v4", "created": "Tue, 17 Dec 2019 12:34:17 GMT"}, {"version": "v5", "created": "Fri, 24 Jan 2020 17:41:29 GMT"}, {"version": "v6", "created": "Sat, 8 Feb 2020 11:34:42 GMT"}, {"version": "v7", "created": "Sun, 7 Jun 2020 22:06:46 GMT"}, {"version": "v8", "created": "Mon, 29 Jun 2020 13:55:23 GMT"}, {"version": "v9", "created": "Sat, 4 Jul 2020 14:45:57 GMT"}], "update_date": "2020-10-06", "authors_parsed": [["Wang", "Xinshao", ""], ["Kodirov", "Elyor", ""], ["Hua", "Yang", ""], ["Robertson", "Neil M.", ""]]}, {"id": "1905.11240", "submitter": "Shang-Yu Su", "authors": "Shang-Yu Su, Yun-Nung Chen", "title": "Bridging Dialogue Generation and Facial Expression Synthesis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spoken dialogue systems that assist users to solve complex tasks such as\nmovie ticket booking have become an emerging research topic in artificial\nintelligence and natural language processing areas. With a well-designed\ndialogue system as an intelligent personal assistant, people can accomplish\ncertain tasks more easily via natural language interactions. Today there are\nseveral virtual intelligent assistants in the market; however, most systems\nonly focus on single modality, such as textual or vocal interaction. A\nmultimodal interface has various advantages: (1) allowing human to communicate\nwith machines in a natural and concise form using the mixture of modalities\nthat most precisely convey the intention to satisfy communication needs, and\n(2) providing more engaging experience by natural and human-like feedback. This\npaper explores a brand new research direction, which aims at bridging dialogue\ngeneration and facial expression synthesis for better multimodal interaction.\nThe goal is to generate dialogue responses and simultaneously synthesize\ncorresponding visual expressions on faces, which is also an ultimate step\ntoward more human-like virtual assistants.\n", "versions": [{"version": "v1", "created": "Fri, 24 May 2019 10:22:16 GMT"}, {"version": "v2", "created": "Tue, 28 May 2019 05:32:21 GMT"}], "update_date": "2019-05-29", "authors_parsed": [["Su", "Shang-Yu", ""], ["Chen", "Yun-Nung", ""]]}, {"id": "1905.11271", "submitter": "Julia Navarro", "authors": "Julia Navarro and Neus Sabater", "title": "Learning Occlusion-Aware View Synthesis for Light Fields", "comments": null, "journal-ref": null, "doi": "10.1007/s10044-021-00956-2", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we present a novel learning-based approach to synthesize new\nviews of a light field image. In particular, given the four corner views of a\nlight field, the presented method estimates any in-between view. We use three\nsequential convolutional neural networks for feature extraction, scene geometry\nestimation and view selection. Compared to state-of-the-art approaches, in\norder to handle occlusions we propose to estimate a different disparity map per\nview. Jointly with the view selection network, this strategy shows to be the\nmost important to have proper reconstructions near object boundaries. Ablation\nstudies and comparison against the state of the art on Lytro light fields show\nthe superior performance of the proposed method. Furthermore, the method is\nadapted and tested on light fields with wide baselines acquired with a camera\narray and, in spite of having to deal with large occluded areas, the proposed\napproach yields very promising results.\n", "versions": [{"version": "v1", "created": "Mon, 27 May 2019 14:41:47 GMT"}], "update_date": "2021-02-19", "authors_parsed": [["Navarro", "Julia", ""], ["Sabater", "Neus", ""]]}, {"id": "1905.11299", "submitter": "Yuzhe Yang", "authors": "Yuzhe Yang, Zhiwen Hu, Kaigui Bian, Lingyang Song", "title": "ImgSensingNet: UAV Vision Guided Aerial-Ground Air Quality Sensing\n  System", "comments": "Preliminary version published in INFOCOM 2019. Code available at\n  https://github.com/YyzHarry/ImgSensingNet", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SY cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given the increasingly serious air pollution problem, the monitoring of air\nquality index (AQI) in urban areas has drawn considerable attention. This paper\npresents ImgSensingNet, a vision guided aerial-ground sensing system, for\nfine-grained air quality monitoring and forecasting using the fusion of haze\nimages taken by the unmanned-aerial-vehicle (UAV) and the AQI data collected by\nan on-ground three-dimensional (3D) wireless sensor network (WSN).\nSpecifically, ImgSensingNet first leverages the computer vision technique to\ntell the AQI scale in different regions from the taken haze images, where\nhaze-relevant features and a deep convolutional neural network (CNN) are\ndesigned for direct learning between haze images and corresponding AQI scale.\nBased on the learnt AQI scale, ImgSensingNet determines whether to wake up\non-ground wireless sensors for small-scale AQI monitoring and inference, which\ncan greatly reduce the energy consumption of the system. An entropy-based model\nis employed for accurate real-time AQI inference at unmeasured locations and\nfuture air quality distribution forecasting. We implement and evaluate\nImgSensingNet on two university campuses since Feb. 2018, and has collected\n17,630 photos and 2.6 millions of AQI data samples. Experimental results\nconfirm that ImgSensingNet can achieve higher inference accuracy while greatly\nreduce the energy consumption, compared to state-of-the-art AQI monitoring\napproaches.\n", "versions": [{"version": "v1", "created": "Mon, 27 May 2019 15:32:59 GMT"}], "update_date": "2019-05-28", "authors_parsed": [["Yang", "Yuzhe", ""], ["Hu", "Zhiwen", ""], ["Bian", "Kaigui", ""], ["Song", "Lingyang", ""]]}, {"id": "1905.11358", "submitter": "Saumya Jetley", "authors": "Laurynas Miksys, Saumya Jetley, Michael Sapienza, Stuart Golodetz,\n  Philip H.S. Torr", "title": "Straight to Shapes++: Real-time Instance Segmentation Made More Accurate", "comments": "Technical report, 27 pages (12 main, 15 supplementary), 17 figures,\n  14 tables", "journal-ref": null, "doi": null, "report-no": "STS-2018", "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Instance segmentation is an important problem in computer vision, with\napplications in autonomous driving, drone navigation and robotic manipulation.\nHowever, most existing methods are not real-time, complicating their deployment\nin time-sensitive contexts. In this work, we extend an existing approach to\nreal-time instance segmentation, called `Straight to Shapes' (STS), which makes\nuse of low-dimensional shape embedding spaces to directly regress to object\nshape masks. The STS model can run at 35 FPS on a high-end desktop, but its\naccuracy is significantly worse than that of offline state-of-the-art methods.\nWe leverage recent advances in the design and training of deep instance\nsegmentation models to improve the performance accuracy of the STS model whilst\nkeeping its real-time capabilities intact. In particular, we find that\nparameter sharing, more aggressive data augmentation and the use of structured\nloss for shape mask prediction all provide a useful boost to the network\nperformance. Our proposed approach, `Straight to Shapes++', achieves a\nremarkable 19.7 point improvement in mAP (at IOU of 0.5) over the original\nmethod as evaluated on the PASCAL VOC dataset, thus redefining the accuracy\nfrontier at real-time speeds. Since the accuracy of instance segmentation is\nclosely tied to that of object bounding box prediction, we also study the error\nprofile of the latter and examine the failure modes of our method for future\nimprovements.\n", "versions": [{"version": "v1", "created": "Mon, 27 May 2019 17:35:19 GMT"}, {"version": "v2", "created": "Tue, 30 Jul 2019 11:09:15 GMT"}], "update_date": "2019-07-31", "authors_parsed": [["Miksys", "Laurynas", ""], ["Jetley", "Saumya", ""], ["Sapienza", "Michael", ""], ["Golodetz", "Stuart", ""], ["Torr", "Philip H. S.", ""]]}, {"id": "1905.11369", "submitter": "Relja Arandjelovi\\'c", "authors": "Relja Arandjelovi\\'c, Andrew Zisserman", "title": "Object Discovery with a Copy-Pasting GAN", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We tackle the problem of object discovery, where objects are segmented for a\ngiven input image, and the system is trained without using any direct\nsupervision whatsoever. A novel copy-pasting GAN framework is proposed, where\nthe generator learns to discover an object in one image by compositing it into\nanother image such that the discriminator cannot tell that the resulting image\nis fake. After carefully addressing subtle issues, such as preventing the\ngenerator from `cheating', this game results in the generator learning to\nselect objects, as copy-pasting objects is most likely to fool the\ndiscriminator. The system is shown to work well on four very different\ndatasets, including large object appearance variations in challenging cluttered\nbackgrounds.\n", "versions": [{"version": "v1", "created": "Mon, 27 May 2019 17:55:05 GMT"}], "update_date": "2019-05-28", "authors_parsed": [["Arandjelovi\u0107", "Relja", ""], ["Zisserman", "Andrew", ""]]}, {"id": "1905.11381", "submitter": "Jirong Yi", "authors": "Jirong Yi, Hui Xie, Leixin Zhou, Xiaodong Wu, Weiyu Xu, Raghuraman\n  Mudumbai", "title": "Trust but Verify: An Information-Theoretic Explanation for the\n  Adversarial Fragility of Machine Learning Systems, and a General Defense\n  against Adversarial Attacks", "comments": "44 Pages, 2 Theorems, 35 Figures, 29 Tables. arXiv admin note:\n  substantial text overlap with arXiv:1901.09413", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep-learning based classification algorithms have been shown to be\nsusceptible to adversarial attacks: minor changes to the input of classifiers\ncan dramatically change their outputs, while being imperceptible to humans. In\nthis paper, we present a simple hypothesis about a feature compression property\nof artificial intelligence (AI) classifiers and present theoretical arguments\nto show that this hypothesis successfully accounts for the observed fragility\nof AI classifiers to small adversarial perturbations. Drawing on ideas from\ninformation and coding theory, we propose a general class of defenses for\ndetecting classifier errors caused by abnormally small input perturbations. We\nfurther show theoretical guarantees for the performance of this detection\nmethod. We present experimental results with (a) a voice recognition system,\nand (b) a digit recognition system using the MNIST database, to demonstrate the\neffectiveness of the proposed defense methods. The ideas in this paper are\nmotivated by a simple analogy between AI classifiers and the standard Shannon\nmodel of a communication system.\n", "versions": [{"version": "v1", "created": "Sat, 25 May 2019 21:57:51 GMT"}], "update_date": "2019-05-29", "authors_parsed": [["Yi", "Jirong", ""], ["Xie", "Hui", ""], ["Zhou", "Leixin", ""], ["Wu", "Xiaodong", ""], ["Xu", "Weiyu", ""], ["Mudumbai", "Raghuraman", ""]]}, {"id": "1905.11387", "submitter": "Santosh Tirunagari", "authors": "Santosh Tirunagari, Norman Poh, Kevin Wells, Miroslaw Bober, Isky\n  Gorden, and David Windridge", "title": "Automatic Delineation of Kidney Region in DCE-MRI", "comments": "arXiv admin note: text overlap with arXiv:1905.10218", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Delineation of the kidney region in dynamic contrast-enhanced magnetic\nresonance Imaging (DCE-MRI) is required during post-acquisition analysis in\norder to quantify various aspects of renal function, such as filtration and\nperfusion or blood flow. However, this can be obfuscated by the Partial Volume\nEffect (PVE), caused due to the mixing of any single voxel with two or more\nsignal intensities from adjacent regions such as liver region and other\ntissues. To avoid this problem, firstly, a kidney region of interest (ROI)\nneeds to be defined for the analysis. A clinician may choose to select a region\navoiding edges where PV mixing is likely to be significant. However, this\napproach is time-consuming and labour intensive. To address this issue, we\npresent Dynamic Mode Decomposition (DMD) coupled with thresholding and blob\nanalysis as a framework for automatic delineation of the kidney region. This\nmethod is first validated on synthetically generated data with ground-truth\navailable and then applied to ten healthy volunteers' kidney DCE-MRI datasets.\nWe found that the result obtained from our proposed framework is comparable to\nthat of a human expert. For example, while our result gives an average Root\nMean Square Error (RMSE) of 0.0097, the baseline achieves an average RMSE of\n0.1196 across the 10 datasets. As a result, we conclude automatic modelling via\nDMD framework is a promising approach.\n", "versions": [{"version": "v1", "created": "Sun, 26 May 2019 23:25:59 GMT"}], "update_date": "2019-05-29", "authors_parsed": [["Tirunagari", "Santosh", ""], ["Poh", "Norman", ""], ["Wells", "Kevin", ""], ["Bober", "Miroslaw", ""], ["Gorden", "Isky", ""], ["Windridge", "David", ""]]}, {"id": "1905.11428", "submitter": "Abhinav Kumar", "authors": "Abhinav Kumar, Thiago Serra and Srikumar Ramalingam", "title": "Equivalent and Approximate Transformations of Deep Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Two networks are equivalent if they produce the same output for any given\ninput. In this paper, we study the possibility of transforming a deep neural\nnetwork to another network with a different number of units or layers, which\ncan be either equivalent, a local exact approximation, or a global linear\napproximation of the original network. On the practical side, we show that\ncertain rectified linear units (ReLUs) can be safely removed from a network if\nthey are always active or inactive for any valid input. If we only need an\nequivalent network for a smaller domain, then more units can be removed and\nsome layers collapsed. On the theoretical side, we constructively show that for\nany feed-forward ReLU network, there exists a global linear approximation to a\n2-hidden-layer shallow network with a fixed number of units. This result is a\nbalance between the increasing number of units for arbitrary approximation with\na single layer and the known upper bound of $\\lceil log(n_0+1)\\rceil +1$ layers\nfor exact representation, where $n_0$ is the input dimension. While the\ntransformed network may require an exponential number of units to capture the\nactivation patterns of the original network, we show that it can be made\nsubstantially smaller by only accounting for the patterns that define linear\nregions. Based on experiments with ReLU networks on the MNIST dataset, we found\nthat $l_1$-regularization and adversarial training reduces the number of linear\nregions significantly as the number of stable units increases due to weight\nsparsity. Therefore, we can also intentionally train ReLU networks to allow for\neffective loss-less compression and approximation.\n", "versions": [{"version": "v1", "created": "Mon, 27 May 2019 18:05:28 GMT"}], "update_date": "2019-05-29", "authors_parsed": [["Kumar", "Abhinav", ""], ["Serra", "Thiago", ""], ["Ramalingam", "Srikumar", ""]]}, {"id": "1905.11447", "submitter": "Lihua Jian", "authors": "Lihua Jian, Xiaomin Yang, Zheng Liu, Gwanggil Jeon, Mingliang Gao, and\n  David Chisholm", "title": "A Symmetric Encoder-Decoder with Residual Block for Infrared and Visible\n  Image Fusion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In computer vision and image processing tasks, image fusion has evolved into\nan attractive research field. However, recent existing image fusion methods are\nmostly built on pixel-level operations, which may produce unacceptable\nartifacts and are time-consuming. In this paper, a symmetric encoder-decoder\nwith a residual block (SEDR) for infrared and visible image fusion is proposed.\nFor the training stage, the SEDR network is trained with a new dataset to\nobtain a fixed feature extractor. For the fusion stage, first, the trained\nmodel is utilized to extract the intermediate features and compensation\nfeatures of two source images. Then, extracted intermediate features are used\nto generate two attention maps, which are multiplied to the input features for\nrefinement. In addition, the compensation features generated by the first two\nconvolutional layers are merged and passed to the corresponding deconvolutional\nlayers. At last, the refined features are fused for decoding to reconstruct the\nfinal fused image. Experimental results demonstrate that the proposed fusion\nmethod (named as SEDRFuse) outperforms the state-of-the-art fusion methods in\nterms of both subjective and objective evaluations.\n", "versions": [{"version": "v1", "created": "Mon, 27 May 2019 18:51:23 GMT"}], "update_date": "2019-05-29", "authors_parsed": [["Jian", "Lihua", ""], ["Yang", "Xiaomin", ""], ["Liu", "Zheng", ""], ["Jeon", "Gwanggil", ""], ["Gao", "Mingliang", ""], ["Chisholm", "David", ""]]}, {"id": "1905.11452", "submitter": "Stefan Uhlich", "authors": "Stefan Uhlich, Lukas Mauch, Fabien Cardinaux, Kazuki Yoshiyama, Javier\n  Alonso Garcia, Stephen Tiedemann, Thomas Kemp, Akira Nakamura", "title": "Mixed Precision DNNs: All you need is a good parametrization", "comments": "International Conference on Learning Representations (ICLR) 2020;\n  Source code at https://github.com/sony/ai-research-code", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Efficient deep neural network (DNN) inference on mobile or embedded devices\ntypically involves quantization of the network parameters and activations. In\nparticular, mixed precision networks achieve better performance than networks\nwith homogeneous bitwidth for the same size constraint. Since choosing the\noptimal bitwidths is not straight forward, training methods, which can learn\nthem, are desirable. Differentiable quantization with straight-through\ngradients allows to learn the quantizer's parameters using gradient methods. We\nshow that a suited parametrization of the quantizer is the key to achieve a\nstable training and a good final performance. Specifically, we propose to\nparametrize the quantizer with the step size and dynamic range. The bitwidth\ncan then be inferred from them. Other parametrizations, which explicitly use\nthe bitwidth, consistently perform worse. We confirm our findings with\nexperiments on CIFAR-10 and ImageNet and we obtain mixed precision DNNs with\nlearned quantization parameters, achieving state-of-the-art performance.\n", "versions": [{"version": "v1", "created": "Mon, 27 May 2019 19:03:40 GMT"}, {"version": "v2", "created": "Sun, 26 Jan 2020 19:24:34 GMT"}, {"version": "v3", "created": "Fri, 22 May 2020 17:02:41 GMT"}], "update_date": "2020-05-25", "authors_parsed": [["Uhlich", "Stefan", ""], ["Mauch", "Lukas", ""], ["Cardinaux", "Fabien", ""], ["Yoshiyama", "Kazuki", ""], ["Garcia", "Javier Alonso", ""], ["Tiedemann", "Stephen", ""], ["Kemp", "Thomas", ""], ["Nakamura", "Akira", ""]]}, {"id": "1905.11455", "submitter": "Fabio De Sousa Ribeiro", "authors": "Fabio De Sousa Ribeiro, Georgios Leontidis, Stefanos Kollias", "title": "Capsule Routing via Variational Bayes", "comments": "AAAI 2020 Accepted Paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Capsule networks are a recently proposed type of neural network shown to\noutperform alternatives in challenging shape recognition tasks. In capsule\nnetworks, scalar neurons are replaced with capsule vectors or matrices, whose\nentries represent different properties of objects. The relationships between\nobjects and their parts are learned via trainable viewpoint-invariant\ntransformation matrices, and the presence of a given object is decided by the\nlevel of agreement among votes from its parts. This interaction occurs between\ncapsule layers and is a process called routing-by-agreement. In this paper, we\npropose a new capsule routing algorithm derived from Variational Bayes for\nfitting a mixture of transforming gaussians, and show it is possible transform\nour capsule network into a Capsule-VAE. Our Bayesian approach addresses some of\nthe inherent weaknesses of MLE based models such as the variance-collapse by\nmodelling uncertainty over capsule pose parameters. We outperform the\nstate-of-the-art on smallNORB using 50% fewer capsules than previously\nreported, achieve competitive performances on CIFAR-10, Fashion-MNIST, SVHN,\nand demonstrate significant improvement in MNIST to affNIST generalisation over\nprevious works.\n", "versions": [{"version": "v1", "created": "Mon, 27 May 2019 19:08:42 GMT"}, {"version": "v2", "created": "Mon, 9 Sep 2019 19:21:06 GMT"}, {"version": "v3", "created": "Tue, 3 Dec 2019 16:49:01 GMT"}], "update_date": "2019-12-04", "authors_parsed": [["Ribeiro", "Fabio De Sousa", ""], ["Leontidis", "Georgios", ""], ["Kollias", "Stefanos", ""]]}, {"id": "1905.11468", "submitter": "Chris Finlay", "authors": "Chris Finlay and Adam M Oberman", "title": "Scaleable input gradient regularization for adversarial robustness", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CR cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we revisit gradient regularization for adversarial robustness\nwith some new ingredients. First, we derive new per-image theoretical\nrobustness bounds based on local gradient information. These bounds strongly\nmotivate input gradient regularization. Second, we implement a scaleable\nversion of input gradient regularization which avoids double backpropagation:\nadversarially robust ImageNet models are trained in 33 hours on four consumer\ngrade GPUs. Finally, we show experimentally and through theoretical\ncertification that input gradient regularization is competitive with\nadversarial training. Moreover we demonstrate that gradient regularization does\nnot lead to gradient obfuscation or gradient masking.\n", "versions": [{"version": "v1", "created": "Mon, 27 May 2019 19:40:52 GMT"}, {"version": "v2", "created": "Fri, 4 Oct 2019 14:12:34 GMT"}], "update_date": "2019-10-07", "authors_parsed": [["Finlay", "Chris", ""], ["Oberman", "Adam M", ""]]}, {"id": "1905.11472", "submitter": "Dinh-Luan Nguyen", "authors": "Dinh-Luan Nguyen and Anil K. Jain", "title": "End-to-End Pore Extraction and Matching in Latent Fingerprints: Going\n  Beyond Minutiae", "comments": "Technical report", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Latent fingerprint recognition is not a new topic but it has attracted a lot\nof attention from researchers in both academia and industry over the past 50\nyears. With the rapid development of pattern recognition techniques, automated\nfingerprint identification systems (AFIS) have become more and more ubiquitous.\nHowever, most AFIS are utilized for live-scan or rolled/slap prints while only\na few systems can work on latent fingerprints with reasonable accuracy. The\nquestion of whether taking higher resolution scans of latent fingerprints and\ntheir rolled/slap mate prints could help improve the identification accuracy\nstill remains an open question in the forensic community. Because pores are one\nof the most reliable features besides minutiae to identify latent fingerprints,\nwe propose an end-to-end automatic pore extraction and matching system to\nanalyze the utility of pores in latent fingerprint identification. Hence, this\npaper answers two questions in the latent fingerprint domain: (i) does the\nincorporation of pores as level-3 features improve the system performance\nsignificantly? and (ii) does the 1,000 ppi image resolution improve the\nrecognition results? We believe that our proposed end-to-end pore extraction\nand matching system will be a concrete baseline for future latent AFIS\ndevelopment.\n", "versions": [{"version": "v1", "created": "Mon, 27 May 2019 19:44:51 GMT"}, {"version": "v2", "created": "Thu, 30 May 2019 02:26:14 GMT"}], "update_date": "2019-05-31", "authors_parsed": [["Nguyen", "Dinh-Luan", ""], ["Jain", "Anil K.", ""]]}, {"id": "1905.11498", "submitter": "Chu Wang", "authors": "Chu Wang, Babak Samari, Vladimir Kim, Siddhartha Chaudhuri, Kaleem\n  Siddiqi", "title": "FAN: Focused Attention Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Attention networks show promise for both vision and language tasks, by\nemphasizing relationships between constituent elements through weighting\nfunctions. Such elements could be regions in an image output by a region\nproposal network, or words in a sentence, represented by word embedding. Thus\nfar the learning of attention weights has been driven solely by the\nminimization of task specific loss functions. We introduce a method for\nlearning attention weights to better emphasize informative pair-wise relations\nbetween entities. The key component is a novel center-mass cross entropy loss,\nwhich can be applied in conjunction with the task specific ones. We further\nintroduce a focused attention backbone to learn these attention weights for\ngeneral tasks. We demonstrate that the focused supervision leads to improved\nattention distribution across meaningful entities, and that it enhances the\nrepresentation by aggregating features from them. Our focused attention module\nleads to state-of-the-art recovery of relations in a relationship proposal task\nand boosts performance for various vision and language tasks.\n", "versions": [{"version": "v1", "created": "Mon, 27 May 2019 20:41:53 GMT"}, {"version": "v2", "created": "Sat, 1 Jun 2019 16:31:24 GMT"}, {"version": "v3", "created": "Thu, 3 Oct 2019 19:55:42 GMT"}], "update_date": "2019-10-07", "authors_parsed": [["Wang", "Chu", ""], ["Samari", "Babak", ""], ["Kim", "Vladimir", ""], ["Chaudhuri", "Siddhartha", ""], ["Siddiqi", "Kaleem", ""]]}, {"id": "1905.11503", "submitter": "Hosnieh Sattar", "authors": "Hosnieh Sattar, Katharina Krombholz, Gerard Pons-Moll, Mario Fritz", "title": "Body Shape Privacy in Images: Understanding Privacy and Preventing\n  Automatic Shape Extraction", "comments": null, "journal-ref": "Proc. of the IEEE European Conference on Computer Vision Workshops\n  (ECCVW), CV-COPS@ECCV2020", "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern approaches to pose and body shape estimation have recently achieved\nstrong performance even under challenging real-world conditions. Even from a\nsingle image of a clothed person, a realistic looking body shape can be\ninferred that captures a users' weight group and body shape type well. This\nopens up a whole spectrum of applications -- in particular in fashion -- where\nvirtual try-on and recommendation systems can make use of these new and\nautomatized cues. However, a realistic depiction of the undressed body is\nregarded highly private and therefore might not be consented by most people.\nHence, we ask if the automatic extraction of such information can be\neffectively evaded. While adversarial perturbations have been shown to be\neffective for manipulating the output of machine learning models -- in\nparticular, end-to-end deep learning approaches -- state of the art shape\nestimation methods are composed of multiple stages. We perform the first\ninvestigation of different strategies that can be used to effectively\nmanipulate the automatic shape estimation while preserving the overall\nappearance of the original image.\n", "versions": [{"version": "v1", "created": "Mon, 27 May 2019 20:57:52 GMT"}, {"version": "v2", "created": "Thu, 9 Jul 2020 12:11:55 GMT"}, {"version": "v3", "created": "Thu, 22 Oct 2020 11:15:45 GMT"}], "update_date": "2020-10-23", "authors_parsed": [["Sattar", "Hosnieh", ""], ["Krombholz", "Katharina", ""], ["Pons-Moll", "Gerard", ""], ["Fritz", "Mario", ""]]}, {"id": "1905.11522", "submitter": "Anuj Pahuja", "authors": "Anuj Pahuja, Avishek Majumder, Anirban Chakraborty, R. Venkatesh Babu", "title": "Enhancing Salient Object Segmentation Through Attention", "comments": "CVPRW - Deep Vision 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Segmenting salient objects in an image is an important vision task with\nubiquitous applications. The problem becomes more challenging in the presence\nof a cluttered and textured background, low resolution and/or low contrast\nimages. Even though existing algorithms perform well in segmenting most of the\nobject(s) of interest, they often end up segmenting false positives due to\nresembling salient objects in the background. In this work, we tackle this\nproblem by iteratively attending to image patches in a recurrent fashion and\nsubsequently enhancing the predicted segmentation mask. Saliency features are\nestimated independently for every image patch, which are further combined using\nan aggregation strategy based on a Convolutional Gated Recurrent Unit (ConvGRU)\nnetwork. The proposed approach works in an end-to-end manner, removing\nbackground noise and false positives incrementally. Through extensive\nevaluation on various benchmark datasets, we show superior performance to the\nexisting approaches without any post-processing.\n", "versions": [{"version": "v1", "created": "Mon, 27 May 2019 21:50:54 GMT"}], "update_date": "2019-05-29", "authors_parsed": [["Pahuja", "Anuj", ""], ["Majumder", "Avishek", ""], ["Chakraborty", "Anirban", ""], ["Babu", "R. Venkatesh", ""]]}, {"id": "1905.11528", "submitter": "Santiago Gonzalez", "authors": "Santiago Gonzalez and Risto Miikkulainen", "title": "Improved Training Speed, Accuracy, and Data Utilization Through Loss\n  Function Optimization", "comments": null, "journal-ref": "Proceedings of the 2020 IEEE Congress on Evolutionary Computation", "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As the complexity of neural network models has grown, it has become\nincreasingly important to optimize their design automatically through\nmetalearning. Methods for discovering hyperparameters, topologies, and learning\nrate schedules have lead to significant increases in performance. This paper\nshows that loss functions can be optimized with metalearning as well, and\nresult in similar improvements. The method, Genetic Loss-function Optimization\n(GLO), discovers loss functions de novo, and optimizes them for a target task.\nLeveraging techniques from genetic programming, GLO builds loss functions\nhierarchically from a set of operators and leaf nodes. These functions are\nrepeatedly recombined and mutated to find an optimal structure, and then a\ncovariance-matrix adaptation evolutionary strategy (CMA-ES) is used to find\noptimal coefficients. Networks trained with GLO loss functions are found to\noutperform the standard cross-entropy loss on standard image classification\ntasks. Training with these new loss functions requires fewer steps, results in\nlower test error, and allows for smaller datasets to be used. Loss-function\noptimization thus provides a new dimension of metalearning, and constitutes an\nimportant step towards AutoML.\n", "versions": [{"version": "v1", "created": "Mon, 27 May 2019 22:24:21 GMT"}, {"version": "v2", "created": "Mon, 10 Feb 2020 16:24:16 GMT"}, {"version": "v3", "created": "Mon, 27 Apr 2020 16:31:53 GMT"}], "update_date": "2020-04-28", "authors_parsed": [["Gonzalez", "Santiago", ""], ["Miikkulainen", "Risto", ""]]}, {"id": "1905.11533", "submitter": "Xiaocong Du", "authors": "Xiaocong Du, Zheng Li, Yu Cao", "title": "CGaP: Continuous Growth and Pruning for Efficient Deep Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Today a canonical approach to reduce the computation cost of Deep Neural\nNetworks (DNNs) is to pre-define an over-parameterized model before training to\nguarantee the learning capacity, and then prune unimportant learning units\n(filters and neurons) during training to improve model compactness. We argue it\nis unnecessary to introduce redundancy at the beginning of the training but\nthen reduce redundancy for the ultimate inference model. In this paper, we\npropose a Continuous Growth and Pruning (CGaP) scheme to minimize the\nredundancy from the beginning. CGaP starts the training from a small network\nseed, then expands the model continuously by reinforcing important learning\nunits, and finally prunes the network to obtain a compact and accurate model.\nAs the growth phase favors important learning units, CGaP provides a clear\nlearning purpose to the pruning phase. Experimental results on representative\ndatasets and DNN architectures demonstrate that CGaP outperforms previous\npruning-only approaches that deal with pre-defined structures. For VGG-19 on\nCIFAR-100 and SVHN datasets, CGaP reduces the number of parameters by 78.9% and\n85.8%, FLOPs by 53.2% and 74.2%, respectively; For ResNet-110 On CIFAR-10, CGaP\nreduces 64.0% number of parameters and 63.3% FLOPs.\n", "versions": [{"version": "v1", "created": "Mon, 27 May 2019 22:54:59 GMT"}, {"version": "v2", "created": "Wed, 29 May 2019 17:07:50 GMT"}], "update_date": "2019-05-30", "authors_parsed": [["Du", "Xiaocong", ""], ["Li", "Zheng", ""], ["Cao", "Yu", ""]]}, {"id": "1905.11539", "submitter": "Yunsheng Li", "authors": "Mandar Dixit, Yunsheng Li, Nuno Vasconcelos", "title": "Semantic Fisher Scores for Task Transfer: Using Objects to Classify\n  Scenes", "comments": "16 pages, 11 figures, accepted by TPAMI", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The transfer of a neural network (CNN) trained to recognize objects to the\ntask of scene classification is considered. A Bag-of-Semantics (BoS)\nrepresentation is first induced, by feeding scene image patches to the object\nCNN, and representing the scene image by the ensuing bag of posterior class\nprobability vectors (semantic posteriors). The encoding of the BoS with a\nFisher vector(FV) is then studied. A link is established between the FV of any\nprobabilistic model and the Q-function of the expectation-maximization(EM)\nalgorithm used to estimate its parameters by maximum likelihood. A network\nimplementation of the MFA Fisher Score (MFA-FS), denoted as the MFAFSNet, is\nfinally proposed to enable end-to-end training. Experiments with various object\nCNNs and datasets show that the approach has state-of-the-art transfer\nperformance. Somewhat surprisingly, the scene classification results are\nsuperior to those of a CNN explicitly trained for scene classification, using a\nlarge scene dataset (Places). This suggests that holistic analysis is\ninsufficient for scene classification. The modeling of local object semantics\nappears to be at least equally important. The two approaches are also shown to\nbe strongly complementary, leading to very large scene classification gains\nwhen combined, and outperforming all previous scene classification approaches\nby a sizeable margin\n", "versions": [{"version": "v1", "created": "Mon, 27 May 2019 23:15:26 GMT"}], "update_date": "2019-05-29", "authors_parsed": [["Dixit", "Mandar", ""], ["Li", "Yunsheng", ""], ["Vasconcelos", "Nuno", ""]]}, {"id": "1905.11543", "submitter": "Zhao Zhang", "authors": "Zhao Zhang, Weiming Jiang, Jie Qin, Li Zhang, Fanzhang Li, Min Zhang\n  and Shuicheng Yan", "title": "Jointly Learning Structured Analysis Discriminative Dictionary and\n  Analysis Multiclass Classifier", "comments": "Accepted by IEEE TNNLS", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose an analysis mechanism based structured Analysis\nDiscriminative Dictionary Learning (ADDL) framework. ADDL seamlessly integrates\nthe analysis discriminative dictionary learning, analysis representation and\nanalysis classifier training into a unified model. The applied analysis\nmechanism can make sure that the learnt dictionaries, representations and\nlinear classifiers over different classes are independent and discriminating as\nmuch as possible. The dictionary is obtained by minimizing a reconstruction\nerror and an analytical incoherence promoting term that encourages the\nsub-dictionaries associated with different classes to be independent. To obtain\nthe representation coefficients, ADDL imposes a sparse l2,1-norm constraint on\nthe coding coefficients instead of using l0 or l1-norm, since the l0 or l1-norm\nconstraint applied in most existing DL criteria makes the training phase time\nconsuming. The codes-extraction projection that bridges data with the sparse\ncodes by extracting special features from the given samples is calculated via\nminimizing a sparse codes approximation term. Then we compute a linear\nclassifier based on the approximated sparse codes by an analysis mechanism to\nsimultaneously consider the classification and representation powers. Thus, the\nclassification approach of our model is very efficient, because it can avoid\nthe extra time-consuming sparse reconstruction process with trained dictionary\nfor each new test data as most existing DL algorithms. Simulations on real\nimage databases demonstrate that our ADDL model can obtain superior performance\nover other state-of-the-arts.\n", "versions": [{"version": "v1", "created": "Mon, 27 May 2019 23:52:46 GMT"}], "update_date": "2019-05-29", "authors_parsed": [["Zhang", "Zhao", ""], ["Jiang", "Weiming", ""], ["Qin", "Jie", ""], ["Zhang", "Li", ""], ["Li", "Fanzhang", ""], ["Zhang", "Min", ""], ["Yan", "Shuicheng", ""]]}, {"id": "1905.11544", "submitter": "Naveed Akhtar Dr.", "authors": "Naveed Akhtar, Mohammad A. A. K. Jalwana, Mohammed Bennamoun, Ajmal\n  Mian", "title": "Label Universal Targeted Attack", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce Label Universal Targeted Attack (LUTA) that makes a deep model\npredict a label of attacker's choice for `any' sample of a given source class\nwith high probability. Our attack stochastically maximizes the log-probability\nof the target label for the source class with first order gradient\noptimization, while accounting for the gradient moments. It also suppresses the\nleakage of attack information to the non-source classes for avoiding the attack\nsuspicions. The perturbations resulting from our attack achieve high fooling\nratios on the large-scale ImageNet and VGGFace models, and transfer well to the\nPhysical World. Given full control over the perturbation scope in LUTA, we also\ndemonstrate it as a tool for deep model autopsy. The proposed attack reveals\ninteresting perturbation patterns and observations regarding the deep models.\n", "versions": [{"version": "v1", "created": "Mon, 27 May 2019 23:53:00 GMT"}, {"version": "v2", "created": "Sat, 1 Jun 2019 05:11:50 GMT"}], "update_date": "2019-06-04", "authors_parsed": [["Akhtar", "Naveed", ""], ["Jalwana", "Mohammad A. A. K.", ""], ["Bennamoun", "Mohammed", ""], ["Mian", "Ajmal", ""]]}, {"id": "1905.11559", "submitter": "Yazhou Yao", "authors": "Huafeng Liu, Yazhou Yao, Zeren Sun, Xiangrui Li, Ke Jia, Zhenmin Tang", "title": "Road Segmentation with Image-LiDAR Data Fusion", "comments": "Accepted by Multimedia Tools and Applications", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Robust road segmentation is a key challenge in self-driving research. Though\nmany image-based methods have been studied and high performances in dataset\nevaluations have been reported, developing robust and reliable road\nsegmentation is still a major challenge. Data fusion across different sensors\nto improve the performance of road segmentation is widely considered an\nimportant and irreplaceable solution. In this paper, we propose a novel\nstructure to fuse image and LiDAR point cloud in an end-to-end semantic\nsegmentation network, in which the fusion is performed at decoder stage instead\nof at, more commonly, encoder stage. During fusion, we improve the multi-scale\nLiDAR map generation to increase the precision of the multi-scale LiDAR map by\nintroducing pyramid projection method. Additionally, we adapted the multi-path\nrefinement network with our fusion strategy and improve the road prediction\ncompared with transpose convolution with skip layers. Our approach has been\ntested on KITTI ROAD dataset and has competitive performance.\n", "versions": [{"version": "v1", "created": "Sun, 26 May 2019 19:18:12 GMT"}], "update_date": "2019-05-29", "authors_parsed": [["Liu", "Huafeng", ""], ["Yao", "Yazhou", ""], ["Sun", "Zeren", ""], ["Li", "Xiangrui", ""], ["Jia", "Ke", ""], ["Tang", "Zhenmin", ""]]}, {"id": "1905.11567", "submitter": "Qicheng Lao", "authors": "Qicheng Lao and Thomas Fevens", "title": "Case-Based Histopathological Malignancy Diagnosis using Convolutional\n  Neural Networks", "comments": null, "journal-ref": "British Machine Vision Conference (BMVC) 2017", "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In practice, histopathological diagnosis of tumor malignancy often requires a\nhuman expert to scan through histopathological images at multiple magnification\nlevels, after which a final diagnosis can be accurately determined. However,\nprevious research on such classification tasks using convolutional neural\nnetworks primarily determine a diagnosis for a single magnification level. In\nthis paper, we propose a case-based approach using deep residual neural\nnetworks for histopathological malignancy diagnosis, where a case is defined as\na sequence of images from the patient at all available levels of magnification.\nEffectively, through mimicking what a human expert would actually do, our\napproach makes a diagnosis decision based on features learned in combination at\nmultiple magnification levels. Our results show that the case-based approach\nachieves better performance than the state-of-the-art methods when evaluated on\nBreaKHis, a histopathological image dataset for breast tumors.\n", "versions": [{"version": "v1", "created": "Tue, 28 May 2019 01:49:34 GMT"}], "update_date": "2019-05-29", "authors_parsed": [["Lao", "Qicheng", ""], ["Fevens", "Thomas", ""]]}, {"id": "1905.11574", "submitter": "Minje Park", "authors": "Minje Park", "title": "JGAN: A Joint Formulation of GAN for Synthesizing Images and Labels", "comments": "To be published in IEEE Access", "journal-ref": null, "doi": "10.1109/ACCESS.2020.3031292", "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image generation with explicit condition or label generally works better than\nunconditional methods. In modern GAN frameworks, both generator and\ndiscriminator are formulated to model the conditional distribution of images\ngiven with labels. In this paper, we provide an alternative formulation of GAN\nwhich models the joint distribution of images and labels. There are two\nadvantages in this joint formulation over conditional approaches. The first\nadvantage is that the joint formulation is more robust to label noises if it's\nproperly modeled. This alleviates the burden of making noise-free labels and\nallows the use of weakly-supervised labels in image generation. The second is\nthat we can use any kinds of weak labels or image features that have\ncorrelations with the original image data to enhance unconditional image\ngeneration. We will show the effectiveness of our joint formulation on CIFAR10,\nCIFAR100, and STL dataset with the state-of-the-art GAN architecture.\n", "versions": [{"version": "v1", "created": "Tue, 28 May 2019 02:19:42 GMT"}, {"version": "v2", "created": "Thu, 13 Jun 2019 05:39:16 GMT"}, {"version": "v3", "created": "Wed, 15 Apr 2020 05:44:25 GMT"}, {"version": "v4", "created": "Mon, 27 Apr 2020 00:59:40 GMT"}, {"version": "v5", "created": "Wed, 21 Oct 2020 01:17:36 GMT"}], "update_date": "2020-10-22", "authors_parsed": [["Park", "Minje", ""]]}, {"id": "1905.11575", "submitter": "Rui Su", "authors": "Rui Su, Wanli Ouyang, Luping Zhou, Dong Xu", "title": "Improving Action Localization by Progressive Cross-stream Cooperation", "comments": "CVPR2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spatio-temporal action localization consists of three levels of tasks:\nspatial localization, action classification, and temporal segmentation. In this\nwork, we propose a new Progressive Cross-stream Cooperation (PCSC) framework to\nuse both region proposals and features from one stream (i.e. Flow/RGB) to help\nanother stream (i.e. RGB/Flow) to iteratively improve action localization\nresults and generate better bounding boxes in an iterative fashion.\nSpecifically, we first generate a larger set of region proposals by combining\nthe latest region proposals from both streams, from which we can readily obtain\na larger set of labelled training samples to help learn better action detection\nmodels. Second, we also propose a new message passing approach to pass\ninformation from one stream to another stream in order to learn better\nrepresentations, which also leads to better action detection models. As a\nresult, our iterative framework progressively improves action localization\nresults at the frame level. To improve action localization results at the video\nlevel, we additionally propose a new strategy to train class-specific\nactionness detectors for better temporal segmentation, which can be readily\nlearnt by focusing on \"confusing\" samples from the same action class.\nComprehensive experiments on two benchmark datasets UCF-101-24 and J-HMDB\ndemonstrate the effectiveness of our newly proposed approaches for\nspatio-temporal action localization in realistic scenarios.\n", "versions": [{"version": "v1", "created": "Tue, 28 May 2019 02:29:12 GMT"}], "update_date": "2019-05-29", "authors_parsed": [["Su", "Rui", ""], ["Ouyang", "Wanli", ""], ["Zhou", "Luping", ""], ["Xu", "Dong", ""]]}, {"id": "1905.11581", "submitter": "Chengxu Zhuang", "authors": "Chengxu Zhuang, Xuehao Ding, Divyanshu Murli, Daniel Yamins", "title": "Local Label Propagation for Large-Scale Semi-Supervised Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A significant issue in training deep neural networks to solve supervised\nlearning tasks is the need for large numbers of labelled datapoints. The goal\nof semi-supervised learning is to leverage ubiquitous unlabelled data, together\nwith small quantities of labelled data, to achieve high task performance.\nThough substantial recent progress has been made in developing semi-supervised\nalgorithms that are effective for comparatively small datasets, many of these\ntechniques do not scale readily to the large (unlaballed) datasets\ncharacteristic of real-world applications. In this paper we introduce a novel\napproach to scalable semi-supervised learning, called Local Label Propagation\n(LLP). Extending ideas from recent work on unsupervised embedding learning, LLP\nfirst embeds datapoints, labelled and otherwise, in a common latent space using\na deep neural network. It then propagates pseudolabels from known to unknown\ndatapoints in a manner that depends on the local geometry of the embedding,\ntaking into account both inter-point distance and local data density as a\nweighting on propagation likelihood. The parameters of the deep embedding are\nthen trained to simultaneously maximize pseudolabel categorization performance\nas well as a metric of the clustering of datapoints within each psuedo-label\ngroup, iteratively alternating stages of network training and label\npropagation. We illustrate the utility of the LLP method on the ImageNet\ndataset, achieving results that outperform previous state-of-the-art scalable\nsemi-supervised learning algorithms by large margins, consistently across a\nwide variety of training regimes. We also show that the feature representation\nlearned with LLP transfers well to scene recognition in the Places 205 dataset.\n", "versions": [{"version": "v1", "created": "Tue, 28 May 2019 02:57:42 GMT"}], "update_date": "2019-05-29", "authors_parsed": [["Zhuang", "Chengxu", ""], ["Ding", "Xuehao", ""], ["Murli", "Divyanshu", ""], ["Yamins", "Daniel", ""]]}, {"id": "1905.11595", "submitter": "Sreenithy Chandran", "authors": "Sreenithy Chandran, Suren Jayasuriya", "title": "Adaptive Lighting for Data-Driven Non-Line-of-Sight 3D Localization and\n  Object Identification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Non-line-of-sight (NLOS) imaging of objects not visible to either the camera\nor illumination source is a challenging task with vital applications including\nsurveillance and robotics. Recent NLOS reconstruction advances have been\nachieved using time-resolved measurements which requires expensive and\nspecialized detectors and laser sources. In contrast, we propose a data-driven\napproach for NLOS 3D localization and object identification requiring only a\nconventional camera and projector. To generalize to complex line-of-sight (LOS)\nscenes with non-planar surfaces and occlusions, we introduce an adaptive\nlighting algorithm. This algorithm, based on radiosity, identifies and\nilluminates scene patches in the LOS which most contribute to the NLOS light\npaths, and can factor in system power constraints. We achieve an average\nidentification of 87.1% object identification for four classes of objects, and\naverage localization of the NLOS object's centroid with a mean-squared error\n(MSE) of 1.97 cm in the occluded region for real data taken from a hardware\nprototype. These results demonstrate the advantage of combining the physics of\nlight transport with active illumination for data-driven NLOS imaging.\n", "versions": [{"version": "v1", "created": "Tue, 28 May 2019 03:40:19 GMT"}, {"version": "v2", "created": "Fri, 26 Jul 2019 23:57:04 GMT"}], "update_date": "2019-07-30", "authors_parsed": [["Chandran", "Sreenithy", ""], ["Jayasuriya", "Suren", ""]]}, {"id": "1905.11624", "submitter": "Zih-Siou Hung", "authors": "Zih-Siou Hung, Arun Mallya, Svetlana Lazebnik", "title": "Contextual Translation Embedding for Visual Relationship Detection and\n  Scene Graph Generation", "comments": null, "journal-ref": null, "doi": "10.1109/TPAMI.2020.2992222", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Relations amongst entities play a central role in image understanding. Due to\nthe complexity of modeling (subject, predicate, object) relation triplets, it\nis crucial to develop a method that can not only recognize seen relations, but\nalso generalize to unseen cases. Inspired by a previously proposed visual\ntranslation embedding model, or VTransE, we propose a context-augmented\ntranslation embedding model that can capture both common and rare relations.\nThe previous VTransE model maps entities and predicates into a low-dimensional\nembedding vector space where the predicate is interpreted as a translation\nvector between the embedded features of the bounding box regions of the subject\nand the object. Our model additionally incorporates the contextual information\ncaptured by the bounding box of the union of the subject and the object, and\nlearns the embeddings guided by the constraint predicate $\\approx$ union\n(subject, object) $-$ subject $-$ object. In a comprehensive evaluation on\nmultiple challenging benchmarks, our approach outperforms previous\ntranslation-based models and comes close to or exceeds the state of the art\nacross a range of settings, from small-scale to large-scale datasets, from\ncommon to previously unseen relations. It also achieves promising results for\nthe recently introduced task of scene graph generation.\n", "versions": [{"version": "v1", "created": "Tue, 28 May 2019 06:10:02 GMT"}, {"version": "v2", "created": "Wed, 20 Nov 2019 16:15:55 GMT"}, {"version": "v3", "created": "Thu, 6 Feb 2020 06:53:33 GMT"}], "update_date": "2020-11-19", "authors_parsed": [["Hung", "Zih-Siou", ""], ["Mallya", "Arun", ""], ["Lazebnik", "Svetlana", ""]]}, {"id": "1905.11634", "submitter": "Songyang Zhang", "authors": "Songyang Zhang, Shipeng Yan, Xuming He", "title": "LatentGNN: Learning Efficient Non-local Relations for Visual Recognition", "comments": "ICML 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Capturing long-range dependencies in feature representations is crucial for\nmany visual recognition tasks. Despite recent successes of deep convolutional\nnetworks, it remains challenging to model non-local context relations between\nvisual features. A promising strategy is to model the feature context by a\nfully-connected graph neural network (GNN), which augments traditional\nconvolutional features with an estimated non-local context representation.\nHowever, most GNN-based approaches require computing a dense graph affinity\nmatrix and hence have difficulty in scaling up to tackle complex real-world\nvisual problems. In this work, we propose an efficient and yet flexible\nnon-local relation representation based on a novel class of graph neural\nnetworks. Our key idea is to introduce a latent space to reduce the complexity\nof graph, which allows us to use a low-rank representation for the graph\naffinity matrix and to achieve a linear complexity in computation. Extensive\nexperimental evaluations on three major visual recognition tasks show that our\nmethod outperforms the prior works with a large margin while maintaining a low\ncomputation cost.\n", "versions": [{"version": "v1", "created": "Tue, 28 May 2019 06:42:23 GMT"}], "update_date": "2019-05-29", "authors_parsed": [["Zhang", "Songyang", ""], ["Yan", "Shipeng", ""], ["He", "Xuming", ""]]}, {"id": "1905.11641", "submitter": "Zitian Chen", "authors": "Zitian Chen, Yanwei Fu, Yu-Xiong Wang, Lin Ma, Wei Liu, Martial Hebert", "title": "Image Deformation Meta-Networks for One-Shot Learning", "comments": "Oral at CVPR2019. Code is available at\n  https://github.com/tankche1/IDeMe-Net", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Humans can robustly learn novel visual concepts even when images undergo\nvarious deformations and lose certain information. Mimicking the same behavior\nand synthesizing deformed instances of new concepts may help visual recognition\nsystems perform better one-shot learning, i.e., learning concepts from one or\nfew examples. Our key insight is that, while the deformed images may not be\nvisually realistic, they still maintain critical semantic information and\ncontribute significantly to formulating classifier decision boundaries.\nInspired by the recent progress of meta-learning, we combine a meta-learner\nwith an image deformation sub-network that produces additional training\nexamples, and optimize both models in an end-to-end manner. The deformation\nsub-network learns to deform images by fusing a pair of images --- a probe\nimage that keeps the visual content and a gallery image that diversifies the\ndeformations. We demonstrate results on the widely used one-shot learning\nbenchmarks (miniImageNet and ImageNet 1K Challenge datasets), which\nsignificantly outperform state-of-the-art approaches. Code is available at\nhttps://github.com/tankche1/IDeMe-Net.\n", "versions": [{"version": "v1", "created": "Tue, 28 May 2019 06:56:52 GMT"}, {"version": "v2", "created": "Thu, 18 Jul 2019 03:51:34 GMT"}], "update_date": "2019-07-19", "authors_parsed": [["Chen", "Zitian", ""], ["Fu", "Yanwei", ""], ["Wang", "Yu-Xiong", ""], ["Ma", "Lin", ""], ["Liu", "Wei", ""], ["Hebert", "Martial", ""]]}, {"id": "1905.11651", "submitter": "Wojciech Michal Matkowski", "authors": "Wojciech Michal Matkowski, Krzysztof Matkowski, Adams Wai-Kin Kong,\n  Cory Lloyd Hall", "title": "The Nipple-Areola Complex for Criminal Identification", "comments": "Accepted in the International Conference on Biometrics (ICB 2019),\n  scheduled for 4-7 June 2019 in Crete, Greece", "journal-ref": null, "doi": "10.1109/ICB45273.2019.8987341", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In digital and multimedia forensics, identification of child sexual offenders\nbased on digital evidence images is highly challenging due to the fact that the\noffender's face or other obvious characteristics such as tattoos are occluded,\ncovered, or not visible at all. Nevertheless, other naked body parts, e.g.,\nchest are still visible. Some researchers proposed skin marks, skin texture,\nvein or androgenic hair patterns for criminal and victim identification. There\nare no available studies of nipple-areola complex (NAC) for offender\nidentification. In this paper, we present a study of offender identification\nbased on the NAC, and we present NTU-Nipple-v1 dataset, which contains 2732\nimages of 428 different male nipple-areolae. Popular deep learning and\nhand-crafted recognition methods are evaluated on the provided dataset. The\nresults indicate that the NAC can be a useful characteristic for offender\nidentification.\n", "versions": [{"version": "v1", "created": "Tue, 28 May 2019 07:27:06 GMT"}], "update_date": "2020-02-18", "authors_parsed": [["Matkowski", "Wojciech Michal", ""], ["Matkowski", "Krzysztof", ""], ["Kong", "Adams Wai-Kin", ""], ["Hall", "Cory Lloyd", ""]]}, {"id": "1905.11656", "submitter": "Yoonho Lee", "authors": "Yoonho Lee, Wonjae Kim, Wonpyo Park, Seungjin Choi", "title": "Discrete Infomax Codes for Supervised Representation Learning", "comments": "19 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning compact discrete representations of data is a key task on its own or\nfor facilitating subsequent processing of data. In this paper we present a\nmodel that produces Discrete InfoMax Codes (DIMCO); we learn a probabilistic\nencoder that yields k-way d-dimensional codes associated with input data. Our\nmodel's learning objective is to maximize the mutual information between codes\nand labels with a regularization, which enforces entries of a codeword to be as\nindependent as possible. We show that the infomax principle also justifies\nprevious loss functions (e.g., cross-entropy) as its special cases. Our\nanalysis also shows that using shorter codes, as DIMCO does, reduces\noverfitting in the context of few-shot classification. Through experiments in\nvarious domains, we observe this implicit meta-regularization effect of DIMCO.\nFurthermore, we show that the codes learned by DIMCO are efficient in terms of\nboth memory and retrieval time compared to previous methods.\n", "versions": [{"version": "v1", "created": "Tue, 28 May 2019 07:38:35 GMT"}, {"version": "v2", "created": "Mon, 24 Feb 2020 04:21:53 GMT"}], "update_date": "2020-02-25", "authors_parsed": [["Lee", "Yoonho", ""], ["Kim", "Wonjae", ""], ["Park", "Wonpyo", ""], ["Choi", "Seungjin", ""]]}, {"id": "1905.11664", "submitter": "Jiashi Li", "authors": "Jiashi Li, Qi Qi, Jingyu Wang, Ce Ge, Yujian Li, Zhangzhang Yue, and\n  Haifeng Sun", "title": "OICSR: Out-In-Channel Sparsity Regularization for Compact Deep Neural\n  Networks", "comments": "Accepted to CVPR 2019, the pruned ResNet-50 model has be released at:\n  https://github.com/dsfour/OICSR", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Channel pruning can significantly accelerate and compress deep neural\nnetworks. Many channel pruning works utilize structured sparsity regularization\nto zero out all the weights in some channels and automatically obtain\nstructure-sparse network in training stage. However, these methods apply\nstructured sparsity regularization on each layer separately where the\ncorrelations between consecutive layers are omitted. In this paper, we first\ncombine one out-channel in current layer and the corresponding in-channel in\nnext layer as a regularization group, namely out-in-channel. Our proposed\nOut-In-Channel Sparsity Regularization (OICSR) considers correlations between\nsuccessive layers to further retain predictive power of the compact network.\nTraining with OICSR thoroughly transfers discriminative features into a\nfraction of out-in-channels. Correspondingly, OICSR measures channel importance\nbased on statistics computed from two consecutive layers, not individual layer.\nFinally, a global greedy pruning algorithm is designed to remove redundant\nout-in-channels in an iterative way. Our method is comprehensively evaluated\nwith various CNN architectures including CifarNet, AlexNet, ResNet, DenseNet\nand PreActSeNet on CIFAR-10, CIFAR-100 and ImageNet-1K datasets. Notably, on\nImageNet-1K, we reduce 37.2% FLOPs on ResNet-50 while outperforming the\noriginal model by 0.22% top-1 accuracy.\n", "versions": [{"version": "v1", "created": "Tue, 28 May 2019 08:09:04 GMT"}, {"version": "v2", "created": "Wed, 29 May 2019 03:53:14 GMT"}, {"version": "v3", "created": "Sat, 1 Jun 2019 10:36:04 GMT"}, {"version": "v4", "created": "Thu, 6 Jun 2019 03:40:27 GMT"}, {"version": "v5", "created": "Mon, 1 Jul 2019 03:53:23 GMT"}], "update_date": "2019-07-02", "authors_parsed": [["Li", "Jiashi", ""], ["Qi", "Qi", ""], ["Wang", "Jingyu", ""], ["Ge", "Ce", ""], ["Li", "Yujian", ""], ["Yue", "Zhangzhang", ""], ["Sun", "Haifeng", ""]]}, {"id": "1905.11666", "submitter": "Wonjae Kim", "authors": "Wonjae Kim and Yoonho Lee", "title": "Learning Dynamics of Attention: Human Prior for Interpretable Machine\n  Reasoning", "comments": "20 pages, 18 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Without relevant human priors, neural networks may learn uninterpretable\nfeatures. We propose Dynamics of Attention for Focus Transition (DAFT) as a\nhuman prior for machine reasoning. DAFT is a novel method that regularizes\nattention-based reasoning by modelling it as a continuous dynamical system\nusing neural ordinary differential equations. As a proof of concept, we augment\na state-of-the-art visual reasoning model with DAFT. Our experiments reveal\nthat applying DAFT yields similar performance to the original model while using\nfewer reasoning steps, showing that it implicitly learns to skip unnecessary\nsteps. We also propose a new metric, Total Length of Transition (TLT), which\nrepresents the effective reasoning step size by quantifying how much a given\nmodel's focus drifts while reasoning about a question. We show that adding DAFT\nresults in lower TLT, demonstrating that our method indeed obeys the human\nprior towards shorter reasoning paths in addition to producing more\ninterpretable attention maps. Our code is available at\nhttps://github.com/kakao/DAFT.\n", "versions": [{"version": "v1", "created": "Tue, 28 May 2019 08:13:37 GMT"}, {"version": "v2", "created": "Sat, 28 Sep 2019 05:50:25 GMT"}, {"version": "v3", "created": "Mon, 23 Dec 2019 05:37:28 GMT"}], "update_date": "2019-12-24", "authors_parsed": [["Kim", "Wonjae", ""], ["Lee", "Yoonho", ""]]}, {"id": "1905.11672", "submitter": "Paul Hand", "authors": "Muhammad Asim, Max Daniels, Oscar Leong, Ali Ahmed, Paul Hand", "title": "Invertible generative models for inverse problems: mitigating\n  representation error and dataset bias", "comments": "Camera ready version for ICML 2020, paper 2655", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Trained generative models have shown remarkable performance as priors for\ninverse problems in imaging -- for example, Generative Adversarial Network\npriors permit recovery of test images from 5-10x fewer measurements than\nsparsity priors. Unfortunately, these models may be unable to represent any\nparticular image because of architectural choices, mode collapse, and bias in\nthe training dataset. In this paper, we demonstrate that invertible neural\nnetworks, which have zero representation error by design, can be effective\nnatural signal priors at inverse problems such as denoising, compressive\nsensing, and inpainting. Given a trained generative model, we study the\nempirical risk formulation of the desired inverse problem under a\nregularization that promotes high likelihood images, either directly by\npenalization or algorithmically by initialization. For compressive sensing,\ninvertible priors can yield higher accuracy than sparsity priors across almost\nall undersampling ratios, and due to their lack of representation error,\ninvertible priors can yield better reconstructions than GAN priors for images\nthat have rare features of variation within the biased training set, including\nout-of-distribution natural images. We additionally compare performance for\ncompressive sensing to unlearned methods, such as the deep decoder, and we\nestablish theoretical bounds on expected recovery error in the case of a linear\ninvertible model.\n", "versions": [{"version": "v1", "created": "Tue, 28 May 2019 08:27:53 GMT"}, {"version": "v2", "created": "Wed, 21 Aug 2019 07:28:06 GMT"}, {"version": "v3", "created": "Fri, 27 Sep 2019 09:12:10 GMT"}, {"version": "v4", "created": "Sun, 12 Jul 2020 22:27:29 GMT"}], "update_date": "2020-07-14", "authors_parsed": [["Asim", "Muhammad", ""], ["Daniels", "Max", ""], ["Leong", "Oscar", ""], ["Ahmed", "Ali", ""], ["Hand", "Paul", ""]]}, {"id": "1905.11697", "submitter": "Daniel Worrall", "authors": "Daniel E. Worrall, Max Welling", "title": "Deep Scale-spaces: Equivariance Over Scale", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce deep scale-spaces (DSS), a generalization of convolutional\nneural networks, exploiting the scale symmetry structure of conventional image\nrecognition tasks. Put plainly, the class of an image is invariant to the scale\nat which it is viewed. We construct scale equivariant cross-correlations based\non a principled extension of convolutions, grounded in the theory of\nscale-spaces and semigroups. As a very basic operation, these\ncross-correlations can be used in almost any modern deep learning architecture\nin a plug-and-play manner. We demonstrate our networks on the Patch Camelyon\nand Cityscapes datasets, to prove their utility and perform introspective\nstudies to further understand their properties.\n", "versions": [{"version": "v1", "created": "Tue, 28 May 2019 09:16:56 GMT"}], "update_date": "2019-05-29", "authors_parsed": [["Worrall", "Daniel E.", ""], ["Welling", "Max", ""]]}, {"id": "1905.11731", "submitter": "Sze Teng Liong", "authors": "Sze-Teng Liong, Y.S. Gan, Yen-Chang Huang, Kun-Hong Liu, Wei-Chuen Yau", "title": "Integrated Neural Network and Machine Vision Approach For Leather Defect\n  Classification", "comments": "12 pages, 8 tables, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Leather is a type of natural, durable, flexible, soft, supple and pliable\nmaterial with smooth texture. It is commonly used as a raw material to\nmanufacture luxury consumer goods for high-end customers. To ensure good\nquality control on the leather products, one of the critical processes is the\nvisual inspection step to spot the random defects on the leather surfaces and\nit is usually conducted by experienced experts. This paper presents an\nautomatic mechanism to perform the leather defect classification. In\nparticular, we focus on detecting tick-bite defects on a specific type of calf\nleather. Both the handcrafted feature extractors (i.e., edge detectors and\nstatistical approach) and data-driven (i.e., artificial neural network) methods\nare utilized to represent the leather patches. Then, multiple classifiers\n(i.e., decision trees, Support Vector Machines, nearest neighbour and ensemble\nclassifiers) are exploited to determine whether the test sample patches contain\ndefective segments. Using the proposed method, we managed to get a\nclassification accuracy rate of 84% from a sample of approximately 2500 pieces\nof 400 * 400 leather patches.\n", "versions": [{"version": "v1", "created": "Tue, 28 May 2019 10:37:58 GMT"}], "update_date": "2019-05-29", "authors_parsed": [["Liong", "Sze-Teng", ""], ["Gan", "Y. S.", ""], ["Huang", "Yen-Chang", ""], ["Liu", "Kun-Hong", ""], ["Yau", "Wei-Chuen", ""]]}, {"id": "1905.11736", "submitter": "Muzammal Naseer", "authors": "Muzammal Naseer, Salman H. Khan, Harris Khan, Fahad Shahbaz Khan,\n  Fatih Porikli", "title": "Cross-Domain Transferability of Adversarial Perturbations", "comments": "Accepted at NeurIPS 2019 (Camera Ready). Source Code along with\n  pretrained adversarial generators is available at\n  https://github.com/Muzammal-Naseer/Cross-domain-perturbations", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adversarial examples reveal the blind spots of deep neural networks (DNNs)\nand represent a major concern for security-critical applications. The\ntransferability of adversarial examples makes real-world attacks possible in\nblack-box settings, where the attacker is forbidden to access the internal\nparameters of the model. The underlying assumption in most adversary generation\nmethods, whether learning an instance-specific or an instance-agnostic\nperturbation, is the direct or indirect reliance on the original\ndomain-specific data distribution. In this work, for the first time, we\ndemonstrate the existence of domain-invariant adversaries, thereby showing\ncommon adversarial space among different datasets and models. To this end, we\npropose a framework capable of launching highly transferable attacks that\ncrafts adversarial patterns to mislead networks trained on wholly different\ndomains. For instance, an adversarial function learned on Paintings, Cartoons\nor Medical images can successfully perturb ImageNet samples to fool the\nclassifier, with success rates as high as $\\sim$99\\% ($\\ell_{\\infty} \\le 10$).\nThe core of our proposed adversarial function is a generative network that is\ntrained using a relativistic supervisory signal that enables domain-invariant\nperturbations. Our approach sets the new state-of-the-art for fooling rates,\nboth under the white-box and black-box scenarios. Furthermore, despite being an\ninstance-agnostic perturbation function, our attack outperforms the\nconventionally much stronger instance-specific attack methods.\n", "versions": [{"version": "v1", "created": "Tue, 28 May 2019 11:00:34 GMT"}, {"version": "v2", "created": "Thu, 30 May 2019 18:12:03 GMT"}, {"version": "v3", "created": "Mon, 10 Jun 2019 10:55:17 GMT"}, {"version": "v4", "created": "Fri, 4 Oct 2019 16:39:31 GMT"}, {"version": "v5", "created": "Mon, 14 Oct 2019 19:13:37 GMT"}], "update_date": "2019-10-16", "authors_parsed": [["Naseer", "Muzammal", ""], ["Khan", "Salman H.", ""], ["Khan", "Harris", ""], ["Khan", "Fahad Shahbaz", ""], ["Porikli", "Fatih", ""]]}, {"id": "1905.11739", "submitter": "Deepayan Das", "authors": "Deepayan Das, Jerin Philip, Minesh Mathew and C. V. Jawahar", "title": "A Cost Efficient Approach to Correct OCR Errors in Large Document\n  Collections", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Word error rate of an ocr is often higher than its character error rate. This\nis especially true when ocrs are designed by recognizing characters. High word\naccuracies are critical to tasks like the creation of content in digital\nlibraries and text-to-speech applications. In order to detect and correct the\nmisrecognised words, it is common for an ocr module to employ a post-processor\nto further improve the word accuracy. However, conventional approaches to\npost-processing like looking up a dictionary or using a statistical language\nmodel (slm), are still limited. In many such scenarios, it is often required to\nremove the outstanding errors manually. We observe that the traditional\npost-processing schemes look at error words sequentially since ocrs process\ndocuments one at a time. We propose a cost-efficient model to address the error\nwords in batches rather than correcting them individually. We exploit the fact\nthat a collection of documents, unlike a single document, has a structure\nleading to repetition of words. Such words, if efficiently grouped together and\ncorrected as a whole can lead to a significant reduction in the cost.\nCorrection can be fully automatic or with a human in the loop. Towards this, we\nemploy a novel clustering scheme to obtain fairly homogeneous clusters. We\ncompare the performance of our model with various baseline approaches including\nthe case where all the errors are removed by a human. We demonstrate the\nefficacy of our solution empirically by reporting more than 70% reduction in\nthe human effort with near perfect error correction. We validate our method on\nBooks from multiple languages.\n", "versions": [{"version": "v1", "created": "Tue, 28 May 2019 11:11:57 GMT"}], "update_date": "2019-05-29", "authors_parsed": [["Das", "Deepayan", ""], ["Philip", "Jerin", ""], ["Mathew", "Minesh", ""], ["Jawahar", "C. V.", ""]]}, {"id": "1905.11773", "submitter": "Amir Bar", "authors": "David Chettrit, Orna Bregman Amitai, Itamar Tamir, Amir Bar, Eldad\n  Elnekave", "title": "PHT-bot: Deep-Learning based system for automatic risk stratification of\n  COPD patients based upon signs of Pulmonary Hypertension", "comments": null, "journal-ref": "Proc. SPIE 10950, Medical Imaging 2019: Computer-Aided Diagnosis,\n  109500O", "doi": "10.1117/12.2512469", "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Chronic Obstructive Pulmonary Disease (COPD) is a leading cause of morbidity\nand mortality worldwide. Identifying those at highest risk of deterioration\nwould allow more effective distribution of preventative and surveillance\nresources. Secondary pulmonary hypertension is a manifestation of advanced\nCOPD, which can be reliably diagnosed by the main Pulmonary Artery (PA) to\nAscending Aorta (Ao) ratio. In effect, a PA diameter to Ao diameter ratio of\ngreater than 1 has been demonstrated to be a reliable marker of increased\npulmonary arterial pressure. Although clinically valuable and readily\nvisualized, the manual assessment of the PA and the Ao diameters is time\nconsuming and under-reported. The present study describes a non invasive method\nto measure the diameters of both the Ao and the PA from contrast-enhanced chest\nComputed Tomography (CT). The solution applies deep learning techniques in\norder to select the correct axial slice to measure, and to segment both\narteries. The system achieves test Pearson correlation coefficient scores of\n93% for the Ao and 92% for the PA. To the best of our knowledge, it is the\nfirst such fully automated solution.\n", "versions": [{"version": "v1", "created": "Tue, 28 May 2019 12:39:05 GMT"}], "update_date": "2019-05-29", "authors_parsed": [["Chettrit", "David", ""], ["Amitai", "Orna Bregman", ""], ["Tamir", "Itamar", ""], ["Bar", "Amir", ""], ["Elnekave", "Eldad", ""]]}, {"id": "1905.11775", "submitter": "Pekka Siirtola", "authors": "Pekka Siirtola, Heli Koskim\\\"aki, Juha R\\\"oning", "title": "Importance of user inputs while using incremental learning to\n  personalize human activity recognition models", "comments": "European Symposium on Artificial Neural Networks, Computational\n  Intelligence and Machine Learning (ESANN) 2019, pages 449-454", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.HC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this study, importance of user inputs is studied in the context of\npersonalizing human activity recognition models using incremental learning.\nInertial sensor data from three body positions are used, and the classification\nis based on Learn++ ensemble method. Three different approaches to update\nmodels are compared: non-supervised, semi-supervised and supervised.\nNon-supervised approach relies fully on predicted labels, supervised fully on\nuser labeled data, and the proposed method for semi-supervised learning, is a\ncombination of these two. In fact, our experiments show that by relying on\npredicted labels with high confidence, and asking the user to label only\nuncertain observations (from 12% to 26% of the observations depending on the\nused base classifier), almost as low error rates can be achieved as by using\nsupervised approach. In fact, the difference was less than 2%-units. Moreover,\nunlike non-supervised approach, semi-supervised approach does not suffer from\ndrastic concept drift, and thus, the error rate of the non-supervised approach\nis over 5%-units higher than using semi-supervised approach.\n", "versions": [{"version": "v1", "created": "Tue, 28 May 2019 12:41:02 GMT"}], "update_date": "2019-05-30", "authors_parsed": [["Siirtola", "Pekka", ""], ["Koskim\u00e4ki", "Heli", ""], ["R\u00f6ning", "Juha", ""]]}, {"id": "1905.11780", "submitter": "Pekka Siirtola", "authors": "Pekka Siirtola, Jukka Komulainen, Vili Kellokumpu", "title": "Effect of context in swipe gesture-based continuous authentication on\n  smartphones", "comments": "European Symposium on Artificial Neural Networks, Computational\n  Intelligence and Machine Learning (ESANN) 2018, pages 639-644", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work investigates how context should be taken into account when\nperforming continuous authentication of a smartphone user based on touchscreen\nand accelerometer readings extracted from swipe gestures. The study is\nconducted on the publicly available HMOG dataset consisting of 100 study\nsubjects performing pre-defined reading and navigation tasks while sitting and\nwalking. It is shown that context-specific models are needed for different\nsmartphone usage and human activity scenarios to minimize authentication error.\nAlso, the experimental results suggests that utilization of phone movement\nimproves swipe gesture-based verification performance only when the user is\nmoving.\n", "versions": [{"version": "v1", "created": "Tue, 28 May 2019 12:49:55 GMT"}], "update_date": "2019-05-30", "authors_parsed": [["Siirtola", "Pekka", ""], ["Komulainen", "Jukka", ""], ["Kellokumpu", "Vili", ""]]}, {"id": "1905.11781", "submitter": "Zhengguang Zhou", "authors": "Zhengguang Zhou, Wengang Zhou, Xutao Lv, Xuan Huang, Xiaoyu Wang,\n  Houqiang Li", "title": "Progressive Learning of Low-Precision Networks", "comments": "10 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent years have witnessed the great advance of deep learning in a variety\nof vision tasks. Many state-of-the-art deep neural networks suffer from large\nsize and high complexity, which makes it difficult to deploy in\nresource-limited platforms such as mobile devices.\n  To this end, low-precision neural networks are widely studied which quantize\nweights or activations into the low-bit format.\n  Though being efficient, low-precision networks are usually hard to train and\nencounter severe accuracy degradation.\n  In this paper, we propose a new training strategy through expanding\nlow-precision networks during training and removing the expanded parts for\nnetwork inference.\n  First, we equip each low-precision convolutional layer with an ancillary\nfull-precision convolutional layer based on a low-precision network structure,\nwhich could guide the network to good local minima.\n  Second, a decay method is introduced to reduce the output of the added\nfull-precision convolution gradually, which keeps the resulted topology\nstructure the same to the original low-precision one.\n  Experiments on SVHN, CIFAR and ILSVRC-2012 datasets prove that the proposed\nmethod can bring faster convergence and higher accuracy for low-precision\nneural networks.\n", "versions": [{"version": "v1", "created": "Tue, 28 May 2019 12:50:01 GMT"}], "update_date": "2019-05-29", "authors_parsed": [["Zhou", "Zhengguang", ""], ["Zhou", "Wengang", ""], ["Lv", "Xutao", ""], ["Huang", "Xuan", ""], ["Wang", "Xiaoyu", ""], ["Li", "Houqiang", ""]]}, {"id": "1905.11784", "submitter": "Nour Karessli", "authors": "Nour Karessli, Romain Guigour\\`es, Reza Shirvany", "title": "SizeNet: Weakly Supervised Learning of Visual Size and Fit in Fashion\n  Images", "comments": "IEEE Conference on Computer Vision and Pattern Recognition Workshop\n  (CVPRW) 2019 Focus on Fashion and Subjective Search - Understanding\n  Subjective Attributes of Data (FFSS-USAD)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Finding clothes that fit is a hot topic in the e-commerce fashion industry.\nMost approaches addressing this problem are based on statistical methods\nrelying on historical data of articles purchased and returned to the store.\nSuch approaches suffer from the cold start problem for the thousands of\narticles appearing on the shopping platforms every day, for which no prior\npurchase history is available. We propose to employ visual data to infer size\nand fit characteristics of fashion articles. We introduce SizeNet, a\nweakly-supervised teacher-student training framework that leverages the power\nof statistical models combined with the rich visual information from article\nimages to learn visual cues for size and fit characteristics, capable of\ntackling the challenging cold start problem. Detailed experiments are performed\non thousands of textile garments, including dresses, trousers, knitwear, tops,\netc. from hundreds of different brands.\n", "versions": [{"version": "v1", "created": "Tue, 28 May 2019 12:58:06 GMT"}], "update_date": "2019-05-29", "authors_parsed": [["Karessli", "Nour", ""], ["Guigour\u00e8s", "Romain", ""], ["Shirvany", "Reza", ""]]}, {"id": "1905.11787", "submitter": "Zhengguang Zhou", "authors": "Zhengguang Zhou, Wengang Zhou, Richang Hong, Houqiang Li", "title": "Online Filter Clustering and Pruning for Efficient Convnets", "comments": "5 pages, 4 figures", "journal-ref": "2018 IEEE International Conference on Image Processing", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pruning filters is an effective method for accelerating deep neural networks\n(DNNs), but most existing approaches prune filters on a pre-trained network\ndirectly which limits in acceleration. Although each filter has its own effect\nin DNNs, but if two filters are the same with each other, we could prune one\nsafely. In this paper, we add an extra cluster loss term in the loss function\nwhich can force filters in each cluster to be similar online. After training,\nwe keep one filter in each cluster and prune others and fine-tune the pruned\nnetwork to compensate for the loss. Particularly, the clusters in every layer\ncan be defined firstly which is effective for pruning DNNs within residual\nblocks. Extensive experiments on CIFAR10 and CIFAR100 benchmarks demonstrate\nthe competitive performance of our proposed filter pruning method.\n", "versions": [{"version": "v1", "created": "Tue, 28 May 2019 13:01:39 GMT"}], "update_date": "2019-05-29", "authors_parsed": [["Zhou", "Zhengguang", ""], ["Zhou", "Wengang", ""], ["Hong", "Richang", ""], ["Li", "Houqiang", ""]]}, {"id": "1905.11799", "submitter": "Yongyi Tang", "authors": "Yongyi Tang, Lin Ma and Lianqiang Zhou", "title": "Hallucinating Optical Flow Features for Video Classification", "comments": "Accepted by IJCAI 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Appearance and motion are two key components to depict and characterize the\nvideo content. Currently, the two-stream models have achieved state-of-the-art\nperformances on video classification. However, extracting motion information,\nspecifically in the form of optical flow features, is extremely computationally\nexpensive, especially for large-scale video classification. In this paper, we\npropose a motion hallucination network, namely MoNet, to imagine the optical\nflow features from the appearance features, with no reliance on the optical\nflow computation. Specifically, MoNet models the temporal relationships of the\nappearance features and exploits the contextual relationships of the optical\nflow features with concurrent connections. Extensive experimental results\ndemonstrate that the proposed MoNet can effectively and efficiently hallucinate\nthe optical flow features, which together with the appearance features\nconsistently improve the video classification performances. Moreover, MoNet can\nhelp cutting down almost a half of computational and data-storage burdens for\nthe two-stream video classification. Our code is available at:\nhttps://github.com/YongyiTang92/MoNet-Features.\n", "versions": [{"version": "v1", "created": "Tue, 28 May 2019 13:25:40 GMT"}, {"version": "v2", "created": "Fri, 21 Jun 2019 04:40:06 GMT"}], "update_date": "2019-06-24", "authors_parsed": [["Tang", "Yongyi", ""], ["Ma", "Lin", ""], ["Zhou", "Lianqiang", ""]]}, {"id": "1905.11805", "submitter": "Jiangning Zhang", "authors": "Jiangning Zhang, Xianfang Zeng, Mengmeng Wang, Yusu Pan, Liang Liu,\n  Yong Liu, Yu Ding, Changjie Fan", "title": "FReeNet: Multi-Identity Face Reenactment", "comments": "Add more experiments; Revise the paper carefully;", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a novel multi-identity face reenactment framework, named\nFReeNet, to transfer facial expressions from an arbitrary source face to a\ntarget face with a shared model. The proposed FReeNet consists of two parts:\nUnified Landmark Converter (ULC) and Geometry-aware Generator (GAG). The ULC\nadopts an encode-decoder architecture to efficiently convert expression in a\nlatent landmark space, which significantly narrows the gap of the face contour\nbetween source and target identities. The GAG leverages the converted landmark\nto reenact the photorealistic image with a reference image of the target\nperson. Moreover, a new triplet perceptual loss is proposed to force the GAG\nmodule to learn appearance and geometry information simultaneously, which also\nenriches facial details of the reenacted images. Further experiments\ndemonstrate the superiority of our approach for generating photorealistic and\nexpression-alike faces, as well as the flexibility for transferring facial\nexpressions between identities.\n", "versions": [{"version": "v1", "created": "Tue, 28 May 2019 13:34:57 GMT"}, {"version": "v2", "created": "Sat, 16 May 2020 07:29:20 GMT"}], "update_date": "2020-05-19", "authors_parsed": [["Zhang", "Jiangning", ""], ["Zeng", "Xianfang", ""], ["Wang", "Mengmeng", ""], ["Pan", "Yusu", ""], ["Liu", "Liang", ""], ["Liu", "Yong", ""], ["Ding", "Yu", ""], ["Fan", "Changjie", ""]]}, {"id": "1905.11826", "submitter": "Adam Kortylewski", "authors": "Adam Kortylewski, Qing Liu, Huiyu Wang, Zhishuai Zhang, Alan Yuille", "title": "Combining Compositional Models and Deep Networks For Robust Object\n  Classification under Occlusion", "comments": "WACV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep convolutional neural networks (DCNNs) are powerful models that yield\nimpressive results at object classification. However, recent work has shown\nthat they do not generalize well to partially occluded objects and to mask\nattacks. In contrast to DCNNs, compositional models are robust to partial\nocclusion, however, they are not as discriminative as deep models. In this\nwork, we combine DCNNs and compositional object models to retain the best of\nboth approaches: a discriminative model that is robust to partial occlusion and\nmask attacks. Our model is learned in two steps. First, a standard DCNN is\ntrained for image classification. Subsequently, we cluster the DCNN features\ninto dictionaries. We show that the dictionary components resemble object part\ndetectors and learn the spatial distribution of parts for each object class. We\npropose mixtures of compositional models to account for large changes in the\nspatial activation patterns (e.g. due to changes in the 3D pose of an object).\nAt runtime, an image is first classified by the DCNN in a feedforward manner.\nThe prediction uncertainty is used to detect partially occluded objects, which\nin turn are classified by the compositional model. Our experimental results\ndemonstrate that combining compositional models and DCNNs resolves a\nfundamental problem of current deep learning approaches to computer vision: The\ncombined model recognizes occluded objects, even when it has not been exposed\nto occluded objects during training, while at the same time maintaining high\ndiscriminative performance for non-occluded objects.\n", "versions": [{"version": "v1", "created": "Tue, 28 May 2019 14:03:46 GMT"}, {"version": "v2", "created": "Wed, 29 May 2019 00:45:58 GMT"}, {"version": "v3", "created": "Wed, 13 Nov 2019 16:07:23 GMT"}, {"version": "v4", "created": "Wed, 29 Jan 2020 14:42:08 GMT"}], "update_date": "2020-01-30", "authors_parsed": [["Kortylewski", "Adam", ""], ["Liu", "Qing", ""], ["Wang", "Huiyu", ""], ["Zhang", "Zhishuai", ""], ["Yuille", "Alan", ""]]}, {"id": "1905.11832", "submitter": "Matthew Inkawhich", "authors": "Matthew Inkawhich, Yiran Chen, Hai Li", "title": "Snooping Attacks on Deep Reinforcement Learning", "comments": "13 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adversarial attacks have exposed a significant security vulnerability in\nstate-of-the-art machine learning models. Among these models include deep\nreinforcement learning agents. The existing methods for attacking reinforcement\nlearning agents assume the adversary either has access to the target agent's\nlearned parameters or the environment that the agent interacts with. In this\nwork, we propose a new class of threat models, called snooping threat models,\nthat are unique to reinforcement learning. In these snooping threat models, the\nadversary does not have the ability to interact with the target agent's\nenvironment, and can only eavesdrop on the action and reward signals being\nexchanged between agent and environment. We show that adversaries operating in\nthese highly constrained threat models can still launch devastating attacks\nagainst the target agent by training proxy models on related tasks and\nleveraging the transferability of adversarial examples.\n", "versions": [{"version": "v1", "created": "Tue, 28 May 2019 14:11:16 GMT"}, {"version": "v2", "created": "Wed, 15 Jan 2020 23:59:17 GMT"}], "update_date": "2020-01-17", "authors_parsed": [["Inkawhich", "Matthew", ""], ["Chen", "Yiran", ""], ["Li", "Hai", ""]]}, {"id": "1905.11862", "submitter": "Peixian Chen", "authors": "Peixian Chen, Pingyang Dai, Qiong Wu, Yuyu Huang", "title": "Video-based Person Re-identification with Two-stream Convolutional\n  Network and Co-attentive Snippet Embedding", "comments": "5 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, the applications of person re-identification in visual surveillance\nand human-computer interaction are sharply increasing, which signifies the\ncritical role of such a problem. In this paper, we propose a two-stream\nconvolutional network (ConvNet) based on the competitive similarity aggregation\nscheme and co-attentive embedding strategy for video-based person\nre-identification. By dividing the long video sequence into multiple short\nvideo snippets, we manage to utilize every snippet's RGB frames, optical flow\nmaps and pose maps to facilitate residual networks, e.g., ResNet, for feature\nextraction in the two-stream ConvNet. The extracted features are embedded by\nthe co-attentive embedding method, which allows for the reduction of the\neffects of noisy frames. Finally, we fuse the outputs of both streams as the\nembedding of a snippet, and apply competitive snippet-similarity aggregation to\nmeasure the similarity between two sequences. Our experiments show that the\nproposed method significantly outperforms current state-of-the-art approaches\non multiple datasets.\n", "versions": [{"version": "v1", "created": "Tue, 28 May 2019 14:47:33 GMT"}], "update_date": "2019-05-29", "authors_parsed": [["Chen", "Peixian", ""], ["Dai", "Pingyang", ""], ["Wu", "Qiong", ""], ["Huang", "Yuyu", ""]]}, {"id": "1905.11893", "submitter": "Marc Ru{\\ss}wurm", "authors": "Marc Ru{\\ss}wurm, Charlotte Pelletier, Maximilian Zollner, S\\'ebastien\n  Lef\\`evre, Marco K\\\"orner", "title": "BreizhCrops: A Time Series Dataset for Crop Type Mapping", "comments": "accepted to ISPRS Archives 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present Breizhcrops, a novel benchmark dataset for the supervised\nclassification of field crops from satellite time series. We aggregated label\ndata and Sentinel-2 top-of-atmosphere as well as bottom-of-atmosphere time\nseries in the region of Brittany (Breizh in local language), north-east France.\nWe compare seven recently proposed deep neural networks along with a Random\nForest baseline. The dataset, model (re-)implementations and pre-trained model\nweights are available at the associated GitHub repository\n(https://github.com/dl4sits/BreizhCrops) that has been designed with\napplicability for practitioners in mind. We plan to maintain the repository\nwith additional data and welcome contributions of novel methods to build a\nstate-of-the-art benchmark on methods for crop type mapping.\n", "versions": [{"version": "v1", "created": "Tue, 28 May 2019 15:40:18 GMT"}, {"version": "v2", "created": "Sun, 10 May 2020 19:01:27 GMT"}], "update_date": "2020-05-12", "authors_parsed": [["Ru\u00dfwurm", "Marc", ""], ["Pelletier", "Charlotte", ""], ["Zollner", "Maximilian", ""], ["Lef\u00e8vre", "S\u00e9bastien", ""], ["K\u00f6rner", "Marco", ""]]}, {"id": "1905.11903", "submitter": "Bor-Chun Chen", "authors": "Bor-Chun Chen, Zuxuan Wu, Larry S. Davis, Ser-Nam Lim", "title": "Efficient Object Embedding for Spliced Image Retrieval", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Detecting spliced images is one of the emerging challenges in computer\nvision. Unlike prior methods that focus on detecting low-level artifacts\ngenerated during the manipulation process, we use an image retrieval approach\nto tackle this problem. When given a spliced query image, our goal is to\nretrieve the original image from a database of authentic images. To achieve\nthis goal, we propose representing an image by its constituent objects based on\nthe intuition that the finest granularity of manipulations is oftentimes at the\nobject-level. We introduce a framework, object embeddings for spliced image\nretrieval (OE-SIR), that utilizes modern object detectors to localize object\nregions. Each region is then embedded and collectively used to represent the\nimage. Further, we propose a student-teacher training paradigm for learning\ndiscriminative embeddings within object regions to avoid expensive multiple\nforward passes. Detailed analysis of the efficacy of different feature\nembedding models is also provided in this study. Extensive experimental results\nshow that the OE-SIR achieves state-of-the-art performance in spliced image\nretrieval.\n", "versions": [{"version": "v1", "created": "Tue, 28 May 2019 16:02:51 GMT"}, {"version": "v2", "created": "Wed, 28 Apr 2021 03:23:16 GMT"}], "update_date": "2021-04-29", "authors_parsed": [["Chen", "Bor-Chun", ""], ["Wu", "Zuxuan", ""], ["Davis", "Larry S.", ""], ["Lim", "Ser-Nam", ""]]}, {"id": "1905.11922", "submitter": "Mohammad Samar Ansari", "authors": "Arpit Jadon, Mohd. Omama, Akshay Varshney, Mohammad Samar Ansari,\n  Rishabh Sharma", "title": "FireNet: A Specialized Lightweight Fire & Smoke Detection Model for\n  Real-Time IoT Applications", "comments": "To be submitted to a conference in the future", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fire disasters typically result in lot of loss to life and property. It is\ntherefore imperative that precise, fast, and possibly portable solutions to\ndetect fire be made readily available to the masses at reasonable prices. There\nhave been several research attempts to design effective and appropriately\npriced fire detection systems with varying degrees of success. However, most of\nthem demonstrate a trade-off between performance and model size (which decides\nthe model's ability to be installed on portable devices). The work presented in\nthis paper is an attempt to deal with both the performance and model size\nissues in one design. Toward that end, a `designed-from-scratch' neural\nnetwork, named FireNet, is proposed which is worthy on both the counts: (i) it\nhas better performance than existing counterparts, and (ii) it is lightweight\nenough to be deploy-able on embedded platforms like Raspberry Pi. Performance\nevaluations on a standard dataset, as well as our own newly introduced\ncustom-compiled fire dataset, are extremely encouraging.\n", "versions": [{"version": "v1", "created": "Tue, 28 May 2019 16:31:52 GMT"}, {"version": "v2", "created": "Wed, 4 Sep 2019 04:09:37 GMT"}], "update_date": "2019-09-05", "authors_parsed": [["Jadon", "Arpit", ""], ["Omama", "Mohd.", ""], ["Varshney", "Akshay", ""], ["Ansari", "Mohammad Samar", ""], ["Sharma", "Rishabh", ""]]}, {"id": "1905.11926", "submitter": "Chengxi Ye", "authors": "Chengxi Ye, Matthew Evanusa, Hua He, Anton Mitrokhin, Tom Goldstein,\n  James A. Yorke, Cornelia Ferm\\\"uller, Yiannis Aloimonos", "title": "Network Deconvolution", "comments": "ICLR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolution is a central operation in Convolutional Neural Networks (CNNs),\nwhich applies a kernel to overlapping regions shifted across the image.\nHowever, because of the strong correlations in real-world image data,\nconvolutional kernels are in effect re-learning redundant data. In this work,\nwe show that this redundancy has made neural network training challenging, and\npropose network deconvolution, a procedure which optimally removes pixel-wise\nand channel-wise correlations before the data is fed into each layer. Network\ndeconvolution can be efficiently calculated at a fraction of the computational\ncost of a convolution layer. We also show that the deconvolution filters in the\nfirst layer of the network resemble the center-surround structure found in\nbiological neurons in the visual regions of the brain. Filtering with such\nkernels results in a sparse representation, a desired property that has been\nmissing in the training of neural networks. Learning from the sparse\nrepresentation promotes faster convergence and superior results without the use\nof batch normalization. We apply our network deconvolution operation to 10\nmodern neural network models by replacing batch normalization within each.\nExtensive experiments show that the network deconvolution operation is able to\ndeliver performance improvement in all cases on the CIFAR-10, CIFAR-100, MNIST,\nFashion-MNIST, Cityscapes, and ImageNet datasets.\n", "versions": [{"version": "v1", "created": "Tue, 28 May 2019 16:38:34 GMT"}, {"version": "v2", "created": "Thu, 26 Sep 2019 17:44:36 GMT"}, {"version": "v3", "created": "Tue, 18 Feb 2020 19:24:00 GMT"}, {"version": "v4", "created": "Tue, 25 Feb 2020 20:48:22 GMT"}], "update_date": "2020-02-27", "authors_parsed": [["Ye", "Chengxi", ""], ["Evanusa", "Matthew", ""], ["He", "Hua", ""], ["Mitrokhin", "Anton", ""], ["Goldstein", "Tom", ""], ["Yorke", "James A.", ""], ["Ferm\u00fcller", "Cornelia", ""], ["Aloimonos", "Yiannis", ""]]}, {"id": "1905.11931", "submitter": "Zeya Wang", "authors": "Zeya Wang, Baoyu Jing, Yang Ni, Nanqing Dong, Pengtao Xie, Eric P.\n  Xing", "title": "Adversarial Domain Adaptation Being Aware of Class Relationships", "comments": null, "journal-ref": "24th European Conference on Artificial Intelligence (ECAI), 2020", "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adversarial training is a useful approach to promote the learning of\ntransferable representations across the source and target domains, which has\nbeen widely applied for domain adaptation (DA) tasks based on deep neural\nnetworks. Until very recently, existing adversarial domain adaptation (ADA)\nmethods ignore the useful information from the label space, which is an\nimportant factor accountable for the complicated data distributions associated\nwith different semantic classes. Especially, the inter-class semantic\nrelationships have been rarely considered and discussed in the current work of\ntransfer learning. In this paper, we propose a novel relationship-aware\nadversarial domain adaptation (RADA) algorithm, which first utilizes a single\nmulti-class domain discriminator to enforce the learning of inter-class\ndependency structure during domain-adversarial training and then aligns this\nstructure with the inter-class dependencies that are characterized from\ntraining the label predictor on source domain. Specifically, we impose a\nregularization term to penalize the structure discrepancy between the\ninter-class dependencies respectively estimated from domain discriminator and\nlabel predictor. Through this alignment, our proposed method makes the\nadversarial domain adaptation aware of the class relationships. Empirical\nstudies show that the incorporation of class relationships significantly\nimproves the performance on benchmark datasets.\n", "versions": [{"version": "v1", "created": "Tue, 28 May 2019 16:52:08 GMT"}, {"version": "v2", "created": "Mon, 30 Mar 2020 00:52:54 GMT"}], "update_date": "2020-03-31", "authors_parsed": [["Wang", "Zeya", ""], ["Jing", "Baoyu", ""], ["Ni", "Yang", ""], ["Dong", "Nanqing", ""], ["Xie", "Pengtao", ""], ["Xing", "Eric P.", ""]]}, {"id": "1905.11940", "submitter": "Boyang Deng", "authors": "Boyang Deng, Simon Kornblith, Geoffrey Hinton", "title": "Cerberus: A Multi-headed Derenderer", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  To generalize to novel visual scenes with new viewpoints and new object\nposes, a visual system needs representations of the shapes of the parts of an\nobject that are invariant to changes in viewpoint or pose. 3D graphics\nrepresentations disentangle visual factors such as viewpoints and lighting from\nobject structure in a natural way. It is possible to learn to invert the\nprocess that converts 3D graphics representations into 2D images, provided the\n3D graphics representations are available as labels. When only the unlabeled\nimages are available, however, learning to derender is much harder. We consider\na simple model which is just a set of free floating parts. Each part has its\nown relation to the camera and its own triangular mesh which can be deformed to\nmodel the shape of the part. At test time, a neural network looks at a single\nimage and extracts the shapes of the parts and their relations to the camera.\nEach part can be viewed as one head of a multi-headed derenderer. During\ntraining, the extracted parts are used as input to a differentiable 3D renderer\nand the reconstruction error is backpropagated to train the neural net. We make\nthe learning task easier by encouraging the deformations of the part meshes to\nbe invariant to changes in viewpoint and invariant to the changes in the\nrelative positions of the parts that occur when the pose of an articulated body\nchanges. Cerberus, our multi-headed derenderer, outperforms previous methods\nfor extracting 3D parts from single images without part annotations, and it\ndoes quite well at extracting natural parts of human figures.\n", "versions": [{"version": "v1", "created": "Tue, 28 May 2019 17:00:03 GMT"}], "update_date": "2019-05-29", "authors_parsed": [["Deng", "Boyang", ""], ["Kornblith", "Simon", ""], ["Hinton", "Geoffrey", ""]]}, {"id": "1905.11945", "submitter": "Alison Cheeseman", "authors": "Alison K. Cheeseman, Hamid Tizhoosh, Edward R. Vrscay", "title": "A Compact Representation of Histopathology Images using Digital Stain\n  Separation & Frequency-Based Encoded Local Projections", "comments": "Accepted for publication in the International Conference on Image\n  Analysis and Recognition (ICIAR 2019)", "journal-ref": null, "doi": "10.1007/978-3-030-27272-2_13", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, histopathology images have been increasingly used as a\ndiagnostic tool in the medical field. The process of accurately diagnosing a\nbiopsy sample requires significant expertise in the field, and as such can be\ntime-consuming and is prone to uncertainty and error. With the advent of\ndigital pathology, using image recognition systems to highlight problem areas\nor locate similar images can aid pathologists in making quick and accurate\ndiagnoses. In this paper, we specifically consider the encoded local\nprojections (ELP) algorithm, which has previously shown some success as a tool\nfor classification and recognition of histopathology images. We build on the\nsuccess of the ELP algorithm as a means for image classification and\nrecognition by proposing a modified algorithm which captures the local\nfrequency information of the image. The proposed algorithm estimates local\nfrequencies by quantifying the changes in multiple projections in local windows\nof greyscale images. By doing so we remove the need to store the full\nprojections, thus significantly reducing the histogram size, and decreasing\ncomputation time for image retrieval and classification tasks. Furthermore, we\ninvestigate the effectiveness of applying our method to histopathology images\nwhich have been digitally separated into their hematoxylin and eosin stain\ncomponents. The proposed algorithm is tested on the publicly available invasive\nductal carcinoma (IDC) data set. The histograms are used to train an SVM to\nclassify the data. The experiments showed that the proposed method outperforms\nthe original ELP algorithm in image retrieval tasks. On classification tasks,\nthe results are found to be comparable to state-of-the-art deep learning\nmethods and better than many handcrafted features from the literature.\n", "versions": [{"version": "v1", "created": "Tue, 28 May 2019 17:04:35 GMT"}], "update_date": "2019-09-24", "authors_parsed": [["Cheeseman", "Alison K.", ""], ["Tizhoosh", "Hamid", ""], ["Vrscay", "Edward R.", ""]]}, {"id": "1905.11946", "submitter": "Mingxing Tan", "authors": "Mingxing Tan and Quoc V. Le", "title": "EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks", "comments": "ICML 2019", "journal-ref": "International Conference on Machine Learning, 2019", "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional Neural Networks (ConvNets) are commonly developed at a fixed\nresource budget, and then scaled up for better accuracy if more resources are\navailable. In this paper, we systematically study model scaling and identify\nthat carefully balancing network depth, width, and resolution can lead to\nbetter performance. Based on this observation, we propose a new scaling method\nthat uniformly scales all dimensions of depth/width/resolution using a simple\nyet highly effective compound coefficient. We demonstrate the effectiveness of\nthis method on scaling up MobileNets and ResNet.\n  To go even further, we use neural architecture search to design a new\nbaseline network and scale it up to obtain a family of models, called\nEfficientNets, which achieve much better accuracy and efficiency than previous\nConvNets. In particular, our EfficientNet-B7 achieves state-of-the-art 84.3%\ntop-1 accuracy on ImageNet, while being 8.4x smaller and 6.1x faster on\ninference than the best existing ConvNet. Our EfficientNets also transfer well\nand achieve state-of-the-art accuracy on CIFAR-100 (91.7%), Flowers (98.8%),\nand 3 other transfer learning datasets, with an order of magnitude fewer\nparameters. Source code is at\nhttps://github.com/tensorflow/tpu/tree/master/models/official/efficientnet.\n", "versions": [{"version": "v1", "created": "Tue, 28 May 2019 17:05:32 GMT"}, {"version": "v2", "created": "Mon, 10 Jun 2019 04:48:48 GMT"}, {"version": "v3", "created": "Sat, 23 Nov 2019 03:13:25 GMT"}, {"version": "v4", "created": "Fri, 4 Sep 2020 05:43:46 GMT"}, {"version": "v5", "created": "Fri, 11 Sep 2020 05:08:01 GMT"}], "update_date": "2020-09-14", "authors_parsed": [["Tan", "Mingxing", ""], ["Le", "Quoc V.", ""]]}, {"id": "1905.11954", "submitter": "Chengxu Zhuang", "authors": "Chengxu Zhuang, Tianwei She, Alex Andonian, Max Sobol Mark, Daniel\n  Yamins", "title": "Unsupervised Learning from Video with Deep Neural Embeddings", "comments": "To appear in CVPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Because of the rich dynamical structure of videos and their ubiquity in\neveryday life, it is a natural idea that video data could serve as a powerful\nunsupervised learning signal for training visual representations in deep neural\nnetworks. However, instantiating this idea, especially at large scale, has\nremained a significant artificial intelligence challenge. Here we present the\nVideo Instance Embedding (VIE) framework, which extends powerful recent\nunsupervised loss functions for learning deep nonlinear embeddings to\nmulti-stream temporal processing architectures on large-scale video datasets.\nWe show that VIE-trained networks substantially advance the state of the art in\nunsupervised learning from video datastreams, both for action recognition in\nthe Kinetics dataset, and object recognition in the ImageNet dataset. We show\nthat a hybrid model with both static and dynamic processing pathways is optimal\nfor both transfer tasks, and provide analyses indicating how the pathways\ndiffer. Taken in context, our results suggest that deep neural embeddings are a\npromising approach to unsupervised visual learning across a wide variety of\ndomains.\n", "versions": [{"version": "v1", "created": "Tue, 28 May 2019 17:24:48 GMT"}, {"version": "v2", "created": "Tue, 10 Mar 2020 21:57:55 GMT"}], "update_date": "2020-03-12", "authors_parsed": [["Zhuang", "Chengxu", ""], ["She", "Tianwei", ""], ["Andonian", "Alex", ""], ["Mark", "Max Sobol", ""], ["Yamins", "Daniel", ""]]}, {"id": "1905.11971", "submitter": "Yuzhe Yang", "authors": "Yuzhe Yang, Guo Zhang, Dina Katabi, Zhi Xu", "title": "ME-Net: Towards Effective Adversarial Robustness with Matrix Estimation", "comments": "ICML 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks are vulnerable to adversarial attacks. The literature is\nrich with algorithms that can easily craft successful adversarial examples. In\ncontrast, the performance of defense techniques still lags behind. This paper\nproposes ME-Net, a defense method that leverages matrix estimation (ME). In\nME-Net, images are preprocessed using two steps: first pixels are randomly\ndropped from the image; then, the image is reconstructed using ME. We show that\nthis process destroys the adversarial structure of the noise, while\nre-enforcing the global structure in the original image. Since humans typically\nrely on such global structures in classifying images, the process makes the\nnetwork mode compatible with human perception. We conduct comprehensive\nexperiments on prevailing benchmarks such as MNIST, CIFAR-10, SVHN, and\nTiny-ImageNet. Comparing ME-Net with state-of-the-art defense mechanisms shows\nthat ME-Net consistently outperforms prior techniques, improving robustness\nagainst both black-box and white-box attacks.\n", "versions": [{"version": "v1", "created": "Tue, 28 May 2019 17:47:33 GMT"}], "update_date": "2019-05-29", "authors_parsed": [["Yang", "Yuzhe", ""], ["Zhang", "Guo", ""], ["Katabi", "Dina", ""], ["Xu", "Zhi", ""]]}, {"id": "1905.12003", "submitter": "Alessandro Lameiras Koerich", "authors": "Daniel Vriesman, Alessandro Zimmer, Alceu S. Britto Jr., Alessandro L.\n  Koerich", "title": "Texture CNN for Thermoelectric Metal Pipe Image Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, the concept of representation learning based on deep neural\nnetworks is applied as an alternative to the use of handcrafted features in a\nmethod for automatic visual inspection of corroded thermoelectric metallic\npipes. A texture convolutional neural network (TCNN) replaces handcrafted\nfeatures based on Local Phase Quantization (LPQ) and Haralick descriptors (HD)\nwith the advantage of learning an appropriate textural representation and the\ndecision boundaries into a single optimization process. Experimental results\nhave shown that it is possible to reach the accuracy of 99.20% in the task of\nidentifying different levels of corrosion in the internal surface of\nthermoelectric pipe walls, while using a compact network that requires much\nless effort in tuning parameters when compared to the handcrafted approach\nsince the TCNN architecture is compact regarding the number of layers and\nconnections. The observed results open up the possibility of using deep neural\nnetworks in real-time applications such as the automatic inspection of\nthermoelectric metal pipes.\n", "versions": [{"version": "v1", "created": "Tue, 28 May 2019 18:02:00 GMT"}], "update_date": "2019-05-30", "authors_parsed": [["Vriesman", "Daniel", ""], ["Zimmer", "Alessandro", ""], ["Britto", "Alceu S.", "Jr."], ["Koerich", "Alessandro L.", ""]]}, {"id": "1905.12005", "submitter": "Alessandro Lameiras Koerich", "authors": "Jonathan de Matos, Alceu de S. Britto Jr., Luiz E. S. de Oliveira,\n  Alessandro L. Koerich", "title": "Texture CNN for Histopathological Image Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Biopsies are the gold standard for breast cancer diagnosis. This task can be\nimproved by the use of Computer Aided Diagnosis (CAD) systems, reducing the\ntime of diagnosis and reducing the inter and intra-observer variability. The\nadvances in computing have brought this type of system closer to reality.\nHowever, datasets of Histopathological Images (HI) from biopsies are quite\nsmall and unbalanced what makes difficult to use modern machine learning\ntechniques such as deep learning. In this paper we propose a compact\narchitecture based on texture filters that has fewer parameters than\ntraditional deep models but is able to capture the difference between malignant\nand benign tissues with relative accuracy. The experimental results on the\nBreakHis dataset have show that the proposed texture CNN achieves almost 90% of\naccuracy for classifying benign and malignant tissues.\n", "versions": [{"version": "v1", "created": "Tue, 28 May 2019 18:04:19 GMT"}], "update_date": "2019-05-30", "authors_parsed": [["de Matos", "Jonathan", ""], ["Britto", "Alceu de S.", "Jr."], ["de Oliveira", "Luiz E. S.", ""], ["Koerich", "Alessandro L.", ""]]}, {"id": "1905.12008", "submitter": "Tomasz Kornuta", "authors": "Tomasz Kornuta and Deepta Rajan and Chaitanya Shivade and Alexis\n  Asseman and Ahmet S. Ozcan", "title": "Leveraging Medical Visual Question Answering with Supporting Facts", "comments": "Working notes from the ImageCLEF 2019 VQA-Med competition", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this working notes paper, we describe IBM Research AI (Almaden) team's\nparticipation in the ImageCLEF 2019 VQA-Med competition. The challenge consists\nof four question-answering tasks based on radiology images. The diversity of\nimaging modalities, organs and disease types combined with a small imbalanced\ntraining set made this a highly complex problem. To overcome these\ndifficulties, we implemented a modular pipeline architecture that utilized\ntransfer learning and multi-task learning. Our findings led to the development\nof a novel model called Supporting Facts Network (SFN). The main idea behind\nSFN is to cross-utilize information from upstream tasks to improve the accuracy\non harder downstream ones. This approach significantly improved the scores\nachieved in the validation set (18 point improvement in F-1 score). Finally, we\nsubmitted four runs to the competition and were ranked seventh.\n", "versions": [{"version": "v1", "created": "Tue, 28 May 2019 18:15:52 GMT"}], "update_date": "2019-05-30", "authors_parsed": [["Kornuta", "Tomasz", ""], ["Rajan", "Deepta", ""], ["Shivade", "Chaitanya", ""], ["Asseman", "Alexis", ""], ["Ozcan", "Ahmet S.", ""]]}, {"id": "1905.12019", "submitter": "Martin Mundt", "authors": "Martin Mundt, Sagnik Majumder, Iuliia Pliushch, Yong Won Hong,\n  Visvanathan Ramesh", "title": "Unified Probabilistic Deep Continual Learning through Generative Replay\n  and Open Set Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a probabilistic approach to unify open set recognition with the\nprevention of catastrophic forgetting in deep continual learning, based on\nvariational Bayesian inference. Our single model combines a joint probabilistic\nencoder with a generative model and a linear classifier that get shared across\nsequentially arriving tasks. In order to successfully distinguish unseen\nunknown data from trained known tasks, we propose to bound the class specific\napproximate posterior by fitting regions of high density on the basis of\ncorrectly classified data points. These bounds are further used to\nsignificantly alleviate catastrophic forgetting by avoiding samples from low\ndensity areas in generative replay. Our approach requires neither storing of\nold, nor upfront knowledge of future data, and is empirically validated on\nvisual and audio tasks in class incremental, as well as cross-dataset scenarios\nacross modalities.\n", "versions": [{"version": "v1", "created": "Tue, 28 May 2019 18:26:04 GMT"}, {"version": "v2", "created": "Thu, 26 Sep 2019 15:24:33 GMT"}, {"version": "v3", "created": "Tue, 18 Feb 2020 15:14:34 GMT"}, {"version": "v4", "created": "Fri, 4 Sep 2020 10:54:47 GMT"}], "update_date": "2020-09-07", "authors_parsed": [["Mundt", "Martin", ""], ["Majumder", "Sagnik", ""], ["Pliushch", "Iuliia", ""], ["Hong", "Yong Won", ""], ["Ramesh", "Visvanathan", ""]]}, {"id": "1905.12028", "submitter": "Thanh-Dat Truong", "authors": "Thanh-Dat Truong, Khoa Luu, Chi Nhan Duong, Ngan Le and Minh-Triet\n  Tran", "title": "Image Alignment in Unseen Domains via Domain Deep Generalization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image alignment across domains has recently become one of the realistic and\npopular topics in the research community. In this problem, a deep\nlearning-based image alignment method is usually trained on an available\nlargescale database. During the testing steps, this trained model is deployed\non unseen images collected under different camera conditions and modalities.\nThe delivered deep network models are unable to be updated, adapted or\nfine-tuned in these scenarios. Thus, recent deep learning techniques, e.g.\ndomain adaptation, feature transferring, and fine-tuning, are unable to be\ndeployed. This paper presents a novel deep learning based approach to tackle\nthe problem of across unseen modalities. The proposed network is then applied\nto image alignment as an illustration. The proposed approach is designed as an\nend-to-end deep convolutional neural network to optimize the deep models to\nimprove the performance. The proposed network has been evaluated in digit\nrecognition when the model is trained on MNIST and then tested on unseen domain\nMNIST-M. Finally, the proposed method is benchmarked in image alignment problem\nwhen training on RGB images and testing on Depth and X-Ray images.\n", "versions": [{"version": "v1", "created": "Tue, 28 May 2019 18:47:21 GMT"}, {"version": "v2", "created": "Fri, 31 May 2019 16:11:00 GMT"}], "update_date": "2019-06-03", "authors_parsed": [["Truong", "Thanh-Dat", ""], ["Luu", "Khoa", ""], ["Duong", "Chi Nhan", ""], ["Le", "Ngan", ""], ["Tran", "Minh-Triet", ""]]}, {"id": "1905.12032", "submitter": "Pu Zhao", "authors": "Pu Zhao, Siyue Wang, Cheng Gongye, Yanzhi Wang, Yunsi Fei, Xue Lin", "title": "Fault Sneaking Attack: a Stealthy Framework for Misleading Deep Neural\n  Networks", "comments": "Accepted by the 56th Design Automation Conference (DAC 2019)", "journal-ref": null, "doi": "10.1145/3316781.3317825", "report-no": null, "categories": "cs.LG cs.CR cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the great achievements of deep neural networks (DNNs), the\nvulnerability of state-of-the-art DNNs raises security concerns of DNNs in many\napplication domains requiring high reliability.We propose the fault sneaking\nattack on DNNs, where the adversary aims to misclassify certain input images\ninto any target labels by modifying the DNN parameters. We apply ADMM\n(alternating direction method of multipliers) for solving the optimization\nproblem of the fault sneaking attack with two constraints: 1) the\nclassification of the other images should be unchanged and 2) the parameter\nmodifications should be minimized. Specifically, the first constraint requires\nus not only to inject designated faults (misclassifications), but also to hide\nthe faults for stealthy or sneaking considerations by maintaining model\naccuracy. The second constraint requires us to minimize the parameter\nmodifications (using L0 norm to measure the number of modifications and L2 norm\nto measure the magnitude of modifications). Comprehensive experimental\nevaluation demonstrates that the proposed framework can inject multiple\nsneaking faults without losing the overall test accuracy performance.\n", "versions": [{"version": "v1", "created": "Tue, 28 May 2019 18:56:44 GMT"}], "update_date": "2019-05-30", "authors_parsed": [["Zhao", "Pu", ""], ["Wang", "Siyue", ""], ["Gongye", "Cheng", ""], ["Wang", "Yanzhi", ""], ["Fei", "Yunsi", ""], ["Lin", "Xue", ""]]}, {"id": "1905.12042", "submitter": "Tejas Gokhale", "authors": "Tejas Gokhale, Shailaja Sampat, Zhiyuan Fang, Yezhou Yang, Chitta\n  Baral", "title": "Blocksworld Revisited: Learning and Reasoning to Generate\n  Event-Sequences from Image Pairs", "comments": "10 pages, 5 figures, for associated dataset, see\n  https://asu-active-perception-group.github.io/bird_dataset_web/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The process of identifying changes or transformations in a scene along with\nthe ability of reasoning about their causes and effects, is a key aspect of\nintelligence. In this work we go beyond recent advances in computational\nperception, and introduce a more challenging task, Image-based Event-Sequencing\n(IES). In IES, the task is to predict a sequence of actions required to\nrearrange objects from the configuration in an input source image to the one in\nthe target image. IES also requires systems to possess inductive\ngeneralizability. Motivated from evidence in cognitive development, we compile\nthe first IES dataset, the Blocksworld Image Reasoning Dataset (BIRD) which\ncontains images of wooden blocks in different configurations, and the sequence\nof moves to rearrange one configuration to the other. We first explore the use\nof existing deep learning architectures and show that these end-to-end methods\nunder-perform in inferring temporal event-sequences and fail at inductive\ngeneralization. We then propose a modular two-step approach: Visual Perception\nfollowed by Event-Sequencing, and demonstrate improved performance by combining\nlearning and reasoning. Finally, by showing an extension of our approach on\nnatural images, we seek to pave the way for future research on event sequencing\nfor real world scenes.\n", "versions": [{"version": "v1", "created": "Tue, 28 May 2019 19:26:19 GMT"}], "update_date": "2019-05-30", "authors_parsed": [["Gokhale", "Tejas", ""], ["Sampat", "Shailaja", ""], ["Fang", "Zhiyuan", ""], ["Yang", "Yezhou", ""], ["Baral", "Chitta", ""]]}, {"id": "1905.12043", "submitter": "Michail C. Doukas", "authors": "Michail C. Doukas, Viktoriia Sharmanska, Stefanos Zafeiriou", "title": "Video-to-Video Translation for Visual Speech Synthesis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite remarkable success in image-to-image translation that celebrates the\nadvancements of generative adversarial networks (GANs), very limited attempts\nare known for video domain translation. We study the task of video-to-video\ntranslation in the context of visual speech generation, where the goal is to\ntransform an input video of any spoken word to an output video of a different\nword. This is a multi-domain translation, where each word forms a domain of\nvideos uttering this word. Adaptation of the state-of-the-art image-to-image\ntranslation model (StarGAN) to this setting falls short with a large vocabulary\nsize. Instead we propose to use character encodings of the words and design a\nnovel character-based GANs architecture for video-to-video translation called\nVisual Speech GAN (ViSpGAN). We are the first to demonstrate video-to-video\ntranslation with a vocabulary of 500 words.\n", "versions": [{"version": "v1", "created": "Tue, 28 May 2019 19:27:39 GMT"}], "update_date": "2019-05-30", "authors_parsed": [["Doukas", "Michail C.", ""], ["Sharmanska", "Viktoriia", ""], ["Zafeiriou", "Stefanos", ""]]}, {"id": "1905.12056", "submitter": "Fran\\c{c}ois Lauze", "authors": "Henrik Gr{\\o}nholt Jensen, Fran\\c{c}ois Lauze, Sune Darkner", "title": "Information-Theoretic Registration with Explicit Reorientation of\n  Diffusion-Weighted Images", "comments": "16 pages, 19 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an information-theoretic approach to the registration of images\nwith directional information, and especially for diffusion-Weighted Images\n(DWI), with explicit optimization over the directional scale. We call it\nLocally Orderless Registration with Directions (LORD). We focus on normalized\nmutual information as a robust information-theoretic similarity measure for\nDWI. The framework is an extension of the LOR-DWI density-based hierarchical\nscale-space model that varies and optimizes the integration, spatial,\ndirectional, and intensity scales. As affine transformations are insufficient\nfor inter-subject registration, we extend the model to non-rigid deformations.\nWe illustrate that the proposed model deforms orientation distribution\nfunctions (ODFs) correctly and is capable of handling the classic complex\nchallenges in DWI-registrations, such as the registration of fiber-crossings\nalong with kissing, fanning, and interleaving fibers. Our experimental results\nclearly illustrate a novel promising regularizing effect, that comes from the\nnonlinear orientation-based cost function. We show the properties of the\ndifferent image scales and, we show that including orientational information in\nour model makes the model better at retrieving deformations in contrast to\nstandard scalar-based registration.\n", "versions": [{"version": "v1", "created": "Tue, 28 May 2019 19:59:30 GMT"}, {"version": "v2", "created": "Wed, 19 Jun 2019 11:17:54 GMT"}, {"version": "v3", "created": "Thu, 21 Jan 2021 23:12:22 GMT"}, {"version": "v4", "created": "Fri, 9 Jul 2021 13:58:21 GMT"}], "update_date": "2021-07-12", "authors_parsed": [["Jensen", "Henrik Gr\u00f8nholt", ""], ["Lauze", "Fran\u00e7ois", ""], ["Darkner", "Sune", ""]]}, {"id": "1905.12079", "submitter": "Benjamin Burchfiel", "authors": "Benjamin Burchfiel and George Konidaris", "title": "Probabilistic Category-Level Pose Estimation via Segmentation and\n  Predicted-Shape Priors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new method for category-level pose estimation which produces a\ndistribution over predicted poses by integrating 3D shape estimates from a\ngenerative object model with segmentation information. Given an input\ndepth-image of an object, our variable-time method uses a mixture density\nnetwork architecture to produce a multi-modal distribution over 3DOF poses;\nthis distribution is then combined with a prior probability encouraging\nsilhouette agreement between the observed input and predicted object pose. Our\napproach significantly outperforms the current state-of-the-art in\ncategory-level 3DOF pose estimation---which outputs a point estimate and does\nnot explicitly incorporate shape and segmentation information---as measured on\nthe Pix3D and ShapeNet datasets.\n", "versions": [{"version": "v1", "created": "Tue, 28 May 2019 20:37:00 GMT"}], "update_date": "2019-05-30", "authors_parsed": [["Burchfiel", "Benjamin", ""], ["Konidaris", "George", ""]]}, {"id": "1905.12082", "submitter": "Alessandro Lameiras Koerich", "authors": "Dylan C. Tannugi, Alceu S. Britto Jr., Alessandro L. Koerich", "title": "Memory Integrity of CNNs for Cross-Dataset Facial Expression Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Facial expression recognition is a major problem in the domain of artificial\nintelligence. One of the best ways to solve this problem is the use of\nconvolutional neural networks (CNNs). However, a large amount of data is\nrequired to train properly these networks but most of the datasets available\nfor facial expression recognition are relatively small. A common way to\ncircumvent the lack of data is to use CNNs trained on large datasets of\ndifferent domains and fine-tuning the layers of such networks to the target\ndomain. However, the fine-tuning process does not preserve the memory integrity\nas CNNs have the tendency to forget patterns they have learned. In this paper,\nwe evaluate different strategies of fine-tuning a CNN with the aim of assessing\nthe memory integrity of such strategies in a cross-dataset scenario. A CNN\npre-trained on a source dataset is used as the baseline and four adaptation\nstrategies have been evaluated: fine-tuning its fully connected layers;\nfine-tuning its last convolutional layer and its fully connected layers;\nretraining the CNN on a target dataset; and the fusion of the source and target\ndatasets and retraining the CNN. Experimental results on four datasets have\nshown that the fusion of the source and the target datasets provides the best\ntrade-off between accuracy and memory integrity.\n", "versions": [{"version": "v1", "created": "Tue, 28 May 2019 20:55:57 GMT"}], "update_date": "2019-05-30", "authors_parsed": [["Tannugi", "Dylan C.", ""], ["Britto", "Alceu S.", "Jr."], ["Koerich", "Alessandro L.", ""]]}, {"id": "1905.12107", "submitter": "Igor Fedorov", "authors": "Igor Fedorov, Ryan P. Adams, Matthew Mattina, Paul N. Whatmough", "title": "SpArSe: Sparse Architecture Search for CNNs on Resource-Constrained\n  Microcontrollers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The vast majority of processors in the world are actually microcontroller\nunits (MCUs), which find widespread use performing simple control tasks in\napplications ranging from automobiles to medical devices and office equipment.\nThe Internet of Things (IoT) promises to inject machine learning into many of\nthese every-day objects via tiny, cheap MCUs. However, these\nresource-impoverished hardware platforms severely limit the complexity of\nmachine learning models that can be deployed. For example, although\nconvolutional neural networks (CNNs) achieve state-of-the-art results on many\nvisual recognition tasks, CNN inference on MCUs is challenging due to severe\nfinite memory limitations. To circumvent the memory challenge associated with\nCNNs, various alternatives have been proposed that do fit within the memory\nbudget of an MCU, albeit at the cost of prediction accuracy. This paper\nchallenges the idea that CNNs are not suitable for deployment on MCUs. We\ndemonstrate that it is possible to automatically design CNNs which generalize\nwell, while also being small enough to fit onto memory-limited MCUs. Our Sparse\nArchitecture Search method combines neural architecture search with pruning in\na single, unified approach, which learns superior models on four popular IoT\ndatasets. The CNNs we find are more accurate and up to $4.35\\times$ smaller\nthan previous approaches, while meeting the strict MCU working memory\nconstraint.\n", "versions": [{"version": "v1", "created": "Tue, 28 May 2019 21:52:08 GMT"}], "update_date": "2019-05-30", "authors_parsed": [["Fedorov", "Igor", ""], ["Adams", "Ryan P.", ""], ["Mattina", "Matthew", ""], ["Whatmough", "Paul N.", ""]]}, {"id": "1905.12120", "submitter": "Ali Hatamizadeh", "authors": "Ali Hatamizadeh, Hamid Hosseini, Zhengyuan Liu, Steven D.Schwartz and\n  Demetri Terzopoulos", "title": "Deep Dilated Convolutional Nets for the Automatic Segmentation of\n  Retinal Vessels", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The reliable segmentation of retinal vasculature can provide the means to\ndiagnose and monitor the progression of a variety of diseases affecting the\nblood vessel network, including diabetes and hypertension. We leverage the\npower of convolutional neural networks to devise a reliable and fully automated\nmethod that can accurately detect, segment, and analyze retinal vessels. In\nparticular, we propose a novel, fully convolutional deep neural network with an\nencoder-decoder architecture that employs dilated spatial pyramid pooling with\nmultiple dilation rates to recover the lost content in the encoder and add\nmultiscale contextual information to the decoder. We also propose a simple yet\neffective way of quantifying and tracking the widths of retinal vessels through\ndirect use of the segmentation predictions. Unlike previous deep-learning-based\napproaches to retinal vessel segmentation that mainly rely on patch-wise\nanalysis, our proposed method leverages a whole-image approach during training\nand inference, resulting in more efficient training and faster inference\nthrough the access of global content in the image. We have tested our method on\ntwo publicly available datasets, and our state-of-the-art results on both the\nDRIVE and CHASE-DB1 datasets attest to the effectiveness of our approach.\n", "versions": [{"version": "v1", "created": "Tue, 28 May 2019 22:37:00 GMT"}, {"version": "v2", "created": "Sun, 21 Jul 2019 00:41:47 GMT"}], "update_date": "2019-07-23", "authors_parsed": [["Hatamizadeh", "Ali", ""], ["Hosseini", "Hamid", ""], ["Liu", "Zhengyuan", ""], ["Schwartz", "Steven D.", ""], ["Terzopoulos", "Demetri", ""]]}, {"id": "1905.12156", "submitter": "Xiangyu Xu", "authors": "Xiangyu Xu, Yongrui Ma, Wenxiu Sun", "title": "Towards Real Scene Super-Resolution with Raw Images", "comments": "Accepted in CVPR 2019, project page:\n  https://sites.google.com/view/xiangyuxu/rawsr_cvpr19", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most existing super-resolution methods do not perform well in real scenarios\ndue to lack of realistic training data and information loss of the model input.\nTo solve the first problem, we propose a new pipeline to generate realistic\ntraining data by simulating the imaging process of digital cameras. And to\nremedy the information loss of the input, we develop a dual convolutional\nneural network to exploit the originally captured radiance information in raw\nimages. In addition, we propose to learn a spatially-variant color\ntransformation which helps more effective color corrections. Extensive\nexperiments demonstrate that super-resolution with raw data helps recover fine\ndetails and clear structures, and more importantly, the proposed network and\ndata generation pipeline achieve superior results for single image\nsuper-resolution in real scenarios.\n", "versions": [{"version": "v1", "created": "Wed, 29 May 2019 01:18:44 GMT"}], "update_date": "2019-05-30", "authors_parsed": [["Xu", "Xiangyu", ""], ["Ma", "Yongrui", ""], ["Sun", "Wenxiu", ""]]}, {"id": "1905.12162", "submitter": "Rohit Pandey", "authors": "Rohit Pandey, Anastasia Tkach, Shuoran Yang, Pavel Pidlypenskyi,\n  Jonathan Taylor, Ricardo Martin-Brualla, Andrea Tagliasacchi, George\n  Papandreou, Philip Davidson, Cem Keskin, Shahram Izadi, Sean Fanello", "title": "Volumetric Capture of Humans with a Single RGBD Camera via\n  Semi-Parametric Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Volumetric (4D) performance capture is fundamental for AR/VR content\ngeneration. Whereas previous work in 4D performance capture has shown\nimpressive results in studio settings, the technology is still far from being\naccessible to a typical consumer who, at best, might own a single RGBD sensor.\nThus, in this work, we propose a method to synthesize free viewpoint renderings\nusing a single RGBD camera. The key insight is to leverage previously seen\n\"calibration\" images of a given user to extrapolate what should be rendered in\na novel viewpoint from the data available in the sensor. Given these past\nobservations from multiple viewpoints, and the current RGBD image from a fixed\nview, we propose an end-to-end framework that fuses both these data sources to\ngenerate novel renderings of the performer. We demonstrate that the method can\nproduce high fidelity images, and handle extreme changes in subject pose and\ncamera viewpoints. We also show that the system generalizes to performers not\nseen in the training data. We run exhaustive experiments demonstrating the\neffectiveness of the proposed semi-parametric model (i.e. calibration images\navailable to the neural network) compared to other state of the art machine\nlearned solutions. Further, we compare the method with more traditional\npipelines that employ multi-view capture. We show that our framework is able to\nachieve compelling results, with substantially less infrastructure than\npreviously required.\n", "versions": [{"version": "v1", "created": "Wed, 29 May 2019 01:29:51 GMT"}], "update_date": "2019-05-30", "authors_parsed": [["Pandey", "Rohit", ""], ["Tkach", "Anastasia", ""], ["Yang", "Shuoran", ""], ["Pidlypenskyi", "Pavel", ""], ["Taylor", "Jonathan", ""], ["Martin-Brualla", "Ricardo", ""], ["Tagliasacchi", "Andrea", ""], ["Papandreou", "George", ""], ["Davidson", "Philip", ""], ["Keskin", "Cem", ""], ["Izadi", "Shahram", ""], ["Fanello", "Sean", ""]]}, {"id": "1905.12190", "submitter": "Zhengqiang Zhang", "authors": "Zhengqiang Zhang, Shujian Yu, Shi Yin, Qinmu Peng, Xinge You", "title": "Closed-Loop Adaptation for Weakly-Supervised Semantic Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Weakly-supervised semantic segmentation aims to assign each pixel a semantic\ncategory under weak supervisions, such as image-level tags. Most of existing\nweakly-supervised semantic segmentation methods do not use any feedback from\nsegmentation output and can be considered as open-loop systems. They are prone\nto accumulated errors because of the static seeds and the sensitive structure\ninformation. In this paper, we propose a generic self-adaptation mechanism for\nexisting weakly-supervised semantic segmentation methods by introducing two\nfeedback chains, thus constituting a closed-loop system. Specifically, the\nfirst chain iteratively produces dynamic seeds by incorporating cross-image\nstructure information, whereas the second chain further expands seed regions by\na customized random walk process to reconcile inner-image structure information\ncharacterized by superpixels. Experiments on PASCAL VOC 2012 suggest that our\nnetwork outperforms state-of-the-art methods with significantly less\ncomputational and memory burden.\n", "versions": [{"version": "v1", "created": "Wed, 29 May 2019 02:59:02 GMT"}], "update_date": "2019-05-30", "authors_parsed": [["Zhang", "Zhengqiang", ""], ["Yu", "Shujian", ""], ["Yin", "Shi", ""], ["Peng", "Qinmu", ""], ["You", "Xinge", ""]]}, {"id": "1905.12218", "submitter": "Pengfei Jin", "authors": "Pengfei Jin, Tianhao Lai, Rongjie Lai, Bin Dong", "title": "NPTC-net: Narrow-Band Parallel Transport Convolutional Neural Network on\n  Point Clouds", "comments": "18 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolution plays a crucial role in various applications in signal and image\nprocessing, analysis, and recognition. It is also the main building block of\nconvolution neural networks (CNNs). Designing appropriate convolution neural\nnetworks on manifold-structured point clouds can inherit and empower recent\nadvances of CNNs to analyzing and processing point cloud data. However, one of\nthe major challenges is to define a proper way to \"sweep\" filters through the\npoint cloud as a natural generalization of the planar convolution and to\nreflect the point cloud's geometry at the same time. In this paper, we consider\ngeneralizing convolution by adapting parallel transport on the point cloud.\nInspired by a triangulated surface-based method [Stefan C. Schonsheck, Bin\nDong, and Rongjie Lai, arXiv:1805.07857.], we propose the Narrow-Band Parallel\nTransport Convolution (NPTC) using a specifically defined connection on a\nvoxel-based narrow-band approximation of point cloud data. With that, we\nfurther propose a deep convolutional neural network based on NPTC (called\nNPTC-net) for point cloud classification and segmentation. Comprehensive\nexperiments show that the proposed NPTC-net achieves similar or better results\nthan current state-of-the-art methods on point cloud classification and\nsegmentation.\n", "versions": [{"version": "v1", "created": "Wed, 29 May 2019 05:07:26 GMT"}, {"version": "v2", "created": "Wed, 25 Sep 2019 14:30:41 GMT"}, {"version": "v3", "created": "Sun, 21 Feb 2021 13:18:10 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["Jin", "Pengfei", ""], ["Lai", "Tianhao", ""], ["Lai", "Rongjie", ""], ["Dong", "Bin", ""]]}, {"id": "1905.12236", "submitter": "Zhao Zhang", "authors": "Zhao Zhang, Lei Jia, Mingbo Zhao, Guangcan Liu, Meng Wang, Shuicheng\n  Yan", "title": "Kernel-Induced Label Propagation by Mapping for Semi-Supervised\n  Classification", "comments": "Accepted by IEEE TBD", "journal-ref": null, "doi": "10.1109/TBDATA.2018.2797977", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Kernel methods have been successfully applied to the areas of pattern\nrecognition and data mining. In this paper, we mainly discuss the issue of\npropagating labels in kernel space. A Kernel-Induced Label Propagation\n(Kernel-LP) framework by mapping is proposed for high-dimensional data\nclassification using the most informative patterns of data in kernel space. The\nessence of Kernel-LP is to perform joint label propagation and adaptive weight\nlearning in a transformed kernel space. That is, our Kernel-LP changes the task\nof label propagation from the commonly-used Euclidean space in most existing\nwork to kernel space. The motivation of our Kernel-LP to propagate labels and\nlearn the adaptive weights jointly by the assumption of an inner product space\nof inputs, i.e., the original linearly inseparable inputs may be mapped to be\nseparable in kernel space. Kernel-LP is based on existing positive and negative\nLP model, i.e., the effects of negative label information are integrated to\nimprove the label prediction power. Also, Kernel-LP performs adaptive weight\nconstruction over the same kernel space, so it can avoid the tricky process of\nchoosing the optimal neighborhood size suffered in traditional criteria. Two\nnovel and efficient out-of-sample approaches for our Kernel-LP to involve new\ntest data are also presented, i.e., (1) direct kernel mapping and (2) kernel\nmapping-induced label reconstruction, both of which purely depend on the kernel\nmatrix between training set and testing set. Owing to the kernel trick, our\nalgorithms will be applicable to handle the high-dimensional real data.\nExtensive results on real datasets demonstrate the effectiveness of our\napproach.\n", "versions": [{"version": "v1", "created": "Wed, 29 May 2019 06:23:49 GMT"}, {"version": "v2", "created": "Fri, 31 May 2019 01:53:35 GMT"}], "update_date": "2019-06-03", "authors_parsed": [["Zhang", "Zhao", ""], ["Jia", "Lei", ""], ["Zhao", "Mingbo", ""], ["Liu", "Guangcan", ""], ["Wang", "Meng", ""], ["Yan", "Shuicheng", ""]]}, {"id": "1905.12243", "submitter": "Aihong Yuan", "authors": "Xuelong Li, Aihong Yuan and Xiaoqiang Lu", "title": "Vision-to-Language Tasks Based on Attributes and Attention Mechanism", "comments": "15 pages, 6 figures, 50 references", "journal-ref": null, "doi": "10.1109/TCYB.2019.2914351", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Vision-to-language tasks aim to integrate computer vision and natural\nlanguage processing together, which has attracted the attention of many\nresearchers. For typical approaches, they encode image into feature\nrepresentations and decode it into natural language sentences. While they\nneglect high-level semantic concepts and subtle relationships between image\nregions and natural language elements. To make full use of these information,\nthis paper attempt to exploit the text guided attention and semantic-guided\nattention (SA) to find the more correlated spatial information and reduce the\nsemantic gap between vision and language. Our method includes two level\nattention networks. One is the text-guided attention network which is used to\nselect the text-related regions. The other is SA network which is used to\nhighlight the concept-related regions and the region-related concepts. At last,\nall these information are incorporated to generate captions or answers.\nPractically, image captioning and visual question answering experiments have\nbeen carried out, and the experimental results have shown the excellent\nperformance of the proposed approach.\n", "versions": [{"version": "v1", "created": "Wed, 29 May 2019 06:55:23 GMT"}], "update_date": "2019-05-30", "authors_parsed": [["Li", "Xuelong", ""], ["Yuan", "Aihong", ""], ["Lu", "Xiaoqiang", ""]]}, {"id": "1905.12260", "submitter": "Karan Singhal", "authors": "Karan Singhal, Karthik Raman, Balder ten Cate", "title": "Learning Multilingual Word Embeddings Using Image-Text Data", "comments": null, "journal-ref": null, "doi": "10.18653/v1/W19-1807", "report-no": "W19-1807", "categories": "cs.CL cs.AI cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There has been significant interest recently in learning multilingual word\nembeddings -- in which semantically similar words across languages have similar\nembeddings. State-of-the-art approaches have relied on expensive labeled data,\nwhich is unavailable for low-resource languages, or have involved post-hoc\nunification of monolingual embeddings. In the present paper, we investigate the\nefficacy of multilingual embeddings learned from weakly-supervised image-text\ndata. In particular, we propose methods for learning multilingual embeddings\nusing image-text data, by enforcing similarity between the representations of\nthe image and that of the text. Our experiments reveal that even without using\nany expensive labeled data, a bag-of-words-based embedding model trained on\nimage-text data achieves performance comparable to the state-of-the-art on\ncrosslingual semantic similarity tasks.\n", "versions": [{"version": "v1", "created": "Wed, 29 May 2019 07:55:17 GMT"}], "update_date": "2020-07-02", "authors_parsed": [["Singhal", "Karan", ""], ["Raman", "Karthik", ""], ["Cate", "Balder ten", ""]]}, {"id": "1905.12261", "submitter": "Che-Han Chang", "authors": "Che-Han Chang, Chun-Hsien Yu, Szu-Ying Chen, Edward Y. Chang", "title": "KG-GAN: Knowledge-Guided Generative Adversarial Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Can generative adversarial networks (GANs) generate roses of various colors\ngiven only roses of red petals as input? The answer is negative, since GANs'\ndiscriminator would reject all roses of unseen petal colors. In this study, we\npropose knowledge-guided GAN (KG-GAN) to fuse domain knowledge with the GAN\nframework. KG-GAN trains two generators; one learns from data whereas the other\nlearns from knowledge with a constraint function. Experimental results\ndemonstrate the effectiveness of KG-GAN in generating unseen flower categories\nfrom seen categories given textual descriptions of the unseen ones.\n", "versions": [{"version": "v1", "created": "Wed, 29 May 2019 07:55:46 GMT"}, {"version": "v2", "created": "Mon, 23 Sep 2019 09:48:33 GMT"}], "update_date": "2019-09-24", "authors_parsed": [["Chang", "Che-Han", ""], ["Yu", "Chun-Hsien", ""], ["Chen", "Szu-Ying", ""], ["Chang", "Edward Y.", ""]]}, {"id": "1905.12281", "submitter": "Diego Valsesia", "authors": "Diego Valsesia, Giulia Fracastoro, Enrico Magli", "title": "Image Denoising with Graph-Convolutional Neural Networks", "comments": "IEEE International Conference on Image Processing (ICIP) 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recovering an image from a noisy observation is a key problem in signal\nprocessing. Recently, it has been shown that data-driven approaches employing\nconvolutional neural networks can outperform classical model-based techniques,\nbecause they can capture more powerful and discriminative features. However,\nsince these methods are based on convolutional operations, they are only\ncapable of exploiting local similarities without taking into account non-local\nself-similarities. In this paper we propose a convolutional neural network that\nemploys graph-convolutional layers in order to exploit both local and non-local\nsimilarities. The graph-convolutional layers dynamically construct\nneighborhoods in the feature space to detect latent correlations in the feature\nmaps produced by the hidden layers. The experimental results show that the\nproposed architecture outperforms classical convolutional neural networks for\nthe denoising task.\n", "versions": [{"version": "v1", "created": "Wed, 29 May 2019 09:17:21 GMT"}], "update_date": "2019-05-30", "authors_parsed": [["Valsesia", "Diego", ""], ["Fracastoro", "Giulia", ""], ["Magli", "Enrico", ""]]}, {"id": "1905.12321", "submitter": "Jesper S\\\"oren Dramsch", "authors": "Jesper S\\\"oren Dramsch, Mikael L\\\"uthje, Anders Nymark Christensen", "title": "Complex-valued neural networks for machine learning on non-stationary\n  physical data", "comments": "17 pages total, 15 pages, 2 pages references, paper, 11 figures, 28\n  networks", "journal-ref": null, "doi": "10.1016/j.cageo.2020.104643", "report-no": null, "categories": "cs.LG cs.CV physics.comp-ph physics.geo-ph stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning has become an area of interest in most scientific areas,\nincluding physical sciences. Modern networks apply real-valued transformations\non the data. Particularly, convolutions in convolutional neural networks\ndiscard phase information entirely. Many deterministic signals, such as seismic\ndata or electrical signals, contain significant information in the phase of the\nsignal. We explore complex-valued deep convolutional networks to leverage\nnon-linear feature maps. Seismic data commonly has a lowcut filter applied, to\nattenuate noise from ocean waves and similar long wavelength contributions.\nDiscarding the phase information leads to low-frequency aliasing analogous to\nthe Nyquist-Shannon theorem for high frequencies. In non-stationary data, the\nphase content can stabilize training and improve the generalizability of neural\nnetworks. While it has been shown that phase content can be restored in deep\nneural networks, we show how including phase information in feature maps\nimproves both training and inference from deterministic physical data.\nFurthermore, we show that the reduction of parameters in a complex network\noutperforms larger real-valued networks.\n", "versions": [{"version": "v1", "created": "Wed, 29 May 2019 10:47:42 GMT"}, {"version": "v2", "created": "Tue, 26 Nov 2019 09:11:41 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["Dramsch", "Jesper S\u00f6ren", ""], ["L\u00fcthje", "Mikael", ""], ["Christensen", "Anders Nymark", ""]]}, {"id": "1905.12337", "submitter": "Gavneet Singh Chadha", "authors": "Gavneet Singh Chadha and Andreas Schwung", "title": "Learning the Non-linearity in Convolutional Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose the introduction of nonlinear operation into the feature\ngeneration process in convolutional neural networks. This nonlinearity can be\nimplemented in various ways. First we discuss the use of nonlinearities in the\nprocess of data augmentation to increase the robustness of the neural networks\nrecognition capacity. To this end, we randomly disturb the input data set by\napplying exponents within a certain numerical range to individual data points\nof the input space. Second we propose nonlinear convolutional neural networks\nwhere we apply the exponential operation to each element of the receptive\nfield. To this end, we define an additional weight matrix of the same dimension\nas the standard kernel weight matrix. The weights of this matrix then\nconstitute the exponents of the corresponding components of the receptive\nfield. In the basic setting, we keep the weight parameters fixed during\ntraining by defining suitable parameters. Alternatively, we make the\nexponential weight parameters end-to-end trainable using a suitable\nparameterization. The network architecture is applied to time series analysis\ndata set showing a considerable increase in the classification performance\ncompared to baseline networks.\n", "versions": [{"version": "v1", "created": "Wed, 29 May 2019 11:32:06 GMT"}], "update_date": "2019-05-30", "authors_parsed": [["Chadha", "Gavneet Singh", ""], ["Schwung", "Andreas", ""]]}, {"id": "1905.12349", "submitter": "Furao Shen", "authors": "Yang Yao, Xu Zhang, Baile Xu, Furao Shen, Jian Zhao", "title": "Super Interaction Neural Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent studies have demonstrated that the convolutional networks heavily rely\non the quality and quantity of generated features. However, in lightweight\nnetworks, there are limited available feature information because these\nnetworks tend to be shallower and thinner due to the efficiency consideration.\nFor farther improving the performance and accuracy of lightweight networks, we\ndevelop Super Interaction Neural Networks (SINet) model from a novel point of\nview: enhancing the information interaction in neural networks. In order to\nachieve information interaction along the width of the deep network, we propose\nExchange Shortcut Connection, which can integrate the information from\ndifferent convolution groups without any extra computation cost. And then, in\norder to achieve information interaction along the depth of the network, we\nproposed Dense Funnel Layer and Attention based Hierarchical Joint Decision,\nwhich are able to make full use of middle layer features. Our experiments show\nthat the superior performance of SINet over other state-of-the-art lightweight\nmodels in ImageNet dataset. Furthermore, we also exhibit the effectiveness and\nuniversality of our proposed components by ablation studies.\n", "versions": [{"version": "v1", "created": "Wed, 29 May 2019 11:54:07 GMT"}], "update_date": "2019-05-30", "authors_parsed": [["Yao", "Yang", ""], ["Zhang", "Xu", ""], ["Xu", "Baile", ""], ["Shen", "Furao", ""], ["Zhao", "Jian", ""]]}, {"id": "1905.12365", "submitter": "Peter Kontschieder", "authors": "Andrea Simonelli, Samuel Rota Rota Bul\\`o, Lorenzo Porzi, Manuel\n  L\\'opez-Antequera, Peter Kontschieder", "title": "Disentangling Monocular 3D Object Detection", "comments": "Project website at\n  https://research.mapillary.com/publication/MonoDIS/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper we propose an approach for monocular 3D object detection from a\nsingle RGB image, which leverages a novel disentangling transformation for 2D\nand 3D detection losses and a novel, self-supervised confidence score for 3D\nbounding boxes. Our proposed loss disentanglement has the twofold advantage of\nsimplifying the training dynamics in the presence of losses with complex\ninteractions of parameters, and sidestepping the issue of balancing independent\nregression terms. Our solution overcomes these issues by isolating the\ncontribution made by groups of parameters to a given loss, without changing its\nnature. We further apply loss disentanglement to another novel, signed\nIntersection-over-Union criterion-driven loss for improving 2D detection\nresults. Besides our methodological innovations, we critically review the AP\nmetric used in KITTI3D, which emerged as the most important dataset for\ncomparing 3D detection results. We identify and resolve a flaw in the 11-point\ninterpolated AP metric, affecting all previously published detection results\nand particularly biases the results of monocular 3D detection. We provide\nextensive experimental evaluations and ablation studies on the KITTI3D and\nnuScenes datasets, setting new state-of-the-art results on object category car\nby large margins.\n", "versions": [{"version": "v1", "created": "Wed, 29 May 2019 12:13:53 GMT"}], "update_date": "2019-05-30", "authors_parsed": [["Simonelli", "Andrea", ""], ["Bul\u00f2", "Samuel Rota Rota", ""], ["Porzi", "Lorenzo", ""], ["L\u00f3pez-Antequera", "Manuel", ""], ["Kontschieder", "Peter", ""]]}, {"id": "1905.12384", "submitter": "HongYu Liu", "authors": "Hongyu Liu and Bin Jiang and Yi Xiao and Chao Yang", "title": "Coherent Semantic Attention for Image Inpainting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The latest deep learning-based approaches have shown promising results for\nthe challenging task of inpainting missing regions of an image. However, the\nexisting methods often generate contents with blurry textures and distorted\nstructures due to the discontinuity of the local pixels. From a semantic-level\nperspective, the local pixel discontinuity is mainly because these methods\nignore the semantic relevance and feature continuity of hole regions. To handle\nthis problem, we investigate the human behavior in repairing pictures and\npropose a fined deep generative model-based approach with a novel coherent\nsemantic attention (CSA) layer, which can not only preserve contextual\nstructure but also make more effective predictions of missing parts by modeling\nthe semantic relevance between the holes features. The task is divided into\nrough, refinement as two steps and model each step with a neural network under\nthe U-Net architecture, where the CSA layer is embedded into the encoder of\nrefinement step. To stabilize the network training process and promote the CSA\nlayer to learn more effective parameters, we propose a consistency loss to\nenforce the both the CSA layer and the corresponding layer of the CSA in\ndecoder to be close to the VGG feature layer of a ground truth image\nsimultaneously. The experiments on CelebA, Places2, and Paris StreetView\ndatasets have validated the effectiveness of our proposed methods in image\ninpainting tasks and can obtain images with a higher quality as compared with\nthe existing state-of-the-art approaches.\n", "versions": [{"version": "v1", "created": "Wed, 29 May 2019 12:46:53 GMT"}, {"version": "v2", "created": "Wed, 12 Jun 2019 08:57:07 GMT"}, {"version": "v3", "created": "Thu, 4 Jul 2019 04:57:29 GMT"}], "update_date": "2019-07-05", "authors_parsed": [["Liu", "Hongyu", ""], ["Jiang", "Bin", ""], ["Xiao", "Yi", ""], ["Yang", "Chao", ""]]}, {"id": "1905.12409", "submitter": "Hefeng Wu", "authors": "Hefeng Wu, Yafei Hu, Keze Wang, Hanhui Li, Lin Nie, Hui Cheng", "title": "Instance-Aware Representation Learning and Association for Online\n  Multi-Person Tracking", "comments": "accepted by Pattern Recognition", "journal-ref": null, "doi": "10.1016/j.patcog.2019.04.018", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-Person Tracking (MPT) is often addressed within the\ndetection-to-association paradigm. In such approaches, human detections are\nfirst extracted in every frame and person trajectories are then recovered by a\nprocedure of data association (usually offline). However, their performances\nusually degenerate in presence of detection errors, mutual interactions and\nocclusions. In this paper, we present a deep learning based MPT approach that\nlearns instance-aware representations of tracked persons and robustly online\ninfers states of the tracked persons. Specifically, we design a multi-branch\nneural network (MBN), which predicts the classification confidences and\nlocations of all targets by taking a batch of candidate regions as input. In\nour MBN architecture, each branch (instance-subnet) corresponds to an\nindividual to be tracked and new branches can be dynamically created for\nhandling newly appearing persons. Then based on the output of MBN, we construct\na joint association matrix that represents meaningful states of tracked persons\n(e.g., being tracked or disappearing from the scene) and solve it by using the\nefficient Hungarian algorithm. Moreover, we allow the instance-subnets to be\nupdated during tracking by online mining hard examples, accounting to person\nappearance variations over time. We comprehensively evaluate our framework on a\npopular MPT benchmark, demonstrating its excellent performance in comparison\nwith recent online MPT methods.\n", "versions": [{"version": "v1", "created": "Wed, 29 May 2019 13:16:12 GMT"}], "update_date": "2019-05-30", "authors_parsed": [["Wu", "Hefeng", ""], ["Hu", "Yafei", ""], ["Wang", "Keze", ""], ["Li", "Hanhui", ""], ["Nie", "Lin", ""], ["Cheng", "Hui", ""]]}, {"id": "1905.12462", "submitter": "Swathikiran Sudhakaran", "authors": "Swathikiran Sudhakaran and Sergio Escalera and Oswald Lanz", "title": "Hierarchical Feature Aggregation Networks for Video Action Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most action recognition methods base on a) a late aggregation of frame level\nCNN features using average pooling, max pooling, or RNN, among others, or b)\nspatio-temporal aggregation via 3D convolutions. The first assume independence\namong frame features up to a certain level of abstraction and then perform\nhigher-level aggregation, while the second extracts spatio-temporal features\nfrom grouped frames as early fusion. In this paper we explore the space in\nbetween these two, by letting adjacent feature branches interact as they\ndevelop into the higher level representation. The interaction happens between\nfeature differencing and averaging at each level of the hierarchy, and it has\nconvolutional structure that learns to select the appropriate mode locally in\ncontrast to previous works that impose one of the modes globally (e.g. feature\ndifferencing) as a design choice. We further constrain this interaction to be\nconservative, e.g. a local feature subtraction in one branch is compensated by\nthe addition on another, such that the total feature flow is preserved. We\nevaluate the performance of our proposal on a number of existing models, i.e.\nTSN, TRN and ECO, to show its flexibility and effectiveness in improving action\nrecognition performance.\n", "versions": [{"version": "v1", "created": "Wed, 29 May 2019 13:58:37 GMT"}], "update_date": "2019-05-30", "authors_parsed": [["Sudhakaran", "Swathikiran", ""], ["Escalera", "Sergio", ""], ["Lanz", "Oswald", ""]]}, {"id": "1905.12487", "submitter": "Kaylen Pfisterer", "authors": "Kaylen J. Pfisterer, Jennifer Boger, Alexander Wong", "title": "Food for thought: Ethical considerations of user trust in computer\n  vision", "comments": "Accepted to CVPR2019: Fairness Accountability Transparency and Ethics\n  in Computer Vision Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.CV cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In computer vision research, especially when novel applications of tools are\ndeveloped, ethical implications around user perceptions of trust in the\nunderlying technology should be considered and supported. Here, we describe an\nexample of the incorporation of such considerations within the long-term care\nsector for tracking resident food and fluid intake. We highlight our recent\nuser study conducted to develop a Goldilocks quality horizontal prototype\ndesigned to support trust cues in which perceived trust in our horizontal\nprototype was higher than the existing system in place. We discuss the\nimportance and need for user engagement as part of ongoing computer\nvision-driven technology development and describe several important factors\nrelated to trust that are relevant to developing decision-making tools.\n", "versions": [{"version": "v1", "created": "Wed, 29 May 2019 14:25:43 GMT"}], "update_date": "2019-05-30", "authors_parsed": [["Pfisterer", "Kaylen J.", ""], ["Boger", "Jennifer", ""], ["Wong", "Alexander", ""]]}, {"id": "1905.12498", "submitter": "Jianxin Lin", "authors": "Jianxin Lin, Yingce Xia, Yijun Wang, Tao Qin, Zhibo Chen", "title": "Image-to-Image Translation with Multi-Path Consistency Regularization", "comments": "8 pages, 6 figures. Accepted by the 28th International Joint\n  Conference on Artificial Intelligence (IJCAI-2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image translation across different domains has attracted much attention in\nboth machine learning and computer vision communities. Taking the translation\nfrom source domain $\\mathcal{D}_s$ to target domain $\\mathcal{D}_t$ as an\nexample, existing algorithms mainly rely on two kinds of loss for training: One\nis the discrimination loss, which is used to differentiate images generated by\nthe models and natural images; the other is the reconstruction loss, which\nmeasures the difference between an original image and the reconstructed version\nthrough $\\mathcal{D}_s\\to\\mathcal{D}_t\\to\\mathcal{D}_s$ translation. In this\nwork, we introduce a new kind of loss, multi-path consistency loss, which\nevaluates the differences between direct translation\n$\\mathcal{D}_s\\to\\mathcal{D}_t$ and indirect translation\n$\\mathcal{D}_s\\to\\mathcal{D}_a\\to\\mathcal{D}_t$ with $\\mathcal{D}_a$ as an\nauxiliary domain, to regularize training. For multi-domain translation (at\nleast, three) which focuses on building translation models between any two\ndomains, at each training iteration, we randomly select three domains, set them\nrespectively as the source, auxiliary and target domains, build the multi-path\nconsistency loss and optimize the network. For two-domain translation, we need\nto introduce an additional auxiliary domain and construct the multi-path\nconsistency loss. We conduct various experiments to demonstrate the\neffectiveness of our proposed methods, including face-to-face translation,\npaint-to-photo translation, and de-raining/de-noising translation.\n", "versions": [{"version": "v1", "created": "Wed, 29 May 2019 14:36:03 GMT"}], "update_date": "2019-05-30", "authors_parsed": [["Lin", "Jianxin", ""], ["Xia", "Yingce", ""], ["Wang", "Yijun", ""], ["Qin", "Tao", ""], ["Chen", "Zhibo", ""]]}, {"id": "1905.12502", "submitter": "Hideaki Hayashi D.Eng.", "authors": "Hideaki Hayashi, Kohtaro Abe, Seiichi Uchida", "title": "GlyphGAN: Style-Consistent Font Generation Based on Generative\n  Adversarial Networks", "comments": "To appear in Knowledge-Based Systems", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose GlyphGAN: style-consistent font generation based on\ngenerative adversarial networks (GANs). GANs are a framework for learning a\ngenerative model using a system of two neural networks competing with each\nother. One network generates synthetic images from random input vectors, and\nthe other discriminates between synthetic and real images. The motivation of\nthis study is to create new fonts using the GAN framework while maintaining\nstyle consistency over all characters. In GlyphGAN, the input vector for the\ngenerator network consists of two vectors: character class vector and style\nvector. The former is a one-hot vector and is associated with the character\nclass of each sample image during training. The latter is a uniform random\nvector without supervised information. In this way, GlyphGAN can generate an\ninfinite variety of fonts with the character and style independently\ncontrolled. Experimental results showed that fonts generated by GlyphGAN have\nstyle consistency and diversity different from the training images without\nlosing their legibility.\n", "versions": [{"version": "v1", "created": "Wed, 29 May 2019 14:44:23 GMT"}, {"version": "v2", "created": "Thu, 30 May 2019 07:30:43 GMT"}], "update_date": "2019-05-31", "authors_parsed": [["Hayashi", "Hideaki", ""], ["Abe", "Kohtaro", ""], ["Uchida", "Seiichi", ""]]}, {"id": "1905.12506", "submitter": "Sjoerd van Steenkiste", "authors": "Sjoerd van Steenkiste, Francesco Locatello, J\\\"urgen Schmidhuber,\n  Olivier Bachem", "title": "Are Disentangled Representations Helpful for Abstract Visual Reasoning?", "comments": "Accepted to NeurIPS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A disentangled representation encodes information about the salient factors\nof variation in the data independently. Although it is often argued that this\nrepresentational format is useful in learning to solve many real-world\ndown-stream tasks, there is little empirical evidence that supports this claim.\nIn this paper, we conduct a large-scale study that investigates whether\ndisentangled representations are more suitable for abstract reasoning tasks.\nUsing two new tasks similar to Raven's Progressive Matrices, we evaluate the\nusefulness of the representations learned by 360 state-of-the-art unsupervised\ndisentanglement models. Based on these representations, we train 3600 abstract\nreasoning models and observe that disentangled representations do in fact lead\nto better down-stream performance. In particular, they enable quicker learning\nusing fewer samples.\n", "versions": [{"version": "v1", "created": "Wed, 29 May 2019 14:52:32 GMT"}, {"version": "v2", "created": "Mon, 21 Oct 2019 17:00:42 GMT"}, {"version": "v3", "created": "Tue, 7 Jan 2020 14:36:07 GMT"}], "update_date": "2020-01-08", "authors_parsed": [["van Steenkiste", "Sjoerd", ""], ["Locatello", "Francesco", ""], ["Schmidhuber", "J\u00fcrgen", ""], ["Bachem", "Olivier", ""]]}, {"id": "1905.12512", "submitter": "Marvin Eisenberger", "authors": "Marvin Eisenberger, Zorah L\\\"ahner, Daniel Cremers", "title": "Smooth Shells: Multi-Scale Shape Registration with Functional Maps", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel 3D shape correspondence method based on the iterative\nalignment of so-called smooth shells. Smooth shells define a series of\ncoarse-to-fine shape approximations designed to work well with multiscale\nalgorithms. The main idea is to first align rough approximations of the\ngeometry and then add more and more details to refine the correspondence. We\nfuse classical shape registration with Functional Maps by embedding the input\nshapes into an intrinsic-extrinsic product space. Moreover, we disambiguate\nintrinsic symmetries by applying a surrogate based Markov chain Monte Carlo\ninitialization. Our method naturally handles various types of noise that\ncommonly occur in real scans, like non-isometry or incompatible meshing.\nFinally, we demonstrate state-of-the-art quantitative results on several\ndatasets and show that our pipeline produces smoother, more realistic results\nthan other automatic matching methods in real world applications.\n", "versions": [{"version": "v1", "created": "Wed, 29 May 2019 15:01:10 GMT"}, {"version": "v2", "created": "Mon, 2 Dec 2019 16:33:20 GMT"}], "update_date": "2019-12-03", "authors_parsed": [["Eisenberger", "Marvin", ""], ["L\u00e4hner", "Zorah", ""], ["Cremers", "Daniel", ""]]}, {"id": "1905.12534", "submitter": "Ricard Durall Lopez", "authors": "Ricard Durall, Franz-Josef Pfreundt, Janis Keuper", "title": "Stabilizing GANs with Soft Octave Convolutions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Motivated by recently published methods using frequency decompositions of\nconvolutions (e.g. Octave Convolutions), we propose a novel convolution scheme\nto stabilize the training and reduce the likelihood of a mode collapse. The\nbasic idea of our approach is to split convolutional filters into additive high\nand low frequency parts, while shifting weight updates from low to high during\nthe training. Intuitively, this method forces GANs to learn low frequency\ncoarse image structures before descending into fine (high frequency) details.\nWe also show, that the use of the proposed soft octave convolutions reduces\ncommon artifacts in the frequency domain of generated images. Our approach is\northogonal and complementary to existing stabilization methods and can simply\nbe plugged into any CNN based GAN architecture. Experiments on the CelebA\ndataset show the effectiveness of the proposed method.\n", "versions": [{"version": "v1", "created": "Wed, 29 May 2019 15:28:54 GMT"}, {"version": "v2", "created": "Wed, 7 Oct 2020 14:13:12 GMT"}, {"version": "v3", "created": "Thu, 17 Dec 2020 15:49:57 GMT"}], "update_date": "2020-12-18", "authors_parsed": [["Durall", "Ricard", ""], ["Pfreundt", "Franz-Josef", ""], ["Keuper", "Janis", ""]]}, {"id": "1905.12536", "submitter": "Heng Yang", "authors": "Heng Yang, Luca Carlone", "title": "A Quaternion-based Certifiably Optimal Solution to the Wahba Problem\n  with Outliers", "comments": "21 pages, accepted for Oral Presentation at ICCV 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Wahba problem, also known as rotation search, seeks to find the best\nrotation to align two sets of vector observations given putative\ncorrespondences, and is a fundamental routine in many computer vision and\nrobotics applications. This work proposes the first polynomial-time certifiably\noptimal approach for solving the Wahba problem when a large number of vector\nobservations are outliers. Our first contribution is to formulate the Wahba\nproblem using a Truncated Least Squares (TLS) cost that is insensitive to a\nlarge fraction of spurious correspondences. The second contribution is to\nrewrite the problem using unit quaternions and show that the TLS cost can be\nframed as a Quadratically-Constrained Quadratic Program (QCQP). Since the\nresulting optimization is still highly non-convex and hard to solve globally,\nour third contribution is to develop a convex Semidefinite Programming (SDP)\nrelaxation. We show that while a naive relaxation performs poorly in general,\nour relaxation is tight even in the presence of large noise and outliers. We\nvalidate the proposed algorithm, named QUASAR (QUAternion-based Semidefinite\nrelAxation for Robust alignment), in both synthetic and real datasets showing\nthat the algorithm outperforms RANSAC, robust local optimization techniques,\nglobal outlier-removal procedures, and Branch-and-Bound methods. QUASAR is able\nto compute certifiably optimal solutions (i.e. the relaxation is exact) even in\nthe case when 95% of the correspondences are outliers.\n", "versions": [{"version": "v1", "created": "Wed, 29 May 2019 15:36:40 GMT"}, {"version": "v2", "created": "Tue, 23 Jul 2019 16:03:25 GMT"}, {"version": "v3", "created": "Fri, 16 Aug 2019 18:19:53 GMT"}, {"version": "v4", "created": "Sun, 22 Sep 2019 14:54:34 GMT"}], "update_date": "2019-09-24", "authors_parsed": [["Yang", "Heng", ""], ["Carlone", "Luca", ""]]}, {"id": "1905.12563", "submitter": "Philipp M. Maier", "authors": "Philipp M. Maier and Sina Keller", "title": "Application of Different Simulated Spectral Data and Machine Learning to\n  Estimate the Chlorophyll a Concentration of Several Inland Waters", "comments": "This contribution was accepted for the IEEE Whispers 2019 in\n  Amsterdam", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Water quality is of great importance for humans and for the environment and\nhas to be monitored continuously. It is determinable through proxies such as\nthe chlorophyll a concentration, which can be monitored by remote sensing\ntechniques. This study focuses on the trade-off between the spatial and the\nspectral resolution of six simulated satellite-based data sets when estimating\nthe chlorophyll a concentration with supervised machine learning models. The\ninitial dataset for the spectral simulation of the satellite missions contains\nspectrometer data and measured chlorophyll a concentration of 13 different\ninland waters. Focusing on the regression performance, it appears that the\nmachine learning models achieve almost as good results with the simulated\nSentinel data as with the simulated hyperspectral data. Regarding the\napplicability, the Sentinel 2 mission is the best choice for small inland\nwaters due to its high spatial and temporal resolution in combination with a\nsuitable spectral resolution.\n", "versions": [{"version": "v1", "created": "Wed, 29 May 2019 16:14:52 GMT"}, {"version": "v2", "created": "Wed, 21 Aug 2019 08:43:01 GMT"}], "update_date": "2019-08-22", "authors_parsed": [["Maier", "Philipp M.", ""], ["Keller", "Sina", ""]]}, {"id": "1905.12571", "submitter": "Chi-Wei Hsiao", "authors": "Chi-Wei Hsiao, Cheng Sun, Min Sun, Hwann-Tzong Chen", "title": "Flat2Layout: Flat Representation for Estimating Layout of General Room\n  Types", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a new approach, Flat2Layout, for estimating general\nindoor room layout from a single-view RGB image whereas existing methods can\nonly produce layout topologies captured from the box-shaped room. The proposed\nflat representation encodes the layout information into row vectors which are\ntreated as the training target of the deep model. A dynamic programming based\npostprocessing is employed to decode the estimated flat output from the deep\nmodel into the final room layout. Flat2Layout achieves state-of-the-art\nperformance on existing room layout benchmark. This paper also constructs a\nbenchmark for validating the performance on general layout topologies, where\nFlat2Layout achieves good performance on general room types. Flat2Layout is\napplicable on more scenario for layout estimation and would have an impact on\napplications of Scene Modeling, Robotics, and Augmented Reality.\n", "versions": [{"version": "v1", "created": "Wed, 29 May 2019 16:26:43 GMT"}], "update_date": "2019-05-30", "authors_parsed": [["Hsiao", "Chi-Wei", ""], ["Sun", "Cheng", ""], ["Sun", "Min", ""], ["Chen", "Hwann-Tzong", ""]]}, {"id": "1905.12596", "submitter": "Michiel Straat", "authors": "Michiel Straat, Jorrit Oosterhof", "title": "Segmentation of blood vessels in retinal fundus images", "comments": "Conference: SC@RUG 2017, 6 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, several automatic segmentation methods have been proposed\nfor blood vessels in retinal fundus images, ranging from using cheap and fast\ntrainable filters to complicated neural networks and even deep learning. One\nexample of a filted-based segmentation method is B-COSFIRE. In this approach\nthe image filter is trained with example prototype patterns, to which the\nfilter becomes selective by finding points in a Difference of Gaussian response\non circles around the center with large intensity variation. In this paper we\ndiscuss and evaluate several of these vessel segmentation methods. We take a\ncloser look at B-COSFIRE and study the performance of B-COSFIRE on the recently\npublished IOSTAR dataset by experiments and we examine how the parameter values\naffect the performance. In the experiment we manage to reach a segmentation\naccuracy of 0.9419. Based on our findings we discuss when B-COSFIRE is the\npreferred method to use and in which circumstances it could be beneficial to\nuse a more (computationally) complex segmentation method. We also shortly\ndiscuss areas beyond blood vessel segmentation where these methods can be used\nto segment elongated structures, such as rivers in satellite images or nerves\nof a leaf.\n", "versions": [{"version": "v1", "created": "Wed, 29 May 2019 17:17:45 GMT"}], "update_date": "2019-05-30", "authors_parsed": [["Straat", "Michiel", ""], ["Oosterhof", "Jorrit", ""]]}, {"id": "1905.12612", "submitter": "Ashish Kumar", "authors": "Ashish Kumar, Saurabh Gupta, Jitendra Malik", "title": "Learning Navigation Subroutines from Egocentric Videos", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Planning at a higher level of abstraction instead of low level torques\nimproves the sample efficiency in reinforcement learning, and computational\nefficiency in classical planning. We propose a method to learn such\nhierarchical abstractions, or subroutines from egocentric video data of experts\nperforming tasks. We learn a self-supervised inverse model on small amounts of\nrandom interaction data to pseudo-label the expert egocentric videos with agent\nactions. Visuomotor subroutines are acquired from these pseudo-labeled videos\nby learning a latent intent-conditioned policy that predicts the inferred\npseudo-actions from the corresponding image observations. We demonstrate our\nproposed approach in context of navigation, and show that we can successfully\nlearn consistent and diverse visuomotor subroutines from passive egocentric\nvideos. We demonstrate the utility of our acquired visuomotor subroutines by\nusing them as is for exploration, and as sub-policies in a hierarchical RL\nframework for reaching point goals and semantic goals. We also demonstrate\nbehavior of our subroutines in the real world, by deploying them on a real\nrobotic platform. Project website:\nhttps://ashishkumar1993.github.io/subroutines/.\n", "versions": [{"version": "v1", "created": "Wed, 29 May 2019 17:50:19 GMT"}, {"version": "v2", "created": "Tue, 15 Oct 2019 08:10:25 GMT"}], "update_date": "2019-10-16", "authors_parsed": [["Kumar", "Ashish", ""], ["Gupta", "Saurabh", ""], ["Malik", "Jitendra", ""]]}, {"id": "1905.12663", "submitter": "Adam Bielski", "authors": "Adam Bielski, Paolo Favaro", "title": "Emergence of Object Segmentation in Perturbed Generative Models", "comments": "33rd Conference on Neural Information Processing Systems (NeurIPS\n  2019), Spotlight presentation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a novel framework to build a model that can learn how to segment\nobjects from a collection of images without any human annotation. Our method\nbuilds on the observation that the location of object segments can be perturbed\nlocally relative to a given background without affecting the realism of a\nscene. Our approach is to first train a generative model of a layered scene.\nThe layered representation consists of a background image, a foreground image\nand the mask of the foreground. A composite image is then obtained by\noverlaying the masked foreground image onto the background. The generative\nmodel is trained in an adversarial fashion against a discriminator, which\nforces the generative model to produce realistic composite images. To force the\ngenerator to learn a representation where the foreground layer corresponds to\nan object, we perturb the output of the generative model by introducing a\nrandom shift of both the foreground image and mask relative to the background.\nBecause the generator is unaware of the shift before computing its output, it\nmust produce layered representations that are realistic for any such random\nperturbation. Finally, we learn to segment an image by defining an autoencoder\nconsisting of an encoder, which we train, and the pre-trained generator as the\ndecoder, which we freeze. The encoder maps an image to a feature vector, which\nis fed as input to the generator to give a composite image matching the\noriginal input image. Because the generator outputs an explicit layered\nrepresentation of the scene, the encoder learns to detect and segment objects.\nWe demonstrate this framework on real images of several object categories.\n", "versions": [{"version": "v1", "created": "Wed, 29 May 2019 18:17:39 GMT"}, {"version": "v2", "created": "Sat, 2 Nov 2019 17:46:33 GMT"}], "update_date": "2019-11-05", "authors_parsed": [["Bielski", "Adam", ""], ["Favaro", "Paolo", ""]]}, {"id": "1905.12678", "submitter": "Rozenn Dahyot", "authors": "Rozenn Dahyot and Hana Alghamdi and Mairead Grogan", "title": "Entropic Regularisation of Robust Optimal Transport", "comments": "8 pages", "journal-ref": "Proceeding of Irish Machine Vision and Image Processing conference\n  IMVIP 2019", "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Grogan et al [11,12] have recently proposed a solution to colour transfer by\nminimising the Euclidean distance L2 between two probability density functions\ncapturing the colour distributions of two images (palette and target). It was\nshown to be very competitive to alternative solutions based on Optimal\nTransport for colour transfer. We show that in fact Grogan et al's formulation\ncan also be understood as a new robust Optimal Transport based framework with\nentropy regularisation over marginals.\n", "versions": [{"version": "v1", "created": "Wed, 29 May 2019 18:51:11 GMT"}], "update_date": "2019-07-15", "authors_parsed": [["Dahyot", "Rozenn", ""], ["Alghamdi", "Hana", ""], ["Grogan", "Mairead", ""]]}, {"id": "1905.12681", "submitter": "Weiyao Wang", "authors": "Weiyao Wang and Du Tran and Matt Feiszli", "title": "What Makes Training Multi-Modal Classification Networks Hard?", "comments": "CVPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider end-to-end training of a multi-modal vs. a single-modal network on a\ntask with multiple input modalities: the multi-modal network receives more\ninformation, so it should match or outperform its single-modal counterpart. In\nour experiments, however, we observe the opposite: the best single-modal\nnetwork always outperforms the multi-modal network. This observation is\nconsistent across different combinations of modalities and on different tasks\nand benchmarks.\n  This paper identifies two main causes for this performance drop: first,\nmulti-modal networks are often prone to overfitting due to increased capacity.\nSecond, different modalities overfit and generalize at different rates, so\ntraining them jointly with a single optimization strategy is sub-optimal. We\naddress these two problems with a technique we call Gradient Blending, which\ncomputes an optimal blend of modalities based on their overfitting behavior. We\ndemonstrate that Gradient Blending outperforms widely-used baselines for\navoiding overfitting and achieves state-of-the-art accuracy on various tasks\nincluding human action recognition, ego-centric action recognition, and\nacoustic event detection.\n", "versions": [{"version": "v1", "created": "Wed, 29 May 2019 19:10:06 GMT"}, {"version": "v2", "created": "Thu, 20 Jun 2019 18:24:31 GMT"}, {"version": "v3", "created": "Sat, 29 Jun 2019 07:04:17 GMT"}, {"version": "v4", "created": "Mon, 9 Dec 2019 22:49:19 GMT"}, {"version": "v5", "created": "Fri, 3 Apr 2020 00:36:42 GMT"}], "update_date": "2020-04-06", "authors_parsed": [["Wang", "Weiyao", ""], ["Tran", "Du", ""], ["Feiszli", "Matt", ""]]}, {"id": "1905.12683", "submitter": "Xiaoke Shen", "authors": "Xiaoke Shen", "title": "A survey of Object Classification and Detection based on 2D/3D data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, by using deep neural network based algorithms, object\nclassification, detection and semantic segmentation solutions are significantly\nimproved. However, one challenge for 2D image-based systems is that they cannot\nprovide accurate 3D location information. This is critical for location\nsensitive applications such as autonomous driving and robot navigation. On the\nother hand, 3D methods, such as RGB-D and RGB-LiDAR based systems, can provide\nsolutions that significantly improve the RGB only approaches. That is why this\nis an interesting research area for both industry and academia. Compared with\n2D image-based systems, 3D-based systems are more complicated due to the\nfollowing five reasons: 1) Data representation itself is more complicated. 3D\nimages can be represented by point clouds, meshes, volumes. 2D images have\npixel grid representations. 2) The computation and memory resource requirement\nis higher as an extra dimension is added. 3) Different distribution of the\nobjects and difference in scene areas between indoor and outdoor make one\nunified framework hard to achieve. 4) 3D data, especially for the outdoor\nscenario, is sparse compared with the dense 2D images which makes the detection\ntask more challenging. Finally, large size labelled datasets, which are\nextremely important for supervised based algorithms, are still under\nconstruction compared with well-built 2D datasets such as ImageNet. Based on\nchallenges listed above, the described systems are organized by application\nscenarios, data representation methods and main tasks addressed. At the same\ntime, critical 2D based systems which greatly influence the 3D ones are also\nintroduced to show the connection between them.\n", "versions": [{"version": "v1", "created": "Wed, 29 May 2019 19:15:01 GMT"}], "update_date": "2019-10-15", "authors_parsed": [["Shen", "Xiaoke", ""]]}, {"id": "1905.12708", "submitter": "Athmanarayanan Lakshmi Narayanan", "authors": "Athma Narayanan, Isht Dwivedi, Behzad Dariush", "title": "Dynamic Traffic Scene Classification with Space-Time Coherence", "comments": "accpeted in (International Conference on Robotics and Automation)ICRA\n  2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  This paper examines the problem of dynamic traffic scene classification under\nspace-time variations in viewpoint that arise from video captured on-board a\nmoving vehicle. Solutions to this problem are important for realization of\neffective driving assistance technologies required to interpret or predict road\nuser behavior. Currently, dynamic traffic scene classification has not been\nadequately addressed due to a lack of benchmark datasets that consider\nspatiotemporal evolution of traffic scenes resulting from a vehicle's\nego-motion. This paper has three main contributions. First, an annotated\ndataset is released to enable dynamic scene classification that includes 80\nhours of diverse high quality driving video data clips collected in the San\nFrancisco Bay area. The dataset includes temporal annotations for road places,\nroad types, weather, and road surface conditions. Second, we introduce novel\nand baseline algorithms that utilize semantic context and temporal nature of\nthe dataset for dynamic classification of road scenes. Finally, we showcase\nalgorithms and experimental results that highlight how extracted features from\nscene classification serve as strong priors and help with tactical driver\nbehavior understanding. The results show significant improvement from\npreviously reported driving behavior detection baselines in the literature.\n", "versions": [{"version": "v1", "created": "Wed, 29 May 2019 20:28:49 GMT"}], "update_date": "2019-05-31", "authors_parsed": [["Narayanan", "Athma", ""], ["Dwivedi", "Isht", ""], ["Dariush", "Behzad", ""]]}, {"id": "1905.12723", "submitter": "Jiawei Mo", "authors": "Jiawei Mo, Junaed Sattar", "title": "Extending Monocular Visual Odometry to Stereo Camera Systems by Scale\n  Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a novel approach for extending monocular visual odometry\nto a stereo camera system. The proposed method uses an additional camera to\naccurately estimate and optimize the scale of the monocular visual odometry,\nrather than triangulating 3D points from stereo matching. Specifically, the 3D\npoints generated by the monocular visual odometry are projected onto the other\ncamera of the stereo pair, and the scale is recovered and optimized by directly\nminimizing the photometric error. It is computationally efficient, adding\nminimal overhead to the stereo vision system compared to straightforward stereo\nmatching, and is robust to repetitive texture. Additionally, direct scale\noptimization enables stereo visual odometry to be purely based on the direct\nmethod. Extensive evaluation on public datasets (e.g., KITTI), and outdoor\nenvironments (both terrestrial and underwater) demonstrates the accuracy and\nefficiency of a stereo visual odometry approach extended by scale optimization,\nand its robustness in environments with challenging textures.\n", "versions": [{"version": "v1", "created": "Wed, 29 May 2019 21:05:46 GMT"}, {"version": "v2", "created": "Mon, 16 Sep 2019 14:50:52 GMT"}, {"version": "v3", "created": "Tue, 17 Sep 2019 04:05:06 GMT"}], "update_date": "2019-09-18", "authors_parsed": [["Mo", "Jiawei", ""], ["Sattar", "Junaed", ""]]}, {"id": "1905.12729", "submitter": "Feihu Huang", "authors": "Feihu Huang, Shangqian Gao, Songcan Chen, Heng Huang", "title": "Zeroth-Order Stochastic Alternating Direction Method of Multipliers for\n  Nonconvex Nonsmooth Optimization", "comments": "To Appear in IJCAI 2019. Supplementary materials are added", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Alternating direction method of multipliers (ADMM) is a popular optimization\ntool for the composite and constrained problems in machine learning. However,\nin many machine learning problems such as black-box attacks and bandit\nfeedback, ADMM could fail because the explicit gradients of these problems are\ndifficult or infeasible to obtain. Zeroth-order (gradient-free) methods can\neffectively solve these problems due to that the objective function values are\nonly required in the optimization. Recently, though there exist a few\nzeroth-order ADMM methods, they build on the convexity of objective function.\nClearly, these existing zeroth-order methods are limited in many applications.\nIn the paper, thus, we propose a class of fast zeroth-order stochastic ADMM\nmethods (i.e., ZO-SVRG-ADMM and ZO-SAGA-ADMM) for solving nonconvex problems\nwith multiple nonsmooth penalties, based on the coordinate smoothing gradient\nestimator. Moreover, we prove that both the ZO-SVRG-ADMM and ZO-SAGA-ADMM have\nconvergence rate of $O(1/T)$, where $T$ denotes the number of iterations. In\nparticular, our methods not only reach the best convergence rate $O(1/T)$ for\nthe nonconvex optimization, but also are able to effectively solve many complex\nmachine learning problems with multiple regularized penalties and constraints.\nFinally, we conduct the experiments of black-box binary classification and\nstructured adversarial attack on black-box deep neural network to validate the\nefficiency of our algorithms.\n", "versions": [{"version": "v1", "created": "Wed, 29 May 2019 21:10:14 GMT"}, {"version": "v2", "created": "Tue, 30 Jul 2019 02:09:00 GMT"}], "update_date": "2019-07-31", "authors_parsed": [["Huang", "Feihu", ""], ["Gao", "Shangqian", ""], ["Chen", "Songcan", ""], ["Huang", "Heng", ""]]}, {"id": "1905.12759", "submitter": "Richard Jiang", "authors": "Ranjith Dinakaran, Philip Easom, Li Zhang, Ahmed Bouridane, Richard\n  Jiang, Eran Edirisinghe", "title": "Distant Pedestrian Detection in the Wild using Single Shot Detector with\n  Deep Convolutional Generative Adversarial Networks", "comments": "arXiv admin note: text overlap with arXiv:1711.08174,\n  arXiv:1511.06434, arXiv:1706.05274 by other authors", "journal-ref": "The 2019 International Joint Conference on Neural Networks (IJCNN)", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this work, we examine the feasibility of applying Deep Convolutional\nGenerative Adversarial Networks (DCGANs) with Single Shot Detector (SSD) as\ndata-processing technique to handle with the challenge of pedestrian detection\nin the wild. Specifically, we attempted to use in-fill completion (where a\nportion of the image is masked) to generate random transformations of images\nwith portions missing to expand existing labelled datasets. In our work, GAN\nhas been trained intensively on low resolution images, in order to neutralize\nthe challenges of the pedestrian detection in the wild, and considered humans,\nand few other classes for detection in smart cities. The object detector\nexperiment performed by training GAN model along with SSD provided a\nsubstantial improvement in the results. This approach presents a very\ninteresting overview in the current state of art on GAN networks for object\ndetection. We used Canadian Institute for Advanced Research (CIFAR), Caltech,\nKITTI data set for training and testing the network under different resolutions\nand the experimental results with comparison been showedbetween DCGAN cascaded\nwith SSD and SSD itself.\n", "versions": [{"version": "v1", "created": "Wed, 29 May 2019 22:42:49 GMT"}], "update_date": "2019-06-02", "authors_parsed": [["Dinakaran", "Ranjith", ""], ["Easom", "Philip", ""], ["Zhang", "Li", ""], ["Bouridane", "Ahmed", ""], ["Jiang", "Richard", ""], ["Edirisinghe", "Eran", ""]]}, {"id": "1905.12760", "submitter": "Miko{\\l}aj Bi\\'nkowski", "authors": "Miko{\\l}aj Bi\\'nkowski, R Devon Hjelm and Aaron Courville", "title": "Batch weight for domain adaptation with mass shift", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unsupervised domain transfer is the task of transferring or translating\nsamples from a source distribution to a different target distribution. Current\nsolutions unsupervised domain transfer often operate on data on which the modes\nof the distribution are well-matched, for instance have the same frequencies of\nclasses between source and target distributions. However, these models do not\nperform well when the modes are not well-matched, as would be the case when\nsamples are drawn independently from two different, but related, domains. This\nmode imbalance is problematic as generative adversarial networks (GANs), a\nsuccessful approach in this setting, are sensitive to mode frequency, which\nresults in a mismatch of semantics between source samples and generated samples\nof the target distribution. We propose a principled method of re-weighting\ntraining samples to correct for such mass shift between the transferred\ndistributions, which we call batch-weight. We also provide rigorous\nprobabilistic setting for domain transfer and new simplified objective for\ntraining transfer networks, an alternative to complex, multi-component loss\nfunctions used in the current state-of-the art image-to-image translation\nmodels. The new objective stems from the discrimination of joint distributions\nand enforces cycle-consistency in an abstract, high-level, rather than\npixel-wise, sense. Lastly, we experimentally show the effectiveness of the\nproposed methods in several image-to-image translation tasks.\n", "versions": [{"version": "v1", "created": "Wed, 29 May 2019 22:43:29 GMT"}], "update_date": "2019-05-31", "authors_parsed": [["Bi\u0144kowski", "Miko\u0142aj", ""], ["Hjelm", "R Devon", ""], ["Courville", "Aaron", ""]]}, {"id": "1905.12775", "submitter": "Ragav Venkatesan", "authors": "Xiang Xu, Xiong Zhou, Ragav Venkatesan, Gurumurthy Swaminathan and\n  Orchid Majumder", "title": "$d$-SNE: Domain Adaptation using Stochastic Neighborhood Embedding", "comments": "Accepted as Oral at CVPR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks often require copious amount of labeled-data to train\ntheir scads of parameters. Training larger and deeper networks is hard without\nappropriate regularization, particularly while using a small dataset.\nLaterally, collecting well-annotated data is expensive, time-consuming and\noften infeasible. A popular way to regularize these networks is to simply train\nthe network with more data from an alternate representative dataset. This can\nlead to adverse effects if the statistics of the representative dataset are\ndissimilar to our target. This predicament is due to the problem of domain\nshift. Data from a shifted domain might not produce bespoke features when a\nfeature extractor from the representative domain is used. In this paper, we\npropose a new technique ($d$-SNE) of domain adaptation that cleverly uses\nstochastic neighborhood embedding techniques and a novel modified-Hausdorff\ndistance. The proposed technique is learnable end-to-end and is therefore,\nideally suited to train neural networks. Extensive experiments demonstrate that\n$d$-SNE outperforms the current states-of-the-art and is robust to the\nvariances in different datasets, even in the one-shot and semi-supervised\nlearning settings. $d$-SNE also demonstrates the ability to generalize to\nmultiple domains concurrently.\n", "versions": [{"version": "v1", "created": "Wed, 29 May 2019 23:16:51 GMT"}], "update_date": "2019-05-31", "authors_parsed": [["Xu", "Xiang", ""], ["Zhou", "Xiong", ""], ["Venkatesan", "Ragav", ""], ["Swaminathan", "Gurumurthy", ""], ["Majumder", "Orchid", ""]]}, {"id": "1905.12794", "submitter": "Xiaoxiao Guo", "authors": "Hui Wu, Yupeng Gao, Xiaoxiao Guo, Ziad Al-Halah, Steven Rennie,\n  Kristen Grauman, Rogerio Feris", "title": "Fashion IQ: A New Dataset Towards Retrieving Images by Natural Language\n  Feedback", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conversational interfaces for the detail-oriented retail fashion domain are\nmore natural, expressive, and user friendly than classical keyword-based search\ninterfaces. In this paper, we introduce the Fashion IQ dataset to support and\nadvance research on interactive fashion image retrieval. Fashion IQ is the\nfirst fashion dataset to provide human-generated captions that distinguish\nsimilar pairs of garment images together with side-information consisting of\nreal-world product descriptions and derived visual attribute labels for these\nimages. We provide a detailed analysis of the characteristics of the Fashion IQ\ndata, and present a transformer-based user simulator and interactive image\nretriever that can seamlessly integrate visual attributes with image features,\nuser feedback, and dialog history, leading to improved performance over the\nstate of the art in dialog-based image retrieval. We believe that our dataset\nwill encourage further work on developing more natural and real-world\napplicable conversational shopping assistants.\n", "versions": [{"version": "v1", "created": "Thu, 30 May 2019 00:15:12 GMT"}, {"version": "v2", "created": "Mon, 30 Dec 2019 21:13:27 GMT"}, {"version": "v3", "created": "Wed, 25 Nov 2020 22:10:37 GMT"}], "update_date": "2020-11-30", "authors_parsed": [["Wu", "Hui", ""], ["Gao", "Yupeng", ""], ["Guo", "Xiaoxiao", ""], ["Al-Halah", "Ziad", ""], ["Rennie", "Steven", ""], ["Grauman", "Kristen", ""], ["Feris", "Rogerio", ""]]}, {"id": "1905.12797", "submitter": "Yuping Lin", "authors": "Yuping Lin, Kasra Ahmadi K. A., Hui Jiang", "title": "Bandlimiting Neural Networks Against Adversarial Attacks", "comments": "Summitted to NeurIPS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.CV cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study the adversarial attack and defence problem in deep\nlearning from the perspective of Fourier analysis. We first explicitly compute\nthe Fourier transform of deep ReLU neural networks and show that there exist\ndecaying but non-zero high frequency components in the Fourier spectrum of\nneural networks. We demonstrate that the vulnerability of neural networks\ntowards adversarial samples can be attributed to these insignificant but\nnon-zero high frequency components. Based on this analysis, we propose to use a\nsimple post-averaging technique to smooth out these high frequency components\nto improve the robustness of neural networks against adversarial attacks.\nExperimental results on the ImageNet dataset have shown that our proposed\nmethod is universally effective to defend many existing adversarial attacking\nmethods proposed in the literature, including FGSM, PGD, DeepFool and C&W\nattacks. Our post-averaging method is simple since it does not require any\nre-training, and meanwhile it can successfully defend over 95% of the\nadversarial samples generated by these methods without introducing any\nsignificant performance degradation (less than 1%) on the original clean\nimages.\n", "versions": [{"version": "v1", "created": "Thu, 30 May 2019 00:34:50 GMT"}], "update_date": "2019-05-31", "authors_parsed": [["Lin", "Yuping", ""], ["A.", "Kasra Ahmadi K.", ""], ["Jiang", "Hui", ""]]}, {"id": "1905.12806", "submitter": "Philipp Seeb\\\"ock", "authors": "Philipp Seeb\\\"ock, Jos\\'e Ignacio Orlando, Thomas Schlegl, Sebastian\n  M. Waldstein, Hrvoje Bogunovi\\'c, Sophie Klimscha, Georg Langs, Ursula\n  Schmidt-Erfurth", "title": "Exploiting Epistemic Uncertainty of Anatomy Segmentation for Anomaly\n  Detection in Retinal OCT", "comments": "Accepted for publication in IEEE Transactions on Medical Imaging,\n  2019", "journal-ref": null, "doi": "10.1109/TMI.2019.2919951", "report-no": null, "categories": "eess.IV cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Diagnosis and treatment guidance are aided by detecting relevant biomarkers\nin medical images. Although supervised deep learning can perform accurate\nsegmentation of pathological areas, it is limited by requiring a-priori\ndefinitions of these regions, large-scale annotations, and a representative\npatient cohort in the training set. In contrast, anomaly detection is not\nlimited to specific definitions of pathologies and allows for training on\nhealthy samples without annotation. Anomalous regions can then serve as\ncandidates for biomarker discovery. Knowledge about normal anatomical structure\nbrings implicit information for detecting anomalies. We propose to take\nadvantage of this property using bayesian deep learning, based on the\nassumption that epistemic uncertainties will correlate with anatomical\ndeviations from a normal training set. A Bayesian U-Net is trained on a\nwell-defined healthy environment using weak labels of healthy anatomy produced\nby existing methods. At test time, we capture epistemic uncertainty estimates\nof our model using Monte Carlo dropout. A novel post-processing technique is\nthen applied to exploit these estimates and transfer their layered appearance\nto smooth blob-shaped segmentations of the anomalies. We experimentally\nvalidated this approach in retinal optical coherence tomography (OCT) images,\nusing weak labels of retinal layers. Our method achieved a Dice index of 0.789\nin an independent anomaly test set of age-related macular degeneration (AMD)\ncases. The resulting segmentations allowed very high accuracy for separating\nhealthy and diseased cases with late wet AMD, dry geographic atrophy (GA),\ndiabetic macular edema (DME) and retinal vein occlusion (RVO). Finally, we\nqualitatively observed that our approach can also detect other deviations in\nnormal scans such as cut edge artifacts.\n", "versions": [{"version": "v1", "created": "Wed, 29 May 2019 08:39:42 GMT"}], "update_date": "2019-05-31", "authors_parsed": [["Seeb\u00f6ck", "Philipp", ""], ["Orlando", "Jos\u00e9 Ignacio", ""], ["Schlegl", "Thomas", ""], ["Waldstein", "Sebastian M.", ""], ["Bogunovi\u0107", "Hrvoje", ""], ["Klimscha", "Sophie", ""], ["Langs", "Georg", ""], ["Schmidt-Erfurth", "Ursula", ""]]}, {"id": "1905.12817", "submitter": "Anh Duc Le Dr.", "authors": "Anh Duc Le, Dung Van Pham, Tuan Anh Nguyen", "title": "Deep Learning Approach for Receipt Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inspired by the recent successes of deep learning on Computer Vision and\nNatural Language Processing, we present a deep learning approach for\nrecognizing scanned receipts. The recognition system has two main modules: text\ndetection based on Connectionist Text Proposal Network and text recognition\nbased on Attention-based Encoder-Decoder. We also proposed pre-processing to\nextract receipt area and OCR verification to ignore handwriting. The\nexperiments on the dataset of the Robust Reading Challenge on Scanned Receipts\nOCR and Information Extraction 2019 demonstrate that the accuracies were\nimproved by integrating the pre-processing and the OCR verification. Our\nrecognition system achieved 71.9% of the F1 score for detection and recognition\ntask.\n", "versions": [{"version": "v1", "created": "Thu, 30 May 2019 01:33:13 GMT"}], "update_date": "2019-05-31", "authors_parsed": [["Le", "Anh Duc", ""], ["Van Pham", "Dung", ""], ["Nguyen", "Tuan Anh", ""]]}, {"id": "1905.12828", "submitter": "Youssef Mroueh", "authors": "Youssef Mroueh", "title": "Wasserstein Style Transfer", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose Gaussian optimal transport for Image style transfer in an\nEncoder/Decoder framework. Optimal transport for Gaussian measures has closed\nforms Monge mappings from source to target distributions. Moreover interpolates\nbetween a content and a style image can be seen as geodesics in the Wasserstein\nGeometry. Using this insight, we show how to mix different target styles ,\nusing Wasserstein barycenter of Gaussian measures. Since Gaussians are closed\nunder Wasserstein barycenter, this allows us a simple style transfer and style\nmixing and interpolation. Moreover we show how mixing different styles can be\nachieved using other geodesic metrics between gaussians such as the Fisher Rao\nmetric, while the transport of the content to the new interpolate style is\nstill performed with Gaussian OT maps. Our simple methodology allows to\ngenerate new stylized content interpolating between many artistic styles. The\nmetric used in the interpolation results in different stylizations.\n", "versions": [{"version": "v1", "created": "Thu, 30 May 2019 02:09:28 GMT"}], "update_date": "2019-05-31", "authors_parsed": [["Mroueh", "Youssef", ""]]}, {"id": "1905.12830", "submitter": "Haijun Liu", "authors": "Haijun Liu, Jian Cheng, Shiguang Wang, Wen Wang", "title": "Attention: A Big Surprise for Cross-Domain Person Re-Identification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we focus on model generalization and adaptation for\ncross-domain person re-identification (Re-ID). Unlike existing cross-domain\nRe-ID methods, leveraging the auxiliary information of those unlabeled\ntarget-domain data, we aim at enhancing the model generalization and adaptation\nby discriminative feature learning, and directly exploiting a pre-trained model\nto new domains (datasets) without any utilization of the information from\ntarget domains. To address the discriminative feature learning problem, we\nsurprisingly find that simply introducing the attention mechanism to adaptively\nextract the person features for every domain is of great effectiveness. We\nadopt two popular type of attention mechanisms, long-range dependency based\nattention and direct generation based attention. Both of them can perform the\nattention via spatial or channel dimensions alone, even the combination of\nspatial and channel dimensions. The outline of different attentions are well\nillustrated. Moreover, we also incorporate the attention results into the final\noutput of model through skip-connection to improve the features with both high\nand middle level semantic visual information. In the manner of directly\nexploiting a pre-trained model to new domains, the attention incorporation\nmethod truly could enhance the model generalization and adaptation to perform\nthe cross-domain person Re-ID. We conduct extensive experiments between three\nlarge datasets, Market-1501, DukeMTMC-reID and MSMT17. Surprisingly,\nintroducing only attention can achieve state-of-the-art performance, even much\nbetter than those cross-domain Re-ID methods utilizing auxiliary information\nfrom the target domain.\n", "versions": [{"version": "v1", "created": "Thu, 30 May 2019 02:17:07 GMT"}], "update_date": "2019-05-31", "authors_parsed": [["Liu", "Haijun", ""], ["Cheng", "Jian", ""], ["Wang", "Shiguang", ""], ["Wang", "Wen", ""]]}, {"id": "1905.12837", "submitter": "Haijun Liu", "authors": "Haijun Liu, Jian Cheng, Wen Wang, Yanzhou Su", "title": "The General Pair-based Weighting Loss for Deep Metric Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep metric learning aims at learning the distance metric between pair of\nsamples, through the deep neural networks to extract the semantic feature\nembeddings where similar samples are close to each other while dissimilar\nsamples are farther apart. A large amount of loss functions based on pair\ndistances have been presented in the literature for guiding the training of\ndeep metric learning. In this paper, we unify them in a general pair-based\nweighting loss function, where the minimizing objective loss is just the\ndistances weighting of informative pairs. The general pair-based weighting loss\nincludes two main aspects, (1) samples mining and (2) pairs weighting. Samples\nmining aims at selecting the informative positive and negative pair sets to\nexploit the structured relationship of samples in a mini-batch and also reduce\nthe number of non-trivial pairs. Pair weighting aims at assigning different\nweights for different pairs according to the pair distances for\ndiscriminatively training the network. We detailedly review those existing\npair-based losses inline with our general loss function, and explore some\npossible methods from the perspective of samples mining and pairs weighting.\nFinally, extensive experiments on three image retrieval datasets show that our\ngeneral pair-based weighting loss obtains new state-of-the-art performance,\ndemonstrating the effectiveness of the pair-based samples mining and pairs\nweighting for deep metric learning.\n", "versions": [{"version": "v1", "created": "Thu, 30 May 2019 02:59:26 GMT"}], "update_date": "2019-05-31", "authors_parsed": [["Liu", "Haijun", ""], ["Cheng", "Jian", ""], ["Wang", "Wen", ""], ["Su", "Yanzhou", ""]]}, {"id": "1905.12844", "submitter": "Ning Wang", "authors": "Ning Wang, Xianhan Zeng, Renjie Xie, Zefei Gao, Yi Zheng, Ziran Liao,\n  Junyan Yang and Qiao Wang", "title": "Unsupervised Classification of Street Architectures Based on InfoGAN", "comments": "arXiv admin note: text overlap with arXiv:1804.08286,\n  arXiv:1606.03657 by other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Street architectures play an essential role in city image and streetscape\nanalysing. However, existing approaches are all supervised which require costly\nlabeled data. To solve this, we propose a street architectural unsupervised\nclassification framework based on Information maximizing Generative Adversarial\nNets (InfoGAN), in which we utilize the auxiliary distribution $Q$ of InfoGAN\nas an unsupervised classifier. Experiments on database of true street view\nimages in Nanjing, China validate the practicality and accuracy of our\nframework. Furthermore, we draw a series of heuristic conclusions from the\nintrinsic information hidden in true images. These conclusions will assist\nplanners to know the architectural categories better.\n", "versions": [{"version": "v1", "created": "Thu, 30 May 2019 03:42:19 GMT"}], "update_date": "2019-06-02", "authors_parsed": [["Wang", "Ning", ""], ["Zeng", "Xianhan", ""], ["Xie", "Renjie", ""], ["Gao", "Zefei", ""], ["Zheng", "Yi", ""], ["Liao", "Ziran", ""], ["Yang", "Junyan", ""], ["Wang", "Qiao", ""]]}, {"id": "1905.12845", "submitter": "Jiechao Ma", "authors": "Xiang Li, Chan Lu, Danni Cheng, Wei-Hong Li, Mei Cao, Bo Liu, Jiechao\n  Ma, Wei-Shi Zheng", "title": "Towards Photo-Realistic Visible Watermark Removal with Conditional\n  Generative Adversarial Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visible watermark plays an important role in image copyright protection and\nthe robustness of a visible watermark to an attack is shown to be essential. To\nevaluate and improve the effectiveness of watermark, watermark removal attracts\nincreasing attention and becomes a hot research top. Current methods cast the\nwatermark removal as an image-to-image translation problem where the\nencode-decode architectures with pixel-wise loss are adopted to transfer the\ntransparent watermarked pixels into unmarked pixels. However, when a number of\nrealistic images are presented, the watermarks are more likely to be unknown\nand diverse (i.e., the watermarks might be opaque or semi-transparent; the\ncategory and pattern of watermarks are unknown). When applying existing methods\nto the real-world scenarios, they mostly can not satisfactorily reconstruct the\nhidden information obscured under the complex and various watermarks (i.e., the\nresidual watermark traces remain and the reconstructed images lack reality). To\naddress this difficulty, in this paper, we present a new watermark processing\nframework using the conditional generative adversarial networks (cGANs) for\nvisible watermark removal in the real-world application. The proposed method\nenables the watermark removal solution to be more closed to the photo-realistic\nreconstruction using a patch-based discriminator conditioned on the watermarked\nimages, which is adversarially trained to differentiate the difference between\nthe recovered images and original watermark-free images. Extensive experimental\nresults on a large-scale visible watermark dataset demonstrate the\neffectiveness of the proposed method and clearly indicate that our proposed\napproach can produce more photo-realistic and convincing results compared with\nthe state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Thu, 30 May 2019 04:08:49 GMT"}, {"version": "v2", "created": "Fri, 31 May 2019 02:50:18 GMT"}, {"version": "v3", "created": "Mon, 19 Aug 2019 05:40:10 GMT"}], "update_date": "2019-08-20", "authors_parsed": [["Li", "Xiang", ""], ["Lu", "Chan", ""], ["Cheng", "Danni", ""], ["Li", "Wei-Hong", ""], ["Cao", "Mei", ""], ["Liu", "Bo", ""], ["Ma", "Jiechao", ""], ["Zheng", "Wei-Shi", ""]]}, {"id": "1905.12853", "submitter": "Sachini Herath", "authors": "Hang Yan, Sachini Herath, Yasutaka Furukawa", "title": "RoNIN: Robust Neural Inertial Navigation in the Wild: Benchmark,\n  Evaluations, and New Methods", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper sets a new foundation for data-driven inertial navigation\nresearch, where the task is the estimation of positions and orientations of a\nmoving subject from a sequence of IMU sensor measurements. More concretely, the\npaper presents 1) a new benchmark containing more than 40 hours of IMU sensor\ndata from 100 human subjects with ground-truth 3D trajectories under natural\nhuman motions; 2) novel neural inertial navigation architectures, making\nsignificant improvements for challenging motion cases; and 3) qualitative and\nquantitative evaluations of the competing methods over three inertial\nnavigation benchmarks. We will share the code and data to promote further\nresearch.\n", "versions": [{"version": "v1", "created": "Thu, 30 May 2019 05:06:57 GMT"}], "update_date": "2019-05-31", "authors_parsed": [["Yan", "Hang", ""], ["Herath", "Sachini", ""], ["Furukawa", "Yasutaka", ""]]}, {"id": "1905.12863", "submitter": "Ye Guo", "authors": "Ye Guo, Yali Li, Shengjin Wang", "title": "CS-R-FCN: Cross-supervised Learning for Large-Scale Object Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generic object detection is one of the most fundamental problems in computer\nvision, yet it is difficult to provide all the bounding-box-level annotations\naiming at large-scale object detection for thousands of categories. In this\npaper, we present a novel cross-supervised learning pipeline for large-scale\nobject detection, denoted as CS-R-FCN. First, we propose to utilize the data\nflow of image-level annotated images in the fully-supervised two-stage object\ndetection framework, leading to cross-supervised learning combining\nbounding-box-level annotated data and image-level annotated data. Second, we\nintroduce a semantic aggregation strategy utilizing the relationships among the\ncross-supervised categories to reduce the unreasonable mutual inhibition\neffects during the feature learning. Experimental results show that the\nproposed CS-R-FCN improves the mAP by a large margin compared to previous\nrelated works.\n", "versions": [{"version": "v1", "created": "Thu, 30 May 2019 05:54:48 GMT"}, {"version": "v2", "created": "Wed, 15 Jan 2020 05:10:05 GMT"}], "update_date": "2020-01-16", "authors_parsed": [["Guo", "Ye", ""], ["Li", "Yali", ""], ["Wang", "Shengjin", ""]]}, {"id": "1905.12871", "submitter": "Hideaki Hayashi D.Eng.", "authors": "Hideaki Hayashi and Seiichi Uchida", "title": "A Trainable Multiplication Layer for Auto-correlation and Co-occurrence\n  Extraction", "comments": null, "journal-ref": "In Proceedings of the 14th Asian Conference on Computer Vision\n  (ACCV 2018)", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a trainable multiplication layer (TML) for a neural\nnetwork that can be used to calculate the multiplication between the input\nfeatures. Taking an image as an input, the TML raises each pixel value to the\npower of a weight and then multiplies them, thereby extracting the higher-order\nlocal auto-correlation from the input image. The TML can also be used to\nextract co-occurrence from the feature map of a convolutional network. The\ntraining of the TML is formulated based on backpropagation with constraints to\nthe weights, enabling us to learn discriminative multiplication patterns in an\nend-to-end manner. In the experiments, the characteristics of the TML are\ninvestigated by visualizing learned kernels and the corresponding output\nfeatures. The applicability of the TML for classification and neural network\ninterpretation is also evaluated using public datasets.\n", "versions": [{"version": "v1", "created": "Thu, 30 May 2019 06:21:54 GMT"}], "update_date": "2019-05-31", "authors_parsed": [["Hayashi", "Hideaki", ""], ["Uchida", "Seiichi", ""]]}, {"id": "1905.12883", "submitter": "Shiwan Zhao Mr", "authors": "Bingzhe Wu, Shiwan Zhao, Guangyu Sun, Xiaolu Zhang, Zhong Su, Caihong\n  Zeng, Zhihong Liu", "title": "P3SGD: Patient Privacy Preserving SGD for Regularizing Deep CNNs in\n  Pathological Image Classification", "comments": "Accepted by CVPR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, deep convolutional neural networks (CNNs) have achieved great\nsuccess in pathological image classification. However, due to the limited\nnumber of labeled pathological images, there are still two challenges to be\naddressed: (1) overfitting: the performance of a CNN model is undermined by the\noverfitting due to its huge amounts of parameters and the insufficiency of\nlabeled training data. (2) privacy leakage: the model trained using a\nconventional method may involuntarily reveal the private information of the\npatients in the training dataset. The smaller the dataset, the worse the\nprivacy leakage. To tackle the above two challenges, we introduce a novel\nstochastic gradient descent (SGD) scheme, named patient privacy preserving SGD\n(P3SGD), which performs the model update of the SGD in the patient level via a\nlarge-step update built upon each patient's data. Specifically, to protect\nprivacy and regularize the CNN model, we propose to inject the well-designed\nnoise into the updates. Moreover, we equip our P3SGD with an elaborated\nstrategy to adaptively control the scale of the injected noise. To validate the\neffectiveness of P3SGD, we perform extensive experiments on a real-world\nclinical dataset and quantitatively demonstrate the superior ability of P3SGD\nin reducing the risk of overfitting. We also provide a rigorous analysis of the\nprivacy cost under differential privacy. Additionally, we find that the models\ntrained with P3SGD are resistant to the model-inversion attack compared with\nthose trained using non-private SGD.\n", "versions": [{"version": "v1", "created": "Thu, 30 May 2019 07:07:29 GMT"}], "update_date": "2019-05-31", "authors_parsed": [["Wu", "Bingzhe", ""], ["Zhao", "Shiwan", ""], ["Sun", "Guangyu", ""], ["Zhang", "Xiaolu", ""], ["Su", "Zhong", ""], ["Zeng", "Caihong", ""], ["Liu", "Zhihong", ""]]}, {"id": "1905.12886", "submitter": "Salman Khan Dr.", "authors": "Syed Waqas Zamir, Aditya Arora, Akshita Gupta, Salman Khan, Guolei\n  Sun, Fahad Shahbaz Khan, Fan Zhu, Ling Shao, Gui-Song Xia, Xiang Bai", "title": "iSAID: A Large-scale Dataset for Instance Segmentation in Aerial Images", "comments": "CVPR'19 Workshops (Detecting Objects in Aerial Images). The dataset\n  is publicly available at: https://captain-whu.github.io/iSAID/index.html", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing Earth Vision datasets are either suitable for semantic segmentation\nor object detection. In this work, we introduce the first benchmark dataset for\ninstance segmentation in aerial imagery that combines instance-level object\ndetection and pixel-level segmentation tasks. In comparison to instance\nsegmentation in natural scenes, aerial images present unique challenges e.g., a\nhuge number of instances per image, large object-scale variations and abundant\ntiny objects. Our large-scale and densely annotated Instance Segmentation in\nAerial Images Dataset (iSAID) comes with 655,451 object instances for 15\ncategories across 2,806 high-resolution images. Such precise per-pixel\nannotations for each instance ensure accurate localization that is essential\nfor detailed scene analysis. Compared to existing small-scale aerial image\nbased instance segmentation datasets, iSAID contains 15$\\times$ the number of\nobject categories and 5$\\times$ the number of instances. We benchmark our\ndataset using two popular instance segmentation approaches for natural images,\nnamely Mask R-CNN and PANet. In our experiments we show that direct application\nof off-the-shelf Mask R-CNN and PANet on aerial images provide suboptimal\ninstance segmentation results, thus requiring specialized solutions from the\nresearch community. The dataset is publicly available at:\nhttps://captain-whu.github.io/iSAID/index.html\n", "versions": [{"version": "v1", "created": "Thu, 30 May 2019 07:18:28 GMT"}, {"version": "v2", "created": "Wed, 28 Aug 2019 05:57:00 GMT"}], "update_date": "2019-08-29", "authors_parsed": [["Zamir", "Syed Waqas", ""], ["Arora", "Aditya", ""], ["Gupta", "Akshita", ""], ["Khan", "Salman", ""], ["Sun", "Guolei", ""], ["Khan", "Fahad Shahbaz", ""], ["Zhu", "Fan", ""], ["Shao", "Ling", ""], ["Xia", "Gui-Song", ""], ["Bai", "Xiang", ""]]}, {"id": "1905.12887", "submitter": "Brady Zhou", "authors": "Brady Zhou, Philipp Kr\\\"ahenb\\\"uhl, and Vladlen Koltun", "title": "Does computer vision matter for action?", "comments": "Published in Science Robotics, 4(30), May 2019", "journal-ref": "Science Robotics 22 May 2019: Vol. 4, Issue 30, eaaw6661", "doi": "10.1126/scirobotics.aaw6661", "report-no": null, "categories": "cs.CV cs.AI cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computer vision produces representations of scene content. Much computer\nvision research is predicated on the assumption that these intermediate\nrepresentations are useful for action. Recent work at the intersection of\nmachine learning and robotics calls this assumption into question by training\nsensorimotor systems directly for the task at hand, from pixels to actions,\nwith no explicit intermediate representations. Thus the central question of our\nwork: Does computer vision matter for action? We probe this question and its\noffshoots via immersive simulation, which allows us to conduct controlled\nreproducible experiments at scale. We instrument immersive three-dimensional\nenvironments to simulate challenges such as urban driving, off-road trail\ntraversal, and battle. Our main finding is that computer vision does matter.\nModels equipped with intermediate representations train faster, achieve higher\ntask performance, and generalize better to previously unseen environments. A\nvideo that summarizes the work and illustrates the results can be found at\nhttps://youtu.be/4MfWa2yZ0Jc\n", "versions": [{"version": "v1", "created": "Thu, 30 May 2019 07:18:33 GMT"}, {"version": "v2", "created": "Tue, 22 Oct 2019 06:33:45 GMT"}], "update_date": "2019-10-23", "authors_parsed": [["Zhou", "Brady", ""], ["Kr\u00e4henb\u00fchl", "Philipp", ""], ["Koltun", "Vladlen", ""]]}, {"id": "1905.12898", "submitter": "Anpei Chen", "authors": "Ziheng Zhang, Anpei Chen, Ling Xie, Jingyi Yu, Shenghua Gao", "title": "Learning Semantics-aware Distance Map with Semantics Layering Network\n  for Amodal Instance Segmentation", "comments": "This paper is submitted to ACMMM19", "journal-ref": null, "doi": "10.1145/3343031.3350911", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we demonstrate yet another approach to tackle the amodal\nsegmentation problem. Specifically, we first introduce a new representation,\nnamely a semantics-aware distance map (sem-dist map), to serve as our target\nfor amodal segmentation instead of the commonly used masks and heatmaps. The\nsem-dist map is a kind of level-set representation, of which the different\nregions of an object are placed into different levels on the map according to\ntheir visibility. It is a natural extension of masks and heatmaps, where modal,\namodal segmentation, as well as depth order information, are all\nwell-described. Then we also introduce a novel convolutional neural network\n(CNN) architecture, which we refer to as semantic layering network, to estimate\nsem-dist maps layer by layer, from the global-level to the instance-level, for\nall objects in an image. Extensive experiments on the COCOA and D2SA datasets\nhave demonstrated that our framework can predict amodal segmentation, occlusion\nand depth order with state-of-the-art performance.\n", "versions": [{"version": "v1", "created": "Thu, 30 May 2019 07:46:54 GMT"}, {"version": "v2", "created": "Thu, 22 Aug 2019 17:34:05 GMT"}], "update_date": "2019-08-23", "authors_parsed": [["Zhang", "Ziheng", ""], ["Chen", "Anpei", ""], ["Xie", "Ling", ""], ["Yu", "Jingyi", ""], ["Gao", "Shenghua", ""]]}, {"id": "1905.12980", "submitter": "\\'Alvaro Peris", "authors": "\\'Alvaro Peris and Francisco Casacuberta", "title": "Interactive-predictive neural multimodal systems", "comments": "To appear at IbPRIA 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Despite the advances achieved by neural models in sequence to sequence\nlearning, exploited in a variety of tasks, they still make errors. In many use\ncases, these are corrected by a human expert in a posterior revision process.\nThe interactive-predictive framework aims to minimize the human effort spent on\nthis process by considering partial corrections for iteratively refining the\nhypothesis. In this work, we generalize the interactive-predictive approach,\ntypically applied in to machine translation field, to tackle other multimodal\nproblems namely, image and video captioning. We study the application of this\nframework to multimodal neural sequence to sequence models. We show that,\nfollowing this framework, we approximately halve the effort spent for\ncorrecting the outputs generated by the automatic systems. Moreover, we deploy\nour systems in a publicly accessible demonstration, that allows to better\nunderstand the behavior of the interactive-predictive framework.\n", "versions": [{"version": "v1", "created": "Thu, 30 May 2019 11:47:04 GMT"}], "update_date": "2019-05-31", "authors_parsed": [["Peris", "\u00c1lvaro", ""], ["Casacuberta", "Francisco", ""]]}, {"id": "1905.12988", "submitter": "Aji Resindra Widya", "authors": "Aji Resindra Widya, Yusuke Monno, Kosuke Imahori, Masatoshi Okutomi,\n  Sho Suzuki, Takuji Gotoda, Kenji Miki", "title": "3D Reconstruction of Whole Stomach from Endoscope Video Using\n  Structure-from-Motion", "comments": "5 pages, 4 figures, accepted in EMBC 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gastric endoscopy is a common clinical practice that enables medical doctors\nto diagnose the stomach inside a body. In order to identify a gastric lesion's\nlocation such as early gastric cancer within the stomach, this work addressed\nto reconstruct the 3D shape of a whole stomach with color texture information\ngenerated from a standard monocular endoscope video. Previous works have tried\nto reconstruct the 3D structures of various organs from endoscope images.\nHowever, they are mainly focused on a partial surface. In this work, we\ninvestigated how to enable structure-from-motion (SfM) to reconstruct the whole\nshape of a stomach from a standard endoscope video. We specifically\ninvestigated the combined effect of chromo-endoscopy and color channel\nselection on SfM. Our study found that 3D reconstruction of the whole stomach\ncan be achieved by using red channel images captured under chromo-endoscopy by\nspreading indigo carmine (IC) dye on the stomach surface.\n", "versions": [{"version": "v1", "created": "Thu, 30 May 2019 12:05:20 GMT"}], "update_date": "2019-05-31", "authors_parsed": [["Widya", "Aji Resindra", ""], ["Monno", "Yusuke", ""], ["Imahori", "Kosuke", ""], ["Okutomi", "Masatoshi", ""], ["Suzuki", "Sho", ""], ["Gotoda", "Takuji", ""], ["Miki", "Kenji", ""]]}, {"id": "1905.12995", "submitter": "Nicolas Gillis", "authors": "Junjun Pan, Nicolas Gillis", "title": "Generalized Separable Nonnegative Matrix Factorization", "comments": "31 pages, 12 figures, 4 tables. We have added discussions about the\n  identifiability of the model, we have modified the first synthetic\n  experiment, we have clarified some aspects of the contribution", "journal-ref": "IEEE Trans. on Pattern Analysis and Machine Intelligence 43 (5),\n  pp. 1546-1561, 2021", "doi": "10.1109/TPAMI.2019.2956046", "report-no": null, "categories": "cs.LG cs.CV math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nonnegative matrix factorization (NMF) is a linear dimensionality technique\nfor nonnegative data with applications such as image analysis, text mining,\naudio source separation and hyperspectral unmixing. Given a data matrix $M$ and\na factorization rank $r$, NMF looks for a nonnegative matrix $W$ with $r$\ncolumns and a nonnegative matrix $H$ with $r$ rows such that $M \\approx WH$.\nNMF is NP-hard to solve in general. However, it can be computed efficiently\nunder the separability assumption which requires that the basis vectors appear\nas data points, that is, that there exists an index set $\\mathcal{K}$ such that\n$W = M(:,\\mathcal{K})$. In this paper, we generalize the separability\nassumption: We only require that for each rank-one factor $W(:,k)H(k,:)$ for\n$k=1,2,\\dots,r$, either $W(:,k) = M(:,j)$ for some $j$ or $H(k,:) = M(i,:)$ for\nsome $i$. We refer to the corresponding problem as generalized separable NMF\n(GS-NMF). We discuss some properties of GS-NMF and propose a convex\noptimization model which we solve using a fast gradient method. We also propose\na heuristic algorithm inspired by the successive projection algorithm. To\nverify the effectiveness of our methods, we compare them with several\nstate-of-the-art separable NMF algorithms on synthetic, document and image data\nsets.\n", "versions": [{"version": "v1", "created": "Thu, 30 May 2019 12:18:25 GMT"}, {"version": "v2", "created": "Tue, 15 Oct 2019 11:22:33 GMT"}], "update_date": "2021-04-14", "authors_parsed": [["Pan", "Junjun", ""], ["Gillis", "Nicolas", ""]]}, {"id": "1905.13038", "submitter": "Chungkwong Chan", "authors": "Chungkwong Chan", "title": "Memory-efficient and fast implementation of local adaptive binarization\n  methods", "comments": "8 pages, 4 figures, corrected typos and added reference to source\n  code", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Binarization is widely used as an image preprocessing step to separate object\nespecially text from background before recognition. For noisy images with\nuneven illumination such as degraded documents, threshold values need to be\ncomputed pixel by pixel to obtain a good segmentation. Since local threshold\nvalues typically depend on moment-based statistics such as mean and variance of\ngray levels inside rectangular windows, integral images which are memory\nconsuming are commonly used to accelerate the calculation. Observed that\nmoment-based statistics as well as quantiles in a sliding window can be\ncomputed recursively, integral images can be avoided without neglecting speed,\nmore binarization methods can be accelerated too. In particular, given a\n$H\\times W$ input image, Sauvola's method and alike can run in $\\Theta (HW)$\ntime independent of window size, while only around $6\\min\\{H,W\\}$ bytes of\nauxiliary space is needed, which is significantly lower than the $16HW$ bytes\noccupied by the two integral images. Since the proposed technique enable\nvarious well-known local adaptive binarization methods to be applied in\nreal-time use cases on devices with limited resources, it has the potential of\nwide application.\n", "versions": [{"version": "v1", "created": "Thu, 30 May 2019 13:17:18 GMT"}, {"version": "v2", "created": "Mon, 1 Jul 2019 12:34:32 GMT"}, {"version": "v3", "created": "Wed, 31 Jul 2019 14:01:21 GMT"}], "update_date": "2019-08-01", "authors_parsed": [["Chan", "Chungkwong", ""]]}, {"id": "1905.13040", "submitter": "Thanh-Dat Truong", "authors": "Thanh-Dat Truong, Chi Nhan Duong, Khoa Luu, Minh-Triet Tran and Ngan\n  Le", "title": "Domain Generalization via Universal Non-volume Preserving Models", "comments": "Accepted to Computer and Robot Vision 2020. arXiv admin note:\n  substantial text overlap with arXiv:1812.03407", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recognition across domains has recently become an active topic in the\nresearch community. However, it has been largely overlooked in the problem of\nrecognition in new unseen domains. Under this condition, the delivered deep\nnetwork models are unable to be updated, adapted, or fine-tuned. Therefore,\nrecent deep learning techniques, such as domain adaptation, feature\ntransferring, and fine-tuning, cannot be applied. This paper presents a novel\napproach to the problem of domain generalization in the context of deep\nlearning. The proposed method is evaluated on different datasets in various\nproblems, i.e. (i) digit recognition on MNIST, SVHN, and MNIST-M, (ii) face\nrecognition on Extended Yale-B, CMU-PIE and CMU-MPIE, and (iii) pedestrian\nrecognition on RGB and Thermal image datasets. The experimental results show\nthat our proposed method consistently improves performance accuracy. It can\nalso be easily incorporated with any other CNN frameworks within an end-to-end\ndeep network design for object detection and recognition problems to improve\ntheir performance.\n", "versions": [{"version": "v1", "created": "Tue, 28 May 2019 18:31:25 GMT"}, {"version": "v2", "created": "Fri, 10 Apr 2020 18:39:22 GMT"}], "update_date": "2020-04-15", "authors_parsed": [["Truong", "Thanh-Dat", ""], ["Duong", "Chi Nhan", ""], ["Luu", "Khoa", ""], ["Tran", "Minh-Triet", ""], ["Le", "Ngan", ""]]}, {"id": "1905.13066", "submitter": "Sanghyun Woo", "authors": "Sanghyun Woo, Dahun Kim, KwanYong Park, Joon-Young Lee, In So Kweon", "title": "Align-and-Attend Network for Globally and Locally Coherent Video\n  Inpainting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel feed-forward network for video inpainting. We use a set of\nsampled video frames as the reference to take visible contents to fill the hole\nof a target frame. Our video inpainting network consists of two stages. The\nfirst stage is an alignment module that uses computed homographies between the\nreference frames and the target frame. The visible patches are then aggregated\nbased on the frame similarity to fill in the target holes roughly. The second\nstage is a non-local attention module that matches the generated patches with\nknown reference patches (in space and time) to refine the previous global\nalignment stage. Both stages consist of large spatial-temporal window size for\nthe reference and thus enable modeling long-range correlations between distant\ninformation and the hole regions. Therefore, even challenging scenes with large\nor slowly moving holes can be handled, which have been hardly modeled by\nexisting flow-based approach. Our network is also designed with a recurrent\npropagation stream to encourage temporal consistency in video results.\nExperiments on video object removal demonstrate that our method inpaints the\nholes with globally and locally coherent contents.\n", "versions": [{"version": "v1", "created": "Thu, 30 May 2019 14:14:08 GMT"}], "update_date": "2019-05-31", "authors_parsed": [["Woo", "Sanghyun", ""], ["Kim", "Dahun", ""], ["Park", "KwanYong", ""], ["Lee", "Joon-Young", ""], ["Kweon", "In So", ""]]}, {"id": "1905.13074", "submitter": "Adnan Siraj Rakin", "authors": "Adnan Siraj Rakin, Zhezhi He, Li Yang, Yanzhi Wang, Liqiang Wang, and\n  Deliang Fan", "title": "Robust Sparse Regularization: Simultaneously Optimizing Neural Network\n  Robustness and Compactness", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Neural Network (DNN) trained by the gradient descent method is known to\nbe vulnerable to maliciously perturbed adversarial input, aka. adversarial\nattack. As one of the countermeasures against adversarial attack, increasing\nthe model capacity for DNN robustness enhancement was discussed and reported as\nan effective approach by many recent works. In this work, we show that\nshrinking the model size through proper weight pruning can even be helpful to\nimprove the DNN robustness under adversarial attack. For obtaining a\nsimultaneously robust and compact DNN model, we propose a multi-objective\ntraining method called Robust Sparse Regularization (RSR), through the fusion\nof various regularization techniques, including channel-wise noise injection,\nlasso weight penalty, and adversarial training. We conduct extensive\nexperiments across popular ResNet-20, ResNet-18 and VGG-16 DNN architectures to\ndemonstrate the effectiveness of RSR against popular white-box (i.e., PGD and\nFGSM) and black-box attacks. Thanks to RSR, 85% weight connections of ResNet-18\ncan be pruned while still achieving 0.68% and 8.72% improvement in clean- and\nperturbed-data accuracy respectively on CIFAR-10 dataset, in comparison to its\nPGD adversarial training baseline.\n", "versions": [{"version": "v1", "created": "Thu, 30 May 2019 14:32:21 GMT"}], "update_date": "2019-05-31", "authors_parsed": [["Rakin", "Adnan Siraj", ""], ["He", "Zhezhi", ""], ["Yang", "Li", ""], ["Wang", "Yanzhi", ""], ["Wang", "Liqiang", ""], ["Fan", "Deliang", ""]]}, {"id": "1905.13077", "submitter": "Simon Kohl", "authors": "Simon A. A. Kohl, Bernardino Romera-Paredes, Klaus H. Maier-Hein,\n  Danilo Jimenez Rezende, S. M. Ali Eslami, Pushmeet Kohli, Andrew Zisserman,\n  Olaf Ronneberger", "title": "A Hierarchical Probabilistic U-Net for Modeling Multi-Scale Ambiguities", "comments": "25 pages, 15 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Medical imaging only indirectly measures the molecular identity of the tissue\nwithin each voxel, which often produces only ambiguous image evidence for\ntarget measures of interest, like semantic segmentation. This diversity and the\nvariations of plausible interpretations are often specific to given image\nregions and may thus manifest on various scales, spanning all the way from the\npixel to the image level. In order to learn a flexible distribution that can\naccount for multiple scales of variations, we propose the Hierarchical\nProbabilistic U-Net, a segmentation network with a conditional variational\nauto-encoder (cVAE) that uses a hierarchical latent space decomposition. We\nshow that this model formulation enables sampling and reconstruction of\nsegmenations with high fidelity, i.e. with finely resolved detail, while\nproviding the flexibility to learn complex structured distributions across\nscales. We demonstrate these abilities on the task of segmenting ambiguous\nmedical scans as well as on instance segmentation of neurobiological and\nnatural images. Our model automatically separates independent factors across\nscales, an inductive bias that we deem beneficial in structured output\nprediction tasks beyond segmentation.\n", "versions": [{"version": "v1", "created": "Thu, 30 May 2019 14:49:08 GMT"}], "update_date": "2019-05-31", "authors_parsed": [["Kohl", "Simon A. A.", ""], ["Romera-Paredes", "Bernardino", ""], ["Maier-Hein", "Klaus H.", ""], ["Rezende", "Danilo Jimenez", ""], ["Eslami", "S. M. Ali", ""], ["Kohli", "Pushmeet", ""], ["Zisserman", "Andrew", ""], ["Ronneberger", "Olaf", ""]]}, {"id": "1905.13124", "submitter": "Ziyuan Zhao", "authors": "Xiaoman Zhang, Ziyuan Zhao, Cen Chen, Songyou Peng, Min Wu, Zhongyao\n  Cheng, Singee Teo, Le Zhang, Zeng Zeng", "title": "A Deep Framework for Bone Age Assessment based on Finger Joint\n  Localization", "comments": "Some changes will be made", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bone age assessment is an important clinical trial to measure skeletal child\nmaturity and diagnose of growth disorders. Conventional approaches such as the\nTanner-Whitehouse (TW) and Greulich and Pyle (GP) may not perform well due to\ntheir large inter-observer and intra-observer variations. In this paper, we\npropose a finger joint localization strategy to filter out most non-informative\nparts of images. When combining with the conventional full image-based deep\nnetwork, we observe a much-improved performance. % Our approach utilizes full\nhand and specific joints images for skeletal maturity prediction. In this\nstudy, we applied powerful deep neural network and explored a process in the\nforecast of skeletal bone age with the specifically combine joints images to\nincrease the performance accuracy compared with the whole hand images.\n", "versions": [{"version": "v1", "created": "Tue, 7 May 2019 08:38:28 GMT"}, {"version": "v2", "created": "Thu, 13 Jun 2019 13:55:41 GMT"}], "update_date": "2019-06-14", "authors_parsed": [["Zhang", "Xiaoman", ""], ["Zhao", "Ziyuan", ""], ["Chen", "Cen", ""], ["Peng", "Songyou", ""], ["Wu", "Min", ""], ["Cheng", "Zhongyao", ""], ["Teo", "Singee", ""], ["Zhang", "Le", ""], ["Zeng", "Zeng", ""]]}, {"id": "1905.13143", "submitter": "Xin Jin", "authors": "Xin Jin, Cuiling Lan, Wenjun Zeng, Guoqiang Wei, Zhibo Chen", "title": "Semantics-Aligned Representation Learning for Person Re-identification", "comments": "Thirty-Fourth AAAI Conference on Artificial Intelligence (AAAI-20),\n  code has been released", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Person re-identification (reID) aims to match person images to retrieve the\nones with the same identity. This is a challenging task, as the images to be\nmatched are generally semantically misaligned due to the diversity of human\nposes and capture viewpoints, incompleteness of the visible bodies (due to\nocclusion), etc. In this paper, we propose a framework that drives the reID\nnetwork to learn semantics-aligned feature representation through delicate\nsupervision designs. Specifically, we build a Semantics Aligning Network (SAN)\nwhich consists of a base network as encoder (SA-Enc) for re-ID, and a decoder\n(SA-Dec) for reconstructing/regressing the densely semantics aligned full\ntexture image. We jointly train the SAN under the supervisions of person\nre-identification and aligned texture generation. Moreover, at the decoder,\nbesides the reconstruction loss, we add Triplet ReID constraints over the\nfeature maps as the perceptual losses. The decoder is discarded in the\ninference and thus our scheme is computationally efficient. Ablation studies\ndemonstrate the effectiveness of our design. We achieve the state-of-the-art\nperformances on the benchmark datasets CUHK03, Market1501, MSMT17, and the\npartial person reID dataset Partial REID. Code for our proposed method is\navailable at:\nhttps://github.com/microsoft/Semantics-Aligned-Representation-Learning-for-Person-Re-identification.\n", "versions": [{"version": "v1", "created": "Thu, 30 May 2019 16:09:28 GMT"}, {"version": "v2", "created": "Wed, 4 Dec 2019 02:47:13 GMT"}, {"version": "v3", "created": "Wed, 18 Mar 2020 13:02:27 GMT"}], "update_date": "2020-03-19", "authors_parsed": [["Jin", "Xin", ""], ["Lan", "Cuiling", ""], ["Zeng", "Wenjun", ""], ["Wei", "Guoqiang", ""], ["Chen", "Zhibo", ""]]}, {"id": "1905.13145", "submitter": "Farzad Khalvati", "authors": "Sunghwan Yoo, Isha Gujrathi, Masoom A. Haider, Farzad Khalvati", "title": "Prostate Cancer Detection using Deep Convolutional Neural Networks", "comments": null, "journal-ref": "Scientific Reports 9, 19518 (2019)", "doi": "10.1038/s41598-019-55972-4", "report-no": null, "categories": "cs.CV eess.IV q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Prostate cancer is one of the most common forms of cancer and the third\nleading cause of cancer death in North America. As an integrated part of\ncomputer-aided detection (CAD) tools, diffusion-weighted magnetic resonance\nimaging (DWI) has been intensively studied for accurate detection of prostate\ncancer. With deep convolutional neural networks (CNNs) significant success in\ncomputer vision tasks such as object detection and segmentation, different CNNs\narchitectures are increasingly investigated in medical imaging research\ncommunity as promising solutions for designing more accurate CAD tools for\ncancer detection. In this work, we developed and implemented an automated\nCNNs-based pipeline for detection of clinically significant prostate cancer\n(PCa) for a given axial DWI image and for each patient. DWI images of 427\npatients were used as the dataset, which contained 175 patients with PCa and\n252 healthy patients. To measure the performance of the proposed pipeline, a\ntest set of 108 (out of 427) patients were set aside and not used in the\ntraining phase. The proposed pipeline achieved area under the receiver\noperating characteristic curve (AUC) of 0.87 (95% Confidence Interval (CI):\n0.84-0.90) and 0.84 (95% CI: 0.76-0.91) at slice level and patient level,\nrespectively.\n", "versions": [{"version": "v1", "created": "Thu, 30 May 2019 16:10:39 GMT"}], "update_date": "2020-06-11", "authors_parsed": [["Yoo", "Sunghwan", ""], ["Gujrathi", "Isha", ""], ["Haider", "Masoom A.", ""], ["Khalvati", "Farzad", ""]]}, {"id": "1905.13146", "submitter": "Rakshit Kothari", "authors": "Rakshit Kothari, Zhizhuo Yang, Christopher Kanan, Reynold Bailey, Jeff\n  Pelz, Gabriel Diaz", "title": "Gaze-in-wild: A dataset for studying eye and head coordination in\n  everyday activities", "comments": "23 pages, 11 figures, 10 tables, Dataset can be found at\n  http://www.cis.rit.edu/~rsk3900/", "journal-ref": null, "doi": "10.1038/s41598-020-59251-5", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The interaction between the vestibular and ocular system has primarily been\nstudied in controlled environments. Consequently, off-the shelf tools for\ncategorization of gaze events (e.g. fixations, pursuits, saccade) fail when\nhead movements are allowed. Our approach was to collect a novel, naturalistic,\nand multimodal dataset of eye+head movements when subjects performed everyday\ntasks while wearing a mobile eye tracker equipped with an inertial measurement\nunit and a 3D stereo camera. This Gaze-in-the-Wild dataset (GW) includes\neye+head rotational velocities (deg/s), infrared eye images and scene imagery\n(RGB+D). A portion was labelled by coders into gaze motion events with a mutual\nagreement of 0.72 sample based Cohen's $\\kappa$. This labelled data was used to\ntrain and evaluate two machine learning algorithms, Random Forest and a\nRecurrent Neural Network model, for gaze event classification. Assessment\ninvolved the application of established and novel event based performance\nmetrics. Classifiers achieve $\\sim$90$\\%$ human performance in detecting\nfixations and saccades but fall short (60$\\%$) on detecting pursuit movements.\nMoreover, pursuit classification is far worse in the absence of head movement\ninformation. A subsequent analysis of feature significance in our\nbest-performing model revealed a reliance upon absolute eye and head velocity,\nindicating that classification does not require spatial alignment of the head\nand eye tracking coordinate systems. The GW dataset, trained classifiers and\nevaluation metrics will be made publicly available with the intention of\nfacilitating growth in the emerging area of head-free gaze event\nclassification.\n", "versions": [{"version": "v1", "created": "Thu, 9 May 2019 16:33:03 GMT"}], "update_date": "2020-06-11", "authors_parsed": [["Kothari", "Rakshit", ""], ["Yang", "Zhizhuo", ""], ["Kanan", "Christopher", ""], ["Bailey", "Reynold", ""], ["Pelz", "Jeff", ""], ["Diaz", "Gabriel", ""]]}, {"id": "1905.13147", "submitter": "Manpreet Minhas", "authors": "Manpreet Singh Minhas, John Zelek", "title": "Anomaly Detection in Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual defect assessment is a form of anomaly detection. This is very\nrelevant in finding faults such as cracks and markings in various surface\ninspection tasks like pavement and automotive parts. The task involves\ndetection of deviation/divergence of anomalous samples from the normal ones.\nTwo of the major challenges in supervised anomaly detection are the lack of\nlabelled training data and the low availability of anomaly instances.\nSemi-supervised methods which learn the underlying distribution of the normal\nsamples and then measure the deviation/divergence from the estimated model as\nthe anomaly score have limitations in their overall ability to detect\nanomalies. This paper proposes the application of network-based deep transfer\nlearning using convolutional neural networks (CNNs) for the task of anomaly\ndetection. Single class SVMs have been used in the past with some success,\nhowever we hypothesize that deeper networks for single class classification\nshould perform better. Results obtained on established anomaly detection\nbenchmarks as well as on a real-world dataset, show that the proposed method\nclearly outperforms the existing state-of-the-art methods, by achieving a\nstaggering average area under the receiver operating characteristic curve value\nof 0.99 for the tested data-sets which is an average improvement of 41% on the\nCIFAR10, 20% on MNIST and 16% on Cement Crack data-sets.\n", "versions": [{"version": "v1", "created": "Thu, 9 May 2019 00:44:19 GMT"}], "update_date": "2019-05-31", "authors_parsed": [["Minhas", "Manpreet Singh", ""], ["Zelek", "John", ""]]}, {"id": "1905.13148", "submitter": "Qun Song", "authors": "Qun Song, Zhenyu Yan, and Rui Tan", "title": "Moving Target Defense for Deep Visual Sensing against Adversarial\n  Examples", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning based visual sensing has achieved attractive accuracy but is\nshown vulnerable to adversarial example attacks. Specifically, once the\nattackers obtain the deep model, they can construct adversarial examples to\nmislead the model to yield wrong classification results. Deployable adversarial\nexamples such as small stickers pasted on the road signs and lanes have been\nshown effective in misleading advanced driver-assistance systems. Many existing\ncountermeasures against adversarial examples build their security on the\nattackers' ignorance of the defense mechanisms. Thus, they fall short of\nfollowing Kerckhoffs's principle and can be subverted once the attackers know\nthe details of the defense. This paper applies the strategy of moving target\ndefense (MTD) to generate multiple new deep models after system deployment,\nthat will collaboratively detect and thwart adversarial examples. Our MTD\ndesign is based on the adversarial examples' minor transferability to models\ndiffering from the one (e.g., the factory-designed model) used for attack\nconstruction. The post-deployment quasi-secret deep models significantly\nincrease the bar for the attackers to construct effective adversarial examples.\nWe also apply the technique of serial data fusion with early stopping to reduce\nthe inference time by a factor of up to 5 while maintaining the sensing and\ndefense performance. Extensive evaluation based on three datasets including a\nroad sign image database and a GPU-equipped Jetson embedded computing board\nshows the effectiveness of our approach.\n", "versions": [{"version": "v1", "created": "Sat, 11 May 2019 08:22:32 GMT"}], "update_date": "2019-05-31", "authors_parsed": [["Song", "Qun", ""], ["Yan", "Zhenyu", ""], ["Tan", "Rui", ""]]}, {"id": "1905.13149", "submitter": "Fangda Han", "authors": "Fangda Han, Ricardo Guerrero, Vladimir Pavlovic", "title": "The Art of Food: Meal Image Synthesis from Ingredients", "comments": "12 pages, 6 figures, 2 tables, under review as a conference paper at\n  BMVC 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we propose a new computational framework, based on generative\ndeep models, for synthesis of photo-realistic food meal images from textual\ndescriptions of its ingredients. Previous works on synthesis of images from\ntext typically rely on pre-trained text models to extract text features,\nfollowed by a generative neural networks (GANs) aimed to generate realistic\nimages conditioned on the text features. These works mainly focus on generating\nspatially compact and well-defined categories of objects, such as birds or\nflowers. In contrast, meal images are significantly more complex, consisting of\nmultiple ingredients whose appearance and spatial qualities are further\nmodified by cooking methods. We propose a method that first builds an\nattention-based ingredients-image association model, which is then used to\ncondition a generative neural network tasked with synthesizing meal images.\nFurthermore, a cycle-consistent constraint is added to further improve image\nquality and control appearance. Extensive experiments show our model is able to\ngenerate meal image corresponding to the ingredients, which could be used to\naugment existing dataset for solving other computational food analysis\nproblems.\n", "versions": [{"version": "v1", "created": "Thu, 9 May 2019 20:57:51 GMT"}], "update_date": "2019-05-31", "authors_parsed": [["Han", "Fangda", ""], ["Guerrero", "Ricardo", ""], ["Pavlovic", "Vladimir", ""]]}, {"id": "1905.13153", "submitter": "Vanya Cohen", "authors": "Vanya Cohen, Benjamin Burchfiel, Thao Nguyen, Nakul Gopalan, Stefanie\n  Tellex, George Konidaris", "title": "Grounding Language Attributes to Objects using Bayesian Eigenobjects", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CL cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a system to disambiguate object instances within the same class\nbased on simple physical descriptions. The system takes as input a natural\nlanguage phrase and a depth image containing a segmented object and predicts\nhow similar the observed object is to the object described by the phrase. Our\nsystem is designed to learn from only a small amount of human-labeled language\ndata and generalize to viewpoints not represented in the language-annotated\ndepth image training set. By decoupling 3D shape representation from language\nrepresentation, this method is able to ground language to novel objects using a\nsmall amount of language-annotated depth-data and a larger corpus of unlabeled\n3D object meshes, even when these objects are partially observed from unusual\nviewpoints. Our system is able to disambiguate between novel objects, observed\nvia depth images, based on natural language descriptions. Our method also\nenables view-point transfer; trained on human-annotated data on a small set of\ndepth images captured from frontal viewpoints, our system successfully\npredicted object attributes from rear views despite having no such depth images\nin its training set. Finally, we demonstrate our approach on a Baxter robot,\nenabling it to pick specific objects based on human-provided natural language\ndescriptions.\n", "versions": [{"version": "v1", "created": "Thu, 30 May 2019 16:15:36 GMT"}, {"version": "v2", "created": "Thu, 1 Aug 2019 19:07:02 GMT"}], "update_date": "2019-08-05", "authors_parsed": [["Cohen", "Vanya", ""], ["Burchfiel", "Benjamin", ""], ["Nguyen", "Thao", ""], ["Gopalan", "Nakul", ""], ["Tellex", "Stefanie", ""], ["Konidaris", "George", ""]]}, {"id": "1905.13192", "submitter": "Simon Du", "authors": "Simon S. Du, Kangcheng Hou, Barnab\\'as P\\'oczos, Ruslan Salakhutdinov,\n  Ruosong Wang, Keyulu Xu", "title": "Graph Neural Tangent Kernel: Fusing Graph Neural Networks with Graph\n  Kernels", "comments": "In NeurIPS 2019. Code available: https://github.com/KangchengHou/gntk", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While graph kernels (GKs) are easy to train and enjoy provable theoretical\nguarantees, their practical performances are limited by their expressive power,\nas the kernel function often depends on hand-crafted combinatorial features of\ngraphs. Compared to graph kernels, graph neural networks (GNNs) usually achieve\nbetter practical performance, as GNNs use multi-layer architectures and\nnon-linear activation functions to extract high-order information of graphs as\nfeatures. However, due to the large number of hyper-parameters and the\nnon-convex nature of the training procedure, GNNs are harder to train.\nTheoretical guarantees of GNNs are also not well-understood. Furthermore, the\nexpressive power of GNNs scales with the number of parameters, and thus it is\nhard to exploit the full power of GNNs when computing resources are limited.\nThe current paper presents a new class of graph kernels, Graph Neural Tangent\nKernels (GNTKs), which correspond to infinitely wide multi-layer GNNs trained\nby gradient descent. GNTKs enjoy the full expressive power of GNNs and inherit\nadvantages of GKs. Theoretically, we show GNTKs provably learn a class of\nsmooth functions on graphs. Empirically, we test GNTKs on graph classification\ndatasets and show they achieve strong performance.\n", "versions": [{"version": "v1", "created": "Thu, 30 May 2019 17:23:23 GMT"}, {"version": "v2", "created": "Mon, 4 Nov 2019 15:30:12 GMT"}], "update_date": "2019-11-05", "authors_parsed": [["Du", "Simon S.", ""], ["Hou", "Kangcheng", ""], ["P\u00f3czos", "Barnab\u00e1s", ""], ["Salakhutdinov", "Ruslan", ""], ["Wang", "Ruosong", ""], ["Xu", "Keyulu", ""]]}, {"id": "1905.13208", "submitter": "Jiayun Li", "authors": "Jiayun Li, Wenyuan Li, Arkadiusz Gertych, Beatrice S. Knudsen, William\n  Speier, Corey W. Arnold", "title": "An attention-based multi-resolution model for prostate whole slide\n  imageclassification and localization", "comments": "8 pages, 4 figures, CVPR 2019 Towards Causal, Explainable and\n  Universal Medical Visual Diagnosis (MVD) Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Histology review is often used as the `gold standard' for disease diagnosis.\nComputer aided diagnosis tools can potentially help improve current pathology\nworkflows by reducing examination time and interobserver variability. Previous\nwork in cancer grading has focused mainly on classifying pre-defined regions of\ninterest (ROIs), or relied on large amounts of fine-grained labels. In this\npaper, we propose a two-stage attention-based multiple instance learning model\nfor slide-level cancer grading and weakly-supervised ROI detection and\ndemonstrate its use in prostate cancer. Compared with existing Gleason\nclassification models, our model goes a step further by utilizing visualized\nsaliency maps to select informative tiles for fine-grained grade\nclassification. The model was primarily developed on a large-scale whole slide\ndataset consisting of 3,521 prostate biopsy slides with only slide-level labels\nfrom 718 patients. The model achieved state-of-the-art performance for prostate\ncancer grading with an accuracy of 85.11\\% for classifying benign, low-grade\n(Gleason grade 3+3 or 3+4), and high-grade (Gleason grade 4+3 or higher) slides\non an independent test set.\n", "versions": [{"version": "v1", "created": "Thu, 30 May 2019 17:50:56 GMT"}], "update_date": "2019-05-31", "authors_parsed": [["Li", "Jiayun", ""], ["Li", "Wenyuan", ""], ["Gertych", "Arkadiusz", ""], ["Knudsen", "Beatrice S.", ""], ["Speier", "William", ""], ["Arnold", "Corey W.", ""]]}, {"id": "1905.13209", "submitter": "Michael S. Ryoo", "authors": "Michael S. Ryoo, AJ Piergiovanni, Mingxing Tan, Anelia Angelova", "title": "AssembleNet: Searching for Multi-Stream Neural Connectivity in Video\n  Architectures", "comments": null, "journal-ref": "ICLR 2020", "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning to represent videos is a very challenging task both algorithmically\nand computationally. Standard video CNN architectures have been designed by\ndirectly extending architectures devised for image understanding to include the\ntime dimension, using modules such as 3D convolutions, or by using two-stream\ndesign to capture both appearance and motion in videos. We interpret a video\nCNN as a collection of multi-stream convolutional blocks connected to each\nother, and propose the approach of automatically finding neural architectures\nwith better connectivity and spatio-temporal interactions for video\nunderstanding. This is done by evolving a population of overly-connected\narchitectures guided by connection weight learning. Architectures combining\nrepresentations that abstract different input types (i.e., RGB and optical\nflow) at multiple temporal resolutions are searched for, allowing different\ntypes or sources of information to interact with each other. Our method,\nreferred to as AssembleNet, outperforms prior approaches on public video\ndatasets, in some cases by a great margin. We obtain 58.6% mAP on Charades and\n34.27% accuracy on Moments-in-Time.\n", "versions": [{"version": "v1", "created": "Thu, 30 May 2019 17:51:03 GMT"}, {"version": "v2", "created": "Mon, 30 Sep 2019 17:54:13 GMT"}, {"version": "v3", "created": "Mon, 27 Apr 2020 17:48:45 GMT"}, {"version": "v4", "created": "Wed, 27 May 2020 15:56:37 GMT"}], "update_date": "2020-05-28", "authors_parsed": [["Ryoo", "Michael S.", ""], ["Piergiovanni", "AJ", ""], ["Tan", "Mingxing", ""], ["Angelova", "Anelia", ""]]}, {"id": "1905.13211", "submitter": "Keyulu Xu", "authors": "Keyulu Xu, Jingling Li, Mozhi Zhang, Simon S. Du, Ken-ichi\n  Kawarabayashi, Stefanie Jegelka", "title": "What Can Neural Networks Reason About?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural networks have succeeded in many reasoning tasks. Empirically, these\ntasks require specialized network structures, e.g., Graph Neural Networks\n(GNNs) perform well on many such tasks, but less structured networks fail.\nTheoretically, there is limited understanding of why and when a network\nstructure generalizes better than others, although they have equal expressive\npower. In this paper, we develop a framework to characterize which reasoning\ntasks a network can learn well, by studying how well its computation structure\naligns with the algorithmic structure of the relevant reasoning process. We\nformally define this algorithmic alignment and derive a sample complexity bound\nthat decreases with better alignment. This framework offers an explanation for\nthe empirical success of popular reasoning models, and suggests their\nlimitations. As an example, we unify seemingly different reasoning tasks, such\nas intuitive physics, visual question answering, and shortest paths, via the\nlens of a powerful algorithmic paradigm, dynamic programming (DP). We show that\nGNNs align with DP and thus are expected to solve these tasks. On several\nreasoning tasks, our theory is supported by empirical results.\n", "versions": [{"version": "v1", "created": "Thu, 30 May 2019 17:53:30 GMT"}, {"version": "v2", "created": "Fri, 31 May 2019 21:50:31 GMT"}, {"version": "v3", "created": "Sun, 29 Sep 2019 20:42:29 GMT"}, {"version": "v4", "created": "Sat, 15 Feb 2020 06:56:25 GMT"}], "update_date": "2020-02-18", "authors_parsed": [["Xu", "Keyulu", ""], ["Li", "Jingling", ""], ["Zhang", "Mozhi", ""], ["Du", "Simon S.", ""], ["Kawarabayashi", "Ken-ichi", ""], ["Jegelka", "Stefanie", ""]]}, {"id": "1905.13214", "submitter": "Ilija Radosavovic", "authors": "Ilija Radosavovic, Justin Johnson, Saining Xie, Wan-Yen Lo, Piotr\n  Doll\\'ar", "title": "On Network Design Spaces for Visual Recognition", "comments": "tech report", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Over the past several years progress in designing better neural network\narchitectures for visual recognition has been substantial. To help sustain this\nrate of progress, in this work we propose to reexamine the methodology for\ncomparing network architectures. In particular, we introduce a new comparison\nparadigm of distribution estimates, in which network design spaces are compared\nby applying statistical techniques to populations of sampled models, while\ncontrolling for confounding factors like network complexity. Compared to\ncurrent methodologies of comparing point and curve estimates of model families,\ndistribution estimates paint a more complete picture of the entire design\nlandscape. As a case study, we examine design spaces used in neural\narchitecture search (NAS). We find significant statistical differences between\nrecent NAS design space variants that have been largely overlooked.\nFurthermore, our analysis reveals that the design spaces for standard model\nfamilies like ResNeXt can be comparable to the more complex ones used in recent\nNAS work. We hope these insights into distribution analysis will enable more\nrobust progress toward discovering better networks for visual recognition.\n", "versions": [{"version": "v1", "created": "Thu, 30 May 2019 17:56:17 GMT"}], "update_date": "2019-05-31", "authors_parsed": [["Radosavovic", "Ilija", ""], ["Johnson", "Justin", ""], ["Xie", "Saining", ""], ["Lo", "Wan-Yen", ""], ["Doll\u00e1r", "Piotr", ""]]}, {"id": "1905.13215", "submitter": "Nga Nguyen", "authors": "Nga T.T. Nguyen and Garrett T. Kenyon", "title": "Image classification using quantum inference on the D-Wave 2X", "comments": "7 pages, 6 figures", "journal-ref": "IEEE Proceedings of the 3rd International Conference on Rebooting\n  Computing (ICRC), November, 2018", "doi": "10.1109/ICRC.2018.8638596", "report-no": null, "categories": "quant-ph cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We use a quantum annealing D-Wave 2X computer to obtain solutions to NP-hard\nsparse coding problems. To reduce the dimensionality of the sparse coding\nproblem to fit on the quantum D-Wave 2X hardware, we passed downsampled MNIST\nimages through a bottleneck autoencoder. To establish a benchmark for\nclassification performance on this reduced dimensional data set, we used an\nAlexNet-like architecture implemented in TensorFlow, obtaining a classification\nscore of $94.54 \\pm 0.7 \\%$. As a control, we showed that the same AlexNet-like\narchitecture produced near-state-of-the-art classification performance $(\\sim\n99\\%)$ on the original MNIST images. To obtain a set of optimized features for\ninferring sparse representations of the reduced dimensional MNIST dataset, we\nimprinted on a random set of $47$ image patches followed by an off-line\nunsupervised learning algorithm using stochastic gradient descent to optimize\nfor sparse coding. Our single-layer of sparse coding matched the stride and\npatch size of the first convolutional layer of the AlexNet-like deep neural\nnetwork and contained $47$ fully-connected features, $47$ being the maximum\nnumber of dictionary elements that could be embedded onto the D-Wave $2$X\nhardware. Recent work suggests that the optimal level of sparsity corresponds\nto a critical value of the trade-off parameter associated with a putative\nsecond order phase transition, an observation supported by a free energy\nanalysis of D-Wave energy states. When the sparse representations inferred by\nthe D-Wave $2$X were passed to a linear support vector machine, we obtained a\nclassification score of $95.68\\%$. Thus, on this problem, we find that a\nsingle-layer of quantum inference is able to outperform a standard deep neural\nnetwork architecture.\n", "versions": [{"version": "v1", "created": "Tue, 28 May 2019 19:21:24 GMT"}], "update_date": "2019-05-31", "authors_parsed": [["Nguyen", "Nga T. T.", ""], ["Kenyon", "Garrett T.", ""]]}, {"id": "1905.13221", "submitter": "Nick Antipa", "authors": "Nick Antipa, Patrick Oare, Emrah Bostan, Ren Ng, Laura Waller", "title": "Video from Stills: Lensless Imaging with Rolling Shutter", "comments": "8 pages, 7 figures, IEEE International Conference on Computational\n  Photography 2019, Tokyo", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Because image sensor chips have a finite bandwidth with which to read out\npixels, recording video typically requires a trade-off between frame rate and\npixel count. Compressed sensing techniques can circumvent this trade-off by\nassuming that the image is compressible. Here, we propose using multiplexing\noptics to spatially compress the scene, enabling information about the whole\nscene to be sampled from a row of sensor pixels, which can be read off quickly\nvia a rolling shutter CMOS sensor. Conveniently, such multiplexing can be\nachieved with a simple lensless, diffuser-based imaging system. Using sparse\nrecovery methods, we are able to recover 140 video frames at over 4,500 frames\nper second, all from a single captured image with a rolling shutter sensor. Our\nproof-of-concept system uses easily-fabricated diffusers paired with an\noff-the-shelf sensor. The resulting prototype enables compressive encoding of\nhigh frame rate video into a single rolling shutter exposure, and exceeds the\nsampling-limited performance of an equivalent global shutter system for\nsufficiently sparse objects.\n", "versions": [{"version": "v1", "created": "Thu, 30 May 2019 17:59:01 GMT"}], "update_date": "2019-05-31", "authors_parsed": [["Antipa", "Nick", ""], ["Oare", "Patrick", ""], ["Bostan", "Emrah", ""], ["Ng", "Ren", ""], ["Waller", "Laura", ""]]}, {"id": "1905.13258", "submitter": "Sultan Khan Daud", "authors": "Sultan Daud Khan and Habib Ullah", "title": "A survey of advances in vision-based vehicle re-identification", "comments": "17 pages; 21 figures; journal paper", "journal-ref": "Computer Vision and Image Understanding 2019", "doi": "10.1016/j.cviu.2019.03.001", "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Vehicle re-identification (V-reID) has become significantly popular in the\ncommunity due to its applications and research significance. In particular, the\nV-reID is an important problem that still faces numerous open challenges. This\npaper reviews different V-reID methods including sensor based methods, hybrid\nmethods, and vision based methods which are further categorized into\nhand-crafted feature based methods and deep feature based methods. The vision\nbased methods make the V-reID problem particularly interesting, and our review\nsystematically addresses and evaluates these methods for the first time. We\nconduct experiments on four comprehensive benchmark datasets and compare the\nperformances of recent hand-crafted feature based methods and deep feature\nbased methods. We present the detail analysis of these methods in terms of mean\naverage precision (mAP) and cumulative matching curve (CMC). These analyses\nprovide objective insight into the strengths and weaknesses of these methods.\nWe also provide the details of different V-reID datasets and critically discuss\nthe challenges and future trends of V-reID methods.\n", "versions": [{"version": "v1", "created": "Thu, 30 May 2019 18:45:40 GMT"}], "update_date": "2019-06-03", "authors_parsed": [["Khan", "Sultan Daud", ""], ["Ullah", "Habib", ""]]}, {"id": "1905.13260", "submitter": "Yue Wu", "authors": "Yue Wu, Yinpeng Chen, Lijuan Wang, Yuancheng Ye, Zicheng Liu, Yandong\n  Guo, Yun Fu", "title": "Large Scale Incremental Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern machine learning suffers from catastrophic forgetting when learning\nnew classes incrementally. The performance dramatically degrades due to the\nmissing data of old classes. Incremental learning methods have been proposed to\nretain the knowledge acquired from the old classes, by using knowledge\ndistilling and keeping a few exemplars from the old classes. However, these\nmethods struggle to scale up to a large number of classes. We believe this is\nbecause of the combination of two factors: (a) the data imbalance between the\nold and new classes, and (b) the increasing number of visually similar classes.\nDistinguishing between an increasing number of visually similar classes is\nparticularly challenging, when the training data is unbalanced. We propose a\nsimple and effective method to address this data imbalance issue. We found that\nthe last fully connected layer has a strong bias towards the new classes, and\nthis bias can be corrected by a linear model. With two bias parameters, our\nmethod performs remarkably well on two large datasets: ImageNet (1000 classes)\nand MS-Celeb-1M (10000 classes), outperforming the state-of-the-art algorithms\nby 11.1% and 13.2% respectively.\n", "versions": [{"version": "v1", "created": "Thu, 30 May 2019 18:51:39 GMT"}], "update_date": "2019-06-03", "authors_parsed": [["Wu", "Yue", ""], ["Chen", "Yinpeng", ""], ["Wang", "Lijuan", ""], ["Ye", "Yuancheng", ""], ["Liu", "Zicheng", ""], ["Guo", "Yandong", ""], ["Fu", "Yun", ""]]}, {"id": "1905.13290", "submitter": "Jennifer Cardona", "authors": "Jennifer L Cardona, Michael F Howland, John O Dabiri", "title": "Seeing the Wind: Visual Wind Speed Prediction with a Coupled\n  Convolutional and Recurrent Neural Network", "comments": "NeurIPS 2019 (to appear). The dataset has been expanded to include\n  videos of a tree canopy in addition to flags. The models were retrained, and\n  results were updated accordingly. The introduction and related work sections\n  were also expand upon. Clarifying details were added to explain author\n  choices such as time averaging windows and to further discuss test set\n  results", "journal-ref": "Advances in Neural Information Processing Systems 32 (2019)", "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG physics.flu-dyn", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Wind energy resource quantification, air pollution monitoring, and weather\nforecasting all rely on rapid, accurate measurement of local wind conditions.\nVisual observations of the effects of wind---the swaying of trees and flapping\nof flags, for example---encode information regarding local wind conditions that\ncan potentially be leveraged for visual anemometry that is inexpensive and\nubiquitous. Here, we demonstrate a coupled convolutional neural network and\nrecurrent neural network architecture that extracts the wind speed encoded in\nvisually recorded flow-structure interactions of a flag and tree in naturally\noccurring wind. Predictions for wind speeds ranging from 0.75-11 m/s showed\nagreement with measurements from a cup anemometer on site, with a\nroot-mean-squared error approaching the natural wind speed variability due to\natmospheric turbulence. Generalizability of the network was demonstrated by\nsuccessful prediction of wind speed based on recordings of other flags in the\nfield and in a controlled wind tunnel test. Furthermore, physics-based scaling\nof the flapping dynamics accurately predicts the dependence of the network\nperformance on the video frame rate and duration.\n", "versions": [{"version": "v1", "created": "Thu, 30 May 2019 20:24:25 GMT"}, {"version": "v2", "created": "Wed, 10 Jul 2019 05:28:16 GMT"}, {"version": "v3", "created": "Mon, 28 Oct 2019 20:55:33 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Cardona", "Jennifer L", ""], ["Howland", "Michael F", ""], ["Dabiri", "John O", ""]]}, {"id": "1905.13291", "submitter": "Karthikeyan Natesan Ramamurthy", "authors": "Min-hwan Oh and Peder Olsen and Karthikeyan Natesan Ramamurthy", "title": "Counting and Segmenting Sorghum Heads", "comments": "23 pages, 23 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Phenotyping is the process of measuring an organism's observable traits.\nManual phenotyping of crops is a labor-intensive, time-consuming, costly, and\nerror prone process. Accurate, automated, high-throughput phenotyping can\nrelieve a huge burden in the crop breeding pipeline. In this paper, we propose\na scalable, high-throughput approach to automatically count and segment\npanicles (heads), a key phenotype, from aerial sorghum crop imagery. Our\ncounting approach uses the image density map obtained from dot or region\nannotation as the target with a novel deep convolutional neural network\narchitecture. We also propose a novel instance segmentation algorithm using the\nestimated density map, to identify the individual panicles in the presence of\nocclusion. With real Sorghum aerial images, we obtain a mean absolute error\n(MAE) of 1.06 for counting which is better than using well-known crowd counting\napproaches such as CCNN, MCNN and CSRNet models. The instance segmentation\nmodel also produces respectable results which will be ultimately useful in\nreducing the manual annotation workload for future data.\n", "versions": [{"version": "v1", "created": "Thu, 30 May 2019 20:27:26 GMT"}], "update_date": "2019-06-03", "authors_parsed": [["Oh", "Min-hwan", ""], ["Olsen", "Peder", ""], ["Ramamurthy", "Karthikeyan Natesan", ""]]}, {"id": "1905.13300", "submitter": "Haizhao Yang", "authors": "Lin Chen and Haizhao Yang", "title": "Generative Imaging and Image Processing via Generative Encoder", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a novel generative encoder (GE) model for generative\nimaging and image processing with applications in compressed sensing and\nimaging, image compression, denoising, inpainting, deblurring, and\nsuper-resolution. The GE model consists of a pre-training phase and a solving\nphase. In the pre-training phase, we separately train two deep neural networks:\na generative adversarial network (GAN) with a generator $\\G$ that captures the\ndata distribution of a given image set, and an auto-encoder (AE) network with\nan encoder $\\EN$ that compresses images following the estimated distribution by\nGAN. In the solving phase, given a noisy image $x=\\mathcal{P}(x^*)$, where\n$x^*$ is the target unknown image, $\\mathcal{P}$ is an operator adding an\naddictive, or multiplicative, or convolutional noise, or equivalently given\nsuch an image $x$ in the compressed domain, i.e., given $m=\\EN(x)$, we solve\nthe optimization problem\n  \\[\n  z^*=\\underset{z}{\\mathrm{argmin}} \\|\\EN(\\G(z))-m\\|_2^2+\\lambda\\|z\\|_2^2\n  \\] to recover the image $x^*$ in a generative way via\n$\\hat{x}:=\\G(z^*)\\approx x^*$, where $\\lambda>0$ is a hyperparameter. The GE\nmodel unifies the generative capacity of GANs and the stability of AEs in an\noptimization framework above instead of stacking GANs and AEs into a single\nnetwork or combining their loss functions into one as in existing literature.\nNumerical experiments show that the proposed model outperforms several\nstate-of-the-art algorithms.\n", "versions": [{"version": "v1", "created": "Thu, 23 May 2019 19:11:00 GMT"}], "update_date": "2019-06-03", "authors_parsed": [["Chen", "Lin", ""], ["Yang", "Haizhao", ""]]}, {"id": "1905.13302", "submitter": "John Pavlopoulos", "authors": "Vasiliki Kougia, John Pavlopoulos and Ion Androutsopoulos", "title": "A Survey on Biomedical Image Captioning", "comments": "SiVL 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image captioning applied to biomedical images can assist and accelerate the\ndiagnosis process followed by clinicians. This article is the first survey of\nbiomedical image captioning, discussing datasets, evaluation measures, and\nstate of the art methods. Additionally, we suggest two baselines, a weak and a\nstronger one; the latter outperforms all current state of the art systems on\none of the datasets.\n", "versions": [{"version": "v1", "created": "Sun, 26 May 2019 15:47:28 GMT"}], "update_date": "2019-06-03", "authors_parsed": [["Kougia", "Vasiliki", ""], ["Pavlopoulos", "John", ""], ["Androutsopoulos", "Ion", ""]]}, {"id": "1905.13305", "submitter": "Tsung Wei Tsai", "authors": "Tsung Wei Tsai, Chongxuan Li, Jun Zhu", "title": "Countering Noisy Labels By Learning From Auxiliary Clean Labels", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the learning from noisy labels (NL) problem which emerges in many\nreal-world applications. In addition to the widely-studied synthetic noise in\nthe NL literature, we also consider the pseudo labels in semi-supervised\nlearning (Semi-SL) as a special case of NL. For both types of noise, we argue\nthat the generalization performance of existing methods is highly coupled with\nthe quality of noisy labels. Therefore, we counter the problem from a novel and\nunified perspective: learning from the auxiliary clean labels. Specifically, we\npropose the Rotational-Decoupling Consistency Regularization (RDCR) framework\nthat integrates the consistency-based methods with the self-supervised rotation\ntask to learn noise-tolerant representations. The experiments show that RDCR\nachieves comparable or superior performance than the state-of-the-art methods\nunder small noise, while outperforms the existing methods significantly when\nthere is large noise.\n", "versions": [{"version": "v1", "created": "Thu, 23 May 2019 10:20:17 GMT"}, {"version": "v2", "created": "Thu, 12 Sep 2019 06:20:03 GMT"}], "update_date": "2019-09-13", "authors_parsed": [["Tsai", "Tsung Wei", ""], ["Li", "Chongxuan", ""], ["Zhu", "Jun", ""]]}, {"id": "1905.13306", "submitter": "Charles Lehman", "authors": "Charles Lehman, Dogancan Temel, and Ghassan AlRegib", "title": "Implicit Background Estimation for Semantic Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scene understanding and semantic segmentation are at the core of many\ncomputer vision tasks, many of which, involve interacting with humans in\npotentially dangerous ways. It is therefore paramount that techniques for\nprincipled design of robust models be developed. In this paper, we provide\nanalytic and empirical evidence that correcting potentially errant non-distinct\nmappings that result from the softmax function can result in improving\nrobustness characteristics on a state-of-the-art semantic segmentation model\nwith minimal impact to performance and minimal changes to the code base.\n", "versions": [{"version": "v1", "created": "Thu, 23 May 2019 09:20:52 GMT"}], "update_date": "2019-06-03", "authors_parsed": [["Lehman", "Charles", ""], ["Temel", "Dogancan", ""], ["AlRegib", "Ghassan", ""]]}, {"id": "1905.13307", "submitter": "Javier Felip Leon", "authors": "Javier Felip and Nilesh Ahuja and David G\\'omez-Guti\\'errez and Omesh\n  Tickoo and Vikash Mansinghka", "title": "Real-time Approximate Bayesian Computation for Scene Understanding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider scene understanding problems such as predicting where a person is\nprobably reaching, or inferring the pose of 3D objects from depth images, or\ninferring the probable street crossings of pedestrians at a busy intersection.\nThis paper shows how to solve these problems using Approximate Bayesian\nComputation. The underlying generative models are built from realistic\nsimulation software, wrapped in a Bayesian error model for the gap between\nsimulation outputs and real data. The simulators are drawn from off-the-shelf\ncomputer graphics, video game, and traffic simulation code. The paper\nintroduces two techniques for speeding up inference that can be used separately\nor in combination. The first is to train neural surrogates of the simulators,\nusing a simple form of domain randomization to make the surrogates more robust\nto the gap between the simulation and reality. The second is to adaptively\ndiscretize the latent variables using a Tree-pyramid approach adapted from\ncomputer graphics. This paper also shows performance and accuracy measurements\non real-world problems, establishing that it is feasible to solve these\nproblems in real-time.\n", "versions": [{"version": "v1", "created": "Wed, 22 May 2019 20:03:13 GMT"}], "update_date": "2019-06-03", "authors_parsed": [["Felip", "Javier", ""], ["Ahuja", "Nilesh", ""], ["G\u00f3mez-Guti\u00e9rrez", "David", ""], ["Tickoo", "Omesh", ""], ["Mansinghka", "Vikash", ""]]}, {"id": "1905.13308", "submitter": "Jesse Livezey", "authors": "Jesse A. Livezey, Ahyeon Hwang, Jacob Yeung, Kristofer E. Bouchard", "title": "Hangul Fonts Dataset: a Hierarchical and Compositional Dataset for\n  Investigating Learned Representations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hierarchy and compositionality are common latent properties in many natural\nand scientific datasets. Determining when a deep network's hidden activations\nrepresent hierarchy and compositionality is important both for understanding\ndeep representation learning and for applying deep networks in domains where\ninterpretability is crucial. However, current benchmark machine learning\ndatasets either have little hierarchical or compositional structure, or the\nstructure is not known. This gap impedes precise analysis of a network's\nrepresentations and thus hinders development of new methods that can learn such\nproperties. To address this gap, we developed a new benchmark dataset with\nknown hierarchical and compositional structure. The Hangul Fonts Dataset (HFD)\nis comprised of 35 fonts from the Korean writing system (Hangul), each with\n11,172 blocks (syllables) composed from the product of initial consonant,\nmedial vowel, and final consonant glyphs. All blocks can be grouped into a few\ngeometric types which induces a hierarchy across blocks. In addition, each\nblock is composed of individual glyphs with rotations, translations, scalings,\nand naturalistic style variation across fonts. We find that both shallow and\ndeep unsupervised methods only show modest evidence of hierarchy and\ncompositionality in their representations of the HFD compared to supervised\ndeep networks. Supervised deep network representations contain structure\nrelated to the geometrical hierarchy of the characters, but the compositional\nstructure of the data is not evident. Thus, HFD enables the identification of\nshortcomings in existing methods, a critical first step toward developing new\nmachine learning algorithms to extract hierarchical and compositional structure\nin the context of naturalistic variability.\n", "versions": [{"version": "v1", "created": "Thu, 23 May 2019 21:20:58 GMT"}, {"version": "v2", "created": "Wed, 9 Jun 2021 17:13:10 GMT"}], "update_date": "2021-06-10", "authors_parsed": [["Livezey", "Jesse A.", ""], ["Hwang", "Ahyeon", ""], ["Yeung", "Jacob", ""], ["Bouchard", "Kristofer E.", ""]]}, {"id": "1905.13309", "submitter": "Jordan Masakuna F", "authors": "Jordan F. Masakuna", "title": "Machine Learning Methods for Shark Detection", "comments": "19 figures, 31 pages. This is a project for masters degree", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This essay reviews human observer-based methods employed in shark spotting in\nMuizenberg Beach. It investigates Machine Learning methods for automated shark\ndetection with the aim of enhancing human observation. A questionnaire and\ninterview were used to collect information about shark spotting, the motivation\nof the actual Shark Spotter program and its limitations. We have defined a list\nof desirable properties for our model and chosen the adequate mathematical\ntechniques. The preliminary results of the research show that we can expect to\nextract useful information from shark images despite the geometric\ntransformations that sharks perform, its features do not change. To conclude,\nwe have partially implemented our model; the remaining implementation requires\ndataset.\n", "versions": [{"version": "v1", "created": "Tue, 21 May 2019 09:34:58 GMT"}], "update_date": "2019-06-03", "authors_parsed": [["Masakuna", "Jordan F.", ""]]}, {"id": "1905.13312", "submitter": "Li Wang", "authors": "Li Wang, Lihui Wang, Qijian Chen, Caixia Sun, Xinyu Cheng, Yuemin Zhu", "title": "Convolutional Restricted Boltzmann Machine Based-Radiomics for\n  Prediction of Pathological Complete Response to Neoadjuvant Chemotherapy in\n  Breast Cancer", "comments": "8 pages, 6 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We proposed a novel convolutional restricted Boltzmann machine CRBM-based\nradiomic method for predicting pathologic complete response (pCR) to\nneoadjuvant chemotherapy treatment (NACT) in breast cancer. The method consists\nof extracting semantic features from CRBM network, and pCR prediction. It was\nevaluated on the dynamic contrast-enhanced magnetic resonance imaging (DCE-MRI)\ndata of 57 patients and using the area under the receiver operating\ncharacteristic curve (AUC). Traditional radiomics features and the semantic\nfeatures learned from CRBM network were extracted from the images acquired\nbefore and after the administration of NACT. After the feature selection, the\nsupport vector machine (SVM), logistic regression (LR) and random forest (RF)\nwere trained to predict the pCR status. Compared to traditional radiomic\nmethods, the proposed CRBM-based radiomic method yielded an AUC of 0.92 for the\nprediction with the images acquired before and after NACT, and an AUC of 0.87\nfor the pretreatment prediction, which was increased by about 38%. The results\nshowed that the CRBM-based radiomic method provided a potential means for\naccurately predicting the pCR to NACT in breast cancer before the treatment,\nwhich is very useful for making more appropriate and personalized treatment\nregimens.\n", "versions": [{"version": "v1", "created": "Thu, 23 May 2019 08:17:08 GMT"}], "update_date": "2019-06-03", "authors_parsed": [["Wang", "Li", ""], ["Wang", "Lihui", ""], ["Chen", "Qijian", ""], ["Sun", "Caixia", ""], ["Cheng", "Xinyu", ""], ["Zhu", "Yuemin", ""]]}, {"id": "1905.13313", "submitter": "Junwei Liang", "authors": "Junwei Liang and Jay D. Aronson and Alexander Hauptmann", "title": "Technical Report of the Video Event Reconstruction and Analysis (VERA)\n  System -- Shooter Localization, Models, Interface, and Beyond", "comments": "The code and models are available at\n  https://github.com/JunweiLiang/VERA_Shooter_Localization . Our system is live\n  at https://vera.cs.cmu.edu/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Every minute, hundreds of hours of video are uploaded to social media sites\nand the Internet from around the world. This material creates a visual record\nof the experiences of a significant percentage of humanity and can help\nilluminate how we live in the present moment. When properly analyzed, this\nvideo can also help analysts to reconstruct events of interest, including war\ncrimes, human rights violations, and terrorist acts. Machine learning and\ncomputer vision can play a crucial role in this process. In this technical\nreport, we describe the Video Event Reconstruction and Analysis (VERA) system.\nThis new tool brings together a variety of capabilities we have developed over\nthe past few years (including video synchronization and geolocation to order\nunstructured videos lacking metadata over time and space, and sound recognition\nalgorithms) to enable the reconstruction and analysis of events captured on\nvideo. Among other uses, VERA enables the localization of a shooter from just a\nfew videos that include the sound of gunshots. To demonstrate the efficacy of\nthis suite of tools, we present the results of estimating the shooter's\nlocation of the Las Vegas Shooting in 2017 and show that VERA accurately\npredicts the shooter's location using only the first few gunshots. We then\npoint out future directions that can help improve the system and further reduce\nunnecessary human labor in the process. All of the components of VERA run\nthrough a web interface that enables human-in-the-loop verification to ensure\naccurate estimations. All relevant source code, including the web interface and\nmachine learning models, is freely available on Github. We hope that\nresearchers and software developers will be inspired to improve and expand this\nsystem moving forward to better meet the needs of human rights and public\nsafety.\n", "versions": [{"version": "v1", "created": "Sun, 26 May 2019 17:55:50 GMT"}, {"version": "v2", "created": "Mon, 3 Jun 2019 21:12:12 GMT"}, {"version": "v3", "created": "Sat, 8 Jun 2019 16:04:11 GMT"}, {"version": "v4", "created": "Tue, 2 Jul 2019 06:15:49 GMT"}, {"version": "v5", "created": "Fri, 5 Jul 2019 05:23:13 GMT"}], "update_date": "2019-07-08", "authors_parsed": [["Liang", "Junwei", ""], ["Aronson", "Jay D.", ""], ["Hauptmann", "Alexander", ""]]}, {"id": "1905.13315", "submitter": "Dong Li", "authors": "Dong Li, Qichao Zhang, Dongbin Zhao, Yuzheng Zhuang, Bin Wang, Wulong\n  Liu, Rasul Tutunov, Jun Wang", "title": "Graph Attention Memory for Visual Navigation", "comments": "13 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual navigation in complex environments is inefficient with traditional\nreactive policy or general-purposed recurrent policy. To address the long-term\nmemory issue, this paper proposes a graph attention memory (GAM) architecture\nconsisting of memory construction module, graph attention module and control\nmodule. The memory construction module builds the topological graph based on\nsupervised learning by taking the exploration prior. Then, guided attention\nfeatures are extracted with the graph attention module. Finally, the deep\nreinforcement learning based control module makes decisions based on visual\nobservations and guided attention features. Detailed convergence analysis of\nGAM is presented in this paper. We evaluate GAM-based navigation system in two\ncomplex 3D environments. Experimental results show that the GAM-based\nnavigation system significantly improves learning efficiency and outperforms\nall baselines in average success rate.\n", "versions": [{"version": "v1", "created": "Sat, 11 May 2019 08:00:54 GMT"}, {"version": "v2", "created": "Tue, 4 Jun 2019 13:40:49 GMT"}], "update_date": "2019-06-05", "authors_parsed": [["Li", "Dong", ""], ["Zhang", "Qichao", ""], ["Zhao", "Dongbin", ""], ["Zhuang", "Yuzheng", ""], ["Wang", "Bin", ""], ["Liu", "Wulong", ""], ["Tutunov", "Rasul", ""], ["Wang", "Jun", ""]]}, {"id": "1905.13339", "submitter": "Pranav Aggarwal", "authors": "Pranav Aggarwal, Zhe Lin, Baldo Faieta, Saeid Motiian", "title": "Multitask Text-to-Visual Embedding with Titles and Clickthrough Data", "comments": "4 pages. Language and Vision Workshop, in conjunction with CVPR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Text-visual (or called semantic-visual) embedding is a central problem in\nvision-language research. It typically involves mapping of an image and a text\ndescription to a common feature space through a CNN image encoder and a RNN\nlanguage encoder. In this paper, we propose a new method for learning\ntext-visual embedding using both image titles and click-through data from an\nimage search engine. We also propose a new triplet loss function by modeling\npositive awareness of the embedding, and introduce a novel mini-batch-based\nhard negative sampling approach for better data efficiency in the learning\nprocess. Experimental results show that our proposed method outperforms\nexisting methods, and is also effective for real-world text-to-visual\nretrieval.\n", "versions": [{"version": "v1", "created": "Thu, 30 May 2019 22:33:15 GMT"}], "update_date": "2019-06-03", "authors_parsed": [["Aggarwal", "Pranav", ""], ["Lin", "Zhe", ""], ["Faieta", "Baldo", ""], ["Motiian", "Saeid", ""]]}, {"id": "1905.13342", "submitter": "Pritish Uplavikar", "authors": "Pritish Uplavikar, Zhenyu Wu and Zhangyang Wang", "title": "All-In-One Underwater Image Enhancement using Domain-Adversarial\n  Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Raw underwater images are degraded due to wavelength dependent light\nattenuation and scattering, limiting their applicability in vision systems.\nAnother factor that makes enhancing underwater images particularly challenging\nis the diversity of the water types in which they are captured. For example,\nimages captured in deep oceanic waters have a different distribution from those\ncaptured in shallow coastal waters. Such diversity makes it hard to train a\nsingle model to enhance underwater images. In this work, we propose a novel\nmodel which nicely handles the diversity of water during the enhancement, by\nadversarially learning the content features of the images by disentangling the\nunwanted nuisances corresponding to water types (viewed as different domains).\nWe use the learned domain agnostic features to generate enhanced underwater\nimages. We train our model on a dataset consisting images of 10 Jerlov water\ntypes. Experimental results show that the proposed model not only outperforms\nthe previous methods in SSIM and PSNR scores for almost all Jerlov water types\nbut also generalizes well on real-world datasets. The performance of a\nhigh-level vision task (object detection) also shows improvement using enhanced\nimages with our model.\n", "versions": [{"version": "v1", "created": "Thu, 30 May 2019 22:42:05 GMT"}], "update_date": "2019-06-03", "authors_parsed": [["Uplavikar", "Pritish", ""], ["Wu", "Zhenyu", ""], ["Wang", "Zhangyang", ""]]}, {"id": "1905.13358", "submitter": "Vihan Jain", "authors": "Haoshuo Huang, Vihan Jain, Harsh Mehta, Jason Baldridge, Eugene Ie", "title": "Multi-modal Discriminative Model for Vision-and-Language Navigation", "comments": "Accepted at SpLU-RoboNLP 2019 (workshop at NAACL)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Vision-and-Language Navigation (VLN) is a natural language grounding task\nwhere agents have to interpret natural language instructions in the context of\nvisual scenes in a dynamic environment to achieve prescribed navigation goals.\nSuccessful agents must have the ability to parse natural language of varying\nlinguistic styles, ground them in potentially unfamiliar scenes, plan and react\nwith ambiguous environmental feedback. Generalization ability is limited by the\namount of human annotated data. In particular, \\emph{paired} vision-language\nsequence data is expensive to collect. We develop a discriminator that\nevaluates how well an instruction explains a given path in VLN task using\nmulti-modal alignment. Our study reveals that only a small fraction of the\nhigh-quality augmented data from \\citet{Fried:2018:Speaker}, as scored by our\ndiscriminator, is useful for training VLN agents with similar performance on\npreviously unseen environments. We also show that a VLN agent warm-started with\npre-trained components from the discriminator outperforms the benchmark success\nrates of 35.5 by 10\\% relative measure on previously unseen environments.\n", "versions": [{"version": "v1", "created": "Fri, 31 May 2019 00:07:24 GMT"}], "update_date": "2019-06-03", "authors_parsed": [["Huang", "Haoshuo", ""], ["Jain", "Vihan", ""], ["Mehta", "Harsh", ""], ["Baldridge", "Jason", ""], ["Ie", "Eugene", ""]]}, {"id": "1905.13382", "submitter": "Mingbao Lin", "authors": "Mingbao Lin, Rongrong Ji, Shen Chen, Feng Zheng, Xiaoshuai Sun,\n  Baochang Zhang, Liujuan Cao, Guodong Guo, Feiyue Huang", "title": "Supervised Online Hashing via Similarity Distribution Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Online hashing has attracted extensive research attention when facing\nstreaming data. Most online hashing methods, learning binary codes based on\npairwise similarities of training instances, fail to capture the semantic\nrelationship, and suffer from a poor generalization in large-scale applications\ndue to large variations. In this paper, we propose to model the similarity\ndistributions between the input data and the hashing codes, upon which a novel\nsupervised online hashing method, dubbed as Similarity Distribution based\nOnline Hashing (SDOH), is proposed, to keep the intrinsic semantic relationship\nin the produced Hamming space. Specifically, we first transform the discrete\nsimilarity matrix into a probability matrix via a Gaussian-based normalization\nto address the extremely imbalanced distribution issue. And then, we introduce\na scaling Student t-distribution to solve the challenging initialization\nproblem, and efficiently bridge the gap between the known and unknown\ndistributions. Lastly, we align the two distributions via minimizing the\nKullback-Leibler divergence (KL-diverence) with stochastic gradient descent\n(SGD), by which an intuitive similarity constraint is imposed to update hashing\nmodel on the new streaming data with a powerful generalizing ability to the\npast data. Extensive experiments on three widely-used benchmarks validate the\nsuperiority of the proposed SDOH over the state-of-the-art methods in the\nonline retrieval task.\n", "versions": [{"version": "v1", "created": "Fri, 31 May 2019 02:12:41 GMT"}], "update_date": "2019-06-03", "authors_parsed": [["Lin", "Mingbao", ""], ["Ji", "Rongrong", ""], ["Chen", "Shen", ""], ["Zheng", "Feng", ""], ["Sun", "Xiaoshuai", ""], ["Zhang", "Baochang", ""], ["Cao", "Liujuan", ""], ["Guo", "Guodong", ""], ["Huang", "Feiyue", ""]]}, {"id": "1905.13386", "submitter": "Kai Rothauge", "authors": "Kai Rothauge, Zhewei Yao, Zixi Hu, Michael W. Mahoney", "title": "Residual Networks as Nonlinear Systems: Stability Analysis using\n  Linearization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We regard pre-trained residual networks (ResNets) as nonlinear systems and\nuse linearization, a common method used in the qualitative analysis of\nnonlinear systems, to understand the behavior of the networks under small\nperturbations of the input images. We work with ResNet-56 and ResNet-110\ntrained on the CIFAR-10 data set. We linearize these networks at the level of\nresidual units and network stages, and the singular value decomposition is used\nin the stability analysis of these components. It is found that most of the\nsingular values of the linearizations of residual units are 1 and, in spite of\nthe fact that the linearizations depend directly on the activation maps, the\nsingular values differ only slightly for different input images. However,\nadjusting the scaling of the skip connection or the values of the weights in a\nresidual unit has a significant impact on the singular value distributions.\nInspection of how random and adversarial perturbations of input images\npropagate through the network reveals that there is a dramatic jump in the\nmagnitude of adversarial perturbations towards the end of the final stage of\nthe network that is not present in the case of random perturbations. We attempt\nto gain a better understanding of this phenomenon by projecting the\nperturbations onto singular vectors of the linearizations of the residual\nunits.\n", "versions": [{"version": "v1", "created": "Fri, 31 May 2019 02:44:28 GMT"}], "update_date": "2019-06-03", "authors_parsed": [["Rothauge", "Kai", ""], ["Yao", "Zhewei", ""], ["Hu", "Zixi", ""], ["Mahoney", "Michael W.", ""]]}, {"id": "1905.13388", "submitter": "Haonan Wang", "authors": "Haonan Wang, Jun Lin, Zhongfeng Wang", "title": "Design Light-weight 3D Convolutional Networks for Video Recognition\n  Temporal Residual, Fully Separable Block, and Fast Algorithm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep 3-dimensional (3D) Convolutional Network (ConvNet) has shown promising\nperformance on video recognition tasks because of its powerful spatio-temporal\ninformation fusion ability. However, the extremely intensive requirements on\nmemory access and computing power prohibit it from being used in\nresource-constrained scenarios, such as portable and edge devices. So in this\npaper, we first propose a two-stage Fully Separable Block (FSB) to\nsignificantly compress the model sizes of 3D ConvNets. Then a feature\nenhancement approach named Temporal Residual Gradient (TRG) is developed to\nimprove the performance of compressed model on video tasks, which provides\nhigher accuracy, faster convergency and better robustness. Moreover, in order\nto further decrease the computing workload, we propose a hybrid Fast Algorithm\n(hFA) to drastically reduce the computation complexity of convolutions. These\nmethods are effectively combined to design a light-weight and efficient ConvNet\nfor video recognition tasks. Experiments on the popular dataset report 2.3x\ncompression rate, 3.6x workload reduction, and 6.3% top-1 accuracy gain, over\nthe state-of-the-art SlowFast model, which is already a highly compact model.\nThe proposed methods also show good adaptability on traditional 3D ConvNet,\ndemonstrating 7.4x more compact model, 11.0x less workload, and 3.0% higher\naccuracy\n", "versions": [{"version": "v1", "created": "Fri, 31 May 2019 02:48:21 GMT"}], "update_date": "2019-06-03", "authors_parsed": [["Wang", "Haonan", ""], ["Lin", "Jun", ""], ["Wang", "Zhongfeng", ""]]}, {"id": "1905.13389", "submitter": "Qigong Sun", "authors": "Qigong Sun and Fanhua Shang and Kang Yang and Xiufang Li and Yan Ren\n  and Licheng Jiao", "title": "Multi-Precision Quantized Neural Networks via Encoding Decomposition of\n  -1 and +1", "comments": "9 pages, 2 figures, Proc. 33rd AAAI Conf. Artif. Intell., 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The training of deep neural networks (DNNs) requires intensive resources both\nfor computation and for storage performance. Thus, DNNs cannot be efficiently\napplied to mobile phones and embedded devices, which seriously limits their\napplicability in industry applications. To address this issue, we propose a\nnovel encoding scheme of using {-1,+1} to decompose quantized neural networks\n(QNNs) into multi-branch binary networks, which can be efficiently implemented\nby bitwise operations (xnor and bitcount) to achieve model compression,\ncomputational acceleration and resource saving. Based on our method, users can\neasily achieve different encoding precisions arbitrarily according to their\nrequirements and hardware resources. The proposed mechanism is very suitable\nfor the use of FPGA and ASIC in terms of data storage and computation, which\nprovides a feasible idea for smart chips. We validate the effectiveness of our\nmethod on both large-scale image classification tasks (e.g., ImageNet) and\nobject detection tasks. In particular, our method with low-bit encoding can\nstill achieve almost the same performance as its full-precision counterparts.\n", "versions": [{"version": "v1", "created": "Fri, 31 May 2019 02:49:08 GMT"}], "update_date": "2019-06-03", "authors_parsed": [["Sun", "Qigong", ""], ["Shang", "Fanhua", ""], ["Yang", "Kang", ""], ["Li", "Xiufang", ""], ["Ren", "Yan", ""], ["Jiao", "Licheng", ""]]}, {"id": "1905.13390", "submitter": "Yao Xiao", "authors": "Yao Xiao", "title": "Vehicle Detection in Deep Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computer vision is developing rapidly with the support of deep learning\ntechniques. This thesis proposes an advanced vehicle-detection model based on\nan improvement to classical convolutional neural networks. The advanced model\nwas applied against a vehicle detection benchmark and was built to detect\non-road objects. First, we propose a high-level architecture for our advanced\nmodel, which utilizes different state-of-the-art deep learning techniques.\nThen, we utilize the residual neural networks and region proposal network to\nachieve competitive performance according to the vehicle detection benchmark.\nLastly, we describe the developing trend of vehicle detection techniques and\nthe future direction of research.\n", "versions": [{"version": "v1", "created": "Wed, 29 May 2019 00:33:28 GMT"}], "update_date": "2019-06-03", "authors_parsed": [["Xiao", "Yao", ""]]}, {"id": "1905.13391", "submitter": "Shah Rukh Qasim", "authors": "Shah Rukh Qasim, Hassan Mahmood, and Faisal Shafait", "title": "Rethinking Table Recognition using Graph Neural Networks", "comments": "Accepted to ICDAR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Document structure analysis, such as zone segmentation and table recognition,\nis a complex problem in document processing and is an active area of research.\nThe recent success of deep learning in solving various computer vision and\nmachine learning problems has not been reflected in document structure analysis\nsince conventional neural networks are not well suited to the input structure\nof the problem. In this paper, we propose an architecture based on graph\nnetworks as a better alternative to standard neural networks for table\nrecognition. We argue that graph networks are a more natural choice for these\nproblems, and explore two gradient-based graph neural networks. Our proposed\narchitecture combines the benefits of convolutional neural networks for visual\nfeature extraction and graph networks for dealing with the problem structure.\nWe empirically demonstrate that our method outperforms the baseline by a\nsignificant margin. In addition, we identify the lack of large scale datasets\nas a major hindrance for deep learning research for structure analysis and\npresent a new large scale synthetic dataset for the problem of table\nrecognition. Finally, we open-source our implementation of dataset generation\nand the training framework of our graph networks to promote reproducible\nresearch in this direction.\n", "versions": [{"version": "v1", "created": "Fri, 31 May 2019 02:58:04 GMT"}, {"version": "v2", "created": "Wed, 3 Jul 2019 17:59:36 GMT"}], "update_date": "2019-07-04", "authors_parsed": [["Qasim", "Shah Rukh", ""], ["Mahmood", "Hassan", ""], ["Shafait", "Faisal", ""]]}, {"id": "1905.13392", "submitter": "V\\'ictor Manuel Vargas Yun", "authors": "V\\'ictor-Manuel Vargas and Pedro-Antonio Guti\\'errez and C\\'esar\n  Herv\\'as-Mart\\'inez", "title": "Cumulative link models for deep ordinal classification", "comments": "24 pages, 3 figures. Submitted to Neurocomputing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a deep convolutional neural network model for ordinal\nregression by considering a family of probabilistic ordinal link functions in\nthe output layer. The link functions are those used for cumulative link models,\nwhich are traditional statistical linear models based on projecting each\npattern into a 1-dimensional space. A set of ordered thresholds splits this\nspace into the different classes of the problem. In our case, the projections\nare estimated by a non-linear deep neural network. To further improve the\nresults, we combine these ordinal models with a loss function that takes into\naccount the distance between the categories, based on the weighted Kappa index.\nThree different link functions are studied in the experimental study, and the\nresults are contrasted with statistical analysis. The experiments run over two\ndifferent ordinal classification problems and the statistical tests confirm\nthat these models improve the results of a nominal model and outperform other\nrobust proposals considered in the literature.\n", "versions": [{"version": "v1", "created": "Mon, 27 May 2019 10:21:47 GMT"}, {"version": "v2", "created": "Thu, 10 Oct 2019 11:46:18 GMT"}], "update_date": "2019-10-11", "authors_parsed": [["Vargas", "V\u00edctor-Manuel", ""], ["Guti\u00e9rrez", "Pedro-Antonio", ""], ["Herv\u00e1s-Mart\u00ednez", "C\u00e9sar", ""]]}, {"id": "1905.13394", "submitter": "Yazhou Yao", "authors": "Huafeng Liu, Xiaofeng Han, Xiangrui Li, Yazhou Yao, Pu Huang, Zhenming\n  Tang", "title": "Deep Representation Learning for Road Detection through Siamese Network", "comments": "Accepted by Multimedia Tools and Applications", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Robust road detection is a key challenge in safe autonomous driving.\nRecently, with the rapid development of 3D sensors, more and more researchers\nare trying to fuse information across different sensors to improve the\nperformance of road detection. Although many successful works have been\nachieved in this field, methods for data fusion under deep learning framework\nis still an open problem. In this paper, we propose a Siamese deep neural\nnetwork based on FCN-8s to detect road region. Our method uses data collected\nfrom a monocular color camera and a Velodyne-64 LiDAR sensor. We project the\nLiDAR point clouds onto the image plane to generate LiDAR images and feed them\ninto one of the branches of the network. The RGB images are fed into another\nbranch of our proposed network. The feature maps that these two branches\nextract in multiple scales are fused before each pooling layer, via padding\nadditional fusion layers. Extensive experimental results on public dataset\nKITTI ROAD demonstrate the effectiveness of our proposed approach.\n", "versions": [{"version": "v1", "created": "Sun, 26 May 2019 19:44:10 GMT"}], "update_date": "2019-06-03", "authors_parsed": [["Liu", "Huafeng", ""], ["Han", "Xiaofeng", ""], ["Li", "Xiangrui", ""], ["Yao", "Yazhou", ""], ["Huang", "Pu", ""], ["Tang", "Zhenming", ""]]}, {"id": "1905.13417", "submitter": "Lin Song", "authors": "Lin Song, Shiwei Zhang, Gang Yu, Hongbin Sun", "title": "TACNet: Transition-Aware Context Network for Spatio-Temporal Action\n  Detection", "comments": "CVPR-2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current state-of-the-art approaches for spatio-temporal action detection have\nachieved impressive results but remain unsatisfactory for temporal extent\ndetection. The main reason comes from that, there are some ambiguous states\nsimilar to the real actions which may be treated as target actions even by a\nwell-trained network. In this paper, we define these ambiguous samples as\n\"transitional states\", and propose a Transition-Aware Context Network (TACNet)\nto distinguish transitional states. The proposed TACNet includes two main\ncomponents, i.e., temporal context detector and transition-aware classifier.\nThe temporal context detector can extract long-term context information with\nconstant time complexity by constructing a recurrent network. The\ntransition-aware classifier can further distinguish transitional states by\nclassifying action and transitional states simultaneously. Therefore, the\nproposed TACNet can substantially improve the performance of spatio-temporal\naction detection. We extensively evaluate the proposed TACNet on UCF101-24 and\nJ-HMDB datasets. The experimental results demonstrate that TACNet obtains\ncompetitive performance on JHMDB and significantly outperforms the\nstate-of-the-art methods on the untrimmed UCF101-24 in terms of both frame-mAP\nand video-mAP.\n", "versions": [{"version": "v1", "created": "Fri, 31 May 2019 05:14:39 GMT"}], "update_date": "2019-06-03", "authors_parsed": [["Song", "Lin", ""], ["Zhang", "Shiwei", ""], ["Yu", "Gang", ""], ["Sun", "Hongbin", ""]]}, {"id": "1905.13445", "submitter": "Zhuyang Xie", "authors": "Zhuyang Xie, Junzhou Chen, Bo Peng", "title": "Point Clouds Learning with Attention-based Graph Convolution Networks", "comments": "11 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Point clouds data, as one kind of representation of 3D objects, are the most\nprimitive output obtained by 3D sensors. Unlike 2D images, point clouds are\ndisordered and unstructured. Hence it is not straightforward to apply\nclassification techniques such as the convolution neural network to point\nclouds analysis directly. To solve this problem, we propose a novel network\nstructure, named Attention-based Graph Convolution Networks (AGCN), to extract\npoint clouds features. Taking the learning process as a message propagation\nbetween adjacent points, we introduce an attention mechanism to AGCN for\nanalyzing the relationships between local features of the points. In addition,\nwe introduce an additional global graph structure network to compensate for the\nrelative information of the individual points in the graph structure network.\nThe proposed network is also extended to an encoder-decoder structure for\nsegmentation tasks. Experimental results show that the proposed network can\nachieve state-of-the-art performance in both classification and segmentation\ntasks.\n", "versions": [{"version": "v1", "created": "Fri, 31 May 2019 07:10:12 GMT"}], "update_date": "2019-06-03", "authors_parsed": [["Xie", "Zhuyang", ""], ["Chen", "Junzhou", ""], ["Peng", "Bo", ""]]}, {"id": "1905.13456", "submitter": "Changhee Han", "authors": "Changhee Han, Leonardo Rundo, Ryosuke Araki, Yudai Nagano, Yujiro\n  Furukawa, Giancarlo Mauri, Hideki Nakayama, Hideaki Hayashi", "title": "Combining Noise-to-Image and Image-to-Image GANs: Brain MR Image\n  Augmentation for Tumor Detection", "comments": "12 pages, 7 figures, accepted to IEEE ACCESS", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional Neural Networks (CNNs) achieve excellent computer-assisted\ndiagnosis with sufficient annotated training data. However, most medical\nimaging datasets are small and fragmented. In this context, Generative\nAdversarial Networks (GANs) can synthesize realistic/diverse additional\ntraining images to fill the data lack in the real image distribution;\nresearchers have improved classification by augmenting data with noise-to-image\n(e.g., random noise samples to diverse pathological images) or image-to-image\nGANs (e.g., a benign image to a malignant one). Yet, no research has reported\nresults combining noise-to-image and image-to-image GANs for further\nperformance boost. Therefore, to maximize the DA effect with the GAN\ncombinations, we propose a two-step GAN-based DA that generates and refines\nbrain Magnetic Resonance (MR) images with/without tumors separately: (i)\nProgressive Growing of GANs (PGGANs), multi-stage noise-to-image GAN for\nhigh-resolution MR image generation, first generates realistic/diverse 256 X\n256 images; (ii) Multimodal UNsupervised Image-to-image Translation (MUNIT)\nthat combines GANs/Variational AutoEncoders or SimGAN that uses a DA-focused\nGAN loss, further refines the texture/shape of the PGGAN-generated images\nsimilarly to the real ones. We thoroughly investigate CNN-based tumor\nclassification results, also considering the influence of pre-training on\nImageNet and discarding weird-looking GAN-generated images. The results show\nthat, when combined with classic DA, our two-step GAN-based DA can\nsignificantly outperform the classic DA alone, in tumor detection (i.e.,\nboosting sensitivity 93.67% to 97.48%) and also in other medical imaging tasks.\n", "versions": [{"version": "v1", "created": "Fri, 31 May 2019 08:14:19 GMT"}, {"version": "v2", "created": "Thu, 19 Sep 2019 17:53:49 GMT"}, {"version": "v3", "created": "Wed, 9 Oct 2019 12:20:15 GMT"}], "update_date": "2019-10-10", "authors_parsed": [["Han", "Changhee", ""], ["Rundo", "Leonardo", ""], ["Araki", "Ryosuke", ""], ["Nagano", "Yudai", ""], ["Furukawa", "Yujiro", ""], ["Mauri", "Giancarlo", ""], ["Nakayama", "Hideki", ""], ["Hayashi", "Hideaki", ""]]}, {"id": "1905.13466", "submitter": "Mengxi Jiang", "authors": "Mengxi Jiang, Zhuliang Yu, Cuihua Li, Yunqi Lei", "title": "Joint Representation of Multiple Geometric Priors via a Shape\n  Decomposition Model for Single Monocular 3D Pose Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we aim to recover the 3D human pose from 2D body joints of a\nsingle image. The major challenge in this task is the depth ambiguity since\ndifferent 3D poses may produce similar 2D poses. Although many recent advances\nin this problem are found in both unsupervised and supervised learning\napproaches, the performances of most of these approaches are greatly affected\nby insufficient diversities and richness of training data. To alleviate this\nissue, we propose an unsupervised learning approach, which is capable of\nestimating various complex poses well under limited available training data.\nSpecifically, we propose a Shape Decomposition Model (SDM) in which a 3D pose\nis considered as the superposition of two parts which are global structure\ntogether with some deformations. Based on SDM, we estimate these two parts\nexplicitly by solving two sets of different distributed combination\ncoefficients of geometric priors. In addition, to obtain geometric priors, a\njoint dictionary learning algorithm is proposed to extract both coarse and fine\npose clues simultaneously from limited training data. Quantitative evaluations\non several widely used datasets demonstrate that our approach yields better\nperformances over other competitive approaches. Especially, on some categories\nwith more complex deformations, significant improvements are achieved by our\napproach. Furthermore, qualitative experiments conducted on in-the-wild images\nalso show the effectiveness of the proposed approach.\n", "versions": [{"version": "v1", "created": "Fri, 31 May 2019 08:47:05 GMT"}], "update_date": "2019-06-03", "authors_parsed": [["Jiang", "Mengxi", ""], ["Yu", "Zhuliang", ""], ["Li", "Cuihua", ""], ["Lei", "Yunqi", ""]]}, {"id": "1905.13523", "submitter": "Mohammed Brahimi", "authors": "Mohammed Brahimi, Said Mahmoudi, Kamel Boukhalfa, Abdelouhab Moussaoui", "title": "Deep interpretable architecture for plant diseases classification", "comments": "10 pages, 8 figures, Submitted to Signal Processing Algorithms,\n  Architectures, Arrangements and Applications (SPA2019),\n  https://github.com/Tahedi1/Teacher_Student_Architecture", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, many works have been inspired by the success of deep learning in\ncomputer vision for plant diseases classification. Unfortunately, these\nend-to-end deep classifiers lack transparency which can limit their adoption in\npractice. In this paper, we propose a new trainable visualization method for\nplant diseases classification based on a Convolutional Neural Network (CNN)\narchitecture composed of two deep classifiers. The first one is named Teacher\nand the second one Student. This architecture leverages the multitask learning\nto train the Teacher and the Student jointly. Then, the communicated\nrepresentation between the Teacher and the Student is used as a proxy to\nvisualize the most important image regions for classification. This new\narchitecture produces sharper visualization than the existing methods in plant\ndiseases context. All experiments are achieved on PlantVillage dataset that\ncontains 54306 plant images.\n", "versions": [{"version": "v1", "created": "Fri, 31 May 2019 11:41:16 GMT"}, {"version": "v2", "created": "Thu, 13 Jun 2019 15:32:53 GMT"}], "update_date": "2019-06-14", "authors_parsed": [["Brahimi", "Mohammed", ""], ["Mahmoudi", "Said", ""], ["Boukhalfa", "Kamel", ""], ["Moussaoui", "Abdelouhab", ""]]}, {"id": "1905.13533", "submitter": "Yantao Lu", "authors": "Yantao Lu, Senem Velipasalar", "title": "Autonomous Human Activity Classification from Ego-vision Camera and\n  Accelerometer Data", "comments": "Accepted for presentation at EPIC@CVPR2019 workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There has been significant amount of research work on human activity\nclassification relying either on Inertial Measurement Unit (IMU) data or data\nfrom static cameras providing a third-person view. Using only IMU data limits\nthe variety and complexity of the activities that can be detected. For\ninstance, the sitting activity can be detected by IMU data, but it cannot be\ndetermined whether the subject has sat on a chair or a sofa, or where the\nsubject is. To perform fine-grained activity classification from egocentric\nvideos, and to distinguish between activities that cannot be differentiated by\nonly IMU data, we present an autonomous and robust method using data from both\nego-vision cameras and IMUs. In contrast to convolutional neural network-based\napproaches, we propose to employ capsule networks to obtain features from\negocentric video data. Moreover, Convolutional Long Short Term Memory framework\nis employed both on egocentric videos and IMU data to capture temporal aspect\nof actions. We also propose a genetic algorithm-based approach to autonomously\nand systematically set various network parameters, rather than using manual\nsettings. Experiments have been performed to perform 9- and 26-label activity\nclassification, and the proposed method, using autonomously set network\nparameters, has provided very promising results, achieving overall accuracies\nof 86.6\\% and 77.2\\%, respectively. The proposed approach combining both\nmodalities also provides increased accuracy compared to using only egovision\ndata and only IMU data.\n", "versions": [{"version": "v1", "created": "Tue, 28 May 2019 22:56:41 GMT"}], "update_date": "2019-06-03", "authors_parsed": [["Lu", "Yantao", ""], ["Velipasalar", "Senem", ""]]}, {"id": "1905.13536", "submitter": "Christopher Canel", "authors": "Christopher Canel, Thomas Kim, Giulio Zhou, Conglong Li, Hyeontaek\n  Lim, David G. Andersen, Michael Kaminsky, Subramanya R. Dulloor", "title": "Scaling Video Analytics on Constrained Edge Nodes", "comments": "This paper is an extended version of a paper with the same title\n  published in the 2nd SysML Conference, SysML '19 (Canel et. al., 2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.PF eess.IV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As video camera deployments continue to grow, the need to process large\nvolumes of real-time data strains wide area network infrastructure. When\nper-camera bandwidth is limited, it is infeasible for applications such as\ntraffic monitoring and pedestrian tracking to offload high-quality video\nstreams to a datacenter. This paper presents FilterForward, a new edge-to-cloud\nsystem that enables datacenter-based applications to process content from\nthousands of cameras by installing lightweight edge filters that backhaul only\nrelevant video frames. FilterForward introduces fast and expressive\nper-application microclassifiers that share computation to simultaneously\ndetect dozens of events on computationally constrained edge nodes. Only\nmatching events are transmitted to the cloud. Evaluation on two real-world\ncamera feed datasets shows that FilterForward reduces bandwidth use by an order\nof magnitude while improving computational efficiency and event detection\naccuracy for challenging video content.\n", "versions": [{"version": "v1", "created": "Fri, 24 May 2019 23:11:07 GMT"}], "update_date": "2019-06-03", "authors_parsed": [["Canel", "Christopher", ""], ["Kim", "Thomas", ""], ["Zhou", "Giulio", ""], ["Li", "Conglong", ""], ["Lim", "Hyeontaek", ""], ["Andersen", "David G.", ""], ["Kaminsky", "Michael", ""], ["Dulloor", "Subramanya R.", ""]]}, {"id": "1905.13538", "submitter": "Guillaume Jaume", "authors": "Guillaume Jaume, Hazim Kemal Ekenel, Jean-Philippe Thiran", "title": "FUNSD: A Dataset for Form Understanding in Noisy Scanned Documents", "comments": "ICDAR'19 OST workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new dataset for form understanding in noisy scanned documents\n(FUNSD) that aims at extracting and structuring the textual content of forms.\nThe dataset comprises 199 real, fully annotated, scanned forms. The documents\nare noisy and vary widely in appearance, making form understanding (FoUn) a\nchallenging task. The proposed dataset can be used for various tasks, including\ntext detection, optical character recognition, spatial layout analysis, and\nentity labeling/linking. To the best of our knowledge, this is the first\npublicly available dataset with comprehensive annotations to address FoUn task.\nWe also present a set of baselines and introduce metrics to evaluate\nperformance on the FUNSD dataset, which can be downloaded at\nhttps://guillaumejaume.github.io/FUNSD/.\n", "versions": [{"version": "v1", "created": "Mon, 27 May 2019 10:40:40 GMT"}, {"version": "v2", "created": "Tue, 29 Oct 2019 15:46:39 GMT"}], "update_date": "2019-10-30", "authors_parsed": [["Jaume", "Guillaume", ""], ["Ekenel", "Hazim Kemal", ""], ["Thiran", "Jean-Philippe", ""]]}, {"id": "1905.13539", "submitter": "Mickael Chen", "authors": "Micka\\\"el Chen, Thierry Arti\\`eres, Ludovic Denoyer", "title": "Unsupervised Object Segmentation by Redrawing", "comments": "Presented at NeurIPS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Object segmentation is a crucial problem that is usually solved by using\nsupervised learning approaches over very large datasets composed of both images\nand corresponding object masks. Since the masks have to be provided at pixel\nlevel, building such a dataset for any new domain can be very time-consuming.\nWe present ReDO, a new model able to extract objects from images without any\nannotation in an unsupervised way. It relies on the idea that it should be\npossible to change the textures or colors of the objects without changing the\noverall distribution of the dataset. Following this assumption, our approach is\nbased on an adversarial architecture where the generator is guided by an input\nsample: given an image, it extracts the object mask, then redraws a new object\nat the same location. The generator is controlled by a discriminator that\nensures that the distribution of generated images is aligned to the original\none. We experiment with this method on different datasets and demonstrate the\ngood quality of extracted masks.\n", "versions": [{"version": "v1", "created": "Mon, 27 May 2019 12:34:55 GMT"}, {"version": "v2", "created": "Sat, 26 Oct 2019 17:33:04 GMT"}, {"version": "v3", "created": "Tue, 29 Oct 2019 16:38:18 GMT"}, {"version": "v4", "created": "Fri, 29 Nov 2019 17:00:48 GMT"}], "update_date": "2019-12-02", "authors_parsed": [["Chen", "Micka\u00ebl", ""], ["Arti\u00e8res", "Thierry", ""], ["Denoyer", "Ludovic", ""]]}, {"id": "1905.13540", "submitter": "Junyeong Kim", "authors": "Junyeong Kim, Minuk Ma, Kyungsu Kim, Sungjin Kim, Chang D. Yoo", "title": "Gaining Extra Supervision via Multi-task learning for Multi-Modal Video\n  Question Answering", "comments": "Accepted to IJCNN2019, oral", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a method to gain extra supervision via multi-task\nlearning for multi-modal video question answering. Multi-modal video question\nanswering is an important task that aims at the joint understanding of vision\nand language. However, establishing large scale dataset for multi-modal video\nquestion answering is expensive and the existing benchmarks are relatively\nsmall to provide sufficient supervision. To overcome this challenge, this paper\nproposes a multi-task learning method which is composed of three main\ncomponents: (1) multi-modal video question answering network that answers the\nquestion based on the both video and subtitle feature, (2) temporal retrieval\nnetwork that predicts the time in the video clip where the question was\ngenerated from and (3) modality alignment network that solves metric learning\nproblem to find correct association of video and subtitle modalities. By\nsimultaneously solving related auxiliary tasks with hierarchically shared\nintermediate layers, the extra synergistic supervisions are provided. Motivated\nby curriculum learning, multi task ratio scheduling is proposed to learn easier\ntask earlier to set inductive bias at the beginning of the training. The\nexperiments on publicly available dataset TVQA shows state-of-the-art results,\nand ablation studies are conducted to prove the statistical validity.\n", "versions": [{"version": "v1", "created": "Tue, 28 May 2019 01:46:20 GMT"}], "update_date": "2019-06-03", "authors_parsed": [["Kim", "Junyeong", ""], ["Ma", "Minuk", ""], ["Kim", "Kyungsu", ""], ["Kim", "Sungjin", ""], ["Yoo", "Chang D.", ""]]}, {"id": "1905.13543", "submitter": "Xiawu Zheng", "authors": "Xiawu Zheng, Rongrong Ji, Lang Tang, Yan Wan, Baochang Zhang, Yongjian\n  Wu, Yunsheng Wu, Ling Shao", "title": "Dynamic Distribution Pruning for Efficient Network Architecture Search", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Network architectures obtained by Neural Architecture Search (NAS) have shown\nstate-of-the-art performance in various computer vision tasks. Despite the\nexciting progress, the computational complexity of the forward-backward\npropagation and the search process makes it difficult to apply NAS in practice.\nIn particular, most previous methods require thousands of GPU days for the\nsearch process to converge. In this paper, we propose a dynamic distribution\npruning method towards extremely efficient NAS, which samples architectures\nfrom a joint categorical distribution. The search space is dynamically pruned\nevery a few epochs to update this distribution, and the optimal neural\narchitecture is obtained when there is only one structure remained. We conduct\nexperiments on two widely-used datasets in NAS. On CIFAR-10, the optimal\nstructure obtained by our method achieves the state-of-the-art $1.9$\\% test\nerror, while the search process is more than $1,000$ times faster (only $1.5$\nGPU hours on a Tesla V100) than the state-of-the-art NAS algorithms. On\nImageNet, our model achieves 75.2\\% top-1 accuracy under the MobileNet\nsettings, with a time cost of only $2$ GPU days that is $100\\%$ acceleration\nover the fastest NAS algorithm. The code is available at \\url{\nhttps://github.com/tanglang96/DDPNAS}\n", "versions": [{"version": "v1", "created": "Tue, 28 May 2019 06:35:52 GMT"}, {"version": "v2", "created": "Sun, 9 Jun 2019 11:42:41 GMT"}], "update_date": "2019-06-11", "authors_parsed": [["Zheng", "Xiawu", ""], ["Ji", "Rongrong", ""], ["Tang", "Lang", ""], ["Wan", "Yan", ""], ["Zhang", "Baochang", ""], ["Wu", "Yongjian", ""], ["Wu", "Yunsheng", ""], ["Shao", "Ling", ""]]}, {"id": "1905.13545", "submitter": "Haohan Wang", "authors": "Haohan Wang, Xindi Wu, Zeyi Huang, and Eric P. Xing", "title": "High Frequency Component Helps Explain the Generalization of\n  Convolutional Neural Networks", "comments": "To appear at CVPR 2020 as an Oral paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the relationship between the frequency spectrum of image data\nand the generalization behavior of convolutional neural networks (CNN). We\nfirst notice CNN's ability in capturing the high-frequency components of\nimages. These high-frequency components are almost imperceptible to a human.\nThus the observation leads to multiple hypotheses that are related to the\ngeneralization behaviors of CNN, including a potential explanation for\nadversarial examples, a discussion of CNN's trade-off between robustness and\naccuracy, and some evidence in understanding training heuristics.\n", "versions": [{"version": "v1", "created": "Tue, 28 May 2019 19:42:04 GMT"}, {"version": "v2", "created": "Sun, 5 Jan 2020 11:57:59 GMT"}, {"version": "v3", "created": "Tue, 24 Mar 2020 20:22:59 GMT"}], "update_date": "2020-03-26", "authors_parsed": [["Wang", "Haohan", ""], ["Wu", "Xindi", ""], ["Huang", "Zeyi", ""], ["Xing", "Eric P.", ""]]}, {"id": "1905.13546", "submitter": "Oliver Struckmeier", "authors": "Oliver Struckmeier", "title": "LeagueAI: Improving object detector performance and flexibility through\n  automatically generated training data and domain randomization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this technical report I present my method for automatic synthetic dataset\ngeneration for object detection and demonstrate it on the video game League of\nLegends. This report furthermore serves as a handbook on how to automatically\ngenerate datasets and as an introduction on the dataset generation part of the\nLeagueAI framework. The LeagueAI framework is a software framework that\nprovides detailed information about the game League of Legends based on the\nsame input a human player would have, namely vision. The framework allows\nresearchers and enthusiasts to develop their own intelligent agents or to\nextract detailed information about the state of the game. A big problem of\nmachine vision applications usually is the laborious work of gathering large\namounts of hand labeled data. Thus, a crucial part of the vision pipeline of\nthe LeagueAI framework, the dataset generation, is presented in this report.\nThe method involves extracting image raw data from the game's 3D models and\ncombining them with the game background to create game-like synthetic images\nand to generate the corresponding labels automatically. In an experiment I\ncompared a model trained on synthetic data to a model trained on hand labeled\ndata and a model trained on a combined dataset. The model trained on the\nsynthetic data showed higher detection precision on more classes and more\nreliable tracking performance of the player character. The model trained on the\ncombined dataset did not perform better because of the different formats of the\nolder hand labeled dataset and the synthetic data.\n", "versions": [{"version": "v1", "created": "Tue, 28 May 2019 21:07:22 GMT"}], "update_date": "2019-06-03", "authors_parsed": [["Struckmeier", "Oliver", ""]]}, {"id": "1905.13549", "submitter": "Haohan Wang", "authors": "Haohan Wang, Songwei Ge, Eric P. Xing, and Zachary C. Lipton", "title": "Learning Robust Global Representations by Penalizing Local Predictive\n  Power", "comments": null, "journal-ref": "NeurIPS 2019", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite their renowned predictive power on i.i.d. data, convolutional neural\nnetworks are known to rely more on high-frequency patterns that humans deem\nsuperficial than on low-frequency patterns that agree better with intuitions\nabout what constitutes category membership. This paper proposes a method for\ntraining robust convolutional networks by penalizing the predictive power of\nthe local representations learned by earlier layers. Intuitively, our networks\nare forced to discard predictive signals such as color and texture that can be\ngleaned from local receptive fields and to rely instead on the global\nstructures of the image. Across a battery of synthetic and benchmark domain\nadaptation tasks, our method confers improved generalization out of the domain.\nAlso, to evaluate cross-domain transfer, we introduce ImageNet-Sketch, a new\ndataset consisting of sketch-like images, that matches the ImageNet\nclassification validation set in categories and scale.\n", "versions": [{"version": "v1", "created": "Wed, 29 May 2019 00:27:54 GMT"}, {"version": "v2", "created": "Tue, 5 Nov 2019 03:56:06 GMT"}], "update_date": "2019-11-06", "authors_parsed": [["Wang", "Haohan", ""], ["Ge", "Songwei", ""], ["Xing", "Eric P.", ""], ["Lipton", "Zachary C.", ""]]}, {"id": "1905.13555", "submitter": "Tony Lindeberg", "authors": "Tony Lindeberg", "title": "Provably scale-covariant continuous hierarchical networks based on\n  scale-normalized differential expressions coupled in cascade", "comments": "29 pages, 16 figures, 3 tables. arXiv admin note: substantial text\n  overlap with arXiv:1903.00289", "journal-ref": "Journal of Mathematical Imaging and Vision, 62(1): 120-148, 2020", "doi": "10.1007/s10851-019-00915-x", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article presents a theory for constructing hierarchical networks in such\na way that the networks are guaranteed to be provably scale covariant. We first\npresent a general sufficiency argument for obtaining scale covariance, which\nholds for a wide class of networks defined from linear and non-linear\ndifferential expressions expressed in terms of scale-normalized scale-space\nderivatives. Then, we present a more detailed development of one example of\nsuch a network constructed from a combination of mathematically derived models\nof receptive fields and biologically inspired computations. Based on a\nfunctional model of complex cells in terms of an oriented quasi quadrature\ncombination of first- and second-order directional Gaussian derivatives, we\ncouple such primitive computations in cascade over combinatorial expansions\nover image orientations. Scale-space properties of the computational primitives\nare analysed and we give explicit proofs of how the resulting representation\nallows for scale and rotation covariance. A prototype application to texture\nanalysis is developed and it is demonstrated that a simplified mean-reduced\nrepresentation of the resulting QuasiQuadNet leads to promising experimental\nresults on three texture datasets.\n", "versions": [{"version": "v1", "created": "Wed, 29 May 2019 18:02:48 GMT"}, {"version": "v2", "created": "Sat, 14 Sep 2019 07:18:56 GMT"}, {"version": "v3", "created": "Mon, 28 Oct 2019 11:55:43 GMT"}], "update_date": "2021-05-20", "authors_parsed": [["Lindeberg", "Tony", ""]]}, {"id": "1905.13560", "submitter": "Xing Liu", "authors": "Xing Liu, Takayuki Okatani", "title": "Evaluating Artificial Systems for Pairwise Ranking Tasks Sensitive to\n  Individual Differences", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Owing to the advancement of deep learning, artificial systems are now rival\nto humans in several pattern recognition tasks, such as visual recognition of\nobject categories. However, this is only the case with the tasks for which\ncorrect answers exist independent of human perception. There is another type of\ntasks for which what to predict is human perception itself, in which there are\noften individual differences. Then, there are no longer single \"correct\"\nanswers to predict, which makes evaluation of artificial systems difficult. In\nthis paper, focusing on pairwise ranking tasks sensitive to individual\ndifferences, we propose an evaluation method. Given a ranking result for\nmultiple item pairs that is generated by an artificial system, our method\nquantifies the probability that the same ranking result will be generated by\nhumans, and judges if it is distinguishable from human-generated results. We\nintroduce a probabilistic model of human ranking behavior, and present an\nefficient computation method for the judgment. To estimate model parameters\naccurately from small-size samples, we present a method that uses confidence\nscores given by annotators for ranking each item pair. Taking as an example a\ntask of ranking image pairs according to material attributes of objects, we\ndemonstrate how the proposed method works.\n", "versions": [{"version": "v1", "created": "Thu, 30 May 2019 00:13:16 GMT"}], "update_date": "2019-06-03", "authors_parsed": [["Liu", "Xing", ""], ["Okatani", "Takayuki", ""]]}, {"id": "1905.13575", "submitter": "John Armstrong", "authors": "John A. Armstrong and Lyndsay Fletcher", "title": "Fast Solar Image Classification Using Deep Learning and its Importance\n  for Automation in Solar Physics", "comments": "19 pages, 9 figures, accepted for publication in Solar Physics", "journal-ref": null, "doi": "10.1007/s11207-019-1473-z", "report-no": null, "categories": "astro-ph.SR astro-ph.IM cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The volume of data being collected in solar physics has exponentially\nincreased over the past decade and with the introduction of the $\\textit{Daniel\nK. Inouye Solar Telescope}$ (DKIST) we will be entering the age of petabyte\nsolar data. Automated feature detection will be an invaluable tool for\npost-processing of solar images to create catalogues of data ready for\nresearchers to use. We propose a deep learning model to accomplish this; a deep\nconvolutional neural network is adept at feature extraction and processing\nimages quickly. We train our network using data from $\\textit{Hinode/Solar\nOptical Telescope}$ (SOT) H$\\alpha$ images of a small subset of solar features\nwith different geometries: filaments, prominences, flare ribbons, sunspots and\nthe quiet Sun ($\\textit{i.e.}$ the absence of any of the other four features).\nWe achieve near perfect performance on classifying unseen images from SOT\n($\\approx$99.9\\%) in 4.66 seconds. We also for the first time explore transfer\nlearning in a solar context. Transfer learning uses pre-trained deep neural\nnetworks to help train new deep learning models $\\textit{i.e.}$ it teaches a\nnew model. We show that our network is robust to changes in resolution by\ndegrading images from SOT resolution ($\\approx$0.33$^{\\prime \\prime}$ at\n$\\lambda$=6563\\AA{}) to $\\textit{Solar Dynamics Observatory/Atmospheric Imaging\nAssembly}$ (SDO/AIA) resolution ($\\approx$1.2$^{\\prime \\prime}$) without a\nchange in performance of our network. However, we also observe where the\nnetwork fails to generalise to sunspots from SDO/AIA bands 1600/1700\\AA{} due\nto small-scale brightenings around the sunspots and prominences in SDO/AIA\n304\\AA{} due to coronal emission.\n", "versions": [{"version": "v1", "created": "Fri, 31 May 2019 12:27:55 GMT"}], "update_date": "2019-07-10", "authors_parsed": [["Armstrong", "John A.", ""], ["Fletcher", "Lyndsay", ""]]}, {"id": "1905.13586", "submitter": "Haoxin Li", "authors": "Haoxin Li, Yijun Cai, Wei-Shi Zheng", "title": "Deep Dual Relation Modeling for Egocentric Interaction Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Egocentric interaction recognition aims to recognize the camera wearer's\ninteractions with the interactor who faces the camera wearer in egocentric\nvideos. In such a human-human interaction analysis problem, it is crucial to\nexplore the relations between the camera wearer and the interactor. However,\nmost existing works directly model the interactions as a whole and lack\nmodeling the relations between the two interacting persons. To exploit the\nstrong relations for egocentric interaction recognition, we introduce a dual\nrelation modeling framework which learns to model the relations between the\ncamera wearer and the interactor based on the individual action representations\nof the two persons. Specifically, we develop a novel interactive LSTM module,\nthe key component of our framework, to explicitly model the relations between\nthe two interacting persons based on their individual action representations,\nwhich are collaboratively learned with an interactor attention module and a\nglobal-local motion module. Experimental results on three egocentric\ninteraction datasets show the effectiveness of our method and advantage over\nstate-of-the-arts.\n", "versions": [{"version": "v1", "created": "Fri, 31 May 2019 12:49:25 GMT"}], "update_date": "2019-06-03", "authors_parsed": [["Li", "Haoxin", ""], ["Cai", "Yijun", ""], ["Zheng", "Wei-Shi", ""]]}, {"id": "1905.13594", "submitter": "Shuming Jiao", "authors": "Shuming Jiao, Yang Gao, Ting Lei, Zhenwei Xie, Xiaocong Yuan", "title": "Known-plaintext attack and ciphertext-only attack for encrypted\n  single-pixel imaging", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many previous works, a single-pixel imaging (SPI) system is constructed as\nan optical image encryption system. Unauthorized users are not able to\nreconstruct the plaintext image from the ciphertext intensity sequence without\nknowing the illumination pattern key. However, little cryptanalysis about\nencrypted SPI has been investigated in the past. In this work, we propose a\nknown-plaintext attack scheme and a ciphertext-only attack scheme to an\nencrypted SPI system for the first time. The known-plaintext attack is\nimplemented by interchanging the roles of illumination patterns and object\nimages in the SPI model. The ciphertext-only attack is implemented based on the\nstatistical features of single-pixel intensity values. The two schemes can\ncrack encrypted SPI systems and successfully recover the key containing correct\nillumination patterns.\n", "versions": [{"version": "v1", "created": "Fri, 31 May 2019 13:01:22 GMT"}], "update_date": "2019-06-03", "authors_parsed": [["Jiao", "Shuming", ""], ["Gao", "Yang", ""], ["Lei", "Ting", ""], ["Xie", "Zhenwei", ""], ["Yuan", "Xiaocong", ""]]}, {"id": "1905.13607", "submitter": "Richard Jiang", "authors": "Gary Storey, Richard Jiang, Shelagh Keogh, Ahmed Bouridane and\n  Chang-Tsun Li", "title": "3DPalsyNet: A Facial Palsy Grading and Motion Recognition Framework\n  using Fully 3D Convolutional Neural Networks", "comments": null, "journal-ref": "IEEE Access 2019", "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.GR cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The capability to perform facial analysis from video sequences has\nsignificant potential to positively impact in many areas of life. One such area\nrelates to the medical domain to specifically aid in the diagnosis and\nrehabilitation of patients with facial palsy. With this application in mind,\nthis paper presents an end-to-end framework, named 3DPalsyNet, for the tasks of\nmouth motion recognition and facial palsy grading. 3DPalsyNet utilizes a 3D CNN\narchitecture with a ResNet backbone for the prediction of these dynamic tasks.\nLeveraging transfer learning from a 3D CNNs pre-trained on the Kinetics data\nset for general action recognition, the model is modified to apply joint\nsupervised learning using center and softmax loss concepts. 3DPalsyNet is\nevaluated on a test set consisting of individuals with varying ranges of facial\npalsy and mouth motions and the results have shown an attractive level of\nclassification accuracy in these task of 82% and 86% respectively. The frame\nduration and the loss function affect was studied in terms of the predictive\nqualities of the proposed 3DPalsyNet, where it was found shorter frame\nduration's of 8 performed best for this specific task. Centre loss and softmax\nhave shown improvements in spatio-temporal feature learning than softmax loss\nalone, this is in agreement with earlier work involving the spatial domain.\n", "versions": [{"version": "v1", "created": "Fri, 31 May 2019 13:24:30 GMT"}], "update_date": "2019-07-30", "authors_parsed": [["Storey", "Gary", ""], ["Jiang", "Richard", ""], ["Keogh", "Shelagh", ""], ["Bouridane", "Ahmed", ""], ["Li", "Chang-Tsun", ""]]}, {"id": "1905.13613", "submitter": "Arnout Devos", "authors": "Arnout Devos, Matthias Grossglauser", "title": "Regression Networks for Meta-Learning Few-Shot Classification", "comments": "7th ICML Workshop on Automated Machine Learning (2020)", "journal-ref": "ICML Workshop on Automated Machine Learning (2020)", "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose regression networks for the problem of few-shot classification,\nwhere a classifier must generalize to new classes not seen in the training set,\ngiven only a small number of examples of each class. In high dimensional\nembedding spaces the direction of data generally contains richer information\nthan magnitude. Next to this, state-of-the-art few-shot metric methods that\ncompare distances with aggregated class representations, have shown superior\nperformance. Combining these two insights, we propose to meta-learn\nclassification of embedded points by regressing the closest approximation in\nevery class subspace while using the regression error as a distance metric.\nSimilarly to recent approaches for few-shot learning, regression networks\nreflect a simple inductive bias that is beneficial in this limited-data regime\nand they achieve excellent results, especially when more aggregate class\nrepresentations can be formed with multiple shots.\n", "versions": [{"version": "v1", "created": "Fri, 31 May 2019 13:35:41 GMT"}, {"version": "v2", "created": "Thu, 18 Jun 2020 20:09:01 GMT"}], "update_date": "2020-06-22", "authors_parsed": [["Devos", "Arnout", ""], ["Grossglauser", "Matthias", ""]]}, {"id": "1905.13628", "submitter": "Tailai Wen", "authors": "Tailai Wen, Roy Keyes", "title": "Time Series Anomaly Detection Using Convolutional Neural Networks and\n  Transfer Learning", "comments": "8 pages, 8 figures, AI for Internet of Things Workshop in IJCAI 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Time series anomaly detection plays a critical role in automated monitoring\nsystems. Most previous deep learning efforts related to time series anomaly\ndetection were based on recurrent neural networks (RNN). In this paper, we\npropose a time series segmentation approach based on convolutional neural\nnetworks (CNN) for anomaly detection. Moreover, we propose a transfer learning\nframework that pretrains a model on a large-scale synthetic univariate time\nseries data set and then fine-tunes its weights on small-scale, univariate or\nmultivariate data sets with previously unseen classes of anomalies. For the\nmultivariate case, we introduce a novel network architecture. The approach was\ntested on multiple synthetic and real data sets successfully.\n", "versions": [{"version": "v1", "created": "Fri, 31 May 2019 14:12:13 GMT"}], "update_date": "2019-06-03", "authors_parsed": [["Wen", "Tailai", ""], ["Keyes", "Roy", ""]]}, {"id": "1905.13648", "submitter": "Ali Furkan Biten", "authors": "Ali Furkan Biten, Ruben Tito, Andres Mafla, Lluis Gomez, Mar\\c{c}al\n  Rusi\\~nol, Ernest Valveny, C.V. Jawahar, Dimosthenis Karatzas", "title": "Scene Text Visual Question Answering", "comments": "International Conference on Computer Vision (ICCV 2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current visual question answering datasets do not consider the rich semantic\ninformation conveyed by text within an image. In this work, we present a new\ndataset, ST-VQA, that aims to highlight the importance of exploiting high-level\nsemantic information present in images as textual cues in the VQA process. We\nuse this dataset to define a series of tasks of increasing difficulty for which\nreading the scene text in the context provided by the visual information is\nnecessary to reason and generate an appropriate answer. We propose a new\nevaluation metric for these tasks to account both for reasoning errors as well\nas shortcomings of the text recognition module. In addition we put forward a\nseries of baseline methods, which provide further insight to the newly released\ndataset, and set the scene for further research.\n", "versions": [{"version": "v1", "created": "Fri, 31 May 2019 14:47:55 GMT"}, {"version": "v2", "created": "Wed, 16 Oct 2019 13:54:22 GMT"}], "update_date": "2019-10-17", "authors_parsed": [["Biten", "Ali Furkan", ""], ["Tito", "Ruben", ""], ["Mafla", "Andres", ""], ["Gomez", "Lluis", ""], ["Rusi\u00f1ol", "Mar\u00e7al", ""], ["Valveny", "Ernest", ""], ["Jawahar", "C. V.", ""], ["Karatzas", "Dimosthenis", ""]]}, {"id": "1905.13653", "submitter": "Aleksei Shestov", "authors": "Aleksei Shestov and Mikhail Kumskov", "title": "A Riemanian Approach to Blob Detection in Manifold-Valued Images", "comments": "Published in GSI 2017 proceedings", "journal-ref": "Third International Conference, GSI 2017, Paris, France, November\n  7-9, 2017, Proceedings, pp. 727-736", "doi": "10.1007/978-3-319-68445-1", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper is devoted to the problem of blob detection in manifold-valued\nimages. Our solution is based on new definitions of blob response functions. We\ndefine the blob response functions by means of curvatures of an image graph,\nconsidered as a submanifold. We call the proposed framework Riemannian blob\ndetection. We prove that our approach can be viewed as a generalization of the\ngrayscale blob detection technique. An expression of the Riemannian blob\nresponse functions through the image Hessian is derived. We provide experiments\nfor the case of vector-valued images on 2D surfaces: the proposed framework is\ntested on the task of chemical compounds classification.\n", "versions": [{"version": "v1", "created": "Fri, 31 May 2019 14:53:44 GMT"}], "update_date": "2019-06-03", "authors_parsed": [["Shestov", "Aleksei", ""], ["Kumskov", "Mikhail", ""]]}, {"id": "1905.13667", "submitter": "Jeffrey Ede BSc MPhys", "authors": "Jeffrey M. Ede, Richard Beanland", "title": "Partial Scanning Transmission Electron Microscopy with Deep Learning", "comments": "20 pages, 11 figures", "journal-ref": "Sci Rep 10, 8332 (2020)", "doi": "10.1038/s41598-020-65261-0", "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Compressed sensing algorithms are used to decrease electron microscope scan\ntime and electron beam exposure with minimal information loss. Following\nsuccessful applications of deep learning to compressed sensing, we have\ndeveloped a two-stage multiscale generative adversarial neural network to\ncomplete realistic 512$\\times$512 scanning transmission electron micrographs\nfrom spiral, jittered gridlike, and other partial scans. For spiral scans and\nmean squared error based pre-training, this enables electron beam coverage to\nbe decreased by 17.9$\\times$ with a 3.8\\% test set root mean squared intensity\nerror, and by 87.0$\\times$ with a 6.2\\% error. Our generator networks are\ntrained on partial scans created from a new dataset of 16227 scanning\ntransmission electron micrographs. High performance is achieved with adaptive\nlearning rate clipping of loss spikes and an auxiliary trainer network. Our\nsource code, new dataset, and pre-trained models have been made publicly\navailable at https://github.com/Jeffrey-Ede/partial-STEM\n", "versions": [{"version": "v1", "created": "Fri, 31 May 2019 15:13:32 GMT"}, {"version": "v2", "created": "Fri, 14 Feb 2020 11:50:23 GMT"}], "update_date": "2020-05-21", "authors_parsed": [["Ede", "Jeffrey M.", ""], ["Beanland", "Richard", ""]]}, {"id": "1905.13694", "submitter": "Charles Ringer", "authors": "Charles Ringer, and James Alfred Walker, and Mihalis A. Nicolaou", "title": "Multimodal Joint Emotion and Game Context Recognition in League of\n  Legends Livestreams", "comments": "8 Pages, IEEE Conference on Games 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video game streaming provides the viewer with a rich set of audio-visual\ndata, conveying information both with regards to the game itself, through game\nfootage and audio, as well as the streamer's emotional state and behaviour via\nwebcam footage and audio. Analysing player behaviour and discovering\ncorrelations with game context is crucial for modelling and understanding\nimportant aspects of livestreams, but comes with a significant set of\nchallenges - such as fusing multimodal data captured by different sensors in\nuncontrolled ('in-the-wild') conditions. Firstly, we present, to our knowledge,\nthe first data set of League of Legends livestreams, annotated for both\nstreamer affect and game context. Secondly, we propose a method that exploits\ntensor decompositions for high-order fusion of multimodal representations. The\nproposed method is evaluated on the problem of jointly predicting game context\nand player affect, compared with a set of baseline fusion approaches such as\nlate and early fusion.\n", "versions": [{"version": "v1", "created": "Fri, 31 May 2019 16:12:24 GMT"}], "update_date": "2019-06-03", "authors_parsed": [["Ringer", "Charles", ""], ["Walker", "James Alfred", ""], ["Nicolaou", "Mihalis A.", ""]]}, {"id": "1905.13725", "submitter": "Jonathan Uesato", "authors": "Jonathan Uesato, Jean-Baptiste Alayrac, Po-Sen Huang, Robert\n  Stanforth, Alhussein Fawzi, Pushmeet Kohli", "title": "Are Labels Required for Improving Adversarial Robustness?", "comments": "Appears in the Thirty-Third Annual Conference on Neural Information\n  Processing Systems (NeurIPS 2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent work has uncovered the interesting (and somewhat surprising) finding\nthat training models to be invariant to adversarial perturbations requires\nsubstantially larger datasets than those required for standard classification.\nThis result is a key hurdle in the deployment of robust machine learning models\nin many real world applications where labeled data is expensive. Our main\ninsight is that unlabeled data can be a competitive alternative to labeled data\nfor training adversarially robust models. Theoretically, we show that in a\nsimple statistical setting, the sample complexity for learning an adversarially\nrobust model from unlabeled data matches the fully supervised case up to\nconstant factors. On standard datasets like CIFAR-10, a simple Unsupervised\nAdversarial Training (UAT) approach using unlabeled data improves robust\naccuracy by 21.7% over using 4K supervised examples alone, and captures over\n95% of the improvement from the same number of labeled examples. Finally, we\nreport an improvement of 4% over the previous state-of-the-art on CIFAR-10\nagainst the strongest known attack by using additional unlabeled data from the\nuncurated 80 Million Tiny Images dataset. This demonstrates that our finding\nextends as well to the more realistic case where unlabeled data is also\nuncurated, therefore opening a new avenue for improving adversarial training.\n", "versions": [{"version": "v1", "created": "Fri, 31 May 2019 17:19:13 GMT"}, {"version": "v2", "created": "Tue, 18 Jun 2019 16:00:26 GMT"}, {"version": "v3", "created": "Fri, 13 Sep 2019 16:41:12 GMT"}, {"version": "v4", "created": "Thu, 5 Dec 2019 16:57:27 GMT"}], "update_date": "2019-12-06", "authors_parsed": [["Uesato", "Jonathan", ""], ["Alayrac", "Jean-Baptiste", ""], ["Huang", "Po-Sen", ""], ["Stanforth", "Robert", ""], ["Fawzi", "Alhussein", ""], ["Kohli", "Pushmeet", ""]]}, {"id": "1905.13736", "submitter": "Yair Carmon", "authors": "Yair Carmon, Aditi Raghunathan, Ludwig Schmidt, Percy Liang and John\n  C. Duchi", "title": "Unlabeled Data Improves Adversarial Robustness", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We demonstrate, theoretically and empirically, that adversarial robustness\ncan significantly benefit from semisupervised learning. Theoretically, we\nrevisit the simple Gaussian model of Schmidt et al. that shows a sample\ncomplexity gap between standard and robust classification. We prove that\nunlabeled data bridges this gap: a simple semisupervised learning procedure\n(self-training) achieves high robust accuracy using the same number of labels\nrequired for achieving high standard accuracy. Empirically, we augment CIFAR-10\nwith 500K unlabeled images sourced from 80 Million Tiny Images and use robust\nself-training to outperform state-of-the-art robust accuracies by over 5 points\nin (i) $\\ell_\\infty$ robustness against several strong attacks via adversarial\ntraining and (ii) certified $\\ell_2$ and $\\ell_\\infty$ robustness via\nrandomized smoothing. On SVHN, adding the dataset's own extra training set with\nthe labels removed provides gains of 4 to 10 points, within 1 point of the gain\nfrom using the extra labels.\n", "versions": [{"version": "v1", "created": "Fri, 31 May 2019 17:41:33 GMT"}, {"version": "v2", "created": "Mon, 10 Jun 2019 17:27:27 GMT"}, {"version": "v3", "created": "Wed, 4 Dec 2019 00:17:16 GMT"}], "update_date": "2019-12-05", "authors_parsed": [["Carmon", "Yair", ""], ["Raghunathan", "Aditi", ""], ["Schmidt", "Ludwig", ""], ["Liang", "Percy", ""], ["Duchi", "John C.", ""]]}, {"id": "1905.13750", "submitter": "Alex Robinson", "authors": "Alex Robinson", "title": "Sketch2code: Generating a website from a paper mockup", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An early stage of developing user-facing applications is creating a wireframe\nto layout the interface. Once a wireframe has been created it is given to a\ndeveloper to implement in code. Developing boiler plate user interface code is\ntime consuming work but still requires an experienced developer. In this\ndissertation we present two approaches which automates this process, one using\nclassical computer vision techniques, and another using a novel application of\ndeep semantic segmentation networks. We release a dataset of websites which can\nbe used to train and evaluate these approaches. Further, we have designed a\nnovel evaluation framework which allows empirical evaluation by creating\nsynthetic sketches. Our evaluation illustrates that our deep learning approach\noutperforms our classical computer vision approach and we conclude that deep\nlearning is the most promising direction for future research.\n", "versions": [{"version": "v1", "created": "Thu, 9 May 2019 10:15:13 GMT"}], "update_date": "2019-06-03", "authors_parsed": [["Robinson", "Alex", ""]]}]