[{"id": "2011.00043", "submitter": "Zhengyuan Yang", "authors": "Zhengyuan Yang, Amanda Kay, Yuncheng Li, Wendi Cross, Jiebo Luo", "title": "Pose-based Body Language Recognition for Emotion and Psychiatric Symptom\n  Interpretation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inspired by the human ability to infer emotions from body language, we\npropose an automated framework for body language based emotion recognition\nstarting from regular RGB videos. In collaboration with psychologists, we\nfurther extend the framework for psychiatric symptom prediction. Because a\nspecific application domain of the proposed framework may only supply a limited\namount of data, the framework is designed to work on a small training set and\npossess a good transferability. The proposed system in the first stage\ngenerates sequences of body language predictions based on human poses estimated\nfrom input videos. In the second stage, the predicted sequences are fed into a\ntemporal network for emotion interpretation and psychiatric symptom prediction.\nWe first validate the accuracy and transferability of the proposed body\nlanguage recognition method on several public action recognition datasets. We\nthen evaluate the framework on a proposed URMC dataset, which consists of\nconversations between a standardized patient and a behavioral health\nprofessional, along with expert annotations of body language, emotions, and\npotential psychiatric symptoms. The proposed framework outperforms other\nmethods on the URMC dataset.\n", "versions": [{"version": "v1", "created": "Fri, 30 Oct 2020 18:45:16 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["Yang", "Zhengyuan", ""], ["Kay", "Amanda", ""], ["Li", "Yuncheng", ""], ["Cross", "Wendi", ""], ["Luo", "Jiebo", ""]]}, {"id": "2011.00052", "submitter": "Tavpritesh Sethi", "authors": "Asmit Kumar Singh, Paras Mehan, Divyanshu Sharma, Rohan Pandey,\n  Tavpritesh Sethi, Ponnurangam Kumaraguru", "title": "(Un)Masked COVID-19 Trends from Social Media", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Wearing masks is a useful protection method against COVID-19, which has\ncaused widespread economic and social impact worldwide. Across the globe,\ngovernments have put mandates for the use of face masks, which have received\nboth positive and negative reaction. Online social media provides an exciting\nplatform to study the use of masks and analyze underlying mask-wearing\npatterns. In this article, we analyze 2.04 million social media images for six\nUS cities. An increase in masks worn in images is seen as the COVID-19 cases\nrose, particularly when their respective states imposed strict regulations. We\nalso found a decrease in the posting of group pictures as stay-at-home laws\nwere put into place. Furthermore, mask compliance in the Black Lives Matter\nprotest was analyzed, eliciting that 40% of the people in group photos wore\nmasks, and 45% of them wore the masks with a fit score of greater than 80%. We\nintroduce two new datasets, VAriety MAsks - Classification (VAMA-C) and VAriety\nMAsks - Segmentation (VAMA-S), for mask detection and mask fit analysis tasks,\nrespectively. For the analysis, we create two frameworks, face mask detector\n(for classifying masked and unmasked faces) and mask fit analyzer (a semantic\nsegmentation based model to calculate a mask-fit score). The face mask detector\nachieved a classification accuracy of 98%, and the semantic segmentation model\nfor the mask fit analyzer achieved an Intersection Over Union (IOU) score of\n98%. We conclude that such a framework can be used to evaluate the\neffectiveness of such public health strategies using social media platforms in\ntimes of pandemic.\n", "versions": [{"version": "v1", "created": "Fri, 30 Oct 2020 19:02:42 GMT"}, {"version": "v2", "created": "Wed, 30 Dec 2020 14:41:19 GMT"}, {"version": "v3", "created": "Fri, 9 Jul 2021 15:36:12 GMT"}], "update_date": "2021-07-12", "authors_parsed": [["Singh", "Asmit Kumar", ""], ["Mehan", "Paras", ""], ["Sharma", "Divyanshu", ""], ["Pandey", "Rohan", ""], ["Sethi", "Tavpritesh", ""], ["Kumaraguru", "Ponnurangam", ""]]}, {"id": "2011.00070", "submitter": "Francesco Caliva PhD", "authors": "Francesco Caliv\\'a, Kaiyang Cheng, Rutwik Shah, Valentina Pedoia", "title": "Adversarial Robust Training of Deep Learning MRI Reconstruction Models", "comments": "32 pages, 9 figures, 6 tables, accepted at MELBA (MIDL 2020 special\n  issue)", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Deep Learning (DL) has shown potential in accelerating Magnetic Resonance\nImage acquisition and reconstruction. Nevertheless, there is a dearth of\ntailored methods to guarantee that the reconstruction of small features is\nachieved with high fidelity. In this work, we employ adversarial attacks to\ngenerate small synthetic perturbations, which are difficult to reconstruct for\na trained DL reconstruction network. Then, we use robust training to increase\nthe network's sensitivity to these small features and encourage their\nreconstruction. Next, we investigate the generalization of said approach to\nreal world features. For this, a musculoskeletal radiologist annotated a set of\ncartilage and meniscal lesions from the knee Fast-MRI dataset, and a\nclassification network was devised to assess the reconstruction of the\nfeatures. Experimental results show that by introducing robust training to a\nreconstruction network, the rate of false negative features (4.8\\%) in image\nreconstruction can be reduced. These results are encouraging, and highlight the\nnecessity for attention to this problem by the image reconstruction community,\nas a milestone for the introduction of DL reconstruction in clinical practice.\nTo support further research, we make our annotations and code publicly\navailable at https://github.com/fcaliva/fastMRI_BB_abnormalities_annotation.\n", "versions": [{"version": "v1", "created": "Fri, 30 Oct 2020 19:26:14 GMT"}, {"version": "v2", "created": "Sun, 18 Apr 2021 17:00:31 GMT"}, {"version": "v3", "created": "Tue, 27 Apr 2021 05:51:44 GMT"}], "update_date": "2021-04-28", "authors_parsed": [["Caliv\u00e1", "Francesco", ""], ["Cheng", "Kaiyang", ""], ["Shah", "Rutwik", ""], ["Pedoia", "Valentina", ""]]}, {"id": "2011.00071", "submitter": "Arissa Wongpanich", "authors": "Arissa Wongpanich, Hieu Pham, James Demmel, Mingxing Tan, Quoc Le,\n  Yang You, Sameer Kumar", "title": "Training EfficientNets at Supercomputer Scale: 83% ImageNet Top-1\n  Accuracy in One Hour", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  EfficientNets are a family of state-of-the-art image classification models\nbased on efficiently scaled convolutional neural networks. Currently,\nEfficientNets can take on the order of days to train; for example, training an\nEfficientNet-B0 model takes 23 hours on a Cloud TPU v2-8 node. In this paper,\nwe explore techniques to scale up the training of EfficientNets on TPU-v3 Pods\nwith 2048 cores, motivated by speedups that can be achieved when training at\nsuch scales. We discuss optimizations required to scale training to a batch\nsize of 65536 on 1024 TPU-v3 cores, such as selecting large batch optimizers\nand learning rate schedules as well as utilizing distributed evaluation and\nbatch normalization techniques. Additionally, we present timing and performance\nbenchmarks for EfficientNet models trained on the ImageNet dataset in order to\nanalyze the behavior of EfficientNets at scale. With our optimizations, we are\nable to train EfficientNet on ImageNet to an accuracy of 83% in 1 hour and 4\nminutes.\n", "versions": [{"version": "v1", "created": "Fri, 30 Oct 2020 19:27:11 GMT"}, {"version": "v2", "created": "Thu, 5 Nov 2020 02:17:22 GMT"}], "update_date": "2020-11-06", "authors_parsed": [["Wongpanich", "Arissa", ""], ["Pham", "Hieu", ""], ["Demmel", "James", ""], ["Tan", "Mingxing", ""], ["Le", "Quoc", ""], ["You", "Yang", ""], ["Kumar", "Sameer", ""]]}, {"id": "2011.00081", "submitter": "Hosein Barzekar", "authors": "Hosein Barzekar, Zeyun Yu", "title": "C-Net: A Reliable Convolutional Neural Network for Biomedical Image\n  Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cancers are the leading cause of death in many developed countries. Early\ndiagnosis plays a crucial role in having proper treatment for this debilitating\ndisease. The automated classification of the type of cancer is a challenging\ntask since pathologists must examine a huge number of histopathological images\nto detect infinitesimal abnormalities. In this study, we propose a novel\nconvolutional neural network (CNN) architecture composed of a Concatenation of\nmultiple Networks, called C-Net, to classify biomedical images. In contrast to\nconventional deep learning models in biomedical image classification, which\nutilize transfer learning to solve the problem, no prior knowledge is employed.\nThe model incorporates multiple CNNs including Outer, Middle, and Inner. The\nfirst two parts of the architecture contain six networks that serve as feature\nextractors to feed into the Inner network to classify the images in terms of\nmalignancy and benignancy. The C-Net is applied for histopathological image\nclassification on two public datasets, including BreakHis and Osteosarcoma. To\nevaluate the performance, the model is tested using several evaluation metrics\nfor its reliability. The C-Net model outperforms all other models on the\nindividual metrics for both datasets and achieves zero misclassification.\n", "versions": [{"version": "v1", "created": "Fri, 30 Oct 2020 20:03:20 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["Barzekar", "Hosein", ""], ["Yu", "Zeyun", ""]]}, {"id": "2011.00133", "submitter": "Pl\\'acido Francisco Lizancos Vidal", "authors": "Pl\\'acido L Vidal, Joaquim de Moura, Jorge Novo, Marcos Ortega", "title": "Multi-stage transfer learning for lung segmentation using portable X-ray\n  devices for patients with COVID-19", "comments": null, "journal-ref": "Expert Systems with Applications 173 (2021) 114677", "doi": "10.1016/j.eswa.2021.114677", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the main challenges in times of sanitary emergency is to quickly\ndevelop computer aided diagnosis systems with a limited number of available\nsamples due to the novelty, complexity of the case and the urgency of its\nimplementation. This is the case during the current pandemic of COVID-19. This\npathogen primarily infects the respiratory system of the afflicted, resulting\nin pneumonia and in a severe case of acute respiratory distress syndrome. This\nresults in the formation of different pathological structures in the lungs that\ncan be detected by the use of chest X-rays. Due to the overload of the health\nservices, portable X-ray devices are recommended during the pandemic,\npreventing the spread of the disease. However, these devices entail different\ncomplications (such as capture quality) that, together with the subjectivity of\nthe clinician, make the diagnostic process more difficult and suggest the\nnecessity for computer-aided diagnosis methodologies despite the scarcity of\nsamples available to do so. To solve this problem, we propose a methodology\nthat allows to adapt the knowledge from a well-known domain with a high number\nof samples to a new domain with a significantly reduced number and greater\ncomplexity. We took advantage of a pre-trained segmentation model from brain\nmagnetic resonance imaging of a unrelated pathology and performed two stages of\nknowledge transfer to obtain a robust system able to segment lung regions from\nportable X-ray devices despite the scarcity of samples and lesser quality. This\nway, our methodology obtained a satisfactory accuracy of $0.9761 \\pm 0.0100$\nfor patients with COVID-19, $0.9801 \\pm 0.0104$ for normal patients and $0.9769\n\\pm 0.0111$ for patients with pulmonary diseases with similar characteristics\nas COVID-19 (such as pneumonia) but not genuine COVID-19.\n", "versions": [{"version": "v1", "created": "Fri, 30 Oct 2020 22:51:06 GMT"}, {"version": "v2", "created": "Sun, 7 Mar 2021 00:29:42 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Vidal", "Pl\u00e1cido L", ""], ["de Moura", "Joaquim", ""], ["Novo", "Jorge", ""], ["Ortega", "Marcos", ""]]}, {"id": "2011.00139", "submitter": "Tengfei Liang", "authors": "Tengfei Liang, Yi Jin, Yidong Li, Tao Wang, Songhe Feng, Congyan Lang", "title": "EDCNN: Edge enhancement-based Densely Connected Network with Compound\n  Loss for Low-Dose CT Denoising", "comments": "8 pages, 7 figures, 3 tables", "journal-ref": "2020 15th IEEE International Conference on Signal Processing\n  (ICSP). 1 (2020) 193-198", "doi": "10.1109/ICSP48669.2020.9320928", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the past few decades, to reduce the risk of X-ray in computed tomography\n(CT), low-dose CT image denoising has attracted extensive attention from\nresearchers, which has become an important research issue in the field of\nmedical images. In recent years, with the rapid development of deep learning\ntechnology, many algorithms have emerged to apply convolutional neural networks\nto this task, achieving promising results. However, there are still some\nproblems such as low denoising efficiency, over-smoothed result, etc. In this\npaper, we propose the Edge enhancement based Densely connected Convolutional\nNeural Network (EDCNN). In our network, we design an edge enhancement module\nusing the proposed novel trainable Sobel convolution. Based on this module, we\nconstruct a model with dense connections to fuse the extracted edge information\nand realize end-to-end image denoising. Besides, when training the model, we\nintroduce a compound loss that combines MSE loss and multi-scales perceptual\nloss to solve the over-smoothed problem and attain a marked improvement in\nimage quality after denoising. Compared with the existing low-dose CT image\ndenoising algorithms, our proposed model has a better performance in preserving\ndetails and suppressing noise.\n", "versions": [{"version": "v1", "created": "Fri, 30 Oct 2020 23:12:09 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Liang", "Tengfei", ""], ["Jin", "Yi", ""], ["Li", "Yidong", ""], ["Wang", "Tao", ""], ["Feng", "Songhe", ""], ["Lang", "Congyan", ""]]}, {"id": "2011.00144", "submitter": "Samarth Gupta", "authors": "Samarth Gupta, Saurabh Amin", "title": "Integer Programming-based Error-Correcting Output Code Design for Robust\n  Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.IT math.IT stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Error-Correcting Output Codes (ECOCs) offer a principled approach for\ncombining simple binary classifiers into multiclass classifiers. In this paper,\nwe investigate the problem of designing optimal ECOCs to achieve both nominal\nand adversarial accuracy using Support Vector Machines (SVMs) and binary deep\nlearning models. In contrast to previous literature, we present an Integer\nProgramming (IP) formulation to design minimal codebooks with desirable error\ncorrecting properties. Our work leverages the advances in IP solvers to\ngenerate codebooks with optimality guarantees. To achieve tractability, we\nexploit the underlying graph-theoretic structure of the constraint set in our\nIP formulation. This enables us to use edge clique covers to substantially\nreduce the constraint set. Our codebooks achieve a high nominal accuracy\nrelative to standard codebooks (e.g., one-vs-all, one-vs-one, and dense/sparse\ncodes). We also estimate the adversarial accuracy of our ECOC-based classifiers\nin a white-box setting. Our IP-generated codebooks provide non-trivial\nrobustness to adversarial perturbations even without any adversarial training.\n", "versions": [{"version": "v1", "created": "Fri, 30 Oct 2020 23:35:18 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["Gupta", "Samarth", ""], ["Amin", "Saurabh", ""]]}, {"id": "2011.00147", "submitter": "Guoliang Kang", "authors": "Guoliang Kang, Yunchao Wei, Yi Yang, Yueting Zhuang, Alexander G.\n  Hauptmann", "title": "Pixel-Level Cycle Association: A New Perspective for Domain Adaptive\n  Semantic Segmentation", "comments": "Accepted by NeurIPS 2020 (oral). Code:\n  https://github.com/kgl-prml/Pixel- Level-Cycle-Association", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Domain adaptive semantic segmentation aims to train a model performing\nsatisfactory pixel-level predictions on the target with only out-of-domain\n(source) annotations. The conventional solution to this task is to minimize the\ndiscrepancy between source and target to enable effective knowledge transfer.\nPrevious domain discrepancy minimization methods are mainly based on the\nadversarial training. They tend to consider the domain discrepancy globally,\nwhich ignore the pixel-wise relationships and are less discriminative. In this\npaper, we propose to build the pixel-level cycle association between source and\ntarget pixel pairs and contrastively strengthen their connections to diminish\nthe domain gap and make the features more discriminative. To the best of our\nknowledge, this is a new perspective for tackling such a challenging task.\nExperiment results on two representative domain adaptation benchmarks, i.e.\nGTAV $\\rightarrow$ Cityscapes and SYNTHIA $\\rightarrow$ Cityscapes, verify the\neffectiveness of our proposed method and demonstrate that our method performs\nfavorably against previous state-of-the-arts. Our method can be trained\nend-to-end in one stage and introduces no additional parameters, which is\nexpected to serve as a general framework and help ease future research in\ndomain adaptive semantic segmentation. Code is available at\nhttps://github.com/kgl-prml/Pixel- Level-Cycle-Association.\n", "versions": [{"version": "v1", "created": "Sat, 31 Oct 2020 00:11:36 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["Kang", "Guoliang", ""], ["Wei", "Yunchao", ""], ["Yang", "Yi", ""], ["Zhuang", "Yueting", ""], ["Hauptmann", "Alexander G.", ""]]}, {"id": "2011.00148", "submitter": "Anindo Saha", "authors": "Anindo Saha, Prem Prasad, Abdullah Thabit", "title": "Leveraging Adaptive Color Augmentation in Convolutional Neural Networks\n  for Deep Skin Lesion Segmentation", "comments": "Accepted to 2020 17th IEEE International Symposium on Biomedical\n  Imaging (ISBI) [oral presentation]", "journal-ref": null, "doi": "10.1109/ISBI45749.2020.9098344", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fully automatic detection of skin lesions in dermatoscopic images can\nfacilitate early diagnosis and repression of malignant melanoma and\nnon-melanoma skin cancer. Although convolutional neural networks are a powerful\nsolution, they are limited by the illumination spectrum of annotated\ndermatoscopic screening images, where color is an important discriminative\nfeature. In this paper, we propose an adaptive color augmentation technique to\namplify data expression and model performance, while regulating color\ndifference and saturation to minimize the risks of using synthetic data.\nThrough deep visualization, we qualitatively identify and verify the semantic\nstructural features learned by the network for discriminating skin lesions\nagainst normal skin tissue. The overall system achieves a Dice Ratio of 0.891\nwith 0.943 sensitivity and 0.932 specificity on the ISIC 2018 Testing Set for\nsegmentation.\n", "versions": [{"version": "v1", "created": "Sat, 31 Oct 2020 00:16:23 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["Saha", "Anindo", ""], ["Prasad", "Prem", ""], ["Thabit", "Abdullah", ""]]}, {"id": "2011.00149", "submitter": "Anindo Saha", "authors": "Anindo Saha, Fakrul I. Tushar, Khrystyna Faryna, Vincent M.\n  D'Anniballe, Rui Hou, Maciej A. Mazurowski, Geoffrey D. Rubin, Joseph Y. Lo", "title": "Weakly Supervised 3D Classification of Chest CT using Aggregated\n  Multi-Resolution Deep Segmentation Features", "comments": "Accepted to 2020 SPIE Medical Imaging: Computer-Aided Diagnosis [oral\n  presentation]", "journal-ref": null, "doi": "10.1117/12.2550857", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Weakly supervised disease classification of CT imaging suffers from poor\nlocalization owing to case-level annotations, where even a positive scan can\nhold hundreds to thousands of negative slices along multiple planes.\nFurthermore, although deep learning segmentation and classification models\nextract distinctly unique combinations of anatomical features from the same\ntarget class(es), they are typically seen as two independent processes in a\ncomputer-aided diagnosis (CAD) pipeline, with little to no feature reuse. In\nthis research, we propose a medical classifier that leverages the semantic\nstructural concepts learned via multi-resolution segmentation feature maps, to\nguide weakly supervised 3D classification of chest CT volumes. Additionally, a\ncomparative analysis is drawn across two different types of feature aggregation\nto explore the vast possibilities surrounding feature fusion. Using a dataset\nof 1593 scans labeled on a case-level basis via rule-based model, we train a\ndual-stage convolutional neural network (CNN) to perform organ segmentation and\nbinary classification of four representative diseases (emphysema,\npneumonia/atelectasis, mass and nodules) in lungs. The baseline model, with\nseparate stages for segmentation and classification, results in AUC of 0.791.\nUsing identical hyperparameters, the connected architecture using static and\ndynamic feature aggregation improves performance to AUC of 0.832 and 0.851,\nrespectively. This study advances the field in two key ways. First, case-level\nreport data is used to weakly supervise a 3D CT classifier of multiple,\nsimultaneous diseases for an organ. Second, segmentation and classification\nmodels are connected with two different feature aggregation strategies to\nenhance the classification performance.\n", "versions": [{"version": "v1", "created": "Sat, 31 Oct 2020 00:16:53 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["Saha", "Anindo", ""], ["Tushar", "Fakrul I.", ""], ["Faryna", "Khrystyna", ""], ["D'Anniballe", "Vincent M.", ""], ["Hou", "Rui", ""], ["Mazurowski", "Maciej A.", ""], ["Rubin", "Geoffrey D.", ""], ["Lo", "Joseph Y.", ""]]}, {"id": "2011.00160", "submitter": "Gustavo Zanoni Felipe", "authors": "Gustavo Z. Felipe, Jacqueline N. Zanoni, Camila C.\n  Sehaber-Sierakowski, Gleison D. P. Bossolani, Sara R. G. Souza, Franklin C.\n  Flores, Luiz E. S. Oliveira, Rodolfo M. Pereira, Yandre M. G. Costa", "title": "Automatic Chronic Degenerative Diseases Identification Using Enteric\n  Nervous System Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Studies recently accomplished on the Enteric Nervous System have shown that\nchronic degenerative diseases affect the Enteric Glial Cells (EGC) and, thus,\nthe development of recognition methods able to identify whether or not the EGC\nare affected by these type of diseases may be helpful in its diagnoses. In this\nwork, we propose the use of pattern recognition and machine learning techniques\nto evaluate if a given animal EGC image was obtained from a healthy individual\nor one affect by a chronic degenerative disease. In the proposed approach, we\nhave performed the classification task with handcrafted features and deep\nlearning based techniques, also known as non-handcrafted features. The\nhandcrafted features were obtained from the textural content of the ECG images\nusing texture descriptors, such as the Local Binary Pattern (LBP). Moreover,\nthe representation learning techniques employed in the approach are based on\ndifferent Convolutional Neural Network (CNN) architectures, such as AlexNet and\nVGG16, with and without transfer learning. The complementarity between the\nhandcrafted and non-handcrafted features was also evaluated with late fusion\ntechniques. The datasets of EGC images used in the experiments, which are also\ncontributions of this paper, are composed of three different chronic\ndegenerative diseases: Cancer, Diabetes Mellitus, and Rheumatoid Arthritis. The\nexperimental results, supported by statistical analysis, shown that the\nproposed approach can distinguish healthy cells from the sick ones with a\nrecognition rate of 89.30% (Rheumatoid Arthritis), 98.45% (Cancer), and 95.13%\n(Diabetes Mellitus), being achieved by combining classifiers obtained both\nfeature scenarios.\n", "versions": [{"version": "v1", "created": "Sat, 31 Oct 2020 01:04:46 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["Felipe", "Gustavo Z.", ""], ["Zanoni", "Jacqueline N.", ""], ["Sehaber-Sierakowski", "Camila C.", ""], ["Bossolani", "Gleison D. P.", ""], ["Souza", "Sara R. G.", ""], ["Flores", "Franklin C.", ""], ["Oliveira", "Luiz E. S.", ""], ["Pereira", "Rodolfo M.", ""], ["Costa", "Yandre M. G.", ""]]}, {"id": "2011.00168", "submitter": "Aniruddha Tamhane", "authors": "Aniruddha Tamhane, Jie Ying Wu, Mathias Unberath", "title": "Multimodal and self-supervised representation learning for automatic\n  gesture recognition in surgical robotics", "comments": "15 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.RO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Self-supervised, multi-modal learning has been successful in holistic\nrepresentation of complex scenarios. This can be useful to consolidate\ninformation from multiple modalities which have multiple, versatile uses. Its\napplication in surgical robotics can lead to simultaneously developing a\ngeneralised machine understanding of the surgical process and reduce the\ndependency on quality, expert annotations which are generally difficult to\nobtain. We develop a self-supervised, multi-modal representation learning\nparadigm that learns representations for surgical gestures from video and\nkinematics. We use an encoder-decoder network configuration that encodes\nrepresentations from surgical videos and decodes them to yield kinematics. We\nquantitatively demonstrate the efficacy of our learnt representations for\ngesture recognition (with accuracy between 69.6 % and 77.8 %), transfer\nlearning across multiple tasks (with accuracy between 44.6 % and 64.8 %) and\nsurgeon skill classification (with accuracy between 76.8 % and 81.2 %).\nFurther, we qualitatively demonstrate that our self-supervised representations\ncluster in semantically meaningful properties (surgeon skill and gestures).\n", "versions": [{"version": "v1", "created": "Sat, 31 Oct 2020 02:20:32 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["Tamhane", "Aniruddha", ""], ["Wu", "Jie Ying", ""], ["Unberath", "Mathias", ""]]}, {"id": "2011.00174", "submitter": "Ryusuke Sagawa", "authors": "Ryusuke Sagawa, Yusuke Higuchi, Hiroshi Kawasaki, Ryo Furukawa,\n  Takahiro Ito", "title": "Dense Pixel-wise Micro-motion Estimation of Object Surface by using Low\n  Dimensional Embedding of Laser Speckle Pattern", "comments": "to be published in ACCV2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a method of estimating micro-motion of an object at each\npixel that is too small to detect under a common setup of camera and\nillumination. The method introduces an active-lighting approach to make the\nmotion visually detectable. The approach is based on speckle pattern, which is\nproduced by the mutual interference of laser light on object's surface and\ncontinuously changes its appearance according to the out-of-plane motion of the\nsurface. In addition, speckle pattern becomes uncorrelated with large motion.\nTo compensate such micro- and large motion, the method estimates the motion\nparameters up to scale at each pixel by nonlinear embedding of the speckle\npattern into low-dimensional space. The out-of-plane motion is calculated by\nmaking the motion parameters spatially consistent across the image. In the\nexperiments, the proposed method is compared with other measuring devices to\nprove the effectiveness of the method.\n", "versions": [{"version": "v1", "created": "Sat, 31 Oct 2020 03:03:00 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["Sagawa", "Ryusuke", ""], ["Higuchi", "Yusuke", ""], ["Kawasaki", "Hiroshi", ""], ["Furukawa", "Ryo", ""], ["Ito", "Takahiro", ""]]}, {"id": "2011.00177", "submitter": "Maoqiang Wu", "authors": "Maoqiang Wu, Xinyue Zhang, Jiahao Ding, Hien Nguyen, Rong Yu, Miao\n  Pan, Stephen T. Wong", "title": "Evaluation of Inference Attack Models for Deep Learning on Medical Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning has attracted broad interest in healthcare and medical\ncommunities. However, there has been little research into the privacy issues\ncreated by deep networks trained for medical applications. Recently developed\ninference attack algorithms indicate that images and text records can be\nreconstructed by malicious parties that have the ability to query deep\nnetworks. This gives rise to the concern that medical images and electronic\nhealth records containing sensitive patient information are vulnerable to these\nattacks. This paper aims to attract interest from researchers in the medical\ndeep learning community to this important problem. We evaluate two prominent\ninference attack models, namely, attribute inference attack and model inversion\nattack. We show that they can reconstruct real-world medical images and\nclinical reports with high fidelity. We then investigate how to protect\npatients' privacy using defense mechanisms, such as label perturbation and\nmodel perturbation. We provide a comparison of attack results between the\noriginal and the medical deep learning models with defenses. The experimental\nevaluations show that our proposed defense approaches can effectively reduce\nthe potential privacy leakage of medical deep learning from the inference\nattacks.\n", "versions": [{"version": "v1", "created": "Sat, 31 Oct 2020 03:18:36 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["Wu", "Maoqiang", ""], ["Zhang", "Xinyue", ""], ["Ding", "Jiahao", ""], ["Nguyen", "Hien", ""], ["Yu", "Rong", ""], ["Pan", "Miao", ""], ["Wong", "Stephen T.", ""]]}, {"id": "2011.00178", "submitter": "Guangyao Chen", "authors": "Guangyao Chen, Limeng Qiao, Yemin Shi, Peixi Peng, Jia Li, Tiejun\n  Huang, Shiliang Pu, Yonghong Tian", "title": "Learning Open Set Network with Discriminative Reciprocal Points", "comments": "ECCV 2020 (spotlight)", "journal-ref": "ECCV 2020", "doi": "10.1007/978-3-030-58580-8_30", "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Open set recognition is an emerging research area that aims to simultaneously\nclassify samples from predefined classes and identify the rest as 'unknown'. In\nthis process, one of the key challenges is to reduce the risk of generalizing\nthe inherent characteristics of numerous unknown samples learned from a small\namount of known data. In this paper, we propose a new concept, Reciprocal\nPoint, which is the potential representation of the extra-class space\ncorresponding to each known category. The sample can be classified to known or\nunknown by the otherness with reciprocal points. To tackle the open set\nproblem, we offer a novel open space risk regularization term. Based on the\nbounded space constructed by reciprocal points, the risk of unknown is reduced\nthrough multi-category interaction. The novel learning framework called\nReciprocal Point Learning (RPL), which can indirectly introduce the unknown\ninformation into the learner with only known classes, so as to learn more\ncompact and discriminative representations. Moreover, we further construct a\nnew large-scale challenging aircraft dataset for open set recognition: Aircraft\n300 (Air-300). Extensive experiments on multiple benchmark datasets indicate\nthat our framework is significantly superior to other existing approaches and\nachieves state-of-the-art performance on standard open set benchmarks.\n", "versions": [{"version": "v1", "created": "Sat, 31 Oct 2020 03:20:31 GMT"}], "update_date": "2020-12-14", "authors_parsed": [["Chen", "Guangyao", ""], ["Qiao", "Limeng", ""], ["Shi", "Yemin", ""], ["Peng", "Peixi", ""], ["Li", "Jia", ""], ["Huang", "Tiejun", ""], ["Pu", "Shiliang", ""], ["Tian", "Yonghong", ""]]}, {"id": "2011.00179", "submitter": "Shuman Peng", "authors": "Shuman Peng, Weilian Song, Martin Ester", "title": "Combining Domain-Specific Meta-Learners in the Parameter Space for\n  Cross-Domain Few-Shot Classification", "comments": "Code coming soon at https://github.com/shumanpng/CosML", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The goal of few-shot classification is to learn a model that can classify\nnovel classes using only a few training examples. Despite the promising results\nshown by existing meta-learning algorithms in solving the few-shot\nclassification problem, there still remains an important challenge: how to\ngeneralize to unseen domains while meta-learning on multiple seen domains? In\nthis paper, we propose an optimization-based meta-learning method, called\nCombining Domain-Specific Meta-Learners (CosML), that addresses the\ncross-domain few-shot classification problem. CosML first trains a set of\nmeta-learners, one for each training domain, to learn prior knowledge (i.e.,\nmeta-parameters) specific to each domain. The domain-specific meta-learners are\nthen combined in the \\emph{parameter space}, by taking a weighted average of\ntheir meta-parameters, which is used as the initialization parameters of a task\nnetwork that is quickly adapted to novel few-shot classification tasks in an\nunseen domain. Our experiments show that CosML outperforms a range of\nstate-of-the-art methods and achieves strong cross-domain generalization\nability.\n", "versions": [{"version": "v1", "created": "Sat, 31 Oct 2020 03:33:39 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["Peng", "Shuman", ""], ["Song", "Weilian", ""], ["Ester", "Martin", ""]]}, {"id": "2011.00184", "submitter": "Gaoang Wang", "authors": "Renshu Gu, Gaoang Wang, Jenq-Neng Hwang", "title": "Exploring Severe Occlusion: Multi-Person 3D Pose Estimation with Gated\n  Convolution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  3D human pose estimation (HPE) is crucial in many fields, such as human\nbehavior analysis, augmented reality/virtual reality (AR/VR) applications, and\nself-driving industry. Videos that contain multiple potentially occluded people\ncaptured from freely moving monocular cameras are very common in real-world\nscenarios, while 3D HPE for such scenarios is quite challenging, partially\nbecause there is a lack of such data with accurate 3D ground truth labels in\nexisting datasets. In this paper, we propose a temporal regression network with\na gated convolution module to transform 2D joints to 3D and recover the missing\noccluded joints in the meantime. A simple yet effective localization approach\nis further conducted to transform the normalized pose to the global trajectory.\nTo verify the effectiveness of our approach, we also collect a new moving\ncamera multi-human (MMHuman) dataset that includes multiple people with heavy\nocclusion captured by moving cameras. The 3D ground truth joints are provided\nby accurate motion capture (MoCap) system. From the experiments on\nstatic-camera based Human3.6M data and our own collected moving-camera based\ndata, we show that our proposed method outperforms most state-of-the-art\n2D-to-3D pose estimation methods, especially for the scenarios with heavy\nocclusions.\n", "versions": [{"version": "v1", "created": "Sat, 31 Oct 2020 04:35:24 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["Gu", "Renshu", ""], ["Wang", "Gaoang", ""], ["Hwang", "Jenq-Neng", ""]]}, {"id": "2011.00186", "submitter": "Chen Wei", "authors": "Chen Wei, Yiping Tang, Chuang Niu, Haihong Hu, Yue Wang and Jimin\n  Liang", "title": "Self-supervised Representation Learning for Evolutionary Neural\n  Architecture Search", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently proposed neural architecture search (NAS) algorithms adopt neural\npredictors to accelerate the architecture search. The capability of neural\npredictors to accurately predict the performance metrics of neural architecture\nis critical to NAS, and the acquisition of training datasets for neural\npredictors is time-consuming. How to obtain a neural predictor with high\nprediction accuracy using a small amount of training data is a central problem\nto neural predictor-based NAS. Here, we firstly design a new architecture\nencoding scheme that overcomes the drawbacks of existing vector-based\narchitecture encoding schemes to calculate the graph edit distance of neural\narchitectures. To enhance the predictive performance of neural predictors, we\ndevise two self-supervised learning methods from different perspectives to\npre-train the architecture embedding part of neural predictors to generate a\nmeaningful representation of neural architectures. The first one is to train a\ncarefully designed two branch graph neural network model to predict the graph\nedit distance of two input neural architectures. The second method is inspired\nby the prevalently contrastive learning, and we present a new contrastive\nlearning algorithm that utilizes a central feature vector as a proxy to\ncontrast positive pairs against negative pairs. Experimental results illustrate\nthat the pre-trained neural predictors can achieve comparable or superior\nperformance compared with their supervised counterparts with several times less\ntraining samples. We achieve state-of-the-art performance on the NASBench-101\nand NASBench201 benchmarks when integrating the pre-trained neural predictors\nwith an evolutionary NAS algorithm.\n", "versions": [{"version": "v1", "created": "Sat, 31 Oct 2020 04:57:16 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["Wei", "Chen", ""], ["Tang", "Yiping", ""], ["Niu", "Chuang", ""], ["Hu", "Haihong", ""], ["Wang", "Yue", ""], ["Liang", "Jimin", ""]]}, {"id": "2011.00189", "submitter": "Amir Jafari", "authors": "Gaofeng Huang and Amir H. Jafari", "title": "Enhanced Balancing GAN: Minority-class Image Generation", "comments": null, "journal-ref": "Neural Computing and Applications, 2021", "doi": "10.1007/s00521-021-06163-8", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative adversarial networks (GANs) are one of the most powerful\ngenerative models, but always require a large and balanced dataset to train.\nTraditional GANs are not applicable to generate minority-class images in a\nhighly imbalanced dataset. Balancing GAN (BAGAN) is proposed to mitigate this\nproblem, but it is unstable when images in different classes look similar, e.g.\nflowers and cells. In this work, we propose a supervised autoencoder with an\nintermediate embedding model to disperse the labeled latent vectors. With the\nimproved autoencoder initialization, we also build an architecture of BAGAN\nwith gradient penalty (BAGAN-GP). Our proposed model overcomes the unstable\nissue in original BAGAN and converges faster to high quality generations. Our\nmodel achieves high performance on the imbalanced scale-down version of MNIST\nFashion, CIFAR-10, and one small-scale medical image dataset.\n", "versions": [{"version": "v1", "created": "Sat, 31 Oct 2020 05:03:47 GMT"}], "update_date": "2021-07-12", "authors_parsed": [["Huang", "Gaofeng", ""], ["Jafari", "Amir H.", ""]]}, {"id": "2011.00209", "submitter": "Sungyong Baik", "authors": "Sungyong Baik, Myungsub Choi, Janghoon Choi, Heewon Kim, Kyoung Mu Lee", "title": "Meta-Learning with Adaptive Hyperparameters", "comments": "NeurIPS 2020. Code at https://github.com/baiksung/alfa. Typo fix in\n  the updated version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite its popularity, several recent works question the effectiveness of\nMAML when test tasks are different from training tasks, thus suggesting various\ntask-conditioned methodology to improve the initialization. Instead of\nsearching for better task-aware initialization, we focus on a complementary\nfactor in MAML framework, inner-loop optimization (or fast adaptation).\nConsequently, we propose a new weight update rule that greatly enhances the\nfast adaptation process. Specifically, we introduce a small meta-network that\ncan adaptively generate per-step hyperparameters: learning rate and weight\ndecay coefficients. The experimental results validate that the Adaptive\nLearning of hyperparameters for Fast Adaptation (ALFA) is the equally important\ningredient that was often neglected in the recent few-shot learning approaches.\nSurprisingly, fast adaptation from random initialization with ALFA can already\noutperform MAML.\n", "versions": [{"version": "v1", "created": "Sat, 31 Oct 2020 08:05:34 GMT"}, {"version": "v2", "created": "Tue, 8 Dec 2020 06:53:01 GMT"}], "update_date": "2020-12-09", "authors_parsed": [["Baik", "Sungyong", ""], ["Choi", "Myungsub", ""], ["Choi", "Janghoon", ""], ["Kim", "Heewon", ""], ["Lee", "Kyoung Mu", ""]]}, {"id": "2011.00250", "submitter": "M\\'arton V\\'eges", "authors": "Marton Veges, Andras Lorincz", "title": "Temporal Smoothing for 3D Human Pose Estimation and Localization for\n  Occluded People", "comments": "ICONIP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In multi-person pose estimation actors can be heavily occluded, even become\nfully invisible behind another person. While temporal methods can still predict\na reasonable estimation for a temporarily disappeared pose using past and\nfuture frames, they exhibit large errors nevertheless. We present an energy\nminimization approach to generate smooth, valid trajectories in time, bridging\ngaps in visibility. We show that it is better than other interpolation based\napproaches and achieves state of the art results. In addition, we present the\nsynthetic MuCo-Temp dataset, a temporal extension of the MuCo-3DHP dataset. Our\ncode is made publicly available.\n", "versions": [{"version": "v1", "created": "Sat, 31 Oct 2020 11:48:31 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["Veges", "Marton", ""], ["Lorincz", "Andras", ""]]}, {"id": "2011.00263", "submitter": "Anindo Saha", "authors": "Anindo Saha, Matin Hosseinzadeh, Henkjan Huisman", "title": "Encoding Clinical Priori in 3D Convolutional Neural Networks for\n  Prostate Cancer Detection in bpMRI", "comments": "Accepted to Medical Imaging Meets NeurIPS Workshop of the 34th\n  Conference on Neural Information Processing Systems (NeurIPS 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We hypothesize that anatomical priors can be viable mediums to infuse\ndomain-specific clinical knowledge into state-of-the-art convolutional neural\nnetworks (CNN) based on the U-Net architecture. We introduce a probabilistic\npopulation prior which captures the spatial prevalence and zonal distinction of\nclinically significant prostate cancer (csPCa), in order to improve its\ncomputer-aided detection (CAD) in bi-parametric MR imaging (bpMRI). To evaluate\nperformance, we train 3D adaptations of the U-Net, U-SEResNet, UNet++ and\nAttention U-Net using 800 institutional training-validation scans, paired with\nradiologically-estimated annotations and our computed prior. For 200\nindependent testing bpMRI scans with histologically-confirmed delineations of\ncsPCa, our proposed method of encoding clinical priori demonstrates a strong\nability to improve patient-based diagnosis (upto 8.70% increase in AUROC) and\nlesion-level detection (average increase of 1.08 pAUC between 0.1-1.0 false\npositive per patient) across all four architectures.\n", "versions": [{"version": "v1", "created": "Sat, 31 Oct 2020 13:10:58 GMT"}, {"version": "v2", "created": "Tue, 3 Nov 2020 02:53:28 GMT"}, {"version": "v3", "created": "Thu, 25 Mar 2021 06:58:51 GMT"}], "update_date": "2021-03-26", "authors_parsed": [["Saha", "Anindo", ""], ["Hosseinzadeh", "Matin", ""], ["Huisman", "Henkjan", ""]]}, {"id": "2011.00265", "submitter": "Weidong Shi", "authors": "Weidong Shi, Guanghui Ren, Yunpeng Chen, Shuicheng Yan", "title": "ProxylessKD: Direct Knowledge Distillation with Inherited Classifier for\n  Face Recognition", "comments": "10pages, 3figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Knowledge Distillation (KD) refers to transferring knowledge from a large\nmodel to a smaller one, which is widely used to enhance model performance in\nmachine learning. It tries to align embedding spaces generated from the teacher\nand the student model (i.e. to make images corresponding to the same semantics\nshare the same embedding across different models). In this work, we focus on\nits application in face recognition. We observe that existing knowledge\ndistillation models optimize the proxy tasks that force the student to mimic\nthe teacher's behavior, instead of directly optimizing the face recognition\naccuracy. Consequently, the obtained student models are not guaranteed to be\noptimal on the target task or able to benefit from advanced constraints, such\nas large margin constraints (e.g. margin-based softmax). We then propose a\nnovel method named ProxylessKD that directly optimizes face recognition\naccuracy by inheriting the teacher's classifier as the student's classifier to\nguide the student to learn discriminative embeddings in the teacher's embedding\nspace. The proposed ProxylessKD is very easy to implement and sufficiently\ngeneric to be extended to other tasks beyond face recognition. We conduct\nextensive experiments on standard face recognition benchmarks, and the results\ndemonstrate that ProxylessKD achieves superior performance over existing\nknowledge distillation methods.\n", "versions": [{"version": "v1", "created": "Sat, 31 Oct 2020 13:14:34 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["Shi", "Weidong", ""], ["Ren", "Guanghui", ""], ["Chen", "Yunpeng", ""], ["Yan", "Shuicheng", ""]]}, {"id": "2011.00269", "submitter": "Pu Sun", "authors": "Pu Sun, Yuezun Li, Honggang Qi and Siwei Lyu", "title": "LandmarkGAN: Synthesizing Faces from Landmarks", "comments": "Under consideration at Pattern Recognition Letters", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Face synthesis is an important problem in computer vision with many\napplications. In this work, we describe a new method, namely LandmarkGAN, to\nsynthesize faces based on facial landmarks as input. Facial landmarks are a\nnatural, intuitive, and effective representation for facial expressions and\norientations, which are independent from the target's texture or color and\nbackground scene. Our method is able to transform a set of facial landmarks\ninto new faces of different subjects, while retains the same facial expression\nand orientation. Experimental results on face synthesis and reenactments\ndemonstrate the effectiveness of our method.\n", "versions": [{"version": "v1", "created": "Sat, 31 Oct 2020 13:27:21 GMT"}, {"version": "v2", "created": "Sat, 6 Feb 2021 04:19:53 GMT"}], "update_date": "2021-02-09", "authors_parsed": [["Sun", "Pu", ""], ["Li", "Yuezun", ""], ["Qi", "Honggang", ""], ["Lyu", "Siwei", ""]]}, {"id": "2011.00301", "submitter": "Zexi Chen", "authors": "Zexi Chen, Jiaxin Guo, Xuecheng Xu, Yunkai Wang, Yue Wang, Rong Xiong", "title": "PREGAN: Pose Randomization and Estimation for Weakly Paired Image Style\n  Translation", "comments": null, "journal-ref": null, "doi": "10.1109/LRA.2021.3061359", "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Utilizing the trained model under different conditions without data\nannotation is attractive for robot applications. Towards this goal, one class\nof methods is to translate the image style from another environment to the one\non which models are trained. In this paper, we propose a weakly-paired setting\nfor the style translation, where the content in the two images is aligned with\nerrors in poses. These images could be acquired by different sensors in\ndifferent conditions that share an overlapping region, e.g. with LiDAR or\nstereo cameras, from sunny days or foggy nights. We consider this setting to be\nmore practical with: (i) easier labeling than the paired data; (ii) better\ninterpretability and detail retrieval than the unpaired data. To translate\nacross such images, we propose PREGAN to train a style translator by\nintentionally transforming the two images with a random pose, and to estimate\nthe given random pose by differentiable non-trainable pose estimator given that\nthe more aligned in style, the better the estimated result is. Such adversarial\ntraining enforces the network to learn the style translation, avoiding being\nentangled with other variations. Finally, PREGAN is validated on both simulated\nand real-world collected data to show the effectiveness. Results on down-stream\ntasks, classification, road segmentation, object detection, and feature\nmatching show its potential for real applications.\nhttps://github.com/wrld/PRoGAN\n", "versions": [{"version": "v1", "created": "Sat, 31 Oct 2020 16:11:11 GMT"}, {"version": "v2", "created": "Sun, 17 Jan 2021 07:18:56 GMT"}], "update_date": "2021-03-11", "authors_parsed": [["Chen", "Zexi", ""], ["Guo", "Jiaxin", ""], ["Xu", "Xuecheng", ""], ["Wang", "Yunkai", ""], ["Wang", "Yue", ""], ["Xiong", "Rong", ""]]}, {"id": "2011.00307", "submitter": "Liang Liao", "authors": "Liang Liao and Stephen John Maybank", "title": "General Data Analytics with Applications to Visual Information Analysis:\n  A Provable Backward-Compatible Semisimple Paradigm over T-Algebra", "comments": "38 page, 12 figures. two typos are removed. Official code repository:\n  https://github.com/liaoliang2020/talgebra", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.MM math.RA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a novel backward-compatible paradigm of general data analytics\nover a recently-reported semisimple algebra (called t-algebra). We study the\nabstract algebraic framework over the t-algebra by representing the elements of\nt-algebra by fix-sized multi-way arrays of complex numbers and the algebraic\nstructure over the t-algebra by a collection of direct-product constituents.\nOver the t-algebra, many algorithms are generalized in a straightforward manner\nusing this new semisimple paradigm. To demonstrate the new paradigm's\nperformance and its backward-compatibility, we generalize some canonical\nalgorithms for visual pattern analysis. Experiments on public datasets show\nthat the generalized algorithms compare favorably with their canonical\ncounterparts.\n", "versions": [{"version": "v1", "created": "Sat, 31 Oct 2020 16:41:09 GMT"}, {"version": "v2", "created": "Wed, 4 Nov 2020 16:25:09 GMT"}, {"version": "v3", "created": "Mon, 16 Nov 2020 10:22:55 GMT"}, {"version": "v4", "created": "Mon, 30 Nov 2020 02:03:34 GMT"}, {"version": "v5", "created": "Tue, 5 Jan 2021 12:48:32 GMT"}, {"version": "v6", "created": "Mon, 25 Jan 2021 11:31:49 GMT"}, {"version": "v7", "created": "Thu, 8 Apr 2021 07:47:40 GMT"}, {"version": "v8", "created": "Sun, 2 May 2021 15:45:32 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Liao", "Liang", ""], ["Maybank", "Stephen John", ""]]}, {"id": "2011.00320", "submitter": "Jhony Kaesemodel Pontes", "authors": "Jhony Kaesemodel Pontes, James Hays, and Simon Lucey", "title": "Scene Flow from Point Clouds with or without Learning", "comments": "International Conference on 3D Vision (3DV 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scene flow is the three-dimensional (3D) motion field of a scene. It provides\ninformation about the spatial arrangement and rate of change of objects in\ndynamic environments. Current learning-based approaches seek to estimate the\nscene flow directly from point clouds and have achieved state-of-the-art\nperformance. However, supervised learning methods are inherently domain\nspecific and require a large amount of labeled data. Annotation of scene flow\non real-world point clouds is expensive and challenging, and the lack of such\ndatasets has recently sparked interest in self-supervised learning methods. How\nto accurately and robustly learn scene flow representations without labeled\nreal-world data is still an open problem. Here we present a simple and\ninterpretable objective function to recover the scene flow from point clouds.\nWe use the graph Laplacian of a point cloud to regularize the scene flow to be\n\"as-rigid-as-possible\". Our proposed objective function can be used with or\nwithout learning---as a self-supervisory signal to learn scene flow\nrepresentations, or as a non-learning-based method in which the scene flow is\noptimized during runtime. Our approach outperforms related works in many\ndatasets. We also show the immediate applications of our proposed method for\ntwo applications: motion segmentation and point cloud densification.\n", "versions": [{"version": "v1", "created": "Sat, 31 Oct 2020 17:24:48 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["Pontes", "Jhony Kaesemodel", ""], ["Hays", "James", ""], ["Lucey", "Simon", ""]]}, {"id": "2011.00325", "submitter": "Ping Wang", "authors": "Ping Wang, Jizong Peng, Marco Pedersoli, Yuanfeng Zhou, Caiming Zhang,\n  Christian Desrosiers", "title": "Self-paced and self-consistent co-training for semi-supervised image\n  segmentation", "comments": null, "journal-ref": "Medical Image Analysis, 2021", "doi": "10.1016/j.media.2021.102146", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep co-training has recently been proposed as an effective approach for\nimage segmentation when annotated data is scarce. In this paper, we improve\nexisting approaches for semi-supervised segmentation with a self-paced and\nself-consistent co-training method. To help distillate information from\nunlabeled images, we first design a self-paced learning strategy for\nco-training that lets jointly-trained neural networks focus on\neasier-to-segment regions first, and then gradually consider harder ones.This\nis achieved via an end-to-end differentiable loss inthe form of a generalized\nJensen Shannon Divergence(JSD). Moreover, to encourage predictions from\ndifferent networks to be both consistent and confident, we enhance this\ngeneralized JSD loss with an uncertainty regularizer based on entropy. The\nrobustness of individual models is further improved using a self-ensembling\nloss that enforces their prediction to be consistent across different training\niterations. We demonstrate the potential of our method on three challenging\nimage segmentation problems with different image modalities, using small\nfraction of labeled data. Results show clear advantages in terms of performance\ncompared to the standard co-training baselines and recently proposed\nstate-of-the-art approaches for semi-supervised segmentation\n", "versions": [{"version": "v1", "created": "Sat, 31 Oct 2020 17:41:03 GMT"}, {"version": "v2", "created": "Mon, 9 Nov 2020 16:45:03 GMT"}, {"version": "v3", "created": "Sat, 19 Dec 2020 19:52:09 GMT"}, {"version": "v4", "created": "Wed, 10 Feb 2021 16:46:23 GMT"}], "update_date": "2021-07-15", "authors_parsed": [["Wang", "Ping", ""], ["Peng", "Jizong", ""], ["Pedersoli", "Marco", ""], ["Zhou", "Yuanfeng", ""], ["Zhang", "Caiming", ""], ["Desrosiers", "Christian", ""]]}, {"id": "2011.00337", "submitter": "Diego Gragnaniello", "authors": "Michela Gravina, Diego Gragnaniello, Luisa Verdoliva, Giovanni Poggi,\n  Iuri Corsini, Carlo Dani, Fabio Meneghin, Gianluca Lista, Salvatore Aversa,\n  Francesco Raimondi, Fiorella Migliaro, Carlo Sansone", "title": "Deep learning in the ultrasound evaluation of neonatal respiratory\n  status", "comments": "7 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Lung ultrasound imaging is reaching growing interest from the scientific\ncommunity. On one side, thanks to its harmlessness and high descriptive power,\nthis kind of diagnostic imaging has been largely adopted in sensitive\napplications, like the diagnosis and follow-up of preterm newborns in neonatal\nintensive care units. On the other side, state-of-the-art image analysis and\npattern recognition approaches have recently proven their ability to fully\nexploit the rich information contained in these data, making them attractive\nfor the research community. In this work, we present a thorough analysis of\nrecent deep learning networks and training strategies carried out on a vast and\nchallenging multicenter dataset comprising 87 patients with different diseases\nand gestational ages. These approaches are employed to assess the lung\nrespiratory status from ultrasound images and are evaluated against a reference\nmarker. The conducted analysis sheds some light on this problem by showing the\ncritical points that can mislead the training procedure and proposes some\nadaptations to the specific data and task. The achieved results sensibly\noutperform those obtained by a previous work, which is based on textural\nfeatures, and narrow the gap with the visual score predicted by the human\nexperts.\n", "versions": [{"version": "v1", "created": "Sat, 31 Oct 2020 18:57:55 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["Gravina", "Michela", ""], ["Gragnaniello", "Diego", ""], ["Verdoliva", "Luisa", ""], ["Poggi", "Giovanni", ""], ["Corsini", "Iuri", ""], ["Dani", "Carlo", ""], ["Meneghin", "Fabio", ""], ["Lista", "Gianluca", ""], ["Aversa", "Salvatore", ""], ["Raimondi", "Francesco", ""], ["Migliaro", "Fiorella", ""], ["Sansone", "Carlo", ""]]}, {"id": "2011.00341", "submitter": "Yasin Almalioglu", "authors": "Yasin Almalioglu, Angel Santamaria-Navarro, Benjamin Morrell,\n  Ali-akbar Agha-mohammadi", "title": "Unsupervised Deep Persistent Monocular Visual Odometry and Depth\n  Estimation in Extreme Environments", "comments": "Submitted to ICRA2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, unsupervised deep learning approaches have received\nsignificant attention to estimate the depth and visual odometry (VO) from\nunlabelled monocular image sequences. However, their performance is limited in\nchallenging environments due to perceptual degradation, occlusions and rapid\nmotions. Moreover, the existing unsupervised methods suffer from the lack of\nscale-consistency constraints across frames, which causes that the VO\nestimators fail to provide persistent trajectories over long sequences. In this\nstudy, we propose an unsupervised monocular deep VO framework that predicts\nsix-degrees-of-freedom pose camera motion and depth map of the scene from\nunlabelled RGB image sequences. We provide detailed quantitative and\nqualitative evaluations of the proposed framework on a) a challenging dataset\ncollected during the DARPA Subterranean challenge; and b) the benchmark KITTI\nand Cityscapes datasets. The proposed approach outperforms both traditional and\nstate-of-the-art unsupervised deep VO methods providing better results for both\npose estimation and depth recovery. The presented approach is part of the\nsolution used by the COSTAR team participating at the DARPA Subterranean\nChallenge.\n", "versions": [{"version": "v1", "created": "Sat, 31 Oct 2020 19:10:27 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["Almalioglu", "Yasin", ""], ["Santamaria-Navarro", "Angel", ""], ["Morrell", "Benjamin", ""], ["Agha-mohammadi", "Ali-akbar", ""]]}, {"id": "2011.00359", "submitter": "Wenshan Wang", "authors": "Wenshan Wang, Yaoyu Hu, Sebastian Scherer", "title": "TartanVO: A Generalizable Learning-based VO", "comments": null, "journal-ref": "The Conference on Robot Learning 2020", "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the first learning-based visual odometry (VO) model, which\ngeneralizes to multiple datasets and real-world scenarios and outperforms\ngeometry-based methods in challenging scenes. We achieve this by leveraging the\nSLAM dataset TartanAir, which provides a large amount of diverse synthetic data\nin challenging environments. Furthermore, to make our VO model generalize\nacross datasets, we propose an up-to-scale loss function and incorporate the\ncamera intrinsic parameters into the model. Experiments show that a single\nmodel, TartanVO, trained only on synthetic data, without any finetuning, can be\ngeneralized to real-world datasets such as KITTI and EuRoC, demonstrating\nsignificant advantages over the geometry-based methods on challenging\ntrajectories. Our code is available at https://github.com/castacks/tartanvo.\n", "versions": [{"version": "v1", "created": "Sat, 31 Oct 2020 20:49:33 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["Wang", "Wenshan", ""], ["Hu", "Yaoyu", ""], ["Scherer", "Sebastian", ""]]}, {"id": "2011.00362", "submitter": "Ashish Jaiswal", "authors": "Ashish Jaiswal, Ashwin Ramesh Babu, Mohammad Zaki Zadeh, Debapriya\n  Banerjee, Fillia Makedon", "title": "A Survey on Contrastive Self-supervised Learning", "comments": "20 pages, 18 figures, 6 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Self-supervised learning has gained popularity because of its ability to\navoid the cost of annotating large-scale datasets. It is capable of adopting\nself-defined pseudo labels as supervision and use the learned representations\nfor several downstream tasks. Specifically, contrastive learning has recently\nbecome a dominant component in self-supervised learning methods for computer\nvision, natural language processing (NLP), and other domains. It aims at\nembedding augmented versions of the same sample close to each other while\ntrying to push away embeddings from different samples. This paper provides an\nextensive review of self-supervised methods that follow the contrastive\napproach. The work explains commonly used pretext tasks in a contrastive\nlearning setup, followed by different architectures that have been proposed so\nfar. Next, we have a performance comparison of different methods for multiple\ndownstream tasks such as image classification, object detection, and action\nrecognition. Finally, we conclude with the limitations of the current methods\nand the need for further techniques and future directions to make substantial\nprogress.\n", "versions": [{"version": "v1", "created": "Sat, 31 Oct 2020 21:05:04 GMT"}, {"version": "v2", "created": "Sun, 20 Dec 2020 05:55:08 GMT"}, {"version": "v3", "created": "Sun, 7 Feb 2021 19:11:55 GMT"}], "update_date": "2021-02-09", "authors_parsed": [["Jaiswal", "Ashish", ""], ["Babu", "Ashwin Ramesh", ""], ["Zadeh", "Mohammad Zaki", ""], ["Banerjee", "Debapriya", ""], ["Makedon", "Fillia", ""]]}, {"id": "2011.00368", "submitter": "Hossein Rahmani", "authors": "Maryam Dialameh and Ali Hamzeh and Hossein Rahmani", "title": "DL-Reg: A Deep Learning Regularization Technique using Linear Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Regularization plays a vital role in the context of deep learning by\npreventing deep neural networks from the danger of overfitting. This paper\nproposes a novel deep learning regularization method named as DL-Reg, which\ncarefully reduces the nonlinearity of deep networks to a certain extent by\nexplicitly enforcing the network to behave as much linear as possible. The key\nidea is to add a linear constraint to the objective function of the deep neural\nnetworks, which is simply the error of a linear mapping from the inputs to the\noutputs of the model. More precisely, the proposed DL-Reg carefully forces the\nnetwork to behave in a linear manner. This linear constraint, which is further\nadjusted by a regularization factor, prevents the network from the risk of\noverfitting. The performance of DL-Reg is evaluated by training\nstate-of-the-art deep network models on several benchmark datasets. The\nexperimental results show that the proposed regularization method: 1) gives\nmajor improvements over the existing regularization techniques, and 2)\nsignificantly improves the performance of deep neural networks, especially in\nthe case of small-sized training datasets.\n", "versions": [{"version": "v1", "created": "Sat, 31 Oct 2020 21:53:24 GMT"}, {"version": "v2", "created": "Tue, 3 Nov 2020 23:22:48 GMT"}], "update_date": "2020-11-05", "authors_parsed": [["Dialameh", "Maryam", ""], ["Hamzeh", "Ali", ""], ["Rahmani", "Hossein", ""]]}, {"id": "2011.00372", "submitter": "Hongyi Ling", "authors": "Jiaming Hu, Hongyi Ling, Priyam Parashar, Aayush Naik and Henrik\n  Christensen", "title": "Pose Estimation of Specular and Symmetrical Objects", "comments": "submitted to ICRA 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the robotic industry, specular and textureless metallic components are\nubiquitous. The 6D pose estimation of such objects with only a monocular RGB\ncamera is difficult because of the absence of rich texture features.\nFurthermore, the appearance of specularity heavily depends on the camera\nviewpoint and environmental light conditions making traditional methods, like\ntemplate matching, fail. In the last 30 years, pose estimation of the specular\nobject has been a consistent challenge, and most related works require massive\nknowledge modeling effort for light setups, environment, or the object surface.\nOn the other hand, recent works exhibit the feasibility of 6D pose estimation\non a monocular camera with convolutional neural networks(CNNs) however they\nmostly use opaque objects for evaluation. This paper provides a data-driven\nsolution to estimate the 6D pose of specular objects for grasping them,\nproposes a cost function for handling symmetry, and demonstrates experimental\nresults showing the system's feasibility.\n", "versions": [{"version": "v1", "created": "Sat, 31 Oct 2020 22:08:46 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["Hu", "Jiaming", ""], ["Ling", "Hongyi", ""], ["Parashar", "Priyam", ""], ["Naik", "Aayush", ""], ["Christensen", "Henrik", ""]]}, {"id": "2011.00376", "submitter": "Ange Lou", "authors": "Ange Lou, Shuyue Guan, Nada Kamona, Murray Loew", "title": "Segmentation of Infrared Breast Images Using MultiResUnet Neural Network", "comments": "6 pages. Accepted by IEEE AIPR 2019 (Oral)", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Breast cancer is the second leading cause of death for women in the U.S.\nEarly detection of breast cancer is key to higher survival rates of breast\ncancer patients. We are investigating infrared (IR) thermography as a\nnoninvasive adjunct to mammography for breast cancer screening. IR imaging is\nradiation-free, pain-free, and non-contact. Automatic segmentation of the\nbreast area from the acquired full-size breast IR images will help limit the\narea for tumor search, as well as reduce the time and effort costs of manual\nsegmentation. Autoencoder-like convolutional and deconvolutional neural\nnetworks (C-DCNN) had been applied to automatically segment the breast area in\nIR images in previous studies. In this study, we applied a state-of-the-art\ndeep-learning segmentation model, MultiResUnet, which consists of an encoder\npart to capture features and a decoder part for precise localization. It was\nused to segment the breast area by using a set of breast IR images, collected\nin our pilot study by imaging breast cancer patients and normal volunteers with\na thermal infrared camera (N2 Imager). The database we used has 450 images,\nacquired from 14 patients and 16 volunteers. We used a thresholding method to\nremove interference in the raw images and remapped them from the original\n16-bit to 8-bit, and then cropped and segmented the 8-bit images manually.\nExperiments using leave-one-out cross-validation (LOOCV) and comparison with\nthe ground-truth images by using Tanimoto similarity show that the average\naccuracy of MultiResUnet is 91.47%, which is about 2% higher than that of the\nautoencoder. MultiResUnet offers a better approach to segment breast IR images\nthan our previous model.\n", "versions": [{"version": "v1", "created": "Sat, 31 Oct 2020 22:15:28 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["Lou", "Ange", ""], ["Guan", "Shuyue", ""], ["Kamona", "Nada", ""], ["Loew", "Murray", ""]]}, {"id": "2011.00380", "submitter": "Mingzhi Zhu", "authors": "Shuonan Pei, Mingzhi Zhu", "title": "Real-Time Text Detection and Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inrecentyears,ConvolutionalNeuralNet-work(CNN) is quite a popular topic, as\nit is a powerful andintelligent technique that can be applied in various\nfields.The YOLO is a technique that uses the algorithms for real-time text\ndetection tasks. However, issues like, photometricdistortion and geometric\ndistortion, could affect the systemYOLO accuracy and cause system failure.\nTherefore, thereare improvements that can make the system work better. Inthis\npaper, we are going to present our solution - a potentialsolution of a fast and\naccurate real-time text direction andrecognition system. The paper covers the\ntopic of Real-TimeText detection and recognition in three major areas: 1.\nvideoand image preprocess, 2. Text detection, 3. Text recognition. Asa mature\ntechnique, there are many existing methods that canpotentially improve the\nsolution. We will go through some ofthose existing methods in the literature\nreview session. In thisway, we are presenting an industrial strength,\nhigh-accuracy,Real-Time Text Detection and recognition tool.\n", "versions": [{"version": "v1", "created": "Sat, 31 Oct 2020 22:36:20 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["Pei", "Shuonan", ""], ["Zhu", "Mingzhi", ""]]}, {"id": "2011.00388", "submitter": "Dan Nguyen", "authors": "Dan Nguyen, Azar Sadeghnejad Barkousaraie, Gyanendra Bohara, Anjali\n  Balagopal, Rafe McBeth, Mu-Han Lin, Steve Jiang", "title": "A comparison of Monte Carlo dropout and bootstrap aggregation on the\n  performance and uncertainty estimation in radiation therapy dose prediction\n  with deep learning neural networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.med-ph cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, artificial intelligence technologies and algorithms have become a\nmajor focus for advancements in treatment planning for radiation therapy. As\nthese are starting to become incorporated into the clinical workflow, a major\nconcern from clinicians is not whether the model is accurate, but whether the\nmodel can express to a human operator when it does not know if its answer is\ncorrect. We propose to use Monte Carlo dropout (MCDO) and the bootstrap\naggregation (bagging) technique on deep learning models to produce uncertainty\nestimations for radiation therapy dose prediction. We show that both models are\ncapable of generating a reasonable uncertainty map, and, with our proposed\nscaling technique, creating interpretable uncertainties and bounds on the\nprediction and any relevant metrics. Performance-wise, bagging provides\nstatistically significant reduced loss value and errors in most of the metrics\ninvestigated in this study. The addition of bagging was able to further reduce\nerrors by another 0.34% for Dmean and 0.19% for Dmax, on average, when compared\nto the baseline framework. Overall, the bagging framework provided\nsignificantly lower MAE of 2.62, as opposed to the baseline framework's MAE of\n2.87. The usefulness of bagging, from solely a performance standpoint, does\nhighly depend on the problem and the acceptable predictive error, and its high\nupfront computational cost during training should be factored in to deciding\nwhether it is advantageous to use it. In terms of deployment with uncertainty\nestimations turned on, both frameworks offer the same performance time of about\n12 seconds. As an ensemble-based metaheuristic, bagging can be used with\nexisting machine learning architectures to improve stability and performance,\nand MCDO can be applied to any deep learning models that have dropout as part\nof their architecture.\n", "versions": [{"version": "v1", "created": "Sun, 1 Nov 2020 00:24:43 GMT"}, {"version": "v2", "created": "Tue, 12 Jan 2021 02:28:03 GMT"}], "update_date": "2021-01-13", "authors_parsed": [["Nguyen", "Dan", ""], ["Barkousaraie", "Azar Sadeghnejad", ""], ["Bohara", "Gyanendra", ""], ["Balagopal", "Anjali", ""], ["McBeth", "Rafe", ""], ["Lin", "Mu-Han", ""], ["Jiang", "Steve", ""]]}, {"id": "2011.00395", "submitter": "Beidi Zhao", "authors": "Beidi Zhao, Shuai Li, Yanbo Gao, Chuankun Li, Wanqing Li", "title": "A Framework of Combining Short-Term Spatial/Frequency Feature Extraction\n  and Long-Term IndRNN for Activity Recognition", "comments": "19 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Smartphone sensors based human activity recognition is attracting increasing\ninterests nowadays with the popularization of smartphones. With the high\nsampling rates of smartphone sensors, it is a highly long-range temporal\nrecognition problem, especially with the large intra-class distances such as\nthe smartphones carried at different locations such as in the bag or on the\nbody, and the small inter-class distances such as taking train or subway. To\naddress this problem, we propose a new framework of combining short-term\nspatial/frequency feature extraction and a long-term Independently Recurrent\nNeural Network (IndRNN) for activity recognition. Considering the periodic\ncharacteristics of the sensor data, short-term temporal features are first\nextracted in the spatial and frequency domains. Then the IndRNN, which is able\nto capture long-term patterns, is used to further obtain the long-term features\nfor classification. In view of the large differences when the smartphone is\ncarried at different locations, a group based location recognition is first\ndeveloped to pinpoint the location of the smartphone. The Sussex-Huawei\nLocomotion (SHL) dataset from the SHL Challenge is used for evaluation. An\nearlier version of the proposed method has won the second place award in the\nSHL Challenge 2020 (the first place if not considering multiple models fusion\napproach). The proposed method is further improved in this paper and achieves\n80.72$\\%$ accuracy, better than the existing methods using a single model.\n", "versions": [{"version": "v1", "created": "Sun, 1 Nov 2020 01:28:23 GMT"}, {"version": "v2", "created": "Fri, 6 Nov 2020 03:02:53 GMT"}], "update_date": "2020-11-09", "authors_parsed": [["Zhao", "Beidi", ""], ["Li", "Shuai", ""], ["Gao", "Yanbo", ""], ["Li", "Chuankun", ""], ["Li", "Wanqing", ""]]}, {"id": "2011.00399", "submitter": "Zhaoen Su", "authors": "Zhaoen Su, Chao Wang, Henggang Cui, Nemanja Djuric, Carlos\n  Vallespi-Gonzalez, David Bradley", "title": "Temporally-Continuous Probabilistic Prediction using Polynomial\n  Trajectory Parameterization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A commonly-used representation for motion prediction of actors is a sequence\nof waypoints (comprising positions and orientations) for each actor at discrete\nfuture time-points. While this approach is simple and flexible, it can exhibit\nunrealistic higher-order derivatives (such as acceleration) and approximation\nerrors at intermediate time steps. To address this issue we propose a simple\nand general representation for temporally continuous probabilistic trajectory\nprediction that is based on polynomial trajectory parameterization. We evaluate\nthe proposed representation on supervised trajectory prediction tasks using two\nlarge self-driving data sets. The results show realistic higher-order\nderivatives and better accuracy at interpolated time-points, as well as the\nbenefits of the inferred noise distributions over the trajectories. Extensive\nexperimental studies based on existing state-of-the-art models demonstrate the\neffectiveness of the proposed approach relative to other representations in\npredicting the future motions of vehicle, bicyclist, and pedestrian traffic\nactors.\n", "versions": [{"version": "v1", "created": "Sun, 1 Nov 2020 01:51:44 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["Su", "Zhaoen", ""], ["Wang", "Chao", ""], ["Cui", "Henggang", ""], ["Djuric", "Nemanja", ""], ["Vallespi-Gonzalez", "Carlos", ""], ["Bradley", "David", ""]]}, {"id": "2011.00402", "submitter": "Kanji Tanaka", "authors": "Koji Takeda, Kanji Tanaka", "title": "Dark Reciprocal-Rank: Boosting Graph-Convolutional Self-Localization\n  Network via Teacher-to-student Knowledge Transfer", "comments": "5 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In visual robot self-localization, graph-based scene representation and\nmatching have recently attracted research interest as robust and discriminative\nmethods for selflocalization. Although effective, their computational and\nstorage costs do not scale well to large-size environments. To alleviate this\nproblem, we formulate self-localization as a graph classification problem and\nattempt to use the graph convolutional neural network (GCN) as a graph\nclassification engine. A straightforward approach is to use visual feature\ndescriptors that are employed by state-of-the-art self-localization systems,\ndirectly as graph node features. However, their superior performance in the\noriginal self-localization system may not necessarily be replicated in\nGCN-based self-localization. To address this issue, we introduce a novel\nteacher-to-student knowledge-transfer scheme based on rank matching, in which\nthe reciprocal-rank vector output by an off-the-shelf state-of-the-art teacher\nself-localization model is used as the dark knowledge to transfer. Experiments\nindicate that the proposed graph-convolutional self-localization network can\nsignificantly outperform state-of-the-art self-localization systems, as well as\nthe teacher classifier.\n", "versions": [{"version": "v1", "created": "Sun, 1 Nov 2020 02:08:43 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["Takeda", "Koji", ""], ["Tanaka", "Kanji", ""]]}, {"id": "2011.00427", "submitter": "Xiaochen Liu", "authors": "Xiaochen Liu", "title": "Efficient Pipelines for Vision-Based Context Sensing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Context awareness is an essential part of mobile and ubiquitous computing.\nIts goal is to unveil situational information about mobile users like locations\nand activities. The sensed context can enable many services like navigation,\nAR, and smarting shopping. Such context can be sensed in different ways\nincluding visual sensors. There is an emergence of vision sources deployed\nworldwide. The cameras could be installed on roadside, in-house, and on mobile\nplatforms. This trend provides huge amount of vision data that could be used\nfor context sensing. However, the vision data collection and analytics are\nstill highly manual today. It is hard to deploy cameras at large scale for data\ncollection. Organizing and labeling context from the data are also labor\nintensive. In recent years, advanced vision algorithms and deep neural networks\nare used to help analyze vision data. But this approach is limited by data\nquality, labeling effort, and dependency on hardware resources. In summary,\nthere are three major challenges for today's vision-based context sensing\nsystems: data collection and labeling at large scale, process large data\nvolumes efficiently with limited hardware resources, and extract accurate\ncontext out of vision data. The thesis explores the design space that consists\nof three dimensions: sensing task, sensor types, and task locations. Our prior\nwork explores several points in this design space. We make contributions by (1)\ndeveloping efficient and scalable solutions for different points in the design\nspace of vision-based sensing tasks; (2) achieving state-of-the-art accuracy in\nthose applications; (3) and developing guidelines for designing such sensing\nsystems.\n", "versions": [{"version": "v1", "created": "Sun, 1 Nov 2020 05:09:13 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["Liu", "Xiaochen", ""]]}, {"id": "2011.00428", "submitter": "Xikai Yang", "authors": "Xikai Yang, Yong Long, Saiprasad Ravishankar", "title": "Two-layer clustering-based sparsifying transform learning for low-dose\n  CT reconstruction", "comments": "5 pages, 3 figures, submitted to ISBI2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Achieving high-quality reconstructions from low-dose computed tomography\n(LDCT) measurements is of much importance in clinical settings. Model-based\nimage reconstruction methods have been proven to be effective in removing\nartifacts in LDCT. In this work, we propose an approach to learn a rich\ntwo-layer clustering-based sparsifying transform model (MCST2), where image\npatches and their subsequent feature maps (filter residuals) are clustered into\ngroups with different learned sparsifying filters per group. We investigate a\npenalized weighted least squares (PWLS) approach for LDCT reconstruction\nincorporating learned MCST2 priors. Experimental results show the superior\nperformance of the proposed PWLS-MCST2 approach compared to other related\nrecent schemes.\n", "versions": [{"version": "v1", "created": "Sun, 1 Nov 2020 05:15:37 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["Yang", "Xikai", ""], ["Long", "Yong", ""], ["Ravishankar", "Saiprasad", ""]]}, {"id": "2011.00443", "submitter": "Varun Behera", "authors": "Ashish Ranjan, Varun Nagesh Jolly Behera, Motahar Reza", "title": "A Parallel Approach for Real-Time Face Recognition from a Large Database", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new facial recognition system, capable of identifying a person,\nprovided their likeness has been previously stored in the system, in real time.\nThe system is based on storing and comparing facial embeddings of the subject,\nand identifying them later within a live video feed. This system is highly\naccurate, and is able to tag people with their ID in real time. It is able to\ndo so, even when using a database containing thousands of facial embeddings, by\nusing a parallelized searching technique. This makes the system quite fast and\nallows it to be highly scalable.\n", "versions": [{"version": "v1", "created": "Sun, 1 Nov 2020 07:40:10 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["Ranjan", "Ashish", ""], ["Behera", "Varun Nagesh Jolly", ""], ["Reza", "Motahar", ""]]}, {"id": "2011.00450", "submitter": "Dzung Doan Anh", "authors": "Anh-Dzung Doan, Yasir Latif, Tat-Jun Chin, Ian Reid", "title": "HM4: Hidden Markov Model with Memory Management for Visual Place\n  Recognition", "comments": "Accepted for publication by IEEE Robotics and Automation Letters", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual place recognition needs to be robust against appearance variability\ndue to natural and man-made causes. Training data collection should thus be an\nongoing process to allow continuous appearance changes to be recorded. However,\nthis creates an unboundedly-growing database that poses time and memory\nscalability challenges for place recognition methods. To tackle the scalability\nissue for visual place recognition in autonomous driving, we develop a Hidden\nMarkov Model approach with a two-tiered memory management. Our algorithm,\ndubbed HM$^4$, exploits temporal look-ahead to transfer promising candidate\nimages between passive storage and active memory when needed. The inference\nprocess takes into account both promising images and a coarse representations\nof the full database. We show that this allows constant time and space\ninference for a fixed coverage area. The coarse representations can also be\nupdated incrementally to absorb new data. To further reduce the memory\nrequirements, we derive a compact image representation inspired by Locality\nSensitive Hashing (LSH). Through experiments on real world data, we demonstrate\nthe excellent scalability and accuracy of the approach under appearance changes\nand provide comparisons against state-of-the-art techniques.\n", "versions": [{"version": "v1", "created": "Sun, 1 Nov 2020 08:49:24 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["Doan", "Anh-Dzung", ""], ["Latif", "Yasir", ""], ["Chin", "Tat-Jun", ""], ["Reid", "Ian", ""]]}, {"id": "2011.00454", "submitter": "Xiaoyu Cui", "authors": "Fengying Che, Ruichuan Shi, Jian Wu, Haoran Li, Shuqin Li, Weixing\n  Chen, Hao Zhang, Zhi Li, and Xiaoyu Cui (Member, IEEE)", "title": "Dynamic radiomics: a new methodology to extract quantitative\n  time-related features from tomographic images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The feature extraction methods of radiomics are mainly based on static\ntomographic images at a certain moment, while the occurrence and development of\ndisease is a dynamic process that cannot be fully reflected by only static\ncharacteristics. This study proposes a new dynamic radiomics feature extraction\nworkflow that uses time-dependent tomographic images of the same patient,\nfocuses on the changes in image features over time, and then quantifies them as\nnew dynamic features for diagnostic or prognostic evaluation. We first define\nthe mathematical paradigm of dynamic radiomics and introduce three specific\nmethods that can describe the transformation process of features over time.\nThree different clinical problems are used to validate the performance of the\nproposed dynamic feature with conventional 2D and 3D static features.\n", "versions": [{"version": "v1", "created": "Sun, 1 Nov 2020 09:23:16 GMT"}, {"version": "v2", "created": "Mon, 21 Dec 2020 08:33:59 GMT"}, {"version": "v3", "created": "Thu, 3 Jun 2021 07:52:06 GMT"}], "update_date": "2021-06-04", "authors_parsed": [["Che", "Fengying", "", "Member, IEEE"], ["Shi", "Ruichuan", "", "Member, IEEE"], ["Wu", "Jian", "", "Member, IEEE"], ["Li", "Haoran", "", "Member, IEEE"], ["Li", "Shuqin", "", "Member, IEEE"], ["Chen", "Weixing", "", "Member, IEEE"], ["Zhang", "Hao", "", "Member, IEEE"], ["Li", "Zhi", "", "Member, IEEE"], ["Cui", "Xiaoyu", "", "Member, IEEE"]]}, {"id": "2011.00496", "submitter": "Niv Pekar", "authors": "Niv Pekar, Yaniv Benny, Lior Wolf", "title": "Generating Correct Answers for Progressive Matrices Intelligence Tests", "comments": "To appear in the 34th Conference on Neural Information Processing\n  Systems (NeurIPS 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Raven's Progressive Matrices are multiple-choice intelligence tests, where\none tries to complete the missing location in a $3\\times 3$ grid of abstract\nimages. Previous attempts to address this test have focused solely on selecting\nthe right answer out of the multiple choices. In this work, we focus, instead,\non generating a correct answer given the grid, without seeing the choices,\nwhich is a harder task, by definition. The proposed neural model combines\nmultiple advances in generative models, including employing multiple pathways\nthrough the same network, using the reparameterization trick along two pathways\nto make their encoding compatible, a dynamic application of variational losses,\nand a complex perceptual loss that is coupled with a selective backpropagation\nprocedure. Our algorithm is able not only to generate a set of plausible\nanswers, but also to be competitive to the state of the art methods in\nmultiple-choice tests.\n", "versions": [{"version": "v1", "created": "Sun, 1 Nov 2020 13:21:07 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["Pekar", "Niv", ""], ["Benny", "Yaniv", ""], ["Wolf", "Lior", ""]]}, {"id": "2011.00526", "submitter": "Xiangde Luo", "authors": "Xu Chen and Xiangde Luo and Yitian Zhao and Shaoting Zhang and Guotai\n  Wang and Yalin Zheng", "title": "Learning Euler's Elastica Model for Medical Image Segmentation", "comments": "9 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image segmentation is a fundamental topic in image processing and has been\nstudied for many decades. Deep learning-based supervised segmentation models\nhave achieved state-of-the-art performance but most of them are limited by\nusing pixel-wise loss functions for training without geometrical constraints.\nInspired by Euler's Elastica model and recent active contour models introduced\ninto the field of deep learning, we propose a novel active contour with\nelastica (ACE) loss function incorporating Elastica (curvature and length) and\nregion information as geometrically-natural constraints for the image\nsegmentation tasks. We introduce the mean curvature i.e. the average of all\nprincipal curvatures, as a more effective image prior to representing curvature\nin our ACE loss function. Furthermore, based on the definition of the mean\ncurvature, we propose a fast solution to approximate the ACE loss in\nthree-dimensional (3D) by using Laplace operators for 3D image segmentation. We\nevaluate our ACE loss function on four 2D and 3D natural and biomedical image\ndatasets. Our results show that the proposed loss function outperforms other\nmainstream loss functions on different segmentation networks. Our source code\nis available at https://github.com/HiLab-git/ACELoss.\n", "versions": [{"version": "v1", "created": "Sun, 1 Nov 2020 15:14:37 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["Chen", "Xu", ""], ["Luo", "Xiangde", ""], ["Zhao", "Yitian", ""], ["Zhang", "Shaoting", ""], ["Wang", "Guotai", ""], ["Zheng", "Yalin", ""]]}, {"id": "2011.00527", "submitter": "Taimur Hassan", "authors": "Taimur Hassan and Bilal Hassan and Ayman El-Baz and Naoufel Werghi", "title": "A Dilated Residual Hierarchically Fashioned Segmentation Framework for\n  Extracting Gleason Tissues and Grading Prostate Cancer from Whole Slide\n  Images", "comments": "Accepted in IEEE SAS-2021, Source Code is available at\n  https://github.com/taimurhassan/cancer", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Prostate cancer (PCa) is the second deadliest form of cancer in males, and it\ncan be clinically graded by examining the structural representations of Gleason\ntissues. This paper proposes \\RV{a new method} for segmenting the Gleason\ntissues \\RV{(patch-wise) in order to grade PCa from the whole slide images\n(WSI).} Also, the proposed approach encompasses two main contributions: 1) A\nsynergy of hybrid dilation factors and hierarchical decomposition of latent\nspace representation for effective Gleason tissues extraction, and 2) A\nthree-tiered loss function which can penalize different semantic segmentation\nmodels for accurately extracting the highly correlated patterns. In addition to\nthis, the proposed framework has been extensively evaluated on a large-scale\nPCa dataset containing 10,516 whole slide scans (with around 71.7M patches),\nwhere it outperforms state-of-the-art schemes by 3.22% (in terms of mean\nintersection-over-union) for extracting the Gleason tissues and 6.91% (in terms\nof F1 score) for grading the progression of PCa.\n", "versions": [{"version": "v1", "created": "Sun, 1 Nov 2020 15:15:30 GMT"}, {"version": "v2", "created": "Wed, 13 Jan 2021 18:50:31 GMT"}, {"version": "v3", "created": "Mon, 25 Jan 2021 17:40:29 GMT"}, {"version": "v4", "created": "Sun, 23 May 2021 11:51:22 GMT"}, {"version": "v5", "created": "Sun, 25 Jul 2021 14:12:01 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Hassan", "Taimur", ""], ["Hassan", "Bilal", ""], ["El-Baz", "Ayman", ""], ["Werghi", "Naoufel", ""]]}, {"id": "2011.00551", "submitter": "Victor Zuanazzi", "authors": "Victor Zuanazzi, Joris van Vugt, Olaf Booij and Pascal Mettes", "title": "Adversarial Self-Supervised Scene Flow Estimation", "comments": "Published at 3DV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  This work proposes a metric learning approach for self-supervised scene flow\nestimation. Scene flow estimation is the task of estimating 3D flow vectors for\nconsecutive 3D point clouds. Such flow vectors are fruitful, \\eg for\nrecognizing actions, or avoiding collisions. Training a neural network via\nsupervised learning for scene flow is impractical, as this requires manual\nannotations for each 3D point at each new timestamp for each scene. To that\nend, we seek for a self-supervised approach, where a network learns a latent\nmetric to distinguish between points translated by flow estimations and the\ntarget point cloud. Our adversarial metric learning includes a multi-scale\ntriplet loss on sequences of two-point clouds as well as a cycle consistency\nloss. Furthermore, we outline a benchmark for self-supervised scene flow\nestimation: the Scene Flow Sandbox. The benchmark consists of five datasets\ndesigned to study individual aspects of flow estimation in progressive order of\ncomplexity, from a moving object to real-world scenes. Experimental evaluation\non the benchmark shows that our approach obtains state-of-the-art\nself-supervised scene flow results, outperforming recent neighbor-based\napproaches. We use our proposed benchmark to expose shortcomings and draw\ninsights on various training setups. We find that our setup captures motion\ncoherence and preserves local geometries. Dealing with occlusions, on the other\nhand, is still an open challenge.\n", "versions": [{"version": "v1", "created": "Sun, 1 Nov 2020 16:37:37 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["Zuanazzi", "Victor", ""], ["van Vugt", "Joris", ""], ["Booij", "Olaf", ""], ["Mettes", "Pascal", ""]]}, {"id": "2011.00553", "submitter": "Guoliang Liu Prof. Dr.", "authors": "Guoliang Liu, Qinghui Zhang, Yichao Cao, Junwei Li, Hao Wu and Guohui\n  Tian", "title": "Memory Group Sampling Based Online Action Recognition Using Kinetic\n  Skeleton Features", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Online action recognition is an important task for human centered intelligent\nservices, which is still difficult to achieve due to the varieties and\nuncertainties of spatial and temporal scales of human actions. In this paper,\nwe propose two core ideas to handle the online action recognition problem.\nFirst, we combine the spatial and temporal skeleton features to depict the\nactions, which include not only the geometrical features, but also multi-scale\nmotion features, such that both the spatial and temporal information of the\naction are covered. Second, we propose a memory group sampling method to\ncombine the previous action frames and current action frames, which is based on\nthe truth that the neighbouring frames are largely redundant, and the sampling\nmechanism ensures that the long-term contextual information is also considered.\nFinally, an improved 1D CNN network is employed for training and testing using\nthe features from sampled frames. The comparison results to the state of the\nart methods using the public datasets show that the proposed method is fast and\nefficient, and has competitive performance\n", "versions": [{"version": "v1", "created": "Sun, 1 Nov 2020 16:43:08 GMT"}, {"version": "v2", "created": "Tue, 3 Nov 2020 05:09:10 GMT"}], "update_date": "2020-11-04", "authors_parsed": [["Liu", "Guoliang", ""], ["Zhang", "Qinghui", ""], ["Cao", "Yichao", ""], ["Li", "Junwei", ""], ["Wu", "Hao", ""], ["Tian", "Guohui", ""]]}, {"id": "2011.00566", "submitter": "Hang Zhou", "authors": "Hang Zhou, Dongdong Chen, Jing Liao, Weiming Zhang, Kejiang Chen,\n  Xiaoyi Dong, Kunlin Liu, Gang Hua and Nenghai Yu", "title": "LG-GAN: Label Guided Adversarial Network for Flexible Targeted Attack of\n  Point Cloud-based Deep Networks", "comments": "CVPR 2020, code available at: https://github.com/RyanHangZhou/LG-GAN", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks have made tremendous progress in 3D point-cloud\nrecognition. Recent works have shown that these 3D recognition networks are\nalso vulnerable to adversarial samples produced from various attack methods,\nincluding optimization-based 3D Carlini-Wagner attack, gradient-based iterative\nfast gradient method, and skeleton-detach based point-dropping. However, after\na careful analysis, these methods are either extremely slow because of the\noptimization/iterative scheme, or not flexible to support targeted attack of a\nspecific category. To overcome these shortcomings, this paper proposes a novel\nlabel guided adversarial network (LG-GAN) for real-time flexible targeted point\ncloud attack. To the best of our knowledge, this is the first generation based\n3D point cloud attack method. By feeding the original point clouds and target\nattack label into LG-GAN, it can learn how to deform the point clouds to\nmislead the recognition network into the specific label only with a single\nforward pass. In detail, LGGAN first leverages one multi-branch adversarial\nnetwork to extract hierarchical features of the input point clouds, then\nincorporates the specified label information into multiple intermediate\nfeatures using the label encoder. Finally, the encoded features will be fed\ninto the coordinate reconstruction decoder to generate the target adversarial\nsample. By evaluating different point-cloud recognition models (e.g., PointNet,\nPointNet++ and DGCNN), we demonstrate that the proposed LG-GAN can support\nflexible targeted attack on the fly while guaranteeing good attack performance\nand higher efficiency simultaneously.\n", "versions": [{"version": "v1", "created": "Sun, 1 Nov 2020 17:17:10 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["Zhou", "Hang", ""], ["Chen", "Dongdong", ""], ["Liao", "Jing", ""], ["Zhang", "Weiming", ""], ["Chen", "Kejiang", ""], ["Dong", "Xiaoyi", ""], ["Liu", "Kunlin", ""], ["Hua", "Gang", ""], ["Yu", "Nenghai", ""]]}, {"id": "2011.00569", "submitter": "C.-H. Huck Yang", "authors": "Jia-Hong Huang, Chao-Han Huck Yang, Fangyu Liu, Meng Tian, Yi-Chieh\n  Liu, Ting-Wei Wu, I-Hung Lin, Kang Wang, Hiromasa Morikawa, Hernghua Chang,\n  Jesper Tegner, Marcel Worring", "title": "DeepOpht: Medical Report Generation for Retinal Images via Deep Models\n  and Visual Explanation", "comments": "Accepted to IEEE WACV 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CL cs.LG cs.MM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this work, we propose an AI-based method that intends to improve the\nconventional retinal disease treatment procedure and help ophthalmologists\nincrease diagnosis efficiency and accuracy. The proposed method is composed of\na deep neural networks-based (DNN-based) module, including a retinal disease\nidentifier and clinical description generator, and a DNN visual explanation\nmodule. To train and validate the effectiveness of our DNN-based module, we\npropose a large-scale retinal disease image dataset. Also, as ground truth, we\nprovide a retinal image dataset manually labeled by ophthalmologists to\nqualitatively show, the proposed AI-based method is effective. With our\nexperimental results, we show that the proposed method is quantitatively and\nqualitatively effective. Our method is capable of creating meaningful retinal\nimage descriptions and visual explanations that are clinically relevant.\n", "versions": [{"version": "v1", "created": "Sun, 1 Nov 2020 17:28:12 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["Huang", "Jia-Hong", ""], ["Yang", "Chao-Han Huck", ""], ["Liu", "Fangyu", ""], ["Tian", "Meng", ""], ["Liu", "Yi-Chieh", ""], ["Wu", "Ting-Wei", ""], ["Lin", "I-Hung", ""], ["Wang", "Kang", ""], ["Morikawa", "Hiromasa", ""], ["Chang", "Hernghua", ""], ["Tegner", "Jesper", ""], ["Worring", "Marcel", ""]]}, {"id": "2011.00574", "submitter": "Omid Taheri", "authors": "Omid Taheri, Hassan Salarieh, Aria Alasty", "title": "Human Leg Motion Tracking by Fusing IMUs and RGB Camera Data Using\n  Extended Kalman Filter", "comments": "This paper results from O. Taheri's MSc Thesis (2017) at the Sharif\n  University of Technology", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human motion capture is frequently used to study rehabilitation and clinical\nproblems, as well as to provide realistic animation for the entertainment\nindustry. IMU-based systems, as well as Marker-based motion tracking systems,\nare the most popular methods to track movement due to their low cost of\nimplementation and lightweight. This paper proposes a quaternion-based Extended\nKalman filter approach to recover the human leg segments motions with a set of\nIMU sensors data fused with camera-marker system data. In this paper, an\nExtended Kalman Filter approach is developed to fuse the data of two IMUs and\none RGB camera for human leg motion tracking. Based on the complementary\nproperties of the inertial sensors and camera-marker system, in the introduced\nnew measurement model, the orientation data of the upper leg and the lower leg\nis updated through three measurement equations. The positioning of the human\nbody is made possible by the tracked position of the pelvis joint by the camera\nmarker system. A mathematical model has been utilized to estimate joints' depth\nin 2D images. The efficiency of the proposed algorithm is evaluated by an\noptical motion tracker system.\n", "versions": [{"version": "v1", "created": "Sun, 1 Nov 2020 17:54:53 GMT"}, {"version": "v2", "created": "Mon, 7 Dec 2020 22:20:27 GMT"}], "update_date": "2020-12-09", "authors_parsed": [["Taheri", "Omid", ""], ["Salarieh", "Hassan", ""], ["Alasty", "Aria", ""]]}, {"id": "2011.00577", "submitter": "Kyo Takano", "authors": "Kyo Takano", "title": "FusiformNet: Extracting Discriminative Facial Features on Different\n  Levels", "comments": "FusiformNet for unsupervised feature extraction", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Over the last several years, research on facial recognition based on Deep\nNeural Network has evolved with approaches like task-specific loss functions,\nimage normalization and augmentation, network architectures, etc. However,\nthere have been few approaches with attention to how human faces differ from\nperson to person. Premising that inter-personal differences are found both\ngenerally and locally on the human face, I propose FusiformNet, a novel\nframework for feature extraction that leverages the nature of discriminative\nfacial features. Tested on Image-Unrestricted setting of Labeled Faces in the\nWild benchmark, this method achieved a state-of-the-art accuracy of 96.67%\nwithout labeled outside data, image augmentation, normalization, or special\nloss functions. Likewise, the method also performed on a par with previous\nstate-of-the-arts when pre-trained on CASIA-WebFace dataset. Considering its\nability to extract both general and local facial features, the utility of\nFusiformNet may not be limited to facial recognition but also extend to other\nDNN-based tasks.\n", "versions": [{"version": "v1", "created": "Sun, 1 Nov 2020 18:00:59 GMT"}, {"version": "v2", "created": "Fri, 6 Nov 2020 19:32:42 GMT"}, {"version": "v3", "created": "Thu, 26 Nov 2020 15:49:48 GMT"}], "update_date": "2020-11-30", "authors_parsed": [["Takano", "Kyo", ""]]}, {"id": "2011.00597", "submitter": "Simon Ging", "authors": "Simon Ging (1), Mohammadreza Zolfaghari (1), Hamed Pirsiavash (2),\n  Thomas Brox (1) ((1) University of Freiburg, (2) University of Maryland\n  Baltimore County)", "title": "COOT: Cooperative Hierarchical Transformer for Video-Text Representation\n  Learning", "comments": "27 pages, 5 figures, 19 tables. To be published in the 34th\n  conference on Neural Information Processing Systems (NeurIPS 2020). The first\n  two authors contributed equally to this work", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Many real-world video-text tasks involve different levels of granularity,\nsuch as frames and words, clip and sentences or videos and paragraphs, each\nwith distinct semantics. In this paper, we propose a Cooperative hierarchical\nTransformer (COOT) to leverage this hierarchy information and model the\ninteractions between different levels of granularity and different modalities.\nThe method consists of three major components: an attention-aware feature\naggregation layer, which leverages the local temporal context (intra-level,\ne.g., within a clip), a contextual transformer to learn the interactions\nbetween low-level and high-level semantics (inter-level, e.g. clip-video,\nsentence-paragraph), and a cross-modal cycle-consistency loss to connect video\nand text. The resulting method compares favorably to the state of the art on\nseveral benchmarks while having few parameters. All code is available\nopen-source at https://github.com/gingsi/coot-videotext\n", "versions": [{"version": "v1", "created": "Sun, 1 Nov 2020 18:54:09 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["Ging", "Simon", ""], ["Zolfaghari", "Mohammadreza", ""], ["Pirsiavash", "Hamed", ""], ["Brox", "Thomas", ""]]}, {"id": "2011.00608", "submitter": "Mike Kasper", "authors": "Mike Kasper, Fernando Nobre, Christoffer Heckman, Nima Keivan", "title": "Unsupervised Metric Relocalization Using Transform Consistency Loss", "comments": "Accepted for publication in the 4th Conference on Robot Learning\n  (CoRL 2020), Cambridge MA, USA", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Training networks to perform metric relocalization traditionally requires\naccurate image correspondences. In practice, these are obtained by restricting\ndomain coverage, employing additional sensors, or capturing large multi-view\ndatasets. We instead propose a self-supervised solution, which exploits a key\ninsight: localizing a query image within a map should yield the same absolute\npose, regardless of the reference image used for registration. Guided by this\nintuition, we derive a novel transform consistency loss. Using this loss\nfunction, we train a deep neural network to infer dense feature and saliency\nmaps to perform robust metric relocalization in dynamic environments. We\nevaluate our framework on synthetic and real-world data, showing our approach\noutperforms other supervised methods when a limited amount of ground-truth\ninformation is available.\n", "versions": [{"version": "v1", "created": "Sun, 1 Nov 2020 19:24:27 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["Kasper", "Mike", ""], ["Nobre", "Fernando", ""], ["Heckman", "Christoffer", ""], ["Keivan", "Nima", ""]]}, {"id": "2011.00618", "submitter": "Sunder Ali Khowaja", "authors": "Kapal Dev, Sunder Ali Khowaja, Ankur Singh Bist, Vaibhav Saini, Surbhi\n  Bhatia", "title": "Triage of Potential COVID-19 Patients from Chest X-ray Images using\n  Hierarchical Convolutional Networks", "comments": "23 pages, 9 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  The current COVID-19 pandemic has motivated the researchers to use artificial\nintelligence techniques for a potential alternative to reverse\ntranscription-polymerase chain reaction (RT-PCR) due to the limited scale of\ntesting. The chest X-ray (CXR) is one of the alternatives to achieve fast\ndiagnosis but the unavailability of large-scale annotated data makes the\nclinical implementation of machine learning-based COVID detection difficult.\nAnother issue is the usage of ImageNet pre-trained networks which does not\nextract reliable feature representations from medical images. In this paper, we\npropose the use of hierarchical convolutional network (HCN) architecture to\nnaturally augment the data along with diversified features. The HCN uses the\nfirst convolution layer from COVIDNet followed by the convolutional layers from\nwell-known pre-trained networks to extract the features. The use of the\nconvolution layer from COVIDNet ensures the extraction of representations\nrelevant to the CXR modality. We also propose the use of ECOC for encoding\nmulticlass problems to binary classification for improving the recognition\nperformance. Experimental results show that HCN architecture is capable of\nachieving better results in comparison to the existing studies. The proposed\nmethod can accurately triage potential COVID-19 patients through CXR images for\nsharing the testing load and increasing the testing capacity.\n", "versions": [{"version": "v1", "created": "Sun, 1 Nov 2020 20:01:22 GMT"}, {"version": "v2", "created": "Tue, 15 Dec 2020 15:47:46 GMT"}], "update_date": "2020-12-16", "authors_parsed": [["Dev", "Kapal", ""], ["Khowaja", "Sunder Ali", ""], ["Bist", "Ankur Singh", ""], ["Saini", "Vaibhav", ""], ["Bhatia", "Surbhi", ""]]}, {"id": "2011.00627", "submitter": "Yixuan Wang", "authors": "Yixuan Wang, Dale McConachie, Dmitry Berenson", "title": "Tracking Partially-Occluded Deformable Objects while Enforcing Geometric\n  Constraints", "comments": "7 pages, 5 figures, submitted to 2021 International Conference on\n  Robotics and Automation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In order to manipulate a deformable object, such as rope or cloth, in\nunstructured environments, robots need a way to estimate its current shape.\nHowever, tracking the shape of a deformable object can be challenging because\nof the object's high flexibility, (self-)occlusion, and interaction with\nobstacles. Building a high-fidelity physics simulation to aid in tracking is\ndifficult for novel environments. Instead we focus on tracking the object based\non RGBD images and geometric motion estimates and obstacles. Our key\ncontributions over previous work in this vein are: 1) A better way to handle\nsevere occlusion by using a motion model to regularize the tracking estimate;\nand 2) The formulation of \\textit{convex} geometric constraints, which allow us\nto prevent self-intersection and penetration into known obstacles via a\npost-processing step. These contributions allow us to outperform previous\nmethods by a large margin in terms of accuracy in scenarios with severe\nocclusion and obstacles.\n", "versions": [{"version": "v1", "created": "Sun, 1 Nov 2020 21:13:18 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["Wang", "Yixuan", ""], ["McConachie", "Dale", ""], ["Berenson", "Dmitry", ""]]}, {"id": "2011.00628", "submitter": "Shadrokh Samavi", "authors": "Zahra SobhaniNia, Nader Karimi, Pejman Khadivi, Roshank Roshandel,\n  Shadrokh Samavi", "title": "Brain Tumor Classification Using Medial Residual Encoder Layers", "comments": "7 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  According to the World Health Organization, cancer is the second leading\ncause of death worldwide, responsible for over 9.5 million deaths in 2018\nalone. Brain tumors count for one out of every four cancer deaths. Accurate and\ntimely diagnosis of brain tumors will lead to more effective treatments. To\ndate, several image classification approaches have been proposed to aid\ndiagnosis and treatment. We propose an encoder layer that uses post-max-pooling\nfeatures for residual learning. Our approach shows promising results by\nimproving the tumor classification accuracy in MR images using a limited\nmedical image dataset. Experimental evaluations of this model on a dataset\nconsisting of 3064 MR images show 95-98% accuracy, which is better than\nprevious studies on this database.\n", "versions": [{"version": "v1", "created": "Sun, 1 Nov 2020 21:19:38 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["SobhaniNia", "Zahra", ""], ["Karimi", "Nader", ""], ["Khadivi", "Pejman", ""], ["Roshandel", "Roshank", ""], ["Samavi", "Shadrokh", ""]]}, {"id": "2011.00631", "submitter": "Shadrokh Samavi", "authors": "Parham Yazdekhasty, Ali Zindar, Zahra Nabizadeh-ShahreBabak, Roshank\n  Roshandel, Pejman Khadivi, Nader Karimi, Shadrokh Samavi", "title": "Bifurcated Autoencoder for Segmentation of COVID-19 Infected Regions in\n  CT Images", "comments": "11 pages, 3 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The new coronavirus infection has shocked the world since early 2020 with its\naggressive outbreak. Rapid detection of the disease saves lives, and relying on\nmedical imaging (Computed Tomography and X-ray) to detect infected lungs has\nshown to be effective. Deep learning and convolutional neural networks have\nbeen used for image analysis in this context. However, accurate identification\nof infected regions has proven challenging for two main reasons. Firstly, the\ncharacteristics of infected areas differ in different images. Secondly,\ninsufficient training data makes it challenging to train various machine\nlearning algorithms, including deep-learning models. This paper proposes an\napproach to segment lung regions infected by COVID-19 to help cardiologists\ndiagnose the disease more accurately, faster, and more manageable. We propose a\nbifurcated 2-D model for two types of segmentation. This model uses a shared\nencoder and a bifurcated connection to two separate decoders. One decoder is\nfor segmentation of the healthy region of the lungs, while the other is for the\nsegmentation of the infected regions. Experiments on publically available\nimages show that the bifurcated structure segments infected regions of the\nlungs better than state of the art.\n", "versions": [{"version": "v1", "created": "Sun, 1 Nov 2020 21:32:00 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["Yazdekhasty", "Parham", ""], ["Zindar", "Ali", ""], ["Nabizadeh-ShahreBabak", "Zahra", ""], ["Roshandel", "Roshank", ""], ["Khadivi", "Pejman", ""], ["Karimi", "Nader", ""], ["Samavi", "Shadrokh", ""]]}, {"id": "2011.00652", "submitter": "Guojun Wang", "authors": "Guojun Wang, Bin Tian, Yachen Zhang, Long Chen, Dongpu Cao, Jian Wu", "title": "Multi-View Adaptive Fusion Network for 3D Object Detection", "comments": "11 pages,9 figures. We change the CV backbone and the details of\n  network to improve performance. Submitted to IEEE transactions on intelligent\n  transportation systems", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  3D object detection based on LiDAR-camera fusion is becoming an emerging\nresearch theme for autonomous driving. However, it has been surprisingly\ndifficult to effectively fuse both modalities without information loss and\ninterference. To solve this issue, we propose a single-stage multi-view fusion\nframework that takes LiDAR bird's-eye view, LiDAR range view and camera view\nimages as inputs for 3D object detection. To effectively fuse multi-view\nfeatures, we propose an attentive pointwise fusion (APF) module to estimate the\nimportance of the three sources with attention mechanisms that can achieve\nadaptive fusion of multi-view features in a pointwise manner. Furthermore, an\nattentive pointwise weighting (APW) module is designed to help the network\nlearn structure information and point feature importance with two extra tasks,\nnamely, foreground classification and center regression, and the predicted\nforeground probability is used to reweight the point features. We design an\nend-to-end learnable network named MVAF-Net to integrate these two components.\nOur evaluations conducted on the KITTI 3D object detection datasets demonstrate\nthat the proposed APF and APW modules offer significant performance gains.\nMoreover, the proposed MVAF-Net achieves the best performance among all\nsingle-stage fusion methods and outperforms most two-stage fusion methods,\nachieving the best trade-off between speed and accuracy on the KITTI benchmark.\n", "versions": [{"version": "v1", "created": "Mon, 2 Nov 2020 00:06:01 GMT"}, {"version": "v2", "created": "Tue, 8 Dec 2020 03:54:51 GMT"}], "update_date": "2020-12-09", "authors_parsed": [["Wang", "Guojun", ""], ["Tian", "Bin", ""], ["Zhang", "Yachen", ""], ["Chen", "Long", ""], ["Cao", "Dongpu", ""], ["Wu", "Jian", ""]]}, {"id": "2011.00674", "submitter": "Byungju Kim", "authors": "Byungju Kim, Junho Yim and Junmo Kim", "title": "Highway Driving Dataset for Semantic Video Segmentation", "comments": "published on BMVC", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scene understanding is an essential technique in semantic segmentation.\nAlthough there exist several datasets that can be used for semantic\nsegmentation, they are mainly focused on semantic image segmentation with large\ndeep neural networks. Therefore, these networks are not useful for real time\napplications, especially in autonomous driving systems. In order to solve this\nproblem, we make two contributions to semantic segmentation task. The first\ncontribution is that we introduce the semantic video dataset, the Highway\nDriving dataset, which is a densely annotated benchmark for a semantic video\nsegmentation task. The Highway Driving dataset consists of 20 video sequences\nhaving a 30Hz frame rate, and every frame is densely annotated. Secondly, we\npropose a baseline algorithm that utilizes a temporal correlation. Together\nwith our attempt to analyze the temporal correlation, we expect the Highway\nDriving dataset to encourage research on semantic video segmentation.\n", "versions": [{"version": "v1", "created": "Mon, 2 Nov 2020 01:50:52 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["Kim", "Byungju", ""], ["Yim", "Junho", ""], ["Kim", "Junmo", ""]]}, {"id": "2011.00694", "submitter": "Lufei Gao", "authors": "Lufei Gao, Ruisong Zhou, Changfeng Dong, Cheng Feng, Zhen Li, Xiang\n  Wan and Li Liu", "title": "Multi-Modal Active Learning for Automatic Liver Fibrosis Diagnosis based\n  on Ultrasound Shear Wave Elastography", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the development of radiomics, noninvasive diagnosis like ultrasound (US)\nimaging plays a very important role in automatic liver fibrosis diagnosis\n(ALFD). Due to the noisy data, expensive annotations of US images, the\napplication of Artificial Intelligence (AI) assisting approaches encounters a\nbottleneck. Besides, the use of mono-modal US data limits the further improve\nof the classification results. In this work, we innovatively propose a\nmulti-modal fusion network with active learning (MMFN-AL) for ALFD to exploit\nthe information of multiple modalities, eliminate the noisy data and reduce the\nannotation cost. Four image modalities including US and three types of shear\nwave elastography (SWEs) are exploited. A new dataset containing these\nmodalities from 214 candidates is well-collected and pre-processed, with the\nlabels obtained from the liver biopsy results. Experimental results show that\nour proposed method outperforms the state-of-the-art performance using less\nthan 30% data, and by using only around 80% data, the proposed fusion network\nachieves high AUC 89.27% and accuracy 70.59%.\n", "versions": [{"version": "v1", "created": "Mon, 2 Nov 2020 03:05:24 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["Gao", "Lufei", ""], ["Zhou", "Ruisong", ""], ["Dong", "Changfeng", ""], ["Feng", "Cheng", ""], ["Li", "Zhen", ""], ["Wan", "Xiang", ""], ["Liu", "Li", ""]]}, {"id": "2011.00728", "submitter": "Keval Doshi", "authors": "Keval Doshi, Yasin Yilmaz", "title": "Road Damage Detection using Deep Ensemble Learning", "comments": "Submitted to IEEE BigData 2020. arXiv admin note: text overlap with\n  arXiv:2008.13101, arXiv:1811.04535 by other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Road damage detection is critical for the maintenance of a road, which\ntraditionally has been performed using expensive high-performance sensors. With\nthe recent advances in technology, especially in computer vision, it is now\npossible to detect and categorize different types of road damages, which can\nfacilitate efficient maintenance and resource management. In this work, we\npresent an ensemble model for efficient detection and classification of road\ndamages, which we have submitted to the IEEE BigData Cup Challenge 2020. Our\nsolution utilizes a state-of-the-art object detector known as You Only Look\nOnce (YOLO-v4), which is trained on images of various types of road damages\nfrom Czech, Japan and India. Our ensemble approach was extensively tested with\nseveral different model versions and it was able to achieve an F1 score of\n0.628 on the test 1 dataset and 0.6358 on the test 2 dataset.\n", "versions": [{"version": "v1", "created": "Fri, 30 Oct 2020 03:18:14 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["Doshi", "Keval", ""], ["Yilmaz", "Yasin", ""]]}, {"id": "2011.00731", "submitter": "Gary Pui-Tung Choi", "authors": "Ho Law, Gary P. T. Choi, Ka Chun Lam, Lok Ming Lui", "title": "Quasiconformal model with CNN features for large deformation image\n  registration", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CG cs.LG math.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image registration has been widely studied over the past several decades,\nwith numerous applications in science, engineering and medicine. Most of the\nconventional mathematical models for large deformation image registration rely\non prescribed landmarks, which usually require tedious manual labeling and are\nprone to error. In recent years, there has been a surge of interest in the use\nof machine learning for image registration. In this paper, we develop a novel\nmethod for large deformation image registration by a fusion of quasiconformal\ntheory and convolutional neural network (CNN). More specifically, we propose a\nquasiconformal energy model with a novel fidelity term that incorporates the\nfeatures extracted using a pre-trained CNN, thereby allowing us to obtain\nmeaningful registration results without any guidance of prescribed landmarks.\nMoreover, unlike many prior image registration methods, the bijectivity of our\nmethod is guaranteed by quasiconformal theory. Experimental results are\npresented to demonstrate the effectiveness of the proposed method. More\nbroadly, our work sheds light on how rigorous mathematical theories and\npractical machine learning approaches can be integrated for developing\ncomputational methods with improved performance.\n", "versions": [{"version": "v1", "created": "Fri, 30 Oct 2020 14:15:31 GMT"}, {"version": "v2", "created": "Mon, 4 Jan 2021 18:14:53 GMT"}, {"version": "v3", "created": "Mon, 28 Jun 2021 10:59:26 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Law", "Ho", ""], ["Choi", "Gary P. T.", ""], ["Lam", "Ka Chun", ""], ["Lui", "Lok Ming", ""]]}, {"id": "2011.00739", "submitter": "Qingjie Meng", "authors": "Qingjie Meng, Jacqueline Matthew, Veronika A. Zimmer, Alberto Gomez,\n  David F.A. Lloyd, Daniel Rueckert, Bernhard Kainz", "title": "Mutual Information-based Disentangled Neural Networks for Classifying\n  Unseen Categories in Different Domains: Application to Fetal Ultrasound\n  Imaging", "comments": "arXiv admin note: substantial text overlap with arXiv:2003.00321", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks exhibit limited generalizability across images with\ndifferent entangled domain features and categorical features. Learning\ngeneralizable features that can form universal categorical decision boundaries\nacross domains is an interesting and difficult challenge. This problem occurs\nfrequently in medical imaging applications when attempts are made to deploy and\nimprove deep learning models across different image acquisition devices, across\nacquisition parameters or if some classes are unavailable in new training\ndatabases. To address this problem, we propose Mutual Information-based\nDisentangled Neural Networks (MIDNet), which extract generalizable categorical\nfeatures to transfer knowledge to unseen categories in a target domain. The\nproposed MIDNet adopts a semi-supervised learning paradigm to alleviate the\ndependency on labeled data. This is important for real-world applications where\ndata annotation is time-consuming, costly and requires training and expertise.\nWe extensively evaluate the proposed method on fetal ultrasound datasets for\ntwo different image classification tasks where domain features are respectively\ndefined by shadow artifacts and image acquisition devices. Experimental results\nshow that the proposed method outperforms the state-of-the-art on the\nclassification of unseen categories in a target domain with sparsely labeled\ntraining data.\n", "versions": [{"version": "v1", "created": "Fri, 30 Oct 2020 17:32:18 GMT"}, {"version": "v2", "created": "Tue, 6 Apr 2021 17:11:52 GMT"}], "update_date": "2021-04-07", "authors_parsed": [["Meng", "Qingjie", ""], ["Matthew", "Jacqueline", ""], ["Zimmer", "Veronika A.", ""], ["Gomez", "Alberto", ""], ["Lloyd", "David F. A.", ""], ["Rueckert", "Daniel", ""], ["Kainz", "Bernhard", ""]]}, {"id": "2011.00768", "submitter": "Feng Cen", "authors": "Feng Cen (1), Xiaoyu Zhao (1), Wuzhuang Li (1) and Guanghui Wang (2)\n  ((1) The Department of Control Science & Engineering, College of Electronics\n  and Information Engineering, Tongji University, Shanghai 201804, China, (2)\n  Department of Computer Science, Ryerson University, Toronto, ON, Canada M5B\n  2K3)", "title": "Deep Feature Augmentation for Occluded Image Classification", "comments": null, "journal-ref": null, "doi": "10.1016/j.patcog.2020.107737", "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to the difficulty in acquiring massive task-specific occluded images, the\nclassification of occluded images with deep convolutional neural networks\n(CNNs) remains highly challenging. To alleviate the dependency on large-scale\noccluded image datasets, we propose a novel approach to improve the\nclassification accuracy of occluded images by fine-tuning the pre-trained\nmodels with a set of augmented deep feature vectors (DFVs). The set of\naugmented DFVs is composed of original DFVs and pseudo-DFVs. The pseudo-DFVs\nare generated by randomly adding difference vectors (DVs), extracted from a\nsmall set of clean and occluded image pairs, to the real DFVs. In the\nfine-tuning, the back-propagation is conducted on the DFV data flow to update\nthe network parameters. The experiments on various datasets and network\nstructures show that the deep feature augmentation significantly improves the\nclassification accuracy of occluded images without a noticeable influence on\nthe performance of clean images. Specifically, on the ILSVRC2012 dataset with\nsynthetic occluded images, the proposed approach achieves 11.21% and 9.14%\naverage increases in classification accuracy for the ResNet50 networks\nfine-tuned on the occlusion-exclusive and occlusion-inclusive training sets,\nrespectively.\n", "versions": [{"version": "v1", "created": "Mon, 2 Nov 2020 06:25:05 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["Cen", "Feng", ""], ["Zhao", "Xiaoyu", ""], ["Li", "Wuzhuang", ""], ["Wang", "Guanghui", ""]]}, {"id": "2011.00774", "submitter": "Pan Ji", "authors": "Pengfei Fang, Pan Ji, Lars Petersson, Mehrtash Harandi", "title": "Set Augmented Triplet Loss for Video Person Re-Identification", "comments": "to appear in WACV 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern video person re-identification (re-ID) machines are often trained\nusing a metric learning approach, supervised by a triplet loss. The triplet\nloss used in video re-ID is usually based on so-called clip features, each\naggregated from a few frame features. In this paper, we propose to model the\nvideo clip as a set and instead study the distance between sets in the\ncorresponding triplet loss. In contrast to the distance between clip\nrepresentations, the distance between clip sets considers the pair-wise\nsimilarity of each element (i.e., frame representation) between two sets. This\nallows the network to directly optimize the feature representation at a frame\nlevel. Apart from the commonly-used set distance metrics (e.g., ordinary\ndistance and Hausdorff distance), we further propose a hybrid distance metric,\ntailored for the set-aware triplet loss. Also, we propose a hard positive set\nconstruction strategy using the learned class prototypes in a batch. Our\nproposed method achieves state-of-the-art results across several standard\nbenchmarks, demonstrating the advantages of the proposed method.\n", "versions": [{"version": "v1", "created": "Mon, 2 Nov 2020 06:45:14 GMT"}, {"version": "v2", "created": "Sat, 7 Nov 2020 03:37:32 GMT"}], "update_date": "2020-11-10", "authors_parsed": [["Fang", "Pengfei", ""], ["Ji", "Pan", ""], ["Petersson", "Lars", ""], ["Harandi", "Mehrtash", ""]]}, {"id": "2011.00784", "submitter": "Tobias Schlagenhauf", "authors": "Tobias Schlagenhauf, Yefeng Xia, J\\\"urgen Fleischer", "title": "Context-based Image Segment Labeling (CBISL)", "comments": "11 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Working with images, one often faces problems with incomplete or unclear\ninformation. Image inpainting can be used to restore missing image regions but\nfocuses, however, on low-level image features such as pixel intensity, pixel\ngradient orientation, and color. This paper aims to recover semantic image\nfeatures (objects and positions) in images. Based on published gated PixelCNNs,\nwe demonstrate a new approach referred to as quadro-directional PixelCNN to\nrecover missing objects and return probable positions for objects based on the\ncontext. We call this approach context-based image segment labeling (CBISL).\nThe results suggest that our four-directional model outperforms one-directional\nmodels (gated PixelCNN) and returns a human-comparable performance.\n", "versions": [{"version": "v1", "created": "Mon, 2 Nov 2020 07:26:55 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["Schlagenhauf", "Tobias", ""], ["Xia", "Yefeng", ""], ["Fleischer", "J\u00fcrgen", ""]]}, {"id": "2011.00786", "submitter": "Jianhua Yang", "authors": "Jianhua Yang, Yan Huang, Kai Niu, Zhanyu Ma, Liang Wang", "title": "Actor and Action Modular Network for Text-based Video Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The actor and action semantic segmentation is a challenging problem that\nrequires joint actor and action understanding, and learns to segment from\npre-defined actor and action label pairs. However, existing methods for this\ntask fail to distinguish those actors that have same super-category and\nidentify the actor-action pairs that outside of the fixed actor and action\nvocabulary. Recent studies have extended this task using textual queries,\ninstead of word-level actor-action pairs, to make the actor and action can be\nflexibly specified. In this paper, we focus on the text-based actor and action\nsegmentation problem, which performs fine-grained actor and action\nunderstanding in the video. Previous works predicted segmentation masks from\nthe merged heterogenous features of a given video and textual query, while they\nignored that the linguistic variation of the textual query and visual semantic\ndiscrepancy of the video, and led to the asymmetric matching between convolved\nvolumes of the video and the global query representation. To alleviate\naforementioned problem, we propose a novel actor and action modular network\nthat individually localizes the actor and action in two separate modules. We\nfirst learn the actor-/action-related content for the video and textual query,\nand then match them in a symmetrical manner to localize the target region. The\ntarget region includes the desired actor and action which is then fed into a\nfully convolutional network to predict the segmentation mask. The whole model\nenables joint learning for the actor-action matching and segmentation, and\nachieves the state-of-the-art performance on A2D Sentences and J-HMDB Sentences\ndatasets.\n", "versions": [{"version": "v1", "created": "Mon, 2 Nov 2020 07:32:39 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["Yang", "Jianhua", ""], ["Huang", "Yan", ""], ["Niu", "Kai", ""], ["Ma", "Zhanyu", ""], ["Wang", "Liang", ""]]}, {"id": "2011.00788", "submitter": "Shang Fu Chen", "authors": "Shang-Fu Chen, Jia-Wei Yan, Ya-Fan Su, Yu-Chiang Frank Wang", "title": "Deep Representation Decomposition for Feature Disentanglement", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Representation disentanglement aims at learning interpretable features, so\nthat the output can be recovered or manipulated accordingly. While existing\nworks like infoGAN and AC-GAN exist, they choose to derive disjoint attribute\ncode for feature disentanglement, which is not applicable for existing/trained\ngenerative models. In this paper, we propose a decomposition-GAN (dec-GAN),\nwhich is able to achieve the decomposition of an existing latent representation\ninto content and attribute features. Guided by the classifier pre-trained on\nthe attributes of interest, our dec-GAN decomposes the attributes of interest\nfrom the latent representation, while data recovery and feature consistency\nobjectives enforce the learning of our proposed method. Our experiments on\nmultiple image datasets confirm the effectiveness and robustness of our dec-GAN\nover recent representation disentanglement models.\n", "versions": [{"version": "v1", "created": "Mon, 2 Nov 2020 07:36:13 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["Chen", "Shang-Fu", ""], ["Yan", "Jia-Wei", ""], ["Su", "Ya-Fan", ""], ["Wang", "Yu-Chiang Frank", ""]]}, {"id": "2011.00789", "submitter": "Yang Zhao", "authors": "Yang Zhao and Hao Zhang", "title": "A topological approach to exploring convolutional neural networks", "comments": "8 pages, 4 figures, pnas manuscript", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by the elusive understanding concerning convolution neural networks\n(CNNs) in view of topology, we present two theoretical frameworks to interpret\ntwo topics by using topological data analysis. The first one reveals the\ntopological essence of CNN filters. Our theory first abstracts a topological\nrepresentation of how the features locate for a CNN filter, named feature\ntopology, and characterises it by defining the starting edge density. We reveal\na principle of CNN filters: tending to organize the feature topologies for the\nsame category, and thus propose the SED Distribution to statistically describe\nsuch an organization. We demonstrate the effectiveness of CNN filters reflects\nin the compactness of SED Distribution, and introduce filter entropy to measure\nit. Remarkably, the variation of filter entropy during training reveals the\nessence of CNN training: a filter-entropy-decrease process. Also, based on the\nprinciple, we give a metric to assess the filter performance. The second one\ninvestigates the inter-class distinguishability in a model-agnostic way. For\neach class, we propose the MBC Distribution, a distribution that could\ndifferentiate categories by characterising the intrinsic organization of the\ngiven category. As for multi-classes, we introduce the category distance which\nmetricizes the distance between two categories, and moreover propose the CD\nMatrix that comprehensively evaluates not just the distinguishability between\neach two category pair but the distinguishable degree for each category.\nFinally, our experiment results confirm our theories.\n", "versions": [{"version": "v1", "created": "Mon, 2 Nov 2020 07:37:31 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["Zhao", "Yang", ""], ["Zhang", "Hao", ""]]}, {"id": "2011.00794", "submitter": "Ruining Deng", "authors": "Ruining Deng, Quan Liu, Shunxing Bao, Aadarsh Jha, Catie Chang, Bryan\n  A. Millis, Matthew J. Tyska, Yuankai Huo", "title": "CaCL: Class-aware Codebook Learning for Weakly Supervised Segmentation\n  on Diffuse Image Patterns", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Weakly supervised learning has been rapidly advanced in biomedical image\nanalysis to achieve pixel-wise labels (segmentation) from image-wise\nannotations (classification), as biomedical images naturally contain image-wise\nlabels in many scenarios. The current weakly supervised learning algorithms\nfrom the computer vision community are largely designed for focal objects\n(e.g., dogs and cats). However, such algorithms are not optimized for diffuse\npatterns in biomedical imaging (e.g., stains and fluorescent in microscopy\nimaging). In this paper, we propose a novel class-aware codebook learning\n(CaCL) algorithm to perform weakly supervised learning for diffuse image\npatterns. Specifically, the CaCL algorithm is deployed to segment protein\nexpressed brush border regions from histological images of human duodenum. This\npaper makes the following contributions: (1) we approach the weakly supervised\nsegmentation from a novel codebook learning perspective; (2) the CaCL algorithm\nsegments diffuse image patterns rather than focal objects; and (3) The proposed\nalgorithm is implemented in a multi-task framework based on Vector\nQuantised-Variational AutoEncoder (VQ-VAE) to perform image reconstruction,\nclassification, feature embedding, and segmentation. The experimental results\nshow that our method achieved superior performance compared with baseline\nweakly supervised algorithms.\n", "versions": [{"version": "v1", "created": "Mon, 2 Nov 2020 07:47:10 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["Deng", "Ruining", ""], ["Liu", "Quan", ""], ["Bao", "Shunxing", ""], ["Jha", "Aadarsh", ""], ["Chang", "Catie", ""], ["Millis", "Bryan A.", ""], ["Tyska", "Matthew J.", ""], ["Huo", "Yuankai", ""]]}, {"id": "2011.00809", "submitter": "Kaushal Bhogale", "authors": "Kaushal Bhogale", "title": "Data-free Knowledge Distillation for Segmentation using Data-Enriching\n  GAN", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distilling knowledge from huge pre-trained networks to improve the\nperformance of tiny networks has favored deep learning models to be used in\nmany real-time and mobile applications. Several approaches that demonstrate\nsuccess in this field have made use of the true training dataset to extract\nrelevant knowledge. In absence of the True dataset, however, extracting\nknowledge from deep networks is still a challenge. Recent works on data-free\nknowledge distillation demonstrate such techniques on classification tasks. To\nthis end, we explore the task of data-free knowledge distillation for\nsegmentation tasks. First, we identify several challenges specific to\nsegmentation. We make use of the DeGAN training framework to propose a novel\nloss function for enforcing diversity in a setting where a few classes are\nunderrepresented. Further, we explore a new training framework for performing\nknowledge distillation in a data-free setting. We get an improvement of 6.93%\nin Mean IoU over previous approaches.\n", "versions": [{"version": "v1", "created": "Mon, 2 Nov 2020 08:16:42 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["Bhogale", "Kaushal", ""]]}, {"id": "2011.00826", "submitter": "Zihao Wang", "authors": "Zihao Wang, Chen Lin, Lu Sheng, Junjie Yan, Jing Shao", "title": "PV-NAS: Practical Neural Architecture Search for Video Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, deep learning has been utilized to solve video recognition problem\ndue to its prominent representation ability. Deep neural networks for video\ntasks is highly customized and the design of such networks requires domain\nexperts and costly trial and error tests. Recent advance in network\narchitecture search has boosted the image recognition performance in a large\nmargin. However, automatic designing of video recognition network is less\nexplored. In this study, we propose a practical solution, namely Practical\nVideo Neural Architecture Search (PV-NAS).Our PV-NAS can efficiently search\nacross tremendous large scale of architectures in a novel spatial-temporal\nnetwork search space using the gradient based search methods. To avoid sticking\ninto sub-optimal solutions, we propose a novel learning rate scheduler to\nencourage sufficient network diversity of the searched models. Extensive\nempirical evaluations show that the proposed PV-NAS achieves state-of-the-art\nperformance with much fewer computational resources. 1) Within light-weight\nmodels, our PV-NAS-L achieves 78.7% and 62.5% Top-1 accuracy on Kinetics-400\nand Something-Something V2, which are better than previous state-of-the-art\nmethods (i.e., TSM) with a large margin (4.6% and 3.4% on each dataset,\nrespectively), and 2) among median-weight models, our PV-NAS-M achieves the\nbest performance (also a new record)in the Something-Something V2 dataset.\n", "versions": [{"version": "v1", "created": "Mon, 2 Nov 2020 08:50:23 GMT"}, {"version": "v2", "created": "Tue, 3 Nov 2020 02:33:49 GMT"}], "update_date": "2020-11-04", "authors_parsed": [["Wang", "Zihao", ""], ["Lin", "Chen", ""], ["Sheng", "Lu", ""], ["Yan", "Junjie", ""], ["Shao", "Jing", ""]]}, {"id": "2011.00840", "submitter": "C\\'ecilia Ostertag", "authors": "Cecilia Ostertag, Marie Beurton-Aimar, Muriel Visani, Thierry Urruty,\n  Karell Bertet", "title": "Predicting Brain Degeneration with a Multimodal Siamese Neural Network", "comments": "Accepted in the 10th International Conference on Image Processing\n  Theory, Tools and Applications (IPTA 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To study neurodegenerative diseases, longitudinal studies are carried on\nvolunteer patients. During a time span of several months to several years, they\ngo through regular medical visits to acquire data from different modalities,\nsuch as biological samples, cognitive tests, structural and functional imaging.\nThese variables are heterogeneous but they all depend on the patient's health\ncondition, meaning that there are possibly unknown relationships between all\nmodalities. Some information may be specific to some modalities, others may be\ncomplementary, and others may be redundant. Some data may also be missing. In\nthis work we present a neural network architecture for multimodal learning,\nable to use imaging and clinical data from two time points to predict the\nevolution of a neurodegenerative disease, and robust to missing values. Our\nmultimodal network achieves 92.5\\% accuracy and an AUC score of 0.978 over a\ntest set of 57 subjects. We also show the superiority of the multimodal\narchitecture, for up to 37.5\\% of missing values in test set subjects' clinical\nmeasurements, compared to a model using only the clinical modality.\n", "versions": [{"version": "v1", "created": "Mon, 2 Nov 2020 09:21:47 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["Ostertag", "Cecilia", ""], ["Beurton-Aimar", "Marie", ""], ["Visani", "Muriel", ""], ["Urruty", "Thierry", ""], ["Bertet", "Karell", ""]]}, {"id": "2011.00844", "submitter": "Xingang Pan", "authors": "Xingang Pan, Bo Dai, Ziwei Liu, Chen Change Loy, Ping Luo", "title": "Do 2D GANs Know 3D Shape? Unsupervised 3D shape reconstruction from 2D\n  Image GANs", "comments": "Accepted to ICLR2021 as an oral presentation. Unsupervised 3D\n  reconstruction via 2D GANs", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Natural images are projections of 3D objects on a 2D image plane. While\nstate-of-the-art 2D generative models like GANs show unprecedented quality in\nmodeling the natural image manifold, it is unclear whether they implicitly\ncapture the underlying 3D object structures. And if so, how could we exploit\nsuch knowledge to recover the 3D shapes of objects in the images? To answer\nthese questions, in this work, we present the first attempt to directly mine 3D\ngeometric cues from an off-the-shelf 2D GAN that is trained on RGB images only.\nThrough our investigation, we found that such a pre-trained GAN indeed contains\nrich 3D knowledge and thus can be used to recover 3D shape from a single 2D\nimage in an unsupervised manner. The core of our framework is an iterative\nstrategy that explores and exploits diverse viewpoint and lighting variations\nin the GAN image manifold. The framework does not require 2D keypoint or 3D\nannotations, or strong assumptions on object shapes (e.g. shapes are\nsymmetric), yet it successfully recovers 3D shapes with high precision for\nhuman faces, cats, cars, and buildings. The recovered 3D shapes immediately\nallow high-quality image editing like relighting and object rotation. We\nquantitatively demonstrate the effectiveness of our approach compared to\nprevious methods in both 3D shape reconstruction and face rotation. Our code is\navailable at https://github.com/XingangPan/GAN2Shape.\n", "versions": [{"version": "v1", "created": "Mon, 2 Nov 2020 09:38:43 GMT"}, {"version": "v2", "created": "Sun, 21 Feb 2021 15:46:29 GMT"}, {"version": "v3", "created": "Mon, 8 Mar 2021 08:45:30 GMT"}, {"version": "v4", "created": "Fri, 12 Mar 2021 04:04:05 GMT"}], "update_date": "2021-03-15", "authors_parsed": [["Pan", "Xingang", ""], ["Dai", "Bo", ""], ["Liu", "Ziwei", ""], ["Loy", "Chen Change", ""], ["Luo", "Ping", ""]]}, {"id": "2011.00848", "submitter": "Fabian Isensee", "authors": "Fabian Isensee, Paul F. Jaeger, Peter M. Full, Philipp Vollmuth, Klaus\n  H. Maier-Hein", "title": "nnU-Net for Brain Tumor Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We apply nnU-Net to the segmentation task of the BraTS 2020 challenge. The\nunmodified nnU-Net baseline configuration already achieves a respectable\nresult. By incorporating BraTS-specific modifications regarding postprocessing,\nregion-based training, a more aggressive data augmentation as well as several\nminor modifications to the nnUNet pipeline we are able to improve its\nsegmentation performance substantially. We furthermore re-implement the BraTS\nranking scheme to determine which of our nnU-Net variants best fits the\nrequirements imposed by it. Our final ensemble took the first place in the\nBraTS 2020 competition with Dice scores of 88.95, 85.06 and 82.03 and HD95\nvalues of 8.498,17.337 and 17.805 for whole tumor, tumor core and enhancing\ntumor, respectively.\n", "versions": [{"version": "v1", "created": "Mon, 2 Nov 2020 09:44:27 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["Isensee", "Fabian", ""], ["Jaeger", "Paul F.", ""], ["Full", "Peter M.", ""], ["Vollmuth", "Philipp", ""], ["Maier-Hein", "Klaus H.", ""]]}, {"id": "2011.00869", "submitter": "Andr\\'as Horv\\'ath", "authors": "D\\'ora Babicz, Soma Kont\\'ar, M\\'ark Pet\\H{o}, Andr\\'as F\\\"ul\\\"op,\n  Gergely Szab\\'o, Andr\\'as Horv\\'ath", "title": "Receptive Field Size Optimization with Continuous Time Pooling", "comments": "Paper accepted for WACV : Workshop on Applications of Computer Vision\n  2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The pooling operation is a cornerstone element of convolutional neural\nnetworks. These elements generate receptive fields for neurons, in which local\nperturbations should have minimal effect on the output activations, increasing\nrobustness and invariance of the network. In this paper we will present an\naltered version of the most commonly applied method, maximum pooling, where\npooling in theory is substituted by a continuous time differential equation,\nwhich generates a location sensitive pooling operation, more similar to\nbiological receptive fields. We will present how this continuous method can be\napproximated numerically using discrete operations which fit ideally on a GPU.\nIn our approach the kernel size is substituted by diffusion strength which is a\ncontinuous valued parameter, this way it can be optimized by gradient descent\nalgorithms. We will evaluate the effect of continuous pooling on accuracy and\ncomputational need using commonly applied network architectures and datasets.\n", "versions": [{"version": "v1", "created": "Mon, 2 Nov 2020 10:21:51 GMT"}, {"version": "v2", "created": "Fri, 6 Nov 2020 21:49:42 GMT"}], "update_date": "2020-11-10", "authors_parsed": [["Babicz", "D\u00f3ra", ""], ["Kont\u00e1r", "Soma", ""], ["Pet\u0151", "M\u00e1rk", ""], ["F\u00fcl\u00f6p", "Andr\u00e1s", ""], ["Szab\u00f3", "Gergely", ""], ["Horv\u00e1th", "Andr\u00e1s", ""]]}, {"id": "2011.00870", "submitter": "Matthieu Fradet", "authors": "Mohammad Rouhani, Matthieu Fradet, Caroline Baillard", "title": "Efficient texture mapping via a non-iterative global texture alignment", "comments": "5 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Texture reconstruction techniques generally suffer from the errors in\nkeyframe poses. We present a non-iterative method for seamless texture\nreconstruction of a given 3D scene. Our method finds the best texture alignment\nin a single shot using a global optimisation framework. First, we automatically\nselect the best keyframe to texture each face of the mesh. This leads to a\ndecomposition of the mesh into small groups of connected faces associated to a\nsame keyframe. We call such groups fragments. Then, we propose a geometry-aware\nmatching technique between the 3D keypoints extracted around the fragment\nborders, where the matching zone is controlled by the margin size. These\nconstraints lead to a least squares (LS) model for finding the optimal\nalignment. Finally, visual seams are further reduced by applying a fast colour\ncorrection. In contrast to pixel-wise methods, we find the optimal alignment by\nsolving a sparse system of linear equations, which is very fast and\nnon-iterative. Experimental results demonstrate low computational complexity\nand outperformance compared to other alignment methods.\n", "versions": [{"version": "v1", "created": "Mon, 2 Nov 2020 10:24:19 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["Rouhani", "Mohammad", ""], ["Fradet", "Matthieu", ""], ["Baillard", "Caroline", ""]]}, {"id": "2011.00912", "submitter": "Sang Dinh", "authors": "In Seop Na, Chung Tran, Dung Nguyen and Sang Dinh", "title": "Facial UV Map Completion for Pose-invariant Face Recognition: A Novel\n  Adversarial Approach based on Coupled Attention Residual UNets", "comments": null, "journal-ref": "Human-centric Computing and Information Sciences 2020", "doi": "10.1186/s13673-020-00250-w", "report-no": "Vol. 10, No. 45", "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pose-invariant face recognition refers to the problem of identifying or\nverifying a person by analyzing face images captured from different poses. This\nproblem is challenging due to the large variation of pose, illumination and\nfacial expression. A promising approach to deal with pose variation is to\nfulfill incomplete UV maps extracted from in-the-wild faces, then attach the\ncompleted UV map to a fitted 3D mesh and finally generate different 2D faces of\narbitrary poses. The synthesized faces increase the pose variation for training\ndeep face recognition models and reduce the pose discrepancy during the testing\nphase. In this paper, we propose a novel generative model called Attention\nResCUNet-GAN to improve the UV map completion. We enhance the original UV-GAN\nby using a couple of U-Nets. Particularly, the skip connections within each\nU-Net are boosted by attention gates. Meanwhile, the features from two U-Nets\nare fused with trainable scalar weights. The experiments on the popular\nbenchmarks, including Multi-PIE, LFW, CPLWF and CFP datasets, show that the\nproposed method yields superior performance compared to other existing methods.\n", "versions": [{"version": "v1", "created": "Mon, 2 Nov 2020 11:46:42 GMT"}], "update_date": "2020-11-11", "authors_parsed": [["Na", "In Seop", ""], ["Tran", "Chung", ""], ["Nguyen", "Dung", ""], ["Dinh", "Sang", ""]]}, {"id": "2011.00923", "submitter": "Rahul Chakwate", "authors": "Rahul Chakwate, Arulkumar Subramaniam, Anurag Mittal", "title": "MARNet: Multi-Abstraction Refinement Network for 3D Point Cloud Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Representation learning from 3D point clouds is challenging due to their\ninherent nature of permutation invariance and irregular distribution in space.\nExisting deep learning methods follow a hierarchical feature extraction\nparadigm in which high-level abstract features are derived from low-level\nfeatures. However, they fail to exploit different granularity of information\ndue to the limited interaction between these features. To this end, we propose\nMulti-Abstraction Refinement Network (MARNet) that ensures an effective\nexchange of information between multi-level features to gain local and global\ncontextual cues while effectively preserving them till the final layer. We\nempirically show the effectiveness of MARNet in terms of state-of-the-art\nresults on two challenging tasks: Shape classification and Coarse-to-fine\ngrained semantic segmentation. MARNet significantly improves the classification\nperformance by 2% over the baseline and outperforms the state-of-the-art\nmethods on semantic segmentation task.\n", "versions": [{"version": "v1", "created": "Mon, 2 Nov 2020 12:07:35 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["Chakwate", "Rahul", ""], ["Subramaniam", "Arulkumar", ""], ["Mittal", "Anurag", ""]]}, {"id": "2011.00927", "submitter": "Zhixin Li", "authors": "Feicheng Huang, Zhixin Li, Haiyang Wei, Canlong Zhang, Huifang Ma", "title": "Boost Image Captioning with Knowledge Reasoning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Automatically generating a human-like description for a given image is a\npotential research in artificial intelligence, which has attracted a great of\nattention recently. Most of the existing attention methods explore the mapping\nrelationships between words in sentence and regions in image, such\nunpredictable matching manner sometimes causes inharmonious alignments that may\nreduce the quality of generated captions. In this paper, we make our efforts to\nreason about more accurate and meaningful captions. We first propose word\nattention to improve the correctness of visual attention when generating\nsequential descriptions word-by-word. The special word attention emphasizes on\nword importance when focusing on different regions of the input image, and\nmakes full use of the internal annotation knowledge to assist the calculation\nof visual attention. Then, in order to reveal those incomprehensible intentions\nthat cannot be expressed straightforwardly by machines, we introduce a new\nstrategy to inject external knowledge extracted from knowledge graph into the\nencoder-decoder framework to facilitate meaningful captioning. Finally, we\nvalidate our model on two freely available captioning benchmarks: Microsoft\nCOCO dataset and Flickr30k dataset. The results demonstrate that our approach\nachieves state-of-the-art performance and outperforms many of the existing\napproaches.\n", "versions": [{"version": "v1", "created": "Mon, 2 Nov 2020 12:19:46 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["Huang", "Feicheng", ""], ["Li", "Zhixin", ""], ["Wei", "Haiyang", ""], ["Zhang", "Canlong", ""], ["Ma", "Huifang", ""]]}, {"id": "2011.00931", "submitter": "Nico Engel", "authors": "Nico Engel, Vasileios Belagiannis and Klaus Dietmayer", "title": "Point Transformer", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we present Point Transformer, a deep neural network that\noperates directly on unordered and unstructured point sets. We design Point\nTransformer to extract local and global features and relate both\nrepresentations by introducing the local-global attention mechanism, which aims\nto capture spatial point relations and shape information. For that purpose, we\npropose SortNet, as part of the Point Transformer, which induces input\npermutation invariance by selecting points based on a learned score. The output\nof Point Transformer is a sorted and permutation invariant feature list that\ncan directly be incorporated into common computer vision applications. We\nevaluate our approach on standard classification and part segmentation\nbenchmarks to demonstrate competitive results compared to the prior work.\n", "versions": [{"version": "v1", "created": "Mon, 2 Nov 2020 12:26:14 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["Engel", "Nico", ""], ["Belagiannis", "Vasileios", ""], ["Dietmayer", "Klaus", ""]]}, {"id": "2011.00940", "submitter": "Dan Zhao", "authors": "Dan Zhao, Guizhi Xu, Zhenghua XU, Thomas Lukasiewicz, Minmin Xue,\n  Zhigang Fu", "title": "Deep Learning in Computer-Aided Diagnosis and Treatment of Tumors: A\n  Survey", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computer-Aided Diagnosis and Treatment of Tumors is a hot topic of deep\nlearning in recent years, which constitutes a series of medical tasks, such as\ndetection of tumor markers, the outline of tumor leisures, subtypes and stages\nof tumors, prediction of therapeutic effect, and drug development. Meanwhile,\nthere are some deep learning models with precise positioning and excellent\nperformance produced in mainstream task scenarios. Thus follow to introduce\ndeep learning methods from task-orient, mainly focus on the improvements for\nmedical tasks. Then to summarize the recent progress in four stages of tumor\ndiagnosis and treatment, which named In-Vitro Diagnosis (IVD), Imaging\nDiagnosis (ID), Pathological Diagnosis (PD), and Treatment Planning (TP).\nAccording to the specific data types and medical tasks of each stage, we\npresent the applications of deep learning in the Computer-Aided Diagnosis and\nTreatment of Tumors and analyzing the excellent works therein. This survey\nconcludes by discussing research issues and suggesting challenges for future\nimprovement.\n", "versions": [{"version": "v1", "created": "Mon, 2 Nov 2020 12:42:19 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["Zhao", "Dan", ""], ["Xu", "Guizhi", ""], ["XU", "Zhenghua", ""], ["Lukasiewicz", "Thomas", ""], ["Xue", "Minmin", ""], ["Fu", "Zhigang", ""]]}, {"id": "2011.00954", "submitter": "Kumar Shubham", "authors": "Kumar Shubham, Gopalakrishnan Venkatesh, Reijul Sachdev, Akshi, Dinesh\n  Babu Jayagopi, G. Srinivasaraghavan", "title": "Learning a Deep Reinforcement Learning Policy Over the Latent Space of a\n  Pre-trained GAN for Semantic Age Manipulation", "comments": "Accepted by the 2021 International Joint Conference on Neural\n  Networks (IJCNN 2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Learning a disentangled representation of the latent space has become one of\nthe most fundamental problems studied in computer vision. Recently, many\nGenerative Adversarial Networks (GANs) have shown promising results in\ngenerating high fidelity images. However, studies to understand the semantic\nlayout of the latent space of pre-trained models are still limited. Several\nworks train conditional GANs to generate faces with required semantic\nattributes. Unfortunately, in these attempts, the generated output is often not\nas photo-realistic as the unconditional state-of-the-art models. Besides, they\nalso require large computational resources and specific datasets to generate\nhigh fidelity images. In our work, we have formulated a Markov Decision Process\n(MDP) over the latent space of a pre-trained GAN model to learn a conditional\npolicy for semantic manipulation along specific attributes under defined\nidentity bounds. Further, we have defined a semantic age manipulation scheme\nusing a locally linear approximation over the latent space. Results show that\nour learned policy samples high fidelity images with required age alterations,\nwhile preserving the identity of the person.\n", "versions": [{"version": "v1", "created": "Mon, 2 Nov 2020 13:15:18 GMT"}, {"version": "v2", "created": "Wed, 28 Apr 2021 09:19:48 GMT"}], "update_date": "2021-04-29", "authors_parsed": [["Shubham", "Kumar", ""], ["Venkatesh", "Gopalakrishnan", ""], ["Sachdev", "Reijul", ""], ["Akshi", "", ""], ["Jayagopi", "Dinesh Babu", ""], ["Srinivasaraghavan", "G.", ""]]}, {"id": "2011.00966", "submitter": "Shweta Mahajan", "authors": "Shweta Mahajan, Stefan Roth", "title": "Diverse Image Captioning with Context-Object Split Latent Spaces", "comments": "To appear at NeurIPS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Diverse image captioning models aim to learn one-to-many mappings that are\ninnate to cross-domain datasets, such as of images and texts. Current methods\nfor this task are based on generative latent variable models, e.g. VAEs with\nstructured latent spaces. Yet, the amount of multimodality captured by prior\nwork is limited to that of the paired training data -- the true diversity of\nthe underlying generative process is not fully captured. To address this\nlimitation, we leverage the contextual descriptions in the dataset that explain\nsimilar contexts in different visual scenes. To this end, we introduce a novel\nfactorization of the latent space, termed context-object split, to model\ndiversity in contextual descriptions across images and texts within the\ndataset. Our framework not only enables diverse captioning through\ncontext-based pseudo supervision, but extends this to images with novel objects\nand without paired captions in the training data. We evaluate our COS-CVAE\napproach on the standard COCO dataset and on the held-out COCO dataset\nconsisting of images with novel objects, showing significant gains in accuracy\nand diversity.\n", "versions": [{"version": "v1", "created": "Mon, 2 Nov 2020 13:33:20 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["Mahajan", "Shweta", ""], ["Roth", "Stefan", ""]]}, {"id": "2011.00971", "submitter": "Jiayuan Gu", "authors": "Tongzhou Mu, Jiayuan Gu, Zhiwei Jia, Hao Tang, Hao Su", "title": "Refactoring Policy for Compositional Generalizability using\n  Self-Supervised Object Proposals", "comments": "34th Conference on Neural Information Processing Systems (NeurIPS\n  2020), Vancouver, Canada", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study how to learn a policy with compositional generalizability. We\npropose a two-stage framework, which refactorizes a high-reward teacher policy\ninto a generalizable student policy with strong inductive bias. Particularly,\nwe implement an object-centric GNN-based student policy, whose input objects\nare learned from images through self-supervised learning. Empirically, we\nevaluate our approach on four difficult tasks that require compositional\ngeneralizability, and achieve superior performance compared to baselines.\n", "versions": [{"version": "v1", "created": "Mon, 26 Oct 2020 17:46:08 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["Mu", "Tongzhou", ""], ["Gu", "Jiayuan", ""], ["Jia", "Zhiwei", ""], ["Tang", "Hao", ""], ["Su", "Hao", ""]]}, {"id": "2011.00980", "submitter": "Benjamin Biggs", "authors": "Benjamin Biggs, S\\'ebastien Ehrhadt, Hanbyul Joo, Benjamin Graham,\n  Andrea Vedaldi and David Novotny", "title": "3D Multi-bodies: Fitting Sets of Plausible 3D Human Models to Ambiguous\n  Image Data", "comments": "NeurIPS 2020 Spotlight; 14 pages including supplementary", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of obtaining dense 3D reconstructions of humans from\nsingle and partially occluded views. In such cases, the visual evidence is\nusually insufficient to identify a 3D reconstruction uniquely, so we aim at\nrecovering several plausible reconstructions compatible with the input data. We\nsuggest that ambiguities can be modelled more effectively by parametrizing the\npossible body shapes and poses via a suitable 3D model, such as SMPL for\nhumans. We propose to learn a multi-hypothesis neural network regressor using a\nbest-of-M loss, where each of the M hypotheses is constrained to lie on a\nmanifold of plausible human poses by means of a generative model. We show that\nour method outperforms alternative approaches in ambiguous pose recovery on\nstandard benchmarks for 3D humans, and in heavily occluded versions of these\nbenchmarks.\n", "versions": [{"version": "v1", "created": "Mon, 2 Nov 2020 13:55:31 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["Biggs", "Benjamin", ""], ["Ehrhadt", "S\u00e9bastien", ""], ["Joo", "Hanbyul", ""], ["Graham", "Benjamin", ""], ["Vedaldi", "Andrea", ""], ["Novotny", "David", ""]]}, {"id": "2011.00988", "submitter": "JuYoung Yang", "authors": "JuYoung Yang, Chanho Lee, Pyunghwan Ahn, Haeil Lee, Eojindl Yi and\n  Junmo Kim", "title": "PBP-Net: Point Projection and Back-Projection Network for 3D Point Cloud\n  Segmentation", "comments": "7 pages, accepted by IROS2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Following considerable development in 3D scanning technologies, many studies\nhave recently been proposed with various approaches for 3D vision tasks,\nincluding some methods that utilize 2D convolutional neural networks (CNNs).\nHowever, even though 2D CNNs have achieved high performance in many 2D vision\ntasks, existing works have not effectively applied them onto 3D vision tasks.\nIn particular, segmentation has not been well studied because of the difficulty\nof dense prediction for each point, which requires rich feature representation.\nIn this paper, we propose a simple and efficient architecture named point\nprojection and back-projection network (PBP-Net), which leverages 2D CNNs for\nthe 3D point cloud segmentation. 3 modules are introduced, each of which\nprojects 3D point cloud onto 2D planes, extracts features using a 2D CNN\nbackbone, and back-projects features onto the original 3D point cloud. To\ndemonstrate effective 3D feature extraction using 2D CNN, we perform various\nexperiments including comparison to recent methods. We analyze the proposed\nmodules through ablation studies and perform experiments on object part\nsegmentation (ShapeNet-Part dataset) and indoor scene semantic segmentation\n(S3DIS dataset). The experimental results show that proposed PBP-Net achieves\ncomparable performance to existing state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Mon, 2 Nov 2020 14:14:30 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["Yang", "JuYoung", ""], ["Lee", "Chanho", ""], ["Ahn", "Pyunghwan", ""], ["Lee", "Haeil", ""], ["Yi", "Eojindl", ""], ["Kim", "Junmo", ""]]}, {"id": "2011.00993", "submitter": "Michael Ying Yang", "authors": "Michael Ying Yang, Saumya Kumaar, Ye Lyu, Francesco Nex", "title": "Real-time Semantic Segmentation with Context Aggregation Network", "comments": "extended version of v1", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the increasing demand of autonomous systems, pixelwise semantic\nsegmentation for visual scene understanding needs to be not only accurate but\nalso efficient for potential real-time applications. In this paper, we propose\nContext Aggregation Network, a dual branch convolutional neural network, with\nsignificantly lower computational costs as compared to the state-of-the-art,\nwhile maintaining a competitive prediction accuracy. Building upon the existing\ndual branch architectures for high-speed semantic segmentation, we design a\ncheap high resolution branch for effective spatial detailing and a context\nbranch with light-weight versions of global aggregation and local distribution\nblocks, potent to capture both long-range and local contextual dependencies\nrequired for accurate semantic segmentation, with low computational overheads.\nWe evaluate our method on two semantic segmentation datasets, namely Cityscapes\ndataset and UAVid dataset. For Cityscapes test set, our model achieves\nstate-of-the-art results with mIOU of 75.9%, at 76 FPS on an NVIDIA RTX 2080Ti\nand 8 FPS on a Jetson Xavier NX. With regards to UAVid dataset, our proposed\nnetwork achieves mIOU score of 63.5% with high execution speed (15 FPS).\n", "versions": [{"version": "v1", "created": "Mon, 2 Nov 2020 14:16:23 GMT"}, {"version": "v2", "created": "Sun, 11 Apr 2021 20:51:58 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Yang", "Michael Ying", ""], ["Kumaar", "Saumya", ""], ["Lyu", "Ye", ""], ["Nex", "Francesco", ""]]}, {"id": "2011.01009", "submitter": "Quan Liu", "authors": "Quan Liu, Isabella M. Gaeta, Mengyang Zhao, Ruining Deng, Aadarsh Jha,\n  Bryan A. Millis, Anita Mahadevan-Jansen, Matthew J. Tyska, Yuankai Huo", "title": "ASIST: Annotation-free synthetic instance segmentation and tracking for\n  microscope video analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Instance object segmentation and tracking provide comprehensive\nquantification of objects across microscope videos. The recent single-stage\npixel-embedding based deep learning approach has shown its superior performance\ncompared with \"segment-then-associate\" two-stage solutions. However, one major\nlimitation of applying a supervised pixel-embedding based method to microscope\nvideos is the resource-intensive manual labeling, which involves tracing\nhundreds of overlapped objects with their temporal associations across video\nframes. Inspired by the recent generative adversarial network (GAN) based\nannotation-free image segmentation, we propose a novel annotation-free\nsynthetic instance segmentation and tracking (ASIST) algorithm for analyzing\nmicroscope videos of sub-cellular microvilli. The contributions of this paper\nare three-fold: (1) proposing a new annotation-free video analysis paradigm is\nproposed. (2) aggregating the embedding based instance segmentation and\ntracking with annotation-free synthetic learning as a holistic framework; and\n(3) to the best of our knowledge, this is first study to investigate microvilli\ninstance segmentation and tracking using embedding based deep learning. From\nthe experimental results, the proposed annotation-free method achieved superior\nperformance compared with supervised learning.\n", "versions": [{"version": "v1", "created": "Mon, 2 Nov 2020 14:39:26 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["Liu", "Quan", ""], ["Gaeta", "Isabella M.", ""], ["Zhao", "Mengyang", ""], ["Deng", "Ruining", ""], ["Jha", "Aadarsh", ""], ["Millis", "Bryan A.", ""], ["Mahadevan-Jansen", "Anita", ""], ["Tyska", "Matthew J.", ""], ["Huo", "Yuankai", ""]]}, {"id": "2011.01018", "submitter": "Mathilde Brousmiche", "authors": "Mathilde Brousmiche and St\\'ephane Dupont and Jean Rouat", "title": "AVECL-UMONS database for audio-visual event classification and\n  localization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CV cs.MM cs.SD", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We introduce the AVECL-UMons dataset for audio-visual event classification\nand localization in the context of office environments. The audio-visual\ndataset is composed of 11 event classes recorded at several realistic positions\nin two different rooms. Two types of sequences are recorded according to the\nnumber of events in the sequence. The dataset comprises 2662 unilabel sequences\nand 2724 multilabel sequences corresponding to a total of 5.24 hours. The\ndataset is publicly accessible online :\nhttps://zenodo.org/record/3965492#.X09wsobgrCI.\n", "versions": [{"version": "v1", "created": "Fri, 2 Oct 2020 14:26:02 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["Brousmiche", "Mathilde", ""], ["Dupont", "St\u00e9phane", ""], ["Rouat", "Jean", ""]]}, {"id": "2011.01022", "submitter": "Zhengqiang Fan", "authors": "Zhengqiang Fan, Na Sun, Quan Qiu, and Chunjiang Zhao", "title": "Depth Ranging Performance Evaluation and Improvement for RGB-D Cameras\n  on Field-Based High-Throughput Phenotyping Robots", "comments": "We want to improve the work of this paper before publishing it\n  publicly", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  RGB-D cameras have been successfully used for indoor High-ThroughpuT\nPhenotyping (HTTP). However, their capability and feasibility for in-field HTTP\nstill need to be evaluated, due to the noise and disturbances generated by\nunstable illumination, specular reflection, and diffuse reflection, etc. To\nsolve these problems, we evaluated the depth-ranging performances of two\nconsumer-level RGB-D cameras (RealSense D435i and Kinect V2) under in-field\nHTTP scenarios, and proposed a strategy to compensate the depth measurement\nerror. For performance evaluation, we focused on determining their optimal\nranging areas for different crop organs. Based on the evaluation results, we\nproposed a brightness-and-distance-based Support Vector Regression Strategy, to\ncompensate the ranging error. Furthermore, we analyzed the depth filling rate\nof two RGB-D cameras under different lighting intensities. Experimental results\nshowed that: 1) For RealSense D435i, its effective ranging area is [0.160,\n1.400] m, and in-field filling rate is approximately 90%. 2) For Kinect V2, it\nhas a high ranging accuracy in the [0.497, 1.200] m, but its in-field filling\nrate is less than 24.9%. 3) Our error compensation model can effectively reduce\nthe influences of lighting intensity and target distance. The maximum MSE and\nminimum R2 of this model are 0.029 and 0.867, respectively. To sum up,\nRealSense D435i has better ranging performances than Kinect V2 on in-field\nHTTP.\n", "versions": [{"version": "v1", "created": "Mon, 2 Nov 2020 15:10:27 GMT"}, {"version": "v2", "created": "Tue, 27 Apr 2021 12:49:17 GMT"}], "update_date": "2021-04-28", "authors_parsed": [["Fan", "Zhengqiang", ""], ["Sun", "Na", ""], ["Qiu", "Quan", ""], ["Zhao", "Chunjiang", ""]]}, {"id": "2011.01045", "submitter": "Theophraste Henry Dr", "authors": "Theophraste Henry, Alexandre Carre, Marvin Lerousseau, Theo Estienne,\n  Charlotte Robert, Nikos Paragios, Eric Deutsch", "title": "Brain tumor segmentation with self-ensembled, deeply-supervised 3D U-net\n  neural networks: a BraTS 2020 challenge solution", "comments": "BraTS 2020 proceedings (LNCS) paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Brain tumor segmentation is a critical task for patient's disease management.\nIn order to automate and standardize this task, we trained multiple U-net like\nneural networks, mainly with deep supervision and stochastic weight averaging,\non the Multimodal Brain Tumor Segmentation Challenge (BraTS) 2020 training\ndataset. Two independent ensembles of models from two different training\npipelines were trained, and each produced a brain tumor segmentation map. These\ntwo labelmaps per patient were then merged, taking into account the performance\nof each ensemble for specific tumor subregions. Our performance on the online\nvalidation dataset with test time augmentation were as follows: Dice of 0.81,\n0.91 and 0.85; Hausdorff (95%) of 20.6, 4,3, 5.7 mm for the enhancing tumor,\nwhole tumor and tumor core, respectively. Similarly, our solution achieved a\nDice of 0.79, 0.89 and 0.84, as well as Hausdorff (95%) of 20.4, 6.7 and 19.5mm\non the final test dataset, ranking us among the top ten teams. More complicated\ntraining schemes and neural network architectures were investigated without\nsignificant performance gain at the cost of greatly increased training time.\nOverall, our approach yielded good and balanced performance for each tumor\nsubregion. Our solution is open sourced at\nhttps://github.com/lescientifik/open_brats2020.\n", "versions": [{"version": "v1", "created": "Fri, 30 Oct 2020 14:36:10 GMT"}, {"version": "v2", "created": "Fri, 27 Nov 2020 15:58:32 GMT"}], "update_date": "2020-11-30", "authors_parsed": [["Henry", "Theophraste", ""], ["Carre", "Alexandre", ""], ["Lerousseau", "Marvin", ""], ["Estienne", "Theo", ""], ["Robert", "Charlotte", ""], ["Paragios", "Nikos", ""], ["Deutsch", "Eric", ""]]}, {"id": "2011.01077", "submitter": "H{\\aa}kon Hukkel{\\aa}s", "authors": "H{\\aa}kon Hukkel{\\aa}s, Frank Lindseth, Rudolf Mester", "title": "Image Inpainting with Learnable Feature Imputation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A regular convolution layer applying a filter in the same way over known and\nunknown areas causes visual artifacts in the inpainted image. Several studies\naddress this issue with feature re-normalization on the output of the\nconvolution. However, these models use a significant amount of learnable\nparameters for feature re-normalization, or assume a binary representation of\nthe certainty of an output. We propose (layer-wise) feature imputation of the\nmissing input values to a convolution. In contrast to learned feature\nre-normalization, our method is efficient and introduces a minimal number of\nparameters. Furthermore, we propose a revised gradient penalty for image\ninpainting, and a novel GAN architecture trained exclusively on adversarial\nloss. Our quantitative evaluation on the FDF dataset reflects that our revised\ngradient penalty and alternative convolution improves generated image quality\nsignificantly. We present comparisons on CelebA-HQ and Places2 to current\nstate-of-the-art to validate our model.\n", "versions": [{"version": "v1", "created": "Mon, 2 Nov 2020 16:05:32 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["Hukkel\u00e5s", "H\u00e5kon", ""], ["Lindseth", "Frank", ""], ["Mester", "Rudolf", ""]]}, {"id": "2011.01082", "submitter": "Robin Ruede", "authors": "Robin Ruede, Verena Heusser, Lukas Frank, Alina Roitberg, Monica\n  Haurilet, Rainer Stiefelhagen", "title": "Multi-Task Learning for Calorie Prediction on a Novel Large-Scale Recipe\n  Dataset Enriched with Nutritional Information", "comments": "Accepted to ICPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  A rapidly growing amount of content posted online, such as food recipes,\nopens doors to new exciting applications at the intersection of vision and\nlanguage. In this work, we aim to estimate the calorie amount of a meal\ndirectly from an image by learning from recipes people have published on the\nInternet, thus skipping time-consuming manual data annotation. Since there are\nfew large-scale publicly available datasets captured in unconstrained\nenvironments, we propose the pic2kcal benchmark comprising 308,000 images from\nover 70,000 recipes including photographs, ingredients and instructions. To\nobtain nutritional information of the ingredients and automatically determine\nthe ground-truth calorie value, we match the items in the recipes with\nstructured information from a food item database.\n  We evaluate various neural networks for regression of the calorie quantity\nand extend them with the multi-task paradigm. Our learning procedure combines\nthe calorie estimation with prediction of proteins, carbohydrates, and fat\namounts as well as a multi-label ingredient classification. Our experiments\ndemonstrate clear benefits of multi-task learning for calorie estimation,\nsurpassing the single-task calorie regression by 9.9%. To encourage further\nresearch on this task, we make the code for generating the dataset and the\nmodels publicly available.\n", "versions": [{"version": "v1", "created": "Mon, 2 Nov 2020 16:11:51 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["Ruede", "Robin", ""], ["Heusser", "Verena", ""], ["Frank", "Lukas", ""], ["Roitberg", "Alina", ""], ["Haurilet", "Monica", ""], ["Stiefelhagen", "Rainer", ""]]}, {"id": "2011.01114", "submitter": "Prateek Manocha", "authors": "Prateek Manocha and Prithwijit Guha", "title": "Facial Keypoint Sequence Generation from Audio", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Whenever we speak, our voice is accompanied by facial movements and\nexpressions. Several recent works have shown the synthesis of highly\nphoto-realistic videos of talking faces, but they either require a source video\nto drive the target face or only generate videos with a fixed head pose. This\nlack of facial movement is because most of these works focus on the lip\nmovement in sync with the audio while assuming the remaining facial keypoints'\nfixed nature. To address this, a unique audio-keypoint dataset of over 150,000\nvideos at 224p and 25fps is introduced that relates the facial keypoint\nmovement for the given audio. This dataset is then further used to train the\nmodel, Audio2Keypoint, a novel approach for synthesizing facial keypoint\nmovement to go with the audio. Given a single image of the target person and an\naudio sequence (in any language), Audio2Keypoint generates a plausible keypoint\nmovement sequence in sync with the input audio, conditioned on the input image\nto preserve the target person's facial characteristics. To the best of our\nknowledge, this is the first work that proposes an audio-keypoint dataset and\nlearns a model to output the plausible keypoint sequence to go with audio of\nany arbitrary length. Audio2Keypoint generalizes across unseen people with a\ndifferent facial structure allowing us to generate the sequence with the voice\nfrom any source or even synthetic voices. Instead of learning a direct mapping\nfrom audio to video domain, this work aims to learn the audio-keypoint mapping\nthat allows for in-plane and out-of-plane head rotations, while preserving the\nperson's identity using a Pose Invariant (PIV) Encoder.\n", "versions": [{"version": "v1", "created": "Mon, 2 Nov 2020 16:47:52 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["Manocha", "Prateek", ""], ["Guha", "Prithwijit", ""]]}, {"id": "2011.01118", "submitter": "Sidike Paheding", "authors": "Nahian Siddique, Paheding Sidike, Colin Elkin and Vijay Devabhaktuni", "title": "U-Net and its variants for medical image segmentation: theory and\n  applications", "comments": "42 pages, in IEEE Access", "journal-ref": null, "doi": "10.1109/ACCESS.2021.3086020", "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  U-net is an image segmentation technique developed primarily for medical\nimage analysis that can precisely segment images using a scarce amount of\ntraining data. These traits provide U-net with a very high utility within the\nmedical imaging community and have resulted in extensive adoption of U-net as\nthe primary tool for segmentation tasks in medical imaging. The success of\nU-net is evident in its widespread use in all major image modalities from CT\nscans and MRI to X-rays and microscopy. Furthermore, while U-net is largely a\nsegmentation tool, there have been instances of the use of U-net in other\napplications. As the potential of U-net is still increasing, in this review we\nlook at the various developments that have been made in the U-net architecture\nand provide observations on recent trends. We examine the various innovations\nthat have been made in deep learning and discuss how these tools facilitate\nU-net. Furthermore, we look at image modalities and application areas where\nU-net has been applied.\n", "versions": [{"version": "v1", "created": "Mon, 2 Nov 2020 16:50:00 GMT"}], "update_date": "2021-06-10", "authors_parsed": [["Siddique", "Nahian", ""], ["Sidike", "Paheding", ""], ["Elkin", "Colin", ""], ["Devabhaktuni", "Vijay", ""]]}, {"id": "2011.01122", "submitter": "Fangwen Shu", "authors": "Fangwen Shu, Paul Lesur, Yaxu Xie, Alain Pagani, Didier Stricker", "title": "SLAM in the Field: An Evaluation of Monocular Mapping and Localization\n  on Challenging Dynamic Agricultural Environment", "comments": "accepted to WACV 2021, acknowledgment added", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper demonstrates a system capable of combining a sparse, indirect,\nmonocular visual SLAM, with both offline and real-time Multi-View Stereo (MVS)\nreconstruction algorithms. This combination overcomes many obstacles\nencountered by autonomous vehicles or robots employed in agricultural\nenvironments, such as overly repetitive patterns, need for very detailed\nreconstructions, and abrupt movements caused by uneven roads. Furthermore, the\nuse of a monocular SLAM makes our system much easier to integrate with an\nexisting device, as we do not rely on a LiDAR (which is expensive and power\nconsuming), or stereo camera (whose calibration is sensitive to external\nperturbation e.g. camera being displaced). To the best of our knowledge, this\npaper presents the first evaluation results for monocular SLAM, and our work\nfurther explores unsupervised depth estimation on this specific application\nscenario by simulating RGB-D SLAM to tackle the scale ambiguity, and shows our\napproach produces reconstructions that are helpful to various agricultural\ntasks. Moreover, we highlight that our experiments provide meaningful insight\nto improve monocular SLAM systems under agricultural settings.\n", "versions": [{"version": "v1", "created": "Mon, 2 Nov 2020 16:53:35 GMT"}, {"version": "v2", "created": "Fri, 6 Nov 2020 15:05:33 GMT"}], "update_date": "2020-11-09", "authors_parsed": [["Shu", "Fangwen", ""], ["Lesur", "Paul", ""], ["Xie", "Yaxu", ""], ["Pagani", "Alain", ""], ["Stricker", "Didier", ""]]}, {"id": "2011.01142", "submitter": "Paul Voigtlaender", "authors": "Paul Voigtlaender and Lishu Luo and Chun Yuan and Yong Jiang and\n  Bastian Leibe", "title": "Reducing the Annotation Effort for Video Object Segmentation Datasets", "comments": "Accepted at WACV 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For further progress in video object segmentation (VOS), larger, more\ndiverse, and more challenging datasets will be necessary. However, densely\nlabeling every frame with pixel masks does not scale to large datasets. We use\na deep convolutional network to automatically create pseudo-labels on a pixel\nlevel from much cheaper bounding box annotations and investigate how far such\npseudo-labels can carry us for training state-of-the-art VOS approaches. A very\nencouraging result of our study is that adding a manually annotated mask in\nonly a single video frame for each object is sufficient to generate\npseudo-labels which can be used to train a VOS method to reach almost the same\nperformance level as when training with fully segmented videos. We use this\nworkflow to create pixel pseudo-labels for the training set of the challenging\ntracking dataset TAO, and we manually annotate a subset of the validation set.\nTogether, we obtain the new TAO-VOS benchmark, which we make publicly available\nat www.vision.rwth-aachen.de/page/taovos. While the performance of\nstate-of-the-art methods on existing datasets starts to saturate, TAO-VOS\nremains very challenging for current algorithms and reveals their shortcomings.\n", "versions": [{"version": "v1", "created": "Mon, 2 Nov 2020 17:34:45 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["Voigtlaender", "Paul", ""], ["Luo", "Lishu", ""], ["Yuan", "Chun", ""], ["Jiang", "Yong", ""], ["Leibe", "Bastian", ""]]}, {"id": "2011.01143", "submitter": "Scott Wisdom", "authors": "Efthymios Tzinis, Scott Wisdom, Aren Jansen, Shawn Hershey, Tal Remez,\n  Daniel P. W. Ellis, John R. Hershey", "title": "Into the Wild with AudioScope: Unsupervised Audio-Visual Separation of\n  On-Screen Sounds", "comments": "ICLR 2021, 27 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.CV eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent progress in deep learning has enabled many advances in sound\nseparation and visual scene understanding. However, extracting sound sources\nwhich are apparent in natural videos remains an open problem. In this work, we\npresent AudioScope, a novel audio-visual sound separation framework that can be\ntrained without supervision to isolate on-screen sound sources from real\nin-the-wild videos. Prior audio-visual separation work assumed artificial\nlimitations on the domain of sound classes (e.g., to speech or music),\nconstrained the number of sources, and required strong sound separation or\nvisual segmentation labels. AudioScope overcomes these limitations, operating\non an open domain of sounds, with variable numbers of sources, and without\nlabels or prior visual segmentation. The training procedure for AudioScope uses\nmixture invariant training (MixIT) to separate synthetic mixtures of mixtures\n(MoMs) into individual sources, where noisy labels for mixtures are provided by\nan unsupervised audio-visual coincidence model. Using the noisy labels, along\nwith attention between video and audio features, AudioScope learns to identify\naudio-visual similarity and to suppress off-screen sounds. We demonstrate the\neffectiveness of our approach using a dataset of video clips extracted from\nopen-domain YFCC100m video data. This dataset contains a wide diversity of\nsound classes recorded in unconstrained conditions, making the application of\nprevious methods unsuitable. For evaluation and semi-supervised experiments, we\ncollected human labels for presence of on-screen and off-screen sounds on a\nsmall subset of clips.\n", "versions": [{"version": "v1", "created": "Mon, 2 Nov 2020 17:36:13 GMT"}, {"version": "v2", "created": "Sun, 30 May 2021 03:47:08 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Tzinis", "Efthymios", ""], ["Wisdom", "Scott", ""], ["Jansen", "Aren", ""], ["Hershey", "Shawn", ""], ["Remez", "Tal", ""], ["Ellis", "Daniel P. W.", ""], ["Hershey", "John R.", ""]]}, {"id": "2011.01153", "submitter": "Mengye Ren", "authors": "Bob Wei, Mengye Ren, Wenyuan Zeng, Ming Liang, Bin Yang, Raquel\n  Urtasun", "title": "Perceive, Attend, and Drive: Learning Spatial Attention for Safe\n  Self-Driving", "comments": "ICRA 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose an end-to-end self-driving network featuring a\nsparse attention module that learns to automatically attend to important\nregions of the input. The attention module specifically targets motion\nplanning, whereas prior literature only applied attention in perception tasks.\nLearning an attention mask directly targeted for motion planning significantly\nimproves the planner safety by performing more focused computation.\nFurthermore, visualizing the attention improves interpretability of end-to-end\nself-driving.\n", "versions": [{"version": "v1", "created": "Mon, 2 Nov 2020 17:47:54 GMT"}, {"version": "v2", "created": "Fri, 26 Mar 2021 03:43:18 GMT"}], "update_date": "2021-03-29", "authors_parsed": [["Wei", "Bob", ""], ["Ren", "Mengye", ""], ["Zeng", "Wenyuan", ""], ["Liang", "Ming", ""], ["Yang", "Bin", ""], ["Urtasun", "Raquel", ""]]}, {"id": "2011.01163", "submitter": "Xinyi Li", "authors": "Xinyi Li, Lin Yuan, Longin Jan Latecki, Haibin Ling", "title": "Pushing the Envelope of Rotation Averaging for Visual SLAM", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As an essential part of structure from motion (SfM) and Simultaneous\nLocalization and Mapping (SLAM) systems, motion averaging has been extensively\nstudied in the past years and continues to attract surging research attention.\nWhile canonical approaches such as bundle adjustment are predominantly\ninherited in most of state-of-the-art SLAM systems to estimate and update the\ntrajectory in the robot navigation, the practical implementation of bundle\nadjustment in SLAM systems is intrinsically limited by the high computational\ncomplexity, unreliable convergence and strict requirements of ideal\ninitializations. In this paper, we lift these limitations and propose a novel\noptimization backbone for visual SLAM systems, where we leverage rotation\naveraging to improve the accuracy, efficiency and robustness of conventional\nmonocular SLAM pipelines. In our approach, we first decouple the rotational and\ntranslational parameters in the camera rigid body transformation and convert\nthe high-dimensional non-convex nonlinear problem into tractable linear\nsubproblems in lower dimensions, and show that the subproblems can be solved\nindependently with proper constraints. We apply the scale parameter with\n$l_1$-norm in the pose-graph optimization to address the rotation averaging\nrobustness against outliers. We further validate the global optimality of our\nproposed approach, revisit and address the initialization schemes, pure\nrotational scene handling and outlier treatments. We demonstrate that our\napproach can exhibit up to 10x faster speed with comparable accuracy against\nthe state of the art on public benchmarks.\n", "versions": [{"version": "v1", "created": "Mon, 2 Nov 2020 18:02:26 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["Li", "Xinyi", ""], ["Yuan", "Lin", ""], ["Latecki", "Longin Jan", ""], ["Ling", "Haibin", ""]]}, {"id": "2011.01177", "submitter": "Hosein Barzekar", "authors": "D M Anisuzzaman, Hosein Barzekar, Ling Tong, Jake Luo, Zeyun Yu", "title": "A Deep Learning Study on Osteosarcoma Detection from Histological Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the U.S, 5-10\\% of new pediatric cases of cancer are primary bone tumors.\nThe most common type of primary malignant bone tumor is osteosarcoma. The\nintention of the present work is to improve the detection and diagnosis of\nosteosarcoma using computer-aided detection (CAD) and diagnosis (CADx). Such\ntools as convolutional neural networks (CNNs) can significantly decrease the\nsurgeon's workload and make a better prognosis of patient conditions. CNNs need\nto be trained on a large amount of data in order to achieve a more trustworthy\nperformance. In this study, transfer learning techniques, pre-trained CNNs, are\nadapted to a public dataset on osteosarcoma histological images to detect\nnecrotic images from non-necrotic and healthy tissues. First, the dataset was\npreprocessed, and different classifications are applied. Then, Transfer\nlearning models including VGG19 and Inception V3 are used and trained on Whole\nSlide Images (WSI) with no patches, to improve the accuracy of the outputs.\nFinally, the models are applied to different classification problems, including\nbinary and multi-class classifiers. Experimental results show that the accuracy\nof the VGG19 has the highest, 96\\%, performance amongst all binary classes and\nmulticlass classification. Our fine-tuned model demonstrates state-of-the-art\nperformance on detecting malignancy of Osteosarcoma based on histologic images.\n", "versions": [{"version": "v1", "created": "Mon, 2 Nov 2020 18:16:17 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["Anisuzzaman", "D M", ""], ["Barzekar", "Hosein", ""], ["Tong", "Ling", ""], ["Luo", "Jake", ""], ["Yu", "Zeyun", ""]]}, {"id": "2011.01215", "submitter": "Qi Mao", "authors": "Qi Mao, Hung-Yu Tseng, Hsin-Ying Lee, Jia-Bin Huang, Siwei Ma,\n  Ming-Hsuan Yang", "title": "Continuous and Diverse Image-to-Image Translation via Signed Attribute\n  Vectors", "comments": "Website: https://helenmao.github.io/SAVI2I/ Code:\n  https://github.com/HelenMao/SAVI2I", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Recent image-to-image (I2I) translation algorithms focus on learning the\nmapping from a source to a target domain. However, the continuous translation\nproblem that synthesizes intermediate results between two domains has not been\nwell-studied in the literature. Generating a smooth sequence of intermediate\nresults bridges the gap of two different domains, facilitating the morphing\neffect across domains. Existing I2I approaches are limited to either\nintra-domain or deterministic inter-domain continuous translation. In this\nwork, we present an effectively signed attribute vector, which enables\ncontinuous translation on diverse mapping paths across various domains. In\nparticular, we introduce a unified attribute space shared by all domains that\nutilize the sign operation to encode the domain information, thereby allowing\nthe interpolation on attribute vectors of different domains. To enhance the\nvisual quality of continuous translation results, we generate a trajectory\nbetween two sign-symmetrical attribute vectors and leverage the domain\ninformation of the interpolated results along the trajectory for adversarial\ntraining. We evaluate the proposed method on a wide range of I2I translation\ntasks. Both qualitative and quantitative results demonstrate that the proposed\nframework generates more high-quality continuous translation results against\nthe state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Mon, 2 Nov 2020 18:59:03 GMT"}, {"version": "v2", "created": "Tue, 3 Nov 2020 07:43:09 GMT"}, {"version": "v3", "created": "Thu, 19 Nov 2020 02:39:28 GMT"}, {"version": "v4", "created": "Sun, 18 Apr 2021 05:48:17 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Mao", "Qi", ""], ["Tseng", "Hung-Yu", ""], ["Lee", "Hsin-Ying", ""], ["Huang", "Jia-Bin", ""], ["Ma", "Siwei", ""], ["Yang", "Ming-Hsuan", ""]]}, {"id": "2011.01280", "submitter": "Simon Niklaus", "authors": "Simon Niklaus and Long Mai and Oliver Wang", "title": "Revisiting Adaptive Convolutions for Video Frame Interpolation", "comments": "WACV 2021, http://sniklaus.com/resepconv", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video frame interpolation, the synthesis of novel views in time, is an\nincreasingly popular research direction with many new papers further advancing\nthe state of the art. But as each new method comes with a host of variables\nthat affect the interpolation quality, it can be hard to tell what is actually\nimportant for this task. In this work, we show, somewhat surprisingly, that it\nis possible to achieve near state-of-the-art results with an older, simpler\napproach, namely adaptive separable convolutions, by a subtle set of low level\nimprovements. In doing so, we propose a number of intuitive but effective\ntechniques to improve the frame interpolation quality, which also have the\npotential to other related applications of adaptive convolutions such as burst\nimage denoising, joint image filtering, or video prediction.\n", "versions": [{"version": "v1", "created": "Mon, 2 Nov 2020 19:52:28 GMT"}], "update_date": "2020-11-04", "authors_parsed": [["Niklaus", "Simon", ""], ["Mai", "Long", ""], ["Wang", "Oliver", ""]]}, {"id": "2011.01353", "submitter": "Yuheng Wang", "authors": "Yuheng Wang, Wen Jie Zhao, Jiahui Xu and Raymond Hong", "title": "Recyclable Waste Identification Using CNN Image Recognition and Gaussian\n  Clustering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Waste recycling is an important way of saving energy and materials in the\nproduction process. In general cases recyclable objects are mixed with\nunrecyclable objects, which raises a need for identification and\nclassification. This paper proposes a convolutional neural network (CNN) model\nto complete both tasks. The model uses transfer learning from a pretrained\nResnet-50 CNN to complete feature extraction. A subsequent fully connected\nlayer for classification was trained on the augmented TrashNet dataset [1]. In\nthe application, sliding-window is used for image segmentation in the\npre-classification stage. In the post-classification stage, the labelled sample\npoints are integrated with Gaussian Clustering to locate the object. The\nresulting model has achieved an overall detection rate of 48.4% in simulation\nand final classification accuracy of 92.4%.\n", "versions": [{"version": "v1", "created": "Mon, 2 Nov 2020 22:26:25 GMT"}], "update_date": "2020-11-04", "authors_parsed": [["Wang", "Yuheng", ""], ["Zhao", "Wen Jie", ""], ["Xu", "Jiahui", ""], ["Hong", "Raymond", ""]]}, {"id": "2011.01354", "submitter": "Kenny Chen", "authors": "Kenny Chen, Alexandra Pogue, Brett T. Lopez, Ali-akbar Agha-mohammadi,\n  and Ankur Mehta", "title": "Unsupervised Monocular Depth Learning with Integrated Intrinsics and\n  Spatio-Temporal Constraints", "comments": "Submitted to the IEEE/RSJ International Conference on Intelligent\n  Robots and Systems (IROS 2021), Prague, Czech Republic", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Monocular depth inference has gained tremendous attention from researchers in\nrecent years and remains as a promising replacement for expensive\ntime-of-flight sensors, but issues with scale acquisition and implementation\noverhead still plague these systems. To this end, this work presents an\nunsupervised learning framework that is able to predict at-scale depth maps and\negomotion, in addition to camera intrinsics, from a sequence of monocular\nimages via a single network. Our method incorporates both spatial and temporal\ngeometric constraints to resolve depth and pose scale factors, which are\nenforced within the supervisory reconstruction loss functions at training time.\nOnly unlabeled stereo sequences are required for training the weights of our\nsingle-network architecture, which reduces overall implementation overhead as\ncompared to previous methods. Our results demonstrate strong performance when\ncompared to the current state-of-the-art on multiple sequences of the KITTI\ndriving dataset and can provide faster training times with its reduced network\ncomplexity.\n", "versions": [{"version": "v1", "created": "Mon, 2 Nov 2020 22:26:58 GMT"}, {"version": "v2", "created": "Sat, 6 Mar 2021 20:36:06 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Chen", "Kenny", ""], ["Pogue", "Alexandra", ""], ["Lopez", "Brett T.", ""], ["Agha-mohammadi", "Ali-akbar", ""], ["Mehta", "Ankur", ""]]}, {"id": "2011.01355", "submitter": "Shreyas Fadnavis", "authors": "Shreyas Fadnavis, Joshua Batson, Eleftherios Garyfallidis", "title": "Patch2Self: Denoising Diffusion MRI with Self-Supervised Learning", "comments": null, "journal-ref": "Thirty-fourth Conference on Neural Information Processing Systems,\n  2020", "doi": null, "report-no": null, "categories": "cs.LG cs.CV q-bio.QM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Diffusion-weighted magnetic resonance imaging (DWI) is the only noninvasive\nmethod for quantifying microstructure and reconstructing white-matter pathways\nin the living human brain. Fluctuations from multiple sources create\nsignificant additive noise in DWI data which must be suppressed before\nsubsequent microstructure analysis. We introduce a self-supervised learning\nmethod for denoising DWI data, Patch2Self, which uses the entire volume to\nlearn a full-rank locally linear denoiser for that volume. By taking advantage\nof the oversampled q-space of DWI data, Patch2Self can separate structure from\nnoise without requiring an explicit model for either. We demonstrate the\neffectiveness of Patch2Self via quantitative and qualitative improvements in\nmicrostructure modeling, tracking (via fiber bundle coherency) and model\nestimation relative to other unsupervised methods on real and simulated data.\n", "versions": [{"version": "v1", "created": "Mon, 2 Nov 2020 22:27:25 GMT"}], "update_date": "2020-11-04", "authors_parsed": [["Fadnavis", "Shreyas", ""], ["Batson", "Joshua", ""], ["Garyfallidis", "Eleftherios", ""]]}, {"id": "2011.01385", "submitter": "Litao Yu", "authors": "Litao Yu, Jian Zhang, Qiang Wu", "title": "Dual Attention on Pyramid Feature Maps for Image Captioning", "comments": "in IEEE Transactions on Multimedia, 2021", "journal-ref": null, "doi": "10.1109/TMM.2021.3072479", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generating natural sentences from images is a fundamental learning task for\nvisual-semantic understanding in multimedia. In this paper, we propose to apply\ndual attention on pyramid image feature maps to fully explore the\nvisual-semantic correlations and improve the quality of generated sentences.\nSpecifically, with the full consideration of the contextual information\nprovided by the hidden state of the RNN controller, the pyramid attention can\nbetter localize the visually indicative and semantically consistent regions in\nimages. On the other hand, the contextual information can help re-calibrate the\nimportance of feature components by learning the channel-wise dependencies, to\nimprove the discriminative power of visual features for better content\ndescription. We conducted comprehensive experiments on three well-known\ndatasets: Flickr8K, Flickr30K and MS COCO, which achieved impressive results in\ngenerating descriptive and smooth natural sentences from images. Using either\nconvolution visual features or more informative bottom-up attention features,\nour composite captioning model achieves very promising performance in a\nsingle-model mode. The proposed pyramid attention and dual attention methods\nare highly modular, which can be inserted into various image captioning modules\nto further improve the performance.\n", "versions": [{"version": "v1", "created": "Mon, 2 Nov 2020 23:42:34 GMT"}, {"version": "v2", "created": "Thu, 10 Jun 2021 01:14:55 GMT"}], "update_date": "2021-06-11", "authors_parsed": [["Yu", "Litao", ""], ["Zhang", "Jian", ""], ["Wu", "Qiang", ""]]}, {"id": "2011.01391", "submitter": "Litao Yu", "authors": "Litao Yu, Yongsheng Gao, Jun Zhou, Jian Zhang", "title": "Parameter Efficient Deep Neural Networks with Bilinear Projections", "comments": null, "journal-ref": "IEEE Transactions on Neural Networks and Learning Systems 2020", "doi": "10.1109/TNNLS.2020.3016688", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent research on deep neural networks (DNNs) has primarily focused on\nimproving the model accuracy. Given a proper deep learning framework, it is\ngenerally possible to increase the depth or layer width to achieve a higher\nlevel of accuracy. However, the huge number of model parameters imposes more\ncomputational and memory usage overhead and leads to the parameter redundancy.\nIn this paper, we address the parameter redundancy problem in DNNs by replacing\nconventional full projections with bilinear projections. For a fully-connected\nlayer with $D$ input nodes and $D$ output nodes, applying bilinear projection\ncan reduce the model space complexity from $\\mathcal{O}(D^2)$ to\n$\\mathcal{O}(2D)$, achieving a deep model with a sub-linear layer size.\nHowever, structured projection has a lower freedom of degree compared to the\nfull projection, causing the under-fitting problem. So we simply scale up the\nmapping size by increasing the number of output channels, which can keep and\neven boosts the model accuracy. This makes it very parameter-efficient and\nhandy to deploy such deep models on mobile systems with memory limitations.\nExperiments on four benchmark datasets show that applying the proposed bilinear\nprojection to deep neural networks can achieve even higher accuracies than\nconventional full DNNs, while significantly reduces the model size.\n", "versions": [{"version": "v1", "created": "Tue, 3 Nov 2020 00:17:24 GMT"}], "update_date": "2020-11-04", "authors_parsed": [["Yu", "Litao", ""], ["Gao", "Yongsheng", ""], ["Zhou", "Jun", ""], ["Zhang", "Jian", ""]]}, {"id": "2011.01404", "submitter": "Haolin Zhang", "authors": "Haolin Zhang, Dongfang Yang, Ekim Yurtsever, Keith A. Redmill and\n  \\\"Umit \\\"Ozg\\\"uner", "title": "Faraway-Frustum: Dealing with Lidar Sparsity for 3D Object Detection\n  using Fusion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learned pointcloud representations do not generalize well with an increase in\ndistance to the sensor. For example, at a range greater than 60 meters, the\nsparsity of lidar pointclouds reaches to a point where even humans cannot\ndiscern object shapes from each other. However, this distance should not be\nconsidered very far for fast-moving vehicles: A vehicle can traverse 60 meters\nunder two seconds while moving at 70 mph. For safe and robust driving\nautomation, acute 3D object detection at these ranges is indispensable. Against\nthis backdrop, we introduce faraway-frustum: a novel fusion strategy for\ndetecting faraway objects. The main strategy is to depend solely on the 2D\nvision for recognizing object class, as object shape does not change\ndrastically with an increase in depth, and use pointcloud data for object\nlocalization in the 3D space for faraway objects. For closer objects, we use\nlearned pointcloud representations instead, following state-of-the-art. This\nstrategy alleviates the main shortcoming of object detection with learned\npointcloud representations. Experiments on the KITTI dataset demonstrate that\nour method outperforms state-of-the-art by a considerable margin for faraway\nobject detection in bird's-eye-view and 3D. Our code is open-source and\npublicly available: https://github.com/dongfang-steven-yang/faraway-frustum.\n", "versions": [{"version": "v1", "created": "Tue, 3 Nov 2020 01:14:51 GMT"}, {"version": "v2", "created": "Thu, 25 Mar 2021 17:50:40 GMT"}, {"version": "v3", "created": "Sat, 27 Mar 2021 15:31:29 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Zhang", "Haolin", ""], ["Yang", "Dongfang", ""], ["Yurtsever", "Ekim", ""], ["Redmill", "Keith A.", ""], ["\u00d6zg\u00fcner", "\u00dcmit", ""]]}, {"id": "2011.01406", "submitter": "Majed El Helou", "authors": "Majed El Helou and Sabine S\\\"usstrunk", "title": "BIGPrior: Towards Decoupling Learned Prior Hallucination and Data\n  Fidelity in Image Restoration", "comments": "Under submission. Code available on\n  https://github.com/majedelhelou/BIGPrior", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image restoration encompasses fundamental image processing tasks that have\nbeen addressed with different algorithms and deep learning methods. Classical\nrestoration algorithms leverage a variety of priors, either implicitly or\nexplicitly. Their priors are hand-designed and their corresponding weights are\nheuristically assigned. Thus, deep learning methods often produce superior\nrestoration quality. Deep networks are, however, capable of strong and\nhardly-predictable hallucinations. Networks jointly and implicitly learn to be\nfaithful to the observed data while learning an image prior, and the separation\nof original and hallucinated data downstream is then not possible. This limits\ntheir wide-spread adoption in restoration applications. Furthermore, it is\noften the hallucinated part that is victim to degradation-model overfitting. We\npresent an approach with decoupled network-prior hallucination and data\nfidelity. We refer to our framework as the Bayesian Integration of a Generative\nPrior (BIGPrior). Our BIGPrior method is rooted in a Bayesian restoration\nframework, and tightly connected to classical restoration methods. In fact, our\napproach can be viewed as a generalization of a large family of classical\nrestoration algorithms. We leverage a recent network inversion method to\nextract image prior information from a generative network. We show on image\ncolorization, inpainting, and denoising that our framework consistently\nimproves the prior results through good integration of data fidelity. Our\nmethod, though partly reliant on the quality of the generative network\ninversion, is competitive with state-of-the-art supervised and task-specific\nrestoration methods. It also provides an additional metric that sets forth the\ndegree of prior reliance per pixel. Indeed, the per pixel contributions of the\ndecoupled data fidelity and prior terms are readily available in our proposed\nframework.\n", "versions": [{"version": "v1", "created": "Tue, 3 Nov 2020 01:16:41 GMT"}, {"version": "v2", "created": "Thu, 7 Jan 2021 18:51:48 GMT"}], "update_date": "2021-01-08", "authors_parsed": [["Helou", "Majed El", ""], ["S\u00fcsstrunk", "Sabine", ""]]}, {"id": "2011.01413", "submitter": "Masha Itkina", "authors": "Julia Nitsch, Masha Itkina, Ransalu Senanayake, Juan Nieto, Max\n  Schmidt, Roland Siegwart, Mykel J. Kochenderfer, and Cesar Cadena", "title": "Out-of-Distribution Detection for Automotive Perception", "comments": "7 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural networks (NNs) are widely used for object recognition tasks in\nautonomous driving. However, NNs can fail on input data not well represented by\nthe training dataset, known as out-of-distribution (OOD) data. A mechanism to\ndetect OOD samples is important in safety-critical applications, such as\nautomotive perception, in order to trigger a safe fallback mode. NNs often rely\non softmax normalization for confidence estimation, which can lead to high\nconfidences being assigned to OOD samples, thus hindering the detection of\nfailures. This paper presents a simple but effective method for determining\nwhether inputs are OOD. We propose an OOD detection approach that combines\nauxiliary training techniques with post hoc statistics. Unlike other\napproaches, our proposed method does not require OOD data during training, and\nit does not increase the computational cost during inference. The latter\nproperty is especially important in automotive applications with limited\ncomputational resources and real-time constraints. Our proposed method\noutperforms state-of-the-art methods on real world automotive datasets.\n", "versions": [{"version": "v1", "created": "Tue, 3 Nov 2020 01:46:35 GMT"}], "update_date": "2020-11-04", "authors_parsed": [["Nitsch", "Julia", ""], ["Itkina", "Masha", ""], ["Senanayake", "Ransalu", ""], ["Nieto", "Juan", ""], ["Schmidt", "Max", ""], ["Siegwart", "Roland", ""], ["Kochenderfer", "Mykel J.", ""], ["Cadena", "Cesar", ""]]}, {"id": "2011.01414", "submitter": "Songyang Zhang", "authors": "Li Sun, Haoqi Zhang, Songyang Zhang, Jiebo Luo", "title": "Content-based Analysis of the Cultural Differences between TikTok and\n  Douyin", "comments": "Accepted by IEEE Big Data 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Short-form video social media shifts away from the traditional media paradigm\nby telling the audience a dynamic story to attract their attention. In\nparticular, different combinations of everyday objects can be employed to\nrepresent a unique scene that is both interesting and understandable. Offered\nby the same company, TikTok and Douyin are popular examples of such new media\nthat has become popular in recent years, while being tailored for different\nmarkets (e.g. the United States and China). The hypothesis that they express\ncultural differences together with media fashion and social idiosyncrasy is the\nprimary target of our research. To that end, we first employ the Faster\nRegional Convolutional Neural Network (Faster R-CNN) pre-trained with the\nMicrosoft Common Objects in COntext (MS-COCO) dataset to perform object\ndetection. Based on a suite of objects detected from videos, we perform\nstatistical analysis including label statistics, label similarity, and\nlabel-person distribution. We further use the Two-Stream Inflated 3D ConvNet\n(I3D) pre-trained with the Kinetics dataset to categorize and analyze human\nactions. By comparing the distributional results of TikTok and Douyin, we\nuncover a wealth of similarity and contrast between the two closely related\nvideo social media platforms along the content dimensions of object quantity,\nobject categories, and human action categories.\n", "versions": [{"version": "v1", "created": "Tue, 3 Nov 2020 01:47:49 GMT"}], "update_date": "2020-11-04", "authors_parsed": [["Sun", "Li", ""], ["Zhang", "Haoqi", ""], ["Zhang", "Songyang", ""], ["Luo", "Jiebo", ""]]}, {"id": "2011.01424", "submitter": "Guo-Hua Wang", "authors": "Guo-Hua Wang, Yifan Ge, Jianxin Wu", "title": "In Defense of Feature Mimicking for Knowledge Distillation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Knowledge distillation (KD) is a popular method to train efficient networks\n(\"student\") with the help of high-capacity networks (\"teacher\"). Traditional\nmethods use the teacher's soft logit as extra supervision to train the student\nnetwork. In this paper, we argue that it is more advantageous to make the\nstudent mimic the teacher's features in the penultimate layer. Not only the\nstudent can directly learn more effective information from the teacher feature,\nfeature mimicking can also be applied for teachers trained without a softmax\nlayer. Experiments show that it can achieve higher accuracy than traditional\nKD. To further facilitate feature mimicking, we decompose a feature vector into\nthe magnitude and the direction. We argue that the teacher should give more\nfreedom to the student feature's magnitude, and let the student pay more\nattention on mimicking the feature direction. To meet this requirement, we\npropose a loss term based on locality-sensitive hashing (LSH). With the help of\nthis new loss, our method indeed mimics feature directions more accurately,\nrelaxes constraints on feature magnitudes, and achieves state-of-the-art\ndistillation accuracy.\n", "versions": [{"version": "v1", "created": "Tue, 3 Nov 2020 02:15:14 GMT"}], "update_date": "2020-11-04", "authors_parsed": [["Wang", "Guo-Hua", ""], ["Ge", "Yifan", ""], ["Wu", "Jianxin", ""]]}, {"id": "2011.01429", "submitter": "Jiacheng Wang", "authors": "Jiacheng Wang, Yue Ma, and Shuang Gao", "title": "Self-semi-supervised Learning to Learn from NoisyLabeled Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The remarkable success of today's deep neural networks highly depends on a\nmassive number of correctly labeled data. However, it is rather costly to\nobtain high-quality human-labeled data, leading to the active research area of\ntraining models robust to noisy labels. To achieve this goal, on the one hand,\nmany papers have been dedicated to differentiating noisy labels from clean ones\nto increase the generalization of DNN. On the other hand, the increasingly\nprevalent methods of self-semi-supervised learning have been proven to benefit\nthe tasks when labels are incomplete. By 'semi' we regard the wrongly labeled\ndata detected as un-labeled data; by 'self' we choose a self-supervised\ntechnique to conduct semi-supervised learning. In this project, we designed\nmethods to more accurately differentiate clean and noisy labels and borrowed\nthe wisdom of self-semi-supervised learning to train noisy labeled data.\n", "versions": [{"version": "v1", "created": "Tue, 3 Nov 2020 02:31:29 GMT"}], "update_date": "2020-11-04", "authors_parsed": [["Wang", "Jiacheng", ""], ["Ma", "Yue", ""], ["Gao", "Shuang", ""]]}, {"id": "2011.01434", "submitter": "Gaurab Banerjee", "authors": "Gaurab Banerjee, Samuel Spinner, Yasmine Mitchell", "title": "\"You eat with your eyes first\": Optimizing Yelp Image Advertising", "comments": "9 pages, 8 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A business's online, photographic representation can play a crucial role in\nits success or failure. We use Yelp's image dataset and star-based review\nsystem as a measurement of an image's effectiveness in promoting a business.\nAfter preprocessing the Yelp dataset, we use transfer learning to train a\nclassifier which accepts Yelp images and predicts star-ratings. Additionally,\nwe then train a GAN to qualitatively investigate the common properties of\nhighly effective images. We achieve 90-98% accuracy in classifying simplified\nstar ratings for various image categories and observe that images containing\nblue skies, open surroundings, and many windows are correlated with higher Yelp\nreviews.\n", "versions": [{"version": "v1", "created": "Tue, 3 Nov 2020 02:49:40 GMT"}], "update_date": "2020-11-04", "authors_parsed": [["Banerjee", "Gaurab", ""], ["Spinner", "Samuel", ""], ["Mitchell", "Yasmine", ""]]}, {"id": "2011.01436", "submitter": "Minho Kim", "authors": "Minho Kim, Doyoung Jeong, Hyoungwoo Choi, Yongil Kim", "title": "Developing High Quality Training Samples for Deep Learning Based Local\n  Climate Zone Classification in Korea", "comments": "7 pages, 7 figures; AI for Earth Workshop at NeurIPS2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Two out of three people will be living in urban areas by 2050, as projected\nby the United Nations, emphasizing the need for sustainable urban development\nand monitoring. Common urban footprint data provide high-resolution city\nextents but lack essential information on the distribution, pattern, and\ncharacteristics. The Local Climate Zone (LCZ) offers an efficient and\nstandardized framework that can delineate the internal structure and\ncharacteristics of urban areas. Global-scale LCZ mapping has been explored, but\nare limited by low accuracy, variable labeling quality, or domain adaptation\nchallenges. Instead, this study developed a custom LCZ data to map key Korean\ncities using a multi-scale convolutional neural network. Results demonstrated\nthat using a novel, custom LCZ data with deep learning can generate more\naccurate LCZ map results compared to conventional community-based LCZ mapping\nwith machine learning as well as transfer learning of the global So2Sat\ndataset.\n", "versions": [{"version": "v1", "created": "Tue, 3 Nov 2020 02:52:37 GMT"}, {"version": "v2", "created": "Thu, 10 Dec 2020 09:20:52 GMT"}], "update_date": "2020-12-11", "authors_parsed": [["Kim", "Minho", ""], ["Jeong", "Doyoung", ""], ["Choi", "Hyoungwoo", ""], ["Kim", "Yongil", ""]]}, {"id": "2011.01437", "submitter": "Jun Gao", "authors": "Jun Gao, Wenzheng Chen, Tommy Xiang, Clement Fuji Tsang, Alec\n  Jacobson, Morgan McGuire, Sanja Fidler", "title": "Learning Deformable Tetrahedral Meshes for 3D Reconstruction", "comments": "Accepted to NeurIPS 2020. Webpage: https://nv-tlabs.github.io/DefTet/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  3D shape representations that accommodate learning-based 3D reconstruction\nare an open problem in machine learning and computer graphics. Previous work on\nneural 3D reconstruction demonstrated benefits, but also limitations, of point\ncloud, voxel, surface mesh, and implicit function representations. We introduce\nDeformable Tetrahedral Meshes (DefTet) as a particular parameterization that\nutilizes volumetric tetrahedral meshes for the reconstruction problem. Unlike\nexisting volumetric approaches, DefTet optimizes for both vertex placement and\noccupancy, and is differentiable with respect to standard 3D reconstruction\nloss functions. It is thus simultaneously high-precision, volumetric, and\namenable to learning-based neural architectures. We show that it can represent\narbitrary, complex topology, is both memory and computationally efficient, and\ncan produce high-fidelity reconstructions with a significantly smaller grid\nsize than alternative volumetric approaches. The predicted surfaces are also\ninherently defined as tetrahedral meshes, thus do not require post-processing.\nWe demonstrate that DefTet matches or exceeds both the quality of the previous\nbest approaches and the performance of the fastest ones. Our approach obtains\nhigh-quality tetrahedral meshes computed directly from noisy point clouds, and\nis the first to showcase high-quality 3D tet-mesh results using only a single\nimage as input. Our project webpage: https://nv-tlabs.github.io/DefTet/\n", "versions": [{"version": "v1", "created": "Tue, 3 Nov 2020 02:57:01 GMT"}, {"version": "v2", "created": "Mon, 23 Nov 2020 19:52:13 GMT"}], "update_date": "2020-11-25", "authors_parsed": [["Gao", "Jun", ""], ["Chen", "Wenzheng", ""], ["Xiang", "Tommy", ""], ["Tsang", "Clement Fuji", ""], ["Jacobson", "Alec", ""], ["McGuire", "Morgan", ""], ["Fidler", "Sanja", ""]]}, {"id": "2011.01461", "submitter": "Beibei Lin", "authors": "Beibei Lin, Shunli Zhang, Xin Yu, Zedong Chu, Haikun Zhang", "title": "Learning Effective Representations from Global and Local Features for\n  Cross-View Gait Recognition", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gait recognition is one of the most important biometric technologies and has\nbeen applied in many fields. Recent gait recognition frameworks represent each\nhuman gait frame by descriptors extracted from either global appearances or\nlocal regions of humans. However, the representations based on global\ninformation often neglect the details of the gait frame, while local region\nbased descriptors cannot capture the relations among neighboring regions, thus\nreducing their discriminativeness. In this paper, we propose a novel feature\nextraction and fusion framework to achieve discriminative feature\nrepresentations for gait recognition. Towards this goal, we take advantage of\nboth global visual information and local region details and develop a Global\nand Local Feature Extractor (GLFE). Specifically, our GLFE module is composed\nof our newly designed multiple global and local convolutional layers (GLConv)\nto ensemble global and local features in a principle manner. Furthermore, we\npresent a novel operation, namely Local Temporal Aggregation (LTA), to further\npreserve the spatial information by reducing the temporal resolution to obtain\nhigher spatial resolution. With the help of our GLFE and LTA, our method\nsignificantly improves the discriminativeness of our visual features, thus\nimproving the gait recognition performance. Extensive experiments demonstrate\nthat our proposed method outperforms state-of-the-art gait recognition methods\non popular widely-used CASIA-B and OUMVLP datasets.\n", "versions": [{"version": "v1", "created": "Tue, 3 Nov 2020 04:07:13 GMT"}], "update_date": "2020-11-04", "authors_parsed": [["Lin", "Beibei", ""], ["Zhang", "Shunli", ""], ["Yu", "Xin", ""], ["Chu", "Zedong", ""], ["Zhang", "Haikun", ""]]}, {"id": "2011.01462", "submitter": "Litao Yu", "authors": "Zhibin Li, Litao Yu, Jian Zhang", "title": "Distribution-aware Margin Calibration for Medical Image Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Jaccard index, also known as Intersection-over-Union (IoU score), is one\nof the most critical evaluation metrics in medical image segmentation. However,\ndirectly optimizing the mean IoU (mIoU) score over multiple objective classes\nis an open problem. Although some algorithms have been proposed to optimize its\nsurrogates, there is no guarantee provided for their generalization ability. In\nthis paper, we present a novel data-distribution-aware margin calibration\nmethod for a better generalization of the mIoU over the whole\ndata-distribution, underpinned by a rigid lower bound. This scheme ensures a\nbetter segmentation performance in terms of IoU scores in practice. We evaluate\nthe effectiveness of the proposed margin calibration method on two medical\nimage segmentation datasets, showing substantial improvements of IoU scores\nover other learning schemes using deep segmentation models.\n", "versions": [{"version": "v1", "created": "Tue, 3 Nov 2020 04:07:47 GMT"}], "update_date": "2020-11-04", "authors_parsed": [["Li", "Zhibin", ""], ["Yu", "Litao", ""], ["Zhang", "Jian", ""]]}, {"id": "2011.01472", "submitter": "Narayanan Chatapuram Krishnan", "authors": "Ashish Kumar, Karan Sehgal, Prerna Garg, Vidhya Kamakshi, and\n  Narayanan C Krishnan", "title": "MACE: Model Agnostic Concept Extractor for Explaining Image\n  Classification Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep convolutional networks have been quite successful at various image\nclassification tasks. The current methods to explain the predictions of a\npre-trained model rely on gradient information, often resulting in saliency\nmaps that focus on the foreground object as a whole. However, humans typically\nreason by dissecting an image and pointing out the presence of smaller\nconcepts. The final output is often an aggregation of the presence or absence\nof these smaller concepts. In this work, we propose MACE: a Model Agnostic\nConcept Extractor, which can explain the working of a convolutional network\nthrough smaller concepts. The MACE framework dissects the feature maps\ngenerated by a convolution network for an image to extract concept based\nprototypical explanations. Further, it estimates the relevance of the extracted\nconcepts to the pre-trained model's predictions, a critical aspect required for\nexplaining the individual class predictions, missing in existing approaches. We\nvalidate our framework using VGG16 and ResNet50 CNN architectures, and on\ndatasets like Animals With Attributes 2 (AWA2) and Places365. Our experiments\ndemonstrate that the concepts extracted by the MACE framework increase the\nhuman interpretability of the explanations, and are faithful to the underlying\npre-trained black-box model.\n", "versions": [{"version": "v1", "created": "Tue, 3 Nov 2020 04:40:49 GMT"}], "update_date": "2020-11-04", "authors_parsed": [["Kumar", "Ashish", ""], ["Sehgal", "Karan", ""], ["Garg", "Prerna", ""], ["Kamakshi", "Vidhya", ""], ["Krishnan", "Narayanan C", ""]]}, {"id": "2011.01477", "submitter": "Chong Peng", "authors": "Chong Peng, Qian Zhang, Zhao Kang, Chenglizhao Chen, and Qiang Cheng", "title": "Kernel Two-Dimensional Ridge Regression for Subspace Clustering", "comments": "accepted to Pattern Recognition", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Subspace clustering methods have been widely studied recently. When the\ninputs are 2-dimensional (2D) data, existing subspace clustering methods\nusually convert them into vectors, which severely damages inherent structures\nand relationships from original data. In this paper, we propose a novel\nsubspace clustering method for 2D data. It directly uses 2D data as inputs such\nthat the learning of representations benefits from inherent structures and\nrelationships of the data. It simultaneously seeks image projection and\nrepresentation coefficients such that they mutually enhance each other and lead\nto powerful data representations. An efficient algorithm is developed to solve\nthe proposed objective function with provable decreasing and convergence\nproperty. Extensive experimental results verify the effectiveness of the new\nmethod.\n", "versions": [{"version": "v1", "created": "Tue, 3 Nov 2020 04:52:46 GMT"}], "update_date": "2020-11-04", "authors_parsed": [["Peng", "Chong", ""], ["Zhang", "Qian", ""], ["Kang", "Zhao", ""], ["Chen", "Chenglizhao", ""], ["Cheng", "Qiang", ""]]}, {"id": "2011.01498", "submitter": "Narayanan Chatapuram Krishnan", "authors": "Sagarika Sharma, Sujit Rai, Narayanan C. Krishnan", "title": "Wheat Crop Yield Prediction Using Deep LSTM Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An in-season early crop yield forecast before harvest can benefit the farmers\nto improve the production and enable various agencies to devise plans\naccordingly. We introduce a reliable and inexpensive method to predict crop\nyields from publicly available satellite imagery. The proposed method works\ndirectly on raw satellite imagery without the need to extract any hand-crafted\nfeatures or perform dimensionality reduction on the images. The approach\nimplicitly models the relevance of the different steps in the growing season\nand the various bands in the satellite imagery. We evaluate the proposed\napproach on tehsil (block) level wheat predictions across several states in\nIndia and demonstrate that it outperforms existing methods by over 50\\%. We\nalso show that incorporating additional contextual information such as the\nlocation of farmlands, water bodies, and urban areas helps in improving the\nyield estimates.\n", "versions": [{"version": "v1", "created": "Tue, 3 Nov 2020 06:11:31 GMT"}], "update_date": "2020-11-04", "authors_parsed": [["Sharma", "Sagarika", ""], ["Rai", "Sujit", ""], ["Krishnan", "Narayanan C.", ""]]}, {"id": "2011.01507", "submitter": "Bochao Wang", "authors": "Bochao Wang, Hang Xu, Jiajin Zhang, Chen Chen, Xiaozhi Fang, Yixing\n  Xu, Ning Kang, Lanqing Hong, Chenhan Jiang, Xinyue Cai, Jiawei Li, Fengwei\n  Zhou, Yong Li, Zhicheng Liu, Xinghao Chen, Kai Han, Han Shu, Dehua Song,\n  Yunhe Wang, Wei Zhang, Chunjing Xu, Zhenguo Li, Wenzhi Liu, Tong Zhang", "title": "VEGA: Towards an End-to-End Configurable AutoML Pipeline", "comments": "AutoML pipeline. Code is open-sourced at\n  https://github.com/huawei-noah/vega", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automated Machine Learning (AutoML) is an important industrial solution for\nautomatic discovery and deployment of the machine learning models. However,\ndesigning an integrated AutoML system faces four great challenges of\nconfigurability, scalability, integrability, and platform diversity. In this\nwork, we present VEGA, an efficient and comprehensive AutoML framework that is\ncompatible and optimized for multiple hardware platforms. a) The VEGA pipeline\nintegrates various modules of AutoML, including Neural Architecture Search\n(NAS), Hyperparameter Optimization (HPO), Auto Data Augmentation, Model\nCompression, and Fully Train. b) To support a variety of search algorithms and\ntasks, we design a novel fine-grained search space and its description language\nto enable easy adaptation to different search algorithms and tasks. c) We\nabstract the common components of deep learning frameworks into a unified\ninterface. VEGA can be executed with multiple back-ends and hardwares.\nExtensive benchmark experiments on multiple tasks demonstrate that VEGA can\nimprove the existing AutoML algorithms and discover new high-performance models\nagainst SOTA methods, e.g. the searched DNet model zoo for Ascend 10x faster\nthan EfficientNet-B5 and 9.2x faster than RegNetX-32GF on ImageNet. VEGA is\nopen-sourced at https://github.com/huawei-noah/vega.\n", "versions": [{"version": "v1", "created": "Tue, 3 Nov 2020 06:53:53 GMT"}, {"version": "v2", "created": "Wed, 4 Nov 2020 10:36:25 GMT"}, {"version": "v3", "created": "Thu, 5 Nov 2020 08:21:59 GMT"}, {"version": "v4", "created": "Thu, 26 Nov 2020 05:28:50 GMT"}], "update_date": "2020-11-30", "authors_parsed": [["Wang", "Bochao", ""], ["Xu", "Hang", ""], ["Zhang", "Jiajin", ""], ["Chen", "Chen", ""], ["Fang", "Xiaozhi", ""], ["Xu", "Yixing", ""], ["Kang", "Ning", ""], ["Hong", "Lanqing", ""], ["Jiang", "Chenhan", ""], ["Cai", "Xinyue", ""], ["Li", "Jiawei", ""], ["Zhou", "Fengwei", ""], ["Li", "Yong", ""], ["Liu", "Zhicheng", ""], ["Chen", "Xinghao", ""], ["Han", "Kai", ""], ["Shu", "Han", ""], ["Song", "Dehua", ""], ["Wang", "Yunhe", ""], ["Zhang", "Wei", ""], ["Xu", "Chunjing", ""], ["Li", "Zhenguo", ""], ["Liu", "Wenzhi", ""], ["Zhang", "Tong", ""]]}, {"id": "2011.01519", "submitter": "Denis Tome", "authors": "Denis Tome, Thiemo Alldieck, Patrick Peluse, Gerard Pons-Moll, Lourdes\n  Agapito, Hernan Badino and Fernando De la Torre", "title": "SelfPose: 3D Egocentric Pose Estimation from a Headset Mounted Camera", "comments": "14 pages. arXiv admin note: substantial text overlap with\n  arXiv:1907.10045", "journal-ref": "IEEE Transactions on Pattern Analysis and Machine Intelligence,\n  2020", "doi": "10.1109/TPAMI.2020.3029700", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We present a solution to egocentric 3D body pose estimation from monocular\nimages captured from downward looking fish-eye cameras installed on the rim of\na head mounted VR device. This unusual viewpoint leads to images with unique\nvisual appearance, with severe self-occlusions and perspective distortions that\nresult in drastic differences in resolution between lower and upper body. We\npropose an encoder-decoder architecture with a novel multi-branch decoder\ndesigned to account for the varying uncertainty in 2D predictions. The\nquantitative evaluation, on synthetic and real-world datasets, shows that our\nstrategy leads to substantial improvements in accuracy over state of the art\negocentric approaches. To tackle the lack of labelled data we also introduced a\nlarge photo-realistic synthetic dataset. xR-EgoPose offers high quality\nrenderings of people with diverse skintones, body shapes and clothing,\nperforming a range of actions. Our experiments show that the high variability\nin our new synthetic training corpus leads to good generalization to real world\nfootage and to state of theart results on real world datasets with ground\ntruth. Moreover, an evaluation on the Human3.6M benchmark shows that the\nperformance of our method is on par with top performing approaches on the more\nclassic problem of 3D human pose from a third person viewpoint.\n", "versions": [{"version": "v1", "created": "Mon, 2 Nov 2020 16:18:06 GMT"}], "update_date": "2020-11-04", "authors_parsed": [["Tome", "Denis", ""], ["Alldieck", "Thiemo", ""], ["Peluse", "Patrick", ""], ["Pons-Moll", "Gerard", ""], ["Agapito", "Lourdes", ""], ["Badino", "Hernan", ""], ["De la Torre", "Fernando", ""]]}, {"id": "2011.01535", "submitter": "Shaul Oron", "authors": "Netalee Efrat, Max Bluvstein, Shaul Oron, Dan Levi, Noa Garnett, Bat\n  El Shlomo", "title": "3D-LaneNet+: Anchor Free Lane Detection using a Semi-Local\n  Representation", "comments": "arXiv admin note: substantial text overlap with arXiv:2003.05257", "journal-ref": "Machine Learning for Autonomous Driving Workshop at the 34th\n  Conference on Neural Information ProcessingSystems (NeurIPS 2020), Vancouver,\n  Canada", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  3D-LaneNet+ is a camera-based DNN method for anchor free 3D lane detection\nwhich is able to detect 3d lanes of any arbitrary topology such as splits,\nmerges, as well as short and perpendicular lanes. We follow recently proposed\n3D-LaneNet, and extend it to enable the detection of these previously\nunsupported lane topologies. Our output representation is an anchor free,\nsemi-local tile representation that breaks down lanes into simple lane segments\nwhose parameters can be learnt. In addition we learn, per lane instance,\nfeature embedding that reasons for the global connectivity of locally detected\nsegments to form full 3d lanes. This combination allows 3D-LaneNet+ to avoid\nusing lane anchors, non-maximum suppression, and lane model fitting as in the\noriginal 3D-LaneNet. We demonstrate the efficacy of 3D-LaneNet+ using both\nsynthetic and real world data. Results show significant improvement relative to\nthe original 3D-LaneNet that can be attributed to better generalization to\ncomplex lane topologies, curvatures and surface geometries.\n", "versions": [{"version": "v1", "created": "Sun, 1 Nov 2020 11:07:13 GMT"}, {"version": "v2", "created": "Wed, 4 Nov 2020 12:25:04 GMT"}], "update_date": "2020-11-05", "authors_parsed": [["Efrat", "Netalee", ""], ["Bluvstein", "Max", ""], ["Oron", "Shaul", ""], ["Levi", "Dan", ""], ["Garnett", "Noa", ""], ["Shlomo", "Bat El", ""]]}, {"id": "2011.01539", "submitter": "Tao Bai", "authors": "Tao Bai, Jinqi Luo, Jun Zhao", "title": "Recent Advances in Understanding Adversarial Robustness of Deep Neural\n  Networks", "comments": "Initial Draft", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adversarial examples are inevitable on the road of pervasive applications of\ndeep neural networks (DNN). Imperceptible perturbations applied on natural\nsamples can lead DNN-based classifiers to output wrong prediction with fair\nconfidence score. It is increasingly important to obtain models with high\nrobustness that are resistant to adversarial examples. In this paper, we survey\nrecent advances in how to understand such intriguing property, i.e. adversarial\nrobustness, from different perspectives. We give preliminary definitions on\nwhat adversarial attacks and robustness are. After that, we study\nfrequently-used benchmarks and mention theoretically-proved bounds for\nadversarial robustness. We then provide an overview on analyzing correlations\namong adversarial robustness and other critical indicators of DNN models.\nLastly, we introduce recent arguments on potential costs of adversarial\ntraining which have attracted wide attention from the research community.\n", "versions": [{"version": "v1", "created": "Tue, 3 Nov 2020 07:42:53 GMT"}], "update_date": "2020-11-04", "authors_parsed": [["Bai", "Tao", ""], ["Luo", "Jinqi", ""], ["Zhao", "Jun", ""]]}, {"id": "2011.01563", "submitter": "Naiyuan Liu", "authors": "Xuanhong Chen, Bingbing Ni, Naiyuan Liu, Ziang Liu, Yiliu Jiang, Loc\n  Truong, and Qi Tian", "title": "CooGAN: A Memory-Efficient Framework for High-Resolution Facial\n  Attribute Editing", "comments": null, "journal-ref": "European Conference on Computer Vision(ECCV) 2020", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In contrast to great success of memory-consuming face editing methods at a\nlow resolution, to manipulate high-resolution (HR) facial images, i.e.,\ntypically larger than 7682 pixels, with very limited memory is still\nchallenging. This is due to the reasons of 1) intractable huge demand of\nmemory; 2) inefficient multi-scale features fusion. To address these issues, we\npropose a NOVEL pixel translation framework called Cooperative GAN(CooGAN) for\nHR facial image editing. This framework features a local path for fine-grained\nlocal facial patch generation (i.e., patch-level HR, LOW memory) and a global\npath for global lowresolution (LR) facial structure monitoring (i.e.,\nimage-level LR, LOW memory), which largely reduce memory requirements. Both\npaths work in a cooperative manner under a local-to-global consistency\nobjective (i.e., for smooth stitching). In addition, we propose a lighter\nselective transfer unit for more efficient multi-scale features fusion,\nyielding higher fidelity facial attributes manipulation. Extensive experiments\non CelebAHQ well demonstrate the memory efficiency as well as the high image\ngeneration quality of the proposed framework.\n", "versions": [{"version": "v1", "created": "Tue, 3 Nov 2020 08:40:00 GMT"}], "update_date": "2020-11-04", "authors_parsed": [["Chen", "Xuanhong", ""], ["Ni", "Bingbing", ""], ["Liu", "Naiyuan", ""], ["Liu", "Ziang", ""], ["Jiang", "Yiliu", ""], ["Truong", "Loc", ""], ["Tian", "Qi", ""]]}, {"id": "2011.01565", "submitter": "Yue Wang", "authors": "Yue Wang, Jing Li, Michael R. Lyu, and Irwin King", "title": "Cross-Media Keyphrase Prediction: A Unified Framework with\n  Multi-Modality Multi-Head Attention and Image Wordings", "comments": "EMNLP 2020 (14 pages)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Social media produces large amounts of contents every day. To help users\nquickly capture what they need, keyphrase prediction is receiving a growing\nattention. Nevertheless, most prior efforts focus on text modeling, largely\nignoring the rich features embedded in the matching images. In this work, we\nexplore the joint effects of texts and images in predicting the keyphrases for\na multimedia post. To better align social media style texts and images, we\npropose: (1) a novel Multi-Modality Multi-Head Attention (M3H-Att) to capture\nthe intricate cross-media interactions; (2) image wordings, in forms of optical\ncharacters and image attributes, to bridge the two modalities. Moreover, we\ndesign a unified framework to leverage the outputs of keyphrase classification\nand generation and couple their advantages. Extensive experiments on a\nlarge-scale dataset newly collected from Twitter show that our model\nsignificantly outperforms the previous state of the art based on traditional\nattention networks. Further analyses show that our multi-head attention is able\nto attend information from various aspects and boost classification or\ngeneration in diverse scenarios.\n", "versions": [{"version": "v1", "created": "Tue, 3 Nov 2020 08:44:18 GMT"}], "update_date": "2020-11-04", "authors_parsed": [["Wang", "Yue", ""], ["Li", "Jing", ""], ["Lyu", "Michael R.", ""], ["King", "Irwin", ""]]}, {"id": "2011.01603", "submitter": "Ren\\'e Schuster", "authors": "Ren\\'e Schuster, Christian Unger, Didier Stricker", "title": "A Deep Temporal Fusion Framework for Scene Flow Using a Learnable Motion\n  Model and Occlusions", "comments": "Accepted to WACV21", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motion estimation is one of the core challenges in computer vision. With\ntraditional dual-frame approaches, occlusions and out-of-view motions are a\nlimiting factor, especially in the context of environmental perception for\nvehicles due to the large (ego-) motion of objects. Our work proposes a novel\ndata-driven approach for temporal fusion of scene flow estimates in a\nmulti-frame setup to overcome the issue of occlusion. Contrary to most previous\nmethods, we do not rely on a constant motion model, but instead learn a generic\ntemporal relation of motion from data. In a second step, a neural network\ncombines bi-directional scene flow estimates from a common reference frame,\nyielding a refined estimate and a natural byproduct of occlusion masks. This\nway, our approach provides a fast multi-frame extension for a variety of scene\nflow estimators, which outperforms the underlying dual-frame approaches.\n", "versions": [{"version": "v1", "created": "Tue, 3 Nov 2020 10:14:11 GMT"}, {"version": "v2", "created": "Wed, 4 Nov 2020 09:13:32 GMT"}], "update_date": "2020-11-05", "authors_parsed": [["Schuster", "Ren\u00e9", ""], ["Unger", "Christian", ""], ["Stricker", "Didier", ""]]}, {"id": "2011.01614", "submitter": "Lucas Fidon", "authors": "Lucas Fidon and Sebastien Ourselin and Tom Vercauteren", "title": "Generalized Wasserstein Dice Score, Distributionally Robust Deep\n  Learning, and Ranger for brain tumor segmentation: BraTS 2020 challenge", "comments": "MICCAI 2020 BrainLes Workshop. Our method ranked fourth out of the\n  693 registered teams for the segmentation task of the BraTS 2020 challenge.\n  v2: Added some clarifications following reviewers' feedback (camera-ready\n  version)", "journal-ref": null, "doi": "10.1007/978-3-030-72087-2_18", "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Training a deep neural network is an optimization problem with four main\ningredients: the design of the deep neural network, the per-sample loss\nfunction, the population loss function, and the optimizer. However, methods\ndeveloped to compete in recent BraTS challenges tend to focus only on the\ndesign of deep neural network architectures, while paying less attention to the\nthree other aspects. In this paper, we experimented with adopting the opposite\napproach. We stuck to a generic and state-of-the-art 3D U-Net architecture and\nexperimented with a non-standard per-sample loss function, the generalized\nWasserstein Dice loss, a non-standard population loss function, corresponding\nto distributionally robust optimization, and a non-standard optimizer, Ranger.\nThose variations were selected specifically for the problem of multi-class\nbrain tumor segmentation. The generalized Wasserstein Dice loss is a per-sample\nloss function that allows taking advantage of the hierarchical structure of the\ntumor regions labeled in BraTS. Distributionally robust optimization is a\ngeneralization of empirical risk minimization that accounts for the presence of\nunderrepresented subdomains in the training dataset. Ranger is a generalization\nof the widely used Adam optimizer that is more stable with small batch size and\nnoisy labels. We found that each of those variations of the optimization of\ndeep neural networks for brain tumor segmentation leads to improvements in\nterms of Dice scores and Hausdorff distances. With an ensemble of three deep\nneural networks trained with various optimization procedures, we achieved\npromising results on the validation dataset of the BraTS 2020 challenge. Our\nensemble ranked fourth out of the 693 registered teams for the segmentation\ntask of the BraTS 2020 challenge.\n", "versions": [{"version": "v1", "created": "Tue, 3 Nov 2020 10:50:48 GMT"}, {"version": "v2", "created": "Mon, 25 Jan 2021 11:41:03 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Fidon", "Lucas", ""], ["Ourselin", "Sebastien", ""], ["Vercauteren", "Tom", ""]]}, {"id": "2011.01619", "submitter": "Yonghao Long", "authors": "Yonghao Long, Jie Ying Wu, Bo Lu, Yueming Jin, Mathias Unberath,\n  Yun-Hui Liu, Pheng Ann Heng and Qi Dou", "title": "Relational Graph Learning on Visual and Kinematics Embeddings for\n  Accurate Gesture Recognition in Robotic Surgery", "comments": "Accepted for ICRA 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic surgical gesture recognition is fundamentally important to enable\nintelligent cognitive assistance in robotic surgery. With recent advancement in\nrobot-assisted minimally invasive surgery, rich information including surgical\nvideos and robotic kinematics can be recorded, which provide complementary\nknowledge for understanding surgical gestures. However, existing methods either\nsolely adopt uni-modal data or directly concatenate multi-modal\nrepresentations, which can not sufficiently exploit the informative\ncorrelations inherent in visual and kinematics data to boost gesture\nrecognition accuracies. In this regard, we propose a novel online approach of\nmulti-modal relational graph network (i.e., MRG-Net) to dynamically integrate\nvisual and kinematics information through interactive message propagation in\nthe latent feature space. In specific, we first extract embeddings from video\nand kinematics sequences with temporal convolutional networks and LSTM units.\nNext, we identify multi-relations in these multi-modal embeddings and leverage\nthem through a hierarchical relational graph learning module. The effectiveness\nof our method is demonstrated with state-of-the-art results on the public\nJIGSAWS dataset, outperforming current uni-modal and multi-modal methods on\nboth suturing and knot typing tasks. Furthermore, we validated our method on\nin-house visual-kinematics datasets collected with da Vinci Research Kit (dVRK)\nplatforms in two centers, with consistent promising performance achieved.\n", "versions": [{"version": "v1", "created": "Tue, 3 Nov 2020 11:00:10 GMT"}, {"version": "v2", "created": "Tue, 29 Jun 2021 05:52:38 GMT"}], "update_date": "2021-06-30", "authors_parsed": [["Long", "Yonghao", ""], ["Wu", "Jie Ying", ""], ["Lu", "Bo", ""], ["Jin", "Yueming", ""], ["Unberath", "Mathias", ""], ["Liu", "Yun-Hui", ""], ["Heng", "Pheng Ann", ""], ["Dou", "Qi", ""]]}, {"id": "2011.01655", "submitter": "Takumi Kawashima", "authors": "Takumi Kawashima and Qing Yu and Akari Asai and Daiki Ikami and\n  Kiyoharu Aizawa", "title": "The Aleatoric Uncertainty Estimation Using a Separate Formulation with\n  Virtual Residuals", "comments": null, "journal-ref": "ICPR2020", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new optimization framework for aleatoric uncertainty estimation\nin regression problems. Existing methods can quantify the error in the target\nestimation, but they tend to underestimate it. To obtain the predictive\nuncertainty inherent in an observation, we propose a new separable formulation\nfor the estimation of a signal and of its uncertainty, avoiding the effect of\noverfitting. By decoupling target estimation and uncertainty estimation, we\nalso control the balance between signal estimation and uncertainty estimation.\nWe conduct three types of experiments: regression with simulation data, age\nestimation, and depth estimation. We demonstrate that the proposed method\noutperforms a state-of-the-art technique for signal and uncertainty estimation.\n", "versions": [{"version": "v1", "created": "Tue, 3 Nov 2020 12:11:27 GMT"}], "update_date": "2021-03-12", "authors_parsed": [["Kawashima", "Takumi", ""], ["Yu", "Qing", ""], ["Asai", "Akari", ""], ["Ikami", "Daiki", ""], ["Aizawa", "Kiyoharu", ""]]}, {"id": "2011.01700", "submitter": "Sean Mullery Mr.", "authors": "Se\\'an Mullery and Paul F. Whelan", "title": "A spatial hue similarity measure for assessment of colourisation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic colourisation of grey-scale images is an ill-posed multi-modal\nproblem. Where full-reference images exist, objective performance measures rely\non pixel-difference techniques such as MSE and PSNR. These measures penalise\nany plausible modes other than the reference ground-truth; They often fail to\nadequately penalise implausible modes if they are close in pixel distance to\nthe ground-truth; As these are pixel-difference methods they cannot assess\nspatial coherency. We use the polar form of the a*b* channels from the\nCIEL*a*b* colour space to separate the multi-modal problems, which we confine\nto the hue channel, and the common-mode which applies to the chroma channel. We\napply SSIM to the chroma channel but reformulate SSIM for the hue channel to a\nmeasure we call the Spatial Hue Similarity Measure (SHSM). This reformulation\nallows spatially-coherent hue channels to achieve a high score while penalising\nspatially-incoherent modes. This method allows qualitative and quantitative\nperformance comparison of SOTA colourisation methods and reduces reliance on\nsubjective human visual inspection.\n", "versions": [{"version": "v1", "created": "Tue, 3 Nov 2020 13:43:36 GMT"}], "update_date": "2020-11-04", "authors_parsed": [["Mullery", "Se\u00e1n", ""], ["Whelan", "Paul F.", ""]]}, {"id": "2011.01730", "submitter": "Gulcin Baykal", "authors": "Gulcin Baykal, Furkan Ozcelik, Gozde Unal", "title": "Exploring DeshuffleGANs in Self-Supervised Generative Adversarial\n  Networks", "comments": "Preprint submitted to Elsevier Pattern Recognition", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative Adversarial Networks (GANs) have become the most used network\nmodels towards solving the problem of image generation. In recent years,\nself-supervised GANs are proposed to aid stabilized GAN training without the\ncatastrophic forgetting problem and to improve the image generation quality\nwithout the need for the class labels of the data. However, the\ngeneralizability of the self-supervision tasks on different GAN architectures\nis not studied before. To that end, we extensively analyze the contribution of\nthe deshuffling task of DeshuffleGANs in the generalizability context. We\nassign the deshuffling task to two different GAN discriminators and study the\neffects of the deshuffling on both architectures. We also evaluate the\nperformance of DeshuffleGANs on various datasets that are mostly used in GAN\nbenchmarks: LSUN-Bedroom, LSUN-Church, and CelebA-HQ. We show that the\nDeshuffleGAN obtains the best FID results for LSUN datasets compared to the\nother self-supervised GANs. Furthermore, we compare the deshuffling with the\nrotation prediction that is firstly deployed to the GAN training and\ndemonstrate that its contribution exceeds the rotation prediction. Lastly, we\nshow the contribution of the self-supervision tasks to the GAN training on loss\nlandscape and present that the effects of the self-supervision tasks may not be\ncooperative to the adversarial training in some settings. Our code can be found\nat https://github.com/gulcinbaykal/DeshuffleGAN.\n", "versions": [{"version": "v1", "created": "Tue, 3 Nov 2020 14:22:54 GMT"}], "update_date": "2020-11-04", "authors_parsed": [["Baykal", "Gulcin", ""], ["Ozcelik", "Furkan", ""], ["Unal", "Gozde", ""]]}, {"id": "2011.01741", "submitter": "Julian Krebs", "authors": "Julian Krebs, Herv\\'e Delingette, Nicholas Ayache and Tommaso Mansi", "title": "Learning a Generative Motion Model from Image Sequences based on a\n  Latent Motion Matrix", "comments": "accepted at IEEE TMI", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose to learn a probabilistic motion model from a sequence of images\nfor spatio-temporal registration. Our model encodes motion in a low-dimensional\nprobabilistic space - the motion matrix - which enables various motion analysis\ntasks such as simulation and interpolation of realistic motion patterns\nallowing for faster data acquisition and data augmentation. More precisely, the\nmotion matrix allows to transport the recovered motion from one subject to\nanother simulating for example a pathological motion in a healthy subject\nwithout the need for inter-subject registration. The method is based on a\nconditional latent variable model that is trained using amortized variational\ninference. This unsupervised generative model follows a novel multivariate\nGaussian process prior and is applied within a temporal convolutional network\nwhich leads to a diffeomorphic motion model. Temporal consistency and\ngeneralizability is further improved by applying a temporal dropout training\nscheme. Applied to cardiac cine-MRI sequences, we show improved registration\naccuracy and spatio-temporally smoother deformations compared to three\nstate-of-the-art registration algorithms. Besides, we demonstrate the model's\napplicability for motion analysis, simulation and super-resolution by an\nimproved motion reconstruction from sequences with missing frames compared to\nlinear and cubic interpolation.\n", "versions": [{"version": "v1", "created": "Tue, 3 Nov 2020 14:44:09 GMT"}, {"version": "v2", "created": "Sun, 31 Jan 2021 13:26:54 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["Krebs", "Julian", ""], ["Delingette", "Herv\u00e9", ""], ["Ayache", "Nicholas", ""], ["Mansi", "Tommaso", ""]]}, {"id": "2011.01747", "submitter": "Vitaly Nikolaev", "authors": "Vitaly Nikolaev", "title": "Convolution Neural Networks for Semantic Segmentation: Application to\n  Small Datasets of Biomedical Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This thesis studies how the segmentation results, produced by convolutional\nneural networks (CNN), is different from each other when applied to small\nbiomedical datasets. We use different architectures, parameters and\nhyper-parameters, trying to find out the better configurations for our task,\nand trying to find out underlying regularities. Two working datasets are from\nbiomedical area of research. We conducted a lot of experiments with the two\ntypes of networks and the received results have shown the preference of some\nconditions of experiments and parameters of the networks over the others. All\ntesting results are given in the tables and some selected resulting graphs and\nsegmentation predictions are shown for better illustration.\n", "versions": [{"version": "v1", "created": "Sun, 1 Nov 2020 19:09:12 GMT"}], "update_date": "2020-11-04", "authors_parsed": [["Nikolaev", "Vitaly", ""]]}, {"id": "2011.01748", "submitter": "Zhaodong Sun", "authors": "Zhaodong Sun", "title": "Solving Inverse Problems with Hybrid Deep Image Priors: the challenge of\n  preventing overfitting", "comments": "Part of the work has been published on ICASSP 2021 with the paper\n  title \"a plug-and-play deep image prior\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We mainly analyze and solve the overfitting problem of deep image prior\n(DIP). Deep image prior can solve inverse problems such as super-resolution,\ninpainting and denoising. The main advantage of DIP over other deep learning\napproaches is that it does not need access to a large dataset. However, due to\nthe large number of parameters of the neural network and noisy data, DIP\noverfits to the noise in the image as the number of iterations grows. In the\nthesis, we use hybrid deep image priors to avoid overfitting. The hybrid priors\nare to combine DIP with an explicit prior such as total variation or with an\nimplicit prior such as a denoising algorithm. We use the alternating direction\nmethod-of-multipliers (ADMM) to incorporate the new prior and try different\nforms of ADMM to avoid extra computation caused by the inner loop of ADMM\nsteps. We also study the relation between the dynamics of gradient descent, and\nthe overfitting phenomenon. The numerical results show the hybrid priors play\nan important role in preventing overfitting. Besides, we try to fit the image\nalong some directions and find this method can reduce overfitting when the\nnoise level is large. When the noise level is small, it does not considerably\nreduce the overfitting problem.\n", "versions": [{"version": "v1", "created": "Tue, 3 Nov 2020 14:50:53 GMT"}, {"version": "v2", "created": "Sun, 21 Feb 2021 17:15:25 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["Sun", "Zhaodong", ""]]}, {"id": "2011.01753", "submitter": "Anubhav Shrimal", "authors": "Anubhav Shrimal, Tanmoy Chakraborty", "title": "Attention Beam: An Image Captioning Approach", "comments": "5 pages, 6 figures, 1 table, in Proceedings of the 35th AAAI\n  Conference on Artificial Intelligence (AAAI-21) Student Abstract", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The aim of image captioning is to generate textual description of a given\nimage. Though seemingly an easy task for humans, it is challenging for machines\nas it requires the ability to comprehend the image (computer vision) and\nconsequently generate a human-like description for the image (natural language\nunderstanding). In recent times, encoder-decoder based architectures have\nachieved state-of-the-art results for image captioning. Here, we present a\nheuristic of beam search on top of the encoder-decoder based architecture that\ngives better quality captions on three benchmark datasets: Flickr8k, Flickr30k\nand MS COCO.\n", "versions": [{"version": "v1", "created": "Tue, 3 Nov 2020 14:57:42 GMT"}, {"version": "v2", "created": "Wed, 11 Nov 2020 15:17:56 GMT"}], "update_date": "2020-11-12", "authors_parsed": [["Shrimal", "Anubhav", ""], ["Chakraborty", "Tanmoy", ""]]}, {"id": "2011.01787", "submitter": "Aniket Maurya", "authors": "Aniket Maurya", "title": "Predicting intubation support requirement of patients using Chest X-ray\n  with Deep Representation Learning", "comments": "This work is in active progress", "journal-ref": null, "doi": "10.13140/RG.2.2.18271.69282", "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Recent developments in medical imaging with Deep Learning presents evidence\nof automated diagnosis and prognosis. It can also be a complement to currently\navailable diagnosis methods. Deep Learning can be leveraged for diagnosis,\nseverity prediction, intubation support prediction and many similar tasks. We\npresent prediction of intubation support requirement for patients from the\nChest X-ray using Deep representation learning. We release our source code\npublicly at https://github.com/aniketmaurya/covid-research.\n", "versions": [{"version": "v1", "created": "Wed, 28 Oct 2020 19:12:50 GMT"}], "update_date": "2020-11-04", "authors_parsed": [["Maurya", "Aniket", ""]]}, {"id": "2011.01789", "submitter": "Shai Bagon", "authors": "Daniel Yaron, Daphna Keidar, Elisha Goldstein, Yair Shachar, Ayelet\n  Blass, Oz Frank, Nir Schipper, Nogah Shabshin, Ahuva Grubstein, Dror Suhami,\n  Naama R. Bogot, Eyal Sela, Amiel A. Dror, Mordehay Vaturi, Federico Mento,\n  Elena Torri, Riccardo Inchingolo, Andrea Smargiassi, Gino Soldati, Tiziano\n  Perrone, Libertario Demi, Meirav Galun, Shai Bagon, Yishai M. Elyada and\n  Yonina C. Eldar", "title": "Point of Care Image Analysis for COVID-19", "comments": "Not approved for arXiv", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Early detection of COVID-19 is key in containing the pandemic. Disease\ndetection and evaluation based on imaging is fast and cheap and therefore plays\nan important role in COVID-19 handling. COVID-19 is easier to detect in chest\nCT, however, it is expensive, non-portable, and difficult to disinfect, making\nit unfit as a point-of-care (POC) modality. On the other hand, chest X-ray\n(CXR) and lung ultrasound (LUS) are widely used, yet, COVID-19 findings in\nthese modalities are not always very clear. Here we train deep neural networks\nto significantly enhance the capability to detect, grade and monitor COVID-19\npatients using CXRs and LUS. Collaborating with several hospitals in Israel we\ncollect a large dataset of CXRs and use this dataset to train a neural network\nobtaining above 90% detection rate for COVID-19. In addition, in collaboration\nwith ULTRa (Ultrasound Laboratory Trento, Italy) and hospitals in Italy we\nobtained POC ultrasound data with annotations of the severity of disease and\ntrained a deep network for automatic severity grading.\n", "versions": [{"version": "v1", "created": "Wed, 28 Oct 2020 06:43:52 GMT"}, {"version": "v2", "created": "Tue, 10 Nov 2020 06:12:19 GMT"}], "update_date": "2020-11-11", "authors_parsed": [["Yaron", "Daniel", ""], ["Keidar", "Daphna", ""], ["Goldstein", "Elisha", ""], ["Shachar", "Yair", ""], ["Blass", "Ayelet", ""], ["Frank", "Oz", ""], ["Schipper", "Nir", ""], ["Shabshin", "Nogah", ""], ["Grubstein", "Ahuva", ""], ["Suhami", "Dror", ""], ["Bogot", "Naama R.", ""], ["Sela", "Eyal", ""], ["Dror", "Amiel A.", ""], ["Vaturi", "Mordehay", ""], ["Mento", "Federico", ""], ["Torri", "Elena", ""], ["Inchingolo", "Riccardo", ""], ["Smargiassi", "Andrea", ""], ["Soldati", "Gino", ""], ["Perrone", "Tiziano", ""], ["Demi", "Libertario", ""], ["Galun", "Meirav", ""], ["Bagon", "Shai", ""], ["Elyada", "Yishai M.", ""], ["Eldar", "Yonina C.", ""]]}, {"id": "2011.01819", "submitter": "Pedro Morgado", "authors": "Pedro Morgado, Yi Li and Nuno Vasconcelos", "title": "Learning Representations from Audio-Visual Spatial Alignment", "comments": "To appear at Advances in Neural Information Processing Systems\n  (NeurIPS), 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a novel self-supervised pretext task for learning\nrepresentations from audio-visual content. Prior work on audio-visual\nrepresentation learning leverages correspondences at the video level.\nApproaches based on audio-visual correspondence (AVC) predict whether audio and\nvideo clips originate from the same or different video instances. Audio-visual\ntemporal synchronization (AVTS) further discriminates negative pairs originated\nfrom the same video instance but at different moments in time. While these\napproaches learn high-quality representations for downstream tasks such as\naction recognition, their training objectives disregard spatial cues naturally\noccurring in audio and visual signals. To learn from these spatial cues, we\ntasked a network to perform contrastive audio-visual spatial alignment of\n360{\\deg} video and spatial audio. The ability to perform spatial alignment is\nenhanced by reasoning over the full spatial content of the 360{\\deg} video\nusing a transformer architecture to combine representations from multiple\nviewpoints. The advantages of the proposed pretext task are demonstrated on a\nvariety of audio and visual downstream tasks, including audio-visual\ncorrespondence, spatial alignment, action recognition, and video semantic\nsegmentation.\n", "versions": [{"version": "v1", "created": "Tue, 3 Nov 2020 16:20:04 GMT"}], "update_date": "2020-11-04", "authors_parsed": [["Morgado", "Pedro", ""], ["Li", "Yi", ""], ["Vasconcelos", "Nuno", ""]]}, {"id": "2011.01844", "submitter": "Eden Belouadah", "authors": "Eden Belouadah, Adrian Popescu and Ioannis Kanellos", "title": "A Comprehensive Study of Class Incremental Learning Algorithms for\n  Visual Tasks", "comments": "Accepted for publication in the Elsevier's Neural Networks journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ability of artificial agents to increment their capabilities when\nconfronted with new data is an open challenge in artificial intelligence. The\nmain challenge faced in such cases is catastrophic forgetting, i.e., the\ntendency of neural networks to underfit past data when new ones are ingested. A\nfirst group of approaches tackles forgetting by increasing deep model capacity\nto accommodate new knowledge. A second type of approaches fix the deep model\nsize and introduce a mechanism whose objective is to ensure a good compromise\nbetween stability and plasticity of the model. While the first type of\nalgorithms were compared thoroughly, this is not the case for methods which\nexploit a fixed size model. Here, we focus on the latter, place them in a\ncommon conceptual and experimental framework and propose the following\ncontributions: (1) define six desirable properties of incremental learning\nalgorithms and analyze them according to these properties, (2) introduce a\nunified formalization of the class-incremental learning problem, (3) propose a\ncommon evaluation framework which is more thorough than existing ones in terms\nof number of datasets, size of datasets, size of bounded memory and number of\nincremental states, (4) investigate the usefulness of herding for past\nexemplars selection, (5) provide experimental evidence that it is possible to\nobtain competitive performance without the use of knowledge distillation to\ntackle catastrophic forgetting and (6) facilitate reproducibility by\nintegrating all tested methods in a common open-source repository. The main\nexperimental finding is that none of the existing algorithms achieves the best\nresults in all evaluated settings. Important differences arise notably if a\nbounded memory of past classes is allowed or not.\n", "versions": [{"version": "v1", "created": "Tue, 3 Nov 2020 16:59:21 GMT"}, {"version": "v2", "created": "Fri, 4 Dec 2020 19:16:46 GMT"}, {"version": "v3", "created": "Wed, 9 Dec 2020 13:02:38 GMT"}, {"version": "v4", "created": "Tue, 15 Dec 2020 16:40:55 GMT"}], "update_date": "2020-12-16", "authors_parsed": [["Belouadah", "Eden", ""], ["Popescu", "Adrian", ""], ["Kanellos", "Ioannis", ""]]}, {"id": "2011.01864", "submitter": "Enrique Sanchez", "authors": "Enrique Sanchez, Adrian Bulat, Anestis Zaganidis, Georgios\n  Tzimiropoulos", "title": "Semi-supervised Facial Action Unit Intensity Estimation with Contrastive\n  Learning", "comments": "ACCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper tackles the challenging problem of estimating the intensity of\nFacial Action Units with few labeled images. Contrary to previous works, our\nmethod does not require to manually select key frames, and produces\nstate-of-the-art results with as little as $2\\%$ of annotated frames, which are\n\\textit{randomly chosen}. To this end, we propose a semi-supervised learning\napproach where a spatio-temporal model combining a feature extractor and a\ntemporal module are learned in two stages. The first stage uses datasets of\nunlabeled videos to learn a strong spatio-temporal representation of facial\nbehavior dynamics based on contrastive learning. To our knowledge we are the\nfirst to build upon this framework for modeling facial behavior in an\nunsupervised manner. The second stage uses another dataset of randomly chosen\nlabeled frames to train a regressor on top of our spatio-temporal model for\nestimating the AU intensity. We show that although backpropagation through time\nis applied only with respect to the output of the network for extremely sparse\nand randomly chosen labeled frames, our model can be effectively trained to\nestimate AU intensity accurately, thanks to the unsupervised pre-training of\nthe first stage. We experimentally validate that our method outperforms\nexisting methods when working with as little as $2\\%$ of randomly chosen data\nfor both DISFA and BP4D datasets, without a careful choice of labeled frames, a\ntime-consuming task still required in previous approaches.\n", "versions": [{"version": "v1", "created": "Tue, 3 Nov 2020 17:35:57 GMT"}, {"version": "v2", "created": "Wed, 4 Nov 2020 09:40:36 GMT"}], "update_date": "2020-11-05", "authors_parsed": [["Sanchez", "Enrique", ""], ["Bulat", "Adrian", ""], ["Zaganidis", "Anestis", ""], ["Tzimiropoulos", "Georgios", ""]]}, {"id": "2011.01869", "submitter": "Bo Li", "authors": "Bo Li, Wiro J. Niessen, Stefan Klein, M. Arfan Ikram, Meike W.\n  Vernooij, Esther E. Bron", "title": "Learning unbiased group-wise registration (LUGR) and joint segmentation:\n  evaluation on longitudinal diffusion MRI", "comments": "SPIE Medical Imaging 2021 (oral)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI eess.IV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Analysis of longitudinal changes in imaging studies often involves both\nsegmentation of structures of interest and registration of multiple timeframes.\nThe accuracy of such analysis could benefit from a tailored framework that\njointly optimizes both tasks to fully exploit the information available in the\nlongitudinal data. Most learning-based registration algorithms, including joint\noptimization approaches, currently suffer from bias due to selection of a fixed\nreference frame and only support pairwise transformations. We here propose an\nanalytical framework based on an unbiased learning strategy for group-wise\nregistration that simultaneously registers images to the mean space of a group\nto obtain consistent segmentations. We evaluate the proposed method on\nlongitudinal analysis of a white matter tract in a brain MRI dataset with 2-3\ntime-points for 3249 individuals, i.e., 8045 images in total. The\nreproducibility of the method is evaluated on test-retest data from 97\nindividuals. The results confirm that the implicit reference image is an\naverage of the input image. In addition, the proposed framework leads to\nconsistent segmentations and significantly lower processing bias than that of a\npair-wise fixed-reference approach. This processing bias is even smaller than\nthose obtained when translating segmentations by only one voxel, which can be\nattributed to subtle numerical instabilities and interpolation. Therefore, we\npostulate that the proposed mean-space learning strategy could be widely\napplied to learning-based registration tasks. In addition, this group-wise\nframework introduces a novel way for learning-based longitudinal studies by\ndirect construction of an unbiased within-subject template and allowing\nreliable and efficient analysis of spatio-temporal imaging biomarkers.\n", "versions": [{"version": "v1", "created": "Tue, 3 Nov 2020 17:44:15 GMT"}, {"version": "v2", "created": "Wed, 24 Feb 2021 09:23:00 GMT"}], "update_date": "2021-02-25", "authors_parsed": [["Li", "Bo", ""], ["Niessen", "Wiro J.", ""], ["Klein", "Stefan", ""], ["Ikram", "M. Arfan", ""], ["Vernooij", "Meike W.", ""], ["Bron", "Esther E.", ""]]}, {"id": "2011.01888", "submitter": "Benjamin Riggan", "authors": "Kshitij Nikhal and Benjamin S. Riggan", "title": "Unsupervised Attention Based Instance Discriminative Learning for Person\n  Re-Identification", "comments": "WACV 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Recent advances in person re-identification have demonstrated enhanced\ndiscriminability, especially with supervised learning or transfer learning.\nHowever, since the data requirements---including the degree of data\ncurations---are becoming increasingly complex and laborious, there is a\ncritical need for unsupervised methods that are robust to large intra-class\nvariations, such as changes in perspective, illumination, articulated motion,\nresolution, etc. Therefore, we propose an unsupervised framework for person\nre-identification which is trained in an end-to-end manner without any\npre-training. Our proposed framework leverages a new attention mechanism that\ncombines group convolutions to (1) enhance spatial attention at multiple scales\nand (2) reduce the number of trainable parameters by 59.6%. Additionally, our\nframework jointly optimizes the network with agglomerative clustering and\ninstance learning to tackle hard samples. We perform extensive analysis using\nthe Market1501 and DukeMTMC-reID datasets to demonstrate that our method\nconsistently outperforms the state-of-the-art methods (with and without\npre-trained weights).\n", "versions": [{"version": "v1", "created": "Tue, 3 Nov 2020 18:08:31 GMT"}], "update_date": "2020-11-04", "authors_parsed": [["Nikhal", "Kshitij", ""], ["Riggan", "Benjamin S.", ""]]}, {"id": "2011.01890", "submitter": "Manuel Marin-Jimenez", "authors": "Rafael Berral-Soler, Francisco J. Madrid-Cuevas, Rafael\n  Mu\\~noz-Salinas, Manuel J. Mar\\'in-Jim\\'enez", "title": "RealHePoNet: a robust single-stage ConvNet for head pose estimation in\n  the wild", "comments": "Accepted for publication at Neural Computing and Applications", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human head pose estimation in images has applications in many fields such as\nhuman-computer interaction or video surveillance tasks. In this work, we\naddress this problem, defined here as the estimation of both vertical\n(tilt/pitch) and horizontal (pan/yaw) angles, through the use of a single\nConvolutional Neural Network (ConvNet) model, trying to balance precision and\ninference speed in order to maximize its usability in real-world applications.\nOur model is trained over the combination of two datasets: 'Pointing'04'\n(aiming at covering a wide range of poses) and 'Annotated Facial Landmarks in\nthe Wild' (in order to improve robustness of our model for its use on\nreal-world images). Three different partitions of the combined dataset are\ndefined and used for training, validation and testing purposes. As a result of\nthis work, we have obtained a trained ConvNet model, coined RealHePoNet, that\ngiven a low-resolution grayscale input image, and without the need of using\nfacial landmarks, is able to estimate with low error both tilt and pan angles\n(~4.4{\\deg} average error on the test partition). Also, given its low inference\ntime (~6 ms per head), we consider our model usable even when paired with\nmedium-spec hardware (i.e. GTX 1060 GPU). * Code available at:\nhttps://github.com/rafabs97/headpose_final * Demo video at:\nhttps://www.youtube.com/watch?v=2UeuXh5DjAE\n", "versions": [{"version": "v1", "created": "Tue, 3 Nov 2020 18:09:05 GMT"}], "update_date": "2020-11-04", "authors_parsed": [["Berral-Soler", "Rafael", ""], ["Madrid-Cuevas", "Francisco J.", ""], ["Mu\u00f1oz-Salinas", "Rafael", ""], ["Mar\u00edn-Jim\u00e9nez", "Manuel J.", ""]]}, {"id": "2011.01901", "submitter": "Shlok Mishra", "authors": "Shlok Mishra, Anshul Shah, Ankan Bansal, Jonghyun Choi, Abhinav\n  Shrivastava, Abhishek Sharma, David Jacobs", "title": "Learning Visual Representations for Transfer Learning by Suppressing\n  Texture", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent literature has shown that features obtained from supervised training\nof CNNs may over-emphasize texture rather than encoding high-level information.\nIn self-supervised learning in particular, texture as a low-level cue may\nprovide shortcuts that prevent the network from learning higher level\nrepresentations. To address these problems we propose to use classic methods\nbased on anisotropic diffusion to augment training using images with suppressed\ntexture. This simple method helps retain important edge information and\nsuppress texture at the same time. We empirically show that our method achieves\nstate-of-the-art results on object detection and image classification with\neight diverse datasets in either supervised or self-supervised learning tasks\nsuch as MoCoV2 and Jigsaw. Our method is particularly effective for transfer\nlearning tasks and we observed improved performance on five standard transfer\nlearning datasets. The large improvements (up to 11.49\\%) on the\nSketch-ImageNet dataset, DTD dataset and additional visual analyses with\nsaliency maps suggest that our approach helps in learning better\nrepresentations that better transfer.\n", "versions": [{"version": "v1", "created": "Tue, 3 Nov 2020 18:27:03 GMT"}, {"version": "v2", "created": "Wed, 4 Nov 2020 16:41:17 GMT"}], "update_date": "2020-11-05", "authors_parsed": [["Mishra", "Shlok", ""], ["Shah", "Anshul", ""], ["Bansal", "Ankan", ""], ["Choi", "Jonghyun", ""], ["Shrivastava", "Abhinav", ""], ["Sharma", "Abhishek", ""], ["Jacobs", "David", ""]]}, {"id": "2011.01902", "submitter": "Ezgi Ozyilkan", "authors": "Ezgi Ozyilkan, Mikolaj Jankowski", "title": "Deep Joint Transmission-Recognition for Multi-View Cameras", "comments": "9 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose joint transmission-recognition schemes for efficient inference at\nthe wireless edge. Motivated by the surveillance applications with wireless\ncameras, we consider the person classification task over a wireless channel\ncarried out by multi-view cameras operating as edge devices. We introduce deep\nneural network (DNN) based compression schemes which incorporate digital\n(separate) transmission and joint source-channel coding (JSCC) methods. We\nevaluate the proposed device-edge communication schemes under different channel\nSNRs, bandwidth and power constraints. We show that the JSCC schemes not only\nimprove the end-to-end accuracy but also simplify the encoding process and\nprovide graceful degradation with channel quality.\n", "versions": [{"version": "v1", "created": "Tue, 3 Nov 2020 18:27:49 GMT"}], "update_date": "2020-11-04", "authors_parsed": [["Ozyilkan", "Ezgi", ""], ["Jankowski", "Mikolaj", ""]]}, {"id": "2011.01908", "submitter": "Marcos Monteiro", "authors": "Marcos Monteiro, Alceu S. Britto Jr, Jean P. Barddal, Luiz S.\n  Oliveira, Robert Sabourin", "title": "Classifier Pool Generation based on a Two-level Diversity Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes a classifier pool generation method guided by the\ndiversity estimated on the data complexity and classifier decisions. First, the\nbehavior of complexity measures is assessed by considering several subsamples\nof the dataset. The complexity measures with high variability across the\nsubsamples are selected for posterior pool adaptation, where an evolutionary\nalgorithm optimizes diversity in both complexity and decision spaces. A robust\nexperimental protocol with 28 datasets and 20 replications is used to evaluate\nthe proposed method. Results show significant accuracy improvements in 69.4% of\nthe experiments when Dynamic Classifier Selection and Dynamic Ensemble\nSelection methods are applied.\n", "versions": [{"version": "v1", "created": "Tue, 3 Nov 2020 18:41:53 GMT"}], "update_date": "2020-11-04", "authors_parsed": [["Monteiro", "Marcos", ""], ["Britto", "Alceu S.", "Jr"], ["Barddal", "Jean P.", ""], ["Oliveira", "Luiz S.", ""], ["Sabourin", "Robert", ""]]}, {"id": "2011.01926", "submitter": "Ke Li", "authors": "Shichong Peng and Ke Li", "title": "Generating Unobserved Alternatives", "comments": "Videos in the article are also available as ancillary files in the\n  previous version (arXiv:2011.01926v3). Website:\n  https://niopeng.github.io/HyperRIM/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider problems where multiple predictions can be considered correct,\nbut only one of them is given as supervision. This setting differs from both\nthe regression and class-conditional generative modelling settings: in the\nformer, there is a unique observed output for each input, which is provided as\nsupervision; in the latter, there are many observed outputs for each input, and\nmany are provided as supervision. Applying either regression methods and\nconditional generative models to the present setting often results in a model\nthat can only make a single prediction for each input. We explore several\nproblems that have this property and develop an approach that can generate\nmultiple high-quality predictions given the same input. As a result, it can be\nused to generate high-quality outputs that are different from the observed\noutput.\n", "versions": [{"version": "v1", "created": "Tue, 3 Nov 2020 18:57:57 GMT"}, {"version": "v2", "created": "Thu, 5 Nov 2020 18:03:05 GMT"}, {"version": "v3", "created": "Fri, 27 Nov 2020 08:20:49 GMT"}, {"version": "v4", "created": "Tue, 1 Dec 2020 10:05:52 GMT"}], "update_date": "2020-12-02", "authors_parsed": [["Peng", "Shichong", ""], ["Li", "Ke", ""]]}, {"id": "2011.01968", "submitter": "Zhenjia Xu", "authors": "Zhenjia Xu, Zhanpeng He, Jiajun Wu, Shuran Song", "title": "Learning 3D Dynamic Scene Representations for Robot Manipulation", "comments": "CoRL 2020. The first two authors contributed equally to this paper.\n  Project page: https://dsr-net.cs.columbia.edu/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  3D scene representation for robot manipulation should capture three key\nobject properties: permanency -- objects that become occluded over time\ncontinue to exist; amodal completeness -- objects have 3D occupancy, even if\nonly partial observations are available; spatiotemporal continuity -- the\nmovement of each object is continuous over space and time. In this paper, we\nintroduce 3D Dynamic Scene Representation (DSR), a 3D volumetric scene\nrepresentation that simultaneously discovers, tracks, reconstructs objects, and\npredicts their dynamics while capturing all three properties. We further\npropose DSR-Net, which learns to aggregate visual observations over multiple\ninteractions to gradually build and refine DSR. Our model achieves\nstate-of-the-art performance in modeling 3D scene dynamics with DSR on both\nsimulated and real data. Combined with model predictive control, DSR-Net\nenables accurate planning in downstream robotic manipulation tasks such as\nplanar pushing. Video is available at https://youtu.be/GQjYG3nQJ80.\n", "versions": [{"version": "v1", "created": "Tue, 3 Nov 2020 19:23:06 GMT"}, {"version": "v2", "created": "Thu, 10 Dec 2020 16:53:29 GMT"}], "update_date": "2020-12-11", "authors_parsed": [["Xu", "Zhenjia", ""], ["He", "Zhanpeng", ""], ["Wu", "Jiajun", ""], ["Song", "Shuran", ""]]}, {"id": "2011.01974", "submitter": "Yara Ali Alnaggar", "authors": "Yara Ali Alnaggar, Mohamed Afifi, Karim Amer, Mohamed Elhelw", "title": "Multi Projection Fusion for Real-time Semantic Segmentation of 3D LiDAR\n  Point Clouds", "comments": "Accepted at the 2021 Winter Conference on Applications of Computer\n  Vision (WACV 2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semantic segmentation of 3D point cloud data is essential for enhanced\nhigh-level perception in autonomous platforms. Furthermore, given the\nincreasing deployment of LiDAR sensors onboard of cars and drones, a special\nemphasis is also placed on non-computationally intensive algorithms that\noperate on mobile GPUs. Previous efficient state-of-the-art methods relied on\n2D spherical projection of point clouds as input for 2D fully convolutional\nneural networks to balance the accuracy-speed trade-off. This paper introduces\na novel approach for 3D point cloud semantic segmentation that exploits\nmultiple projections of the point cloud to mitigate the loss of information\ninherent in single projection methods. Our Multi-Projection Fusion (MPF)\nframework analyzes spherical and bird's-eye view projections using two separate\nhighly-efficient 2D fully convolutional models then combines the segmentation\nresults of both views. The proposed framework is validated on the SemanticKITTI\ndataset where it achieved a mIoU of 55.5 which is higher than state-of-the-art\nprojection-based methods RangeNet++ and PolarNet while being 1.6x faster than\nthe former and 3.1x faster than the latter.\n", "versions": [{"version": "v1", "created": "Tue, 3 Nov 2020 19:40:43 GMT"}, {"version": "v2", "created": "Fri, 6 Nov 2020 17:00:05 GMT"}], "update_date": "2020-11-09", "authors_parsed": [["Alnaggar", "Yara Ali", ""], ["Afifi", "Mohamed", ""], ["Amer", "Karim", ""], ["Elhelw", "Mohamed", ""]]}, {"id": "2011.01975", "submitter": "Vladlen Koltun", "authors": "Dhruv Batra, Angel X. Chang, Sonia Chernova, Andrew J. Davison, Jia\n  Deng, Vladlen Koltun, Sergey Levine, Jitendra Malik, Igor Mordatch, Roozbeh\n  Mottaghi, Manolis Savva, Hao Su", "title": "Rearrangement: A Challenge for Embodied AI", "comments": "Authors are listed in alphabetical order", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CV cs.LG cs.RO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We describe a framework for research and evaluation in Embodied AI. Our\nproposal is based on a canonical task: Rearrangement. A standard task can focus\nthe development of new techniques and serve as a source of trained models that\ncan be transferred to other settings. In the rearrangement task, the goal is to\nbring a given physical environment into a specified state. The goal state can\nbe specified by object poses, by images, by a description in language, or by\nletting the agent experience the environment in the goal state. We characterize\nrearrangement scenarios along different axes and describe metrics for\nbenchmarking rearrangement performance. To facilitate research and exploration,\nwe present experimental testbeds of rearrangement scenarios in four different\nsimulation environments. We anticipate that other datasets will be released and\nnew simulation platforms will be built to support training of rearrangement\nagents and their deployment on physical systems.\n", "versions": [{"version": "v1", "created": "Tue, 3 Nov 2020 19:42:32 GMT"}], "update_date": "2020-11-05", "authors_parsed": [["Batra", "Dhruv", ""], ["Chang", "Angel X.", ""], ["Chernova", "Sonia", ""], ["Davison", "Andrew J.", ""], ["Deng", "Jia", ""], ["Koltun", "Vladlen", ""], ["Levine", "Sergey", ""], ["Malik", "Jitendra", ""], ["Mordatch", "Igor", ""], ["Mottaghi", "Roozbeh", ""], ["Savva", "Manolis", ""], ["Su", "Hao", ""]]}, {"id": "2011.02018", "submitter": "Maya Aghaei", "authors": "Maya Aghaei, Matteo Bustreo, Yiming Wang, Gianluca Bailo, Pietro\n  Morerio, Alessio Del Bue", "title": "Single Image Human Proxemics Estimation for Visual Social Distancing", "comments": "Paper accepted at WACV 2021 conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we address the problem of estimating the so-called \"Social\nDistancing\" given a single uncalibrated image in unconstrained scenarios. Our\napproach proposes a semi-automatic solution to approximate the homography\nmatrix between the scene ground and image plane. With the estimated homography,\nwe then leverage an off-the-shelf pose detector to detect body poses on the\nimage and to reason upon their inter-personal distances using the length of\ntheir body-parts. Inter-personal distances are further locally inspected to\ndetect possible violations of the social distancing rules. We validate our\nproposed method quantitatively and qualitatively against baselines on public\ndomain datasets for which we provided groundtruth on inter-personal distances.\nBesides, we demonstrate the application of our method deployed in a real\ntesting scenario where statistics on the inter-personal distances are currently\nused to improve the safety in a critical environment.\n", "versions": [{"version": "v1", "created": "Tue, 3 Nov 2020 21:49:13 GMT"}, {"version": "v2", "created": "Thu, 5 Nov 2020 14:13:24 GMT"}], "update_date": "2020-11-06", "authors_parsed": [["Aghaei", "Maya", ""], ["Bustreo", "Matteo", ""], ["Wang", "Yiming", ""], ["Bailo", "Gianluca", ""], ["Morerio", "Pietro", ""], ["Del Bue", "Alessio", ""]]}, {"id": "2011.02045", "submitter": "Kiran Raja Dr", "authors": "Sushma Venkatesh, Raghavendra Ramachandra, Kiran Raja, Christoph Busch", "title": "Face Morphing Attack Generation & Detection: A Comprehensive Survey", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CR cs.CY", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The vulnerability of Face Recognition System (FRS) to various kind of attacks\n(both direct and in-direct attacks) and face morphing attacks has received a\ngreat interest from the biometric community. The goal of a morphing attack is\nto subvert the FRS at Automatic Border Control (ABC) gates by presenting the\nElectronic Machine Readable Travel Document (eMRTD) or e-passport that is\nobtained based on the morphed face image. Since the application process for the\ne-passport in the majority countries requires a passport photo to be presented\nby the applicant, a malicious actor and the accomplice can generate the morphed\nface image and to obtain the e-passport. An e-passport with a morphed face\nimages can be used by both the malicious actor and the accomplice to cross the\nborder as the morphed face image can be verified against both of them. This can\nresult in a significant threat as a malicious actor can cross the border\nwithout revealing the track of his/her criminal background while the details of\naccomplice are recorded in the log of the access control system. This survey\naims to present a systematic overview of the progress made in the area of face\nmorphing in terms of both morph generation and morph detection. In this paper,\nwe describe and illustrate various aspects of face morphing attacks, including\ndifferent techniques for generating morphed face images but also the\nstate-of-the-art regarding Morph Attack Detection (MAD) algorithms based on a\nstringent taxonomy and finally the availability of public databases, which\nallow to benchmark new MAD algorithms in a reproducible manner. The outcomes of\ncompetitions/benchmarking, vulnerability assessments and performance evaluation\nmetrics are also provided in a comprehensive manner. Furthermore, we discuss\nthe open challenges and potential future works that need to be addressed in\nthis evolving field of biometrics.\n", "versions": [{"version": "v1", "created": "Tue, 3 Nov 2020 22:36:27 GMT"}], "update_date": "2020-11-05", "authors_parsed": [["Venkatesh", "Sushma", ""], ["Ramachandra", "Raghavendra", ""], ["Raja", "Kiran", ""], ["Busch", "Christoph", ""]]}, {"id": "2011.02055", "submitter": "Shanxin Yuan", "authors": "Lin Liu, Shanxin Yuan, Jianzhuang Liu, Liping Bao, Gregory Slabaugh,\n  Qi Tian", "title": "Self-Adaptively Learning to Demoire from Focused and Defocused Image\n  Pairs", "comments": "Accepted to NeurIPS 2020. Project page:\n  \"http://home.ustc.edu.cn/~ll0825/project_FDNet.html\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Moire artifacts are common in digital photography, resulting from the\ninterference between high-frequency scene content and the color filter array of\nthe camera. Existing deep learning-based demoireing methods trained on large\nscale datasets are limited in handling various complex moire patterns, and\nmainly focus on demoireing of photos taken of digital displays. Moreover,\nobtaining moire-free ground-truth in natural scenes is difficult but needed for\ntraining. In this paper, we propose a self-adaptive learning method for\ndemoireing a high-frequency image, with the help of an additional defocused\nmoire-free blur image. Given an image degraded with moire artifacts and a\nmoire-free blur image, our network predicts a moire-free clean image and a blur\nkernel with a self-adaptive strategy that does not require an explicit training\nstage, instead performing test-time adaptation. Our model has two sub-networks\nand works iteratively. During each iteration, one sub-network takes the moire\nimage as input, removing moire patterns and restoring image details, and the\nother sub-network estimates the blur kernel from the blur image. The two\nsub-networks are jointly optimized. Extensive experiments demonstrate that our\nmethod outperforms state-of-the-art methods and can produce high-quality\ndemoired results. It can generalize well to the task of removing moire\nartifacts caused by display screens. In addition, we build a new moire dataset,\nincluding images with screen and texture moire artifacts. As far as we know,\nthis is the first dataset with real texture moire patterns.\n", "versions": [{"version": "v1", "created": "Tue, 3 Nov 2020 23:09:02 GMT"}, {"version": "v2", "created": "Thu, 5 Nov 2020 10:19:40 GMT"}], "update_date": "2020-11-06", "authors_parsed": [["Liu", "Lin", ""], ["Yuan", "Shanxin", ""], ["Liu", "Jianzhuang", ""], ["Bao", "Liping", ""], ["Slabaugh", "Gregory", ""], ["Tian", "Qi", ""]]}, {"id": "2011.02062", "submitter": "Zitong Yu", "authors": "Zitong Yu, Jun Wan, Yunxiao Qin, Xiaobai Li, Stan Z. Li, Guoying Zhao", "title": "NAS-FAS: Static-Dynamic Central Difference Network Search for Face\n  Anti-Spoofing", "comments": "Accepted by IEEE Transactions on Pattern Analysis and Machine\n  Intelligence (TPAMI); the CASIA-SURF 3DMask dataset is available at\n  http://www.cbsr.ia.ac.cn/users/jwan/database/3DMask.pdf", "journal-ref": null, "doi": "10.1109/TPAMI.2020.3036338", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Face anti-spoofing (FAS) plays a vital role in securing face recognition\nsystems. Existing methods heavily rely on the expert-designed networks, which\nmay lead to a sub-optimal solution for FAS task. Here we propose the first FAS\nmethod based on neural architecture search (NAS), called NAS-FAS, to discover\nthe well-suited task-aware networks. Unlike previous NAS works mainly focus on\ndeveloping efficient search strategies in generic object classification, we pay\nmore attention to study the search spaces for FAS task. The challenges of\nutilizing NAS for FAS are in two folds: the networks searched on 1) a specific\nacquisition condition might perform poorly in unseen conditions, and 2)\nparticular spoofing attacks might generalize badly for unseen attacks. To\novercome these two issues, we develop a novel search space consisting of\ncentral difference convolution and pooling operators. Moreover, an efficient\nstatic-dynamic representation is exploited for fully mining the FAS-aware\nspatio-temporal discrepancy. Besides, we propose Domain/Type-aware Meta-NAS,\nwhich leverages cross-domain/type knowledge for robust searching. Finally, in\norder to evaluate the NAS transferability for cross datasets and unknown attack\ntypes, we release a large-scale 3D mask dataset, namely CASIA-SURF 3DMask, for\nsupporting the new 'cross-dataset cross-type' testing protocol. Experiments\ndemonstrate that the proposed NAS-FAS achieves state-of-the-art performance on\nnine FAS benchmark datasets with four testing protocols.\n", "versions": [{"version": "v1", "created": "Tue, 3 Nov 2020 23:34:40 GMT"}], "update_date": "2020-11-05", "authors_parsed": [["Yu", "Zitong", ""], ["Wan", "Jun", ""], ["Qin", "Yunxiao", ""], ["Li", "Xiaobai", ""], ["Li", "Stan Z.", ""], ["Zhao", "Guoying", ""]]}, {"id": "2011.02067", "submitter": "Han Liu", "authors": "Han Liu, Can Cui, Dario J. Englot, Benoit M. Dawant", "title": "Uncertainty Estimation in Medical Image Localization: Towards Robust\n  Anterior Thalamus Targeting for Deep Brain Stimulation", "comments": "8 pages, 3 figures", "journal-ref": null, "doi": "10.1007/978-3-030-61166-8_14", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Atlas-based methods are the standard approaches for automatic targeting of\nthe Anterior Nucleus of the Thalamus (ANT) for Deep Brain Stimulation (DBS),\nbut these are known to lack robustness when anatomic differences between\natlases and subjects are large. To improve the localization robustness, we\npropose a novel two-stage deep learning (DL) framework, where the first stage\nidentifies and crops the thalamus regions from the whole brain MRI and the\nsecond stage performs per-voxel regression on the cropped volume to localize\nthe targets at the finest resolution scale. To address the issue of data\nscarcity, we train the models with the pseudo labels which are created based on\nthe available labeled data using multi-atlas registration. To assess the\nperformance of the proposed framework, we validate two sampling-based\nuncertainty estimation techniques namely Monte Carlo Dropout (MCDO) and\nTest-Time Augmentation (TTA) on the second-stage localization network.\nMoreover, we propose a novel uncertainty estimation metric called maximum\nactivation dispersion (MAD) to estimate the image-wise uncertainty for\nlocalization tasks. Our results show that the proposed method achieved more\nrobust localization performance than the traditional multi-atlas method and TTA\ncould further improve the robustness. Moreover, the epistemic and hybrid\nuncertainty estimated by MAD could be used to detect the unreliable\nlocalizations and the magnitude of the uncertainty estimated by MAD could\nreflect the degree of unreliability for the rejected predictions.\n", "versions": [{"version": "v1", "created": "Tue, 3 Nov 2020 23:43:52 GMT"}], "update_date": "2020-11-05", "authors_parsed": [["Liu", "Han", ""], ["Cui", "Can", ""], ["Englot", "Dario J.", ""], ["Dawant", "Benoit M.", ""]]}, {"id": "2011.02119", "submitter": "Xun Yuan", "authors": "Xun Yuan, Ke Hu, and Song Chen", "title": "Realtime CNN-based Keypoint Detector with Sobel Filter and CNN-based\n  Descriptor Trained with Keypoint Candidates", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The local feature detector and descriptor are essential in many computer\nvision tasks, such as SLAM and 3D reconstruction. In this paper, we introduce\ntwo separate CNNs, lightweight SobelNet and DesNet, to detect key points and to\ncompute dense local descriptors. The detector and the descriptor work in\nparallel. Sobel filter provides the edge structure of the input images as the\ninput of CNN. The locations of key points will be obtained after exerting the\nnon-maximum suppression (NMS) process on the output map of the CNN. We design\nGaussian loss for the training process of SobelNet to detect corner points as\nkeypoints. At the same time, the input of DesNet is the original grayscale\nimage, and circle loss is used to train DesNet. Besides, output maps of\nSobelNet are needed while training DesNet. We have evaluated our method on\nseveral benchmarks including HPatches benchmark, ETH benchmark, and FM-Bench.\nSobelNet achieves better or comparable performance with less computation\ncompared with SOTA methods in recent years. The inference time of an image of\n640x480 is 7.59ms and 1.09ms for SobelNet and DesNet respectively on RTX 2070\nSUPER.\n", "versions": [{"version": "v1", "created": "Wed, 4 Nov 2020 04:11:37 GMT"}], "update_date": "2020-11-05", "authors_parsed": [["Yuan", "Xun", ""], ["Hu", "Ke", ""], ["Chen", "Song", ""]]}, {"id": "2011.02120", "submitter": "Li Tian", "authors": "Li Tian, Liyan Ma, Zhijie Wen, Shaorong Xie, Yupeng Xu", "title": "Learning Discriminative Representations for Fine-Grained Diabetic\n  Retinopathy Grading", "comments": "5 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Diabetic retinopathy (DR) is one of the leading causes of blindness. However,\nno specific symptoms of early DR lead to a delayed diagnosis, which results in\ndisease progression in patients. To determine the disease severity levels,\nophthalmologists need to focus on the discriminative parts of the fundus\nimages. In recent years, deep learning has achieved great success in medical\nimage analysis. However, most works directly employ algorithms based on\nconvolutional neural networks (CNNs), which ignore the fact that the difference\namong classes is subtle and gradual. Hence, we consider automatic image grading\nof DR as a fine-grained classification task, and construct a bilinear model to\nidentify the pathologically discriminative areas. In order to leverage the\nordinal information among classes, we use an ordinal regression method to\nobtain the soft labels. In addition, other than only using a categorical loss\nto train our network, we also introduce the metric loss to learn a more\ndiscriminative feature space. Experimental results demonstrate the superior\nperformance of the proposed method on two public IDRiD and DeepDR datasets.\n", "versions": [{"version": "v1", "created": "Wed, 4 Nov 2020 04:16:55 GMT"}], "update_date": "2020-11-05", "authors_parsed": [["Tian", "Li", ""], ["Ma", "Liyan", ""], ["Wen", "Zhijie", ""], ["Xie", "Shaorong", ""], ["Xu", "Yupeng", ""]]}, {"id": "2011.02146", "submitter": "He Zhang", "authors": "He Zhang, Jianming Zhang, Federico Perazzi, Zhe Lin, Vishal M. Patel", "title": "Deep Image Compositing", "comments": "WACV-2021. A better portrait segmentation technology has been shipped\n  in Photoshop 2020. Check this out if you are not sure how to use it.\n  https://www.youtube.com/watch?v=v_kitSYKr3s&t=138s", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image compositing is a task of combining regions from different images to\ncompose a new image. A common use case is background replacement of portrait\nimages. To obtain high quality composites, professionals typically manually\nperform multiple editing steps such as segmentation, matting and foreground\ncolor decontamination, which is very time consuming even with sophisticated\nphoto editing tools. In this paper, we propose a new method which can\nautomatically generate high-quality image compositing without any user input.\nOur method can be trained end-to-end to optimize exploitation of contextual and\ncolor information of both foreground and background images, where the\ncompositing quality is considered in the optimization. Specifically, inspired\nby Laplacian pyramid blending, a dense-connected multi-stream fusion network is\nproposed to effectively fuse the information from the foreground and background\nimages at different scales. In addition, we introduce a self-taught strategy to\nprogressively train from easy to complex cases to mitigate the lack of training\ndata. Experiments show that the proposed method can automatically generate\nhigh-quality composites and outperforms existing methods both qualitatively and\nquantitatively.\n", "versions": [{"version": "v1", "created": "Wed, 4 Nov 2020 06:12:24 GMT"}], "update_date": "2020-11-05", "authors_parsed": [["Zhang", "He", ""], ["Zhang", "Jianming", ""], ["Perazzi", "Federico", ""], ["Lin", "Zhe", ""], ["Patel", "Vishal M.", ""]]}, {"id": "2011.02155", "submitter": "Yu-Jen Chen", "authors": "Shao-Cheng Wen, Yu-Jen Chen, Zihao Liu, Wujie Wen, Xiaowei Xu, Yiyu\n  Shi, Tsung-Yi Ho, Qianjun Jia, Meiping Huang, Jian Zhuang", "title": "Do Noises Bother Human and Neural Networks In the Same Way? A Medical\n  Image Analysis Perspective", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning had already demonstrated its power in medical images, including\ndenoising, classification, segmentation, etc. All these applications are\nproposed to automatically analyze medical images beforehand, which brings more\ninformation to radiologists during clinical assessment for accuracy\nimprovement. Recently, many medical denoising methods had shown their\nsignificant artifact reduction result and noise removal both quantitatively and\nqualitatively. However, those existing methods are developed around\nhuman-vision, i.e., they are designed to minimize the noise effect that can be\nperceived by human eyes. In this paper, we introduce an application-guided\ndenoising framework, which focuses on denoising for the following neural\nnetworks. In our experiments, we apply the proposed framework to different\ndatasets, models, and use cases. Experimental results show that our proposed\nframework can achieve a better result than human-vision denoising network.\n", "versions": [{"version": "v1", "created": "Wed, 4 Nov 2020 06:58:09 GMT"}], "update_date": "2020-11-05", "authors_parsed": [["Wen", "Shao-Cheng", ""], ["Chen", "Yu-Jen", ""], ["Liu", "Zihao", ""], ["Wen", "Wujie", ""], ["Xu", "Xiaowei", ""], ["Shi", "Yiyu", ""], ["Ho", "Tsung-Yi", ""], ["Jia", "Qianjun", ""], ["Huang", "Meiping", ""], ["Zhuang", "Jian", ""]]}, {"id": "2011.02156", "submitter": "Hengli Wang", "authors": "Hengli Wang, Rui Fan, Ming Liu", "title": "CoT-AMFlow: Adaptive Modulation Network with Co-Teaching Strategy for\n  Unsupervised Optical Flow Estimation", "comments": "13 pages, 3 figures and 6 tables. This paper is accepted by CoRL 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The interpretation of ego motion and scene change is a fundamental task for\nmobile robots. Optical flow information can be employed to estimate motion in\nthe surroundings. Recently, unsupervised optical flow estimation has become a\nresearch hotspot. However, unsupervised approaches are often easy to be\nunreliable on partially occluded or texture-less regions. To deal with this\nproblem, we propose CoT-AMFlow in this paper, an unsupervised optical flow\nestimation approach. In terms of the network architecture, we develop an\nadaptive modulation network that employs two novel module types, flow\nmodulation modules (FMMs) and cost volume modulation modules (CMMs), to remove\noutliers in challenging regions. As for the training paradigm, we adopt a\nco-teaching strategy, where two networks simultaneously teach each other about\nchallenging regions to further improve accuracy. Experimental results on the\nMPI Sintel, KITTI Flow and Middlebury Flow benchmarks demonstrate that our\nCoT-AMFlow outperforms all other state-of-the-art unsupervised approaches,\nwhile still running in real time. Our project page is available at\nhttps://sites.google.com/view/cot-amflow.\n", "versions": [{"version": "v1", "created": "Wed, 4 Nov 2020 07:01:25 GMT"}], "update_date": "2020-11-05", "authors_parsed": [["Wang", "Hengli", ""], ["Fan", "Rui", ""], ["Liu", "Ming", ""]]}, {"id": "2011.02164", "submitter": "Tanzila Rahman", "authors": "Tanzila Rahman, Shih-Han Chou, Leonid Sigal and Giuseppe Carenini", "title": "An Improved Attention for Visual Question Answering", "comments": "8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of Visual Question Answering (VQA). Given an image\nand a free-form, open-ended, question, expressed in natural language, the goal\nof VQA system is to provide accurate answer to this question with respect to\nthe image. The task is challenging because it requires simultaneous and\nintricate understanding of both visual and textual information. Attention,\nwhich captures intra- and inter-modal dependencies, has emerged as perhaps the\nmost widely used mechanism for addressing these challenges. In this paper, we\npropose an improved attention-based architecture to solve VQA. We incorporate\nan Attention on Attention (AoA) module within encoder-decoder framework, which\nis able to determine the relation between attention results and queries.\nAttention module generates weighted average for each query. On the other hand,\nAoA module first generates an information vector and an attention gate using\nattention results and current context; and then adds another attention to\ngenerate final attended information by multiplying the two. We also propose\nmultimodal fusion module to combine both visual and textual information. The\ngoal of this fusion module is to dynamically decide how much information should\nbe considered from each modality. Extensive experiments on VQA-v2 benchmark\ndataset show that our method achieves the state-of-the-art performance.\n", "versions": [{"version": "v1", "created": "Wed, 4 Nov 2020 07:34:54 GMT"}, {"version": "v2", "created": "Sat, 7 Nov 2020 21:30:01 GMT"}, {"version": "v3", "created": "Thu, 3 Jun 2021 19:59:08 GMT"}], "update_date": "2021-06-07", "authors_parsed": [["Rahman", "Tanzila", ""], ["Chou", "Shih-Han", ""], ["Sigal", "Leonid", ""], ["Carenini", "Giuseppe", ""]]}, {"id": "2011.02166", "submitter": "Ning Liu", "authors": "Yushuo Guan, Ning Liu, Pengyu Zhao, Zhengping Che, Kaigui Bian, Yanzhi\n  Wang, Jian Tang", "title": "DAIS: Automatic Channel Pruning via Differentiable Annealing Indicator\n  Search", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The convolutional neural network has achieved great success in fulfilling\ncomputer vision tasks despite large computation overhead against efficient\ndeployment. Structured (channel) pruning is usually applied to reduce the model\nredundancy while preserving the network structure, such that the pruned network\ncan be easily deployed in practice. However, existing structured pruning\nmethods require hand-crafted rules which may lead to tremendous pruning space.\nIn this paper, we introduce Differentiable Annealing Indicator Search (DAIS)\nthat leverages the strength of neural architecture search in the channel\npruning and automatically searches for the effective pruned model with given\nconstraints on computation overhead. Specifically, DAIS relaxes the binarized\nchannel indicators to be continuous and then jointly learns both indicators and\nmodel parameters via bi-level optimization. To bridge the non-negligible\ndiscrepancy between the continuous model and the target binarized model, DAIS\nproposes an annealing-based procedure to steer the indicator convergence\ntowards binarized states. Moreover, DAIS designs various regularizations based\non a priori structural knowledge to control the pruning sparsity and to improve\nmodel performance. Experimental results show that DAIS outperforms\nstate-of-the-art pruning methods on CIFAR-10, CIFAR-100, and ImageNet.\n", "versions": [{"version": "v1", "created": "Wed, 4 Nov 2020 07:43:01 GMT"}], "update_date": "2020-11-05", "authors_parsed": [["Guan", "Yushuo", ""], ["Liu", "Ning", ""], ["Zhao", "Pengyu", ""], ["Che", "Zhengping", ""], ["Bian", "Kaigui", ""], ["Wang", "Yanzhi", ""], ["Tang", "Jian", ""]]}, {"id": "2011.02172", "submitter": "Naoki Kato", "authors": "Naoki Kato, Hiroto Honda, Yusuke Uchida", "title": "Leveraging Temporal Joint Depths for Improving 3D Human Pose Estimation\n  in Video", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The effectiveness of the approaches to predict 3D poses from 2D poses\nestimated in each frame of a video has been demonstrated for 3D human pose\nestimation. However, 2D poses without appearance information of persons have\nmuch ambiguity with respect to the joint depths. In this paper, we propose to\nestimate a 3D pose in each frame of a video and refine it considering temporal\ninformation. The proposed approach reduces the ambiguity of the joint depths\nand improves the 3D pose estimation accuracy.\n", "versions": [{"version": "v1", "created": "Wed, 4 Nov 2020 08:23:41 GMT"}], "update_date": "2020-11-05", "authors_parsed": [["Kato", "Naoki", ""], ["Honda", "Hiroto", ""], ["Uchida", "Yusuke", ""]]}, {"id": "2011.02188", "submitter": "Pawel Plawiak", "authors": "Filip Pa{\\l}ka, Wojciech Ksi\\k{a}\\.zek, Pawe{\\l} P{\\l}awiak, Micha{\\l}\n  Romaszewski, Kamil Ksi\\k{a}\\.zek", "title": "Hyperspectral classification of blood-like substances using machine\n  learning methods combined with genetic algorithms in transductive and\n  inductive scenarios", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This study is focused on applying genetic algorithms (GA) to model and band\nselection in hyperspectral image classification. We use a forensic-inspired\ndata set of seven hyperspectral images with blood and five visually similar\nsubstances to test GA-optimised classifiers in two scenarios: when the training\nand test data come from the same image and when they come from different\nimages, which is a more challenging task due to significant spectra\ndifferences. In our experiments we compare GA with a classic model optimisation\nthrough grid search. Our results show that GA-based model optimisation can\nreduce the number of bands and create an accurate classifier that outperforms\nthe GS-based reference models, provided that during model optimisation it has\naccess to examples similar to test data. We illustrate this with experiment\nhighlighting the importance of a validation set.\n", "versions": [{"version": "v1", "created": "Wed, 4 Nov 2020 09:18:16 GMT"}], "update_date": "2020-11-05", "authors_parsed": [["Pa\u0142ka", "Filip", ""], ["Ksi\u0105\u017cek", "Wojciech", ""], ["P\u0142awiak", "Pawe\u0142", ""], ["Romaszewski", "Micha\u0142", ""], ["Ksi\u0105\u017cek", "Kamil", ""]]}, {"id": "2011.02193", "submitter": "Ujjwal Verma", "authors": "Shantam Shorewala, Armaan Ashfaque, Sidharth R and Ujjwal Verma", "title": "Weed Density and Distribution Estimation for Precision Agriculture using\n  Semi-Supervised Learning", "comments": "Included few details about the dataset. This paper has been accepted\n  in IEEE Access. Citation: S. Shorewala, A. Ashfaque, S. R and U. Verma, \"Weed\n  Density and Distribution Estimation for Precision Agriculture using\n  Semi-Supervised Learning,\" in IEEE Access, doi: 10.1109/ACCESS.2021.3057912", "journal-ref": "IEEE Access, vol 9, 2021", "doi": "10.1109/ACCESS.2021.3057912", "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Uncontrolled growth of weeds can severely affect the crop yield and quality.\nUnrestricted use of herbicide for weed removal alters biodiversity and cause\nenvironmental pollution. Instead, identifying weed-infested regions can aid\nselective chemical treatment of these regions. Advances in analyzing farm\nimages have resulted in solutions to identify weed plants. However, a majority\nof these approaches are based on supervised learning methods which requires\nhuge amount of manually annotated images. As a result, these supervised\napproaches are economically infeasible for the individual farmer because of the\nwide variety of plant species being cultivated. In this paper, we propose a\ndeep learning-based semi-supervised approach for robust estimation of weed\ndensity and distribution across farmlands using only limited color images\nacquired from autonomous robots. This weed density and distribution can be\nuseful in a site-specific weed management system for selective treatment of\ninfected areas using autonomous robots. In this work, the foreground vegetation\npixels containing crops and weeds are first identified using a Convolutional\nNeural Network (CNN) based unsupervised segmentation. Subsequently, the weed\ninfected regions are identified using a fine-tuned CNN, eliminating the need\nfor designing hand-crafted features. The approach is validated on two datasets\nof different crop/weed species (1) Crop Weed Field Image Dataset (CWFID), which\nconsists of carrot plant images and the (2) Sugar Beets dataset. The proposed\nmethod is able to localize weed-infested regions a maximum recall of 0.99 and\nestimate weed density with a maximum accuracy of 82.13%. Hence, the proposed\napproach is shown to generalize to different plant species without the need for\nextensive labeled data.\n", "versions": [{"version": "v1", "created": "Wed, 4 Nov 2020 09:35:53 GMT"}, {"version": "v2", "created": "Thu, 18 Feb 2021 14:05:01 GMT"}], "update_date": "2021-02-22", "authors_parsed": [["Shorewala", "Shantam", ""], ["Ashfaque", "Armaan", ""], ["R", "Sidharth", ""], ["Verma", "Ujjwal", ""]]}, {"id": "2011.02197", "submitter": "Pilar Cano", "authors": "Prosenjit Bose, Pilar Cano, Rodrigo I. Silveira", "title": "Affine invariant triangulations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study affine invariant 2D triangulation methods. That is, methods that\nproduce the same triangulation for a point set $S$ for any (unknown) affine\ntransformation of $S$. Our work is based on a method by Nielson [A\ncharacterization of an affine invariant triangulation. Geom. Mod, 191-210.\nSpringer, 1993] that uses the inverse of the covariance matrix of $S$ to define\nan affine invariant norm, denoted $A_{S}$, and an affine invariant\ntriangulation, denoted ${DT}_{A_{S}}[S]$. We revisit the $A_{S}$-norm from a\ngeometric perspective, and show that ${DT}_{A_{S}}[S]$ can be seen as a\nstandard Delaunay triangulation of a transformed point set based on $S$. We\nprove that it retains all of its well-known properties such as being 1-tough,\ncontaining a perfect matching, and being a constant spanner of the complete\ngeometric graph of $S$. We show that the $A_{S}$-norm extends to a hierarchy of\nrelated geometric structures such as the minimum spanning tree, nearest\nneighbor graph, Gabriel graph, relative neighborhood graph, and higher order\nversions of these graphs. In addition, we provide different affine invariant\nsorting methods of a point set $S$ and of the vertices of a polygon $P$ that\ncan be combined with known algorithms to obtain other affine invariant\ntriangulation methods of $S$ and of $P$.\n", "versions": [{"version": "v1", "created": "Wed, 4 Nov 2020 09:41:16 GMT"}], "update_date": "2020-11-05", "authors_parsed": [["Bose", "Prosenjit", ""], ["Cano", "Pilar", ""], ["Silveira", "Rodrigo I.", ""]]}, {"id": "2011.02206", "submitter": "Haruka Aoki", "authors": "Haruka Aoki, Koki Tsubota, Hikaru Ikuta, Kiyoharu Aizawa", "title": "Few-Shot Font Generation with Deep Metric Learning", "comments": "Accepted to ICPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Designing fonts for languages with a large number of characters, such as\nJapanese and Chinese, is an extremely labor-intensive and time-consuming task.\nIn this study, we addressed the problem of automatically generating Japanese\ntypographic fonts from only a few font samples, where the synthesized glyphs\nare expected to have coherent characteristics, such as skeletons, contours, and\nserifs. Existing methods often fail to generate fine glyph images when the\nnumber of style reference glyphs is extremely limited. Herein, we proposed a\nsimple but powerful framework for extracting better style features. This\nframework introduces deep metric learning to style encoders. We performed\nexperiments using black-and-white and shape-distinctive font datasets and\ndemonstrated the effectiveness of the proposed framework.\n", "versions": [{"version": "v1", "created": "Wed, 4 Nov 2020 10:12:10 GMT"}], "update_date": "2020-11-05", "authors_parsed": [["Aoki", "Haruka", ""], ["Tsubota", "Koki", ""], ["Ikuta", "Hikaru", ""], ["Aizawa", "Kiyoharu", ""]]}, {"id": "2011.02208", "submitter": "Yuki Inoue", "authors": "Yuki Inoue and Hiroto Nagayoshi", "title": "Crack Detection as a Weakly-Supervised Problem: Towards Achieving Less\n  Annotation-Intensive Crack Detectors", "comments": "Accepted to ICPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic crack detection is a critical task that has the potential to\ndrastically reduce labor-intensive building and road inspections currently\nbeing done manually. Recent studies in this field have significantly improved\nthe detection accuracy. However, the methods often heavily rely on costly\nannotation processes. In addition, to handle a wide variety of target domains,\nnew batches of annotations are usually required for each new environment. This\nmakes the data annotation cost a significant bottleneck when deploying crack\ndetection systems in real life. To resolve this issue, we formulate the crack\ndetection problem as a weakly-supervised problem and propose a two-branched\nframework. By combining predictions of a supervised model trained on low\nquality annotations with predictions based on pixel brightness, our framework\nis less affected by the annotation quality. Experimental results show that the\nproposed framework retains high detection accuracy even when provided with low\nquality annotations. Implementation of the proposed framework is publicly\navailable at https://github.com/hitachi-rd-cv/weakly-sup-crackdet.\n", "versions": [{"version": "v1", "created": "Wed, 4 Nov 2020 10:14:33 GMT"}], "update_date": "2020-11-05", "authors_parsed": [["Inoue", "Yuki", ""], ["Nagayoshi", "Hiroto", ""]]}, {"id": "2011.02222", "submitter": "Biel Tura Vecino", "authors": "Biel Tura Vecino, Mart\\'i Cobos and Philippe Salembier", "title": "Low cost enhanced security face recognition with stereo cameras", "comments": "5 pages, 9 figures, code available at\n  https://github.com/bieltura/Automotive_face_detector", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article explores a face recognition alternative which seeks to\ncontribute to resolve current security vulnerabilities in most recognition\narchitectures. Current low cost facial authentication software in the market\ncan be fooled by a printed picture of a face due to the lack of depth\ninformation. The presented software creates a depth map of the face with the\nhelp of a stereo setup, offering a higher level of security than traditional\nrecognition programs. Analysis of the person's identity and facial depth map\nare processed through deep convolutional neural networks, providing a secure\nlow cost real-time face authentication method.\n", "versions": [{"version": "v1", "created": "Wed, 4 Nov 2020 10:55:40 GMT"}], "update_date": "2020-11-05", "authors_parsed": [["Vecino", "Biel Tura", ""], ["Cobos", "Mart\u00ed", ""], ["Salembier", "Philippe", ""]]}, {"id": "2011.02229", "submitter": "Felix J\\\"aremo Lawin", "authors": "Felix J\\\"aremo Lawin, Per-Erik Forss\\'en", "title": "Registration Loss Learning for Deep Probabilistic Point Set Registration", "comments": "3DV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Probabilistic methods for point set registration have interesting theoretical\nproperties, such as linear complexity in the number of used points, and they\neasily generalize to joint registration of multiple point sets. In this work,\nwe improve their recognition performance to match state of the art. This is\ndone by incorporating learned features, by adding a von Mises-Fisher feature\nmodel in each mixture component, and by using learned attention weights. We\nlearn these jointly using a registration loss learning strategy (RLL) that\ndirectly uses the registration error as a loss, by back-propagating through the\nregistration iterations. This is possible as the probabilistic registration is\nfully differentiable, and the result is a learning framework that is truly\nend-to-end. We perform extensive experiments on the 3DMatch and Kitti datasets.\nThe experiments demonstrate that our approach benefits significantly from the\nintegration of the learned features and our learning strategy, outperforming\nthe state-of-the-art on Kitti. Code is available at\nhttps://github.com/felja633/RLLReg.\n", "versions": [{"version": "v1", "created": "Wed, 4 Nov 2020 11:05:44 GMT"}], "update_date": "2020-11-05", "authors_parsed": [["Lawin", "Felix J\u00e4remo", ""], ["Forss\u00e9n", "Per-Erik", ""]]}, {"id": "2011.02241", "submitter": "Benjamin Hadwiger", "authors": "Benjamin Hadwiger, Christian Riess", "title": "The Forchheim Image Database for Camera Identification in the Wild", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image provenance can represent crucial knowledge in criminal investigation\nand journalistic fact checking. In the last two decades, numerous algorithms\nhave been proposed for obtaining information on the source camera and\ndistribution history of an image. For a fair ranking of these techniques, it is\nimportant to rigorously assess their performance on practically relevant test\ncases. To this end, a number of datasets have been proposed. However, we argue\nthat there is a gap in existing databases: to our knowledge, there is currently\nno dataset that simultaneously satisfies two goals, namely a) to cleanly\nseparate scene content and forensic traces, and b) to support realistic\npost-processing like social media recompression. In this work, we propose the\nForchheim Image Database (FODB) to close this gap. It consists of more than\n23,000 images of 143 scenes by 27 smartphone cameras, and it allows to cleanly\nseparate image content from forensic artifacts. Each image is provided in 6\ndifferent qualities: the original camera-native version, and five copies from\nsocial networks. We demonstrate the usefulness of FODB in an evaluation of\nmethods for camera identification. We report three findings. First, the\nrecently proposed general-purpose EfficientNet remarkably outperforms several\ndedicated forensic CNNs both on clean and compressed images. Second,\nclassifiers obtain a performance boost even on unknown post-processing after\naugmentation by artificial degradations. Third, FODB's clean separation of\nscene content and forensic traces imposes important, rigorous boundary\nconditions for algorithm benchmarking.\n", "versions": [{"version": "v1", "created": "Wed, 4 Nov 2020 11:54:54 GMT"}], "update_date": "2020-11-05", "authors_parsed": [["Hadwiger", "Benjamin", ""], ["Riess", "Christian", ""]]}, {"id": "2011.02242", "submitter": "Ming Qian", "authors": "Ming Qian, Congyu Qiao, Jiamin Lin, Zhenyu Guo, Chenghua Li, Cong\n  Leng, Jian Cheng", "title": "BGGAN: Bokeh-Glass Generative Adversarial Network for Rendering\n  Realistic Bokeh", "comments": "accepted by ECCV workshop 2020", "journal-ref": "Proceedings of the European Conference on Computer Vision\n  Workshops. 2020: 229-244", "doi": "10.1007/978-3-030-67070-2_14", "report-no": "2020: 229-244", "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A photo captured with bokeh effect often means objects in focus are sharp\nwhile the out-of-focus areas are all blurred. DSLR can easily render this kind\nof effect naturally. However, due to the limitation of sensors, smartphones\ncannot capture images with depth-of-field effects directly. In this paper, we\npropose a novel generator called Glass-Net, which generates bokeh images not\nrelying on complex hardware. Meanwhile, the GAN-based method and perceptual\nloss are combined for rendering a realistic bokeh effect in the stage of\nfinetuning the model. Moreover, Instance Normalization(IN) is reimplemented in\nour network, which ensures our tflite model with IN can be accelerated on\nsmartphone GPU. Experiments show that our method is able to render a\nhigh-quality bokeh effect and process one $1024 \\times 1536$ pixel image in 1.9\nseconds on all smartphone chipsets. This approach ranked First in AIM 2020\nRendering Realistic Bokeh Challenge Track 1 \\& Track 2.\n", "versions": [{"version": "v1", "created": "Wed, 4 Nov 2020 11:56:34 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Qian", "Ming", ""], ["Qiao", "Congyu", ""], ["Lin", "Jiamin", ""], ["Guo", "Zhenyu", ""], ["Li", "Chenghua", ""], ["Leng", "Cong", ""], ["Cheng", "Jian", ""]]}, {"id": "2011.02250", "submitter": "Gelareh Mohammadi", "authors": "Nuha Aldausari, Arcot Sowmya, Nadine Marcus, Gelareh Mohammadi", "title": "Video Generative Adversarial Networks: A Review", "comments": "24 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the increasing interest in the content creation field in multiple\nsectors such as media, education, and entertainment, there is an increasing\ntrend in the papers that uses AI algorithms to generate content such as images,\nvideos, audio, and text. Generative Adversarial Networks (GANs) in one of the\npromising models that synthesizes data samples that are similar to real data\nsamples. While the variations of GANs models, in general, have been covered to\nsome extent in several survey papers, to the best of our knowledge, this is\namong the first survey papers that reviews the state-of-the-art video GANs\nmodels. This paper first categorized GANs review papers into general GANs\nreview papers, image GANs review papers, and special field GANs review papers\nsuch as anomaly detection, medical imaging, or cybersecurity. The paper then\nsummarizes the main improvements in GANs frameworks that are not initially\ndeveloped for the video domain but have been adopted in multiple video GANs\nvariations. Then, a comprehensive review of video GANs models is provided under\ntwo main divisions according to the presence or non-presence of a condition.\nThe conditional models then further grouped according to the type of condition\ninto audio, text, video, and image. The paper is concluded by highlighting the\nmain challenges and limitations of the current video GANs models. A\ncomprehensive list of datasets, applied loss functions, and evaluation metrics\nis provided in the supplementary material.\n", "versions": [{"version": "v1", "created": "Wed, 4 Nov 2020 12:16:05 GMT"}], "update_date": "2020-11-05", "authors_parsed": [["Aldausari", "Nuha", ""], ["Sowmya", "Arcot", ""], ["Marcus", "Nadine", ""], ["Mohammadi", "Gelareh", ""]]}, {"id": "2011.02264", "submitter": "Christian Bartz", "authors": "Christian Bartz, Hendrik R\\\"atz, Christoph Meinel", "title": "Handwriting Classification for the Analysis of Art-Historical Documents", "comments": "Code available at\n  https://github.com/hendraet/handwriting-classification", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Digitized archives contain and preserve the knowledge of generations of\nscholars in millions of documents. The size of these archives calls for\nautomatic analysis since a manual analysis by specialists is often too\nexpensive. In this paper, we focus on the analysis of handwriting in scanned\ndocuments from the art-historic archive of the WPI. Since the archive consists\nof documents written in several languages and lacks annotated training data for\nthe creation of recognition models, we propose the task of handwriting\nclassification as a new step for a handwriting OCR pipeline. We propose a\nhandwriting classification model that labels extracted text fragments, eg,\nnumbers, dates, or words, based on their visual structure. Such a\nclassification supports historians by highlighting documents that contain a\nspecific class of text without the need to read the entire content. To this\nend, we develop and compare several deep learning-based models for text\nclassification. In extensive experiments, we show the advantages and\ndisadvantages of our proposed approach and discuss possible usage scenarios on\na real-world dataset.\n", "versions": [{"version": "v1", "created": "Wed, 4 Nov 2020 13:06:46 GMT"}], "update_date": "2020-11-05", "authors_parsed": [["Bartz", "Christian", ""], ["R\u00e4tz", "Hendrik", ""], ["Meinel", "Christoph", ""]]}, {"id": "2011.02265", "submitter": "Yuan Cheng Ph.D", "authors": "Yuan Cheng, Yuchao Yang, Hai-Bao Chen, Ngai Wong, Hao Yu", "title": "S3-Net: A Fast and Lightweight Video Scene Understanding Network by\n  Single-shot Segmentation", "comments": "WACV2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Real-time understanding in video is crucial in various AI applications such\nas autonomous driving. This work presents a fast single-shot segmentation\nstrategy for video scene understanding. The proposed net, called S3-Net,\nquickly locates and segments target sub-scenes, meanwhile extracts structured\ntime-series semantic features as inputs to an LSTM-based spatio-temporal model.\nUtilizing tensorization and quantization techniques, S3-Net is intended to be\nlightweight for edge computing. Experiments using CityScapes, UCF11, HMDB51 and\nMOMENTS datasets demonstrate that the proposed S3-Net achieves an accuracy\nimprovement of 8.1% versus the 3D-CNN based approach on UCF11, a storage\nreduction of 6.9x and an inference speed of 22.8 FPS on CityScapes with a\nGTX1080Ti GPU.\n", "versions": [{"version": "v1", "created": "Wed, 4 Nov 2020 13:09:26 GMT"}], "update_date": "2020-11-05", "authors_parsed": [["Cheng", "Yuan", ""], ["Yang", "Yuchao", ""], ["Chen", "Hai-Bao", ""], ["Wong", "Ngai", ""], ["Yu", "Hao", ""]]}, {"id": "2011.02272", "submitter": "Mayank Vatsa", "authors": "Richa Singh, Mayank Vatsa, Nalini Ratha", "title": "Trustworthy AI", "comments": "ACM CODS-COMAD 2021 Tutorial", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.CR cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern AI systems are reaping the advantage of novel learning methods. With\ntheir increasing usage, we are realizing the limitations and shortfalls of\nthese systems. Brittleness to minor adversarial changes in the input data,\nability to explain the decisions, address the bias in their training data, high\nopacity in terms of revealing the lineage of the system, how they were trained\nand tested, and under which parameters and conditions they can reliably\nguarantee a certain level of performance, are some of the most prominent\nlimitations. Ensuring the privacy and security of the data, assigning\nappropriate credits to data sources, and delivering decent outputs are also\nrequired features of an AI system. We propose the tutorial on Trustworthy AI to\naddress six critical issues in enhancing user and public trust in AI systems,\nnamely: (i) bias and fairness, (ii) explainability, (iii) robust mitigation of\nadversarial attacks, (iv) improved privacy and security in model building, (v)\nbeing decent, and (vi) model attribution, including the right level of credit\nassignment to the data sources, model architectures, and transparency in\nlineage.\n", "versions": [{"version": "v1", "created": "Mon, 2 Nov 2020 20:04:18 GMT"}], "update_date": "2020-11-05", "authors_parsed": [["Singh", "Richa", ""], ["Vatsa", "Mayank", ""], ["Ratha", "Nalini", ""]]}, {"id": "2011.02284", "submitter": "Lena Maier-Hein", "authors": "Lena Maier-Hein, Matthias Eisenmann, Duygu Sarikaya, Keno M\\\"arz, Toby\n  Collins, Anand Malpani, Johannes Fallert, Hubertus Feussner, Stamatia\n  Giannarou, Pietro Mascagni, Hirenkumar Nakawala, Adrian Park, Carla Pugh,\n  Danail Stoyanov, Swaroop S. Vedula, Beat Peter M\\\"uller, Kevin Cleary, Gabor\n  Fichtinger, Germain Forestier, Bernard Gibaud, Teodor Grantcharov, Makoto\n  Hashizume, Hannes Kenngott, Ron Kikinis, Lars M\\\"undermann, Nassir Navab,\n  Sinan Onogur, Raphael Sznitman, Russell Taylor, Minu Dietlinde Tizabi, Martin\n  Wagner, Gregory D. Hager, Thomas Neumuth, Nicolas Padoy, Pierre Jannin,\n  Stefanie Speidel", "title": "Surgical Data Science -- from Concepts to Clinical Translation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.CV cs.LG eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recent developments in data science in general and machine learning in\nparticular have transformed the way experts envision the future of surgery.\nSurgical data science is a new research field that aims to improve the quality\nof interventional healthcare through the capture, organization, analysis and\nmodeling of data. While an increasing number of data-driven approaches and\nclinical applications have been studied in the fields of radiological and\nclinical data science, translational success stories are still lacking in\nsurgery. In this publication, we shed light on the underlying reasons and\nprovide a roadmap for future advances in the field. Based on an international\nworkshop involving leading researchers in the field of surgical data science,\nwe review current practice, key achievements and initiatives as well as\navailable standards and tools for a number of topics relevant to the field,\nnamely (1) technical infrastructure for data acquisition, storage and access in\nthe presence of regulatory constraints, (2) data annotation and sharing and (3)\ndata analytics. Drawing from this extensive review, we present current\nchallenges for technology development and (4) describe a roadmap for faster\nclinical translation and exploitation of the full potential of surgical data\nscience.\n", "versions": [{"version": "v1", "created": "Fri, 30 Oct 2020 14:20:16 GMT"}], "update_date": "2020-11-05", "authors_parsed": [["Maier-Hein", "Lena", ""], ["Eisenmann", "Matthias", ""], ["Sarikaya", "Duygu", ""], ["M\u00e4rz", "Keno", ""], ["Collins", "Toby", ""], ["Malpani", "Anand", ""], ["Fallert", "Johannes", ""], ["Feussner", "Hubertus", ""], ["Giannarou", "Stamatia", ""], ["Mascagni", "Pietro", ""], ["Nakawala", "Hirenkumar", ""], ["Park", "Adrian", ""], ["Pugh", "Carla", ""], ["Stoyanov", "Danail", ""], ["Vedula", "Swaroop S.", ""], ["M\u00fcller", "Beat Peter", ""], ["Cleary", "Kevin", ""], ["Fichtinger", "Gabor", ""], ["Forestier", "Germain", ""], ["Gibaud", "Bernard", ""], ["Grantcharov", "Teodor", ""], ["Hashizume", "Makoto", ""], ["Kenngott", "Hannes", ""], ["Kikinis", "Ron", ""], ["M\u00fcndermann", "Lars", ""], ["Navab", "Nassir", ""], ["Onogur", "Sinan", ""], ["Sznitman", "Raphael", ""], ["Taylor", "Russell", ""], ["Tizabi", "Minu Dietlinde", ""], ["Wagner", "Martin", ""], ["Hager", "Gregory D.", ""], ["Neumuth", "Thomas", ""], ["Padoy", "Nicolas", ""], ["Jannin", "Pierre", ""], ["Speidel", "Stefanie", ""]]}, {"id": "2011.02293", "submitter": "Ruisong Zhang", "authors": "Ruisong Zhang, Weize Quan, Baoyuan Wu, Zhifeng Li, Dong-Ming Yan", "title": "Pixel-wise Dense Detector for Image Inpainting", "comments": "12 pages, 9 figures, accepted by Computer Graphics Forum,\n  supplementary material link:\n  https://evergrow.github.io/GDN_Inpainting_files/GDN_Inpainting_Supplement.pdf", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent GAN-based image inpainting approaches adopt an average strategy to\ndiscriminate the generated image and output a scalar, which inevitably lose the\nposition information of visual artifacts. Moreover, the adversarial loss and\nreconstruction loss (e.g., l1 loss) are combined with tradeoff weights, which\nare also difficult to tune. In this paper, we propose a novel detection-based\ngenerative framework for image inpainting, which adopts the min-max strategy in\nan adversarial process. The generator follows an encoder-decoder architecture\nto fill the missing regions, and the detector using weakly supervised learning\nlocalizes the position of artifacts in a pixel-wise manner. Such position\ninformation makes the generator pay attention to artifacts and further enhance\nthem. More importantly, we explicitly insert the output of the detector into\nthe reconstruction loss with a weighting criterion, which balances the weight\nof the adversarial loss and reconstruction loss automatically rather than\nmanual operation. Experiments on multiple public datasets show the superior\nperformance of the proposed framework. The source code is available at\nhttps://github.com/Evergrow/GDN_Inpainting.\n", "versions": [{"version": "v1", "created": "Wed, 4 Nov 2020 13:45:27 GMT"}, {"version": "v2", "created": "Tue, 17 Nov 2020 09:27:30 GMT"}], "update_date": "2020-11-18", "authors_parsed": [["Zhang", "Ruisong", ""], ["Quan", "Weize", ""], ["Wu", "Baoyuan", ""], ["Li", "Zhifeng", ""], ["Yan", "Dong-Ming", ""]]}, {"id": "2011.02298", "submitter": "Yuqi Gong", "authors": "Yuqi Gong, Xuehui Yu, Yao Ding, Xiaoke Peng, Jian Zhao, Zhenjun Han", "title": "Effective Fusion Factor in FPN for Tiny Object Detection", "comments": "accepted by WACV2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  FPN-based detectors have made significant progress in general object\ndetection, e.g., MS COCO and PASCAL VOC. However, these detectors fail in\ncertain application scenarios, e.g., tiny object detection. In this paper, we\nargue that the top-down connections between adjacent layers in FPN bring\ntwo-side influences for tiny object detection, not only positive. We propose a\nnovel concept, fusion factor, to control information that deep layers deliver\nto shallow layers, for adapting FPN to tiny object detection. After series of\nexperiments and analysis, we explore how to estimate an effective value of\nfusion factor for a particular dataset by a statistical method. The estimation\nis dependent on the number of objects distributed in each layer. Comprehensive\nexperiments are conducted on tiny object detection datasets, e.g., TinyPerson\nand Tiny CityPersons. Our results show that when configuring FPN with a proper\nfusion factor, the network is able to achieve significant performance gains\nover the baseline on tiny object detection datasets. Codes and models will be\nreleased.\n", "versions": [{"version": "v1", "created": "Wed, 4 Nov 2020 13:55:10 GMT"}, {"version": "v2", "created": "Mon, 9 Nov 2020 09:40:08 GMT"}], "update_date": "2020-11-10", "authors_parsed": [["Gong", "Yuqi", ""], ["Yu", "Xuehui", ""], ["Ding", "Yao", ""], ["Peng", "Xiaoke", ""], ["Zhao", "Jian", ""], ["Han", "Zhenjun", ""]]}, {"id": "2011.02307", "submitter": "Kaicong Sun", "authors": "Kaicong Sun and Sven Simon", "title": "FDRN: A Fast Deformable Registration Network for Medical Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deformable image registration is a fundamental task in medical imaging. Due\nto the large computational complexity of deformable registration of volumetric\nimages, conventional iterative methods usually face the tradeoff between the\nregistration accuracy and the computation time in practice. In order to boost\nthe registration performance in both accuracy and runtime, we propose a fast\nconvolutional neural network. Specially, to efficiently utilize the memory\nresources and enlarge the model capacity, we adopt additive forwarding instead\nof channel concatenation and deepen the network in each encoder and decoder\nstage. To facilitate the learning efficiency, we leverage skip connection\nwithin the encoder and decoder stages to enable residual learning and employ an\nauxiliary loss at the bottom layer with lowest resolution to involve deep\nsupervision. Particularly, the low-resolution auxiliary loss is weighted by an\nexponentially decayed parameter during the training phase. In conjunction with\nthe main loss in high-resolution grid, a coarse-to-fine learning strategy is\nachieved. Last but not least, we introduce an auxiliary loss based on the\nsegmentation prior to improve the registration performance in Dice score.\nComparing to the auxiliary loss using average Dice score, the proposed\nmulti-label segmentation loss does not induce additional memory cost in the\ntraining phase and can be employed on images with arbitrary amount of\ncategories. In the experiments, we show FDRN outperforms the existing\nstate-of-the-art registration methods for brain MR images by resorting to the\ncompact network structure and efficient learning. Besides, FDRN is a\ngeneralized framework for image registration which is not confined to a\nparticular type of medical images or anatomy.\n", "versions": [{"version": "v1", "created": "Wed, 4 Nov 2020 14:09:51 GMT"}, {"version": "v2", "created": "Fri, 5 Mar 2021 21:42:08 GMT"}, {"version": "v3", "created": "Wed, 14 Apr 2021 20:38:55 GMT"}, {"version": "v4", "created": "Wed, 23 Jun 2021 18:43:32 GMT"}], "update_date": "2021-06-25", "authors_parsed": [["Sun", "Kaicong", ""], ["Simon", "Sven", ""]]}, {"id": "2011.02356", "submitter": "Xin Jin", "authors": "Qi Kuang, Xin Jin, Qinping Zhao, Bin Zhou", "title": "Deep Multimodality Learning for UAV Video Aesthetic Quality Assessment", "comments": "IEEE Trans. on Multimedia, 2020", "journal-ref": null, "doi": "10.1109/TMM.2019.2960656", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the growing number of unmanned aerial vehicles (UAVs) and aerial\nvideos, there is a paucity of studies focusing on the aesthetics of aerial\nvideos that can provide valuable information for improving the aesthetic\nquality of aerial photography. In this article, we present a method of deep\nmultimodality learning for UAV video aesthetic quality assessment. More\nspecifically, a multistream framework is designed to exploit aesthetic\nattributes from multiple modalities, including spatial appearance, drone camera\nmotion, and scene structure. A novel specially designed motion stream network\nis proposed for this new multistream framework. We construct a dataset with\n6,000 UAV video shots captured by drone cameras. Our model can judge whether a\nUAV video was shot by professional photographers or amateurs together with the\nscene type classification. The experimental results reveal that our method\noutperforms the video classification methods and traditional SVM-based methods\nfor video aesthetics. In addition, we present three application examples of UAV\nvideo grading, professional segment detection and aesthetic-based UAV path\nplanning using the proposed method.\n", "versions": [{"version": "v1", "created": "Wed, 4 Nov 2020 15:37:49 GMT"}], "update_date": "2020-11-05", "authors_parsed": [["Kuang", "Qi", ""], ["Jin", "Xin", ""], ["Zhao", "Qinping", ""], ["Zhou", "Bin", ""]]}, {"id": "2011.02365", "submitter": "Savyasachi Gupta", "authors": "Savyasachi Gupta, Rudraksh Kapil, Goutham Kanahasabai, Shreyas\n  Srinivas Joshi, and Aniruddha Srinivas Joshi", "title": "SD-Measure: A Social Distancing Detector", "comments": "Contains 6 pages & 7 figures. Published in 12th CICN 2020", "journal-ref": "12th CICN, 2020, pp. 306-311", "doi": "10.1109/CICN49253.2020.9242628", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The practice of social distancing is imperative to curbing the spread of\ncontagious diseases and has been globally adopted as a non-pharmaceutical\nprevention measure during the COVID-19 pandemic. This work proposes a novel\nframework named SD-Measure for detecting social distancing from video footages.\nThe proposed framework leverages the Mask R-CNN deep neural network to detect\npeople in a video frame. To consistently identify whether social distancing is\npracticed during the interaction between people, a centroid tracking algorithm\nis utilised to track the subjects over the course of the footage. With the aid\nof authentic algorithms for approximating the distance of people from the\ncamera and between themselves, we determine whether the social distancing\nguidelines are being adhered to. The framework attained a high accuracy value\nin conjunction with a low false alarm rate when tested on Custom Video Footage\nDataset (CVFD) and Custom Personal Images Dataset (CPID), where it manifested\nits effectiveness in determining whether social distancing guidelines were\npracticed.\n", "versions": [{"version": "v1", "created": "Wed, 4 Nov 2020 15:47:14 GMT"}], "update_date": "2020-11-05", "authors_parsed": [["Gupta", "Savyasachi", ""], ["Kapil", "Rudraksh", ""], ["Kanahasabai", "Goutham", ""], ["Joshi", "Shreyas Srinivas", ""], ["Joshi", "Aniruddha Srinivas", ""]]}, {"id": "2011.02371", "submitter": "Savyasachi Gupta", "authors": "Aniruddha Srinivas Joshi, Shreyas Srinivas Joshi, Goutham Kanahasabai,\n  Rudraksh Kapil, and Savyasachi Gupta", "title": "Deep Learning Framework to Detect Face Masks from Video Footage", "comments": "Contains 6 pages and 6 figures. Published in 12th CICN 2020", "journal-ref": "12th CICN, 2020, pp. 435-440", "doi": "10.1109/CICN49253.2020.9242625", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The use of facial masks in public spaces has become a social obligation since\nthe wake of the COVID-19 global pandemic and the identification of facial masks\ncan be imperative to ensure public safety. Detection of facial masks in video\nfootages is a challenging task primarily due to the fact that the masks\nthemselves behave as occlusions to face detection algorithms due to the absence\nof facial landmarks in the masked regions. In this work, we propose an approach\nfor detecting facial masks in videos using deep learning. The proposed\nframework capitalizes on the MTCNN face detection model to identify the faces\nand their corresponding facial landmarks present in the video frame. These\nfacial images and cues are then processed by a neoteric classifier that\nutilises the MobileNetV2 architecture as an object detector for identifying\nmasked regions. The proposed framework was tested on a dataset which is a\ncollection of videos capturing the movement of people in public spaces while\ncomplying with COVID-19 safety protocols. The proposed methodology demonstrated\nits effectiveness in detecting facial masks by achieving high precision,\nrecall, and accuracy.\n", "versions": [{"version": "v1", "created": "Wed, 4 Nov 2020 16:02:03 GMT"}], "update_date": "2020-12-22", "authors_parsed": [["Joshi", "Aniruddha Srinivas", ""], ["Joshi", "Shreyas Srinivas", ""], ["Kanahasabai", "Goutham", ""], ["Kapil", "Rudraksh", ""], ["Gupta", "Savyasachi", ""]]}, {"id": "2011.02382", "submitter": "Felix Thomsen", "authors": "Felix Thomsen and Jos\\'e M. Fuertes Garc\\'ia and Manuel Lucena and\n  Juan Pisula and Rodrigo de Luis Garc\\'ia and Jan Broggrefe and Claudio\n  Delrieux", "title": "Noise Reduction to Compute Tissue Mineral Density and Trabecular Bone\n  Volume Fraction from Low Resolution QCT", "comments": "A revised version of this manuscript was accepted for publication in\n  Computerized Medical Imaging and Graphics", "journal-ref": null, "doi": "10.1016/j.compmedimag.2020.101816", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a 3D neural network with specific loss functions for quantitative\ncomputed tomography (QCT) noise reduction to compute micro-structural\nparameters such as tissue mineral density (TMD) and bone volume ratio (BV/TV)\nwith significantly higher accuracy than using no or standard noise reduction\nfilters. The vertebra-phantom study contained high resolution peripheral and\nclinical CT scans with simulated in vivo CT noise and nine repetitions of three\ndifferent tube currents (100, 250 and 360 mAs). Five-fold cross validation was\nperformed on 20466 purely spongy pairs of noisy and ground-truth patches.\nComparison of training and test errors revealed high robustness against\nover-fitting. While not showing effects for the assessment of BMD and\nvoxel-wise densities, the filter improved thoroughly the computation of TMD and\nBV/TV with respect to the unfiltered data. Root-mean-square and accuracy errors\nof low resolution TMD and BV/TV decreased to less than 17% of the initial\nvalues. Furthermore filtered low resolution scans revealed still more TMD- and\nBV/TV-relevant information than high resolution CT scans, either unfiltered or\nfiltered with two state-of-the-art standard denoising methods. The proposed\narchitecture is threshold and rotational invariant, applicable on a wide range\nof image resolutions at once, and likely serves for an accurate computation of\nfurther micro-structural parameters. Furthermore, it is less prone for\nover-fitting than neural networks that compute structural parameters directly.\nIn conclusion, the method is potentially important for the diagnosis of\nosteoporosis and other bone diseases since it allows to assess relevant 3D\nmicro-structural information from standard low exposure CT protocols such as\n100 mAs and 120 kVp.\n", "versions": [{"version": "v1", "created": "Wed, 4 Nov 2020 16:17:24 GMT"}], "update_date": "2020-11-11", "authors_parsed": [["Thomsen", "Felix", ""], ["Garc\u00eda", "Jos\u00e9 M. Fuertes", ""], ["Lucena", "Manuel", ""], ["Pisula", "Juan", ""], ["Garc\u00eda", "Rodrigo de Luis", ""], ["Broggrefe", "Jan", ""], ["Delrieux", "Claudio", ""]]}, {"id": "2011.02389", "submitter": "Kakeru Mitsuno", "authors": "Kakeru Mitsuno and Takio Kurita", "title": "Filter Pruning using Hierarchical Group Sparse Regularization for Deep\n  Convolutional Neural Networks", "comments": "Accepted to ICPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Since the convolutional neural networks are often trained with redundant\nparameters, it is possible to reduce redundant kernels or filters to obtain a\ncompact network without dropping the classification accuracy. In this paper, we\npropose a filter pruning method using the hierarchical group sparse\nregularization. It is shown in our previous work that the hierarchical group\nsparse regularization is effective in obtaining sparse networks in which\nfilters connected to unnecessary channels are automatically close to zero.\nAfter training the convolutional neural network with the hierarchical group\nsparse regularization, the unnecessary filters are selected based on the\nincrease of the classification loss of the randomly selected training samples\nto obtain a compact network. It is shown that the proposed method can reduce\nmore than 50% parameters of ResNet for CIFAR-10 with only 0.3% decrease in the\naccuracy of test samples. Also, 34% parameters of ResNet are reduced for\nTinyImageNet-200 with higher accuracy than the baseline network.\n", "versions": [{"version": "v1", "created": "Wed, 4 Nov 2020 16:29:41 GMT"}], "update_date": "2020-11-05", "authors_parsed": [["Mitsuno", "Kakeru", ""], ["Kurita", "Takio", ""]]}, {"id": "2011.02390", "submitter": "Kakeru Mitsuno", "authors": "Kakeru Mitsuno, Yuichiro Nomura and Takio Kurita", "title": "Channel Planting for Deep Neural Networks using Knowledge Distillation", "comments": "Accepted to ICPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, deeper and wider neural networks have shown excellent\nperformance in computer vision tasks, while their enormous amount of parameters\nresults in increased computational cost and overfitting. Several methods have\nbeen proposed to compress the size of the networks without reducing network\nperformance. Network pruning can reduce redundant and unnecessary parameters\nfrom a network. Knowledge distillation can transfer the knowledge of deeper and\nwider networks to smaller networks. The performance of the smaller network\nobtained by these methods is bounded by the predefined network. Neural\narchitecture search has been proposed, which can search automatically the\narchitecture of the networks to break the structure limitation. Also, there is\na dynamic configuration method to train networks incrementally as sub-networks.\nIn this paper, we present a novel incremental training algorithm for deep\nneural networks called planting. Our planting can search the optimal network\narchitecture with smaller number of parameters for improving the network\nperformance by augmenting channels incrementally to layers of the initial\nnetworks while keeping the earlier trained parameters fixed. Also, we propose\nusing the knowledge distillation method for training the channels planted. By\ntransferring the knowledge of deeper and wider networks, we can grow the\nnetworks effectively and efficiently. We evaluate the effectiveness of the\nproposed method on different datasets such as CIFAR-10/100 and STL-10. For the\nSTL-10 dataset, we show that we are able to achieve comparable performance with\nonly 7% parameters compared to the larger network and reduce the overfitting\ncaused by a small amount of the data.\n", "versions": [{"version": "v1", "created": "Wed, 4 Nov 2020 16:29:59 GMT"}], "update_date": "2020-11-05", "authors_parsed": [["Mitsuno", "Kakeru", ""], ["Nomura", "Yuichiro", ""], ["Kurita", "Takio", ""]]}, {"id": "2011.02395", "submitter": "Tiago de Freitas Pereira", "authors": "Tiago de Freitas Pereira and S\\'ebastien Marcel", "title": "Fairness in Biometrics: a figure of merit to assess biometric\n  verification systems", "comments": "11 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning-based (ML) systems are being largely deployed since the last\ndecade in a myriad of scenarios impacting several instances in our daily lives.\nWith this vast sort of applications, aspects of fairness start to rise in the\nspotlight due to the social impact that this can get in minorities. In this\nwork aspects of fairness in biometrics are addressed. First, we introduce the\nfirst figure of merit that is able to evaluate and compare fairness aspects\nbetween multiple biometric verification systems, the so-called Fairness\nDiscrepancy Rate (FDR). A use case with two synthetic biometric systems is\nintroduced and demonstrates the potential of this figure of merit in extreme\ncases of fair and unfair behavior. Second, a use case using face biometrics is\npresented where several systems are evaluated compared with this new figure of\nmerit using three public datasets exploring gender and race demographics.\n", "versions": [{"version": "v1", "created": "Wed, 4 Nov 2020 16:46:37 GMT"}, {"version": "v2", "created": "Tue, 30 Mar 2021 07:23:41 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Pereira", "Tiago de Freitas", ""], ["Marcel", "S\u00e9bastien", ""]]}, {"id": "2011.02426", "submitter": "Subramanyam Natarajan", "authors": "Arvind Srinivasan, Aprameya Bharadwaj, Aveek Saha, Subramanyam\n  Natarajan", "title": "Graph Based Temporal Aggregation for Video Retrieval", "comments": "6 pages, 6 figures, 7 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large scale video retrieval is a field of study with a lot of ongoing\nresearch. Most of the work in the field is on video retrieval through text\nqueries using techniques such as VSE++. However, there is little research done\non video retrieval through image queries, and the work that has been done in\nthis field either uses image queries from within the video dataset or iterates\nthrough videos frame by frame. These approaches are not generalized for queries\nfrom outside the dataset and do not scale well for large video datasets. To\novercome these issues, we propose a new approach for video retrieval through\nimage queries where an undirected graph is constructed from the combined set of\nframes from all videos to be searched. The node features of this graph are used\nin the task of video retrieval. Experimentation is done on the MSR-VTT dataset\nby using query images from outside the dataset. To evaluate this novel approach\nP@5, P@10 and P@20 metrics are calculated. Two different ResNet models namely,\nResNet-152 and ResNet-50 are used in this study.\n", "versions": [{"version": "v1", "created": "Wed, 4 Nov 2020 17:23:14 GMT"}], "update_date": "2020-11-05", "authors_parsed": [["Srinivasan", "Arvind", ""], ["Bharadwaj", "Aprameya", ""], ["Saha", "Aveek", ""], ["Natarajan", "Subramanyam", ""]]}, {"id": "2011.02427", "submitter": "Saurabh Goswami", "authors": "Saurabh Goswami, Aakanksha, Rajagopalan A. N", "title": "Robust Super-Resolution of Real Faces using Smooth Features", "comments": "Accepted in the Adversarial Robustness in Real World (AROW) workshop\n  in ECCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Real low-resolution (LR) face images contain degradations which are too\nvaried and complex to be captured by known downsampling kernels and\nsignal-independent noises. So, in order to successfully super-resolve real\nfaces, a method needs to be robust to a wide range of noise, blur, compression\nartifacts etc. Some of the recent works attempt to model these degradations\nfrom a dataset of real images using a Generative Adversarial Network (GAN).\nThey generate synthetically degraded LR images and use them with corresponding\nreal high-resolution(HR) image to train a super-resolution (SR) network using a\ncombination of a pixel-wise loss and an adversarial loss. In this paper, we\npropose a two module super-resolution network where the feature extractor\nmodule extracts robust features from the LR image, and the SR module generates\nan HR estimate using only these robust features. We train a degradation GAN to\nconvert bicubically downsampled clean images to real degraded images, and\ninterpolate between the obtained degraded LR image and its clean LR\ncounterpart. This interpolated LR image is then used along with it's\ncorresponding HR counterpart to train the super-resolution network from end to\nend. Entropy Regularized Wasserstein Divergence is used to force the encoded\nfeatures learnt from the clean and degraded images to closely resemble those\nextracted from the interpolated image to ensure robustness.\n", "versions": [{"version": "v1", "created": "Wed, 4 Nov 2020 17:25:54 GMT"}], "update_date": "2020-11-05", "authors_parsed": [["Goswami", "Saurabh", ""], ["Aakanksha", "", ""], ["N", "Rajagopalan A.", ""]]}, {"id": "2011.02451", "submitter": "Zheheng Jiang", "authors": "Zheheng Jiang, Feixiang Zhou, Aite Zhao, Xin Li, Ling Li, Dacheng Tao,\n  Xuelong Li and Huiyu Zhou", "title": "Muti-view Mouse Social Behaviour Recognition with Deep Graphical Model", "comments": "17 pages, 11 figures", "journal-ref": null, "doi": "10.1109/TIP.2021.3083079", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Home-cage social behaviour analysis of mice is an invaluable tool to assess\ntherapeutic efficacy of neurodegenerative diseases. Despite tremendous efforts\nmade within the research community, single-camera video recordings are mainly\nused for such analysis. Because of the potential to create rich descriptions of\nmouse social behaviors, the use of multi-view video recordings for rodent\nobservations is increasingly receiving much attention. However, identifying\nsocial behaviours from various views is still challenging due to the lack of\ncorrespondence across data sources. To address this problem, we here propose a\nnovel multiview latent-attention and dynamic discriminative model that jointly\nlearns view-specific and view-shared sub-structures, where the former captures\nunique dynamics of each view whilst the latter encodes the interaction between\nthe views. Furthermore, a novel multi-view latent-attention variational\nautoencoder model is introduced in learning the acquired features, enabling us\nto learn discriminative features in each view. Experimental results on the\nstandard CRMI13 and our multi-view Parkinson's Disease Mouse Behaviour (PDMB)\ndatasets demonstrate that our model outperforms the other state of the arts\ntechnologies and effectively deals with the imbalanced data problem.\n", "versions": [{"version": "v1", "created": "Wed, 4 Nov 2020 18:09:58 GMT"}, {"version": "v2", "created": "Wed, 30 Jun 2021 07:44:00 GMT"}], "update_date": "2021-07-07", "authors_parsed": [["Jiang", "Zheheng", ""], ["Zhou", "Feixiang", ""], ["Zhao", "Aite", ""], ["Li", "Xin", ""], ["Li", "Ling", ""], ["Tao", "Dacheng", ""], ["Li", "Xuelong", ""], ["Zhou", "Huiyu", ""]]}, {"id": "2011.02514", "submitter": "Wang Zhou", "authors": "Wang Zhou, Levente Klein", "title": "Monitoring the Impact of Wildfires on Tree Species with Deep Learning", "comments": null, "journal-ref": "Neural Information Processing Systems (NeurIPS 2020) Workshop", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the impacts of climate change is the difficulty of tree regrowth after\nwildfires over areas that traditionally were covered by certain tree species.\nHere a deep learning model is customized to classify land covers from four-band\naerial imagery before and after wildfires to study the prolonged consequences\nof wildfires on tree species. The tree species labels are generated from\nmanually delineated maps for five land cover classes: Conifer, Hardwood, Shrub,\nReforestedTree and Barren land. With an accuracy of $92\\%$ on the test split,\nthe model is applied to three wildfires on data from 2009 to 2018. The model\naccurately delineates areas damaged by wildfires, changes in tree species and\nrebound of burned areas. The result shows clear evidence of wildfires impacting\nthe local ecosystem and the outlined approach can help monitor reforested\nareas, observe changes in forest composition and track wildfire impact on tree\nspecies.\n", "versions": [{"version": "v1", "created": "Wed, 4 Nov 2020 19:42:04 GMT"}, {"version": "v2", "created": "Thu, 12 Nov 2020 06:28:38 GMT"}], "update_date": "2020-11-13", "authors_parsed": [["Zhou", "Wang", ""], ["Klein", "Levente", ""]]}, {"id": "2011.02519", "submitter": "Wang Zhou", "authors": "Chulin Wang, Eloisa Bentivegna, Wang Zhou, Levente Klein, Bruce\n  Elmegreen", "title": "Physics-Informed Neural Network Super Resolution for Advection-Diffusion\n  Models", "comments": null, "journal-ref": "Neural Information Processing Systems (NeurIPS 2020) Workshop", "doi": null, "report-no": null, "categories": "cs.CV physics.geo-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Physics-informed neural networks (NN) are an emerging technique to improve\nspatial resolution and enforce physical consistency of data from physics models\nor satellite observations. A super-resolution (SR) technique is explored to\nreconstruct high-resolution images ($4\\times$) from lower resolution images in\nan advection-diffusion model of atmospheric pollution plumes. SR performance is\ngenerally increased when the advection-diffusion equation constrains the NN in\naddition to conventional pixel-based constraints. The ability of SR techniques\nto also reconstruct missing data is investigated by randomly removing image\npixels from the simulations and allowing the system to learn the content of\nmissing data. Improvements in S/N of $11\\%$ are demonstrated when physics\nequations are included in SR with $40\\%$ pixel loss. Physics-informed NNs\naccurately reconstruct corrupted images and generate better results compared to\nthe standard SR approaches.\n", "versions": [{"version": "v1", "created": "Wed, 4 Nov 2020 19:56:11 GMT"}, {"version": "v2", "created": "Sat, 12 Dec 2020 05:27:41 GMT"}], "update_date": "2020-12-15", "authors_parsed": [["Wang", "Chulin", ""], ["Bentivegna", "Eloisa", ""], ["Zhou", "Wang", ""], ["Klein", "Levente", ""], ["Elmegreen", "Bruce", ""]]}, {"id": "2011.02523", "submitter": "Mike Roberts", "authors": "Mike Roberts, Nathan Paczan", "title": "Hypersim: A Photorealistic Synthetic Dataset for Holistic Indoor Scene\n  Understanding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For many fundamental scene understanding tasks, it is difficult or impossible\nto obtain per-pixel ground truth labels from real images. We address this\nchallenge by introducing Hypersim, a photorealistic synthetic dataset for\nholistic indoor scene understanding. To create our dataset, we leverage a large\nrepository of synthetic scenes created by professional artists, and we generate\n77,400 images of 461 indoor scenes with detailed per-pixel labels and\ncorresponding ground truth geometry. Our dataset: (1) relies exclusively on\npublicly available 3D assets; (2) includes complete scene geometry, material\ninformation, and lighting information for every scene; (3) includes dense\nper-pixel semantic instance segmentations for every image; and (4) factors\nevery image into diffuse reflectance, diffuse illumination, and a non-diffuse\nresidual term that captures view-dependent lighting effects. Together, these\nfeatures make our dataset well-suited for geometric learning problems that\nrequire direct 3D supervision, multi-task learning problems that require\nreasoning jointly over multiple input and output modalities, and inverse\nrendering problems. We analyze our dataset at the level of scenes, objects, and\npixels, and we analyze costs in terms of money, annotation effort, and\ncomputation time. Remarkably, we find that it is possible to generate our\nentire dataset from scratch, for roughly half the cost of training a\nstate-of-the-art natural language processing model. All the code we used to\ngenerate our dataset is available online.\n", "versions": [{"version": "v1", "created": "Wed, 4 Nov 2020 20:12:07 GMT"}, {"version": "v2", "created": "Sat, 7 Nov 2020 20:43:43 GMT"}, {"version": "v3", "created": "Tue, 10 Nov 2020 08:05:56 GMT"}, {"version": "v4", "created": "Tue, 17 Nov 2020 02:54:24 GMT"}], "update_date": "2020-11-18", "authors_parsed": [["Roberts", "Mike", ""], ["Paczan", "Nathan", ""]]}, {"id": "2011.02543", "submitter": "Aleksandr Petiushko", "authors": "Stepan Komkov, Maksim Dzabraev, Aleksandr Petiushko", "title": "Mutual Modality Learning for Video Action Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The construction of models for video action classification progresses\nrapidly. However, the performance of those models can still be easily improved\nby ensembling with the same models trained on different modalities (e.g.\nOptical flow). Unfortunately, it is computationally expensive to use several\nmodalities during inference. Recent works examine the ways to integrate\nadvantages of multi-modality into a single RGB-model. Yet, there is still a\nroom for improvement. In this paper, we explore the various methods to embed\nthe ensemble power into a single model. We show that proper initialization, as\nwell as mutual modality learning, enhances single-modality models. As a result,\nwe achieve state-of-the-art results in the Something-Something-v2 benchmark.\n", "versions": [{"version": "v1", "created": "Wed, 4 Nov 2020 21:20:08 GMT"}], "update_date": "2020-11-06", "authors_parsed": [["Komkov", "Stepan", ""], ["Dzabraev", "Maksim", ""], ["Petiushko", "Aleksandr", ""]]}, {"id": "2011.02553", "submitter": "Yuanxin Zhong", "authors": "Yuanxin Zhong, Minghan Zhu and Huei Peng", "title": "Uncertainty-Aware Voxel based 3D Object Detection and Tracking with\n  von-Mises Loss", "comments": "Submitted to ICRA 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Object detection and tracking is a key task in autonomy. Specifically, 3D\nobject detection and tracking have been an emerging hot topic recently.\nAlthough various methods have been proposed for object detection, uncertainty\nin the 3D detection and tracking tasks has been less explored. Uncertainty\nhelps us tackle the error in the perception system and improve robustness. In\nthis paper, we propose a method for improving target tracking performance by\nadding uncertainty regression to the SECOND detector, which is one of the most\nrepresentative algorithms of 3D object detection. Our method estimates\npositional and dimensional uncertainties with Gaussian Negative Log-Likelihood\n(NLL) Loss for estimation and introduces von-Mises NLL Loss for angular\nuncertainty estimation. We fed the uncertainty output into a classical object\ntracking framework and proved that our method increased the tracking\nperformance compared against the vanilla tracker with constant covariance\nassumption.\n", "versions": [{"version": "v1", "created": "Wed, 4 Nov 2020 21:53:31 GMT"}], "update_date": "2020-11-06", "authors_parsed": [["Zhong", "Yuanxin", ""], ["Zhu", "Minghan", ""], ["Peng", "Huei", ""]]}, {"id": "2011.02570", "submitter": "Rahul Venkatesh", "authors": "Rahul Venkatesh, Sarthak Sharma, Aurobrata Ghosh, Laszlo Jeni, Maneesh\n  Singh", "title": "DUDE: Deep Unsigned Distance Embeddings for Hi-Fidelity Representation\n  of Complex 3D Surfaces", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High fidelity representation of shapes with arbitrary topology is an\nimportant problem for a variety of vision and graphics applications. Owing to\ntheir limited resolution, classical discrete shape representations using point\nclouds, voxels and meshes produce low quality results when used in these\napplications. Several implicit 3D shape representation approaches using deep\nneural networks have been proposed leading to significant improvements in both\nquality of representations as well as the impact on downstream applications.\nHowever, these methods can only be used to represent topologically closed\nshapes which greatly limits the class of shapes that they can represent. As a\nconsequence, they also often require clean, watertight meshes for training. In\nthis work, we propose DUDE - a Deep Unsigned Distance Embedding method which\nalleviates both of these shortcomings. DUDE is a disentangled shape\nrepresentation that utilizes an unsigned distance field (uDF) to represent\nproximity to a surface, and a normal vector field (nVF) to represent surface\norientation. We show that a combination of these two (uDF+nVF) can be used to\nlearn high fidelity representations for arbitrary open/closed shapes. As\nopposed to prior work such as DeepSDF, our shape representations can be\ndirectly learnt from noisy triangle soups, and do not need watertight meshes.\nAdditionally, we propose novel algorithms for extracting and rendering\niso-surfaces from the learnt representations. We validate DUDE on benchmark 3D\ndatasets and demonstrate that it produces significant improvements over the\nstate of the art.\n", "versions": [{"version": "v1", "created": "Wed, 4 Nov 2020 22:49:05 GMT"}, {"version": "v2", "created": "Mon, 14 Dec 2020 03:40:00 GMT"}], "update_date": "2020-12-15", "authors_parsed": [["Venkatesh", "Rahul", ""], ["Sharma", "Sarthak", ""], ["Ghosh", "Aurobrata", ""], ["Jeni", "Laszlo", ""], ["Singh", "Maneesh", ""]]}, {"id": "2011.02572", "submitter": "Litao Yu", "authors": "Litao Yu, Yongsheng Gao, Jun Zhou, Jian Zhang, Qiang Wu", "title": "Multi-layer Feature Aggregation for Deep Scene Parsing Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scene parsing from images is a fundamental yet challenging problem in visual\ncontent understanding. In this dense prediction task, the parsing model assigns\nevery pixel to a categorical label, which requires the contextual information\nof adjacent image patches. So the challenge for this learning task is to\nsimultaneously describe the geometric and semantic properties of objects or a\nscene. In this paper, we explore the effective use of multi-layer feature\noutputs of the deep parsing networks for spatial-semantic consistency by\ndesigning a novel feature aggregation module to generate the appropriate global\nrepresentation prior, to improve the discriminative power of features. The\nproposed module can auto-select the intermediate visual features to correlate\nthe spatial and semantic information. At the same time, the multiple skip\nconnections form a strong supervision, making the deep parsing network easy to\ntrain. Extensive experiments on four public scene parsing datasets prove that\nthe deep parsing network equipped with the proposed feature aggregation module\ncan achieve very promising results.\n", "versions": [{"version": "v1", "created": "Wed, 4 Nov 2020 23:07:07 GMT"}], "update_date": "2020-11-06", "authors_parsed": [["Yu", "Litao", ""], ["Gao", "Yongsheng", ""], ["Zhou", "Jun", ""], ["Zhang", "Jian", ""], ["Wu", "Qiang", ""]]}, {"id": "2011.02578", "submitter": "Kihyuk Sohn", "authors": "Kihyuk Sohn, Chun-Liang Li, Jinsung Yoon, Minho Jin, Tomas Pfister", "title": "Learning and Evaluating Representations for Deep One-class\n  Classification", "comments": "Published at International Conference on Learning Representation\n  (ICLR) 2021. The first two authors contributed equally", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present a two-stage framework for deep one-class classification. We first\nlearn self-supervised representations from one-class data, and then build\none-class classifiers on learned representations. The framework not only allows\nto learn better representations, but also permits building one-class\nclassifiers that are faithful to the target task. We argue that classifiers\ninspired by the statistical perspective in generative or discriminative models\nare more effective than existing approaches, such as a normality score from a\nsurrogate classifier. We thoroughly evaluate different self-supervised\nrepresentation learning algorithms under the proposed framework for one-class\nclassification. Moreover, we present a novel distribution-augmented contrastive\nlearning that extends training distributions via data augmentation to obstruct\nthe uniformity of contrastive representations. In experiments, we demonstrate\nstate-of-the-art performance on visual domain one-class classification\nbenchmarks, including novelty and anomaly detection. Finally, we present visual\nexplanations, confirming that the decision-making process of deep one-class\nclassifiers is intuitive to humans. The code is available at\nhttps://github.com/google-research/deep_representation_one_class.\n", "versions": [{"version": "v1", "created": "Wed, 4 Nov 2020 23:33:41 GMT"}, {"version": "v2", "created": "Thu, 25 Mar 2021 23:11:23 GMT"}], "update_date": "2021-03-29", "authors_parsed": [["Sohn", "Kihyuk", ""], ["Li", "Chun-Liang", ""], ["Yoon", "Jinsung", ""], ["Jin", "Minho", ""], ["Pfister", "Tomas", ""]]}, {"id": "2011.02579", "submitter": "Kunjal Panchal", "authors": "Kunjal Panchal", "title": "Improved Algorithm for Seamlessly Creating Infinite Loops from a Video\n  Clip, while Preserving Variety in Textures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This project implements the paper \"Video Textures\" by Szeliski. The aim is to\ncreate a \"Moving Picture\" or as we popularly call it, a GIF; which is\n\"somewhere between a photograph and a video\". The idea is to input a video\nwhich has some repeated motion (the texture), such as a flag waving, rain, or a\ncandle flame. The output is a new video that infinitely extends the original\nvideo in a seamless way. In practice, the output isn't really infinte, but is\ninstead looped using a video player and is sufficiently long as to appear to\nnever repeat.\n  Our goal from this implementation was to: improve distance metric by\nswitching from a crude sum of squared distance to most sophisticated\nwavelet-based distance; add intensity normalization, cross-fading and morphing\nto the suggested basic algorithm. We also experiment on the trade-off between\nvariety and smoothness.\n", "versions": [{"version": "v1", "created": "Wed, 4 Nov 2020 23:38:51 GMT"}], "update_date": "2020-11-06", "authors_parsed": [["Panchal", "Kunjal", ""]]}, {"id": "2011.02580", "submitter": "Yunguan Fu", "authors": "Yunguan Fu, Nina Monta\\~na Brown, Shaheer U. Saeed, Adri\\`a\n  Casamitjana, Zachary M. C. Baum, R\\'emi Delaunay, Qianye Yang, Alexander\n  Grimwood, Zhe Min, Stefano B. Blumberg, Juan Eugenio Iglesias, Dean C.\n  Barratt, Ester Bonmati, Daniel C. Alexander, Matthew J. Clarkson, Tom\n  Vercauteren, Yipeng Hu", "title": "DeepReg: a deep learning toolkit for medical image registration", "comments": "Accepted in The Journal of Open Source Software (JOSS)", "journal-ref": null, "doi": "10.21105/joss.02705", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  DeepReg (https://github.com/DeepRegNet/DeepReg) is a community-supported\nopen-source toolkit for research and education in medical image registration\nusing deep learning.\n", "versions": [{"version": "v1", "created": "Wed, 4 Nov 2020 23:39:02 GMT"}], "update_date": "2020-11-06", "authors_parsed": [["Fu", "Yunguan", ""], ["Brown", "Nina Monta\u00f1a", ""], ["Saeed", "Shaheer U.", ""], ["Casamitjana", "Adri\u00e0", ""], ["Baum", "Zachary M. C.", ""], ["Delaunay", "R\u00e9mi", ""], ["Yang", "Qianye", ""], ["Grimwood", "Alexander", ""], ["Min", "Zhe", ""], ["Blumberg", "Stefano B.", ""], ["Iglesias", "Juan Eugenio", ""], ["Barratt", "Dean C.", ""], ["Bonmati", "Ester", ""], ["Alexander", "Daniel C.", ""], ["Clarkson", "Matthew J.", ""], ["Vercauteren", "Tom", ""], ["Hu", "Yipeng", ""]]}, {"id": "2011.02594", "submitter": "Yueming Yin", "authors": "Yueming Yin, Zhen Yang, Haifeng Hu, and Xiaofu Wu", "title": "Universal Multi-Source Domain Adaptation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unsupervised domain adaptation enables intelligent models to transfer\nknowledge from a labeled source domain to a similar but unlabeled target\ndomain. Recent study reveals that knowledge can be transferred from one source\ndomain to another unknown target domain, called Universal Domain Adaptation\n(UDA). However, in the real-world application, there are often more than one\nsource domain to be exploited for domain adaptation. In this paper, we formally\npropose a more general domain adaptation setting, universal multi-source domain\nadaptation (UMDA), where the label sets of multiple source domains can be\ndifferent and the label set of target domain is completely unknown. The main\nchallenges in UMDA are to identify the common label set between each source\ndomain and target domain, and to keep the model scalable as the number of\nsource domains increases. To address these challenges, we propose a universal\nmulti-source adaptation network (UMAN) to solve the domain adaptation problem\nwithout increasing the complexity of the model in various UMDA settings. In\nUMAN, we estimate the reliability of each known class in the common label set\nvia the prediction margin, which helps adversarial training to better align the\ndistributions of multiple source domains and target domain in the common label\nset. Moreover, the theoretical guarantee for UMAN is also provided. Massive\nexperimental results show that existing UDA and multi-source DA (MDA) methods\ncannot be directly applied to UMDA and the proposed UMAN achieves the\nstate-of-the-art performance in various UMDA settings.\n", "versions": [{"version": "v1", "created": "Thu, 5 Nov 2020 00:20:38 GMT"}], "update_date": "2020-11-06", "authors_parsed": [["Yin", "Yueming", ""], ["Yang", "Zhen", ""], ["Hu", "Haifeng", ""], ["Wu", "Xiaofu", ""]]}, {"id": "2011.02606", "submitter": "Palakorn Achananuparp", "authors": "V N S Rama Krishna Pinnimty, Matt Zhao, Palakorn Achananuparp, and\n  Ee-Peng Lim", "title": "Transforming Facial Weight of Real Images by Editing Latent Space of\n  StyleGAN", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an invert-and-edit framework to automatically transform facial\nweight of an input face image to look thinner or heavier by leveraging semantic\nfacial attributes encoded in the latent space of Generative Adversarial\nNetworks (GANs). Using a pre-trained StyleGAN as the underlying generator, we\nfirst employ an optimization-based embedding method to invert the input image\ninto the StyleGAN latent space. Then, we identify the facial-weight attribute\ndirection in the latent space via supervised learning and edit the inverted\nlatent code by moving it positively or negatively along the extracted feature\naxis. Our framework is empirically shown to produce high-quality and realistic\nfacial-weight transformations without requiring training GANs with a large\namount of labeled face images from scratch. Ultimately, our framework can be\nutilized as part of an intervention to motivate individuals to make healthier\nfood choices by visualizing the future impacts of their behavior on appearance.\n", "versions": [{"version": "v1", "created": "Thu, 5 Nov 2020 01:45:18 GMT"}], "update_date": "2020-11-06", "authors_parsed": [["Pinnimty", "V N S Rama Krishna", ""], ["Zhao", "Matt", ""], ["Achananuparp", "Palakorn", ""], ["Lim", "Ee-Peng", ""]]}, {"id": "2011.02608", "submitter": "Jingxi Xu", "authors": "Huy Ha, Jingxi Xu, Shuran Song", "title": "Learning a Decentralized Multi-arm Motion Planner", "comments": "CoRL 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.CV cs.LG cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a closed-loop multi-arm motion planner that is scalable and\nflexible with team size. Traditional multi-arm robot systems have relied on\ncentralized motion planners, whose runtimes often scale exponentially with team\nsize, and thus, fail to handle dynamic environments with open-loop control. In\nthis paper, we tackle this problem with multi-agent reinforcement learning,\nwhere a decentralized policy is trained to control one robot arm in the\nmulti-arm system to reach its target end-effector pose given observations of\nits workspace state and target end-effector pose. The policy is trained using\nSoft Actor-Critic with expert demonstrations from a sampling-based motion\nplanning algorithm (i.e., BiRRT). By leveraging classical planning algorithms,\nwe can improve the learning efficiency of the reinforcement learning algorithm\nwhile retaining the fast inference time of neural networks. The resulting\npolicy scales sub-linearly and can be deployed on multi-arm systems with\nvariable team sizes. Thanks to the closed-loop and decentralized formulation,\nour approach generalizes to 5-10 multi-arm systems and dynamic moving targets\n(>90% success rate for a 10-arm system), despite being trained on only 1-4 arm\nplanning tasks with static targets. Code and data links can be found at\nhttps://multiarm.cs.columbia.edu.\n", "versions": [{"version": "v1", "created": "Thu, 5 Nov 2020 01:47:23 GMT"}], "update_date": "2020-11-06", "authors_parsed": [["Ha", "Huy", ""], ["Xu", "Jingxi", ""], ["Song", "Shuran", ""]]}, {"id": "2011.02631", "submitter": "Hao Zhu", "authors": "Hao Zhu, Yi Li, Feixia Zhu, Aihua Zheng, Ran He", "title": "Lets Play Music: Audio-driven Performance Video Generation", "comments": "ICPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new task named Audio-driven Per-formance Video Generation\n(APVG), which aims to synthesizethe video of a person playing a certain\ninstrument guided bya given music audio clip. It is a challenging task to\ngener-ate the high-dimensional temporal consistent videos from low-dimensional\naudio modality. In this paper, we propose a multi-staged framework to achieve\nthis new task to generate realisticand synchronized performance video from\ngiven music. Firstly,we provide both global appearance and local spatial\ninformationby generating the coarse videos and keypoints of body and handsfrom\na given music respectively. Then, we propose to transformthe generated\nkeypoints to heatmap via a differentiable spacetransformer, since the heatmap\noffers more spatial informationbut is harder to generate directly from audio.\nFinally, wepropose a Structured Temporal UNet (STU) to extract bothintra-frame\nstructured information and inter-frame temporalconsistency. They are obtained\nvia graph-based structure module,and CNN-GRU based high-level temporal module\nrespectively forfinal video generation. Comprehensive experiments validate\ntheeffectiveness of our proposed framework.\n", "versions": [{"version": "v1", "created": "Thu, 5 Nov 2020 03:13:46 GMT"}], "update_date": "2020-11-06", "authors_parsed": [["Zhu", "Hao", ""], ["Li", "Yi", ""], ["Zhu", "Feixia", ""], ["Zheng", "Aihua", ""], ["He", "Ran", ""]]}, {"id": "2011.02635", "submitter": "Jinglun Feng", "authors": "Jinglun Feng, Liang Yang, Ejup Hoxha, Diar Sanakov, Stanislav\n  Sotnikov, Jizhong Xiao", "title": "GPR-based Model Reconstruction System for Underground Utilities Using\n  GPRNet", "comments": "Accepted by ICRA 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Ground Penetrating Radar (GPR) is one of the most important non-destructive\nevaluation (NDE) instruments to detect and locate underground objects (i.e.,\nrebars, utility pipes). Many previous researches focus on GPR image-based\nfeature detection only, and none can process sparse GPR measurements to\nsuccessfully reconstruct a very fine and detailed 3D model of underground\nobjects for better visualization. To address this problem, this paper presents\na novel robotic system to collect GPR data, localize the underground utilities,\nand reconstruct the underground objects' dense point cloud model. This system\nis composed of three modules: 1) visual-inertial-based GPR data collection\nmodule, which tags the GPR measurements with positioning information provided\nby an omnidirectional robot; 2) a deep neural network (DNN) migration module to\ninterpret the raw GPR B-scan image into a cross-section of object model; 3) a\nDNN-based 3D reconstruction module, i.e., GPRNet, to generate underground\nutility model with the fine 3D point cloud. In this paper, both the\nquantitative and qualitative experiment results verify our method that can\ngenerate a dense and complete point cloud model of pipe-shaped utilities based\non a sparse input, i.e., GPR raw data incompleteness and various noise. The\nexperiment results on synthetic data and field test data further support the\neffectiveness of our approach.\n", "versions": [{"version": "v1", "created": "Thu, 5 Nov 2020 03:26:01 GMT"}, {"version": "v2", "created": "Wed, 5 May 2021 21:01:49 GMT"}, {"version": "v3", "created": "Tue, 18 May 2021 16:06:31 GMT"}], "update_date": "2021-05-19", "authors_parsed": [["Feng", "Jinglun", ""], ["Yang", "Liang", ""], ["Hoxha", "Ejup", ""], ["Sanakov", "Diar", ""], ["Sotnikov", "Stanislav", ""], ["Xiao", "Jizhong", ""]]}, {"id": "2011.02638", "submitter": "Kanglin Liu", "authors": "Kanglin Liu and Gaofeng Cao and Fei Zhou and Bozhi Liu and Jiang Duan\n  and Guoping Qiu", "title": "Towards Disentangling Latent Space for Unsupervised Semantic Face\n  Editing", "comments": "11pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Facial attributes in StyleGAN generated images are entangled in the latent\nspace which makes it very difficult to independently control a specific\nattribute without affecting the others. Supervised attribute editing requires\nannotated training data which is difficult to obtain and limits the editable\nattributes to those with labels. Therefore, unsupervised attribute editing in\nan disentangled latent space is key to performing neat and versatile semantic\nface editing. In this paper, we present a new technique termed\nStructure-Texture Independent Architecture with Weight Decomposition and\nOrthogonal Regularization (STIA-WO) to disentangle the latent space for\nunsupervised semantic face editing. By applying STIA-WO to GAN, we have\ndeveloped a StyleGAN termed STGAN-WO which performs weight decomposition\nthrough utilizing the style vector to construct a fully controllable weight\nmatrix to regulate image synthesis, and employs orthogonal regularization to\nensure each entry of the style vector only controls one independent feature\nmatrix. To further disentangle the facial attributes, STGAN-WO introduces a\nstructure-texture independent architecture which utilizes two independently and\nidentically distributed (i.i.d.) latent vectors to control the synthesis of the\ntexture and structure components in a disentangled way. Unsupervised semantic\nediting is achieved by moving the latent code in the coarse layers along its\northogonal directions to change texture related attributes or changing the\nlatent code in the fine layers to manipulate structure related ones. We present\nexperimental results which show that our new STGAN-WO can achieve better\nattribute editing than state of the art methods.\n", "versions": [{"version": "v1", "created": "Thu, 5 Nov 2020 03:29:24 GMT"}, {"version": "v2", "created": "Mon, 19 Jul 2021 01:21:52 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Liu", "Kanglin", ""], ["Cao", "Gaofeng", ""], ["Zhou", "Fei", ""], ["Liu", "Bozhi", ""], ["Duan", "Jiang", ""], ["Qiu", "Guoping", ""]]}, {"id": "2011.02655", "submitter": "Haidong Zhu", "authors": "Haidong Zhu, Arka Sadhu, Zhaoheng Zheng, Ram Nevatia", "title": "Utilizing Every Image Object for Semi-supervised Phrase Grounding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Phrase grounding models localize an object in the image given a referring\nexpression. The annotated language queries available during training are\nlimited, which also limits the variations of language combinations that a model\ncan see during training. In this paper, we study the case applying objects\nwithout labeled queries for training the semi-supervised phrase grounding. We\npropose to use learned location and subject embedding predictors (LSEP) to\ngenerate the corresponding language embeddings for objects lacking annotated\nqueries in the training set. With the assistance of the detector, we also apply\nLSEP to train a grounding model on images without any annotation. We evaluate\nour method based on MAttNet on three public datasets: RefCOCO, RefCOCO+, and\nRefCOCOg. We show that our predictors allow the grounding system to learn from\nthe objects without labeled queries and improve accuracy by 34.9\\% relatively\nwith the detection results.\n", "versions": [{"version": "v1", "created": "Thu, 5 Nov 2020 04:25:25 GMT"}], "update_date": "2020-11-06", "authors_parsed": [["Zhu", "Haidong", ""], ["Sadhu", "Arka", ""], ["Zheng", "Zhaoheng", ""], ["Nevatia", "Ram", ""]]}, {"id": "2011.02658", "submitter": "Akash Sharma", "authors": "Akash Sharma, Wei Dong, and Michael Kaess", "title": "Compositional Scalable Object SLAM", "comments": "Submitted to the 2021 IEEE International Conference on Robotics and\n  Automation (ICRA) 7 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a fast, scalable, and accurate Simultaneous Localization and\nMapping (SLAM) system that represents indoor scenes as a graph of objects.\nLeveraging the observation that artificial environments are structured and\noccupied by recognizable objects, we show that a compositional scalable object\nmapping formulation is amenable to a robust SLAM solution for drift-free large\nscale indoor reconstruction. To achieve this, we propose a novel semantically\nassisted data association strategy that obtains unambiguous persistent object\nlandmarks, and a 2.5D compositional rendering method that enables reliable\nframe-to-model RGB-D tracking. Consequently, we deliver an optimized online\nimplementation that can run at near frame rate with a single graphics card, and\nprovide a comprehensive evaluation against state of the art baselines. An open\nsource implementation will be provided at https://placeholder.\n", "versions": [{"version": "v1", "created": "Thu, 5 Nov 2020 04:46:25 GMT"}], "update_date": "2020-11-06", "authors_parsed": [["Sharma", "Akash", ""], ["Dong", "Wei", ""], ["Kaess", "Michael", ""]]}, {"id": "2011.02666", "submitter": "SeulGi Hong", "authors": "SeulGi Hong, Heonjin Ha, Junmo Kim, Min-Kook Choi", "title": "Deep Active Learning with Augmentation-based Consistency Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In active learning, the focus is mainly on the selection strategy of\nunlabeled data for enhancing the generalization capability of the next learning\ncycle. For this, various uncertainty measurement methods have been proposed. On\nthe other hand, with the advent of data augmentation metrics as the regularizer\non general deep learning, we notice that there can be a mutual influence\nbetween the method of unlabeled data selection and the data augmentation-based\nregularization techniques in active learning scenarios. Through various\nexperiments, we confirmed that consistency-based regularization from analytical\nlearning theory could affect the generalization capability of the classifier in\ncombination with the existing uncertainty measurement method. By this fact, we\npropose a methodology to improve generalization ability, by applying data\naugmentation-based techniques to an active learning scenario. For the data\naugmentation-based regularization loss, we redefined cutout (co) and cutmix\n(cm) strategies as quantitative metrics and applied at both model training and\nunlabeled data selection steps. We have shown that the augmentation-based\nregularizer can lead to improved performance on the training step of active\nlearning, while that same approach can be effectively combined with the\nuncertainty measurement metrics proposed so far. We used datasets such as\nFashionMNIST, CIFAR10, CIFAR100, and STL10 to verify the performance of the\nproposed active learning technique for multiple image classification tasks. Our\nexperiments show consistent performance gains for each dataset and budget\nscenario.\n", "versions": [{"version": "v1", "created": "Thu, 5 Nov 2020 05:22:58 GMT"}], "update_date": "2020-11-06", "authors_parsed": [["Hong", "SeulGi", ""], ["Ha", "Heonjin", ""], ["Kim", "Junmo", ""], ["Choi", "Min-Kook", ""]]}, {"id": "2011.02674", "submitter": "Hao Zhu", "authors": "Hao Zhu, Chaoyou Fu, Qianyi Wu, Wayne Wu, Chen Qian, Ran He", "title": "AOT: Appearance Optimal Transport Based Identity Swapping for Forgery\n  Detection", "comments": "NeurIPS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent studies have shown that the performance of forgery detection can be\nimproved with diverse and challenging Deepfakes datasets. However, due to the\nlack of Deepfakes datasets with large variance in appearance, which can be\nhardly produced by recent identity swapping methods, the detection algorithm\nmay fail in this situation. In this work, we provide a new identity swapping\nalgorithm with large differences in appearance for face forgery detection. The\nappearance gaps mainly arise from the large discrepancies in illuminations and\nskin colors that widely exist in real-world scenarios. However, due to the\ndifficulties of modeling the complex appearance mapping, it is challenging to\ntransfer fine-grained appearances adaptively while preserving identity traits.\nThis paper formulates appearance mapping as an optimal transport problem and\nproposes an Appearance Optimal Transport model (AOT) to formulate it in both\nlatent and pixel space. Specifically, a relighting generator is designed to\nsimulate the optimal transport plan. It is solved via minimizing the\nWasserstein distance of the learned features in the latent space, enabling\nbetter performance and less computation than conventional optimization. To\nfurther refine the solution of the optimal transport plan, we develop a\nsegmentation game to minimize the Wasserstein distance in the pixel space. A\ndiscriminator is introduced to distinguish the fake parts from a mix of real\nand fake image patches. Extensive experiments reveal that the superiority of\nour method when compared with state-of-the-art methods and the ability of our\ngenerated data to improve the performance of face forgery detection.\n", "versions": [{"version": "v1", "created": "Thu, 5 Nov 2020 06:17:04 GMT"}], "update_date": "2020-11-06", "authors_parsed": [["Zhu", "Hao", ""], ["Fu", "Chaoyou", ""], ["Wu", "Qianyi", ""], ["Wu", "Wayne", ""], ["Qian", "Chen", ""], ["He", "Ran", ""]]}, {"id": "2011.02675", "submitter": "Camilo Pestana", "authors": "Camilo Pestana, Wei Liu, David Glance, Ajmal Mian", "title": "Defense-friendly Images in Adversarial Attacks: Dataset and Metrics for\n  Perturbation Difficulty", "comments": "Paper Accepted at WACV 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dataset bias is a problem in adversarial machine learning, especially in the\nevaluation of defenses. An adversarial attack or defense algorithm may show\nbetter results on the reported dataset than can be replicated on other\ndatasets. Even when two algorithms are compared, their relative performance can\nvary depending on the dataset. Deep learning offers state-of-the-art solutions\nfor image recognition, but deep models are vulnerable even to small\nperturbations. Research in this area focuses primarily on adversarial attacks\nand defense algorithms. In this paper, we report for the first time, a class of\nrobust images that are both resilient to attacks and that recover better than\nrandom images under adversarial attacks using simple defense techniques. Thus,\na test dataset with a high proportion of robust images gives a misleading\nimpression about the performance of an adversarial attack or defense. We\npropose three metrics to determine the proportion of robust images in a dataset\nand provide scoring to determine the dataset bias. We also provide an\nImageNet-R dataset of 15000+ robust images to facilitate further research on\nthis intriguing phenomenon of image strength under attack. Our dataset,\ncombined with the proposed metrics, is valuable for unbiased benchmarking of\nadversarial attack and defense algorithms.\n", "versions": [{"version": "v1", "created": "Thu, 5 Nov 2020 06:21:24 GMT"}, {"version": "v2", "created": "Sat, 7 Nov 2020 02:57:50 GMT"}], "update_date": "2020-11-10", "authors_parsed": [["Pestana", "Camilo", ""], ["Liu", "Wei", ""], ["Glance", "David", ""], ["Mian", "Ajmal", ""]]}, {"id": "2011.02679", "submitter": "Jiayun Li", "authors": "Jiayun Li, Wenyuan Li, Anthony Sisk, Huihui Ye, W. Dean Wallace,\n  William Speier, Corey W. Arnold", "title": "A Multi-resolution Model for Histopathology Image Classification and\n  Localization with Multiple Instance Learning", "comments": "9 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Histopathological images provide rich information for disease diagnosis.\nLarge numbers of histopathological images have been digitized into high\nresolution whole slide images, opening opportunities in developing\ncomputational image analysis tools to reduce pathologists' workload and\npotentially improve inter- and intra- observer agreement. Most previous work on\nwhole slide image analysis has focused on classification or segmentation of\nsmall pre-selected regions-of-interest, which requires fine-grained annotation\nand is non-trivial to extend for large-scale whole slide analysis. In this\npaper, we proposed a multi-resolution multiple instance learning model that\nleverages saliency maps to detect suspicious regions for fine-grained grade\nprediction. Instead of relying on expensive region- or pixel-level annotations,\nour model can be trained end-to-end with only slide-level labels. The model is\ndeveloped on a large-scale prostate biopsy dataset containing 20,229 slides\nfrom 830 patients. The model achieved 92.7% accuracy, 81.8% Cohen's Kappa for\nbenign, low grade (i.e. Grade group 1) and high grade (i.e. Grade group >= 2)\nprediction, an area under the receiver operating characteristic curve (AUROC)\nof 98.2% and an average precision (AP) of 97.4% for differentiating malignant\nand benign slides. The model obtained an AUROC of 99.4% and an AP of 99.8% for\ncancer detection on an external dataset.\n", "versions": [{"version": "v1", "created": "Thu, 5 Nov 2020 06:42:39 GMT"}], "update_date": "2020-11-06", "authors_parsed": [["Li", "Jiayun", ""], ["Li", "Wenyuan", ""], ["Sisk", "Anthony", ""], ["Ye", "Huihui", ""], ["Wallace", "W. Dean", ""], ["Speier", "William", ""], ["Arnold", "Corey W.", ""]]}, {"id": "2011.02697", "submitter": "Hao Li", "authors": "Hao Li, Xiaopeng Zhang, Ruoyu Sun, Hongkai Xiong and Qi Tian", "title": "Center-wise Local Image Mixture For Contrastive Representation Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recent advances in unsupervised representation learning have experienced\nremarkable progress, especially with the achievements of contrastive learning,\nwhich regards each image as well its augmentations as a separate class, while\ndoes not consider the semantic similarity among images. This paper proposes a\nnew kind of data augmentation, named Center-wise Local Image Mixture, to expand\nthe neighborhood space of an image. CLIM encourages both local similarity and\nglobal aggregation while pulling similar images. This is achieved by searching\nlocal similar samples of an image, and only selecting images that are closer to\nthe corresponding cluster center, which we denote as center-wise local\nselection. As a result, similar representations are progressively approaching\nthe clusters, while do not break the local similarity. Furthermore, image\nmixture is used as a smoothing regularization to avoid overconfidence on the\nselected samples. Besides, we introduce multi-resolution augmentation, which\nenables the representation to be scale invariant. Integrating the two\naugmentations produces better feature representation on several unsupervised\nbenchmarks. Notably, we reach 75.5% top-1 accuracy with linear evaluation over\nResNet-50, and 59.3% top-1 accuracy when fine-tuned with only 1% labels, as\nwell as consistently outperforming supervised pretraining on several downstream\ntransfer tasks.\n", "versions": [{"version": "v1", "created": "Thu, 5 Nov 2020 08:20:31 GMT"}, {"version": "v2", "created": "Fri, 27 Nov 2020 09:17:24 GMT"}], "update_date": "2020-11-30", "authors_parsed": [["Li", "Hao", ""], ["Zhang", "Xiaopeng", ""], ["Sun", "Ruoyu", ""], ["Xiong", "Hongkai", ""], ["Tian", "Qi", ""]]}, {"id": "2011.02709", "submitter": "Zhenxing Zhang", "authors": "Zhenxing Zhang and Lambert Schomaker", "title": "DTGAN: Dual Attention Generative Adversarial Networks for Text-to-Image\n  Generation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most existing text-to-image generation methods adopt a multi-stage modular\narchitecture which has three significant problems: 1) Training multiple\nnetworks increases the run time and affects the convergence and stability of\nthe generative model; 2) These approaches ignore the quality of early-stage\ngenerator images; 3) Many discriminators need to be trained. To this end, we\npropose the Dual Attention Generative Adversarial Network (DTGAN) which can\nsynthesize high-quality and semantically consistent images only employing a\nsingle generator/discriminator pair. The proposed model introduces\nchannel-aware and pixel-aware attention modules that can guide the generator to\nfocus on text-relevant channels and pixels based on the global sentence vector\nand to fine-tune original feature maps using attention weights. Also,\nConditional Adaptive Instance-Layer Normalization (CAdaILN) is presented to\nhelp our attention modules flexibly control the amount of change in shape and\ntexture by the input natural-language description. Furthermore, a new type of\nvisual loss is utilized to enhance the image resolution by ensuring vivid shape\nand perceptually uniform color distributions of generated images. Experimental\nresults on benchmark datasets demonstrate the superiority of our proposed\nmethod compared to the state-of-the-art models with a multi-stage framework.\nVisualization of the attention maps shows that the channel-aware attention\nmodule is able to localize the discriminative regions, while the pixel-aware\nattention module has the ability to capture the globally visual contents for\nthe generation of an image.\n", "versions": [{"version": "v1", "created": "Thu, 5 Nov 2020 08:57:15 GMT"}, {"version": "v2", "created": "Mon, 16 Nov 2020 09:43:19 GMT"}, {"version": "v3", "created": "Sat, 21 Nov 2020 23:59:25 GMT"}], "update_date": "2020-11-24", "authors_parsed": [["Zhang", "Zhenxing", ""], ["Schomaker", "Lambert", ""]]}, {"id": "2011.02719", "submitter": "Suiyi Ling", "authors": "Kevin Riou, Jingwen Zhu, Suiyi Ling, Mathis Piquet, Vincent Truffault,\n  Patrick Le Callet", "title": "Few-Shot Object Detection in Real Life: Case Study on Auto-Harvest", "comments": "6 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Confinement during COVID-19 has caused serious effects on agriculture all\nover the world. As one of the efficient solutions, mechanical\nharvest/auto-harvest that is based on object detection and robotic harvester\nbecomes an urgent need. Within the auto-harvest system, robust few-shot object\ndetection model is one of the bottlenecks, since the system is required to deal\nwith new vegetable/fruit categories and the collection of large-scale annotated\ndatasets for all the novel categories is expensive. There are many few-shot\nobject detection models that were developed by the community. Yet whether they\ncould be employed directly for real life agricultural applications is still\nquestionable, as there is a context-gap between the commonly used training\ndatasets and the images collected in real life agricultural scenarios. To this\nend, in this study, we present a novel cucumber dataset and propose two data\naugmentation strategies that help to bridge the context-gap. Experimental\nresults show that 1) the state-of-the-art few-shot object detection model\nperforms poorly on the novel `cucumber' category; and 2) the proposed\naugmentation strategies outperform the commonly used ones.\n", "versions": [{"version": "v1", "created": "Thu, 5 Nov 2020 09:24:33 GMT"}], "update_date": "2020-11-06", "authors_parsed": [["Riou", "Kevin", ""], ["Zhu", "Jingwen", ""], ["Ling", "Suiyi", ""], ["Piquet", "Mathis", ""], ["Truffault", "Vincent", ""], ["Callet", "Patrick Le", ""]]}, {"id": "2011.02727", "submitter": "Nicolas Gonthier", "authors": "Nicolas Gonthier and Yann Gousseau and Sa\\\"id Ladjal", "title": "An analysis of the transfer learning of convolutional neural networks\n  for artistic images", "comments": "Accepted at Workshop on Fine Art Pattern Extraction and Recognition\n  (FAPER), ICPR, 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transfer learning from huge natural image datasets, fine-tuning of deep\nneural networks and the use of the corresponding pre-trained networks have\nbecome de facto the core of art analysis applications. Nevertheless, the\neffects of transfer learning are still poorly understood. In this paper, we\nfirst use techniques for visualizing the network internal representations in\norder to provide clues to the understanding of what the network has learned on\nartistic images. Then, we provide a quantitative analysis of the changes\nintroduced by the learning process thanks to metrics in both the feature and\nparameter spaces, as well as metrics computed on the set of maximal activation\nimages. These analyses are performed on several variations of the transfer\nlearning procedure. In particular, we observed that the network could\nspecialize some pre-trained filters to the new image modality and also that\nhigher layers tend to concentrate classes. Finally, we have shown that a double\nfine-tuning involving a medium-size artistic dataset can improve the\nclassification on smaller datasets, even when the task changes.\n", "versions": [{"version": "v1", "created": "Thu, 5 Nov 2020 09:45:32 GMT"}, {"version": "v2", "created": "Tue, 24 Nov 2020 13:18:23 GMT"}], "update_date": "2020-11-25", "authors_parsed": [["Gonthier", "Nicolas", ""], ["Gousseau", "Yann", ""], ["Ladjal", "Sa\u00efd", ""]]}, {"id": "2011.02751", "submitter": "Hung Tran", "authors": "Hung Tran, Vuong Le, Truyen Tran", "title": "Goal-driven Long-Term Trajectory Prediction", "comments": "Accepted at WACV 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The prediction of humans' short-term trajectories has advanced significantly\nwith the use of powerful sequential modeling and rich environment feature\nextraction. However, long-term prediction is still a major challenge for the\ncurrent methods as the errors could accumulate along the way. Indeed,\nconsistent and stable prediction far to the end of a trajectory inherently\nrequires deeper analysis into the overall structure of that trajectory, which\nis related to the pedestrian's intention on the destination of the journey. In\nthis work, we propose to model a hypothetical process that determines\npedestrians' goals and the impact of such process on long-term future\ntrajectories. We design Goal-driven Trajectory Prediction model - a\ndual-channel neural network that realizes such intuition. The two channels of\nthe network take their dedicated roles and collaborate to generate future\ntrajectories. Different than conventional goal-conditioned, planning-based\nmethods, the model architecture is designed to generalize the patterns and work\nacross different scenes with arbitrary geometrical and semantic structures. The\nmodel is shown to outperform the state-of-the-art in various settings,\nespecially in large prediction horizons. This result is another evidence for\nthe effectiveness of adaptive structured representation of visual and\ngeometrical features in human behavior analysis.\n", "versions": [{"version": "v1", "created": "Thu, 5 Nov 2020 10:47:33 GMT"}, {"version": "v2", "created": "Fri, 6 Nov 2020 04:11:11 GMT"}], "update_date": "2020-11-09", "authors_parsed": [["Tran", "Hung", ""], ["Le", "Vuong", ""], ["Tran", "Truyen", ""]]}, {"id": "2011.02763", "submitter": "Zhengping Che", "authors": "Xuanzhao Wang, Zhengping Che, Bo Jiang, Ning Xiao, Ke Yang, Jian Tang,\n  Jieping Ye, Jingyu Wang, Qi Qi", "title": "Robust Unsupervised Video Anomaly Detection by Multi-Path Frame\n  Prediction", "comments": "Paper accepted by IEEE Transactions on Neural Networks and Learning\n  Systems (TNNLS). Article DOI: 10.1109/TNNLS.2021.3083152", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video anomaly detection is commonly used in many applications such as\nsecurity surveillance and is very challenging.A majority of recent video\nanomaly detection approaches utilize deep reconstruction models, but their\nperformance is often suboptimal because of insufficient reconstruction error\ndifferences between normal and abnormal video frames in practice. Meanwhile,\nframe prediction-based anomaly detection methods have shown promising\nperformance. In this paper, we propose a novel and robust unsupervised video\nanomaly detection method by frame prediction with proper design which is more\nin line with the characteristics of surveillance videos. The proposed method is\nequipped with a multi-path ConvGRU-based frame prediction network that can\nbetter handle semantically informative objects and areas of different scales\nand capture spatial-temporal dependencies in normal videos. A noise tolerance\nloss is introduced during training to mitigate the interference caused by\nbackground noise. Extensive experiments have been conducted on the CUHK Avenue,\nShanghaiTech Campus, and UCSD Pedestrian datasets, and the results show that\nour proposed method outperforms existing state-of-the-art approaches.\nRemarkably, our proposed method obtains the frame-level AUROC score of 88.3% on\nthe CUHK Avenue dataset.\n", "versions": [{"version": "v1", "created": "Thu, 5 Nov 2020 11:34:12 GMT"}, {"version": "v2", "created": "Thu, 27 May 2021 05:53:57 GMT"}], "update_date": "2021-05-28", "authors_parsed": [["Wang", "Xuanzhao", ""], ["Che", "Zhengping", ""], ["Jiang", "Bo", ""], ["Xiao", "Ning", ""], ["Yang", "Ke", ""], ["Tang", "Jian", ""], ["Ye", "Jieping", ""], ["Wang", "Jingyu", ""], ["Qi", "Qi", ""]]}, {"id": "2011.02780", "submitter": "Bo Jiang", "authors": "Yue Shi, Bo Jiang, Zhengping Che, Jian Tang", "title": "Fast Object Detection with Latticed Multi-Scale Feature Fusion", "comments": "The first two authors contributed equally to this work", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scale variance is one of the crucial challenges in multi-scale object\ndetection. Early approaches address this problem by exploiting the image and\nfeature pyramid, which raises suboptimal results with computation burden and\nconstrains from inherent network structures. Pioneering works also propose\nmulti-scale (i.e., multi-level and multi-branch) feature fusions to remedy the\nissue and have achieved encouraging progress. However, existing fusions still\nhave certain limitations such as feature scale inconsistency, ignorance of\nlevel-wise semantic transformation, and coarse granularity. In this work, we\npresent a novel module, the Fluff block, to alleviate drawbacks of current\nmulti-scale fusion methods and facilitate multi-scale object detection.\nSpecifically, Fluff leverages both multi-level and multi-branch schemes with\ndilated convolutions to have rapid, effective and finer-grained feature\nfusions. Furthermore, we integrate Fluff to SSD as FluffNet, a powerful\nreal-time single-stage detector for multi-scale object detection. Empirical\nresults on MS COCO and PASCAL VOC have demonstrated that FluffNet obtains\nremarkable efficiency with state-of-the-art accuracy. Additionally, we indicate\nthe great generality of the Fluff block by showing how to embed it to other\nwidely-used detectors as well.\n", "versions": [{"version": "v1", "created": "Thu, 5 Nov 2020 12:16:30 GMT"}], "update_date": "2020-11-06", "authors_parsed": [["Shi", "Yue", ""], ["Jiang", "Bo", ""], ["Che", "Zhengping", ""], ["Tang", "Jian", ""]]}, {"id": "2011.02785", "submitter": "Yingming Li", "authors": "Dingyi Zhang, Yingming Li, Zhongfei Zhang", "title": "Deep Metric Learning with Spherical Embedding", "comments": "To appear in NeurIPS 2020. Code is available at\n  https://github.com/Dyfine/SphericalEmbedding", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep metric learning has attracted much attention in recent years, due to\nseamlessly combining the distance metric learning and deep neural network. Many\nendeavors are devoted to design different pair-based angular loss functions,\nwhich decouple the magnitude and direction information for embedding vectors\nand ensure the training and testing measure consistency. However, these\ntraditional angular losses cannot guarantee that all the sample embeddings are\non the surface of the same hypersphere during the training stage, which would\nresult in unstable gradient in batch optimization and may influence the quick\nconvergence of the embedding learning. In this paper, we first investigate the\neffect of the embedding norm for deep metric learning with angular distance,\nand then propose a spherical embedding constraint (SEC) to regularize the\ndistribution of the norms. SEC adaptively adjusts the embeddings to fall on the\nsame hypersphere and performs more balanced direction update. Extensive\nexperiments on deep metric learning, face recognition, and contrastive\nself-supervised learning show that the SEC-based angular space learning\nstrategy significantly improves the performance of the state-of-the-art.\n", "versions": [{"version": "v1", "created": "Thu, 5 Nov 2020 12:32:12 GMT"}], "update_date": "2020-11-06", "authors_parsed": [["Zhang", "Dingyi", ""], ["Li", "Yingming", ""], ["Zhang", "Zhongfei", ""]]}, {"id": "2011.02803", "submitter": "Ting Chen", "authors": "Ting Chen and Calvin Luo and Lala Li", "title": "Intriguing Properties of Contrastive Losses", "comments": "Technical report. Code at\n  https://github.com/google-research/simclr/tree/master/colabs/intriguing_properties", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Contrastive loss and its variants have become very popular recently for\nlearning visual representations without supervision. In this work, we study\nthree intriguing properties of contrastive learning. We first generalize the\nstandard contrastive loss to a broader family of losses, and we find that\nvarious instantiations of the generalized loss perform similarly under the\npresence of a multi-layer non-linear projection head. We then study if\ninstance-based contrastive learning (such as in SimCLR, MoCo, BYOL, and so on,\nwhich are based on global image representation) can learn well on images with\nmultiple objects present. We find that meaningful hierarchical local features\ncan be learned despite the fact that these objectives operate on global\ninstance-level features.\n  Finally, we study an intriguing phenomenon of feature suppression among\ncompeting features shared across augmented views, such as \"color distribution\"\nvs \"object class\". We construct datasets with explicit and controllable\ncompeting features, and show that, for contrastive learning, a few bits of\neasy-to-learn shared features can suppress, and even fully prevent, the\nlearning of other sets of competing features. In scenarios where there are\nmultiple objects in an image, the dominant object would suppress the learning\nof smaller objects. Existing contrastive learning methods critically rely on\ndata augmentation to favor certain sets of features over others, and face\npotential limitation for scenarios where existing augmentations cannot fully\naddress the feature suppression. This poses open challenges to existing\ncontrastive learning techniques.\n", "versions": [{"version": "v1", "created": "Thu, 5 Nov 2020 13:19:48 GMT"}, {"version": "v2", "created": "Sat, 19 Jun 2021 01:17:22 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Chen", "Ting", ""], ["Luo", "Calvin", ""], ["Li", "Lala", ""]]}, {"id": "2011.02840", "submitter": "Jordan Colman", "authors": "Jordan Colman, Lei Zhang, Wenting Duan and Xujiong Ye", "title": "DR-Unet104 for Multimodal MRI brain tumor segmentation", "comments": "Part of the Multimodal Brain Tumor Segmentation 2020 Challenge\n  conference proceedings", "journal-ref": "BrainLes 2020. Lecture Notes in Computer Science, vol 12659, pp\n  410-419", "doi": "10.1007/978-3-030-72087-2_36", "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose a 2D deep residual Unet with 104 convolutional\nlayers (DR-Unet104) for lesion segmentation in brain MRIs. We make multiple\nadditions to the Unet architecture, including adding the 'bottleneck' residual\nblock to the Unet encoder and adding dropout after each convolution block\nstack. We verified the effect of introducing the regularisation of dropout with\nsmall rate (e.g. 0.2) on the architecture, and found a dropout of 0.2 improved\nthe overall performance compared to no dropout, or a dropout of 0.5. We\nevaluated the proposed architecture as part of the Multimodal Brain Tumor\nSegmentation (BraTS) 2020 Challenge and compared our method to DeepLabV3+ with\na ResNet-V2-152 backbone. We found that the DR-Unet104 achieved a mean dice\nscore coefficient of 0.8862, 0.6756 and 0.6721 for validation data, whole\ntumor, enhancing tumor and tumor core respectively, an overall improvement on\n0.8770, 0.65242 and 0.68134 achieved by DeepLabV3+. Our method produced a final\nmean DSC of 0.8673, 0.7514 and 0.7983 on whole tumor, enhancing tumor and tumor\ncore on the challenge's testing data. We produced a competitive lesion\nsegmentation architecture, despite only 2D convolutions, having the added\nbenefit that it can be used on lower power computers than a 3D architecture.\nThe source code and trained model for this work is openly available at\nhttps://github.com/jordan-colman/DR-Unet104.\n", "versions": [{"version": "v1", "created": "Wed, 4 Nov 2020 01:24:26 GMT"}, {"version": "v2", "created": "Tue, 4 May 2021 14:25:49 GMT"}], "update_date": "2021-05-05", "authors_parsed": [["Colman", "Jordan", ""], ["Zhang", "Lei", ""], ["Duan", "Wenting", ""], ["Ye", "Xujiong", ""]]}, {"id": "2011.02853", "submitter": "Ilker Bozcan", "authors": "Ilker Bozcan and Erdal Kayacan", "title": "UAV-AdNet: Unsupervised Anomaly Detection using Deep Neural Networks for\n  Aerial Surveillance", "comments": "7 pages, 4 figures, 2 tables, accepted to the 2020 IEEE/RSJ\n  International Conference on Intelligent Robots and Systems (IROS 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Anomaly detection is a key goal of autonomous surveillance systems that\nshould be able to alert unusual observations. In this paper, we propose a\nholistic anomaly detection system using deep neural networks for surveillance\nof critical infrastructures (e.g., airports, harbors, warehouses) using an\nunmanned aerial vehicle (UAV). First, we present a heuristic method for the\nexplicit representation of spatial layouts of objects in bird-view images.\nThen, we propose a deep neural network architecture for unsupervised anomaly\ndetection (UAV-AdNet), which is trained on environment representations and GPS\nlabels of bird-view images jointly. Unlike studies in the literature, we\ncombine GPS and image data to predict abnormal observations. We evaluate our\nmodel against several baselines on our aerial surveillance dataset and show\nthat it performs better in scene reconstruction and several anomaly detection\ntasks. The codes, trained models, dataset, and video will be available at\nhttps://bozcani.github.io/uavadnet.\n", "versions": [{"version": "v1", "created": "Thu, 5 Nov 2020 14:26:29 GMT"}], "update_date": "2020-11-06", "authors_parsed": [["Bozcan", "Ilker", ""], ["Kayacan", "Erdal", ""]]}, {"id": "2011.02863", "submitter": "Meike Nauta", "authors": "Meike Nauta, Annemarie Jutte, Jesper Provoost, Christin Seifert", "title": "This Looks Like That, Because ... Explaining Prototypes for\n  Interpretable Image Recognition", "comments": "10 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image recognition with prototypes is considered an interpretable alternative\nfor black box deep learning models. Classification depends on the extent to\nwhich a test image \"looks like\" a prototype. However, perceptual similarity for\nhumans can be different from the similarity learned by the classification\nmodel. Hence, only visualising prototypes can be insufficient for a user to\nunderstand what a prototype exactly represents, and why the model considers a\nprototype and an image to be similar. We address this ambiguity and argue that\nprototypes should be explained. We improve interpretability by automatically\nenhancing visual prototypes with textual quantitative information about visual\ncharacteristics deemed important by the classification model. Specifically, our\nmethod clarifies the meaning of a prototype by quantifying the influence of\ncolour hue, shape, texture, contrast and saturation and can generate both\nglobal and local explanations. Because of the generality of our approach, it\ncan improve the interpretability of any similarity-based method for\nprototypical image recognition. In our experiments, we apply our method to the\nexisting Prototypical Part Network (ProtoPNet). Our analysis confirms that the\nglobal explanations are generalisable, and often correspond to the visually\nperceptible properties of a prototype. Our explanations are especially relevant\nfor prototypes which might have been interpreted incorrectly otherwise. By\nexplaining such 'misleading' prototypes, we improve the interpretability and\nsimulatability of a prototype-based classification model. We also use our\nmethod to check whether visually similar prototypes have similar explanations,\nand are able to discover redundancy. Code is available at\nhttps://github.com/M-Nauta/Explaining_Prototypes .\n", "versions": [{"version": "v1", "created": "Thu, 5 Nov 2020 14:43:07 GMT"}, {"version": "v2", "created": "Wed, 31 Mar 2021 07:13:23 GMT"}], "update_date": "2021-04-01", "authors_parsed": [["Nauta", "Meike", ""], ["Jutte", "Annemarie", ""], ["Provoost", "Jesper", ""], ["Seifert", "Christin", ""]]}, {"id": "2011.02879", "submitter": "Mehdi Khoshboresh-Masouleh", "authors": "Mehdi Khoshboresh-Masouleh, Mohammad R. Saradjian", "title": "Robust building footprint extraction from big multi-sensor data using\n  deep competition network", "comments": "8 pages, 5 figures", "journal-ref": "The International Archives of the Photogrammetry, Remote Sensing\n  and Spatial Information Sciences, Volume XLII-4/W18, 2019", "doi": "10.5194/isprs-archives-XLII-4-W18-615-2019", "report-no": null, "categories": "cs.CV eess.IV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Building footprint extraction (BFE) from multi-sensor data such as optical\nimages and light detection and ranging (LiDAR) point clouds is widely used in\nvarious fields of remote sensing applications. However, it is still challenging\nresearch topic due to relatively inefficient building extraction techniques\nfrom variety of complex scenes in multi-sensor data. In this study, we develop\nand evaluate a deep competition network (DCN) that fuses very high spatial\nresolution optical remote sensing images with LiDAR data for robust BFE. DCN is\na deep superpixelwise convolutional encoder-decoder architecture using the\nencoder vector quantization with classified structure. DCN consists of five\nencoding-decoding blocks with convolutional weights for robust binary\nrepresentation (superpixel) learning. DCN is trained and tested in a big\nmulti-sensor dataset obtained from the state of Indiana in the United States\nwith multiple building scenes. Comparison results of the accuracy assessment\nshowed that DCN has competitive BFE performance in comparison with other deep\nsemantic binary segmentation architectures. Therefore, we conclude that the\nproposed model is a suitable solution to the robust BFE from big multi-sensor\ndata.\n", "versions": [{"version": "v1", "created": "Wed, 4 Nov 2020 09:04:38 GMT"}, {"version": "v2", "created": "Mon, 16 Nov 2020 09:11:12 GMT"}, {"version": "v3", "created": "Sat, 28 Nov 2020 13:06:36 GMT"}], "update_date": "2020-12-01", "authors_parsed": [["Khoshboresh-Masouleh", "Mehdi", ""], ["Saradjian", "Mohammad R.", ""]]}, {"id": "2011.02880", "submitter": "Bochuan Zheng", "authors": "Haijun Gao, Bochuan Zheng, Dazhi Pan, Xiangyin Zeng", "title": "Covariance Self-Attention Dual Path UNet for Rectal Tumor Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning algorithms are preferable for rectal tumor segmentation.\nHowever, it is still a challenge task to accurately segment and identify the\nlocations and sizes of rectal tumors by using deep learning methods. To\nincrease the capability of extracting enough feature information for rectal\ntumor segmentation, we propose a Covariance Self-Attention Dual Path UNet\n(CSA-DPUNet). The proposed network mainly includes two improvements on UNet: 1)\nmodify UNet that has only one path structure to consist of two contracting path\nand two expansive paths (nam new network as DPUNet), which can help extract\nmore feature information from CT images; 2) employ the criss-cross\nself-attention module into DPUNet, meanwhile, replace the original calculation\nmethod of correlation operation with covariance operation, which can further\nenhances the characterization ability of DPUNet and improves the segmentation\naccuracy of rectal tumors. Experiments illustrate that compared with the\ncurrent state-of-the-art results, CSA-DPUNet brings 15.31%, 7.2%, 11.8%, and\n9.5% improvement in Dice coefficient, P, R, F1, respectively, which\ndemonstrates that our proposed CSA-DPUNet is effective for rectal tumor\nsegmentation.\n", "versions": [{"version": "v1", "created": "Wed, 4 Nov 2020 08:01:19 GMT"}, {"version": "v2", "created": "Tue, 5 Jan 2021 09:38:09 GMT"}], "update_date": "2021-01-06", "authors_parsed": [["Gao", "Haijun", ""], ["Zheng", "Bochuan", ""], ["Pan", "Dazhi", ""], ["Zeng", "Xiangyin", ""]]}, {"id": "2011.02881", "submitter": "Hai Shu", "authors": "Chenggang Lyu, Hai Shu", "title": "A Two-Stage Cascade Model with Variational Autoencoders and Attention\n  Gates for MRI Brain Tumor Segmentation", "comments": null, "journal-ref": "Brainlesion: Glioma, Multiple Sclerosis, Stroke and Traumatic\n  Brain Injuries (BrainLes 2020)", "doi": "10.1007/978-3-030-72084-1_39", "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic MRI brain tumor segmentation is of vital importance for the disease\ndiagnosis, monitoring, and treatment planning. In this paper, we propose a\ntwo-stage encoder-decoder based model for brain tumor subregional segmentation.\nVariational autoencoder regularization is utilized in both stages to prevent\nthe overfitting issue. The second-stage network adopts attention gates and is\ntrained additionally using an expanded dataset formed by the first-stage\noutputs. On the BraTS 2020 validation dataset, the proposed method achieves the\nmean Dice score of 0.9041, 0.8350, and 0.7958, and Hausdorff distance (95%) of\n4.953, 6.299, and 23.608 for the whole tumor, tumor core, and enhancing tumor,\nrespectively. The corresponding results on the BraTS 2020 testing dataset are\n0.8729, 0.8357, and 0.8205 for Dice score, and 11.4288, 19.9690, and 15.6711\nfor Hausdorff distance. The code is publicly available at\nhttps://github.com/shu-hai/two-stage-VAE-Attention-gate-BraTS2020.\n", "versions": [{"version": "v1", "created": "Wed, 4 Nov 2020 05:55:06 GMT"}, {"version": "v2", "created": "Sat, 28 Nov 2020 06:48:15 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Lyu", "Chenggang", ""], ["Shu", "Hai", ""]]}, {"id": "2011.02888", "submitter": "William Prew", "authors": "William Prew, Toby Breckon, Magnus Bordewich, Ulrik Beierholm", "title": "Improving Robotic Grasping on Monocular Images Via Multi-Task Learning\n  and Positional Loss", "comments": "8 pages, 6 figures, Accepted at the International Conference on\n  Pattern Recognition 2020 (ICPR)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce two methods of improving real-time object\ngrasping performance from monocular colour images in an end-to-end CNN\narchitecture. The first is the addition of an auxiliary task during model\ntraining (multi-task learning). Our multi-task CNN model improves grasping\nperformance from a baseline average of 72.04% to 78.14% on the large Jacquard\ngrasping dataset when performing a supplementary depth reconstruction task. The\nsecond is introducing a positional loss function that emphasises loss per pixel\nfor secondary parameters (gripper angle and width) only on points of an object\nwhere a successful grasp can take place. This increases performance from a\nbaseline average of 72.04% to 78.92% as well as reducing the number of training\nepochs required. These methods can be also performed in tandem resulting in a\nfurther performance increase to 79.12% while maintaining sufficient inference\nspeed to afford real-time grasp processing.\n", "versions": [{"version": "v1", "created": "Thu, 5 Nov 2020 14:58:30 GMT"}], "update_date": "2020-11-06", "authors_parsed": [["Prew", "William", ""], ["Breckon", "Toby", ""], ["Bordewich", "Magnus", ""], ["Beierholm", "Ulrik", ""]]}, {"id": "2011.02904", "submitter": "Gourav Wadhwa", "authors": "Gourav Wadhwa, Abhinav Dhall, Subrahmanyam Murala, Usman Tariq", "title": "Hyperrealistic Image Inpainting with Hypergraphs", "comments": "Accepted at IEEE Winter Conference on Applications of Computer Vision\n  (WACV), 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image inpainting is a non-trivial task in computer vision due to multiple\npossibilities for filling the missing data, which may be dependent on the\nglobal information of the image. Most of the existing approaches use the\nattention mechanism to learn the global context of the image. This attention\nmechanism produces semantically plausible but blurry results because of\nincapability to capture the global context. In this paper, we introduce\nhypergraph convolution on spatial features to learn the complex relationship\namong the data. We introduce a trainable mechanism to connect nodes using\nhyperedges for hypergraph convolution. To the best of our knowledge, hypergraph\nconvolution have never been used on spatial features for any image-to-image\ntasks in computer vision. Further, we introduce gated convolution in the\ndiscriminator to enforce local consistency in the predicted image. The\nexperiments on Places2, CelebA-HQ, Paris Street View, and Facades datasets,\nshow that our approach achieves state-of-the-art results.\n", "versions": [{"version": "v1", "created": "Thu, 5 Nov 2020 15:28:57 GMT"}], "update_date": "2020-11-06", "authors_parsed": [["Wadhwa", "Gourav", ""], ["Dhall", "Abhinav", ""], ["Murala", "Subrahmanyam", ""], ["Tariq", "Usman", ""]]}, {"id": "2011.02910", "submitter": "Zhaoshuo Li", "authors": "Zhaoshuo Li, Xingtong Liu, Nathan Drenkow, Andy Ding, Francis X.\n  Creighton, Russell H. Taylor, Mathias Unberath", "title": "Revisiting Stereo Depth Estimation From a Sequence-to-Sequence\n  Perspective with Transformers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stereo depth estimation relies on optimal correspondence matching between\npixels on epipolar lines in the left and right images to infer depth. In this\nwork, we revisit the problem from a sequence-to-sequence correspondence\nperspective to replace cost volume construction with dense pixel matching using\nposition information and attention. This approach, named STereo TRansformer\n(STTR), has several advantages: It 1) relaxes the limitation of a fixed\ndisparity range, 2) identifies occluded regions and provides confidence\nestimates, and 3) imposes uniqueness constraints during the matching process.\nWe report promising results on both synthetic and real-world datasets and\ndemonstrate that STTR generalizes across different domains, even without\nfine-tuning. Our code is available at\nhttps://github.com/mli0603/stereo-transformer.\n", "versions": [{"version": "v1", "created": "Thu, 5 Nov 2020 15:35:46 GMT"}, {"version": "v2", "created": "Fri, 6 Nov 2020 03:20:39 GMT"}, {"version": "v3", "created": "Sat, 27 Mar 2021 16:03:38 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Li", "Zhaoshuo", ""], ["Liu", "Xingtong", ""], ["Drenkow", "Nathan", ""], ["Ding", "Andy", ""], ["Creighton", "Francis X.", ""], ["Taylor", "Russell H.", ""], ["Unberath", "Mathias", ""]]}, {"id": "2011.02917", "submitter": "Alessandro Suglia", "authors": "Alessandro Suglia, Antonio Vergari, Ioannis Konstas, Yonatan Bisk,\n  Emanuele Bastianelli, Andrea Vanzo, Oliver Lemon", "title": "Imagining Grounded Conceptual Representations from Perceptual\n  Information in Situated Guessing Games", "comments": "Accepted to the International Conference on Computational Linguistics\n  (COLING) 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In visual guessing games, a Guesser has to identify a target object in a\nscene by asking questions to an Oracle. An effective strategy for the players\nis to learn conceptual representations of objects that are both discriminative\nand expressive enough to ask questions and guess correctly. However, as shown\nby Suglia et al. (2020), existing models fail to learn truly multi-modal\nrepresentations, relying instead on gold category labels for objects in the\nscene both at training and inference time. This provides an unnatural\nperformance advantage when categories at inference time match those at training\ntime, and it causes models to fail in more realistic \"zero-shot\" scenarios\nwhere out-of-domain object categories are involved. To overcome this issue, we\nintroduce a novel \"imagination\" module based on Regularized Auto-Encoders, that\nlearns context-aware and category-aware latent embeddings without relying on\ncategory labels at inference time. Our imagination module outperforms\nstate-of-the-art competitors by 8.26% gameplay accuracy in the CompGuessWhat?!\nzero-shot scenario (Suglia et al., 2020), and it improves the Oracle and\nGuesser accuracy by 2.08% and 12.86% in the GuessWhat?! benchmark, when no gold\ncategories are available at inference time. The imagination module also boosts\nreasoning about object properties and attributes.\n", "versions": [{"version": "v1", "created": "Thu, 5 Nov 2020 15:42:29 GMT"}], "update_date": "2020-11-06", "authors_parsed": [["Suglia", "Alessandro", ""], ["Vergari", "Antonio", ""], ["Konstas", "Ioannis", ""], ["Bisk", "Yonatan", ""], ["Bastianelli", "Emanuele", ""], ["Vanzo", "Andrea", ""], ["Lemon", "Oliver", ""]]}, {"id": "2011.02956", "submitter": "David Peer", "authors": "David Peer, Sebastian Stabinger, Antonio Rodriguez-Sanchez", "title": "Conflicting Bundles: Adapting Architectures Towards the Improved\n  Training of Deep Neural Networks", "comments": "Accepted at WACV2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Designing neural network architectures is a challenging task and knowing\nwhich specific layers of a model must be adapted to improve the performance is\nalmost a mystery. In this paper, we introduce a novel theory and metric to\nidentify layers that decrease the test accuracy of the trained models, this\nidentification is done as early as at the beginning of training. In the\nworst-case, such a layer could lead to a network that can not be trained at\nall. More precisely, we identified those layers that worsen the performance\nbecause they produce conflicting training bundles as we show in our novel\ntheoretical analysis, complemented by our extensive empirical studies. Based on\nthese findings, a novel algorithm is introduced to remove performance\ndecreasing layers automatically. Architectures found by this algorithm achieve\na competitive accuracy when compared against the state-of-the-art\narchitectures. While keeping such high accuracy, our approach drastically\nreduces memory consumption and inference time for different computer vision\ntasks.\n", "versions": [{"version": "v1", "created": "Thu, 5 Nov 2020 16:41:04 GMT"}], "update_date": "2020-11-06", "authors_parsed": [["Peer", "David", ""], ["Stabinger", "Sebastian", ""], ["Rodriguez-Sanchez", "Antonio", ""]]}, {"id": "2011.03006", "submitter": "Adnan Siraj Rakin", "authors": "Adnan Siraj Rakin, Yukui Luo, Xiaolin Xu and Deliang Fan", "title": "Deep-Dup: An Adversarial Weight Duplication Attack Framework to Crush\n  Deep Neural Network in Multi-Tenant FPGA", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The wide deployment of Deep Neural Networks (DNN) in high-performance cloud\ncomputing platforms has emerged field-programmable gate arrays (FPGA) as a\npopular choice of accelerator to boost performance due to its hardware\nreprogramming flexibility. To improve the efficiency of hardware resource\nutilization, growing efforts have been invested in FPGA virtualization,\nenabling the co-existence of multiple independent tenants in a shared FPGA\nchip. Such a multi-tenant FPGA setup for DNN acceleration potentially exposes\nthe DNN interference task under severe threat from malicious users. This work,\nto the best of our knowledge, is the first to explore DNN model vulnerabilities\nin multi-tenant FPGAs. We propose a novel adversarial attack framework:\nDeep-Dup, in which the adversarial tenant can inject faults to the DNN model of\nvictim tenant in FPGA. Specifically, she can aggressively overload the shared\npower distribution system of FPGA with malicious power-plundering circuits,\nachieving adversarial weight duplication (AWD) hardware attack that duplicates\ncertain DNN weight packages during data transmission between off-chip memory\nand on-chip buffer, with the objective to hijack DNN function of the victim\ntenant. Further, to identify the most vulnerable DNN weight packages for a\ngiven malicious objective, we propose a generic vulnerable weight package\nsearching algorithm, called Progressive Differential Evolution Search (P-DES),\nwhich is, for the first time, adaptive to both deep learning white-box and\nblack-box attack models. Unlike prior works only working in a deep learning\nwhite-box setup, our adaptiveness mainly comes from the fact that the proposed\nP-DES does not require any gradient information of DNN model.\n", "versions": [{"version": "v1", "created": "Thu, 5 Nov 2020 17:59:14 GMT"}], "update_date": "2020-11-06", "authors_parsed": [["Rakin", "Adnan Siraj", ""], ["Luo", "Yukui", ""], ["Xu", "Xiaolin", ""], ["Fan", "Deliang", ""]]}, {"id": "2011.03029", "submitter": "Jean B\\'egaint", "authors": "Jean B\\'egaint, Fabien Racap\\'e, Simon Feltman, Akshay Pushparaja", "title": "CompressAI: a PyTorch library and evaluation platform for end-to-end\n  compression research", "comments": "19 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents CompressAI, a platform that provides custom operations,\nlayers, models and tools to research, develop and evaluate end-to-end image and\nvideo compression codecs. In particular, CompressAI includes pre-trained models\nand evaluation tools to compare learned methods with traditional codecs.\nMultiple models from the state-of-the-art on learned end-to-end compression\nhave thus been reimplemented in PyTorch and trained from scratch. We also\nreport objective comparison results using PSNR and MS-SSIM metrics vs.\nbit-rate, using the Kodak image dataset as test set. Although this framework\ncurrently implements models for still-picture compression, it is intended to be\nsoon extended to the video compression domain.\n", "versions": [{"version": "v1", "created": "Thu, 5 Nov 2020 18:40:50 GMT"}], "update_date": "2020-11-06", "authors_parsed": [["B\u00e9gaint", "Jean", ""], ["Racap\u00e9", "Fabien", ""], ["Feltman", "Simon", ""], ["Pushparaja", "Akshay", ""]]}, {"id": "2011.03042", "submitter": "Fukang Guo", "authors": "Jingjing Cao, Fukang Guo, Xin Lai, Qiang Zhou, Jinshan Dai", "title": "A Tree-structure Convolutional Neural Network for Temporal Features\n  Exaction on Sensor-based Multi-resident Activity Recognition", "comments": "12 pages, 4 figures", "journal-ref": "International Conference on Neural Computing for Advanced\n  Applications NCAA 2020: Neural Computing for Advanced Applications pp 513-525", "doi": "10.1007/978-981-15-7670-6_43", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the propagation of sensor devices applied in smart home, activity\nrecognition has ignited huge interest and most existing works assume that there\nis only one habitant. While in reality, there are generally multiple residents\nat home, which brings greater challenge to recognize activities. In addition,\nmany conventional approaches rely on manual time series data segmentation\nignoring the inherent characteristics of events and their heuristic\nhand-crafted feature generation algorithms are difficult to exploit distinctive\nfeatures to accurately classify different activities. To address these issues,\nwe propose an end-to-end Tree-Structure Convolutional neural network based\nframework for Multi-Resident Activity Recognition (TSC-MRAR). First, we treat\neach sample as an event and obtain the current event embedding through the\nprevious sensor readings in the sliding window without splitting the time\nseries data. Then, in order to automatically generate the temporal features, a\ntree-structure network is designed to derive the temporal dependence of nearby\nreadings. The extracted features are fed into the fully connected layer, which\ncan jointly learn the residents labels and the activity labels simultaneously.\nFinally, experiments on CASAS datasets demonstrate the high performance in\nmulti-resident activity recognition of our model compared to state-of-the-art\ntechniques.\n", "versions": [{"version": "v1", "created": "Thu, 5 Nov 2020 14:31:00 GMT"}], "update_date": "2020-11-09", "authors_parsed": [["Cao", "Jingjing", ""], ["Guo", "Fukang", ""], ["Lai", "Xin", ""], ["Zhou", "Qiang", ""], ["Dai", "Jinshan", ""]]}, {"id": "2011.03043", "submitter": "Nolan Dey", "authors": "Nolan S. Dey and J. Eric Taylor and Bryan P. Tripp and Alexander Wong\n  and Graham W. Taylor", "title": "Identifying and interpreting tuning dimensions in deep networks", "comments": "15 pages, 12 figures, Camera-ready for Shared Visual Representations\n  in Human & Machine Intelligence NeurIPS Workshop 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In neuroscience, a tuning dimension is a stimulus attribute that accounts for\nmuch of the activation variance of a group of neurons. These are commonly used\nto decipher the responses of such groups. While researchers have attempted to\nmanually identify an analogue to these tuning dimensions in deep neural\nnetworks, we are unaware of an automatic way to discover them. This work\ncontributes an unsupervised framework for identifying and interpreting \"tuning\ndimensions\" in deep networks. Our method correctly identifies the tuning\ndimensions of a synthetic Gabor filter bank and tuning dimensions of the first\ntwo layers of InceptionV1 trained on ImageNet.\n", "versions": [{"version": "v1", "created": "Thu, 5 Nov 2020 21:26:03 GMT"}, {"version": "v2", "created": "Tue, 8 Dec 2020 00:01:04 GMT"}], "update_date": "2020-12-09", "authors_parsed": [["Dey", "Nolan S.", ""], ["Taylor", "J. Eric", ""], ["Tripp", "Bryan P.", ""], ["Wong", "Alexander", ""], ["Taylor", "Graham W.", ""]]}, {"id": "2011.03077", "submitter": "Nitin J. Sanket", "authors": "Nitin J. Sanket, Chahat Deep Singh, Varun Asthana, Cornelia\n  Ferm\\\"uller, Yiannis Aloimonos", "title": "MorphEyes: Variable Baseline Stereo For Quadrotor Navigation", "comments": "7 pages, 10 figures, 1 table. Under review in ICRA 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Morphable design and depth-based visual control are two upcoming trends\nleading to advancements in the field of quadrotor autonomy. Stereo-cameras have\nstruck the perfect balance of weight and accuracy of depth estimation but\nsuffer from the problem of depth range being limited and dictated by the\nbaseline chosen at design time. In this paper, we present a framework for\nquadrotor navigation based on a stereo camera system whose baseline can be\nadapted on-the-fly. We present a method to calibrate the system at a small\nnumber of discrete baselines and interpolate the parameters for the entire\nbaseline range. We present an extensive theoretical analysis of calibration and\nsynchronization errors. We showcase three different applications of such a\nsystem for quadrotor navigation: (a) flying through a forest, (b) flying\nthrough an unknown shaped/location static/dynamic gap, and (c) accurate 3D pose\ndetection of an independently moving object. We show that our variable baseline\nsystem is more accurate and robust in all three scenarios. To our knowledge,\nthis is the first work that applies the concept of morphable design to achieve\na variable baseline stereo vision system on a quadrotor.\n", "versions": [{"version": "v1", "created": "Thu, 5 Nov 2020 20:04:35 GMT"}], "update_date": "2020-11-09", "authors_parsed": [["Sanket", "Nitin J.", ""], ["Singh", "Chahat Deep", ""], ["Asthana", "Varun", ""], ["Ferm\u00fcller", "Cornelia", ""], ["Aloimonos", "Yiannis", ""]]}, {"id": "2011.03083", "submitter": "Souvik Kundu", "authors": "Souvik Kundu, Mahdi Nazemi, Peter A. Beerel, Massoud Pedram", "title": "A Tunable Robust Pruning Framework Through Dynamic Network Rewiring of\n  DNNs", "comments": "8 pages, 4 figures, conference paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a dynamic network rewiring (DNR) method to generate\npruned deep neural network (DNN) models that are robust against adversarial\nattacks yet maintain high accuracy on clean images. In particular, the\ndisclosed DNR method is based on a unified constrained optimization formulation\nusing a hybrid loss function that merges ultra-high model compression with\nrobust adversarial training. This training strategy dynamically adjusts\ninter-layer connectivity based on per-layer normalized momentum computed from\nthe hybrid loss function. In contrast to existing robust pruning frameworks\nthat require multiple training iterations, the proposed learning strategy\nachieves an overall target pruning ratio with only a single training iteration\nand can be tuned to support both irregular and structured channel pruning. To\nevaluate the merits of DNR, experiments were performed with two widely accepted\nmodels, namely VGG16 and ResNet-18, on CIFAR-10, CIFAR-100 as well as with\nVGG16 on Tiny-ImageNet. Compared to the baseline uncompressed models, DNR\nprovides over20x compression on all the datasets with no significant drop in\neither clean or adversarial classification accuracy. Moreover, our experiments\nshow that DNR consistently finds compressed models with better clean and\nadversarial image classification performance than what is achievable through\nstate-of-the-art alternatives.\n", "versions": [{"version": "v1", "created": "Tue, 3 Nov 2020 19:49:00 GMT"}, {"version": "v2", "created": "Tue, 24 Nov 2020 17:40:52 GMT"}], "update_date": "2020-11-25", "authors_parsed": [["Kundu", "Souvik", ""], ["Nazemi", "Mahdi", ""], ["Beerel", "Peter A.", ""], ["Pedram", "Massoud", ""]]}, {"id": "2011.03091", "submitter": "Kristijan Bartol", "authors": "Kristijan Bartol and David Bojanic and Tomislav Petkovic and Tomislav\n  Pribanic and Yago Diez Donoso", "title": "Towards Keypoint Guided Self-Supervised Depth Estimation", "comments": null, "journal-ref": "15th International Joint Conference on Computer Vision, Imaging\n  and Computer Graphics Theory and Applications, 2019", "doi": "10.5220/0009190005830589", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes to use keypoints as a self-supervision clue for learning\ndepth map estimation from a collection of input images. As ground truth depth\nfrom real images is difficult to obtain, there are many unsupervised and\nself-supervised approaches to depth estimation that have been proposed. Most of\nthese unsupervised approaches use depth map and ego-motion estimations to\nreproject the pixels from the current image into the adjacent image from the\nimage collection. Depth and ego-motion estimations are evaluated based on pixel\nintensity differences between the correspondent original and reprojected\npixels. Instead of reprojecting the individual pixels, we propose to first\nselect image keypoints in both images and then reproject and compare the\ncorrespondent keypoints of the two images. The keypoints should describe the\ndistinctive image features well. By learning a deep model with and without the\nkeypoint extraction technique, we show that using the keypoints improve the\ndepth estimation learning. We also propose some future directions for\nkeypoint-guided learning of structure-from-motion problems.\n", "versions": [{"version": "v1", "created": "Thu, 5 Nov 2020 20:45:03 GMT"}], "update_date": "2020-11-09", "authors_parsed": [["Bartol", "Kristijan", ""], ["Bojanic", "David", ""], ["Petkovic", "Tomislav", ""], ["Pribanic", "Tomislav", ""], ["Donoso", "Yago Diez", ""]]}, {"id": "2011.03098", "submitter": "Yongsheng Bai", "authors": "Yongsheng Bai, Halil Sezen, Alper Yilmaz", "title": "End-to-end Deep Learning Methods for Automated Damage Detection in\n  Extreme Events at Various Scales", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Robust Mask R-CNN (Mask Regional Convolu-tional Neural Network) methods are\nproposed and tested for automatic detection of cracks on structures or their\ncomponents that may be damaged during extreme events, such as earth-quakes. We\ncurated a new dataset with 2,021 labeled images for training and validation and\naimed to find end-to-end deep neural networks for crack detection in the field.\nWith data augmentation and parameters fine-tuning, Path Aggregation Network\n(PANet) with spatial attention mechanisms and High-resolution Network (HRNet)\nare introduced into Mask R-CNNs. The tests on three public datasets with low-\nor high-resolution images demonstrate that the proposed methods can achieve a\nbig improvement over alternative networks, so the proposed method may be\nsufficient for crack detection for a variety of scales in real applications.\n", "versions": [{"version": "v1", "created": "Thu, 5 Nov 2020 21:21:19 GMT"}], "update_date": "2020-11-20", "authors_parsed": [["Bai", "Yongsheng", ""], ["Sezen", "Halil", ""], ["Yilmaz", "Alper", ""]]}, {"id": "2011.03102", "submitter": "Kristijan Bartol", "authors": "Tomislav Pribanic and Tomislav Petkovic and David Bojanic and\n  Kristijan Bartol", "title": "Smart Time-Multiplexing of Quads Solves the Multicamera Interference\n  Problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Time-of-flight (ToF) cameras are becoming increasingly popular for 3D\nimaging. Their optimal usage has been studied from the several aspects. One of\nthe open research problems is the possibility of a multicamera interference\nproblem when two or more ToF cameras are operating simultaneously. In this work\nwe present an efficient method to synchronize multiple operating ToF cameras.\nOur method is based on the time-division multiplexing, but unlike traditional\ntime multiplexing, it does not decrease the effective camera frame rate.\nAdditionally, for unsynchronized cameras, we provide a robust method to extract\nfrom their corresponding video streams, frames which are not subject to\nmulticamera interference problem. We demonstrate our approach through a series\nof experiments and with a different level of support available for triggering,\nranging from a hardware triggering to purely random software triggering.\n", "versions": [{"version": "v1", "created": "Thu, 5 Nov 2020 21:29:21 GMT"}], "update_date": "2020-11-09", "authors_parsed": [["Pribanic", "Tomislav", ""], ["Petkovic", "Tomislav", ""], ["Bojanic", "David", ""], ["Bartol", "Kristijan", ""]]}, {"id": "2011.03104", "submitter": "Kristijan Bartol", "authors": "Kristijan Bartol and Tomislav Pribanic and David Bojanic and Tomislav\n  Petkovic", "title": "Can Human Sex Be Learned Using Only 2D Body Keypoint Estimations?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we analyze human male and female sex recognition problem and\npresent a fully automated classification system using only 2D keypoints. The\nkeypoints represent human joints. A keypoint set consists of 15 joints and the\nkeypoint estimations are obtained using an OpenPose 2D keypoint detector. We\nlearn a deep learning model to distinguish males and females using the\nkeypoints as input and binary labels as output. We use two public datasets in\nthe experimental section - 3DPeople and PETA. On PETA dataset, we report a 77%\naccuracy. We provide model performance details on both PETA and 3DPeople. To\nmeasure the effect of noisy 2D keypoint detections on the performance, we run\nseparate experiments on 3DPeople ground truth and noisy keypoint data. Finally,\nwe extract a set of factors that affect the classification accuracy and propose\nfuture work. The advantage of the approach is that the input is small and the\narchitecture is simple, which enables us to run many experiments and keep the\nreal-time performance in inference. The source code, with the experiments and\ndata preparation scripts, are available on GitHub\n(https://github.com/kristijanbartol/human-sex-classifier).\n", "versions": [{"version": "v1", "created": "Thu, 5 Nov 2020 21:30:51 GMT"}], "update_date": "2020-11-09", "authors_parsed": [["Bartol", "Kristijan", ""], ["Pribanic", "Tomislav", ""], ["Bojanic", "David", ""], ["Petkovic", "Tomislav", ""]]}, {"id": "2011.03106", "submitter": "Jiawei Mo", "authors": "Jiawei Mo, Md Jahidul Islam, Junaed Sattar", "title": "Learning Rolling Shutter Correction from Real Data without Camera Motion\n  Assumption", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The rolling shutter mechanism in modern cameras generates distortions as the\nimages are formed on the sensor through a row-by-row readout process; this is\nhighly undesirable for photography and vision-based algorithms (e.g.,\nstructure-from-motion and visual SLAM). In this paper, we propose a deep neural\nnetwork to predict depth and camera poses for single-frame rolling shutter\ncorrection. Compared to the state-of-the-art, the proposed method has no\nassumptions on camera motion. It is enabled by training on real images captured\nby rolling shutter cameras instead of synthetic ones generated with certain\nmotion assumption. Consequently, the proposed method performs better for real\nrolling shutter images. This makes it possible for numerous vision-based\nalgorithms to use imagery captured using rolling shutter cameras and produce\nhighly accurate results. Our evaluations on the TUM rolling shutter dataset\nusing DSO and COLMAP validate the accuracy and robustness of the proposed\nmethod.\n", "versions": [{"version": "v1", "created": "Thu, 5 Nov 2020 21:33:25 GMT"}], "update_date": "2020-11-09", "authors_parsed": [["Mo", "Jiawei", ""], ["Islam", "Md Jahidul", ""], ["Sattar", "Junaed", ""]]}, {"id": "2011.03114", "submitter": "Henggang Cui", "authors": "Henggang Cui, Fang-Chieh Chou, Jake Charland, Carlos\n  Vallespi-Gonzalez, Nemanja Djuric", "title": "Uncertainty-Aware Vehicle Orientation Estimation for Joint\n  Detection-Prediction Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Object detection is a critical component of a self-driving system, tasked\nwith inferring the current states of the surrounding traffic actors. While\nthere exist a number of studies on the problem of inferring the position and\nshape of vehicle actors, understanding actors' orientation remains a challenge\nfor existing state-of-the-art detectors. Orientation is an important property\nfor downstream modules of an autonomous system, particularly relevant for\nmotion prediction of stationary or reversing actors where current approaches\nstruggle. We focus on this task and present a method that extends the existing\nmodels that perform joint object detection and motion prediction, allowing us\nto more accurately infer vehicle orientations. In addition, the approach is\nable to quantify prediction uncertainty, outputting the probability that the\ninferred orientation is flipped, which allows for improved motion prediction\nand safer autonomous operations. Empirical results show the benefits of the\napproach, obtaining state-of-the-art performance on the open-sourced nuScenes\ndata set.\n", "versions": [{"version": "v1", "created": "Thu, 5 Nov 2020 21:59:44 GMT"}], "update_date": "2020-11-09", "authors_parsed": [["Cui", "Henggang", ""], ["Chou", "Fang-Chieh", ""], ["Charland", "Jake", ""], ["Vallespi-Gonzalez", "Carlos", ""], ["Djuric", "Nemanja", ""]]}, {"id": "2011.03139", "submitter": "Henggang Cui", "authors": "Henggang Cui, Hoda Shajari, Sai Yalamanchi, Nemanja Djuric", "title": "Ellipse Loss for Scene-Compliant Motion Prediction", "comments": "Henggang Cui and Hoda Shajari contributed equally to this work.\n  Accepted for publication at IEEE International Conference on Robotics and\n  Automation (ICRA) 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motion prediction is a critical part of self-driving technology, responsible\nfor inferring future behavior of traffic actors in autonomous vehicle's\nsurroundings. In order to ensure safe and efficient operations, prediction\nmodels need to output accurate trajectories that obey the map constraints. In\nthis paper, we address this task and propose a novel ellipse loss that allows\nthe models to better reason about scene compliance and predict more realistic\ntrajectories. Ellipse loss penalizes off-road predictions directly in a\nsupervised manner, by projecting the output trajectories into the top-down map\nframe using a differentiable trajectory rasterizer module. Moreover, it takes\ninto account actor dimensions and orientation, providing more direct training\nsignals to the model. We applied ellipse loss to a recently proposed\nstate-of-the-art joint detection-prediction model to showcase its benefits.\nEvaluation on large-scale autonomous driving data strongly indicates that the\nmethod allows for more accurate and more realistic trajectory predictions.\n", "versions": [{"version": "v1", "created": "Thu, 5 Nov 2020 23:33:56 GMT"}, {"version": "v2", "created": "Thu, 25 Mar 2021 21:32:55 GMT"}], "update_date": "2021-03-29", "authors_parsed": [["Cui", "Henggang", ""], ["Shajari", "Hoda", ""], ["Yalamanchi", "Sai", ""], ["Djuric", "Nemanja", ""]]}, {"id": "2011.03149", "submitter": "Issam Hadj Laradji", "authors": "Issam Laradji, Alzayat Saleh, Pau Rodriguez, Derek Nowrouzezahrai,\n  Mostafa Rahimi Azghadi, David Vazquez", "title": "Affinity LCFCN: Learning to Segment Fish with Weak Supervision", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Aquaculture industries rely on the availability of accurate fish body\nmeasurements, e.g., length, width and mass. Manual methods that rely on\nphysical tools like rulers are time and labour intensive. Leading automatic\napproaches rely on fully-supervised segmentation models to acquire these\nmeasurements but these require collecting per-pixel labels -- also time\nconsuming and laborious: i.e., it can take up to two minutes per fish to\ngenerate accurate segmentation labels, almost always requiring at least some\nmanual intervention. We propose an automatic segmentation model efficiently\ntrained on images labeled with only point-level supervision, where each fish is\nannotated with a single click. This labeling process requires significantly\nless manual intervention, averaging roughly one second per fish. Our approach\nuses a fully convolutional neural network with one branch that outputs\nper-pixel scores and another that outputs an affinity matrix. We aggregate\nthese two outputs using a random walk to obtain the final, refined per-pixel\nsegmentation output. We train the entire model end-to-end with an LCFCN loss,\nresulting in our A-LCFCN method. We validate our model on the DeepFish dataset,\nwhich contains many fish habitats from the north-eastern Australian region. Our\nexperimental results confirm that A-LCFCN outperforms a fully-supervised\nsegmentation model at fixed annotation budget. Moreover, we show that A-LCFCN\nachieves better segmentation results than LCFCN and a standard baseline. We\nhave released the code at \\url{https://github.com/IssamLaradji/affinity_lcfcn}.\n", "versions": [{"version": "v1", "created": "Fri, 6 Nov 2020 00:33:20 GMT"}], "update_date": "2020-11-09", "authors_parsed": [["Laradji", "Issam", ""], ["Saleh", "Alzayat", ""], ["Rodriguez", "Pau", ""], ["Nowrouzezahrai", "Derek", ""], ["Azghadi", "Mostafa Rahimi", ""], ["Vazquez", "David", ""]]}, {"id": "2011.03154", "submitter": "Zhuowei Wang Mr", "authors": "Bingcong Li, Bo Han, Zhuowei Wang, Jing Jiang, Guodong Long", "title": "Confusable Learning for Large-class Few-Shot Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Few-shot image classification is challenging due to the lack of ample samples\nin each class. Such a challenge becomes even tougher when the number of classes\nis very large, i.e., the large-class few-shot scenario. In this novel scenario,\nexisting approaches do not perform well because they ignore confusable classes,\nnamely similar classes that are difficult to distinguish from each other. These\nclasses carry more information. In this paper, we propose a biased learning\nparadigm called Confusable Learning, which focuses more on confusable classes.\nOur method can be applied to mainstream meta-learning algorithms. Specifically,\nour method maintains a dynamically updating confusion matrix, which analyzes\nconfusable classes in the dataset. Such a confusion matrix helps meta learners\nto emphasize on confusable classes. Comprehensive experiments on Omniglot,\nFungi, and ImageNet demonstrate the efficacy of our method over\nstate-of-the-art baselines.\n", "versions": [{"version": "v1", "created": "Fri, 6 Nov 2020 01:44:37 GMT"}], "update_date": "2020-11-09", "authors_parsed": [["Li", "Bingcong", ""], ["Han", "Bo", ""], ["Wang", "Zhuowei", ""], ["Jiang", "Jing", ""], ["Long", "Guodong", ""]]}, {"id": "2011.03170", "submitter": "Linhang Cai", "authors": "Linhang Cai, Zhulin An, Yongjun Xu", "title": "GHFP: Gradually Hard Filter Pruning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Filter pruning is widely used to reduce the computation of deep learning,\nenabling the deployment of Deep Neural Networks (DNNs) in resource-limited\ndevices. Conventional Hard Filter Pruning (HFP) method zeroizes pruned filters\nand stops updating them, thus reducing the search space of the model. On the\ncontrary, Soft Filter Pruning (SFP) simply zeroizes pruned filters, keeping\nupdating them in the following training epochs, thus maintaining the capacity\nof the network. However, SFP, together with its variants, converges much slower\nthan HFP due to its larger search space. Our question is whether SFP-based\nmethods and HFP can be combined to achieve better performance and speed up\nconvergence. Firstly, we generalize SFP-based methods and HFP to analyze their\ncharacteristics. Then we propose a Gradually Hard Filter Pruning (GHFP) method\nto smoothly switch from SFP-based methods to HFP during training and pruning,\nthus maintaining a large search space at first, gradually reducing the capacity\nof the model to ensure a moderate convergence speed. Experimental results on\nCIFAR-10/100 show that our method achieves the state-of-the-art performance.\n", "versions": [{"version": "v1", "created": "Fri, 6 Nov 2020 03:09:52 GMT"}], "update_date": "2020-11-20", "authors_parsed": [["Cai", "Linhang", ""], ["An", "Zhulin", ""], ["Xu", "Yongjun", ""]]}, {"id": "2011.03174", "submitter": "Hao Li", "authors": "Hao Li, Huai Yu, Wen Yang, Lei Yu and Sebastian Scherer", "title": "ULSD: Unified Line Segment Detection across Pinhole, Fisheye, and\n  Spherical Cameras", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Line segment detection is essential for high-level tasks in computer vision\nand robotics. Currently, most stateof-the-art (SOTA) methods are dedicated to\ndetecting straight line segments in undistorted pinhole images, thus\ndistortions on fisheye or spherical images may largely degenerate their\nperformance. Targeting at the unified line segment detection (ULSD) for both\ndistorted and undistorted images, we propose to represent line segments with\nthe Bezier curve model. Then the line segment detection is tackled by the\nBezier curve regression with an end-to-end network, which is model-free and\nwithout any undistortion preprocessing. Experimental results on the pinhole,\nfisheye, and spherical image datasets validate the superiority of the proposed\nULSD to the SOTA methods both in accuracy and efficiency (40.6fps for pinhole\nimages). The source code is available at\nhttps://github.com/lh9171338/Unified-LineSegment-Detection.\n", "versions": [{"version": "v1", "created": "Fri, 6 Nov 2020 03:30:17 GMT"}], "update_date": "2020-11-09", "authors_parsed": [["Li", "Hao", ""], ["Yu", "Huai", ""], ["Yang", "Wen", ""], ["Yu", "Lei", ""], ["Scherer", "Sebastian", ""]]}, {"id": "2011.03188", "submitter": "Yading Yuan", "authors": "Yading Yuan", "title": "Automatic Brain Tumor Segmentation with Scale Attention Network", "comments": "10 pages, 3 figures, to appear in LNCS Brain Tumore Segmentation\n  (BraTS)", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic segmentation of brain tumors is an essential but challenging step\nfor extracting quantitative imaging biomarkers for accurate tumor detection,\ndiagnosis, prognosis, treatment planning and assessment. Multimodal Brain Tumor\nSegmentation Challenge 2020 (BraTS 2020) provides a common platform for\ncomparing different automatic algorithms on multi-parametric Magnetic Resonance\nImaging (mpMRI) in tasks of 1) Brain tumor segmentation MRI scans; 2)\nPrediction of patient overall survival (OS) from pre-operative MRI scans; 3)\nDistinction of true tumor recurrence from treatment related effects and 4)\nEvaluation of uncertainty measures in segmentation. We participate the image\nsegmentation challenge by developing a fully automatic segmentation network\nbased on encoder-decoder architecture. In order to better integrate information\nacross different scales, we propose a dynamic scale attention mechanism that\nincorporates low-level details with high-level semantics from feature maps at\ndifferent scales. Our framework was trained using the 369 challenge training\ncases provided by BraTS 2020, and achieved an average Dice Similarity\nCoefficient (DSC) of 0.8828, 0.8433 and 0.8177, as well as 95% Hausdorff\ndistance (in millimeter) of 5.2176, 17.9697 and 13.4298 on 166 testing cases\nfor whole tumor, tumor core and enhanced tumor, respectively, which ranked\nitself as the 3rd place among 693 registrations in the BraTS 2020 challenge.\n", "versions": [{"version": "v1", "created": "Fri, 6 Nov 2020 04:45:49 GMT"}, {"version": "v2", "created": "Tue, 10 Nov 2020 20:43:10 GMT"}, {"version": "v3", "created": "Wed, 25 Nov 2020 19:16:09 GMT"}], "update_date": "2020-11-30", "authors_parsed": [["Yuan", "Yading", ""]]}, {"id": "2011.03207", "submitter": "Dongseok Shim", "authors": "Dongseok Shim and H. Jin Kim", "title": "Learning a Geometric Representation for Data-Efficient Depth Estimation\n  via Gradient Field and Contrastive Loss", "comments": "IEEE ICRA 2021 accepted", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.RO", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Estimating a depth map from a single RGB image has been investigated widely\nfor localization, mapping, and 3-dimensional object detection. Recent studies\non a single-view depth estimation are mostly based on deep Convolutional neural\nNetworks (ConvNets) which require a large amount of training data paired with\ndensely annotated labels. Depth annotation tasks are both expensive and\ninefficient, so it is inevitable to leverage RGB images which can be collected\nvery easily to boost the performance of ConvNets without depth labels. However,\nmost self-supervised learning algorithms are focused on capturing the semantic\ninformation of images to improve the performance in classification or object\ndetection, not in depth estimation. In this paper, we show that existing\nself-supervised methods do not perform well on depth estimation and propose a\ngradient-based self-supervised learning algorithm with momentum contrastive\nloss to help ConvNets extract the geometric information with unlabeled images.\nAs a result, the network can estimate the depth map accurately with a\nrelatively small amount of annotated data. To show that our method is\nindependent of the model structure, we evaluate our method with two different\nmonocular depth estimation algorithms. Our method outperforms the previous\nstate-of-the-art self-supervised learning algorithms and shows the efficiency\nof labeled data in triple compared to random initialization on the NYU Depth v2\ndataset.\n", "versions": [{"version": "v1", "created": "Fri, 6 Nov 2020 06:47:19 GMT"}, {"version": "v2", "created": "Wed, 17 Mar 2021 05:59:46 GMT"}], "update_date": "2021-03-18", "authors_parsed": [["Shim", "Dongseok", ""], ["Kim", "H. Jin", ""]]}, {"id": "2011.03216", "submitter": "Sandeep Chinchali", "authors": "Manabu Nakanoya, Sandeep Chinchali, Alexandros Anemogiannis, Akul\n  Datta, Sachin Katti, Marco Pavone", "title": "Task-relevant Representation Learning for Networked Robotic Perception", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV cs.IT cs.NI cs.SY eess.SY math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Today, even the most compute-and-power constrained robots can measure\ncomplex, high data-rate video and LIDAR sensory streams. Often, such robots,\nranging from low-power drones to space and subterranean rovers, need to\ntransmit high-bitrate sensory data to a remote compute server if they are\nuncertain or cannot scalably run complex perception or mapping tasks locally.\nHowever, today's representations for sensory data are mostly designed for\nhuman, not robotic, perception and thus often waste precious compute or\nwireless network resources to transmit unimportant parts of a scene that are\nunnecessary for a high-level robotic task. This paper presents an algorithm to\nlearn task-relevant representations of sensory data that are co-designed with a\npre-trained robotic perception model's ultimate objective. Our algorithm\naggressively compresses robotic sensory data by up to 11x more than competing\nmethods. Further, it achieves high accuracy and robust generalization on\ndiverse tasks including Mars terrain classification with low-power deep\nlearning accelerators, neural motion planning, and environmental timeseries\nclassification.\n", "versions": [{"version": "v1", "created": "Fri, 6 Nov 2020 07:39:08 GMT"}], "update_date": "2020-11-09", "authors_parsed": [["Nakanoya", "Manabu", ""], ["Chinchali", "Sandeep", ""], ["Anemogiannis", "Alexandros", ""], ["Datta", "Akul", ""], ["Katti", "Sachin", ""], ["Pavone", "Marco", ""]]}, {"id": "2011.03234", "submitter": "Sourin Chakrabarti", "authors": "Sourin Chakrabarti", "title": "Efficient image retrieval using multi neural hash codes and bloom\n  filters", "comments": "2020 IEEE International Conference for Innovation in Technology.\n  Asian Journal for Convergence in Technology(AJCT) Volume VI Issue III", "journal-ref": null, "doi": "10.1109/INOCON50539.2020.9298228", "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper aims to deliver an efficient and modified approach for image\nretrieval using multiple neural hash codes and limiting the number of queries\nusing bloom filters by identifying false positives beforehand. Traditional\napproaches involving neural networks for image retrieval tasks tend to use\nhigher layers for feature extraction. But it has been seen that the activations\nof lower layers have proven to be more effective in a number of scenarios. In\nour approach, we have leveraged the use of local deep convolutional neural\nnetworks which combines the powers of both the features of lower and higher\nlayers for creating feature maps which are then compressed using PCA and fed to\na bloom filter after binary sequencing using a modified multi k-means approach.\nThe feature maps obtained are further used in the image retrieval process in a\nhierarchical coarse-to-fine manner by first comparing the images in the higher\nlayers for semantically similar images and then gradually moving towards the\nlower layers searching for structural similarities. While searching, the neural\nhashes for the query image are again calculated and queried in the bloom filter\nwhich tells us whether the query image is absent in the set or maybe present.\nIf the bloom filter doesn't necessarily rule out the query, then it goes into\nthe image retrieval process. This approach can be particularly helpful in cases\nwhere the image store is distributed since the approach supports parallel\nquerying.\n", "versions": [{"version": "v1", "created": "Fri, 6 Nov 2020 08:46:31 GMT"}, {"version": "v2", "created": "Wed, 18 Nov 2020 12:09:08 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Chakrabarti", "Sourin", ""]]}, {"id": "2011.03240", "submitter": "Yangchun Yan", "authors": "Yangchun Yan, Rongzuo Guo, Chao Li, Kang Yang, Yongjun Xu", "title": "Channel Pruning via Multi-Criteria based on Weight Dependency", "comments": "8 pages,IJCNN2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Channel pruning has demonstrated its effectiveness in compressing ConvNets.\nIn many related arts, the importance of an output feature map is only\ndetermined by its associated filter. However, these methods ignore a small part\nof weights in the next layer which disappears as the feature map is removed.\nThey ignore the phenomenon of weight dependency. Besides, many pruning methods\nuse only one criterion for evaluation and find a sweet spot of pruning\nstructure and accuracy in a trial-and-error fashion, which can be\ntime-consuming. In this paper, we proposed a channel pruning algorithm via\nmulti-criteria based on weight dependency, CPMC, which can compress a\npre-trained model directly. CPMC defines channel importance in three aspects,\nincluding its associated weight value, computational cost, and parameter\nquantity. According to the phenomenon of weight dependency, CPMC gets channel\nimportance by assessing its associated filter and the corresponding partial\nweights in the next layer. Then CPMC uses global normalization to achieve\ncross-layer comparison. Finally, CPMC removes less important channels by global\nranking. CPMC can compress various CNN models, including VGGNet, ResNet, and\nDenseNet on various image classification datasets. Extensive experiments have\nshown CPMC outperforms the others significantly.\n", "versions": [{"version": "v1", "created": "Fri, 6 Nov 2020 09:12:00 GMT"}, {"version": "v2", "created": "Sat, 5 Dec 2020 07:55:59 GMT"}, {"version": "v3", "created": "Tue, 8 Dec 2020 03:20:03 GMT"}, {"version": "v4", "created": "Tue, 20 Apr 2021 07:41:11 GMT"}], "update_date": "2021-04-21", "authors_parsed": [["Yan", "Yangchun", ""], ["Guo", "Rongzuo", ""], ["Li", "Chao", ""], ["Yang", "Kang", ""], ["Xu", "Yongjun", ""]]}, {"id": "2011.03247", "submitter": "Shiqi Tian", "authors": "Shiqi Tian, Ailong Ma, Zhuo Zheng, Yanfei Zhong", "title": "Hi-UCD: A Large-scale Dataset for Urban Semantic Change Detection in\n  Remote Sensing Imagery", "comments": "Presented at NeurIPS 2020 Workshop on Machine Learning for the\n  Developing World", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the acceleration of the urban expansion, urban change detection (UCD),\nas a significant and effective approach, can provide the change information\nwith respect to geospatial objects for dynamical urban analysis. However,\nexisting datasets suffer from three bottlenecks: (1) lack of high spatial\nresolution images; (2) lack of semantic annotation; (3) lack of long-range\nmulti-temporal images. In this paper, we propose a large scale benchmark\ndataset, termed Hi-UCD. This dataset uses aerial images with a spatial\nresolution of 0.1 m provided by the Estonia Land Board, including three-time\nphases, and semantically annotated with nine classes of land cover to obtain\nthe direction of ground objects change. It can be used for detecting and\nanalyzing refined urban changes. We benchmark our dataset using some classic\nmethods in binary and multi-class change detection. Experimental results show\nthat Hi-UCD is challenging yet useful. We hope the Hi-UCD can become a strong\nbenchmark accelerating future research.\n", "versions": [{"version": "v1", "created": "Fri, 6 Nov 2020 09:20:54 GMT"}, {"version": "v2", "created": "Tue, 10 Nov 2020 13:33:52 GMT"}, {"version": "v3", "created": "Wed, 11 Nov 2020 07:08:20 GMT"}, {"version": "v4", "created": "Fri, 27 Nov 2020 06:58:42 GMT"}, {"version": "v5", "created": "Tue, 1 Dec 2020 13:20:47 GMT"}, {"version": "v6", "created": "Thu, 3 Dec 2020 07:24:19 GMT"}, {"version": "v7", "created": "Mon, 28 Dec 2020 01:47:48 GMT"}], "update_date": "2020-12-29", "authors_parsed": [["Tian", "Shiqi", ""], ["Ma", "Ailong", ""], ["Zheng", "Zhuo", ""], ["Zhong", "Yanfei", ""]]}, {"id": "2011.03277", "submitter": "Samuli Laine", "authors": "Samuli Laine, Janne Hellsten, Tero Karras, Yeongho Seol, Jaakko\n  Lehtinen, Timo Aila", "title": "Modular Primitives for High-Performance Differentiable Rendering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a modular differentiable renderer design that yields performance\nsuperior to previous methods by leveraging existing, highly optimized hardware\ngraphics pipelines. Our design supports all crucial operations in a modern\ngraphics pipeline: rasterizing large numbers of triangles, attribute\ninterpolation, filtered texture lookups, as well as user-programmable shading\nand geometry processing, all in high resolutions. Our modular primitives allow\ncustom, high-performance graphics pipelines to be built directly within\nautomatic differentiation frameworks such as PyTorch or TensorFlow. As a\nmotivating application, we formulate facial performance capture as an inverse\nrendering problem and show that it can be solved efficiently using our tools.\nOur results indicate that this simple and straightforward approach achieves\nexcellent geometric correspondence between rendered results and reference\nimagery.\n", "versions": [{"version": "v1", "created": "Fri, 6 Nov 2020 10:48:43 GMT"}], "update_date": "2020-11-09", "authors_parsed": [["Laine", "Samuli", ""], ["Hellsten", "Janne", ""], ["Karras", "Tero", ""], ["Seol", "Yeongho", ""], ["Lehtinen", "Jaakko", ""], ["Aila", "Timo", ""]]}, {"id": "2011.03279", "submitter": "Wout Boerdijk", "authors": "Wout Boerdijk, Martin Sundermeyer, Maximilian Durner, Rudolph Triebel", "title": "\"What's This?\" -- Learning to Segment Unknown Objects from Manipulation\n  Sequences", "comments": "8 pages, 6 figures,in Proceedings of the IEEE International\n  Conference on Robotics and Automation (ICRA), 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel framework for self-supervised grasped object segmentation\nwith a robotic manipulator. Our method successively learns an agnostic\nforeground segmentation followed by a distinction between manipulator and\nobject solely by observing the motion between consecutive RGB frames. In\ncontrast to previous approaches, we propose a single, end-to-end trainable\narchitecture which jointly incorporates motion cues and semantic knowledge.\nFurthermore, while the motion of the manipulator and the object are substantial\ncues for our algorithm, we present means to robustly deal with distraction\nobjects moving in the background, as well as with completely static scenes. Our\nmethod neither depends on any visual registration of a kinematic robot or 3D\nobject models, nor on precise hand-eye calibration or any additional sensor\ndata. By extensive experimental evaluation we demonstrate the superiority of\nour framework and provide detailed insights on its capability of dealing with\nthe aforementioned extreme cases of motion. We also show that training a\nsemantic segmentation network with the automatically labeled data achieves\nresults on par with manually annotated training data. Code and pretrained model\nare available at https://github.com/DLR-RM/DistinctNet.\n", "versions": [{"version": "v1", "created": "Fri, 6 Nov 2020 10:55:28 GMT"}, {"version": "v2", "created": "Thu, 17 Jun 2021 09:00:03 GMT"}], "update_date": "2021-06-18", "authors_parsed": [["Boerdijk", "Wout", ""], ["Sundermeyer", "Martin", ""], ["Durner", "Maximilian", ""], ["Triebel", "Rudolph", ""]]}, {"id": "2011.03290", "submitter": "Delei Kong", "authors": "Delei Kong, Zheng Fang, Haojia Li, Kuanxu Hou, Sonya Coleman and\n  Dermot Kerr", "title": "Event-VPR: End-to-End Weakly Supervised Network Architecture for\n  Event-based Visual Place Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditional visual place recognition (VPR) methods generally use frame-based\ncameras, which is easy to fail due to dramatic illumination changes or fast\nmotions. In this paper, we propose an end-to-end visual place recognition\nnetwork for event cameras, which can achieve good place recognition performance\nin challenging environments. The key idea of the proposed algorithm is firstly\nto characterize the event streams with the EST voxel grid, then extract\nfeatures using a convolution network, and finally aggregate features using an\nimproved VLAD network to realize end-to-end visual place recognition using\nevent streams. To verify the effectiveness of the proposed algorithm, we\ncompare the proposed method with classical VPR methods on the event-based\ndriving datasets (MVSEC, DDD17) and the synthetic datasets (Oxford RobotCar).\nExperimental results show that the proposed method can achieve much better\nperformance in challenging scenarios. To our knowledge, this is the first\nend-to-end event-based VPR method. The accompanying source code is available at\nhttps://github.com/kongdelei/Event-VPR.\n", "versions": [{"version": "v1", "created": "Fri, 6 Nov 2020 11:32:04 GMT"}], "update_date": "2020-11-09", "authors_parsed": [["Kong", "Delei", ""], ["Fang", "Zheng", ""], ["Li", "Haojia", ""], ["Hou", "Kuanxu", ""], ["Coleman", "Sonya", ""], ["Kerr", "Dermot", ""]]}, {"id": "2011.03298", "submitter": "Riccardo Spezialetti", "authors": "Riccardo Spezialetti, Federico Stella, Marlon Marcon, Luciano Silva,\n  Samuele Salti, Luigi Di Stefano", "title": "Learning to Orient Surfaces by Self-supervised Spherical CNNs", "comments": "Accepted to NeurIPS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Defining and reliably finding a canonical orientation for 3D surfaces is key\nto many Computer Vision and Robotics applications. This task is commonly\naddressed by handcrafted algorithms exploiting geometric cues deemed as\ndistinctive and robust by the designer. Yet, one might conjecture that humans\nlearn the notion of the inherent orientation of 3D objects from experience and\nthat machines may do so alike. In this work, we show the feasibility of\nlearning a robust canonical orientation for surfaces represented as point\nclouds. Based on the observation that the quintessential property of a\ncanonical orientation is equivariance to 3D rotations, we propose to employ\nSpherical CNNs, a recently introduced machinery that can learn equivariant\nrepresentations defined on the Special Orthogonal group SO(3). Specifically,\nspherical correlations compute feature maps whose elements define 3D rotations.\nOur method learns such feature maps from raw data by a self-supervised training\nprocedure and robustly selects a rotation to transform the input point cloud\ninto a learned canonical orientation. Thereby, we realize the first end-to-end\nlearning approach to define and extract the canonical orientation of 3D shapes,\nwhich we aptly dub Compass. Experiments on several public datasets prove its\neffectiveness at orienting local surface patches as well as whole objects.\n", "versions": [{"version": "v1", "created": "Fri, 6 Nov 2020 11:43:57 GMT"}, {"version": "v2", "created": "Fri, 13 Nov 2020 09:25:28 GMT"}], "update_date": "2020-11-16", "authors_parsed": [["Spezialetti", "Riccardo", ""], ["Stella", "Federico", ""], ["Marcon", "Marlon", ""], ["Silva", "Luciano", ""], ["Salti", "Samuele", ""], ["Di Stefano", "Luigi", ""]]}, {"id": "2011.03303", "submitter": "Siamak Mehrkanoon", "authors": "Jes\\'us Garc\\'ia Fern\\'andez, Ismail Alaoui Abdellaoui, Siamak\n  Mehrkanoon", "title": "Deep coastal sea elements forecasting using U-Net based models", "comments": "12 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Due to the development of deep learning techniques applied to satellite\nimagery, weather forecasting that uses remote sensing data has also been the\nsubject of major progress. The present paper investigates multiple steps ahead\nframe prediction for coastal sea elements in the Netherlands using U-Net based\narchitectures. Hourly data from the Copernicus observation programme spanned\nover a period of 2 years has been used to train the models and make the\nforecasting, including seasonal predictions. We propose a variation of the\nU-Net architecture and also extend this novel model using residual connections,\nparallel convolutions and asymmetric convolutions in order to propose three\nadditional architectures. In particular, we show that the architecture equipped\nwith parallel and asymmetric convolutions as well as skip connections is\nparticularly suited for this task, outperforming the other three discussed\nmodels.\n", "versions": [{"version": "v1", "created": "Fri, 6 Nov 2020 12:02:31 GMT"}], "update_date": "2020-11-20", "authors_parsed": [["Fern\u00e1ndez", "Jes\u00fas Garc\u00eda", ""], ["Abdellaoui", "Ismail Alaoui", ""], ["Mehrkanoon", "Siamak", ""]]}, {"id": "2011.03308", "submitter": "Xiangtai Li", "authors": "Xiangtai Li, Xia Li, Ansheng You, Li Zhang, Guangliang Cheng, Kuiyuan\n  Yang, Yunhai Tong, Zhouchen Lin", "title": "Towards Efficient Scene Understanding via Squeeze Reasoning", "comments": "Accepted by IEEE-TIP", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph-based convolutional model such as non-local block has shown to be\neffective for strengthening the context modeling ability in convolutional\nneural networks (CNNs). However, its pixel-wise computational overhead is\nprohibitive which renders it unsuitable for high resolution imagery. In this\npaper, we explore the efficiency of context graph reasoning and propose a novel\nframework called Squeeze Reasoning. Instead of propagating information on the\nspatial map, we first learn to squeeze the input feature into a channel-wise\nglobal vector and perform reasoning within the single vector where the\ncomputation cost can be significantly reduced. Specifically, we build the node\ngraph in the vector where each node represents an abstract semantic concept.\nThe refined feature within the same semantic category results to be consistent,\nwhich is thus beneficial for downstream tasks. We show that our approach can be\nmodularized as an end-to-end trained block and can be easily plugged into\nexisting networks. {Despite its simplicity and being lightweight, the proposed\nstrategy allows us to establish the considerable results on different semantic\nsegmentation datasets and shows significant improvements with respect to strong\nbaselines on various other scene understanding tasks including object\ndetection, instance segmentation and panoptic segmentation.} Code is available\nat \\url{https://github.com/lxtGH/SFSegNets}.\n", "versions": [{"version": "v1", "created": "Fri, 6 Nov 2020 12:17:01 GMT"}, {"version": "v2", "created": "Mon, 29 Mar 2021 08:27:34 GMT"}, {"version": "v3", "created": "Tue, 20 Jul 2021 06:29:08 GMT"}], "update_date": "2021-07-21", "authors_parsed": [["Li", "Xiangtai", ""], ["Li", "Xia", ""], ["You", "Ansheng", ""], ["Zhang", "Li", ""], ["Cheng", "Guangliang", ""], ["Yang", "Kuiyuan", ""], ["Tong", "Yunhai", ""], ["Lin", "Zhouchen", ""]]}, {"id": "2011.03322", "submitter": "Shen Gao", "authors": "Shen Gao, Xiuying Chen, Li Liu, Dongyan Zhao and Rui Yan", "title": "Learning to Respond with Your Favorite Stickers: A Framework of Unifying\n  Multi-Modality and User Preference in Multi-Turn Dialog", "comments": "Accepted by TOIS. arXiv admin note: substantial text overlap with\n  arXiv:2003.04679", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stickers with vivid and engaging expressions are becoming increasingly\npopular in online messaging apps, and some works are dedicated to automatically\nselect sticker response by matching the stickers image with previous\nutterances. However, existing methods usually focus on measuring the matching\ndegree between the dialog context and sticker image, which ignores the user\npreference of using stickers. Hence, in this paper, we propose to recommend an\nappropriate sticker to user based on multi-turn dialog context and sticker\nusing history of user. Two main challenges are confronted in this task. One is\nto model the sticker preference of user based on the previous sticker selection\nhistory. Another challenge is to jointly fuse the user preference and the\nmatching between dialog context and candidate sticker into final prediction\nmaking. To tackle these challenges, we propose a \\emph{Preference Enhanced\nSticker Response Selector} (PESRS) model. Specifically, PESRS first employs a\nconvolutional based sticker image encoder and a self-attention based multi-turn\ndialog encoder to obtain the representation of stickers and utterances. Next,\ndeep interaction network is proposed to conduct deep matching between the\nsticker and each utterance. Then, we model the user preference by using the\nrecently selected stickers as input, and use a key-value memory network to\nstore the preference representation. PESRS then learns the short-term and\nlong-term dependency between all interaction results by a fusion network, and\ndynamically fuse the user preference representation into the final sticker\nselection prediction. Extensive experiments conducted on a large-scale\nreal-world dialog dataset show that our model achieves the state-of-the-art\nperformance for all commonly-used metrics. Experiments also verify the\neffectiveness of each component of PESRS.\n", "versions": [{"version": "v1", "created": "Thu, 5 Nov 2020 03:31:17 GMT"}], "update_date": "2020-11-09", "authors_parsed": [["Gao", "Shen", ""], ["Chen", "Xiuying", ""], ["Liu", "Li", ""], ["Zhao", "Dongyan", ""], ["Yan", "Rui", ""]]}, {"id": "2011.03350", "submitter": "Haoyue Zhang", "authors": "Haoyue Zhang, Jennifer S Polson, Kambiz Nael, Noriko Salamon, Bryan\n  Yoo, Suzie El-Saden, Fabien Scalzo, William Speier, Corey W Arnold", "title": "Intra-Domain Task-Adaptive Transfer Learning to Determine Acute Ischemic\n  Stroke Onset Time", "comments": null, "journal-ref": "Computerized Medical Imaging and Graphics Volume 90, June 2021,\n  101926", "doi": "10.1016/j.compmedimag.2021.101926", "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Treatment of acute ischemic strokes (AIS) is largely contingent upon the time\nsince stroke onset (TSS). However, TSS may not be readily available in up to\n25% of patients with unwitnessed AIS. Current clinical guidelines for patients\nwith unknown TSS recommend the use of MRI to determine eligibility for\nthrombolysis, but radiology assessments have high inter-reader variability. In\nthis work, we present deep learning models that leverage MRI diffusion series\nto classify TSS based on clinically validated thresholds. We propose an\nintra-domain task-adaptive transfer learning method, which involves training a\nmodel on an easier clinical task (stroke detection) and then refining the model\nwith different binary thresholds of TSS. We apply this approach to both 2D and\n3D CNN architectures with our top model achieving an ROC-AUC value of 0.74,\nwith a sensitivity of 0.70 and a specificity of 0.81 for classifying TSS < 4.5\nhours. Our pretrained models achieve better classification metrics than the\nmodels trained from scratch, and these metrics exceed those of previously\npublished models applied to our dataset. Furthermore, our pipeline accommodates\na more inclusive patient cohort than previous work, as we did not exclude\nimaging studies based on clinical, demographic, or image processing criteria.\nWhen applied to this broad spectrum of patients, our deep learning model\nachieves an overall accuracy of 75.78% when classifying TSS < 4.5 hours,\ncarrying potential therapeutic implications for patients with unknown TSS.\n", "versions": [{"version": "v1", "created": "Thu, 5 Nov 2020 18:28:54 GMT"}], "update_date": "2021-05-03", "authors_parsed": [["Zhang", "Haoyue", ""], ["Polson", "Jennifer S", ""], ["Nael", "Kambiz", ""], ["Salamon", "Noriko", ""], ["Yoo", "Bryan", ""], ["El-Saden", "Suzie", ""], ["Scalzo", "Fabien", ""], ["Speier", "William", ""], ["Arnold", "Corey W", ""]]}, {"id": "2011.03353", "submitter": "Yaroslav Zharov", "authors": "Yaroslav Zharov, Alexey Ershov, Tilo Baumbach", "title": "Self Supervised Learning for Object Localisation in 3D Tomographic\n  Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While a lot of work is dedicated to self-supervised learning, most of it is\ndealing with 2D images of natural scenes and objects. In this paper, we focus\non \\textit{volumetric} images obtained by means of the X-Ray Computed\nTomography (CT). We describe two pretext training tasks which are designed\ntaking into account the specific properties of volumetric data. We propose two\nways to transfer a trained network to the downstream task of object\nlocalization with a zero amount of manual markup. Despite its simplicity, the\nproposed method shows its applicability to practical tasks of object\nlocalization and data reduction.\n", "versions": [{"version": "v1", "created": "Fri, 6 Nov 2020 13:34:00 GMT"}], "update_date": "2020-11-09", "authors_parsed": [["Zharov", "Yaroslav", ""], ["Ershov", "Alexey", ""], ["Baumbach", "Tilo", ""]]}, {"id": "2011.03363", "submitter": "Xiaobin Liu", "authors": "Xiaobin Liu and Shiliang Zhang", "title": "Domain Adaptive Person Re-Identification via Coupling Optimization", "comments": "ACM MM 2020 Oral", "journal-ref": null, "doi": "10.1145/3394171.3413904", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Domain adaptive person Re-Identification (ReID) is challenging owing to the\ndomain gap and shortage of annotations on target scenarios. To handle those two\nchallenges, this paper proposes a coupling optimization method including the\nDomain-Invariant Mapping (DIM) method and the Global-Local distance\nOptimization (GLO), respectively. Different from previous methods that transfer\nknowledge in two stages, the DIM achieves a more efficient one-stage knowledge\ntransfer by mapping images in labeled and unlabeled datasets to a shared\nfeature space. GLO is designed to train the ReID model with unsupervised\nsetting on the target domain. Instead of relying on existing optimization\nstrategies designed for supervised training, GLO involves more images in\ndistance optimization, and achieves better robustness to noisy label\nprediction. GLO also integrates distance optimizations in both the global\ndataset and local training batch, thus exhibits better training efficiency.\nExtensive experiments on three large-scale datasets, i.e., Market-1501,\nDukeMTMC-reID, and MSMT17, show that our coupling optimization outperforms\nstate-of-the-art methods by a large margin. Our method also works well in\nunsupervised training, and even outperforms several recent domain adaptive\nmethods.\n", "versions": [{"version": "v1", "created": "Fri, 6 Nov 2020 14:01:03 GMT"}], "update_date": "2020-11-09", "authors_parsed": [["Liu", "Xiaobin", ""], ["Zhang", "Shiliang", ""]]}, {"id": "2011.03367", "submitter": "Shamit Lal", "authors": "Mihir Prabhudesai, Shamit Lal, Darshan Patil, Hsiao-Yu Tung, Adam W\n  Harley, Katerina Fragkiadaki", "title": "Disentangling 3D Prototypical Networks For Few-Shot Concept Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present neural architectures that disentangle RGB-D images into objects'\nshapes and styles and a map of the background scene, and explore their\napplications for few-shot 3D object detection and few-shot concept\nclassification. Our networks incorporate architectural biases that reflect the\nimage formation process, 3D geometry of the world scene, and shape-style\ninterplay. They are trained end-to-end self-supervised by predicting views in\nstatic scenes, alongside a small number of 3D object boxes. Objects and scenes\nare represented in terms of 3D feature grids in the bottleneck of the network.\nWe show that the proposed 3D neural representations are compositional: they can\ngenerate novel 3D scene feature maps by mixing object shapes and styles,\nresizing and adding the resulting object 3D feature maps over background scene\nfeature maps. We show that classifiers for object categories, color, materials,\nand spatial relationships trained over the disentangled 3D feature sub-spaces\ngeneralize better with dramatically fewer examples than the current\nstate-of-the-art, and enable a visual question answering system that uses them\nas its modules to generalize one-shot to novel objects in the scene.\n", "versions": [{"version": "v1", "created": "Fri, 6 Nov 2020 14:08:27 GMT"}, {"version": "v2", "created": "Sat, 3 Apr 2021 17:58:34 GMT"}, {"version": "v3", "created": "Tue, 20 Jul 2021 19:07:01 GMT"}], "update_date": "2021-07-22", "authors_parsed": [["Prabhudesai", "Mihir", ""], ["Lal", "Shamit", ""], ["Patil", "Darshan", ""], ["Tung", "Hsiao-Yu", ""], ["Harley", "Adam W", ""], ["Fragkiadaki", "Katerina", ""]]}, {"id": "2011.03384", "submitter": "Chuang Niu", "authors": "Chuang Niu, Fenglei Fan, Qing Lyu, and Ge Wang", "title": "Noise2Sim -- Similarity-based Self-Learning for Image Denoising", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite its best performance in image denoising, the supervised deep\ndenoising methods require paired noise-clean data, which are often unavailable.\nTo address this challenge, Noise2Noise was designed based on the fact that\npaired noise-clean images can be replaced by paired noise-noise images that are\neasier to collect. However, in many scenarios the collection of paired\nnoise-noise images is still impractical. To bypass labeled images, Noise2Void\nmethods predict masked pixels from their surroundings with single noisy images\nonly and give improved denoising results that still need improvements. An\nobservation on classic denoising methods is that non-local mean (NLM) outcomes\nare typically superior to locally denoised results. In contrast, Noise2Void and\nits variants do not utilize self-similarities in an image as the NLM-based\nmethods do. Here we propose Noise2Sim, an NLM-inspired self-learning method for\nimage denoising. Specifically, Noise2Sim leverages the self-similarity of image\npixels to train the denoising network, requiring single noisy images only. Our\ntheoretical analysis shows that Noise2Sim tends to be equivalent to Noise2Noise\nunder mild conditions. To efficiently manage the computational burden for\nglobally searching similar pixels, we design a two-step procedure to provide\ndata for Noise2Sim training. Extensive experiments demonstrate the superiority\nof Noise2Sim on common benchmark datasets.\n", "versions": [{"version": "v1", "created": "Fri, 6 Nov 2020 14:31:08 GMT"}, {"version": "v2", "created": "Wed, 11 Nov 2020 05:23:32 GMT"}, {"version": "v3", "created": "Wed, 17 Mar 2021 01:50:50 GMT"}, {"version": "v4", "created": "Fri, 18 Jun 2021 07:02:58 GMT"}], "update_date": "2021-06-21", "authors_parsed": [["Niu", "Chuang", ""], ["Fan", "Fenglei", ""], ["Lyu", "Qing", ""], ["Wang", "Ge", ""]]}, {"id": "2011.03428", "submitter": "Steve Dias Da Cruz", "authors": "Steve Dias Da Cruz, Bertram Taetz, Thomas Stifter, Didier Stricker", "title": "Illumination Normalization by Partially Impossible Encoder-Decoder Cost\n  Function", "comments": "This paper is accepted at IEEE Winter Conference on Applications of\n  Computer Vision (WACV), 2021. Supplementary material is available under\n  https://sviro.kl.dfki.de/downloads/papers/wacv2021_supplementary.pdf", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Images recorded during the lifetime of computer vision based systems undergo\na wide range of illumination and environmental conditions affecting the\nreliability of previously trained machine learning models. Image normalization\nis hence a valuable preprocessing component to enhance the models' robustness.\nTo this end, we introduce a new strategy for the cost function formulation of\nencoder-decoder networks to average out all the unimportant information in the\ninput images (e.g. environmental features and illumination changes) to focus on\nthe reconstruction of the salient features (e.g. class instances). Our method\nexploits the availability of identical sceneries under different illumination\nand environmental conditions for which we formulate a partially impossible\nreconstruction target: the input image will not convey enough information to\nreconstruct the target in its entirety. Its applicability is assessed on three\npublicly available datasets. We combine the triplet loss as a regularizer in\nthe latent space representation and a nearest neighbour search to improve the\ngeneralization to unseen illuminations and class instances. The importance of\nthe aforementioned post-processing is highlighted on an automotive application.\nTo this end, we release a synthetic dataset of sceneries from three different\npassenger compartments where each scenery is rendered under ten different\nillumination and environmental conditions: see https://sviro.kl.dfki.de\n", "versions": [{"version": "v1", "created": "Fri, 6 Nov 2020 15:25:26 GMT"}, {"version": "v2", "created": "Mon, 9 Nov 2020 15:43:42 GMT"}], "update_date": "2020-11-10", "authors_parsed": [["Da Cruz", "Steve Dias", ""], ["Taetz", "Bertram", ""], ["Stifter", "Thomas", ""], ["Stricker", "Didier", ""]]}, {"id": "2011.03451", "submitter": "Rongcheng Tu", "authors": "Rong-Cheng Tu, Xian-Ling Mao, Rongxin Tu, Binbin Bian, Wei Wei, Heyan\n  Huang", "title": "Deep Cross-modal Hashing via Margin-dynamic-softmax Loss", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to their high retrieval efficiency and low storage cost for cross-modal\nsearch task, cross-modal hashing methods have attracted considerable attention.\nFor the supervised cross-modal hashing methods, how to make the learned hash\ncodes preserve semantic information sufficiently contained in the label of\ndatapoints is the key to further enhance the retrieval performance. Hence,\nalmost all supervised cross-modal hashing methods usually depends on defining a\nsimilarity between datapoints with the label information to guide the hashing\nmodel learning fully or partly. However, the defined similarity between\ndatapoints can only capture the label information of datapoints partially and\nmisses abundant semantic information, then hinders the further improvement of\nretrieval performance. Thus, in this paper, different from previous works, we\npropose a novel cross-modal hashing method without defining the similarity\nbetween datapoints, called Deep Cross-modal Hashing via\n\\textit{Margin-dynamic-softmax Loss} (DCHML). Specifically, DCHML first trains\na proxy hashing network to transform each category information of a dataset\ninto a semantic discriminative hash code, called proxy hash code. Each proxy\nhash code can preserve the semantic information of its corresponding category\nwell. Next, without defining the similarity between datapoints to supervise the\ntraining process of the modality-specific hashing networks , we propose a novel\n\\textit{margin-dynamic-softmax loss} to directly utilize the proxy hashing\ncodes as supervised information. Finally, by minimizing the novel\n\\textit{margin-dynamic-softmax loss}, the modality-specific hashing networks\ncan be trained to generate hash codes which can simultaneously preserve the\ncross-modal similarity and abundant semantic information well.\n", "versions": [{"version": "v1", "created": "Fri, 6 Nov 2020 16:02:35 GMT"}, {"version": "v2", "created": "Tue, 18 May 2021 15:02:27 GMT"}], "update_date": "2021-05-19", "authors_parsed": [["Tu", "Rong-Cheng", ""], ["Mao", "Xian-Ling", ""], ["Tu", "Rongxin", ""], ["Bian", "Binbin", ""], ["Wei", "Wei", ""], ["Huang", "Heyan", ""]]}, {"id": "2011.03462", "submitter": "Kong Zhaoming", "authors": "Zhaoming Kong, Xiaowei Yang and Lifang He", "title": "A Comprehensive Comparison of Multi-Dimensional Image Denoising Methods", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Filtering multi-dimensional images such as color images, color videos,\nmultispectral images and magnetic resonance images is challenging in terms of\nboth effectiveness and efficiency. Leveraging the nonlocal self-similarity\n(NLSS) characteristic of images and sparse representation in the transform\ndomain, the block-matching and 3D filtering (BM3D) based methods show powerful\ndenoising performance. Recently, numerous new approaches with different\nregularization terms, transforms and advanced deep neural network (DNN)\narchitectures are proposed to improve denoising quality. In this paper, we\nextensively compare over 60 methods on both synthetic and real-world datasets.\nWe also introduce a new color image and video dataset for benchmarking, and our\nevaluations are performed from four different perspectives including\nquantitative metrics, visual effects, human ratings and computational cost.\nComprehensive experiments demonstrate: (i) the effectiveness and efficiency of\nthe BM3D family for various denoising tasks, (ii) a simple matrix-based\nalgorithm could produce similar results compared with its tensor counterparts,\nand (iii) several DNN models trained with synthetic Gaussian noise show\nstate-of-the-art performance on real-world color image and video datasets.\nDespite the progress in recent years, we discuss shortcomings and possible\nextensions of existing techniques. Datasets and codes for evaluation are made\npublicly available at https://github.com/ZhaomingKong/Denoising-Comparison.\n", "versions": [{"version": "v1", "created": "Fri, 6 Nov 2020 16:28:17 GMT"}], "update_date": "2020-11-09", "authors_parsed": [["Kong", "Zhaoming", ""], ["Yang", "Xiaowei", ""], ["He", "Lifang", ""]]}, {"id": "2011.03530", "submitter": "Brendan Shillingford", "authors": "Yi Yang, Brendan Shillingford, Yannis Assael, Miaosen Wang, Wendi Liu,\n  Yutian Chen, Yu Zhang, Eren Sezener, Luis C. Cobo, Misha Denil, Yusuf Aytar,\n  Nando de Freitas", "title": "Large-scale multilingual audio visual dubbing", "comments": "26 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe a system for large-scale audiovisual translation and dubbing,\nwhich translates videos from one language to another. The source language's\nspeech content is transcribed to text, translated, and automatically\nsynthesized into target language speech using the original speaker's voice. The\nvisual content is translated by synthesizing lip movements for the speaker to\nmatch the translated audio, creating a seamless audiovisual experience in the\ntarget language. The audio and visual translation subsystems each contain a\nlarge-scale generic synthesis model trained on thousands of hours of data in\nthe corresponding domain. These generic models are fine-tuned to a specific\nspeaker before translation, either using an auxiliary corpus of data from the\ntarget speaker, or using the video to be translated itself as the input to the\nfine-tuning process. This report gives an architectural overview of the full\nsystem, as well as an in-depth discussion of the video dubbing component. The\nrole of the audio and text components in relation to the full system is\noutlined, but their design is not discussed in detail. Translated and dubbed\ndemo videos generated using our system can be viewed at\nhttps://www.youtube.com/playlist?list=PLSi232j2ZA6_1Exhof5vndzyfbxAhhEs5\n", "versions": [{"version": "v1", "created": "Fri, 6 Nov 2020 18:58:15 GMT"}], "update_date": "2020-11-09", "authors_parsed": [["Yang", "Yi", ""], ["Shillingford", "Brendan", ""], ["Assael", "Yannis", ""], ["Wang", "Miaosen", ""], ["Liu", "Wendi", ""], ["Chen", "Yutian", ""], ["Zhang", "Yu", ""], ["Sezener", "Eren", ""], ["Cobo", "Luis C.", ""], ["Denil", "Misha", ""], ["Aytar", "Yusuf", ""], ["de Freitas", "Nando", ""]]}, {"id": "2011.03577", "submitter": "Philipp Andermatt", "authors": "Philipp Andermatt, Radu Timofte", "title": "A Weakly Supervised Convolutional Network for Change Segmentation and\n  Classification", "comments": "17 pages, 5 figures. Accepted at the Machine Learning and Computing\n  for Visual Semantic Analysis (MLCSA2020) Workshop which is held in\n  conjunction with ACCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fully supervised change detection methods require difficult to procure\npixel-level labels, while weakly supervised approaches can be trained with\nimage-level labels. However, most of these approaches require a combination of\nchanged and unchanged image pairs for training. Thus, these methods can not\ndirectly be used for datasets where only changed image pairs are available. We\npresent W-CDNet, a novel weakly supervised change detection network that can be\ntrained with image-level semantic labels. Additionally, W-CDNet can be trained\nwith two different types of datasets, either containing changed image pairs\nonly or a mixture of changed and unchanged image pairs. Since we use\nimage-level semantic labels for training, we simultaneously create a change\nmask and label the changed object for single-label images. W-CDNet employs a\nW-shaped siamese U-net to extract feature maps from an image pair which then\nget compared in order to create a raw change mask. The core part of our model,\nthe Change Segmentation and Classification (CSC) module, learns an accurate\nchange mask at a hidden layer by using a custom Remapping Block and then\nsegmenting the current input image with the change mask. The segmented image is\nused to predict the image-level semantic label. The correct label can only be\npredicted if the change mask actually marks relevant change. This forces the\nmodel to learn an accurate change mask. We demonstrate the segmentation and\nclassification performance of our approach and achieve top results on AICD and\nHRSCD, two public aerial imaging change detection datasets as well as on a Food\nWaste change detection dataset. Our code is available at\nhttps://github.com/PhiAbs/W-CDNet .\n", "versions": [{"version": "v1", "created": "Fri, 6 Nov 2020 20:20:45 GMT"}], "update_date": "2020-11-10", "authors_parsed": [["Andermatt", "Philipp", ""], ["Timofte", "Radu", ""]]}, {"id": "2011.03585", "submitter": "Xiao Qi", "authors": "Xiao Qi, Lloyd Brown, David J. Foran, Ilker Hacihaliloglu", "title": "Chest X-ray Image Phase Features for Improved Diagnosis of COVID-19\n  Using Convolutional Neural Network", "comments": "16 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, the outbreak of the novel Coronavirus disease 2019 (COVID-19)\npandemic has seriously endangered human health and life. Due to limited\navailability of test kits, the need for auxiliary diagnostic approach has\nincreased. Recent research has shown radiography of COVID-19 patient, such as\nCT and X-ray, contains salient information about the COVID-19 virus and could\nbe used as an alternative diagnosis method. Chest X-ray (CXR) due to its faster\nimaging time, wide availability, low cost and portability gains much attention\nand becomes very promising. Computational methods with high accuracy and\nrobustness are required for rapid triaging of patients and aiding radiologist\nin the interpretation of the collected data. In this study, we design a novel\nmulti-feature convolutional neural network (CNN) architecture for multi-class\nimproved classification of COVID-19 from CXR images. CXR images are enhanced\nusing a local phase-based image enhancement method. The enhanced images,\ntogether with the original CXR data, are used as an input to our proposed CNN\narchitecture. Using ablation studies, we show the effectiveness of the enhanced\nimages in improving the diagnostic accuracy. We provide quantitative evaluation\non two datasets and qualitative results for visual inspection. Quantitative\nevaluation is performed on data consisting of 8,851 normal (healthy), 6,045\npneumonia, and 3,323 Covid-19 CXR scans. In Dataset-1, our model achieves\n95.57\\% average accuracy for a three classes classification, 99\\% precision,\nrecall, and F1-scores for COVID-19 cases. For Dataset-2, we have obtained\n94.44\\% average accuracy, and 95\\% precision, recall, and F1-scores for\ndetection of COVID-19. Our proposed multi-feature guided CNN achieves improved\nresults compared to single-feature CNN proving the importance of the local\nphase-based CXR image enhancement\n(https://github.com/endiqq/Fus-CNNs_COVID-19).\n", "versions": [{"version": "v1", "created": "Fri, 6 Nov 2020 20:26:26 GMT"}, {"version": "v2", "created": "Wed, 14 Apr 2021 16:08:35 GMT"}], "update_date": "2021-04-15", "authors_parsed": [["Qi", "Xiao", ""], ["Brown", "Lloyd", ""], ["Foran", "David J.", ""], ["Hacihaliloglu", "Ilker", ""]]}, {"id": "2011.03614", "submitter": "Stanley Chan", "authors": "Abhiram Gnanasambandam and Stanley H. Chan", "title": "HDR Imaging with Quanta Image Sensors: Theoretical Limits and Optimal\n  Reconstruction", "comments": null, "journal-ref": "IEEE Transactions on Computational Imaging, 2020", "doi": "10.1109/TCI.2020.3041093", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High dynamic range (HDR) imaging is one of the biggest achievements in modern\nphotography. Traditional solutions to HDR imaging are designed for and applied\nto CMOS image sensors (CIS). However, the mainstream one-micron CIS cameras\ntoday generally have a high read noise and low frame-rate. These, in turn,\nlimit the acquisition speed and quality, making the cameras slow in the HDR\nmode. In this paper, we propose a new computational photography technique for\nHDR imaging. Recognizing the limitations of CIS, we use the Quanta Image Sensor\n(QIS) to trade the spatial-temporal resolution with bit-depth. QIS is a\nsingle-photon image sensor that has comparable pixel pitch to CIS but\nsubstantially lower dark current and read noise. We provide a complete\ntheoretical characterization of the sensor in the context of HDR imaging, by\nproving the fundamental limits in the dynamic range that QIS can offer and the\ntrade-offs with noise and speed. In addition, we derive an optimal\nreconstruction algorithm for single-bit and multi-bit QIS. Our algorithm is\ntheoretically optimal for \\emph{all} linear reconstruction schemes based on\nexposure bracketing. Experimental results confirm the validity of the theory\nand algorithm, based on synthetic and real QIS data.\n", "versions": [{"version": "v1", "created": "Fri, 6 Nov 2020 22:08:03 GMT"}, {"version": "v2", "created": "Wed, 2 Dec 2020 20:28:52 GMT"}], "update_date": "2020-12-04", "authors_parsed": [["Gnanasambandam", "Abhiram", ""], ["Chan", "Stanley H.", ""]]}, {"id": "2011.03630", "submitter": "Philipp Ladwig", "authors": "Philipp Ladwig, Alexander Pech, Ralf D\\\"orner and Christian Geiger", "title": "Unmasking Communication Partners: A Low-Cost AI Solution for Digitally\n  Removing Head-Mounted Displays in VR-Based Telepresence", "comments": "9 pages, IEEE 3rd International Conference on Artificial Intelligence\n  & Virtual Reality", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Face-to-face conversation in Virtual Reality (VR) is a challenge when\nparticipants wear head-mounted displays (HMD). A significant portion of a\nparticipant's face is hidden and facial expressions are difficult to perceive.\nPast research has shown that high-fidelity face reconstruction with personal\navatars in VR is possible under laboratory conditions with high-cost hardware.\nIn this paper, we propose one of the first low-cost systems for this task which\nuses only open source, free software and affordable hardware. Our approach is\nto track the user's face underneath the HMD utilizing a Convolutional Neural\nNetwork (CNN) and generate corresponding expressions with Generative\nAdversarial Networks (GAN) for producing RGBD images of the person's face. We\nuse commodity hardware with low-cost extensions such as 3D-printed mounts and\nminiature cameras. Our approach learns end-to-end without manual intervention,\nruns in real time, and can be trained and executed on an ordinary gaming\ncomputer. We report evaluation results showing that our low-cost system does\nnot achieve the same fidelity of research prototypes using high-end hardware\nand closed source software, but it is capable of creating individual facial\navatars with person-specific characteristics in movements and expressions.\n", "versions": [{"version": "v1", "created": "Fri, 6 Nov 2020 23:17:12 GMT"}], "update_date": "2020-11-10", "authors_parsed": [["Ladwig", "Philipp", ""], ["Pech", "Alexander", ""], ["D\u00f6rner", "Ralf", ""], ["Geiger", "Christian", ""]]}, {"id": "2011.03631", "submitter": "Zhigang Jia", "authors": "Yong Chen, Zhi-Gang Jia, Ya-Xin Peng, Yan Peng", "title": "Efficient Robust Watermarking Based on Quaternion Singular Value\n  Decomposition and Coefficient Pair Selection", "comments": "11 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.NA math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quaternion singular value decomposition (QSVD) is a robust technique of\ndigital watermarking which can extract high quality watermarks from watermarked\nimages with low distortion. In this paper, QSVD technique is further\ninvestigated and an efficient robust watermarking scheme is proposed. The\nimproved algebraic structure-preserving method is proposed to handle the\nproblem of \"explosion of complexity\" occurred in the conventional QSVD design.\nSecret information is transmitted blindly by incorporating in QSVD two new\nstrategies, namely, coefficient pair selection and adaptive embedding. Unlike\nconventional QSVD which embeds watermarks in a single imaginary unit, we\npropose to adaptively embed the watermark into the optimal hiding position\nusing the Normalized Cross-Correlation (NC) method. This avoids the selection\nof coefficient pair with less correlation, and thus, it reduces embedding\nimpact by decreasing the maximum modification of coefficient values. In this\nway, compared with conventional QSVD, the proposed watermarking strategy avoids\nmore modifications to a single color image layer and a better visual quality of\nthe watermarked image is observed. Meanwhile, adaptive QSVD resists some common\ngeometric attacks, and it improves the robustness of conventional QSVD. With\nthese improvements, our method outperforms conventional QSVD. Its superiority\nover other state-of-the-art methods is also demonstrated experimentally.\n", "versions": [{"version": "v1", "created": "Fri, 6 Nov 2020 23:27:36 GMT"}], "update_date": "2020-11-10", "authors_parsed": [["Chen", "Yong", ""], ["Jia", "Zhi-Gang", ""], ["Peng", "Ya-Xin", ""], ["Peng", "Yan", ""]]}, {"id": "2011.03633", "submitter": "Yaochen Xie", "authors": "Yaochen Xie, Yu Ding, Shuiwang Ji", "title": "Augmented Equivariant Attention Networks for Microscopy Image\n  Reconstruction", "comments": "11 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is time-consuming and expensive to take high-quality or high-resolution\nelectron microscopy (EM) and fluorescence microscopy (FM) images. Taking these\nimages could be even invasive to samples and may damage certain subtleties in\nthe samples after long or intense exposures, often necessary for achieving\nhigh-quality or high resolution in the first place. Advances in deep learning\nenable us to perform image-to-image transformation tasks for various types of\nmicroscopy image reconstruction, computationally producing high-quality images\nfrom the physically acquired low-quality ones. When training image-to-image\ntransformation models on pairs of experimentally acquired microscopy images,\nprior models suffer from performance loss due to their inability to capture\ninter-image dependencies and common features shared among images. Existing\nmethods that take advantage of shared features in image classification tasks\ncannot be properly applied to image reconstruction tasks because they fail to\npreserve the equivariance property under spatial permutations, something\nessential in image-to-image transformation. To address these limitations, we\npropose the augmented equivariant attention networks (AEANets) with better\ncapability to capture inter-image dependencies, while preserving the\nequivariance property. The proposed AEANets captures inter-image dependencies\nand shared features via two augmentations on the attention mechanism, which are\nthe shared references and the batch-aware attention during training. We\ntheoretically derive the equivariance property of the proposed augmented\nattention model and experimentally demonstrate its consistent superiority in\nboth quantitative and visual results over the baseline methods.\n", "versions": [{"version": "v1", "created": "Fri, 6 Nov 2020 23:37:49 GMT"}, {"version": "v2", "created": "Thu, 10 Dec 2020 17:29:29 GMT"}, {"version": "v3", "created": "Sun, 4 Apr 2021 01:01:57 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Xie", "Yaochen", ""], ["Ding", "Yu", ""], ["Ji", "Shuiwang", ""]]}, {"id": "2011.03635", "submitter": "Shahrokh Paravarzar", "authors": "Shahrokh Paravarzar and Belqes Mohammad", "title": "Motion Prediction on Self-driving Cars: A Review", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The autonomous vehicle motion prediction literature is reviewed. Motion\nprediction is the most challenging task in autonomous vehicles and self-drive\ncars. These challenges have been discussed. Later on, the state-of-theart has\nreviewed based on the most recent literature and the current challenges are\ndiscussed. The state-of-the-art consists of classical and physical methods,\ndeep learning networks, and reinforcement learning. prons and cons of the\nmethods and gap of the research presented in this review. Finally, the\nliterature surrounding object tracking and motion will be presented. As a\nresult, deep reinforcement learning is the best candidate to tackle\nself-driving cars.\n", "versions": [{"version": "v1", "created": "Fri, 6 Nov 2020 23:40:37 GMT"}], "update_date": "2020-11-10", "authors_parsed": [["Paravarzar", "Shahrokh", ""], ["Mohammad", "Belqes", ""]]}, {"id": "2011.03651", "submitter": "Pedro Machado", "authors": "Samuel Brandenburg, Pedro Machado, Nikesh Lama, T.M. McGinnity", "title": "Strawberry Detection Using a Heterogeneous Multi-Processor Platform", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Over the last few years, the number of precision farming projects has\nincreased specifically in harvesting robots and many of which have made\ncontinued progress from identifying crops to grasping the desired fruit or\nvegetable. One of the most common issues found in precision farming projects is\nthat successful application is heavily dependent not just on identifying the\nfruit but also on ensuring that localisation allows for accurate navigation.\nThese issues become significant factors when the robot is not operating in a\nprearranged environment, or when vegetation becomes too thick, thus covering\ncrop. Moreover, running a state-of-the-art deep learning algorithm on an\nembedded platform is also very challenging, resulting most of the times in low\nframe rates. This paper proposes using the You Only Look Once version 3\n(YOLOv3) Convolutional Neural Network (CNN) in combination with utilising image\nprocessing techniques for the application of precision farming robots targeting\nstrawberry detection, accelerated on a heterogeneous multiprocessor platform.\nThe results show a performance acceleration by five times when implemented on a\nField-Programmable Gate Array (FPGA) when compared with the same algorithm\nrunning on the processor side with an accuracy of 78.3\\% over the test set\ncomprised of 146 images.\n", "versions": [{"version": "v1", "created": "Sat, 7 Nov 2020 01:08:21 GMT"}], "update_date": "2020-11-10", "authors_parsed": [["Brandenburg", "Samuel", ""], ["Machado", "Pedro", ""], ["Lama", "Nikesh", ""], ["McGinnity", "T. M.", ""]]}, {"id": "2011.03659", "submitter": "Jingnan Shi", "authors": "Jingnan Shi, Heng Yang, Luca Carlone", "title": "ROBIN: a Graph-Theoretic Approach to Reject Outliers in Robust\n  Estimation using Invariants", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many estimation problems in robotics, computer vision, and learning require\nestimating unknown quantities in the face of outliers. Outliers are typically\nthe result of incorrect data association or feature matching, and it is common\nto have problems where more than 90% of the measurements used for estimation\nare outliers. While current approaches for robust estimation are able to deal\nwith moderate amounts of outliers, they fail to produce accurate estimates in\nthe presence of many outliers. This paper develops an approach to prune\noutliers. First, we develop a theory of invariance that allows us to quickly\ncheck if a subset of measurements are mutually compatible without explicitly\nsolving the estimation problem. Second, we develop a graph-theoretic framework,\nwhere measurements are modeled as vertices and mutual compatibility is captured\nby edges. We generalize existing results showing that the inliers form a clique\nin this graph and typically belong to the maximum clique. We also show that in\npractice the maximum k-core of the compatibility graph provides an\napproximation of the maximum clique, while being faster to compute in large\nproblems. These two contributions leads to ROBIN, our approach to Reject\nOutliers Based on INvariants, which allows us to quickly prune outliers in\ngeneric estimation problems. We demonstrate ROBIN in four geometric perception\nproblems and show it boosts robustness of existing solvers while running in\nmilliseconds in large problems.\n", "versions": [{"version": "v1", "created": "Sat, 7 Nov 2020 02:09:33 GMT"}, {"version": "v2", "created": "Tue, 23 Mar 2021 20:02:00 GMT"}], "update_date": "2021-03-25", "authors_parsed": [["Shi", "Jingnan", ""], ["Yang", "Heng", ""], ["Carlone", "Luca", ""]]}, {"id": "2011.03667", "submitter": "Yunhao Yang", "authors": "Yunhao Yang, Andrew Whinston", "title": "Identifying Mislabeled Images in Supervised Learning Utilizing\n  Autoencoder", "comments": "UTCS Tech Report: Honors Thesis. 12 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Supervised learning is based on the assumption that the ground truth in the\ntraining data is accurate. However, this may not be guaranteed in real-world\nsettings. Inaccurate training data will result in some unexpected predictions.\nIn image classification, incorrect labels may cause the classification model to\nbe inaccurate as well. In this paper, I am going to apply unsupervised\ntechniques to the training data before training the classification network. A\nconvolutional autoencoder is applied to encode and reconstruct images. The\nencoder will project the image data on to latent space. In the latent space,\nimage features are preserved in a lower dimension. The assumption is that data\nsamples with similar features are likely to have the same label. Noised samples\ncan be classified in the latent space by the Density-Base Scan (DBSCAN)\nclustering algorithm. These incorrectly labeled data are visualized as outliers\nin the latent space. Therefore, the outliers identified by the DBSCAN algorithm\ncan be classified as incorrectly labeled samples. After the outliers are\ndetected, all the outliers are treated as mislabeled data samples and removed\nfrom the dataset. Thus the training data can be directly used in training the\nsupervised learning network. The algorithm can detect and remove above 67\\% of\nmislabeled data in the experimental dataset.\n", "versions": [{"version": "v1", "created": "Sat, 7 Nov 2020 03:09:34 GMT"}, {"version": "v2", "created": "Mon, 18 Jan 2021 22:59:44 GMT"}], "update_date": "2021-01-20", "authors_parsed": [["Yang", "Yunhao", ""], ["Whinston", "Andrew", ""]]}, {"id": "2011.03677", "submitter": "Murari Mandal", "authors": "Aditya Mehta, Harsh Sinha, Murari Mandal, Pratik Narang", "title": "Domain-Aware Unsupervised Hyperspectral Reconstruction for Aerial Image\n  Dehazing", "comments": "WACV-2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Haze removal in aerial images is a challenging problem due to considerable\nvariation in spatial details and varying contrast. Changes in particulate\nmatter density often lead to degradation in visibility. Therefore, several\napproaches utilize multi-spectral data as auxiliary information for haze\nremoval. In this paper, we propose SkyGAN for haze removal in aerial images.\nSkyGAN consists of 1) a domain-aware hazy-to-hyperspectral (H2H) module, and 2)\na conditional GAN (cGAN) based multi-cue image-to-image translation module\n(I2I) for dehazing. The proposed H2H module reconstructs several visual bands\nfrom RGB images in an unsupervised manner, which overcomes the lack of hazy\nhyperspectral aerial image datasets. The module utilizes task supervision and\ndomain adaptation in order to create a \"hyperspectral catalyst\" for image\ndehazing. The I2I module uses the hyperspectral catalyst along with a\n12-channel multi-cue input and performs effective image dehazing by utilizing\nthe entire visual spectrum. In addition, this work introduces a new dataset,\ncalled Hazy Aerial-Image (HAI) dataset, that contains more than 65,000 pairs of\nhazy and ground truth aerial images with realistic, non-homogeneous haze of\nvarying density. The performance of SkyGAN is evaluated on the recent\nSateHaze1k dataset as well as the HAI dataset. We also present a comprehensive\nevaluation of HAI dataset with a representative set of state-of-the-art\ntechniques in terms of PSNR and SSIM.\n", "versions": [{"version": "v1", "created": "Sat, 7 Nov 2020 03:30:52 GMT"}], "update_date": "2020-11-10", "authors_parsed": [["Mehta", "Aditya", ""], ["Sinha", "Harsh", ""], ["Mandal", "Murari", ""], ["Narang", "Pratik", ""]]}, {"id": "2011.03683", "submitter": "Shenghua He", "authors": "Shenghua He, Kyaw Thu Minn, Lilianna Solnica-Krezel, Mark A. Anastasio\n  and Hua Li", "title": "Deeply-Supervised Density Regression for Automatic Cell Counting in\n  Microscopy Images", "comments": "Medical Image Analysis 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurately counting the number of cells in microscopy images is required in\nmany medical diagnosis and biological studies. This task is tedious,\ntime-consuming, and prone to subjective errors. However, designing automatic\ncounting methods remains challenging due to low image contrast, complex\nbackground, large variance in cell shapes and counts, and significant cell\nocclusions in two-dimensional microscopy images. In this study, we proposed a\nnew density regression-based method for automatically counting cells in\nmicroscopy images. The proposed method processes two innovations compared to\nother state-of-the-art density regression-based methods. First, the density\nregression model (DRM) is designed as a concatenated fully convolutional\nregression network (C-FCRN) to employ multi-scale image features for the\nestimation of cell density maps from given images. Second, auxiliary\nconvolutional neural networks (AuxCNNs) are employed to assist in the training\nof intermediate layers of the designed C-FCRN to improve the DRM performance on\nunseen datasets. Experimental studies evaluated on four datasets demonstrate\nthe superior performance of the proposed method.\n", "versions": [{"version": "v1", "created": "Sat, 7 Nov 2020 04:02:47 GMT"}, {"version": "v2", "created": "Tue, 10 Nov 2020 01:57:30 GMT"}], "update_date": "2020-11-11", "authors_parsed": [["He", "Shenghua", ""], ["Minn", "Kyaw Thu", ""], ["Solnica-Krezel", "Lilianna", ""], ["Anastasio", "Mark A.", ""], ["Li", "Hua", ""]]}, {"id": "2011.03696", "submitter": "Jia Peng", "authors": "Peng Jia, Ruiyu Ning, Ruiqi Sun, Xiaoshan Yang and Dongmei Cai", "title": "Data--driven Image Restoration with Option--driven Learning for Big and\n  Small Astronomical Image Datasets", "comments": "11 pages. Submitted to MNRAS with minor revision", "journal-ref": null, "doi": "10.1093/mnras/staa3535", "report-no": null, "categories": "astro-ph.IM astro-ph.GA astro-ph.SR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image restoration methods are commonly used to improve the quality of\nastronomical images. In recent years, developments of deep neural networks and\nincrements of the number of astronomical images have evoked a lot of\ndata--driven image restoration methods. However, most of these methods belong\nto supervised learning algorithms, which require paired images either from real\nobservations or simulated data as training set. For some applications, it is\nhard to get enough paired images from real observations and simulated images\nare quite different from real observed ones. In this paper, we propose a new\ndata--driven image restoration method based on generative adversarial networks\nwith option--driven learning. Our method uses several high resolution images as\nreferences and applies different learning strategies when the number of\nreference images is different. For sky surveys with variable observation\nconditions, our method can obtain very stable image restoration results,\nregardless of the number of reference images.\n", "versions": [{"version": "v1", "created": "Sat, 7 Nov 2020 05:05:55 GMT"}], "update_date": "2020-11-25", "authors_parsed": [["Jia", "Peng", ""], ["Ning", "Ruiyu", ""], ["Sun", "Ruiqi", ""], ["Yang", "Xiaoshan", ""], ["Cai", "Dongmei", ""]]}, {"id": "2011.03701", "submitter": "Guoqing Bao", "authors": "Guoqing Bao, Manuel B. Graeber and Xiuying Wang", "title": "Depthwise Multiception Convolution for Reducing Network Parameters\n  without Sacrificing Accuracy", "comments": "This paper was accepted by ICARCV 2020", "journal-ref": null, "doi": "10.1109/ICARCV50220.2020.9305369", "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep convolutional neural networks have been proven successful in multiple\nbenchmark challenges in recent years. However, the performance improvements are\nheavily reliant on increasingly complex network architecture and a high number\nof parameters, which require ever increasing amounts of storage and memory\ncapacity. Depthwise separable convolution (DSConv) can effectively reduce the\nnumber of required parameters through decoupling standard convolution into\nspatial and cross-channel convolution steps. However, the method causes a\ndegradation of accuracy. To address this problem, we present depthwise\nmultiception convolution, termed Multiception, which introduces layer-wise\nmultiscale kernels to learn multiscale representations of all individual input\nchannels simultaneously. We have carried out the experiment on four benchmark\ndatasets, i.e. Cifar-10, Cifar-100, STL-10 and ImageNet32x32, using five\npopular CNN models, Multiception achieved accuracy promotion in all models and\ndemonstrated higher accuracy performance compared to related works. Meanwhile,\nMultiception significantly reduces the number of parameters of standard\nconvolution-based models by 32.48% on average while still preserving accuracy.\n", "versions": [{"version": "v1", "created": "Sat, 7 Nov 2020 05:33:54 GMT"}], "update_date": "2021-02-01", "authors_parsed": [["Bao", "Guoqing", ""], ["Graeber", "Manuel B.", ""], ["Wang", "Xiuying", ""]]}, {"id": "2011.03703", "submitter": "Yujia Zhang", "authors": "Yujia Zhang, Qianzhong Li, Xiaoguang Zhao, Min Tan", "title": "TB-Net: A Three-Stream Boundary-Aware Network for Fine-Grained Pavement\n  Disease Segmentation", "comments": "WACV 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Regular pavement inspection plays a significant role in road maintenance for\nsafety assurance. Existing methods mainly address the tasks of crack detection\nand segmentation that are only tailored for long-thin crack disease. However,\nthere are many other types of diseases with a wider variety of sizes and\npatterns that are also essential to segment in practice, bringing more\nchallenges towards fine-grained pavement inspection. In this paper, our goal is\nnot only to automatically segment cracks, but also to segment other complex\npavement diseases as well as typical landmarks (markings, runway lights, etc.)\nand commonly seen water/oil stains in a single model. To this end, we propose a\nthree-stream boundary-aware network (TB-Net). It consists of three streams\nfusing the low-level spatial and the high-level contextual representations as\nwell as the detailed boundary information. Specifically, the spatial stream\ncaptures rich spatial features. The context stream, where an attention\nmechanism is utilized, models the contextual relationships over local features.\nThe boundary stream learns detailed boundaries using a global-gated convolution\nto further refine the segmentation outputs. The network is trained using a\ndual-task loss in an end-to-end manner, and experiments on a newly collected\nfine-grained pavement disease dataset show the effectiveness of our TB-Net.\n", "versions": [{"version": "v1", "created": "Sat, 7 Nov 2020 05:39:31 GMT"}], "update_date": "2020-11-10", "authors_parsed": [["Zhang", "Yujia", ""], ["Li", "Qianzhong", ""], ["Zhao", "Xiaoguang", ""], ["Tan", "Min", ""]]}, {"id": "2011.03705", "submitter": "Indra Deep Mastan", "authors": "Harshil Jain, Rohit Patil, Indra Deep Mastan, and Shanmuganathan Raman", "title": "Blind Motion Deblurring through SinGAN Architecture", "comments": "Deep Internal Learning: Training with no prior examples. ECCV'2020\n  Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Blind motion deblurring involves reconstructing a sharp image from an\nobservation that is blurry. It is a problem that is ill-posed and lies in the\ncategories of image restoration problems. The training data-based methods for\nimage deblurring mostly involve training models that take a lot of time. These\nmodels are data-hungry i.e., they require a lot of training data to generate\nsatisfactory results. Recently, there are various image feature learning\nmethods developed which relieve us of the need for training data and perform\nimage restoration and image synthesis, e.g., DIP, InGAN, and SinGAN. SinGAN is\na generative model that is unconditional and could be learned from a single\nnatural image. This model primarily captures the internal distribution of the\npatches which are present in the image and is capable of generating samples of\nvaried diversity while preserving the visual content of the image. Images\ngenerated from the model are very much like real natural images. In this paper,\nwe focus on blind motion deblurring through SinGAN architecture.\n", "versions": [{"version": "v1", "created": "Sat, 7 Nov 2020 06:09:16 GMT"}], "update_date": "2020-11-10", "authors_parsed": [["Jain", "Harshil", ""], ["Patil", "Rohit", ""], ["Mastan", "Indra Deep", ""], ["Raman", "Shanmuganathan", ""]]}, {"id": "2011.03712", "submitter": "Indra Deep Mastan", "authors": "Indra Deep Mastan and Shanmuganathan Raman", "title": "DeepCFL: Deep Contextual Features Learning from a Single Image", "comments": "IEEE Winter Conference on Applications of Computer Vision (WACV\n  2021), Waikoloa, US, Jan. 5-9, 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recently, there is a vast interest in developing image feature learning\nmethods that are independent of the training data, such as deep image prior,\nInGAN, SinGAN, and DCIL. These methods are unsupervised and are used to perform\nlow-level vision tasks such as image restoration, image editing, and image\nsynthesis. In this work, we proposed a new training data-independent framework,\ncalled Deep Contextual Features Learning (DeepCFL), to perform image synthesis\nand image restoration based on the semantics of the input image. The contextual\nfeatures are simply the high dimensional vectors representing the semantics of\nthe given image. DeepCFL is a single image GAN framework that learns the\ndistribution of the context vectors from the input image. We show the\nperformance of contextual learning in various challenging scenarios:\noutpainting, inpainting, and restoration of randomly removed pixels. DeepCFL is\napplicable when the input source image and the generated target image are not\naligned. We illustrate image synthesis using DeepCFL for the task of image\nresizing.\n", "versions": [{"version": "v1", "created": "Sat, 7 Nov 2020 06:54:59 GMT"}], "update_date": "2020-11-10", "authors_parsed": [["Mastan", "Indra Deep", ""], ["Raman", "Shanmuganathan", ""]]}, {"id": "2011.03721", "submitter": "Liangzi Rong", "authors": "Liangzi Rong, Chunping Li", "title": "Coarse- and Fine-grained Attention Network with Background-aware Loss\n  for Crowd Density Map Estimation", "comments": "Accepted by WACV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a novel method Coarse- and Fine-grained Attention\nNetwork (CFANet) for generating high-quality crowd density maps and people\ncount estimation by incorporating attention maps to better focus on the crowd\narea. We devise a from-coarse-to-fine progressive attention mechanism by\nintegrating Crowd Region Recognizer (CRR) and Density Level Estimator (DLE)\nbranch, which can suppress the influence of irrelevant background and assign\nattention weights according to the crowd density levels, because generating\naccurate fine-grained attention maps directly is normally difficult. We also\nemploy a multi-level supervision mechanism to assist the backpropagation of\ngradient and reduce overfitting. Besides, we propose a Background-aware\nStructural Loss (BSL) to reduce the false recognition ratio while improving the\nstructural similarity to groundtruth. Extensive experiments on commonly used\ndatasets show that our method can not only outperform previous state-of-the-art\nmethods in terms of count accuracy but also improve the image quality of\ndensity maps as well as reduce the false recognition ratio.\n", "versions": [{"version": "v1", "created": "Sat, 7 Nov 2020 08:05:54 GMT"}], "update_date": "2020-11-10", "authors_parsed": [["Rong", "Liangzi", ""], ["Li", "Chunping", ""]]}, {"id": "2011.03725", "submitter": "Liangzi Rong", "authors": "Liangzi Rong, Chunping Li", "title": "A Strong Baseline for Crowd Counting and Unsupervised People\n  Localization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we explore a strong baseline for crowd counting and an\nunsupervised people localization algorithm based on estimated density maps.\nFirstly, existing methods achieve state-of-the-art performance based on\ndifferent backbones and kinds of training tricks. We collect different\nbackbones and training tricks and evaluate the impact of changing them and\ndevelop an efficient pipeline for crowd counting, which decreases MAE and RMSE\nsignificantly on multiple datasets. We also propose a clustering algorithm\nnamed isolated KMeans to locate the heads in density maps. This method can\ndivide the density maps into subregions and find the centers under local count\nconstraints without training any parameter and can be integrated with existing\nmethods easily.\n", "versions": [{"version": "v1", "created": "Sat, 7 Nov 2020 08:29:03 GMT"}], "update_date": "2020-11-10", "authors_parsed": [["Rong", "Liangzi", ""], ["Li", "Chunping", ""]]}, {"id": "2011.03737", "submitter": "Jun Wen", "authors": "Jun Wen, Changjian Shui, Kun Kuang, Junsong Yuan, Zenan Huang, Zhefeng\n  Gong, Nenggan Zheng", "title": "Interventional Domain Adaptation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Domain adaptation (DA) aims to transfer discriminative features learned from\nsource domain to target domain. Most of DA methods focus on enhancing feature\ntransferability through domain-invariance learning. However, source-learned\ndiscriminability itself might be tailored to be biased and unsafely\ntransferable by spurious correlations, \\emph{i.e.}, part of source-specific\nfeatures are correlated with category labels. We find that standard\ndomain-invariance learning suffers from such correlations and incorrectly\ntransfers the source-specifics. To address this issue, we intervene in the\nlearning of feature discriminability using unlabeled target data to guide it to\nget rid of the domain-specific part and be safely transferable. Concretely, we\ngenerate counterfactual features that distinguish the domain-specifics from\ndomain-sharable part through a novel feature intervention strategy. To prevent\nthe residence of domain-specifics, the feature discriminability is trained to\nbe invariant to the mutations in the domain-specifics of counterfactual\nfeatures. Experimenting on typical \\emph{one-to-one} unsupervised domain\nadaptation and challenging domain-agnostic adaptation tasks, the consistent\nperformance improvements of our method over state-of-the-art approaches\nvalidate that the learned discriminative features are more safely transferable\nand generalize well to novel domains.\n", "versions": [{"version": "v1", "created": "Sat, 7 Nov 2020 09:53:13 GMT"}], "update_date": "2020-11-10", "authors_parsed": [["Wen", "Jun", ""], ["Shui", "Changjian", ""], ["Kuang", "Kun", ""], ["Yuan", "Junsong", ""], ["Huang", "Zenan", ""], ["Gong", "Zhefeng", ""], ["Zheng", "Nenggan", ""]]}, {"id": "2011.03749", "submitter": "Pengchao Han", "authors": "Pengchao Han, Jihong Park, Shiqiang Wang, Yejun Liu", "title": "Robustness and Diversity Seeking Data-Free Knowledge Distillation", "comments": "Accepted in IEEE ICASSP 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.SY eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Knowledge distillation (KD) has enabled remarkable progress in model\ncompression and knowledge transfer. However, KD requires a large volume of\noriginal data or their representation statistics that are not usually available\nin practice. Data-free KD has recently been proposed to resolve this problem,\nwherein teacher and student models are fed by a synthetic sample generator\ntrained from the teacher. Nonetheless, existing data-free KD methods rely on\nfine-tuning of weights to balance multiple losses, and ignore the diversity of\ngenerated samples, resulting in limited accuracy and robustness. To overcome\nthis challenge, we propose robustness and diversity seeking data-free KD\n(RDSKD) in this paper. The generator loss function is crafted to produce\nsamples with high authenticity, class diversity, and inter-sample diversity.\nWithout real data, the objectives of seeking high sample authenticity and class\ndiversity often conflict with each other, causing frequent loss fluctuations.\nWe mitigate this by exponentially penalizing loss increments. With MNIST,\nCIFAR-10, and SVHN datasets, our experiments show that RDSKD achieves higher\naccuracy with more robustness over different hyperparameter settings, compared\nto other data-free KD methods such as DAFL, MSKD, ZSKD, and DeepInversion.\n", "versions": [{"version": "v1", "created": "Sat, 7 Nov 2020 10:57:53 GMT"}, {"version": "v2", "created": "Sat, 6 Feb 2021 03:45:51 GMT"}, {"version": "v3", "created": "Wed, 10 Feb 2021 09:47:13 GMT"}], "update_date": "2021-02-11", "authors_parsed": [["Han", "Pengchao", ""], ["Park", "Jihong", ""], ["Wang", "Shiqiang", ""], ["Liu", "Yejun", ""]]}, {"id": "2011.03756", "submitter": "Jinming Liu", "authors": "Jinming Liu, Ke Li, Baolin Song, Li Zhao", "title": "A Multi-stream Convolutional Neural Network for Micro-expression\n  Recognition Using Optical Flow and EVM", "comments": "ICEIC 2020, Barcelona, Spain, January 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Micro-expression (ME) recognition plays a crucial role in a wide range of\napplications, particularly in public security and psychotherapy. Recently,\ntraditional methods rely excessively on machine learning design and the\nrecognition rate is not high enough for its practical application because of\nits short duration and low intensity. On the other hand, some methods based on\ndeep learning also cannot get high accuracy due to problems such as the\nimbalance of databases. To address these problems, we design a multi-stream\nconvolutional neural network (MSCNN) for ME recognition in this paper.\nSpecifically, we employ EVM and optical flow to magnify and visualize subtle\nmovement changes in MEs and extract the masks from the optical flow images. And\nthen, we add the masks, optical flow images, and grayscale images into the\nMSCNN. After that, in order to overcome the imbalance of databases, we added a\nrandom over-sampler after the Dense Layer of the neural network. Finally,\nextensive experiments are conducted on two public ME databases: CASME II and\nSAMM. Compared with many recent state-of-the-art approaches, our method\nachieves more promising recognition results.\n", "versions": [{"version": "v1", "created": "Sat, 7 Nov 2020 11:28:53 GMT"}, {"version": "v2", "created": "Tue, 10 Nov 2020 10:34:27 GMT"}], "update_date": "2020-11-11", "authors_parsed": [["Liu", "Jinming", ""], ["Li", "Ke", ""], ["Song", "Baolin", ""], ["Zhao", "Li", ""]]}, {"id": "2011.03772", "submitter": "Liangzhi Li", "authors": "Liangzhi Li, Manisha Verma, Bowen Wang, Yuta Nakashima, Ryo Kawasaki,\n  Hajime Nagahara", "title": "Grading the Severity of Arteriolosclerosis from Retinal Arterio-venous\n  Crossing Patterns", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The status of retinal arteriovenous crossing is of great significance for\nclinical evaluation of arteriolosclerosis and systemic hypertension. As an\nophthalmology diagnostic criteria, Scheie's classification has been used to\ngrade the severity of arteriolosclerosis. In this paper, we propose a deep\nlearning approach to support the diagnosis process, which, to the best of our\nknowledge, is one of the earliest attempts in medical imaging. The proposed\npipeline is three-fold. First, we adopt segmentation and classification models\nto automatically obtain vessels in a retinal image with the corresponding\nartery/vein labels and find candidate arteriovenous crossing points. Second, we\nuse a classification model to validate the true crossing point. At last, the\ngrade of severity for the vessel crossings is classified. To better address the\nproblem of label ambiguity and imbalanced label distribution, we propose a new\nmodel, named multi-diagnosis team network (MDTNet), in which the sub-models\nwith different structures or different loss functions provide different\ndecisions. MDTNet unifies these diverse theories to give the final decision\nwith high accuracy. Our severity grading method was able to validate crossing\npoints with precision and recall of 96.3% and 96.3%, respectively. Among\ncorrectly detected crossing points, the kappa value for the agreement between\nthe grading by a retina specialist and the estimated score was 0.85, with an\naccuracy of 0.92. The numerical results demonstrate that our method can achieve\na good performance in both arteriovenous crossing validation and severity\ngrading tasks. By the proposed models, we could build a pipeline reproducing\nretina specialist's subjective grading without feature extractions. The code is\navailable for reproducibility.\n", "versions": [{"version": "v1", "created": "Sat, 7 Nov 2020 13:15:17 GMT"}], "update_date": "2020-11-10", "authors_parsed": [["Li", "Liangzhi", ""], ["Verma", "Manisha", ""], ["Wang", "Bowen", ""], ["Nakashima", "Yuta", ""], ["Kawasaki", "Ryo", ""], ["Nagahara", "Hajime", ""]]}, {"id": "2011.03775", "submitter": "Jing Yu Koh", "authors": "Jing Yu Koh, Jason Baldridge, Honglak Lee, Yinfei Yang", "title": "Text-to-Image Generation Grounded by Fine-Grained User Attention", "comments": "To appear in WACV 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Localized Narratives is a dataset with detailed natural language descriptions\nof images paired with mouse traces that provide a sparse, fine-grained visual\ngrounding for phrases. We propose TReCS, a sequential model that exploits this\ngrounding to generate images. TReCS uses descriptions to retrieve segmentation\nmasks and predict object labels aligned with mouse traces. These alignments are\nused to select and position masks to generate a fully covered segmentation\ncanvas; the final image is produced by a segmentation-to-image generator using\nthis canvas. This multi-step, retrieval-based approach outperforms existing\ndirect text-to-image generation models on both automatic metrics and human\nevaluations: overall, its generated images are more photo-realistic and better\nmatch descriptions.\n", "versions": [{"version": "v1", "created": "Sat, 7 Nov 2020 13:23:31 GMT"}, {"version": "v2", "created": "Tue, 30 Mar 2021 19:52:34 GMT"}], "update_date": "2021-04-01", "authors_parsed": [["Koh", "Jing Yu", ""], ["Baldridge", "Jason", ""], ["Lee", "Honglak", ""], ["Yang", "Yinfei", ""]]}, {"id": "2011.03790", "submitter": "Rohan Pratap Singh", "authors": "Rohan Pratap Singh, Mehdi Benallegue, Yusuke Yoshiyasu, Fumio Kanehiro", "title": "Rapid Pose Label Generation through Sparse Representation of Unknown\n  Objects", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Convolutional Neural Networks (CNNs) have been successfully deployed on\nrobots for 6-DoF object pose estimation through visual perception. However,\nobtaining labeled data on a scale required for the supervised training of CNNs\nis a difficult task - exacerbated if the object is novel and a 3D model is\nunavailable. To this end, this work presents an approach for rapidly generating\nreal-world, pose-annotated RGB-D data for unknown objects. Our method not only\ncircumvents the need for a prior 3D object model (textured or otherwise) but\nalso bypasses complicated setups of fiducial markers, turntables, and sensors.\nWith the help of a human user, we first source minimalistic labelings of an\nordered set of arbitrarily chosen keypoints over a set of RGB-D videos. Then,\nby solving an optimization problem, we combine these labels under a world frame\nto recover a sparse, keypoint-based representation of the object. The sparse\nrepresentation leads to the development of a dense model and the pose labels\nfor each image frame in the set of scenes. We show that the sparse model can\nalso be efficiently used for scaling to a large number of new scenes. We\ndemonstrate the practicality of the generated labeled dataset by training a\npipeline for 6-DoF object pose estimation and a pixel-wise segmentation\nnetwork.\n", "versions": [{"version": "v1", "created": "Sat, 7 Nov 2020 15:14:03 GMT"}], "update_date": "2020-11-10", "authors_parsed": [["Singh", "Rohan Pratap", ""], ["Benallegue", "Mehdi", ""], ["Yoshiyasu", "Yusuke", ""], ["Kanehiro", "Fumio", ""]]}, {"id": "2011.03794", "submitter": "Muhammad Hassan", "authors": "Muhammad Hassan (1), Yan Wang (1), Di Wang (2), Daixi Li (3), Yanchun\n  Liang (1), You Zhou (1,2) and Dong Xu (4) ((1) Computer Science and\n  Technology, Jilin University, Changchun, (2) Joint NTU-UBC Research Centre of\n  Excellence in Active Living for the Elderly, Nanyang Technological\n  University, Singapore, (3) Everspray Science and Technology Company Ltd., (4)\n  Department of Electrical Engineering and Computer Science, University of\n  Missouri, Columbia)", "title": "Deep Learning Analysis and Age Prediction from Shoeprints", "comments": "24 pages, 20 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human walking and gaits involve several complex body parts and are influenced\nby personality, mood, social and cultural traits, and aging. These factors are\nreflected in shoeprints, which in turn can be used to predict age, a problem\nnot systematically addressed using any computational approach. We collected\n100,000 shoeprints of subjects ranging from 7 to 80 years old and used the data\nto develop a deep learning end-to-end model ShoeNet to analyze age-related\npatterns and predict age. The model integrates various convolutional neural\nnetwork models together using a skip mechanism to extract age-related features,\nespecially in pressure and abrasion regions from pair-wise shoeprints. The\nresults show that 40.23% of the subjects had prediction errors within 5-years\nof age and the prediction accuracy for gender classification reached 86.07%.\nInterestingly, the age-related features mostly reside in the asymmetric\ndifferences between left and right shoeprints. The analysis also reveals\ninteresting age-related and gender-related patterns in the pressure\ndistributions on shoeprints; in particular, the pressure forces spread from the\nmiddle of the toe toward outside regions over age with gender-specific\nvariations on heel regions. Such statistics provide insight into new methods\nfor forensic investigations, medical studies of gait-pattern disorders,\nbiometrics, and sport studies.\n", "versions": [{"version": "v1", "created": "Sat, 7 Nov 2020 15:36:11 GMT"}, {"version": "v2", "created": "Mon, 23 Nov 2020 16:37:22 GMT"}], "update_date": "2020-11-24", "authors_parsed": [["Hassan", "Muhammad", ""], ["Wang", "Yan", ""], ["Wang", "Di", ""], ["Li", "Daixi", ""], ["Liang", "Yanchun", ""], ["Zhou", "You", ""], ["Xu", "Dong", ""]]}, {"id": "2011.03799", "submitter": "Jianqiang Wang", "authors": "Jianqiang Wang, Dandan Ding, Zhu Li, Zhan Ma", "title": "Multiscale Point Cloud Geometry Compression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent years have witnessed the growth of point cloud based applications\nbecause of its realistic and fine-grained representation of 3D objects and\nscenes. However, it is a challenging problem to compress sparse, unstructured,\nand high-precision 3D points for efficient communication. In this paper,\nleveraging the sparsity nature of point cloud, we propose a multiscale\nend-to-end learning framework which hierarchically reconstructs the 3D Point\nCloud Geometry (PCG) via progressive re-sampling. The framework is developed on\ntop of a sparse convolution based autoencoder for point cloud compression and\nreconstruction. For the input PCG which has only the binary occupancy\nattribute, our framework translates it to a downscaled point cloud at the\nbottleneck layer which possesses both geometry and associated feature\nattributes. Then, the geometric occupancy is losslessly compressed using an\noctree codec and the feature attributes are lossy compressed using a learned\nprobabilistic context model.Compared to state-of-the-art Video-based Point\nCloud Compression (V-PCC) and Geometry-based PCC (G-PCC) schemes standardized\nby the Moving Picture Experts Group (MPEG), our method achieves more than 40%\nand 70% BD-Rate (Bjontegaard Delta Rate) reduction, respectively. Its encoding\nruntime is comparable to that of G-PCC, which is only 1.5% of V-PCC.\n", "versions": [{"version": "v1", "created": "Sat, 7 Nov 2020 16:11:16 GMT"}], "update_date": "2020-11-10", "authors_parsed": [["Wang", "Jianqiang", ""], ["Ding", "Dandan", ""], ["Li", "Zhu", ""], ["Ma", "Zhan", ""]]}, {"id": "2011.03802", "submitter": "Yingqian Wang", "authors": "Yingqian Wang, Xinyi Ying, Longguang Wang, Jungang Yang, Wei An, Yulan\n  Guo", "title": "Symmetric Parallax Attention for Stereo Image Super-Resolution", "comments": "Accepted to NTIRE workshop at CVPR 2021. The first two authors\n  contribute equally to this work", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although recent years have witnessed the great advances in stereo image\nsuper-resolution (SR), the beneficial information provided by binocular systems\nhas not been fully used. Since stereo images are highly symmetric under\nepipolar constraint, in this paper, we improve the performance of stereo image\nSR by exploiting symmetry cues in stereo image pairs. Specifically, we propose\na symmetric bi-directional parallax attention module (biPAM) and an inline\nocclusion handling scheme to effectively interact cross-view information. Then,\nwe design a Siamese network equipped with a biPAM to super-resolve both sides\nof views in a highly symmetric manner. Finally, we design several\nilluminance-robust losses to enhance stereo consistency. Experiments on four\npublic datasets demonstrate the superior performance of our method. Source code\nis available at https://github.com/YingqianWang/iPASSR.\n", "versions": [{"version": "v1", "created": "Sat, 7 Nov 2020 16:28:35 GMT"}, {"version": "v2", "created": "Tue, 20 Apr 2021 07:37:52 GMT"}], "update_date": "2021-04-21", "authors_parsed": [["Wang", "Yingqian", ""], ["Ying", "Xinyi", ""], ["Wang", "Longguang", ""], ["Yang", "Jungang", ""], ["An", "Wei", ""], ["Guo", "Yulan", ""]]}, {"id": "2011.03807", "submitter": "Peter Anderson", "authors": "Peter Anderson, Ayush Shrivastava, Joanne Truong, Arjun Majumdar, Devi\n  Parikh, Dhruv Batra, Stefan Lee", "title": "Sim-to-Real Transfer for Vision-and-Language Navigation", "comments": "CoRL 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the challenging problem of releasing a robot in a previously unseen\nenvironment, and having it follow unconstrained natural language navigation\ninstructions. Recent work on the task of Vision-and-Language Navigation (VLN)\nhas achieved significant progress in simulation. To assess the implications of\nthis work for robotics, we transfer a VLN agent trained in simulation to a\nphysical robot. To bridge the gap between the high-level discrete action space\nlearned by the VLN agent, and the robot's low-level continuous action space, we\npropose a subgoal model to identify nearby waypoints, and use domain\nrandomization to mitigate visual domain differences. For accurate sim and real\ncomparisons in parallel environments, we annotate a 325m2 office space with\n1.3km of navigation instructions, and create a digitized replica in simulation.\nWe find that sim-to-real transfer to an environment not seen in training is\nsuccessful if an occupancy map and navigation graph can be collected and\nannotated in advance (success rate of 46.8% vs. 55.9% in sim), but much more\nchallenging in the hardest setting with no prior mapping at all (success rate\nof 22.5%).\n", "versions": [{"version": "v1", "created": "Sat, 7 Nov 2020 16:49:04 GMT"}], "update_date": "2020-11-10", "authors_parsed": [["Anderson", "Peter", ""], ["Shrivastava", "Ayush", ""], ["Truong", "Joanne", ""], ["Majumdar", "Arjun", ""], ["Parikh", "Devi", ""], ["Batra", "Dhruv", ""], ["Lee", "Stefan", ""]]}, {"id": "2011.03822", "submitter": "Chen Chen", "authors": "Weiping Yu and Taojiannan Yang and Chen Chen", "title": "Towards Resolving the Challenge of Long-tail Distribution in UAV Images\n  for Object Detection", "comments": "WACV 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing methods for object detection in UAV images ignored an important\nchallenge - imbalanced class distribution in UAV images - which leads to poor\nperformance on tail classes. We systematically investigate existing solutions\nto long-tail problems and unveil that re-balancing methods that are effective\non natural image datasets cannot be trivially applied to UAV datasets. To this\nend, we rethink long-tailed object detection in UAV images and propose the Dual\nSampler and Head detection Network (DSHNet), which is the first work that aims\nto resolve long-tail distribution in UAV images. The key components in DSHNet\ninclude Class-Biased Samplers (CBS) and Bilateral Box Heads (BBH), which are\ndeveloped to cope with tail classes and head classes in a dual-path manner.\nWithout bells and whistles, DSHNet significantly boosts the performance of tail\nclasses on different detection frameworks. Moreover, DSHNet significantly\noutperforms base detectors and generic approaches for long-tail problems on\nVisDrone and UAVDT datasets. It achieves new state-of-the-art performance when\ncombining with image cropping methods. Code is available at\nhttps://github.com/we1pingyu/DSHNet\n", "versions": [{"version": "v1", "created": "Sat, 7 Nov 2020 17:53:12 GMT"}], "update_date": "2020-11-10", "authors_parsed": [["Yu", "Weiping", ""], ["Yang", "Taojiannan", ""], ["Chen", "Chen", ""]]}, {"id": "2011.03833", "submitter": "Negar Heidari", "authors": "Negar Heidari, Alexandros Iosifidis", "title": "On the spatial attention in Spatio-Temporal Graph Convolutional Networks\n  for skeleton-based human action recognition", "comments": "Accepted by the 2021 International Joint Conference on Neural\n  Networks (IJCNN 2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph convolutional networks (GCNs) achieved promising performance in\nskeleton-based human action recognition by modeling a sequence of skeletons as\na spatio-temporal graph. Most of the recently proposed GCN-based methods\nimprove the performance by learning the graph structure at each layer of the\nnetwork using a spatial attention applied on a predefined graph Adjacency\nmatrix that is optimized jointly with model's parameters in an end-to-end\nmanner. In this paper, we analyze the spatial attention used in spatio-temporal\nGCN layers and propose a symmetric spatial attention for better reflecting the\nsymmetric property of the relative positions of the human body joints when\nexecuting actions. We also highlight the connection of spatio-temporal GCN\nlayers employing additive spatial attention to bilinear layers, and we propose\nthe spatio-temporal bilinear network (ST-BLN) which does not require the use of\npredefined Adjacency matrices and allows for more flexible design of the model.\nExperimental results show that the three models lead to effectively the same\nperformance. Moreover, by exploiting the flexibility provided by the proposed\nST-BLN, one can increase the efficiency of the model.\n", "versions": [{"version": "v1", "created": "Sat, 7 Nov 2020 19:03:04 GMT"}, {"version": "v2", "created": "Thu, 22 Apr 2021 17:53:15 GMT"}], "update_date": "2021-04-23", "authors_parsed": [["Heidari", "Negar", ""], ["Iosifidis", "Alexandros", ""]]}, {"id": "2011.03838", "submitter": "Mashnoon Islam", "authors": "Mashnoon Islam, Touhid Ahmed, Abu Tammam Bin Nuruddin, Mashuda Islam,\n  Shahnewaz Siddique", "title": "Autonomous Intruder Detection Using a ROS-Based Multi-Robot System\n  Equipped with 2D-LiDAR Sensors", "comments": "Accepted Version, 2020 IEEE International Symposium on Safety,\n  Security, and Rescue Robotics (SSRR) November 4-6, Abu Dhabi, UAE", "journal-ref": null, "doi": "10.1109/SSRR50563.2020.9292639", "report-no": null, "categories": "cs.RO cs.CV cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The application of autonomous mobile robots in robotic security platforms is\nbecoming a promising field of innovation due to their adaptive capability of\nresponding to potential disturbances perceived through a wide range of sensors.\nResearchers have proposed systems that either focus on utilizing a single\nmobile robot or a system of cooperative multiple robots. However, very few of\nthe proposed works, particularly in the field of multi-robot systems, are\ncompletely dependent on LiDAR sensors for achieving various tasks. This is\nessential when other sensors on a robot fail to provide peak performance in\nparticular conditions, such as a camera operating in the absence of light. This\npaper proposes a multi-robot system that is developed using ROS (Robot\nOperating System) for intruder detection in a single-range-sensor-per-robot\nscenario with centralized processing of detections from all robots by our\ncentral bot MIDNet (Multiple Intruder Detection Network). This work is aimed at\nproviding an autonomous multi-robot security solution for a warehouse in the\nabsence of human personnel.\n", "versions": [{"version": "v1", "created": "Sat, 7 Nov 2020 19:49:07 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Islam", "Mashnoon", ""], ["Ahmed", "Touhid", ""], ["Nuruddin", "Abu Tammam Bin", ""], ["Islam", "Mashuda", ""], ["Siddique", "Shahnewaz", ""]]}, {"id": "2011.03841", "submitter": "Jean Pablo Vieira de Mello", "authors": "Jean Pablo Vieira de Mello, Lucas Tabelini, Rodrigo F. Berriel, Thiago\n  M. Paix\\~ao, Alberto F. de Souza, Claudine Badue, Nicu Sebe, Thiago\n  Oliveira-Santos", "title": "Deep traffic light detection by overlaying synthetic context on\n  arbitrary natural images", "comments": null, "journal-ref": "Computers & Graphics (2020)", "doi": "10.1016/j.cag.2020.09.012", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Deep neural networks come as an effective solution to many problems\nassociated with autonomous driving. By providing real image samples with\ntraffic context to the network, the model learns to detect and classify\nelements of interest, such as pedestrians, traffic signs, and traffic lights.\nHowever, acquiring and annotating real data can be extremely costly in terms of\ntime and effort. In this context, we propose a method to generate artificial\ntraffic-related training data for deep traffic light detectors. This data is\ngenerated using basic non-realistic computer graphics to blend fake traffic\nscenes on top of arbitrary image backgrounds that are not related to the\ntraffic domain. Thus, a large amount of training data can be generated without\nannotation efforts. Furthermore, it also tackles the intrinsic data imbalance\nproblem in traffic light datasets, caused mainly by the low amount of samples\nof the yellow state. Experiments show that it is possible to achieve results\ncomparable to those obtained with real training data from the problem domain,\nyielding an average mAP and an average F1-score which are each nearly 4 p.p.\nhigher than the respective metrics obtained with a real-world reference model.\n", "versions": [{"version": "v1", "created": "Sat, 7 Nov 2020 19:57:22 GMT"}, {"version": "v2", "created": "Tue, 10 Nov 2020 02:30:51 GMT"}, {"version": "v3", "created": "Thu, 10 Dec 2020 22:44:41 GMT"}], "update_date": "2020-12-14", "authors_parsed": [["de Mello", "Jean Pablo Vieira", ""], ["Tabelini", "Lucas", ""], ["Berriel", "Rodrigo F.", ""], ["Paix\u00e3o", "Thiago M.", ""], ["de Souza", "Alberto F.", ""], ["Badue", "Claudine", ""], ["Sebe", "Nicu", ""], ["Oliveira-Santos", "Thiago", ""]]}, {"id": "2011.03856", "submitter": "Christopher Clark", "authors": "Christopher Clark, Mark Yatskar, and Luke Zettlemoyer", "title": "Learning to Model and Ignore Dataset Bias with Mixed Capacity Ensembles", "comments": "In EMNLP Findings", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many datasets have been shown to contain incidental correlations created by\nidiosyncrasies in the data collection process. For example, sentence entailment\ndatasets can have spurious word-class correlations if nearly all contradiction\nsentences contain the word \"not\", and image recognition datasets can have\ntell-tale object-background correlations if dogs are always indoors. In this\npaper, we propose a method that can automatically detect and ignore these kinds\nof dataset-specific patterns, which we call dataset biases. Our method trains a\nlower capacity model in an ensemble with a higher capacity model. During\ntraining, the lower capacity model learns to capture relatively shallow\ncorrelations, which we hypothesize are likely to reflect dataset bias. This\nfrees the higher capacity model to focus on patterns that should generalize\nbetter. We ensure the models learn non-overlapping approaches by introducing a\nnovel method to make them conditionally independent. Importantly, our approach\ndoes not require the bias to be known in advance. We evaluate performance on\nsynthetic datasets, and four datasets built to penalize models that exploit\nknown biases on textual entailment, visual question answering, and image\nrecognition tasks. We show improvement in all settings, including a 10 point\ngain on the visual question answering dataset.\n", "versions": [{"version": "v1", "created": "Sat, 7 Nov 2020 22:20:03 GMT"}], "update_date": "2020-11-10", "authors_parsed": [["Clark", "Christopher", ""], ["Yatskar", "Mark", ""], ["Zettlemoyer", "Luke", ""]]}, {"id": "2011.03864", "submitter": "Cade Gordon", "authors": "Cade Gordon, Natalie Parde", "title": "Latent Neural Differential Equations for Video Generation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative Adversarial Networks have recently shown promise for video\ngeneration, building off of the success of image generation while also\naddressing a new challenge: time. Although time was analyzed in some early\nwork, the literature has not adequately grown with temporal modeling\ndevelopments. We study the effects of Neural Differential Equations to model\nthe temporal dynamics of video generation. The paradigm of Neural Differential\nEquations presents many theoretical strengths including the first continuous\nrepresentation of time within video generation. In order to address the effects\nof Neural Differential Equations, we investigate how changes in temporal models\naffect generated video quality. Our results give support to the usage of Neural\nDifferential Equations as a simple replacement for older temporal generators.\nWhile keeping run times similar and decreasing parameter count, we produce a\nnew state-of-the-art model in 64$\\times$64 pixel unconditional video\ngeneration, with an Inception Score of 15.20.\n", "versions": [{"version": "v1", "created": "Sat, 7 Nov 2020 23:08:29 GMT"}, {"version": "v2", "created": "Mon, 30 Nov 2020 15:40:28 GMT"}, {"version": "v3", "created": "Tue, 11 May 2021 05:31:13 GMT"}], "update_date": "2021-05-12", "authors_parsed": [["Gordon", "Cade", ""], ["Parde", "Natalie", ""]]}, {"id": "2011.03891", "submitter": "Weiwei Fang", "authors": "Mengran Liu and Weiwei Fang and Xiaodong Ma and Wenyuan Xu and Naixue\n  Xiong and Yi Ding", "title": "Channel Pruning Guided by Spatial and Channel Attention for DNNs in\n  Intelligent Edge Computing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Neural Networks (DNNs) have achieved remarkable success in many computer\nvision tasks recently, but the huge number of parameters and the high\ncomputation overhead hinder their deployments on resource-constrained edge\ndevices. It is worth noting that channel pruning is an effective approach for\ncompressing DNN models. A critical challenge is to determine which channels are\nto be removed, so that the model accuracy will not be negatively affected. In\nthis paper, we first propose Spatial and Channel Attention (SCA), a new\nattention module combining both spatial and channel attention that respectively\nfocuses on \"where\" and \"what\" are the most informative parts. Guided by the\nscale values generated by SCA for measuring channel importance, we further\npropose a new channel pruning approach called Channel Pruning guided by Spatial\nand Channel Attention (CPSCA). Experimental results indicate that SCA achieves\nthe best inference accuracy, while incurring negligibly extra resource\nconsumption, compared to other state-of-the-art attention modules. Our\nevaluation on two benchmark datasets shows that, with the guidance of SCA, our\nCPSCA approach achieves higher inference accuracy than other state-of-the-art\npruning methods under the same pruning ratios.\n", "versions": [{"version": "v1", "created": "Sun, 8 Nov 2020 02:40:06 GMT"}, {"version": "v2", "created": "Mon, 21 Jun 2021 12:48:48 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Liu", "Mengran", ""], ["Fang", "Weiwei", ""], ["Ma", "Xiaodong", ""], ["Xu", "Wenyuan", ""], ["Xiong", "Naixue", ""], ["Ding", "Yi", ""]]}, {"id": "2011.03908", "submitter": "Xiaoang Shen", "authors": "Guokai Zhang, Xiaoang Shen, Ye Luo, Jihao Luo, Zeju Wang, Weigang\n  Wang, Binghui Zhao, Jianwei Lu", "title": "Cross-Modal Self-Attention Distillation for Prostate Cancer Segmentation", "comments": "2020 IEEE International Conference on Bioinformatics and Biomedicine\n  (BIBM)", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic segmentation of the prostate cancer from the multi-modal magnetic\nresonance images is of critical importance for the initial staging and\nprognosis of patients. However, how to use the multi-modal image features more\nefficiently is still a challenging problem in the field of medical image\nsegmentation. In this paper, we develop a cross-modal self-attention\ndistillation network by fully exploiting the encoded information of the\nintermediate layers from different modalities, and the extracted attention maps\nof different modalities enable the model to transfer the significant spatial\ninformation with more details. Moreover, a novel spatial correlated feature\nfusion module is further employed for learning more complementary correlation\nand non-linear information of different modality images. We evaluate our model\nin five-fold cross-validation on 358 MRI with biopsy confirmed. Extensive\nexperiment results demonstrate that our proposed network achieves\nstate-of-the-art performance.\n", "versions": [{"version": "v1", "created": "Sun, 8 Nov 2020 06:19:13 GMT"}], "update_date": "2020-11-10", "authors_parsed": [["Zhang", "Guokai", ""], ["Shen", "Xiaoang", ""], ["Luo", "Ye", ""], ["Luo", "Jihao", ""], ["Wang", "Zeju", ""], ["Wang", "Weigang", ""], ["Zhao", "Binghui", ""], ["Lu", "Jianwei", ""]]}, {"id": "2011.03910", "submitter": "Falak Shah", "authors": "Parthesh Soni, Falak Shah, Nisarg Vyas", "title": "Faster object tracking pipeline for real time tracking", "comments": "12 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-object tracking (MOT) is a challenging practical problem for vision\nbased applications. Most recent approaches for MOT use precomputed detections\nfrom models such as Faster RCNN, performing fine-tuning of bounding boxes and\nassociation in subsequent phases. However, this is not suitable for actual\nindustrial applications due to unavailability of detections upfront. In their\nrecent work, Wang et al. proposed a tracking pipeline that uses a Joint\ndetection and embedding model and performs target localization and association\nin realtime. Upon investigating the tracking by detection paradigm, we find\nthat the tracking pipeline can be made faster by performing localization and\nassociation tasks parallely with model prediction. This, and other\ncomputational optimizations such as using mixed precision model and performing\nbatchwise detection result in a speed-up of the tracking pipeline by 57.8\\% (19\nFPS to 30 FPS) on FullHD resolution. Moreover, the speed is independent of the\nobject density in image sequence. The main contribution of this paper is\nshowcasing a generic pipeline which can be used to speed up detection based\nobject tracking methods. We also reviewed different batch sizes for optimal\nperformance, taking into consideration GPU memory usage and speed.\n", "versions": [{"version": "v1", "created": "Sun, 8 Nov 2020 06:33:48 GMT"}], "update_date": "2020-11-10", "authors_parsed": [["Soni", "Parthesh", ""], ["Shah", "Falak", ""], ["Vyas", "Nisarg", ""]]}, {"id": "2011.03920", "submitter": "Kyle Min", "authors": "Kyle Min, Jason J. Corso", "title": "Integrating Human Gaze into Attention for Egocentric Activity\n  Recognition", "comments": "WACV 2021 camera ready (Supplementary material: on CVF soon)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is well known that human gaze carries significant information about visual\nattention. However, there are three main difficulties in incorporating the gaze\ndata in an attention mechanism of deep neural networks: 1) the gaze fixation\npoints are likely to have measurement errors due to blinking and rapid eye\nmovements; 2) it is unclear when and how much the gaze data is correlated with\nvisual attention; and 3) gaze data is not always available in many real-world\nsituations. In this work, we introduce an effective probabilistic approach to\nintegrate human gaze into spatiotemporal attention for egocentric activity\nrecognition. Specifically, we represent the locations of gaze fixation points\nas structured discrete latent variables to model their uncertainties. In\naddition, we model the distribution of gaze fixations using a variational\nmethod. The gaze distribution is learned during the training process so that\nthe ground-truth annotations of gaze locations are no longer needed in testing\nsituations since they are predicted from the learned gaze distribution. The\npredicted gaze locations are used to provide informative attentional cues to\nimprove the recognition performance. Our method outperforms all the previous\nstate-of-the-art approaches on EGTEA, which is a large-scale dataset for\negocentric activity recognition provided with gaze measurements. We also\nperform an ablation study and qualitative analysis to demonstrate that our\nattention mechanism is effective.\n", "versions": [{"version": "v1", "created": "Sun, 8 Nov 2020 08:02:30 GMT"}], "update_date": "2020-11-10", "authors_parsed": [["Min", "Kyle", ""], ["Corso", "Jason J.", ""]]}, {"id": "2011.03921", "submitter": "Dimple Shajahan", "authors": "Dimple A Shajahan, Mukund Varma T and Ramanathan Muthuganapathy", "title": "Point Transformer for Shape Classification and Retrieval of 3D and ALS\n  Roof PointClouds", "comments": "Submitted on June, 16 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The success of deep learning methods led to significant breakthroughs in 3-D\npoint cloud processing tasks with applications in remote sensing. Existing\nmethods utilize convolutions that have some limitations, as they assume a\nuniform input distribution and cannot learn long-range dependencies. Recent\nworks have shown that adding attention in conjunction with these methods\nimproves performance. This raises a question: can attention layers completely\nreplace convolutions? This paper proposes a fully attentional model - {\\em\nPoint Transformer}, for deriving a rich point cloud representation. The model's\nshape classification and retrieval performance are evaluated on a large-scale\nurban dataset - RoofN3D and a standard benchmark dataset ModelNet40. Extensive\nexperiments are conducted to test the model's robustness to unseen point\ncorruptions for analyzing its effectiveness on real datasets. The proposed\nmethod outperforms other state-of-the-art models in the RoofN3D dataset, gives\ncompetitive results in the ModelNet40 benchmark, and showcases high robustness\nto various unseen point corruptions. Furthermore, the model is highly memory\nand space efficient when compared to other methods.\n", "versions": [{"version": "v1", "created": "Sun, 8 Nov 2020 08:11:02 GMT"}, {"version": "v2", "created": "Sat, 20 Feb 2021 08:54:37 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["Shajahan", "Dimple A", ""], ["T", "Mukund Varma", ""], ["Muthuganapathy", "Ramanathan", ""]]}, {"id": "2011.03949", "submitter": "Alexandros Stergiou MSc", "authors": "Alexandros Stergiou and Ronald Poppe", "title": "Multi-Temporal Convolutions for Human Action Recognition in Videos", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Effective extraction of temporal patterns is crucial for the recognition of\ntemporally varying actions in video. We argue that the fixed-sized\nspatio-temporal convolution kernels used in convolutional neural networks\n(CNNs) can be improved to extract informative motions that are executed at\ndifferent time scales. To address this challenge, we present a novel\nspatio-temporal convolution block that is capable of extracting spatio-temporal\npatterns at multiple temporal resolutions. Our proposed multi-temporal\nconvolution (MTConv) blocks utilize two branches that focus on brief and\nprolonged spatio-temporal patterns, respectively. The extracted time-varying\nfeatures are aligned in a third branch, with respect to global motion patterns\nthrough recurrent cells. The proposed blocks are lightweight and can be\nintegrated into any 3D-CNN architecture. This introduces a substantial\nreduction in computational costs. Extensive experiments on Kinetics, Moments in\nTime and HACS action recognition benchmark datasets demonstrate competitive\nperformance of MTConvs compared to the state-of-the-art with a significantly\nlower computational footprint.\n", "versions": [{"version": "v1", "created": "Sun, 8 Nov 2020 10:40:26 GMT"}, {"version": "v2", "created": "Wed, 31 Mar 2021 15:02:49 GMT"}], "update_date": "2021-04-01", "authors_parsed": [["Stergiou", "Alexandros", ""], ["Poppe", "Ronald", ""]]}, {"id": "2011.03958", "submitter": "Basura Fernando", "authors": "Vinoj Jayasundara, Debaditya Roy, Basura Fernando", "title": "FlowCaps: Optical Flow Estimation with Capsule Networks For Action\n  Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Capsule networks (CapsNets) have recently shown promise to excel in most\ncomputer vision tasks, especially pertaining to scene understanding. In this\npaper, we explore CapsNet's capabilities in optical flow estimation, a task at\nwhich convolutional neural networks (CNNs) have already outperformed other\napproaches. We propose a CapsNet-based architecture, termed FlowCaps, which\nattempts to a) achieve better correspondence matching via finer-grained,\nmotion-specific, and more-interpretable encoding crucial for optical flow\nestimation, b) perform better-generalizable optical flow estimation, c) utilize\nlesser ground truth data, and d) significantly reduce the computational\ncomplexity in achieving good performance, in comparison to its\nCNN-counterparts.\n", "versions": [{"version": "v1", "created": "Sun, 8 Nov 2020 11:35:08 GMT"}], "update_date": "2020-11-10", "authors_parsed": [["Jayasundara", "Vinoj", ""], ["Roy", "Debaditya", ""], ["Fernando", "Basura", ""]]}, {"id": "2011.03970", "submitter": "Birgitta Dresp-Langley", "authors": "John M Wandeto, Birgitta Dresp-Langley", "title": "The quantization error in a Self-Organizing Map as a contrast and colour\n  specific indicator of single-pixel change in large random patterns", "comments": null, "journal-ref": "Neural Networks, 2019 Nov;119:273-285. PMID: 31473578", "doi": "10.1016/j.neunet.2019.08.014", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The quantization error in a fixed-size Self-Organizing Map (SOM) with\nunsupervised winner-take-all learning has previously been used successfully to\ndetect, in minimal computation time, highly meaningful changes across images in\nmedical time series and in time series of satellite images. Here, the\nfunctional properties of the quantization error in SOM are explored further to\nshow that the metric is capable of reliably discriminating between the finest\ndifferences in local contrast intensities and contrast signs. While this\ncapability of the QE is akin to functional characteristics of a specific class\nof retinal ganglion cells (the so-called Y-cells) in the visual systems of the\nprimate and the cat, the sensitivity of the QE surpasses the capacity limits of\nhuman visual detection. Here, the quantization error in the SOM is found to\nreliably signal changes in contrast or colour when contrast information is\nremoved from or added to the image, but not when the amount and relative weight\nof contrast information is constant and only the local spatial position of\ncontrast elements in the pattern changes. While the RGB Mean reflects coarser\nchanges in colour or contrast well enough, the SOM-QE is shown to outperform\nthe RGB Mean in the detection of single-pixel changes in images with up to five\nmillion pixels. This could have important implications in the context of\nunsupervised image learning and computational building block approaches to\nlarge sets of image data (big data), including deep learning blocks, and\nautomatic detection of contrast change at the nanoscale in Transmission or\nScanning Electron Micrographs (TEM, SEM), or at the subpixel level in\nmultispectral and hyper-spectral imaging data.\n", "versions": [{"version": "v1", "created": "Sun, 8 Nov 2020 12:41:24 GMT"}], "update_date": "2020-11-10", "authors_parsed": [["Wandeto", "John M", ""], ["Dresp-Langley", "Birgitta", ""]]}, {"id": "2011.03972", "submitter": "Yunjie Tian", "authors": "Chang Liu and Yunjie Tian and Jianbin Jiao and Qixiang Ye", "title": "Adaptive Linear Span Network for Object Skeleton Detection", "comments": "13 pages, 9 figures", "journal-ref": null, "doi": "10.1109/TIP.2021.3078079", "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conventional networks for object skeleton detection are usually hand-crafted.\nAlthough effective, they require intensive priori knowledge to configure\nrepresentative features for objects in different scale granularity.In this\npaper, we propose adaptive linear span network (AdaLSN), driven by neural\narchitecture search (NAS), to automatically configure and integrate scale-aware\nfeatures for object skeleton detection. AdaLSN is formulated with the theory of\nlinear span, which provides one of the earliest explanations for multi-scale\ndeep feature fusion. AdaLSN is materialized by defining a mixed unit-pyramid\nsearch space, which goes beyond many existing search spaces using unit-level or\npyramid-level features.Within the mixed space, we apply genetic architecture\nsearch to jointly optimize unit-level operations and pyramid-level connections\nfor adaptive feature space expansion. AdaLSN substantiates its versatility by\nachieving significantly higher accuracy and latency trade-off compared with\nstate-of-the-arts. It also demonstrates general applicability to image-to-mask\ntasks such as edge detection and road extraction. Code is available at\n\\href{https://github.com/sunsmarterjie/SDL-Skeleton}{\\color{magenta}github.com/sunsmarterjie/SDL-Skeleton}.\n", "versions": [{"version": "v1", "created": "Sun, 8 Nov 2020 12:51:14 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Liu", "Chang", ""], ["Tian", "Yunjie", ""], ["Jiao", "Jianbin", ""], ["Ye", "Qixiang", ""]]}, {"id": "2011.03981", "submitter": "Hongkai Ye", "authors": "Lizi Wang, Hongkai Ye, Qianhao Wang, Yuman Gao, Chao Xu and Fei Gao", "title": "Learning-based 3D Occupancy Prediction for Autonomous Navigation in\n  Occluded Environments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In autonomous navigation of mobile robots, sensors suffer from massive\nocclusion in cluttered environments, leaving significant amount of space\nunknown during planning. In practice, treating the unknown space in optimistic\nor pessimistic ways both set limitations on planning performance, thus\naggressiveness and safety cannot be satisfied at the same time. However, humans\ncan infer the exact shape of the obstacles from only partial observation and\ngenerate non-conservative trajectories that avoid possible collisions in\noccluded space. Mimicking human behavior, in this paper, we propose a method\nbased on deep neural network to predict occupancy distribution of unknown space\nreliably. Specifically, the proposed method utilizes contextual information of\nenvironments and learns from prior knowledge to predict obstacle distributions\nin occluded space. We use unlabeled and no-ground-truth data to train our\nnetwork and successfully apply it to real-time navigation in unseen\nenvironments without any refinement. Results show that our method leverages the\nperformance of a kinodynamic planner by improving security with no reduction of\nspeed in clustered environments.\n", "versions": [{"version": "v1", "created": "Sun, 8 Nov 2020 13:51:34 GMT"}, {"version": "v2", "created": "Sat, 27 Mar 2021 10:54:16 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Wang", "Lizi", ""], ["Ye", "Hongkai", ""], ["Wang", "Qianhao", ""], ["Gao", "Yuman", ""], ["Xu", "Chao", ""], ["Gao", "Fei", ""]]}, {"id": "2011.04003", "submitter": "Ruoxi Wang", "authors": "Ruoxi Wang, Dandan Zhang, Qingbiao Li, Xiao-Yun Zhou, Benny Lo", "title": "Real-time Surgical Environment Enhancement for Robot-Assisted Minimally\n  Invasive Surgery Based on Super-Resolution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In Robot-Assisted Minimally Invasive Surgery (RAMIS), a camera assistant is\nnormally required to control the position and zooming ratio of the laparoscope,\nfollowing the surgeon's instructions. However, moving the laparoscope\nfrequently may lead to unstable and suboptimal views, while the adjustment of\nzooming ratio may interrupt the workflow of the surgical operation. To this\nend, we propose a multi-scale Generative Adversarial Network (GAN)-based video\nsuper-resolution method to construct a framework for automatic zooming ratio\nadjustment. It can provide automatic real-time zooming for high-quality\nvisualization of the Region Of Interest (ROI) during the surgical operation. In\nthe pipeline of the framework, the Kernel Correlation Filter (KCF) tracker is\nused for tracking the tips of the surgical tools, while the Semi-Global Block\nMatching (SGBM) based depth estimation and Recurrent Neural Network (RNN)-based\ncontext-awareness are developed to determine the upscaling ratio for zooming.\nThe framework is validated with the JIGSAW dataset and Hamlyn Centre\nLaparoscopic/Endoscopic Video Datasets, with results demonstrating its\npracticability.\n", "versions": [{"version": "v1", "created": "Sun, 8 Nov 2020 15:40:05 GMT"}], "update_date": "2020-11-10", "authors_parsed": [["Wang", "Ruoxi", ""], ["Zhang", "Dandan", ""], ["Li", "Qingbiao", ""], ["Zhou", "Xiao-Yun", ""], ["Lo", "Benny", ""]]}, {"id": "2011.04006", "submitter": "Yi Tay", "authors": "Yi Tay, Mostafa Dehghani, Samira Abnar, Yikang Shen, Dara Bahri,\n  Philip Pham, Jinfeng Rao, Liu Yang, Sebastian Ruder, Donald Metzler", "title": "Long Range Arena: A Benchmark for Efficient Transformers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CL cs.CV cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transformers do not scale very well to long sequence lengths largely because\nof quadratic self-attention complexity. In the recent months, a wide spectrum\nof efficient, fast Transformers have been proposed to tackle this problem, more\noften than not claiming superior or comparable model quality to vanilla\nTransformer models. To this date, there is no well-established consensus on how\nto evaluate this class of models. Moreover, inconsistent benchmarking on a wide\nspectrum of tasks and datasets makes it difficult to assess relative model\nquality amongst many models. This paper proposes a systematic and unified\nbenchmark, LRA, specifically focused on evaluating model quality under\nlong-context scenarios. Our benchmark is a suite of tasks consisting of\nsequences ranging from $1K$ to $16K$ tokens, encompassing a wide range of data\ntypes and modalities such as text, natural, synthetic images, and mathematical\nexpressions requiring similarity, structural, and visual-spatial reasoning. We\nsystematically evaluate ten well-established long-range Transformer models\n(Reformers, Linformers, Linear Transformers, Sinkhorn Transformers, Performers,\nSynthesizers, Sparse Transformers, and Longformers) on our newly proposed\nbenchmark suite. LRA paves the way towards better understanding this class of\nefficient Transformer models, facilitates more research in this direction, and\npresents new challenging tasks to tackle. Our benchmark code will be released\nat https://github.com/google-research/long-range-arena.\n", "versions": [{"version": "v1", "created": "Sun, 8 Nov 2020 15:53:56 GMT"}], "update_date": "2020-11-10", "authors_parsed": [["Tay", "Yi", ""], ["Dehghani", "Mostafa", ""], ["Abnar", "Samira", ""], ["Shen", "Yikang", ""], ["Bahri", "Dara", ""], ["Pham", "Philip", ""], ["Rao", "Jinfeng", ""], ["Yang", "Liu", ""], ["Ruder", "Sebastian", ""], ["Metzler", "Donald", ""]]}, {"id": "2011.04052", "submitter": "Shreyas Labhsetwar", "authors": "Shreyas Rajesh Labhsetwar, Raj Sunil Salvi, Piyush Arvind Kolte,\n  Veerasai Subramaniam venkatesh, Alistair Michael Baretto", "title": "Predictive Analysis of Diabetic Retinopathy with Transfer Learning", "comments": "ICNTE", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the prevalence of Diabetes, the Diabetes Mellitus Retinopathy (DR) is\nbecoming a major health problem across the world. The long-term medical\ncomplications arising due to DR have a significant impact on the patient as\nwell as the society, as the disease mostly affects individuals in their most\nproductive years. Early detection and treatment can help reduce the extent of\ndamage to the patients. The rise of Convolutional Neural Networks for\npredictive analysis in the medical field paves the way for a robust solution to\nDR detection. This paper studies the performance of several highly efficient\nand scalable CNN architectures for Diabetic Retinopathy Classification with the\nhelp of Transfer Learning. The research focuses on VGG16, Resnet50 V2 and\nEfficientNet B0 models. The classification performance is analyzed using\nseveral performance metrics including True Positive Rate, False Positive Rate,\nAccuracy, etc. Also, several performance graphs are plotted for visualizing the\narchitecture performance including Confusion Matrix, ROC Curve, etc. The\nresults indicate that Transfer Learning with ImageNet weights using VGG 16\nmodel demonstrates the best classification performance with the best Accuracy\nof 95%. It is closely followed by ResNet50 V2 architecture with the best\nAccuracy of 93%. This paper shows that predictive analysis of DR from retinal\nimages is achieved with Transfer Learning on Convolutional Neural Networks.\n", "versions": [{"version": "v1", "created": "Sun, 8 Nov 2020 18:54:57 GMT"}, {"version": "v2", "created": "Mon, 21 Dec 2020 05:40:46 GMT"}], "update_date": "2020-12-22", "authors_parsed": [["Labhsetwar", "Shreyas Rajesh", ""], ["Salvi", "Raj Sunil", ""], ["Kolte", "Piyush Arvind", ""], ["venkatesh", "Veerasai Subramaniam", ""], ["Baretto", "Alistair Michael", ""]]}, {"id": "2011.04056", "submitter": "Shreyas Labhsetwar", "authors": "Shreyas Rajesh Labhsetwar, Soumya Haridas, Riyali Panmand, Rutuja\n  Deshpande, Piyush Arvind Kolte, Sandhya Pati", "title": "Performance Analysis of Optimizers for Plant Disease Classification with\n  Convolutional Neural Networks", "comments": "conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Crop failure owing to pests & diseases are inherent within Indian\nagriculture, leading to annual losses of 15 to 25% of productivity, resulting\nin a huge economic loss. This research analyzes the performance of various\noptimizers for predictive analysis of plant diseases with deep learning\napproach. The research uses Convolutional Neural Networks for classification of\nfarm or plant leaf samples of 3 crops into 15 classes. The various optimizers\nused in this research include RMSprop, Adam and AMSgrad. Optimizers Performance\nis visualised by plotting the Training and Validation Accuracy and Loss curves,\nROC curves and Confusion Matrix. The best performance is achieved using Adam\noptimizer, with the maximum validation accuracy being 98%. This paper focuses\non the research analysis proving that plant diseases can be predicted and\npre-empted using deep learning methodology with the help of satellite, drone\nbased or mobile based images that result in reducing crop failure and\nagricultural losses.\n", "versions": [{"version": "v1", "created": "Sun, 8 Nov 2020 19:03:02 GMT"}, {"version": "v2", "created": "Tue, 22 Dec 2020 07:10:05 GMT"}], "update_date": "2020-12-23", "authors_parsed": [["Labhsetwar", "Shreyas Rajesh", ""], ["Haridas", "Soumya", ""], ["Panmand", "Riyali", ""], ["Deshpande", "Rutuja", ""], ["Kolte", "Piyush Arvind", ""], ["Pati", "Sandhya", ""]]}, {"id": "2011.04057", "submitter": "Shreyas Labhsetwar", "authors": "Shreyas Rajesh Labhsetwar, Alistair Michael Baretto, Raj Sunil Salvi,\n  Piyush Arvind Kolte, Veerasai Subramaniam Venkatesh", "title": "Analysis of Dimensional Influence of Convolutional Neural Networks for\n  Histopathological Cancer Classification", "comments": "conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional Neural Networks can be designed with different levels of\ncomplexity depending upon the task at hand. This paper analyzes the effect of\ndimensional changes to the CNN architecture on its performance on the task of\nHistopathological Cancer Classification. The research starts with a baseline\n10-layer CNN model with (3 X 3) convolution filters. Thereafter, the baseline\narchitecture is scaled in multiple dimensions including width, depth,\nresolution and a combination of all of these. Width scaling involves\ninculcating greater number of neurons per CNN layer, whereas depth scaling\ninvolves deepening the hierarchical layered structure. Resolution scaling is\nperformed by increasing the dimensions of the input image, and compound scaling\ninvolves a hybrid combination of width, depth and resolution scaling. The\nresults indicate that histopathological cancer scans are very complex in nature\nand hence require high resolution images fed to a large hierarchy of\nConvolution, MaxPooling, Dropout and Batch Normalization layers to extract all\nthe intricacies and perform perfect classification. Since compound scaling the\nbaseline model ensures that all the three dimensions: width, depth and\nresolution are scaled, the best performance is obtained with compound scaling.\nThis research shows that better performance of CNN models is achieved by\ncompound scaling of the baseline model for the task of Histopathological Cancer\nClassification.\n", "versions": [{"version": "v1", "created": "Sun, 8 Nov 2020 19:07:43 GMT"}, {"version": "v2", "created": "Tue, 22 Dec 2020 07:03:38 GMT"}], "update_date": "2020-12-23", "authors_parsed": [["Labhsetwar", "Shreyas Rajesh", ""], ["Baretto", "Alistair Michael", ""], ["Salvi", "Raj Sunil", ""], ["Kolte", "Piyush Arvind", ""], ["Venkatesh", "Veerasai Subramaniam", ""]]}, {"id": "2011.04064", "submitter": "Peri Akiva", "authors": "Peri Akiva and Benjamin Planche and Aditi Roy and Kristin Dana and\n  Peter Oudemans and Michael Mars", "title": "AI on the Bog: Monitoring and Evaluating Cranberry Crop Risk", "comments": "IEEE Winter Conference on Applications of Computer Vision (WACV) 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine vision for precision agriculture has attracted considerable research\ninterest in recent years. The goal of this paper is to develop an end-to-end\ncranberry health monitoring system to enable and support real time cranberry\nover-heating assessment to facilitate informed decisions that may sustain the\neconomic viability of the farm. Toward this goal, we propose two main deep\nlearning-based modules for: 1) cranberry fruit segmentation to delineate the\nexact fruit regions in the cranberry field image that are exposed to sun, 2)\nprediction of cloud coverage conditions and sun irradiance to estimate the\ninner temperature of exposed cranberries. We develop drone-based field data and\nground-based sky data collection systems to collect video imagery at multiple\ntime points for use in crop health analysis. Extensive evaluation on the data\nset shows that it is possible to predict exposed fruit's inner temperature with\nhigh accuracy (0.02% MAPE). The sun irradiance prediction error was found to be\n8.41-20.36% MAPE in the 5-20 minutes time horizon. With 62.54% mIoU for\nsegmentation and 13.46 MAE for counting accuracies in exposed fruit\nidentification, this system is capable of giving informed feedback to growers\nto take precautionary action (e.g. irrigation) in identified crop field regions\nwith higher risk of sunburn in the near future. Though this novel system is\napplied for cranberry health monitoring, it represents a pioneering step\nforward for efficient farming and is useful in precision agriculture beyond the\nproblem of cranberry overheating.\n", "versions": [{"version": "v1", "created": "Sun, 8 Nov 2020 20:03:20 GMT"}], "update_date": "2020-11-10", "authors_parsed": [["Akiva", "Peri", ""], ["Planche", "Benjamin", ""], ["Roy", "Aditi", ""], ["Dana", "Kristin", ""], ["Oudemans", "Peter", ""], ["Mars", "Michael", ""]]}, {"id": "2011.04076", "submitter": "Qiang Li", "authors": "Qiang Li", "title": "A Psychophysical Oriented Saliency Map Prediction Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Visual attention is one of the most significant characteristics for selecting\nand understanding the visual redundancy of the external world. Complex scenes\ninclude enormous redundancy. The human vision system cannot process all\ninformation simultaneously, due to the visual information bottleneck. The human\nvisual system mainly focuses on dominant parts of scenes, in order to reduce\nthe redundant input of visual information. This is commonly known as visual\nattention prediction or visual saliency map prediction. This paper proposes a\nnew psychophysical saliency prediction architecture, WECSF, inspired by\nmulti-channel model of visual cortex functioning in humans. The model consists\nof opponent color channels, a wavelet transform and wavelet energy map, and a\ncontrast sensitivity function for extracting low-level image features and\nproviding maximum approximation to the human visual system. In this paper, the\nproposed model is evaluated using several data sets, including the MIT1003,\nMIT300, TORONTO, SID4VAM, and UCF Sports data sets, in order to demonstrate its\nefficiency. We also quantitatively and qualitatively compare the saliency\nprediction performance with that of other state-of-the-art models. Our model\nachieved stable and very good performance. Additionally, Fourier and\nspectral-inspired saliency prediction models outperformed other\nstate-of-the-art non-neural networks (and even deep neural network) models on\npsychophysical synthetic images. Finally, the proposed model can also be\napplied to spatial-temporal saliency prediction and achieved superior\nperformance in the evaluation.\n", "versions": [{"version": "v1", "created": "Sun, 8 Nov 2020 20:58:05 GMT"}, {"version": "v10", "created": "Fri, 9 Jul 2021 11:29:57 GMT"}, {"version": "v2", "created": "Wed, 11 Nov 2020 11:59:43 GMT"}, {"version": "v3", "created": "Mon, 16 Nov 2020 13:21:22 GMT"}, {"version": "v4", "created": "Tue, 9 Feb 2021 14:58:19 GMT"}, {"version": "v5", "created": "Wed, 10 Feb 2021 11:07:16 GMT"}, {"version": "v6", "created": "Sun, 14 Feb 2021 13:58:03 GMT"}, {"version": "v7", "created": "Tue, 13 Apr 2021 19:53:53 GMT"}, {"version": "v8", "created": "Thu, 20 May 2021 20:17:41 GMT"}, {"version": "v9", "created": "Mon, 14 Jun 2021 20:45:46 GMT"}], "update_date": "2021-07-12", "authors_parsed": [["Li", "Qiang", ""]]}, {"id": "2011.04087", "submitter": "Yun Chang", "authors": "Yun Chang, Yulun Tian, Jonathan P. How, Luca Carlone", "title": "Kimera-Multi: a System for Distributed Multi-Robot Metric-Semantic\n  Simultaneous Localization and Mapping", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the first fully distributed multi-robot system for dense\nmetric-semantic Simultaneous Localization and Mapping (SLAM). Our system,\ndubbed Kimera-Multi, is implemented by a team of robots equipped with\nvisual-inertial sensors, and builds a 3D mesh model of the environment in\nreal-time, where each face of the mesh is annotated with a semantic label\n(e.g., building, road, objects). In Kimera-Multi, each robot builds a local\ntrajectory estimate and a local mesh using Kimera. Then, when two robots are\nwithin communication range, they initiate a distributed place recognition and\nrobust pose graph optimization protocol with a novel incremental maximum clique\noutlier rejection; the protocol allows the robots to improve their local\ntrajectory estimates by leveraging inter-robot loop closures. Finally, each\nrobot uses its improved trajectory estimate to correct the local mesh using\nmesh deformation techniques. We demonstrate Kimera-Multi in photo-realistic\nsimulations and real data. Kimera-Multi (i) is able to build accurate 3D\nmetric-semantic meshes, (ii) is robust to incorrect loop closures while\nrequiring less computation than state-of-the-art distributed SLAM back-ends,\nand (iii) is efficient, both in terms of computation at each robot as well as\ncommunication bandwidth.\n", "versions": [{"version": "v1", "created": "Sun, 8 Nov 2020 21:38:12 GMT"}], "update_date": "2020-11-10", "authors_parsed": [["Chang", "Yun", ""], ["Tian", "Yulun", ""], ["How", "Jonathan P.", ""], ["Carlone", "Luca", ""]]}, {"id": "2011.04094", "submitter": "Foivos Ntelemis", "authors": "Foivos Ntelemis, Yaochu Jin, Spencer A. Thomas", "title": "Image Clustering using an Augmented Generative Adversarial Network and\n  Information Maximization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Image clustering has recently attracted significant attention due to the\nincreased availability of unlabelled datasets. The efficiency of traditional\nclustering algorithms heavily depends on the distance functions used and the\ndimensionality of the features. Therefore, performance degradation is often\nobserved when tackling either unprocessed images or high-dimensional features\nextracted from processed images. To deal with these challenges, we propose a\ndeep clustering framework consisting of a modified generative adversarial\nnetwork (GAN) and an auxiliary classifier. The modification employs Sobel\noperations prior to the discriminator of the GAN to enhance the separability of\nthe learned features. The discriminator is then leveraged to generate\nrepresentations as the input to an auxiliary classifier. An adaptive objective\nfunction is utilised to train the auxiliary classifier for clustering the\nrepresentations, aiming to increase the robustness by minimizing the divergence\nof multiple representations generated by the discriminator. The auxiliary\nclassifier is implemented with a group of multiple cluster-heads, where a\ntolerance hyper-parameter is used to tackle imbalanced data. Our results\nindicate that the proposed method significantly outperforms state-of-the-art\nclustering methods on CIFAR-10 and CIFAR-100, and is competitive on the STL10\nand MNIST datasets.\n", "versions": [{"version": "v1", "created": "Sun, 8 Nov 2020 22:20:33 GMT"}], "update_date": "2020-11-10", "authors_parsed": [["Ntelemis", "Foivos", ""], ["Jin", "Yaochu", ""], ["Thomas", "Spencer A.", ""]]}, {"id": "2011.04121", "submitter": "Tareq Tayeh", "authors": "Tareq Tayeh, Sulaiman Aburakhia, Ryan Myers, and Abdallah Shami", "title": "Distance-Based Anomaly Detection for Industrial Surfaces Using Triplet\n  Networks", "comments": "6 pages, 8 figures, 2020 IEEE 11th Annual Information Technology,\n  Electronics and Mobile Communication Conference (Best Paper Award in the\n  category of Image Processing and Artificial Intelligence)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Surface anomaly detection plays an important quality control role in many\nmanufacturing industries to reduce scrap production. Machine-based visual\ninspections have been utilized in recent years to conduct this task instead of\nhuman experts. In particular, deep learning Convolutional Neural Networks\n(CNNs) have been at the forefront of these image processing-based solutions due\nto their predictive accuracy and efficiency. Training a CNN on a classification\nobjective requires a sufficiently large amount of defective data, which is\noften not available. In this paper, we address that challenge by training the\nCNN on surface texture patches with a distance-based anomaly detection\nobjective instead. A deep residual-based triplet network model is utilized, and\ndefective training samples are synthesized exclusively from non-defective\nsamples via random erasing techniques to directly learn a similarity metric\nbetween the same-class samples and out-of-class samples. Evaluation results\ndemonstrate the approach's strength in detecting different types of anomalies,\nsuch as bent, broken, or cracked surfaces, for known surfaces that are part of\nthe training data and unseen novel surfaces.\n", "versions": [{"version": "v1", "created": "Mon, 9 Nov 2020 00:35:21 GMT"}, {"version": "v2", "created": "Tue, 10 Nov 2020 04:20:49 GMT"}], "update_date": "2020-11-11", "authors_parsed": [["Tayeh", "Tareq", ""], ["Aburakhia", "Sulaiman", ""], ["Myers", "Ryan", ""], ["Shami", "Abdallah", ""]]}, {"id": "2011.04122", "submitter": "Yan Zuo", "authors": "Gil Avraham, Yan Zuo and Tom Drummond", "title": "Localising In Complex Scenes Using Balanced Adversarial Adaptation", "comments": "Accepted at 3DV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Domain adaptation and generative modelling have collectively mitigated the\nexpensive nature of data collection and labelling by leveraging the rich\nabundance of accurate, labelled data in simulation environments. In this work,\nwe study the performance gap that exists between representations optimised for\nlocalisation on simulation environments and the application of such\nrepresentations in a real-world setting. Our method exploits the shared\ngeometric similarities between simulation and real-world environments whilst\nmaintaining invariance towards visual discrepancies. This is achieved by\noptimising a representation extractor to project both simulated and real\nrepresentations into a shared representation space. Our method uses a\nsymmetrical adversarial approach which encourages the representation extractor\nto conceal the domain that features are extracted from and simultaneously\npreserves robust attributes between source and target domains that are\nbeneficial for localisation. We evaluate our method by adapting representations\noptimised for indoor Habitat simulated environments (Matterport3D and Replica)\nto a real-world indoor environment (Active Vision Dataset), showing that it\ncompares favourably against fully-supervised approaches.\n", "versions": [{"version": "v1", "created": "Mon, 9 Nov 2020 00:40:50 GMT"}], "update_date": "2020-11-10", "authors_parsed": [["Avraham", "Gil", ""], ["Zuo", "Yan", ""], ["Drummond", "Tom", ""]]}, {"id": "2011.04123", "submitter": "Qing Li", "authors": "Qing Li, Jiasong Zhu, Jun Liu, Rui Cao, Qingquan Li, Sen Jia, Guoping\n  Qiu", "title": "Deep Learning based Monocular Depth Prediction: Datasets, Methods and\n  Applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimating depth from RGB images can facilitate many computer vision tasks,\nsuch as indoor localization, height estimation, and simultaneous localization\nand mapping (SLAM). Recently, monocular depth estimation has obtained great\nprogress owing to the rapid development of deep learning techniques. They\nsurpass traditional machine learning-based methods by a large margin in terms\nof accuracy and speed. Despite the rapid progress in this topic, there are\nlacking of a comprehensive review, which is needed to summarize the current\nprogress and provide the future directions. In this survey, we first introduce\nthe datasets for depth estimation, and then give a comprehensive introduction\nof the methods from three perspectives: supervised learning-based methods,\nunsupervised learning-based methods, and sparse samples guidance-based methods.\nIn addition, downstream applications that benefit from the progress have also\nbeen illustrated. Finally, we point out the future directions and conclude the\npaper.\n", "versions": [{"version": "v1", "created": "Mon, 9 Nov 2020 01:03:13 GMT"}], "update_date": "2020-11-10", "authors_parsed": [["Li", "Qing", ""], ["Zhu", "Jiasong", ""], ["Liu", "Jun", ""], ["Cao", "Rui", ""], ["Li", "Qingquan", ""], ["Jia", "Sen", ""], ["Qiu", "Guoping", ""]]}, {"id": "2011.04145", "submitter": "Shuqiang Wang", "authors": "Senrong You and Yong Liu and Baiying Lei and Shuqiang Wang", "title": "Fine Perceptive GANs for Brain MR Image Super-Resolution in Wavelet\n  Domain", "comments": "9 pages, 11figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Magnetic resonance imaging plays an important role in computer-aided\ndiagnosis and brain exploration. However, limited by hardware, scanning time\nand cost, it's challenging to acquire high-resolution (HR) magnetic resonance\n(MR) image clinically. In this paper, fine perceptive generative adversarial\nnetworks (FP-GANs) is proposed to produce HR MR images from low-resolution\ncounterparts. It can cope with the detail insensitive problem of the existing\nsuper-resolution model in a divide-and-conquer manner. Specifically, FP-GANs\nfirstly divides an MR image into low-frequency global approximation and\nhigh-frequency anatomical texture in wavelet domain. Then each sub-band\ngenerative adversarial network (sub-band GAN) conquers the super-resolution\nprocedure of each single sub-band image. Meanwhile, sub-band attention is\ndeployed to tune focus between global and texture information. It can focus on\nsub-band images instead of feature maps to further enhance the anatomical\nreconstruction ability of FP-GANs. In addition, inverse discrete wavelet\ntransformation (IDWT) is integrated into model for taking the reconstruction of\nwhole image into account. Experiments on MultiRes_7T dataset demonstrate that\nFP-GANs outperforms the competing methods quantitatively and qualitatively.\n", "versions": [{"version": "v1", "created": "Mon, 9 Nov 2020 02:09:44 GMT"}], "update_date": "2020-11-10", "authors_parsed": [["You", "Senrong", ""], ["Liu", "Yong", ""], ["Lei", "Baiying", ""], ["Wang", "Shuqiang", ""]]}, {"id": "2011.04173", "submitter": "Huaiyang Huang", "authors": "Huaiyang Huang, Haoyang Ye, Jianhao Jiao, Yuxiang Sun, Ming Liu", "title": "Geometric Structure Aided Visual Inertial Localization", "comments": "submitted to ICRA2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual Localization is an essential component in autonomous navigation.\nExisting approaches are either based on the visual structure from SLAM/SfM or\nthe geometric structure from dense mapping. To take the advantages of both, in\nthis work, we present a complete visual inertial localization system based on a\nhybrid map representation to reduce the computational cost and increase the\npositioning accuracy. Specially, we propose two modules for data association\nand batch optimization, respectively. To this end, we develop an efficient data\nassociation module to associate map components with local features, which takes\nonly $2$ms to generate temporal landmarks. For batch optimization, instead of\nusing visual factors, we develop a module to estimate a pose prior from the\ninstant localization results to constrain poses. The experimental results on\nthe EuRoC MAV dataset demonstrate a competitive performance compared to the\nstate of the arts. Specially, our system achieves an average position error in\n1.7 cm with 100% recall. The timings show that the proposed modules reduce the\ncomputational cost by 20-30%. We will make our implementation open source at\nhttp://github.com/hyhuang1995/gmmloc.\n", "versions": [{"version": "v1", "created": "Mon, 9 Nov 2020 03:48:39 GMT"}], "update_date": "2020-11-10", "authors_parsed": [["Huang", "Huaiyang", ""], ["Ye", "Haoyang", ""], ["Jiao", "Jianhao", ""], ["Sun", "Yuxiang", ""], ["Liu", "Ming", ""]]}, {"id": "2011.04181", "submitter": "Peiyan Wang", "authors": "Chengkang Shen, Peiyan Wang and Wei Tang", "title": "Two-Stream Appearance Transfer Network for Person Image Generation", "comments": "9 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pose guided person image generation means to generate a photo-realistic\nperson image conditioned on an input person image and a desired pose. This task\nrequires spatial manipulation of the source image according to the target pose.\nHowever, the generative adversarial networks (GANs) widely used for image\ngeneration and translation rely on spatially local and translation equivariant\noperators, i.e., convolution, pooling and unpooling, which cannot handle large\nimage deformation. This paper introduces a novel two-stream appearance transfer\nnetwork (2s-ATN) to address this challenge. It is a multi-stage architecture\nconsisting of a source stream and a target stream. Each stage features an\nappearance transfer module and several two-stream feature fusion modules. The\nformer finds the dense correspondence between the two-stream feature maps and\nthen transfers the appearance information from the source stream to the target\nstream. The latter exchange local information between the two streams and\nsupplement the non-local appearance transfer. Both quantitative and qualitative\nresults indicate the proposed 2s-ATN can effectively handle large spatial\ndeformation and occlusion while retaining the appearance details. It\noutperforms prior states of the art on two widely used benchmarks.\n", "versions": [{"version": "v1", "created": "Mon, 9 Nov 2020 04:21:02 GMT"}], "update_date": "2020-11-10", "authors_parsed": [["Shen", "Chengkang", ""], ["Wang", "Peiyan", ""], ["Tang", "Wei", ""]]}, {"id": "2011.04197", "submitter": "Jeremy Tan", "authors": "Jeremy Tan, Benjamin Hou, James Batten, Huaqi Qiu, Bernhard Kainz", "title": "Detecting Outliers with Foreign Patch Interpolation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In medical imaging, outliers can contain hypo/hyper-intensities, minor\ndeformations, or completely altered anatomy. To detect these irregularities it\nis helpful to learn the features present in both normal and abnormal images.\nHowever this is difficult because of the wide range of possible abnormalities\nand also the number of ways that normal anatomy can vary naturally. As such, we\nleverage the natural variations in normal anatomy to create a range of\nsynthetic abnormalities. Specifically, the same patch region is extracted from\ntwo independent samples and replaced with an interpolation between both\npatches. The interpolation factor, patch size, and patch location are randomly\nsampled from uniform distributions. A wide residual encoder decoder is trained\nto give a pixel-wise prediction of the patch and its interpolation factor. This\nencourages the network to learn what features to expect normally and to\nidentify where foreign patterns have been introduced. The estimate of the\ninterpolation factor lends itself nicely to the derivation of an outlier score.\nMeanwhile the pixel-wise output allows for pixel- and subject- level\npredictions using the same model.\n", "versions": [{"version": "v1", "created": "Mon, 9 Nov 2020 05:26:38 GMT"}], "update_date": "2020-11-10", "authors_parsed": [["Tan", "Jeremy", ""], ["Hou", "Benjamin", ""], ["Batten", "James", ""], ["Qiu", "Huaqi", ""], ["Kainz", "Bernhard", ""]]}, {"id": "2011.04212", "submitter": "Chenqian Yan", "authors": "Huixia Li, Chenqian Yan, Shaohui Lin, Xiawu Zheng, Yuchao Li, Baochang\n  Zhang, Fan Yang, Rongrong Ji", "title": "PAMS: Quantized Super-Resolution via Parameterized Max Scale", "comments": "ECCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep convolutional neural networks (DCNNs) have shown dominant performance in\nthe task of super-resolution (SR). However, their heavy memory cost and\ncomputation overhead significantly restrict their practical deployments on\nresource-limited devices, which mainly arise from the floating-point storage\nand operations between weights and activations. Although previous endeavors\nmainly resort to fixed-point operations, quantizing both weights and\nactivations with fixed coding lengths may cause significant performance drop,\nespecially on low bits. Specifically, most state-of-the-art SR models without\nbatch normalization have a large dynamic quantization range, which also serves\nas another cause of performance drop. To address these two issues, we propose a\nnew quantization scheme termed PArameterized Max Scale (PAMS), which applies\nthe trainable truncated parameter to explore the upper bound of the\nquantization range adaptively. Finally, a structured knowledge transfer (SKT)\nloss is introduced to fine-tune the quantized network. Extensive experiments\ndemonstrate that the proposed PAMS scheme can well compress and accelerate the\nexisting SR models such as EDSR and RDN. Notably, 8-bit PAMS-EDSR improves PSNR\non Set5 benchmark from 32.095dB to 32.124dB with 2.42$\\times$ compression\nratio, which achieves a new state-of-the-art.\n", "versions": [{"version": "v1", "created": "Mon, 9 Nov 2020 06:16:05 GMT"}], "update_date": "2020-11-10", "authors_parsed": [["Li", "Huixia", ""], ["Yan", "Chenqian", ""], ["Lin", "Shaohui", ""], ["Zheng", "Xiawu", ""], ["Li", "Yuchao", ""], ["Zhang", "Baochang", ""], ["Yang", "Fan", ""], ["Ji", "Rongrong", ""]]}, {"id": "2011.04214", "submitter": "Rui Geng", "authors": "Rui Geng, Yixuan Ma, Wanhong Huang", "title": "An improved helmet detection method for YOLOv3 on an unbalanced dataset", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The YOLOv3 target detection algorithm is widely used in industry due to its\nhigh speed and high accuracy, but it has some limitations, such as the accuracy\ndegradation of unbalanced datasets. The YOLOv3 target detection algorithm is\nbased on a Gaussian fuzzy data augmentation approach to pre-process the data\nset and improve the YOLOv3 target detection algorithm. Through the efficient\npre-processing, the confidence level of YOLOv3 is generally improved by\n0.01-0.02 without changing the recognition speed of YOLOv3, and the processed\nimages also perform better in image localization due to effective feature\nfusion, which is more in line with the requirement of recognition speed and\naccuracy in production.\n", "versions": [{"version": "v1", "created": "Mon, 9 Nov 2020 06:17:30 GMT"}, {"version": "v2", "created": "Tue, 1 Dec 2020 02:39:21 GMT"}], "update_date": "2020-12-02", "authors_parsed": [["Geng", "Rui", ""], ["Ma", "Yixuan", ""], ["Huang", "Wanhong", ""]]}, {"id": "2011.04233", "submitter": "Ruijin Liu", "authors": "Ruijin Liu, Zejian Yuan, Tie Liu, Zhiliang Xiong", "title": "End-to-end Lane Shape Prediction with Transformers", "comments": "9 pages, 7 figures, accepted by WACV 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Lane detection, the process of identifying lane markings as approximated\ncurves, is widely used for lane departure warning and adaptive cruise control\nin autonomous vehicles. The popular pipeline that solves it in two steps --\nfeature extraction plus post-processing, while useful, is too inefficient and\nflawed in learning the global context and lanes' long and thin structures. To\ntackle these issues, we propose an end-to-end method that directly outputs\nparameters of a lane shape model, using a network built with a transformer to\nlearn richer structures and context. The lane shape model is formulated based\non road structures and camera pose, providing physical interpretation for\nparameters of network output. The transformer models non-local interactions\nwith a self-attention mechanism to capture slender structures and global\ncontext. The proposed method is validated on the TuSimple benchmark and shows\nstate-of-the-art accuracy with the most lightweight model size and fastest\nspeed. Additionally, our method shows excellent adaptability to a challenging\nself-collected lane detection dataset, showing its powerful deployment\npotential in real applications. Codes are available at\nhttps://github.com/liuruijin17/LSTR.\n", "versions": [{"version": "v1", "created": "Mon, 9 Nov 2020 07:42:55 GMT"}, {"version": "v2", "created": "Sat, 28 Nov 2020 10:55:44 GMT"}], "update_date": "2020-12-01", "authors_parsed": [["Liu", "Ruijin", ""], ["Yuan", "Zejian", ""], ["Liu", "Tie", ""], ["Xiong", "Zhiliang", ""]]}, {"id": "2011.04234", "submitter": "Jingyi Zhang", "authors": "Jingyi Zhang, Yong Zhang, Baoyuan Wu, Yanbo Fan, Fumin Shen and Heng\n  Tao Shen", "title": "Dual ResGCN for Balanced Scene GraphGeneration", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual scene graph generation is a challenging task. Previous works have\nachieved great progress, but most of them do not explicitly consider the class\nimbalance issue in scene graph generation. Models learned without considering\nthe class imbalance tend to predict the majority classes, which leads to a good\nperformance on trivial frequent predicates, but poor performance on informative\ninfrequent predicates. However, predicates of minority classes often carry more\nsemantic and precise information~(\\textit{e.g.}, \\emph{`on'} v.s \\emph{`parked\non'}). % which leads to a good score of recall, but a poor score of mean\nrecall. To alleviate the influence of the class imbalance, we propose a novel\nmodel, dubbed \\textit{dual ResGCN}, which consists of an object residual graph\nconvolutional network and a relation residual graph convolutional network. The\ntwo networks are complementary to each other. The former captures object-level\ncontext information, \\textit{i.e.,} the connections among objects. We propose a\nnovel ResGCN that enhances object features in a cross attention manner.\nBesides, we stack multiple contextual coefficients to alleviate the imbalance\nissue and enrich the prediction diversity. The latter is carefully designed to\nexplicitly capture relation-level context information \\textit{i.e.,} the\nconnections among relations. We propose to incorporate the prior about the\nco-occurrence of relation pairs into the graph to further help alleviate the\nclass imbalance issue. Extensive evaluations of three tasks are performed on\nthe large-scale database VG to demonstrate the superiority of the proposed\nmethod.\n", "versions": [{"version": "v1", "created": "Mon, 9 Nov 2020 07:44:17 GMT"}], "update_date": "2020-11-10", "authors_parsed": [["Zhang", "Jingyi", ""], ["Zhang", "Yong", ""], ["Wu", "Baoyuan", ""], ["Fan", "Yanbo", ""], ["Shen", "Fumin", ""], ["Shen", "Heng Tao", ""]]}, {"id": "2011.04244", "submitter": "Zicong Jiang", "authors": "Zicong Jiang, Liquan Zhao, Shuaiyang Li, Yanfei Jia", "title": "Real-time object detection method based on improved YOLOv4-tiny", "comments": "14pages,7figures,2tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The \"You only look once v4\"(YOLOv4) is one type of object detection methods\nin deep learning. YOLOv4-tiny is proposed based on YOLOv4 to simple the network\nstructure and reduce parameters, which makes it be suitable for developing on\nthe mobile and embedded devices. To improve the real-time of object detection,\na fast object detection method is proposed based on YOLOv4-tiny. It firstly\nuses two ResBlock-D modules in ResNet-D network instead of two CSPBlock modules\nin Yolov4-tiny, which reduces the computation complexity. Secondly, it designs\nan auxiliary residual network block to extract more feature information of\nobject to reduce detection error. In the design of auxiliary network, two\nconsecutive 3x3 convolutions are used to obtain 5x5 receptive fields to extract\nglobal features, and channel attention and spatial attention are also used to\nextract more effective information. In the end, it merges the auxiliary network\nand backbone network to construct the whole network structure of improved\nYOLOv4-tiny. Simulation results show that the proposed method has faster object\ndetection than YOLOv4-tiny and YOLOv3-tiny, and almost the same mean value of\naverage precision as the YOLOv4-tiny. It is more suitable for real-time object\ndetection.\n", "versions": [{"version": "v1", "created": "Mon, 9 Nov 2020 08:26:28 GMT"}, {"version": "v2", "created": "Wed, 2 Dec 2020 09:19:32 GMT"}], "update_date": "2020-12-03", "authors_parsed": [["Jiang", "Zicong", ""], ["Zhao", "Liquan", ""], ["Li", "Shuaiyang", ""], ["Jia", "Yanfei", ""]]}, {"id": "2011.04258", "submitter": "Bastien Vanderplaetse", "authors": "Bastien Vanderplaetse, St\\'ephane Dupont", "title": "Improved Soccer Action Spotting using both Audio and Video Streams", "comments": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\n  Recognition (CVPR) Workshops, 2020, pp. 896-897", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a study on multi-modal (audio and video) action\nspotting and classification in soccer videos. Action spotting and\nclassification are the tasks that consist in finding the temporal anchors of\nevents in a video and determine which event they are. This is an important\napplication of general activity understanding. Here, we propose an experimental\nstudy on combining audio and video information at different stages of deep\nneural network architectures. We used the SoccerNet benchmark dataset, which\ncontains annotated events for 500 soccer game videos from the Big Five European\nleagues. Through this work, we evaluated several ways to integrate audio stream\ninto video-only-based architectures. We observed an average absolute\nimprovement of the mean Average Precision (mAP) metric of $7.43\\%$ for the\naction classification task and of $4.19\\%$ for the action spotting task.\n", "versions": [{"version": "v1", "created": "Mon, 9 Nov 2020 09:12:44 GMT"}], "update_date": "2020-11-10", "authors_parsed": [["Vanderplaetse", "Bastien", ""], ["Dupont", "St\u00e9phane", ""]]}, {"id": "2011.04260", "submitter": "Lijian Lin", "authors": "Lijian Lin, Haosheng Chen, Yanjie Liang, Yan Yan, Hanzi Wang", "title": "Robust Visual Tracking via Statistical Positive Sample Generation and\n  Gradient Aware Learning", "comments": "6 pages", "journal-ref": "ACM MM Asia2019", "doi": "10.1145/3338533.3366556", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, Convolutional Neural Network (CNN) based trackers have\nachieved state-of-the-art performance on multiple benchmark datasets. Most of\nthese trackers train a binary classifier to distinguish the target from its\nbackground. However, they suffer from two limitations. Firstly, these trackers\ncannot effectively handle significant appearance variations due to the limited\nnumber of positive samples. Secondly, there exists a significant imbalance of\ngradient contributions between easy and hard samples, where the easy samples\nusually dominate the computation of gradient. In this paper, we propose a\nrobust tracking method via Statistical Positive sample generation and Gradient\nAware learning (SPGA) to address the above two limitations. To enrich the\ndiversity of positive samples, we present an effective and efficient\nstatistical positive sample generation algorithm to generate positive samples\nin the feature space. Furthermore, to handle the issue of imbalance between\neasy and hard samples, we propose a gradient sensitive loss to harmonize the\ngradient contributions between easy and hard samples. Extensive experiments on\nthree challenging benchmark datasets including OTB50, OTB100 and VOT2016\ndemonstrate that the proposed SPGA performs favorably against several\nstate-of-the-art trackers.\n", "versions": [{"version": "v1", "created": "Mon, 9 Nov 2020 09:14:58 GMT"}], "update_date": "2020-11-10", "authors_parsed": [["Lin", "Lijian", ""], ["Chen", "Haosheng", ""], ["Liang", "Yanjie", ""], ["Yan", "Yan", ""], ["Wang", "Hanzi", ""]]}, {"id": "2011.04263", "submitter": "Dingquan Li", "authors": "Dingquan Li, Tingting Jiang, Ming Jiang", "title": "Unified Quality Assessment of In-the-Wild Videos with Mixed Datasets\n  Training", "comments": "20 pages, 12 figures, 7 tables, accepted by IJCV. This is the version\n  provided to IJCV office", "journal-ref": null, "doi": "10.1007/s11263-020-01408-w", "report-no": null, "categories": "cs.CV cs.MM eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video quality assessment (VQA) is an important problem in computer vision.\nThe videos in computer vision applications are usually captured in the wild. We\nfocus on automatically assessing the quality of in-the-wild videos, which is a\nchallenging problem due to the absence of reference videos, the complexity of\ndistortions, and the diversity of video contents. Moreover, the video contents\nand distortions among existing datasets are quite different, which leads to\npoor performance of data-driven methods in the cross-dataset evaluation\nsetting. To improve the performance of quality assessment models, we borrow\nintuitions from human perception, specifically, content dependency and\ntemporal-memory effects of human visual system. To face the cross-dataset\nevaluation challenge, we explore a mixed datasets training strategy for\ntraining a single VQA model with multiple datasets. The proposed unified\nframework explicitly includes three stages: relative quality assessor,\nnonlinear mapping, and dataset-specific perceptual scale alignment, to jointly\npredict relative quality, perceptual quality, and subjective quality.\nExperiments are conducted on four publicly available datasets for VQA in the\nwild, i.e., LIVE-VQC, LIVE-Qualcomm, KoNViD-1k, and CVD2014. The experimental\nresults verify the effectiveness of the mixed datasets training strategy and\nprove the superior performance of the unified model in comparison with the\nstate-of-the-art models. For reproducible research, we make the PyTorch\nimplementation of our method available at https://github.com/lidq92/MDTVSFA.\n", "versions": [{"version": "v1", "created": "Mon, 9 Nov 2020 09:22:57 GMT"}, {"version": "v2", "created": "Sun, 15 Nov 2020 09:13:58 GMT"}], "update_date": "2021-02-11", "authors_parsed": [["Li", "Dingquan", ""], ["Jiang", "Tingting", ""], ["Jiang", "Ming", ""]]}, {"id": "2011.04264", "submitter": "Adam Fisch", "authors": "Adam Fisch, Kenton Lee, Ming-Wei Chang, Jonathan H. Clark, Regina\n  Barzilay", "title": "CapWAP: Captioning with a Purpose", "comments": "EMNLP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The traditional image captioning task uses generic reference captions to\nprovide textual information about images. Different user populations, however,\nwill care about different visual aspects of images. In this paper, we propose a\nnew task, Captioning with a Purpose (CapWAP). Our goal is to develop systems\nthat can be tailored to be useful for the information needs of an intended\npopulation, rather than merely provide generic information about an image. In\nthis task, we use question-answer (QA) pairs---a natural expression of\ninformation need---from users, instead of reference captions, for both training\nand post-inference evaluation. We show that it is possible to use reinforcement\nlearning to directly optimize for the intended information need, by rewarding\noutputs that allow a question answering model to provide correct answers to\nsampled user questions. We convert several visual question answering datasets\ninto CapWAP datasets, and demonstrate that under a variety of scenarios our\npurposeful captioning system learns to anticipate and fulfill specific\ninformation needs better than its generic counterparts, as measured by QA\nperformance on user questions from unseen images, when using the caption alone\nas context.\n", "versions": [{"version": "v1", "created": "Mon, 9 Nov 2020 09:23:55 GMT"}], "update_date": "2020-11-10", "authors_parsed": [["Fisch", "Adam", ""], ["Lee", "Kenton", ""], ["Chang", "Ming-Wei", ""], ["Clark", "Jonathan H.", ""], ["Barzilay", "Regina", ""]]}, {"id": "2011.04267", "submitter": "Claudio Michaelis", "authors": "Claudio Michaelis, Matthias Bethge, Alexander S. Ecker", "title": "Closing the Generalization Gap in One-Shot Object Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite substantial progress in object detection and few-shot learning,\ndetecting objects based on a single example - one-shot object detection -\nremains a challenge: trained models exhibit a substantial generalization gap,\nwhere object categories used during training are detected much more reliably\nthan novel ones. Here we show that this generalization gap can be nearly closed\nby increasing the number of object categories used during training. Our results\nshow that the models switch from memorizing individual categories to learning\nobject similarity over the category distribution, enabling strong\ngeneralization at test time. Importantly, in this regime standard methods to\nimprove object detection models like stronger backbones or longer training\nschedules also benefit novel categories, which was not the case for smaller\ndatasets like COCO. Our results suggest that the key to strong few-shot\ndetection models may not lie in sophisticated metric learning approaches, but\ninstead in scaling the number of categories. Future data annotation efforts\nshould therefore focus on wider datasets and annotate a larger number of\ncategories rather than gathering more images or instances per category.\n", "versions": [{"version": "v1", "created": "Mon, 9 Nov 2020 09:31:17 GMT"}], "update_date": "2020-11-10", "authors_parsed": [["Michaelis", "Claudio", ""], ["Bethge", "Matthias", ""], ["Ecker", "Alexander S.", ""]]}, {"id": "2011.04280", "submitter": "Yunkui Pang", "authors": "Yunkui Pang, Zhiqing Pan, Ruiyang Sun, Shuchong Wang", "title": "Sketch-Inspector: a Deep Mixture Model for High-Quality Sketch\n  Generation of Cats", "comments": "12 pages, 7 figures, ISVC 2020 accepted", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the involvement of artificial intelligence (AI), sketches can be\nautomatically generated under certain topics. Even though breakthroughs have\nbeen made in previous studies in this area, a relatively high proportion of the\ngenerated figures are too abstract to recognize, which illustrates that AIs\nfail to learn the general pattern of the target object when drawing. This paper\nposits that supervising the process of stroke generation can lead to a more\naccurate sketch interpretation. Based on that, a sketch generating system with\nan assistant convolutional neural network (CNN) predictor to suggest the shape\nof the next stroke is presented in this paper. In addition, a CNN-based\ndiscriminator is introduced to judge the recognizability of the end product.\nSince the base-line model is ineffective at generating multi-class sketches, we\nrestrict the model to produce one category. Because the image of a cat is easy\nto identify, we consider cat sketches selected from the QuickDraw data set.\nThis paper compares the proposed model with the original Sketch-RNN on 75K\nhuman-drawn cat sketches. The result indicates that our model produces sketches\nwith higher quality than human's sketches.\n", "versions": [{"version": "v1", "created": "Mon, 9 Nov 2020 09:53:03 GMT"}], "update_date": "2020-11-10", "authors_parsed": [["Pang", "Yunkui", ""], ["Pan", "Zhiqing", ""], ["Sun", "Ruiyang", ""], ["Wang", "Shuchong", ""]]}, {"id": "2011.04305", "submitter": "Jiacheng Chen", "authors": "Jiacheng Chen, Hexiang Hu, Hao Wu, Yuning Jiang, Changhu Wang", "title": "Learning the Best Pooling Strategy for Visual Semantic Embedding", "comments": "CVPR 2021 camera-ready (oral). The new version fixes a few typos and\n  updates citations", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual Semantic Embedding (VSE) is a dominant approach for vision-language\nretrieval, which aims at learning a deep embedding space such that visual data\nare embedded close to their semantic text labels or descriptions. Recent VSE\nmodels use complex methods to better contextualize and aggregate multi-modal\nfeatures into holistic embeddings. However, we discover that surprisingly\nsimple (but carefully selected) global pooling functions (e.g., max pooling)\noutperform those complex models, across different feature extractors. Despite\nits simplicity and effectiveness, seeking the best pooling function for\ndifferent data modality and feature extractor is costly and tedious, especially\nwhen the size of features varies (e.g., text, video). Therefore, we propose a\nGeneralized Pooling Operator (GPO), which learns to automatically adapt itself\nto the best pooling strategy for different features, requiring no manual tuning\nwhile staying effective and efficient. We extend the VSE model using this\nproposed GPO and denote it as VSE$\\infty$.\n  Without bells and whistles, VSE$\\infty$ outperforms previous VSE methods\nsignificantly on image-text retrieval benchmarks across popular feature\nextractors. With a simple adaptation, variants of VSE$\\infty$ further\ndemonstrate its strength by achieving the new state of the art on two\nvideo-text retrieval datasets. Comprehensive experiments and visualizations\nconfirm that GPO always discovers the best pooling strategy and can be a\nplug-and-play feature aggregation module for standard VSE models. Code and\npre-trained models are available at https://vse-infty.github.io.\n", "versions": [{"version": "v1", "created": "Mon, 9 Nov 2020 10:22:35 GMT"}, {"version": "v2", "created": "Mon, 5 Apr 2021 12:43:06 GMT"}, {"version": "v3", "created": "Wed, 7 Apr 2021 14:08:23 GMT"}, {"version": "v4", "created": "Thu, 8 Apr 2021 01:03:21 GMT"}, {"version": "v5", "created": "Tue, 6 Jul 2021 14:22:26 GMT"}], "update_date": "2021-07-07", "authors_parsed": [["Chen", "Jiacheng", ""], ["Hu", "Hexiang", ""], ["Wu", "Hao", ""], ["Jiang", "Yuning", ""], ["Wang", "Changhu", ""]]}, {"id": "2011.04307", "submitter": "Yannick Bukschat", "authors": "Yannick Bukschat, Marcus Vetter", "title": "EfficientPose: An efficient, accurate and scalable end-to-end 6D multi\n  object pose estimation approach", "comments": "fixed typo", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we introduce EfficientPose, a new approach for 6D object pose\nestimation. Our method is highly accurate, efficient and scalable over a wide\nrange of computational resources. Moreover, it can detect the 2D bounding box\nof multiple objects and instances as well as estimate their full 6D poses in a\nsingle shot. This eliminates the significant increase in runtime when dealing\nwith multiple objects other approaches suffer from. These approaches aim to\nfirst detect 2D targets, e.g. keypoints, and solve a Perspective-n-Point\nproblem for their 6D pose for each object afterwards. We also propose a novel\naugmentation method for direct 6D pose estimation approaches to improve\nperformance and generalization, called 6D augmentation. Our approach achieves a\nnew state-of-the-art accuracy of 97.35% in terms of the ADD(-S) metric on the\nwidely-used 6D pose estimation benchmark dataset Linemod using RGB input, while\nstill running end-to-end at over 27 FPS. Through the inherent handling of\nmultiple objects and instances and the fused single shot 2D object detection as\nwell as 6D pose estimation, our approach runs even with multiple objects\n(eight) end-to-end at over 26 FPS, making it highly attractive to many real\nworld scenarios. Code will be made publicly available at\nhttps://github.com/ybkscht/EfficientPose.\n", "versions": [{"version": "v1", "created": "Mon, 9 Nov 2020 10:23:55 GMT"}, {"version": "v2", "created": "Wed, 18 Nov 2020 10:47:33 GMT"}], "update_date": "2020-11-19", "authors_parsed": [["Bukschat", "Yannick", ""], ["Vetter", "Marcus", ""]]}, {"id": "2011.04349", "submitter": "Hieu Phung", "authors": "Hieu Trong Phung (1 and 2), Anh Tuan Vu (1), Tung Dinh Nguyen (1), Lam\n  Thanh Do (1 and 2), Giang Nam Ngo (1), Trung Thanh Tran (1) and Ngoc C. L\\^e\n  (1 and 2) ((1) PIXTA Vietnam, Hanoi, Vietnam. (2) Hanoi University of Science\n  and Technology, Ha Noi, Viet Nam.)", "title": "MAGNeto: An Efficient Deep Learning Method for the Extractive Tags\n  Summarization Problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we study a new image annotation task named Extractive Tags\nSummarization (ETS). The goal is to extract important tags from the context\nlying in an image and its corresponding tags. We adjust some state-of-the-art\ndeep learning models to utilize both visual and textual information. Our\nproposed solution consists of different widely used blocks like convolutional\nand self-attention layers, together with a novel idea of combining auxiliary\nloss functions and the gating mechanism to glue and elevate these fundamental\ncomponents and form a unified architecture. Besides, we introduce a loss\nfunction that aims to reduce the imbalance of the training data and a simple\nbut effective data augmentation technique dedicated to alleviates the effect of\noutliers on the final results. Last but not least, we explore an unsupervised\npre-training strategy to further boost the performance of the model by making\nuse of the abundant amount of available unlabeled data. Our model shows the\ngood results as 90% $F_\\text{1}$ score on the public NUS-WIDE benchmark, and\n50% $F_\\text{1}$ score on a noisy large-scale real-world private dataset.\nSource code for reproducing the experiments is publicly available at:\nhttps://github.com/pixta-dev/labteam\n", "versions": [{"version": "v1", "created": "Mon, 9 Nov 2020 11:34:21 GMT"}], "update_date": "2020-11-10", "authors_parsed": [["Phung", "Hieu Trong", "", "1 and 2"], ["Vu", "Anh Tuan", "", "1 and 2"], ["Nguyen", "Tung Dinh", "", "1 and 2"], ["Do", "Lam Thanh", "", "1 and 2"], ["Ngo", "Giang Nam", "", "1 and 2"], ["Tran", "Trung Thanh", "", "1 and 2"], ["L\u00ea", "Ngoc C.", "", "1 and 2"]]}, {"id": "2011.04359", "submitter": "Shrishti Saha Shetu", "authors": "Shrishti Saha Shetu, Soumitro Chakrabarty and Emanu\\\"el A. P. Habets", "title": "An Empirical Study of Visual Features for DNN based Audio-Visual Speech\n  Enhancement in Multi-talker Environments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.CV cs.LG cs.SD eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Audio-visual speech enhancement (AVSE) methods use both audio and visual\nfeatures for the task of speech enhancement and the use of visual features has\nbeen shown to be particularly effective in multi-speaker scenarios. In the\nmajority of deep neural network (DNN) based AVSE methods, the audio and visual\ndata are first processed separately using different sub-networks, and then the\nlearned features are fused to utilize the information from both modalities.\nThere have been various studies on suitable audio input features and network\narchitectures, however, to the best of our knowledge, there is no published\nstudy that has investigated which visual features are best suited for this\nspecific task. In this work, we perform an empirical study of the most commonly\nused visual features for DNN based AVSE, the pre-processing requirements for\neach of these features, and investigate their influence on the performance. Our\nstudy shows that despite the overall better performance of embedding-based\nfeatures, their computationally intensive pre-processing make their use\ndifficult in low resource systems. For such systems, optical flow or raw\npixels-based features might be better suited.\n", "versions": [{"version": "v1", "created": "Mon, 9 Nov 2020 11:48:14 GMT"}], "update_date": "2020-11-10", "authors_parsed": [["Shetu", "Shrishti Saha", ""], ["Chakrabarty", "Soumitro", ""], ["Habets", "Emanu\u00ebl A. P.", ""]]}, {"id": "2011.04389", "submitter": "Hoang-An Le", "authors": "Hoang-An Le, Thomas Mensink, Partha Das, Sezer Karaoglu, Theo Gevers", "title": "EDEN: Multimodal Synthetic Dataset of Enclosed GarDEN Scenes", "comments": "Accepted for publishing at WACV 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multimodal large-scale datasets for outdoor scenes are mostly designed for\nurban driving problems. The scenes are highly structured and semantically\ndifferent from scenarios seen in nature-centered scenes such as gardens or\nparks. To promote machine learning methods for nature-oriented applications,\nsuch as agriculture and gardening, we propose the multimodal synthetic dataset\nfor Enclosed garDEN scenes (EDEN). The dataset features more than 300K images\ncaptured from more than 100 garden models. Each image is annotated with various\nlow/high-level vision modalities, including semantic segmentation, depth,\nsurface normals, intrinsic colors, and optical flow. Experimental results on\nthe state-of-the-art methods for semantic segmentation and monocular depth\nprediction, two important tasks in computer vision, show positive impact of\npre-training deep networks on our dataset for unstructured natural scenes. The\ndataset and related materials will be available at\nhttps://lhoangan.github.io/eden.\n", "versions": [{"version": "v1", "created": "Mon, 9 Nov 2020 12:44:29 GMT"}, {"version": "v2", "created": "Tue, 10 Nov 2020 20:11:31 GMT"}], "update_date": "2020-11-12", "authors_parsed": [["Le", "Hoang-An", ""], ["Mensink", "Thomas", ""], ["Das", "Partha", ""], ["Karaoglu", "Sezer", ""], ["Gevers", "Theo", ""]]}, {"id": "2011.04408", "submitter": "Hanjiang Hu", "authors": "Hanjiang Hu, Baoquan Yang, Zhijian Qiao, Ding Zhao, Hesheng Wang", "title": "SeasonDepth: Cross-Season Monocular Depth Prediction Dataset and\n  Benchmark under Multiple Environments", "comments": "21 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Different environments pose a great challenge on the outdoor robust visual\nperception for long-term autonomous driving and the generalization of\nlearning-based algorithms on different environmental effects is still an open\nproblem. Although monocular depth prediction has been well studied recently,\nthere is few work focusing on the robust learning-based depth prediction across\ndifferent environments, e.g., changing illumination and seasons, owing to the\nlack of such a multi-environment real-world dataset and benchmark. To this end,\nthe first cross-season monocular depth prediction dataset and benchmark\nSeasonDepth (available on https://seasondepth.github.io/) is built based on CMU\nVisual Localization dataset. To benchmark the depth estimation performance\nunder different environments, we investigate representative and recent\nstate-of-the-art open-source supervised, self-supervised and domain adaptation\ndepth prediction methods from KITTI benchmark using several newly-formulated\nmetrics. Through extensive experimental evaluation on the proposed dataset, the\ninfluence of multiple environments on performance and robustness is analyzed\nboth qualitatively and quantitatively, showing that the long-term monocular\ndepth prediction is far from solved even with fine-tuning. We further give\npromising avenues that self-supervised training and stereo geometry constraint\nhelp to enhance the robustness to changing environments.\n", "versions": [{"version": "v1", "created": "Mon, 9 Nov 2020 13:24:45 GMT"}, {"version": "v2", "created": "Tue, 8 Jun 2021 14:35:07 GMT"}, {"version": "v3", "created": "Wed, 14 Jul 2021 09:31:15 GMT"}], "update_date": "2021-07-15", "authors_parsed": [["Hu", "Hanjiang", ""], ["Yang", "Baoquan", ""], ["Qiao", "Zhijian", ""], ["Zhao", "Ding", ""], ["Wang", "Hesheng", ""]]}, {"id": "2011.04439", "submitter": "Soumya Tripathy", "authors": "Soumya Tripathy, Juho Kannala and Esa Rahtu", "title": "FACEGAN: Facial Attribute Controllable rEenactment GAN", "comments": "Accepted to WACV-2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The face reenactment is a popular facial animation method where the person's\nidentity is taken from the source image and the facial motion from the driving\nimage. Recent works have demonstrated high quality results by combining the\nfacial landmark based motion representations with the generative adversarial\nnetworks. These models perform best if the source and driving images depict the\nsame person or if the facial structures are otherwise very similar. However, if\nthe identity differs, the driving facial structures leak to the output\ndistorting the reenactment result. We propose a novel Facial Attribute\nControllable rEenactment GAN (FACEGAN), which transfers the facial motion from\nthe driving face via the Action Unit (AU) representation. Unlike facial\nlandmarks, the AUs are independent of the facial structure preventing the\nidentity leak. Moreover, AUs provide a human interpretable way to control the\nreenactment. FACEGAN processes background and face regions separately for\noptimized output quality. The extensive quantitative and qualitative\ncomparisons show a clear improvement over the state-of-the-art in a single\nsource reenactment task. The results are best illustrated in the reenactment\nvideo provided in the supplementary material. The source code will be made\navailable upon publication of the paper.\n", "versions": [{"version": "v1", "created": "Mon, 9 Nov 2020 14:04:15 GMT"}], "update_date": "2020-11-10", "authors_parsed": [["Tripathy", "Soumya", ""], ["Kannala", "Juho", ""], ["Rahtu", "Esa", ""]]}, {"id": "2011.04445", "submitter": "Hyojin Park", "authors": "Hyojin Park, Ganesh Venkatesh, Nojun Kwak", "title": "TTVOS: Lightweight Video Object Segmentation with Adaptive Template\n  Attention Module and Temporal Consistency Loss", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Semi-supervised video object segmentation (semi-VOS) is widely used in many\napplications. This task is tracking class-agnostic objects from a given target\nmask. For doing this, various approaches have been developed based on\nonline-learning, memory networks, and optical flow. These methods show high\naccuracy but are hard to be utilized in real-world applications due to slow\ninference time and tremendous complexity. To resolve this problem, template\nmatching methods are devised for fast processing speed but sacrificing lots of\nperformance in previous models. We introduce a novel semi-VOS model based on a\ntemplate matching method and a temporal consistency loss to reduce the\nperformance gap from heavy models while expediting inference time a lot. Our\ntemplate matching method consists of short-term and long-term matching. The\nshort-term matching enhances target object localization, while long-term\nmatching improves fine details and handles object shape-changing through the\nnewly proposed adaptive template attention module. However, the long-term\nmatching causes error-propagation due to the inflow of the past estimated\nresults when updating the template. To mitigate this problem, we also propose a\ntemporal consistency loss for better temporal coherence between neighboring\nframes by adopting the concept of a transition matrix. Our model obtains 79.5%\nJ&F score at the speed of 73.8 FPS on the DAVIS16 benchmark. The code is\navailable in https://github.com/HYOJINPARK/TTVOS.\n", "versions": [{"version": "v1", "created": "Mon, 9 Nov 2020 14:09:54 GMT"}, {"version": "v2", "created": "Mon, 8 Mar 2021 17:01:03 GMT"}, {"version": "v3", "created": "Sun, 4 Apr 2021 10:02:52 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Park", "Hyojin", ""], ["Venkatesh", "Ganesh", ""], ["Kwak", "Nojun", ""]]}, {"id": "2011.04463", "submitter": "Susana Lai-Yuen", "authors": "Maria Baldeon Calisto and Susana Lai-Yuen", "title": "Neural Architecture Search with an Efficient Multiobjective Evolutionary\n  Framework", "comments": "11 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning methods have become very successful at solving many complex\ntasks such as image classification and segmentation, speech recognition and\nmachine translation. Nevertheless, manually designing a neural network for a\nspecific problem is very difficult and time-consuming due to the massive\nhyperparameter search space, long training times, and lack of technical\nguidelines for the hyperparameter selection. Moreover, most networks are highly\ncomplex, task specific and over-parametrized. Recently, multiobjective neural\narchitecture search (NAS) methods have been proposed to automate the design of\naccurate and efficient architectures. However, they only optimize either the\nmacro- or micro-structure of the architecture requiring the unset\nhyperparameters to be manually defined, and do not use the information produced\nduring the optimization process to increase the efficiency of the search. In\nthis work, we propose EMONAS, an Efficient MultiObjective Neural Architecture\nSearch framework for the automatic design of neural architectures while\noptimizing the network's accuracy and size. EMONAS is composed of a search\nspace that considers both the macro- and micro-structure of the architecture,\nand a surrogate-assisted multiobjective evolutionary based algorithm that\nefficiently searches for the best hyperparameters using a Random Forest\nsurrogate and guiding selection probabilities. EMONAS is evaluated on the task\nof 3D cardiac segmentation from the MICCAI ACDC challenge, which is crucial for\ndisease diagnosis, risk evaluation, and therapy decision. The architecture\nfound with EMONAS is ranked within the top 10 submissions of the challenge in\nall evaluation metrics, performing better or comparable to other approaches\nwhile reducing the search time by more than 50% and having considerably fewer\nnumber of parameters.\n", "versions": [{"version": "v1", "created": "Mon, 9 Nov 2020 14:41:10 GMT"}], "update_date": "2020-11-10", "authors_parsed": [["Calisto", "Maria Baldeon", ""], ["Lai-Yuen", "Susana", ""]]}, {"id": "2011.04464", "submitter": "\\'Angel F. Garc\\'ia-Fern\\'andez", "authors": "\\'Angel F. Garc\\'ia-Fern\\'andez, Jason L. Williams, Lennart Svensson,\n  Yuxuan Xia", "title": "A Poisson multi-Bernoulli mixture filter for coexisting point and\n  extended targets", "comments": "Matlab files can be found at\n  https://github.com/Agarciafernandez/Coexisting-point-extended-target-PMBM-filter\n  and\n  https://github.com/yuhsuansia/Coexisting-point-extended-target-PMBM-filter. A\n  relevant multi-object tracking course can be found at\n  https://www.youtube.com/channel/UCa2-fpj6AV8T6JK1uTRuFpw", "journal-ref": "in IEEE Transactions on Signal Processing, vol. 69, pp. 2600-2610,\n  2021", "doi": "10.1109/TSP.2021.3072006", "report-no": null, "categories": "stat.ME cs.CV stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a Poisson multi-Bernoulli mixture (PMBM) filter for\ncoexisting point and extended targets, i.e., for scenarios where there may be\nsimultaneous point and extended targets. The PMBM filter provides a recursion\nto compute the multi-target filtering posterior based on probabilistic\ninformation on data associations, and single-target predictions and updates. In\nthis paper, we first derive the PMBM filter update for a generalised\nmeasurement model, which can include measurements originated from point and\nextended targets. Second, we propose a single-target space that accommodates\nboth point and extended targets and derive the filtering recursion that\npropagates Gaussian densities for point targets and gamma Gaussian inverse\nWishart densities for extended targets. As a computationally efficient\napproximation of the PMBM filter, we also develop a Poisson multi-Bernoulli\n(PMB) filter for coexisting point and extended targets. The resulting filters\nare analysed via numerical simulations.\n", "versions": [{"version": "v1", "created": "Mon, 9 Nov 2020 14:41:40 GMT"}, {"version": "v2", "created": "Tue, 18 May 2021 06:28:22 GMT"}], "update_date": "2021-05-19", "authors_parsed": [["Garc\u00eda-Fern\u00e1ndez", "\u00c1ngel F.", ""], ["Williams", "Jason L.", ""], ["Svensson", "Lennart", ""], ["Xia", "Yuxuan", ""]]}, {"id": "2011.04465", "submitter": "Rinat Mukhometzianov", "authors": "Oleg Michailovich and Rinat Mukhometzianov", "title": "Towards a quantitative assessment of neurodegeneration in Alzheimer's\n  disease", "comments": "26 pages, 5 figures, submitted to IEEE Access", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Alzheimer's disease (AD) is an irreversible neurodegenerative disorder that\nprogressively destroys memory and other cognitive domains of the brain. While\neffective therapeutic management of AD is still in development, it seems\nreasonable to expect their prospective outcomes to depend on the severity of\nbaseline pathology. For this reason, substantial research efforts have been\ninvested in the development of effective means of non-invasive diagnosis of AD\nat its earliest possible stages. In pursuit of the same objective, the present\npaper addresses the problem of the quantitative diagnosis of AD by means of\nDiffusion Magnetic Resonance Imaging (dMRI). In particular, the paper\nintroduces the notion of a pathology specific imaging contrast (PSIC), which,\nin addition to supplying a valuable diagnostic score, can serve as a means of\nvisual representation of the spatial extent of neurodegeneration. The values of\nPSIC are computed by a dedicated deep neural network (DNN), which has been\nspecially adapted to the processing of dMRI signals. Once available, such\nvalues can be used for several important purposes, including stratification of\nstudy subjects. In particular, experiments confirm the DNN-based classification\ncan outperform a wide range of alternative approaches in application to the\nbasic problem of stratification of cognitively normal (CN) and AD subjects.\nNotwithstanding its preliminary nature, this result suggests a strong rationale\nfor further extension and improvement of the explorative methodology described\nin this paper.\n", "versions": [{"version": "v1", "created": "Fri, 6 Nov 2020 05:56:29 GMT"}], "update_date": "2020-11-10", "authors_parsed": [["Michailovich", "Oleg", ""], ["Mukhometzianov", "Rinat", ""]]}, {"id": "2011.04475", "submitter": "Emma Rocheteau", "authors": "Emma Rocheteau, Doyoon Kim", "title": "Deep Transfer Learning for Automated Diagnosis of Skin Lesions from\n  Photographs", "comments": "Machine Learning for Mobile Health (ML4MH) Workshop at NeurIPS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Melanoma is not the most common form of skin cancer, but it is the most\ndeadly. Currently, the disease is diagnosed by expert dermatologists, which is\ncostly and requires timely access to medical treatment. Recent advances in deep\nlearning have the potential to improve diagnostic performance, expedite urgent\nreferrals and reduce burden on clinicians. Through smart phones, the technology\ncould reach people who would not normally have access to such healthcare\nservices, e.g. in remote parts of the world, due to financial constraints or in\n2020, COVID-19 cancellations. To this end, we have investigated various\ntransfer learning approaches by leveraging model parameters pre-trained on\nImageNet with finetuning on melanoma detection. We compare EfficientNet,\nMnasNet, MobileNet, DenseNet, SqueezeNet, ShuffleNet, GoogleNet, ResNet,\nResNeXt, VGG and a simple CNN with and without transfer learning. We find the\nmobile network, EfficientNet (with transfer learning) achieves the best mean\nperformance with an area under the receiver operating characteristic curve\n(AUROC) of 0.931$\\pm$0.005 and an area under the precision recall curve (AUPRC)\nof 0.840$\\pm$0.010. This is significantly better than general practitioners\n(0.83$\\pm$0.03 AUROC) and dermatologists (0.91$\\pm$0.02 AUROC).\n", "versions": [{"version": "v1", "created": "Fri, 6 Nov 2020 16:49:40 GMT"}, {"version": "v2", "created": "Mon, 16 Nov 2020 21:21:36 GMT"}, {"version": "v3", "created": "Wed, 18 Nov 2020 14:30:04 GMT"}], "update_date": "2020-11-19", "authors_parsed": [["Rocheteau", "Emma", ""], ["Kim", "Doyoon", ""]]}, {"id": "2011.04482", "submitter": "Suyoung Lee", "authors": "Suyoung Lee, Myungsub Choi, Kyoung Mu Lee", "title": "DynaVSR: Dynamic Adaptive Blind Video Super-Resolution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most conventional supervised super-resolution (SR) algorithms assume that\nlow-resolution (LR) data is obtained by downscaling high-resolution (HR) data\nwith a fixed known kernel, but such an assumption often does not hold in real\nscenarios. Some recent blind SR algorithms have been proposed to estimate\ndifferent downscaling kernels for each input LR image. However, they suffer\nfrom heavy computational overhead, making them infeasible for direct\napplication to videos. In this work, we present DynaVSR, a novel\nmeta-learning-based framework for real-world video SR that enables efficient\ndownscaling model estimation and adaptation to the current input. Specifically,\nwe train a multi-frame downscaling module with various types of synthetic blur\nkernels, which is seamlessly combined with a video SR network for input-aware\nadaptation. Experimental results show that DynaVSR consistently improves the\nperformance of the state-of-the-art video SR models by a large margin, with an\norder of magnitude faster inference time compared to the existing blind SR\napproaches.\n", "versions": [{"version": "v1", "created": "Mon, 9 Nov 2020 15:07:32 GMT"}], "update_date": "2020-11-10", "authors_parsed": [["Lee", "Suyoung", ""], ["Choi", "Myungsub", ""], ["Lee", "Kyoung Mu", ""]]}, {"id": "2011.04522", "submitter": "Shen Cai", "authors": "Hui Cao, Jie Wang, Yuqi Liu, Siyu Zhang and Shen Cai", "title": "Fast Hybrid Cascade for Voxel-based 3D Object Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Voxel-based 3D object classification has been frequently studied in recent\nyears. The previous methods often directly convert the classic 2D convolution\ninto a 3D form applied to an object with binary voxel representation. In this\npaper, we investigate the reason why binary voxel representation is not very\nsuitable for 3D convolution and how to simultaneously improve the performance\nboth in accuracy and speed. We show that by giving each voxel a signed distance\nvalue, the accuracy will gain about 30% promotion compared with binary voxel\nrepresentation using a two-layer fully connected network. We then propose a\nfast fully connected and convolution hybrid cascade network for voxel-based 3D\nobject classification. This threestage cascade network can divide 3D models\ninto three categories: easy, moderate and hard. Consequently, the mean\ninference time (0.3ms) can speedup about 5x and 2x compared with the\nstate-of-the-art point cloud and voxel based methods respectively, while\nachieving the highest accuracy in the latter category of methods (92%).\nExperiments with ModelNet andMNIST verify the performance of the proposed\nhybrid cascade network.\n", "versions": [{"version": "v1", "created": "Mon, 9 Nov 2020 15:58:33 GMT"}, {"version": "v2", "created": "Mon, 11 Jan 2021 12:42:00 GMT"}], "update_date": "2021-01-12", "authors_parsed": [["Cao", "Hui", ""], ["Wang", "Jie", ""], ["Liu", "Yuqi", ""], ["Zhang", "Siyu", ""], ["Cai", "Shen", ""]]}, {"id": "2011.04530", "submitter": "Jacek Komorowski", "authors": "Jacek Komorowski", "title": "MinkLoc3D: Point Cloud Based Large-Scale Place Recognition", "comments": "Winter Conference on Applications of Computer Vision (WACV) 2021.\n  Project web site: https://github.com/jac99/MinkLoc3D", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  The paper presents a learning-based method for computing a discriminative 3D\npoint cloud descriptor for place recognition purposes. Existing methods, such\nas PointNetVLAD, are based on unordered point cloud representation. They use\nPointNet as the first processing step to extract local features, which are\nlater aggregated into a global descriptor. The PointNet architecture is not\nwell suited to capture local geometric structures. Thus, state-of-the-art\nmethods enhance vanilla PointNet architecture by adding different mechanism to\ncapture local contextual information, such as graph convolutional networks or\nusing hand-crafted features. We present an alternative approach, dubbed\nMinkLoc3D, to compute a discriminative 3D point cloud descriptor, based on a\nsparse voxelized point cloud representation and sparse 3D convolutions. The\nproposed method has a simple and efficient architecture. Evaluation on standard\nbenchmarks proves that MinkLoc3D outperforms current state-of-the-art. Our code\nis publicly available on the project website:\nhttps://github.com/jac99/MinkLoc3D\n", "versions": [{"version": "v1", "created": "Mon, 9 Nov 2020 16:11:52 GMT"}], "update_date": "2020-11-10", "authors_parsed": [["Komorowski", "Jacek", ""]]}, {"id": "2011.04539", "submitter": "Dominik Winkelbauer", "authors": "Dominik Winkelbauer, Maximilian Denninger, Rudolph Triebel", "title": "Learning to Localize in New Environments from Synthetic Training Data", "comments": "7 pages, 3 figures; in Proceedings of the IEEE International\n  Conference on Robotics and Automation (ICRA), 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most existing approaches for visual localization either need a detailed 3D\nmodel of the environment or, in the case of learning-based methods, must be\nretrained for each new scene. This can either be very expensive or simply\nimpossible for large, unknown environments, for example in search-and-rescue\nscenarios. Although there are learning-based approaches that operate\nscene-agnostically, the generalization capability of these methods is still\noutperformed by classical approaches. In this paper, we present an approach\nthat can generalize to new scenes by applying specific changes to the model\narchitecture, including an extended regression part, the use of hierarchical\ncorrelation layers, and the exploitation of scale and uncertainty information.\nOur approach outperforms the 5-point algorithm using SIFT features on equally\nbig images and additionally surpasses all previous learning-based approaches\nthat were trained on different data. It is also superior to most of the\napproaches that were specifically trained on the respective scenes. We also\nevaluate our approach in a scenario where only very few reference images are\navailable, showing that under such more realistic conditions our learning-based\napproach considerably exceeds both existing learning-based and classical\nmethods.\n", "versions": [{"version": "v1", "created": "Mon, 9 Nov 2020 16:19:35 GMT"}, {"version": "v2", "created": "Mon, 21 Jun 2021 08:34:34 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Winkelbauer", "Dominik", ""], ["Denninger", "Maximilian", ""], ["Triebel", "Rudolph", ""]]}, {"id": "2011.04554", "submitter": "Ece Takmaz", "authors": "Ece Takmaz, Mario Giulianelli, Sandro Pezzelle, Arabella Sinclair,\n  Raquel Fern\\'andez", "title": "Refer, Reuse, Reduce: Generating Subsequent References in Visual and\n  Conversational Contexts", "comments": "In Proceedings of the 2020 Conference on Empirical Methods in Natural\n  Language Processing (EMNLP 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dialogue participants often refer to entities or situations repeatedly within\na conversation, which contributes to its cohesiveness. Subsequent references\nexploit the common ground accumulated by the interlocutors and hence have\nseveral interesting properties, namely, they tend to be shorter and reuse\nexpressions that were effective in previous mentions. In this paper, we tackle\nthe generation of first and subsequent references in visually grounded\ndialogue. We propose a generation model that produces referring utterances\ngrounded in both the visual and the conversational context. To assess the\nreferring effectiveness of its output, we also implement a reference resolution\nsystem. Our experiments and analyses show that the model produces better, more\neffective referring utterances than a model not grounded in the dialogue\ncontext, and generates subsequent references that exhibit linguistic patterns\nakin to humans.\n", "versions": [{"version": "v1", "created": "Mon, 9 Nov 2020 16:53:54 GMT"}], "update_date": "2020-11-10", "authors_parsed": [["Takmaz", "Ece", ""], ["Giulianelli", "Mario", ""], ["Pezzelle", "Sandro", ""], ["Sinclair", "Arabella", ""], ["Fern\u00e1ndez", "Raquel", ""]]}, {"id": "2011.04556", "submitter": "Han Wang", "authors": "Han Wang", "title": "Masked Face Image Classification with Sparse Representation based on\n  Majority Voting Mechanism", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Sparse approximation is the problem to find the sparsest linear combination\nfor a signal from a redundant dictionary, which is widely applied in signal\nprocessing and compressed sensing. In this project, I manage to implement the\nOrthogonal Matching Pursuit (OMP) algorithm and Sparse Representation-based\nClassification (SRC) algorithm, then use them to finish the task of masked\nimage classification with majority voting. Here the experiment was token on the\nAR data-set, and the result shows the superiority of OMP algorithm combined\nwith SRC algorithm over masked face image classification with an accuracy of\n98.4%.\n", "versions": [{"version": "v1", "created": "Mon, 9 Nov 2020 16:55:14 GMT"}], "update_date": "2020-11-10", "authors_parsed": [["Wang", "Han", ""]]}, {"id": "2011.04566", "submitter": "Armin Mehri", "authors": "Armin Mehri, Parichehr B.Ardakani, Angel D.Sappa", "title": "MPRNet: Multi-Path Residual Network for Lightweight Image Super\n  Resolution", "comments": "10 pages, 5 figures, conference, accepted by WACV2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Lightweight super resolution networks have extremely importance for\nreal-world applications. In recent years several SR deep learning approaches\nwith outstanding achievement have been introduced by sacrificing memory and\ncomputational cost. To overcome this problem, a novel lightweight super\nresolution network is proposed, which improves the SOTA performance in\nlightweight SR and performs roughly similar to computationally expensive\nnetworks. Multi-Path Residual Network designs with a set of Residual\nconcatenation Blocks stacked with Adaptive Residual Blocks: ($i$) to adaptively\nextract informative features and learn more expressive spatial context\ninformation; ($ii$) to better leverage multi-level representations before\nup-sampling stage; and ($iii$) to allow an efficient information and gradient\nflow within the network. The proposed architecture also contains a new\nattention mechanism, Two-Fold Attention Module, to maximize the representation\nability of the model. Extensive experiments show the superiority of our model\nagainst other SOTA SR approaches.\n", "versions": [{"version": "v1", "created": "Mon, 9 Nov 2020 17:11:15 GMT"}], "update_date": "2020-11-10", "authors_parsed": [["Mehri", "Armin", ""], ["Ardakani", "Parichehr B.", ""], ["Sappa", "Angel D.", ""]]}, {"id": "2011.04592", "submitter": "Ece Takmaz", "authors": "Ece Takmaz, Sandro Pezzelle, Lisa Beinborn, Raquel Fern\\'andez", "title": "Generating Image Descriptions via Sequential Cross-Modal Alignment\n  Guided by Human Gaze", "comments": "In Proceedings of the 2020 Conference on Empirical Methods in Natural\n  Language Processing (EMNLP 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When speakers describe an image, they tend to look at objects before\nmentioning them. In this paper, we investigate such sequential cross-modal\nalignment by modelling the image description generation process\ncomputationally. We take as our starting point a state-of-the-art image\ncaptioning system and develop several model variants that exploit information\nfrom human gaze patterns recorded during language production. In particular, we\npropose the first approach to image description generation where visual\nprocessing is modelled $\\textit{sequentially}$. Our experiments and analyses\nconfirm that better descriptions can be obtained by exploiting gaze-driven\nattention and shed light on human cognitive processes by comparing different\nways of aligning the gaze modality with language production. We find that\nprocessing gaze data sequentially leads to descriptions that are better aligned\nto those produced by speakers, more diverse, and more natural${-}$particularly\nwhen gaze is encoded with a dedicated recurrent component.\n", "versions": [{"version": "v1", "created": "Mon, 9 Nov 2020 17:45:32 GMT"}], "update_date": "2020-11-10", "authors_parsed": [["Takmaz", "Ece", ""], ["Pezzelle", "Sandro", ""], ["Beinborn", "Lisa", ""], ["Fern\u00e1ndez", "Raquel", ""]]}, {"id": "2011.04612", "submitter": "Miaojing Shi", "authors": "Yanlin Qian and Miaojing Shi and Joni-Kristian K\\\"am\\\"ar\\\"ainen and\n  Jiri Matas", "title": "Fast Fourier Intrinsic Network", "comments": "WACV 2021 - camera ready", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We address the problem of decomposing an image into albedo and shading. We\npropose the Fast Fourier Intrinsic Network, FFI-Net in short, that operates in\nthe spectral domain, splitting the input into several spectral bands. Weights\nin FFI-Net are optimized in the spectral domain, allowing faster convergence to\na lower error. FFI-Net is lightweight and does not need auxiliary networks for\ntraining. The network is trained end-to-end with a novel spectral loss which\nmeasures the global distance between the network prediction and corresponding\nground truth. FFI-Net achieves state-of-the-art performance on MPI-Sintel, MIT\nIntrinsic, and IIW datasets.\n", "versions": [{"version": "v1", "created": "Mon, 9 Nov 2020 18:14:39 GMT"}], "update_date": "2020-11-10", "authors_parsed": [["Qian", "Yanlin", ""], ["Shi", "Miaojing", ""], ["K\u00e4m\u00e4r\u00e4inen", "Joni-Kristian", ""], ["Matas", "Jiri", ""]]}, {"id": "2011.04626", "submitter": "Erik Stammes", "authors": "Erik Stammes, Tom F.H. Runia, Michael Hofmann, Mohsen Ghafoorian", "title": "Find it if You Can: End-to-End Adversarial Erasing for Weakly-Supervised\n  Semantic Segmentation", "comments": "11 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semantic segmentation is a task that traditionally requires a large dataset\nof pixel-level ground truth labels, which is time-consuming and expensive to\nobtain. Recent advancements in the weakly-supervised setting show that\nreasonable performance can be obtained by using only image-level labels.\nClassification is often used as a proxy task to train a deep neural network\nfrom which attention maps are extracted. However, the classification task needs\nonly the minimum evidence to make predictions, hence it focuses on the most\ndiscriminative object regions. To overcome this problem, we propose a novel\nformulation of adversarial erasing of the attention maps. In contrast to\nprevious adversarial erasing methods, we optimize two networks with opposing\nloss functions, which eliminates the requirement of certain suboptimal\nstrategies; for instance, having multiple training steps that complicate the\ntraining process or a weight sharing policy between networks operating on\ndifferent distributions that might be suboptimal for performance. The proposed\nsolution does not require saliency masks, instead it uses a regularization loss\nto prevent the attention maps from spreading to less discriminative object\nregions. Our experiments on the Pascal VOC dataset demonstrate that our\nadversarial approach increases segmentation performance by 2.1 mIoU compared to\nour baseline and by 1.0 mIoU compared to previous adversarial erasing\napproaches.\n", "versions": [{"version": "v1", "created": "Mon, 9 Nov 2020 18:35:35 GMT"}], "update_date": "2020-11-10", "authors_parsed": [["Stammes", "Erik", ""], ["Runia", "Tom F. H.", ""], ["Hofmann", "Michael", ""], ["Ghafoorian", "Mohsen", ""]]}, {"id": "2011.04714", "submitter": "Eric M\\\"uller-Budack", "authors": "Eric M\\\"uller-Budack, Matthias Springstein, Sherzod Hakimov, Kevin\n  Mrutzek, Ralph Ewerth", "title": "Ontology-driven Event Type Classification in Images", "comments": "Accepted for publication in: IEEE Winter Conference on Applications\n  of Computer Vision (WACV) 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Event classification can add valuable information for semantic search and the\nincreasingly important topic of fact validation in news. So far, only few\napproaches address image classification for newsworthy event types such as\nnatural disasters, sports events, or elections. Previous work distinguishes\nonly between a limited number of event types and relies on rather small\ndatasets for training. In this paper, we present a novel ontology-driven\napproach for the classification of event types in images. We leverage a large\nnumber of real-world news events to pursue two objectives: First, we create an\nontology based on Wikidata comprising the majority of event types. Second, we\nintroduce a novel large-scale dataset that was acquired through Web crawling.\nSeveral baselines are proposed including an ontology-driven learning approach\nthat aims to exploit structured information of a knowledge graph to learn\nrelevant event relations using deep neural networks. Experimental results on\nexisting as well as novel benchmark datasets demonstrate the superiority of the\nproposed ontology-driven approach.\n", "versions": [{"version": "v1", "created": "Mon, 9 Nov 2020 19:43:55 GMT"}], "update_date": "2020-11-11", "authors_parsed": [["M\u00fcller-Budack", "Eric", ""], ["Springstein", "Matthias", ""], ["Hakimov", "Sherzod", ""], ["Mrutzek", "Kevin", ""], ["Ewerth", "Ralph", ""]]}, {"id": "2011.04728", "submitter": "Dishant Parikh", "authors": "Dishant Parikh, Shambhavi Aggarwal", "title": "Similarity-Based Clustering for Enhancing Image Classification\n  Architectures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional networks are at the center of best in class computer vision\napplications for a wide assortment of undertakings. Since 2014, profound amount\nof work began to make better convolutional architectures, yielding generous\nadditions in different benchmarks. Albeit expanded model size and computational\ncost will, in general, mean prompt quality increases for most undertakings but,\nthe architectures now need to have some additional information to increase the\nperformance. We show empirical evidence that with the amalgamation of\ncontent-based image similarity and deep learning models, we can provide the\nflow of information which can be used in making clustered learning possible. We\nshow how parallel training of sub-dataset clusters not only reduces the cost of\ncomputation but also increases the benchmark accuracies by 5-11 percent.\n", "versions": [{"version": "v1", "created": "Tue, 3 Nov 2020 17:03:28 GMT"}], "update_date": "2020-11-11", "authors_parsed": [["Parikh", "Dishant", ""], ["Aggarwal", "Shambhavi", ""]]}, {"id": "2011.04750", "submitter": "Mohamed Bakhouya", "authors": "Nadir Maaroufi, Mehdi Najib, Mohamed Bakhouya", "title": "Predicting the Future is like Completing a Painting!", "comments": "25 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article is an introductory work towards a larger research framework\nrelative to Scientific Prediction. It is a mixed between science and philosophy\nof science, therefore we can talk about Experimental Philosophy of Science. As\na first result, we introduce a new forecasting method based on image\ncompletion, named Forecasting Method by Image Inpainting (FM2I). In fact, time\nseries forecasting is transformed into fully images- and signal-based\nprocessing procedures. After transforming a time series data into its\ncorresponding image, the problem of data forecasting becomes essentially a\nproblem of image inpainting problem, i.e., completing missing data in the\nimage. An extensive experimental evaluation is conducted using a large dataset\nproposed by the well-known M3-competition. Results show that FM2I represents an\nefficient and robust tool for time series forecasting. It has achieved\nprominent results in terms of accuracy and outperforms the best M3 forecasting\nmethods.\n", "versions": [{"version": "v1", "created": "Mon, 9 Nov 2020 20:48:06 GMT"}], "update_date": "2020-11-11", "authors_parsed": [["Maaroufi", "Nadir", ""], ["Najib", "Mehdi", ""], ["Bakhouya", "Mohamed", ""]]}, {"id": "2011.04755", "submitter": "Fangyin Wei", "authors": "Fangyin Wei, Elena Sizikova, Avneesh Sud, Szymon Rusinkiewicz, Thomas\n  Funkhouser", "title": "Learning to Infer Semantic Parameters for 3D Shape Editing", "comments": "22 pages and 19 figures including supplementary material; to be\n  published in the proceedings of 3DV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many applications in 3D shape design and augmentation require the ability to\nmake specific edits to an object's semantic parameters (e.g., the pose of a\nperson's arm or the length of an airplane's wing) while preserving as much\nexisting details as possible. We propose to learn a deep network that infers\nthe semantic parameters of an input shape and then allows the user to\nmanipulate those parameters. The network is trained jointly on shapes from an\nauxiliary synthetic template and unlabeled realistic models, ensuring\nrobustness to shape variability while relieving the need to label realistic\nexemplars. At testing time, edits within the parameter space drive deformations\nto be applied to the original shape, which provides semantically-meaningful\nmanipulation while preserving the details. This is in contrast to prior methods\nthat either use autoencoders with a limited latent-space dimensionality,\nfailing to preserve arbitrary detail, or drive deformations with\npurely-geometric controls, such as cages, losing the ability to update local\npart regions. Experiments with datasets of chairs, airplanes, and human bodies\ndemonstrate that our method produces more natural edits than prior work.\n", "versions": [{"version": "v1", "created": "Mon, 9 Nov 2020 20:58:49 GMT"}], "update_date": "2020-11-11", "authors_parsed": [["Wei", "Fangyin", ""], ["Sizikova", "Elena", ""], ["Sud", "Avneesh", ""], ["Rusinkiewicz", "Szymon", ""], ["Funkhouser", "Thomas", ""]]}, {"id": "2011.04761", "submitter": "Xiaodan Hu", "authors": "Xiaodan Hu, Pengfei Yu, Kevin Knight, Heng Ji, Bo Li, Honghui Shi", "title": "MUSE: Illustrating Textual Attributes by Portrait Generation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel approach, MUSE, to illustrate textual attributes visually\nvia portrait generation. MUSE takes a set of attributes written in text, in\naddition to facial features extracted from a photo of the subject as input. We\npropose 11 attribute types to represent inspirations from a subject's profile,\nemotion, story, and environment. We propose a novel stacked neural network\narchitecture by extending an image-to-image generative model to accept textual\nattributes. Experiments show that our approach significantly outperforms\nseveral state-of-the-art methods without using textual attributes, with\nInception Score score increased by 6% and Fr\\'echet Inception Distance (FID)\nscore decreased by 11%, respectively. We also propose a new attribute\nreconstruction metric to evaluate whether the generated portraits preserve the\nsubject's attributes. Experiments show that our approach can accurately\nillustrate 78% textual attributes, which also help MUSE capture the subject in\na more creative and expressive way.\n", "versions": [{"version": "v1", "created": "Mon, 9 Nov 2020 21:05:21 GMT"}], "update_date": "2020-11-11", "authors_parsed": [["Hu", "Xiaodan", ""], ["Yu", "Pengfei", ""], ["Knight", "Kevin", ""], ["Ji", "Heng", ""], ["Li", "Bo", ""], ["Shi", "Honghui", ""]]}, {"id": "2011.04762", "submitter": "Shahine Bouabid", "authors": "Shahine Bouabid, Maxim Chernetskiy, Maxime Rischard and Jevgenij\n  Gamper", "title": "Predicting Landsat Reflectance with Deep Generative Fusion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Public satellite missions are commonly bound to a trade-off between spatial\nand temporal resolution as no single sensor provides fine-grained acquisitions\nwith frequent coverage. This hinders their potential to assist vegetation\nmonitoring or humanitarian actions, which require detecting rapid and detailed\nterrestrial surface changes. In this work, we probe the potential of deep\ngenerative models to produce high-resolution optical imagery by fusing products\nwith different spatial and temporal characteristics. We introduce a dataset of\nco-registered Moderate Resolution Imaging Spectroradiometer (MODIS) and Landsat\nsurface reflectance time series and demonstrate the ability of our generative\nmodel to blend coarse daily reflectance information into low-paced finer\nacquisitions. We benchmark our proposed model against state-of-the-art\nreflectance fusion algorithms.\n", "versions": [{"version": "v1", "created": "Mon, 9 Nov 2020 21:06:04 GMT"}], "update_date": "2020-11-20", "authors_parsed": [["Bouabid", "Shahine", ""], ["Chernetskiy", "Maxim", ""], ["Rischard", "Maxime", ""], ["Gamper", "Jevgenij", ""]]}, {"id": "2011.04764", "submitter": "Joshua Romoff", "authors": "Eloi Alonso, Maxim Peter, David Goumard, Joshua Romoff", "title": "Deep Reinforcement Learning for Navigation in AAA Video Games", "comments": "Accepted to the NeurIPS 2020 Challenges of Real-World RL workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In video games, non-player characters (NPCs) are used to enhance the players'\nexperience in a variety of ways, e.g., as enemies, allies, or innocent\nbystanders. A crucial component of NPCs is navigation, which allows them to\nmove from one point to another on the map. The most popular approach for NPC\nnavigation in the video game industry is to use a navigation mesh (NavMesh),\nwhich is a graph representation of the map, with nodes and edges indicating\ntraversable areas. Unfortunately, complex navigation abilities that extend the\ncharacter's capacity for movement, e.g., grappling hooks, jetpacks,\nteleportation, or double-jumps, increases the complexity of the NavMesh, making\nit intractable in many practical scenarios. Game designers are thus constrained\nto only add abilities that can be handled by a NavMesh if they want to have NPC\nnavigation. As an alternative, we propose to use Deep Reinforcement Learning\n(Deep RL) to learn how to navigate 3D maps using any navigation ability. We\ntest our approach on complex 3D environments in the Unity game engine that are\nnotably an order of magnitude larger than maps typically used in the Deep RL\nliterature. One of these maps is directly modeled after a Ubisoft AAA game. We\nfind that our approach performs surprisingly well, achieving at least $90\\%$\nsuccess rate on all tested scenarios. A video of our results is available at\nhttps://youtu.be/WFIf9Wwlq8M.\n", "versions": [{"version": "v1", "created": "Mon, 9 Nov 2020 21:07:56 GMT"}, {"version": "v2", "created": "Tue, 17 Nov 2020 19:09:57 GMT"}], "update_date": "2020-11-19", "authors_parsed": [["Alonso", "Eloi", ""], ["Peter", "Maxim", ""], ["Goumard", "David", ""], ["Romoff", "Joshua", ""]]}, {"id": "2011.04776", "submitter": "Siddha Ganju", "authors": "Siddha Ganju, Anirudh Koul, Alexander Lavin, Josh Veitch-Michaelis,\n  Meher Kasam, James Parr", "title": "Learnings from Frontier Development Lab and SpaceML -- AI Accelerators\n  for NASA and ESA", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "astro-ph.IM astro-ph.GA cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Research with AI and ML technologies lives in a variety of settings with\noften asynchronous goals and timelines: academic labs and government\norganizations pursue open-ended research focusing on discoveries with long-term\nvalue, while research in industry is driven by commercial pursuits and hence\nfocuses on short-term timelines and return on investment. The journey from\nresearch to product is often tacit or ad hoc, resulting in technology\ntransition failures, further exacerbated when research and development is\ninterorganizational and interdisciplinary. Even more, much of the ability to\nproduce results remains locked in the private repositories and know-how of the\nindividual researcher, slowing the impact on future research by others and\ncontributing to the ML community's challenges in reproducibility. With research\norganizations focused on an exploding array of fields, opportunities for the\nhandover and maturation of interdisciplinary research reduce. With these\ntensions, we see an emerging need to measure the correctness, impact, and\nrelevance of research during its development to enable better collaboration,\nimproved reproducibility, faster progress, and more trusted outcomes. We\nperform a case study of the Frontier Development Lab (FDL), an AI accelerator\nunder a public-private partnership from NASA and ESA. FDL research follows\nprincipled practices that are grounded in responsible development, conduct, and\ndissemination of AI research, enabling FDL to churn successful\ninterdisciplinary and interorganizational research projects, measured through\nNASA's Technology Readiness Levels. We also take a look at the SpaceML Open\nSource Research Program, which helps accelerate and transition FDL's research\nto deployable projects with wide spread adoption amongst citizen scientists.\n", "versions": [{"version": "v1", "created": "Mon, 9 Nov 2020 21:23:03 GMT"}], "update_date": "2021-02-17", "authors_parsed": [["Ganju", "Siddha", ""], ["Koul", "Anirudh", ""], ["Lavin", "Alexander", ""], ["Veitch-Michaelis", "Josh", ""], ["Kasam", "Meher", ""], ["Parr", "James", ""]]}, {"id": "2011.04779", "submitter": "Mohamed Karim Belaid", "authors": "Mohamed Karim Belaid", "title": "After All, Only The Last Neuron Matters: Comparing Multi-modal Fusion\n  Functions for Scene Graph Generation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  From object segmentation to word vector representations, Scene Graph\nGeneration (SGG) became a complex task built upon numerous research results. In\nthis paper, we focus on the last module of this model: the fusion function. The\nrole of this latter is to combine three hidden states. We perform an ablation\ntest in order to compare different implementations. First, we reproduce the\nstate-of-the-art results using SUM, and GATE functions. Then we expand the\noriginal solution by adding more model-agnostic functions: an adapted version\nof DIST and a mixture between MFB and GATE. On the basis of the\nstate-of-the-art configuration, DIST performed the best Recall @ K, which makes\nit now part of the state-of-the-art.\n", "versions": [{"version": "v1", "created": "Mon, 9 Nov 2020 21:27:32 GMT"}], "update_date": "2020-11-11", "authors_parsed": [["Belaid", "Mohamed Karim", ""]]}, {"id": "2011.04837", "submitter": "Zhengyi Luo", "authors": "Zhengyi Luo, Ryo Hachiuma, Ye Yuan, Shun Iwase, Kris M. Kitani", "title": "Kinematics-Guided Reinforcement Learning for Object-Aware 3D Ego-Pose\n  Estimation", "comments": "Project website:\n  https://zhengyiluo.github.io/projects/contextegopose/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We propose a method for incorporating object interaction and human body\ndynamics into the task of 3D ego-pose estimation using a head-mounted camera.\nWe use a kinematics model of the human body to represent the entire range of\nhuman motion, and a dynamics model of the body to interact with objects inside\na physics simulator. By bringing together object modeling, kinematics modeling,\nand dynamics modeling in a reinforcement learning (RL) framework, we enable\nobject-aware 3D ego-pose estimation. We devise several representational\ninnovations through the design of the state and action space to incorporate 3D\nscene context and improve pose estimation quality. We also construct a\nfine-tuning step to correct the drift and refine the estimated human-object\ninteraction. This is the first work to estimate a physically valid 3D full-body\ninteraction sequence with objects (e.g., chairs, boxes, obstacles) from\negocentric videos. Experiments with both controlled and in-the-wild settings\nshow that our method can successfully extract an object-conditioned 3D ego-pose\nsequence that is consistent with the laws of physics.\n", "versions": [{"version": "v1", "created": "Tue, 10 Nov 2020 00:06:43 GMT"}, {"version": "v2", "created": "Sun, 15 Nov 2020 16:04:16 GMT"}, {"version": "v3", "created": "Wed, 9 Dec 2020 03:11:03 GMT"}], "update_date": "2020-12-10", "authors_parsed": [["Luo", "Zhengyi", ""], ["Hachiuma", "Ryo", ""], ["Yuan", "Ye", ""], ["Iwase", "Shun", ""], ["Kitani", "Kris M.", ""]]}, {"id": "2011.04841", "submitter": "Ramin Nabati", "authors": "Ramin Nabati, Hairong Qi", "title": "CenterFusion: Center-based Radar and Camera Fusion for 3D Object\n  Detection", "comments": "WACV 2021", "journal-ref": null, "doi": "10.1109/WACV48630.2021.00157", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The perception system in autonomous vehicles is responsible for detecting and\ntracking the surrounding objects. This is usually done by taking advantage of\nseveral sensing modalities to increase robustness and accuracy, which makes\nsensor fusion a crucial part of the perception system. In this paper, we focus\non the problem of radar and camera sensor fusion and propose a middle-fusion\napproach to exploit both radar and camera data for 3D object detection. Our\napproach, called CenterFusion, first uses a center point detection network to\ndetect objects by identifying their center points on the image. It then solves\nthe key data association problem using a novel frustum-based method to\nassociate the radar detections to their corresponding object's center point.\nThe associated radar detections are used to generate radar-based feature maps\nto complement the image features, and regress to object properties such as\ndepth, rotation and velocity. We evaluate CenterFusion on the challenging\nnuScenes dataset, where it improves the overall nuScenes Detection Score (NDS)\nof the state-of-the-art camera-based algorithm by more than 12%. We further\nshow that CenterFusion significantly improves the velocity estimation accuracy\nwithout using any additional temporal information. The code is available at\nhttps://github.com/mrnabati/CenterFusion .\n", "versions": [{"version": "v1", "created": "Tue, 10 Nov 2020 00:20:23 GMT"}], "update_date": "2021-01-07", "authors_parsed": [["Nabati", "Ramin", ""], ["Qi", "Hairong", ""]]}, {"id": "2011.04844", "submitter": "Shenyi Pan", "authors": "Shenyi Pan, Shuxian Fan, Samuel W.K. Wong, James V. Zidek, Helge\n  Rhodin", "title": "Ellipse Detection and Localization with Applications to Knots in Sawn\n  Lumber Images", "comments": "Accepted at WACV 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While general object detection has seen tremendous progress, localization of\nelliptical objects has received little attention in the literature. Our\nmotivating application is the detection of knots in sawn timber images, which\nis an important problem since the number and types of knots are visual\ncharacteristics that adversely affect the quality of sawn timber. We\ndemonstrate how models can be tailored to the elliptical shape and thereby\nimprove on general purpose detectors; more generally, elliptical defects are\ncommon in industrial production, such as enclosed air bubbles when casting\nglass or plastic. In this paper, we adapt the Faster R-CNN with its Region\nProposal Network (RPN) to model elliptical objects with a Gaussian function,\nand extend the existing Gaussian Proposal Network (GPN) architecture by adding\nthe region-of-interest pooling and regression branches, as well as using the\nWasserstein distance as the loss function to predict the precise locations of\nelliptical objects. Our proposed method has promising results on the lumber\nknot dataset: knots are detected with an average intersection over union of\n73.05%, compared to 63.63% for general purpose detectors. Specific to the\nlumber application, we also propose an algorithm to correct any misalignment in\nthe raw timber images during scanning, and contribute the first open-source\nlumber knot dataset by labeling the elliptical knots in the preprocessed\nimages.\n", "versions": [{"version": "v1", "created": "Tue, 10 Nov 2020 00:39:56 GMT"}], "update_date": "2020-11-11", "authors_parsed": [["Pan", "Shenyi", ""], ["Fan", "Shuxian", ""], ["Wong", "Samuel W. K.", ""], ["Zidek", "James V.", ""], ["Rhodin", "Helge", ""]]}, {"id": "2011.04853", "submitter": "Srikanth Malla", "authors": "Srikanth Malla, Chiho Choi, Behzad Dariush", "title": "Social-STAGE: Spatio-Temporal Multi-Modal Future Trajectory Forecast", "comments": "ICRA 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  This paper considers the problem of multi-modal future trajectory forecast\nwith ranking. Here, multi-modality and ranking refer to the multiple plausible\npath predictions and the confidence in those predictions, respectively. We\npropose Social-STAGE, Social interaction-aware Spatio-Temporal multi-Attention\nGraph convolution network with novel Evaluation for multi-modality. Our main\ncontributions include analysis and formulation of multi-modality with ranking\nusing interaction and multi-attention, and introduction of new metrics to\nevaluate the diversity and associated confidence of multi-modal predictions. We\nevaluate our approach on existing public datasets ETH and UCY and show that the\nproposed algorithm outperforms the state of the arts on these datasets.\n", "versions": [{"version": "v1", "created": "Tue, 10 Nov 2020 01:18:57 GMT"}, {"version": "v2", "created": "Wed, 24 Mar 2021 21:32:44 GMT"}], "update_date": "2021-03-26", "authors_parsed": [["Malla", "Srikanth", ""], ["Choi", "Chiho", ""], ["Dariush", "Behzad", ""]]}, {"id": "2011.04860", "submitter": "Arpita Vats", "authors": "Arpita Vats", "title": "Understanding the hand-gestures using Convolutional Neural Networks and\n  Generative Adversial Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, it is introduced a hand gesture recognition system to\nrecognize the characters in the real time. The system consists of three\nmodules: real time hand tracking, training gesture and gesture recognition\nusing Convolutional Neural Networks. Camshift algorithm and hand blobs analysis\nfor hand tracking are being used to obtain motion descriptors and hand region.\nIt is fairy robust to background cluster and uses skin color for hand gesture\ntracking and recognition. Furthermore, the techniques have been proposed to\nimprove the performance of the recognition and the accuracy using the\napproaches like selection of the training images and the adaptive threshold\ngesture to remove non-gesture pattern that helps to qualify an input pattern as\na gesture. In the experiments, it has been tested to the vocabulary of 36\ngestures including the alphabets and digits, and results effectiveness of the\napproach.\n", "versions": [{"version": "v1", "created": "Tue, 10 Nov 2020 02:20:43 GMT"}], "update_date": "2020-11-11", "authors_parsed": [["Vats", "Arpita", ""]]}, {"id": "2011.04862", "submitter": "Jiaqi Yang", "authors": "Jiaqi Yang, Zhiqiang Huang, Siwen Quan, Qian Zhang, Yanning Zhang,\n  Zhiguo Cao", "title": "On Efficient and Robust Metrics for RANSAC Hypotheses and 3D Rigid\n  Registration", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper focuses on developing efficient and robust evaluation metrics for\nRANSAC hypotheses to achieve accurate 3D rigid registration. Estimating\nsix-degree-of-freedom (6-DoF) pose from feature correspondences remains a\npopular approach to 3D rigid registration, where random sample consensus\n(RANSAC) is a de-facto choice to this problem. However, existing metrics for\nRANSAC hypotheses are either time-consuming or sensitive to common nuisances,\nparameter variations, and different application scenarios, resulting in\nperformance deterioration in overall registration accuracy and speed. We\nalleviate this problem by first analyzing the contributions of inliers and\noutliers, and then proposing several efficient and robust metrics with\ndifferent designing motivations for RANSAC hypotheses. Comparative experiments\non four standard datasets with different nuisances and application scenarios\nverify that the proposed metrics can significantly improve the registration\nperformance and are more robust than several state-of-the-art competitors,\nmaking them good gifts to practical applications. This work also draws an\ninteresting conclusion, i.e., not all inliers are equal while all outliers\nshould be equal, which may shed new light on this research problem.\n", "versions": [{"version": "v1", "created": "Tue, 10 Nov 2020 02:22:45 GMT"}], "update_date": "2020-11-11", "authors_parsed": [["Yang", "Jiaqi", ""], ["Huang", "Zhiqiang", ""], ["Quan", "Siwen", ""], ["Zhang", "Qian", ""], ["Zhang", "Yanning", ""], ["Cao", "Zhiguo", ""]]}, {"id": "2011.04863", "submitter": "Yichao Cao", "authors": "Yichao Cao, Qingfei Tang, Xiaobo Lu, Fan Li, and Jinde Cao", "title": "STCNet: Spatio-Temporal Cross Network for Industrial Smoke Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Industrial smoke emissions present a serious threat to natural ecosystems and\nhuman health. Prior works have shown that using computer vision techniques to\nidentify smoke is a low cost and convenient method. However, industrial smoke\ndetection is a challenging task because industrial emission particles are often\ndecay rapidly outside the stacks or facilities and steam is very similar to\nsmoke. To overcome these problems, a novel Spatio-Temporal Cross Network\n(STCNet) is proposed to recognize industrial smoke emissions. The proposed\nSTCNet involves a spatial pathway to extract texture features and a temporal\npathway to capture smoke motion information. We assume that spatial and\ntemporal pathway could guide each other. For example, the spatial path can\neasily recognize the obvious interference such as trees and buildings, and the\ntemporal path can highlight the obscure traces of smoke movement. If the two\npathways could guide each other, it will be helpful for the smoke detection\nperformance. In addition, we design an efficient and concise spatio-temporal\ndual pyramid architecture to ensure better fusion of multi-scale spatiotemporal\ninformation. Finally, extensive experiments on public dataset show that our\nSTCNet achieves clear improvements on the challenging RISE industrial smoke\ndetection dataset against the best competitors by 6.2%. The code will be\navailable at: https://github.com/Caoyichao/STCNet.\n", "versions": [{"version": "v1", "created": "Tue, 10 Nov 2020 02:28:47 GMT"}], "update_date": "2020-11-11", "authors_parsed": [["Cao", "Yichao", ""], ["Tang", "Qingfei", ""], ["Lu", "Xiaobo", ""], ["Li", "Fan", ""], ["Cao", "Jinde", ""]]}, {"id": "2011.04884", "submitter": "Mohamed Mhiri", "authors": "Mohamed Mhiri, Samuel Myer, Vikrant Singh Tomar", "title": "A low latency ASR-free end to end spoken language understanding system", "comments": null, "journal-ref": "Interspeech 2020", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, developing a speech understanding system that classifies a\nwaveform to structured data, such as intents and slots, without first\ntranscribing the speech to text has emerged as an interesting research problem.\nThis work proposes such as system with an additional constraint of designing a\nsystem that has a small enough footprint to run on small micro-controllers and\nembedded systems with minimal latency. Given a streaming input speech signal,\nthe proposed system can process it segment-by-segment without the need to have\nthe entire stream at the moment of processing. The proposed system is evaluated\non the publicly available Fluent Speech Commands dataset. Experiments show that\nthe proposed system yields state-of-the-art performance with the advantage of\nlow latency and a much smaller model when compared to other published works on\nthe same task.\n", "versions": [{"version": "v1", "created": "Tue, 10 Nov 2020 04:16:56 GMT"}], "update_date": "2020-11-11", "authors_parsed": [["Mhiri", "Mohamed", ""], ["Myer", "Samuel", ""], ["Tomar", "Vikrant Singh", ""]]}, {"id": "2011.04887", "submitter": "Runmin Cong", "authors": "Qijian Zhang, Runmin Cong, Junhui Hou, Chongyi Li, Yao Zhao", "title": "CoADNet: Collaborative Aggregation-and-Distribution Networks for\n  Co-Salient Object Detection", "comments": "Accepted by NeurIPS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Co-Salient Object Detection (CoSOD) aims at discovering salient objects that\nrepeatedly appear in a given query group containing two or more relevant\nimages. One challenging issue is how to effectively capture co-saliency cues by\nmodeling and exploiting inter-image relationships. In this paper, we present an\nend-to-end collaborative aggregation-and-distribution network (CoADNet) to\ncapture both salient and repetitive visual patterns from multiple images.\nFirst, we integrate saliency priors into the backbone features to suppress the\nredundant background information through an online intra-saliency guidance\nstructure. After that, we design a two-stage aggregate-and-distribute\narchitecture to explore group-wise semantic interactions and produce the\nco-saliency features. In the first stage, we propose a group-attentional\nsemantic aggregation module that models inter-image relationships to generate\nthe group-wise semantic representations. In the second stage, we propose a\ngated group distribution module that adaptively distributes the learned group\nsemantics to different individuals in a dynamic gating mechanism. Finally, we\ndevelop a group consistency preserving decoder tailored for the CoSOD task,\nwhich maintains group constraints during feature decoding to predict more\nconsistent full-resolution co-saliency maps. The proposed CoADNet is evaluated\non four prevailing CoSOD benchmark datasets, which demonstrates the remarkable\nperformance improvement over ten state-of-the-art competitors.\n", "versions": [{"version": "v1", "created": "Tue, 10 Nov 2020 04:28:11 GMT"}], "update_date": "2020-11-11", "authors_parsed": [["Zhang", "Qijian", ""], ["Cong", "Runmin", ""], ["Hou", "Junhui", ""], ["Li", "Chongyi", ""], ["Zhao", "Yao", ""]]}, {"id": "2011.04898", "submitter": "Jeff Calder", "authors": "Katrina Yezzi-Woodley, Jeff Calder, Peter J. Olver, Annie Melton,\n  Paige Cody, Thomas Huffstutler, Alexander Terwilliger, Martha Tappen, Reed\n  Coil, Gilbert Tostevin", "title": "The Virtual Goniometer: A new method for measuring angles on 3D models\n  of fragmentary bone and lithics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.CG cs.CV cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The contact goniometer is a commonly used tool in lithic and\nzooarchaeological analysis, despite suffering from a number of shortcomings due\nto the physical interaction between the measuring implement, the object being\nmeasured, and the individual taking the measurements. However, lacking a simple\nand efficient alternative, researchers in a variety of fields continue to use\nthe contact goniometer to this day. In this paper, we present a new goniometric\nmethod that we call the virtual goniometer, which takes angle measurements\nvirtually on a 3D model of an object. The virtual goniometer allows for rapid\ndata collection, and for the measurement of many angles that cannot be\nphysically accessed by a manual goniometer. We compare the intra-observer\nvariability of the manual and virtual goniometers, and find that the virtual\ngoniometer is far more consistent and reliable. Furthermore, the virtual\ngoniometer allows for precise replication of angle measurements, even among\nmultiple users, which is important for reproducibility of goniometric-based\nresearch. The virtual goniometer is available as a plug-in in the open source\nmesh processing packages Meshlab and Blender, making it easily accessible to\nresearchers exploring the potential for goniometry to improve archaeological\nmethods and address anthropological questions.\n", "versions": [{"version": "v1", "created": "Tue, 10 Nov 2020 05:13:29 GMT"}, {"version": "v2", "created": "Wed, 25 Nov 2020 14:01:18 GMT"}], "update_date": "2020-11-26", "authors_parsed": [["Yezzi-Woodley", "Katrina", ""], ["Calder", "Jeff", ""], ["Olver", "Peter J.", ""], ["Melton", "Annie", ""], ["Cody", "Paige", ""], ["Huffstutler", "Thomas", ""], ["Terwilliger", "Alexander", ""], ["Tappen", "Martha", ""], ["Coil", "Reed", ""], ["Tostevin", "Gilbert", ""]]}, {"id": "2011.04908", "submitter": "Mingyang Zhang", "authors": "Mingyang Zhang and Linlin Ou", "title": "Stage-wise Channel Pruning for Model Compression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Auto-ML pruning methods aim at searching a pruning strategy automatically to\nreduce the computational complexity of deep Convolutional Neural Networks(deep\nCNNs). However, some previous works found that the results of many Auto-ML\npruning methods even cannot surpass the results of the uniformly pruning\nmethod. In this paper, we first analyze the reason for the ineffectiveness of\nAuto-ML pruning. Subsequently, a stage-wise pruning(SP) method is proposed to\nsolve the above problem. As with most of the previous Auto-ML pruning methods,\nSP also trains a super-net that can provide proxy performance for sub-nets and\nsearch the best sub-net who has the best proxy performance. Different from\nprevious works, we split a deep CNN into several stages and use a full-net\nwhere all layers are not pruned to supervise the training and the searching of\nsub-nets. Remarkably, the proxy performance of sub-nets trained with SP is\ncloser to the actual performance than most of the previous Auto-ML pruning\nworks. Therefore, SP achieves the state-of-the-art on both CIFAR-10 and\nImageNet under the mobile setting.\n", "versions": [{"version": "v1", "created": "Tue, 10 Nov 2020 05:19:05 GMT"}], "update_date": "2020-11-11", "authors_parsed": [["Zhang", "Mingyang", ""], ["Ou", "Linlin", ""]]}, {"id": "2011.04926", "submitter": "Ruoyu Sun", "authors": "Ruoyu Sun, Tiantian Fang, Alex Schwing", "title": "Towards a Better Global Loss Landscape of GANs", "comments": "Accepted to NeurIPS 2020 (oral). 43 pages, 20 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV cs.IT math.IT math.OC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Understanding of GAN training is still very limited. One major challenge is\nits non-convex-non-concave min-max objective, which may lead to sub-optimal\nlocal minima. In this work, we perform a global landscape analysis of the\nempirical loss of GANs. We prove that a class of separable-GAN, including the\noriginal JS-GAN, has exponentially many bad basins which are perceived as\nmode-collapse. We also study the relativistic pairing GAN (RpGAN) loss which\ncouples the generated samples and the true samples. We prove that RpGAN has no\nbad basins. Experiments on synthetic data show that the predicted bad basin can\nindeed appear in training. We also perform experiments to support our theory\nthat RpGAN has a better landscape than separable-GAN. For instance, we\nempirically show that RpGAN performs better than separable-GAN with relatively\nnarrow neural nets. The code is available at https://github.com/AilsaF/RS-GAN.\n", "versions": [{"version": "v1", "created": "Tue, 10 Nov 2020 06:10:52 GMT"}], "update_date": "2020-11-11", "authors_parsed": [["Sun", "Ruoyu", ""], ["Fang", "Tiantian", ""], ["Schwing", "Alex", ""]]}, {"id": "2011.04943", "submitter": "Brojeshwar Bhowmick", "authors": "Junaid Ahmed Ansari and Brojeshwar Bhowmick", "title": "Simple means Faster: Real-Time Human Motion Forecasting in Monocular\n  First Person Videos on CPU", "comments": null, "journal-ref": "IROS 2020", "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a simple, fast, and light-weight RNN based framework for\nforecasting future locations of humans in first person monocular videos. The\nprimary motivation for this work was to design a network which could accurately\npredict future trajectories at a very high rate on a CPU. Typical applications\nof such a system would be a social robot or a visual assistance system for all,\nas both cannot afford to have high compute power to avoid getting heavier, less\npower efficient, and costlier. In contrast to many previous methods which rely\non multiple type of cues such as camera ego-motion or 2D pose of the human, we\nshow that a carefully designed network model which relies solely on bounding\nboxes can not only perform better but also predicts trajectories at a very high\nrate while being quite low in size of approximately 17 MB. Specifically, we\ndemonstrate that having an auto-encoder in the encoding phase of the past\ninformation and a regularizing layer in the end boosts the accuracy of\npredictions with negligible overhead. We experiment with three first person\nvideo datasets: CityWalks, FPL and JAAD. Our simple method trained on CityWalks\nsurpasses the prediction accuracy of state-of-the-art method (STED) while being\n9.6x faster on a CPU (STED runs on a GPU). We also demonstrate that our model\ncan transfer zero-shot or after just 15% fine-tuning to other similar datasets\nand perform on par with the state-of-the-art methods on such datasets (FPL and\nDTP). To the best of our knowledge, we are the first to accurately forecast\ntrajectories at a very high prediction rate of 78 trajectories per second on\nCPU.\n", "versions": [{"version": "v1", "created": "Tue, 10 Nov 2020 07:06:37 GMT"}], "update_date": "2020-11-11", "authors_parsed": [["Ansari", "Junaid Ahmed", ""], ["Bhowmick", "Brojeshwar", ""]]}, {"id": "2011.04945", "submitter": "Harshala Gammulle", "authors": "Harshala Gammulle, Simon Denman, Sridha Sridharan, Clinton Fookes", "title": "Multi-modal Fusion for Single-Stage Continuous Gesture Recognition", "comments": "Submitted to IEEE Transactions on Image Processing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gesture recognition is a much studied research area which has myriad\nreal-world applications including robotics and human-machine interaction.\nCurrent gesture recognition methods have heavily focused on isolated gestures,\nand existing continuous gesture recognition methods are limited by a two-stage\napproach where independent models are required for detection and\nclassification, with the performance of the latter being constrained by\ndetection performance. In contrast, we introduce a single-stage continuous\ngesture recognition model, that can detect and classify multiple gestures in a\nsingle video via a single model. This approach learns the natural transitions\nbetween gestures and non-gestures without the need for a pre-processing\nsegmentation stage to detect individual gestures. To enable this, we introduce\na multi-modal fusion mechanism to support the integration of important\ninformation that flows from multi-modal inputs, and is scalable to any number\nof modes. Additionally, we propose Unimodal Feature Mapping (UFM) and\nMulti-modal Feature Mapping (MFM) models to map uni-modal features and the\nfused multi-modal features respectively. To further enhance the performance we\npropose a mid-point based loss function that encourages smooth alignment\nbetween the ground truth and the prediction. We demonstrate the utility of our\nproposed framework which can handle variable-length input videos, and\noutperforms the state-of-the-art on two challenging datasets, EgoGesture, and\nIPN hand. Furthermore, ablative experiments show the importance of different\ncomponents of the proposed framework.\n", "versions": [{"version": "v1", "created": "Tue, 10 Nov 2020 07:09:35 GMT"}], "update_date": "2020-11-11", "authors_parsed": [["Gammulle", "Harshala", ""], ["Denman", "Simon", ""], ["Sridharan", "Sridha", ""], ["Fookes", "Clinton", ""]]}, {"id": "2011.04965", "submitter": "Yuhe Ding", "authors": "Yuhe Ding, Xin Ma, Mandi Luo, Aihua Zheng, Ran He", "title": "Unsupervised Contrastive Photo-to-Caricature Translation based on\n  Auto-distortion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Photo-to-caricature translation aims to synthesize the caricature as a\nrendered image exaggerating the features through sketching, pencil strokes, or\nother artistic drawings. Style rendering and geometry deformation are the most\nimportant aspects in photo-to-caricature translation task. To take both into\nconsideration, we propose an unsupervised contrastive photo-to-caricature\ntranslation architecture. Considering the intuitive artifacts in the existing\nmethods, we propose a contrastive style loss for style rendering to enforce the\nsimilarity between the style of rendered photo and the caricature, and\nsimultaneously enhance its discrepancy to the photos. To obtain an exaggerating\ndeformation in an unpaired/unsupervised fashion, we propose a Distortion\nPrediction Module (DPM) to predict a set of displacements vectors for each\ninput image while fixing some controlling points, followed by the thin plate\nspline interpolation for warping. The model is trained on unpaired photo and\ncaricature while can offer bidirectional synthesizing via inputting either a\nphoto or a caricature. Extensive experiments demonstrate that the proposed\nmodel is effective to generate hand-drawn like caricatures compared with\nexisting competitors.\n", "versions": [{"version": "v1", "created": "Tue, 10 Nov 2020 08:14:36 GMT"}], "update_date": "2020-11-11", "authors_parsed": [["Ding", "Yuhe", ""], ["Ma", "Xin", ""], ["Luo", "Mandi", ""], ["Zheng", "Aihua", ""], ["He", "Ran", ""]]}, {"id": "2011.04971", "submitter": "Suresh Kirthi Kumaraswamy", "authors": "Suresh Kirthi Kumaraswamy (1), Miaojing Shi (2) and Ewa Kijak (3) ((1)\n  Univ Le Mans, CNRS, IRISA, (2) Kings College London, (3) Univ Rennes, Inria,\n  CNRS, IRISA)", "title": "Detecting Human-Object Interaction with Mixed Supervision", "comments": "WACV 2021 - camera ready", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Human object interaction (HOI) detection is an important task in image\nunderstanding and reasoning. It is in a form of HOI triplet <human; verb;\nobject>, requiring bounding boxes for human and object, and action between them\nfor the task completion. In other words, this task requires strong supervision\nfor training that is however hard to procure. A natural solution to overcome\nthis is to pursue weakly-supervised learning, where we only know the presence\nof certain HOI triplets in images but their exact location is unknown. Most\nweakly-supervised learning methods do not make provision for leveraging data\nwith strong supervision, when they are available; and indeed a na\\\"ive\ncombination of this two paradigms in HOI detection fails to make contributions\nto each other. In this regard we propose a mixed-supervised HOI detection\npipeline: thanks to a specific design of momentum-independent learning that\nlearns seamlessly across these two types of supervision. Moreover, in light of\nthe annotation insufficiency in mixed supervision, we introduce an HOI element\nswapping technique to synthesize diverse and hard negatives across images and\nimprove the robustness of the model. Our method is evaluated on the challenging\nHICO-DET dataset. It performs close to or even better than many\nfully-supervised methods by using a mixed amount of strong and weak\nannotations; furthermore, it outperforms representative state of the art weakly\nand fully-supervised methods under the same supervision.\n", "versions": [{"version": "v1", "created": "Tue, 10 Nov 2020 08:42:31 GMT"}, {"version": "v2", "created": "Thu, 12 Nov 2020 14:14:21 GMT"}], "update_date": "2020-11-13", "authors_parsed": [["Kumaraswamy", "Suresh Kirthi", ""], ["Shi", "Miaojing", ""], ["Kijak", "Ewa", ""]]}, {"id": "2011.04976", "submitter": "Jianhui Chang", "authors": "Jianhui Chang, Zhenghui Zhao, Chuanmin Jia, Shiqi Wang, Lingbo Yang,\n  Jian Zhang and Siwei Ma", "title": "Conceptual Compression via Deep Structure and Texture Synthesis", "comments": "11 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Existing compression methods typically focus on the removal of signal-level\nredundancies, while the potential and versatility of decomposing visual data\ninto compact conceptual components still lack further study. To this end, we\npropose a novel conceptual compression framework that encodes visual data into\ncompact structure and texture representations, then decodes in a deep synthesis\nfashion, aiming to achieve better visual reconstruction quality, flexible\ncontent manipulation, and potential support for various vision tasks. In\nparticular, we propose to compress images by a dual-layered model consisting of\ntwo complementary visual features: 1) structure layer represented by structural\nmaps and 2) texture layer characterized by low-dimensional deep\nrepresentations. At the encoder side, the structural maps and texture\nrepresentations are individually extracted and compressed, generating the\ncompact, interpretable, inter-operable bitstreams. During the decoding stage, a\nhierarchical fusion GAN (HF-GAN) is proposed to learn the synthesis paradigm\nwhere the textures are rendered into the decoded structural maps, leading to\nhigh-quality reconstruction with remarkable visual realism. Extensive\nexperiments on diverse images have demonstrated the superiority of our\nframework with lower bitrates, higher reconstruction quality, and increased\nversatility towards visual analysis and content manipulation tasks.\n", "versions": [{"version": "v1", "created": "Tue, 10 Nov 2020 08:48:32 GMT"}], "update_date": "2020-11-11", "authors_parsed": [["Chang", "Jianhui", ""], ["Zhao", "Zhenghui", ""], ["Jia", "Chuanmin", ""], ["Wang", "Shiqi", ""], ["Yang", "Lingbo", ""], ["Zhang", "Jian", ""], ["Ma", "Siwei", ""]]}, {"id": "2011.04977", "submitter": "Jaehoon Choi", "authors": "Jaehoon Choi, Dongki Jung, Yonghan Lee, Deokhwa Kim, Dinesh Manocha,\n  and Donghwan Lee", "title": "SelfDeco: Self-Supervised Monocular Depth Completion in Challenging\n  Indoor Environments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present a novel algorithm for self-supervised monocular depth completion.\nOur approach is based on training a neural network that requires only sparse\ndepth measurements and corresponding monocular video sequences without dense\ndepth labels. Our self-supervised algorithm is designed for challenging indoor\nenvironments with textureless regions, glossy and transparent surface,\nnon-Lambertian surfaces, moving people, longer and diverse depth ranges and\nscenes captured by complex ego-motions. Our novel architecture leverages both\ndeep stacks of sparse convolution blocks to extract sparse depth features and\npixel-adaptive convolutions to fuse image and depth features. We compare with\nexisting approaches in NYUv2, KITTI, and NAVERLABS indoor datasets, and observe\n5-34 % improvements in root-means-square error (RMSE) reduction.\n", "versions": [{"version": "v1", "created": "Tue, 10 Nov 2020 08:55:07 GMT"}, {"version": "v2", "created": "Sun, 11 Apr 2021 04:54:17 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Choi", "Jaehoon", ""], ["Jung", "Dongki", ""], ["Lee", "Yonghan", ""], ["Kim", "Deokhwa", ""], ["Manocha", "Dinesh", ""], ["Lee", "Donghwan", ""]]}, {"id": "2011.04988", "submitter": "Radu Timofte", "authors": "Andrey Ignatov, Radu Timofte, Ming Qian, Congyu Qiao, Jiamin Lin,\n  Zhenyu Guo, Chenghua Li, Cong Leng, Jian Cheng, Juewen Peng, Xianrui Luo, Ke\n  Xian, Zijin Wu, Zhiguo Cao, Densen Puthussery, Jiji C V, Hrishikesh P S,\n  Melvin Kuriakose, Saikat Dutta, Sourya Dipta Das, Nisarg A. Shah, Kuldeep\n  Purohit, Praveen Kandula, Maitreya Suin, A. N. Rajagopalan, Saagara M B,\n  Minnu A L, Sanjana A R, Praseeda S, Ge Wu, Xueqin Chen, Tengyao Wang, Max\n  Zheng, Hulk Wong, Jay Zou", "title": "AIM 2020 Challenge on Rendering Realistic Bokeh", "comments": "Published in ECCV 2020 Workshop (Advances in Image Manipulation),\n  https://data.vision.ee.ethz.ch/cvl/aim20/", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper reviews the second AIM realistic bokeh effect rendering challenge\nand provides the description of the proposed solutions and results. The\nparticipating teams were solving a real-world bokeh simulation problem, where\nthe goal was to learn a realistic shallow focus technique using a large-scale\nEBB! bokeh dataset consisting of 5K shallow / wide depth-of-field image pairs\ncaptured using the Canon 7D DSLR camera. The participants had to render bokeh\neffect based on only one single frame without any additional data from other\ncameras or sensors. The target metric used in this challenge combined the\nruntime and the perceptual quality of the solutions measured in the user study.\nTo ensure the efficiency of the submitted models, we measured their runtime on\nstandard desktop CPUs as well as were running the models on smartphone GPUs.\nThe proposed solutions significantly improved the baseline results, defining\nthe state-of-the-art for practical bokeh effect rendering problem.\n", "versions": [{"version": "v1", "created": "Tue, 10 Nov 2020 09:15:38 GMT"}], "update_date": "2020-11-11", "authors_parsed": [["Ignatov", "Andrey", ""], ["Timofte", "Radu", ""], ["Qian", "Ming", ""], ["Qiao", "Congyu", ""], ["Lin", "Jiamin", ""], ["Guo", "Zhenyu", ""], ["Li", "Chenghua", ""], ["Leng", "Cong", ""], ["Cheng", "Jian", ""], ["Peng", "Juewen", ""], ["Luo", "Xianrui", ""], ["Xian", "Ke", ""], ["Wu", "Zijin", ""], ["Cao", "Zhiguo", ""], ["Puthussery", "Densen", ""], ["C", "Jiji", "V"], ["S", "Hrishikesh P", ""], ["Kuriakose", "Melvin", ""], ["Dutta", "Saikat", ""], ["Das", "Sourya Dipta", ""], ["Shah", "Nisarg A.", ""], ["Purohit", "Kuldeep", ""], ["Kandula", "Praveen", ""], ["Suin", "Maitreya", ""], ["Rajagopalan", "A. N.", ""], ["B", "Saagara M", ""], ["L", "Minnu A", ""], ["R", "Sanjana A", ""], ["S", "Praseeda", ""], ["Wu", "Ge", ""], ["Chen", "Xueqin", ""], ["Wang", "Tengyao", ""], ["Zheng", "Max", ""], ["Wong", "Hulk", ""], ["Zou", "Jay", ""]]}, {"id": "2011.04994", "submitter": "Radu Timofte", "authors": "Andrey Ignatov, Radu Timofte, Zhilu Zhang, Ming Liu, Haolin Wang,\n  Wangmeng Zuo, Jiawei Zhang, Ruimao Zhang, Zhanglin Peng, Sijie Ren, Linhui\n  Dai, Xiaohong Liu, Chengqi Li, Jun Chen, Yuichi Ito, Bhavya Vasudeva, Puneesh\n  Deora, Umapada Pal, Zhenyu Guo, Yu Zhu, Tian Liang, Chenghua Li, Cong Leng,\n  Zhihong Pan, Baopu Li, Byung-Hoon Kim, Joonyoung Song, Jong Chul Ye, JaeHyun\n  Baek, Magauiya Zhussip, Yeskendir Koishekenov, Hwechul Cho Ye, Xin Liu,\n  Xueying Hu, Jun Jiang, Jinwei Gu, Kai Li, Pengliang Tan, Bingxin Hou", "title": "AIM 2020 Challenge on Learned Image Signal Processing Pipeline", "comments": "Published in ECCV 2020 Workshops (Advances in Image Manipulation),\n  https://data.vision.ee.ethz.ch/cvl/aim20/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper reviews the second AIM learned ISP challenge and provides the\ndescription of the proposed solutions and results. The participating teams were\nsolving a real-world RAW-to-RGB mapping problem, where to goal was to map the\noriginal low-quality RAW images captured by the Huawei P20 device to the same\nphotos obtained with the Canon 5D DSLR camera. The considered task embraced a\nnumber of complex computer vision subtasks, such as image demosaicing,\ndenoising, white balancing, color and contrast correction, demoireing, etc. The\ntarget metric used in this challenge combined fidelity scores (PSNR and SSIM)\nwith solutions' perceptual results measured in a user study. The proposed\nsolutions significantly improved the baseline results, defining the\nstate-of-the-art for practical image signal processing pipeline modeling.\n", "versions": [{"version": "v1", "created": "Tue, 10 Nov 2020 09:25:47 GMT"}], "update_date": "2020-11-11", "authors_parsed": [["Ignatov", "Andrey", ""], ["Timofte", "Radu", ""], ["Zhang", "Zhilu", ""], ["Liu", "Ming", ""], ["Wang", "Haolin", ""], ["Zuo", "Wangmeng", ""], ["Zhang", "Jiawei", ""], ["Zhang", "Ruimao", ""], ["Peng", "Zhanglin", ""], ["Ren", "Sijie", ""], ["Dai", "Linhui", ""], ["Liu", "Xiaohong", ""], ["Li", "Chengqi", ""], ["Chen", "Jun", ""], ["Ito", "Yuichi", ""], ["Vasudeva", "Bhavya", ""], ["Deora", "Puneesh", ""], ["Pal", "Umapada", ""], ["Guo", "Zhenyu", ""], ["Zhu", "Yu", ""], ["Liang", "Tian", ""], ["Li", "Chenghua", ""], ["Leng", "Cong", ""], ["Pan", "Zhihong", ""], ["Li", "Baopu", ""], ["Kim", "Byung-Hoon", ""], ["Song", "Joonyoung", ""], ["Ye", "Jong Chul", ""], ["Baek", "JaeHyun", ""], ["Zhussip", "Magauiya", ""], ["Koishekenov", "Yeskendir", ""], ["Ye", "Hwechul Cho", ""], ["Liu", "Xin", ""], ["Hu", "Xueying", ""], ["Jiang", "Jun", ""], ["Gu", "Jinwei", ""], ["Li", "Kai", ""], ["Tan", "Pengliang", ""], ["Hou", "Bingxin", ""]]}, {"id": "2011.04997", "submitter": "Niklas Holzwarth", "authors": "Niklas Holzwarth, Melanie Schellenberg, Janek Gr\\\"ohl, Kris Dreher,\n  Jan-Hinrich N\\\"olke, Alexander Seitel, Minu D. Tizabi, Beat P.\n  M\\\"uller-Stich, Lena Maier-Hein", "title": "Tattoo tomography: Freehand 3D photoacoustic image reconstruction with\n  an optical pattern", "comments": "12 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.med-ph cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Purpose: Photoacoustic tomography (PAT) is a novel imaging technique that can\nspatially resolve both morphological and functional tissue properties, such as\nthe vessel topology and tissue oxygenation. While this capacity makes PAT a\npromising modality for the diagnosis, treatment and follow-up of various\ndiseases, a current drawback is the limited field-of-view (FoV) provided by the\nconventionally applied 2D probes.\n  Methods: In this paper, we present a novel approach to 3D reconstruction of\nPAT data (Tattoo tomography) that does not require an external tracking system\nand can smoothly be integrated into clinical workflows. It is based on an\noptical pattern placed on the region of interest prior to image acquisition.\nThis pattern is designed in a way that a tomographic image of it enables the\nrecovery of the probe pose relative to the coordinate system of the pattern.\nThis allows the transformation of a sequence of acquired PA images into one\ncommon global coordinate system and thus the consistent 3D reconstruction of\nPAT imaging data.\n  Results: An initial feasibility study conducted with experimental phantom\ndata and in vivo forearm data indicates that the Tattoo approach is well-suited\nfor 3D reconstruction of PAT data with high accuracy and precision.\n  Conclusion: In contrast to previous approaches to 3D ultrasound (US) or PAT\nreconstruction, the Tattoo approach neither requires complex external hardware\nnor training data acquired for a specific application. It could thus become a\nvaluable tool for clinical freehand PAT.\n", "versions": [{"version": "v1", "created": "Tue, 10 Nov 2020 09:27:56 GMT"}, {"version": "v2", "created": "Wed, 11 Nov 2020 08:38:25 GMT"}], "update_date": "2020-11-12", "authors_parsed": [["Holzwarth", "Niklas", ""], ["Schellenberg", "Melanie", ""], ["Gr\u00f6hl", "Janek", ""], ["Dreher", "Kris", ""], ["N\u00f6lke", "Jan-Hinrich", ""], ["Seitel", "Alexander", ""], ["Tizabi", "Minu D.", ""], ["M\u00fcller-Stich", "Beat P.", ""], ["Maier-Hein", "Lena", ""]]}, {"id": "2011.05002", "submitter": "Neo Christopher Chung <", "authors": "Lennart Brocki, Neo Christopher Chung", "title": "Input Bias in Rectified Gradients and Modified Saliency Maps", "comments": "2021 IEEE International Conference on Big Data and Smart Computing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Interpretation and improvement of deep neural networks relies on better\nunderstanding of their underlying mechanisms. In particular, gradients of\nclasses or concepts with respect to the input features (e.g., pixels in images)\nare often used as importance scores or estimators, which are visualized in\nsaliency maps. Thus, a family of saliency methods provide an intuitive way to\nidentify input features with substantial influences on classifications or\nlatent concepts. Several modifications to conventional saliency maps, such as\nRectified Gradients and Layer-wise Relevance Propagation (LRP), have been\nintroduced to allegedly denoise and improve interpretability. While visually\ncoherent in certain cases, Rectified Gradients and other modified saliency maps\nintroduce a strong input bias (e.g., brightness in the RGB space) because of\ninappropriate uses of the input features. We demonstrate that dark areas of an\ninput image are not highlighted by a saliency map using Rectified Gradients,\neven if it is relevant for the class or concept. Even in the scaled images, the\ninput bias exists around an artificial point in color spectrum. Our\nmodification, which simply eliminates multiplication with input features,\nremoves this bias. This showcases how a visual criteria may not align with true\nexplainability of deep learning models.\n", "versions": [{"version": "v1", "created": "Tue, 10 Nov 2020 09:45:13 GMT"}, {"version": "v2", "created": "Sat, 14 Nov 2020 10:48:06 GMT"}, {"version": "v3", "created": "Tue, 1 Dec 2020 10:34:25 GMT"}], "update_date": "2020-12-02", "authors_parsed": [["Brocki", "Lennart", ""], ["Chung", "Neo Christopher", ""]]}, {"id": "2011.05003", "submitter": "Mathis Hoffmann", "authors": "Mathis Hoffmann, Thomas K\\\"ohler, Bernd Doll, Frank Schebesch, Florian\n  Talkenberg, Ian Marius Peters, Christoph J. Brabec, Andreas Maier, Vincent\n  Christlein", "title": "Joint Super-Resolution and Rectification for Solar Cell Inspection", "comments": null, "journal-ref": null, "doi": "10.1109/JPHOTOV.2021.3072229", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual inspection of solar modules is an important monitoring facility in\nphotovoltaic power plants. Since a single measurement of fast CMOS sensors is\nlimited in spatial resolution and often not sufficient to reliably detect small\ndefects, we apply multi-frame super-resolution (MFSR) to a sequence of low\nresolution measurements. In addition, the rectification and removal of lens\ndistortion simplifies subsequent analysis. Therefore, we propose to fuse this\npre-processing with standard MFSR algorithms. This is advantageous, because we\nomit a separate processing step, the motion estimation becomes more stable and\nthe spacing of high-resolution (HR) pixels on the rectified module image\nbecomes uniform w. r. t. the module plane, regardless of perspective\ndistortion. We present a comprehensive user study showing that MFSR is\nbeneficial for defect recognition by human experts and that the proposed method\nperforms better than the state of the art. Furthermore, we apply automated\ncrack segmentation and show that the proposed method performs 3x better than\nbicubic upsampling and 2x better than the state of the art for automated\ninspection.\n", "versions": [{"version": "v1", "created": "Tue, 10 Nov 2020 09:47:21 GMT"}, {"version": "v2", "created": "Wed, 7 Apr 2021 13:26:38 GMT"}], "update_date": "2021-05-06", "authors_parsed": [["Hoffmann", "Mathis", ""], ["K\u00f6hler", "Thomas", ""], ["Doll", "Bernd", ""], ["Schebesch", "Frank", ""], ["Talkenberg", "Florian", ""], ["Peters", "Ian Marius", ""], ["Brabec", "Christoph J.", ""], ["Maier", "Andreas", ""], ["Christlein", "Vincent", ""]]}, {"id": "2011.05005", "submitter": "Yikai Wang", "authors": "Yikai Wang, Wenbing Huang, Fuchun Sun, Tingyang Xu, Yu Rong, Junzhou\n  Huang", "title": "Deep Multimodal Fusion by Channel Exchanging", "comments": "NeurIPS 2020. Code and models: https://github.com/yikaiw/CEN", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep multimodal fusion by using multiple sources of data for classification\nor regression has exhibited a clear advantage over the unimodal counterpart on\nvarious applications. Yet, current methods including aggregation-based and\nalignment-based fusion are still inadequate in balancing the trade-off between\ninter-modal fusion and intra-modal processing, incurring a bottleneck of\nperformance improvement. To this end, this paper proposes\nChannel-Exchanging-Network (CEN), a parameter-free multimodal fusion framework\nthat dynamically exchanges channels between sub-networks of different\nmodalities. Specifically, the channel exchanging process is self-guided by\nindividual channel importance that is measured by the magnitude of\nBatch-Normalization (BN) scaling factor during training. The validity of such\nexchanging process is also guaranteed by sharing convolutional filters yet\nkeeping separate BN layers across modalities, which, as an add-on benefit,\nallows our multimodal architecture to be almost as compact as a unimodal\nnetwork. Extensive experiments on semantic segmentation via RGB-D data and\nimage translation through multi-domain input verify the effectiveness of our\nCEN compared to current state-of-the-art methods. Detailed ablation studies\nhave also been carried out, which provably affirm the advantage of each\ncomponent we propose. Our code is available at https://github.com/yikaiw/CEN.\n", "versions": [{"version": "v1", "created": "Tue, 10 Nov 2020 09:53:20 GMT"}, {"version": "v2", "created": "Sat, 5 Dec 2020 05:42:46 GMT"}], "update_date": "2020-12-08", "authors_parsed": [["Wang", "Yikai", ""], ["Huang", "Wenbing", ""], ["Sun", "Fuchun", ""], ["Xu", "Tingyang", ""], ["Rong", "Yu", ""], ["Huang", "Junzhou", ""]]}, {"id": "2011.05010", "submitter": "Angel Martinez-Gonzalez", "authors": "Angel Mart\\'inez-Gonz\\'alez, Michael Villamizar, Olivier Can\\'evet and\n  Jean-Marc Odobez", "title": "Residual Pose: A Decoupled Approach for Depth-based 3D Human Pose\n  Estimation", "comments": "Published in IROS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose to leverage recent advances in reliable 2D pose estimation with\nConvolutional Neural Networks (CNN) to estimate the 3D pose of people from\ndepth images in multi-person Human-Robot Interaction (HRI) scenarios. Our\nmethod is based on the observation that using the depth information to obtain\n3D lifted points from 2D body landmark detections provides a rough estimate of\nthe true 3D human pose, thus requiring only a refinement step. In that line our\ncontributions are threefold. (i) we propose to perform 3D pose estimation from\ndepth images by decoupling 2D pose estimation and 3D pose refinement; (ii) we\npropose a deep-learning approach that regresses the residual pose between the\nlifted 3D pose and the true 3D pose; (iii) we show that despite its simplicity,\nour approach achieves very competitive results both in accuracy and speed on\ntwo public datasets and is therefore appealing for multi-person HRI compared to\nrecent state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Tue, 10 Nov 2020 10:08:13 GMT"}], "update_date": "2020-11-11", "authors_parsed": [["Mart\u00ednez-Gonz\u00e1lez", "Angel", ""], ["Villamizar", "Michael", ""], ["Can\u00e9vet", "Olivier", ""], ["Odobez", "Jean-Marc", ""]]}, {"id": "2011.05014", "submitter": "Masaki Yoshii", "authors": "Masaki Yoshii, Ikuko Shimizu", "title": "Point Cloud Registration Based on Consistency Evaluation of Rigid\n  Transformation in Parameter Space", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We can use a method called registration to integrate some point clouds that\nrepresent the shape of the real world. In this paper, we propose highly\naccurate and stable registration method. Our method detects keypoints from\npoint clouds and generates triplets using multiple descriptors. Furthermore,\nour method evaluates the consistency of rigid transformation parameters of each\ntriplet with histograms and obtains the rigid transformation between the point\nclouds. In the experiment of this paper, our method had minimul errors and no\nmajor failures. As a result, we obtained sufficiently accurate and stable\nregistration results compared to the comparative methods.\n", "versions": [{"version": "v1", "created": "Tue, 10 Nov 2020 10:13:15 GMT"}], "update_date": "2020-11-11", "authors_parsed": [["Yoshii", "Masaki", ""], ["Shimizu", "Ikuko", ""]]}, {"id": "2011.05025", "submitter": "Baudouin Denis de Senneville PhD", "authors": "Baudouin Denis de Senneville, Pierrick Coup\\'e, Mario Ries, Laurent\n  Facq, Chrit Moonen", "title": "Deep correction of breathing-related artifacts in real-time\n  MR-thermometry", "comments": "21 pages, 9 figures, 1 table", "journal-ref": "Computerized Medical Imaging and Graphics, 2020", "doi": null, "report-no": null, "categories": "physics.med-ph cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Real-time MR-imaging has been clinically adapted for monitoring thermal\ntherapies since it can provide on-the-fly temperature maps simultaneously with\nanatomical information. However, proton resonance frequency based thermometry\nof moving targets remains challenging since temperature artifacts are induced\nby the respiratory as well as physiological motion. If left uncorrected, these\nartifacts lead to severe errors in temperature estimates and impair therapy\nguidance. In this study, we evaluated deep learning for on-line correction of\nmotion related errors in abdominal MR-thermometry. For this, a convolutional\nneural network (CNN) was designed to learn the apparent temperature\nperturbation from images acquired during a preparative learning stage prior to\nhyperthermia. The input of the designed CNN is the most recent magnitude image\nand no surrogate of motion is needed. During the subsequent hyperthermia\nprocedure, the recent magnitude image is used as an input for the CNN-model in\norder to generate an on-line correction for the current temperature map. The\nmethod's artifact suppression performance was evaluated on 12 free breathing\nvolunteers and was found robust and artifact-free in all examined cases.\nFurthermore, thermometric precision and accuracy was assessed for in vivo\nablation using high intensity focused ultrasound. All calculations involved at\nthe different stages of the proposed workflow were designed to be compatible\nwith the clinical time constraints of a therapeutic procedure.\n", "versions": [{"version": "v1", "created": "Tue, 10 Nov 2020 10:30:41 GMT"}, {"version": "v2", "created": "Thu, 19 Nov 2020 12:53:10 GMT"}, {"version": "v3", "created": "Tue, 22 Dec 2020 09:20:21 GMT"}], "update_date": "2020-12-23", "authors_parsed": [["de Senneville", "Baudouin Denis", ""], ["Coup\u00e9", "Pierrick", ""], ["Ries", "Mario", ""], ["Facq", "Laurent", ""], ["Moonen", "Chrit", ""]]}, {"id": "2011.05049", "submitter": "Zongheng Tang", "authors": "Zongheng Tang, Yue Liao, Si Liu, Guanbin Li, Xiaojie Jin, Hongxu\n  Jiang, Qian Yu, Dong Xu", "title": "Human-centric Spatio-Temporal Video Grounding With Visual Transformers", "comments": "Accept at TCSVT", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we introduce a novel task - Humancentric Spatio-Temporal Video\nGrounding (HC-STVG). Unlike the existing referring expression tasks in images\nor videos, by focusing on humans, HC-STVG aims to localize a spatiotemporal\ntube of the target person from an untrimmed video based on a given textural\ndescription. This task is useful, especially for healthcare and\nsecurity-related applications, where the surveillance videos can be extremely\nlong but only a specific person during a specific period of time is concerned.\nHC-STVG is a video grounding task that requires both spatial (where) and\ntemporal (when) localization. Unfortunately, the existing grounding methods\ncannot handle this task well. We tackle this task by proposing an effective\nbaseline method named Spatio-Temporal Grounding with Visual Transformers\n(STGVT), which utilizes Visual Transformers to extract cross-modal\nrepresentations for video-sentence matching and temporal localization. To\nfacilitate this task, we also contribute an HC-STVG dataset consisting of 5,660\nvideo-sentence pairs on complex multi-person scenes. Specifically, each video\nlasts for 20 seconds, pairing with a natural query sentence with an average of\n17.25 words. Extensive experiments are conducted on this dataset, demonstrating\nthe newly-proposed method outperforms the existing baseline methods.\n", "versions": [{"version": "v1", "created": "Tue, 10 Nov 2020 11:23:38 GMT"}, {"version": "v2", "created": "Wed, 2 Jun 2021 06:51:34 GMT"}], "update_date": "2021-06-03", "authors_parsed": [["Tang", "Zongheng", ""], ["Liao", "Yue", ""], ["Liu", "Si", ""], ["Li", "Guanbin", ""], ["Jin", "Xiaojie", ""], ["Jiang", "Hongxu", ""], ["Yu", "Qian", ""], ["Xu", "Dong", ""]]}, {"id": "2011.05054", "submitter": "Bo Li", "authors": "Bo Li, Sam Leroux, Pieter Simoens", "title": "Decoupled Appearance and Motion Learning for Efficient Anomaly Detection\n  in Surveillance Video", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automating the analysis of surveillance video footage is of great interest\nwhen urban environments or industrial sites are monitored by a large number of\ncameras. As anomalies are often context-specific, it is hard to predefine\nevents of interest and collect labelled training data. A purely unsupervised\napproach for automated anomaly detection is much more suitable. For every\ncamera, a separate algorithm could then be deployed that learns over time a\nbaseline model of appearance and motion related features of the objects within\nthe camera viewport. Anything that deviates from this baseline is flagged as an\nanomaly for further analysis downstream. We propose a new neural network\narchitecture that learns the normal behavior in a purely unsupervised fashion.\nIn contrast to previous work, we use latent code predictions as our anomaly\nmetric. We show that this outperforms reconstruction-based and frame\nprediction-based methods on different benchmark datasets both in terms of\naccuracy and robustness against changing lighting and weather conditions. By\ndecoupling an appearance and a motion model, our model can also process 16 to\n45 times more frames per second than related approaches which makes our model\nsuitable for deploying on the camera itself or on other edge devices.\n", "versions": [{"version": "v1", "created": "Tue, 10 Nov 2020 11:40:06 GMT"}, {"version": "v2", "created": "Thu, 12 Nov 2020 08:56:57 GMT"}], "update_date": "2020-11-13", "authors_parsed": [["Li", "Bo", ""], ["Leroux", "Sam", ""], ["Simoens", "Pieter", ""]]}, {"id": "2011.05088", "submitter": "Lei Ding", "authors": "Lei Ding, Kai Zheng, Dong Lin, Yuxing Chen, Bing Liu, Jiansheng Li and\n  Lorenzo Bruzzone", "title": "MP-ResNet: Multi-path Residual Network for the Semantic segmentation of\n  High-Resolution PolSAR Images", "comments": null, "journal-ref": null, "doi": "10.1109/LGRS.2021.3079925", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There are limited studies on the semantic segmentation of high-resolution\nPolarimetric Synthetic Aperture Radar (PolSAR) images due to the scarcity of\ntraining data and the inference of speckle noises. The Gaofen contest has\nprovided open access of a high-quality PolSAR semantic segmentation dataset.\nTaking this chance, we propose a Multi-path ResNet (MP-ResNet) architecture for\nthe semantic segmentation of high-resolution PolSAR images. Compared to\nconventional U-shape encoder-decoder convolutional neural network (CNN)\narchitectures, the MP-ResNet learns semantic context with its parallel\nmulti-scale branches, which greatly enlarges its valid receptive fields and\nimproves the embedding of local discriminative features. In addition, MP-ResNet\nadopts a multi-level feature fusion design in its decoder to make the best use\nof the features learned from its different branches. Ablation studies show that\nthe MPResNet has significant advantages over its baseline method (FCN with\nResNet34). It also surpasses several classic state-of-the-art methods in terms\nof overall accuracy (OA), mean F1 and fwIoU, whereas its computational costs\nare not much increased. This CNN architecture can be used as a baseline method\nfor future studies on the semantic segmentation of PolSAR images. The code is\navailable at: https://github.com/ggsDing/SARSeg.\n", "versions": [{"version": "v1", "created": "Tue, 10 Nov 2020 13:28:36 GMT"}, {"version": "v2", "created": "Mon, 16 Nov 2020 14:02:58 GMT"}], "update_date": "2021-06-30", "authors_parsed": [["Ding", "Lei", ""], ["Zheng", "Kai", ""], ["Lin", "Dong", ""], ["Chen", "Yuxing", ""], ["Liu", "Bing", ""], ["Li", "Jiansheng", ""], ["Bruzzone", "Lorenzo", ""]]}, {"id": "2011.05105", "submitter": "Mikhail Papkov", "authors": "Mikhail Papkov, Kenny Roberts, Lee Ann Madissoon, Omer Bayraktar,\n  Dmytro Fishman, Kaupo Palo, Leopold Parts", "title": "Noise2Stack: Improving Image Restoration by Learning from Volumetric\n  Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Biomedical images are noisy. The imaging equipment itself has physical\nlimitations, and the consequent experimental trade-offs between signal-to-noise\nratio, acquisition speed, and imaging depth exacerbate the problem. Denoising\nis, therefore, an essential part of any image processing pipeline, and\nconvolutional neural networks are currently the method of choice for this task.\nOne popular approach, Noise2Noise, does not require clean ground truth, and\ninstead, uses a second noisy copy as a training target. Self-supervised\nmethods, like Noise2Self and Noise2Void, relax data requirements by learning\nthe signal without an explicit target but are limited by the lack of\ninformation in a single image. Here, we introduce Noise2Stack, an extension of\nthe Noise2Noise method to image stacks that takes advantage of a shared signal\nbetween spatially neighboring planes. Our experiments on magnetic resonance\nbrain scans and newly acquired multiplane microscopy data show that learning\nonly from image neighbors in a stack is sufficient to outperform Noise2Noise\nand Noise2Void and close the gap to supervised denoising methods. Our findings\npoint towards low-cost, high-reward improvement in the denoising pipeline of\nmultiplane biomedical images. As a part of this work, we release a microscopy\ndataset to establish a benchmark for the multiplane image denoising.\n", "versions": [{"version": "v1", "created": "Tue, 10 Nov 2020 14:01:47 GMT"}], "update_date": "2020-11-11", "authors_parsed": [["Papkov", "Mikhail", ""], ["Roberts", "Kenny", ""], ["Madissoon", "Lee Ann", ""], ["Bayraktar", "Omer", ""], ["Fishman", "Dmytro", ""], ["Palo", "Kaupo", ""], ["Parts", "Leopold", ""]]}, {"id": "2011.05108", "submitter": "Shubham Vatsal", "authors": "Shubham Vatsal, Nikhil Arora, Gopi Ramena, Sukumar Moharana, Dhruval\n  Jain, Naresh Purre, Rachit S Munjal", "title": "On-Device Language Identification of Text in Images using Diacritic\n  Characters", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Diacritic characters can be considered as a unique set of characters\nproviding us with adequate and significant clue in identifying a given language\nwith considerably high accuracy. Diacritics, though associated with phonetics\noften serve as a distinguishing feature for many languages especially the ones\nwith a Latin script. In this proposed work, we aim to identify language of text\nin images using the presence of diacritic characters in order to improve\nOptical Character Recognition (OCR) performance in any given automated\nenvironment. We showcase our work across 13 Latin languages encompassing 85\ndiacritic characters. We use an architecture similar to Squeezedet for object\ndetection of diacritic characters followed by a shallow network to finally\nidentify the language. OCR systems when accompanied with identified language\nparameter tends to produce better results than sole deployment of OCR systems.\nThe discussed work apart from guaranteeing an improvement in OCR results also\ntakes on-device (mobile phone) constraints into consideration in terms of model\nsize and inference time.\n", "versions": [{"version": "v1", "created": "Tue, 10 Nov 2020 14:10:06 GMT"}], "update_date": "2020-11-11", "authors_parsed": [["Vatsal", "Shubham", ""], ["Arora", "Nikhil", ""], ["Ramena", "Gopi", ""], ["Moharana", "Sukumar", ""], ["Jain", "Dhruval", ""], ["Purre", "Naresh", ""], ["Munjal", "Rachit S", ""]]}, {"id": "2011.05127", "submitter": "Juan F. Hern\\'andez Albarrac\\'in", "authors": "Juan F. H. Albarrac\\'in, Rafael S. Oliveira, Marina Hirota, Jefersson\n  A. dos Santos, Ricardo da S. Torres", "title": "A Soft Computing Approach for Selecting and Combining Spectral Bands", "comments": "MDPI Remote Sensing - Special Issue \"Current Limits and New\n  Challenges and Opportunities in Soft Computing, Machine Learning and\n  Computational Intelligence for Remote Sensing\"", "journal-ref": "Remote Sens. 2020, 12(14), 2267", "doi": "10.3390/rs12142267", "report-no": null, "categories": "cs.NE cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We introduce a soft computing approach for automatically selecting and\ncombining indices from remote sensing multispectral images that can be used for\nclassification tasks. The proposed approach is based on a Genetic-Programming\n(GP) framework, a technique successfully used in a wide variety of optimization\nproblems. Through GP, it is possible to learn indices that maximize the\nseparability of samples from two different classes. Once the indices\nspecialized for all the pairs of classes are obtained, they are used in\npixelwise classification tasks. We used the GP-based solution to evaluate\ncomplex classification problems, such as those that are related to the\ndiscrimination of vegetation types within and between tropical biomes. Using\ntime series defined in terms of the learned spectral indices, we show that the\nGP framework leads to superior results than other indices that are used to\ndiscriminate and classify tropical biomes.\n", "versions": [{"version": "v1", "created": "Tue, 10 Nov 2020 14:51:05 GMT"}], "update_date": "2020-11-11", "authors_parsed": [["Albarrac\u00edn", "Juan F. H.", ""], ["Oliveira", "Rafael S.", ""], ["Hirota", "Marina", ""], ["Santos", "Jefersson A. dos", ""], ["Torres", "Ricardo da S.", ""]]}, {"id": "2011.05132", "submitter": "Rajesh Menon", "authors": "Soren Nelson and Rajesh Menon", "title": "Classification of optics-free images with deep neural networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV physics.optics", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The thinnest possible camera is achieved by removing all optics, leaving only\nthe image sensor. We train deep neural networks to perform multi-class\ndetection and binary classification (with accuracy of 92%) on optics-free\nimages without the need for anthropocentric image reconstructions. Inferencing\nfrom optics-free images has the potential for enhanced privacy and power\nefficiency.\n", "versions": [{"version": "v1", "created": "Tue, 10 Nov 2020 15:02:19 GMT"}], "update_date": "2020-11-11", "authors_parsed": [["Nelson", "Soren", ""], ["Menon", "Rajesh", ""]]}, {"id": "2011.05139", "submitter": "Domonkos Varga", "authors": "Domonkos Varga", "title": "Multi-pooled Inception features for no-reference image quality\n  assessment", "comments": null, "journal-ref": "Algorithms 313 (2020)", "doi": "10.3390/app10062186", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Image quality assessment (IQA) is an important element of a broad spectrum of\napplications ranging from automatic video streaming to display technology.\nFurthermore, the measurement of image quality requires a balanced investigation\nof image content and features. Our proposed approach extracts visual features\nby attaching global average pooling (GAP) layers to multiple Inception modules\nof on an ImageNet database pretrained convolutional neural network (CNN). In\ncontrast to previous methods, we do not take patches from the input image.\nInstead, the input image is treated as a whole and is run through a pretrained\nCNN body to extract resolution-independent, multi-level deep features. As a\nconsequence, our method can be easily generalized to any input image size and\npretrained CNNs. Thus, we present a detailed parameter study with respect to\nthe CNN base architectures and the effectiveness of different deep features. We\ndemonstrate that our best proposal - called MultiGAP-NRIQA - is able to provide\nstate-of-the-art results on three benchmark IQA databases. Furthermore, these\nresults were also confirmed in a cross database test using the LIVE In the Wild\nImage Quality Challenge database.\n", "versions": [{"version": "v1", "created": "Tue, 10 Nov 2020 15:09:49 GMT"}], "update_date": "2020-12-02", "authors_parsed": [["Varga", "Domonkos", ""]]}, {"id": "2011.05142", "submitter": "Qingyu Chen", "authors": "Qingyu Chen, Tiarnan D. L. Keenan, Alexis Allot, Yifan Peng, Elvira\n  Agr\\'on, Amitha Domalpally, Caroline C. W. Klaver, Daniel T. Luttikhuizen,\n  Marcus H. Colyer, Catherine A. Cukras, Henry E. Wiley, M. Teresa Magone,\n  Chantal Cousineau-Krieger, Wai T. Wong, Yingying Zhu, Emily Y. Chew, Zhiyong\n  Lu (for the AREDS2 Deep Learning Research Group)", "title": "Multi-modal, multi-task, multi-attention (M3) deep learning detection of\n  reticular pseudodrusen: towards automated and accessible classification of\n  age-related macular degeneration", "comments": "5 figures and 4 tables, To appear in Journal of the American Medical\n  Informatics Association", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Objective Reticular pseudodrusen (RPD), a key feature of age-related macular\ndegeneration (AMD), are poorly detected by human experts on standard color\nfundus photography (CFP) and typically require advanced imaging modalities such\nas fundus autofluorescence (FAF). The objective was to develop and evaluate the\nperformance of a novel 'M3' deep learning framework on RPD detection. Materials\nand Methods A deep learning framework M3 was developed to detect RPD presence\naccurately using CFP alone, FAF alone, or both, employing >8000 CFP-FAF image\npairs obtained prospectively (Age-Related Eye Disease Study 2). The M3\nframework includes multi-modal (detection from single or multiple image\nmodalities), multi-task (training different tasks simultaneously to improve\ngeneralizability), and multi-attention (improving ensembled feature\nrepresentation) operation. Performance on RPD detection was compared with\nstate-of-the-art deep learning models and 13 ophthalmologists; performance on\ndetection of two other AMD features (geographic atrophy and pigmentary\nabnormalities) was also evaluated. Results For RPD detection, M3 achieved area\nunder receiver operating characteristic (AUROC) 0.832, 0.931, and 0.933 for CFP\nalone, FAF alone, and both, respectively. M3 performance on CFP was very\nsubstantially superior to human retinal specialists (median F1-score 0.644\nversus 0.350). External validation (on Rotterdam Study, Netherlands)\ndemonstrated high accuracy on CFP alone (AUROC 0.965). The M3 framework also\naccurately detected geographic atrophy and pigmentary abnormalities (AUROC\n0.909 and 0.912, respectively), demonstrating its generalizability. Conclusion\nThis study demonstrates the successful development, robust evaluation, and\nexternal validation of a novel deep learning framework that enables accessible,\naccurate, and automated AMD diagnosis and prognosis.\n", "versions": [{"version": "v1", "created": "Mon, 9 Nov 2020 03:26:38 GMT"}, {"version": "v2", "created": "Wed, 11 Nov 2020 13:26:39 GMT"}], "update_date": "2020-11-12", "authors_parsed": [["Chen", "Qingyu", "", "for the AREDS2 Deep Learning Research Group"], ["Keenan", "Tiarnan D. L.", "", "for the AREDS2 Deep Learning Research Group"], ["Allot", "Alexis", "", "for the AREDS2 Deep Learning Research Group"], ["Peng", "Yifan", "", "for the AREDS2 Deep Learning Research Group"], ["Agr\u00f3n", "Elvira", "", "for the AREDS2 Deep Learning Research Group"], ["Domalpally", "Amitha", "", "for the AREDS2 Deep Learning Research Group"], ["Klaver", "Caroline C. W.", "", "for the AREDS2 Deep Learning Research Group"], ["Luttikhuizen", "Daniel T.", "", "for the AREDS2 Deep Learning Research Group"], ["Colyer", "Marcus H.", "", "for the AREDS2 Deep Learning Research Group"], ["Cukras", "Catherine A.", "", "for the AREDS2 Deep Learning Research Group"], ["Wiley", "Henry E.", "", "for the AREDS2 Deep Learning Research Group"], ["Magone", "M. Teresa", "", "for the AREDS2 Deep Learning Research Group"], ["Cousineau-Krieger", "Chantal", "", "for the AREDS2 Deep Learning Research Group"], ["Wong", "Wai T.", "", "for the AREDS2 Deep Learning Research Group"], ["Zhu", "Yingying", "", "for the AREDS2 Deep Learning Research Group"], ["Chew", "Emily Y.", "", "for the AREDS2 Deep Learning Research Group"], ["Lu", "Zhiyong", "", "for the AREDS2 Deep Learning Research Group"]]}, {"id": "2011.05151", "submitter": "M. F. Mridha", "authors": "Muhammad Mohsin Kabir, Abu Quwsar Ohi, M. F. Mridha", "title": "A Multi-Plant Disease Diagnosis Method using Convolutional Neural\n  Network", "comments": "Accepted in book chapter \"CVML in Agriculture\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A disease that limits a plant from its maximal capacity is defined as plant\ndisease. From the perspective of agriculture, diagnosing plant disease is\ncrucial, as diseases often limit plants' production capacity. However, manual\napproaches to recognize plant diseases are often temporal, challenging, and\ntime-consuming. Therefore, computerized recognition of plant diseases is highly\ndesired in the field of agricultural automation. Due to the recent improvement\nof computer vision, identifying diseases using leaf images of a particular\nplant has already been introduced. Nevertheless, the most introduced model can\nonly diagnose diseases of a specific plant. Hence, in this chapter, we\ninvestigate an optimal plant disease identification model combining the\ndiagnosis of multiple plants. Despite relying on multi-class classification,\nthe model inherits a multilabel classification method to identify the plant and\nthe type of disease in parallel. For the experiment and evaluation, we\ncollected data from various online sources that included leaf images of six\nplants, including tomato, potato, rice, corn, grape, and apple. In our\ninvestigation, we implement numerous popular convolutional neural network (CNN)\narchitectures. The experimental results validate that the Xception and DenseNet\narchitectures perform better in multi-label plant disease classification tasks.\nThrough architectural investigation, we imply that skip connections, spatial\nconvolutions, and shorter hidden layer connectivity cause better results in\nplant disease classification.\n", "versions": [{"version": "v1", "created": "Tue, 10 Nov 2020 15:18:52 GMT"}], "update_date": "2020-11-11", "authors_parsed": [["Kabir", "Muhammad Mohsin", ""], ["Ohi", "Abu Quwsar", ""], ["Mridha", "M. F.", ""]]}, {"id": "2011.05153", "submitter": "Sadra Rahimi Kari", "authors": "S. Rahimi Kari", "title": "Principles of Stochastic Computing: Fundamental Concepts and\n  Applications", "comments": "11 pages, 19 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.ET cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The semiconductor and IC industry is facing the issue of high energy\nconsumption. In modern days computers and processing systems are designed based\non the Turing machine and Von Neumann's architecture. This architecture mainly\nfocused on designing systems based on deterministic behaviors. To tackle energy\nconsumption and reliability in systems, Stochastic Computing was introduced. In\nthis research, we aim to review and study the principles behind stochastic\ncomputing and its implementation techniques. By utilizing stochastic computing,\nwe can achieve higher energy efficiency and smaller area sizes in terms of\ndesigning arithmetic units. Also, we aim to popularize the affiliation of\nStochastic systems in designing futuristic BLSI and Neuromorphic systems.\n", "versions": [{"version": "v1", "created": "Sun, 8 Nov 2020 10:40:59 GMT"}], "update_date": "2020-11-11", "authors_parsed": [["Kari", "S. Rahimi", ""]]}, {"id": "2011.05157", "submitter": "Tianjin Huang", "authors": "Tianjin Huang, Vlado Menkovski, Yulong Pei, Mykola Pechenizkiy", "title": "Bridging the Performance Gap between FGSM and PGD Adversarial Training", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning achieves state-of-the-art performance in many tasks but exposes\nto the underlying vulnerability against adversarial examples. Across existing\ndefense techniques, adversarial training with the projected gradient decent\nattack (adv.PGD) is considered as one of the most effective ways to achieve\nmoderate adversarial robustness. However, adv.PGD requires too much training\ntime since the projected gradient attack (PGD) takes multiple iterations to\ngenerate perturbations. On the other hand, adversarial training with the fast\ngradient sign method (adv.FGSM) takes much less training time since the fast\ngradient sign method (FGSM) takes one step to generate perturbations but fails\nto increase adversarial robustness. In this work, we extend adv.FGSM to make it\nachieve the adversarial robustness of adv.PGD. We demonstrate that the large\ncurvature along FGSM perturbed direction leads to a large difference in\nperformance of adversarial robustness between adv.FGSM and adv.PGD, and\ntherefore propose combining adv.FGSM with a curvature regularization\n(adv.FGSMR) in order to bridge the performance gap between adv.FGSM and\nadv.PGD. The experiments show that adv.FGSMR has higher training efficiency\nthan adv.PGD. In addition, it achieves comparable performance of adversarial\nrobustness on MNIST dataset under white-box attack, and it achieves better\nperformance than adv.PGD under white-box attack and effectively defends the\ntransferable adversarial attack on CIFAR-10 dataset.\n", "versions": [{"version": "v1", "created": "Sat, 7 Nov 2020 09:08:54 GMT"}], "update_date": "2020-11-11", "authors_parsed": [["Huang", "Tianjin", ""], ["Menkovski", "Vlado", ""], ["Pei", "Yulong", ""], ["Pechenizkiy", "Mykola", ""]]}, {"id": "2011.05186", "submitter": "Tao Tan", "authors": "Tao Tan, Bipul Das, Ravi Soni, Mate Fejes, Sohan Ranjan, Daniel Attila\n  Szabo, Vikram Melapudi, K S Shriram, Utkarsh Agrawal, Laszlo Rusko, Zita\n  Herczeg, Barbara Darazs, Pal Tegzes, Lehel Ferenczi, Rakesh Mullick, Gopal\n  Avinash", "title": "Pristine annotations-based multi-modal trained artificial intelligence\n  solution to triage chest X-ray for COVID-19", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  The COVID-19 pandemic continues to spread and impact the well-being of the\nglobal population. The front-line modalities including computed tomography (CT)\nand X-ray play an important role for triaging COVID patients. Considering the\nlimited access of resources (both hardware and trained personnel) and\ndecontamination considerations, CT may not be ideal for triaging suspected\nsubjects. Artificial intelligence (AI) assisted X-ray based applications for\ntriaging and monitoring require experienced radiologists to identify COVID\npatients in a timely manner and to further delineate the disease region\nboundary are seen as a promising solution. Our proposed solution differs from\nexisting solutions by industry and academic communities, and demonstrates a\nfunctional AI model to triage by inferencing using a single x-ray image, while\nthe deep-learning model is trained using both X-ray and CT data. We report on\nhow such a multi-modal training improves the solution compared to X-ray only\ntraining. The multi-modal solution increases the AUC (area under the receiver\noperating characteristic curve) from 0.89 to 0.93 and also positively impacts\nthe Dice coefficient (0.59 to 0.62) for localizing the pathology. To the best\nour knowledge, it is the first X-ray solution by leveraging multi-modal\ninformation for the development.\n", "versions": [{"version": "v1", "created": "Tue, 10 Nov 2020 15:36:08 GMT"}], "update_date": "2020-11-11", "authors_parsed": [["Tan", "Tao", ""], ["Das", "Bipul", ""], ["Soni", "Ravi", ""], ["Fejes", "Mate", ""], ["Ranjan", "Sohan", ""], ["Szabo", "Daniel Attila", ""], ["Melapudi", "Vikram", ""], ["Shriram", "K S", ""], ["Agrawal", "Utkarsh", ""], ["Rusko", "Laszlo", ""], ["Herczeg", "Zita", ""], ["Darazs", "Barbara", ""], ["Tegzes", "Pal", ""], ["Ferenczi", "Lehel", ""], ["Mullick", "Rakesh", ""], ["Avinash", "Gopal", ""]]}, {"id": "2011.05203", "submitter": "Remi Ronfard", "authors": "R\\'emi Ronfard and R\\'emi Colin de Verdi\\`ere", "title": "OpenKinoAI: An Open Source Framework for Intelligent Cinematography and\n  Editing of Live Performances", "comments": "8 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  OpenKinoAI is an open source framework for post-production of ultra high\ndefinition video which makes it possible to emulate professional multiclip\nediting techniques for the case of single camera recordings. OpenKinoAI\nincludes tools for uploading raw video footage of live performances on a remote\nweb server, detecting, tracking and recognizing the performers in the original\nmaterial, reframing the raw video into a large choice of cinematographic\nrushes, editing the rushes into movies, and annotating rushes and movies for\ndocumentation purposes. OpenKinoAI is made available to promote research in\nmulticlip video editing of ultra high definition video, and to allow performing\nartists and companies to use this research for archiving, documenting and\nsharing their work online in an innovative fashion.\n", "versions": [{"version": "v1", "created": "Fri, 30 Oct 2020 14:21:53 GMT"}], "update_date": "2020-11-11", "authors_parsed": [["Ronfard", "R\u00e9mi", ""], ["de Verdi\u00e8re", "R\u00e9mi Colin", ""]]}, {"id": "2011.05209", "submitter": "Birgitta Dresp-Langley", "authors": "Birgitta Dresp-Langley, John M. Wandeto", "title": "Pixel precise unsupervised detection of viral particle proliferation in\n  cellular imaging data", "comments": null, "journal-ref": "Informatics in Medecine Unlocked. 2020;20:100433", "doi": "10.1016/j.imu.2020.100433", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cellular and molecular imaging techniques and models have been developed to\ncharacterize single stages of viral proliferation after focal infection of\ncells in vitro. The fast and automatic classification of cell imaging data may\nprove helpful prior to any further comparison of representative experimental\ndata to mathematical models of viral propagation in host cells. Here, we use\ncomputer generated images drawn from a reproduction of an imaging model from a\npreviously published study of experimentally obtained cell imaging data\nrepresenting progressive viral particle proliferation in host cell monolayers.\nInspired by experimental time-based imaging data, here in this study viral\nparticle increase in time is simulated by a one-by-one increase, across images,\nin black or gray single pixels representing dead or partially infected cells,\nand hypothetical remission by a one-by-one increase in white pixels coding for\nliving cells in the original image model. The image simulations are submitted\nto unsupervised learning by a Self-Organizing Map (SOM) and the Quantization\nError in the SOM output (SOM-QE) is used for automatic classification of the\nimage simulations as a function of the represented extent of viral particle\nproliferation or cell recovery. Unsupervised classification by SOM-QE of 160\nmodel images, each with more than three million pixels, is shown to provide a\nstatistically reliable, pixel precise, and fast classification model that\noutperforms human computer-assisted image classification by RGB image mean\ncomputation. The automatic classification procedure proposed here provides a\npowerful approach to understand finely tuned mechanisms in the infection and\nproliferation of virus in cell lines in vitro or other cells.\n", "versions": [{"version": "v1", "created": "Tue, 10 Nov 2020 16:06:03 GMT"}], "update_date": "2020-11-11", "authors_parsed": [["Dresp-Langley", "Birgitta", ""], ["Wandeto", "John M.", ""]]}, {"id": "2011.05227", "submitter": "Th\\'eo Ayral", "authors": "Th\\'eo Ayral, Marco Pedersoli, Simon Bacon and Eric Granger", "title": "Temporal Stochastic Softmax for 3D CNNs: An Application in Facial\n  Expression Recognition", "comments": "Accepted to WACV 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Training deep learning models for accurate spatiotemporal recognition of\nfacial expressions in videos requires significant computational resources. For\npractical reasons, 3D Convolutional Neural Networks (3D CNNs) are usually\ntrained with relatively short clips randomly extracted from videos. However,\nsuch uniform sampling is generally sub-optimal because equal importance is\nassigned to each temporal clip. In this paper, we present a strategy for\nefficient video-based training of 3D CNNs. It relies on softmax temporal\npooling and a weighted sampling mechanism to select the most relevant training\nclips. The proposed softmax strategy provides several advantages: a reduced\ncomputational complexity due to efficient clip sampling, and an improved\naccuracy since temporal weighting focuses on more relevant clips during both\ntraining and inference. Experimental results obtained with the proposed method\non several facial expression recognition benchmarks show the benefits of\nfocusing on more informative clips in training videos. In particular, our\napproach improves performance and computational cost by reducing the impact of\ninaccurate trimming and coarse annotation of videos, and heterogeneous\ndistribution of visual information across time.\n", "versions": [{"version": "v1", "created": "Tue, 10 Nov 2020 16:40:00 GMT"}], "update_date": "2020-11-11", "authors_parsed": [["Ayral", "Th\u00e9o", ""], ["Pedersoli", "Marco", ""], ["Bacon", "Simon", ""], ["Granger", "Eric", ""]]}, {"id": "2011.05243", "submitter": "Mete Ahishali", "authors": "Mete Ahishali, Serkan Kiranyaz, Turker Ince, Moncef Gabbouj", "title": "Classification of Polarimetric SAR Images Using Compact Convolutional\n  Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Classification of polarimetric synthetic aperture radar (PolSAR) images is an\nactive research area with a major role in environmental applications. The\ntraditional Machine Learning (ML) methods proposed in this domain generally\nfocus on utilizing highly discriminative features to improve the classification\nperformance, but this task is complicated by the well-known \"curse of\ndimensionality\" phenomena. Other approaches based on deep Convolutional Neural\nNetworks (CNNs) have certain limitations and drawbacks, such as high\ncomputational complexity, an unfeasibly large training set with ground-truth\nlabels, and special hardware requirements. In this work, to address the\nlimitations of traditional ML and deep CNN based methods, a novel and\nsystematic classification framework is proposed for the classification of\nPolSAR images, based on a compact and adaptive implementation of CNNs using a\nsliding-window classification approach. The proposed approach has three\nadvantages. First, there is no requirement for an extensive feature extraction\nprocess. Second, it is computationally efficient due to utilized compact\nconfigurations. In particular, the proposed compact and adaptive CNN model is\ndesigned to achieve the maximum classification accuracy with minimum training\nand computational complexity. This is of considerable importance considering\nthe high costs involved in labelling in PolSAR classification. Finally, the\nproposed approach can perform classification using smaller window sizes than\ndeep CNNs. Experimental evaluations have been performed over the most\ncommonly-used four benchmark PolSAR images: AIRSAR L-Band and RADARSAT-2 C-Band\ndata of San Francisco Bay and Flevoland areas. Accordingly, the best obtained\noverall accuracies range between 92.33 - 99.39% for these benchmark study\nsites.\n", "versions": [{"version": "v1", "created": "Tue, 10 Nov 2020 17:09:11 GMT"}], "update_date": "2020-11-11", "authors_parsed": [["Ahishali", "Mete", ""], ["Kiranyaz", "Serkan", ""], ["Ince", "Turker", ""], ["Gabbouj", "Moncef", ""]]}, {"id": "2011.05254", "submitter": "Yongwei Wang", "authors": "Yongwei Wang, Mingquan Feng, Rabab Ward, Z. Jane Wang, Lanjun Wang", "title": "Perception Improvement for Free: Exploring Imperceptible Black-box\n  Adversarial Attacks on Image Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CR cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks are vulnerable to adversarial attacks. White-box\nadversarial attacks can fool neural networks with small adversarial\nperturbations, especially for large size images. However, keeping successful\nadversarial perturbations imperceptible is especially challenging for\ntransfer-based black-box adversarial attacks. Often such adversarial examples\ncan be easily spotted due to their unpleasantly poor visual qualities, which\ncompromises the threat of adversarial attacks in practice. In this study, to\nimprove the image quality of black-box adversarial examples perceptually, we\npropose structure-aware adversarial attacks by generating adversarial images\nbased on psychological perceptual models. Specifically, we allow higher\nperturbations on perceptually insignificant regions, while assigning lower or\nno perturbation on visually sensitive regions. In addition to the proposed\nspatial-constrained adversarial perturbations, we also propose a novel\nstructure-aware frequency adversarial attack method in the discrete cosine\ntransform (DCT) domain. Since the proposed attacks are independent of the\ngradient estimation, they can be directly incorporated with existing\ngradient-based attacks. Experimental results show that, with the comparable\nattack success rate (ASR), the proposed methods can produce adversarial\nexamples with considerably improved visual quality for free. With the\ncomparable perceptual quality, the proposed approaches achieve higher attack\nsuccess rates: particularly for the frequency structure-aware attacks, the\naverage ASR improves more than 10% over the baseline attacks.\n", "versions": [{"version": "v1", "created": "Fri, 30 Oct 2020 07:17:12 GMT"}], "update_date": "2020-11-11", "authors_parsed": [["Wang", "Yongwei", ""], ["Feng", "Mingquan", ""], ["Ward", "Rabab", ""], ["Wang", "Z. Jane", ""], ["Wang", "Lanjun", ""]]}, {"id": "2011.05289", "submitter": "Nicholas Vadivelu", "authors": "Nicholas Vadivelu, Mengye Ren, James Tu, Jingkang Wang, Raquel Urtasun", "title": "Learning to Communicate and Correct Pose Errors", "comments": "Conference on Robot Learning (CoRL) 2020. 16 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learned communication makes multi-agent systems more effective by aggregating\ndistributed information. However, it also exposes individual agents to the\nthreat of erroneous messages they might receive. In this paper, we study the\nsetting proposed in V2VNet, where nearby self-driving vehicles jointly perform\nobject detection and motion forecasting in a cooperative manner. Despite a huge\nperformance boost when the agents solve the task together, the gain is quickly\ndiminished in the presence of pose noise since the communication relies on\nspatial transformations. Hence, we propose a novel neural reasoning framework\nthat learns to communicate, to estimate potential errors, and finally, to reach\na consensus about those errors. Experiments confirm that our proposed framework\nsignificantly improves the robustness of multi-agent self-driving perception\nand motion forecasting systems under realistic and severe localization noise.\n", "versions": [{"version": "v1", "created": "Tue, 10 Nov 2020 18:19:40 GMT"}], "update_date": "2020-11-11", "authors_parsed": [["Vadivelu", "Nicholas", ""], ["Ren", "Mengye", ""], ["Tu", "James", ""], ["Wang", "Jingkang", ""], ["Urtasun", "Raquel", ""]]}, {"id": "2011.05308", "submitter": "Jiun Lee", "authors": "Jiun Lee, Jaekwang Kim, Inyong Yun", "title": "EPSR: Edge Profile Super resolution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose Edge Profile Super Resolution(EPSR) method to\npreserve structure information and to restore texture. We make EPSR by stacking\nmodified Fractal Residual Network(mFRN) structures hierarchically and\nrepeatedly. mFRN is made up of lots of Residual Edge Profile Blocks(REPBs)\nconsisting of three different modules such as Residual Efficient Channel\nAttention Block(RECAB) module, Edge Profile(EP) module, and Context Network(CN)\nmodule. RECAB produces more informative features with high frequency\ncomponents. From the feature, EP module produce structure informed features by\ngenerating edge profile itself. Finally, CN module captures details by\nexploiting high frequency information such as texture and structure with proper\nsharpness. As repeating the procedure in mFRN structure, our EPSR could extract\nhigh-fidelity features and thus it prevents texture loss and preserves\nstructure with appropriate sharpness. Experimental results present that our\nEPSR achieves competitive performance against state-of-the-art methods in PSNR\nand SSIM evaluation metrics as well as visual results.\n", "versions": [{"version": "v1", "created": "Mon, 9 Nov 2020 06:58:07 GMT"}, {"version": "v2", "created": "Mon, 10 May 2021 12:54:55 GMT"}, {"version": "v3", "created": "Wed, 12 May 2021 08:49:25 GMT"}], "update_date": "2021-05-13", "authors_parsed": [["Lee", "Jiun", ""], ["Kim", "Jaekwang", ""], ["Yun", "Inyong", ""]]}, {"id": "2011.05315", "submitter": "Nicholas Carlini", "authors": "Nicholas Carlini, Samuel Deng, Sanjam Garg, Somesh Jha, Saeed\n  Mahloujifar, Mohammad Mahmoody, Shuang Song, Abhradeep Thakurta, Florian\n  Tramer", "title": "Is Private Learning Possible with Instance Encoding?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A private machine learning algorithm hides as much as possible about its\ntraining data while still preserving accuracy. In this work, we study whether a\nnon-private learning algorithm can be made private by relying on an\ninstance-encoding mechanism that modifies the training inputs before feeding\nthem to a normal learner. We formalize both the notion of instance encoding and\nits privacy by providing two attack models. We first prove impossibility\nresults for achieving a (stronger) model. Next, we demonstrate practical\nattacks in the second (weaker) attack model on InstaHide, a recent proposal by\nHuang, Song, Li and Arora [ICML'20] that aims to use instance encoding for\nprivacy.\n", "versions": [{"version": "v1", "created": "Tue, 10 Nov 2020 18:55:20 GMT"}, {"version": "v2", "created": "Wed, 28 Apr 2021 01:18:36 GMT"}], "update_date": "2021-04-29", "authors_parsed": [["Carlini", "Nicholas", ""], ["Deng", "Samuel", ""], ["Garg", "Sanjam", ""], ["Jha", "Somesh", ""], ["Mahloujifar", "Saeed", ""], ["Mahmoody", "Mohammad", ""], ["Song", "Shuang", ""], ["Thakurta", "Abhradeep", ""], ["Tramer", "Florian", ""]]}, {"id": "2011.05317", "submitter": "Hammam Alshazly", "authors": "Hammam Alshazly and Christoph Linse and Erhardt Barth and Thomas\n  Martinetz", "title": "Explainable COVID-19 Detection Using Chest CT Scans and Deep Learning", "comments": null, "journal-ref": "Sensors - 2021", "doi": "10.3390/s21020455", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper explores how well deep learning models trained on chest CT images\ncan diagnose COVID-19 infected people in a fast and automated process. To this\nend, we adopt advanced deep network architectures and propose a transfer\nlearning strategy using custom-sized input tailored for each deep architecture\nto achieve the best performance. We conduct extensive sets of experiments on\ntwo CT image datasets, namely the SARS-CoV-2 CT-scan and the COVID19-CT. The\nobtained results show superior performances for our models compared with\nprevious studies, where our best models achieve average accuracy, precision,\nsensitivity, specificity and F1 score of 99.4%, 99.6%, 99.8%, 99.6% and 99.4%\non the SARS-CoV-2 dataset; and 92.9%, 91.3%, 93.7%, 92.2% and 92.5% on the\nCOVID19-CT dataset, respectively. Furthermore, we apply two visualization\ntechniques to provide visual explanations for the models' predictions. The\nvisualizations show well-separated clusters for CT images of COVID-19 from\nother lung diseases, and accurate localizations of the COVID-19 associated\nregions.\n", "versions": [{"version": "v1", "created": "Mon, 9 Nov 2020 17:37:31 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Alshazly", "Hammam", ""], ["Linse", "Christoph", ""], ["Barth", "Erhardt", ""], ["Martinetz", "Thomas", ""]]}, {"id": "2011.05358", "submitter": "Di Yang", "authors": "Di Yang, Rui Dai, Yaohui Wang, Rupayan Mallick, Luca Minciullo,\n  Gianpiero Francesca, Francois Bremond", "title": "Selective Spatio-Temporal Aggregation Based Pose Refinement System:\n  Towards Understanding Human Activities in Real-World Videos", "comments": "WACV2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Taking advantage of human pose data for understanding human activities has\nattracted much attention these days. However, state-of-the-art pose estimators\nstruggle in obtaining high-quality 2D or 3D pose data due to occlusion,\ntruncation and low-resolution in real-world un-annotated videos. Hence, in this\nwork, we propose 1) a Selective Spatio-Temporal Aggregation mechanism, named\nSST-A, that refines and smooths the keypoint locations extracted by multiple\nexpert pose estimators, 2) an effective weakly-supervised self-training\nframework which leverages the aggregated poses as pseudo ground-truth instead\nof handcrafted annotations for real-world pose estimation. Extensive\nexperiments are conducted for evaluating not only the upstream pose refinement\nbut also the downstream action recognition performance on four datasets, Toyota\nSmarthome, NTU-RGB+D, Charades, and Kinetics-50. We demonstrate that the\nskeleton data refined by our Pose-Refinement system (SSTA-PRS) is effective at\nboosting various existing action recognition models, which achieves competitive\nor state-of-the-art performance.\n", "versions": [{"version": "v1", "created": "Tue, 10 Nov 2020 19:19:51 GMT"}], "update_date": "2020-11-12", "authors_parsed": [["Yang", "Di", ""], ["Dai", "Rui", ""], ["Wang", "Yaohui", ""], ["Mallick", "Rupayan", ""], ["Minciullo", "Luca", ""], ["Francesca", "Gianpiero", ""], ["Bremond", "Francois", ""]]}, {"id": "2011.05370", "submitter": "Peter Ondruska", "authors": "Lukas Platinsky, Michal Szabados, Filip Hlasek, Ross Hemsley, Luca Del\n  Pero, Andrej Pancik, Bryan Baum, Hugo Grimmett, Peter Ondruska", "title": "Collaborative Augmented Reality on Smartphones via Life-long City-scale\n  Maps", "comments": "Published at ISMAR 2020, http://www.bluevisionlabs.org", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper we present the first published end-to-end production\ncomputer-vision system for powering city-scale shared augmented reality\nexperiences on mobile devices. In doing so we propose a new formulation for an\nexperience-based mapping framework as an effective solution to the key issues\nof city-scale SLAM scalability, robustness, map updates and all-time\nall-weather performance required by a production system. Furthermore, we\npropose an effective way of synchronising SLAM systems to deliver seamless\nreal-time localisation of multiple edge devices at the same time. All this in\nthe presence of network latency and bandwidth limitations. The resulting system\nis deployed and tested at scale in San Francisco where it delivers AR\nexperiences in a mapped area of several hundred kilometers. To foster further\ndevelopment of this area we offer the data set to the public, constituting the\nlargest of this kind to date.\n", "versions": [{"version": "v1", "created": "Tue, 10 Nov 2020 19:45:06 GMT"}], "update_date": "2020-11-12", "authors_parsed": [["Platinsky", "Lukas", ""], ["Szabados", "Michal", ""], ["Hlasek", "Filip", ""], ["Hemsley", "Ross", ""], ["Del Pero", "Luca", ""], ["Pancik", "Andrej", ""], ["Baum", "Bryan", ""], ["Grimmett", "Hugo", ""], ["Ondruska", "Peter", ""]]}, {"id": "2011.05406", "submitter": "Qi Tang", "authors": "Qi Tang and Vardaan Kishore Kumar", "title": "Deep Learning Derived Histopathology Image Score for Increasing Phase 3\n  Clinical Trial Probability of Success", "comments": "Machine Learning for Health (ML4H) at NeurIPS 2020 - Extended\n  Abstract", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Failures in Phase 3 clinical trials contribute to expensive cost of drug\ndevelopment in oncology. To drastically reduce such cost, responders to an\noncology treatment need to be identified early on in the drug development\nprocess with limited amount of patient data before the planning of Phase 3\nclinical trials. Despite the challenge of small sample size, we pioneered the\nuse of deep-learning derived digital pathology scores to identify responders\nbased on the immunohistochemistry images of the target antigen expressed in\ntumor biopsy samples from a Phase 1 Non-small Cell Lung Cancer clinical trial.\nBased on repeated 10-fold cross validations, the deep-learning derived score on\naverage achieved 4% higher AUC of ROC curve and 6% higher AUC of\nPrecision-Recall curve comparing to the tumor proportion score (TPS) based\nclinical benchmark. In a small independent testing set of patients, we also\ndemonstrated that the deep-learning derived score achieved numerically at least\n25% higher responder rate in the enriched population than the TPS clinical\nbenchmark.\n", "versions": [{"version": "v1", "created": "Tue, 10 Nov 2020 21:26:13 GMT"}], "update_date": "2020-11-12", "authors_parsed": [["Tang", "Qi", ""], ["Kumar", "Vardaan Kishore", ""]]}, {"id": "2011.05410", "submitter": "Tomasz Pieciak", "authors": "Azam Hamidinekoo, Tomasz Pieciak, Maryam Afzali, Otar Akanyeti, Yinyin\n  Yuan", "title": "Glioma Classification Using Multimodal Radiology and Histology Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gliomas are brain tumours with a high mortality rate. There are various\ngrades and sub-types of this tumour, and the treatment procedure varies\naccordingly. Clinicians and oncologists diagnose and categorise these tumours\nbased on visual inspection of radiology and histology data. However, this\nprocess can be time-consuming and subjective. The computer-assisted methods can\nhelp clinicians to make better and faster decisions. In this paper, we propose\na pipeline for automatic classification of gliomas into three sub-types:\noligodendroglioma, astrocytoma, and glioblastoma, using both radiology and\nhistopathology images. The proposed approach implements distinct classification\nmodels for radiographic and histologic modalities and combines them through an\nensemble method. The classification algorithm initially carries out tile-level\n(for histology) and slice-level (for radiology) classification via a deep\nlearning method, then tile/slice-level latent features are combined for a\nwhole-slide and whole-volume sub-type prediction. The classification algorithm\nwas evaluated using the data set provided in the CPM-RadPath 2020 challenge.\nThe proposed pipeline achieved the F1-Score of 0.886, Cohen's Kappa score of\n0.811 and Balance accuracy of 0.860. The ability of the proposed model for\nend-to-end learning of diverse features enables it to give a comparable\nprediction of glioma tumour sub-types.\n", "versions": [{"version": "v1", "created": "Tue, 10 Nov 2020 21:38:26 GMT"}], "update_date": "2020-11-12", "authors_parsed": [["Hamidinekoo", "Azam", ""], ["Pieciak", "Tomasz", ""], ["Afzali", "Maryam", ""], ["Akanyeti", "Otar", ""], ["Yuan", "Yinyin", ""]]}, {"id": "2011.05421", "submitter": "Alan Smeaton", "authors": "Simranjeet Singh and Rajneesh Sharma and Alan F. Smeaton", "title": "Using GANs to Synthesise Minimum Training Data for Deepfake Generation", "comments": "13 pages, 6 figures, 2 tables, appears in Proceedings of 28th Irish\n  Conference on Artificial Intelligence and Cognitive Science AICS2020,\n  December 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  There are many applications of Generative Adversarial Networks (GANs) in\nfields like computer vision, natural language processing, speech synthesis, and\nmore. Undoubtedly the most notable results have been in the area of image\nsynthesis and in particular in the generation of deepfake videos. While\ndeepfakes have received much negative media coverage, they can be a useful\ntechnology in applications like entertainment, customer relations, or even\nassistive care. One problem with generating deepfakes is the requirement for a\nlot of image training data of the subject which is not an issue if the subject\nis a celebrity for whom many images already exist. If there are only a small\nnumber of training images then the quality of the deepfake will be poor. Some\nmedia reports have indicated that a good deepfake can be produced with as few\nas 500 images but in practice, quality deepfakes require many thousands of\nimages, one of the reasons why deepfakes of celebrities and politicians have\nbecome so popular. In this study, we exploit the property of a GAN to produce\nimages of an individual with variable facial expressions which we then use to\ngenerate a deepfake. We observe that with such variability in facial\nexpressions of synthetic GAN-generated training images and a reduced quantity\nof them, we can produce a near-realistic deepfake videos.\n", "versions": [{"version": "v1", "created": "Tue, 10 Nov 2020 22:05:38 GMT"}], "update_date": "2020-11-12", "authors_parsed": [["Singh", "Simranjeet", ""], ["Sharma", "Rajneesh", ""], ["Smeaton", "Alan F.", ""]]}, {"id": "2011.05428", "submitter": "Seong Tae Kim", "authors": "Abinav Ravi Venkatakrishnan, Seong Tae Kim, Rami Eisawy, Franz\n  Pfister, Nassir Navab", "title": "Self-Supervised Out-of-Distribution Detection in Brain CT Scans", "comments": "Accepted at Medical Imaging Meets NeurIPS Workshop at NeurIPS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Medical imaging data suffers from the limited availability of annotation\nbecause annotating 3D medical data is a time-consuming and expensive task.\nMoreover, even if the annotation is available, supervised learning-based\napproaches suffer highly imbalanced data. Most of the scans during the\nscreening are from normal subjects, but there are also large variations in\nabnormal cases. To address these issues, recently, unsupervised deep anomaly\ndetection methods that train the model on large-sized normal scans and detect\nabnormal scans by calculating reconstruction error have been reported. In this\npaper, we propose a novel self-supervised learning technique for anomaly\ndetection. Our architecture largely consists of two parts: 1) Reconstruction\nand 2) predicting geometric transformations. By training the network to predict\ngeometric transformations, the model could learn better image features and\ndistribution of normal scans. In the test time, the geometric transformation\npredictor can assign the anomaly score by calculating the error between\ngeometric transformation and prediction. Moreover, we further use\nself-supervised learning with context restoration for pretraining our model. By\ncomparative experiments on clinical brain CT scans, the effectiveness of the\nproposed method has been verified.\n", "versions": [{"version": "v1", "created": "Tue, 10 Nov 2020 22:21:48 GMT"}], "update_date": "2020-11-12", "authors_parsed": [["Venkatakrishnan", "Abinav Ravi", ""], ["Kim", "Seong Tae", ""], ["Eisawy", "Rami", ""], ["Pfister", "Franz", ""], ["Navab", "Nassir", ""]]}, {"id": "2011.05429", "submitter": "Julius Adebayo", "authors": "Julius Adebayo, Michael Muelly, Ilaria Liccardi, Been Kim", "title": "Debugging Tests for Model Explanations", "comments": "A shorter version of this work will appear at Neurips 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We investigate whether post-hoc model explanations are effective for\ndiagnosing model errors--model debugging. In response to the challenge of\nexplaining a model's prediction, a vast array of explanation methods have been\nproposed. Despite increasing use, it is unclear if they are effective. To\nstart, we categorize \\textit{bugs}, based on their source, into:~\\textit{data,\nmodel, and test-time} contamination bugs. For several explanation methods, we\nassess their ability to: detect spurious correlation artifacts (data\ncontamination), diagnose mislabeled training examples (data contamination),\ndifferentiate between a (partially) re-initialized model and a trained one\n(model contamination), and detect out-of-distribution inputs (test-time\ncontamination). We find that the methods tested are able to diagnose a spurious\nbackground bug, but not conclusively identify mislabeled training examples. In\naddition, a class of methods, that modify the back-propagation algorithm are\ninvariant to the higher layer parameters of a deep network; hence, ineffective\nfor diagnosing model contamination. We complement our analysis with a human\nsubject study, and find that subjects fail to identify defective models using\nattributions, but instead rely, primarily, on model predictions. Taken\ntogether, our results provide guidance for practitioners and researchers\nturning to explanations as tools for model debugging.\n", "versions": [{"version": "v1", "created": "Tue, 10 Nov 2020 22:23:25 GMT"}], "update_date": "2020-11-12", "authors_parsed": [["Adebayo", "Julius", ""], ["Muelly", "Michael", ""], ["Liccardi", "Ilaria", ""], ["Kim", "Been", ""]]}, {"id": "2011.05437", "submitter": "Rogerio Bonatti", "authors": "Arthur Bucker, Rogerio Bonatti and Sebastian Scherer", "title": "Do You See What I See? Coordinating Multiple Aerial Cameras for Robot\n  Cinematography", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV cs.MA", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Aerial cinematography is significantly expanding the capabilities of\nfilm-makers. Recent progress in autonomous unmanned aerial vehicles (UAVs) has\nfurther increased the potential impact of aerial cameras, with systems that can\nsafely track actors in unstructured cluttered environments. Professional\nproductions, however, require the use of multiple cameras simultaneously to\nrecord different viewpoints of the same scene, which are edited into the final\nfootage either in real time or in post-production. Such extreme motion\ncoordination is particularly hard for unscripted action scenes, which are a\ncommon use case of aerial cameras. In this work we develop a real-time\nmulti-UAV coordination system that is capable of recording dynamic targets\nwhile maximizing shot diversity and avoiding collisions and mutual visibility\nbetween cameras. We validate our approach in multiple cluttered environments of\na photo-realistic simulator, and deploy the system using two UAVs in real-world\nexperiments. We show that our coordination scheme has low computational cost\nand takes only 1.17 ms on average to plan for a team of 3 UAVs over a 10 s time\nhorizon. Supplementary video: https://youtu.be/m2R3anv2ADE\n", "versions": [{"version": "v1", "created": "Tue, 10 Nov 2020 22:43:25 GMT"}, {"version": "v2", "created": "Wed, 31 Mar 2021 22:02:07 GMT"}], "update_date": "2021-04-02", "authors_parsed": [["Bucker", "Arthur", ""], ["Bonatti", "Rogerio", ""], ["Scherer", "Sebastian", ""]]}, {"id": "2011.05438", "submitter": "Tharindu Fernando", "authors": "Tharindu Fernando, Simon Denman, Sridha Sridharan, Clinton Fookes", "title": "Fast & Slow Learning: Incorporating Synthetic Gradients in Neural Memory\n  Controllers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural Memory Networks (NMNs) have received increased attention in recent\nyears compared to deep architectures that use a constrained memory. Despite\ntheir new appeal, the success of NMNs hinges on the ability of the\ngradient-based optimiser to perform incremental training of the NMN\ncontrollers, determining how to leverage their high capacity for knowledge\nretrieval. This means that while excellent performance can be achieved when the\ntraining data is consistent and well distributed, rare data samples are hard to\nlearn from as the controllers fail to incorporate them effectively during model\ntraining. Drawing inspiration from the human cognition process, in particular\nthe utilisation of neuromodulators in the human brain, we propose to decouple\nthe learning process of the NMN controllers to allow them to achieve flexible,\nrapid adaptation in the presence of new information. This trait is highly\nbeneficial for meta-learning tasks where the memory controllers must quickly\ngrasp abstract concepts in the target domain, and adapt stored knowledge. This\nallows the NMN controllers to quickly determine which memories are to be\nretained and which are to be erased, and swiftly adapt their strategy to the\nnew task at hand. Through both quantitative and qualitative evaluations on\nmultiple public benchmarks, including classification and regression tasks, we\ndemonstrate the utility of the proposed approach. Our evaluations not only\nhighlight the ability of the proposed NMN architecture to outperform the\ncurrent state-of-the-art methods, but also provide insights on how the proposed\naugmentations help achieve such superior results. In addition, we demonstrate\nthe practical implications of the proposed learning strategy, where the\nfeedback path can be shared among multiple neural memory networks as a\nmechanism for knowledge sharing.\n", "versions": [{"version": "v1", "created": "Tue, 10 Nov 2020 22:44:27 GMT"}], "update_date": "2020-11-12", "authors_parsed": [["Fernando", "Tharindu", ""], ["Denman", "Simon", ""], ["Sridharan", "Sridha", ""], ["Fookes", "Clinton", ""]]}, {"id": "2011.05459", "submitter": "Juntao Tan", "authors": "Juntao Tan, Changkyu Song, Abdeslam Boularias", "title": "A Self-supervised Learning System for Object Detection in Videos Using\n  Random Walks on Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a new self-supervised system for learning to detect novel\nand previously unseen categories of objects in images. The proposed system\nreceives as input several unlabeled videos of scenes containing various\nobjects. The frames of the videos are segmented into objects using depth\ninformation, and the segments are tracked along each video. The system then\nconstructs a weighted graph that connects sequences based on the similarities\nbetween the objects that they contain. The similarity between two sequences of\nobjects is measured by using generic visual features, after automatically\nre-arranging the frames in the two sequences to align the viewpoints of the\nobjects. The graph is used to sample triplets of similar and dissimilar\nexamples by performing random walks. The triplet examples are finally used to\ntrain a siamese neural network that projects the generic visual features into a\nlow-dimensional manifold. Experiments on three public datasets, YCB-Video,\nCORe50 and RGBD-Object, show that the projected low-dimensional features\nimprove the accuracy of clustering unknown objects into novel categories, and\noutperform several recent unsupervised clustering techniques.\n", "versions": [{"version": "v1", "created": "Tue, 10 Nov 2020 23:37:40 GMT"}, {"version": "v2", "created": "Tue, 6 Apr 2021 21:36:59 GMT"}], "update_date": "2021-04-08", "authors_parsed": [["Tan", "Juntao", ""], ["Song", "Changkyu", ""], ["Boularias", "Abdeslam", ""]]}, {"id": "2011.05479", "submitter": "Jeremy Irvin", "authors": "Jeremy Irvin, Hao Sheng, Neel Ramachandran, Sonja Johnson-Yu, Sharon\n  Zhou, Kyle Story, Rose Rustowicz, Cooper Elsworth, Kemen Austin, Andrew Y. Ng", "title": "ForestNet: Classifying Drivers of Deforestation in Indonesia using Deep\n  Learning on Satellite Imagery", "comments": "Tackling Climate Change with Machine Learning at NeurIPS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Characterizing the processes leading to deforestation is critical to the\ndevelopment and implementation of targeted forest conservation and management\npolicies. In this work, we develop a deep learning model called ForestNet to\nclassify the drivers of primary forest loss in Indonesia, a country with one of\nthe highest deforestation rates in the world. Using satellite imagery,\nForestNet identifies the direct drivers of deforestation in forest loss patches\nof any size. We curate a dataset of Landsat 8 satellite images of known forest\nloss events paired with driver annotations from expert interpreters. We use the\ndataset to train and validate the models and demonstrate that ForestNet\nsubstantially outperforms other standard driver classification approaches. In\norder to support future research on automated approaches to deforestation\ndriver classification, the dataset curated in this study is publicly available\nat https://stanfordmlgroup.github.io/projects/forestnet .\n", "versions": [{"version": "v1", "created": "Wed, 11 Nov 2020 00:28:40 GMT"}], "update_date": "2020-11-12", "authors_parsed": [["Irvin", "Jeremy", ""], ["Sheng", "Hao", ""], ["Ramachandran", "Neel", ""], ["Johnson-Yu", "Sonja", ""], ["Zhou", "Sharon", ""], ["Story", "Kyle", ""], ["Rustowicz", "Rose", ""], ["Elsworth", "Cooper", ""], ["Austin", "Kemen", ""], ["Ng", "Andrew Y.", ""]]}, {"id": "2011.05490", "submitter": "Zhengyang Lu", "authors": "Zhengyang Lu and Ying Chen", "title": "Dense U-net for super-resolution with shuffle pooling layer", "comments": "17 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent researches have achieved great progress on single image\nsuper-resolution(SISR) due to the development of deep learning in the field of\ncomputer vision. In these method, the high resolution input image is\ndown-scaled to low resolution space using a single filter, commonly\nmax-pooling, before feature extraction. This means that the feature extraction\nis performed in biased filtered feature space. We demonstrate that this is\nsub-optimal and causes information loss. In this work, we proposed a\nstate-of-the-art convolutional neural network method called Dense U-net with\nshuffle pooling. To achieve this, a modified U-net with dense blocks, called\ndense U-net, is proposed for SISR. Then, a new pooling strategy called shuffle\npooling is designed, which is aimed to replace the dense U-Net for down-scale\noperation. By doing so, we effectively replace the handcrafted filter in the\nSISR pipeline with more lossy down-sampling filters specifically trained for\neach feature map, whilst also reducing the information loss of the overall SISR\noperation. In addition, a mix loss function, which combined with Mean Square\nError(MSE), Structural Similarity Index(SSIM) and Mean Gradient Error (MGE),\ncomes up to reduce the perception loss and high-level information loss. Our\nproposed method achieves superior accuracy over previous state-of-the-art on\nthe three benchmark datasets: SET14, BSD300, ICDAR2003. Code is available\nonline.\n", "versions": [{"version": "v1", "created": "Wed, 11 Nov 2020 00:59:43 GMT"}, {"version": "v2", "created": "Sat, 9 Jan 2021 05:58:08 GMT"}], "update_date": "2021-01-12", "authors_parsed": [["Lu", "Zhengyang", ""], ["Chen", "Ying", ""]]}, {"id": "2011.05499", "submitter": "Pedro O. Pinheiro", "authors": "Pedro O. Pinheiro, Amjad Almahairi, Ryan Y. Benmalek, Florian Golemo,\n  Aaron Courville", "title": "Unsupervised Learning of Dense Visual Representations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Contrastive self-supervised learning has emerged as a promising approach to\nunsupervised visual representation learning. In general, these methods learn\nglobal (image-level) representations that are invariant to different views\n(i.e., compositions of data augmentation) of the same image. However, many\nvisual understanding tasks require dense (pixel-level) representations. In this\npaper, we propose View-Agnostic Dense Representation (VADeR) for unsupervised\nlearning of dense representations. VADeR learns pixelwise representations by\nforcing local features to remain constant over different viewing conditions.\nSpecifically, this is achieved through pixel-level contrastive learning:\nmatching features (that is, features that describes the same location of the\nscene on different views) should be close in an embedding space, while\nnon-matching features should be apart. VADeR provides a natural representation\nfor dense prediction tasks and transfers well to downstream tasks. Our method\noutperforms ImageNet supervised pretraining (and strong unsupervised baselines)\nin multiple dense prediction tasks.\n", "versions": [{"version": "v1", "created": "Wed, 11 Nov 2020 01:28:11 GMT"}, {"version": "v2", "created": "Mon, 7 Dec 2020 20:16:40 GMT"}], "update_date": "2020-12-09", "authors_parsed": [["Pinheiro", "Pedro O.", ""], ["Almahairi", "Amjad", ""], ["Benmalek", "Ryan Y.", ""], ["Golemo", "Florian", ""], ["Courville", "Aaron", ""]]}, {"id": "2011.05506", "submitter": "Mohsen Jafarzadeh", "authors": "Mohsen Jafarzadeh, Touqeer Ahmad, Akshay Raj Dhamija, Chunchun Li,\n  Steve Cruz, Terrance E. Boult", "title": "Automatic Open-World Reliability Assessment", "comments": "2021 IEEE Winter Conference on Applications of Computer Vision (WACV)", "journal-ref": "2021 IEEE Winter Conference on Applications of Computer Vision\n  (WACV)", "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image classification in the open-world must handle out-of-distribution (OOD)\nimages. Systems should ideally reject OOD images, or they will map atop of\nknown classes and reduce reliability. Using open-set classifiers that can\nreject OOD inputs can help. However, optimal accuracy of open-set classifiers\ndepend on the frequency of OOD data. Thus, for either standard or open-set\nclassifiers, it is important to be able to determine when the world changes and\nincreasing OOD inputs will result in reduced system reliability. However,\nduring operations, we cannot directly assess accuracy as there are no labels.\nThus, the reliability assessment of these classifiers must be done by human\noperators, made more complex because networks are not 100% accurate, so some\nfailures are to be expected. To automate this process, herein, we formalize the\nopen-world recognition reliability problem and propose multiple automatic\nreliability assessment policies to address this new problem using only the\ndistribution of reported scores/probability data. The distributional algorithms\ncan be applied to both classic classifiers with SoftMax as well as the\nopen-world Extreme Value Machine (EVM) to provide automated reliability\nassessment. We show that all of the new algorithms significantly outperform\ndetection using the mean of SoftMax.\n", "versions": [{"version": "v1", "created": "Wed, 11 Nov 2020 01:56:23 GMT"}, {"version": "v2", "created": "Mon, 14 Dec 2020 01:35:18 GMT"}], "update_date": "2020-12-15", "authors_parsed": [["Jafarzadeh", "Mohsen", ""], ["Ahmad", "Touqeer", ""], ["Dhamija", "Akshay Raj", ""], ["Li", "Chunchun", ""], ["Cruz", "Steve", ""], ["Boult", "Terrance E.", ""]]}, {"id": "2011.05523", "submitter": "Shang Jiang", "authors": "Shang Jiang, Haoran Qin, Bingli Zhang, Jieyu Zheng", "title": "Optimized Loss Functions for Object detection: A Case Study on Nighttime\n  Vehicle Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Loss functions is a crucial factor that affecting the detection precision in\nobject detection task. In this paper, we optimize both two loss functions for\nclassification and localization simultaneously. Firstly, by multiplying an\nIoU-based coefficient by the standard cross entropy loss in classification loss\nfunction, the correlation between localization and classification is\nestablished. Compared to the existing studies, in which the correlation is only\napplied to improve the localization accuracy for positive samples, this paper\nutilizes the correlation to obtain the really hard negative samples and aims to\ndecrease the misclassified rate for negative samples. Besides, a novel\nlocalization loss named MIoU is proposed by incorporating a Mahalanobis\ndistance between predicted box and target box, which eliminate the gradients\ninconsistency problem in the DIoU loss, further improving the localization\naccuracy. Finally, sufficient experiments for nighttime vehicle detection have\nbeen done on two datasets. Our results show than when train with the proposed\nloss functions, the detection performance can be outstandingly improved. The\nsource code and trained models are available at\nhttps://github.com/therebellll/NegIoU-PosIoU-Miou.\n", "versions": [{"version": "v1", "created": "Wed, 11 Nov 2020 03:00:49 GMT"}, {"version": "v2", "created": "Wed, 18 Nov 2020 10:34:48 GMT"}], "update_date": "2020-11-19", "authors_parsed": [["Jiang", "Shang", ""], ["Qin", "Haoran", ""], ["Zhang", "Bingli", ""], ["Zheng", "Jieyu", ""]]}, {"id": "2011.05532", "submitter": "Prasan Shedligeri", "authors": "Prasan Shedligeri, Anupama S, Kaushik Mitra", "title": "A Unified Framework for Compressive Video Recovery from Coded Exposure\n  Techniques", "comments": "10 pages, 6 figures, 4 tables. Accepted to be published at WACV 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Several coded exposure techniques have been proposed for acquiring high frame\nrate videos at low bandwidth. Most recently, a Coded-2-Bucket camera has been\nproposed that can acquire two compressed measurements in a single exposure,\nunlike previously proposed coded exposure techniques, which can acquire only a\nsingle measurement. Although two measurements are better than one for an\neffective video recovery, we are yet unaware of the clear advantage of two\nmeasurements, either quantitatively or qualitatively. Here, we propose a\nunified learning-based framework to make such a qualitative and quantitative\ncomparison between those which capture only a single coded image (Flutter\nShutter, Pixel-wise coded exposure) and those that capture two measurements per\nexposure (C2B). Our learning-based framework consists of a shift-variant\nconvolutional layer followed by a fully convolutional deep neural network. Our\nproposed unified framework achieves the state of the art reconstructions in all\nthree sensing techniques. Further analysis shows that when most scene points\nare static, the C2B sensor has a significant advantage over acquiring a single\npixel-wise coded measurement. However, when most scene points undergo motion,\nthe C2B sensor has only a marginal benefit over the single pixel-wise coded\nexposure measurement.\n", "versions": [{"version": "v1", "created": "Wed, 11 Nov 2020 03:45:31 GMT"}], "update_date": "2020-11-12", "authors_parsed": [["Shedligeri", "Prasan", ""], ["S", "Anupama", ""], ["Mitra", "Kaushik", ""]]}, {"id": "2011.05543", "submitter": "Sagar Kora Venu", "authors": "Sagar Kora Venu", "title": "An ensemble-based approach by fine-tuning the deep transfer learning\n  models to classify pneumonia from chest X-ray images", "comments": null, "journal-ref": "SciTePress 2021", "doi": "10.5220/0010377403900401", "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Pneumonia is caused by viruses, bacteria, or fungi that infect the lungs,\nwhich, if not diagnosed, can be fatal and lead to respiratory failure. More\nthan 250,000 individuals in the United States, mainly adults, are diagnosed\nwith pneumonia each year, and 50,000 die from the disease. Chest Radiography\n(X-ray) is widely used by radiologists to detect pneumonia. It is not uncommon\nto overlook pneumonia detection for a well-trained radiologist, which triggers\nthe need for improvement in the diagnosis's accuracy. In this work, we propose\nusing transfer learning, which can reduce the neural network's training time\nand minimize the generalization error. We trained, fine-tuned the\nstate-of-the-art deep learning models such as InceptionResNet, MobileNetV2,\nXception, DenseNet201, and ResNet152V2 to classify pneumonia accurately. Later,\nwe created a weighted average ensemble of these models and achieved a test\naccuracy of 98.46%, precision of 98.38%, recall of 99.53%, and f1 score of\n98.96%. These performance metrics of accuracy, precision, and f1 score are at\ntheir highest levels ever reported in the literature, which can be considered a\nbenchmark for the accurate pneumonia classification.\n", "versions": [{"version": "v1", "created": "Wed, 11 Nov 2020 04:50:06 GMT"}], "update_date": "2021-02-18", "authors_parsed": [["Venu", "Sagar Kora", ""]]}, {"id": "2011.05552", "submitter": "Alice Xue", "authors": "Alice Xue", "title": "End-to-End Chinese Landscape Painting Creation Using Generative\n  Adversarial Networks", "comments": "This research is an extension of Alice Xue's senior thesis at\n  Princeton University. The paper will be published in the proceedings of IEEE\n  Winter Conference on Applications of Computer Vision (WACV) 2021 and\n  presented at the conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current GAN-based art generation methods produce unoriginal artwork due to\ntheir dependence on conditional input. Here, we propose Sketch-And-Paint GAN\n(SAPGAN), the first model which generates Chinese landscape paintings from end\nto end, without conditional input. SAPGAN is composed of two GANs: SketchGAN\nfor generation of edge maps, and PaintGAN for subsequent edge-to-painting\ntranslation. Our model is trained on a new dataset of traditional Chinese\nlandscape paintings never before used for generative research. A 242-person\nVisual Turing Test study reveals that SAPGAN paintings are mistaken as human\nartwork with 55% frequency, significantly outperforming paintings from baseline\nGANs. Our work lays a groundwork for truly machine-original art generation.\n", "versions": [{"version": "v1", "created": "Wed, 11 Nov 2020 05:20:42 GMT"}], "update_date": "2020-11-12", "authors_parsed": [["Xue", "Alice", ""]]}, {"id": "2011.05558", "submitter": "Menglin Jia", "authors": "Menglin Jia and Zuxuan Wu and Austin Reiter and Claire Cardie and\n  Serge Belongie and Ser-Nam Lim", "title": "Intentonomy: a Dataset and Study towards Human Intent Understanding", "comments": "CVPR2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.SI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  An image is worth a thousand words, conveying information that goes beyond\nthe physical visual content therein. In this paper, we study the intent behind\nsocial media images with an aim to analyze how visual information can help the\nrecognition of human intent. Towards this goal, we introduce an intent dataset,\nIntentonomy, comprising 14K images covering a wide range of everyday scenes.\nThese images are manually annotated with 28 intent categories that are derived\nfrom a social psychology taxonomy. We then systematically study whether, and to\nwhat extent, commonly used visual information, i.e., object and context,\ncontribute to human motive understanding. Based on our findings, we conduct\nfurther study to quantify the effect of attending to object and context classes\nas well as textual information in the form of hashtags when training an intent\nclassifier. Our results quantitatively and qualitatively shed light on how\nvisual and textual information can produce observable effects when predicting\nintent.\n", "versions": [{"version": "v1", "created": "Wed, 11 Nov 2020 05:39:00 GMT"}, {"version": "v2", "created": "Sun, 28 Mar 2021 02:24:46 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Jia", "Menglin", ""], ["Wu", "Zuxuan", ""], ["Reiter", "Austin", ""], ["Cardie", "Claire", ""], ["Belongie", "Serge", ""], ["Lim", "Ser-Nam", ""]]}, {"id": "2011.05586", "submitter": "Andrew Geiss", "authors": "Andrew Geiss and Joseph C. Hardin", "title": "Invertible CNN-Based Super Resolution with Downsampling Awareness", "comments": "16 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Single image super resolution involves artificially increasing the resolution\nof an image. Recently, convolutional neural networks have been demonstrated as\nvery powerful tools for this problem. These networks are typically trained by\nartificially degrading high resolution images and training the neural network\nto reproduce the original. Because these neural networks are learning an\ninverse function for an image downsampling scheme, their high-resolution\noutputs should ideally re-produce the corresponding low-resolution input when\nthe same downsampling scheme is applied. This constraint has not historically\nbeen explicitly and strictly imposed during training however. Here, a method\nfor \"downsampling aware\" super resolution networks is proposed. A\ndifferentiable operator is applied as the final output layer of the neural\nnetwork that forces the downsampled output to match the low resolution input\ndata under 2D-average downsampling. It is demonstrated that appending this\noperator to a selection of state-of-the-art deep-learning-based super\nresolution schemes improves training time and overall performance on most of\nthe common image super resolution benchmark datasets. In addition to this\nperformance improvement for images, this method has potentially broad and\nsignificant impacts in the physical sciences. This scheme can be applied to\ndata produced by medical scans, precipitation radars, gridded numerical\nsimulations, satellite imagers, and many other sources. In such applications,\nthe proposed method's guarantee of strict adherence to physical conservation\nlaws is of critical importance.\n", "versions": [{"version": "v1", "created": "Wed, 11 Nov 2020 06:18:54 GMT"}], "update_date": "2020-11-12", "authors_parsed": [["Geiss", "Andrew", ""], ["Hardin", "Joseph C.", ""]]}, {"id": "2011.05621", "submitter": "Peng Jiang Dr.", "authors": "Zhiyi Pan, Peng Jiang, Changhe Tu", "title": "Scribble-Supervised Semantic Segmentation by Random Walk on Neural\n  Representation and Self-Supervision on Neural Eigenspace", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scribble-supervised semantic segmentation has gained much attention recently\nfor its promising performance without high-quality annotations. Many approaches\nhave been proposed. Typically, they handle this problem to either introduce a\nwell-labeled dataset from another related task, turn to iterative refinement\nand post-processing with the graphical model, or manipulate the scribble label.\nThis work aims to achieve semantic segmentation supervised by scribble label\ndirectly without auxiliary information and other intermediate manipulation.\nSpecifically, we impose diffusion on neural representation by random walk and\nconsistency on neural eigenspace by self-supervision, which forces the neural\nnetwork to produce dense and consistent predictions over the whole dataset. The\nrandom walk embedded in the network will compute a probabilistic transition\nmatrix, with which the neural representation diffused to be uniform. Moreover,\ngiven the probabilistic transition matrix, we apply the self-supervision on its\neigenspace for consistency in the image's main parts. In addition to comparing\nthe common scribble dataset, we also conduct experiments on the modified\ndatasets that randomly shrink and even drop the scribbles on image objects. The\nresults demonstrate the superiority of the proposed method and are even\ncomparable to some full-label supervised ones. The code and datasets are\navailable at https://github.com/panzhiyi/RW-SS.\n", "versions": [{"version": "v1", "created": "Wed, 11 Nov 2020 08:22:25 GMT"}, {"version": "v2", "created": "Thu, 12 Nov 2020 05:53:26 GMT"}], "update_date": "2020-11-13", "authors_parsed": [["Pan", "Zhiyi", ""], ["Jiang", "Peng", ""], ["Tu", "Changhe", ""]]}, {"id": "2011.05623", "submitter": "Li Yuan", "authors": "Li Yuan, Will Xiao, Gabriel Kreiman, Francis E.H. Tay, Jiashi Feng,\n  Margaret S. Livingstone", "title": "Adversarial images for the primate brain", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.CV cs.NE eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep artificial neural networks have been proposed as a model of primate\nvision. However, these networks are vulnerable to adversarial attacks, whereby\nintroducing minimal noise can fool networks into misclassifying images. Primate\nvision is thought to be robust to such adversarial images. We evaluated this\nassumption by designing adversarial images to fool primate vision. To do so, we\nfirst trained a model to predict responses of face-selective neurons in macaque\ninferior temporal cortex. Next, we modified images, such as human faces, to\nmatch their model-predicted neuronal responses to a target category, such as\nmonkey faces. These adversarial images elicited neuronal responses similar to\nthe target category. Remarkably, the same images fooled monkeys and humans at\nthe behavioral level. These results challenge fundamental assumptions about the\nsimilarity between computer and primate vision and show that a model of\nneuronal activity can selectively direct primate visual behavior.\n", "versions": [{"version": "v1", "created": "Wed, 11 Nov 2020 08:30:54 GMT"}], "update_date": "2020-11-12", "authors_parsed": [["Yuan", "Li", ""], ["Xiao", "Will", ""], ["Kreiman", "Gabriel", ""], ["Tay", "Francis E. H.", ""], ["Feng", "Jiashi", ""], ["Livingstone", "Margaret S.", ""]]}, {"id": "2011.05626", "submitter": "Isinsu Katircioglu", "authors": "Isinsu Katircioglu, Helge Rhodin, Victor Constantin, J\\\"org Sp\\\"orri,\n  Mathieu Salzmann, Pascal Fua", "title": "Self-supervised Segmentation via Background Inpainting", "comments": "arXiv admin note: text overlap with arXiv:1907.08051", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While supervised object detection and segmentation methods achieve impressive\naccuracy, they generalize poorly to images whose appearance significantly\ndiffers from the data they have been trained on. To address this when\nannotating data is prohibitively expensive, we introduce a self-supervised\ndetection and segmentation approach that can work with single images captured\nby a potentially moving camera. At the heart of our approach lies the\nobservation that object segmentation and background reconstruction are linked\ntasks, and that, for structured scenes, background regions can be\nre-synthesized from their surroundings, whereas regions depicting the moving\nobject cannot. We encode this intuition into a self-supervised loss function\nthat we exploit to train a proposal-based segmentation network. To account for\nthe discrete nature of the proposals, we develop a Monte Carlo-based training\nstrategy that allows the algorithm to explore the large space of object\nproposals. We apply our method to human detection and segmentation in images\nthat visually depart from those of standard benchmarks and outperform existing\nself-supervised methods.\n", "versions": [{"version": "v1", "created": "Wed, 11 Nov 2020 08:34:40 GMT"}], "update_date": "2020-11-12", "authors_parsed": [["Katircioglu", "Isinsu", ""], ["Rhodin", "Helge", ""], ["Constantin", "Victor", ""], ["Sp\u00f6rri", "J\u00f6rg", ""], ["Salzmann", "Mathieu", ""], ["Fua", "Pascal", ""]]}, {"id": "2011.05627", "submitter": "Hongfeng Li", "authors": "Hongfeng Li, Yini Pan, Jie Zhao and Li Zhang", "title": "Skin disease diagnosis with deep learning: a review", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Skin cancer is one of the most threatening diseases worldwide. However,\ndiagnosing skin cancer correctly is challenging. Recently, deep learning\nalgorithms have emerged to achieve excellent performance on various tasks.\nParticularly, they have been applied to the skin disease diagnosis tasks. In\nthis paper, we present a review on deep learning methods and their applications\nin skin disease diagnosis. We first present a brief introduction to skin\ndiseases and image acquisition methods in dermatology, and list several\npublicly available skin datasets for training and testing algorithms. Then, we\nintroduce the conception of deep learning and review popular deep learning\narchitectures. Thereafter, popular deep learning frameworks facilitating the\nimplementation of deep learning algorithms and performance evaluation metrics\nare presented. As an important part of this article, we then review the\nliterature involving deep learning methods for skin disease diagnosis from\nseveral aspects according to the specific tasks. Additionally, we discuss the\nchallenges faced in the area and suggest possible future research directions.\nThe major purpose of this article is to provide a conceptual and systematically\nreview of the recent works on skin disease diagnosis with deep learning. Given\nthe popularity of deep learning, there remains great challenges in the area, as\nwell as opportunities that we can explore in the future.\n", "versions": [{"version": "v1", "created": "Wed, 11 Nov 2020 08:35:21 GMT"}, {"version": "v2", "created": "Sun, 6 Dec 2020 14:16:58 GMT"}], "update_date": "2020-12-08", "authors_parsed": [["Li", "Hongfeng", ""], ["Pan", "Yini", ""], ["Zhao", "Jie", ""], ["Zhang", "Li", ""]]}, {"id": "2011.05648", "submitter": "Hongfeng Li", "authors": "Hongfeng Li", "title": "Semi-supervised Sparse Representation with Graph Regularization for\n  Image Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG math.OC", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Image classification is a challenging problem for computer in reality. Large\nnumbers of methods can achieve satisfying performances with sufficient labeled\nimages. However, labeled images are still highly limited for certain image\nclassification tasks. Instead, lots of unlabeled images are available and easy\nto be obtained. Therefore, making full use of the available unlabeled data can\nbe a potential way to further improve the performance of current image\nclassification methods. In this paper, we propose a discriminative\nsemi-supervised sparse representation algorithm for image classification. In\nthe algorithm, the classification process is combined with the sparse coding to\nlearn a data-driven linear classifier. To obtain discriminative predictions,\nthe predicted labels are regularized with three graphs, i.e., the global\nmanifold structure graph, the within-class graph and the between-classes graph.\nThe constructed graphs are able to extract structure information included in\nboth the labeled and unlabeled data. Moreover, the proposed method is extended\nto a kernel version for dealing with data that cannot be linearly classified.\nAccordingly, efficient algorithms are developed to solve the corresponding\noptimization problems. Experimental results on several challenging databases\ndemonstrate that the proposed algorithm achieves excellent performances\ncompared with related popular methods.\n", "versions": [{"version": "v1", "created": "Wed, 11 Nov 2020 09:16:48 GMT"}], "update_date": "2020-11-12", "authors_parsed": [["Li", "Hongfeng", ""]]}, {"id": "2011.05653", "submitter": "Mauricio Perez", "authors": "Mauricio Perez, Jun Liu and Alex C. Kot", "title": "Skeleton-based Relational Reasoning for Group Activity Analysis", "comments": "25 pages, 5 figures, to be published in Elsevier Pattern Recognition", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Research on group activity recognition mostly leans on the standard\ntwo-stream approach (RGB and Optical Flow) as their input features. Few have\nexplored explicit pose information, with none using it directly to reason about\nthe persons interactions. In this paper, we leverage the skeleton information\nto learn the interactions between the individuals straight from it. With our\nproposed method GIRN, multiple relationship types are inferred from independent\nmodules, that describe the relations between the body joints pair-by-pair.\nAdditionally to the joints relations, we also experiment with the previously\nunexplored relationship between individuals and relevant objects (e.g.\nvolleyball). The individuals distinct relations are then merged through an\nattention mechanism, that gives more importance to those individuals more\nrelevant for distinguishing the group activity. We evaluate our method in the\nVolleyball dataset, obtaining competitive results to the state-of-the-art. Our\nexperiments demonstrate the potential of skeleton-based approaches for modeling\nmulti-person interactions.\n", "versions": [{"version": "v1", "created": "Wed, 11 Nov 2020 09:25:53 GMT"}, {"version": "v2", "created": "Wed, 21 Jul 2021 04:07:58 GMT"}], "update_date": "2021-07-22", "authors_parsed": [["Perez", "Mauricio", ""], ["Liu", "Jun", ""], ["Kot", "Alex C.", ""]]}, {"id": "2011.05668", "submitter": "Negar Heidari", "authors": "Negar Heidari and Alexandros Iosifidis", "title": "Progressive Spatio-Temporal Graph Convolutional Network for\n  Skeleton-Based Human Action Recognition", "comments": "Accepted by the 2021 IEEE International Conference on Acoustics,\n  Speech and Signal Processing (ICASSP 2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph convolutional networks (GCNs) have been very successful in\nskeleton-based human action recognition where the sequence of skeletons is\nmodeled as a graph. However, most of the GCN-based methods in this area train a\ndeep feed-forward network with a fixed topology that leads to high\ncomputational complexity and restricts their application in low computation\nscenarios. In this paper, we propose a method to automatically find a compact\nand problem-specific topology for spatio-temporal graph convolutional networks\nin a progressive manner. Experimental results on two widely used datasets for\nskeleton-based human action recognition indicate that the proposed method has\ncompetitive or even better classification performance compared to the\nstate-of-the-art methods with much lower computational complexity.\n", "versions": [{"version": "v1", "created": "Wed, 11 Nov 2020 09:57:49 GMT"}, {"version": "v2", "created": "Mon, 26 Apr 2021 20:43:10 GMT"}], "update_date": "2021-04-28", "authors_parsed": [["Heidari", "Negar", ""], ["Iosifidis", "Alexandros", ""]]}, {"id": "2011.05669", "submitter": "Rebecca K\\\"onig", "authors": "Rebecca K\\\"onig and Bertram Drost", "title": "A Hybrid Approach for 6DoF Pose Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We propose a method for 6DoF pose estimation of rigid objects that uses a\nstate-of-the-art deep learning based instance detector to segment object\ninstances in an RGB image, followed by a point-pair based voting method to\nrecover the object's pose. We additionally use an automatic method selection\nthat chooses the instance detector and the training set as that with the\nhighest performance on the validation set. This hybrid approach leverages the\nbest of learning and classic approaches, using CNNs to filter highly\nunstructured data and cut through the clutter, and a local geometric approach\nwith proven convergence for robust pose estimation. The method is evaluated on\nthe BOP core datasets where it significantly exceeds the baseline method and is\nthe best fast method in the BOP 2020 Challenge.\n", "versions": [{"version": "v1", "created": "Wed, 11 Nov 2020 09:58:23 GMT"}], "update_date": "2020-11-12", "authors_parsed": [["K\u00f6nig", "Rebecca", ""], ["Drost", "Bertram", ""]]}, {"id": "2011.05670", "submitter": "Zhuo Zheng", "authors": "Zhuo Zheng, Yanfei Zhong, Ailong Ma, Liangpei Zhang", "title": "FPGA: Fast Patch-Free Global Learning Framework for Fully End-to-End\n  Hyperspectral Image Classification", "comments": "16 pages, 15 figures, IEEE Transactions on Geoscience and Remote\n  Sensing, 2020", "journal-ref": null, "doi": "10.1109/TGRS.2020.2967821", "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning techniques have provided significant improvements in\nhyperspectral image (HSI) classification. The current deep learning based HSI\nclassifiers follow a patch-based learning framework by dividing the image into\noverlapping patches. As such, these methods are local learning methods, which\nhave a high computational cost. In this paper, a fast patch-free global\nlearning (FPGA) framework is proposed for HSI classification. In FPGA, an\nencoder-decoder based FCN is utilized to consider the global spatial\ninformation by processing the whole image, which results in fast inference.\nHowever, it is difficult to directly utilize the encoder-decoder based FCN for\nHSI classification as it always fails to converge due to the insufficiently\ndiverse gradients caused by the limited training samples. To solve the\ndivergence problem and maintain the abilities of FCN of fast inference and\nglobal spatial information mining, a global stochastic stratified sampling\nstrategy is first proposed by transforming all the training samples into a\nstochastic sequence of stratified samples. This strategy can obtain diverse\ngradients to guarantee the convergence of the FCN in the FPGA framework. For a\nbetter design of FCN architecture, FreeNet, which is a fully end-to-end network\nfor HSI classification, is proposed to maximize the exploitation of the global\nspatial information and boost the performance via a spectral attention based\nencoder and a lightweight decoder. A lateral connection module is also designed\nto connect the encoder and decoder, fusing the spatial details in the encoder\nand the semantic features in the decoder. The experimental results obtained\nusing three public benchmark datasets suggest that the FPGA framework is\nsuperior to the patch-based framework in both speed and accuracy for HSI\nclassification. Code has been made available at:\nhttps://github.com/Z-Zheng/FreeNet.\n", "versions": [{"version": "v1", "created": "Wed, 11 Nov 2020 09:59:48 GMT"}], "update_date": "2020-11-12", "authors_parsed": [["Zheng", "Zhuo", ""], ["Zhong", "Yanfei", ""], ["Ma", "Ailong", ""], ["Zhang", "Liangpei", ""]]}, {"id": "2011.05680", "submitter": "Radu Timofte", "authors": "Samarth Shukla, Andr\\'es Romero, Luc Van Gool, Radu Timofte", "title": "Zero-Pair Image to Image Translation using Domain Conditional\n  Normalization", "comments": "Paper accepted for publication at WACV 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose an approach based on domain conditional\nnormalization (DCN) for zero-pair image-to-image translation, i.e., translating\nbetween two domains which have no paired training data available but each have\npaired training data with a third domain. We employ a single generator which\nhas an encoder-decoder structure and analyze different implementations of\ndomain conditional normalization to obtain the desired target domain output.\nThe validation benchmark uses RGB-depth pairs and RGB-semantic pairs for\ntraining and compares performance for the depth-semantic translation task. The\nproposed approaches improve in qualitative and quantitative terms over the\ncompared methods, while using much fewer parameters. Code available at\nhttps://github.com/samarthshukla/dcn\n", "versions": [{"version": "v1", "created": "Wed, 11 Nov 2020 10:20:47 GMT"}], "update_date": "2020-11-12", "authors_parsed": [["Shukla", "Samarth", ""], ["Romero", "Andr\u00e9s", ""], ["Van Gool", "Luc", ""], ["Timofte", "Radu", ""]]}, {"id": "2011.05684", "submitter": "Sutanu Bera", "authors": "Sutanu Bera, Prabir Kumar Biswas", "title": "Noise Conscious Training of Non Local Neural Network powered by Self\n  Attentive Spectral Normalized Markovian Patch GAN for Low Dose CT Denoising", "comments": null, "journal-ref": "IEEE Transactions on Medical Imaging 2021", "doi": "10.1109/TMI.2021.3094525", "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The explosive rise of the use of Computer tomography (CT) imaging in medical\npractice has heightened public concern over the patient's associated radiation\ndose. However, reducing the radiation dose leads to increased noise and\nartifacts, which adversely degrades the scan's interpretability. Consequently,\nan advanced image reconstruction algorithm to improve the diagnostic\nperformance of low dose ct arose as the primary concern among the researchers,\nwhich is challenging due to the ill-posedness of the problem. In recent times,\nthe deep learning-based technique has emerged as a dominant method for low dose\nCT(LDCT) denoising. However, some common bottleneck still exists, which hinders\ndeep learning-based techniques from furnishing the best performance. In this\nstudy, we attempted to mitigate these problems with three novel accretions.\nFirst, we propose a novel convolutional module as the first attempt to utilize\nneighborhood similarity of CT images for denoising tasks. Our proposed module\nassisted in boosting the denoising by a significant margin. Next, we moved\ntowards the problem of non-stationarity of CT noise and introduced a new noise\naware mean square error loss for LDCT denoising. Moreover, the loss mentioned\nabove also assisted to alleviate the laborious effort required while training\nCT denoising network using image patches. Lastly, we propose a novel\ndiscriminator function for CT denoising tasks. The conventional vanilla\ndiscriminator tends to overlook the fine structural details and focus on the\nglobal agreement. Our proposed discriminator leverage self-attention and\npixel-wise GANs for restoring the diagnostic quality of LDCT images. Our method\nvalidated on a publicly available dataset of the 2016 NIH-AAPM-Mayo Clinic Low\nDose CT Grand Challenge performed remarkably better than the existing state of\nthe art method.\n", "versions": [{"version": "v1", "created": "Wed, 11 Nov 2020 10:44:52 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["Bera", "Sutanu", ""], ["Biswas", "Prabir Kumar", ""]]}, {"id": "2011.05702", "submitter": "Shidong Wang", "authors": "Shidong Wang, Yi Ren, Gerard Parr, Yu Guan and Ling Shao", "title": "Invariant Deep Compressible Covariance Pooling for Aerial Scene\n  Categorization", "comments": "This article has been accepted for inclusion in a future issue of\n  IEEE Transactions on Geoscience and Remote Sensing. Content is final as\n  presented, with the exception of pagination", "journal-ref": null, "doi": "10.1109/TGRS.2020.3026221", "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning discriminative and invariant feature representation is the key to\nvisual image categorization. In this article, we propose a novel invariant deep\ncompressible covariance pooling (IDCCP) to solve nuisance variations in aerial\nscene categorization. We consider transforming the input image according to a\nfinite transformation group that consists of multiple confounding orthogonal\nmatrices, such as the D4 group. Then, we adopt a Siamese-style network to\ntransfer the group structure to the representation space, where we can derive a\ntrivial representation that is invariant under the group action. The linear\nclassifier trained with trivial representation will also be possessed with\ninvariance. To further improve the discriminative power of representation, we\nextend the representation to the tensor space while imposing orthogonal\nconstraints on the transformation matrix to effectively reduce feature\ndimensions. We conduct extensive experiments on the publicly released aerial\nscene image data sets and demonstrate the superiority of this method compared\nwith state-of-the-art methods. In particular, with using ResNet architecture,\nour IDCCP model can reduce the dimension of the tensor representation by about\n98% without sacrificing accuracy (i.e., <0.5%).\n", "versions": [{"version": "v1", "created": "Wed, 11 Nov 2020 11:13:07 GMT"}], "update_date": "2020-11-12", "authors_parsed": [["Wang", "Shidong", ""], ["Ren", "Yi", ""], ["Parr", "Gerard", ""], ["Guan", "Yu", ""], ["Shao", "Ling", ""]]}, {"id": "2011.05704", "submitter": "Ragav Sachdeva", "authors": "Ragav Sachdeva, Filipe R. Cordeiro, Vasileios Belagiannis, Ian Reid,\n  Gustavo Carneiro", "title": "EvidentialMix: Learning with Combined Open-set and Closed-set Noisy\n  Labels", "comments": "Paper accepted at WACV'21: Winter Conference on Applications of\n  Computer Vision", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The efficacy of deep learning depends on large-scale data sets that have been\ncarefully curated with reliable data acquisition and annotation processes.\nHowever, acquiring such large-scale data sets with precise annotations is very\nexpensive and time-consuming, and the cheap alternatives often yield data sets\nthat have noisy labels. The field has addressed this problem by focusing on\ntraining models under two types of label noise: 1) closed-set noise, where some\ntraining samples are incorrectly annotated to a training label other than their\nknown true class; and 2) open-set noise, where the training set includes\nsamples that possess a true class that is (strictly) not contained in the set\nof known training labels. In this work, we study a new variant of the noisy\nlabel problem that combines the open-set and closed-set noisy labels, and\nintroduce a benchmark evaluation to assess the performance of training\nalgorithms under this setup. We argue that such problem is more general and\nbetter reflects the noisy label scenarios in practice. Furthermore, we propose\na novel algorithm, called EvidentialMix, that addresses this problem and\ncompare its performance with the state-of-the-art methods for both closed-set\nand open-set noise on the proposed benchmark. Our results show that our method\nproduces superior classification results and better feature representations\nthan previous state-of-the-art methods. The code is available at\nhttps://github.com/ragavsachdeva/EvidentialMix.\n", "versions": [{"version": "v1", "created": "Wed, 11 Nov 2020 11:15:32 GMT"}], "update_date": "2020-11-12", "authors_parsed": [["Sachdeva", "Ragav", ""], ["Cordeiro", "Filipe R.", ""], ["Belagiannis", "Vasileios", ""], ["Reid", "Ian", ""], ["Carneiro", "Gustavo", ""]]}, {"id": "2011.05719", "submitter": "Tobias Scheck", "authors": "Tobias Scheck, Roman Seidel, Gangolf Hirtz", "title": "Learning from THEODORE: A Synthetic Omnidirectional Top-View Indoor\n  Dataset for Deep Transfer Learning", "comments": "Paper accepted in WACV 2020", "journal-ref": null, "doi": "10.1109/WACV45572.2020.9093563", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recent work about synthetic indoor datasets from perspective views has shown\nsignificant improvements of object detection results with Convolutional Neural\nNetworks(CNNs). In this paper, we introduce THEODORE: a novel, large-scale\nindoor dataset containing 100,000 high-resolution diversified fisheye images\nwith 14 classes. To this end, we create 3D virtual environments of living\nrooms, different human characters and interior textures. Beside capturing\nfisheye images from virtual environments we create annotations for semantic\nsegmentation, instance masks and bounding boxes for object detection tasks. We\ncompare our synthetic dataset to state of the art real-world datasets for\nomnidirectional images. Based on MS COCO weights, we show that our dataset is\nwell suited for fine-tuning CNNs for object detection. Through a high\ngeneralization of our models by means of image synthesis and domain\nrandomization, we reach an AP up to 0.84 for class person on High-Definition\nAnalytics dataset.\n", "versions": [{"version": "v1", "created": "Wed, 11 Nov 2020 11:46:33 GMT"}], "update_date": "2020-11-12", "authors_parsed": [["Scheck", "Tobias", ""], ["Seidel", "Roman", ""], ["Hirtz", "Gangolf", ""]]}, {"id": "2011.05734", "submitter": "Tobias Scheck", "authors": "Tobias Scheck, Ana Perez Grassi, Gangolf Hirtz", "title": "A CNN-based Feature Space for Semi-supervised Incremental Learning in\n  Assisted Living Applications", "comments": "Accepted in VISAPP 2020", "journal-ref": null, "doi": "10.5220/0008871302170224", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A Convolutional Neural Network (CNN) is sometimes confronted with objects of\nchanging appearance ( new instances) that exceed its generalization capability.\nThis requires the CNN to incorporate new knowledge, i.e., to learn\nincrementally. In this paper, we are concerned with this problem in the context\nof assisted living. We propose using the feature space that results from the\ntraining dataset to automatically label problematic images that could not be\nproperly recognized by the CNN. The idea is to exploit the extra information in\nthe feature space for a semi-supervised labeling and to employ problematic\nimages to improve the CNN's classification model. Among other benefits, the\nresulting semi-supervised incremental learning process allows improving the\nclassification accuracy of new instances by 40% as illustrated by extensive\nexperiments.\n", "versions": [{"version": "v1", "created": "Wed, 11 Nov 2020 12:31:48 GMT"}], "update_date": "2020-11-12", "authors_parsed": [["Scheck", "Tobias", ""], ["Grassi", "Ana Perez", ""], ["Hirtz", "Gangolf", ""]]}, {"id": "2011.05735", "submitter": "Steffen Czolbe", "authors": "Steffen Czolbe, Oswin Krause, Aasa Feragen", "title": "DeepSim: Semantic similarity metrics for learned image registration", "comments": "Talk given at Medical Imaging Meets NeurIPS, NeurIPS 2020 workshop.\n  Extended Abstract", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose a semantic similarity metric for image registration. Existing\nmetrics like euclidean distance or normalized cross-correlation focus on\naligning intensity values, giving difficulties with low intensity contrast or\nnoise. Our semantic approach learns dataset-specific features that drive the\noptimization of a learning-based registration model. Comparing to existing\nunsupervised and supervised methods across multiple image modalities and\napplications, we achieve consistently high registration accuracy and faster\nconvergence than state of the art, and the learned invariance to noise gives\nsmoother transformations on low-quality images.\n", "versions": [{"version": "v1", "created": "Wed, 11 Nov 2020 12:35:07 GMT"}], "update_date": "2020-11-12", "authors_parsed": [["Czolbe", "Steffen", ""], ["Krause", "Oswin", ""], ["Feragen", "Aasa", ""]]}, {"id": "2011.05740", "submitter": "Araceli Morales", "authors": "Araceli Morales, Gemma Piella and Federico M. Sukno", "title": "Survey on 3D face reconstruction from uncalibrated images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Recently, a lot of attention has been focused on the incorporation of 3D data\ninto face analysis and its applications. Despite providing a more accurate\nrepresentation of the face, 3D facial images are more complex to acquire than\n2D pictures. As a consequence, great effort has been invested in developing\nsystems that reconstruct 3D faces from an uncalibrated 2D image. However, the\n3D-from-2D face reconstruction problem is ill-posed, thus prior knowledge is\nneeded to restrict the solutions space. In this work, we review 3D face\nreconstruction methods proposed in the last decade, focusing on those that only\nuse 2D pictures captured under uncontrolled conditions. We present a\nclassification of the proposed methods based on the technique used to add prior\nknowledge, considering three main strategies, namely, statistical model\nfitting, photometry, and deep learning, and reviewing each of them separately.\nIn addition, given the relevance of statistical 3D facial models as prior\nknowledge, we explain the construction procedure and provide a list of the most\npopular publicly available 3D facial models. After the exhaustive study of\n3D-from-2D face reconstruction approaches, we observe that the deep learning\nstrategy is rapidly growing since the last few years, becoming the standard\nchoice in replacement of the widespread statistical model fitting. Unlike the\nother two strategies, photometry-based methods have decreased in number due to\nthe need for strong underlying assumptions that limit the quality of their\nreconstructions compared to statistical model fitting and deep learning\nmethods. The review also identifies current challenges and suggests avenues for\nfuture research.\n", "versions": [{"version": "v1", "created": "Wed, 11 Nov 2020 12:48:11 GMT"}, {"version": "v2", "created": "Fri, 26 Feb 2021 08:32:32 GMT"}], "update_date": "2021-03-01", "authors_parsed": [["Morales", "Araceli", ""], ["Piella", "Gemma", ""], ["Sukno", "Federico M.", ""]]}, {"id": "2011.05746", "submitter": "Serkan Budak", "authors": "Umut \\\"Ozkaya, \\c{S}aban \\\"Ozt\\\"urk, Serkan Budak, Farid Melgani,\n  Kemal Polat", "title": "Classification of COVID-19 in Chest CT Images using Convolutional\n  Support Vector Machines", "comments": "20 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Purpose: Coronavirus 2019 (COVID-19), which emerged in Wuhan, China and\naffected the whole world, has cost the lives of thousands of people. Manual\ndiagnosis is inefficient due to the rapid spread of this virus. For this\nreason, automatic COVID-19 detection studies are carried out with the support\nof artificial intelligence algorithms. Methods: In this study, a deep learning\nmodel that detects COVID-19 cases with high performance is presented. The\nproposed method is defined as Convolutional Support Vector Machine (CSVM) and\ncan automatically classify Computed Tomography (CT) images. Unlike the\npre-trained Convolutional Neural Networks (CNN) trained with the transfer\nlearning method, the CSVM model is trained as a scratch. To evaluate the\nperformance of the CSVM method, the dataset is divided into two parts as\ntraining (%75) and testing (%25). The CSVM model consists of blocks containing\nthree different numbers of SVM kernels. Results: When the performance of\npre-trained CNN networks and CSVM models is assessed, CSVM (7x7, 3x3, 1x1)\nmodel shows the highest performance with 94.03% ACC, 96.09% SEN, 92.01% SPE,\n92.19% PRE, 94.10% F1-Score, 88.15% MCC and 88.07% Kappa metric values.\nConclusion: The proposed method is more effective than other methods. It has\nproven in experiments performed to be an inspiration for combating COVID and\nfor future studies.\n", "versions": [{"version": "v1", "created": "Wed, 11 Nov 2020 13:04:38 GMT"}], "update_date": "2020-11-12", "authors_parsed": [["\u00d6zkaya", "Umut", ""], ["\u00d6zt\u00fcrk", "\u015eaban", ""], ["Budak", "Serkan", ""], ["Melgani", "Farid", ""], ["Polat", "Kemal", ""]]}, {"id": "2011.05756", "submitter": "Bj\\\"orn Barz", "authors": "Bj\\\"orn Barz, Kai Schr\\\"oter, Ann-Christin Kra, Joachim Denzler", "title": "Finding Relevant Flood Images on Twitter using Content-based Filters", "comments": "ICPR 2020 Workshop on Machine Learning Advances Environmental Science\n  (MAES)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The analysis of natural disasters such as floods in a timely manner often\nsuffers from limited data due to coarsely distributed sensors or sensor\nfailures. At the same time, a plethora of information is buried in an abundance\nof images of the event posted on social media platforms such as Twitter. These\nimages could be used to document and rapidly assess the situation and derive\nproxy-data not available from sensors, e.g., the degree of water pollution.\nHowever, not all images posted online are suitable or informative enough for\nthis purpose. Therefore, we propose an automatic filtering approach using\nmachine learning techniques for finding Twitter images that are relevant for\none of the following information objectives: assessing the flooded area, the\ninundation depth, and the degree of water pollution. Instead of relying on\ntextual information present in the tweet, the filter analyzes the image\ncontents directly. We evaluate the performance of two different approaches and\nvarious features on a case-study of two major flooding events. Our image-based\nfilter is able to enhance the quality of the results substantially compared\nwith a keyword-based filter, improving the mean average precision from 23% to\n53% on average.\n", "versions": [{"version": "v1", "created": "Wed, 11 Nov 2020 13:16:54 GMT"}], "update_date": "2020-11-12", "authors_parsed": [["Barz", "Bj\u00f6rn", ""], ["Schr\u00f6ter", "Kai", ""], ["Kra", "Ann-Christin", ""], ["Denzler", "Joachim", ""]]}, {"id": "2011.05784", "submitter": "Yi Gu", "authors": "Yi Gu, Yuting Gao, Jie Li, Chentao Wu, Weijia Jia", "title": "Generative and Discriminative Learning for Distorted Image Restoration", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Liquify is a common technique for image editing, which can be used for image\ndistortion. Due to the uncertainty in the distortion variation, restoring\ndistorted images caused by liquify filter is a challenging task. To edit images\nin an efficient way, distorted images are expected to be restored\nautomatically. This paper aims at the distorted image restoration, which is\ncharacterized by seeking the appropriate warping and completion of a distorted\nimage. Existing methods focus on the hardware assistance or the geometric\nprinciple to solve the specific regular deformation caused by natural\nphenomena, but they cannot handle the irregularity and uncertainty of\nartificial distortion in this task. To address this issue, we propose a novel\ngenerative and discriminative learning method based on deep neural networks,\nwhich can learn various reconstruction mappings and represent complex and\nhigh-dimensional data. This method decomposes the task into a rectification\nstage and a refinement stage. The first stage generative network predicts the\nmapping from the distorted images to the rectified ones. The second stage\ngenerative network then further optimizes the perceptual quality. Since there\nis no available dataset or benchmark to explore this task, we create a\nDistorted Face Dataset (DFD) by forward distortion mapping based on CelebA\ndataset. Extensive experimental evaluation on the proposed benchmark and the\napplication demonstrates that our method is an effective way for distorted\nimage restoration.\n", "versions": [{"version": "v1", "created": "Wed, 11 Nov 2020 14:01:29 GMT"}, {"version": "v2", "created": "Thu, 12 Nov 2020 14:09:03 GMT"}, {"version": "v3", "created": "Fri, 27 Nov 2020 06:09:41 GMT"}], "update_date": "2020-11-30", "authors_parsed": [["Gu", "Yi", ""], ["Gao", "Yuting", ""], ["Li", "Jie", ""], ["Wu", "Chentao", ""], ["Jia", "Weijia", ""]]}, {"id": "2011.05787", "submitter": "Cinjon Resnick", "authors": "Cinjon Resnick, Or Litany, Hugo Larochelle, Joan Bruna, Kyunghyun Cho", "title": "Learned Equivariant Rendering without Transformation Supervision", "comments": "Workshop on Differentiable Vision, Graphics, and Physics in Machine\n  Learning at NeurIPS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a self-supervised framework to learn scene representations from\nvideo that are automatically delineated into objects and background. Our method\nrelies on moving objects being equivariant with respect to their transformation\nacross frames and the background being constant. After training, we can\nmanipulate and render the scenes in real time to create unseen combinations of\nobjects, transformations, and backgrounds. We show results on moving MNIST with\nbackgrounds.\n", "versions": [{"version": "v1", "created": "Wed, 11 Nov 2020 14:05:05 GMT"}], "update_date": "2020-11-12", "authors_parsed": [["Resnick", "Cinjon", ""], ["Litany", "Or", ""], ["Larochelle", "Hugo", ""], ["Bruna", "Joan", ""], ["Cho", "Kyunghyun", ""]]}, {"id": "2011.05813", "submitter": "Stefan Lionar", "authors": "Stefan Lionar, Daniil Emtsev, Dusan Svilarkovic, Songyou Peng", "title": "Dynamic Plane Convolutional Occupancy Networks", "comments": "To be presented at WACV 2021. Equal contribution between the first\n  three authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning-based 3D reconstruction using implicit neural representations has\nshown promising progress not only at the object level but also in more\ncomplicated scenes. In this paper, we propose Dynamic Plane Convolutional\nOccupancy Networks, a novel implicit representation pushing further the quality\nof 3D surface reconstruction. The input noisy point clouds are encoded into\nper-point features that are projected onto multiple 2D dynamic planes. A\nfully-connected network learns to predict plane parameters that best describe\nthe shapes of objects or scenes. To further exploit translational equivariance,\nconvolutional neural networks are applied to process the plane features. Our\nmethod shows superior performance in surface reconstruction from unoriented\npoint clouds in ShapeNet as well as an indoor scene dataset. Moreover, we also\nprovide interesting observations on the distribution of learned dynamic planes.\n", "versions": [{"version": "v1", "created": "Wed, 11 Nov 2020 14:24:52 GMT"}], "update_date": "2020-11-12", "authors_parsed": [["Lionar", "Stefan", ""], ["Emtsev", "Daniil", ""], ["Svilarkovic", "Dusan", ""], ["Peng", "Songyou", ""]]}, {"id": "2011.05822", "submitter": "Tobias Scheck", "authors": "Tobias Scheck, Adarsh Mallandur, Christian Wiede, Gangolf Hirtz", "title": "Where to drive: free space detection with one fisheye camera", "comments": "Accepted at International Conference on Machine Vision 2019 (ICMV\n  2019)", "journal-ref": "Proceedings Volume 11433, Twelfth International Conference on\n  Machine Vision (ICMV 2019); 114332V", "doi": "10.1117/12.2556380", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The development in the field of autonomous driving goes hand in hand with\never new developments in the field of image processing and machine learning\nmethods. In order to fully exploit the advantages of deep learning, it is\nnecessary to have sufficient labeled training data available. This is\nespecially not the case for omnidirectional fisheye cameras. As a solution, we\npropose in this paper to use synthetic training data based on Unity3D. A\nfive-pass algorithm is used to create a virtual fisheye camera. This synthetic\ntraining data is evaluated for the application of free space detection for\ndifferent deep learning network architectures. The results indicate that\nsynthetic fisheye images can be used in deep learning context.\n", "versions": [{"version": "v1", "created": "Wed, 11 Nov 2020 14:36:45 GMT"}], "update_date": "2020-11-12", "authors_parsed": [["Scheck", "Tobias", ""], ["Mallandur", "Adarsh", ""], ["Wiede", "Christian", ""], ["Hirtz", "Gangolf", ""]]}, {"id": "2011.05867", "submitter": "Yaxing Wang", "authors": "Yaxing Wang, Lu Yu, Joost van de Weijer", "title": "DeepI2I: Enabling Deep Hierarchical Image-to-Image Translation by\n  Transferring from GANs", "comments": "NeurIPS2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image-to-image translation has recently achieved remarkable results. But\ndespite current success, it suffers from inferior performance when translations\nbetween classes require large shape changes. We attribute this to the\nhigh-resolution bottlenecks which are used by current state-of-the-art\nimage-to-image methods. Therefore, in this work, we propose a novel deep\nhierarchical Image-to-Image Translation method, called DeepI2I. We learn a\nmodel by leveraging hierarchical features: (a) structural information contained\nin the shallow layers and (b) semantic information extracted from the deep\nlayers. To enable the training of deep I2I models on small datasets, we propose\na novel transfer learning method, that transfers knowledge from pre-trained\nGANs. Specifically, we leverage the discriminator of a pre-trained GANs (i.e.\nBigGAN or StyleGAN) to initialize both the encoder and the discriminator and\nthe pre-trained generator to initialize the generator of our model. Applying\nknowledge transfer leads to an alignment problem between the encoder and\ngenerator. We introduce an adaptor network to address this. On many-class\nimage-to-image translation on three datasets (Animal faces, Birds, and Foods)\nwe decrease mFID by at least 35% when compared to the state-of-the-art.\nFurthermore, we qualitatively and quantitatively demonstrate that transfer\nlearning significantly improves the performance of I2I systems, especially for\nsmall datasets. Finally, we are the first to perform I2I translations for\ndomains with over 100 classes.\n", "versions": [{"version": "v1", "created": "Wed, 11 Nov 2020 16:03:03 GMT"}], "update_date": "2020-11-12", "authors_parsed": [["Wang", "Yaxing", ""], ["Yu", "Lu", ""], ["van de Weijer", "Joost", ""]]}, {"id": "2011.05873", "submitter": "Giulio Gambardella", "authors": "Ussama Zahid, Giulio Gambardella, Nicholas J. Fraser, Michaela Blott,\n  Kees Vissers", "title": "FAT: Training Neural Networks for Reliable Inference Under Hardware\n  Faults", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks (DNNs) are state-of-the-art algorithms for multiple\napplications, spanning from image classification to speech recognition. While\nproviding excellent accuracy, they often have enormous compute and memory\nrequirements. As a result of this, quantized neural networks (QNNs) are\nincreasingly being adopted and deployed especially on embedded devices, thanks\nto their high accuracy, but also since they have significantly lower compute\nand memory requirements compared to their floating point equivalents. QNN\ndeployment is also being evaluated for safety-critical applications, such as\nautomotive, avionics, medical or industrial. These systems require functional\nsafety, guaranteeing failure-free behaviour even in the presence of hardware\nfaults. In general fault tolerance can be achieved by adding redundancy to the\nsystem, which further exacerbates the overall computational demands and makes\nit difficult to meet the power and performance requirements. In order to\ndecrease the hardware cost for achieving functional safety, it is vital to\nexplore domain-specific solutions which can exploit the inherent features of\nDNNs. In this work we present a novel methodology called fault-aware training\n(FAT), which includes error modeling during neural network (NN) training, to\nmake QNNs resilient to specific fault models on the device. Our experiments\nshow that by injecting faults in the convolutional layers during training,\nhighly accurate convolutional neural networks (CNNs) can be trained which\nexhibits much better error tolerance compared to the original. Furthermore, we\nshow that redundant systems which are built from QNNs trained with FAT achieve\nhigher worse-case accuracy at lower hardware cost. This has been validated for\nnumerous classification tasks including CIFAR10, GTSRB, SVHN and ImageNet.\n", "versions": [{"version": "v1", "created": "Wed, 11 Nov 2020 16:09:39 GMT"}], "update_date": "2020-11-12", "authors_parsed": [["Zahid", "Ussama", ""], ["Gambardella", "Giulio", ""], ["Fraser", "Nicholas J.", ""], ["Blott", "Michaela", ""], ["Vissers", "Kees", ""]]}, {"id": "2011.05895", "submitter": "Vinayaka R Kamath", "authors": "Vinayaka R Kamath, Vishal S, Varun M", "title": "Transferred Fusion Learning using Skipped Networks", "comments": "9 Pages, 7 figures, Conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Identification of an entity that is of interest is prominent in any\nintelligent system. The visual intelligence of the model is enhanced when the\ncapability of recognition is added. Several methods such as transfer learning\nand zero shot learning help to reuse the existing models or augment the\nexisting model to achieve improved performance at the task of object\nrecognition. Transferred fusion learning is one such mechanism that intends to\nuse the best of both worlds and build a model that is capable of outperforming\nthe models involved in the system. We propose a novel mechanism to amplify the\nprocess of transfer learning by introducing a student architecture where the\nnetworks learn from each other.\n", "versions": [{"version": "v1", "created": "Wed, 11 Nov 2020 16:41:55 GMT"}], "update_date": "2020-11-12", "authors_parsed": [["Kamath", "Vinayaka R", ""], ["S", "Vishal", ""], ["M", "Varun", ""]]}, {"id": "2011.05897", "submitter": "Daksha Yadav", "authors": "Daksha Yadav, Naman Kohli, Mayank Vatsa, Richa Singh, Afzel Noore", "title": "Age Gap Reducer-GAN for Recognizing Age-Separated Faces", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  In this paper, we propose a novel algorithm for matching faces with temporal\nvariations caused due to age progression. The proposed generative adversarial\nnetwork algorithm is a unified framework that combines facial age estimation\nand age-separated face verification. The key idea of this approach is to learn\nthe age variations across time by conditioning the input image on the subject's\ngender and the target age group to which the face needs to be progressed. The\nloss function accounts for reducing the age gap between the original image and\ngenerated face image as well as preserving the identity. Both visual fidelity\nand quantitative evaluations demonstrate the efficacy of the proposed\narchitecture on different facial age databases for age-separated face\nrecognition.\n", "versions": [{"version": "v1", "created": "Wed, 11 Nov 2020 16:43:32 GMT"}], "update_date": "2020-11-12", "authors_parsed": [["Yadav", "Daksha", ""], ["Kohli", "Naman", ""], ["Vatsa", "Mayank", ""], ["Singh", "Richa", ""], ["Noore", "Afzel", ""]]}, {"id": "2011.05940", "submitter": "Sri Jamiya S", "authors": "Sri Jamiya S, Esther Rani P", "title": "LittleYOLO-SPP: A Delicate Real-Time Vehicle Detection Algorithm", "comments": "18 pages, 8 Figures, 7 Tables", "journal-ref": "Optik - International Journal for Light and Electron optics Volume\n  225, 2021, 165818, ISSN 0030-4026", "doi": "10.1016/j.ijleo.2020.165818", "report-no": null, "categories": "cs.CV eess.IV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Vehicle detection in real-time is a challenging and important task. The\nexisting real-time vehicle detection lacks accuracy and speed. Real-time\nsystems must detect and locate vehicles during criminal activities like theft\nof vehicle and road traffic violations with high accuracy. Detection of\nvehicles in complex scenes with occlusion is also extremely difficult. In this\nstudy, a lightweight model of deep neural network LittleYOLO-SPP based on the\nYOLOv3-tiny network is proposed to detect vehicles effectively in real-time.\nThe YOLOv3-tiny object detection network is improved by modifying its feature\nextraction network to increase the speed and accuracy of vehicle detection. The\nproposed network incorporated Spatial pyramid pooling into the network, which\nconsists of different scales of pooling layers for concatenation of features to\nenhance network learning capability. The Mean square error (MSE) and\nGeneralized IoU (GIoU) loss function for bounding box regression is used to\nincrease the performance of the network. The network training includes\nvehicle-based classes from PASCAL VOC 2007,2012 and MS COCO 2014 datasets such\nas car, bus, and truck. LittleYOLO-SPP network detects the vehicle in real-time\nwith high accuracy regardless of video frame and weather conditions. The\nimproved network achieves a higher mAP of 77.44% on PASCAL VOC and 52.95% mAP\non MS COCO datasets.\n", "versions": [{"version": "v1", "created": "Wed, 11 Nov 2020 17:57:49 GMT"}], "update_date": "2020-11-12", "authors_parsed": [["S", "Sri Jamiya", ""], ["P", "Esther Rani", ""]]}, {"id": "2011.05970", "submitter": "Sudeep Dasari", "authors": "Sudeep Dasari, Abhinav Gupta", "title": "Transformers for One-Shot Visual Imitation", "comments": "For code and project video please check our website:\n  https://oneshotfeatures.github.io/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.RO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Humans are able to seamlessly visually imitate others, by inferring their\nintentions and using past experience to achieve the same end goal. In other\nwords, we can parse complex semantic knowledge from raw video and efficiently\ntranslate that into concrete motor control. Is it possible to give a robot this\nsame capability? Prior research in robot imitation learning has created agents\nwhich can acquire diverse skills from expert human operators. However,\nexpanding these techniques to work with a single positive example during test\ntime is still an open challenge. Apart from control, the difficulty stems from\nmismatches between the demonstrator and robot domains. For example, objects may\nbe placed in different locations (e.g. kitchen layouts are different in every\nhouse). Additionally, the demonstration may come from an agent with different\nmorphology and physical appearance (e.g. human), so one-to-one action\ncorrespondences are not available. This paper investigates techniques which\nallow robots to partially bridge these domain gaps, using their past\nexperience. A neural network is trained to mimic ground truth robot actions\ngiven context video from another agent, and must generalize to unseen task\ninstances when prompted with new videos during test time. We hypothesize that\nour policy representations must be both context driven and dynamics aware in\norder to perform these tasks. These assumptions are baked into the neural\nnetwork using the Transformers attention mechanism and a self-supervised\ninverse dynamics loss. Finally, we experimentally determine that our method\naccomplishes a $\\sim 2$x improvement in terms of task success rate over prior\nbaselines in a suite of one-shot manipulation tasks.\n", "versions": [{"version": "v1", "created": "Wed, 11 Nov 2020 18:41:07 GMT"}], "update_date": "2020-11-12", "authors_parsed": [["Dasari", "Sudeep", ""], ["Gupta", "Abhinav", ""]]}, {"id": "2011.05976", "submitter": "Rui Zhao", "authors": "Rui Zhao", "title": "The Vulnerability of the Neural Networks Against Adversarial Examples in\n  Deep Learning Algorithms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  With further development in the fields of computer vision, network security,\nnatural language processing and so on so forth, deep learning technology\ngradually exposed certain security risks. The existing deep learning algorithms\ncannot effectively describe the essential characteristics of data, making the\nalgorithm unable to give the correct result in the face of malicious input.\nBased on current security threats faced by deep learning, this paper introduces\nthe problem of adversarial examples in deep learning, sorts out the existing\nattack and defense methods of the black box and white box, and classifies them.\nIt briefly describes the application of some adversarial examples in different\nscenarios in recent years, compares several defense technologies of adversarial\nexamples, and finally summarizes the problems in this research field and\nprospects for its future development. This paper introduces the common white\nbox attack methods in detail, and further compares the similarities and\ndifferences between the attack of the black and white box. Correspondingly, the\nauthor also introduces the defense methods, and analyzes the performance of\nthese methods against the black and white box attack.\n", "versions": [{"version": "v1", "created": "Mon, 2 Nov 2020 04:41:08 GMT"}, {"version": "v2", "created": "Tue, 17 Nov 2020 12:57:38 GMT"}], "update_date": "2020-11-18", "authors_parsed": [["Zhao", "Rui", ""]]}, {"id": "2011.05980", "submitter": "Lin Cheng", "authors": "Lin Cheng, Zijiang Yang", "title": "GRCNN: Graph Recognition Convolutional Neural Network for Synthesizing\n  Programs from Flow Charts", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Program synthesis is the task to automatically generate programs based on\nuser specification. In this paper, we present a framework that synthesizes\nprograms from flow charts that serve as accurate and intuitive specifications.\nIn order doing so, we propose a deep neural network called GRCNN that\nrecognizes graph structure from its image. GRCNN is trained end-to-end, which\ncan predict edge and node information of the flow chart simultaneously.\nExperiments show that the accuracy rate to synthesize a program is 66.4%, and\nthe accuracy rates to recognize edge and nodes are 94.1% and 67.9%,\nrespectively. On average, it takes about 60 milliseconds to synthesize a\nprogram.\n", "versions": [{"version": "v1", "created": "Wed, 11 Nov 2020 18:52:25 GMT"}], "update_date": "2020-11-12", "authors_parsed": [["Cheng", "Lin", ""], ["Yang", "Zijiang", ""]]}, {"id": "2011.06033", "submitter": "Andr\\'e Pedersen", "authors": "Andr\\'e Pedersen, Marit Valla, Anna M. Bofin, Javier P\\'erez de\n  Frutos, Ingerid Reinertsen and Erik Smistad", "title": "FastPathology: An open-source platform for deep learning-based research\n  and decision support in digital pathology", "comments": "12 pages, 4 figures, submitted to IEEE Access", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep convolutional neural networks (CNNs) are the current state-of-the-art\nfor digital analysis of histopathological images. The large size of whole-slide\nmicroscopy images (WSIs) requires advanced memory handling to read, display and\nprocess these images. There are several open-source platforms for working with\nWSIs, but few support deployment of CNN models. These applications use\nthird-party solutions for inference, making them less user-friendly and\nunsuitable for high-performance image analysis. To make deployment of CNNs\nuser-friendly and feasible on low-end machines, we have developed a new\nplatform, FastPathology, using the FAST framework and C++. It minimizes memory\nusage for reading and processing WSIs, deployment of CNN models, and real-time\ninteractive visualization of results. Runtime experiments were conducted on\nfour different use cases, using different architectures, inference engines,\nhardware configurations and operating systems. Memory usage for reading,\nvisualizing, zooming and panning a WSI were measured, using FastPathology and\nthree existing platforms. FastPathology performed similarly in terms of memory\nto the other C++ based application, while using considerably less than the two\nJava-based platforms. The choice of neural network model, inference engine,\nhardware and processors influenced runtime considerably. Thus, FastPathology\nincludes all steps needed for efficient visualization and processing of WSIs in\na single application, including inference of CNNs with real-time display of the\nresults. Source code, binary releases and test data can be found online on\nGitHub at https://github.com/SINTEFMedtek/FAST-Pathology/.\n", "versions": [{"version": "v1", "created": "Wed, 11 Nov 2020 19:35:31 GMT"}], "update_date": "2020-11-13", "authors_parsed": [["Pedersen", "Andr\u00e9", ""], ["Valla", "Marit", ""], ["Bofin", "Anna M.", ""], ["de Frutos", "Javier P\u00e9rez", ""], ["Reinertsen", "Ingerid", ""], ["Smistad", "Erik", ""]]}, {"id": "2011.06037", "submitter": "Nadine Behrmann", "authors": "Nadine Behrmann and Juergen Gall and Mehdi Noroozi", "title": "Unsupervised Video Representation Learning by Bidirectional Feature\n  Prediction", "comments": "Accepted at WACV 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a novel method for self-supervised video representation\nlearning via feature prediction. In contrast to the previous methods that focus\non future feature prediction, we argue that a supervisory signal arising from\nunobserved past frames is complementary to one that originates from the future\nframes. The rationale behind our method is to encourage the network to explore\nthe temporal structure of videos by distinguishing between future and past\ngiven present observations. We train our model in a contrastive learning\nframework, where joint encoding of future and past provides us with a\ncomprehensive set of temporal hard negatives via swapping. We empirically show\nthat utilizing both signals enriches the learned representations for the\ndownstream task of action recognition. It outperforms independent prediction of\nfuture and past.\n", "versions": [{"version": "v1", "created": "Wed, 11 Nov 2020 19:42:31 GMT"}], "update_date": "2020-11-13", "authors_parsed": [["Behrmann", "Nadine", ""], ["Gall", "Juergen", ""], ["Noroozi", "Mehdi", ""]]}, {"id": "2011.06089", "submitter": "Li Duan", "authors": "Li Duan, Gerardo Aragon-Camarasa", "title": "Continuous Perception for Classifying Shapes and Weights of Garmentsfor\n  Robotic Vision Applications", "comments": "Submitted to ICRA 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present an approach to continuous perception for robotic laundry tasks.\nOur assumption is that the visual prediction of a garment's shapes and weights\nis possible via a neural network that learns the dynamic changes of garments\nfrom video sequences. Continuous perception is leveraged during training by\ninputting consecutive frames, of which the network learns how a garment\ndeforms. To evaluate our hypothesis, we captured a dataset of 40K RGB and 40K\ndepth video sequences while a garment is being manipulated. We also conducted\nablation studies to understand whether the neural network learns the physical\nand dynamic properties of garments. Our findings suggest that a modified\nAlexNet-LSTM architecture has the best classification performance for the\ngarment's shape and weights. To further provide evidence that continuous\nperception facilitates the prediction of the garment's shapes and weights, we\nevaluated our network on unseen video sequences and computed the 'Moving\nAverage' over a sequence of predictions. We found that our network has a\nclassification accuracy of 48% and 60% for shapes and weights of garments,\nrespectively.\n", "versions": [{"version": "v1", "created": "Wed, 11 Nov 2020 21:55:58 GMT"}], "update_date": "2020-11-13", "authors_parsed": [["Duan", "Li", ""], ["Aragon-Camarasa", "Gerardo", ""]]}, {"id": "2011.06123", "submitter": "Loris Nanni", "authors": "Loris Nanni, Eugenio De Luca, Marco Ludovico Facin, Gianluca Maguolo", "title": "Deep learning and hand-crafted features for virus image classification", "comments": null, "journal-ref": "Journal of Imaging 2020, 6(12), 143", "doi": "10.3390/jimaging6120143", "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this work, we present an ensemble of descriptors for the classification of\ntransmission electron microscopy images of viruses. We propose to combine\nhandcrafted and deep learning approaches for virus image classification. The\nset of handcrafted is mainly based on Local Binary Pattern variants, for each\ndescriptor a different Support Vector Machine is trained, then the set of\nclassifiers is combined by sum rule. The deep learning approach is a\ndensenet201 pretrained on ImageNet and then tuned in the virus dataset, the net\nis used as features extractor for feeding another Support Vector Machine, in\nparticular the last average pooling layer is used as feature extractor.\nFinally, classifiers trained on handcrafted features and classifier trained on\ndeep learning features are combined by sum rule. The proposed fusion strongly\nboosts the performance obtained by each stand-alone approach, obtaining state\nof the art performance.\n", "versions": [{"version": "v1", "created": "Wed, 11 Nov 2020 23:46:16 GMT"}, {"version": "v2", "created": "Sun, 13 Dec 2020 00:29:22 GMT"}], "update_date": "2020-12-25", "authors_parsed": [["Nanni", "Loris", ""], ["De Luca", "Eugenio", ""], ["Facin", "Marco Ludovico", ""], ["Maguolo", "Gianluca", ""]]}, {"id": "2011.06129", "submitter": "Anirudh Joshi", "authors": "Pranav Rajpurkar, Anirudh Joshi, Anuj Pareek, Jeremy Irvin, Andrew Y.\n  Ng, Matthew Lungren", "title": "CheXphotogenic: Generalization of Deep Learning Models for Chest X-ray\n  Interpretation to Photos of Chest X-rays", "comments": "Machine Learning for Health (ML4H) at NeurIPS 2020 - Extended\n  Abstract", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The use of smartphones to take photographs of chest x-rays represents an\nappealing solution for scaled deployment of deep learning models for chest\nx-ray interpretation. However, the performance of chest x-ray algorithms on\nphotos of chest x-rays has not been thoroughly investigated. In this study, we\nmeasured the diagnostic performance for 8 different chest x-ray models when\napplied to photos of chest x-rays. All models were developed by different\ngroups and submitted to the CheXpert challenge, and re-applied to smartphone\nphotos of x-rays in the CheXphoto dataset without further tuning. We found that\nseveral models had a drop in performance when applied to photos of chest\nx-rays, but even with this drop, some models still performed comparably to\nradiologists. Further investigation could be directed towards understanding how\ndifferent model training procedures may affect model generalization to photos\nof chest x-rays.\n", "versions": [{"version": "v1", "created": "Thu, 12 Nov 2020 00:16:51 GMT"}], "update_date": "2020-11-13", "authors_parsed": [["Rajpurkar", "Pranav", ""], ["Joshi", "Anirudh", ""], ["Pareek", "Anuj", ""], ["Irvin", "Jeremy", ""], ["Ng", "Andrew Y.", ""], ["Lungren", "Matthew", ""]]}, {"id": "2011.06133", "submitter": "Yulia Gryaditskaya Dr", "authors": "Yue Zhong, Yulia Gryaditskaya, Honggang Zhang, Yi-Zhe Song", "title": "Deep Sketch-Based Modeling: Tips and Tricks", "comments": null, "journal-ref": null, "doi": "10.1109/3DV50981.2020.00064", "report-no": null, "categories": "cs.CV cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep image-based modeling received lots of attention in recent years, yet the\nparallel problem of sketch-based modeling has only been briefly studied, often\nas a potential application. In this work, for the first time, we identify the\nmain differences between sketch and image inputs: (i) style variance, (ii)\nimprecise perspective, and (iii) sparsity. We discuss why each of these\ndifferences can pose a challenge, and even make a certain class of image-based\nmethods inapplicable. We study alternative solutions to address each of the\ndifference. By doing so, we drive out a few important insights: (i) sparsity\ncommonly results in an incorrect prediction of foreground versus background,\n(ii) diversity of human styles, if not taken into account, can lead to very\npoor generalization properties, and finally (iii) unless a dedicated sketching\ninterface is used, one can not expect sketches to match a perspective of a\nfixed viewpoint. Finally, we compare a set of representative deep single-image\nmodeling solutions and show how their performance can be improved to tackle\nsketch input by taking into consideration the identified critical differences.\n", "versions": [{"version": "v1", "created": "Thu, 12 Nov 2020 00:34:08 GMT"}, {"version": "v2", "created": "Tue, 17 Nov 2020 13:26:57 GMT"}, {"version": "v3", "created": "Mon, 26 Apr 2021 09:23:32 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Zhong", "Yue", ""], ["Gryaditskaya", "Yulia", ""], ["Zhang", "Honggang", ""], ["Song", "Yi-Zhe", ""]]}, {"id": "2011.06165", "submitter": "Sean Segal", "authors": "Sean Segal, Eric Kee, Wenjie Luo, Abbas Sadat, Ersin Yumer, Raquel\n  Urtasun", "title": "Universal Embeddings for Spatio-Temporal Tagging of Self-Driving Logs", "comments": "CoRL 2020 (Oral)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we tackle the problem of spatio-temporal tagging of\nself-driving scenes from raw sensor data. Our approach learns a universal\nembedding for all tags, enabling efficient tagging of many attributes and\nfaster learning of new attributes with limited data. Importantly, the embedding\nis spatio-temporally aware, allowing the model to naturally output\nspatio-temporal tag values. Values can then be pooled over arbitrary regions,\nin order to, for example, compute the pedestrian density in front of the SDV,\nor determine if a car is blocking another car at a 4-way intersection. We\ndemonstrate the effectiveness of our approach on a new large scale self-driving\ndataset, SDVScenes, containing 15 attributes relating to vehicle and pedestrian\ndensity, the actions of each actor, the speed of each actor, interactions\nbetween actors, and the topology of the road map.\n", "versions": [{"version": "v1", "created": "Thu, 12 Nov 2020 02:18:16 GMT"}], "update_date": "2020-11-13", "authors_parsed": [["Segal", "Sean", ""], ["Kee", "Eric", ""], ["Luo", "Wenjie", ""], ["Sadat", "Abbas", ""], ["Yumer", "Ersin", ""], ["Urtasun", "Raquel", ""]]}, {"id": "2011.06182", "submitter": "Mingsheng Long", "authors": "Jincheng Zhong, Ximei Wang, Zhi Kou, Jianmin Wang, Mingsheng Long", "title": "Bi-tuning of Pre-trained Representations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  It is common within the deep learning community to first pre-train a deep\nneural network from a large-scale dataset and then fine-tune the pre-trained\nmodel to a specific downstream task. Recently, both supervised and unsupervised\npre-training approaches to learning representations have achieved remarkable\nadvances, which exploit the discriminative knowledge of labels and the\nintrinsic structure of data, respectively. It follows natural intuition that\nboth discriminative knowledge and intrinsic structure of the downstream task\ncan be useful for fine-tuning, however, existing fine-tuning methods mainly\nleverage the former and discard the latter. A question arises: How to fully\nexplore the intrinsic structure of data for boosting fine-tuning? In this\npaper, we propose Bi-tuning, a general learning framework to fine-tuning both\nsupervised and unsupervised pre-trained representations to downstream tasks.\nBi-tuning generalizes the vanilla fine-tuning by integrating two heads upon the\nbackbone of pre-trained representations: a classifier head with an improved\ncontrastive cross-entropy loss to better leverage the label information in an\ninstance-contrast way, and a projector head with a newly-designed categorical\ncontrastive learning loss to fully exploit the intrinsic structure of data in a\ncategory-consistent way. Comprehensive experiments confirm that Bi-tuning\nachieves state-of-the-art results for fine-tuning tasks of both supervised and\nunsupervised pre-trained models by large margins (e.g. 10.7\\% absolute rise in\naccuracy on CUB in low-data regime).\n", "versions": [{"version": "v1", "created": "Thu, 12 Nov 2020 03:32:25 GMT"}], "update_date": "2020-11-13", "authors_parsed": [["Zhong", "Jincheng", ""], ["Wang", "Ximei", ""], ["Kou", "Zhi", ""], ["Wang", "Jianmin", ""], ["Long", "Mingsheng", ""]]}, {"id": "2011.06190", "submitter": "Dongseok Shim", "authors": "Dongseok Shim and H. Jin Kim", "title": "Gaussian RAM: Lightweight Image Classification via Stochastic\n  Retina-Inspired Glimpse and Reinforcement Learning", "comments": "ICCAS 2020 Accepted and Student Best Paper Finalist", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Previous studies on image classification have mainly focused on the\nperformance of the networks, not on real-time operation or model compression.\nWe propose a Gaussian Deep Recurrent visual Attention Model (GDRAM)- a\nreinforcement learning based lightweight deep neural network for large scale\nimage classification that outperforms the conventional CNN (Convolutional\nNeural Network) which uses the entire image as input. Highly inspired by the\nbiological visual recognition process, our model mimics the stochastic location\nof the retina with Gaussian distribution. We evaluate the model on Large\ncluttered MNIST, Large CIFAR-10 and Large CIFAR-100 datasets which are resized\nto 128 in both width and height.\n", "versions": [{"version": "v1", "created": "Thu, 12 Nov 2020 04:27:06 GMT"}], "update_date": "2020-11-13", "authors_parsed": [["Shim", "Dongseok", ""], ["Kim", "H. Jin", ""]]}, {"id": "2011.06207", "submitter": "Tharindu Fernando", "authors": "Theekshana Dissanayake, Tharindu Fernando, Simon Denman, Houman\n  Ghaemmaghami, Sridha Sridharan, Clinton Fookes", "title": "Domain Generalization in Biosignal Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Objective: When training machine learning models, we often assume that the\ntraining data and evaluation data are sampled from the same distribution.\nHowever, this assumption is violated when the model is evaluated on another\nunseen but similar database, even if that database contains the same classes.\nThis problem is caused by domain-shift and can be solved using two approaches:\ndomain adaptation and domain generalization. Simply, domain adaptation methods\ncan access data from unseen domains during training; whereas in domain\ngeneralization, the unseen data is not available during training. Hence, domain\ngeneralization concerns models that perform well on inaccessible,\ndomain-shifted data. Method: Our proposed domain generalization method\nrepresents an unseen domain using a set of known basis domains, afterwhich we\nclassify the unseen domain using classifier fusion. To demonstrate our system,\nwe employ a collection of heart sound databases that contain normal and\nabnormal sounds (classes). Results: Our proposed classifier fusion method\nachieves accuracy gains of up to 16% for four completely unseen domains.\nConclusion: Recognizing the complexity induced by the inherent temporal nature\nof biosignal data, the two-stage method proposed in this study is able to\neffectively simplify the whole process of domain generalization while\ndemonstrating good results on unseen domains and the adopted basis domains.\nSignificance: To our best knowledge, this is the first study that investigates\ndomain generalization for biosignal data. Our proposed learning strategy can be\nused to effectively learn domain-relevant features while being aware of the\nclass differences in the data.\n", "versions": [{"version": "v1", "created": "Thu, 12 Nov 2020 05:15:46 GMT"}], "update_date": "2020-11-13", "authors_parsed": [["Dissanayake", "Theekshana", ""], ["Fernando", "Tharindu", ""], ["Denman", "Simon", ""], ["Ghaemmaghami", "Houman", ""], ["Sridharan", "Sridha", ""], ["Fookes", "Clinton", ""]]}, {"id": "2011.06210", "submitter": "Sulaiman Aburakhia", "authors": "Sulaiman Aburakhia, Tareq Tayeh, Ryan Myers, Abdallah Shami", "title": "A Transfer Learning Framework for Anomaly Detection Using Model of\n  Normality", "comments": "7 pages, 4 figures, 2 tables, conference: The 11th Annual IEEE\n  Information Technology, Electronics and Mobile Communication Conference \"IEEE\n  IEMCON\", Vancouver, Canada, November 2020. IEEE IEMCON'20' best paper award\n  in the category of Industrial Automation and Control Systems Technology", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Convolutional Neural Network (CNN) techniques have proven to be very useful\nin image-based anomaly detection applications. CNN can be used as deep features\nextractor where other anomaly detection techniques are applied on these\nfeatures. For this scenario, using transfer learning is common since pretrained\nmodels provide deep feature representations that are useful for anomaly\ndetection tasks. Consequentially, anomaly can be detected by applying similarly\nmeasure between extracted features and a defined model of normality. A key\nfactor in such approaches is the decision threshold used for detecting anomaly.\nWhile most of the proposed methods focus on the approach itself, slight\nattention has been paid to address decision threshold settings. In this paper,\nwe tackle this problem and propose a welldefined method to set the\nworking-point decision threshold that improves detection accuracy. We introduce\na transfer learning framework for anomaly detection based on similarity measure\nwith a Model of Normality (MoN) and show that with the proposed threshold\nsettings, a significant performance improvement can be achieved. Moreover, the\nframework has low complexity with relaxed computational requirements.\n", "versions": [{"version": "v1", "created": "Thu, 12 Nov 2020 05:26:32 GMT"}], "update_date": "2020-11-13", "authors_parsed": [["Aburakhia", "Sulaiman", ""], ["Tayeh", "Tareq", ""], ["Myers", "Ryan", ""], ["Shami", "Abdallah", ""]]}, {"id": "2011.06214", "submitter": "Zhe Xu", "authors": "Zhe Xu, Jiangpeng Yan, Jie Luo, William Wells, Xiu Li, Jayender\n  Jagadeesan", "title": "Unimodal Cyclic Regularization for Training Multimodal Image\n  Registration Networks", "comments": "5 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The loss function of an unsupervised multimodal image registration framework\nhas two terms, i.e., a metric for similarity measure and regularization. In the\ndeep learning era, researchers proposed many approaches to automatically learn\nthe similarity metric, which has been shown effective in improving registration\nperformance. However, for the regularization term, most existing multimodal\nregistration approaches still use a hand-crafted formula to impose artificial\nproperties on the estimated deformation field. In this work, we propose a\nunimodal cyclic regularization training pipeline, which learns task-specific\nprior knowledge from simpler unimodal registration, to constrain the\ndeformation field of multimodal registration. In the experiment of abdominal\nCT-MR registration, the proposed method yields better results over conventional\nregularization methods, especially for severely deformed local regions.\n", "versions": [{"version": "v1", "created": "Thu, 12 Nov 2020 05:37:30 GMT"}], "update_date": "2020-11-13", "authors_parsed": [["Xu", "Zhe", ""], ["Yan", "Jiangpeng", ""], ["Luo", "Jie", ""], ["Wells", "William", ""], ["Li", "Xiu", ""], ["Jagadeesan", "Jayender", ""]]}, {"id": "2011.06216", "submitter": "Zhe Xu", "authors": "Zhe Xu, Jiangpeng Yan, Jie Luo, Xiu Li, Jayender Jagadeesan", "title": "Unsupervised Multimodal Image Registration with Adaptative Gradient\n  Guidance", "comments": "5 pages, under review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multimodal image registration (MIR) is a fundamental procedure in many\nimage-guided therapies. Recently, unsupervised learning-based methods have\ndemonstrated promising performance over accuracy and efficiency in deformable\nimage registration. However, the estimated deformation fields of the existing\nmethods fully rely on the to-be-registered image pair. It is difficult for the\nnetworks to be aware of the mismatched boundaries, resulting in unsatisfactory\norgan boundary alignment. In this paper, we propose a novel multimodal\nregistration framework, which leverages the deformation fields estimated from\nboth: (i) the original to-be-registered image pair, (ii) their corresponding\ngradient intensity maps, and adaptively fuses them with the proposed gated\nfusion module. With the help of auxiliary gradient-space guidance, the network\ncan concentrate more on the spatial relationship of the organ boundary.\nExperimental results on two clinically acquired CT-MRI datasets demonstrate the\neffectiveness of our proposed approach.\n", "versions": [{"version": "v1", "created": "Thu, 12 Nov 2020 05:47:20 GMT"}], "update_date": "2020-11-13", "authors_parsed": [["Xu", "Zhe", ""], ["Yan", "Jiangpeng", ""], ["Luo", "Jie", ""], ["Li", "Xiu", ""], ["Jagadeesan", "Jayender", ""]]}, {"id": "2011.06219", "submitter": "Qianli Feng", "authors": "Stuart Synakowski, Qianli Feng, Aleix Martinez", "title": "Adding Knowledge to Unsupervised Algorithms for the Recognition of\n  Intent", "comments": "This is a pre-print of an article published in International Journal\n  of Computer Vision. The final authenticated version is available online at:\n  https://doi.org/10.1007/s11263-020-01404-0", "journal-ref": null, "doi": "10.1007/s11263-020-01404-0", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Computer vision algorithms performance are near or superior to humans in the\nvisual problems including object recognition (especially those of fine-grained\ncategories), segmentation, and 3D object reconstruction from 2D views. Humans\nare, however, capable of higher-level image analyses. A clear example,\ninvolving theory of mind, is our ability to determine whether a perceived\nbehavior or action was performed intentionally or not. In this paper, we derive\nan algorithm that can infer whether the behavior of an agent in a scene is\nintentional or unintentional based on its 3D kinematics, using the knowledge of\nself-propelled motion, Newtonian motion and their relationship. We show how the\naddition of this basic knowledge leads to a simple, unsupervised algorithm. To\ntest the derived algorithm, we constructed three dedicated datasets from\nabstract geometric animation to realistic videos of agents performing\nintentional and non-intentional actions. Experiments on these datasets show\nthat our algorithm can recognize whether an action is intentional or not, even\nwithout training data. The performance is comparable to various supervised\nbaselines quantitatively, with sensible intentionality segmentation\nqualitatively.\n", "versions": [{"version": "v1", "created": "Thu, 12 Nov 2020 05:57:09 GMT"}], "update_date": "2020-11-13", "authors_parsed": [["Synakowski", "Stuart", ""], ["Feng", "Qianli", ""], ["Martinez", "Aleix", ""]]}, {"id": "2011.06224", "submitter": "Kazuma Kobayashi", "authors": "Kazuma Kobayashi, Ryuichiro Hataya, Yusuke Kurose, Tatsuya Harada,\n  Ryuji Hamamoto", "title": "Decomposing Normal and Abnormal Features of Medical Images for\n  Content-based Image Retrieval", "comments": "Machine Learning for Health (ML4H) at NeurIPS 2020 - Extended\n  Abstract", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Medical images can be decomposed into normal and abnormal features, which is\nconsidered as the compositionality. Based on this idea, we propose an\nencoder-decoder network to decompose a medical image into two discrete latent\ncodes: a normal anatomy code and an abnormal anatomy code. Using these latent\ncodes, we demonstrate a similarity retrieval by focusing on either normal or\nabnormal features of medical images.\n", "versions": [{"version": "v1", "created": "Thu, 12 Nov 2020 06:25:49 GMT"}], "update_date": "2020-11-13", "authors_parsed": [["Kobayashi", "Kazuma", ""], ["Hataya", "Ryuichiro", ""], ["Kurose", "Yusuke", ""], ["Harada", "Tatsuya", ""], ["Hamamoto", "Ryuji", ""]]}, {"id": "2011.06225", "submitter": "Moloud Abdar", "authors": "Moloud Abdar, Farhad Pourpanah, Sadiq Hussain, Dana Rezazadegan, Li\n  Liu, Mohammad Ghavamzadeh, Paul Fieguth, Xiaochun Cao, Abbas Khosravi, U\n  Rajendra Acharya, Vladimir Makarenkov, Saeid Nahavandi", "title": "A Review of Uncertainty Quantification in Deep Learning: Techniques,\n  Applications and Challenges", "comments": null, "journal-ref": "2021", "doi": "10.1016/j.inffus.2021.05.008", "report-no": "INFFUS_1411]", "categories": "cs.LG cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Uncertainty quantification (UQ) plays a pivotal role in reduction of\nuncertainties during both optimization and decision making processes. It can be\napplied to solve a variety of real-world applications in science and\nengineering. Bayesian approximation and ensemble learning techniques are two\nmost widely-used UQ methods in the literature. In this regard, researchers have\nproposed different UQ methods and examined their performance in a variety of\napplications such as computer vision (e.g., self-driving cars and object\ndetection), image processing (e.g., image restoration), medical image analysis\n(e.g., medical image classification and segmentation), natural language\nprocessing (e.g., text classification, social media texts and recidivism\nrisk-scoring), bioinformatics, etc. This study reviews recent advances in UQ\nmethods used in deep learning. Moreover, we also investigate the application of\nthese methods in reinforcement learning (RL). Then, we outline a few important\napplications of UQ methods. Finally, we briefly highlight the fundamental\nresearch challenges faced by UQ methods and discuss the future research\ndirections in this field.\n", "versions": [{"version": "v1", "created": "Thu, 12 Nov 2020 06:41:05 GMT"}, {"version": "v2", "created": "Mon, 16 Nov 2020 13:07:02 GMT"}, {"version": "v3", "created": "Tue, 17 Nov 2020 02:58:51 GMT"}, {"version": "v4", "created": "Wed, 6 Jan 2021 01:58:12 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Abdar", "Moloud", ""], ["Pourpanah", "Farhad", ""], ["Hussain", "Sadiq", ""], ["Rezazadegan", "Dana", ""], ["Liu", "Li", ""], ["Ghavamzadeh", "Mohammad", ""], ["Fieguth", "Paul", ""], ["Cao", "Xiaochun", ""], ["Khosravi", "Abbas", ""], ["Acharya", "U Rajendra", ""], ["Makarenkov", "Vladimir", ""], ["Nahavandi", "Saeid", ""]]}, {"id": "2011.06228", "submitter": "JiangTao Kong", "authors": "Jiangtao Kong, Yu Cheng, Benjia Zhou, Kai Li, Junliang Xing", "title": "DSAM: A Distance Shrinking with Angular Marginalizing Loss for High\n  Performance Vehicle Re-identificatio", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Vehicle Re-identification (ReID) is an important yet challenging problem in\ncomputer vision. Compared to other visual objects like faces and persons,\nvehicles simultaneously exhibit much larger intraclass viewpoint variations and\ninterclass visual similarities, making most exiting loss functions designed for\nface recognition and person ReID unsuitable for vehicle ReID. To obtain a\nhigh-performance vehicle ReID model, we present a novel Distance Shrinking with\nAngular Marginalizing (DSAM) loss function to perform hybrid learning in both\nthe Original Feature Space (OFS) and the Feature Angular Space (FAS) using the\nlocal verification and the global identification information. Specifically, it\nshrinks the distance between samples of the same class locally in the Original\nFeature Space while keeps samples of different classes far away in the Feature\nAngular Space. The shrinking and marginalizing operations are performed during\neach iteration of the training process and are suitable for different SoftMax\nbased loss functions. We evaluate the DSAM loss function on three large vehicle\nReID datasets with detailed analyses and extensive comparisons with many\ncompeting vehicle ReID methods. Experimental results show that our DSAM loss\nenhances the SoftMax loss by a large margin on the PKU-VD1-Large dataset:\n10.41% for mAP, 5.29% for cmc1, and 4.60% for cmc5. Moreover, the mAP is\nincreased by 9.34% on the PKU-VehicleID dataset and 8.73% on the VeRi-776\ndataset. Source code will be released to facilitate further studies in this\nresearch direction.\n", "versions": [{"version": "v1", "created": "Thu, 12 Nov 2020 06:48:31 GMT"}, {"version": "v2", "created": "Wed, 25 Nov 2020 16:52:02 GMT"}], "update_date": "2020-11-26", "authors_parsed": [["Kong", "Jiangtao", ""], ["Cheng", "Yu", ""], ["Zhou", "Benjia", ""], ["Li", "Kai", ""], ["Xing", "Junliang", ""]]}, {"id": "2011.06231", "submitter": "Wenting Tang", "authors": "Wenting Tang, Xingxing Wei, Bo Li", "title": "Automated Model Compression by Jointly Applied Pruning and Quantization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the traditional deep compression framework, iteratively performing network\npruning and quantization can reduce the model size and computation cost to meet\nthe deployment requirements. However, such a step-wise application of pruning\nand quantization may lead to suboptimal solutions and unnecessary time\nconsumption. In this paper, we tackle this issue by integrating network pruning\nand quantization as a unified joint compression problem and then use AutoML to\nautomatically solve it. We find the pruning process can be regarded as the\nchannel-wise quantization with 0 bit. Thus, the separate two-step pruning and\nquantization can be simplified as the one-step quantization with mixed\nprecision. This unification not only simplifies the compression pipeline but\nalso avoids the compression divergence. To implement this idea, we propose the\nautomated model compression by jointly applied pruning and quantization (AJPQ).\nAJPQ is designed with a hierarchical architecture: the layer controller\ncontrols the layer sparsity, and the channel controller decides the bit-width\nfor each kernel. Following the same importance criterion, the layer controller\nand the channel controller collaboratively decide the compression strategy.\nWith the help of reinforcement learning, our one-step compression is\nautomatically achieved. Compared with the state-of-the-art automated\ncompression methods, our method obtains a better accuracy while reducing the\nstorage considerably. For fixed precision quantization, AJPQ can reduce more\nthan five times model size and two times computation with a slight performance\nincrease for Skynet in remote sensing object detection. When mixed-precision is\nallowed, AJPQ can reduce five times model size with only 1.06% top-5 accuracy\ndecline for MobileNet in the classification task.\n", "versions": [{"version": "v1", "created": "Thu, 12 Nov 2020 07:06:29 GMT"}], "update_date": "2020-11-13", "authors_parsed": [["Tang", "Wenting", ""], ["Wei", "Xingxing", ""], ["Li", "Bo", ""]]}, {"id": "2011.06243", "submitter": "Andreas Doering", "authors": "Andreas Doering and Di Chen and Shanshan Zhang and Bernt Schiele and\n  Juergen Gall", "title": "PoseTrackReID: Dataset Description", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current datasets for video-based person re-identification (re-ID) do not\ninclude structural knowledge in form of human pose annotations for the persons\nof interest. Nonetheless, pose information is very helpful to disentangle\nuseful feature information from background or occlusion noise. Especially\nreal-world scenarios, such as surveillance, contain a lot of occlusions in\nhuman crowds or by obstacles. On the other hand, video-based person re-ID can\nbenefit other tasks such as multi-person pose tracking in terms of robust\nfeature matching. For that reason, we present PoseTrackReID, a large-scale\ndataset for multi-person pose tracking and video-based person re-ID. With\nPoseTrackReID, we want to bridge the gap between person re-ID and multi-person\npose tracking. Additionally, this dataset provides a good benchmark for current\nstate-of-the-art methods on multi-frame person re-ID.\n", "versions": [{"version": "v1", "created": "Thu, 12 Nov 2020 07:44:25 GMT"}], "update_date": "2020-11-13", "authors_parsed": [["Doering", "Andreas", ""], ["Chen", "Di", ""], ["Zhang", "Shanshan", ""], ["Schiele", "Bernt", ""], ["Gall", "Juergen", ""]]}, {"id": "2011.06246", "submitter": "Shuai Han", "authors": "Chengshuai Li, Shuai Han, Jianping Xing", "title": "VCE: Variational Convertor-Encoder for One-Shot Generalization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Variational Convertor-Encoder (VCE) converts an image to various styles; we\npresent this novel architecture for the problem of one-shot generalization and\nits transfer to new tasks not seen before without additional training. We also\nimprove the performance of variational auto-encoder (VAE) to filter those\nblurred points using a novel algorithm proposed by us, namely large margin VAE\n(LMVAE). Two samples with the same property are input to the encoder, and then\na convertor is required to processes one of them from the noisy outputs of the\nencoder; finally, the noise represents a variety of transformation rules and is\nused to convert new images. The algorithm that combines and improves the\ncondition variational auto-encoder (CVAE) and introspective VAE, we propose\nthis new framework aim to transform graphics instead of generating them; it is\nused for the one-shot generative process. No sequential inference algorithmic\nis needed in training. Compared to recent Omniglot datasets, the results show\nthat our model produces more realistic and diverse images.\n", "versions": [{"version": "v1", "created": "Thu, 12 Nov 2020 07:58:14 GMT"}], "update_date": "2020-11-13", "authors_parsed": [["Li", "Chengshuai", ""], ["Han", "Shuai", ""], ["Xing", "Jianping", ""]]}, {"id": "2011.06252", "submitter": "Md Jahidul Islam", "authors": "Md Jahidul Islam, Ruobing Wang, Karin de Langis and Junaed Sattar", "title": "SVAM: Saliency-guided Visual Attention Modeling by Autonomous Underwater\n  Robots", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a holistic approach to saliency-guided visual attention\nmodeling (SVAM) for use by autonomous underwater robots. Our proposed model,\nnamed SVAM-Net, integrates deep visual features at various scales and semantics\nfor effective salient object detection (SOD) in natural underwater images. The\nSVAM-Net architecture is configured in a unique way to jointly accommodate\nbottom-up and top-down learning within two separate branches of the network\nwhile sharing the same encoding layers. We design dedicated spatial attention\nmodules (SAMs) along these learning pathways to exploit the coarse-level and\nfine-level semantic features for SOD at four stages of abstractions. The\nbottom-up branch performs a rough yet reasonably accurate saliency estimation\nat a fast rate, whereas the deeper top-down branch incorporates a residual\nrefinement module (RRM) that provides fine-grained localization of the salient\nobjects. Extensive performance evaluation of SVAM-Net on benchmark datasets\nclearly demonstrates its effectiveness for underwater SOD. We also validate its\ngeneralization performance by several ocean trials' data that include test\nimages of diverse underwater scenes and waterbodies, and also images with\nunseen natural objects. Moreover, we analyze its computational feasibility for\nrobotic deployments and demonstrate its utility in several important use cases\nof visual attention modeling.\n", "versions": [{"version": "v1", "created": "Thu, 12 Nov 2020 08:17:21 GMT"}], "update_date": "2020-11-13", "authors_parsed": [["Islam", "Md Jahidul", ""], ["Wang", "Ruobing", ""], ["de Langis", "Karin", ""], ["Sattar", "Junaed", ""]]}, {"id": "2011.06259", "submitter": "Adrian Bojko", "authors": "Adrian Bojko, Romain Dupont, Mohamed Tamaazousti and Herv\\'e Le Borgne", "title": "Learning to Segment Dynamic Objects using SLAM Outliers", "comments": "Accepted to ICPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a method to automatically learn to segment dynamic objects using\nSLAM outliers. It requires only one monocular sequence per dynamic object for\ntraining and consists in localizing dynamic objects using SLAM outliers,\ncreating their masks, and using these masks to train a semantic segmentation\nnetwork. We integrate the trained network in ORB-SLAM 2 and LDSO. At runtime we\nremove features on dynamic objects, making the SLAM unaffected by them. We also\npropose a new stereo dataset and new metrics to evaluate SLAM robustness. Our\ndataset includes consensus inversions, i.e., situations where the SLAM uses\nmore features on dynamic objects that on the static background. Consensus\ninversions are challenging for SLAM as they may cause major SLAM failures. Our\napproach performs better than the State-of-the-Art on the TUM RGB-D dataset in\nmonocular mode and on our dataset in both monocular and stereo modes.\n", "versions": [{"version": "v1", "created": "Thu, 12 Nov 2020 08:36:54 GMT"}], "update_date": "2020-11-13", "authors_parsed": [["Bojko", "Adrian", ""], ["Dupont", "Romain", ""], ["Tamaazousti", "Mohamed", ""], ["Borgne", "Herv\u00e9 Le", ""]]}, {"id": "2011.06288", "submitter": "Pankaj Mishra", "authors": "Pankaj Mishra, Claudio Piciarelli, Gian Luca Foresti", "title": "Image Anomaly Detection by Aggregating Deep Pyramidal Representations", "comments": "Published in First International Conference of Industrial Machine\n  Learning ICPR2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Anomaly detection consists in identifying, within a dataset, those samples\nthat significantly differ from the majority of the data, representing the\nnormal class. It has many practical applications, e.g. ranging from defective\nproduct detection in industrial systems to medical imaging. This paper focuses\non image anomaly detection using a deep neural network with multiple pyramid\nlevels to analyze the image features at different scales. We propose a network\nbased on encoding-decoding scheme, using a standard convolutional autoencoders,\ntrained on normal data only in order to build a model of normality. Anomalies\ncan be detected by the inability of the network to reconstruct its input.\nExperimental results show a good accuracy on MNIST, FMNIST and the recent MVTec\nAnomaly Detection dataset\n", "versions": [{"version": "v1", "created": "Thu, 12 Nov 2020 09:58:27 GMT"}], "update_date": "2020-11-13", "authors_parsed": [["Mishra", "Pankaj", ""], ["Piciarelli", "Claudio", ""], ["Foresti", "Gian Luca", ""]]}, {"id": "2011.06294", "submitter": "Zhewei Huang", "authors": "Zhewei Huang, Tianyuan Zhang, Wen Heng, Boxin Shi, Shuchang Zhou", "title": "RIFE: Real-Time Intermediate Flow Estimation for Video Frame\n  Interpolation", "comments": "10 pages, 7 figures, preprint", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose RIFE, a Real-time Intermediate Flow Estimation algorithm for Video\nFrame Interpolation (VFI). Most existing flow-based methods first estimate the\nbi-directional optical flows, then scale and reverse them to approximate\nintermediate flows, leading to artifacts on motion boundaries. RIFE uses a\nneural network named IFNet that can directly estimate the intermediate flows\nfrom images with much better speed. Based on our proposed leakage distillation\nloss, RIFE can be trained in an end-to-end fashion. Experiments demonstrate\nthat our method is flexible and can achieve impressive performance on several\npublic benchmarks. The code is available at\nhttps://github.com/hzwer/arXiv2020-RIFE.\n", "versions": [{"version": "v1", "created": "Thu, 12 Nov 2020 10:12:06 GMT"}, {"version": "v2", "created": "Tue, 17 Nov 2020 09:33:17 GMT"}, {"version": "v3", "created": "Mon, 8 Mar 2021 12:51:04 GMT"}, {"version": "v4", "created": "Tue, 9 Mar 2021 08:18:05 GMT"}, {"version": "v5", "created": "Thu, 18 Mar 2021 04:40:29 GMT"}], "update_date": "2021-03-19", "authors_parsed": [["Huang", "Zhewei", ""], ["Zhang", "Tianyuan", ""], ["Heng", "Wen", ""], ["Shi", "Boxin", ""], ["Zhou", "Shuchang", ""]]}, {"id": "2011.06319", "submitter": "Veysel Kocaman Vk", "authors": "Veysel Kocaman, Ofer M. Shir, Thomas B\\\"ack", "title": "Improving Model Accuracy for Imbalanced Image Classification Tasks by\n  Adding a Final Batch Normalization Layer: An Empirical Study", "comments": "Accepted for presentation and inclusion in ICPR 2020, the 25th\n  International Conference on Pattern Recognition", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Some real-world domains, such as Agriculture and Healthcare, comprise\nearly-stage disease indications whose recording constitutes a rare event, and\nyet, whose precise detection at that stage is critical. In this type of highly\nimbalanced classification problems, which encompass complex features, deep\nlearning (DL) is much needed because of its strong detection capabilities. At\nthe same time, DL is observed in practice to favor majority over minority\nclasses and consequently suffer from inaccurate detection of the targeted\nearly-stage indications. To simulate such scenarios, we artificially generate\nskewness (99% vs. 1%) for certain plant types out of the PlantVillage dataset\nas a basis for classification of scarce visual cues through transfer learning.\nBy randomly and unevenly picking healthy and unhealthy samples from certain\nplant types to form a training set, we consider a base experiment as\nfine-tuning ResNet34 and VGG19 architectures and then testing the model\nperformance on a balanced dataset of healthy and unhealthy images. We\nempirically observe that the initial F1 test score jumps from 0.29 to 0.95 for\nthe minority class upon adding a final Batch Normalization (BN) layer just\nbefore the output layer in VGG19. We demonstrate that utilizing an additional\nBN layer before the output layer in modern CNN architectures has a considerable\nimpact in terms of minimizing the training time and testing error for minority\nclasses in highly imbalanced data sets. Moreover, when the final BN is\nemployed, minimizing the loss function may not be the best way to assure a high\nF1 test score for minority classes in such problems. That is, the network might\nperform better even if it is not confident enough while making a prediction;\nleading to another discussion about why softmax output is not a good\nuncertainty measure for DL models.\n", "versions": [{"version": "v1", "created": "Thu, 12 Nov 2020 11:27:40 GMT"}], "update_date": "2020-11-13", "authors_parsed": [["Kocaman", "Veysel", ""], ["Shir", "Ofer M.", ""], ["B\u00e4ck", "Thomas", ""]]}, {"id": "2011.06337", "submitter": "Jong Chul Ye", "authors": "Gyutaek Oh, Jeong Eun Lee, and Jong Chul Ye", "title": "Unsupervised MR Motion Artifact Deep Learning using Outlier-Rejecting\n  Bootstrap Aggregation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recently, deep learning approaches for MR motion artifact correction have\nbeen extensively studied. Although these approaches have shown high performance\nand reduced computational complexity compared to classical methods, most of\nthem require supervised training using paired artifact-free and\nartifact-corrupted images, which may prohibit its use in many important\nclinical applications. For example, transient severe motion (TSM) due to acute\ntransient dyspnea in Gd-EOB-DTPA-enhanced MR is difficult to control and model\nfor paired data generation. To address this issue, here we propose a novel\nunsupervised deep learning scheme through outlier-rejecting bootstrap\nsubsampling and aggregation. This is inspired by the observation that motions\nusually cause sparse k-space outliers in the phase encoding direction, so\nk-space subsampling along the phase encoding direction can remove some outliers\nand the aggregation step can further improve the results from the\nreconstruction network. Our method does not require any paired data because the\ntraining step only requires artifact-free images. Furthermore, to address the\nsmoothing from potential bias to the artifact-free images, the network is\ntrained in an unsupervised manner using optimal transport driven cycleGAN. We\nverify that our method can be applied for artifact correction from simulated\nmotion as well as real motion from TSM successfully, outperforming existing\nstate-of-the-art deep learning methods.\n", "versions": [{"version": "v1", "created": "Thu, 12 Nov 2020 12:10:58 GMT"}], "update_date": "2020-11-13", "authors_parsed": [["Oh", "Gyutaek", ""], ["Lee", "Jeong Eun", ""], ["Ye", "Jong Chul", ""]]}, {"id": "2011.06372", "submitter": "Jong-Chan Kim", "authors": "Wonseok Jang, Hansaem Jeong, Kyungtae Kang, Nikil Dutt, Jong-Chan Kim", "title": "R-TOD: Real-Time Object Detector with Minimized End-to-End Delay for\n  Autonomous Driving", "comments": "14 pages, 16 figures. Accepted to the 41st IEEE Real-Time Systems\n  Symposium (RTSS), 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For realizing safe autonomous driving, the end-to-end delays of real-time\nobject detection systems should be thoroughly analyzed and minimized. However,\ndespite recent development of neural networks with minimized inference delays,\nsurprisingly little attention has been paid to their end-to-end delays from an\nobject's appearance until its detection is reported. With this motivation, this\npaper aims to provide more comprehensive understanding of the end-to-end delay,\nthrough which precise best- and worst-case delay predictions are formulated,\nand three optimization methods are implemented: (i) on-demand capture, (ii)\nzero-slack pipeline, and (iii) contention-free pipeline. Our experimental\nresults show a 76% reduction in the end-to-end delay of Darknet YOLO (You Only\nLook Once) v3 (from 1070 ms to 261 ms), thereby demonstrating the great\npotential of exploiting the end-to-end delay analysis for autonomous driving.\nFurthermore, as we only modify the system architecture and do not change the\nneural network architecture itself, our approach incurs no penalty on the\ndetection accuracy.\n", "versions": [{"version": "v1", "created": "Fri, 23 Oct 2020 01:03:46 GMT"}], "update_date": "2020-11-13", "authors_parsed": [["Jang", "Wonseok", ""], ["Jeong", "Hansaem", ""], ["Kang", "Kyungtae", ""], ["Dutt", "Nikil", ""], ["Kim", "Jong-Chan", ""]]}, {"id": "2011.06409", "submitter": "Fabien Racape", "authors": "Lahiru D. Chamain, Fabien Racap\\'e, Jean B\\'egaint, Akshay Pushparaja,\n  Simon Feltman", "title": "End-to-end optimized image compression for machines, a study", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An increasing share of image and video content is analyzed by machines rather\nthan viewed by humans, and therefore it becomes relevant to optimize codecs for\nsuch applications where the analysis is performed remotely. Unfortunately,\nconventional coding tools are challenging to specialize for machine tasks as\nthey were originally designed for human perception. However, neural network\nbased codecs can be jointly trained end-to-end with any convolutional neural\nnetwork (CNN)-based task model. In this paper, we propose to study an\nend-to-end framework enabling efficient image compression for remote machine\ntask analysis, using a chain composed of a compression module and a task\nalgorithm that can be optimized end-to-end. We show that it is possible to\nsignificantly improve the task accuracy when fine-tuning jointly the codec and\nthe task networks, especially at low bit-rates. Depending on training or\ndeployment constraints, selective fine-tuning can be applied only on the\nencoder, decoder or task network and still achieve rate-accuracy improvements\nover an off-the-shelf codec and task network. Our results also demonstrate the\nflexibility of end-to-end pipelines for practical applications.\n", "versions": [{"version": "v1", "created": "Tue, 10 Nov 2020 20:10:43 GMT"}], "update_date": "2020-11-13", "authors_parsed": [["Chamain", "Lahiru D.", ""], ["Racap\u00e9", "Fabien", ""], ["B\u00e9gaint", "Jean", ""], ["Pushparaja", "Akshay", ""], ["Feltman", "Simon", ""]]}, {"id": "2011.06425", "submitter": "Davi Frossard", "authors": "Davi Frossard, Simon Suo, Sergio Casas, James Tu, Rui Hu, Raquel\n  Urtasun", "title": "StrObe: Streaming Object Detection from LiDAR Packets", "comments": "To be presented at the 4th Conference on Robot Learning (CoRL 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many modern robotics systems employ LiDAR as their main sensing modality due\nto its geometrical richness. Rolling shutter LiDARs are particularly common, in\nwhich an array of lasers scans the scene from a rotating base. Points are\nemitted as a stream of packets, each covering a sector of the 360{\\deg}\ncoverage. Modern perception algorithms wait for the full sweep to be built\nbefore processing the data, which introduces an additional latency. For typical\n10Hz LiDARs this will be 100ms. As a consequence, by the time an output is\nproduced, it no longer accurately reflects the state of the world. This poses a\nchallenge, as robotics applications require minimal reaction times, such that\nmaneuvers can be quickly planned in the event of a safety-critical situation.\nIn this paper we propose StrObe, a novel approach that minimizes latency by\ningesting LiDAR packets and emitting a stream of detections without waiting for\nthe full sweep to be built. StrObe reuses computations from previous packets\nand iteratively updates a latent spatial representation of the scene, which\nacts as a memory, as new evidence comes in, resulting in accurate low-latency\nperception. We demonstrate the effectiveness of our approach on a large scale\nreal-world dataset, showing that StrObe far outperforms the state-of-the-art\nwhen latency is taken into account, and matches the performance in the\ntraditional setting.\n", "versions": [{"version": "v1", "created": "Thu, 12 Nov 2020 14:57:44 GMT"}, {"version": "v2", "created": "Fri, 13 Nov 2020 17:59:04 GMT"}], "update_date": "2020-11-16", "authors_parsed": [["Frossard", "Davi", ""], ["Suo", "Simon", ""], ["Casas", "Sergio", ""], ["Tu", "James", ""], ["Hu", "Rui", ""], ["Urtasun", "Raquel", ""]]}, {"id": "2011.06431", "submitter": "Adithyavairavan Murali", "authors": "Adithyavairavan Murali, Weiyu Liu, Kenneth Marino, Sonia Chernova,\n  Abhinav Gupta", "title": "Same Object, Different Grasps: Data and Semantic Knowledge for\n  Task-Oriented Grasping", "comments": "Accepted to Conference on Robot Learning (CoRL) 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the enormous progress and generalization in robotic grasping in\nrecent years, existing methods have yet to scale and generalize task-oriented\ngrasping to the same extent. This is largely due to the scale of the datasets\nboth in terms of the number of objects and tasks studied. We address these\nconcerns with the TaskGrasp dataset which is more diverse both in terms of\nobjects and tasks, and an order of magnitude larger than previous datasets. The\ndataset contains 250K task-oriented grasps for 56 tasks and 191 objects along\nwith their RGB-D information. We take advantage of this new breadth and\ndiversity in the data and present the GCNGrasp framework which uses the\nsemantic knowledge of objects and tasks encoded in a knowledge graph to\ngeneralize to new object instances, classes and even new tasks. Our framework\nshows a significant improvement of around 12% on held-out settings compared to\nbaseline methods which do not use semantics. We demonstrate that our dataset\nand model are applicable for the real world by executing task-oriented grasps\non a real robot on unknown objects. Code, data and supplementary video could be\nfound at https://sites.google.com/view/taskgrasp\n", "versions": [{"version": "v1", "created": "Thu, 12 Nov 2020 15:08:15 GMT"}, {"version": "v2", "created": "Fri, 13 Nov 2020 15:28:44 GMT"}], "update_date": "2020-11-16", "authors_parsed": [["Murali", "Adithyavairavan", ""], ["Liu", "Weiyu", ""], ["Marino", "Kenneth", ""], ["Chernova", "Sonia", ""], ["Gupta", "Abhinav", ""]]}, {"id": "2011.06450", "submitter": "Manish Bhattarai", "authors": "Manish Bhattarai and Manel Martinez-Ramon", "title": "A deep Q-Learning based Path Planning and Navigation System for\n  Firefighting Environments", "comments": "Accepted to ICAART2021", "journal-ref": null, "doi": "10.5220/0010267102670277", "report-no": null, "categories": "cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Live fire creates a dynamic, rapidly changing environment that presents a\nworthy challenge for deep learning and artificial intelligence methodologies to\nassist firefighters with scene comprehension in maintaining their situational\nawareness, tracking and relay of important features necessary for key decisions\nas they tackle these catastrophic events. We propose a deep Q-learning based\nagent who is immune to stress induced disorientation and anxiety and thus able\nto make clear decisions for navigation based on the observed and stored facts\nin live fire environments. As a proof of concept, we imitate structural fire in\na gaming engine called Unreal Engine which enables the interaction of the agent\nwith the environment. The agent is trained with a deep Q-learning algorithm\nbased on a set of rewards and penalties as per its actions on the environment.\nWe exploit experience replay to accelerate the learning process and augment the\nlearning of the agent with human-derived experiences. The agent trained under\nthis deep Q-learning approach outperforms agents trained through alternative\npath planning systems and demonstrates this methodology as a promising\nfoundation on which to build a path planning navigation assistant capable of\nsafely guiding fire fighters through live fire environments.\n", "versions": [{"version": "v1", "created": "Thu, 12 Nov 2020 15:43:17 GMT"}], "update_date": "2021-07-23", "authors_parsed": [["Bhattarai", "Manish", ""], ["Martinez-Ramon", "Manel", ""]]}, {"id": "2011.06464", "submitter": "Xian Zhou", "authors": "Hsiao-Yu Fish Tung, Zhou Xian, Mihir Prabhudesai, Shamit Lal, Katerina\n  Fragkiadaki", "title": "3D-OES: Viewpoint-Invariant Object-Factorized Environment Simulators", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an action-conditioned dynamics model that predicts scene changes\ncaused by object and agent interactions in a viewpoint-invariant 3D neural\nscene representation space, inferred from RGB-D videos. In this 3D feature\nspace, objects do not interfere with one another and their appearance persists\nover time and across viewpoints. This permits our model to predict future\nscenes long in the future by simply \"moving\" 3D object features based on\ncumulative object motion predictions. Object motion predictions are computed by\na graph neural network that operates over the object features extracted from\nthe 3D neural scene representation. Our model's simulations can be decoded by a\nneural renderer into2D image views from any desired viewpoint, which aids the\ninterpretability of our latent 3D simulation space. We show our model\ngeneralizes well its predictions across varying number and appearances of\ninteracting objects as well as across camera viewpoints, outperforming existing\n2D and 3D dynamics models. We further demonstrate sim-to-real transfer of the\nlearnt dynamics by applying our model trained solely in simulation to\nmodel-based control for pushing objects to desired locations under clutter on a\nreal robotic setup\n", "versions": [{"version": "v1", "created": "Thu, 12 Nov 2020 16:15:52 GMT"}], "update_date": "2020-12-29", "authors_parsed": [["Tung", "Hsiao-Yu Fish", ""], ["Xian", "Zhou", ""], ["Prabhudesai", "Mihir", ""], ["Lal", "Shamit", ""], ["Fragkiadaki", "Katerina", ""]]}, {"id": "2011.06490", "submitter": "Bj\\\"orn Barz", "authors": "Bj\\\"orn Barz, Joachim Denzler", "title": "Content-based Image Retrieval and the Semantic Gap in the Deep Learning\n  Era", "comments": "CBIR workshop at ICPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Content-based image retrieval has seen astonishing progress over the past\ndecade, especially for the task of retrieving images of the same object that is\ndepicted in the query image. This scenario is called instance or object\nretrieval and requires matching fine-grained visual patterns between images.\nSemantics, however, do not play a crucial role. This brings rise to the\nquestion: Do the recent advances in instance retrieval transfer to more generic\nimage retrieval scenarios? To answer this question, we first provide a brief\noverview of the most relevant milestones of instance retrieval. We then apply\nthem to a semantic image retrieval task and find that they perform inferior to\nmuch less sophisticated and more generic methods in a setting that requires\nimage understanding. Following this, we review existing approaches to closing\nthis so-called semantic gap by integrating prior world knowledge. We conclude\nthat the key problem for the further advancement of semantic image retrieval\nlies in the lack of a standardized task definition and an appropriate benchmark\ndataset.\n", "versions": [{"version": "v1", "created": "Thu, 12 Nov 2020 17:00:08 GMT"}], "update_date": "2020-11-13", "authors_parsed": [["Barz", "Bj\u00f6rn", ""], ["Denzler", "Joachim", ""]]}, {"id": "2011.06496", "submitter": "Shiv Ram Dubey", "authors": "Roshan Reddy Yedla and Shiv Ram Dubey", "title": "On the Performance of Convolutional Neural Networks under High and Low\n  Frequency Information", "comments": "Accepted in Fifth IAPR International Conference on Computer Vision\n  and Image Processing (CVIP), 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional neural networks (CNNs) have shown very promising performance in\nrecent years for different problems, including object recognition, face\nrecognition, medical image analysis, etc. However, generally the trained CNN\nmodels are tested over the test set which is very similar to the trained set.\nThe generalizability and robustness of the CNN models are very important\naspects to make it to work for the unseen data. In this letter, we study the\nperformance of CNN models over the high and low frequency information of the\nimages. We observe that the trained CNN fails to generalize over the high and\nlow frequency images. In order to make the CNN robust against high and low\nfrequency images, we propose the stochastic filtering based data augmentation\nduring training. A satisfactory performance improvement has been observed in\nterms of the high and low frequency generalization and robustness with the\nproposed stochastic filtering based data augmentation approach. The\nexperimentations are performed using ResNet50 model over the CIFAR-10 dataset\nand ResNet101 model over Tiny-ImageNet dataset.\n", "versions": [{"version": "v1", "created": "Fri, 30 Oct 2020 17:54:45 GMT"}], "update_date": "2020-11-13", "authors_parsed": [["Yedla", "Roshan Reddy", ""], ["Dubey", "Shiv Ram", ""]]}, {"id": "2011.06498", "submitter": "Shubham Agrawal", "authors": "Huy Ha, Shubham Agrawal, Shuran Song", "title": "Fit2Form: 3D Generative Model for Robot Gripper Form Design", "comments": "Conference on Robot Learning 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The 3D shape of a robot's end-effector plays a critical role in determining\nit's functionality and overall performance. Many industrial applications rely\non task-specific gripper designs to ensure the system's robustness and\naccuracy. However, the process of manual hardware design is both costly and\ntime-consuming, and the quality of the resulting design is dependent on the\nengineer's experience and domain expertise, which can easily be out-dated or\ninaccurate. The goal of this work is to use machine learning algorithms to\nautomate the design of task-specific gripper fingers. We propose Fit2Form, a 3D\ngenerative design framework that generates pairs of finger shapes to maximize\ndesign objectives (i.e., grasp success, stability, and robustness) for target\ngrasp objects. We model the design objectives by training a Fitness network to\npredict their values for pairs of gripper fingers and their corresponding grasp\nobjects. This Fitness network then provides supervision to a 3D Generative\nnetwork that produces a pair of 3D finger geometries for the target grasp\nobject. Our experiments demonstrate that the proposed 3D generative design\nframework generates parallel jaw gripper finger shapes that achieve more stable\nand robust grasps compared to other general-purpose and task-specific gripper\ndesign algorithms. Video can be found at https://youtu.be/utKHP3qb1bg.\n", "versions": [{"version": "v1", "created": "Thu, 12 Nov 2020 17:09:36 GMT"}], "update_date": "2020-11-13", "authors_parsed": [["Ha", "Huy", ""], ["Agrawal", "Shubham", ""], ["Song", "Shuran", ""]]}, {"id": "2011.06507", "submitter": "Karl Schmeckpeper", "authors": "Karl Schmeckpeper, Oleh Rybkin, Kostas Daniilidis, Sergey Levine,\n  Chelsea Finn", "title": "Reinforcement Learning with Videos: Combining Offline Observations with\n  Interaction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reinforcement learning is a powerful framework for robots to acquire skills\nfrom experience, but often requires a substantial amount of online data\ncollection. As a result, it is difficult to collect sufficiently diverse\nexperiences that are needed for robots to generalize broadly. Videos of humans,\non the other hand, are a readily available source of broad and interesting\nexperiences. In this paper, we consider the question: can we perform\nreinforcement learning directly on experience collected by humans? This problem\nis particularly difficult, as such videos are not annotated with actions and\nexhibit substantial visual domain shift relative to the robot's embodiment. To\naddress these challenges, we propose a framework for reinforcement learning\nwith videos (RLV). RLV learns a policy and value function using experience\ncollected by humans in combination with data collected by robots. In our\nexperiments, we find that RLV is able to leverage such videos to learn\nchallenging vision-based skills with less than half as many samples as RL\nmethods that learn from scratch.\n", "versions": [{"version": "v1", "created": "Thu, 12 Nov 2020 17:15:48 GMT"}], "update_date": "2020-11-13", "authors_parsed": [["Schmeckpeper", "Karl", ""], ["Rybkin", "Oleh", ""], ["Daniilidis", "Kostas", ""], ["Levine", "Sergey", ""], ["Finn", "Chelsea", ""]]}, {"id": "2011.06539", "submitter": "Thomas Pock", "authors": "Thomas Pinetz and Erich Kobler and Thomas Pock and Alexander Effland", "title": "Shared Prior Learning of Energy-Based Models for Image Reconstruction", "comments": "37 pages, 19 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NA eess.IV math.NA math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel learning-based framework for image reconstruction\nparticularly designed for training without ground truth data, which has three\nmajor building blocks: energy-based learning, a patch-based Wasserstein loss\nfunctional, and shared prior learning. In energy-based learning, the parameters\nof an energy functional composed of a learned data fidelity term and a\ndata-driven regularizer are computed in a mean-field optimal control problem.\nIn the absence of ground truth data, we change the loss functional to a\npatch-based Wasserstein functional, in which local statistics of the output\nimages are compared to uncorrupted reference patches. Finally, in shared prior\nlearning, both aforementioned optimal control problems are optimized\nsimultaneously with shared learned parameters of the regularizer to further\nenhance unsupervised image reconstruction. We derive several time\ndiscretization schemes of the gradient flow and verify their consistency in\nterms of Mosco convergence. In numerous numerical experiments, we demonstrate\nthat the proposed method generates state-of-the-art results for various image\nreconstruction applications--even if no ground truth images are available for\ntraining.\n", "versions": [{"version": "v1", "created": "Thu, 12 Nov 2020 17:56:05 GMT"}, {"version": "v2", "created": "Fri, 13 Nov 2020 08:54:13 GMT"}], "update_date": "2020-11-16", "authors_parsed": [["Pinetz", "Thomas", ""], ["Kobler", "Erich", ""], ["Pock", "Thomas", ""], ["Effland", "Alexander", ""]]}, {"id": "2011.06639", "submitter": "Sangeeta Satish Rao", "authors": "Sangeeta Satish Rao, Nikunj Phutela, V R Badri Prasad", "title": "Empirical Performance Analysis of Conventional Deep Learning Models for\n  Recognition of Objects in 2-D Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Artificial Neural Networks, an essential part of Deep Learning, are derived\nfrom the structure and functionality of the human brain. It has a broad range\nof applications ranging from medical analysis to automated driving. Over the\npast few years, deep learning techniques have improved drastically - models can\nnow be customized to a much greater extent by varying the network architecture,\nnetwork parameters, among others. We have varied parameters like learning rate,\nfilter size, the number of hidden layers, stride size and the activation\nfunction among others to analyze the performance of the model and thus produce\na model with the highest performance. The model classifies images into 3\ncategories, namely, cars, faces and aeroplanes.\n", "versions": [{"version": "v1", "created": "Thu, 12 Nov 2020 20:14:03 GMT"}], "update_date": "2020-11-16", "authors_parsed": [["Rao", "Sangeeta Satish", ""], ["Phutela", "Nikunj", ""], ["Prasad", "V R Badri", ""]]}, {"id": "2011.06671", "submitter": "Florian Schiffers", "authors": "Florian Schiffers, Thomas Bochynek, Andre Aichert, Tobias W\\\"urfl,\n  Michael Rubenstein, Oliver Cossairt", "title": "Disassemblable Fieldwork CT Scanner Using a 3D-printed Calibration\n  Phantom", "comments": "This paper was originally published at the 6th International\n  Conference on Image Formation in X-Ray Computed Tomography (CTmeeting 2020)", "journal-ref": "CT Meeting 2020", "doi": null, "report-no": null, "categories": "eess.IV cs.CV physics.ins-det", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The use of computed tomography (CT) imaging has become of increasing interest\nto academic areas outside of the field of medical imaging and industrial\ninspection, e.g., to biology and cultural heritage research. The pecularities\nof these fields, however, sometimes require that objects need to be imaged\non-site, e.g., in field-work conditions or in museum collections. Under these\ncircumstances, it is often not possible to use a commercial device and a custom\nsolution is the only viable option. In order to achieve high image quality\nunder adverse conditions, reliable calibration and trajectory reproduction are\nusually key requirements for any custom CT scanning system. Here, we introduce\nthe construction of a low-cost disassemblable CT scanner that allows\ncalibration even when trajectory reproduction is not possible due to the\nlimitations imposed by the project conditions. Using 3D-printed in-image\ncalibration phantoms, we compute a projection matrix directly from each\ncaptured X-ray projection. We describe our method in detail and show successful\ntomographic reconstructions of several specimen as proof of concept.\n", "versions": [{"version": "v1", "created": "Thu, 12 Nov 2020 22:07:29 GMT"}], "update_date": "2020-11-16", "authors_parsed": [["Schiffers", "Florian", ""], ["Bochynek", "Thomas", ""], ["Aichert", "Andre", ""], ["W\u00fcrfl", "Tobias", ""], ["Rubenstein", "Michael", ""], ["Cossairt", "Oliver", ""]]}, {"id": "2011.06679", "submitter": "Ross Greer", "authors": "Ross Greer, Nachiket Deo, and Mohan Trivedi", "title": "Trajectory Prediction in Autonomous Driving with a Lane Heading\n  Auxiliary Loss", "comments": "8 pages, 18 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Predicting a vehicle's trajectory is an essential ability for autonomous\nvehicles navigating through complex urban traffic scenes. Bird's-eye-view\nroadmap information provides valuable information for making trajectory\npredictions, and while state-of-the-art models extract this information via\nimage convolution, auxiliary loss functions can augment patterns inferred from\ndeep learning by further encoding common knowledge of social and legal driving\nbehaviors. Since human driving behavior is inherently multimodal, models which\nallow for multimodal output tend to outperform single-prediction models on\nstandard metrics. We propose a loss function which enhances such models by\nenforcing expected driving rules on all predicted modes. Our contribution to\ntrajectory prediction is twofold; we propose a new metric which addresses\nfailure cases of the off-road rate metric by penalizing trajectories that\noppose the ascribed heading (flow direction) of a driving lane, and we show\nthis metric to be differentiable and therefore suitable as an auxiliary loss\nfunction. We then use this auxiliary loss to extend the the standard multiple\ntrajectory prediction (MTP) and MultiPath models, achieving improved results on\nthe nuScenes prediction benchmark by predicting trajectories which better\nconform to the lane-following rules of the road.\n", "versions": [{"version": "v1", "created": "Thu, 12 Nov 2020 22:51:25 GMT"}, {"version": "v2", "created": "Thu, 28 Jan 2021 18:47:34 GMT"}], "update_date": "2021-01-29", "authors_parsed": [["Greer", "Ross", ""], ["Deo", "Nachiket", ""], ["Trivedi", "Mohan", ""]]}, {"id": "2011.06690", "submitter": "Zhengyu Zhao", "authors": "Zhengyu Zhao and Zhuoran Liu and Martha Larson", "title": "Adversarial Robustness Against Image Color Transformation within\n  Parametric Filter Space", "comments": "Code is available at\n  https://github.com/ZhengyuZhao/ACE/tree/master/Journal_version. This work has\n  been submitted to the IEEE for possible publication. Copyright may be\n  transferred without notice, after which this version may no longer be\n  accessible", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CR cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We propose Adversarial Color Enhancement (ACE), a novel approach to\ngenerating non-suspicious adversarial images by optimizing a color\ntransformation within a parametric filter space. The filter we use approximates\nhuman-understandable color curve adjustment, constraining ACE with a single,\ncontinuous function. This property gives rise to a principled adversarial\naction space explicitly controlled by filter parameters. Existing color\ntransformation attacks are not guided by a parametric space, and, consequently,\nadditional pixel-related constraints such as regularization and sampling are\nnecessary. These constraints make methodical analysis difficult. In this paper,\nwe carry out a systematic robustness analysis of ACE from both the attack and\ndefense perspectives by varying the bound of the color filter parameters. We\ninvestigate a general formulation of ACE and also a variant targeting\nparticularly appealing color styles, as achieved with popular image filters.\nFrom the attack perspective, we provide extensive experiments on the\nvulnerability of image classifiers, but also explore the vulnerability of\nsegmentation and aesthetic quality assessment algorithms, in both the white-box\nand black-box scenarios. From the defense perspective, more experiments provide\ninsight into the stability of ACE against input transformation-based defenses\nand show the potential of adversarial training for improving model robustness\nagainst ACE.\n", "versions": [{"version": "v1", "created": "Thu, 12 Nov 2020 23:51:37 GMT"}], "update_date": "2020-11-16", "authors_parsed": [["Zhao", "Zhengyu", ""], ["Liu", "Zhuoran", ""], ["Larson", "Martha", ""]]}, {"id": "2011.06698", "submitter": "Bryan Chen", "authors": "Bryan Chen, Alexander Sax, Gene Lewis, Iro Armeni, Silvio Savarese,\n  Amir Zamir, Jitendra Malik, Lerrel Pinto", "title": "Robust Policies via Mid-Level Visual Representations: An Experimental\n  Study in Manipulation and Navigation", "comments": "Extended version of CoRL 2020 camera ready. Supplementary released\n  separately", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Vision-based robotics often separates the control loop into one module for\nperception and a separate module for control. It is possible to train the whole\nsystem end-to-end (e.g. with deep RL), but doing it \"from scratch\" comes with a\nhigh sample complexity cost and the final result is often brittle, failing\nunexpectedly if the test environment differs from that of training.\n  We study the effects of using mid-level visual representations (features\nlearned asynchronously for traditional computer vision objectives), as a\ngeneric and easy-to-decode perceptual state in an end-to-end RL framework.\nMid-level representations encode invariances about the world, and we show that\nthey aid generalization, improve sample complexity, and lead to a higher final\nperformance. Compared to other approaches for incorporating invariances, such\nas domain randomization, asynchronously trained mid-level representations scale\nbetter: both to harder problems and to larger domain shifts. In practice, this\nmeans that mid-level representations could be used to successfully train\npolicies for tasks where domain randomization and learning-from-scratch failed.\nWe report results on both manipulation and navigation tasks, and for navigation\ninclude zero-shot sim-to-real experiments on real robots.\n", "versions": [{"version": "v1", "created": "Fri, 13 Nov 2020 00:16:05 GMT"}], "update_date": "2020-11-16", "authors_parsed": [["Chen", "Bryan", ""], ["Sax", "Alexander", ""], ["Lewis", "Gene", ""], ["Armeni", "Iro", ""], ["Savarese", "Silvio", ""], ["Zamir", "Amir", ""], ["Malik", "Jitendra", ""], ["Pinto", "Lerrel", ""]]}, {"id": "2011.06704", "submitter": "Troy Luhman", "authors": "Troy Luhman, Eric Luhman", "title": "Diffusion models for Handwriting Generation", "comments": "17 figures, 17 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  In this paper, we propose a diffusion probabilistic model for handwriting\ngeneration. Diffusion models are a class of generative models where samples\nstart from Gaussian noise and are gradually denoised to produce output. Our\nmethod of handwriting generation does not require using any text-recognition\nbased, writer-style based, or adversarial loss functions, nor does it require\ntraining of auxiliary networks. Our model is able to incorporate writer\nstylistic features directly from image data, eliminating the need for user\ninteraction during sampling. Experiments reveal that our model is able to\ngenerate realistic , high quality images of handwritten text in a similar style\nto a given writer. Our implementation can be found at\nhttps://github.com/tcl9876/Diffusion-Handwriting-Generation\n", "versions": [{"version": "v1", "created": "Fri, 13 Nov 2020 00:31:22 GMT"}], "update_date": "2020-11-16", "authors_parsed": [["Luhman", "Troy", ""], ["Luhman", "Eric", ""]]}, {"id": "2011.06722", "submitter": "Pankaj Roy", "authors": "Pankaj Raj Roy, Guillaume-Alexandre Bilodeau and Lama Seoud", "title": "Local Anomaly Detection in Videos using Object-Centric Adversarial\n  Learning", "comments": "Accepted for The First International Workshop on Deep Learning for\n  Human-Centric Activity Understanding (ICPR2020 workshop)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel unsupervised approach based on a two-stage object-centric\nadversarial framework that only needs object regions for detecting frame-level\nlocal anomalies in videos. The first stage consists in learning the\ncorrespondence between the current appearance and past gradient images of\nobjects in scenes deemed normal, allowing us to either generate the past\ngradient from current appearance or the reverse. The second stage extracts the\npartial reconstruction errors between real and generated images (appearance and\npast gradient) with normal object behaviour, and trains a discriminator in an\nadversarial fashion. In inference mode, we employ the trained image generators\nwith the adversarially learned binary classifier for outputting region-level\nanomaly detection scores. We tested our method on four public benchmarks, UMN,\nUCSD, Avenue and ShanghaiTech and our proposed object-centric adversarial\napproach yields competitive or even superior results compared to\nstate-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Fri, 13 Nov 2020 02:02:37 GMT"}], "update_date": "2020-11-16", "authors_parsed": [["Roy", "Pankaj Raj", ""], ["Bilodeau", "Guillaume-Alexandre", ""], ["Seoud", "Lama", ""]]}, {"id": "2011.06733", "submitter": "Vivswan Shitole", "authors": "Vivswan Shitole, Li Fuxin, Minsuk Kahng, Prasad Tadepalli, Alan Fern", "title": "Structured Attention Graphs for Understanding Deep Image Classifications", "comments": "26 pages, 25 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Attention maps are a popular way of explaining the decisions of convolutional\nnetworks for image classification. Typically, for each image of interest, a\nsingle attention map is produced, which assigns weights to pixels based on\ntheir importance to the classification. A single attention map, however,\nprovides an incomplete understanding since there are often many other maps that\nexplain a classification equally well. In this paper, we introduce structured\nattention graphs (SAGs), which compactly represent sets of attention maps for\nan image by capturing how different combinations of image regions impact a\nclassifier's confidence. We propose an approach to compute SAGs and a\nvisualization for SAGs so that deeper insight can be gained into a classifier's\ndecisions. We conduct a user study comparing the use of SAGs to traditional\nattention maps for answering counterfactual questions about image\nclassifications. Our results show that the users are more correct when\nanswering comparative counterfactual questions based on SAGs compared to the\nbaselines.\n", "versions": [{"version": "v1", "created": "Fri, 13 Nov 2020 02:51:54 GMT"}, {"version": "v2", "created": "Tue, 8 Dec 2020 03:56:28 GMT"}], "update_date": "2020-12-09", "authors_parsed": [["Shitole", "Vivswan", ""], ["Fuxin", "Li", ""], ["Kahng", "Minsuk", ""], ["Tadepalli", "Prasad", ""], ["Fern", "Alan", ""]]}, {"id": "2011.06751", "submitter": "Jun Nishikawa", "authors": "Jun Nishikawa, Ryoji Ikegaya", "title": "Filter Pre-Pruning for Improved Fine-tuning of Quantized Deep Neural\n  Networks", "comments": "updated for ICLR2021 OpenReview rebuttal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Neural Networks(DNNs) have many parameters and activation data, and\nthese both are expensive to implement. One method to reduce the size of the DNN\nis to quantize the pre-trained model by using a low-bit expression for weights\nand activations, using fine-tuning to recover the drop in accuracy. However, it\nis generally difficult to train neural networks which use low-bit expressions.\nOne reason is that the weights in the middle layer of the DNN have a wide\ndynamic range and so when quantizing the wide dynamic range into a few bits,\nthe step size becomes large, which leads to a large quantization error and\nfinally a large degradation in accuracy. To solve this problem, this paper\nmakes the following three contributions without using any additional learning\nparameters and hyper-parameters. First, we analyze how batch normalization,\nwhich causes the aforementioned problem, disturbs the fine-tuning of the\nquantized DNN. Second, based on these results, we propose a new pruning method\ncalled Pruning for Quantization (PfQ) which removes the filters that disturb\nthe fine-tuning of the DNN while not affecting the inferred result as far as\npossible. Third, we propose a workflow of fine-tuning for quantized DNNs using\nthe proposed pruning method(PfQ). Experiments using well-known models and\ndatasets confirmed that the proposed method achieves higher performance with a\nsimilar model size than conventional quantization methods including\nfine-tuning.\n", "versions": [{"version": "v1", "created": "Fri, 13 Nov 2020 04:12:54 GMT"}, {"version": "v2", "created": "Wed, 25 Nov 2020 05:22:16 GMT"}], "update_date": "2020-11-26", "authors_parsed": [["Nishikawa", "Jun", ""], ["Ikegaya", "Ryoji", ""]]}, {"id": "2011.06773", "submitter": "Xuehui Wang", "authors": "Xuehui Wang, Qing Wang, Yuzhi Zhao, Junchi Yan, Lei Fan, Long Chen", "title": "Lightweight Single-Image Super-Resolution Network with Attentive\n  Auxiliary Feature Learning", "comments": "Accepted by ACCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Despite convolutional network-based methods have boosted the performance of\nsingle image super-resolution (SISR), the huge computation costs restrict their\npractical applicability. In this paper, we develop a computation efficient yet\naccurate network based on the proposed attentive auxiliary features (A$^2$F)\nfor SISR. Firstly, to explore the features from the bottom layers, the\nauxiliary feature from all the previous layers are projected into a common\nspace. Then, to better utilize these projected auxiliary features and filter\nthe redundant information, the channel attention is employed to select the most\nimportant common feature based on current layer feature. We incorporate these\ntwo modules into a block and implement it with a lightweight network.\nExperimental results on large-scale dataset demonstrate the effectiveness of\nthe proposed model against the state-of-the-art (SOTA) SR methods. Notably,\nwhen parameters are less than 320k, A$^2$F outperforms SOTA methods for all\nscales, which proves its ability to better utilize the auxiliary features.\nCodes are available at https://github.com/wxxxxxxh/A2F-SR.\n", "versions": [{"version": "v1", "created": "Fri, 13 Nov 2020 06:01:46 GMT"}], "update_date": "2020-11-16", "authors_parsed": [["Wang", "Xuehui", ""], ["Wang", "Qing", ""], ["Zhao", "Yuzhi", ""], ["Yan", "Junchi", ""], ["Fan", "Lei", ""], ["Chen", "Long", ""]]}, {"id": "2011.06776", "submitter": "Sung Eun Kim Dr.", "authors": "Sung Eun Kim, Hongkyu Yoon, and Jonghyun Lee", "title": "Fast and Scalable Earth Texture Synthesis using Spatially Assembled\n  Generative Adversarial Neural Networks", "comments": "17 pages, 11 figures, 2 tables, and a table in Appendix", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV physics.flu-dyn", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  The earth texture with complex morphological geometry and compositions such\nas shale and carbonate rocks, is typically characterized with sparse field\nsamples because of an expensive and time-consuming characterization process.\nAccordingly, generating arbitrary large size of the geological texture with\nsimilar topological structures at a low computation cost has become one of the\nkey tasks for realistic geomaterial reconstruction. Recently, generative\nadversarial neural networks (GANs) have demonstrated a potential of\nsynthesizing input textural images and creating equiprobable geomaterial\nimages. However, the texture synthesis with the GANs framework is often limited\nby the computational cost and scalability of the output texture size. In this\nstudy, we proposed a spatially assembled GANs (SAGANs) that can generate output\nimages of an arbitrary large size regardless of the size of training images\nwith computational efficiency. The performance of the SAGANs was evaluated with\ntwo and three dimensional (2D and 3D) rock image samples widely used in\ngeostatistical reconstruction of the earth texture. We demonstrate SAGANs can\ngenerate the arbitrary large size of statistical realizations with connectivity\nand structural properties similar to training images, and also can generate a\nvariety of realizations even on a single training image. In addition, the\ncomputational time was significantly improved compared to standard GANs\nframeworks.\n", "versions": [{"version": "v1", "created": "Fri, 13 Nov 2020 06:18:09 GMT"}], "update_date": "2020-11-16", "authors_parsed": [["Kim", "Sung Eun", ""], ["Yoon", "Hongkyu", ""], ["Lee", "Jonghyun", ""]]}, {"id": "2011.06788", "submitter": "Wonjik Kim", "authors": "Wonjik Kim, Masayuki Tanaka, Masatoshi Okutomi, Yoko Sasaki", "title": "Adaptive Future Frame Prediction with Ensemble Network", "comments": "Accepted at 25th International Conference on Pattern Recognition\n  Workshop (ICPRW 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Future frame prediction in videos is a challenging problem because videos\ninclude complicated movements and large appearance changes. Learning-based\nfuture frame prediction approaches have been proposed in kinds of literature. A\ncommon limitation of the existing learning-based approaches is a mismatch of\ntraining data and test data. In the future frame prediction task, we can obtain\nthe ground truth data by just waiting for a few frames. It means we can update\nthe prediction model online in the test phase. Then, we propose an adaptive\nupdate framework for the future frame prediction task. The proposed adaptive\nupdating framework consists of a pre-trained prediction network, a\ncontinuous-updating prediction network, and a weight estimation network. We\nalso show that our pre-trained prediction model achieves comparable performance\nto the existing state-of-the-art approaches. We demonstrate that our approach\noutperforms existing methods especially for dynamically changing scenes.\n", "versions": [{"version": "v1", "created": "Fri, 13 Nov 2020 07:08:06 GMT"}, {"version": "v2", "created": "Mon, 16 Nov 2020 01:43:53 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["Kim", "Wonjik", ""], ["Tanaka", "Masayuki", ""], ["Okutomi", "Masatoshi", ""], ["Sasaki", "Yoko", ""]]}, {"id": "2011.06798", "submitter": "Jiajun Zhang", "authors": "Jiajun Zhang, Pengyuan Ren and Jianmin Li", "title": "Deep Template Matching for Pedestrian Attribute Recognition with the\n  Auxiliary Supervision of Attribute-wise Keypoints", "comments": "8 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pedestrian Attribute Recognition (PAR) has aroused extensive attention due to\nits important role in video surveillance scenarios. In most cases, the\nexistence of a particular attribute is strongly related to a partial region.\nRecent works design complicated modules, e.g., attention mechanism and proposal\nof body parts to localize the attribute corresponding region. These works\nfurther prove that localization of attribute specific regions precisely will\nhelp in improving performance. However, these part-information-based methods\nare still not accurate as well as increasing model complexity which makes it\nhard to deploy on realistic applications. In this paper, we propose a Deep\nTemplate Matching based method to capture body parts features with less\ncomputation. Further, we also proposed an auxiliary supervision method that use\nhuman pose keypoints to guide the learning toward discriminative local cues.\nExtensive experiments show that the proposed method outperforms and has lower\ncomputational complexity, compared with the state-of-the-art approaches on\nlarge-scale pedestrian attribute datasets, including PETA, PA-100K, RAP, and\nRAPv2 zs.\n", "versions": [{"version": "v1", "created": "Fri, 13 Nov 2020 07:52:26 GMT"}], "update_date": "2020-11-16", "authors_parsed": [["Zhang", "Jiajun", ""], ["Ren", "Pengyuan", ""], ["Li", "Jianmin", ""]]}, {"id": "2011.06813", "submitter": "Vladim\\'ir Petr\\'ik", "authors": "Vladim\\'ir Petr\\'ik, Makarand Tapaswi, Ivan Laptev, Josef Sivic", "title": "Learning Object Manipulation Skills via Approximate State Estimation\n  from Real Videos", "comments": "CoRL 2020, code at\n  https://github.com/makarandtapaswi/Real2Sim_CoRL2020, project page at\n  https://data.ciirc.cvut.cz/public/projects/2020Real2Sim/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Humans are adept at learning new tasks by watching a few instructional\nvideos. On the other hand, robots that learn new actions either require a lot\nof effort through trial and error, or use expert demonstrations that are\nchallenging to obtain. In this paper, we explore a method that facilitates\nlearning object manipulation skills directly from videos. Leveraging recent\nadvances in 2D visual recognition and differentiable rendering, we develop an\noptimization based method to estimate a coarse 3D state representation for the\nhand and the manipulated object(s) without requiring any supervision. We use\nthese trajectories as dense rewards for an agent that learns to mimic them\nthrough reinforcement learning. We evaluate our method on simple single- and\ntwo-object actions from the Something-Something dataset. Our approach allows an\nagent to learn actions from single videos, while watching multiple\ndemonstrations makes the policy more robust. We show that policies learned in a\nsimulated environment can be easily transferred to a real robot.\n", "versions": [{"version": "v1", "created": "Fri, 13 Nov 2020 08:53:47 GMT"}], "update_date": "2020-11-16", "authors_parsed": [["Petr\u00edk", "Vladim\u00edr", ""], ["Tapaswi", "Makarand", ""], ["Laptev", "Ivan", ""], ["Sivic", "Josef", ""]]}, {"id": "2011.06822", "submitter": "Raghav Brahmadesam Venkataramaiyer", "authors": "Raghav B. Venkataramaiyer, Abhishek Joshi, Saisha Narang and Vinay P.\n  Namboodiri", "title": "SHAD3S: A model to Sketch, Shade and Shadow", "comments": "10 pages, 11 figures, 2 tables Accepted to WACV 2021. Project Page:\n  https://bvraghav.com/shad3s/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Hatching is a common method used by artists to accentuate the third dimension\nof a sketch, and to illuminate the scene. Our system SHAD3S attempts to compete\nwith a human at hatching generic three-dimensional (3D) shapes, and also tries\nto assist her in a form exploration exercise. The novelty of our approach lies\nin the fact that we make no assumptions about the input other than that it\nrepresents a 3D shape, and yet, given a contextual information of illumination\nand texture, we synthesise an accurate hatch pattern over the sketch, without\naccess to 3D or pseudo 3D. In the process, we contribute towards a) a cheap yet\neffective method to synthesise a sufficiently large high fidelity dataset,\npertinent to task; b) creating a pipeline with conditional generative\nadversarial network (CGAN); and c) creating an interactive utility with GIMP,\nthat is a tool for artists to engage with automated hatching or a\nform-exploration exercise. User evaluation of the tool suggests that the model\nperformance does generalise satisfactorily over diverse input, both in terms of\nstyle as well as shape. A simple comparison of inception scores suggest that\nthe generated distribution is as diverse as the ground truth.\n", "versions": [{"version": "v1", "created": "Fri, 13 Nov 2020 09:25:46 GMT"}, {"version": "v2", "created": "Thu, 19 Nov 2020 10:41:46 GMT"}], "update_date": "2020-11-20", "authors_parsed": [["Venkataramaiyer", "Raghav B.", ""], ["Joshi", "Abhishek", ""], ["Narang", "Saisha", ""], ["Namboodiri", "Vinay P.", ""]]}, {"id": "2011.06825", "submitter": "Md Saif Hassan Onim", "authors": "Md. Saif Hassan Onim, Aiman Rafeed Ehtesham, Amreen Anbar, A. K. M.\n  Nazrul Islam, A. K. M. Mahbubur Rahman", "title": "LULC classification by semantic segmentation of satellite images using\n  FastFCN", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper analyses how well a Fast Fully Convolutional Network (FastFCN)\nsemantically segments satellite images and thus classifies Land Use/Land\nCover(LULC) classes. Fast-FCN was used on Gaofen-2 Image Dataset (GID-2) to\nsegment them in five different classes: BuiltUp, Meadow, Farmland, Water and\nForest. The results showed better accuracy (0.93), precision (0.99), recall\n(0.98) and mean Intersection over Union (mIoU)(0.97) than other approaches like\nusing FCN-8 or eCognition, a readily available software. We presented a\ncomparison between the results. We propose FastFCN to be both faster and more\naccurate automated method than other existing methods for LULC classification.\n", "versions": [{"version": "v1", "created": "Fri, 13 Nov 2020 09:33:03 GMT"}, {"version": "v2", "created": "Thu, 3 Dec 2020 19:50:31 GMT"}], "update_date": "2020-12-07", "authors_parsed": [["Onim", "Md. Saif Hassan", ""], ["Ehtesham", "Aiman Rafeed", ""], ["Anbar", "Amreen", ""], ["Islam", "A. K. M. Nazrul", ""], ["Rahman", "A. K. M. Mahbubur", ""]]}, {"id": "2011.06837", "submitter": "Rapha\\\"el Candelier", "authors": "Benjamin Gallois and Rapha\\\"el Candelier", "title": "FastTrack: an open-source software for tracking varying numbers of\n  deformable objects", "comments": null, "journal-ref": null, "doi": "10.1371/journal.pcbi.1008697", "report-no": null, "categories": "q-bio.QM cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Analyzing the dynamical properties of mobile objects requires to extract\ntrajectories from recordings, which is often done by tracking movies. We\ncompiled a database of two-dimensional movies for very different biological and\nphysical systems spanning a wide range of length scales and developed a\ngeneral-purpose, optimized, open-source, cross-platform, easy to install and\nuse, self-updating software called FastTrack. It can handle a changing number\nof deformable objects in a region of interest, and is particularly suitable for\nanimal and cell tracking in two-dimensions. Furthermore, we introduce the\nprobability of incursions as a new measure of a movie's trackability that\ndoesn't require the knowledge of ground truth trajectories, since it is\nresilient to small amounts of errors and can be computed on the basis of an ad\nhoc tracking. We also leveraged the versatility and speed of FastTrack to\nimplement an iterative algorithm determining a set of nearly-optimized tracking\nparameters -- yet further reducing the amount of human intervention -- and\ndemonstrate that FastTrack can be used to explore the space of tracking\nparameters to optimize the number of swaps for a batch of similar movies. A\nbenchmark shows that FastTrack is orders of magnitude faster than\nstate-of-the-art tracking algorithms, with a comparable tracking accuracy. The\nsource code is available under the GNU GPLv3 at\nhttps://github.com/FastTrackOrg/FastTrack and pre-compiled binaries for\nWindows, Mac and Linux are available at http://www.fasttrack.sh.\n", "versions": [{"version": "v1", "created": "Fri, 13 Nov 2020 09:52:58 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Gallois", "Benjamin", ""], ["Candelier", "Rapha\u00ebl", ""]]}, {"id": "2011.06838", "submitter": "Marco Camurri", "authors": "David Wisth, Marco Camurri, Sandipan Das, Maurice Fallon", "title": "Unified Multi-Modal Landmark Tracking for Tightly Coupled\n  Lidar-Visual-Inertial Odometry", "comments": "Video: https://youtu.be/MjXYAHurWe8", "journal-ref": null, "doi": "10.1109/LRA.2021.3056380", "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an efficient multi-sensor odometry system for mobile platforms\nthat jointly optimizes visual, lidar, and inertial information within a single\nintegrated factor graph. This runs in real-time at full framerate using fixed\nlag smoothing. To perform such tight integration, a new method to extract 3D\nline and planar primitives from lidar point clouds is presented. This approach\novercomes the suboptimality of typical frame-to-frame tracking methods by\ntreating the primitives as landmarks and tracking them over multiple scans.\nTrue integration of lidar features with standard visual features and IMU is\nmade possible using a subtle passive synchronization of lidar and camera\nframes. The lightweight formulation of the 3D features allows for real-time\nexecution on a single CPU. Our proposed system has been tested on a variety of\nplatforms and scenarios, including underground exploration with a legged robot\nand outdoor scanning with a dynamically moving handheld device, for a total\nduration of 96 min and 2.4 km traveled distance. In these test sequences, using\nonly one exteroceptive sensor leads to failure due to either underconstrained\ngeometry (affecting lidar) or textureless areas caused by aggressive lighting\nchanges (affecting vision). In these conditions, our factor graph naturally\nuses the best information available from each sensor modality without any hard\nswitches.\n", "versions": [{"version": "v1", "created": "Fri, 13 Nov 2020 09:54:03 GMT"}, {"version": "v2", "created": "Thu, 4 Feb 2021 15:48:30 GMT"}, {"version": "v3", "created": "Wed, 17 Feb 2021 11:42:25 GMT"}], "update_date": "2021-02-18", "authors_parsed": [["Wisth", "David", ""], ["Camurri", "Marco", ""], ["Das", "Sandipan", ""], ["Fallon", "Maurice", ""]]}, {"id": "2011.06850", "submitter": "Patrick Bordes Mr", "authors": "Patrick Bordes, Eloi Zablocki, Benjamin Piwowarski, Patrick Gallinari", "title": "Transductive Zero-Shot Learning using Cross-Modal CycleGAN", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In Computer Vision, Zero-Shot Learning (ZSL) aims at classifying unseen\nclasses -- classes for which no matching training image exists. Most of ZSL\nworks learn a cross-modal mapping between images and class labels for seen\nclasses. However, the data distribution of seen and unseen classes might\ndiffer, causing a domain shift problem. Following this observation,\ntransductive ZSL (T-ZSL) assumes that unseen classes and their associated\nimages are known during training, but not their correspondence. As current\nT-ZSL approaches do not scale efficiently when the number of seen classes is\nhigh, we tackle this problem with a new model for T-ZSL based upon CycleGAN.\nOur model jointly (i) projects images on their seen class labels with a\nsupervised objective and (ii) aligns unseen class labels and visual exemplars\nwith adversarial and cycle-consistency objectives. We show the efficiency of\nour Cross-Modal CycleGAN model (CM-GAN) on the ImageNet T-ZSL task where we\nobtain state-of-the-art results. We further validate CM-GAN on a language\ngrounding task, and on a new task that we propose: zero-shot sentence-to-image\nmatching on MS COCO.\n", "versions": [{"version": "v1", "created": "Fri, 13 Nov 2020 10:37:29 GMT"}], "update_date": "2020-11-16", "authors_parsed": [["Bordes", "Patrick", ""], ["Zablocki", "Eloi", ""], ["Piwowarski", "Benjamin", ""], ["Gallinari", "Patrick", ""]]}, {"id": "2011.06852", "submitter": "Jingzheng Tu", "authors": "J. Tu, C. Chen, X. Huang, J. He and X. Guan", "title": "Discriminative Feature Representation with Spatio-temporal Cues for\n  Vehicle Re-identification", "comments": "12 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Vehicle re-identification (re-ID) aims to discover and match the target\nvehicles from a gallery image set taken by different cameras on a wide range of\nroad networks. It is crucial for lots of applications such as security\nsurveillance and traffic management. The remarkably similar appearances of\ndistinct vehicles and the significant changes of viewpoints and illumination\nconditions take grand challenges to vehicle re-ID. Conventional solutions focus\non designing global visual appearances without sufficient consideration of\nvehicles' spatiotamporal relationships in different images. In this paper, we\npropose a novel discriminative feature representation with spatiotemporal clues\n(DFR-ST) for vehicle re-ID. It is capable of building robust features in the\nembedding space by involving appearance and spatio-temporal information. Based\non this multi-modal information, the proposed DFR-ST constructs an appearance\nmodel for a multi-grained visual representation by a two-stream architecture\nand a spatio-temporal metric to provide complementary information. Experimental\nresults on two public datasets demonstrate DFR-ST outperforms the\nstate-of-the-art methods, which validate the effectiveness of the proposed\nmethod.\n", "versions": [{"version": "v1", "created": "Fri, 13 Nov 2020 10:50:21 GMT"}], "update_date": "2020-11-16", "authors_parsed": [["Tu", "J.", ""], ["Chen", "C.", ""], ["Huang", "X.", ""], ["He", "J.", ""], ["Guan", "X.", ""]]}, {"id": "2011.06878", "submitter": "Giulia Cisotto", "authors": "Giulia Cisotto", "title": "REPAC: Reliable estimation of phase-amplitude coupling in brain networks", "comments": null, "journal-ref": "2021 IEEE International Conference on Acoustics, Speech and Signal\n  Processing (ICASSP)", "doi": "10.1109/ICASSP39728.2021.9414749", "report-no": null, "categories": "eess.SP cs.CV cs.LG q-bio.NC q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent evidence has revealed cross-frequency coupling and, particularly,\nphase-amplitude coupling (PAC) as an important strategy for the brain to\naccomplish a variety of high-level cognitive and sensory functions. However,\ndecoding PAC is still challenging. This contribution presents REPAC, a reliable\nand robust algorithm for modeling and detecting PAC events in EEG signals.\nFirst, we explain the synthesis of PAC-like EEG signals, with special attention\nto the most critical parameters that characterize PAC, i.e., SNR, modulation\nindex, duration of coupling. Second, REPAC is introduced in detail. We use\ncomputer simulations to generate a set of random PAC-like EEG signals and test\nthe performance of REPAC with regard to a baseline method. REPAC is shown to\noutperform the baseline method even with realistic values of SNR, e.g., -10 dB.\nThey both reach accuracy levels around 99%, but REPAC leads to a significant\nimprovement of sensitivity, from 20.11% to 65.21%, with comparable specificity\n(around 99%). REPAC is also applied to a real EEG signal showing preliminary\nencouraging results.\n", "versions": [{"version": "v1", "created": "Fri, 13 Nov 2020 12:26:54 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Cisotto", "Giulia", ""]]}, {"id": "2011.06922", "submitter": "Yoav Shalev", "authors": "Yoav Shalev, Lior Wolf", "title": "Image Animation with Perturbed Masks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present a novel approach for image-animation of a source image by a\ndriving video, both depicting the same type of object. We do not assume the\nexistence of pose models and our method is able to animate arbitrary objects\nwithout knowledge of the object's structure. Furthermore, both the driving\nvideo and the source image are only seen during test-time. Our method is based\non a shared mask generator, which separates the foreground object from its\nbackground, and captures the object's general pose and shape. A mask-refinement\nmodule then replaces, in the mask extracted from the driver image, the identity\nof the driver with the identity of the source. Conditioned on the source image,\nthe transformed mask is then decoded by a multi-scale generator that renders a\nrealistic image, in which the content of the source frame is animated by the\npose in the driving video. Due to lack of fully supervised data, we train on\nthe task of reconstructing frames from the same video the source image is taken\nfrom. In order to control {the} source of the identity of the output frame, we\nemploy during training perturbations that remove the unwanted identity\ninformation. Our method is shown to greatly outperform the state of the art\nmethods on multiple benchmarks. Our code and samples are available at\nhttps://github.com/itsyoavshalev/Image-Animation-with-Perturbed-Masks\n", "versions": [{"version": "v1", "created": "Fri, 13 Nov 2020 14:17:17 GMT"}, {"version": "v2", "created": "Wed, 18 Nov 2020 19:23:52 GMT"}], "update_date": "2020-11-20", "authors_parsed": [["Shalev", "Yoav", ""], ["Wolf", "Lior", ""]]}, {"id": "2011.06923", "submitter": "Richard Schoonhoven", "authors": "Richard Schoonhoven, Allard A. Hendriksen, Dani\\\"el M. Pelt, K. Joost\n  Batenburg", "title": "LEAN: graph-based pruning for convolutional neural networks by\n  extracting longest chains", "comments": "8 pages + 2 pages references. Code will be made public via GitHub\n  soon", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional neural networks (CNNs) have proven to be highly successful at a\nrange of image-to-image tasks. CNNs can be computationally expensive, which can\nlimit their applicability in practice. Model pruning can improve computational\nefficiency by sparsifying trained networks. Common methods for pruning CNNs\ndetermine what convolutional filters to remove by ranking filters on an\nindividual basis. However, filters are not independent, as CNNs consist of\nchains of convolutions, which can result in sub-optimal filter selection.\n  We propose a novel pruning method, LongEst-chAiN (LEAN) pruning, which takes\nthe interdependency between the convolution operations into account. We propose\nto prune CNNs by using graph-based algorithms to select relevant chains of\nconvolutions. A CNN is interpreted as a graph, with the operator norm of each\nconvolution as distance metric for the edges. LEAN pruning iteratively extracts\nthe highest value path from the graph to keep. In our experiments, we test LEAN\npruning for several image-to-image tasks, including the well-known CamVid\ndataset. LEAN pruning enables us to keep just 0.5%-2% of the convolutions\nwithout significant loss of accuracy. When pruning CNNs with LEAN, we achieve a\nhigher accuracy than pruning filters individually, and different pruned\nsubstructures emerge.\n", "versions": [{"version": "v1", "created": "Fri, 13 Nov 2020 14:17:51 GMT"}], "update_date": "2020-11-16", "authors_parsed": [["Schoonhoven", "Richard", ""], ["Hendriksen", "Allard A.", ""], ["Pelt", "Dani\u00ebl M.", ""], ["Batenburg", "K. Joost", ""]]}, {"id": "2011.06928", "submitter": "MD Tanzil Shahriar", "authors": "Md Tanzil Shahriar, Huyue Li", "title": "A Study of Image Pre-processing for Faster Object Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quality of image always plays a vital role in in-creasing object recognition\nor classification rate. A good quality image gives better recognition or\nclassification rate than any unprocessed noisy images. It is more difficult to\nextract features from such unprocessed images which in-turn reduces object\nrecognition or classification rate. To overcome problems occurred due to low\nquality image, typically pre-processing is done before extracting features from\nthe image. Our project proposes an image pre-processing method, so that the\nperformance of selected Machine Learning algorithms or Deep Learning algorithms\nincreases in terms of increased accuracy or reduced the number of training\nimages. In the later part, we compare the performance results by using our\nmethod with the previous used approaches.\n", "versions": [{"version": "v1", "created": "Sat, 31 Oct 2020 02:55:17 GMT"}], "update_date": "2020-11-16", "authors_parsed": [["Shahriar", "Md Tanzil", ""], ["Li", "Huyue", ""]]}, {"id": "2011.06958", "submitter": "Guillaume Vaudaux-Ruth", "authors": "Guillaume Vaudaux-Ruth, Adrien Chan-Hon-Tong, Catherine Achard", "title": "SALAD: Self-Assessment Learning for Action Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Literature on self-assessment in machine learning mainly focuses on the\nproduction of well-calibrated algorithms through consensus frameworks i.e.\ncalibration is seen as a problem. Yet, we observe that learning to be properly\nconfident could behave like a powerful regularization and thus, could be an\nopportunity to improve performance.Precisely, we show that used within a\nframework of action detection, the learning of a self-assessment score is able\nto improve the whole action localization process.Experimental results show that\nour approach outperforms the state-of-the-art on two action detection\nbenchmarks. On THUMOS14 dataset, the mAP at tIoU@0.5 is improved from 42.8\\% to\n44.6\\%, and from 50.4\\% to 51.7\\% on ActivityNet1.3 dataset. For lower tIoU\nvalues, we achieve even more significant improvements on both datasets.\n", "versions": [{"version": "v1", "created": "Fri, 13 Nov 2020 15:10:40 GMT"}], "update_date": "2020-11-16", "authors_parsed": [["Vaudaux-Ruth", "Guillaume", ""], ["Chan-Hon-Tong", "Adrien", ""], ["Achard", "Catherine", ""]]}, {"id": "2011.06961", "submitter": "Daniel Seichter", "authors": "Daniel Seichter, Mona K\\\"ohler, Benjamin Lewandowski, Tim Wengefeld\n  and Horst-Michael Gross", "title": "Efficient RGB-D Semantic Segmentation for Indoor Scene Analysis", "comments": "To be published in IEEE International Conference on Robotics and\n  Automation (ICRA) 2021; fixed reference in Fig. 4", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Analyzing scenes thoroughly is crucial for mobile robots acting in different\nenvironments. Semantic segmentation can enhance various subsequent tasks, such\nas (semantically assisted) person perception, (semantic) free space detection,\n(semantic) mapping, and (semantic) navigation. In this paper, we propose an\nefficient and robust RGB-D segmentation approach that can be optimized to a\nhigh degree using NVIDIA TensorRT and, thus, is well suited as a common initial\nprocessing step in a complex system for scene analysis on mobile robots. We\nshow that RGB-D segmentation is superior to processing RGB images solely and\nthat it can still be performed in real time if the network architecture is\ncarefully designed. We evaluate our proposed Efficient Scene Analysis Network\n(ESANet) on the common indoor datasets NYUv2 and SUNRGB-D and show that we\nreach state-of-the-art performance while enabling faster inference.\nFurthermore, our evaluation on the outdoor dataset Cityscapes shows that our\napproach is suitable for other areas of application as well. Finally, instead\nof presenting benchmark results only, we also show qualitative results in one\nof our indoor application scenarios.\n", "versions": [{"version": "v1", "created": "Fri, 13 Nov 2020 15:17:31 GMT"}, {"version": "v2", "created": "Tue, 6 Apr 2021 14:25:58 GMT"}, {"version": "v3", "created": "Wed, 7 Apr 2021 14:41:24 GMT"}], "update_date": "2021-04-08", "authors_parsed": [["Seichter", "Daniel", ""], ["K\u00f6hler", "Mona", ""], ["Lewandowski", "Benjamin", ""], ["Wengefeld", "Tim", ""], ["Gross", "Horst-Michael", ""]]}, {"id": "2011.06978", "submitter": "Nicolas Pugeault Dr", "authors": "Faisal Alamri, Sinan Kalkan and Nicolas Pugeault", "title": "Transformer-Encoder Detector Module: Using Context to Improve Robustness\n  to Adversarial Attacks on Object Detection", "comments": "Accepted for the 25th International Conference on Pattern Recognition\n  (ICPR'2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Deep neural network approaches have demonstrated high performance in object\nrecognition (CNN) and detection (Faster-RCNN) tasks, but experiments have shown\nthat such architectures are vulnerable to adversarial attacks (FFF, UAP): low\namplitude perturbations, barely perceptible by the human eye, can lead to a\ndrastic reduction in labeling performance. This article proposes a new context\nmodule, called \\textit{Transformer-Encoder Detector Module}, that can be\napplied to an object detector to (i) improve the labeling of object instances;\nand (ii) improve the detector's robustness to adversarial attacks. The proposed\nmodel achieves higher mAP, F1 scores and AUC average score of up to 13\\%\ncompared to the baseline Faster-RCNN detector, and an mAP score 8 points higher\non images subjected to FFF or UAP attacks due to the inclusion of both\ncontextual and visual features extracted from scene and encoded into the model.\nThe result demonstrates that a simple ad-hoc context module can improve the\nreliability of object detectors significantly.\n", "versions": [{"version": "v1", "created": "Fri, 13 Nov 2020 15:52:53 GMT"}], "update_date": "2020-11-16", "authors_parsed": [["Alamri", "Faisal", ""], ["Kalkan", "Sinan", ""], ["Pugeault", "Nicolas", ""]]}, {"id": "2011.06982", "submitter": "Raghavendra Selvan", "authors": "Raghavendra Selvan, Silas {\\O}rting, Erik B Dam", "title": "Multi-layered tensor networks for image classification", "comments": "Updated version with exact computation costs. 6 pages. Accepted to\n  the First Workshop on Quantum Tensor Networks in Machine Learning. In\n  conjunction with 34th NeurIPS, 2020. Source code at\n  https://github.com/raghavian/mltn", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The recently introduced locally orderless tensor network (LoTeNet) for\nsupervised image classification uses matrix product state (MPS) operations on\ngrids of transformed image patches. The resulting patch representations are\ncombined back together into the image space and aggregated hierarchically using\nmultiple MPS blocks per layer to obtain the final decision rules. In this work,\nwe propose a non-patch based modification to LoTeNet that performs one MPS\noperation per layer, instead of several patch-level operations. The spatial\ninformation in the input images to MPS blocks at each layer is squeezed into\nthe feature dimension, similar to LoTeNet, to maximise retained spatial\ncorrelation between pixels when images are flattened into 1D vectors. The\nproposed multi-layered tensor network (MLTN) is capable of learning linear\ndecision boundaries in high dimensional spaces in a multi-layered setting,\nwhich results in a reduction in the computation cost compared to LoTeNet\nwithout any degradation in performance.\n", "versions": [{"version": "v1", "created": "Fri, 13 Nov 2020 16:01:26 GMT"}, {"version": "v2", "created": "Fri, 19 Mar 2021 11:37:15 GMT"}], "update_date": "2021-03-22", "authors_parsed": [["Selvan", "Raghavendra", ""], ["\u00d8rting", "Silas", ""], ["Dam", "Erik B", ""]]}, {"id": "2011.06984", "submitter": "Lipei Zhang", "authors": "Guanwen Qiu, Xiaobing Yu, Baolin Sun, Yunpeng Wang, Lipei Zhang", "title": "Metastatic Cancer Image Classification Based On Deep Learning Method", "comments": "4 pages, 3 figures, 1 table, accepted by ICCECE", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Using histopathological images to automatically classify cancer is a\ndifficult task for accurately detecting cancer, especially to identify\nmetastatic cancer in small image patches obtained from larger digital pathology\nscans. Computer diagnosis technology has attracted wide attention from\nresearchers. In this paper, we propose a noval method which combines the deep\nlearning algorithm in image classification, the DenseNet169 framework and\nRectified Adam optimization algorithm. The connectivity pattern of DenseNet is\ndirect connections from any layer to all consecutive layers, which can\neffectively improve the information flow between different layers. With the\nfact that RAdam is not easy to fall into a local optimal solution, and it can\nconverge quickly in model training. The experimental results shows that our\nmodel achieves superior performance over the other classical convolutional\nneural networks approaches, such as Vgg19, Resnet34, Resnet50. In particular,\nthe Auc-Roc score of our DenseNet169 model is 1.77% higher than Vgg19 model,\nand the Accuracy score is 1.50% higher. Moreover, we also study the\nrelationship between loss value and batches processed during the training stage\nand validation stage, and obtain some important and interesting findings.\n", "versions": [{"version": "v1", "created": "Fri, 13 Nov 2020 16:04:39 GMT"}], "update_date": "2020-11-16", "authors_parsed": [["Qiu", "Guanwen", ""], ["Yu", "Xiaobing", ""], ["Sun", "Baolin", ""], ["Wang", "Yunpeng", ""], ["Zhang", "Lipei", ""]]}, {"id": "2011.07008", "submitter": "Jan Hausberg", "authors": "Jan Hausberg, Ryoichi Ishikawa, Menandro Roxas, Takeshi Oishi", "title": "Relative Drone-Ground Vehicle Localization using LiDAR and Fisheye\n  Cameras through Direct and Indirect Observations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimating the pose of an unmanned aerial vehicle (UAV) or drone is a\nchallenging task. It is useful for many applications such as navigation,\nsurveillance, tracking objects on the ground, and 3D reconstruction. In this\nwork, we present a LiDAR-camera-based relative pose estimation method between a\ndrone and a ground vehicle, using a LiDAR sensor and a fisheye camera on the\nvehicle's roof and another fisheye camera mounted under the drone. The LiDAR\nsensor directly observes the drone and measures its position, and the two\ncameras estimate the relative orientation using indirect observation of the\nsurrounding objects. We propose a dynamically adaptive kernel-based method for\ndrone detection and tracking using the LiDAR. We detect vanishing points in\nboth cameras and find their correspondences to estimate the relative\norientation. Additionally, we propose a rotation correction technique by\nrelying on the observed motion of the drone through the LiDAR. In our\nexperiments, we were able to achieve very fast initial detection and real-time\ntracking of the drone. Our method is fully automatic.\n", "versions": [{"version": "v1", "created": "Fri, 13 Nov 2020 16:41:55 GMT"}, {"version": "v2", "created": "Mon, 16 Nov 2020 11:14:13 GMT"}, {"version": "v3", "created": "Tue, 17 Nov 2020 10:37:49 GMT"}], "update_date": "2020-11-18", "authors_parsed": [["Hausberg", "Jan", ""], ["Ishikawa", "Ryoichi", ""], ["Roxas", "Menandro", ""], ["Oishi", "Takeshi", ""]]}, {"id": "2011.07010", "submitter": "Pasquale Antonante", "authors": "Pasquale Antonante, David I. Spivak, Luca Carlone", "title": "Monitoring and Diagnosability of Perception Systems", "comments": "Updated version of arXiv:2005.11816", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Perception is a critical component of high-integrity applications of robotics\nand autonomous systems, such as self-driving vehicles. In these applications,\nfailure of perception systems may put human life at risk, and a broad adoption\nof these technologies requires the development of methodologies to guarantee\nand monitor safe operation. Despite the paramount importance of perception\nsystems, currently there is no formal approach for system-level monitoring. In\nthis work, we propose a mathematical model for runtime monitoring and fault\ndetection and identification in perception systems. Towards this goal, we draw\nconnections with the literature on diagnosability in multiprocessor systems,\nand generalize it to account for modules with heterogeneous outputs that\ninteract over time. The resulting temporal diagnostic graphs (i) provide a\nframework to reason over the consistency of perception outputs -- across\nmodules and over time -- thus enabling fault detection, (ii) allow us to\nestablish formal guarantees on the maximum number of faults that can be\nuniquely identified in a given perception system, and (iii) enable the design\nof efficient algorithms for fault identification. We demonstrate our monitoring\nsystem, dubbed PerSyS, in realistic simulations using the LGSVL self-driving\nsimulator and the Apollo Auto autonomy software stack, and show that PerSyS is\nable to detect failures in challenging scenarios (including scenarios that have\ncaused self-driving car accidents in recent years), and is able to correctly\nidentify faults while entailing a minimal computation overhead (< 5 ms on a\nsingle-core CPU).\n", "versions": [{"version": "v1", "created": "Wed, 11 Nov 2020 23:03:14 GMT"}, {"version": "v2", "created": "Mon, 16 Nov 2020 18:30:29 GMT"}, {"version": "v3", "created": "Thu, 19 Nov 2020 16:51:50 GMT"}, {"version": "v4", "created": "Wed, 10 Mar 2021 22:23:17 GMT"}], "update_date": "2021-03-12", "authors_parsed": [["Antonante", "Pasquale", ""], ["Spivak", "David I.", ""], ["Carlone", "Luca", ""]]}, {"id": "2011.07017", "submitter": "Paula Harder", "authors": "Paula Harder, William Jones, Redouane Lguensat, Shahine Bouabid, James\n  Fulton, D\\'anell Quesada-Chac\\'on, Aris Marcolongo, Sofija Stefanovi\\'c,\n  Yuhan Rao, Peter Manshausen, Duncan Watson-Parris", "title": "NightVision: Generating Nighttime Satellite Imagery from Infra-Red\n  Observations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The recent explosion in applications of machine learning to satellite imagery\noften rely on visible images and therefore suffer from a lack of data during\nthe night. The gap can be filled by employing available infra-red observations\nto generate visible images. This work presents how deep learning can be applied\nsuccessfully to create those images by using U-Net based architectures. The\nproposed methods show promising results, achieving a structural similarity\nindex (SSIM) up to 86\\% on an independent test set and providing visually\nconvincing output images, generated from infra-red observations.\n", "versions": [{"version": "v1", "created": "Fri, 13 Nov 2020 16:55:46 GMT"}, {"version": "v2", "created": "Tue, 8 Dec 2020 15:43:52 GMT"}], "update_date": "2020-12-09", "authors_parsed": [["Harder", "Paula", ""], ["Jones", "William", ""], ["Lguensat", "Redouane", ""], ["Bouabid", "Shahine", ""], ["Fulton", "James", ""], ["Quesada-Chac\u00f3n", "D\u00e1nell", ""], ["Marcolongo", "Aris", ""], ["Stefanovi\u0107", "Sofija", ""], ["Rao", "Yuhan", ""], ["Manshausen", "Peter", ""], ["Watson-Parris", "Duncan", ""]]}, {"id": "2011.07019", "submitter": "Edward Chen", "authors": "Edward Chen and Tejas Sudharshan Mathai and Vinit Sarode and Howie\n  Choset and John Galeotti", "title": "A Study of Domain Generalization on Ultrasound-based Multi-Class\n  Segmentation of Arteries, Veins, Ligaments, and Nerves Using Transfer\n  Learning", "comments": "Machine Learning for Health (ML4H) at NeurIPS 2020 - Extended\n  Abstract", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Identifying landmarks in the femoral area is crucial for ultrasound (US)\n-based robot-guided catheter insertion, and their presentation varies when\nimaged with different scanners. As such, the performance of past deep\nlearning-based approaches is also narrowly limited to the training data\ndistribution; this can be circumvented by fine-tuning all or part of the model,\nyet the effects of fine-tuning are seldom discussed. In this work, we study the\nUS-based segmentation of multiple classes through transfer learning by\nfine-tuning different contiguous blocks within the model, and evaluating on a\ngamut of US data from different scanners and settings. We propose a simple\nmethod for predicting generalization on unseen datasets and observe\nstatistically significant differences between the fine-tuning methods while\nworking towards domain generalization.\n", "versions": [{"version": "v1", "created": "Fri, 13 Nov 2020 16:59:20 GMT"}], "update_date": "2020-11-16", "authors_parsed": [["Chen", "Edward", ""], ["Mathai", "Tejas Sudharshan", ""], ["Sarode", "Vinit", ""], ["Choset", "Howie", ""], ["Galeotti", "John", ""]]}, {"id": "2011.07025", "submitter": "J\\\"org Sander", "authors": "J\\\"org Sander, Bob D. de Vos and Ivana I\\v{s}gum", "title": "Automatic segmentation with detection of local segmentation failures in\n  cardiac MRI", "comments": null, "journal-ref": "currently under press - Scientific Reports (2020)", "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Segmentation of cardiac anatomical structures in cardiac magnetic resonance\nimages (CMRI) is a prerequisite for automatic diagnosis and prognosis of\ncardiovascular diseases. To increase robustness and performance of segmentation\nmethods this study combines automatic segmentation and assessment of\nsegmentation uncertainty in CMRI to detect image regions containing local\nsegmentation failures. Three state-of-the-art convolutional neural networks\n(CNN) were trained to automatically segment cardiac anatomical structures and\nobtain two measures of predictive uncertainty: entropy and a measure derived by\nMC-dropout. Thereafter, using the uncertainties another CNN was trained to\ndetect local segmentation failures that potentially need correction by an\nexpert. Finally, manual correction of the detected regions was simulated. Using\npublicly available CMR scans from the MICCAI 2017 ACDC challenge, the impact of\nCNN architecture and loss function for segmentation, and the uncertainty\nmeasure was investigated. Performance was evaluated using the Dice coefficient\nand 3D Hausdorff distance between manual and automatic segmentation. The\nexperiments reveal that combining automatic segmentation with simulated manual\ncorrection of detected segmentation failures leads to statistically significant\nperformance increase.\n", "versions": [{"version": "v1", "created": "Fri, 13 Nov 2020 17:19:05 GMT"}], "update_date": "2020-11-16", "authors_parsed": [["Sander", "J\u00f6rg", ""], ["de Vos", "Bob D.", ""], ["I\u0161gum", "Ivana", ""]]}, {"id": "2011.07048", "submitter": "C\\'ecilia Ostertag", "authors": "Cecilia Ostertag, Marie Beurton-Aimar", "title": "Using Graph Neural Networks to Reconstruct Ancient Documents", "comments": "Accepted at Pattern Recognition for Cultural Heritage (PatReCH) -\n  Workshop in conjunction with ICPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, machine learning and deep learning approaches such as\nartificial neural networks have gained in popularity for the resolution of\nautomatic puzzle resolution problems. Indeed, these methods are able to extract\nhigh-level representations from images, and then can be trained to separate\nmatching image pieces from non-matching ones. These applications have many\nsimilarities to the problem of ancient document reconstruction from partially\nrecovered fragments. In this work we present a solution based on a Graph Neural\nNetwork, using pairwise patch information to assign labels to edges\nrepresenting the spatial relationships between pairs. This network classifies\nthe relationship between a source and a target patch as being one of Up, Down,\nLeft, Right or None. By doing so for all edges, our model outputs a new graph\nrepresenting a reconstruction proposal. Finally, we show that our model is not\nonly able to provide correct classifications at the edge-level, but also to\ngenerate partial or full reconstruction graphs from a set of patches.\n", "versions": [{"version": "v1", "created": "Fri, 13 Nov 2020 18:36:36 GMT"}], "update_date": "2020-11-16", "authors_parsed": [["Ostertag", "Cecilia", ""], ["Beurton-Aimar", "Marie", ""]]}, {"id": "2011.07068", "submitter": "Santiago L\\'opez-Tapia", "authors": "Santiago L\\'opez-Tapia and Nicol\\'as P\\'erez de la Blanca", "title": "Fast and Robust Cascade Model for Multiple Degradation Single Image\n  Super-Resolution", "comments": "12 pages and 11 figures (8 figures and 3 tables)", "journal-ref": null, "doi": "10.1109/TIP.2021.3074821", "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Single Image Super-Resolution (SISR) is one of the low-level computer vision\nproblems that has received increased attention in the last few years. Current\napproaches are primarily based on harnessing the power of deep learning models\nand optimization techniques to reverse the degradation model. Owing to its\nhardness, isotropic blurring or Gaussians with small anisotropic deformations\nhave been mainly considered. Here, we widen this scenario by including large\nnon-Gaussian blurs that arise in real camera movements. Our approach leverages\nthe degradation model and proposes a new formulation of the Convolutional\nNeural Network (CNN) cascade model, where each network sub-module is\nconstrained to solve a specific degradation: deblurring or upsampling. A new\ndensely connected CNN-architecture is proposed where the output of each\nsub-module is restricted using some external knowledge to focus it on its\nspecific task. As far we know this use of domain-knowledge to module-level is a\nnovelty in SISR. To fit the finest model, a final sub-module takes care of the\nresidual errors propagated by the previous sub-modules. We check our model with\nthree state of the art (SOTA) datasets in SISR and compare the results with the\nSOTA models. The results show that our model is the only one able to manage our\nwider set of deformations. Furthermore, our model overcomes all current SOTA\nmethods for a standard set of deformations. In terms of computational load, our\nmodel also improves on the two closest competitors in terms of efficiency.\nAlthough the approach is non-blind and requires an estimation of the blur\nkernel, it shows robustness to blur kernel estimation errors, making it a good\nalternative to blind models.\n", "versions": [{"version": "v1", "created": "Mon, 16 Nov 2020 18:59:49 GMT"}], "update_date": "2021-05-19", "authors_parsed": [["L\u00f3pez-Tapia", "Santiago", ""], ["de la Blanca", "Nicol\u00e1s P\u00e9rez", ""]]}, {"id": "2011.07092", "submitter": "Ramyad Hadidi", "authors": "Ramyad Hadidi, Jiashen Cao, Michael S. Ryoo, Hyesoon Kim", "title": "Reducing Inference Latency with Concurrent Architectures for Image\n  Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Satisfying the high computation demand of modern deep learning architectures\nis challenging for achieving low inference latency. The current approaches in\ndecreasing latency only increase parallelism within a layer. This is because\narchitectures typically capture a single-chain dependency pattern that prevents\nefficient distribution with a higher concurrency (i.e., simultaneous execution\nof one inference among devices). Such single-chain dependencies are so\nwidespread that even implicitly biases recent neural architecture search (NAS)\nstudies. In this visionary paper, we draw attention to an entirely new space of\nNAS that relaxes the single-chain dependency to provide higher concurrency and\ndistribution opportunities. To quantitatively compare these architectures, we\npropose a score that encapsulates crucial metrics such as communication,\nconcurrency, and load balancing. Additionally, we propose a new generator and\ntransformation block that consistently deliver superior architectures compared\nto current state-of-the-art methods. Finally, our preliminary results show that\nthese new architectures reduce the inference latency and deserve more\nattention.\n", "versions": [{"version": "v1", "created": "Fri, 13 Nov 2020 19:15:02 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["Hadidi", "Ramyad", ""], ["Cao", "Jiashen", ""], ["Ryoo", "Michael S.", ""], ["Kim", "Hyesoon", ""]]}, {"id": "2011.07112", "submitter": "Raghad Alghonaim", "authors": "Raghad Alghonaim and Edward Johns", "title": "Benchmarking Domain Randomisation for Visual Sim-to-Real Transfer", "comments": "Published at ICRA 2021. For project page, please visit:\n  https://www.robot-learning.uk/benchmarking-domain-randomisation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Domain randomisation is a very popular method for visual sim-to-real transfer\nin robotics, due to its simplicity and ability to achieve transfer without any\nreal-world images at all. Nonetheless, a number of design choices must be made\nto achieve optimal transfer. In this paper, we perform a comprehensive\nbenchmarking study on these different choices, with two key experiments\nevaluated on a real-world object pose estimation task. First, we study the\nrendering quality, and find that a small number of high-quality images is\nsuperior to a large number of low-quality images. Second, we study the type of\nrandomisation, and find that both distractors and textures are important for\ngeneralisation to novel environments.\n", "versions": [{"version": "v1", "created": "Fri, 13 Nov 2020 20:13:18 GMT"}, {"version": "v2", "created": "Sun, 4 Apr 2021 06:26:56 GMT"}, {"version": "v3", "created": "Fri, 21 May 2021 12:22:11 GMT"}], "update_date": "2021-05-24", "authors_parsed": [["Alghonaim", "Raghad", ""], ["Johns", "Edward", ""]]}, {"id": "2011.07118", "submitter": "Luis Riera", "authors": "Luis G Riera, Matthew E. Carroll, Zhisheng Zhang, Johnathon M. Shook,\n  Sambuddha Ghosal, Tianshuang Gao, Arti Singh, Sourabh Bhattacharya, Baskar\n  Ganapathysubramanian, Asheesh K. Singh, Soumik Sarkar", "title": "Deep Multi-view Image Fusion for Soybean Yield Estimation in Breeding\n  Applications Deep Multi-view Image Fusion for Soybean Yield Estimation in\n  Breeding Applications", "comments": "18 pages, 8 figures, and 3 Tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Reliable seed yield estimation is an indispensable step in plant breeding\nprograms geared towards cultivar development in major row crops. The objective\nof this study is to develop a machine learning (ML) approach adept at soybean\n[\\textit{Glycine max} L. (Merr.)] pod counting to enable genotype seed yield\nrank prediction from in-field video data collected by a ground robot. To meet\nthis goal, we developed a multi-view image-based yield estimation framework\nutilizing deep learning architectures. Plant images captured from different\nangles were fused to estimate the yield and subsequently to rank soybean\ngenotypes for application in breeding decisions. We used data from controlled\nimaging environment in field, as well as from plant breeding test plots in\nfield to demonstrate the efficacy of our framework via comparing performance\nwith manual pod counting and yield estimation.\n  Our results demonstrate the promise of ML models in making breeding decisions\nwith significant reduction of time and human effort, and opening new breeding\nmethods avenues to develop cultivars.\n", "versions": [{"version": "v1", "created": "Fri, 13 Nov 2020 20:37:04 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["Riera", "Luis G", ""], ["Carroll", "Matthew E.", ""], ["Zhang", "Zhisheng", ""], ["Shook", "Johnathon M.", ""], ["Ghosal", "Sambuddha", ""], ["Gao", "Tianshuang", ""], ["Singh", "Arti", ""], ["Bhattacharya", "Sourabh", ""], ["Ganapathysubramanian", "Baskar", ""], ["Singh", "Asheesh K.", ""], ["Sarkar", "Soumik", ""]]}, {"id": "2011.07142", "submitter": "Alec Koppel", "authors": "Abhishek Chakraborty, Ketan Rajawat, Alec Koppel", "title": "Sparse Representations of Positive Functions via Projected Pseudo-Mirror\n  Descent", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG math.OC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We consider the problem of expected risk minimization when the population\nloss is strongly convex and the target domain of the decision variable is\nrequired to be nonnegative, motivated by the settings of maximum likelihood\nestimation (MLE) and trajectory optimization. We restrict focus to the case\nthat the decision variable belongs to a nonparametric Reproducing Kernel\nHilbert Space (RKHS). To solve it, we consider stochastic mirror descent that\nemploys (i) pseudo-gradients and (ii) projections. Compressive projections are\nexecuted via kernel orthogonal matching pursuit (KOMP), and overcome the fact\nthat the vanilla RKHS parameterization grows unbounded with time. Moreover,\npseudo-gradients are needed, e.g., when stochastic gradients themselves define\nintegrals over unknown quantities that must be evaluated numerically, as in\nestimating the intensity parameter of an inhomogeneous Poisson Process, and\nmulti-class kernel logistic regression with latent multi-kernels. We establish\ntradeoffs between accuracy of convergence in mean and the projection budget\nparameter under constant step-size and compression budget, as well as\nnon-asymptotic bounds on the model complexity. Experiments demonstrate that we\nachieve state-of-the-art accuracy and complexity tradeoffs for inhomogeneous\nPoisson Process intensity estimation and multi-class kernel logistic\nregression.\n", "versions": [{"version": "v1", "created": "Fri, 13 Nov 2020 21:54:28 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["Chakraborty", "Abhishek", ""], ["Rajawat", "Ketan", ""], ["Koppel", "Alec", ""]]}, {"id": "2011.07173", "submitter": "Joao Florindo", "authors": "Jardel Vieira, Eduardo Abreu, Joao B. Florindo", "title": "Texture image classification based on a pseudo-parabolic diffusion model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.NA math.NA", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This work proposes a novel method based on a pseudo-parabolic diffusion\nprocess to be employed for texture recognition. The proposed operator is\napplied over a range of time scales giving rise to a family of images\ntransformed by nonlinear filters. Therefore each of those images are encoded by\na local descriptor (we use local binary patterns for that purpose) and they are\nsummarized by a simple histogram, yielding in this way the image feature\nvector. The proposed approach is tested on the classification of well\nestablished benchmark texture databases and on a practical task of plant\nspecies recognition. In both cases, it is compared with several\nstate-of-the-art methodologies employed for texture recognition. Our proposal\noutperforms those methods in terms of classification accuracy, confirming its\ncompetitiveness. The good performance can be justified to a large extent by the\nability of the pseudo-parabolic operator to smooth possibly noisy details\ninside homogeneous regions of the image at the same time that it preserves\ndiscontinuities that convey critical information for the object description.\nSuch results also confirm that model-based approaches like the proposed one can\nstill be competitive with the omnipresent learning-based approaches, especially\nwhen the user does not have access to a powerful computational structure and a\nlarge amount of labeled data for training.\n", "versions": [{"version": "v1", "created": "Sat, 14 Nov 2020 00:04:07 GMT"}, {"version": "v2", "created": "Sun, 24 Jan 2021 00:39:00 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Vieira", "Jardel", ""], ["Abreu", "Eduardo", ""], ["Florindo", "Joao B.", ""]]}, {"id": "2011.07184", "submitter": "Rajesh Menon", "authors": "Ruipeng Guo, Soren Nelson, and Rajesh Menon", "title": "A needle-based deep-neural-network camera", "comments": null, "journal-ref": null, "doi": "10.1364/AO.415059", "report-no": null, "categories": "eess.IV cs.CV physics.optics", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We experimentally demonstrate a camera whose primary optic is a cannula\n(diameter=0.22mm and length=12.5mm) that acts a lightpipe transporting light\nintensity from an object plane (35cm away) to its opposite end. Deep neural\nnetworks (DNNs) are used to reconstruct color and grayscale images with field\nof view of 180 and angular resolution of ~0.40. When trained on images with\ndepth information, the DNN can create depth maps. Finally, we show DNN-based\nclassification of the EMNIST dataset without and with image reconstructions.\nThe former could be useful for imaging with enhanced privacy.\n", "versions": [{"version": "v1", "created": "Sat, 14 Nov 2020 01:39:14 GMT"}], "update_date": "2021-04-07", "authors_parsed": [["Guo", "Ruipeng", ""], ["Nelson", "Soren", ""], ["Menon", "Rajesh", ""]]}, {"id": "2011.07188", "submitter": "Chenglong Li", "authors": "Andong Lu, Cun Qian, Chenglong Li, Jin Tang and Liang Wang", "title": "Duality-Gated Mutual Condition Network for RGBT Tracking", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Low-quality modalities contain not only a lot of noisy information but also\nsome discriminative features in RGBT tracking. However, the potentials of\nlow-quality modalities are not well explored in existing RGBT tracking\nalgorithms. In this work, we propose a novel duality-gated mutual condition\nnetwork to fully exploit the discriminative information of all modalities while\nsuppressing the effects of data noise. In specific, we design a mutual\ncondition module, which takes the discriminative information of a modality as\nthe condition to guide feature learning of target appearance in another\nmodality. Such module can effectively enhance target representations of all\nmodalities even in the presence of low-quality modalities. To improve the\nquality of conditions and further reduce data noise, we propose a duality-gated\nmechanism and integrate it into the mutual condition module. To deal with the\ntracking failure caused by sudden camera motion, which often occurs in RGBT\ntracking, we design a resampling strategy based on optical flow algorithms. It\ndoes not increase much computational cost since we perform optical flow\ncalculation only when the model prediction is unreliable and then execute\nresampling when the sudden camera motion is detected. Extensive experiments on\nfour RGBT tracking benchmark datasets show that our method performs favorably\nagainst the state-of-the-art tracking algorithms\n", "versions": [{"version": "v1", "created": "Sat, 14 Nov 2020 01:48:36 GMT"}, {"version": "v2", "created": "Mon, 30 Nov 2020 13:11:43 GMT"}], "update_date": "2020-12-01", "authors_parsed": [["Lu", "Andong", ""], ["Qian", "Cun", ""], ["Li", "Chenglong", ""], ["Tang", "Jin", ""], ["Wang", "Liang", ""]]}, {"id": "2011.07189", "submitter": "Chenglong Li", "authors": "Andong Lu, Chenglong Li, Yuqing Yan, Jin Tang, and Bin Luo", "title": "RGBT Tracking via Multi-Adapter Network with Hierarchical Divergence\n  Loss", "comments": "Accepted by IEEE TIP", "journal-ref": null, "doi": "10.1109/TIP.2021.3087341", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  RGBT tracking has attracted increasing attention since RGB and thermal\ninfrared data have strong complementary advantages, which could make trackers\nall-day and all-weather work. However, how to effectively represent RGBT data\nfor visual tracking remains unstudied well. Existing works usually focus on\nextracting modality-shared or modality-specific information, but the potentials\nof these two cues are not well explored and exploited in RGBT tracking. In this\npaper, we propose a novel multi-adapter network to jointly perform\nmodality-shared, modality-specific and instance-aware target representation\nlearning for RGBT tracking. To this end, we design three kinds of adapters\nwithin an end-to-end deep learning framework. In specific, we use the modified\nVGG-M as the generality adapter to extract the modality-shared target\nrepresentations.To extract the modality-specific features while reducing the\ncomputational complexity, we design a modality adapter, which adds a small\nblock to the generality adapter in each layer and each modality in a parallel\nmanner. Such a design could learn multilevel modality-specific representations\nwith a modest number of parameters as the vast majority of parameters are\nshared with the generality adapter. We also design instance adapter to capture\nthe appearance properties and temporal variations of a certain target.\nMoreover, to enhance the shared and specific features, we employ the loss of\nmultiple kernel maximum mean discrepancy to measure the distribution divergence\nof different modal features and integrate it into each layer for more robust\nrepresentation learning. Extensive experiments on two RGBT tracking benchmark\ndatasets demonstrate the outstanding performance of the proposed tracker\nagainst the state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Sat, 14 Nov 2020 01:50:46 GMT"}, {"version": "v2", "created": "Wed, 10 Mar 2021 14:06:54 GMT"}, {"version": "v3", "created": "Fri, 4 Jun 2021 06:53:08 GMT"}], "update_date": "2021-07-07", "authors_parsed": [["Lu", "Andong", ""], ["Li", "Chenglong", ""], ["Yan", "Yuqing", ""], ["Tang", "Jin", ""], ["Luo", "Bin", ""]]}, {"id": "2011.07197", "submitter": "Andrew Pryhuber", "authors": "Andrew Pryhuber, Rainer Sinn, Rekha R. Thomas", "title": "Existence of Two View Chiral Reconstructions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV math.AG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A fundamental question in computer vision is whether a set of point pairs is\nthe image of a scene that lies in front of two cameras. Such a scene and the\ncameras together are known as a chiral reconstruction of the point pairs. In\nthis paper we provide a complete classification of k point pairs for which a\nchiral reconstruction exists. The existence of chiral reconstructions is\nequivalent to the non-emptiness of certain semialgebraic sets. For up to three\npoint pairs, we prove that a chiral reconstruction always exists while the set\nof five or more point pairs that do not have a chiral reconstruction is\nZariski-dense. We show that for five generic point pairs, the chiral region is\nbounded by line segments in a Schl\\\"afli double six on a cubic surface with 27\nreal lines. Four point pairs have a chiral reconstruction unless they belong to\ntwo non-generic combinatorial types, in which case they may or may not.\n", "versions": [{"version": "v1", "created": "Sat, 14 Nov 2020 02:27:20 GMT"}, {"version": "v2", "created": "Fri, 20 Nov 2020 17:08:54 GMT"}], "update_date": "2020-11-23", "authors_parsed": [["Pryhuber", "Andrew", ""], ["Sinn", "Rainer", ""], ["Thomas", "Rekha R.", ""]]}, {"id": "2011.07205", "submitter": "Yuhong Guo", "authors": "Zhen Zhao, Yuhong Guo, and Jieping Ye", "title": "Bi-Dimensional Feature Alignment for Cross-Domain Object Detection", "comments": "ECCV20 TASK-CV Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently the problem of cross-domain object detection has started drawing\nattention in the computer vision community. In this paper, we propose a novel\nunsupervised cross-domain detection model that exploits the annotated data in a\nsource domain to train an object detector for a different target domain. The\nproposed model mitigates the cross-domain representation divergence for object\ndetection by performing cross-domain feature alignment in two dimensions, the\ndepth dimension and the spatial dimension. In the depth dimension of channel\nlayers, it uses inter-channel information to bridge the domain divergence with\nrespect to image style alignment. In the dimension of spatial layers, it\ndeploys spatial attention modules to enhance detection relevant regions and\nsuppress irrelevant regions with respect to cross-domain feature alignment.\nExperiments are conducted on a number of benchmark cross-domain detection\ndatasets. The empirical results show the proposed method outperforms the\nstate-of-the-art comparison methods.\n", "versions": [{"version": "v1", "created": "Sat, 14 Nov 2020 03:03:11 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["Zhao", "Zhen", ""], ["Guo", "Yuhong", ""], ["Ye", "Jieping", ""]]}, {"id": "2011.07221", "submitter": "Soufiane Belharbi", "authors": "Soufiane Belharbi, J\\'er\\^ome Rony, Jose Dolz, Ismail Ben Ayed, Luke\n  McCaffrey, Eric Granger", "title": "Deep Interpretable Classification and Weakly-Supervised Segmentation of\n  Histology Images via Max-Min Uncertainty", "comments": "15 pages, 5 figures, under review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Weakly supervised learning (WSL) has recently triggered substantial interest\nas it mitigates the lack of pixel-wise annotations, while enabling\ninterpretable models. Given global image labels, WSL methods yield pixel-level\npredictions (segmentations). Despite their recent success, mostly with natural\nimages, such methods could be seriously challenged when the foreground and\nbackground regions have similar visual cues, yielding high false-positive rates\nin segmentations, as is the case of challenging histology images. WSL training\nis commonly driven by standard classification losses, which implicitly maximize\nmodel confidence and find the discriminative regions linked to classification\ndecisions. Therefore, they lack mechanisms for modeling explicitly\nnon-discriminative regions and reducing false-positive rates. We propose new\nregularization terms, which enable the model to seek both non-discriminative\nand discriminative regions, while discouraging unbalanced segmentations. We\nintroduce high uncertainty as a criterion to localize non-discriminative\nregions that do not affect classifier decision, and describe it with original\nKullback-Leibler (KL) divergence losses evaluating the deviation of posterior\npredictions from the uniform distribution. Our KL terms encourage high\nuncertainty of the model when the latter takes the latent non-discriminative\nregions as input. Our loss integrates: (i) a cross-entropy seeking a\nforeground, where model confidence about class prediction is high; (ii) a KL\nregularizer seeking a background, where model uncertainty is high; and (iii)\nlog-barrier terms discouraging unbalanced segmentations. Comprehensive\nexperiments and ablation studies over the public GlaS colon cancer data show\nsubstantial improvements over state-of-the-art WSL methods, and confirm the\neffect of our new regularizers. Our code is publicly available.\n", "versions": [{"version": "v1", "created": "Sat, 14 Nov 2020 04:45:07 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["Belharbi", "Soufiane", ""], ["Rony", "J\u00e9r\u00f4me", ""], ["Dolz", "Jose", ""], ["Ayed", "Ismail Ben", ""], ["McCaffrey", "Luke", ""], ["Granger", "Eric", ""]]}, {"id": "2011.07227", "submitter": "Hao Sheng", "authors": "Hao Sheng, Jeremy Irvin, Sasankh Munukutla, Shawn Zhang, Christopher\n  Cross, Kyle Story, Rose Rustowicz, Cooper Elsworth, Zutao Yang, Mark Omara,\n  Ritesh Gautam, Robert B. Jackson, Andrew Y. Ng", "title": "OGNet: Towards a Global Oil and Gas Infrastructure Database using Deep\n  Learning on Remotely Sensed Imagery", "comments": "Tackling Climate Change with Machine Learning at NeurIPS 2020\n  (Spotlight talk)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  At least a quarter of the warming that the Earth is experiencing today is due\nto anthropogenic methane emissions. There are multiple satellites in orbit and\nplanned for launch in the next few years which can detect and quantify these\nemissions; however, to attribute methane emissions to their sources on the\nground, a comprehensive database of the locations and characteristics of\nemission sources worldwide is essential. In this work, we develop deep learning\nalgorithms that leverage freely available high-resolution aerial imagery to\nautomatically detect oil and gas infrastructure, one of the largest\ncontributors to global methane emissions. We use the best algorithm, which we\ncall OGNet, together with expert review to identify the locations of oil\nrefineries and petroleum terminals in the U.S. We show that OGNet detects many\nfacilities which are not present in four standard public datasets of oil and\ngas infrastructure. All detected facilities are associated with characteristics\nknown to contribute to methane emissions, including the infrastructure type and\nthe number of storage tanks. The data curated and produced in this study is\nfreely available at http://stanfordmlgroup.github.io/projects/ognet .\n", "versions": [{"version": "v1", "created": "Sat, 14 Nov 2020 06:20:21 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["Sheng", "Hao", ""], ["Irvin", "Jeremy", ""], ["Munukutla", "Sasankh", ""], ["Zhang", "Shawn", ""], ["Cross", "Christopher", ""], ["Story", "Kyle", ""], ["Rustowicz", "Rose", ""], ["Elsworth", "Cooper", ""], ["Yang", "Zutao", ""], ["Omara", "Mark", ""], ["Gautam", "Ritesh", ""], ["Jackson", "Robert B.", ""], ["Ng", "Andrew Y.", ""]]}, {"id": "2011.07230", "submitter": "Yu-Shih Chen", "authors": "Yu-Shih Chen, Melissa Goh, Norm Matloff", "title": "TDAsweep: A Novel Dimensionality Reduction Method for Image\n  Classification Tasks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  One of the most celebrated achievements of modern machine learning technology\nis automatic classification of images. However, success is typically achieved\nonly with major computational costs. Here we introduce TDAsweep, a machine\nlearning tool aimed at improving the efficiency of automatic classification of\nimages.\n", "versions": [{"version": "v1", "created": "Sat, 14 Nov 2020 06:55:31 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["Chen", "Yu-Shih", ""], ["Goh", "Melissa", ""], ["Matloff", "Norm", ""]]}, {"id": "2011.07231", "submitter": "Linchao Zhu", "authors": "Linchao Zhu, Yi Yang", "title": "ActBERT: Learning Global-Local Video-Text Representations", "comments": "A few new results are included", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce ActBERT for self-supervised learning of joint\nvideo-text representations from unlabeled data. First, we leverage global\naction information to catalyze the mutual interactions between linguistic texts\nand local regional objects. It uncovers global and local visual clues from\npaired video sequences and text descriptions for detailed visual and text\nrelation modeling. Second, we introduce an ENtangled Transformer block (ENT) to\nencode three sources of information, i.e., global actions, local regional\nobjects, and linguistic descriptions. Global-local correspondences are\ndiscovered via judicious clues extraction from contextual information. It\nenforces the joint videotext representation to be aware of fine-grained objects\nas well as global human intention. We validate the generalization capability of\nActBERT on downstream video-and language tasks, i.e., text-video clip\nretrieval, video captioning, video question answering, action segmentation, and\naction step localization. ActBERT significantly outperforms the\nstate-of-the-arts, demonstrating its superiority in video-text representation\nlearning.\n", "versions": [{"version": "v1", "created": "Sat, 14 Nov 2020 07:14:08 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["Zhu", "Linchao", ""], ["Yang", "Yi", ""]]}, {"id": "2011.07233", "submitter": "Gernot Riegler", "authors": "Gernot Riegler, Vladlen Koltun", "title": "Stable View Synthesis", "comments": "Published at CVPR 2021, https://youtu.be/gqgXIY09htI", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present Stable View Synthesis (SVS). Given a set of source images\ndepicting a scene from freely distributed viewpoints, SVS synthesizes new views\nof the scene. The method operates on a geometric scaffold computed via\nstructure-from-motion and multi-view stereo. Each point on this 3D scaffold is\nassociated with view rays and corresponding feature vectors that encode the\nappearance of this point in the input images. The core of SVS is view-dependent\non-surface feature aggregation, in which directional feature vectors at each 3D\npoint are processed to produce a new feature vector for a ray that maps this\npoint into the new target view. The target view is then rendered by a\nconvolutional network from a tensor of features synthesized in this way for all\npixels. The method is composed of differentiable modules and is trained\nend-to-end. It supports spatially-varying view-dependent importance weighting\nand feature transformation of source images at each point; spatial and temporal\nstability due to the smooth dependence of on-surface feature aggregation on the\ntarget view; and synthesis of view-dependent effects such as specular\nreflection. Experimental results demonstrate that SVS outperforms\nstate-of-the-art view synthesis methods both quantitatively and qualitatively\non three diverse real-world datasets, achieving unprecedented levels of realism\nin free-viewpoint video of challenging large-scale scenes. Code is available at\nhttps://github.com/intel-isl/StableViewSynthesis\n", "versions": [{"version": "v1", "created": "Sat, 14 Nov 2020 07:24:43 GMT"}, {"version": "v2", "created": "Sun, 2 May 2021 12:20:20 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Riegler", "Gernot", ""], ["Koltun", "Vladlen", ""]]}, {"id": "2011.07236", "submitter": "Shihao Xu", "authors": "Shihao Xu, Haocong Rao, Xiping Hu, Bin Hu", "title": "Prototypical Contrast and Reverse Prediction: Unsupervised Skeleton\n  Based Action Recognition", "comments": "Codes are available at https://github.com/Mikexu007/PCRP", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we focus on unsupervised representation learning for\nskeleton-based action recognition. Existing approaches usually learn action\nrepresentations by sequential prediction but they suffer from the inability to\nfully learn semantic information. To address this limitation, we propose a\nnovel framework named Prototypical Contrast and Reverse Prediction (PCRP),\nwhich not only creates reverse sequential prediction to learn low-level\ninformation (e.g., body posture at every frame) and high-level pattern (e.g.,\nmotion order), but also devises action prototypes to implicitly encode semantic\nsimilarity shared among sequences. In general, we regard action prototypes as\nlatent variables and formulate PCRP as an expectation-maximization task.\nSpecifically, PCRP iteratively runs (1) E-step as determining the distribution\nof prototypes by clustering action encoding from the encoder, and (2) M-step as\noptimizing the encoder by minimizing the proposed ProtoMAE loss, which helps\nsimultaneously pull the action encoding closer to its assigned prototype and\nperform reverse prediction task. Extensive experiments on N-UCLA, NTU 60, and\nNTU 120 dataset present that PCRP outperforms state-of-the-art unsupervised\nmethods and even achieves superior performance over some of supervised methods.\nCodes are available at https://github.com/Mikexu007/PCRP.\n", "versions": [{"version": "v1", "created": "Sat, 14 Nov 2020 08:04:23 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["Xu", "Shihao", ""], ["Rao", "Haocong", ""], ["Hu", "Xiping", ""], ["Hu", "Bin", ""]]}, {"id": "2011.07252", "submitter": "Fanqing Lin", "authors": "Fanqing Lin, Tony Martinez", "title": "Ego2Hands: A Dataset for Egocentric Two-hand Segmentation and Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Hand segmentation and detection in truly unconstrained RGB-based settings is\nimportant for many applications. However, existing datasets are far from\nsufficient both in terms of size and variety due to the infeasibility of manual\nannotation of large amounts of segmentation and detection data. As a result,\ncurrent methods are limited by many underlying assumptions such as constrained\nenvironment, consistent skin color and lighting. In this work, we present a\nlarge-scale RGB-based egocentric hand segmentation/detection dataset Ego2Hands\nthat is automatically annotated and a color-invariant compositing-based data\ngeneration technique capable of creating unlimited training data with variety.\nFor quantitative analysis, we manually annotated an evaluation set that\nsignificantly exceeds existing benchmarks in quantity, diversity and annotation\naccuracy. We provide cross-dataset evaluation as well as thorough analysis on\nthe performance of state-of-the-art models on Ego2Hands to show that our\ndataset and data generation technique can produce models that generalize to\nunseen environments without domain adaptation.\n", "versions": [{"version": "v1", "created": "Sat, 14 Nov 2020 10:12:35 GMT"}, {"version": "v2", "created": "Tue, 17 Nov 2020 05:04:14 GMT"}, {"version": "v3", "created": "Mon, 29 Mar 2021 10:54:05 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Lin", "Fanqing", ""], ["Martinez", "Tony", ""]]}, {"id": "2011.07255", "submitter": "Metod Jazbec", "authors": "Metod Jazbec, Michael Pearce, Vincent Fortuin", "title": "Factorized Gaussian Process Variational Autoencoders", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Variational autoencoders often assume isotropic Gaussian priors and\nmean-field posteriors, hence do not exploit structure in scenarios where we may\nexpect similarity or consistency across latent variables. Gaussian process\nvariational autoencoders alleviate this problem through the use of a latent\nGaussian process, but lead to a cubic inference time complexity. We propose a\nmore scalable extension of these models by leveraging the independence of the\nauxiliary features, which is present in many datasets. Our model factorizes the\nlatent kernel across these features in different dimensions, leading to a\nsignificant speed-up (in theory and practice), while empirically performing\ncomparably to existing non-scalable approaches. Moreover, our approach allows\nfor additional modeling of global latent information and for more general\nextrapolation to unseen input combinations.\n", "versions": [{"version": "v1", "created": "Sat, 14 Nov 2020 10:24:10 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["Jazbec", "Metod", ""], ["Pearce", "Michael", ""], ["Fortuin", "Vincent", ""]]}, {"id": "2011.07279", "submitter": "Ashish Mishra", "authors": "Vinay Kumar Verma, Ashish Mishra, Anubha Pandey, Hema A. Murthy and\n  Piyush Rai", "title": "Towards Zero-Shot Learning with Fewer Seen Class Examples", "comments": "Accepted in WACV 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present a meta-learning based generative model for zero-shot learning\n(ZSL) towards a challenging setting when the number of training examples from\neach \\emph{seen} class is very few. This setup contrasts with the conventional\nZSL approaches, where training typically assumes the availability of a\nsufficiently large number of training examples from each of the seen classes.\nThe proposed approach leverages meta-learning to train a deep generative model\nthat integrates variational autoencoder and generative adversarial networks. We\npropose a novel task distribution where meta-train and meta-validation classes\nare disjoint to simulate the ZSL behaviour in training. Once trained, the model\ncan generate synthetic examples from seen and unseen classes. Synthesize\nsamples can then be used to train the ZSL framework in a supervised manner. The\nmeta-learner enables our model to generates high-fidelity samples using only a\nsmall number of training examples from seen classes. We conduct extensive\nexperiments and ablation studies on four benchmark datasets of ZSL and observe\nthat the proposed model outperforms state-of-the-art approaches by a\nsignificant margin when the number of examples per seen class is very small.\n", "versions": [{"version": "v1", "created": "Sat, 14 Nov 2020 11:58:35 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["Verma", "Vinay Kumar", ""], ["Mishra", "Ashish", ""], ["Pandey", "Anubha", ""], ["Murthy", "Hema A.", ""], ["Rai", "Piyush", ""]]}, {"id": "2011.07294", "submitter": "Matthias Grimm", "authors": "Matthias Grimm, Javier Esteban, Mathias Unberath and Nassir Navab", "title": "Pose-dependent weights and Domain Randomization for fully automatic\n  X-ray to CT Registration", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Fully automatic X-ray to CT registration requires a solid initialization to\nprovide an initial alignment within the capture range of existing\nintensity-based registrations. This work adresses that need by providing a\nnovel automatic initialization, which enables end to end registration. First, a\nneural network is trained once to detect a set of anatomical landmarks on\nsimulated X-rays. A domain randomization scheme is proposed to enable the\nnetwork to overcome the challenge of being trained purely on simulated data and\nrun inference on real Xrays. Then, for each patient CT, a patient-specific\nlandmark extraction scheme is used. It is based on backprojecting and\nclustering the previously trained networks predictions on a set of simulated\nX-rays. Next, the network is retrained to detect the new landmarks. Finally the\ncombination of network and 3D landmark locations is used to compute the\ninitialization using a perspective-n-point algorithm. During the computation of\nthe pose, a weighting scheme is introduced to incorporate the confidence of the\nnetwork in detecting the landmarks. The algorithm is evaluated on the pelvis\nusing both real and simulated x-rays. The mean (+-standard deviation) target\nregistration error in millimetres is 4.1 +- 4.3 for simulated X-rays with a\nsuccess rate of 92% and 4.2 +- 3.9 for real X-rays with a success rate of\n86.8%, where a success is defined as a translation error of less than 30mm.\n", "versions": [{"version": "v1", "created": "Sat, 14 Nov 2020 12:50:32 GMT"}, {"version": "v2", "created": "Thu, 15 Apr 2021 09:54:28 GMT"}], "update_date": "2021-04-16", "authors_parsed": [["Grimm", "Matthias", ""], ["Esteban", "Javier", ""], ["Unberath", "Mathias", ""], ["Navab", "Nassir", ""]]}, {"id": "2011.07340", "submitter": "Ravindra Yadav", "authors": "Ravindra Yadav, Ashish Sardana, Vinay P Namboodiri, Rajesh M Hegde", "title": "Speech Prediction in Silent Videos using Variational Autoencoders", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Understanding the relationship between the auditory and visual signals is\ncrucial for many different applications ranging from computer-generated imagery\n(CGI) and video editing automation to assisting people with hearing or visual\nimpairments. However, this is challenging since the distribution of both audio\nand visual modality is inherently multimodal. Therefore, most of the existing\nmethods ignore the multimodal aspect and assume that there only exists a\ndeterministic one-to-one mapping between the two modalities. It can lead to\nlow-quality predictions as the model collapses to optimizing the average\nbehavior rather than learning the full data distributions. In this paper, we\npresent a stochastic model for generating speech in a silent video. The\nproposed model combines recurrent neural networks and variational deep\ngenerative models to learn the auditory signal's conditional distribution given\nthe visual signal. We demonstrate the performance of our model on the GRID\ndataset based on standard benchmarks.\n", "versions": [{"version": "v1", "created": "Sat, 14 Nov 2020 17:09:03 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["Yadav", "Ravindra", ""], ["Sardana", "Ashish", ""], ["Namboodiri", "Vinay P", ""], ["Hegde", "Rajesh M", ""]]}, {"id": "2011.07353", "submitter": "Benedikt Graf", "authors": "Benedikt Graf, Arkadiusz Sitek, Amin Katouzian, Yen-Fu Lu, Arun\n  Krishnan, Justin Rafael, Kirstin Small, Yiting Xie", "title": "Pneumothorax and chest tube classification on chest x-rays for detection\n  of missed pneumothorax", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Chest x-ray imaging is widely used for the diagnosis of pneumothorax and\nthere has been significant interest in developing automated methods to assist\nin image interpretation. We present an image classification pipeline which\ndetects pneumothorax as well as the various types of chest tubes that are\ncommonly used to treat pneumothorax. Our multi-stage algorithm is based on lung\nsegmentation followed by pneumothorax classification, including classification\nof patches that are most likely to contain pneumothorax. This algorithm\nachieves state of the art performance for pneumothorax classification on an\nopen-source benchmark dataset. Unlike previous work, this algorithm shows\ncomparable performance on data with and without chest tubes and thus has an\nimproved clinical utility. To evaluate these algorithms in a realistic clinical\nscenario, we demonstrate the ability to identify real cases of missed\npneumothorax in a large dataset of chest x-ray studies.\n", "versions": [{"version": "v1", "created": "Sat, 14 Nov 2020 18:06:06 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["Graf", "Benedikt", ""], ["Sitek", "Arkadiusz", ""], ["Katouzian", "Amin", ""], ["Lu", "Yen-Fu", ""], ["Krishnan", "Arun", ""], ["Rafael", "Justin", ""], ["Small", "Kirstin", ""], ["Xie", "Yiting", ""]]}, {"id": "2011.07369", "submitter": "Issam Hadj Laradji", "authors": "Issam Laradji, Pau Rodriguez, Freddie Kalaitzis, David Vazquez, Ross\n  Young, Ed Davey, and Alexandre Lacoste", "title": "Counting Cows: Tracking Illegal Cattle Ranching From High-Resolution\n  Satellite Imagery", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Cattle farming is responsible for 8.8\\% of greenhouse gas emissions\nworldwide. In addition to the methane emitted due to their digestive process,\nthe growing need for grazing areas is an important driver of deforestation.\nWhile some regulations are in place for preserving the Amazon against\ndeforestation, these are being flouted in various ways, hence the need to scale\nand automate the monitoring of cattle ranching activities. Through a\npartnership with \\textit{Global Witness}, we explore the feasibility of\ntracking and counting cattle at the continental scale from satellite imagery.\nWith a license from Maxar Technologies, we obtained satellite imagery of the\nAmazon at 40cm resolution, and compiled a dataset of 903 images containing a\ntotal of 28498 cattle. Our experiments show promising results and highlight\nimportant directions for the next steps on both counting algorithms and the\ndata collection process for solving such challenges. The code is available at\n\\url{https://github.com/IssamLaradji/cownter_strike}.\n", "versions": [{"version": "v1", "created": "Sat, 14 Nov 2020 19:07:39 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["Laradji", "Issam", ""], ["Rodriguez", "Pau", ""], ["Kalaitzis", "Freddie", ""], ["Vazquez", "David", ""], ["Young", "Ross", ""], ["Davey", "Ed", ""], ["Lacoste", "Alexandre", ""]]}, {"id": "2011.07375", "submitter": "Peng Sun", "authors": "Peng Sun, Gabriel Draughon, Jerome Lynch", "title": "An Autonomous Approach to Measure Social Distances and Hygienic\n  Practices during COVID-19 Pandemic in Public Open Spaces", "comments": "11 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Coronavirus has been spreading around the world since the end of 2019. The\nvirus can cause acute respiratory syndrome, which can be lethal, and is easily\ntransmitted between hosts. Most states have issued state-at-home executive\norders, however, parks and other public open spaces have largely remained open\nand are seeing sharp increases in public use. Therefore, in order to ensure\npublic safety, it is imperative for patrons of public open spaces to practice\nsafe hygiene and take preventative measures. This work provides a scalable\nsensing approach to detect physical activities within public open spaces and\nmonitor adherence to social distancing guidelines suggested by the US Centers\nfor Disease Control and Prevention (CDC). A deep learning-based computer vision\nsensing framework is designed to investigate the careful and proper utilization\nof parks and park facilities with hard surfaces (e.g. benches, fence poles, and\ntrash cans) using video feeds from a pre-installed surveillance camera network.\nThe sensing framework consists of a CNN-based object detector, a multi-target\ntracker, a mapping module, and a group reasoning module. The experiments are\ncarried out during the COVID-19 pandemic between March 2020 and May 2020 across\nseveral key locations at the Detroit Riverfront Parks in Detroit, Michigan. The\nsensing framework is validated by comparing automatic sensing results with\nmanually labeled ground-truth results. The proposed approach significantly\nimproves the efficiency of providing spatial and temporal statistics of users\nin public open spaces by creating straightforward data visualizations for\nfederal and state agencies. The results can also provide on-time triggering\ninformation for an alarming or actuator system which can later be added to\nintervene inappropriate behavior during this pandemic.\n", "versions": [{"version": "v1", "created": "Sat, 14 Nov 2020 19:35:09 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["Sun", "Peng", ""], ["Draughon", "Gabriel", ""], ["Lynch", "Jerome", ""]]}, {"id": "2011.07384", "submitter": "Valts Blukis", "authors": "Valts Blukis, Ross A. Knepper, Yoav Artzi", "title": "Few-shot Object Grounding and Mapping for Natural Language Robot\n  Instruction Following", "comments": "4th Conference on Robot Learning (CoRL 2020), Cambridge MA, USA", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.CL cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of learning a robot policy to follow natural language\ninstructions that can be easily extended to reason about new objects. We\nintroduce a few-shot language-conditioned object grounding method trained from\naugmented reality data that uses exemplars to identify objects and align them\nto their mentions in instructions. We present a learned map representation that\nencodes object locations and their instructed use, and construct it from our\nfew-shot grounding output. We integrate this mapping approach into an\ninstruction-following policy, thereby allowing it to reason about previously\nunseen objects at test-time by simply adding exemplars. We evaluate on the task\nof learning to map raw observations and instructions to continuous control of a\nphysical quadcopter. Our approach significantly outperforms the prior state of\nthe art in the presence of new objects, even when the prior approach observes\nall objects during training.\n", "versions": [{"version": "v1", "created": "Sat, 14 Nov 2020 20:35:20 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["Blukis", "Valts", ""], ["Knepper", "Ross A.", ""], ["Artzi", "Yoav", ""]]}, {"id": "2011.07387", "submitter": "Youya Xia", "authors": "Youya Xia, Yifan Tang, Yuhan Hu and Guy Hoffman", "title": "Privacy-Preserving Pose Estimation for Human-Robot Interaction", "comments": "Submitted for review at ICRA2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Pose estimation is an important technique for nonverbal human-robot\ninteraction. That said, the presence of a camera in a person's space raises\nprivacy concerns and could lead to distrust of the robot. In this paper, we\npropose a privacy-preserving camera-based pose estimation method. The proposed\nsystem consists of a user-controlled translucent filter that covers the camera\nand an image enhancement module designed to facilitate pose estimation from the\nfiltered (shadow) images, while never capturing clear images of the user. We\nevaluate the system's performance on a new filtered image dataset, considering\nthe effects of distance from the camera, background clutter, and film\nthickness. Based on our findings, we conclude that our system can protect\nhumans' privacy while detecting humans' pose information effectively.\n", "versions": [{"version": "v1", "created": "Sat, 14 Nov 2020 21:09:53 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["Xia", "Youya", ""], ["Tang", "Yifan", ""], ["Hu", "Yuhan", ""], ["Hoffman", "Guy", ""]]}, {"id": "2011.07394", "submitter": "Robert Henderson", "authors": "Robert D. E. Henderson, Xin Yi, Scott J. Adams and Paul Babyn", "title": "Automatic classification of multiple catheters in neonatal radiographs\n  with deep learning", "comments": "10 pages, 5 figures (+1 suppl.), 2 tables (+2 suppl.). Submitted to\n  Journal of Digital Imaging", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop and evaluate a deep learning algorithm to classify multiple\ncatheters on neonatal chest and abdominal radiographs. A convolutional neural\nnetwork (CNN) was trained using a dataset of 777 neonatal chest and abdominal\nradiographs, with a split of 81%-9%-10% for training-validation-testing,\nrespectively. We employed ResNet-50 (a CNN), pre-trained on ImageNet. Ground\ntruth labelling was limited to tagging each image to indicate the presence or\nabsence of endotracheal tubes (ETTs), nasogastric tubes (NGTs), and umbilical\narterial and venous catheters (UACs, UVCs). The data set included 561 images\ncontaining 2 or more catheters, 167 images with only one, and 49 with none.\nPerformance was measured with average precision (AP), calculated from the area\nunder the precision-recall curve. On our test data, the algorithm achieved an\noverall AP (95% confidence interval) of 0.977 (0.679-0.999) for NGTs, 0.989\n(0.751-1.000) for ETTs, 0.979 (0.873-0.997) for UACs, and 0.937 (0.785-0.984)\nfor UVCs. Performance was similar for the set of 58 test images consisting of 2\nor more catheters, with an AP of 0.975 (0.255-1.000) for NGTs, 0.997\n(0.009-1.000) for ETTs, 0.981 (0.797-0.998) for UACs, and 0.937 (0.689-0.990)\nfor UVCs. Our network thus achieves strong performance in the simultaneous\ndetection of these four catheter types. Radiologists may use such an algorithm\nas a time-saving mechanism to automate reporting of catheters on radiographs.\n", "versions": [{"version": "v1", "created": "Sat, 14 Nov 2020 21:27:21 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["Henderson", "Robert D. E.", ""], ["Yi", "Xin", ""], ["Adams", "Scott J.", ""], ["Babyn", "Paul", ""]]}, {"id": "2011.07421", "submitter": "Md Taufeeq Uddin", "authors": "Md Taufeeq Uddin, Shaun Canavan, Ghada Zamzmi", "title": "Accounting for Affect in Pain Level Recognition", "comments": "Machine Learning for Health (ML4H) at NeurIPS 2020 - Extended\n  Abstract", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we address the importance of affect in automated pain\nassessment and the implications in real-world settings. To achieve this, we\ncurate a new physiological dataset by merging the publicly available bioVid\npain and emotion datasets. We then investigate pain level recognition on this\ndataset simulating participants' naturalistic affective behaviors. Our findings\ndemonstrate that acknowledging affect in pain assessment is essential. We\nobserve degradation in recognition performance when simulating the existence of\naffect to validate pain assessment models that do not account for it.\nConversely, we observe a performance boost in recognition when we account for\naffect.\n", "versions": [{"version": "v1", "created": "Sun, 15 Nov 2020 00:23:31 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["Uddin", "Md Taufeeq", ""], ["Canavan", "Shaun", ""], ["Zamzmi", "Ghada", ""]]}, {"id": "2011.07428", "submitter": "Amirreza Mahbod", "authors": "Amirreza Mahbod, Gerald Schaefer, Rupert Ecker, Isabella Ellinger", "title": "Pollen Grain Microscopic Image Classification Using an Ensemble of\n  Fine-Tuned Deep Convolutional Neural Networks", "comments": "Accepted for the Artificial Intelligence for Healthcare Applications\n  workshop at the 25th International Conference on Pattern Recognition (ICPR\n  2020)", "journal-ref": null, "doi": "10.1007/978-3-030-68763-2_26", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Pollen grain micrograph classification has multiple applications in medicine\nand biology. Automatic pollen grain image classification can alleviate the\nproblems of manual categorisation such as subjectivity and time constraints.\nWhile a number of computer-based methods have been introduced in the literature\nto perform this task, classification performance needs to be improved for these\nmethods to be useful in practice.\n  In this paper, we present an ensemble approach for pollen grain microscopic\nimage classification into four categories: Corylus Avellana well-developed\npollen grain, Corylus Avellana anomalous pollen grain, Alnus well-developed\npollen grain, and non-pollen (debris) instances. In our approach, we develop a\nclassification strategy that is based on fusion of four state-of-the-art\nfine-tuned convolutional neural networks, namely EfficientNetB0,\nEfficientNetB1, EfficientNetB2 and SeResNeXt-50 deep models. These models are\ntrained with images of three fixed sizes (224x224, 240x240, and 260x260 pixels)\nand their prediction probability vectors are then fused in an ensemble method\nto form a final classification vector for a given pollen grain image.\n  Our proposed method is shown to yield excellent classification performance,\nobtaining an accuracy of of 94.48% and a weighted F1-score of 94.54% on the\nICPR 2020 Pollen Grain Classification Challenge training dataset based on\nfive-fold cross-validation. Evaluated on the test set of the challenge, our\napproach achieved a very competitive performance in comparison to the top\nranked approaches with an accuracy and a weighted F1-score of 96.28% and\n96.30%, respectively.\n", "versions": [{"version": "v1", "created": "Sun, 15 Nov 2020 01:25:46 GMT"}], "update_date": "2021-03-25", "authors_parsed": [["Mahbod", "Amirreza", ""], ["Schaefer", "Gerald", ""], ["Ecker", "Rupert", ""], ["Ellinger", "Isabella", ""]]}, {"id": "2011.07430", "submitter": "Juncheng Li", "authors": "Juncheng B Li, Kaixin Ma, Shuhui Qu, Po-Yao Huang, Florian Metze", "title": "Audio-Visual Event Recognition through the lens of Adversary", "comments": "4 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As audio/visual classification models are widely deployed for sensitive tasks\nlike content filtering at scale, it is critical to understand their robustness\nalong with improving the accuracy. This work aims to study several key\nquestions related to multimodal learning through the lens of adversarial\nnoises: 1) The trade-off between early/middle/late fusion affecting its\nrobustness and accuracy 2) How do different frequency/time domain features\ncontribute to the robustness? 3) How do different neural modules contribute to\nthe adversarial noise? In our experiment, we construct adversarial examples to\nattack state-of-the-art neural models trained on Google AudioSet. We compare\nhow much attack potency in terms of adversarial perturbation of size $\\epsilon$\nusing different $L_p$ norms we would need to \"deactivate\" the victim model.\nUsing adversarial noise to ablate multimodal models, we are able to provide\ninsights into what is the best potential fusion strategy to balance the model\nparameters/accuracy and robustness trade-off and distinguish the robust\nfeatures versus the non-robust features that various neural networks model tend\nto learn.\n", "versions": [{"version": "v1", "created": "Sun, 15 Nov 2020 01:36:09 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["Li", "Juncheng B", ""], ["Ma", "Kaixin", ""], ["Qu", "Shuhui", ""], ["Huang", "Po-Yao", ""], ["Metze", "Florian", ""]]}, {"id": "2011.07431", "submitter": "Yijun Zhao", "authors": "Yao Xiao and Yijun Zhao", "title": "Enhance Gender and Identity Preservation in Face Aging Simulation for\n  Infants and Toddlers", "comments": "8 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Realistic age-progressed photos provide invaluable biometric information in a\nwide range of applications. In recent years, deep learning-based approaches\nhave made remarkable progress in modeling the aging process of the human face.\nNevertheless, it remains a challenging task to generate accurate age-progressed\nfaces from infant or toddler photos. In particular, the lack of visually\ndetectable gender characteristics and the drastic appearance changes in early\nlife contribute to the difficulty of the task. We propose a new deep learning\nmethod inspired by the successful Conditional Adversarial Autoencoder (CAAE,\n2017) model. In our approach, we extend the CAAE architecture to 1) incorporate\ngender information, and 2) augment the model's overall architecture with an\nidentity-preserving component based on facial features. We trained our model\nusing the publicly available UTKFace dataset and evaluated our model by\nsimulating up to 100 years of aging on 1,156 male and 1,207 female infant and\ntoddler face photos. Compared to the CAAE approach, our new model demonstrates\nnoticeable visual improvements. Quantitatively, our model exhibits an overall\ngain of 77.0% (male) and 13.8% (female) in gender fidelity measured by a gender\nclassifier for the simulated photos across the age spectrum. Our model also\ndemonstrates a 22.4% gain in identity preservation measured by a facial\nrecognition neural network.\n", "versions": [{"version": "v1", "created": "Sun, 15 Nov 2020 01:40:36 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["Xiao", "Yao", ""], ["Zhao", "Yijun", ""]]}, {"id": "2011.07449", "submitter": "Devesh Walawalkar", "authors": "Devesh Walawalkar, Zhiqiang Shen, Marios Savvides", "title": "Online Ensemble Model Compression using Knowledge Distillation", "comments": null, "journal-ref": null, "doi": "10.1007/978-3-030-58529-7_2", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper presents a novel knowledge distillation based model compression\nframework consisting of a student ensemble. It enables distillation of\nsimultaneously learnt ensemble knowledge onto each of the compressed student\nmodels. Each model learns unique representations from the data distribution due\nto its distinct architecture. This helps the ensemble generalize better by\ncombining every model's knowledge. The distilled students and ensemble teacher\nare trained simultaneously without requiring any pretrained weights. Moreover,\nour proposed method can deliver multi-compressed students with single training,\nwhich is efficient and flexible for different scenarios. We provide\ncomprehensive experiments using state-of-the-art classification models to\nvalidate our framework's effectiveness. Notably, using our framework a 97%\ncompressed ResNet110 student model managed to produce a 10.64% relative\naccuracy gain over its individual baseline training on CIFAR100 dataset.\nSimilarly a 95% compressed DenseNet-BC(k=12) model managed a 8.17% relative\naccuracy gain.\n", "versions": [{"version": "v1", "created": "Sun, 15 Nov 2020 04:46:29 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["Walawalkar", "Devesh", ""], ["Shen", "Zhiqiang", ""], ["Savvides", "Marios", ""]]}, {"id": "2011.07453", "submitter": "Kurtis Evan David", "authors": "Kurtis Evan David, Qiang Liu, Ruth Fong", "title": "Debiasing Convolutional Neural Networks via Meta Orthogonalization", "comments": "Accepted to NeuRIPS 2020 Workshop on Algorithmic Fairness through the\n  Lens of Causality and Interpretability (AFCI). Supplemental materials\n  provided at:\n  https://drive.google.com/drive/folders/1klIAqZDgg3sCVmzFjLw5Y_T-GTc2E3oh?usp=sharing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV cs.CY", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  While deep learning models often achieve strong task performance, their\nsuccesses are hampered by their inability to disentangle spurious correlations\nfrom causative factors, such as when they use protected attributes (e.g., race,\ngender, etc.) to make decisions. In this work, we tackle the problem of\ndebiasing convolutional neural networks (CNNs) in such instances. Building off\nof existing work on debiasing word embeddings and model interpretability, our\nMeta Orthogonalization method encourages the CNN representations of different\nconcepts (e.g., gender and class labels) to be orthogonal to one another in\nactivation space while maintaining strong downstream task performance. Through\na variety of experiments, we systematically test our method and demonstrate\nthat it significantly mitigates model bias and is competitive against current\nadversarial debiasing methods.\n", "versions": [{"version": "v1", "created": "Sun, 15 Nov 2020 05:13:22 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["David", "Kurtis Evan", ""], ["Liu", "Qiang", ""], ["Fong", "Ruth", ""]]}, {"id": "2011.07460", "submitter": "Jacob Ouyang", "authors": "Jacob Ouyang, Isaac R Galatzer-Levy, Vidya Koesmahargyo, Li Zhang", "title": "Direct Classification of Emotional Intensity", "comments": "7 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we present a model that can directly predict emotion intensity\nscore from video inputs, instead of deriving from action units. Using a 3d DNN\nincorporated with dynamic emotion information, we train a model using videos of\ndifferent people smiling that outputs an intensity score from 0-10. Each video\nis labeled framewise using a normalized action-unit based intensity score. Our\nmodel then employs an adaptive learning technique to improve performance when\ndealing with new subjects. Compared to other models, our model excels in\ngeneralization between different people as well as provides a new framework to\ndirectly classify emotional intensity.\n", "versions": [{"version": "v1", "created": "Sun, 15 Nov 2020 06:32:48 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["Ouyang", "Jacob", ""], ["Galatzer-Levy", "Isaac R", ""], ["Koesmahargyo", "Vidya", ""], ["Zhang", "Li", ""]]}, {"id": "2011.07466", "submitter": "Xin Ding", "authors": "Xin Ding and Yongwei Wang and Zuheng Xu and William J. Welch and Z.\n  Jane Wang", "title": "Continuous Conditional Generative Adversarial Networks for Image\n  Generation: Novel Losses and Label Input Mechanisms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  This work proposes the continuous conditional generative adversarial network\n(CcGAN), the first generative model for image generation conditional on\ncontinuous, scalar conditions (termed regression labels). Existing conditional\nGANs (cGANs) are mainly designed for categorical conditions (eg, class labels);\nconditioning on regression labels is mathematically distinct and raises two\nfundamental problems:(P1) Since there may be very few (even zero) real images\nfor some regression labels, minimizing existing empirical versions of cGAN\nlosses (aka empirical cGAN losses) often fails in practice;(P2) Since\nregression labels are scalar and infinitely many, conventional label input\nmethods are not applicable. The proposed CcGAN solves the above problems,\nrespectively, by (S1) reformulating existing empirical cGAN losses to be\nappropriate for the continuous scenario; and (S2) proposing a naive label input\n(NLI) method and an improved label input (ILI) method to incorporate regression\nlabels into the generator and the discriminator. The reformulation in (S1)\nleads to two novel empirical discriminator losses, termed the hard vicinal\ndiscriminator loss (HVDL) and the soft vicinal discriminator loss (SVDL)\nrespectively, and a novel empirical generator loss. The error bounds of a\ndiscriminator trained with HVDL and SVDL are derived under mild assumptions in\nthis work. Two new benchmark datasets (RC-49 and Cell-200) and a novel\nevaluation metric (Sliding Fr\\'echet Inception Distance) are also proposed for\nthis continuous scenario. Our experiments on the Circular 2-D Gaussians, RC-49,\nUTKFace, Cell-200, and Steering Angle datasets show that CcGAN is able to\ngenerate diverse, high-quality samples from the image distribution conditional\non a given regression label. Moreover, in these experiments, CcGAN\nsubstantially outperforms cGAN both visually and quantitatively.\n", "versions": [{"version": "v1", "created": "Sun, 15 Nov 2020 07:29:41 GMT"}, {"version": "v2", "created": "Mon, 21 Dec 2020 00:05:12 GMT"}, {"version": "v3", "created": "Fri, 22 Jan 2021 22:34:54 GMT"}, {"version": "v4", "created": "Wed, 27 Jan 2021 02:33:02 GMT"}, {"version": "v5", "created": "Sun, 9 May 2021 06:30:47 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Ding", "Xin", ""], ["Wang", "Yongwei", ""], ["Xu", "Zuheng", ""], ["Welch", "William J.", ""], ["Wang", "Z. Jane", ""]]}, {"id": "2011.07482", "submitter": "Mehak Aggarwal", "authors": "Mehak Aggarwal, Nishanth Arun, Sharut Gupta, Ashwin Vaswani, Bryan\n  Chen, Matthew Li, Ken Chang, Jay Patel, Katherine Hoebel, Mishka Gidwani,\n  Jayashree Kalpathy-Cramer, Praveer Singh", "title": "Towards Trainable Saliency Maps in Medical Imaging", "comments": "Machine Learning for Health (ML4H) at NeurIPS 2020 - Extended\n  Abstract", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  While success of Deep Learning (DL) in automated diagnosis can be\ntransformative to the medicinal practice especially for people with little or\nno access to doctors, its widespread acceptability is severely limited by\ninherent black-box decision making and unsafe failure modes. While saliency\nmethods attempt to tackle this problem in non-medical contexts, their apriori\nexplanations do not transfer well to medical usecases. With this study we\nvalidate a model design element agnostic to both architecture complexity and\nmodel task, and show how introducing this element gives an inherently\nself-explanatory model. We compare our results with state of the art\nnon-trainable saliency maps on RSNA Pneumonia Dataset and demonstrate a much\nhigher localization efficacy using our adopted technique. We also compare, with\na fully supervised baseline and provide a reasonable alternative to it's high\ndata labelling overhead. We further investigate the validity of our claims\nthrough qualitative evaluation from an expert reader.\n", "versions": [{"version": "v1", "created": "Sun, 15 Nov 2020 09:01:55 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["Aggarwal", "Mehak", ""], ["Arun", "Nishanth", ""], ["Gupta", "Sharut", ""], ["Vaswani", "Ashwin", ""], ["Chen", "Bryan", ""], ["Li", "Matthew", ""], ["Chang", "Ken", ""], ["Patel", "Jay", ""], ["Hoebel", "Katherine", ""], ["Gidwani", "Mishka", ""], ["Kalpathy-Cramer", "Jayashree", ""], ["Singh", "Praveer", ""]]}, {"id": "2011.07491", "submitter": "Radu Tudor Ionescu", "authors": "Mariana-Iuliana Georgescu, Antonio Barbalau, Radu Tudor Ionescu, Fahad\n  Shahbaz Khan, Marius Popescu, Mubarak Shah", "title": "Anomaly Detection in Video via Self-Supervised and Multi-Task Learning", "comments": "Accepted at CVPR 2021. Main paper and supplementary are both included", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Anomaly detection in video is a challenging computer vision problem. Due to\nthe lack of anomalous events at training time, anomaly detection requires the\ndesign of learning methods without full supervision. In this paper, we approach\nanomalous event detection in video through self-supervised and multi-task\nlearning at the object level. We first utilize a pre-trained detector to detect\nobjects. Then, we train a 3D convolutional neural network to produce\ndiscriminative anomaly-specific information by jointly learning multiple proxy\ntasks: three self-supervised and one based on knowledge distillation. The\nself-supervised tasks are: (i) discrimination of forward/backward moving\nobjects (arrow of time), (ii) discrimination of objects in\nconsecutive/intermittent frames (motion irregularity) and (iii) reconstruction\nof object-specific appearance information. The knowledge distillation task\ntakes into account both classification and detection information, generating\nlarge prediction discrepancies between teacher and student models when\nanomalies occur. To the best of our knowledge, we are the first to approach\nanomalous event detection in video as a multi-task learning problem,\nintegrating multiple self-supervised and knowledge distillation proxy tasks in\na single architecture. Our lightweight architecture outperforms the\nstate-of-the-art methods on three benchmarks: Avenue, ShanghaiTech and UCSD\nPed2. Additionally, we perform an ablation study demonstrating the importance\nof integrating self-supervised learning and normality-specific distillation in\na multi-task learning setting.\n", "versions": [{"version": "v1", "created": "Sun, 15 Nov 2020 10:21:28 GMT"}, {"version": "v2", "created": "Sat, 6 Mar 2021 21:14:40 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Georgescu", "Mariana-Iuliana", ""], ["Barbalau", "Antonio", ""], ["Ionescu", "Radu Tudor", ""], ["Khan", "Fahad Shahbaz", ""], ["Popescu", "Marius", ""], ["Shah", "Mubarak", ""]]}, {"id": "2011.07499", "submitter": "M. F. Mridha", "authors": "M. F. Mridha, Abu Quwsar Ohi, M. Ameer Ali, Mazedul Islam Emon,\n  Muhammad Mohsin Kabir", "title": "BanglaWriting: A multi-purpose offline Bangla handwriting dataset", "comments": "Accepted in journal Data in Brief. The dataset is available on\n  https://data.mendeley.com/datasets/r43wkvdk4w/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article presents a Bangla handwriting dataset named BanglaWriting that\ncontains single-page handwritings of 260 individuals of different personalities\nand ages. Each page includes bounding-boxes that bounds each word, along with\nthe unicode representation of the writing. This dataset contains 21,234 words\nand 32,787 characters in total. Moreover, this dataset includes 5,470 unique\nwords of Bangla vocabulary. Apart from the usual words, the dataset comprises\n261 comprehensible overwriting and 450 handwritten strikes and mistakes. All of\nthe bounding-boxes and word labels are manually-generated. The dataset can be\nused for complex optical character/word recognition, writer identification,\nhandwritten word segmentation, and word generation. Furthermore, this dataset\nis suitable for extracting age-based and gender-based variation of handwriting.\n", "versions": [{"version": "v1", "created": "Sun, 15 Nov 2020 11:08:53 GMT"}, {"version": "v2", "created": "Tue, 8 Dec 2020 09:30:02 GMT"}], "update_date": "2020-12-09", "authors_parsed": [["Mridha", "M. F.", ""], ["Ohi", "Abu Quwsar", ""], ["Ali", "M. Ameer", ""], ["Emon", "Mazedul Islam", ""], ["Kabir", "Muhammad Mohsin", ""]]}, {"id": "2011.07513", "submitter": "David Svitov", "authors": "David Svitov, Sergey Alyamkin", "title": "AmphibianDetector: adaptive computation for moving objects detection", "comments": "12 pages, 6 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional neural networks (CNN) allow achieving the highest accuracy for\nthe task of object detection in images. Major challenges in further development\nof object detectors are false-positive detections and high demand of processing\npower. In this paper, we propose an approach to object detection which makes it\npossible to reduce the number of false-positive detections by processing only\nmoving objects and reduce the required processing power for algorithm\ninference. The proposed approach is a modification of CNN already trained for\nobject detection task. This method can be used to improve the accuracy of an\nexisting system by applying minor changes to the algorithm. The efficiency of\nthe proposed approach was demonstrated on the open dataset \"CDNet2014\npedestrian\". The implementation of the method proposed in the article is\navailable on the GitHub: https://github.com/david-svitov/AmphibianDetector\n", "versions": [{"version": "v1", "created": "Sun, 15 Nov 2020 12:37:44 GMT"}, {"version": "v2", "created": "Fri, 25 Dec 2020 09:09:54 GMT"}], "update_date": "2020-12-29", "authors_parsed": [["Svitov", "David", ""], ["Alyamkin", "Sergey", ""]]}, {"id": "2011.07517", "submitter": "Jianan Wang", "authors": "Jianan Wang, Boyang Li, Xiangyu Fan, Jing Lin and Yanwei Fu", "title": "Data-efficient Alignment of Multimodal Sequences by Aligning Gradient\n  Updates and Internal Feature Distributions", "comments": "This paper is accepted to WACV2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The task of video and text sequence alignment is a prerequisite step toward\njoint understanding of movie videos and screenplays. However, supervised\nmethods face the obstacle of limited realistic training data. With this paper,\nwe attempt to enhance data efficiency of the end-to-end alignment network\nNeuMATCH [15]. Recent research [56] suggests that network components dealing\nwith different modalities may overfit and generalize at different speeds,\ncreating difficulties for training. We propose to employ (1) layer-wise\nadaptive rate scaling (LARS) to align the magnitudes of gradient updates in\ndifferent layers and balance the pace of learning and (2) sequence-wise batch\nnormalization (SBN) to align the internal feature distributions from different\nmodalities. Finally, we leverage random projection to reduce the dimensionality\nof input features. On the YouTube Movie Summary dataset, the combined use of\nthese technique closes the performance gap when the pretraining on the LSMDC\ndataset is omitted and achieves the state-of-the-art result. Extensive\nempirical comparisons and analysis reveal that these techniques improve\noptimization and regularize the network more effectively than two different\nsetups of layer normalization.\n", "versions": [{"version": "v1", "created": "Sun, 15 Nov 2020 13:04:25 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["Wang", "Jianan", ""], ["Li", "Boyang", ""], ["Fan", "Xiangyu", ""], ["Lin", "Jing", ""], ["Fu", "Yanwei", ""]]}, {"id": "2011.07526", "submitter": "Zidong Guo", "authors": "Zidong Guo, Zejian Yuan, Chong Zhang, Wanchao Chi, Yonggen Ling, and\n  Shenghao Zhang", "title": "Domain Adaptation Gaze Estimation by Embedding with Prediction\n  Consistency", "comments": "16 pages, 6 figures, ACCV 2020 (oral)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Gaze is the essential manifestation of human attention. In recent years, a\nseries of work has achieved high accuracy in gaze estimation. However, the\ninter-personal difference limits the reduction of the subject-independent gaze\nestimation error. This paper proposes an unsupervised method for domain\nadaptation gaze estimation to eliminate the impact of inter-personal diversity.\nIn domain adaption, we design an embedding representation with prediction\nconsistency to ensure that the linear relationship between gaze directions in\ndifferent domains remains consistent on gaze space and embedding space.\nSpecifically, we employ source gaze to form a locally linear representation in\nthe gaze space for each target domain prediction. Then the same linear\ncombinations are applied in the embedding space to generate hypothesis\nembedding for the target domain sample, remaining prediction consistency. The\ndeviation between the target and source domain is reduced by approximating the\npredicted and hypothesis embedding for the target domain sample. Guided by the\nproposed strategy, we design Domain Adaptation Gaze Estimation Network(DAGEN),\nwhich learns embedding with prediction consistency and achieves\nstate-of-the-art results on both the MPIIGaze and the EYEDIAP datasets.\n", "versions": [{"version": "v1", "created": "Sun, 15 Nov 2020 13:33:43 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["Guo", "Zidong", ""], ["Yuan", "Zejian", ""], ["Zhang", "Chong", ""], ["Chi", "Wanchao", ""], ["Ling", "Yonggen", ""], ["Zhang", "Shenghao", ""]]}, {"id": "2011.07534", "submitter": "Chang Qi", "authors": "Chang Qi, Junyang Chen, Guizhi Xu, Zhenghua Xu, Thomas Lukasiewicz,\n  Yang Liu", "title": "SAG-GAN: Semi-Supervised Attention-Guided GANs for Data Augmentation on\n  Medical Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently deep learning methods, in particular, convolutional neural networks\n(CNNs), have led to a massive breakthrough in the range of computer vision.\nAlso, the large-scale annotated dataset is the essential key to a successful\ntraining procedure. However, it is a huge challenge to get such datasets in the\nmedical domain. Towards this, we present a data augmentation method for\ngenerating synthetic medical images using cycle-consistency Generative\nAdversarial Networks (GANs). We add semi-supervised attention modules to\ngenerate images with convincing details. We treat tumor images and normal\nimages as two domains. The proposed GANs-based model can generate a tumor image\nfrom a normal image, and in turn, it can also generate a normal image from a\ntumor image. Furthermore, we show that generated medical images can be used for\nimproving the performance of ResNet18 for medical image classification. Our\nmodel is applied to three limited datasets of tumor MRI images. We first\ngenerate MRI images on limited datasets, then we trained three popular\nclassification models to get the best model for tumor classification. Finally,\nwe train the classification model using real images with classic data\naugmentation methods and classification models using synthetic images. The\nclassification results between those trained models showed that the proposed\nSAG-GAN data augmentation method can boost Accuracy and AUC compare with\nclassic data augmentation methods. We believe the proposed data augmentation\nmethod can apply to other medical image domains, and improve the accuracy of\ncomputer-assisted diagnosis.\n", "versions": [{"version": "v1", "created": "Sun, 15 Nov 2020 14:01:24 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["Qi", "Chang", ""], ["Chen", "Junyang", ""], ["Xu", "Guizhi", ""], ["Xu", "Zhenghua", ""], ["Lukasiewicz", "Thomas", ""], ["Liu", "Yang", ""]]}, {"id": "2011.07557", "submitter": "Dalu Feng", "authors": "Dalu Feng, Shuang Yang, Shiguang Shan, Xilin Chen", "title": "Learn an Effective Lip Reading Model without Pains", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Lip reading, also known as visual speech recognition, aims to recognize the\nspeech content from videos by analyzing the lip dynamics. There have been\nseveral appealing progress in recent years, benefiting much from the rapidly\ndeveloped deep learning techniques and the recent large-scale lip-reading\ndatasets. Most existing methods obtained high performance by constructing a\ncomplex neural network, together with several customized training strategies\nwhich were always given in a very brief description or even shown only in the\nsource code. We find that making proper use of these strategies could always\nbring exciting improvements without changing much of the model. Considering the\nnon-negligible effects of these strategies and the existing tough status to\ntrain an effective lip reading model, we perform a comprehensive quantitative\nstudy and comparative analysis, for the first time, to show the effects of\nseveral different choices for lip reading. By only introducing some easy-to-get\nrefinements to the baseline pipeline, we obtain an obvious improvement of the\nperformance from 83.7% to 88.4% and from 38.2% to 55.7% on two largest public\navailable lip reading datasets, LRW and LRW-1000, respectively. They are\ncomparable and even surpass the existing state-of-the-art results.\n", "versions": [{"version": "v1", "created": "Sun, 15 Nov 2020 15:29:19 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["Feng", "Dalu", ""], ["Yang", "Shuang", ""], ["Shan", "Shiguang", ""], ["Chen", "Xilin", ""]]}, {"id": "2011.07584", "submitter": "Alfredo Kalaitzis", "authors": "Dolores Garcia, Gonzalo Mateo-Garcia, Hannes Bernhardt, Ron\n  Hagensieker, Ignacio G. Lopez Francos, Jonathan Stock, Guy Schumann, Kevin\n  Dobbs, Freddie Kalaitzis", "title": "Pix2Streams: Dynamic Hydrology Maps from Satellite-LiDAR Fusion", "comments": "Work completed during the 2020 Frontier Development Lab research\n  accelerator, a private-public partnership with NASA in the US, and ESA in\n  Europe. Accepted as a spotlight/long oral talk at AI for Earth Sciences\n  Workshop at NeurIPS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Where are the Earth's streams flowing right now? Inland surface waters expand\nwith floods and contract with droughts, so there is no one map of our streams.\nCurrent satellite approaches are limited to monthly observations that map only\nthe widest streams. These are fed by smaller tributaries that make up much of\nthe dendritic surface network but whose flow is unobserved. A complete map of\nour daily waters can give us an early warning for where droughts are born: the\nreceding tips of the flowing network. Mapping them over years can give us a map\nof impermanence of our waters, showing where to expect water, and where not to.\nTo that end, we feed the latest high-res sensor data to multiple deep learning\nmodels in order to map these flowing networks every day, stacking the times\nseries maps over many years. Specifically, i) we enhance water segmentation to\n$50$ cm/pixel resolution, a 60$\\times$ improvement over previous\nstate-of-the-art results. Our U-Net trained on 30-40cm WorldView3 images can\ndetect streams as narrow as 1-3m (30-60$\\times$ over SOTA). Our multi-sensor,\nmulti-res variant, WasserNetz, fuses a multi-day window of 3m PlanetScope\nimagery with 1m LiDAR data, to detect streams 5-7m wide. Both U-Nets produce a\nwater probability map at the pixel-level. ii) We integrate this water map over\na DEM-derived synthetic valley network map to produce a snapshot of flow at the\nstream level. iii) We apply this pipeline, which we call Pix2Streams, to a\n2-year daily PlanetScope time-series of three watersheds in the US to produce\nthe first high-fidelity dynamic map of stream flow frequency. The end result is\na new map that, if applied at the national scale, could fundamentally improve\nhow we manage our water resources around the world.\n", "versions": [{"version": "v1", "created": "Sun, 15 Nov 2020 17:14:28 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["Garcia", "Dolores", ""], ["Mateo-Garcia", "Gonzalo", ""], ["Bernhardt", "Hannes", ""], ["Hagensieker", "Ron", ""], ["Francos", "Ignacio G. Lopez", ""], ["Stock", "Jonathan", ""], ["Schumann", "Guy", ""], ["Dobbs", "Kevin", ""], ["Kalaitzis", "Freddie", ""]]}, {"id": "2011.07589", "submitter": "Ajay Tanwani", "authors": "Ajay Kumar Tanwani", "title": "DIRL: Domain-Invariant Representation Learning for Sim-to-Real Transfer", "comments": "4th Conference on Robot Learning (CoRL), 2020 [plenary talk, Best\n  System Paper Award Finalist]", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Generating large-scale synthetic data in simulation is a feasible alternative\nto collecting/labelling real data for training vision-based deep learning\nmodels, albeit the modelling inaccuracies do not generalize to the physical\nworld. In this paper, we present a domain-invariant representation learning\n(DIRL) algorithm to adapt deep models to the physical environment with a small\namount of real data. Existing approaches that only mitigate the covariate shift\nby aligning the marginal distributions across the domains and assume the\nconditional distributions to be domain-invariant can lead to ambiguous transfer\nin real scenarios. We propose to jointly align the marginal (input domains) and\nthe conditional (output labels) distributions to mitigate the covariate and the\nconditional shift across the domains with adversarial learning, and combine it\nwith a triplet distribution loss to make the conditional distributions disjoint\nin the shared feature space. Experiments on digit domains yield\nstate-of-the-art performance on challenging benchmarks, while sim-to-real\ntransfer of object recognition for vision-based decluttering with a mobile\nrobot improves from 26.8 % to 91.0 %, resulting in 86.5 % grasping accuracy of\na wide variety of objects. Code and supplementary details are available at\nhttps://sites.google.com/view/dirl\n", "versions": [{"version": "v1", "created": "Sun, 15 Nov 2020 17:39:01 GMT"}, {"version": "v2", "created": "Fri, 27 Nov 2020 02:57:24 GMT"}, {"version": "v3", "created": "Thu, 7 Jan 2021 06:08:20 GMT"}], "update_date": "2021-01-08", "authors_parsed": [["Tanwani", "Ajay Kumar", ""]]}, {"id": "2011.07590", "submitter": "Jerry Liu", "authors": "Sourav Biswas, Jerry Liu, Kelvin Wong, Shenlong Wang, Raquel Urtasun", "title": "MuSCLE: Multi Sweep Compression of LiDAR using Deep Entropy Models", "comments": "NeurIPS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel compression algorithm for reducing the storage of LiDAR\nsensor data streams. Our model exploits spatio-temporal relationships across\nmultiple LiDAR sweeps to reduce the bitrate of both geometry and intensity\nvalues. Towards this goal, we propose a novel conditional entropy model that\nmodels the probabilities of the octree symbols by considering both coarse level\ngeometry and previous sweeps' geometric and intensity information. We then use\nthe learned probability to encode the full data stream into a compact one. Our\nexperiments demonstrate that our method significantly reduces the joint\ngeometry and intensity bitrate over prior state-of-the-art LiDAR compression\nmethods, with a reduction of 7-17% and 15-35% on the UrbanCity and\nSemanticKITTI datasets respectively.\n", "versions": [{"version": "v1", "created": "Sun, 15 Nov 2020 17:41:14 GMT"}, {"version": "v2", "created": "Fri, 8 Jan 2021 21:58:57 GMT"}], "update_date": "2021-01-12", "authors_parsed": [["Biswas", "Sourav", ""], ["Liu", "Jerry", ""], ["Wong", "Kelvin", ""], ["Wang", "Shenlong", ""], ["Urtasun", "Raquel", ""]]}, {"id": "2011.07592", "submitter": "Peter M. Full", "authors": "Peter M. Full, Fabian Isensee, Paul F. J\\\"ager, and Klaus Maier-Hein", "title": "Studying Robustness of Semantic Segmentation under Domain Shift in\n  cardiac MRI", "comments": "12 pages, 3 Figures, Contribution to the STACOM Workshop at MICCAI\n  2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Cardiac magnetic resonance imaging (cMRI) is an integral part of diagnosis in\nmany heart related diseases. Recently, deep neural networks have demonstrated\nsuccessful automatic segmentation, thus alleviating the burden of\ntime-consuming manual contouring of cardiac structures. Moreover, frameworks\nsuch as nnU-Net provide entirely automatic model configuration to unseen\ndatasets enabling out-of-the-box application even by non-experts. However,\ncurrent studies commonly neglect the clinically realistic scenario, in which a\ntrained network is applied to data from a different domain such as deviating\nscanners or imaging protocols. This potentially leads to unexpected performance\ndrops of deep learning models in real life applications. In this work, we\nsystematically study challenges and opportunities of domain transfer across\nimages from multiple clinical centres and scanner vendors. In order to maintain\nout-of-the-box usability, we build upon a fixed U-Net architecture configured\nby the nnU-net framework to investigate various data augmentation techniques\nand batch normalization layers as an easy-to-customize pipeline component and\nprovide general guidelines on how to improve domain generalizability abilities\nin existing deep learning methods. Our proposed method ranked first at the\nMulti-Centre, Multi-Vendor & Multi-Disease Cardiac Image Segmentation Challenge\n(M&Ms).\n", "versions": [{"version": "v1", "created": "Sun, 15 Nov 2020 17:50:23 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["Full", "Peter M.", ""], ["Isensee", "Fabian", ""], ["J\u00e4ger", "Paul F.", ""], ["Maier-Hein", "Klaus", ""]]}, {"id": "2011.07613", "submitter": "Swapnil Daga", "authors": "Swapnil Daga, Gokul B. Nair, Anirudha Ramesh, Rahul Sajnani, Junaid\n  Ahmed Ansari and K. Madhava Krishna", "title": "BirdSLAM: Monocular Multibody SLAM in Bird's-Eye View", "comments": "Accepted in VISIGRAPP (VISAPP) 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we present BirdSLAM, a novel simultaneous localization and\nmapping (SLAM) system for the challenging scenario of autonomous driving\nplatforms equipped with only a monocular camera. BirdSLAM tackles challenges\nfaced by other monocular SLAM systems (such as scale ambiguity in monocular\nreconstruction, dynamic object localization, and uncertainty in feature\nrepresentation) by using an orthographic (bird's-eye) view as the configuration\nspace in which localization and mapping are performed. By assuming only the\nheight of the ego-camera above the ground, BirdSLAM leverages single-view\nmetrology cues to accurately localize the ego-vehicle and all other traffic\nparticipants in bird's-eye view. We demonstrate that our system outperforms\nprior work that uses strictly greater information, and highlight the relevance\nof each design decision via an ablation analysis.\n", "versions": [{"version": "v1", "created": "Sun, 15 Nov 2020 19:37:24 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["Daga", "Swapnil", ""], ["Nair", "Gokul B.", ""], ["Ramesh", "Anirudha", ""], ["Sajnani", "Rahul", ""], ["Ansari", "Junaid Ahmed", ""], ["Krishna", "K. Madhava", ""]]}, {"id": "2011.07631", "submitter": "Debesh Jha", "authors": "Debesh Jha, Sharib Ali, Nikhil Kumar Tomar, H{\\aa}vard D. Johansen,\n  Dag D. Johansen, Jens Rittscher, Michael A. Riegler, and P{\\aa}l Halvorsen", "title": "Real-Time Polyp Detection, Localization and Segmentation in Colonoscopy\n  Using Deep Learning", "comments": null, "journal-ref": "Published in: IEEE Access, Page(s): 40496 - 40510, Date of\n  Publication: 04 March 2021, Electronic ISSN: 2169-3536, PubMed ID: 33747684\n  Publisher: IEEE", "doi": "10.1109/ACCESS.2021.3063716", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Computer-aided detection, localisation, and segmentation methods can help\nimprove colonoscopy procedures. Even though many methods have been built to\ntackle automatic detection and segmentation of polyps, benchmarking of\nstate-of-the-art methods still remains an open problem. This is due to the\nincreasing number of researched computer vision methods that can be applied to\npolyp datasets. Benchmarking of novel methods can provide a direction to the\ndevelopment of automated polyp detection and segmentation tasks. Furthermore,\nit ensures that the produced results in the community are reproducible and\nprovide a fair comparison of developed methods. In this paper, we benchmark\nseveral recent state-of-the-art methods using Kvasir-SEG, an open-access\ndataset of colonoscopy images for polyp detection, localisation, and\nsegmentation evaluating both method accuracy and speed. Whilst, most methods in\nliterature have competitive performance over accuracy, we show that the\nproposed ColonSegNet achieved a better trade-off between an average precision\nof 0.8000 and mean IoU of 0.8100, and the fastest speed of 180 frames per\nsecond for the detection and localisation task. Likewise, the proposed\nColonSegNet achieved a competitive dice coefficient of 0.8206 and the best\naverage speed of 182.38 frames per second for the segmentation task. Our\ncomprehensive comparison with various state-of-the-art methods reveals the\nimportance of benchmarking the deep learning methods for automated real-time\npolyp identification and delineations that can potentially transform current\nclinical practices and minimise miss-detection rates.\n", "versions": [{"version": "v1", "created": "Sun, 15 Nov 2020 21:14:50 GMT"}, {"version": "v2", "created": "Wed, 31 Mar 2021 20:21:06 GMT"}], "update_date": "2021-04-02", "authors_parsed": [["Jha", "Debesh", ""], ["Ali", "Sharib", ""], ["Tomar", "Nikhil Kumar", ""], ["Johansen", "H\u00e5vard D.", ""], ["Johansen", "Dag D.", ""], ["Rittscher", "Jens", ""], ["Riegler", "Michael A.", ""], ["Halvorsen", "P\u00e5l", ""]]}, {"id": "2011.07658", "submitter": "Lukun Zheng", "authors": "Chandra Kundu, Lukun Zheng", "title": "Deep multi-modal networks for book genre classification based on its\n  cover", "comments": "23 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Book covers are usually the very first impression to its readers and they\noften convey important information about the content of the book. Book genre\nclassification based on its cover would be utterly beneficial to many modern\nretrieval systems, considering that the complete digitization of books is an\nextremely expensive task. At the same time, it is also an extremely challenging\ntask due to the following reasons: First, there exists a wide variety of book\ngenres, many of which are not concretely defined. Second, book covers, as\ngraphic designs, vary in many different ways such as colors, styles, textual\ninformation, etc, even for books of the same genre. Third, book cover designs\nmay vary due to many external factors such as country, culture, target reader\npopulations, etc. With the growing competitiveness in the book industry, the\nbook cover designers and typographers push the cover designs to its limit in\nthe hope of attracting sales. The cover-based book classification systems\nbecome a particularly exciting research topic in recent years. In this paper,\nwe propose a multi-modal deep learning framework to solve this problem. The\ncontribution of this paper is four-fold. First, our method adds an extra\nmodality by extracting texts automatically from the book covers. Second,\nimage-based and text-based, state-of-the-art models are evaluated thoroughly\nfor the task of book cover classification. Third, we develop an efficient and\nsalable multi-modal framework based on the images and texts shown on the covers\nonly. Fourth, a thorough analysis of the experimental results is given and\nfuture works to improve the performance is suggested. The results show that the\nmulti-modal framework significantly outperforms the current state-of-the-art\nimage-based models. However, more efforts and resources are needed for this\nclassification task in order to reach a satisfactory level.\n", "versions": [{"version": "v1", "created": "Sun, 15 Nov 2020 23:27:43 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["Kundu", "Chandra", ""], ["Zheng", "Lukun", ""]]}, {"id": "2011.07660", "submitter": "Hyounghun Kim", "authors": "Hyounghun Kim, Abhay Zala, Graham Burri, Hao Tan, Mohit Bansal", "title": "ArraMon: A Joint Navigation-Assembly Instruction Interpretation Task in\n  Dynamic Environments", "comments": "EMNLP Findings 2020 (18 pages; extended to Hindi)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For embodied agents, navigation is an important ability but not an isolated\ngoal. Agents are also expected to perform specific tasks after reaching the\ntarget location, such as picking up objects and assembling them into a\nparticular arrangement. We combine Vision-and-Language Navigation, assembling\nof collected objects, and object referring expression comprehension, to create\na novel joint navigation-and-assembly task, named ArraMon. During this task,\nthe agent (similar to a PokeMON GO player) is asked to find and collect\ndifferent target objects one-by-one by navigating based on natural language\ninstructions in a complex, realistic outdoor environment, but then also ARRAnge\nthe collected objects part-by-part in an egocentric grid-layout environment. To\nsupport this task, we implement a 3D dynamic environment simulator and collect\na dataset (in English; and also extended to Hindi) with human-written\nnavigation and assembling instructions, and the corresponding ground truth\ntrajectories. We also filter the collected instructions via a verification\nstage, leading to a total of 7.7K task instances (30.8K instructions and\npaths). We present results for several baseline models (integrated and biased)\nand metrics (nDTW, CTC, rPOD, and PTC), and the large model-human performance\ngap demonstrates that our task is challenging and presents a wide scope for\nfuture work. Our dataset, simulator, and code are publicly available at:\nhttps://arramonunc.github.io\n", "versions": [{"version": "v1", "created": "Sun, 15 Nov 2020 23:30:36 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["Kim", "Hyounghun", ""], ["Zala", "Abhay", ""], ["Burri", "Graham", ""], ["Tan", "Hao", ""], ["Bansal", "Mohit", ""]]}, {"id": "2011.07661", "submitter": "Luca Parisi", "authors": "Luca Parisi, Renfei Ma, Narrendar RaviChandran and Matteo Lanzillotta", "title": "hyper-sinh: An Accurate and Reliable Function from Shallow to Deep\n  Learning in TensorFlow and Keras", "comments": "19 pages, 6 listings/Python code snippets, 4 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CL cs.LG cs.NE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper presents the 'hyper-sinh', a variation of the m-arcsinh activation\nfunction suitable for Deep Learning (DL)-based algorithms for supervised\nlearning, such as Convolutional Neural Networks (CNN). hyper-sinh, developed in\nthe open source Python libraries TensorFlow and Keras, is thus described and\nvalidated as an accurate and reliable activation function for both shallow and\ndeep neural networks. Improvements in accuracy and reliability in image and\ntext classification tasks on five (N = 5) benchmark data sets available from\nKeras are discussed. Experimental results demonstrate the overall competitive\nclassification performance of both shallow and deep neural networks, obtained\nvia this novel function. This function is evaluated with respect to gold\nstandard activation functions, demonstrating its overall competitive accuracy\nand reliability for both image and text classification.\n", "versions": [{"version": "v1", "created": "Sun, 15 Nov 2020 23:38:59 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["Parisi", "Luca", ""], ["Ma", "Renfei", ""], ["RaviChandran", "Narrendar", ""], ["Lanzillotta", "Matteo", ""]]}, {"id": "2011.07682", "submitter": "Scott Freitas", "authors": "Scott Freitas, Yuxiao Dong, Joshua Neil, Duen Horng Chau", "title": "A Large-Scale Database for Graph Representation Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CR cs.CV cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the rapid emergence of graph representation learning, the construction\nof new large-scale datasets are necessary to distinguish model capabilities and\naccurately assess the strengths and weaknesses of each technique. By carefully\nanalyzing existing graph databases, we identify 3 critical components important\nfor advancing the field of graph representation learning: (1) large graphs, (2)\nmany graphs, and (3) class diversity. To date, no single graph database offers\nall of these desired properties. We introduce MalNet, the largest public graph\ndatabase ever constructed, representing a large-scale ontology of software\nfunction call graphs. MalNet contains over 1.2 million graphs, averaging over\n17k nodes and 39k edges per graph, across a hierarchy of 47 types and 696\nfamilies. Compared to the popular REDDIT-12K database, MalNet offers 105x more\ngraphs, 44x larger graphs on average, and 63x the classes. We provide a\ndetailed analysis of MalNet, discussing its properties and provenance. The\nunprecedented scale and diversity of MalNet offers exciting opportunities to\nadvance the frontiers of graph representation learning---enabling new\ndiscoveries and research into imbalanced classification, explainability and the\nimpact of class hardness. The database is publically available at\nwww.mal-net.org.\n", "versions": [{"version": "v1", "created": "Mon, 16 Nov 2020 01:50:21 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["Freitas", "Scott", ""], ["Dong", "Yuxiao", ""], ["Neil", "Joshua", ""], ["Chau", "Duen Horng", ""]]}, {"id": "2011.07689", "submitter": "Yi Luo", "authors": "Yi Luo (1), Siyi Chen (2), X.-G. Ma (2) ((1) School of Energy and\n  Environment, Southeast University, Nanjing, China (2) International Institute\n  for Urban Systems Engineering, Southeast University, Nanjing, China)", "title": "Drone LAMS: A Drone-based Face Detection Dataset with Large Angles and\n  Many Scenarios", "comments": "8 pages, 6 figures,conference or other essential info", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work presented a new drone-based face detection dataset Drone LAMS in\norder to solve issues of low performance of drone-based face detection in\nscenarios such as large angles which was a predominant working condition when a\ndrone flies high. The proposed dataset captured images from 261 videos with\nover 43k annotations and 4.0k images with pitch or yaw angle in the range of\n-90{\\deg} to 90{\\deg}. Drone LAMS showed significant improvement over currently\navailable drone-based face detection datasets in terms of detection\nperformance, especially with large pitch and yaw angle. Detailed analysis of\nhow key factors, such as duplication rate, annotation method, etc., impact\ndataset performance was also provided to facilitate further usage of a drone on\nface detection.\n", "versions": [{"version": "v1", "created": "Mon, 16 Nov 2020 02:26:05 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["Luo", "Yi", ""], ["Chen", "Siyi", ""], ["Ma", "X. -G.", ""]]}, {"id": "2011.07697", "submitter": "MaungMaung AprilPyone", "authors": "MaungMaung AprilPyone and Hitoshi Kiya", "title": "Ensemble of Models Trained by Key-based Transformed Images for\n  Adversarially Robust Defense Against Black-box Attacks", "comments": "Under review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a voting ensemble of models trained by using block-wise\ntransformed images with secret keys for an adversarially robust defense.\nKey-based adversarial defenses were demonstrated to outperform state-of-the-art\ndefenses against gradient-based (white-box) attacks. However, the key-based\ndefenses are not effective enough against gradient-free (black-box) attacks\nwithout requiring any secret keys. Accordingly, we aim to enhance robustness\nagainst black-box attacks by using a voting ensemble of models. In the proposed\nensemble, a number of models are trained by using images transformed with\ndifferent keys and block sizes, and then a voting ensemble is applied to the\nmodels. In image classification experiments, the proposed defense is\ndemonstrated to defend state-of-the-art attacks. The proposed defense achieves\na clean accuracy of 95.56 % and an attack success rate of less than 9 % under\nattacks with a noise distance of 8/255 on the CIFAR-10 dataset.\n", "versions": [{"version": "v1", "created": "Mon, 16 Nov 2020 02:48:37 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["AprilPyone", "MaungMaung", ""], ["Kiya", "Hitoshi", ""]]}, {"id": "2011.07704", "submitter": "Peng Gao", "authors": "Peng Gao, Rui Guo, Hongsheng Lu and Hao Zhang", "title": "Multi-view Sensor Fusion by Integrating Model-based Estimation and Graph\n  Learning for Collaborative Object Localization", "comments": "Revise several typos and change the Fig2 to be more illustrative", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Collaborative object localization aims to collaboratively estimate locations\nof objects observed from multiple views or perspectives, which is a critical\nability for multi-agent systems such as connected vehicles. To enable\ncollaborative localization, several model-based state estimation and\nlearning-based localization methods have been developed. Given their\nencouraging performance, model-based state estimation often lacks the ability\nto model the complex relationships among multiple objects, while learning-based\nmethods are typically not able to fuse the observations from an arbitrary\nnumber of views and cannot well model uncertainty. In this paper, we introduce\na novel spatiotemporal graph filter approach that integrates graph learning and\nmodel-based estimation to perform multi-view sensor fusion for collaborative\nobject localization. Our approach models complex object relationships using a\nnew spatiotemporal graph representation and fuses multi-view observations in a\nBayesian fashion to improve location estimation under uncertainty. We evaluate\nour approach in the applications of connected autonomous driving and multiple\npedestrian localization. Experimental results show that our approach\noutperforms previous techniques and achieves the state-of-the-art performance\non collaboration localization.\n", "versions": [{"version": "v1", "created": "Mon, 16 Nov 2020 03:33:28 GMT"}, {"version": "v2", "created": "Sun, 7 Mar 2021 01:31:36 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Gao", "Peng", ""], ["Guo", "Rui", ""], ["Lu", "Hongsheng", ""], ["Zhang", "Hao", ""]]}, {"id": "2011.07706", "submitter": "Seungkyu Lee", "authors": "Gahye Lee and Seungkyu Lee", "title": "Mode Penalty Generative Adversarial Network with adapted Auto-encoder", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative Adversarial Networks (GAN) are trained to generate sample images\nof interest distribution. To this end, generator network of GAN learns implicit\ndistribution of real data set from the classification with candidate generated\nsamples. Recently, various GANs have suggested novel ideas for stable\noptimizing of its networks. However, in real implementation, sometimes they\nstill represent a only narrow part of true distribution or fail to converge. We\nassume this ill posed problem comes from poor gradient from objective function\nof discriminator, which easily trap the generator in a bad situation. To\naddress this problem, we propose a mode penalty GAN combined with pre-trained\nauto encoder for explicit representation of generated and real data samples in\nthe encoded space. In this space, we make a generator manifold to follow a real\nmanifold by finding entire modes of target distribution. In addition, penalty\nfor uncovered modes of target distribution is given to the generator which\nencourages it to find overall target distribution. We demonstrate that applying\nthe proposed method to GANs helps generator's optimization becoming more stable\nand having faster convergence through experimental evaluations.\n", "versions": [{"version": "v1", "created": "Mon, 16 Nov 2020 03:39:53 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["Lee", "Gahye", ""], ["Lee", "Seungkyu", ""]]}, {"id": "2011.07713", "submitter": "Shalabh Gupta", "authors": "Jing Yang and James P. Wilson and Shalabh Gupta", "title": "DARE: AI-based Diver Action Recognition System using Multi-Channel CNNs\n  for AUV Supervision", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.HC cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the growth of sensing, control and robotic technologies, autonomous\nunderwater vehicles (AUVs) have become useful assistants to human divers for\nperforming various underwater operations. In the current practice, the divers\nare required to carry expensive, bulky, and waterproof keyboards or\njoystick-based controllers for supervision and control of AUVs. Therefore,\ndiver action-based supervision is becoming increasingly popular because it is\nconvenient, easier to use, faster, and cost effective. However, the various\nenvironmental, diver and sensing uncertainties present underwater makes it\nchallenging to train a robust and reliable diver action recognition system. In\nthis regard, this paper presents DARE, a diver action recognition system, that\nis trained based on Cognitive Autonomous Driving Buddy (CADDY) dataset, which\nis a rich set of data containing images of different diver gestures and poses\nin several different and realistic underwater environments. DARE is based on\nfusion of stereo-pairs of camera images using a multi-channel convolutional\nneural network supported with a systematically trained tree-topological deep\nneural network classifier to enhance the classification performance. DARE is\nfast and requires only a few milliseconds to classify one stereo-pair, thus\nmaking it suitable for real-time underwater implementation. DARE is\ncomparatively evaluated against several existing classifier architectures and\nthe results show that DARE supersedes the performance of all classifiers for\ndiver action recognition in terms of overall as well as individual class\naccuracies and F1-scores.\n", "versions": [{"version": "v1", "created": "Mon, 16 Nov 2020 04:05:32 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["Yang", "Jing", ""], ["Wilson", "James P.", ""], ["Gupta", "Shalabh", ""]]}, {"id": "2011.07733", "submitter": "Zhaoqun Li", "authors": "Zhaoqun Li", "title": "Gram Regularization for Multi-view 3D Shape Retrieval", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  How to obtain the desirable representation of a 3D shape is a key challenge\nin 3D shape retrieval task. Most existing 3D shape retrieval methods focus on\ncapturing shape representation with different neural network architectures,\nwhile the learning ability of each layer in the network is neglected. A common\nand tough issue that limits the capacity of the network is overfitting. To\ntackle this, L2 regularization is applied widely in existing deep learning\nframeworks. However,the effect on the generalization ability with L2\nregularization is limited as it only controls large value in parameters. To\nmake up the gap, in this paper, we propose a novel regularization term called\nGram regularization which reinforces the learning ability of the network by\nencouraging the weight kernels to extract different information on the\ncorresponding feature map. By forcing the variance between weight kernels to be\nlarge, the regularizer can help to extract discriminative features. The\nproposed Gram regularization is data independent and can converge stably and\nquickly without bells and whistles. Moreover, it can be easily plugged into\nexisting off-the-shelf architectures. Extensive experimental results on the\npopular 3D object retrieval benchmark ModelNet demonstrate the effectiveness of\nour method.\n", "versions": [{"version": "v1", "created": "Mon, 16 Nov 2020 05:37:24 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["Li", "Zhaoqun", ""]]}, {"id": "2011.07735", "submitter": "Aman Chadha Mr.", "authors": "Aman Chadha, Gurneet Arora, Navpreet Kaloty", "title": "iPerceive: Applying Common-Sense Reasoning to Multi-Modal Dense Video\n  Captioning and Video Question Answering", "comments": "13 pages, 6 figures, 4 tables, Project Page:\n  https://iperceive.amanchadha.com", "journal-ref": "IEEE Winter Conference on Applications of Computer Vision (WACV)\n  2021", "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG cs.MM eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most prior art in visual understanding relies solely on analyzing the \"what\"\n(e.g., event recognition) and \"where\" (e.g., event localization), which in some\ncases, fails to describe correct contextual relationships between events or\nleads to incorrect underlying visual attention. Part of what defines us as\nhuman and fundamentally different from machines is our instinct to seek\ncausality behind any association, say an event Y that happened as a direct\nresult of event X. To this end, we propose iPerceive, a framework capable of\nunderstanding the \"why\" between events in a video by building a common-sense\nknowledge base using contextual cues to infer causal relationships between\nobjects in the video. We demonstrate the effectiveness of our technique using\nthe dense video captioning (DVC) and video question answering (VideoQA) tasks.\nFurthermore, while most prior work in DVC and VideoQA relies solely on visual\ninformation, other modalities such as audio and speech are vital for a human\nobserver's perception of an environment. We formulate DVC and VideoQA tasks as\nmachine translation problems that utilize multiple modalities. By evaluating\nthe performance of iPerceive DVC and iPerceive VideoQA on the ActivityNet\nCaptions and TVQA datasets respectively, we show that our approach furthers the\nstate-of-the-art. Code and samples are available at: iperceive.amanchadha.com.\n", "versions": [{"version": "v1", "created": "Mon, 16 Nov 2020 05:44:45 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["Chadha", "Aman", ""], ["Arora", "Gurneet", ""], ["Kaloty", "Navpreet", ""]]}, {"id": "2011.07747", "submitter": "Shivaank Agarwal", "authors": "Shivaank Agarwal, Ravindra Gudi, Paresh Saxena", "title": "Application of Computer Vision Techniques for Segregation of\n  PlasticWaste based on Resin Identification Code", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper presents methods to identify the plastic waste based on its resin\nidentification code to provide an efficient recycling of post-consumer plastic\nwaste. We propose the design, training and testing of different machine\nlearning techniques to (i) identify a plastic waste that belongs to the known\ncategories of plastic waste when the system is trained and (ii) identify a new\nplastic waste that do not belong the any known categories of plastic waste\nwhile the system is trained. For the first case,we propose the use of one-shot\nlearning techniques using Siamese and Triplet loss networks. Our proposed\napproach does not require any augmentation to increase the size of the database\nand achieved a high accuracy of 99.74%. For the second case, we propose the use\nof supervised and unsupervised dimensionality reduction techniques and achieved\nan accuracy of 95% to correctly identify a new plastic waste.\n", "versions": [{"version": "v1", "created": "Mon, 16 Nov 2020 06:50:32 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["Agarwal", "Shivaank", ""], ["Gudi", "Ravindra", ""], ["Saxena", "Paresh", ""]]}, {"id": "2011.07748", "submitter": "Guanya Shi", "authors": "Guanya Shi, Yifeng Zhu, Jonathan Tremblay, Stan Birchfield, Fabio\n  Ramos, Animashree Anandkumar, Yuke Zhu", "title": "Fast Uncertainty Quantification for Deep Object Pose Estimation", "comments": "Video and code are available at https://sites.google.com/view/fastuq", "journal-ref": "International Conferenceon Robotics and Automation (ICRA), 2021", "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning-based object pose estimators are often unreliable and\noverconfident especially when the input image is outside the training domain,\nfor instance, with sim2real transfer. Efficient and robust uncertainty\nquantification (UQ) in pose estimators is critically needed in many robotic\ntasks. In this work, we propose a simple, efficient, and plug-and-play UQ\nmethod for 6-DoF object pose estimation. We ensemble 2-3 pre-trained models\nwith different neural network architectures and/or training data sources, and\ncompute their average pairwise disagreement against one another to obtain the\nuncertainty quantification. We propose four disagreement metrics, including a\nlearned metric, and show that the average distance (ADD) is the best\nlearning-free metric and it is only slightly worse than the learned metric,\nwhich requires labeled target data. Our method has several advantages compared\nto the prior art: 1) our method does not require any modification of the\ntraining process or the model inputs; and 2) it needs only one forward pass for\neach model. We evaluate the proposed UQ method on three tasks where our\nuncertainty quantification yields much stronger correlations with pose\nestimation errors than the baselines. Moreover, in a real robot grasping task,\nour method increases the grasping success rate from 35% to 90%.\n", "versions": [{"version": "v1", "created": "Mon, 16 Nov 2020 06:51:55 GMT"}, {"version": "v2", "created": "Mon, 22 Mar 2021 20:38:01 GMT"}, {"version": "v3", "created": "Fri, 26 Mar 2021 05:13:32 GMT"}], "update_date": "2021-03-29", "authors_parsed": [["Shi", "Guanya", ""], ["Zhu", "Yifeng", ""], ["Tremblay", "Jonathan", ""], ["Birchfield", "Stan", ""], ["Ramos", "Fabio", ""], ["Anandkumar", "Animashree", ""], ["Zhu", "Yuke", ""]]}, {"id": "2011.07750", "submitter": "Feras Dayoub", "authors": "Quazi Marufur Rahman, Niko S\\\"underhauf, Feras Dayoub", "title": "Online Monitoring of Object Detection Performance During Deployment", "comments": "V2 with more experimental results and improved clarity of\n  presentation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  During deployment, an object detector is expected to operate at a similar\nperformance level reported on its testing dataset. However, when deployed\nonboard mobile robots that operate under varying and complex environmental\nconditions, the detector's performance can fluctuate and occasionally degrade\nseverely without warning. Undetected, this can lead the robot to take unsafe\nand risky actions based on low-quality and unreliable object detections. We\naddress this problem and introduce a cascaded neural network that monitors the\nperformance of the object detector by predicting the quality of its mean\naverage precision (mAP) on a sliding window of the input frames. The proposed\ncascaded network exploits the internal features from the deep neural network of\nthe object detector. We evaluate our proposed approach using different\ncombinations of autonomous driving datasets and object detectors.\n", "versions": [{"version": "v1", "created": "Mon, 16 Nov 2020 07:01:43 GMT"}, {"version": "v2", "created": "Tue, 9 Mar 2021 06:20:05 GMT"}], "update_date": "2021-03-10", "authors_parsed": [["Rahman", "Quazi Marufur", ""], ["S\u00fcnderhauf", "Niko", ""], ["Dayoub", "Feras", ""]]}, {"id": "2011.07756", "submitter": "Shaohua Wang", "authors": "Shaohua Wang, Yaping Dai", "title": "Zero Cost Improvements for General Object Detection Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Modern object detection networks pursuit higher precision on general object\ndetection datasets, at the same time the computation burden is also increasing\nalong with the improvement of precision. Nevertheless, the inference time and\nprecision are both critical to object detection system which needs to be\nreal-time. It is necessary to research precision improvement without extra\ncomputation cost. In this work, two modules are proposed to improve detection\nprecision with zero cost, which are focus on FPN and detection head improvement\nfor general object detection networks. We employ the scale attention mechanism\nto efficiently fuse multi-level feature maps with less parameters, which is\ncalled SA-FPN module. Considering the correlation of classification head and\nregression head, we use sequential head to take the place of widely-used\nparallel head, which is called Seq-HEAD module. To evaluate the effectiveness,\nwe apply the two modules to some modern state-of-art object detection networks,\nincluding anchor-based and anchor-free. Experiment results on coco dataset show\nthat the networks with the two modules can surpass original networks by 1.1 AP\nand 0.8 AP with zero cost for anchor-based and anchor-free networks,\nrespectively. Code will be available at https://git.io/JTFGl.\n", "versions": [{"version": "v1", "created": "Mon, 16 Nov 2020 07:21:57 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["Wang", "Shaohua", ""], ["Dai", "Yaping", ""]]}, {"id": "2011.07774", "submitter": "Zekun Li", "authors": "Zekun Li, Yufan Liu, Bing Li, Weiming Hu", "title": "DSIC: Dynamic Sample-Individualized Connector for Multi-Scale Object\n  Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Although object detection has reached a milestone thanks to the great success\nof deep learning, the scale variation is still the key challenge. Integrating\nmulti-level features is presented to alleviate the problems, like the classic\nFeature Pyramid Network (FPN) and its improvements. However, the specifically\ndesigned feature integration modules of these methods may not have the optimal\narchitecture for feature fusion. Moreover, these models have fixed\narchitectures and data flow paths, when fed with various samples. They cannot\nadjust and be compatible with each kind of data. To overcome the above\nlimitations, we propose a Dynamic Sample-Individualized Connector (DSIC) for\nmulti-scale object detection. It dynamically adjusts network connections to fit\ndifferent samples. In particular, DSIC consists of two components: Intra-scale\nSelection Gate (ISG) and Cross-scale Selection Gate (CSG). ISG adaptively\nextracts multi-level features from backbone as the input of feature\nintegration. CSG automatically activate informative data flow paths based on\nthe multi-level features. Furthermore, these two components are both\nplug-and-play and can be embedded in any backbone. Experimental results\ndemonstrate that the proposed method outperforms the state-of-the-arts.\n", "versions": [{"version": "v1", "created": "Mon, 16 Nov 2020 08:13:58 GMT"}, {"version": "v2", "created": "Thu, 25 Mar 2021 02:14:16 GMT"}], "update_date": "2021-03-26", "authors_parsed": [["Li", "Zekun", ""], ["Liu", "Yufan", ""], ["Li", "Bing", ""], ["Hu", "Weiming", ""]]}, {"id": "2011.07777", "submitter": "Jun Wan", "authors": "Jun Wan, Zhihui Lai, Linlin Shen, Jie Zhou, Can Gao, Gang Xiao and\n  Xianxu Hou", "title": "Robust Facial Landmark Detection by Cross-order Cross-semantic Deep\n  Network", "comments": "This paper has been accepted by Neural Networks, November 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recently, convolutional neural networks (CNNs)-based facial landmark\ndetection methods have achieved great success. However, most of existing\nCNN-based facial landmark detection methods have not attempted to activate\nmultiple correlated facial parts and learn different semantic features from\nthem that they can not accurately model the relationships among the local\ndetails and can not fully explore more discriminative and fine semantic\nfeatures, thus they suffer from partial occlusions and large pose variations.\nTo address these problems, we propose a cross-order cross-semantic deep network\n(CCDN) to boost the semantic features learning for robust facial landmark\ndetection. Specifically, a cross-order two-squeeze multi-excitation (CTM)\nmodule is proposed to introduce the cross-order channel correlations for more\ndiscriminative representations learning and multiple attention-specific part\nactivation. Moreover, a novel cross-order cross-semantic (COCS) regularizer is\ndesigned to drive the network to learn cross-order cross-semantic features from\ndifferent activation for facial landmark detection. It is interesting to show\nthat by integrating the CTM module and COCS regularizer, the proposed CCDN can\neffectively activate and learn more fine and complementary cross-order\ncross-semantic features to improve the accuracy of facial landmark detection\nunder extremely challenging scenarios. Experimental results on challenging\nbenchmark datasets demonstrate the superiority of our CCDN over\nstate-of-the-art facial landmark detection methods.\n", "versions": [{"version": "v1", "created": "Mon, 16 Nov 2020 08:19:26 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["Wan", "Jun", ""], ["Lai", "Zhihui", ""], ["Shen", "Linlin", ""], ["Zhou", "Jie", ""], ["Gao", "Can", ""], ["Xiao", "Gang", ""], ["Hou", "Xianxu", ""]]}, {"id": "2011.07784", "submitter": "Chi Zhang", "authors": "Zhen Yang and Chi Zhang and Huiming Guo and Zhaoxiang Zhang", "title": "Manual-Label Free 3D Detection via An Open-Source Simulator", "comments": "8 pages, 4 figures, ICPR2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  LiDAR based 3D object detectors typically need a large amount of\ndetailed-labeled point cloud data for training, but these detailed labels are\ncommonly expensive to acquire. In this paper, we propose a manual-label free 3D\ndetection algorithm that leverages the CARLA simulator to generate a large\namount of self-labeled training samples and introduces a novel Domain Adaptive\nVoxelNet (DA-VoxelNet) that can cross the distribution gap from the synthetic\ndata to the real scenario. The self-labeled training samples are generated by a\nset of high quality 3D models embedded in a CARLA simulator and a proposed\nLiDAR-guided sampling algorithm. Then a DA-VoxelNet that integrates both a\nsample-level DA module and an anchor-level DA module is proposed to enable the\ndetector trained by the synthetic data to adapt to real scenario. Experimental\nresults show that the proposed unsupervised DA 3D detector on KITTI evaluation\nset can achieve 76.66% and 56.64% mAP on BEV mode and 3D mode respectively. The\nresults reveal a promising perspective of training a LIDAR-based 3D detector\nwithout any hand-tagged label.\n", "versions": [{"version": "v1", "created": "Mon, 16 Nov 2020 08:29:01 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["Yang", "Zhen", ""], ["Zhang", "Chi", ""], ["Guo", "Huiming", ""], ["Zhang", "Zhaoxiang", ""]]}, {"id": "2011.07787", "submitter": "Jinmiao Cai", "authors": "Jinmiao Cai, Nianjuan Jiang, Xiaoguang Han, Kui Jia, Jiangbo Lu", "title": "JOLO-GCN: Mining Joint-Centered Light-Weight Information for\n  Skeleton-Based Action Recognition", "comments": "Accepted at IEEE WACV 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Skeleton-based action recognition has attracted research attentions in recent\nyears. One common drawback in currently popular skeleton-based human action\nrecognition methods is that the sparse skeleton information alone is not\nsufficient to fully characterize human motion. This limitation makes several\nexisting methods incapable of correctly classifying action categories which\nexhibit only subtle motion differences. In this paper, we propose a novel\nframework for employing human pose skeleton and joint-centered light-weight\ninformation jointly in a two-stream graph convolutional network, namely,\nJOLO-GCN. Specifically, we use Joint-aligned optical Flow Patches (JFP) to\ncapture the local subtle motion around each joint as the pivotal joint-centered\nvisual information. Compared to the pure skeleton-based baseline, this hybrid\nscheme effectively boosts performance, while keeping the computational and\nmemory overheads low. Experiments on the NTU RGB+D, NTU RGB+D 120, and the\nKinetics-Skeleton dataset demonstrate clear accuracy improvements attained by\nthe proposed method over the state-of-the-art skeleton-based methods.\n", "versions": [{"version": "v1", "created": "Mon, 16 Nov 2020 08:39:22 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["Cai", "Jinmiao", ""], ["Jiang", "Nianjuan", ""], ["Han", "Xiaoguang", ""], ["Jia", "Kui", ""], ["Lu", "Jiangbo", ""]]}, {"id": "2011.07792", "submitter": "Edoardo Daniele Cannas", "authors": "Luca Bondi, Edoardo Daniele Cannas, Paolo Bestagini, Stefano Tubaro", "title": "Training Strategies and Data Augmentations in CNN-based DeepFake Video\n  Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.MM eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The fast and continuous growth in number and quality of deepfake videos calls\nfor the development of reliable detection systems capable of automatically\nwarning users on social media and on the Internet about the potential\nuntruthfulness of such contents. While algorithms, software, and smartphone\napps are getting better every day in generating manipulated videos and swapping\nfaces, the accuracy of automated systems for face forgery detection in videos\nis still quite limited and generally biased toward the dataset used to design\nand train a specific detection system. In this paper we analyze how different\ntraining strategies and data augmentation techniques affect CNN-based deepfake\ndetectors when training and testing on the same dataset or across different\ndatasets.\n", "versions": [{"version": "v1", "created": "Mon, 16 Nov 2020 08:50:56 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["Bondi", "Luca", ""], ["Cannas", "Edoardo Daniele", ""], ["Bestagini", "Paolo", ""], ["Tubaro", "Stefano", ""]]}, {"id": "2011.07795", "submitter": "Connah Kendrick", "authors": "David Gillespie, Connah Kendrick, Ian Boon, Cheng Boon, Tim Rattay,\n  Moi Hoon Yap", "title": "Deep learning in magnetic resonance prostate segmentation: A review and\n  a new perspective", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Prostate radiotherapy is a well established curative oncology modality, which\nin future will use Magnetic Resonance Imaging (MRI)-based radiotherapy for\ndaily adaptive radiotherapy target definition. However the time needed to\ndelineate the prostate from MRI data accurately is a time consuming process.\nDeep learning has been identified as a potential new technology for the\ndelivery of precision radiotherapy in prostate cancer, where accurate prostate\nsegmentation helps in cancer detection and therapy. However, the trained models\ncan be limited in their application to clinical setting due to different\nacquisition protocols, limited publicly available datasets, where the size of\nthe datasets are relatively small. Therefore, to explore the field of prostate\nsegmentation and to discover a generalisable solution, we review the\nstate-of-the-art deep learning algorithms in MR prostate segmentation; provide\ninsights to the field by discussing their limitations and strengths; and\npropose an optimised 2D U-Net for MR prostate segmentation. We evaluate the\nperformance on four publicly available datasets using Dice Similarity\nCoefficient (DSC) as performance metric. Our experiments include within dataset\nevaluation and cross-dataset evaluation. The best result is achieved by\ncomposite evaluation (DSC of 0.9427 on Decathlon test set) and the poorest\nresult is achieved by cross-dataset evaluation (DSC of 0.5892, Prostate X\ntraining set, Promise 12 testing set). We outline the challenges and provide\nrecommendations for future work. Our research provides a new perspective to MR\nprostate segmentation and more importantly, we provide standardised experiment\nsettings for researchers to evaluate their algorithms. Our code is available at\nhttps://github.com/AIEMMU/MRI\\_Prostate.\n", "versions": [{"version": "v1", "created": "Mon, 16 Nov 2020 08:58:38 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["Gillespie", "David", ""], ["Kendrick", "Connah", ""], ["Boon", "Ian", ""], ["Boon", "Cheng", ""], ["Rattay", "Tim", ""], ["Yap", "Moi Hoon", ""]]}, {"id": "2011.07815", "submitter": "Hao Su", "authors": "Hao Su, Jianwei Niu, Xuefeng Liu, Qingfeng Li, Ji Wan, Mingliang Xu,\n  Tao Ren", "title": "An End-to-end Method for Producing Scanning-robust Stylized QR Codes", "comments": "11 pages, 16 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quick Response (QR) code is one of the most worldwide used two-dimensional\ncodes.~Traditional QR codes appear as random collections of black-and-white\nmodules that lack visual semantics and aesthetic elements, which inspires the\nrecent works to beautify the appearances of QR codes. However, these works\nadopt fixed generation algorithms and therefore can only generate QR codes with\na pre-defined style. In this paper, combining the Neural Style Transfer\ntechnique, we propose a novel end-to-end method, named ArtCoder, to generate\nthe stylized QR codes that are personalized, diverse, attractive, and\nscanning-robust.~To guarantee that the generated stylized QR codes are still\nscanning-robust, we propose a Sampling-Simulation layer, a module-based code\nloss, and a competition mechanism. The experimental results show that our\nstylized QR codes have high-quality in both the visual effect and the\nscanning-robustness, and they are able to support the real-world application.\n", "versions": [{"version": "v1", "created": "Mon, 16 Nov 2020 09:38:27 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["Su", "Hao", ""], ["Niu", "Jianwei", ""], ["Liu", "Xuefeng", ""], ["Li", "Qingfeng", ""], ["Wan", "Ji", ""], ["Xu", "Mingliang", ""], ["Ren", "Tao", ""]]}, {"id": "2011.07915", "submitter": "Sanqing Qu", "authors": "Sanqing Qu, Guang Chen, Dan Xu, Jinhu Dong, Fan Lu, Alois Knoll", "title": "LAP-Net: Adaptive Features Sampling via Learning Action Progression for\n  Online Action Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Online action detection is a task with the aim of identifying ongoing actions\nfrom streaming videos without any side information or access to future frames.\nRecent methods proposed to aggregate fixed temporal ranges of invisible but\nanticipated future frames representations as supplementary features and\nachieved promising performance. They are based on the observation that human\nbeings often detect ongoing actions by contemplating the future vision\nsimultaneously. However, we observed that at different action progressions, the\noptimal supplementary features should be obtained from distinct temporal ranges\ninstead of simply fixed future temporal ranges. To this end, we introduce an\nadaptive features sampling strategy to overcome the mentioned variable-ranges\nof optimal supplementary features. Specifically, in this paper, we propose a\nnovel Learning Action Progression Network termed LAP-Net, which integrates an\nadaptive features sampling strategy. At each time step, this sampling strategy\nfirst estimates current action progression and then decide what temporal ranges\nshould be used to aggregate the optimal supplementary features. We evaluated\nour LAP-Net on three benchmark datasets, TVSeries, THUMOS-14 and HDD. The\nextensive experiments demonstrate that with our adaptive feature sampling\nstrategy, the proposed LAP-Net can significantly outperform current\nstate-of-the-art methods with a large margin.\n", "versions": [{"version": "v1", "created": "Mon, 16 Nov 2020 13:08:47 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["Qu", "Sanqing", ""], ["Chen", "Guang", ""], ["Xu", "Dan", ""], ["Dong", "Jinhu", ""], ["Lu", "Fan", ""], ["Knoll", "Alois", ""]]}, {"id": "2011.07945", "submitter": "Victor Zuanazzi", "authors": "Victor Zuanazzi", "title": "Do not trust the neighbors! Adversarial Metric Learning for\n  Self-Supervised Scene Flow Estimation", "comments": "Master Thesis", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Scene flow is the task of estimating 3D motion vectors to individual points\nof a dynamic 3D scene. Motion vectors have shown to be beneficial for\ndownstream tasks such as action classification and collision avoidance.\nHowever, data collected via LiDAR sensors and stereo cameras are computation\nand labor intensive to precisely annotate for scene flow. We address this\nannotation bottleneck on two ends. We propose a 3D scene flow benchmark and a\nnovel self-supervised setup for training flow models. The benchmark consists of\ndatasets designed to study individual aspects of flow estimation in progressive\norder of complexity, from a single object in motion to real-world scenes.\nFurthermore, we introduce Adversarial Metric Learning for self-supervised flow\nestimation. The flow model is fed with sequences of point clouds to perform\nflow estimation. A second model learns a latent metric to distinguish between\nthe points translated by the flow estimations and the target point cloud. This\nlatent metric is learned via a Multi-Scale Triplet loss, which uses\nintermediary feature vectors for the loss calculation. We use our proposed\nbenchmark to draw insights about the performance of the baselines and of\ndifferent models when trained using our setup. We find that our setup is able\nto keep motion coherence and preserve local geometries, which many\nself-supervised baselines fail to grasp. Dealing with occlusions, on the other\nhand, is still an open challenge.\n", "versions": [{"version": "v1", "created": "Sun, 1 Nov 2020 17:41:32 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["Zuanazzi", "Victor", ""]]}, {"id": "2011.07946", "submitter": "C\\'edric Beaulac", "authors": "C\\'edric Beaulac, Jeffrey S. Rosenthal", "title": "Analysis of a high-resolution hand-written digits data set with writer\n  characteristics", "comments": "Data set available here :\n  https://drive.google.com/drive/folders/1f2o1kjXLvcxRgtmMMuDkA2PQ5Zato4Or?usp=sharing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The contributions in this article are two-fold. First, we introduce a new\nhand-written digit data set that we collected. It contains high-resolution\nimages of hand-written digits together with various writer characteristics\nwhich are not available in the well-known MNIST database. The data set is\npublicly available and is designed to create new research opportunities.\nSecond, we perform a first analysis of this new data set. We begin with simple\nsupervised tasks. We assess the predictability of the writer characteristics\ngathered, the effect of using some of those characteristics as predictors in\nclassification task and the effect of higher resolution images on\nclassification accuracy. We also explore semi-supervised applications; we can\nleverage the high quantity of hand-written digits data sets already existing\nonline to improve the accuracy of various classifications task with noticeable\nsuccess. Finally, we also demonstrate the generative perspective offered by\nthis new data set; we are able to generate images that mimics the writing style\nof specific writers. The data set provides new research opportunities and our\nanalysis establishes benchmarks and showcases some of the new opportunities\nmade possible with this new data set.\n", "versions": [{"version": "v1", "created": "Wed, 4 Nov 2020 18:18:43 GMT"}, {"version": "v2", "created": "Wed, 12 May 2021 18:37:41 GMT"}], "update_date": "2021-05-14", "authors_parsed": [["Beaulac", "C\u00e9dric", ""], ["Rosenthal", "Jeffrey S.", ""]]}, {"id": "2011.07948", "submitter": "Jos\\'e Solomon", "authors": "Jose Solomon and Francois Charette", "title": "A Follow-the-Leader Strategy using Hierarchical Deep Neural Networks\n  with Grouped Convolutions", "comments": "11 pages, 7 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The task of following-the-leader is implemented using a hierarchical Deep\nNeural Network (DNN) end-to-end driving model to match the direction and speed\nof a target pedestrian. The model uses a classifier DNN to determine if the\npedestrian is within the field of view of the camera sensor. If the pedestrian\nis present, the image stream from the camera is fed to a regression DNN which\nsimultaneously adjusts the autonomous vehicle's steering and throttle to keep\ncadence with the pedestrian. If the pedestrian is not visible, the vehicle uses\na straightforward exploratory search strategy to reacquire the tracking\nobjective. The classifier and regression DNNs incorporate grouped convolutions\nto boost model performance as well as to significantly reduce parameter count\nand compute latency. The models are trained on the Intelligence Processing Unit\n(IPU) to leverage its fine-grain compute capabilities in order to minimize\ntime-to-train. The results indicate very robust tracking behavior on the part\nof the autonomous vehicle in terms of its steering and throttle profiles, while\nrequiring minimal data collection to produce. The throughput in terms of\nprocessing training samples has been boosted by the use of the IPU in\nconjunction with grouped convolutions by a factor ~3.5 for training of the\nclassifier and a factor of ~7 for the regression network. A recording of the\nvehicle tracking a pedestrian has been produced and is available on the web.\nThis is a preprint of an article published in SN Computer Science. The final\nauthenticated version is available online at:\nhttps://doi.org/https://doi.org/10.1007/s42979-021-00572-1.\n", "versions": [{"version": "v1", "created": "Wed, 4 Nov 2020 16:04:42 GMT"}, {"version": "v2", "created": "Fri, 4 Dec 2020 18:48:05 GMT"}, {"version": "v3", "created": "Sat, 17 Apr 2021 17:21:00 GMT"}, {"version": "v4", "created": "Wed, 28 Apr 2021 18:43:50 GMT"}], "update_date": "2021-04-30", "authors_parsed": [["Solomon", "Jose", ""], ["Charette", "Francois", ""]]}, {"id": "2011.07949", "submitter": "Mingkui Tan", "authors": "Peihao Chen and Deng Huang and Dongliang He and Xiang Long and Runhao\n  Zeng and Shilei Wen and Mingkui Tan and Chuang Gan", "title": "RSPNet: Relative Speed Perception for Unsupervised Video Representation\n  Learning", "comments": "Accepted by AAAI-2021. Code and pre-trained models can be found at\n  https://github.com/PeihaoChen/RSPNet", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study unsupervised video representation learning that seeks to learn both\nmotion and appearance features from unlabeled video only, which can be reused\nfor downstream tasks such as action recognition. This task, however, is\nextremely challenging due to 1) the highly complex spatial-temporal information\nin videos; and 2) the lack of labeled data for training. Unlike the\nrepresentation learning for static images, it is difficult to construct a\nsuitable self-supervised task to well model both motion and appearance\nfeatures. More recently, several attempts have been made to learn video\nrepresentation through video playback speed prediction. However, it is\nnon-trivial to obtain precise speed labels for the videos. More critically, the\nlearnt models may tend to focus on motion pattern and thus may not learn\nappearance features well. In this paper, we observe that the relative playback\nspeed is more consistent with motion pattern, and thus provide more effective\nand stable supervision for representation learning. Therefore, we propose a new\nway to perceive the playback speed and exploit the relative speed between two\nvideo clips as labels. In this way, we are able to well perceive speed and\nlearn better motion features. Moreover, to ensure the learning of appearance\nfeatures, we further propose an appearance-focused task, where we enforce the\nmodel to perceive the appearance difference between two video clips. We show\nthat optimizing the two tasks jointly consistently improves the performance on\ntwo downstream tasks, namely action recognition and video retrieval.\nRemarkably, for action recognition on UCF101 dataset, we achieve 93.7% accuracy\nwithout the use of labeled data for pre-training, which outperforms the\nImageNet supervised pre-trained model. Code and pre-trained models can be found\nat https://github.com/PeihaoChen/RSPNet.\n", "versions": [{"version": "v1", "created": "Tue, 27 Oct 2020 16:42:50 GMT"}, {"version": "v2", "created": "Mon, 15 Mar 2021 10:52:53 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Chen", "Peihao", ""], ["Huang", "Deng", ""], ["He", "Dongliang", ""], ["Long", "Xiang", ""], ["Zeng", "Runhao", ""], ["Wen", "Shilei", ""], ["Tan", "Mingkui", ""], ["Gan", "Chuang", ""]]}, {"id": "2011.07950", "submitter": "Domonkos Varga", "authors": "Domonkos Varga", "title": "Comprehensive evaluation of no-reference image quality assessment\n  algorithms on authentic distortions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Objective image quality assessment deals with the prediction of digital\nimages' perceptual quality. No-reference image quality assessment predicts the\nquality of a given input image without any knowledge or information about its\npristine (distortion free) counterpart. Machine learning algorithms are heavily\nused in no-reference image quality assessment because it is very complicated to\nmodel the human visual system's quality perception. Moreover, no-reference\nimage quality assessment algorithms are evaluated on publicly available\nbenchmark databases. These databases contain images with their corresponding\nquality scores. In this study, we evaluate several machine learning based\nNR-IQA methods and one opinion unaware method on databases consisting of\nauthentic distortions. Specifically, LIVE In the Wild and KonIQ-10k databases\nwere applied to evaluate the state-of-the-art. For machine learning based\nmethods, appx. 80% were used for training and the remaining 20% were used for\ntesting. Furthermore, average PLCC, SROCC, and KROCC values were reported over\n100 random train-test splits. The statistics of PLCC, SROCC, and KROCC values\nwere also published using boxplots. Our evaluation results may be helpful to\nobtain a clear understanding about the status of state-of-the-art no-reference\nimage quality assessment methods.\n", "versions": [{"version": "v1", "created": "Mon, 26 Oct 2020 21:25:46 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["Varga", "Domonkos", ""]]}, {"id": "2011.07953", "submitter": "Richard Savery", "authors": "Richard Savery, Gil Weinberg", "title": "Shimon the Robot Film Composer and DeepScore: An LSTM for Generation of\n  Film Scores based on Visual Analysis", "comments": "Computer Simulation of Musical Creativity, 20th-22nd August,\n  University College Dublin", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Composing for a film requires developing an understanding of the film, its\ncharacters and the film aesthetic choices made by the director. We propose\nusing existing visual analysis systems as a core technology for film music\ngeneration. We extract film features including main characters and their\nemotions to develop a computer understanding of the film's narrative arc. This\narc is combined with visually analyzed director aesthetic choices including\npacing and levels of movement. Two systems are presented, the first using a\nrobotic film composer and marimbist to generate film scores in real-time\nperformance. The second software-based system builds on the results from the\nrobot film composer to create narrative driven film scores.\n", "versions": [{"version": "v1", "created": "Mon, 26 Oct 2020 19:41:47 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["Savery", "Richard", ""], ["Weinberg", "Gil", ""]]}, {"id": "2011.07954", "submitter": "Levi Kassel", "authors": "Levi Kassel, Michael Werman", "title": "Using a Supervised Method without supervision for foreground\n  segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural networks are a powerful framework for foreground segmentation in video\nacquired by static cameras, segmenting moving objects from the background in a\nrobust way in various challenging scenarios. The premier methods are those\nbased on supervision requiring a final training stage on a database of tens to\nhundreds of manually segmented images from the specific static camera. In this\nwork, we propose a method to automatically create an \"artificial\" database that\nis sufficient for training the supervised methods so that it performs better\nthan current unsupervised methods. It is based on combining a weak foreground\nsegmenter, compared to the supervised method, to extract suitable objects from\nthe training images and randomly inserting these objects back into a background\nimage. Test results are shown on the test sequences in CDnet.\n", "versions": [{"version": "v1", "created": "Mon, 26 Oct 2020 16:42:37 GMT"}, {"version": "v2", "created": "Wed, 7 Apr 2021 09:41:08 GMT"}, {"version": "v3", "created": "Mon, 7 Jun 2021 13:50:02 GMT"}, {"version": "v4", "created": "Sun, 20 Jun 2021 20:26:33 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Kassel", "Levi", ""], ["Werman", "Michael", ""]]}, {"id": "2011.07995", "submitter": "Mateusz Buda", "authors": "Mateusz Buda, Ashirbani Saha, Ruth Walsh, Sujata Ghate, Nianyi Li,\n  Albert \\'Swi\\k{e}cicki, Joseph Y. Lo, Maciej A. Mazurowski", "title": "Detection of masses and architectural distortions in digital breast\n  tomosynthesis: a publicly available dataset of 5,060 patients and a deep\n  learning model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Breast cancer screening is one of the most common radiological tasks with\nover 39 million exams performed each year. While breast cancer screening has\nbeen one of the most studied medical imaging applications of artificial\nintelligence, the development and evaluation of the algorithms are hindered due\nto the lack of well-annotated large-scale publicly available datasets. This is\nparticularly an issue for digital breast tomosynthesis (DBT) which is a\nrelatively new breast cancer screening modality. We have curated and made\npublicly available a large-scale dataset of digital breast tomosynthesis\nimages. It contains 22,032 reconstructed DBT volumes belonging to 5,610 studies\nfrom 5,060 patients. This included four groups: (1) 5,129 normal studies, (2)\n280 studies where additional imaging was needed but no biopsy was performed,\n(3) 112 benign biopsied studies, and (4) 89 studies with cancer. Our dataset\nincluded masses and architectural distortions which were annotated by two\nexperienced radiologists. Additionally, we developed a single-phase deep\nlearning detection model and tested it using our dataset to serve as a baseline\nfor future research. Our model reached a sensitivity of 65% at 2 false\npositives per breast. Our large, diverse, and highly-curated dataset will\nfacilitate development and evaluation of AI algorithms for breast cancer\nscreening through providing data for training as well as common set of cases\nfor model validation. The performance of the model developed in our study shows\nthat the task remains challenging and will serve as a baseline for future model\ndevelopment.\n", "versions": [{"version": "v1", "created": "Fri, 13 Nov 2020 18:33:31 GMT"}, {"version": "v2", "created": "Sun, 20 Dec 2020 12:25:42 GMT"}, {"version": "v3", "created": "Fri, 1 Jan 2021 21:28:21 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Buda", "Mateusz", ""], ["Saha", "Ashirbani", ""], ["Walsh", "Ruth", ""], ["Ghate", "Sujata", ""], ["Li", "Nianyi", ""], ["\u015awi\u0119cicki", "Albert", ""], ["Lo", "Joseph Y.", ""], ["Mazurowski", "Maciej A.", ""]]}, {"id": "2011.08001", "submitter": "Omid Haji Maghsoudi", "authors": "Omid Haji Maghsoudi, Aimilia Gastounioti, Christopher Scott, Lauren\n  Pantalone, Fang-Fang Wu, Eric A. Cohen, Stacey Winham, Emily F. Conant,\n  Celine Vachon, Despina Kontos", "title": "Deep-LIBRA: Artificial intelligence method for robust quantification of\n  breast density with independent validation in breast cancer risk assessment", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Breast density is an important risk factor for breast cancer that also\naffects the specificity and sensitivity of screening mammography. Current\nfederal legislation mandates reporting of breast density for all women\nundergoing breast screening. Clinically, breast density is assessed visually\nusing the American College of Radiology Breast Imaging Reporting And Data\nSystem (BI-RADS) scale. Here, we introduce an artificial intelligence (AI)\nmethod to estimate breast percentage density (PD) from digital mammograms. Our\nmethod leverages deep learning (DL) using two convolutional neural network\narchitectures to accurately segment the breast area. A machine-learning\nalgorithm combining superpixel generation, texture feature analysis, and\nsupport vector machine is then applied to differentiate dense from non-dense\ntissue regions, from which PD is estimated. Our method has been trained and\nvalidated on a multi-ethnic, multi-institutional dataset of 15,661 images\n(4,437 women), and then tested on an independent dataset of 6,368 digital\nmammograms (1,702 women; cases=414) for both PD estimation and discrimination\nof breast cancer. On the independent dataset, PD estimates from Deep-LIBRA and\nan expert reader were strongly correlated (Spearman correlation coefficient =\n0.90). Moreover, Deep-LIBRA yielded a higher breast cancer discrimination\nperformance (area under the ROC curve, AUC = 0.611 [95% confidence interval\n(CI): 0.583, 0.639]) compared to four other widely-used research and commercial\nPD assessment methods (AUCs = 0.528 to 0.588). Our results suggest a strong\nagreement of PD estimates between Deep-LIBRA and gold-standard assessment by an\nexpert reader, as well as improved performance in breast cancer risk assessment\nover state-of-the-art open-source and commercial methods.\n", "versions": [{"version": "v1", "created": "Fri, 13 Nov 2020 15:21:17 GMT"}, {"version": "v2", "created": "Tue, 17 Nov 2020 20:52:48 GMT"}], "update_date": "2020-11-19", "authors_parsed": [["Maghsoudi", "Omid Haji", ""], ["Gastounioti", "Aimilia", ""], ["Scott", "Christopher", ""], ["Pantalone", "Lauren", ""], ["Wu", "Fang-Fang", ""], ["Cohen", "Eric A.", ""], ["Winham", "Stacey", ""], ["Conant", "Emily F.", ""], ["Vachon", "Celine", ""], ["Kontos", "Despina", ""]]}, {"id": "2011.08007", "submitter": "Divya Kothandaraman", "authors": "Divya Kothandaraman, Athira Nambiar, Anurag Mittal", "title": "Domain Adaptive Knowledge Distillation for Driving Scene Semantic\n  Segmentation", "comments": "Accepted for publication at Autonomous Vehicle Vision Workshop at\n  WACV 2021, 10 pages (including references)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Practical autonomous driving systems face two crucial challenges: memory\nconstraints and domain gap issues. In this paper, we present a novel approach\nto learn domain adaptive knowledge in models with limited memory, thus\nbestowing the model with the ability to deal with these issues in a\ncomprehensive manner. We term this as \"Domain Adaptive Knowledge Distillation\"\nand address the same in the context of unsupervised domain-adaptive semantic\nsegmentation by proposing a multi-level distillation strategy to effectively\ndistil knowledge at different levels. Further, we introduce a novel cross\nentropy loss that leverages pseudo labels from the teacher. These pseudo\nteacher labels play a multifaceted role towards: (i) knowledge distillation\nfrom the teacher network to the student network & (ii) serving as a proxy for\nthe ground truth for target domain images, where the problem is completely\nunsupervised. We introduce four paradigms for distilling domain adaptive\nknowledge and carry out extensive experiments and ablation studies on\nreal-to-real as well as synthetic-to-real scenarios. Our experiments\ndemonstrate the profound success of our proposed method.\n", "versions": [{"version": "v1", "created": "Tue, 3 Nov 2020 03:01:09 GMT"}, {"version": "v2", "created": "Thu, 26 Nov 2020 13:02:27 GMT"}], "update_date": "2020-11-30", "authors_parsed": [["Kothandaraman", "Divya", ""], ["Nambiar", "Athira", ""], ["Mittal", "Anurag", ""]]}, {"id": "2011.08008", "submitter": "Laura von Rueden", "authors": "Laura von Rueden, Tim Wirtz, Fabian Hueger, Jan David Schneider,\n  Christian Bauckhage", "title": "Towards Map-Based Validation of Semantic Segmentation Masks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Artificial intelligence for autonomous driving must meet strict requirements\non safety and robustness. We propose to validate machine learning models for\nself-driving vehicles not only with given ground truth labels, but also with\nadditional a-priori knowledge. In particular, we suggest to validate the\ndrivable area in semantic segmentation masks using given street map data. We\npresent first results, which indicate that prediction errors can be uncovered\nby map-based validation.\n", "versions": [{"version": "v1", "created": "Tue, 3 Nov 2020 11:07:22 GMT"}, {"version": "v2", "created": "Thu, 26 Nov 2020 15:28:58 GMT"}], "update_date": "2020-11-30", "authors_parsed": [["von Rueden", "Laura", ""], ["Wirtz", "Tim", ""], ["Hueger", "Fabian", ""], ["Schneider", "Jan David", ""], ["Bauckhage", "Christian", ""]]}, {"id": "2011.08009", "submitter": "Sek Chai", "authors": "Thu Dinh, Andrey Melnikov, Vasilios Daskalopoulos, Sek Chai", "title": "Subtensor Quantization for Mobilenets", "comments": "Embedded Vision Workshop, 16th European Conference on Computer Vision\n  (ECCV), Aug 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quantization for deep neural networks (DNN) have enabled developers to deploy\nmodels with less memory and more efficient low-power inference. However, not\nall DNN designs are friendly to quantization. For example, the popular\nMobilenet architecture has been tuned to reduce parameter size and\ncomputational latency with separable depth-wise convolutions, but not all\nquantization algorithms work well and the accuracy can suffer against its float\npoint versions. In this paper, we analyzed several root causes of quantization\nloss and proposed alternatives that do not rely on per-channel or\ntraining-aware approaches. We evaluate the image classification task on\nImageNet dataset, and our post-training quantized 8-bit inference top-1\naccuracy in within 0.7% of the floating point version.\n", "versions": [{"version": "v1", "created": "Wed, 4 Nov 2020 15:41:47 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["Dinh", "Thu", ""], ["Melnikov", "Andrey", ""], ["Daskalopoulos", "Vasilios", ""], ["Chai", "Sek", ""]]}, {"id": "2011.08010", "submitter": "Veda Sunkara", "authors": "Veda Sunkara, Matthew Purri, Bertrand Le Saux, Jennifer Adams", "title": "Street to Cloud: Improving Flood Maps With Crowdsourcing and Semantic\n  Segmentation", "comments": "5 pages, 2 figures, Tackling Climate Change with Machine Learning\n  workshop at NeurIPS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To address the mounting destruction caused by floods in climate-vulnerable\nregions, we propose Street to Cloud, a machine learning pipeline for\nincorporating crowdsourced ground truth data into the segmentation of satellite\nimagery of floods. We propose this approach as a solution to the\nlabor-intensive task of generating high-quality, hand-labeled training data,\nand demonstrate successes and failures of different plausible crowdsourcing\napproaches in our model. Street to Cloud leverages community reporting and\nmachine learning to generate novel, near-real time insights into the extent of\nfloods to be used for emergency response.\n", "versions": [{"version": "v1", "created": "Thu, 5 Nov 2020 16:36:58 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["Sunkara", "Veda", ""], ["Purri", "Matthew", ""], ["Saux", "Bertrand Le", ""], ["Adams", "Jennifer", ""]]}, {"id": "2011.08014", "submitter": "Sabrina Narimene Benassou", "authors": "Sabrina Narimene Benassou, Wuzhen Shi, Feng Jiang, Abdallah Benzine", "title": "Hierarchical Complementary Learning for Weakly Supervised Object\n  Localization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Weakly supervised object localization (WSOL) is a challenging problem which\naims to localize objects with only image-level labels. Due to the lack of\nground truth bounding boxes, class labels are mainly employed to train the\nmodel. This model generates a class activation map (CAM) which activates the\nmost discriminate features. However, the main drawback of CAM is the ability to\ndetect just a part of the object. To solve this problem, some researchers have\nremoved parts from the detected object \\cite{b1, b2, b4}, or the image\n\\cite{b3}. The aim of removing parts from image or detected parts of the object\nis to force the model to detect the other features. However, these methods\nrequire one or many hyper-parameters to erase the appropriate pixels on the\nimage, which could involve a loss of information. In contrast, this paper\nproposes a Hierarchical Complementary Learning Network method (HCLNet) that\nhelps the CNN to perform better classification and localization of objects on\nthe images. HCLNet uses a complementary map to force the network to detect the\nother parts of the object. Unlike previous works, this method does not need any\nextras hyper-parameters to generate different CAMs, as well as does not\nintroduce a big loss of information. In order to fuse these different maps, two\ndifferent fusion strategies known as the addition strategy and the l1-norm\nstrategy have been used. These strategies allowed to detect the whole object\nwhile excluding the background. Extensive experiments show that HCLNet obtains\nbetter performance than state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Mon, 16 Nov 2020 14:58:51 GMT"}], "update_date": "2020-11-24", "authors_parsed": [["Benassou", "Sabrina Narimene", ""], ["Shi", "Wuzhen", ""], ["Jiang", "Feng", ""], ["Benzine", "Abdallah", ""]]}, {"id": "2011.08018", "submitter": "Rosana El Jurdi", "authors": "Rosana El Jurdi, Caroline Petitjean, Paul Honeine, Veronika\n  Cheplygina, Fahed Abdallah", "title": "High-level Prior-based Loss Functions for Medical Image Segmentation: A\n  Survey", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Today, deep convolutional neural networks (CNNs) have demonstrated state of\nthe art performance for supervised medical image segmentation, across various\nimaging modalities and tasks. Despite early success, segmentation networks may\nstill generate anatomically aberrant segmentations, with holes or inaccuracies\nnear the object boundaries. To mitigate this effect, recent research works have\nfocused on incorporating spatial information or prior knowledge to enforce\nanatomically plausible segmentation. If the integration of prior knowledge in\nimage segmentation is not a new topic in classical optimization approaches, it\nis today an increasing trend in CNN based image segmentation, as shown by the\ngrowing literature on the topic. In this survey, we focus on high level prior,\nembedded at the loss function level. We categorize the articles according to\nthe nature of the prior: the object shape, size, topology, and the\ninter-regions constraints. We highlight strengths and limitations of current\napproaches, discuss the challenge related to the design and the integration of\nprior-based losses, and the optimization strategies, and draw future research\ndirections.\n", "versions": [{"version": "v1", "created": "Mon, 16 Nov 2020 15:12:05 GMT"}, {"version": "v2", "created": "Sun, 22 Nov 2020 22:16:52 GMT"}], "update_date": "2020-11-24", "authors_parsed": [["Jurdi", "Rosana El", ""], ["Petitjean", "Caroline", ""], ["Honeine", "Paul", ""], ["Cheplygina", "Veronika", ""], ["Abdallah", "Fahed", ""]]}, {"id": "2011.08019", "submitter": "Anjith George", "authors": "Anjith George and Sebastien Marcel", "title": "On the Effectiveness of Vision Transformers for Zero-shot Face\n  Anti-Spoofing", "comments": "8 pages, 3 figures, Accepted for Publication in IJCB2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The vulnerability of face recognition systems to presentation attacks has\nlimited their application in security-critical scenarios. Automatic methods of\ndetecting such malicious attempts are essential for the safe use of facial\nrecognition technology. Although various methods have been suggested for\ndetecting such attacks, most of them over-fit the training set and fail in\ngeneralizing to unseen attacks and environments. In this work, we use transfer\nlearning from the vision transformer model for the zero-shot anti-spoofing\ntask. The effectiveness of the proposed approach is demonstrated through\nexperiments in publicly available datasets. The proposed approach outperforms\nthe state-of-the-art methods in the zero-shot protocols in the HQ-WMCA and\nSiW-M datasets by a large margin. Besides, the model achieves a significant\nboost in cross-database performance as well.\n", "versions": [{"version": "v1", "created": "Mon, 16 Nov 2020 15:14:59 GMT"}, {"version": "v2", "created": "Wed, 2 Jun 2021 10:37:30 GMT"}], "update_date": "2021-06-03", "authors_parsed": [["George", "Anjith", ""], ["Marcel", "Sebastien", ""]]}, {"id": "2011.08026", "submitter": "Tristan Aumentado-Armstrong", "authors": "Tristan Aumentado-Armstrong, Alex Levinshtein, Stavros Tsogkas,\n  Konstantinos G. Derpanis, and Allan D. Jepson", "title": "Cycle-Consistent Generative Rendering for 2D-3D Modality Translation", "comments": "3DV 2020 (oral). Project page: https://ttaa9.github.io/genren/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  For humans, visual understanding is inherently generative: given a 3D shape,\nwe can postulate how it would look in the world; given a 2D image, we can infer\nthe 3D structure that likely gave rise to it. We can thus translate between the\n2D visual and 3D structural modalities of a given object. In the context of\ncomputer vision, this corresponds to a learnable module that serves two\npurposes: (i) generate a realistic rendering of a 3D object (shape-to-image\ntranslation) and (ii) infer a realistic 3D shape from an image (image-to-shape\ntranslation). In this paper, we learn such a module while being conscious of\nthe difficulties in obtaining large paired 2D-3D datasets. By leveraging\ngenerative domain translation methods, we are able to define a learning\nalgorithm that requires only weak supervision, with unpaired data. The\nresulting model is not only able to perform 3D shape, pose, and texture\ninference from 2D images, but can also generate novel textured 3D shapes and\nrenders, similar to a graphics pipeline. More specifically, our method (i)\ninfers an explicit 3D mesh representation, (ii) utilizes example shapes to\nregularize inference, (iii) requires only an image mask (no keypoints or camera\nextrinsics), and (iv) has generative capabilities. While prior work explores\nsubsets of these properties, their combination is novel. We demonstrate the\nutility of our learned representation, as well as its performance on image\ngeneration and unpaired 3D shape inference tasks.\n", "versions": [{"version": "v1", "created": "Mon, 16 Nov 2020 15:23:03 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["Aumentado-Armstrong", "Tristan", ""], ["Levinshtein", "Alex", ""], ["Tsogkas", "Stavros", ""], ["Derpanis", "Konstantinos G.", ""], ["Jepson", "Allan D.", ""]]}, {"id": "2011.08036", "submitter": "Alexey Bochkovskiy", "authors": "Chien-Yao Wang, Alexey Bochkovskiy, Hong-Yuan Mark Liao", "title": "Scaled-YOLOv4: Scaling Cross Stage Partial Network", "comments": "Added references. Corrected typos. The new results are slightly\n  better", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that the YOLOv4 object detection neural network based on the CSP\napproach, scales both up and down and is applicable to small and large networks\nwhile maintaining optimal speed and accuracy. We propose a network scaling\napproach that modifies not only the depth, width, resolution, but also\nstructure of the network. YOLOv4-large model achieves state-of-the-art results:\n55.5% AP (73.4% AP50) for the MS COCO dataset at a speed of ~16 FPS on Tesla\nV100, while with the test time augmentation, YOLOv4-large achieves 56.0% AP\n(73.3 AP50). To the best of our knowledge, this is currently the highest\naccuracy on the COCO dataset among any published work. The YOLOv4-tiny model\nachieves 22.0% AP (42.0% AP50) at a speed of 443 FPS on RTX 2080Ti, while by\nusing TensorRT, batch size = 4 and FP16-precision the YOLOv4-tiny achieves 1774\nFPS.\n", "versions": [{"version": "v1", "created": "Mon, 16 Nov 2020 15:42:00 GMT"}, {"version": "v2", "created": "Mon, 22 Feb 2021 01:32:18 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["Wang", "Chien-Yao", ""], ["Bochkovskiy", "Alexey", ""], ["Liao", "Hong-Yuan Mark", ""]]}, {"id": "2011.08061", "submitter": "Young-Keun Kim", "authors": "Seontaek Oh, Ji-Hwan You, Young-Keun Kim", "title": "FRDet: Balanced and Lightweight Object Detector based on Fire-Residual\n  Modules for Embedded Processor of Autonomous Driving", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  For deployment on an embedded processor for autonomous driving, the object\ndetection network should satisfy all of the accuracy, real-time inference, and\nlight model size requirements. Conventional deep CNN-based detectors aim for\nhigh accuracy, making their model size heavy for an embedded system with\nlimited memory space. In contrast, lightweight object detectors are greatly\ncompressed but at a significant sacrifice of accuracy. Therefore, we propose\nFRDet, a lightweight one-stage object detector that is balanced to satisfy all\nthe constraints of accuracy, model size, and real-time processing on an\nembedded GPU processor for autonomous driving applications. Our network aims to\nmaximize the compression of the model while achieving or surpassing YOLOv3\nlevel of accuracy. This paper proposes the Fire-Residual (FR) module to design\na lightweight network with low accuracy loss by adapting fire modules with\nresidual skip connections. In addition, the Gaussian uncertainty modeling of\nthe bounding box is applied to further enhance the localization accuracy.\nExperiments on the KITTI dataset showed that FRDet reduced the memory size by\n50.8% but achieved higher accuracy by 1.12% mAP compared to YOLOv3. Moreover,\nthe real-time detection speed reached 31.3 FPS on an embedded GPU board(NVIDIA\nXavier). The proposed network achieved higher compression with comparable\naccuracy compared to other deep CNN object detectors while showing improved\naccuracy than the lightweight detector baselines. Therefore, the proposed FRDet\nis a well-balanced and efficient object detector for practical application in\nautonomous driving that can satisfies all the criteria of accuracy, real-time\ninference, and light model size.\n", "versions": [{"version": "v1", "created": "Mon, 16 Nov 2020 16:15:43 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["Oh", "Seontaek", ""], ["You", "Ji-Hwan", ""], ["Kim", "Young-Keun", ""]]}, {"id": "2011.08062", "submitter": "Tim Prangemeier", "authors": "Tim Prangemeier, Christian Wildner, Andr\\'e O. Fran\\c{c}ani, Christoph\n  Reich, Heinz Koeppl", "title": "Multiclass Yeast Segmentation in Microstructured Environments with Deep\n  Learning", "comments": "IEEE CIBCB 2020 (accepted)", "journal-ref": null, "doi": "10.1109/CIBCB48159.2020.9277693", "report-no": null, "categories": "q-bio.QM cs.CV eess.IV physics.ins-det", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cell segmentation is a major bottleneck in extracting quantitative\nsingle-cell information from microscopy data. The challenge is exasperated in\nthe setting of microstructured environments. While deep learning approaches\nhave proven useful for general cell segmentation tasks, existing segmentation\ntools for the yeast-microstructure setting rely on traditional machine learning\napproaches. Here we present convolutional neural networks trained for\nmulticlass segmenting of individual yeast cells and discerning these from\ncell-similar microstructures. We give an overview of the datasets recorded for\ntraining, validating and testing the networks, as well as a typical use-case.\nWe showcase the method's contribution to segmenting yeast in microstructured\nenvironments with a typical synthetic biology application in mind. The models\nachieve robust segmentation results, outperforming the previous\nstate-of-the-art in both accuracy and speed. The combination of fast and\naccurate segmentation is not only beneficial for a posteriori data processing,\nit also makes online monitoring of thousands of trapped cells or closed-loop\noptimal experimental design feasible from an image processing perspective.\n", "versions": [{"version": "v1", "created": "Mon, 16 Nov 2020 16:16:13 GMT"}, {"version": "v2", "created": "Thu, 19 Nov 2020 14:28:20 GMT"}], "update_date": "2021-01-08", "authors_parsed": [["Prangemeier", "Tim", ""], ["Wildner", "Christian", ""], ["Fran\u00e7ani", "Andr\u00e9 O.", ""], ["Reich", "Christoph", ""], ["Koeppl", "Heinz", ""]]}, {"id": "2011.08065", "submitter": "Debesh Jha", "authors": "Debesh Jha, Sharib Ali, Krister Emanuelsen, Steven A. Hicks,\n  VajiraThambawita, Enrique Garcia-Ceja, Michael A. Riegler, Thomas de Lange,\n  Peter T. Schmidt, H{\\aa}vard D. Johansen, Dag Johansen, and P{\\aa}l Halvorsen", "title": "Kvasir-Instrument: Diagnostic and therapeutic tool segmentation dataset\n  in gastrointestinal endoscopy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.med-ph cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gastrointestinal (GI) pathologies are periodically screened, biopsied, and\nresected using surgical tools. Usually the procedures and the treated or\nresected areas are not specifically tracked or analysed during or after\ncolonoscopies. Information regarding disease borders, development and amount\nand size of the resected area get lost. This can lead to poor follow-up and\nbothersome reassessment difficulties post-treatment. To improve the current\nstandard and also to foster more research on the topic we have released the\n``Kvasir-Instrument'' dataset which consists of $590$ annotated frames\ncontaining GI procedure tools such as snares, balloons and biopsy forceps, etc.\nBeside of the images, the dataset includes ground truth masks and bounding\nboxes and has been verified by two expert GI endoscopists. Additionally, we\nprovide a baseline for the segmentation of the GI tools to promote research and\nalgorithm development. We obtained a dice coefficient score of 0.9158 and a\nJaccard index of 0.8578 using a classical U-Net architecture. A similar dice\ncoefficient score was observed for DoubleUNet. The qualitative results showed\nthat the model did not work for the images with specularity and the frames with\nmultiple instruments, while the best result for both methods was observed on\nall other types of images. Both, qualitative and quantitative results show that\nthe model performs reasonably good, but there is a large potential for further\nimprovements. Benchmarking using the dataset provides an opportunity for\nresearchers to contribute to the field of automatic endoscopic diagnostic and\ntherapeutic tool segmentation for GI endoscopy.\n", "versions": [{"version": "v1", "created": "Fri, 23 Oct 2020 18:14:36 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["Jha", "Debesh", ""], ["Ali", "Sharib", ""], ["Emanuelsen", "Krister", ""], ["Hicks", "Steven A.", ""], ["VajiraThambawita", "", ""], ["Garcia-Ceja", "Enrique", ""], ["Riegler", "Michael A.", ""], ["de Lange", "Thomas", ""], ["Schmidt", "Peter T.", ""], ["Johansen", "H\u00e5vard D.", ""], ["Johansen", "Dag", ""], ["Halvorsen", "P\u00e5l", ""]]}, {"id": "2011.08068", "submitter": "Kasyap Chakravadhanula", "authors": "Kasyap Chakravadhanula", "title": "Smartphone-Based Test and Predictive Models for Rapid, Non-Invasive, and\n  Point-of-Care Monitoring of Ocular and Cardiovascular Complications Related\n  to Diabetes", "comments": "25 Pages, 12 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.med-ph cs.CV cs.LG eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Among the most impactful diabetic complications are diabetic retinopathy, the\nleading cause of blindness among working class adults, and cardiovascular\ndisease, the leading cause of death worldwide. This study describes the\ndevelopment of improved machine learning based screening of these conditions.\nFirst, a random forest model was developed by retrospectively analyzing the\ninfluence of various risk factors (obtained quickly and non-invasively) on\ncardiovascular risk. Next, a deep-learning model was developed for prediction\nof diabetic retinopathy from retinal fundus images by a modified and re-trained\nInceptionV3 image classification model. The input was simplified by\nautomatically segmenting the blood vessels in the retinal image. The technique\nof transfer learning enables the model to capitalize on existing infrastructure\non the target device, meaning more versatile deployment, especially helpful in\nlow-resource settings. The models were integrated into a smartphone-based\ndevice, combined with an inexpensive 3D-printed retinal imaging attachment.\nAccuracy scores, as well as the receiver operating characteristic curve, the\nlearning curve, and other gauges, were promising. This test is much cheaper and\nfaster, enabling continuous monitoring for two damaging complications of\ndiabetes. It has the potential to replace the manual methods of diagnosing both\ndiabetic retinopathy and cardiovascular risk, which are time consuming and\ncostly processes only done by medical professionals away from the point of\ncare, and to prevent irreversible blindness and heart-related complications\nthrough faster, cheaper, and safer monitoring of diabetic complications. As\nwell, tracking of cardiovascular and ocular complications of diabetes can\nenable improved detection of other diabetic complications, leading to earlier\nand more efficient treatment on a global scale.\n", "versions": [{"version": "v1", "created": "Sun, 25 Oct 2020 00:57:35 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["Chakravadhanula", "Kasyap", ""]]}, {"id": "2011.08076", "submitter": "Sebastian Niehaus", "authors": "Nastassya Horlava, Alisa Mironenko, Sebastian Niehaus, Sebastian\n  Wagner, Ingo Roeder, Nico Scherf", "title": "A comparative study of semi- and self-supervised semantic segmentation\n  of biomedical microscopy data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, Convolutional Neural Networks (CNNs) have become the\nstate-of-the-art method for biomedical image analysis. However, these networks\nare usually trained in a supervised manner, requiring large amounts of labelled\ntraining data. These labelled data sets are often difficult to acquire in the\nbiomedical domain. In this work, we validate alternative ways to train CNNs\nwith fewer labels for biomedical image segmentation using. We adapt two semi-\nand self-supervised image classification methods and analyse their performance\nfor semantic segmentation of biomedical microscopy images.\n", "versions": [{"version": "v1", "created": "Wed, 11 Nov 2020 20:57:10 GMT"}, {"version": "v2", "created": "Mon, 23 Nov 2020 13:03:10 GMT"}], "update_date": "2020-11-24", "authors_parsed": [["Horlava", "Nastassya", ""], ["Mironenko", "Alisa", ""], ["Niehaus", "Sebastian", ""], ["Wagner", "Sebastian", ""], ["Roeder", "Ingo", ""], ["Scherf", "Nico", ""]]}, {"id": "2011.08102", "submitter": "Fabio Carrara PhD", "authors": "Fabio Carrara (1), Giuseppe Amato (1), Luca Brombin, Fabrizio Falchi\n  (1), Claudio Gennaro (1) ((1) ISTI CNR, Pisa, Italy)", "title": "Combining GANs and AutoEncoders for Efficient Anomaly Detection", "comments": "8 pages, 5 figures, 3 tables, pre-print, to be published in the\n  proceedings of the 25th International Conference on Pattern Recognition\n  (ICPR2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we propose CBiGAN -- a novel method for anomaly detection in\nimages, where a consistency constraint is introduced as a regularization term\nin both the encoder and decoder of a BiGAN. Our model exhibits fairly good\nmodeling power and reconstruction consistency capability. We evaluate the\nproposed method on MVTec AD -- a real-world benchmark for unsupervised anomaly\ndetection on high-resolution images -- and compare against standard baselines\nand state-of-the-art approaches. Experiments show that the proposed method\nimproves the performance of BiGAN formulations by a large margin and performs\ncomparably to expensive state-of-the-art iterative methods while reducing the\ncomputational cost. We also observe that our model is particularly effective in\ntexture-type anomaly detection, as it sets a new state of the art in this\ncategory. Our code is available at https://github.com/fabiocarrara/cbigan-ad/.\n", "versions": [{"version": "v1", "created": "Mon, 16 Nov 2020 17:07:55 GMT"}, {"version": "v2", "created": "Thu, 26 Nov 2020 16:09:50 GMT"}], "update_date": "2020-11-30", "authors_parsed": [["Carrara", "Fabio", "", "ISTI CNR, Pisa, Italy"], ["Amato", "Giuseppe", "", "ISTI CNR, Pisa, Italy"], ["Brombin", "Luca", "", "ISTI CNR, Pisa, Italy"], ["Falchi", "Fabrizio", "", "ISTI CNR, Pisa, Italy"], ["Gennaro", "Claudio", "", "ISTI CNR, Pisa, Italy"]]}, {"id": "2011.08106", "submitter": "Ze Yang", "authors": "Ze Yang, Siva Manivasagam, Ming Liang, Bin Yang, Wei-Chiu Ma, Raquel\n  Urtasun", "title": "Recovering and Simulating Pedestrians in the Wild", "comments": "CoRL 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Sensor simulation is a key component for testing the performance of\nself-driving vehicles and for data augmentation to better train perception\nsystems. Typical approaches rely on artists to create both 3D assets and their\nanimations to generate a new scenario. This, however, does not scale. In\ncontrast, we propose to recover the shape and motion of pedestrians from sensor\nreadings captured in the wild by a self-driving car driving around. Towards\nthis goal, we formulate the problem as energy minimization in a deep structured\nmodel that exploits human shape priors, reprojection consistency with 2D poses\nextracted from images, and a ray-caster that encourages the reconstructed mesh\nto agree with the LiDAR readings. Importantly, we do not require any\nground-truth 3D scans or 3D pose annotations. We then incorporate the\nreconstructed pedestrian assets bank in a realistic LiDAR simulation system by\nperforming motion retargeting, and show that the simulated LiDAR data can be\nused to significantly reduce the amount of annotated real-world data required\nfor visual perception tasks.\n", "versions": [{"version": "v1", "created": "Mon, 16 Nov 2020 17:16:32 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["Yang", "Ze", ""], ["Manivasagam", "Siva", ""], ["Liang", "Ming", ""], ["Yang", "Bin", ""], ["Ma", "Wei-Chiu", ""], ["Urtasun", "Raquel", ""]]}, {"id": "2011.08114", "submitter": "Zhengxia Zou", "authors": "Zhengxia Zou (1), Tianyang Shi (2), Shuang Qiu (1), Yi Yuan (2),\n  Zhenwei Shi (3) ((1) University of Michigan, Ann Arbor, (2) NetEase Fuxi AI\n  Lab, (3) Beihang University)", "title": "Stylized Neural Painting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper proposes an image-to-painting translation method that generates\nvivid and realistic painting artworks with controllable styles. Different from\nprevious image-to-image translation methods that formulate the translation as\npixel-wise prediction, we deal with such an artistic creation process in a\nvectorized environment and produce a sequence of physically meaningful stroke\nparameters that can be further used for rendering. Since a typical vector\nrender is not differentiable, we design a novel neural renderer which imitates\nthe behavior of the vector renderer and then frame the stroke prediction as a\nparameter searching process that maximizes the similarity between the input and\nthe rendering output. We explored the zero-gradient problem on parameter\nsearching and propose to solve this problem from an optimal transportation\nperspective. We also show that previous neural renderers have a parameter\ncoupling problem and we re-design the rendering network with a rasterization\nnetwork and a shading network that better handles the disentanglement of shape\nand color. Experiments show that the paintings generated by our method have a\nhigh degree of fidelity in both global appearance and local textures. Our\nmethod can be also jointly optimized with neural style transfer that further\ntransfers visual style from other images. Our code and animated results are\navailable at \\url{https://jiupinjia.github.io/neuralpainter/}.\n", "versions": [{"version": "v1", "created": "Mon, 16 Nov 2020 17:24:21 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["Zou", "Zhengxia", ""], ["Shi", "Tianyang", ""], ["Qiu", "Shuang", ""], ["Yuan", "Yi", ""], ["Shi", "Zhenwei", ""]]}, {"id": "2011.08129", "submitter": "Mou-Cheng Xu", "authors": "Mou-Cheng Xu", "title": "Tissue characterization based on the analysis on i3DUS data for\n  diagnosis support in neurosurgery", "comments": "MRes thesis", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.AI cs.CV cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Brain shift makes the pre-operative MRI navigation highly inaccurate hence\nthe intraoperative modalities are adopted in surgical theatre. Due to the\nexcellent economic and portability merits, the Ultrasound imaging is used at\nour collaborating hospital, Charing Cross Hospital, Imperial College London,\nUK. However, it is found that intraoperative diagnosis on Ultrasound images is\nnot straightforward and consistent, even for very experienced clinical experts.\nHence, there is a demand to design a Computer-aided-diagnosis system to provide\na robust second opinion to help the surgeons. The proposed CAD system based on\n\"Mixed-Attention Res-U-net with asymmetric loss function\" achieves the\nstate-of-the-art results comparing to the ground truth by classification at\npixel-level directly, it also outperforms all the current main stream\npixel-level classification methods (e.g. U-net, FCN) in all the evaluation\nmetrices.\n", "versions": [{"version": "v1", "created": "Sat, 24 Oct 2020 10:44:49 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["Xu", "Mou-Cheng", ""]]}, {"id": "2011.08144", "submitter": "Arwen Bradley", "authors": "Arwen Bradley, Jason Klivington, Joseph Triscari, Rudolph van der\n  Merwe", "title": "Cinematic-L1 Video Stabilization with a Log-Homography Model", "comments": "8 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a method for stabilizing handheld video that simulates the camera\nmotions cinematographers achieve with equipment like tripods, dollies, and\nSteadicams. We formulate a constrained convex optimization problem minimizing\nthe $\\ell_1$-norm of the first three derivatives of the stabilized motion. Our\napproach extends the work of Grundmann et al. [9] by solving with full\nhomographies (rather than affinities) in order to correct perspective,\npreserving linearity by working in log-homography space. We also construct crop\nconstraints that preserve field-of-view; model the problem as a quadratic\n(rather than linear) program to allow for an $\\ell_2$ term encouraging fidelity\nto the original trajectory; and add constraints and objectives to reduce\ndistortion. Furthermore, we propose new methods for handling salient objects\nvia both inclusion constraints and centering objectives. Finally, we describe a\nwindowing strategy to approximate the solution in linear time and bounded\nmemory. Our method is computationally efficient, running at 300fps on an iPhone\nXS, and yields high-quality results, as we demonstrate with a collection of\nstabilized videos, quantitative and qualitative comparisons to [9] and other\nmethods, and an ablation study.\n", "versions": [{"version": "v1", "created": "Mon, 16 Nov 2020 18:10:57 GMT"}, {"version": "v2", "created": "Fri, 20 Nov 2020 19:14:09 GMT"}], "update_date": "2020-11-24", "authors_parsed": [["Bradley", "Arwen", ""], ["Klivington", "Jason", ""], ["Triscari", "Joseph", ""], ["van der Merwe", "Rudolph", ""]]}, {"id": "2011.08145", "submitter": "Quanming Yao", "authors": "Hui Zhang, Quanming Yao", "title": "Decoupling Representation and Classifier for Noisy Label Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Since convolutional neural networks (ConvNets) can easily memorize noisy\nlabels, which are ubiquitous in visual classification tasks, it has been a\ngreat challenge to train ConvNets against them robustly. Various solutions,\ne.g., sample selection, label correction, and robustifying loss functions, have\nbeen proposed for this challenge, and most of them stick to the end-to-end\ntraining of the representation (feature extractor) and classifier. In this\npaper, by a deep rethinking and careful re-examining on learning behaviors of\nthe representation and classifier, we discover that the representation is much\nmore fragile in the presence of noisy labels than the classifier. Thus, we are\nmotivated to design a new method, i.e., REED, to leverage above discoveries to\nlearn from noisy labels robustly. The proposed method contains three stages,\ni.e., obtaining the representation by self-supervised learning without any\nlabels, transferring the noisy label learning problem into a semisupervised one\nby the classifier directly and reliably trained with noisy labels, and joint\nsemi-supervised retraining of both the representation and classifier. Extensive\nexperiments are performed on both synthetic and real benchmark datasets.\nResults demonstrate that the proposed method can beat the state-of-the-art ones\nby a large margin, especially under high noise level.\n", "versions": [{"version": "v1", "created": "Mon, 16 Nov 2020 18:13:41 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["Zhang", "Hui", ""], ["Yao", "Quanming", ""]]}, {"id": "2011.08184", "submitter": "Jan Egger", "authors": "Jan Egger, Antonio Pepe, Christina Gsaxner, Jianning Li", "title": "Deep Learning -- A first Meta-Survey of selected Reviews across\n  Scientific Disciplines and their Research Impact", "comments": "39 pages, 5 tables, 80 references", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning belongs to the field of artificial intelligence, where machines\nperform tasks that typically require some kind of human intelligence. Deep\nlearning tries to achieve this by mimicking the learning of a human brain.\nSimilar to the basic structure of a brain, which consists of (billions of)\nneurons and connections between them, a deep learning algorithm consists of an\nartificial neural network, which resembles the biological brain structure.\nMimicking the learning process of humans with their senses, deep learning\nnetworks are fed with (sensory) data, like texts, images, videos or sounds.\nThese networks outperform the state-of-the-art methods in different tasks and,\nbecause of this, the whole field saw an exponential growth during the last\nyears. This growth resulted in way over 10 000 publications per year in the\nlast years. For example, the search engine PubMed alone, which covers only a\nsub-set of all publications in the medical field, provides over 11 000 results\nfor the search term $'$deep learning$'$ in Q3 2020, and ~90% of these results\nare from the last three years. Consequently, a complete overview over the field\nof deep learning is already impossible to obtain and, in the near future, it\nwill potentially become difficult to obtain an overview over a subfield.\nHowever, there are several review articles about deep learning, which are\nfocused on specific scientific fields or applications, for example deep\nlearning advances in computer vision or in specific tasks like object\ndetection. With these surveys as a foundation, the aim of this contribution is\nto provide a first high-level, categorized meta-analysis of selected reviews on\ndeep learning across different scientific disciplines and outline the research\nimpact that they already have during a short period of time.\n", "versions": [{"version": "v1", "created": "Mon, 16 Nov 2020 13:14:18 GMT"}], "update_date": "2020-11-18", "authors_parsed": [["Egger", "Jan", ""], ["Pepe", "Antonio", ""], ["Gsaxner", "Christina", ""], ["Li", "Jianning", ""]]}, {"id": "2011.08185", "submitter": "Sahithi Ankireddy", "authors": "Sahithi Ankireddy", "title": "Assistive Diagnostic Tool for Brain Tumor Detection using Computer\n  Vision", "comments": "Accepted and presented at 2020 IEEE MIT URTC", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Today, over 700,000 people are living with brain tumors in the United States.\nBrain tumors can spread very quickly to other parts of the brain and the spinal\ncord unless necessary preventive action is taken. Thus, the survival rate for\nthis disease is less than 40% for both men and women. A conclusive and early\ndiagnosis of a brain tumor could be the difference between life and death for\nsome. However, brain tumor detection and segmentation are tedious and\ntime-consuming processes as it can only be done by radiologists and clinical\nexperts. The use of computer vision techniques, such as Mask R Convolutional\nNeural Network (Mask R CNN), to detect and segment brain tumors can mitigate\nthe possibility of human error while increasing prediction accuracy rates. The\ngoal of this project is to create an assistive diagnostics tool for brain tumor\ndetection and segmentation. Transfer learning was used with the Mask R CNN, and\nnecessary parameters were accordingly altered, as a starting point. The model\nwas trained with 20 epochs and later tested. The prediction segmentation\nmatched 90% with the ground truth. This suggests that the model was able to\nperform at a high level. Once the model was finalized, the application running\non Flask was created. The application will serve as a tool for medical\nprofessionals. It allows doctors to upload patient brain tumor MRI images in\norder to receive immediate results on the diagnosis and segmentation for each\npatient.\n", "versions": [{"version": "v1", "created": "Tue, 17 Nov 2020 04:58:33 GMT"}], "update_date": "2020-11-18", "authors_parsed": [["Ankireddy", "Sahithi", ""]]}, {"id": "2011.08261", "submitter": "M. Ali Vosoughi", "authors": "M. Ali Vosoughi, Axel Wismuller", "title": "Large-scale kernelized GRANGER causality to infer topology of directed\n  graphs with applications to brain networks", "comments": "5 pages, 3 figures, conference paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Graph topology inference of network processes with co-evolving and\ninteracting time-series is crucial for network studies. Vector autoregressive\nmodels (VAR) are popular approaches for topology inference of directed graphs;\nhowever, in large networks with short time-series, topology estimation becomes\nill-posed. The present paper proposes a novel nonlinearity-preserving topology\ninference method for directed networks with co-evolving nodal processes that\nsolves the ill-posedness problem. The proposed method, large-scale kernelized\nGranger causality (lsKGC), uses kernel functions to transform data into a\nlow-dimensional feature space and solves the autoregressive problem in the\nfeature space, then finds the pre-images in the input space to infer the\ntopology. Extensive simulations on synthetic datasets with nonlinear and linear\ndependencies and known ground-truth demonstrate significant improvement in the\nArea Under the receiver operating characteristic Curve ( AUC ) of the receiver\noperating characteristic for network recovery compared to existing methods.\nFurthermore, tests on real datasets from a functional magnetic resonance\nimaging (fMRI) study demonstrate 96.3 percent accuracy in diagnosis tasks of\nschizophrenia patients, which is the highest in the literature with only brain\ntime-series information.\n", "versions": [{"version": "v1", "created": "Mon, 16 Nov 2020 20:30:19 GMT"}], "update_date": "2020-11-18", "authors_parsed": [["Vosoughi", "M. Ali", ""], ["Wismuller", "Axel", ""]]}, {"id": "2011.08277", "submitter": "Meera Hahn", "authors": "Meera Hahn, Jacob Krantz, Dhruv Batra, Devi Parikh, James M. Rehg,\n  Stefan Lee and Peter Anderson", "title": "Where Are You? Localization from Embodied Dialog", "comments": null, "journal-ref": "EMNLP 2020", "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present Where Are You? (WAY), a dataset of ~6k dialogs in which two humans\n-- an Observer and a Locator -- complete a cooperative localization task. The\nObserver is spawned at random in a 3D environment and can navigate from\nfirst-person views while answering questions from the Locator. The Locator must\nlocalize the Observer in a detailed top-down map by asking questions and giving\ninstructions. Based on this dataset, we define three challenging tasks:\nLocalization from Embodied Dialog or LED (localizing the Observer from dialog\nhistory), Embodied Visual Dialog (modeling the Observer), and Cooperative\nLocalization (modeling both agents). In this paper, we focus on the LED task --\nproviding a strong baseline model with detailed ablations characterizing both\ndataset biases and the importance of various modeling choices. Our best model\nachieves 32.7% success at identifying the Observer's location within 3m in\nunseen buildings, vs. 70.4% for human Locators.\n", "versions": [{"version": "v1", "created": "Mon, 16 Nov 2020 21:09:43 GMT"}], "update_date": "2020-11-18", "authors_parsed": [["Hahn", "Meera", ""], ["Krantz", "Jacob", ""], ["Batra", "Dhruv", ""], ["Parikh", "Devi", ""], ["Rehg", "James M.", ""], ["Lee", "Stefan", ""], ["Anderson", "Peter", ""]]}, {"id": "2011.08306", "submitter": "Valanarasu Jeya Maria Jose", "authors": "Jeya Maria Jose Valanarasu, Vishal M. Patel", "title": "Overcomplete Deep Subspace Clustering Networks", "comments": "WACV 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Subspace Clustering Networks (DSC) provide an efficient solution to the\nproblem of unsupervised subspace clustering by using an undercomplete deep\nauto-encoder with a fully-connected layer to exploit the self expressiveness\nproperty. This method uses undercomplete representations of the input data\nwhich makes it not so robust and more dependent on pre-training. To overcome\nthis, we propose a simple yet efficient alternative method - Overcomplete Deep\nSubspace Clustering Networks (ODSC) where we use overcomplete representations\nfor subspace clustering. In our proposed method, we fuse the features from both\nundercomplete and overcomplete auto-encoder networks before passing them\nthrough the self-expressive layer thus enabling us to extract a more meaningful\nand robust representation of the input data for clustering. Experimental\nresults on four benchmark datasets show the effectiveness of the proposed\nmethod over DSC and other clustering methods in terms of clustering error. Our\nmethod is also not as dependent as DSC is on where pre-training should be\nstopped to get the best performance and is also more robust to noise. Code -\n\\href{https://github.com/jeya-maria-jose/Overcomplete-Deep-Subspace-Clustering}{https://github.com/jeya-maria-jose/Overcomplete-Deep-Subspace-Clustering\n", "versions": [{"version": "v1", "created": "Mon, 16 Nov 2020 22:07:18 GMT"}], "update_date": "2020-11-18", "authors_parsed": [["Valanarasu", "Jeya Maria Jose", ""], ["Patel", "Vishal M.", ""]]}, {"id": "2011.08317", "submitter": "Ehsan Emad Marvasti", "authors": "Ehsan Emad Marvasti, Arash Raftari, Amir Emad Marvasti, Yaser\n  P.Fallah, Rui Guo, Hongsheng Lu", "title": "Feature Sharing and Integration for Cooperative Cognition and Perception\n  with Volumetric Sensors", "comments": "12 pages, 12 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The recent advancement in computational and communication systems has led to\nthe introduction of high-performing neural networks and high-speed wireless\nvehicular communication networks. As a result, new technologies such as\ncooperative perception and cognition have emerged, addressing the inherent\nlimitations of sensory devices by providing solutions for the detection of\npartially occluded targets and expanding the sensing range. However, designing\na reliable cooperative cognition or perception system requires addressing the\nchallenges caused by limited network resources and discrepancies between the\ndata shared by different sources. In this paper, we examine the requirements,\nlimitations, and performance of different cooperative perception techniques,\nand present an in-depth analysis of the notion of Deep Feature Sharing (DFS).\nWe explore different cooperative object detection designs and evaluate their\nperformance in terms of average precision. We use the Volony dataset for our\nexperimental study. The results confirm that the DFS methods are significantly\nless sensitive to the localization error caused by GPS noise. Furthermore, the\nresults attest that detection gain of DFS methods caused by adding more\ncooperative participants in the scenes is comparable to raw information sharing\ntechnique while DFS enables flexibility in design toward satisfying\ncommunication requirements.\n", "versions": [{"version": "v1", "created": "Mon, 16 Nov 2020 22:43:44 GMT"}, {"version": "v2", "created": "Tue, 24 Nov 2020 21:18:33 GMT"}, {"version": "v3", "created": "Fri, 4 Dec 2020 17:58:14 GMT"}], "update_date": "2020-12-07", "authors_parsed": [["Marvasti", "Ehsan Emad", ""], ["Raftari", "Arash", ""], ["Marvasti", "Amir Emad", ""], ["Fallah", "Yaser P.", ""], ["Guo", "Rui", ""], ["Lu", "Hongsheng", ""]]}, {"id": "2011.08325", "submitter": "Pedro Henrique Silva Souza Barros", "authors": "Pedro H. Barros, Fabiane Queiroz, Flavio Figueredo, Jefersson A. dos\n  Santos, Heitor S. Ramos", "title": "A New Similarity Space Tailored for Supervised Deep Metric Learning", "comments": "47 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose a novel deep metric learning method. Differently from many works\non this area, we defined a novel latent space obtained through an autoencoder.\nThe new space, namely S-space, is divided into different regions that describe\nthe positions where pairs of objects are similar/dissimilar. We locate makers\nto identify these regions. We estimate the similarities between objects through\na kernel-based t-student distribution to measure the markers' distance and the\nnew data representation. In our approach, we simultaneously estimate the\nmarkers' position in the S-space and represent the objects in the same space.\nMoreover, we propose a new regularization function to avoid similar markers to\ncollapse altogether. We present evidences that our proposal can represent\ncomplex spaces, for instance, when groups of similar objects are located in\ndisjoint regions. We compare our proposal to 9 different distance metric\nlearning approaches (four of them are based on deep-learning) on 28 real-world\nheterogeneous datasets. According to the four quantitative metrics used, our\nmethod overcomes all the nine strategies from the literature.\n", "versions": [{"version": "v1", "created": "Mon, 16 Nov 2020 22:58:06 GMT"}, {"version": "v2", "created": "Wed, 18 Nov 2020 21:24:02 GMT"}], "update_date": "2020-11-20", "authors_parsed": [["Barros", "Pedro H.", ""], ["Queiroz", "Fabiane", ""], ["Figueredo", "Flavio", ""], ["Santos", "Jefersson A. dos", ""], ["Ramos", "Heitor S.", ""]]}, {"id": "2011.08332", "submitter": "Yang Jiao", "authors": "Yang Jiao, Trac D. Tran and Guangming Shi", "title": "EffiScene: Efficient Per-Pixel Rigidity Inference for Unsupervised Joint\n  Learning of Optical Flow, Depth, Camera Pose and Motion Segmentation", "comments": "Accpeted by IEEE Conf. on Computer Vision and Pattern Recognition\n  (CVPR) 2021; v1 - original submit v2 - camera ready version v3 - correct\n  small bugs in Equation(2)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the challenging unsupervised scene flow estimation\nproblem by jointly learning four low-level vision sub-tasks: optical flow\n$\\textbf{F}$, stereo-depth $\\textbf{D}$, camera pose $\\textbf{P}$ and motion\nsegmentation $\\textbf{S}$. Our key insight is that the rigidity of the scene\nshares the same inherent geometrical structure with object movements and scene\ndepth. Hence, rigidity from $\\textbf{S}$ can be inferred by jointly coupling\n$\\textbf{F}$, $\\textbf{D}$ and $\\textbf{P}$ to achieve more robust estimation.\nTo this end, we propose a novel scene flow framework named EffiScene with\nefficient joint rigidity learning, going beyond the existing pipeline with\nindependent auxiliary structures. In EffiScene, we first estimate optical flow\nand depth at the coarse level and then compute camera pose by\nPerspective-$n$-Points method. To jointly learn local rigidity, we design a\nnovel Rigidity From Motion (RfM) layer with three principal components:\n\\emph{}{(i)} correlation extraction; \\emph{}{(ii)} boundary learning; and\n\\emph{}{(iii)} outlier exclusion. Final outputs are fused based on the rigid\nmap $M_R$ from RfM at finer levels. To efficiently train EffiScene, two new\nlosses $\\mathcal{L}_{bnd}$ and $\\mathcal{L}_{unc}$ are designed to prevent\ntrivial solutions and to regularize the flow boundary discontinuity. Extensive\nexperiments on scene flow benchmark KITTI show that our method is effective and\nsignificantly improves the state-of-the-art approaches for all sub-tasks, i.e.\noptical flow ($5.19 \\rightarrow 4.20$), depth estimation ($3.78 \\rightarrow\n3.46$), visual odometry ($0.012 \\rightarrow 0.011$) and motion segmentation\n($0.57 \\rightarrow 0.62$).\n", "versions": [{"version": "v1", "created": "Mon, 16 Nov 2020 23:28:22 GMT"}, {"version": "v2", "created": "Mon, 29 Mar 2021 16:23:55 GMT"}, {"version": "v3", "created": "Sat, 15 May 2021 03:45:42 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Jiao", "Yang", ""], ["Tran", "Trac D.", ""], ["Shi", "Guangming", ""]]}, {"id": "2011.08333", "submitter": "Yang Jiao", "authors": "Yang Jiao, Yi Niu, Trac D. Tran, Guangming Shi", "title": "2D+3D Facial Expression Recognition via Discriminative Dynamic Range\n  Enhancement and Multi-Scale Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In 2D+3D facial expression recognition (FER), existing methods generate\nmulti-view geometry maps to enhance the depth feature representation. However,\nthis may introduce false estimations due to local plane fitting from incomplete\npoint clouds. In this paper, we propose a novel Map Generation technique from\nthe viewpoint of information theory, to boost the slight 3D expression\ndifferences from strong personality variations. First, we examine the HDR depth\ndata to extract the discriminative dynamic range $r_{dis}$, and maximize the\nentropy of $r_{dis}$ to a global optimum. Then, to prevent the large\ndeformation caused by over-enhancement, we introduce a depth distortion\nconstraint and reduce the complexity from $O(KN^2)$ to $O(KN\\tau)$.\nFurthermore, the constrained optimization is modeled as a $K$-edges maximum\nweight path problem in a directed acyclic graph, and we solve it efficiently\nvia dynamic programming. Finally, we also design an efficient Facial Attention\nstructure to automatically locate subtle discriminative facial parts for\nmulti-scale learning, and train it with a proposed loss function\n$\\mathcal{L}_{FA}$ without any facial landmarks. Experimental results on\ndifferent datasets show that the proposed method is effective and outperforms\nthe state-of-the-art 2D+3D FER methods in both FER accuracy and the output\nentropy of the generated maps.\n", "versions": [{"version": "v1", "created": "Mon, 16 Nov 2020 23:29:50 GMT"}], "update_date": "2020-11-18", "authors_parsed": [["Jiao", "Yang", ""], ["Niu", "Yi", ""], ["Tran", "Trac D.", ""], ["Shi", "Guangming", ""]]}, {"id": "2011.08341", "submitter": "Li Chen", "authors": "Li Chen, David Yang, Purvi Goel, Ilknur Kabul", "title": "Robust Deep Learning with Active Noise Cancellation for Spatial\n  Computing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes CANC, a Co-teaching Active Noise Cancellation method,\napplied in spatial computing to address deep learning trained with extreme\nnoisy labels. Deep learning algorithms have been successful in spatial\ncomputing for land or building footprint recognition. However a lot of noise\nexists in ground truth labels due to how labels are collected in spatial\ncomputing and satellite imagery. Existing methods to deal with extreme label\nnoise conduct clean sample selection and do not utilize the remaining samples.\nSuch techniques can be wasteful due to the cost of data retrieval. Our proposed\nCANC algorithm not only conserves high-cost training samples but also provides\nactive label correction to better improve robust deep learning with extreme\nnoisy labels. We demonstrate the effectiveness of CANC for building footprint\nrecognition for spatial computing.\n", "versions": [{"version": "v1", "created": "Mon, 16 Nov 2020 23:56:14 GMT"}], "update_date": "2020-11-18", "authors_parsed": [["Chen", "Li", ""], ["Yang", "David", ""], ["Goel", "Purvi", ""], ["Kabul", "Ilknur", ""]]}, {"id": "2011.08362", "submitter": "Yao Sun", "authors": "Yao Sun, Yuansheng Hua, Lichao Mou, Xiao Xiang Zhu", "title": "CG-Net: Conditional GIS-aware Network for Individual Building\n  Segmentation in VHR SAR Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Object retrieval and reconstruction from very high resolution (VHR) synthetic\naperture radar (SAR) images are of great importance for urban SAR applications,\nyet highly challenging owing to the complexity of SAR data. This paper\naddresses the issue of individual building segmentation from a single VHR SAR\nimage in large-scale urban areas. To achieve this, we introduce building\nfootprints from GIS data as complementary information and propose a novel\nconditional GIS-aware network (CG-Net). The proposed model learns multi-level\nvisual features and employs building footprints to normalize the features for\npredicting building masks in the SAR image. We validate our method using a high\nresolution spotlight TerraSAR-X image collected over Berlin. Experimental\nresults show that the proposed CG-Net effectively brings improvements with\nvariant backbones. We further compare two representations of building\nfootprints, namely complete building footprints and sensor-visible footprint\nsegments, for our task, and conclude that the use of the former leads to better\nsegmentation results. Moreover, we investigate the impact of inaccurate GIS\ndata on our CG-Net, and this study shows that CG-Net is robust against\npositioning errors in GIS data. In addition, we propose an approach of ground\ntruth generation of buildings from an accurate digital elevation model (DEM),\nwhich can be used to generate large-scale SAR image datasets. The segmentation\nresults can be applied to reconstruct 3D building models at level-of-detail\n(LoD) 1, which is demonstrated in our experiments.\n", "versions": [{"version": "v1", "created": "Tue, 17 Nov 2020 01:52:22 GMT"}], "update_date": "2020-11-21", "authors_parsed": [["Sun", "Yao", ""], ["Hua", "Yuansheng", ""], ["Mou", "Lichao", ""], ["Zhu", "Xiao Xiang", ""]]}, {"id": "2011.08363", "submitter": "Nasim Nematzadeh", "authors": "Nasim Nematzadeh, David MW Powers, Trent Lewis", "title": "Vis-CRF, A Classical Receptive Field Model for VISION", "comments": "This study reflects the core of the lead author's PhD research\n  (Nematzadeh, 2018) which focused on illusions in which lines of tiles or\n  squares were perceived to bend or tilt (such as Caf\\'e Wall and checkerboard\n  illusions)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Over the last decade, a variety of new neurophysiological experiments have\nled to new insights as to how, when and where retinal processing takes place,\nand the nature of the retinal representation encoding sent to the cortex for\nfurther processing. Based on these neurobiological discoveries, in our previous\nwork, we provided computer simulation evidence to suggest that Geometrical\nillusions are explained in part, by the interaction of multiscale visual\nprocessing performed in the retina. The output of our retinal stage model,\nnamed Vis-CRF, is presented here for a sample of natural image and for several\ntypes of Tilt Illusion, in which the final tilt percept arises from multiple\nscale processing of Difference of Gaussians (DoG) and the perceptual\ninteraction of foreground and background elements (Nematzadeh and Powers, 2019;\nNematzadeh, 2018; Nematzadeh, Powers and Lewis, 2017; Nematzadeh, Lewis and\nPowers, 2015).\n", "versions": [{"version": "v1", "created": "Tue, 17 Nov 2020 01:52:33 GMT"}], "update_date": "2020-11-18", "authors_parsed": [["Nematzadeh", "Nasim", ""], ["Powers", "David MW", ""], ["Lewis", "Trent", ""]]}, {"id": "2011.08367", "submitter": "Mingjie Sun", "authors": "Mingjie Sun, Jianguo Li, Changshui Zhang", "title": "Extreme Value Preserving Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recent evidence shows that convolutional neural networks (CNNs) are biased\ntowards textures so that CNNs are non-robust to adversarial perturbations over\ntextures, while traditional robust visual features like SIFT (scale-invariant\nfeature transforms) are designed to be robust across a substantial range of\naffine distortion, addition of noise, etc with the mimic of human perception\nnature. This paper aims to leverage good properties of SIFT to renovate CNN\narchitectures towards better accuracy and robustness. We borrow the scale-space\nextreme value idea from SIFT, and propose extreme value preserving networks\n(EVPNets). Experiments demonstrate that EVPNets can achieve similar or better\naccuracy than conventional CNNs, while achieving much better robustness on a\nset of adversarial attacks (FGSM,PGD,etc) even without adversarial training.\n", "versions": [{"version": "v1", "created": "Tue, 17 Nov 2020 02:06:52 GMT"}], "update_date": "2020-11-18", "authors_parsed": [["Sun", "Mingjie", ""], ["Li", "Jianguo", ""], ["Zhang", "Changshui", ""]]}, {"id": "2011.08382", "submitter": "Mingbao Lin", "authors": "Shaojie Li, Mingbao Lin, Yan Wang, Fei Chao, Xudong Mao, Mingliang Xu,\n  Yongjian Wu, Feiyue Huang, Ling Shao, Rongrong Ji", "title": "Learning Efficient GANs for Image Translation via Differentiable Masks\n  and co-Attention Distillation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative Adversarial Networks (GANs) have been widely-used in image\ntranslation, but their high computational and storage costs impede the\ndeployment on mobile devices. Prevalent methods for CNN compression cannot be\ndirectly applied to GANs due to the complicated generator architecture and the\nunstable adversarial training. To solve these, in this paper, we introduce a\nnovel GAN compression method, termed DMAD, by proposing a Differentiable Mask\nand a co-Attention Distillation. The former searches for a light-weight\ngenerator architecture in a training-adaptive manner. To overcome channel\ninconsistency when pruning the residual connections, an adaptive cross-block\ngroup sparsity is further incorporated. The latter simultaneously distills\ninformative attention maps from both the generator and discriminator of a\npre-trained model to the searched generator, effectively stabilizing the\nadversarial training of our light-weight model. Experiments show that DMAD can\nreduce the Multiply Accumulate Operations (MACs) of CycleGAN by 13$\\times$ and\nthat of Pix2Pix by 4$\\times$ while retaining a comparable performance against\nthe full model. Our code can be available at https://github.com/SJLeo/DMAD.\n", "versions": [{"version": "v1", "created": "Tue, 17 Nov 2020 02:39:19 GMT"}, {"version": "v2", "created": "Sat, 21 Nov 2020 19:28:23 GMT"}, {"version": "v3", "created": "Fri, 16 Jul 2021 01:22:06 GMT"}], "update_date": "2021-07-19", "authors_parsed": [["Li", "Shaojie", ""], ["Lin", "Mingbao", ""], ["Wang", "Yan", ""], ["Chao", "Fei", ""], ["Mao", "Xudong", ""], ["Xu", "Mingliang", ""], ["Wu", "Yongjian", ""], ["Huang", "Feiyue", ""], ["Shao", "Ling", ""], ["Ji", "Rongrong", ""]]}, {"id": "2011.08388", "submitter": "Puneet Kumar", "authors": "Puneet Kumar and Balasubramanian Raman", "title": "Domain Adaptation based Technique for Image Emotion Recognition using\n  Pre-trained Facial Expression Recognition Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, a domain adaptation based technique for recognizing the\nemotions in images containing facial, non-facial, and non-human components has\nbeen proposed. We have also proposed a novel technique to explain the proposed\nsystem's predictions in terms of Intersection Score. Image emotion recognition\nis useful for graphics, gaming, animation, entertainment, and cinematography.\nHowever, well-labeled large scale datasets and pre-trained models are not\navailable for image emotion recognition. To overcome this challenge, we have\nproposed a deep learning approach based on an attentional convolutional network\nthat adapts pre-trained facial expression recognition models. It detects the\nvisual features of an image and performs emotion classification based on them.\nThe experiments have been performed on the Flickr image dataset, and the images\nhave been classified in 'angry,' 'happy,' 'sad,' and 'neutral' emotion classes.\nThe proposed system has demonstrated better performance than the benchmark\nresults with an accuracy of 63.87% for image emotion recognition. We have also\nanalyzed the embedding plots for various emotion classes to explain the\nproposed system's predictions.\n", "versions": [{"version": "v1", "created": "Tue, 17 Nov 2020 02:55:16 GMT"}], "update_date": "2020-11-18", "authors_parsed": [["Kumar", "Puneet", ""], ["Raman", "Balasubramanian", ""]]}, {"id": "2011.08408", "submitter": "Seungkyu Lee", "authors": "Gahye Lee and Seungkyu Lee", "title": "Sub-clusters of Normal Data for Anomaly Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Anomaly detection in data analysis is an interesting but still challenging\nresearch topic in real world applications. As the complexity of data dimension\nincreases, it requires to understand the semantic contexts in its description\nfor effective anomaly characterization. However, existing anomaly detection\nmethods show limited performances with high dimensional data such as ImageNet.\nExisting studies have evaluated their performance on low dimensional, clean and\nwell separated data set such as MNIST and CIFAR-10. In this paper, we study\nanomaly detection with high dimensional and complex normal data. Our\nobservation is that, in general, anomaly data is defined by semantically\nexplainable features which are able to be used in defining semantic\nsub-clusters of normal data as well. We hypothesize that if there exists\nreasonably good feature space semantically separating sub-clusters of given\nnormal data, unseen anomaly also can be well distinguished in the space from\nthe normal data. We propose to perform semantic clustering on given normal data\nand train a classifier to learn the discriminative feature space where anomaly\ndetection is finally performed. Based on our careful and extensive experimental\nevaluations with MNIST, CIFAR-10, and ImageNet with various combinations of\nnormal and anomaly data, we show that our anomaly detection scheme outperforms\nstate of the art methods especially with high dimensional real world images.\n", "versions": [{"version": "v1", "created": "Tue, 17 Nov 2020 03:53:31 GMT"}], "update_date": "2020-11-18", "authors_parsed": [["Lee", "Gahye", ""], ["Lee", "Seungkyu", ""]]}, {"id": "2011.08410", "submitter": "Xiaoyuan Ni", "authors": "Xiaoyuan Ni, Sizhe Song, Yu-Wing Tai, Chi-Keung Tang", "title": "Semi-Supervised Few-Shot Atomic Action Recognition", "comments": "7 pages, 3 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Despite excellent progress has been made, the performance on action\nrecognition still heavily relies on specific datasets, which are difficult to\nextend new action classes due to labor-intensive labeling. Moreover, the high\ndiversity in Spatio-temporal appearance requires robust and representative\naction feature aggregation and attention. To address the above issues, we focus\non atomic actions and propose a novel model for semi-supervised few-shot atomic\naction recognition. Our model features unsupervised and contrastive video\nembedding, loose action alignment, multi-head feature comparison, and\nattention-based aggregation, together of which enables action recognition with\nonly a few training examples through extracting more representative features\nand allowing flexibility in spatial and temporal alignment and variations in\nthe action. Experiments show that our model can attain high accuracy on\nrepresentative atomic action datasets outperforming their respective\nstate-of-the-art classification accuracy in full supervision setting.\n", "versions": [{"version": "v1", "created": "Tue, 17 Nov 2020 03:59:05 GMT"}], "update_date": "2020-11-18", "authors_parsed": [["Ni", "Xiaoyuan", ""], ["Song", "Sizhe", ""], ["Tai", "Yu-Wing", ""], ["Tang", "Chi-Keung", ""]]}, {"id": "2011.08413", "submitter": "Riccardo Barbano", "authors": "Riccardo Barbano, \\v{Z}eljko Kereta, Chen Zhang, Andreas Hauptmann,\n  Simon Arridge, Bangti Jin", "title": "Quantifying Sources of Uncertainty in Deep Learning-Based Image\n  Reconstruction", "comments": null, "journal-ref": "NeurIPS 2020 Workshop on Deep Learning and Inverse Problems", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Image reconstruction methods based on deep neural networks have shown\noutstanding performance, equalling or exceeding the state-of-the-art results of\nconventional approaches, but often do not provide uncertainty information about\nthe reconstruction. In this work we propose a scalable and efficient framework\nto simultaneously quantify aleatoric and epistemic uncertainties in learned\niterative image reconstruction. We build on a Bayesian deep gradient descent\nmethod for quantifying epistemic uncertainty, and incorporate the\nheteroscedastic variance of the noise to account for the aleatoric uncertainty.\nWe show that our method exhibits competitive performance against conventional\nbenchmarks for computed tomography with both sparse view and limited angle\ndata. The estimated uncertainty captures the variability in the\nreconstructions, caused by the restricted measurement model, and by missing\ninformation, due to the limited angle geometry.\n", "versions": [{"version": "v1", "created": "Tue, 17 Nov 2020 04:12:52 GMT"}, {"version": "v2", "created": "Mon, 30 Nov 2020 03:47:56 GMT"}], "update_date": "2020-12-01", "authors_parsed": [["Barbano", "Riccardo", ""], ["Kereta", "\u017deljko", ""], ["Zhang", "Chen", ""], ["Hauptmann", "Andreas", ""], ["Arridge", "Simon", ""], ["Jin", "Bangti", ""]]}, {"id": "2011.08419", "submitter": "Hengtao Guo", "authors": "Hengtao Guo, Sheng Xu, Bradford J. Wood, Pingkun Yan", "title": "Transducer Adaptive Ultrasound Volume Reconstruction", "comments": "In submission to ISBI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Reconstructed 3D ultrasound volume provides more context information compared\nto a sequence of 2D scanning frames, which is desirable for various clinical\napplications such as ultrasound-guided prostate biopsy. Nevertheless, 3D volume\nreconstruction from freehand 2D scans is a very challenging problem, especially\nwithout the use of external tracking devices. Recent deep learning based\nmethods demonstrate the potential of directly estimating inter-frame motion\nbetween consecutive ultrasound frames. However, such algorithms are specific to\nparticular transducers and scanning trajectories associated with the training\ndata, which may not be generalized to other image acquisition settings. In this\npaper, we tackle the data acquisition difference as a domain shift problem and\npropose a novel domain adaptation strategy to adapt deep learning algorithms to\ndata acquired with different transducers. Specifically, feature extractors that\ngenerate transducer-invariant features from different datasets are trained by\nminimizing the discrepancy between deep features of paired samples in a latent\nspace. Our results show that the proposed domain adaptation method can\nsuccessfully align different feature distributions while preserving the\ntransducer-specific information for universal freehand ultrasound volume\nreconstruction.\n", "versions": [{"version": "v1", "created": "Tue, 17 Nov 2020 04:46:57 GMT"}], "update_date": "2020-11-18", "authors_parsed": [["Guo", "Hengtao", ""], ["Xu", "Sheng", ""], ["Wood", "Bradford J.", ""], ["Yan", "Pingkun", ""]]}, {"id": "2011.08435", "submitter": "Guo-Jun Qi", "authors": "Qianjiang Hu, Xiao Wang, Wei Hu, Guo-Jun Qi", "title": "AdCo: Adversarial Contrast for Efficient Learning of Unsupervised\n  Representations from Self-Trained Negative Adversaries", "comments": "Appendices with more results on symmetric loss, different numbers of\n  negative samples, computing costs is presented. We also discuss \"whether we\n  still need negative examples\" in Appendix C, a question emerging from the\n  comparison with the BYOL. The source code is also available at\n  https://github.com/maple-research-lab/AdCo/", "journal-ref": "IEEE/CVF Conference on Computer Vision and Pattern Recognition\n  (CVPR), June 19th - June 25th, 2021", "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Contrastive learning relies on constructing a collection of negative examples\nthat are sufficiently hard to discriminate against positive queries when their\nrepresentations are self-trained. Existing contrastive learning methods either\nmaintain a queue of negative samples over minibatches while only a small\nportion of them are updated in an iteration, or only use the other examples\nfrom the current minibatch as negatives. They could not closely track the\nchange of the learned representation over iterations by updating the entire\nqueue as a whole, or discard the useful information from the past minibatches.\nAlternatively, we present to directly learn a set of negative adversaries\nplaying against the self-trained representation. Two players, the\nrepresentation network and negative adversaries, are alternately updated to\nobtain the most challenging negative examples against which the representation\nof positive queries will be trained to discriminate. We further show that the\nnegative adversaries are updated towards a weighted combination of positive\nqueries by maximizing the adversarial contrastive loss, thereby allowing them\nto closely track the change of representations over time. Experiment results\ndemonstrate the proposed Adversarial Contrastive (AdCo) model not only achieves\nsuperior performances (a top-1 accuracy of 73.2\\% over 200 epochs and 75.7\\%\nover 800 epochs with linear evaluation on ImageNet), but also can be\npre-trained more efficiently with fewer epochs.\n", "versions": [{"version": "v1", "created": "Tue, 17 Nov 2020 05:45:46 GMT"}, {"version": "v2", "created": "Mon, 23 Nov 2020 23:56:08 GMT"}, {"version": "v3", "created": "Mon, 4 Jan 2021 03:01:28 GMT"}, {"version": "v4", "created": "Tue, 26 Jan 2021 08:44:44 GMT"}, {"version": "v5", "created": "Fri, 5 Mar 2021 07:01:19 GMT"}], "update_date": "2021-03-08", "authors_parsed": [["Hu", "Qianjiang", ""], ["Wang", "Xiao", ""], ["Hu", "Wei", ""], ["Qi", "Guo-Jun", ""]]}, {"id": "2011.08436", "submitter": "Chiho Choi", "authors": "Chiho Choi, Joon Hee Choi, Jiachen Li, Srikanth Malla", "title": "Shared Cross-Modal Trajectory Prediction for Autonomous Driving", "comments": "CVPR 2021 [Oral]", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.RO", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Predicting future trajectories of traffic agents in highly interactive\nenvironments is an essential and challenging problem for the safe operation of\nautonomous driving systems. On the basis of the fact that self-driving vehicles\nare equipped with various types of sensors (e.g., LiDAR scanner, RGB camera,\nradar, etc.), we propose a Cross-Modal Embedding framework that aims to benefit\nfrom the use of multiple input modalities. At training time, our model learns\nto embed a set of complementary features in a shared latent space by jointly\noptimizing the objective functions across different types of input data. At\ntest time, a single input modality (e.g., LiDAR data) is required to generate\npredictions from the input perspective (i.e., in the LiDAR space), while taking\nadvantages from the model trained with multiple sensor modalities. An extensive\nevaluation is conducted to show the efficacy of the proposed framework using\ntwo benchmark driving datasets.\n", "versions": [{"version": "v1", "created": "Sun, 15 Nov 2020 07:18:50 GMT"}, {"version": "v2", "created": "Sat, 27 Mar 2021 09:05:58 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Choi", "Chiho", ""], ["Choi", "Joon Hee", ""], ["Li", "Jiachen", ""], ["Malla", "Srikanth", ""]]}, {"id": "2011.08446", "submitter": "William McNally", "authors": "William McNally, Kanav Vats, Alexander Wong, John McPhee", "title": "EvoPose2D: Pushing the Boundaries of 2D Human Pose Estimation using\n  Neuroevolution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural architecture search has proven to be highly effective in the design of\ncomputationally efficient, task-specific convolutional neural networks across\nseveral areas of computer vision. In 2D human pose estimation, however, its\napplication has been limited by high computational demands. Hypothesizing that\nneural architecture search holds great potential for 2D human pose estimation,\nwe propose a new weight transfer scheme that relaxes function-preserving\nmutations, enabling us to accelerate neuroevolution in a flexible manner. Our\nmethod produces 2D human pose network designs that are more efficient and more\naccurate than state-of-the-art hand-designed networks. In fact, the generated\nnetworks can process images at higher resolutions using less computation than\nprevious networks at lower resolutions, permitting us to push the boundaries of\n2D human pose estimation. Our baseline network designed using neuroevolution,\nwhich we refer to as EvoPose2D-S, provides comparable accuracy to\nSimpleBaseline while using 4.9x fewer floating-point operations and 13.5x fewer\nparameters. Our largest network, EvoPose2D-L, achieves new state-of-the-art\naccuracy on the Microsoft COCO Keypoints benchmark while using 2.0x fewer\noperations and 4.3x fewer parameters than its nearest competitor.\n", "versions": [{"version": "v1", "created": "Tue, 17 Nov 2020 05:56:16 GMT"}], "update_date": "2020-11-18", "authors_parsed": [["McNally", "William", ""], ["Vats", "Kanav", ""], ["Wong", "Alexander", ""], ["McPhee", "John", ""]]}, {"id": "2011.08459", "submitter": "SeongHo Lee", "authors": "Seong-Ho Lee and Seung-Hwan Bae", "title": "SRF-GAN: Super-Resolved Feature GAN for Multi-Scale Representation", "comments": "10 pages, 2figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent convolutional object detectors exploit multi-scale feature\nrepresentations added with top-down pathway in order to detect objects at\ndifferent scales and learn stronger semantic feature responses. In general,\nduring the top-down feature propagation, the coarser feature maps are upsampled\nto be combined with the features forwarded from bottom-up pathway, and the\ncombined stronger semantic features are inputs of detector's headers. However,\nsimple interpolation methods (e.g. nearest neighbor and bilinear) are still\nused for increasing feature resolutions although they cause noisy and blurred\nfeatures. In this paper, we propose a novel generator for super-resolving\nfeatures of the convolutional object detectors. To achieve this, we first\ndesign super-resolved feature GAN (SRF-GAN) consisting of a detection-based\ngenerator and a feature patch discriminator. In addition, we present SRF-GAN\nlosses for generating the high quality of super-resolved features and improving\ndetection accuracy together. Our SRF generator can substitute for the\ntraditional interpolation methods, and easily fine-tuned combined with other\nconventional detectors. To prove this, we have implemented our SRF-GAN by using\nthe several recent one-stage and two-stage detectors, and improved detection\naccuracy over those detectors. Code is available at\nhttps://github.com/SHLee-cv/SRF-GAN.\n", "versions": [{"version": "v1", "created": "Tue, 17 Nov 2020 06:27:32 GMT"}], "update_date": "2020-11-18", "authors_parsed": [["Lee", "Seong-Ho", ""], ["Bae", "Seung-Hwan", ""]]}, {"id": "2011.08464", "submitter": "Shichao Li", "authors": "Shichao Li, Zengqiang Yan, Hongyang Li, Kwang-Ting Cheng", "title": "Exploring intermediate representation for monocular vehicle pose\n  estimation", "comments": "CVPR 2021 with supplementary materials", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new learning-based framework to recover vehicle pose in SO(3)\nfrom a single RGB image. In contrast to previous works that map from local\nappearance to observation angles, we explore a progressive approach by\nextracting meaningful Intermediate Geometrical Representations (IGRs) to\nestimate egocentric vehicle orientation. This approach features a deep model\nthat transforms perceived intensities to IGRs, which are mapped to a 3D\nrepresentation encoding object orientation in the camera coordinate system.\nCore problems are what IGRs to use and how to learn them more effectively. We\nanswer the former question by designing IGRs based on an interpolated cuboid\nthat derives from primitive 3D annotation readily. The latter question\nmotivates us to incorporate geometry knowledge with a new loss function based\non a projective invariant. This loss function allows unlabeled data to be used\nin the training stage to improve representation learning. Without additional\nlabels, our system outperforms previous monocular RGB-based methods for joint\nvehicle detection and pose estimation on the KITTI benchmark, achieving\nperformance even comparable to stereo methods. Code and pre-trained models are\navailable at this https URL.\n", "versions": [{"version": "v1", "created": "Tue, 17 Nov 2020 06:30:51 GMT"}, {"version": "v2", "created": "Sun, 31 Jan 2021 05:02:20 GMT"}, {"version": "v3", "created": "Tue, 16 Mar 2021 07:20:09 GMT"}, {"version": "v4", "created": "Tue, 30 Mar 2021 09:11:57 GMT"}, {"version": "v5", "created": "Mon, 12 Jul 2021 12:09:45 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Li", "Shichao", ""], ["Yan", "Zengqiang", ""], ["Li", "Hongyang", ""], ["Cheng", "Kwang-Ting", ""]]}, {"id": "2011.08465", "submitter": "Cristian Jes\\'us Vaca Rubio", "authors": "Cristian J. Vaca-Rubio, Pablo Ramirez-Espinosa, Kimmo Kansanen,\n  Zheng-Hua Tan, Elisabeth de Carvalho, Petar Popovski", "title": "Assessing Wireless Sensing Potential with Large Intelligent Surfaces", "comments": "arXiv admin note: text overlap with arXiv:2006.06563", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sensing capability is one of the most highlighted new feature of future 6G\nwireless networks. This paper addresses the sensing potential of Large\nIntelligent Surfaces (LIS) in an exemplary Industry 4.0 scenario. Besides the\nattention received by LIS in terms of communication aspects, it can offer a\nhigh-resolution rendering of the propagation environment. This is because, in\nan indoor setting, it can be placed in proximity to the sensed phenomena, while\nthe high resolution is offered by densely spaced tiny antennas deployed over a\nlarge area. By treating an LIS as a radio image of the environment relying on\nthe received signal power, we develop techniques to sense the environment, by\nleveraging the tools of image processing and machine learning. Once a\nholographic image is obtained, a Denoising Autoencoder (DAE) network can be\nused for constructing a super-resolution image leading to sensing advantages\nnot available in traditional sensing systems. Also, we derive a statistical\ntest based on the Generalized Likelihood Ratio (GLRT) as a benchmark for the\nmachine learning solution. We test these methods for a scenario where we need\nto detect whether an industrial robot deviates from a predefined route. The\nresults show that the LIS-based sensing offers high precision and has a high\napplication potential in indoor industrial environments.\n", "versions": [{"version": "v1", "created": "Mon, 16 Nov 2020 15:50:22 GMT"}, {"version": "v2", "created": "Thu, 18 Feb 2021 11:27:06 GMT"}, {"version": "v3", "created": "Mon, 12 Apr 2021 19:49:36 GMT"}], "update_date": "2021-04-14", "authors_parsed": [["Vaca-Rubio", "Cristian J.", ""], ["Ramirez-Espinosa", "Pablo", ""], ["Kansanen", "Kimmo", ""], ["Tan", "Zheng-Hua", ""], ["de Carvalho", "Elisabeth", ""], ["Popovski", "Petar", ""]]}, {"id": "2011.08502", "submitter": "Marvin Klingner", "authors": "Marvin Klingner, Jan-Aike Term\\\"ohlen, Jacob Ritterbach, Tim\n  Fingscheidt", "title": "Unsupervised BatchNorm Adaptation (UBNA): A Domain Adaptation Method for\n  Semantic Segmentation Without Using Source Domain Representations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present a solution to the task of \"unsupervised domain\nadaptation (UDA) of a pre-trained semantic segmentation model without relying\non any source domain representations\". Previous UDA approaches for semantic\nsegmentation either employed simultaneous training of the model in the source\nand target domains, or they relied on a generator network, replaying source\ndomain data to the model during adaptation. In contrast, we present our novel\nUnsupervised BatchNorm Adaptation (UBNA) method, which adapts a pre-trained\nmodel to an unseen target domain without using---beyond the existing model\nparameters from pre-training---any source domain representations (neither data,\nnor generators) and which can also be applied in an online setting or using\njust a few unlabeled images from the target domain in a few-shot manner.\nSpecifically, we partially adapt the normalization layer statistics to the\ntarget domain using an exponentially decaying momentum factor, thereby mixing\nthe statistics from both domains. By evaluation on standard UDA benchmarks for\nsemantic segmentation we show that this is superior to a model without\nadaptation and to baseline approaches using statistics from the target domain\nonly. Compared to standard UDA approaches we report a trade-off between\nperformance and usage of source domain representations.\n", "versions": [{"version": "v1", "created": "Tue, 17 Nov 2020 08:37:40 GMT"}], "update_date": "2020-11-18", "authors_parsed": [["Klingner", "Marvin", ""], ["Term\u00f6hlen", "Jan-Aike", ""], ["Ritterbach", "Jacob", ""], ["Fingscheidt", "Tim", ""]]}, {"id": "2011.08505", "submitter": "Kunhong Yu", "authors": "Kunhong Yu and Yuze Zhang", "title": "Digging Deeper into CRNN Model in Chinese Text Images Recognition", "comments": "16 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic text image recognition is a prevalent application in computer\nvision field. One efficient way is use Convolutional Recurrent Neural\nNetwork(CRNN) to accomplish task in an end-to-end(End2End) fashion. However,\nCRNN notoriously fails to detect multi-row images and excel-like images. In\nthis paper, we present one alternative to first recognize single-row images,\nthen extend the same architecture to recognize multi-row images with proposed\nmultiple methods. To recognize excel-like images containing box lines, we\npropose Line-Deep Denoising Convolutional AutoEncoder(Line-DDeCAE) to recover\nbox lines. Finally, we present one Knowledge Distillation(KD) method to\ncompress original CRNN model without loss of generality. To carry out\nexperiments, we first generate artificial samples from one Chinese novel book,\nthen conduct various experiments to verify our methods.\n", "versions": [{"version": "v1", "created": "Tue, 17 Nov 2020 08:46:37 GMT"}], "update_date": "2020-11-18", "authors_parsed": [["Yu", "Kunhong", ""], ["Zhang", "Yuze", ""]]}, {"id": "2011.08508", "submitter": "Chandan Gautam", "authors": "Chandan Gautam, Sethupathy Parameswaran, Ashish Mishra, Suresh\n  Sundaram", "title": "Generalized Continual Zero-Shot Learning", "comments": "Zero-shot Learning, Continual Learning, Incremental Learning", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, zero-shot learning (ZSL) emerged as an exciting topic and attracted\na lot of attention. ZSL aims to classify unseen classes by transferring the\nknowledge from seen classes to unseen classes based on the class description.\nDespite showing promising performance, ZSL approaches assume that the training\nsamples from all seen classes are available during the training, which is\npractically not feasible. To address this issue, we propose a more generalized\nand practical setup for ZSL, i.e., continual ZSL (CZSL), where classes arrive\nsequentially in the form of a task and it actively learns from the changing\nenvironment by leveraging the past experience. Further, to enhance the\nreliability, we develop CZSL for a single head continual learning setting where\ntask identity is revealed during the training process but not during the\ntesting. To avoid catastrophic forgetting and intransigence, we use knowledge\ndistillation and storing and replay the few samples from previous tasks using a\nsmall episodic memory. We develop baselines and evaluate generalized CZSL on\nfive ZSL benchmark datasets for two different settings of continual learning:\nwith and without class incremental. Moreover, CZSL is developed for two types\nof variational autoencoders, which generates two types of features for\nclassification: (i) generated features at output space and (ii) generated\ndiscriminative features at the latent space. The experimental results clearly\nindicate the single head CZSL is more generalizable and suitable for practical\napplications.\n", "versions": [{"version": "v1", "created": "Tue, 17 Nov 2020 08:47:54 GMT"}, {"version": "v2", "created": "Thu, 21 Jan 2021 23:38:24 GMT"}, {"version": "v3", "created": "Mon, 1 Feb 2021 02:28:33 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["Gautam", "Chandan", ""], ["Parameswaran", "Sethupathy", ""], ["Mishra", "Ashish", ""], ["Sundaram", "Suresh", ""]]}, {"id": "2011.08513", "submitter": "Marco Trombini", "authors": "Marco Trombini, Paolo Borro, Sebastiano Ziola, Silvana Dellepiane", "title": "A Digital Image Processing Approach for Hepatic Diseases Staging based\n  on the Glisson's Capsule", "comments": "Paper accepted with content unaltered to publish with IEEE Xplore\n  Digital Library - 2020 IEEE Malaysian International Biomedical Conference\n  (MIBEC)", "journal-ref": null, "doi": "10.1109/ICECIE50279.2020.9309633", "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to the need for quick and effective treatments for liver diseases, which\nare among the most common health problems in the world, staging fibrosis\nthrough non-invasive and economic methods has become of great importance.\nTaking inspiration from diagnostic laparoscopy, used in the past for hepatic\ndiseases, in this paper ultrasound images of the liver are studied, focusing on\na specific region of the organ where the Glisson's capsule is visible. In\nultrasound images, the Glisson's capsule appears in the shape of a line which\ncan be extracted via classical methods in literature. By making use of a\ncombination of standard image processing techniques and Convolutional Neural\nNetwork approaches, the scope of this work is to give evidence to the idea that\na great informative potential relies on smoothness of the Glisson's capsule\nsurface. To this purpose, several classifiers are taken into consideration,\nwhich deal with different type of data, namely ultrasound images, binary images\ndepicting the Glisson's line, and features vector extracted from the original\nimage. This is a preliminary study that has been retrospectively conducted,\nbased on the results of the elastosonography examination.\n", "versions": [{"version": "v1", "created": "Tue, 17 Nov 2020 09:05:54 GMT"}], "update_date": "2021-01-08", "authors_parsed": [["Trombini", "Marco", ""], ["Borro", "Paolo", ""], ["Ziola", "Sebastiano", ""], ["Dellepiane", "Silvana", ""]]}, {"id": "2011.08516", "submitter": "Jiahe Cui", "authors": "Jiahe Cui, Jianwei Niu, Zhenchao Ouyang, Yunxiang He and Dian Liu", "title": "ACSC: Automatic Calibration for Non-repetitive Scanning Solid-State\n  LiDAR and Camera Systems", "comments": "conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, the rapid development of Solid-State LiDAR (SSL) enables low-cost\nand efficient obtainment of 3D point clouds from the environment, which has\ninspired a large quantity of studies and applications. However, the\nnon-uniformity of its scanning pattern, and the inconsistency of the ranging\nerror distribution bring challenges to its calibration task. In this paper, we\nproposed a fully automatic calibration method for the non-repetitive scanning\nSSL and camera systems. First, a temporal-spatial-based geometric feature\nrefinement method is presented, to extract effective features from SSL point\nclouds; then, the 3D corners of the calibration target (a printed checkerboard)\nare estimated with the reflectance distribution of points. Based on the above,\na target-based extrinsic calibration method is finally proposed. We evaluate\nthe proposed method on different types of LiDAR and camera sensor combinations\nin real conditions, and achieve accuracy and robustness calibration results.\nThe code is available at https://github.com/HViktorTsoi/ACSC.git .\n", "versions": [{"version": "v1", "created": "Tue, 17 Nov 2020 09:11:28 GMT"}], "update_date": "2020-11-18", "authors_parsed": [["Cui", "Jiahe", ""], ["Niu", "Jianwei", ""], ["Ouyang", "Zhenchao", ""], ["He", "Yunxiang", ""], ["Liu", "Dian", ""]]}, {"id": "2011.08517", "submitter": "Frederik Hagelskjaer", "authors": "Frederik Hagelskjaer and Anders Glent Buch", "title": "Bridging the Reality Gap for Pose Estimation Networks using Sensor-Based\n  Domain Randomization", "comments": "10 pages, 5 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Since the introduction of modern deep learning methods for object pose\nestimation, test accuracy and efficiency has increased significantly. For\ntraining, however, large amounts of annotated training data are required for\ngood performance. While the use of synthetic training data prevents the need\nfor manual annotation, there is currently a large performance gap between\nmethods trained on real and synthetic data. This paper introduces a new method,\nwhich bridges this gap.\n  Most methods trained on synthetic data use 2D images, as domain randomization\nin 2D is more developed. To obtain precise poses, many of these methods perform\na final refinement using 3D data. Our method integrates the 3D data into the\nnetwork to increase the accuracy of the pose estimation. To allow for domain\nrandomization in 3D, a sensor-based data augmentation has been developed.\nAdditionally, we introduce the SparseEdge feature, which uses a wider search\nspace during point cloud propagation to avoid relying on specific features\nwithout increasing run-time.\n  Experiments on three large pose estimation benchmarks show that the presented\nmethod outperforms previous methods trained on synthetic data and achieves\ncomparable results to existing methods trained on real data.\n", "versions": [{"version": "v1", "created": "Tue, 17 Nov 2020 09:12:11 GMT"}, {"version": "v2", "created": "Tue, 18 May 2021 13:59:33 GMT"}], "update_date": "2021-05-19", "authors_parsed": [["Hagelskjaer", "Frederik", ""], ["Buch", "Anders Glent", ""]]}, {"id": "2011.08518", "submitter": "Marvin Chanc\\'an", "authors": "Marvin Chanc\\'an, Michael Milford", "title": "DeepSeqSLAM: A Trainable CNN+RNN for Joint Global Description and\n  Sequence-based Place Recognition", "comments": "9 pages, 6 figures, 2 tables", "journal-ref": "NeurIPS 2020 Workshop on Machine Learning for Autonomous Driving\n  (ML4AD)", "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG cs.RO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Sequence-based place recognition methods for all-weather navigation are\nwell-known for producing state-of-the-art results under challenging day-night\nor summer-winter transitions. These systems, however, rely on complex\nhandcrafted heuristics for sequential matching - which are applied on top of a\npre-computed pairwise similarity matrix between reference and query image\nsequences of a single route - to further reduce false-positive rates compared\nto single-frame retrieval methods. As a result, performing multi-frame place\nrecognition can be extremely slow for deployment on autonomous vehicles or\nevaluation on large datasets, and fail when using relatively short parameter\nvalues such as a sequence length of 2 frames. In this paper, we propose\nDeepSeqSLAM: a trainable CNN+RNN architecture for jointly learning visual and\npositional representations from a single monocular image sequence of a route.\nWe demonstrate our approach on two large benchmark datasets, Nordland and\nOxford RobotCar - recorded over 728 km and 10 km routes, respectively, each\nduring 1 year with multiple seasons, weather, and lighting conditions. On\nNordland, we compare our method to two state-of-the-art sequence-based methods\nacross the entire route under summer-winter changes using a sequence length of\n2 and show that our approach can get over 72% AUC compared to 27% AUC for Delta\nDescriptors and 2% AUC for SeqSLAM; while drastically reducing the deployment\ntime from around 1 hour to 1 minute against both. The framework code and video\nare available at https://mchancan.github.io/deepseqslam\n", "versions": [{"version": "v1", "created": "Tue, 17 Nov 2020 09:14:02 GMT"}], "update_date": "2020-11-18", "authors_parsed": [["Chanc\u00e1n", "Marvin", ""], ["Milford", "Michael", ""]]}, {"id": "2011.08525", "submitter": "Naoki Sugimoto", "authors": "Naoki Sugimoto, Yoshihito Ebine, Kiyoharu Aizawa", "title": "Building Movie Map -- A Tool for Exploring Areas in a City -- and its\n  Evaluation", "comments": null, "journal-ref": "ACM Multimedia 2020", "doi": "10.1145/3394171.3413881", "report-no": null, "categories": "cs.MM cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new Movie Map system, with an interface for exploring cities.\nThe system consists of four stages; acquisition, analysis, management, and\ninteraction. In the acquisition stage, omnidirectional videos are taken along\nstreets in target areas. Frames of the video are localized on the map,\nintersections are detected, and videos are segmented. Turning views at\nintersections are subsequently generated. By connecting the video segments\nfollowing the specified movement in an area, we can view the streets better.\nThe interface allows for easy exploration of a target area, and it can show\nvirtual billboards of stores in the view. We conducted user studies to compare\nour system to the GSV in a scenario where users could freely move and explore\nto find a landmark. The experiment showed that our system had a better user\nexperience than GSV.\n", "versions": [{"version": "v1", "created": "Tue, 17 Nov 2020 09:24:05 GMT"}], "update_date": "2021-03-12", "authors_parsed": [["Sugimoto", "Naoki", ""], ["Ebine", "Yoshihito", ""], ["Aizawa", "Kiyoharu", ""]]}, {"id": "2011.08528", "submitter": "Hamza Ilhan", "authors": "Hamza Osman Ilhan, Gorkem Serbes, Nizamettin Aydin", "title": "Decision and Feature Level Fusion of Deep Features Extracted from Public\n  COVID-19 Data-sets", "comments": "20 Pages, 9 Figures, 4 Tables and submitted a journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  The Coronavirus (COVID-19), which is an infectious pulmonary disorder, has\naffected millions of people and has been declared as a global pandemic by the\nWHO. Due to highly contagious nature of COVID-19 and its high possibility of\ncausing severe conditions in the patients, the development of rapid and\naccurate diagnostic tools have gained importance. The real-time reverse\ntranscription-polymerize chain reaction (RT-PCR) is used to detect the presence\nof Coronavirus RNA by using the mucus and saliva mixture samples. But, RT-PCR\nsuffers from having low-sensitivity especially in the early stage. Therefore,\nthe usage of chest radiography has been increasing in the early diagnosis of\nCOVID-19 due to its fast imaging speed, significantly low cost and low dosage\nexposure of radiation. In our study, a computer-aided diagnosis system for\nX-ray images based on convolutional neural networks (CNNs), which can be used\nby radiologists as a supporting tool in COVID-19 detection, has been proposed.\nDeep feature sets extracted by using CNNs were concatenated for feature level\nfusion and fed to multiple classifiers in terms of decision level fusion idea\nwith the aim of discriminating COVID-19, pneumonia and no-finding classes. In\nthe decision level fusion idea, a majority voting scheme was applied to the\nresultant decisions of classifiers. The obtained accuracy values and confusion\nmatrix based evaluation criteria were presented for three progressively created\ndata-sets. The aspects of the proposed method that are superior to existing\nCOVID-19 detection studies have been discussed and the fusion performance of\nproposed approach was validated visually by using Class Activation Mapping\ntechnique. The experimental results show that the proposed approach has\nattained high COVID-19 detection performance that was proven by its comparable\naccuracy and superior precision/recall values with the existing studies.\n", "versions": [{"version": "v1", "created": "Tue, 17 Nov 2020 09:36:21 GMT"}], "update_date": "2020-11-18", "authors_parsed": [["Ilhan", "Hamza Osman", ""], ["Serbes", "Gorkem", ""], ["Aydin", "Nizamettin", ""]]}, {"id": "2011.08529", "submitter": "Zhaoyi Wan", "authors": "Zhaoyi Wan, Yimin Chen, Sutao Deng, Kunpeng Chen, Cong Yao, Jiebo Luo", "title": "Slender Object Detection: Diagnoses and Improvements", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we are concerned with the detection of a particular type of\nobjects with extreme aspect ratios, namely \\textbf{slender objects}. In\nreal-world scenarios, slender objects are actually very common and crucial to\nthe objective of a detection system. However, this type of objects has been\nlargely overlooked by previous object detection algorithms. Upon our\ninvestigation, for a classical object detection method, a drastic drop of\n$18.9\\%$ mAP on COCO is observed, if solely evaluated on slender objects.\nTherefore, we systematically study the problem of slender object detection in\nthis work. Accordingly, an analytical framework with carefully designed\nbenchmark and evaluation protocols is established, in which different\nalgorithms and modules can be inspected and compared. \\New Our study reveals\nthat effective slender object detection can be achieved ~\\textbf{with none of}\n(1) anchor-based localization; (2) specially designed box representations.\nInstead, \\textbf{the critical aspect of improving slender object detection is\nfeature adaptation}. It identifies and extends the insights of existing methods\nthat are previously underexploited. Furthermore, we propose a feature adaption\nstrategy that achieves clear and consistent improvements over current\nrepresentative object detection methods.\n", "versions": [{"version": "v1", "created": "Tue, 17 Nov 2020 09:39:42 GMT"}, {"version": "v2", "created": "Sat, 21 Nov 2020 05:33:07 GMT"}, {"version": "v3", "created": "Thu, 24 Dec 2020 09:14:36 GMT"}, {"version": "v4", "created": "Wed, 7 Apr 2021 02:35:15 GMT"}], "update_date": "2021-04-08", "authors_parsed": [["Wan", "Zhaoyi", ""], ["Chen", "Yimin", ""], ["Deng", "Sutao", ""], ["Chen", "Kunpeng", ""], ["Yao", "Cong", ""], ["Luo", "Jiebo", ""]]}, {"id": "2011.08534", "submitter": "Riccardo Spezialetti", "authors": "Riccardo Spezialetti, David Joseph Tan, Alessio Tonioni, Keisuke\n  Tateno, Federico Tombari", "title": "A Divide et Impera Approach for 3D Shape Reconstruction from Multiple\n  Views", "comments": "Accepted to 3DV 2020 as oral", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimating the 3D shape of an object from a single or multiple images has\ngained popularity thanks to the recent breakthroughs powered by deep learning.\nMost approaches regress the full object shape in a canonical pose, possibly\nextrapolating the occluded parts based on the learned priors. However, their\nviewpoint invariant technique often discards the unique structures visible from\nthe input images. In contrast, this paper proposes to rely on viewpoint variant\nreconstructions by merging the visible information from the given views. Our\napproach is divided into three steps. Starting from the sparse views of the\nobject, we first align them into a common coordinate system by estimating the\nrelative pose between all the pairs. Then, inspired by the traditional voxel\ncarving, we generate an occupancy grid of the object taken from the silhouette\non the images and their relative poses. Finally, we refine the initial\nreconstruction to build a clean 3D model which preserves the details from each\nviewpoint. To validate the proposed method, we perform a comprehensive\nevaluation on the ShapeNet reference benchmark in terms of relative pose\nestimation and 3D shape reconstruction.\n", "versions": [{"version": "v1", "created": "Tue, 17 Nov 2020 09:59:32 GMT"}, {"version": "v2", "created": "Wed, 18 Nov 2020 09:16:53 GMT"}], "update_date": "2020-11-19", "authors_parsed": [["Spezialetti", "Riccardo", ""], ["Tan", "David Joseph", ""], ["Tonioni", "Alessio", ""], ["Tateno", "Keisuke", ""], ["Tombari", "Federico", ""]]}, {"id": "2011.08543", "submitter": "Thu Nguyen", "authors": "Thu Nguyen, Duy Phung, Minh Hoai, Thien Huu Nguyen", "title": "Structural and Functional Decomposition for Personality Image Captioning\n  in a Communication Game", "comments": "10 pages, EMNLP-Findings 2020", "journal-ref": "EMNLP-Findings 2020", "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Personality image captioning (PIC) aims to describe an image with a natural\nlanguage caption given a personality trait. In this work, we introduce a novel\nformulation for PIC based on a communication game between a speaker and a\nlistener. The speaker attempts to generate natural language captions while the\nlistener encourages the generated captions to contain discriminative\ninformation about the input images and personality traits. In this way, we\nexpect that the generated captions can be improved to naturally represent the\nimages and express the traits. In addition, we propose to adapt the language\nmodel GPT2 to perform caption generation for PIC. This enables the speaker and\nlistener to benefit from the language encoding capacity of GPT2. Our\nexperiments show that the proposed model achieves the state-of-the-art\nperformance for PIC.\n", "versions": [{"version": "v1", "created": "Tue, 17 Nov 2020 10:19:27 GMT"}], "update_date": "2020-11-18", "authors_parsed": [["Nguyen", "Thu", ""], ["Phung", "Duy", ""], ["Hoai", "Minh", ""], ["Nguyen", "Thien Huu", ""]]}, {"id": "2011.08555", "submitter": "Daniel M. Lang", "authors": "Daniel M. Lang, Jan C. Peeken, Stephanie E. Combs, Jan J. Wilkens,\n  Stefan Bartzsch", "title": "Deep Learning Based HPV Status Prediction for Oropharyngeal Cancer\n  Patients", "comments": "15 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigated the ability of deep learning models for imaging based HPV\nstatus detection. To overcome the problem of small medical datasets we used a\ntransfer learning approach. A 3D convolutional network pre-trained on sports\nvideo clips was fine tuned such that full 3D information in the CT images could\nbe exploited. The video pre-trained model was able to differentiate\nHPV-positive from HPV-negative cases with an area under the receiver operating\ncharacteristic curve (AUC) of 0.81 for an external test set. In comparison to a\n3D convolutional neural network (CNN) trained from scratch and a 2D\narchitecture pre-trained on ImageNet the video pre-trained model performed\nbest.\n", "versions": [{"version": "v1", "created": "Tue, 17 Nov 2020 10:40:44 GMT"}], "update_date": "2020-11-18", "authors_parsed": [["Lang", "Daniel M.", ""], ["Peeken", "Jan C.", ""], ["Combs", "Stephanie E.", ""], ["Wilkens", "Jan J.", ""], ["Bartzsch", "Stefan", ""]]}, {"id": "2011.08559", "submitter": "Olivier Rukundo", "authors": "Olivier Rukundo", "title": "Normalized Weighting Schemes for Image Interpolation Algorithms", "comments": "8 pages, 14 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper presents and evaluates four weighting schemes for image\ninterpolation algorithms. The first scheme is based on the normalized area of\nthe circle, whose diameter is equal to the minimum side of a tetragon. The\nsecond scheme is based on the normalized area of the circle, whose radius is\nequal to the hypotenuse. The third scheme is based on the normalized area of\nthe triangle, whose base and height are equal to the hypotenuse and virtual\npixel length, respectively. The fourth weighting scheme is based on the\nnormalized area of the circle, whose radius is equal to the virtual pixel\nlength-based hypotenuse. Experiments demonstrated debatable algorithm\nperformances and the need for further research.\n", "versions": [{"version": "v1", "created": "Tue, 17 Nov 2020 10:47:50 GMT"}, {"version": "v2", "created": "Sun, 22 Nov 2020 17:57:40 GMT"}], "update_date": "2020-11-24", "authors_parsed": [["Rukundo", "Olivier", ""]]}, {"id": "2011.08577", "submitter": "Yuan Jianlong", "authors": "Yuan Jianlong and Zelu Deng and Wang Shu and Luo Zhenbo", "title": "Multi Receptive Field Network for Semantic Segmentation", "comments": "Accept by WACV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semantic segmentation is one of the key tasks in computer vision, which is to\nassign a category label to each pixel in an image. Despite significant progress\nachieved recently, most existing methods still suffer from two challenging\nissues: 1) the size of objects and stuff in an image can be very diverse,\ndemanding for incorporating multi-scale features into the fully convolutional\nnetworks (FCNs); 2) the pixels close to or at the boundaries of object/stuff\nare hard to classify due to the intrinsic weakness of convolutional networks.\nTo address the first issue, we propose a new Multi-Receptive Field Module\n(MRFM), explicitly taking multi-scale features into account. For the second\nissue, we design an edge-aware loss which is effective in distinguishing the\nboundaries of object/stuff. With these two designs, our Multi Receptive Field\nNetwork achieves new state-of-the-art results on two widely-used semantic\nsegmentation benchmark datasets. Specifically, we achieve a mean IoU of 83.0 on\nthe Cityscapes dataset and 88.4 mean IoU on the Pascal VOC2012 dataset.\n", "versions": [{"version": "v1", "created": "Tue, 17 Nov 2020 11:52:23 GMT"}], "update_date": "2020-11-18", "authors_parsed": [["Jianlong", "Yuan", ""], ["Deng", "Zelu", ""], ["Shu", "Wang", ""], ["Zhenbo", "Luo", ""]]}, {"id": "2011.08612", "submitter": "Jing Zhang", "authors": "Jing Zhang and Dacheng Tao", "title": "Empowering Things with Intelligence: A Survey of the Progress,\n  Challenges, and Opportunities in Artificial Intelligence of Things", "comments": "Accepted by IEEE Internet of Things Journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CV cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the Internet of Things (IoT) era, billions of sensors and devices collect\nand process data from the environment, transmit them to cloud centers, and\nreceive feedback via the internet for connectivity and perception. However,\ntransmitting massive amounts of heterogeneous data, perceiving complex\nenvironments from these data, and then making smart decisions in a timely\nmanner are difficult. Artificial intelligence (AI), especially deep learning,\nis now a proven success in various areas including computer vision, speech\nrecognition, and natural language processing. AI introduced into the IoT\nheralds the era of artificial intelligence of things (AIoT). This paper\npresents a comprehensive survey on AIoT to show how AI can empower the IoT to\nmake it faster, smarter, greener, and safer. Specifically, we briefly present\nthe AIoT architecture in the context of cloud computing, fog computing, and\nedge computing. Then, we present progress in AI research for IoT from four\nperspectives: perceiving, learning, reasoning, and behaving. Next, we summarize\nsome promising applications of AIoT that are likely to profoundly reshape our\nworld. Finally, we highlight the challenges facing AIoT and some potential\nresearch opportunities.\n", "versions": [{"version": "v1", "created": "Tue, 17 Nov 2020 13:14:28 GMT"}], "update_date": "2020-11-18", "authors_parsed": [["Zhang", "Jing", ""], ["Tao", "Dacheng", ""]]}, {"id": "2011.08614", "submitter": "P Aditya Sreekar", "authors": "P Aditya Sreekar, Ujjwal Tiwari and Anoop Namboodiri", "title": "Mutual Information Based Method for Unsupervised Disentanglement of\n  Video Representation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Video Prediction is an interesting and challenging task of predicting future\nframes from a given set context frames that belong to a video sequence. Video\nprediction models have found prospective applications in Maneuver Planning,\nHealth care, Autonomous Navigation and Simulation. One of the major challenges\nin future frame generation is due to the high dimensional nature of visual\ndata. In this work, we propose Mutual Information Predictive Auto-Encoder\n(MIPAE) framework, that reduces the task of predicting high dimensional video\nframes by factorising video representations into content and low dimensional\npose latent variables that are easy to predict. A standard LSTM network is used\nto predict these low dimensional pose representations. Content and the\npredicted pose representations are decoded to generate future frames. Our\napproach leverages the temporal structure of the latent generative factors of a\nvideo and a novel mutual information loss to learn disentangled video\nrepresentations. We also propose a metric based on mutual information gap (MIG)\nto quantitatively access the effectiveness of disentanglement on DSprites and\nMPI3D-real datasets. MIG scores corroborate with the visual superiority of\nframes predicted by MIPAE. We also compare our method quantitatively on\nevaluation metrics LPIPS, SSIM and PSNR.\n", "versions": [{"version": "v1", "created": "Tue, 17 Nov 2020 13:16:07 GMT"}], "update_date": "2020-11-18", "authors_parsed": [["Sreekar", "P Aditya", ""], ["Tiwari", "Ujjwal", ""], ["Namboodiri", "Anoop", ""]]}, {"id": "2011.08621", "submitter": "Lingxi Xie", "authors": "Longhui Wei, Lingxi Xie, Jianzhong He, Jianlong Chang, Xiaopeng Zhang,\n  Wengang Zhou, Houqiang Li, Qi Tian", "title": "Can Semantic Labels Assist Self-Supervised Visual Representation\n  Learning?", "comments": "10 pages, 4 figures, 8 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, contrastive learning has largely advanced the progress of\nunsupervised visual representation learning. Pre-trained on ImageNet, some\nself-supervised algorithms reported higher transfer learning performance\ncompared to fully-supervised methods, seeming to deliver the message that human\nlabels hardly contribute to learning transferrable visual features. In this\npaper, we defend the usefulness of semantic labels but point out that\nfully-supervised and self-supervised methods are pursuing different kinds of\nfeatures. To alleviate this issue, we present a new algorithm named Supervised\nContrastive Adjustment in Neighborhood (SCAN) that maximally prevents the\nsemantic guidance from damaging the appearance feature embedding. In a series\nof downstream tasks, SCAN achieves superior performance compared to previous\nfully-supervised and self-supervised methods, and sometimes the gain is\nsignificant. More importantly, our study reveals that semantic labels are\nuseful in assisting self-supervised methods, opening a new direction for the\ncommunity.\n", "versions": [{"version": "v1", "created": "Tue, 17 Nov 2020 13:25:00 GMT"}], "update_date": "2020-11-18", "authors_parsed": [["Wei", "Longhui", ""], ["Xie", "Lingxi", ""], ["He", "Jianzhong", ""], ["Chang", "Jianlong", ""], ["Zhang", "Xiaopeng", ""], ["Zhou", "Wengang", ""], ["Li", "Houqiang", ""], ["Tian", "Qi", ""]]}, {"id": "2011.08627", "submitter": "Hongsuk Choi", "authors": "Hongsuk Choi, Gyeongsik Moon, Ju Yong Chang, Kyoung Mu Lee", "title": "Beyond Static Features for Temporally Consistent 3D Human Pose and Shape\n  from a Video", "comments": "Accepted to CVPR 2021, 10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Despite the recent success of single image-based 3D human pose and shape\nestimation methods, recovering temporally consistent and smooth 3D human motion\nfrom a video is still challenging. Several video-based methods have been\nproposed; however, they fail to resolve the single image-based methods'\ntemporal inconsistency issue due to a strong dependency on a static feature of\nthe current frame. In this regard, we present a temporally consistent mesh\nrecovery system (TCMR). It effectively focuses on the past and future frames'\ntemporal information without being dominated by the current static feature. Our\nTCMR significantly outperforms previous video-based methods in temporal\nconsistency with better per-frame 3D pose and shape accuracy. We also release\nthe codes. For the demo video, see https://youtu.be/WB3nTnSQDII. For the codes,\nsee https://github.com/hongsukchoi/TCMR_RELEASE.\n", "versions": [{"version": "v1", "created": "Tue, 17 Nov 2020 13:41:34 GMT"}, {"version": "v2", "created": "Thu, 26 Nov 2020 06:06:36 GMT"}, {"version": "v3", "created": "Thu, 1 Apr 2021 07:04:32 GMT"}, {"version": "v4", "created": "Tue, 27 Apr 2021 06:54:19 GMT"}], "update_date": "2021-04-28", "authors_parsed": [["Choi", "Hongsuk", ""], ["Moon", "Gyeongsik", ""], ["Chang", "Ju Yong", ""], ["Lee", "Kyoung Mu", ""]]}, {"id": "2011.08634", "submitter": "Hamed Damirchi", "authors": "Hamed Damirchi, Rooholla Khorrambakht and Hamid D. Taghirad", "title": "Exploring Self-Attention for Visual Odometry", "comments": "8 pages, 7 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.RO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Visual odometry networks commonly use pretrained optical flow networks in\norder to derive the ego-motion between consecutive frames. The features\nextracted by these networks represent the motion of all the pixels between\nframes. However, due to the existence of dynamic objects and texture-less\nsurfaces in the scene, the motion information for every image region might not\nbe reliable for inferring odometry due to the ineffectiveness of dynamic\nobjects in derivation of the incremental changes in position. Recent works in\nthis area lack attention mechanisms in their structures to facilitate dynamic\nreweighing of the feature maps for extracting more refined egomotion\ninformation. In this paper, we explore the effectiveness of self-attention in\nvisual odometry. We report qualitative and quantitative results against the\nSOTA methods. Furthermore, saliency-based studies alongside specially designed\nexperiments are utilized to investigate the effect of self-attention on VO. Our\nexperiments show that using self-attention allows for the extraction of better\nfeatures while achieving a better odometry performance compared to networks\nthat lack such structures.\n", "versions": [{"version": "v1", "created": "Tue, 17 Nov 2020 13:53:26 GMT"}], "update_date": "2020-11-18", "authors_parsed": [["Damirchi", "Hamed", ""], ["Khorrambakht", "Rooholla", ""], ["Taghirad", "Hamid D.", ""]]}, {"id": "2011.08641", "submitter": "Farhad Pourpanah Dr.", "authors": "Farhad Pourpanah and Moloud Abdar and Yuxuan Luo and Xinlei Zhou and\n  Ran Wang and Chee Peng Lim and Xi-Zhao Wang", "title": "A Review of Generalized Zero-Shot Learning Methods", "comments": "24 pages, 17 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generalized zero-shot learning (GZSL) aims to train a model for classifying\ndata samples under the condition that some output classes are unknown during\nsupervised learning. To address this challenging task, GZSL leverages semantic\ninformation of the seen (source) and unseen (target) classes to bridge the gap\nbetween both seen and unseen classes. Since its introduction, many GZSL models\nhave been formulated. In this review paper, we present a comprehensive review\non GZSL. Firstly, we provide an overview of GZSL including the problems and\nchallenges. Then, we introduce a hierarchical categorization for the GZSL\nmethods and discuss the representative methods in each category. In addition,\nwe discuss the available benchmark data sets and applications of GZSL, along\nwith a discussion on the research gaps and directions for future\ninvestigations.\n", "versions": [{"version": "v1", "created": "Tue, 17 Nov 2020 14:00:30 GMT"}, {"version": "v2", "created": "Wed, 12 May 2021 04:26:54 GMT"}, {"version": "v3", "created": "Wed, 19 May 2021 12:15:22 GMT"}], "update_date": "2021-05-20", "authors_parsed": [["Pourpanah", "Farhad", ""], ["Abdar", "Moloud", ""], ["Luo", "Yuxuan", ""], ["Zhou", "Xinlei", ""], ["Wang", "Ran", ""], ["Lim", "Chee Peng", ""], ["Wang", "Xi-Zhao", ""]]}, {"id": "2011.08652", "submitter": "Mohsen Fayyaz", "authors": "Mohsen Fayyaz, Emad Bahrami, Ali Diba, Mehdi Noroozi, Ehsan Adeli, Luc\n  Van Gool, Juergen Gall", "title": "3D CNNs with Adaptive Temporal Feature Resolutions", "comments": "CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While state-of-the-art 3D Convolutional Neural Networks (CNN) achieve very\ngood results on action recognition datasets, they are computationally very\nexpensive and require many GFLOPs. While the GFLOPs of a 3D CNN can be\ndecreased by reducing the temporal feature resolution within the network, there\nis no setting that is optimal for all input clips. In this work, we therefore\nintroduce a differentiable Similarity Guided Sampling (SGS) module, which can\nbe plugged into any existing 3D CNN architecture. SGS empowers 3D CNNs by\nlearning the similarity of temporal features and grouping similar features\ntogether. As a result, the temporal feature resolution is not anymore static\nbut it varies for each input video clip. By integrating SGS as an additional\nlayer within current 3D CNNs, we can convert them into much more efficient 3D\nCNNs with adaptive temporal feature resolutions (ATFR). Our evaluations show\nthat the proposed module improves the state-of-the-art by reducing the\ncomputational cost (GFLOPs) by half while preserving or even improving the\naccuracy. We evaluate our module by adding it to multiple state-of-the-art 3D\nCNNs on various datasets such as Kinetics-600, Kinetics-400, mini-Kinetics,\nSomething-Something~V2, UCF101, and HMDB51.\n", "versions": [{"version": "v1", "created": "Tue, 17 Nov 2020 14:34:05 GMT"}, {"version": "v2", "created": "Tue, 30 Mar 2021 13:06:10 GMT"}, {"version": "v3", "created": "Thu, 1 Apr 2021 10:31:57 GMT"}], "update_date": "2021-04-02", "authors_parsed": [["Fayyaz", "Mohsen", ""], ["Bahrami", "Emad", ""], ["Diba", "Ali", ""], ["Noroozi", "Mehdi", ""], ["Adeli", "Ehsan", ""], ["Van Gool", "Luc", ""], ["Gall", "Juergen", ""]]}, {"id": "2011.08655", "submitter": "Domenico Gatti", "authors": "Haikal Abdulah, Benjamin Huber, Sinan Lal, Hassan Abdallah, Hamid\n  Soltanian-Zadeh, Domenico L. Gatti", "title": "Lung Segmentation in Chest X-rays with Res-CR-Net", "comments": "8 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Deep Neural Networks (DNN) are widely used to carry out segmentation tasks in\nbiomedical images. Most DNNs developed for this purpose are based on some\nvariation of the encoder-decoder U-Net architecture. Here we show that\nRes-CR-Net, a new type of fully convolutional neural network, which was\noriginally developed for the semantic segmentation of microscopy images, and\nwhich does not adopt a U-Net architecture, is very effective at segmenting the\nlung fields in chest X-rays from either healthy patients or patients with a\nvariety of lung pathologies.\n", "versions": [{"version": "v1", "created": "Sat, 14 Nov 2020 04:06:28 GMT"}], "update_date": "2020-11-18", "authors_parsed": [["Abdulah", "Haikal", ""], ["Huber", "Benjamin", ""], ["Lal", "Sinan", ""], ["Abdallah", "Hassan", ""], ["Soltanian-Zadeh", "Hamid", ""], ["Gatti", "Domenico L.", ""]]}, {"id": "2011.08673", "submitter": "Marius Stan", "authors": "Jessica Pan, Joseph A. Libera, Noah H. Paulson and Marius Stan", "title": "Flame Stability Analysis of Flame Spray Pyrolysis by Artificial\n  Intelligence", "comments": "25 pages, 8 figures. International Journal of Advanced Manufacturing\n  Technology 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Flame spray pyrolysis (FSP) is a process used to synthesize nanoparticles\nthrough the combustion of an atomized precursor solution; this process has\napplications in catalysts, battery materials, and pigments. Current limitations\nrevolve around understanding how to consistently achieve a stable flame and the\nreliable production of nanoparticles. Machine learning and artificial\nintelligence algorithms that detect unstable flame conditions in real time may\nbe a means of streamlining the synthesis process and improving FSP efficiency.\nIn this study, the FSP flame stability is first quantified by analyzing the\nbrightness of the flame's anchor point. This analysis is then used to label\ndata for both unsupervised and supervised machine learning approaches. The\nunsupervised learning approach allows for autonomous labelling and\nclassification of new data by representing data in a reduced dimensional space\nand identifying combinations of features that most effectively cluster it. The\nsupervised learning approach, on the other hand, requires human labeling of\ntraining and test data, but is able to classify multiple objects of interest\n(such as the burner and pilot flames) within the video feed. The accuracy of\neach of these techniques is compared against the evaluations of human experts.\nBoth the unsupervised and supervised approaches can track and classify FSP\nflame conditions in real time to alert users of unstable flame conditions. This\nresearch has the potential to autonomously track and manage flame spray\npyrolysis as well as other flame technologies by monitoring and classifying the\nflame stability.\n", "versions": [{"version": "v1", "created": "Thu, 22 Oct 2020 22:52:13 GMT"}], "update_date": "2020-11-18", "authors_parsed": [["Pan", "Jessica", ""], ["Libera", "Joseph A.", ""], ["Paulson", "Noah H.", ""], ["Stan", "Marius", ""]]}, {"id": "2011.08674", "submitter": "Xi Zhang", "authors": "Xi Zhang and Xiaolin Wu", "title": "On Numerosity of Deep Neural Networks", "comments": "Accepted to NeurIPS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, a provocative claim was published that number sense spontaneously\nemerges in a deep neural network trained merely for visual object recognition.\nThis has, if true, far reaching significance to the fields of machine learning\nand cognitive science alike. In this paper, we prove the above claim to be\nunfortunately incorrect. The statistical analysis to support the claim is\nflawed in that the sample set used to identify number-aware neurons is too\nsmall, compared to the huge number of neurons in the object recognition\nnetwork. By this flawed analysis one could mistakenly identify number-sensing\nneurons in any randomly initialized deep neural networks that are not trained\nat all. With the above critique we ask the question what if a deep\nconvolutional neural network is carefully trained for numerosity? Our findings\nare mixed. Even after being trained with number-depicting images, the deep\nlearning approach still has difficulties to acquire the abstract concept of\nnumbers, a cognitive task that preschoolers perform with ease. But on the other\nhand, we do find some encouraging evidences suggesting that deep neural\nnetworks are more robust to distribution shift for small numbers than for large\nnumbers.\n", "versions": [{"version": "v1", "created": "Sun, 1 Nov 2020 15:30:43 GMT"}], "update_date": "2020-11-18", "authors_parsed": [["Zhang", "Xi", ""], ["Wu", "Xiaolin", ""]]}, {"id": "2011.08675", "submitter": "Zhigang Jia", "authors": "Zhigang Jia and Qiyu Jin and Michael K. Ng and Xile Zhao", "title": "Non-Local Robust Quaternion Matrix Completion for Large-Scale Color\n  Images and Videos Inpainting", "comments": "22 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.NA math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The image nonlocal self-similarity (NSS) prior refers to the fact that a\nlocal patch often has many nonlocal similar patches to it across the image. In\nthis paper we apply such NSS prior to enhance the robust quaternion matrix\ncompletion (QMC) method and significantly improve the inpainting performance. A\npatch group based NSS prior learning scheme is proposed to learn explicit NSS\nmodels from natural color images. The NSS-based QMC algorithm computes an\noptimal low-rank approximation to the high-rank color image, resulting in high\nPSNR and SSIM measures and particularly the better visual quality. A new joint\nNSS-base QMC method is also presented to solve the color video inpainting\nproblem based quaternion tensor representation. The numerical experiments on\nlarge-scale color images and videos indicate the advantages of NSS-based QMC\nover the state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Tue, 17 Nov 2020 14:50:40 GMT"}], "update_date": "2020-11-18", "authors_parsed": [["Jia", "Zhigang", ""], ["Jin", "Qiyu", ""], ["Ng", "Michael K.", ""], ["Zhao", "Xile", ""]]}, {"id": "2011.08676", "submitter": "Talha Bin Masood", "authors": "Wito Engelke, Talha Bin Masood, Jakob Beran, Rodrigo Caballero and\n  Ingrid Hotz", "title": "Topology-Based Feature Design and Tracking for Multi-Center Cyclones", "comments": "13 pages, 9 figures, 8th workshop on Topological Methods in Data\n  Analysis and Visualization (TopoInVis 2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CG cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a concept to design, track, and compare\napplication-specific feature definitions expressed as sets of critical points.\nOur work has been inspired by the observation that in many applications a large\nvariety of different feature definitions for the same concept are used. Often,\nthese definitions compete with each other and it is unclear which definition\nshould be used in which context. A prominent example is the definition of\ncyclones in climate research. Despite the differences, frequently these feature\ndefinitions can be related to topological concepts.\n  In our approach, we provide a cyclone tracking framework that supports\ninteractive feature definition and comparison based on a precomputed tracking\ngraph that stores all extremal points as well as their temporal correspondents.\nThe framework combines a set of independent building blocks: critical point\nextraction, critical point tracking, feature definition, and track exploration.\nOne of the major advantages of such an approach is the flexibility it provides,\nthat is, each block is exchangeable. Moreover, it also enables us to perform\nthe most expensive analysis, the construction of a full tracking graph, as a\nprepossessing step, while keeping the feature definition interactive. Different\nfeature definitions can be explored and compared interactively based on this\ntracking graph. Features are specified by rules for grouping critical points,\nwhile feature tracking corresponds to filtering and querying the full tracking\ngraph by specific requests. We demonstrate this method for cyclone\nidentification and tracking in the context of climate research.\n", "versions": [{"version": "v1", "created": "Sun, 1 Nov 2020 17:39:27 GMT"}], "update_date": "2020-11-18", "authors_parsed": [["Engelke", "Wito", ""], ["Masood", "Talha Bin", ""], ["Beran", "Jakob", ""], ["Caballero", "Rodrigo", ""], ["Hotz", "Ingrid", ""]]}, {"id": "2011.08682", "submitter": "Aniket Bera", "authors": "Venkatraman Narayanan and Bala Murali Manoghar and Rama Prashanth RV\n  and Aniket Bera", "title": "SeekNet: Improved Human Instance Segmentation via Reinforcement Learning\n  Based Optimized Robot Relocation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Amodal recognition is the ability of the system to detect occluded objects.\nMost state-of-the-art Visual Recognition systems lack the ability to perform\namodal recognition. Few studies have achieved amodal recognition through\npassive prediction or embodied recognition approaches. However, these\napproaches suffer from challenges in real-world applications, such as dynamic\nobjects. We propose SeekNet, an improved optimization method for amodal\nrecognition through embodied visual recognition. Additionally, we implement\nSeekNet for social robots, where there are multiple interactions with crowded\nhumans. Hence, we focus on occluded human detection & tracking and showcase the\nsuperiority of our algorithm over other baselines. We also experiment with\nSeekNet to improve the confidence of COVID-19 symptoms pre-screening algorithms\nusing our efficient embodied recognition system.\n", "versions": [{"version": "v1", "created": "Tue, 17 Nov 2020 15:03:30 GMT"}], "update_date": "2020-11-18", "authors_parsed": [["Narayanan", "Venkatraman", ""], ["Manoghar", "Bala Murali", ""], ["RV", "Rama Prashanth", ""], ["Bera", "Aniket", ""]]}, {"id": "2011.08690", "submitter": "Aniket Bera", "authors": "Pooja Guhan and Manas Agarwal and Naman Awasthi and Gloria Reeves and\n  Dinesh Manocha and Aniket Bera", "title": "ABC-Net: Semi-Supervised Multimodal GAN-based Engagement Detection using\n  an Affective, Behavioral and Cognitive Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present ABC-Net, a novel semi-supervised multimodal GAN framework to\ndetect engagement levels in video conversations based on psychology literature.\nWe use three constructs: behavioral, cognitive, and affective engagement, to\nextract various features that can effectively capture engagement levels. We\nfeed these features to our semi-supervised GAN network that does regression\nusing these latent representations to obtain the corresponding valence and\narousal values, which are then categorized into different levels of\nengagements. We demonstrate the efficiency of our network through experiments\non the RECOLA database. To evaluate our method, we analyze and compare our\nperformance on RECOLA and report a relative performance improvement of more\nthan 5% over the baseline methods. To the best of our knowledge, our approach\nis the first method to classify engagement based on a multimodal\nsemi-supervised network.\n", "versions": [{"version": "v1", "created": "Tue, 17 Nov 2020 15:18:38 GMT"}], "update_date": "2020-11-18", "authors_parsed": [["Guhan", "Pooja", ""], ["Agarwal", "Manas", ""], ["Awasthi", "Naman", ""], ["Reeves", "Gloria", ""], ["Manocha", "Dinesh", ""], ["Bera", "Aniket", ""]]}, {"id": "2011.08692", "submitter": "Nina Varney", "authors": "Nina Varney, Vijayan K. Asari and Quinn Graehling", "title": "Pyramid Point: A Multi-Level Focusing Network for Revisiting Feature\n  Layers", "comments": "10 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present a method to learn a diverse group of object categories from an\nunordered point set. We propose our Pyramid Point network, which uses a dense\npyramid structure instead of the traditional 'U' shape, typically seen in\nsemantic segmentation networks. This pyramid structure gives a second look,\nallowing the network to revisit different layers simultaneously, increasing the\ncontextual information by creating additional layers with less noise. We\nintroduce a Focused Kernel Point convolution (FKP Conv), which expands on the\ntraditional point convolutions by adding an attention mechanism to the kernel\noutputs. This FKP Conv increases our feature quality and allows us to weigh the\nkernel outputs dynamically. These FKP Convs are the central part of our\nRecurrent FKP Bottleneck block, which makes up the backbone of our encoder.\nWith this distinct network, we demonstrate competitive performance on three\nbenchmark data sets. We also perform an ablation study to show the positive\neffects of each element in our FKP Conv.\n", "versions": [{"version": "v1", "created": "Tue, 17 Nov 2020 15:23:27 GMT"}, {"version": "v2", "created": "Mon, 23 Nov 2020 16:35:43 GMT"}], "update_date": "2020-11-24", "authors_parsed": [["Varney", "Nina", ""], ["Asari", "Vijayan K.", ""], ["Graehling", "Quinn", ""]]}, {"id": "2011.08698", "submitter": "Zaccharie Ramzi", "authors": "Zaccharie Ramzi, Benjamin Remy, Francois Lanusse, Jean-Luc Starck,\n  Philippe Ciuciu", "title": "Denoising Score-Matching for Uncertainty Quantification in Inverse\n  Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG eess.SP physics.med-ph", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Deep neural networks have proven extremely efficient at solving a wide\nrangeof inverse problems, but most often the uncertainty on the solution they\nprovideis hard to quantify. In this work, we propose a generic Bayesian\nframework forsolving inverse problems, in which we limit the use of deep neural\nnetworks tolearning a prior distribution on the signals to recover. We adopt\nrecent denoisingscore matching techniques to learn this prior from data, and\nsubsequently use it aspart of an annealed Hamiltonian Monte-Carlo scheme to\nsample the full posteriorof image inverse problems. We apply this framework to\nMagnetic ResonanceImage (MRI) reconstruction and illustrate how this approach\nnot only yields highquality reconstructions but can also be used to assess the\nuncertainty on particularfeatures of a reconstructed image.\n", "versions": [{"version": "v1", "created": "Mon, 16 Nov 2020 18:33:06 GMT"}], "update_date": "2020-11-18", "authors_parsed": [["Ramzi", "Zaccharie", ""], ["Remy", "Benjamin", ""], ["Lanusse", "Francois", ""], ["Starck", "Jean-Luc", ""], ["Ciuciu", "Philippe", ""]]}, {"id": "2011.08712", "submitter": "Aria Khoshsirat", "authors": "Aria Khoshsirat", "title": "A Simple Framework to Quantify Different Types of Uncertainty in Deep\n  Neural Networks for Image Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Quantifying uncertainty in a model's predictions is important as it enables\nthe safety of an AI system to be increased by acting on the model's output in\nan informed manner. This is crucial for applications where the cost of an error\nis high, such as in autonomous vehicle control, medical image analysis,\nfinancial estimations or legal fields. Deep Neural Networks are powerful\npredictors that have recently achieved state-of-the-art performance on a wide\nspectrum of tasks. Quantifying predictive uncertainty in DNNs is a challenging\nand yet on-going problem. In this paper we propose a complete framework to\ncapture and quantify three known types of uncertainty in DNNs for the task of\nimage classification. This framework includes an ensemble of CNNs for model\nuncertainty, a supervised reconstruction auto-encoder to capture distributional\nuncertainty and using the output of activation functions in the last layer of\nthe network, to capture data uncertainty. Finally we demonstrate the efficiency\nof our method on popular image datasets for classification.\n", "versions": [{"version": "v1", "created": "Tue, 17 Nov 2020 15:36:42 GMT"}, {"version": "v2", "created": "Mon, 23 Nov 2020 07:35:42 GMT"}, {"version": "v3", "created": "Fri, 18 Dec 2020 17:44:35 GMT"}, {"version": "v4", "created": "Thu, 20 May 2021 20:03:34 GMT"}, {"version": "v5", "created": "Fri, 28 May 2021 15:33:37 GMT"}], "update_date": "2021-05-31", "authors_parsed": [["Khoshsirat", "Aria", ""]]}, {"id": "2011.08722", "submitter": "Aniket Bera", "authors": "Videsh Suman and Aniket Bera", "title": "RAIST: Learning Risk Aware Traffic Interactions via Spatio-Temporal\n  Graph Convolutional Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A key aspect of driving a road vehicle is to interact with the other road\nusers, assess their intentions and make risk-aware tactical decisions. An\nintuitive approach of enabling an intelligent automated driving system would be\nto incorporate some aspects of the human driving behavior. To this end, we\npropose a novel driving framework for egocentric views, which is based on\nspatio-temporal traffic graphs. The traffic graphs not only model the spatial\ninteractions amongst the road users, but also their individual intentions\nthrough temporally associated message passing. We leverage spatio-temporal\ngraph convolutional network (ST-GCN) to train the graph edges. These edges are\nformulated using parameterized functions of 3D positions and scene-aware\nappearance features of road agents. Along with tactical behavior prediction, it\nis crucial to evaluate the risk assessing ability of the proposed framework. We\nclaim that our framework learns risk aware representations by improving on the\ntask of risk object identification, especially in identifying objects with\nvulnerable interactions like pedestrians and cyclists.\n", "versions": [{"version": "v1", "created": "Tue, 17 Nov 2020 15:49:22 GMT"}], "update_date": "2020-11-18", "authors_parsed": [["Suman", "Videsh", ""], ["Bera", "Aniket", ""]]}, {"id": "2011.08726", "submitter": "Nicolai Dorka", "authors": "Nicolai Dorka, Johannes Meyer, Wolfram Burgard", "title": "Modality-Buffet for Real-Time Object Detection", "comments": "Accepted at the 2020 IEEE/RSJ International Conference on Intelligent\n  Robots and Systems (IROS)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Real-time object detection in videos using lightweight hardware is a crucial\ncomponent of many robotic tasks. Detectors using different modalities and with\nvarying computational complexities offer different trade-offs. One option is to\nhave a very lightweight model that can predict from all modalities at once for\neach frame. However, in some situations (e.g., in static scenes) it might be\nbetter to have a more complex but more accurate model and to extrapolate from\nprevious predictions for the frames coming in at processing time. We formulate\nthis task as a sequential decision making problem and use reinforcement\nlearning (RL) to generate a policy that decides from the RGB input which\ndetector out of a portfolio of different object detectors to take for the next\nprediction. The objective of the RL agent is to maximize the accuracy of the\npredictions per image. We evaluate the approach on the Waymo Open Dataset and\nshow that it exceeds the performance of each single detector.\n", "versions": [{"version": "v1", "created": "Tue, 17 Nov 2020 15:57:06 GMT"}], "update_date": "2020-11-18", "authors_parsed": [["Dorka", "Nicolai", ""], ["Meyer", "Johannes", ""], ["Burgard", "Wolfram", ""]]}, {"id": "2011.08740", "submitter": "Deeksha Arya", "authors": "Deeksha Arya (1, 2), Hiroya Maeda (2), Sanjay Kumar Ghosh (1), Durga\n  Toshniwal (1), Hiroshi Omata (2), Takehiro Kashiyama (2) and Yoshihide\n  Sekimoto (2) ((1) Indian Institute of Technology Roorkee, India, (2) The\n  University of Tokyo, Japan)", "title": "Global Road Damage Detection: State-of-the-art Solutions", "comments": "11 Pages, 2 Figures, 3 Tables", "journal-ref": null, "doi": "10.1109/BigData50022.2020.9377790", "report-no": null, "categories": "cs.CV cs.CY", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  This paper summarizes the Global Road Damage Detection Challenge (GRDDC), a\nBig Data Cup organized as a part of the IEEE International Conference on Big\nData'2020. The Big Data Cup challenges involve a released dataset and a\nwell-defined problem with clear evaluation metrics. The challenges run on a\ndata competition platform that maintains a leaderboard for the participants. In\nthe presented case, the data constitute 26336 road images collected from India,\nJapan, and the Czech Republic to propose methods for automatically detecting\nroad damages in these countries. In total, 121 teams from several countries\nregistered for this competition. The submitted solutions were evaluated using\ntwo datasets test1 and test2, comprising 2,631 and 2,664 images. This paper\nencapsulates the top 12 solutions proposed by these teams. The best performing\nmodel utilizes YOLO-based ensemble learning to yield an F1 score of 0.67 on\ntest1 and 0.66 on test2. The paper concludes with a review of the facets that\nworked well for the presented challenge and those that could be improved in\nfuture challenges.\n", "versions": [{"version": "v1", "created": "Tue, 17 Nov 2020 16:19:02 GMT"}], "update_date": "2021-03-24", "authors_parsed": [["Arya", "Deeksha", ""], ["Maeda", "Hiroya", ""], ["Ghosh", "Sanjay Kumar", ""], ["Toshniwal", "Durga", ""], ["Omata", "Hiroshi", ""], ["Kashiyama", "Takehiro", ""], ["Sekimoto", "Yoshihide", ""]]}, {"id": "2011.08752", "submitter": "Shan Lin", "authors": "Shan Lin, Fangbo Qin, Haonan Peng, Randall A. Bly, Kris S. Moe, Blake\n  Hannaford", "title": "Multi-frame Feature Aggregation for Real-time Instrument Segmentation in\n  Endoscopic Video", "comments": "Published in IEEE Robotics and Automation Letters (Early Access)", "journal-ref": null, "doi": "10.1109/LRA.2021.3096156", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning-based methods have achieved promising results on surgical\ninstrument segmentation. However, the high computation cost may limit the\napplication of deep models to time-sensitive tasks such as online surgical\nvideo analysis for robotic-assisted surgery. Moreover, current methods may\nstill suffer from challenging conditions in surgical images such as various\nlighting conditions and the presence of blood. We propose a novel Multi-frame\nFeature Aggregation (MFFA) module to aggregate video frame features temporally\nand spatially in a recurrent mode. By distributing the computation load of deep\nfeature extraction over sequential frames, we can use a lightweight encoder to\nreduce the computation costs at each time step. Moreover, public surgical\nvideos usually are not labeled frame by frame, so we develop a method that can\nrandomly synthesize a surgical frame sequence from a single labeled frame to\nassist network training. We demonstrate that our approach achieves superior\nperformance to corresponding deeper segmentation models on two public surgery\ndatasets.\n", "versions": [{"version": "v1", "created": "Tue, 17 Nov 2020 16:27:27 GMT"}, {"version": "v2", "created": "Mon, 26 Jul 2021 00:39:27 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Lin", "Shan", ""], ["Qin", "Fangbo", ""], ["Peng", "Haonan", ""], ["Bly", "Randall A.", ""], ["Moe", "Kris S.", ""], ["Hannaford", "Blake", ""]]}, {"id": "2011.08761", "submitter": "Ke Zhang", "authors": "Ke Zhang and Xiahai Zhuang", "title": "Recognition and standardization of cardiac MRI orientation via\n  multi-tasking learning and deep neural networks", "comments": "10 pages, 2 figures, to be published in STACOM 2020 (MICCAI Workshop)", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we study the problem of imaging orientation in cardiac MRI,\nand propose a framework to categorize the orientation for recognition and\nstandardization via deep neural networks. The method uses a new multi-tasking\nstrategy, where both the tasks of cardiac segmentation and orientation\nrecognition are simultaneously achieved. For multiple sequences and modalities\nof MRI, we propose a transfer learning strategy, which adapts our proposed\nmodel from a single modality to multiple modalities. We embed the orientation\nrecognition network in a Cardiac MRI Orientation Adjust Tool, i.e.,\nCMRadjustNet. We implemented two versions of CMRadjustNet, including a\nuser-interface (UI) software, and a command-line tool. The former version\nsupports MRI image visualization, orientation prediction, adjustment, and\nstorage operations; and the latter version enables the batch operations. The\nsource code, neural network models and tools have been released and open via\nhttps://zmiclab.github.io/projects.html.\n", "versions": [{"version": "v1", "created": "Tue, 17 Nov 2020 16:41:31 GMT"}], "update_date": "2020-11-21", "authors_parsed": [["Zhang", "Ke", ""], ["Zhuang", "Xiahai", ""]]}, {"id": "2011.08769", "submitter": "Ke Zhang", "authors": "Yuncheng Zhou and Ke Zhang and Xinzhe Luo and Sihan Wang and Xiahai\n  Zhuang", "title": "Anatomy Prior Based U-net for Pathology Segmentation with Attention", "comments": "8 pages, 3 figures, to be published in STACOM 2020 (MICCAI Workshop)", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Pathological area segmentation in cardiac magnetic resonance (MR) images\nplays a vital role in the clinical diagnosis of cardiovascular diseases.\nBecause of the irregular shape and small area, pathological segmentation has\nalways been a challenging task. We propose an anatomy prior based framework,\nwhich combines the U-net segmentation network with the attention technique.\nLeveraging the fact that the pathology is inclusive, we propose a neighborhood\npenalty strategy to gauge the inclusion relationship between the myocardium and\nthe myocardial infarction and no-reflow areas. This neighborhood penalty\nstrategy can be applied to any two labels with inclusive relationships (such as\nthe whole infarction and myocardium, etc.) to form a neighboring loss. The\nproposed framework is evaluated on the EMIDEC dataset. Results show that our\nframework is effective in pathological area segmentation.\n", "versions": [{"version": "v1", "created": "Tue, 17 Nov 2020 16:52:29 GMT"}], "update_date": "2020-11-21", "authors_parsed": [["Zhou", "Yuncheng", ""], ["Zhang", "Ke", ""], ["Luo", "Xinzhe", ""], ["Wang", "Sihan", ""], ["Zhuang", "Xiahai", ""]]}, {"id": "2011.08771", "submitter": "Minglei Lu", "authors": "Minglei Lu, Yu Guo, Fei Wang, Zheng Dang", "title": "A Method to Generate High Precision Mesh Model and RGB-D Datasetfor 6D\n  Pose Estimation Task", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recently, 3D version has been improved greatly due to the development of deep\nneural networks. A high quality dataset is important to the deep learning\nmethod. Existing datasets for 3D vision has been constructed, such as Bigbird\nand YCB. However, the depth sensors used to make these datasets are out of\ndate, which made the resolution and accuracy of the datasets cannot full fill\nthe higher standards of demand. Although the equipment and technology got\nbetter, but no one was trying to collect new and better dataset. Here we are\ntrying to fill that gap. To this end, we propose a new method for object\nreconstruction, which takes into account the speed, accuracy and robustness.\nOur method could be used to produce large dataset with better and more accurate\nannotation. More importantly, our data is more close to the rendering data,\nwhich shrinking the gap between the real data and synthetic data further.\n", "versions": [{"version": "v1", "created": "Tue, 17 Nov 2020 16:56:57 GMT"}], "update_date": "2020-11-18", "authors_parsed": [["Lu", "Minglei", ""], ["Guo", "Yu", ""], ["Wang", "Fei", ""], ["Dang", "Zheng", ""]]}, {"id": "2011.08785", "submitter": "Aleksandr Setkov", "authors": "Thomas Defard, Aleksandr Setkov, Angelique Loesch, Romaric Audigier", "title": "PaDiM: a Patch Distribution Modeling Framework for Anomaly Detection and\n  Localization", "comments": "7 pages, 2 figures, 8 tables, accepted at the 1st International\n  Workshop on Industrial Machine Learning, ICPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new framework for Patch Distribution Modeling, PaDiM, to\nconcurrently detect and localize anomalies in images in a one-class learning\nsetting. PaDiM makes use of a pretrained convolutional neural network (CNN) for\npatch embedding, and of multivariate Gaussian distributions to get a\nprobabilistic representation of the normal class. It also exploits correlations\nbetween the different semantic levels of CNN to better localize anomalies.\nPaDiM outperforms current state-of-the-art approaches for both anomaly\ndetection and localization on the MVTec AD and STC datasets. To match\nreal-world visual industrial inspection, we extend the evaluation protocol to\nassess performance of anomaly localization algorithms on non-aligned dataset.\nThe state-of-the-art performance and low complexity of PaDiM make it a good\ncandidate for many industrial applications.\n", "versions": [{"version": "v1", "created": "Tue, 17 Nov 2020 17:29:18 GMT"}], "update_date": "2020-11-18", "authors_parsed": [["Defard", "Thomas", ""], ["Setkov", "Aleksandr", ""], ["Loesch", "Angelique", ""], ["Audigier", "Romaric", ""]]}, {"id": "2011.08790", "submitter": "Jonathan Ventura", "authors": "Jonathan Ventura", "title": "P1AC: Revisiting Absolute Pose From a Single Affine Correspondence", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a novel solution to the problem of estimating the pose of a\ncalibrated camera given a single observation of an oriented point and an affine\ncorrespondence to a reference image. Affine correspondences have traditionally\nbeen used to improve feature matching over wide baselines; however, little\nprevious work has considered the use of such correspondences for absolute\ncamera pose computation. The advantage of our approach (P1AC) is that it\nrequires only a single correspondence in the minimal case in comparison to the\ntraditional point-based approach (P3P) which requires at least three points.\nOur method removes the limiting assumptions made in previous work and provides\na general solution that is applicable to large-scale image-based localization.\nOur evaluation on synthetic data shows that our approach is numerically stable\nand more robust to point observation noise than P3P. We also evaluate the\napplication of our approach for large-scale image-based localization and\ndemonstrate a practical reduction in the number of iterations and computation\ntime required to robustly localize an image.\n", "versions": [{"version": "v1", "created": "Tue, 17 Nov 2020 17:36:16 GMT"}], "update_date": "2020-11-18", "authors_parsed": [["Ventura", "Jonathan", ""]]}, {"id": "2011.08809", "submitter": "Aythami Morales", "authors": "Alejandro Pe\\~na and Ignacio Serna and Aythami Morales and Julian\n  Fierrez and Agata Lapedriza", "title": "Facial Expressions as a Vulnerability in Face Recognition", "comments": "Proc. of IEEE Int. Conf. on Image Processing (ICIP)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  This work explores facial expression bias as a security vulnerability of face\nrecognition systems. Despite the great performance achieved by state-of-the-art\nface recognition systems, the algorithms are still sensitive to a large range\nof covariates. We present a comprehensive analysis of how facial expression\nbias impacts the performance of face recognition technologies. Our study\nanalyzes: i) facial expression biases in the most popular face recognition\ndatabases; and ii) the impact of facial expression in face recognition\nperformances. Our experimental framework includes two face detectors, three\nface recognition models, and three different databases. Our results demonstrate\na huge facial expression bias in the most widely used databases, as well as a\nrelated impact of face expression in the performance of state-of-the-art\nalgorithms. This work opens the door to new research lines focused on\nmitigating the observed vulnerability.\n", "versions": [{"version": "v1", "created": "Tue, 17 Nov 2020 18:12:41 GMT"}, {"version": "v2", "created": "Fri, 18 Jun 2021 08:27:24 GMT"}], "update_date": "2021-06-21", "authors_parsed": [["Pe\u00f1a", "Alejandro", ""], ["Serna", "Ignacio", ""], ["Morales", "Aythami", ""], ["Fierrez", "Julian", ""], ["Lapedriza", "Agata", ""]]}, {"id": "2011.08819", "submitter": "Nikhil Churamani", "authors": "Nikhil Churamani, Sinan Kalkan and Hatice Gunes", "title": "Spatio-Temporal Analysis of Facial Actions using Lifecycle-Aware Capsule\n  Networks", "comments": "Updated Figure 6 and the Acknowledgements. Corrected typos. 11 pages,\n  6 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most state-of-the-art approaches for Facial Action Unit (AU) detection rely\nupon evaluating facial expressions from static frames, encoding a snapshot of\nheightened facial activity. In real-world interactions, however, facial\nexpressions are usually more subtle and evolve in a temporal manner requiring\nAU detection models to learn spatial as well as temporal information. In this\npaper, we focus on both spatial and spatio-temporal features encoding the\ntemporal evolution of facial AU activation. For this purpose, we propose the\nAction Unit Lifecycle-Aware Capsule Network (AULA-Caps) that performs AU\ndetection using both frame and sequence-level features. While at the\nframe-level the capsule layers of AULA-Caps learn spatial feature primitives to\ndetermine AU activations, at the sequence-level, it learns temporal\ndependencies between contiguous frames by focusing on relevant spatio-temporal\nsegments in the sequence. The learnt feature capsules are routed together such\nthat the model learns to selectively focus more on spatial or spatio-temporal\ninformation depending upon the AU lifecycle. The proposed model is evaluated on\nthe commonly used BP4D and GFT benchmark datasets obtaining state-of-the-art\nresults on both the datasets.\n", "versions": [{"version": "v1", "created": "Tue, 17 Nov 2020 18:36:38 GMT"}, {"version": "v2", "created": "Thu, 4 Mar 2021 02:41:43 GMT"}], "update_date": "2021-03-05", "authors_parsed": [["Churamani", "Nikhil", ""], ["Kalkan", "Sinan", ""], ["Gunes", "Hatice", ""]]}, {"id": "2011.08822", "submitter": "Zack Dulberg", "authors": "Zachary Dulberg and Jonathan Cohen", "title": "Learning Canonical Transformations", "comments": "NeurIPS 2020 Workshop on BabyMind", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Humans understand a set of canonical geometric transformations (such as\ntranslation and rotation) that support generalization by being untethered to\nany specific object. We explore inductive biases that help a neural network\nmodel learn these transformations in pixel space in a way that can generalize\nout-of-domain. Specifically, we find that high training set diversity is\nsufficient for the extrapolation of translation to unseen shapes and scales,\nand that an iterative training scheme achieves significant extrapolation of\nrotation in time.\n", "versions": [{"version": "v1", "created": "Tue, 17 Nov 2020 18:41:07 GMT"}], "update_date": "2020-11-18", "authors_parsed": [["Dulberg", "Zachary", ""], ["Cohen", "Jonathan", ""]]}, {"id": "2011.08826", "submitter": "Udaranga Wickramasinghe", "authors": "Udaranga Wickramasinghe and Graham Knott and Pascal Fua", "title": "Deep Active Surface Models", "comments": "11 pages, 7 figures, 6 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Active Surface Models have a long history of being useful to model complex 3D\nsurfaces but only Active Contours have been used in conjunction with deep\nnetworks, and then only to produce the data term as well as meta-parameter maps\ncontrolling them. In this paper, we advocate a much tighter integration. We\nintroduce layers that implement them that can be integrated seamlessly into\nGraph Convolutional Networks to enforce sophisticated smoothness priors at an\nacceptable computational cost. We will show that the resulting Deep Active\nSurface Models outperform equivalent architectures that use traditional\nregularization loss terms to impose smoothness priors for 3D surface\nreconstruction from 2D images and for 3D volume segmentation.\n", "versions": [{"version": "v1", "created": "Tue, 17 Nov 2020 18:48:28 GMT"}, {"version": "v2", "created": "Wed, 18 Nov 2020 18:25:58 GMT"}, {"version": "v3", "created": "Fri, 20 Nov 2020 23:19:36 GMT"}, {"version": "v4", "created": "Tue, 1 Jun 2021 21:31:20 GMT"}, {"version": "v5", "created": "Mon, 7 Jun 2021 22:19:00 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Wickramasinghe", "Udaranga", ""], ["Knott", "Graham", ""], ["Fua", "Pascal", ""]]}, {"id": "2011.08877", "submitter": "Xinyi Xu", "authors": "Xinyi Xu, Zhengyang Wang, Cheng Deng, Hao Yuan, and Shuiwang Ji", "title": "Towards Improved and Interpretable Deep Metric Learning via Attentive\n  Grouping", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Grouping has been commonly used in deep metric learning for computing diverse\nfeatures. However, current methods are prone to overfitting and lack\ninterpretability. In this work, we propose an improved and interpretable\ngrouping method to be integrated flexibly with any metric learning framework.\nOur method is based on the attention mechanism with a learnable query for each\ngroup. The query is fully trainable and can capture group-specific information\nwhen combined with the diversity loss. An appealing property of our method is\nthat it naturally lends itself interpretability. The attention scores between\nthe learnable query and each spatial position can be interpreted as the\nimportance of that position. We formally show that our proposed grouping method\nis invariant to spatial permutations of features. When used as a module in\nconvolutional neural networks, our method leads to translational invariance. We\nconduct comprehensive experiments to evaluate our method. Our quantitative\nresults indicate that the proposed method outperforms prior methods\nconsistently and significantly across different datasets, evaluation metrics,\nbase models, and loss functions. For the first time to the best of our\nknowledge, our interpretation results clearly demonstrate that the proposed\nmethod enables the learning of distinct and diverse features across groups.\n", "versions": [{"version": "v1", "created": "Tue, 17 Nov 2020 19:08:24 GMT"}, {"version": "v2", "created": "Thu, 19 Nov 2020 03:19:55 GMT"}], "update_date": "2020-11-20", "authors_parsed": [["Xu", "Xinyi", ""], ["Wang", "Zhengyang", ""], ["Deng", "Cheng", ""], ["Yuan", "Hao", ""], ["Ji", "Shuiwang", ""]]}, {"id": "2011.08891", "submitter": "Rachel Draelos", "authors": "Rachel Lea Draelos, Lawrence Carin", "title": "HiResCAM: Faithful Location Representation in Visual Attention for\n  Explainable 3D Medical Image Classification", "comments": "Paper and supplement: 26 pages, 14 figures, 6 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding model predictions is critical in healthcare, to facilitate\nrapid verification of model correctness and to guard against the use of models\nthat exploit confounding variables. Here we address the challenging new task of\nexplainable multilabel classification of volumetric medical images. We first\nillustrate a previously unrecognized limitation of the popular model\nexplanation method Grad-CAM: as a side effect of the gradient averaging step,\nGrad-CAM sometimes highlights the wrong location. To solve this problem, we\npropose HiResCAM, a novel label-specific attention mechanism that is provably\nguaranteed to highlight only the locations the model used to make each\nprediction. Next, we introduce a mask loss that leverages HiResCAM to encourage\nthe model to predict abnormalities based only on the organs in which those\nabnormalities appear. Our innovations produce a 37% improvement in weakly\nsupervised organ localization of multiple abnormalities in the RAD-ChestCT data\nset of 36,316 CT volumes, resulting in state-of-the-art performance. We also\ndemonstrate on PASCAL VOC 2012 the different properties of HiResCAM and\nGrad-CAM on natural images. Overall, this work advances convolutional neural\nnetwork explanation approaches and the clinical applicability of multiple\nabnormality modeling in volumetric medical images.\n", "versions": [{"version": "v1", "created": "Tue, 17 Nov 2020 19:26:14 GMT"}, {"version": "v2", "created": "Fri, 26 Mar 2021 00:07:20 GMT"}, {"version": "v3", "created": "Thu, 22 Apr 2021 19:46:33 GMT"}], "update_date": "2021-04-26", "authors_parsed": [["Draelos", "Rachel Lea", ""], ["Carin", "Lawrence", ""]]}, {"id": "2011.08894", "submitter": "Lihao Liu", "authors": "Lihao Liu, Angelica I Aviles-Rivero, Carola-Bibiane Sch\\\"onlieb", "title": "Contrastive Registration for Unsupervised Medical Image Segmentation", "comments": "11 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Medical image segmentation is a relevant task as it serves as the first step\nfor several diagnosis processes, thus it is indispensable in clinical usage.\nWhilst major success has been reported using supervised techniques, they assume\na large and well-representative labelled set. This is a strong assumption in\nthe medical domain where annotations are expensive, time-consuming, and\ninherent to human bias. To address this problem, unsupervised techniques have\nbeen proposed in the literature yet it is still an open problem due to the\ndifficulty of learning any transformation pattern. In this work, we present a\nnovel optimisation model framed into a new CNN-based contrastive registration\narchitecture for unsupervised medical image segmentation. The core of our\napproach is to exploit image-level registration and feature-level from a\ncontrastive learning mechanism, to perform registration-based segmentation.\nFirstly, we propose an architecture to capture the image-to-image\ntransformation pattern via registration for unsupervised medical image\nsegmentation. Secondly, we embed a contrastive learning mechanism into the\nregistration architecture to enhance the discriminating capacity of the network\nin the feature-level. We show that our proposed technique mitigates the major\ndrawbacks of existing unsupervised techniques. We demonstrate, through\nnumerical and visual experiments, that our technique substantially outperforms\nthe current state-of-the-art unsupervised segmentation methods on two major\nmedical image datasets.\n", "versions": [{"version": "v1", "created": "Tue, 17 Nov 2020 19:29:08 GMT"}, {"version": "v2", "created": "Wed, 17 Mar 2021 22:29:49 GMT"}], "update_date": "2021-03-19", "authors_parsed": [["Liu", "Lihao", ""], ["Aviles-Rivero", "Angelica I", ""], ["Sch\u00f6nlieb", "Carola-Bibiane", ""]]}, {"id": "2011.08898", "submitter": "Ali Almadan", "authors": "Anoop Krishnan, Ali Almadan, Ajita Rattani", "title": "Probing Fairness of Mobile Ocular Biometrics Methods Across Gender on\n  VISOB 2.0 Dataset", "comments": null, "journal-ref": "25th International Conference on Pattern Recognition (ICPR) 2020|\n  Milan, Italy", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent research has questioned the fairness of face-based recognition and\nattribute classification methods (such as gender and race) for dark-skinned\npeople and women. Ocular biometrics in the visible spectrum is an alternate\nsolution over face biometrics, thanks to its accuracy, security, robustness\nagainst facial expression, and ease of use in mobile devices. With the recent\nCOVID-19 crisis, ocular biometrics has a further advantage over face biometrics\nin the presence of a mask. However, fairness of ocular biometrics has not been\nstudied till now. This first study aims to explore the fairness of ocular-based\nauthentication and gender classification methods across males and females. To\nthis aim, VISOB $2.0$ dataset, along with its gender annotations, is used for\nthe fairness analysis of ocular biometrics methods based on ResNet-50,\nMobileNet-V2 and lightCNN-29 models. Experimental results suggest the\nequivalent performance of males and females for ocular-based mobile\nuser-authentication in terms of genuine match rate (GMR) at lower false match\nrates (FMRs) and an overall Area Under Curve (AUC). For instance, an AUC of\n0.96 for females and 0.95 for males was obtained for lightCNN-29 on an average.\nHowever, males significantly outperformed females in deep learning based gender\nclassification models based on ocular-region.\n", "versions": [{"version": "v1", "created": "Tue, 17 Nov 2020 19:32:56 GMT"}], "update_date": "2020-11-19", "authors_parsed": [["Krishnan", "Anoop", ""], ["Almadan", "Ali", ""], ["Rattani", "Ajita", ""]]}, {"id": "2011.08899", "submitter": "Frederik Pahde", "authors": "Frederik Pahde, Mihai Puscas, Tassilo Klein, Moin Nabi", "title": "Multimodal Prototypical Networks for Few-shot Learning", "comments": "To appear at WACV 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although providing exceptional results for many computer vision tasks,\nstate-of-the-art deep learning algorithms catastrophically struggle in low data\nscenarios. However, if data in additional modalities exist (e.g. text) this can\ncompensate for the lack of data and improve the classification results. To\novercome this data scarcity, we design a cross-modal feature generation\nframework capable of enriching the low populated embedding space in few-shot\nscenarios, leveraging data from the auxiliary modality. Specifically, we train\na generative model that maps text data into the visual feature space to obtain\nmore reliable prototypes. This allows to exploit data from additional\nmodalities (e.g. text) during training while the ultimate task at test time\nremains classification with exclusively visual data. We show that in such cases\nnearest neighbor classification is a viable approach and outperform\nstate-of-the-art single-modal and multimodal few-shot learning methods on the\nCUB-200 and Oxford-102 datasets.\n", "versions": [{"version": "v1", "created": "Tue, 17 Nov 2020 19:32:59 GMT"}], "update_date": "2020-11-19", "authors_parsed": [["Pahde", "Frederik", ""], ["Puscas", "Mihai", ""], ["Klein", "Tassilo", ""], ["Nabi", "Moin", ""]]}, {"id": "2011.08900", "submitter": "Satoshi Tsutsui", "authors": "Satoshi Tsutsui, Yanwei Fu, David Crandall", "title": "Whose hand is this? Person Identification from Egocentric Hand Gestures", "comments": "Accepted to IEEE Winter Conference on Applications of Computer Vision\n  (WACV) 2021 (First round acceptance)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recognizing people by faces and other biometrics has been extensively studied\nin computer vision. But these techniques do not work for identifying the wearer\nof an egocentric (first-person) camera because that person rarely (if ever)\nappears in their own first-person view. But while one's own face is not\nfrequently visible, their hands are: in fact, hands are among the most common\nobjects in one's own field of view. It is thus natural to ask whether the\nappearance and motion patterns of people's hands are distinctive enough to\nrecognize them. In this paper, we systematically study the possibility of\nEgocentric Hand Identification (EHI) with unconstrained egocentric hand\ngestures. We explore several different visual cues, including color, shape,\nskin texture, and depth maps to identify users' hands. Extensive ablation\nexperiments are conducted to analyze the properties of hands that are most\ndistinctive. Finally, we show that EHI can improve generalization of other\ntasks, such as gesture recognition, by training adversarially to encourage\nthese models to ignore differences between users.\n", "versions": [{"version": "v1", "created": "Tue, 17 Nov 2020 19:35:27 GMT"}], "update_date": "2020-11-19", "authors_parsed": [["Tsutsui", "Satoshi", ""], ["Fu", "Yanwei", ""], ["Crandall", "David", ""]]}, {"id": "2011.08916", "submitter": "Tanvirul Alam", "authors": "Firoj Alam, Ferda Ofli, Muhammad Imran, Tanvirul Alam and Umair Qazi", "title": "Deep Learning Benchmarks and Datasets for Social Media Image\n  Classification for Disaster Response", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  During a disaster event, images shared on social media helps crisis managers\ngain situational awareness and assess incurred damages, among other response\ntasks. Recent advances in computer vision and deep neural networks have enabled\nthe development of models for real-time image classification for a number of\ntasks, including detecting crisis incidents, filtering irrelevant images,\nclassifying images into specific humanitarian categories, and assessing the\nseverity of damage. Despite several efforts, past works mainly suffer from\nlimited resources (i.e., labeled images) available to train more robust deep\nlearning models. In this study, we propose new datasets for disaster type\ndetection, and informativeness classification, and damage severity assessment.\nMoreover, we relabel existing publicly available datasets for new tasks. We\nidentify exact- and near-duplicates to form non-overlapping data splits, and\nfinally consolidate them to create larger datasets. In our extensive\nexperiments, we benchmark several state-of-the-art deep learning models and\nachieve promising results. We release our datasets and models publicly, aiming\nto provide proper baselines as well as to spur further research in the crisis\ninformatics community.\n", "versions": [{"version": "v1", "created": "Tue, 17 Nov 2020 20:15:49 GMT"}], "update_date": "2020-11-19", "authors_parsed": [["Alam", "Firoj", ""], ["Ofli", "Ferda", ""], ["Imran", "Muhammad", ""], ["Alam", "Tanvirul", ""], ["Qazi", "Umair", ""]]}, {"id": "2011.08927", "submitter": "Arda Mavi", "authors": "Arda Mavi", "title": "A New Dataset and Proposed Convolutional Neural Network Architecture for\n  Classification of American Sign Language Digits", "comments": "5 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  According to interviews with people who work with speech impaired persons,\nspeech impaired people have difficulties in communicating with other people\naround them who do not know the sign language, and this situation may cause\nthem to isolate themselves from society and lose their sense of independence.\nWith this paper, to increase the quality of life of individuals with\nfacilitating communication between individuals who use sign language and who do\nnot know this language, a new American Sign Language (ASL) digits dataset that\ncan help to create machine learning algorithms which need to large and varied\ndata to be successful created and published as Sign Language Digits Dataset on\nKaggle Datasets web page, a proposal Convolutional Neural Network (CNN)\narchitecture that can get 98% test accuracy on our dataset presented, and\ncompared with the existing popular CNN models.\n", "versions": [{"version": "v1", "created": "Mon, 16 Nov 2020 18:32:22 GMT"}, {"version": "v2", "created": "Tue, 23 Feb 2021 14:28:09 GMT"}], "update_date": "2021-02-24", "authors_parsed": [["Mavi", "Arda", ""]]}, {"id": "2011.08932", "submitter": "Max Ehrlich", "authors": "Max Ehrlich, Larry Davis, Ser-Nam Lim, Abhinav Shrivastava", "title": "Analyzing and Mitigating Compression Defects in Deep Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the proliferation of deep learning methods, many computer vision\nproblems which were considered academic are now viable in the consumer setting.\nOne drawback of consumer applications is lossy compression, which is necessary\nfrom an engineering standpoint to efficiently and cheaply store and transmit\nuser images. Despite this, there has been little study of the effect of\ncompression on deep neural networks and benchmark datasets are often losslessly\ncompressed or compressed at high quality. Here we present a unified study of\nthe effects of JPEG compression on a range of common tasks and datasets. We\nshow that there is a significant penalty on common performance metrics for high\ncompression. We test several methods for mitigating this penalty, including a\nnovel method based on artifact correction which requires no labels to train.\n", "versions": [{"version": "v1", "created": "Tue, 17 Nov 2020 20:32:57 GMT"}], "update_date": "2020-11-19", "authors_parsed": [["Ehrlich", "Max", ""], ["Davis", "Larry", ""], ["Lim", "Ser-Nam", ""], ["Shrivastava", "Abhinav", ""]]}, {"id": "2011.08939", "submitter": "Bin Li", "authors": "Bin Li, Yin Li, Kevin W. Eliceiri", "title": "Dual-stream Multiple Instance Learning Network for Whole Slide Image\n  Classification with Self-supervised Contrastive Learning", "comments": "CVPR 2021, accepted for oral presentation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the challenging problem of whole slide image (WSI) classification.\nWSIs have very high resolutions and usually lack localized annotations. WSI\nclassification can be cast as a multiple instance learning (MIL) problem when\nonly slide-level labels are available. We propose a MIL-based method for WSI\nclassification and tumor detection that does not require localized annotations.\nOur method has three major components. First, we introduce a novel MIL\naggregator that models the relations of the instances in a dual-stream\narchitecture with trainable distance measurement. Second, since WSIs can\nproduce large or unbalanced bags that hinder the training of MIL models, we\npropose to use self-supervised contrastive learning to extract good\nrepresentations for MIL and alleviate the issue of prohibitive memory cost for\nlarge bags. Third, we adopt a pyramidal fusion mechanism for multiscale WSI\nfeatures, and further improve the accuracy of classification and localization.\nOur model is evaluated on two representative WSI datasets. The classification\naccuracy of our model compares favorably to fully-supervised methods, with less\nthan 2% accuracy gap across datasets. Our results also outperform all previous\nMIL-based methods. Additional benchmark results on standard MIL datasets\nfurther demonstrate the superior performance of our MIL aggregator on general\nMIL problems. GitHub repository: https://github.com/binli123/dsmil-wsi\n", "versions": [{"version": "v1", "created": "Tue, 17 Nov 2020 20:51:15 GMT"}, {"version": "v2", "created": "Mon, 29 Mar 2021 20:09:13 GMT"}, {"version": "v3", "created": "Fri, 2 Apr 2021 16:48:03 GMT"}], "update_date": "2021-04-05", "authors_parsed": [["Li", "Bin", ""], ["Li", "Yin", ""], ["Eliceiri", "Kevin W.", ""]]}, {"id": "2011.08961", "submitter": "Wei Yang", "authors": "Wei Yang, Chris Paxton, Arsalan Mousavian, Yu-Wei Chao, Maya Cakmak,\n  Dieter Fox", "title": "Reactive Human-to-Robot Handovers of Arbitrary Objects", "comments": "Accepted to the International Conference on Robotics and Automation\n  (ICRA) 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human-robot object handovers have been an actively studied area of robotics\nover the past decade; however, very few techniques and systems have addressed\nthe challenge of handing over diverse objects with arbitrary appearance, size,\nshape, and rigidity. In this paper, we present a vision-based system that\nenables reactive human-to-robot handovers of unknown objects. Our approach\ncombines closed-loop motion planning with real-time, temporally-consistent\ngrasp generation to ensure reactivity and motion smoothness. Our system is\nrobust to different object positions and orientations, and can grasp both rigid\nand non-rigid objects. We demonstrate the generalizability, usability, and\nrobustness of our approach on a novel benchmark set of 26 diverse household\nobjects, a user study with naive users (N=6) handing over a subset of 15\nobjects, and a systematic evaluation examining different ways of handing\nobjects. More results and videos can be found at\nhttps://sites.google.com/nvidia.com/handovers-of-arbitrary-objects.\n", "versions": [{"version": "v1", "created": "Tue, 17 Nov 2020 21:52:22 GMT"}, {"version": "v2", "created": "Thu, 3 Jun 2021 20:48:44 GMT"}], "update_date": "2021-06-07", "authors_parsed": [["Yang", "Wei", ""], ["Paxton", "Chris", ""], ["Mousavian", "Arsalan", ""], ["Chao", "Yu-Wei", ""], ["Cakmak", "Maya", ""], ["Fox", "Dieter", ""]]}, {"id": "2011.08965", "submitter": "Yun Liu", "authors": "Ellery Wulczyn, David F. Steiner, Melissa Moran, Markus Plass, Robert\n  Reihs, Fraser Tan, Isabelle Flament-Auvigne, Trissia Brown, Peter Regitnig,\n  Po-Hsuan Cameron Chen, Narayan Hegde, Apaar Sadhwani, Robert MacDonald, Benny\n  Ayalew, Greg S. Corrado, Lily H. Peng, Daniel Tse, Heimo M\\\"uller, Zhaoyang\n  Xu, Yun Liu, Martin C. Stumpe, Kurt Zatloukal, Craig H. Mermel", "title": "Interpretable Survival Prediction for Colorectal Cancer using Deep\n  Learning", "comments": null, "journal-ref": null, "doi": "10.1038/s41746-021-00427-2", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deriving interpretable prognostic features from deep-learning-based\nprognostic histopathology models remains a challenge. In this study, we\ndeveloped a deep learning system (DLS) for predicting disease specific survival\nfor stage II and III colorectal cancer using 3,652 cases (27,300 slides). When\nevaluated on two validation datasets containing 1,239 cases (9,340 slides) and\n738 cases (7,140 slides) respectively, the DLS achieved a 5-year\ndisease-specific survival AUC of 0.70 (95%CI 0.66-0.73) and 0.69 (95%CI\n0.64-0.72), and added significant predictive value to a set of 9\nclinicopathologic features. To interpret the DLS, we explored the ability of\ndifferent human-interpretable features to explain the variance in DLS scores.\nWe observed that clinicopathologic features such as T-category, N-category, and\ngrade explained a small fraction of the variance in DLS scores (R2=18% in both\nvalidation sets). Next, we generated human-interpretable histologic features by\nclustering embeddings from a deep-learning based image-similarity model and\nshowed that they explain the majority of the variance (R2 of 73% to 80%).\nFurthermore, the clustering-derived feature most strongly associated with high\nDLS scores was also highly prognostic in isolation. With a distinct visual\nappearance (poorly differentiated tumor cell clusters adjacent to adipose\ntissue), this feature was identified by annotators with 87.0-95.5% accuracy.\nOur approach can be used to explain predictions from a prognostic deep learning\nmodel and uncover potentially-novel prognostic features that can be reliably\nidentified by people for future validation studies.\n", "versions": [{"version": "v1", "created": "Tue, 17 Nov 2020 21:57:16 GMT"}], "update_date": "2021-05-13", "authors_parsed": [["Wulczyn", "Ellery", ""], ["Steiner", "David F.", ""], ["Moran", "Melissa", ""], ["Plass", "Markus", ""], ["Reihs", "Robert", ""], ["Tan", "Fraser", ""], ["Flament-Auvigne", "Isabelle", ""], ["Brown", "Trissia", ""], ["Regitnig", "Peter", ""], ["Chen", "Po-Hsuan Cameron", ""], ["Hegde", "Narayan", ""], ["Sadhwani", "Apaar", ""], ["MacDonald", "Robert", ""], ["Ayalew", "Benny", ""], ["Corrado", "Greg S.", ""], ["Peng", "Lily H.", ""], ["Tse", "Daniel", ""], ["M\u00fcller", "Heimo", ""], ["Xu", "Zhaoyang", ""], ["Liu", "Yun", ""], ["Stumpe", "Martin C.", ""], ["Zatloukal", "Kurt", ""], ["Mermel", "Craig H.", ""]]}, {"id": "2011.08981", "submitter": "Xiangyu Gao", "authors": "Xiangyu Gao, Guanbin Xing, Sumit Roy, and Hui Liu", "title": "RAMP-CNN: A Novel Neural Network for Enhanced Automotive Radar Object\n  Recognition", "comments": "15 pages", "journal-ref": "IEEE Sensor Journal, 2020", "doi": "10.1109/JSEN.2020.3036047", "report-no": null, "categories": "eess.SP cs.AI cs.CV cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Millimeter-wave (mmW) radars are being increasingly integrated into\ncommercial vehicles to support new advanced driver-assistance systems (ADAS) by\nenabling robust and high-performance object detection, localization, as well as\nrecognition - a key component of new environmental perception. In this paper,\nwe propose a novel radar multiple-perspectives convolutional neural network\n(RAMP-CNN) that extracts the location and class of objects based on further\nprocessing of the range-velocity-angle (RVA) heatmap sequences. To bypass the\ncomplexity of 4D convolutional neural networks (NN), we propose to combine\nseveral lower-dimension NN models within our RAMP-CNN model that nonetheless\napproaches the performance upper-bound with lower complexity. The extensive\nexperiments show that the proposed RAMP-CNN model achieves better average\nrecall (AR) and average precision (AP) than prior works in all testing\nscenarios (see Table. III). Besides, the RAMP-CNN model is validated to work\nrobustly under the nighttime, which enables low-cost radars as a potential\nsubstitute for pure optical sensing under severe conditions.\n", "versions": [{"version": "v1", "created": "Fri, 13 Nov 2020 19:12:12 GMT"}], "update_date": "2021-01-27", "authors_parsed": [["Gao", "Xiangyu", ""], ["Xing", "Guanbin", ""], ["Roy", "Sumit", ""], ["Liu", "Hui", ""]]}, {"id": "2011.08988", "submitter": "Yaroslava Lochman", "authors": "Yaroslava Lochman, Oles Dobosevych, Rostyslav Hryniv, James Pritts", "title": "Minimal Solvers for Single-View Lens-Distorted Camera Auto-Calibration", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes minimal solvers that use combinations of imaged\ntranslational symmetries and parallel scene lines to jointly estimate lens\nundistortion with either affine rectification or focal length and absolute\norientation. We use constraints provided by orthogonal scene planes to recover\nthe focal length. We show that solvers using feature combinations can recover\nmore accurate calibrations than solvers using only one feature type on scenes\nthat have a balance of lines and texture. We also show that the proposed\nsolvers are complementary and can be used together in a RANSAC-based estimator\nto improve auto-calibration accuracy. State-of-the-art performance is\ndemonstrated on a standard dataset of lens-distorted urban images. The code is\navailable at https://github.com/ylochman/single-view-autocalib.\n", "versions": [{"version": "v1", "created": "Tue, 17 Nov 2020 22:32:17 GMT"}], "update_date": "2020-11-19", "authors_parsed": [["Lochman", "Yaroslava", ""], ["Dobosevych", "Oles", ""], ["Hryniv", "Rostyslav", ""], ["Pritts", "James", ""]]}, {"id": "2011.08992", "submitter": "Jayant Gupta", "authors": "Jayant Gupta (1), Yiqun Xie (1) and Shashi Shekhar (1) ((1) University\n  of Minnesota)", "title": "Towards Spatial Variability Aware Deep Neural Networks (SVANN): A\n  Summary of Results", "comments": "Accepted in 1st ACM SIGKDD Workshop on Deep Learning for\n  Spatiotemporal Data, Applications, and Systems (Deepspatial 2020), San Diego,\n  CA, August 24, 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Spatial variability has been observed in many geo-phenomena including\nclimatic zones, USDA plant hardiness zones, and terrestrial habitat types\n(e.g., forest, grasslands, wetlands, and deserts). However, current deep\nlearning methods follow a spatial-one-size-fits-all(OSFA) approach to train\nsingle deep neural network models that do not account for spatial variability.\nIn this work, we propose and investigate a spatial-variability aware deep\nneural network(SVANN) approach, where distinct deep neural network models are\nbuilt for each geographic area. We evaluate this approach using aerial imagery\nfrom two geographic areas for the task of mapping urban gardens. The\nexperimental results show that SVANN provides better performance than OSFA in\nterms of precision, recall,and F1-score to identify urban gardens.\n", "versions": [{"version": "v1", "created": "Tue, 17 Nov 2020 22:52:40 GMT"}], "update_date": "2020-11-19", "authors_parsed": [["Gupta", "Jayant", ""], ["Xie", "Yiqun", ""], ["Shekhar", "Shashi", ""]]}, {"id": "2011.09011", "submitter": "Dilin Wang", "authors": "Dilin Wang, Meng Li, Chengyue Gong, Vikas Chandra", "title": "AttentiveNAS: Improving Neural Architecture Search via Attentive\n  Sampling", "comments": "2021 Conference on Computer Vision and Pattern Recognition", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Neural architecture search (NAS) has shown great promise in designing\nstate-of-the-art (SOTA) models that are both accurate and efficient. Recently,\ntwo-stage NAS, e.g. BigNAS, decouples the model training and searching process\nand achieves remarkable search efficiency and accuracy. Two-stage NAS requires\nsampling from the search space during training, which directly impacts the\naccuracy of the final searched models. While uniform sampling has been widely\nused for its simplicity, it is agnostic of the model performance Pareto front,\nwhich is the main focus in the search process, and thus, misses opportunities\nto further improve the model accuracy. In this work, we propose AttentiveNAS\nthat focuses on improving the sampling strategy to achieve better performance\nPareto. We also propose algorithms to efficiently and effectively identify the\nnetworks on the Pareto during training. Without extra re-training or\npost-processing, we can simultaneously obtain a large number of networks across\na wide range of FLOPs. Our discovered model family, AttentiveNAS models,\nachieves top-1 accuracy from 77.3% to 80.7% on ImageNet, and outperforms SOTA\nmodels, including BigNAS and Once-for-All networks. We also achieve ImageNet\naccuracy of 80.1% with only 491 MFLOPs. Our training code and pretrained models\nare available at https://github.com/facebookresearch/AttentiveNAS.\n", "versions": [{"version": "v1", "created": "Wed, 18 Nov 2020 00:15:23 GMT"}, {"version": "v2", "created": "Tue, 13 Apr 2021 19:17:16 GMT"}], "update_date": "2021-04-15", "authors_parsed": [["Wang", "Dilin", ""], ["Li", "Meng", ""], ["Gong", "Chengyue", ""], ["Chandra", "Vikas", ""]]}, {"id": "2011.09017", "submitter": "Dingwen Tao", "authors": "Sian Jin, Guanpeng Li, Shuaiwen Leon Song, Dingwen Tao", "title": "A Novel Memory-Efficient Deep Learning Training Framework via\n  Error-Bounded Lossy Compression", "comments": "11 pages, 11 figures, 1 table, accepted by PPoPP '21 as a poster", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Deep neural networks (DNNs) are becoming increasingly deeper, wider, and\nnon-linear due to the growing demands on prediction accuracy and analysis\nquality. When training a DNN model, the intermediate activation data must be\nsaved in the memory during forward propagation and then restored for backward\npropagation. However, state-of-the-art accelerators such as GPUs are only\nequipped with very limited memory capacities due to hardware design\nconstraints, which significantly limits the maximum batch size and hence\nperformance speedup when training large-scale DNNs.\n  In this paper, we propose a novel memory-driven high performance DNN training\nframework that leverages error-bounded lossy compression to significantly\nreduce the memory requirement for training in order to allow training larger\nnetworks. Different from the state-of-the-art solutions that adopt image-based\nlossy compressors such as JPEG to compress the activation data, our framework\npurposely designs error-bounded lossy compression with a strict\nerror-controlling mechanism. Specifically, we provide theoretical analysis on\nthe compression error propagation from the altered activation data to the\ngradients, and then empirically investigate the impact of altered gradients\nover the entire training process. Based on these analyses, we then propose an\nimproved lossy compressor and an adaptive scheme to dynamically configure the\nlossy compression error-bound and adjust the training batch size to further\nutilize the saved memory space for additional speedup. We evaluate our design\nagainst state-of-the-art solutions with four popular DNNs and the ImageNet\ndataset. Results demonstrate that our proposed framework can significantly\nreduce the training memory consumption by up to 13.5x and 1.8x over the\nbaseline training and state-of-the-art framework with compression,\nrespectively, with little or no accuracy loss.\n", "versions": [{"version": "v1", "created": "Wed, 18 Nov 2020 00:47:21 GMT"}], "update_date": "2020-11-24", "authors_parsed": [["Jin", "Sian", ""], ["Li", "Guanpeng", ""], ["Song", "Shuaiwen Leon", ""], ["Tao", "Dingwen", ""]]}, {"id": "2011.09023", "submitter": "He Dai", "authors": "He Dai, Xuchong Zhang, Yongli Zhao, Hongbin Sun", "title": "ADCPNet: Adaptive Disparity Candidates Prediction Network for Efficient\n  Real-Time Stereo Matching", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Efficient real-time disparity estimation is critical for the application of\nstereo vision systems in various areas. Recently, stereo network based on\ncoarse-to-fine method has largely relieved the memory constraints and speed\nlimitations of large-scale network models. Nevertheless, all of the previous\ncoarse-to-fine designs employ constant offsets and three or more stages to\nprogressively refine the coarse disparity map, still resulting in\nunsatisfactory computation accuracy and inference time when deployed on mobile\ndevices. This paper claims that the coarse matching errors can be corrected\nefficiently with fewer stages as long as more accurate disparity candidates can\nbe provided. Therefore, we propose a dynamic offset prediction module to meet\ndifferent correction requirements of diverse objects and design an efficient\ntwo-stage framework. Besides, we propose a disparity-independent convolution to\nfurther improve the performance since it is more consistent with the local\nstatistical characteristics of the compact cost volume. The evaluation results\non multiple datasets and platforms clearly demonstrate that, the proposed\nnetwork outperforms the state-of-the-art lightweight models especially for\nmobile devices in terms of accuracy and speed. Code will be made available.\n", "versions": [{"version": "v1", "created": "Wed, 18 Nov 2020 01:18:52 GMT"}], "update_date": "2020-11-19", "authors_parsed": [["Dai", "He", ""], ["Zhang", "Xuchong", ""], ["Zhao", "Yongli", ""], ["Sun", "Hongbin", ""]]}, {"id": "2011.09040", "submitter": "Dongliang Chang", "authors": "Dongliang Chang, Kaiyue Pang, Yixiao Zheng, Zhanyu Ma, Yi-Zhe Song,\n  and Jun Guo", "title": "Your \"Flamingo\" is My \"Bird\": Fine-Grained, or Not", "comments": "Accepted as an oral of CVPR2021. Code:\n  https://github.com/PRIS-CV/Fine-Grained-or-Not", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Whether what you see in Figure 1 is a \"flamingo\" or a \"bird\", is the question\nwe ask in this paper. While fine-grained visual classification (FGVC) strives\nto arrive at the former, for the majority of us non-experts just \"bird\" would\nprobably suffice. The real question is therefore -- how can we tailor for\ndifferent fine-grained definitions under divergent levels of expertise. For\nthat, we re-envisage the traditional setting of FGVC, from single-label\nclassification, to that of top-down traversal of a pre-defined coarse-to-fine\nlabel hierarchy -- so that our answer becomes\n\"bird\"-->\"Phoenicopteriformes\"-->\"Phoenicopteridae\"-->\"flamingo\". To approach\nthis new problem, we first conduct a comprehensive human study where we confirm\nthat most participants prefer multi-granularity labels, regardless whether they\nconsider themselves experts. We then discover the key intuition that:\ncoarse-level label prediction exacerbates fine-grained feature learning, yet\nfine-level feature betters the learning of coarse-level classifier. This\ndiscovery enables us to design a very simple albeit surprisingly effective\nsolution to our new problem, where we (i) leverage level-specific\nclassification heads to disentangle coarse-level features with fine-grained\nones, and (ii) allow finer-grained features to participate in coarser-grained\nlabel predictions, which in turn helps with better disentanglement. Experiments\nshow that our method achieves superior performance in the new FGVC setting, and\nperforms better than state-of-the-art on traditional single-label FGVC problem\nas well. Thanks to its simplicity, our method can be easily implemented on top\nof any existing FGVC frameworks and is parameter-free.\n", "versions": [{"version": "v1", "created": "Wed, 18 Nov 2020 02:24:54 GMT"}, {"version": "v2", "created": "Wed, 17 Feb 2021 03:50:32 GMT"}, {"version": "v3", "created": "Sun, 28 Mar 2021 06:17:51 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Chang", "Dongliang", ""], ["Pang", "Kaiyue", ""], ["Zheng", "Yixiao", ""], ["Ma", "Zhanyu", ""], ["Song", "Yi-Zhe", ""], ["Guo", "Jun", ""]]}, {"id": "2011.09041", "submitter": "Charley Gros", "authors": "Charley Gros, Andreanne Lemay, Julien Cohen-Adad", "title": "SoftSeg: Advantages of soft versus binary training for image\n  segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Most image segmentation algorithms are trained on binary masks formulated as\na classification task per pixel. However, in applications such as medical\nimaging, this \"black-and-white\" approach is too constraining because the\ncontrast between two tissues is often ill-defined, i.e., the voxels located on\nobjects' edges contain a mixture of tissues. Consequently, assigning a single\n\"hard\" label can result in a detrimental approximation. Instead, a soft\nprediction containing non-binary values would overcome that limitation. We\nintroduce SoftSeg, a deep learning training approach that takes advantage of\nsoft ground truth labels, and is not bound to binary predictions. SoftSeg aims\nat solving a regression instead of a classification problem. This is achieved\nby using (i) no binarization after preprocessing and data augmentation, (ii) a\nnormalized ReLU final activation layer (instead of sigmoid), and (iii) a\nregression loss function (instead of the traditional Dice loss). We assess the\nimpact of these three features on three open-source MRI segmentation datasets\nfrom the spinal cord gray matter, the multiple sclerosis brain lesion, and the\nmultimodal brain tumor segmentation challenges. Across multiple\ncross-validation iterations, SoftSeg outperformed the conventional approach,\nleading to an increase in Dice score of 2.0% on the gray matter dataset\n(p=0.001), 3.3% for the MS lesions, and 6.5% for the brain tumors. SoftSeg\nproduces consistent soft predictions at tissues' interfaces and shows an\nincreased sensitivity for small objects. The richness of soft labels could\nrepresent the inter-expert variability, the partial volume effect, and\ncomplement the model uncertainty estimation. The developed training pipeline\ncan easily be incorporated into most of the existing deep learning\narchitectures. It is already implemented in the freely-available deep learning\ntoolbox ivadomed (https://ivadomed.org).\n", "versions": [{"version": "v1", "created": "Wed, 18 Nov 2020 02:25:09 GMT"}], "update_date": "2020-11-19", "authors_parsed": [["Gros", "Charley", ""], ["Lemay", "Andreanne", ""], ["Cohen-Adad", "Julien", ""]]}, {"id": "2011.09046", "submitter": "Bowen Zhang", "authors": "Bowen Zhang, Hexiang Hu, Joonseok Lee, Ming Zhao, Sheide Chammas,\n  Vihan Jain, Eugene Ie, Fei Sha", "title": "A Hierarchical Multi-Modal Encoder for Moment Localization in Video\n  Corpus", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Identifying a short segment in a long video that semantically matches a text\nquery is a challenging task that has important application potentials in\nlanguage-based video search, browsing, and navigation. Typical retrieval\nsystems respond to a query with either a whole video or a pre-defined video\nsegment, but it is challenging to localize undefined segments in untrimmed and\nunsegmented videos where exhaustively searching over all possible segments is\nintractable. The outstanding challenge is that the representation of a video\nmust account for different levels of granularity in the temporal domain. To\ntackle this problem, we propose the HierArchical Multi-Modal EncodeR (HAMMER)\nthat encodes a video at both the coarse-grained clip level and the fine-grained\nframe level to extract information at different scales based on multiple\nsubtasks, namely, video retrieval, segment temporal localization, and masked\nlanguage modeling. We conduct extensive experiments to evaluate our model on\nmoment localization in video corpus on ActivityNet Captions and TVR datasets.\nOur approach outperforms the previous methods as well as strong baselines,\nestablishing new state-of-the-art for this task.\n", "versions": [{"version": "v1", "created": "Wed, 18 Nov 2020 02:42:36 GMT"}, {"version": "v2", "created": "Tue, 24 Nov 2020 04:11:13 GMT"}], "update_date": "2020-11-25", "authors_parsed": [["Zhang", "Bowen", ""], ["Hu", "Hexiang", ""], ["Lee", "Joonseok", ""], ["Zhao", "Ming", ""], ["Chammas", "Sheide", ""], ["Jain", "Vihan", ""], ["Ie", "Eugene", ""], ["Sha", "Fei", ""]]}, {"id": "2011.09052", "submitter": "Naftali Cohen", "authors": "Naftali Cohen, Srijan Sood, Zhen Zeng, Tucker Balch, and Manuela\n  Veloso", "title": "Visual Forecasting of Time Series with Image-to-Image Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG econ.EM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Time series forecasting is essential for agents to make decisions in many\ndomains. Existing models rely on classical statistical methods to predict\nfuture values based on previously observed numerical information. Yet,\npractitioners often rely on visualizations such as charts and plots to reason\nabout their predictions. Inspired by the end-users, we re-imagine the topic by\ncreating a framework to produce visual forecasts, similar to the way humans\nintuitively do. In this work, we take a novel approach by leveraging advances\nin deep learning to extend the field of time series forecasting to a visual\nsetting. We do this by transforming the numerical analysis problem into the\ncomputer vision domain. Using visualizations of time series data as input, we\ntrain a convolutional autoencoder to produce corresponding visual forecasts. We\nexamine various synthetic and real datasets with diverse degrees of complexity.\nOur experiments show that visual forecasting is effective for cyclic data but\nsomewhat less for irregular data such as stock price. Importantly, we find the\nproposed visual forecasting method to outperform numerical baselines. We\nattribute the success of the visual forecasting approach to the fact that we\nconvert the continuous numerical regression problem into a discrete domain with\nquantization of the continuous target signal into pixel space.\n", "versions": [{"version": "v1", "created": "Wed, 18 Nov 2020 02:51:37 GMT"}], "update_date": "2020-11-19", "authors_parsed": [["Cohen", "Naftali", ""], ["Sood", "Srijan", ""], ["Zeng", "Zhen", ""], ["Balch", "Tucker", ""], ["Veloso", "Manuela", ""]]}, {"id": "2011.09055", "submitter": "Wen Liu", "authors": "Wen Liu, Zhixin Piao, Zhi Tu, Wenhan Luo, Lin Ma and Shenghua Gao", "title": "Liquid Warping GAN with Attention: A Unified Framework for Human Image\n  Synthesis", "comments": "Under review of IEEE Transactions on Pattern Analysis and Machine\n  Intelligence. arXiv admin note: text overlap with arXiv:1909.12224", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We tackle human image synthesis, including human motion imitation, appearance\ntransfer, and novel view synthesis, within a unified framework. It means that\nthe model, once being trained, can be used to handle all these tasks. The\nexisting task-specific methods mainly use 2D keypoints to estimate the human\nbody structure. However, they only express the position information with no\nabilities to characterize the personalized shape of the person and model the\nlimb rotations. In this paper, we propose to use a 3D body mesh recovery module\nto disentangle the pose and shape. It can not only model the joint location and\nrotation but also characterize the personalized body shape. To preserve the\nsource information, such as texture, style, color, and face identity, we\npropose an Attentional Liquid Warping GAN with Attentional Liquid Warping Block\n(AttLWB) that propagates the source information in both image and feature\nspaces to the synthesized reference. Specifically, the source features are\nextracted by a denoising convolutional auto-encoder for characterizing the\nsource identity well. Furthermore, our proposed method can support a more\nflexible warping from multiple sources. To further improve the generalization\nability of the unseen source images, a one/few-shot adversarial learning is\napplied. In detail, it firstly trains a model in an extensive training set.\nThen, it finetunes the model by one/few-shot unseen image(s) in a\nself-supervised way to generate high-resolution (512 x 512 and 1024 x 1024)\nresults. Also, we build a new dataset, namely iPER dataset, for the evaluation\nof human motion imitation, appearance transfer, and novel view synthesis.\nExtensive experiments demonstrate the effectiveness of our methods in terms of\npreserving face identity, shape consistency, and clothes details. All codes and\ndataset are available on\nhttps://impersonator.org/work/impersonator-plus-plus.html.\n", "versions": [{"version": "v1", "created": "Wed, 18 Nov 2020 02:57:47 GMT"}, {"version": "v2", "created": "Mon, 23 Nov 2020 04:50:43 GMT"}], "update_date": "2020-11-24", "authors_parsed": [["Liu", "Wen", ""], ["Piao", "Zhixin", ""], ["Tu", "Zhi", ""], ["Luo", "Wenhan", ""], ["Ma", "Lin", ""], ["Gao", "Shenghua", ""]]}, {"id": "2011.09058", "submitter": "Maxwell Horton", "authors": "Maxwell Horton, Yanzi Jin, Ali Farhadi, Mohammad Rastegari", "title": "Layer-Wise Data-Free CNN Compression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a computationally efficient method for compressing a trained\nneural network without using any data. We break the problem of data-free\nnetwork compression into independent layer-wise compressions. We show how to\nefficiently generate layer-wise training data, and how to precondition the\nnetwork to maintain accuracy during layer-wise compression. Our generic\ntechnique can be used with any compression method. We outperform related works\nfor data-free low-bit-width quantization on MobileNetV1, MobileNetV2, and\nResNet18. We also demonstrate the efficacy of our layer-wise method when\napplied to pruning. We outperform baselines in the low-computation regime\nsuitable for on-device edge compression while using orders of magnitude less\nmemory and compute time than comparable generative methods. In the\nhigh-computation regime, we show how to combine our method with generative\nmethods to improve upon state-of-the-art performance for several networks.\n", "versions": [{"version": "v1", "created": "Wed, 18 Nov 2020 03:00:05 GMT"}, {"version": "v2", "created": "Thu, 25 Mar 2021 17:31:11 GMT"}], "update_date": "2021-03-26", "authors_parsed": [["Horton", "Maxwell", ""], ["Jin", "Yanzi", ""], ["Farhadi", "Ali", ""], ["Rastegari", "Mohammad", ""]]}, {"id": "2011.09065", "submitter": "Alexander J. Nettekoven", "authors": "Alexander Nettekoven, Scott Fish, Joseph Beaman, Ufuk Topcu", "title": "Towards Online Monitoring and Data-driven Control: A Study of\n  Segmentation Algorithms for Laser Powder Bed Fusion Processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An increasing number of laser powder bed fusion machines use off-axis\ninfrared cameras to improve online monitoring and data-driven control\ncapabilities. However, there is still a severe lack of algorithmic solutions to\nproperly process the infrared images from these cameras that has led to several\nkey limitations: a lack of online monitoring capabilities for the laser tracks,\ninsufficient pre-processing of the infrared images for data-driven methods, and\nlarge memory requirements for storing the infrared images. To address these\nlimitations, we study over 30 segmentation algorithms that segment each\ninfrared image into a foreground and background. By evaluating each algorithm\nbased on its segmentation accuracy, computational speed, and spatter detection\ncharacteristics, we identify promising algorithmic solutions. The identified\nalgorithms can be readily applied to the laser powder bed fusion machines to\naddress each of the above limitations and thus, significantly improve process\ncontrol.\n", "versions": [{"version": "v1", "created": "Wed, 18 Nov 2020 03:30:16 GMT"}, {"version": "v2", "created": "Thu, 10 Jun 2021 19:49:10 GMT"}], "update_date": "2021-06-14", "authors_parsed": [["Nettekoven", "Alexander", ""], ["Fish", "Scott", ""], ["Beaman", "Joseph", ""], ["Topcu", "Ufuk", ""]]}, {"id": "2011.09066", "submitter": "Weitao Wan", "authors": "Weitao Wan, Jiansheng Chen, Cheng Yu, Tong Wu, Yuanyi Zhong,\n  Ming-Hsuan Yang", "title": "Shaping Deep Feature Space towards Gaussian Mixture for Visual\n  Classification", "comments": "Under review of TPAMI", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The softmax cross-entropy loss function has been widely used to train deep\nmodels for various tasks. In this work, we propose a Gaussian mixture (GM) loss\nfunction for deep neural networks for visual classification. Unlike the softmax\ncross-entropy loss, our method explicitly shapes the deep feature space towards\na Gaussian Mixture distribution. With a classification margin and a likelihood\nregularization, the GM loss facilitates both high classification performance\nand accurate modeling of the feature distribution. The GM loss can be readily\nused to distinguish abnormal inputs, such as the adversarial examples, based on\nthe discrepancy between feature distributions of the inputs and the training\nset. Furthermore, theoretical analysis shows that a symmetric feature space can\nbe achieved by using the GM loss, which enables the models to perform robustly\nagainst adversarial attacks. The proposed model can be implemented easily and\nefficiently without using extra trainable parameters. Extensive evaluations\ndemonstrate that the proposed method performs favorably not only on image\nclassification but also on robust detection of adversarial examples generated\nby strong attacks under different threat models.\n", "versions": [{"version": "v1", "created": "Wed, 18 Nov 2020 03:32:27 GMT"}], "update_date": "2020-11-19", "authors_parsed": [["Wan", "Weitao", ""], ["Chen", "Jiansheng", ""], ["Yu", "Cheng", ""], ["Wu", "Tong", ""], ["Zhong", "Yuanyi", ""], ["Yang", "Ming-Hsuan", ""]]}, {"id": "2011.09080", "submitter": "Ruixuan Yu", "authors": "Ruixuan Yu, Xin Wei, Federico Tombari, and Jian Sun", "title": "Deep Positional and Relational Feature Learning for Rotation-Invariant\n  Point Cloud Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose a rotation-invariant deep network for point clouds\nanalysis. Point-based deep networks are commonly designed to recognize roughly\naligned 3D shapes based on point coordinates, but suffer from performance drops\nwith shape rotations. Some geometric features, e.g., distances and angles of\npoints as inputs of network, are rotation-invariant but lose positional\ninformation of points. In this work, we propose a novel deep network for point\nclouds by incorporating positional information of points as inputs while\nyielding rotation-invariance. The network is hierarchical and relies on two\nmodules: a positional feature embedding block and a relational feature\nembedding block. Both modules and the whole network are proven to be\nrotation-invariant when processing point clouds as input. Experiments show\nstate-of-the-art classification and segmentation performances on benchmark\ndatasets, and ablation studies demonstrate effectiveness of the network design.\n", "versions": [{"version": "v1", "created": "Wed, 18 Nov 2020 04:16:51 GMT"}], "update_date": "2020-11-19", "authors_parsed": [["Yu", "Ruixuan", ""], ["Wei", "Xin", ""], ["Tombari", "Federico", ""], ["Sun", "Jian", ""]]}, {"id": "2011.09084", "submitter": "Tianxiang Ma", "authors": "Tianxiang Ma, Bo Peng, Wei Wang, Jing Dong", "title": "MUST-GAN: Multi-level Statistics Transfer for Self-driven Person Image\n  Generation", "comments": "Accepted by CVPR2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pose-guided person image generation usually involves using paired\nsource-target images to supervise the training, which significantly increases\nthe data preparation effort and limits the application of the models. To deal\nwith this problem, we propose a novel multi-level statistics transfer model,\nwhich disentangles and transfers multi-level appearance features from person\nimages and merges them with pose features to reconstruct the source person\nimages themselves. So that the source images can be used as supervision for\nself-driven person image generation. Specifically, our model extracts\nmulti-level features from the appearance encoder and learns the optimal\nappearance representation through attention mechanism and attributes\nstatistics. Then we transfer them to a pose-guided generator for re-fusion of\nappearance and pose. Our approach allows for flexible manipulation of person\nappearance and pose properties to perform pose transfer and clothes style\ntransfer tasks. Experimental results on the DeepFashion dataset demonstrate our\nmethod's superiority compared with state-of-the-art supervised and unsupervised\nmethods. In addition, our approach also performs well in the wild.\n", "versions": [{"version": "v1", "created": "Wed, 18 Nov 2020 04:38:48 GMT"}, {"version": "v2", "created": "Mon, 23 Nov 2020 02:03:31 GMT"}, {"version": "v3", "created": "Fri, 9 Apr 2021 02:22:46 GMT"}], "update_date": "2021-04-12", "authors_parsed": [["Ma", "Tianxiang", ""], ["Peng", "Bo", ""], ["Wang", "Wei", ""], ["Dong", "Jing", ""]]}, {"id": "2011.09094", "submitter": "Bolun Cai", "authors": "Zhigang Dai, Bolun Cai, Yugeng Lin, Junying Chen", "title": "UP-DETR: Unsupervised Pre-training for Object Detection with\n  Transformers", "comments": "Accepted by CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Object detection with transformers (DETR) reaches competitive performance\nwith Faster R-CNN via a transformer encoder-decoder architecture. Inspired by\nthe great success of pre-training transformers in natural language processing,\nwe propose a pretext task named random query patch detection to Unsupervisedly\nPre-train DETR (UP-DETR) for object detection. Specifically, we randomly crop\npatches from the given image and then feed them as queries to the decoder. The\nmodel is pre-trained to detect these query patches from the original image.\nDuring the pre-training, we address two critical issues: multi-task learning\nand multi-query localization. (1) To trade off classification and localization\npreferences in the pretext task, we freeze the CNN backbone and propose a patch\nfeature reconstruction branch which is jointly optimized with patch detection.\n(2) To perform multi-query localization, we introduce UP-DETR from single-query\npatch and extend it to multi-query patches with object query shuffle and\nattention mask. In our experiments, UP-DETR significantly boosts the\nperformance of DETR with faster convergence and higher average precision on\nobject detection, one-shot detection and panoptic segmentation. Code and\npre-training models: https://github.com/dddzg/up-detr.\n", "versions": [{"version": "v1", "created": "Wed, 18 Nov 2020 05:16:11 GMT"}, {"version": "v2", "created": "Wed, 7 Apr 2021 15:15:49 GMT"}], "update_date": "2021-04-08", "authors_parsed": [["Dai", "Zhigang", ""], ["Cai", "Bolun", ""], ["Lin", "Yugeng", ""], ["Chen", "Junying", ""]]}, {"id": "2011.09099", "submitter": "Chenglong Li", "authors": "Aihua Zheng, Xia Sun, Chenglong Li, Jin Tang", "title": "Viewpoint-aware Progressive Clustering for Unsupervised Vehicle\n  Re-identification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Vehicle re-identification (Re-ID) is an active task due to its importance in\nlarge-scale intelligent monitoring in smart cities. Despite the rapid progress\nin recent years, most existing methods handle vehicle Re-ID task in a\nsupervised manner, which is both time and labor-consuming and limits their\napplication to real-life scenarios. Recently, unsupervised person Re-ID methods\nachieve impressive performance by exploring domain adaption or clustering-based\ntechniques. However, one cannot directly generalize these methods to vehicle\nRe-ID since vehicle images present huge appearance variations in different\nviewpoints. To handle this problem, we propose a novel viewpoint-aware\nclustering algorithm for unsupervised vehicle Re-ID. In particular, we first\ndivide the entire feature space into different subspaces according to the\npredicted viewpoints and then perform a progressive clustering to mine the\naccurate relationship among samples. Comprehensive experiments against the\nstate-of-the-art methods on two multi-viewpoint benchmark datasets VeRi and\nVeRi-Wild validate the promising performance of the proposed method in both\nwith and without domain adaption scenarios while handling unsupervised vehicle\nRe-ID.\n", "versions": [{"version": "v1", "created": "Wed, 18 Nov 2020 05:40:14 GMT"}], "update_date": "2020-11-19", "authors_parsed": [["Zheng", "Aihua", ""], ["Sun", "Xia", ""], ["Li", "Chenglong", ""], ["Tang", "Jin", ""]]}, {"id": "2011.09104", "submitter": "Arbish Akram", "authors": "Nazar Khan, Arbish Akram, Arif Mahmood, Sania Ashraf, Kashif Murtaza", "title": "Masked Linear Regression for Learning Local Receptive Fields for Facial\n  Expression Synthesis", "comments": "IJCV Journal", "journal-ref": "International Journal of Computer Vision, vol 128, no 5, 2020", "doi": "10.1007/s11263-019-01256-3", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Compared to facial expression recognition, expression synthesis requires a\nvery high-dimensional mapping. This problem exacerbates with increasing image\nsizes and limits existing expression synthesis approaches to relatively small\nimages. We observe that facial expressions often constitute sparsely\ndistributed and locally correlated changes from one expression to another. By\nexploiting this observation, the number of parameters in an expression\nsynthesis model can be significantly reduced. Therefore, we propose a\nconstrained version of ridge regression that exploits the local and sparse\nstructure of facial expressions. We consider this model as masked regression\nfor learning local receptive fields. In contrast to the existing approaches,\nour proposed model can be efficiently trained on larger image sizes.\nExperiments using three publicly available datasets demonstrate that our model\nis significantly better than $\\ell_0, \\ell_1$ and $\\ell_2$-regression, SVD\nbased approaches, and kernelized regression in terms of mean-squared-error,\nvisual quality as well as computational and spatial complexities. The reduction\nin the number of parameters allows our method to generalize better even after\ntraining on smaller datasets. The proposed algorithm is also compared with\nstate-of-the-art GANs including Pix2Pix, CycleGAN, StarGAN and GANimation.\nThese GANs produce photo-realistic results as long as the testing and the\ntraining distributions are similar. In contrast, our results demonstrate\nsignificant generalization of the proposed algorithm over out-of-dataset human\nphotographs, pencil sketches and even animal faces.\n", "versions": [{"version": "v1", "created": "Wed, 18 Nov 2020 06:04:24 GMT"}], "update_date": "2020-11-19", "authors_parsed": [["Khan", "Nazar", ""], ["Akram", "Arbish", ""], ["Mahmood", "Arif", ""], ["Ashraf", "Sania", ""], ["Murtaza", "Kashif", ""]]}, {"id": "2011.09113", "submitter": "Gaurav Kumar Nayak", "authors": "Gaurav Kumar Nayak, Konda Reddy Mopuri, Anirban Chakraborty", "title": "Effectiveness of Arbitrary Transfer Sets for Data-free Knowledge\n  Distillation", "comments": "Accepted in WACV 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Knowledge Distillation is an effective method to transfer the learning across\ndeep neural networks. Typically, the dataset originally used for training the\nTeacher model is chosen as the \"Transfer Set\" to conduct the knowledge transfer\nto the Student. However, this original training data may not always be freely\navailable due to privacy or sensitivity concerns. In such scenarios, existing\napproaches either iteratively compose a synthetic set representative of the\noriginal training dataset, one sample at a time or learn a generative model to\ncompose such a transfer set. However, both these approaches involve complex\noptimization (GAN training or several backpropagation steps to synthesize one\nsample) and are often computationally expensive. In this paper, as a simple\nalternative, we investigate the effectiveness of \"arbitrary transfer sets\" such\nas random noise, publicly available synthetic, and natural datasets, all of\nwhich are completely unrelated to the original training dataset in terms of\ntheir visual or semantic contents. Through extensive experiments on multiple\nbenchmark datasets such as MNIST, FMNIST, CIFAR-10 and CIFAR-100, we discover\nand validate surprising effectiveness of using arbitrary data to conduct\nknowledge distillation when this dataset is \"target-class balanced\". We believe\nthat this important observation can potentially lead to designing baselines for\nthe data-free knowledge distillation task.\n", "versions": [{"version": "v1", "created": "Wed, 18 Nov 2020 06:33:20 GMT"}], "update_date": "2020-11-19", "authors_parsed": [["Nayak", "Gaurav Kumar", ""], ["Mopuri", "Konda Reddy", ""], ["Chakraborty", "Anirban", ""]]}, {"id": "2011.09114", "submitter": "Yuki Fujimura", "authors": "Yuki Fujimura, Motoharu Sonogashira, Masaaki Iiyama", "title": "Dehazing Cost Volume for Deep Multi-view Stereo in Scattering Media with\n  Airlight and Scattering Coefficient Estimation", "comments": "14 pages, extended version of our ACCV2020 paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a learning-based multi-view stereo (MVS) method in scattering\nmedia, such as fog or smoke, with a novel cost volume, called the dehazing cost\nvolume. Images captured in scattering media are degraded due to light\nscattering and attenuation caused by suspended particles. This degradation\ndepends on scene depth; thus, it is difficult for traditional MVS methods to\nevaluate photometric consistency because the depth is unknown before\nthree-dimensional (3D) reconstruction. The dehazing cost volume can solve this\nchicken-and-egg problem of depth estimation and image restoration by computing\nthe scattering effect using swept planes in the cost volume. We also propose a\nmethod of estimating scattering parameters, such as airlight, and a scattering\ncoefficient, which are required for our dehazing cost volume. The output depth\nof a network with our dehazing cost volume can be regarded as a function of\nthese parameters; thus, they are geometrically optimized with a sparse 3D point\ncloud obtained at a structure-from-motion step. Experimental results on\nsynthesized hazy images indicate the effectiveness of our dehazing cost volume\nagainst the ordinary cost volume regarding scattering media. We also\ndemonstrated the applicability of our dehazing cost volume to real foggy\nscenes.\n", "versions": [{"version": "v1", "created": "Wed, 18 Nov 2020 06:33:47 GMT"}, {"version": "v2", "created": "Wed, 2 Dec 2020 05:21:21 GMT"}], "update_date": "2020-12-03", "authors_parsed": [["Fujimura", "Yuki", ""], ["Sonogashira", "Motoharu", ""], ["Iiyama", "Masaaki", ""]]}, {"id": "2011.09116", "submitter": "Guoxiang Zhang", "authors": "Guoxiang Zhang and YangQuan Chen", "title": "More Informed Random Sample Consensus", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Random sample consensus (RANSAC) is a robust model-fitting algorithm. It is\nwidely used in many fields including image-stitching and point cloud\nregistration. In RANSAC, data is uniformly sampled for hypothesis generation.\nHowever, this uniform sampling strategy does not fully utilize all the\ninformation on many problems. In this paper, we propose a method that samples\ndata with a L\\'{e}vy distribution together with a data sorting algorithm. In\nthe hypothesis sampling step of the proposed method, data is sorted with a\nsorting algorithm we proposed, which sorts data based on the likelihood of a\ndata point being in the inlier set. Then, hypotheses are sampled from the\nsorted data with L\\'{e}vy distribution. The proposed method is evaluated on\nboth simulation and real-world public datasets. Our method shows better results\ncompared with the uniform baseline method.\n", "versions": [{"version": "v1", "created": "Wed, 18 Nov 2020 06:43:50 GMT"}], "update_date": "2020-11-19", "authors_parsed": [["Zhang", "Guoxiang", ""], ["Chen", "YangQuan", ""]]}, {"id": "2011.09123", "submitter": "Arezoo Rajabi", "authors": "Arezoo Rajabi, Rakesh B. Bobba", "title": "Adversarial Profiles: Detecting Out-Distribution & Adversarial Samples\n  in Pre-trained CNNs", "comments": "Accepted on DSN Workshop on Dependable and Secure Machine Learning\n  2019", "journal-ref": "DSN Workshop on Dependable and Secure Machine Learning (DSML 2019)", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite high accuracy of Convolutional Neural Networks (CNNs), they are\nvulnerable to adversarial and out-distribution examples. There are many\nproposed methods that tend to detect or make CNNs robust against these fooling\nexamples. However, most such methods need access to a wide range of fooling\nexamples to retrain the network or to tune detection parameters. Here, we\npropose a method to detect adversarial and out-distribution examples against a\npre-trained CNN without needing to retrain the CNN or needing access to a wide\nvariety of fooling examples. To this end, we create adversarial profiles for\neach class using only one adversarial attack generation technique. We then wrap\na detector around the pre-trained CNN that applies the created adversarial\nprofile to each input and uses the output to decide whether or not the input is\nlegitimate. Our initial evaluation of this approach using MNIST dataset show\nthat adversarial profile based detection is effective in detecting at least 92\nof out-distribution examples and 59% of adversarial examples.\n", "versions": [{"version": "v1", "created": "Wed, 18 Nov 2020 07:10:13 GMT"}], "update_date": "2020-11-19", "authors_parsed": [["Rajabi", "Arezoo", ""], ["Bobba", "Rakesh B.", ""]]}, {"id": "2011.09127", "submitter": "Huan Fu", "authors": "Huan Fu, Bowen Cai, Lin Gao, Lingxiao Zhang, Jiaming Wang Cao Li,\n  Zengqi Xun, Chengyue Sun, Rongfei Jia, Binqiang Zhao, Hao Zhang", "title": "3D-FRONT: 3D Furnished Rooms with layOuts and semaNTics", "comments": "Project page:\n  https://tianchi.aliyun.com/specials/promotion/alibaba-3d-scene-dataset", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce 3D-FRONT (3D Furnished Rooms with layOuts and semaNTics), a new,\nlarge-scale, and comprehensive repository of synthetic indoor scenes\nhighlighted by professionally designed layouts and a large number of rooms\npopulated by high-quality textured 3D models with style compatibility. From\nlayout semantics down to texture details of individual objects, our dataset is\nfreely available to the academic community and beyond. Currently, 3D-FRONT\ncontains 18,968 rooms diversely furnished by 3D objects, far surpassing all\npublicly available scene datasets. In addition, the 13,151 furniture objects\nall come with high-quality textures. While the floorplans and layout designs\nare directly sourced from professional creations, the interior designs in terms\nof furniture styles, color, and textures have been carefully curated based on a\nrecommender system we develop to attain consistent styles as expert designs.\nFurthermore, we release Trescope, a light-weight rendering tool, to support\nbenchmark rendering of 2D images and annotations from 3D-FRONT. We demonstrate\ntwo applications, interior scene synthesis and texture synthesis, that are\nespecially tailored to the strengths of our new dataset. The project page is\nat: https://tianchi.aliyun.com/specials/promotion/alibaba-3d-scene-dataset.\n", "versions": [{"version": "v1", "created": "Wed, 18 Nov 2020 07:14:55 GMT"}, {"version": "v2", "created": "Fri, 14 May 2021 02:39:44 GMT"}], "update_date": "2021-05-17", "authors_parsed": [["Fu", "Huan", ""], ["Cai", "Bowen", ""], ["Gao", "Lin", ""], ["Zhang", "Lingxiao", ""], ["Li", "Jiaming Wang Cao", ""], ["Xun", "Zengqi", ""], ["Sun", "Chengyue", ""], ["Jia", "Rongfei", ""], ["Zhao", "Binqiang", ""], ["Zhang", "Hao", ""]]}, {"id": "2011.09128", "submitter": "Moshe Eliasof", "authors": "Moshe Eliasof, Jonathan Ephrath, Lars Ruthotto, Eran Treister", "title": "Multigrid-in-Channels Neural Network Architectures", "comments": "This paper supersedes arXiv:2006.06799", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a multigrid-in-channels (MGIC) approach that tackles the quadratic\ngrowth of the number of parameters with respect to the number of channels in\nstandard convolutional neural networks (CNNs). It has been shown that there is\na redundancy in standard CNNs, as networks with light or sparse convolution\noperators yield similar performance to full networks. However, the number of\nparameters in the former networks also scales quadratically in width, while in\nthe latter case, the parameters typically have random sparsity patterns,\nhampering hardware efficiency. Our approach for building CNN architectures\nscales linearly with respect to the network's width while retaining full\ncoupling of the channels as in standard CNNs. To this end, we replace each\nconvolution block with its MGIC block utilizing a hierarchy of lightweight\nconvolutions. Our extensive experiments on image classification, segmentation,\nand point cloud classification show that applying this strategy to different\narchitectures like ResNet and MobileNetV3 considerably reduces the number of\nparameters while obtaining similar or better accuracy. For example, we obtain\n76.1% top-1 accuracy on ImageNet with a lightweight network with similar\nparameters and FLOPs to MobileNetV3.\n", "versions": [{"version": "v1", "created": "Tue, 17 Nov 2020 11:29:10 GMT"}, {"version": "v2", "created": "Thu, 19 Nov 2020 08:29:31 GMT"}], "update_date": "2020-11-20", "authors_parsed": [["Eliasof", "Moshe", ""], ["Ephrath", "Jonathan", ""], ["Ruthotto", "Lars", ""], ["Treister", "Eran", ""]]}, {"id": "2011.09141", "submitter": "Christoph Rist", "authors": "Christoph B. Rist, David Emmerichs, Markus Enzweiler and Dariu M.\n  Gavrila", "title": "Semantic Scene Completion using Local Deep Implicit Functions on LiDAR\n  Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semantic scene completion is the task of jointly estimating 3D geometry and\nsemantics of objects and surfaces within a given extent. This is a particularly\nchallenging task on real-world data that is sparse and occluded. We propose a\nscene segmentation network based on local Deep Implicit Functions as a novel\nlearning-based method for scene completion. Unlike previous work on scene\ncompletion, our method produces a continuous scene representation that is not\nbased on voxelization. We encode raw point clouds into a latent space locally\nand at multiple spatial resolutions. A global scene completion function is\nsubsequently assembled from the localized function patches. We show that this\ncontinuous representation is suitable to encode geometric and semantic\nproperties of extensive outdoor scenes without the need for spatial\ndiscretization (thus avoiding the trade-off between level of scene detail and\nthe scene extent that can be covered).\n  We train and evaluate our method on semantically annotated LiDAR scans from\nthe Semantic KITTI dataset. Our experiments verify that our method generates a\npowerful representation that can be decoded into a dense 3D description of a\ngiven scene. The performance of our method surpasses the state of the art on\nthe Semantic KITTI Scene Completion Benchmark in terms of geometric completion\nintersection-over-union (IoU).\n", "versions": [{"version": "v1", "created": "Wed, 18 Nov 2020 07:39:13 GMT"}, {"version": "v2", "created": "Thu, 19 Nov 2020 08:11:23 GMT"}, {"version": "v3", "created": "Mon, 12 Apr 2021 20:40:12 GMT"}], "update_date": "2021-04-14", "authors_parsed": [["Rist", "Christoph B.", ""], ["Emmerichs", "David", ""], ["Enzweiler", "Markus", ""], ["Gavrila", "Dariu M.", ""]]}, {"id": "2011.09149", "submitter": "Mehran Maghoumi", "authors": "Mehran Maghoumi, Eugene M. Taranta II, Joseph J. LaViola Jr", "title": "DeepNAG: Deep Non-Adversarial Gesture Generation", "comments": "13 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Synthetic data generation to improve classification performance (data\naugmentation) is a well-studied problem. Recently, generative adversarial\nnetworks (GAN) have shown superior image data augmentation performance, but\ntheir suitability in gesture synthesis has received inadequate attention.\nFurther, GANs prohibitively require simultaneous generator and discriminator\nnetwork training. We tackle both issues in this work. We first discuss a novel,\ndevice-agnostic GAN model for gesture synthesis called DeepGAN. Thereafter, we\nformulate DeepNAG by introducing a new differentiable loss function based on\ndynamic time warping and the average Hausdorff distance, which allows us to\ntrain DeepGAN's generator without requiring a discriminator. Through\nevaluations, we compare the utility of DeepGAN and DeepNAG against two\nalternative techniques for training five recognizers using data augmentation\nover six datasets. We further investigate the perceived quality of synthesized\nsamples via an Amazon Mechanical Turk user study based on the HYPE benchmark.\nWe find that DeepNAG outperforms DeepGAN in accuracy, training time (up to 17x\nfaster), and realism, thereby opening the door to a new line of research in\ngenerator network design and training for gesture synthesis. Our source code is\navailable at https://www.deepnag.com.\n", "versions": [{"version": "v1", "created": "Wed, 18 Nov 2020 08:00:12 GMT"}], "update_date": "2020-11-19", "authors_parsed": [["Maghoumi", "Mehran", ""], ["Taranta", "Eugene M.", "II"], ["LaViola", "Joseph J.", "Jr"]]}, {"id": "2011.09153", "submitter": "Yang Fang", "authors": "Yang Fang, Geun-Sik Jo and Chang-Hee Lee", "title": "RSINet: Rotation-Scale Invariant Network for Online Visual Tracking", "comments": "8 pages, 5 figures, the paper has been accepted by international\n  conference on pattern recognition 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Most Siamese network-based trackers perform the tracking process without\nmodel update, and cannot learn targetspecific variation adaptively. Moreover,\nSiamese-based trackers infer the new state of tracked objects by generating\naxis-aligned bounding boxes, which contain extra background noise, and are\nunable to accurately estimate the rotation and scale transformation of moving\nobjects, thus potentially reducing tracking performance. In this paper, we\npropose a novel Rotation-Scale Invariant Network (RSINet) to address the above\nproblem. Our RSINet tracker consists of a target-distractor discrimination\nbranch and a rotation-scale estimation branch, the rotation and scale knowledge\ncan be explicitly learned by a multi-task learning method in an end-to-end\nmanner. In addtion, the tracking model is adaptively optimized and updated\nunder spatio-temporal energy control, which ensures model stability and\nreliability, as well as high tracking efficiency. Comprehensive experiments on\nOTB-100, VOT2018, and LaSOT benchmarks demonstrate that our proposed RSINet\ntracker yields new state-of-the-art performance compared with recent trackers,\nwhile running at real-time speed about 45 FPS.\n", "versions": [{"version": "v1", "created": "Wed, 18 Nov 2020 08:19:14 GMT"}], "update_date": "2020-11-19", "authors_parsed": [["Fang", "Yang", ""], ["Jo", "Geun-Sik", ""], ["Lee", "Chang-Hee", ""]]}, {"id": "2011.09157", "submitter": "Chunhua Shen", "authors": "Xinlong Wang, Rufeng Zhang, Chunhua Shen, Tao Kong, Lei Li", "title": "Dense Contrastive Learning for Self-Supervised Visual Pre-Training", "comments": "11 pages. Accepted to IEEE/CVF Conf. Comp. Vision Pattern Recognition\n  (CVPR) 2021; Oral paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  To date, most existing self-supervised learning methods are designed and\noptimized for image classification. These pre-trained models can be sub-optimal\nfor dense prediction tasks due to the discrepancy between image-level\nprediction and pixel-level prediction. To fill this gap, we aim to design an\neffective, dense self-supervised learning method that directly works at the\nlevel of pixels (or local features) by taking into account the correspondence\nbetween local features. We present dense contrastive learning, which implements\nself-supervised learning by optimizing a pairwise contrastive (dis)similarity\nloss at the pixel level between two views of input images. Compared to the\nbaseline method MoCo-v2, our method introduces negligible computation overhead\n(only <1% slower), but demonstrates consistently superior performance when\ntransferring to downstream dense prediction tasks including object detection,\nsemantic segmentation and instance segmentation; and outperforms the\nstate-of-the-art methods by a large margin. Specifically, over the strong\nMoCo-v2 baseline, our method achieves significant improvements of 2.0% AP on\nPASCAL VOC object detection, 1.1% AP on COCO object detection, 0.9% AP on COCO\ninstance segmentation, 3.0% mIoU on PASCAL VOC semantic segmentation and 1.8%\nmIoU on Cityscapes semantic segmentation. Code is available at:\nhttps://git.io/AdelaiDet\n", "versions": [{"version": "v1", "created": "Wed, 18 Nov 2020 08:42:32 GMT"}, {"version": "v2", "created": "Sun, 4 Apr 2021 11:41:26 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Wang", "Xinlong", ""], ["Zhang", "Rufeng", ""], ["Shen", "Chunhua", ""], ["Kong", "Tao", ""], ["Li", "Lei", ""]]}, {"id": "2011.09158", "submitter": "Peisen Zhao", "authors": "Peisen Zhao, Lingxi Xie, Ya Zhang, Yanfeng Wang, Qi Tian", "title": "Privileged Knowledge Distillation for Online Action Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Online Action Detection (OAD) in videos is proposed as a per-frame labeling\ntask to address the real-time prediction tasks that can only obtain the\nprevious and current video frames. This paper presents a novel\nlearning-with-privileged based framework for online action detection where the\nfuture frames only observable at the training stages are considered as a form\nof privileged information. Knowledge distillation is employed to transfer the\nprivileged information from the offline teacher to the online student. We note\nthat this setting is different from conventional KD because the difference\nbetween the teacher and student models mostly lies in input data rather than\nthe network architecture. We propose Privileged Knowledge Distillation (PKD)\nwhich (i) schedules a curriculum learning procedure and (ii) inserts auxiliary\nnodes to the student model, both for shrinking the information gap and\nimproving learning performance. Compared to other OAD methods that explicitly\npredict future frames, our approach avoids learning unpredictable unnecessary\nyet inconsistent visual contents and achieves state-of-the-art accuracy on two\npopular OAD benchmarks, TVSeries and THUMOS14.\n", "versions": [{"version": "v1", "created": "Wed, 18 Nov 2020 08:52:15 GMT"}, {"version": "v2", "created": "Thu, 3 Dec 2020 12:52:54 GMT"}], "update_date": "2020-12-04", "authors_parsed": [["Zhao", "Peisen", ""], ["Xie", "Lingxi", ""], ["Zhang", "Ya", ""], ["Wang", "Yanfeng", ""], ["Tian", "Qi", ""]]}, {"id": "2011.09161", "submitter": "Yuanjun Xiong", "authors": "Sijie Yan, Yuanjun Xiong, Kaustav Kundu, Shuo Yang, Siqi Deng, Meng\n  Wang, Wei Xia, Stefano Soatto", "title": "Positive-Congruent Training: Towards Regression-Free Model Updates", "comments": "Accepted to CVPR 2021 (oral)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reducing inconsistencies in the behavior of different versions of an AI\nsystem can be as important in practice as reducing its overall error. In image\nclassification, sample-wise inconsistencies appear as \"negative flips\": A new\nmodel incorrectly predicts the output for a test sample that was correctly\nclassified by the old (reference) model. Positive-congruent (PC) training aims\nat reducing error rate while at the same time reducing negative flips, thus\nmaximizing congruency with the reference model only on positive predictions,\nunlike model distillation. We propose a simple approach for PC training, Focal\nDistillation, which enforces congruence with the reference model by giving more\nweights to samples that were correctly classified. We also found that, if the\nreference model itself can be chosen as an ensemble of multiple deep neural\nnetworks, negative flips can be further reduced without affecting the new\nmodel's accuracy.\n", "versions": [{"version": "v1", "created": "Wed, 18 Nov 2020 09:00:44 GMT"}, {"version": "v2", "created": "Fri, 20 Nov 2020 17:00:29 GMT"}, {"version": "v3", "created": "Mon, 17 May 2021 20:10:51 GMT"}], "update_date": "2021-05-19", "authors_parsed": [["Yan", "Sijie", ""], ["Xiong", "Yuanjun", ""], ["Kundu", "Kaustav", ""], ["Yang", "Shuo", ""], ["Deng", "Siqi", ""], ["Wang", "Meng", ""], ["Xia", "Wei", ""], ["Soatto", "Stefano", ""]]}, {"id": "2011.09170", "submitter": "Jiale Cao", "authors": "Yanwei Pang and Jiale Cao and Yazhao Li and Jin Xie and Hanqing Sun\n  and Jinfeng Gong", "title": "TJU-DHD: A Diverse High-Resolution Dataset for Object Detection", "comments": "object detection and pedestrian detection. website:\n  https://github.com/tjubiit/TJU-DHD", "journal-ref": "IEEE Transactions on Image Processing, 2020", "doi": "10.1109/TIP.2020.3034487", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Vehicles, pedestrians, and riders are the most important and interesting\nobjects for the perception modules of self-driving vehicles and video\nsurveillance. However, the state-of-the-art performance of detecting such\nimportant objects (esp. small objects) is far from satisfying the demand of\npractical systems. Large-scale, rich-diversity, and high-resolution datasets\nplay an important role in developing better object detection methods to satisfy\nthe demand. Existing public large-scale datasets such as MS COCO collected from\nwebsites do not focus on the specific scenarios. Moreover, the popular datasets\n(e.g., KITTI and Citypersons) collected from the specific scenarios are limited\nin the number of images and instances, the resolution, and the diversity. To\nattempt to solve the problem, we build a diverse high-resolution dataset\n(called TJU-DHD). The dataset contains 115,354 high-resolution images (52%\nimages have a resolution of 1624$\\times$1200 pixels and 48% images have a\nresolution of at least 2,560$\\times$1,440 pixels) and 709,330 labeled objects\nin total with a large variance in scale and appearance. Meanwhile, the dataset\nhas a rich diversity in season variance, illumination variance, and weather\nvariance. In addition, a new diverse pedestrian dataset is further built. With\nthe four different detectors (i.e., the one-stage RetinaNet, anchor-free FCOS,\ntwo-stage FPN, and Cascade R-CNN), experiments about object detection and\npedestrian detection are conducted. We hope that the newly built dataset can\nhelp promote the research on object detection and pedestrian detection in these\ntwo scenes. The dataset is available at https://github.com/tjubiit/TJU-DHD.\n", "versions": [{"version": "v1", "created": "Wed, 18 Nov 2020 09:32:24 GMT"}], "update_date": "2020-11-19", "authors_parsed": [["Pang", "Yanwei", ""], ["Cao", "Jiale", ""], ["Li", "Yazhao", ""], ["Xie", "Jin", ""], ["Sun", "Hanqing", ""], ["Gong", "Jinfeng", ""]]}, {"id": "2011.09190", "submitter": "Fan Zhang Dr", "authors": "Di Ma, Fan Zhang and David R. Bull", "title": "CVEGAN: A Perceptually-inspired GAN for Compressed Video Enhancement", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  We propose a new Generative Adversarial Network for Compressed Video quality\nEnhancement (CVEGAN). The CVEGAN generator benefits from the use of a novel\nMul2Res block (with multiple levels of residual learning branches), an enhanced\nresidual non-local block (ERNB) and an enhanced convolutional block attention\nmodule (ECBAM). The ERNB has also been employed in the discriminator to improve\nthe representational capability. The training strategy has also been\nre-designed specifically for video compression applications, to employ a\nrelativistic sphere GAN (ReSphereGAN) training methodology together with new\nperceptual loss functions. The proposed network has been fully evaluated in the\ncontext of two typical video compression enhancement tools: post-processing\n(PP) and spatial resolution adaptation (SRA). CVEGAN has been fully integrated\ninto the MPEG HEVC video coding test model (HM16.20) and experimental results\ndemonstrate significant coding gains (up to 28% for PP and 38% for SRA compared\nto the anchor) over existing state-of-the-art architectures for both coding\ntools across multiple datasets.\n", "versions": [{"version": "v1", "created": "Wed, 18 Nov 2020 10:24:38 GMT"}, {"version": "v2", "created": "Thu, 26 Nov 2020 20:17:53 GMT"}], "update_date": "2020-11-30", "authors_parsed": [["Ma", "Di", ""], ["Zhang", "Fan", ""], ["Bull", "David R.", ""]]}, {"id": "2011.09214", "submitter": "Yanwu Ge", "authors": "Yanwu Ge, Mingliang Song", "title": "Res-GCNN: A Lightweight Residual Graph Convolutional Neural Networks for\n  Human Trajectory Forecasting", "comments": "8 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Autonomous driving vehicles (ADVs) hold great hopes to solve traffic\ncongestion problems and reduce the number of traffic accidents. Accurate\ntrajectories prediction of other traffic agents around ADVs is of key\nimportance to achieve safe and efficient driving. Pedestrians, particularly,\nare more challenging to forecast due to their complex social in-teractions and\nrandomly moving patterns. We propose a Residual Graph Convolutional Neural\nNetwork (Res-GCNN), which models the interactive behaviors of pedes-trians by\nusing the adjacent matrix of the constructed graph for the current scene.\nThough the proposed Res-GCNN is quite lightweight with only about 6.4 kilo\nparameters which outperforms all other methods in terms of parameters size, our\nexperimental results show an improvement over the state of art by 13.3% on the\nFinal Displacement Error (FDE) which reaches 0.65 meter. As for the Average\nDis-placement Error (ADE), we achieve a suboptimal result (the value is 0.37\nmeter), which is also very competitive. The Res-GCNN is evaluated in the\nplatform with an NVIDIA GeForce RTX1080Ti GPU, and its mean inference time of\nthe whole dataset is only about 2.2 microseconds. Compared with other methods,\nthe proposed method shows strong potential for onboard application accounting\nfor forecasting accuracy and time efficiency. The code will be made publicly\navailable on GitHub.\n", "versions": [{"version": "v1", "created": "Wed, 18 Nov 2020 11:18:16 GMT"}], "update_date": "2020-11-19", "authors_parsed": [["Ge", "Yanwu", ""], ["Song", "Mingliang", ""]]}, {"id": "2011.09216", "submitter": "Nishant Bhattacharya", "authors": "Nishant Bhattacharya and Suresh Sundaram", "title": "CGAP2: Context and gap aware predictive pose framework for early\n  detection of gestures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  With a growing interest in autonomous vehicles' operation, there is an\nequally increasing need for efficient anticipatory gesture recognition systems\nfor human-vehicle interaction. Existing gesture-recognition algorithms have\nbeen primarily restricted to historical data. In this paper, we propose a novel\ncontext and gap aware pose prediction framework(CGAP2), which predicts future\npose data for anticipatory recognition of gestures in an online fashion. CGAP2\nimplements an encoder-decoder architecture paired with a pose prediction module\nto anticipate future frames followed by a shallow classifier. CGAP2 pose\nprediction module uses 3D convolutional layers and depends on the number of\npose frames supplied, the time difference between each pose frame, and the\nnumber of predicted pose frames. The performance of CGAP2 is evaluated on the\nHuman3.6M dataset with the MPJPE metric. For pose prediction of 15 frames in\nadvance, an error of 79.0mm is achieved. The pose prediction module consists of\nonly 26M parameters and can run at 50 FPS on the NVidia RTX Titan. Furthermore,\nthe ablation study indicates supplying higher context information to the pose\nprediction module can be detrimental for anticipatory recognition. CGAP2 has a\n1-second time advantage compared to other gesture recognition systems, which\ncan be crucial for autonomous vehicles.\n", "versions": [{"version": "v1", "created": "Wed, 18 Nov 2020 11:21:04 GMT"}], "update_date": "2020-11-19", "authors_parsed": [["Bhattacharya", "Nishant", ""], ["Sundaram", "Suresh", ""]]}, {"id": "2011.09230", "submitter": "Jaemin Na", "authors": "Jaemin Na, Heechul Jung, Hyung Jin Chang, Wonjun Hwang", "title": "FixBi: Bridging Domain Spaces for Unsupervised Domain Adaptation", "comments": "Accepted to CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unsupervised domain adaptation (UDA) methods for learning domain invariant\nrepresentations have achieved remarkable progress. However, most of the studies\nwere based on direct adaptation from the source domain to the target domain and\nhave suffered from large domain discrepancies. In this paper, we propose a UDA\nmethod that effectively handles such large domain discrepancies. We introduce a\nfixed ratio-based mixup to augment multiple intermediate domains between the\nsource and target domain. From the augmented-domains, we train the\nsource-dominant model and the target-dominant model that have complementary\ncharacteristics. Using our confidence-based learning methodologies, e.g.,\nbidirectional matching with high-confidence predictions and self-penalization\nusing low-confidence predictions, the models can learn from each other or from\nits own results. Through our proposed methods, the models gradually transfer\ndomain knowledge from the source to the target domain. Extensive experiments\ndemonstrate the superiority of our proposed method on three public benchmarks:\nOffice-31, Office-Home, and VisDA-2017.\n", "versions": [{"version": "v1", "created": "Wed, 18 Nov 2020 11:58:19 GMT"}, {"version": "v2", "created": "Thu, 25 Mar 2021 07:22:12 GMT"}], "update_date": "2021-03-26", "authors_parsed": [["Na", "Jaemin", ""], ["Jung", "Heechul", ""], ["Chang", "Hyung Jin", ""], ["Hwang", "Wonjun", ""]]}, {"id": "2011.09236", "submitter": "Frank Glavin", "authors": "Preeti Jagdish Sajjan and Frank G. Glavin", "title": "A Multi-class Approach -- Building a Visual Classifier based on Textual\n  Descriptions using Zero-Shot Learning", "comments": "AICS 2020: Irish Conference on Artificial Intelligence and Cognitive\n  Science", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Machine Learning (ML) techniques for image classification routinely require\nmany labelled images for training the model and while testing, we ought to use\nimages belonging to the same domain as those used for training. In this paper,\nwe overcome the two main hurdles of ML, i.e. scarcity of data and constrained\nprediction of the classification model. We do this by introducing a visual\nclassifier which uses a concept of transfer learning, namely Zero-Shot Learning\n(ZSL), and standard Natural Language Processing techniques. We train a\nclassifier by mapping labelled images to their textual description instead of\ntraining it for specific classes. Transfer learning involves transferring\nknowledge across domains that are similar. ZSL intelligently applies the\nknowledge learned while training for future recognition tasks. ZSL\ndifferentiates classes as two types: seen and unseen classes. Seen classes are\nthe classes upon which we have trained our model and unseen classes are the\nclasses upon which we test our model. The examples from unseen classes have not\nbeen encountered in the training phase. Earlier research in this domain focused\non developing a binary classifier but, in this paper, we present a multi-class\nclassifier with a Zero-Shot Learning approach.\n", "versions": [{"version": "v1", "created": "Wed, 18 Nov 2020 12:06:55 GMT"}], "update_date": "2020-11-19", "authors_parsed": [["Sajjan", "Preeti Jagdish", ""], ["Glavin", "Frank G.", ""]]}, {"id": "2011.09247", "submitter": "Fahdi Kanavati", "authors": "Fahdi Kanavati, Shin Ichihara, Michael Rambeau, Osamu Iizuka, Koji\n  Arihiro, Masayuki Tsuneki", "title": "Deep learning models for gastric signet ring cell carcinoma\n  classification in whole slide images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Signet ring cell carcinoma (SRCC) of the stomach is a rare type of cancer\nwith a slowly rising incidence. It tends to be more difficult to detect by\npathologists mainly due to its cellular morphology and diffuse invasion manner,\nand it has poor prognosis when detected at an advanced stage. Computational\npathology tools that can assist pathologists in detecting SRCC would be of a\nmassive benefit. In this paper, we trained deep learning models using transfer\nlearning, fully-supervised learning, and weakly-supervised learning to predict\nSRCC in Whole Slide Images (WSIs) using a training set of 1,765 WSIs. We\nevaluated the models on four different test sets of about 500 images each. The\nbest model achieved a Receiver Operator Curve (ROC) area under the curve (AUC)\nof at least 0.99 on all four test sets, setting a top baseline performance for\nSRCC WSI classification.\n", "versions": [{"version": "v1", "created": "Wed, 18 Nov 2020 12:39:51 GMT"}], "update_date": "2020-11-19", "authors_parsed": [["Kanavati", "Fahdi", ""], ["Ichihara", "Shin", ""], ["Rambeau", "Michael", ""], ["Iizuka", "Osamu", ""], ["Arihiro", "Koji", ""], ["Tsuneki", "Masayuki", ""]]}, {"id": "2011.09257", "submitter": "Pablo Pino", "authors": "Pablo Pino, Denis Parra, Pablo Messina, Cecilia Besa, Sergio Uribe", "title": "Inspecting state of the art performance and NLP metrics in image-based\n  medical report generation", "comments": "3 pages, 1 figure, 1 table. Accepted in LatinX in AI workshop at\n  NeurIPS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Several deep learning architectures have been proposed over the last years to\ndeal with the problem of generating a written report given an imaging exam as\ninput. Most works evaluate the generated reports using standard Natural\nLanguage Processing (NLP) metrics (e.g. BLEU, ROUGE), reporting significant\nprogress. In this article, we contrast this progress by comparing state of the\nart (SOTA) models against weak baselines. We show that simple and even naive\napproaches yield near SOTA performance on most traditional NLP metrics. We\nconclude that evaluation methods in this task should be further studied towards\ncorrectly measuring clinical accuracy, ideally involving physicians to\ncontribute to this end.\n", "versions": [{"version": "v1", "created": "Wed, 18 Nov 2020 13:09:12 GMT"}, {"version": "v2", "created": "Sat, 21 Nov 2020 17:58:40 GMT"}], "update_date": "2020-11-24", "authors_parsed": [["Pino", "Pablo", ""], ["Parra", "Denis", ""], ["Messina", "Pablo", ""], ["Besa", "Cecilia", ""], ["Uribe", "Sergio", ""]]}, {"id": "2011.09265", "submitter": "Farzad Khalvati", "authors": "Ruqian Hao, Khashayar Namdar, Lin Liu, Farzad Khalvati", "title": "A Transfer Learning Based Active Learning Framework for Brain Tumor\n  Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Brain tumor is one of the leading causes of cancer-related death globally\namong children and adults. Precise classification of brain tumor grade\n(low-grade and high-grade glioma) at early stage plays a key role in successful\nprognosis and treatment planning. With recent advances in deep learning,\nArtificial Intelligence-enabled brain tumor grading systems can assist\nradiologists in the interpretation of medical images within seconds. The\nperformance of deep learning techniques is, however, highly depended on the\nsize of the annotated dataset. It is extremely challenging to label a large\nquantity of medical images given the complexity and volume of medical data. In\nthis work, we propose a novel transfer learning based active learning framework\nto reduce the annotation cost while maintaining stability and robustness of the\nmodel performance for brain tumor classification. We employed a 2D slice-based\napproach to train and finetune our model on the Magnetic Resonance Imaging\n(MRI) training dataset of 203 patients and a validation dataset of 66 patients\nwhich was used as the baseline. With our proposed method, the model achieved\nArea Under Receiver Operating Characteristic (ROC) Curve (AUC) of 82.89% on a\nseparate test dataset of 66 patients, which was 2.92% higher than the baseline\nAUC while saving at least 40% of labeling cost. In order to further examine the\nrobustness of our method, we created a balanced dataset, which underwent the\nsame procedure. The model achieved AUC of 82% compared with AUC of 78.48% for\nthe baseline, which reassures the robustness and stability of our proposed\ntransfer learning augmented with active learning framework while significantly\nreducing the size of training data.\n", "versions": [{"version": "v1", "created": "Mon, 16 Nov 2020 21:11:40 GMT"}], "update_date": "2020-11-19", "authors_parsed": [["Hao", "Ruqian", ""], ["Namdar", "Khashayar", ""], ["Liu", "Lin", ""], ["Khalvati", "Farzad", ""]]}, {"id": "2011.09280", "submitter": "Alessandro Lameiras Koerich", "authors": "Thomas Teixeira, Eric Granger, Alessandro Lameiras Koerich", "title": "Continuous Emotion Recognition with Spatiotemporal Convolutional Neural\n  Networks", "comments": "33 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Facial expressions are one of the most powerful ways for depicting specific\npatterns in human behavior and describing human emotional state. Despite the\nimpressive advances of affective computing over the last decade, automatic\nvideo-based systems for facial expression recognition still cannot handle\nproperly variations in facial expression among individuals as well as\ncross-cultural and demographic aspects. Nevertheless, recognizing facial\nexpressions is a difficult task even for humans. In this paper, we investigate\nthe suitability of state-of-the-art deep learning architectures based on\nconvolutional neural networks (CNNs) for continuous emotion recognition using\nlong video sequences captured in-the-wild. This study focuses on deep learning\nmodels that allow encoding spatiotemporal relations in videos considering a\ncomplex and multi-dimensional emotion space, where values of valence and\narousal must be predicted. We have developed and evaluated convolutional\nrecurrent neural networks combining 2D-CNNs and long short term-memory units,\nand inflated 3D-CNN models, which are built by inflating the weights of a\npre-trained 2D-CNN model during fine-tuning, using application-specific videos.\nExperimental results on the challenging SEWA-DB dataset have shown that these\narchitectures can effectively be fine-tuned to encode the spatiotemporal\ninformation from successive raw pixel images and achieve state-of-the-art\nresults on such a dataset.\n", "versions": [{"version": "v1", "created": "Wed, 18 Nov 2020 13:42:05 GMT"}, {"version": "v2", "created": "Fri, 15 Jan 2021 14:49:00 GMT"}], "update_date": "2021-01-18", "authors_parsed": [["Teixeira", "Thomas", ""], ["Granger", "Eric", ""], ["Koerich", "Alessandro Lameiras", ""]]}, {"id": "2011.09300", "submitter": "Shan You", "authors": "Tao Huang, Shan You, Yibo Yang, Zhuozhuo Tu, Fei Wang, Chen Qian,\n  Changshui Zhang", "title": "Explicitly Learning Topology for Differentiable Neural Architecture\n  Search", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Differentiable neural architecture search (DARTS) has gained much success in\ndiscovering more flexible and diverse cell types. Current methods couple the\noperations and topology during search, and simply derive optimal topology by a\nhand-craft rule. However, topology also matters for neural architectures since\nit controls the interactions between features of operations. In this paper, we\nhighlight the topology learning in differentiable NAS, and propose an explicit\ntopology modeling method, named TopoNAS, to directly decouple the operation\nselection and topology during search. Concretely, we introduce a set of\ntopological variables and a combinatorial probabilistic distribution to\nexplicitly indicate the target topology. Besides, we also leverage a\npassive-aggressive regularization to suppress invalid topology within supernet.\nOur introduced topological variables can be jointly learned with operation\nvariables and supernet weights, and apply to various DARTS variants. Extensive\nexperiments on CIFAR-10 and ImageNet validate the effectiveness of our proposed\nTopoNAS. The results show that TopoNAS does enable to search cells with more\ndiverse and complex topology, and boost the performance significantly. For\nexample, TopoNAS can improve DARTS by 0.16\\% accuracy on CIFAR-10 dataset with\n40\\% parameters reduced or 0.35\\% with similar parameters.\n", "versions": [{"version": "v1", "created": "Wed, 18 Nov 2020 14:15:51 GMT"}], "update_date": "2020-11-19", "authors_parsed": [["Huang", "Tao", ""], ["You", "Shan", ""], ["Yang", "Yibo", ""], ["Tu", "Zhuozhuo", ""], ["Wang", "Fei", ""], ["Qian", "Chen", ""], ["Zhang", "Changshui", ""]]}, {"id": "2011.09303", "submitter": "Manvel Avetisian", "authors": "Konstantin Egorov, Elena Sokolova, Manvel Avetisian, Alexander\n  Tuzhilin", "title": "Noise-Resilient Automatic Interpretation of Holter ECG Recordings", "comments": "Accepted for publication on BIOSIGNALS 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Holter monitoring, a long-term ECG recording (24-hours and more), contains a\nlarge amount of valuable diagnostic information about the patient. Its\ninterpretation becomes a difficult and time-consuming task for the doctor who\nanalyzes them because every heartbeat needs to be classified, thus requiring\nhighly accurate methods for automatic interpretation. In this paper, we present\na three-stage process for analysing Holter recordings with robustness to noisy\nsignal. First stage is a segmentation neural network (NN) with encoderdecoder\narchitecture which detects positions of heartbeats. Second stage is a\nclassification NN which will classify heartbeats as wide or narrow. Third stage\nin gradient boosting decision trees (GBDT) on top of NN features that\nincorporates patient-wise features and further increases performance of our\napproach. As a part of this work we acquired 5095 Holter recordings of patients\nannotated by an experienced cardiologist. A committee of three cardiologists\nserved as a ground truth annotators for the 291 examples in the test set. We\nshow that the proposed method outperforms the selected baselines, including two\ncommercial-grade software packages and some methods previously published in the\nliterature.\n", "versions": [{"version": "v1", "created": "Tue, 17 Nov 2020 16:15:49 GMT"}], "update_date": "2020-11-19", "authors_parsed": [["Egorov", "Konstantin", ""], ["Sokolova", "Elena", ""], ["Avetisian", "Manvel", ""], ["Tuzhilin", "Alexander", ""]]}, {"id": "2011.09315", "submitter": "Peng Gao", "authors": "Minghang Zheng, Peng Gao, Xiaogang Wang, Hongsheng Li, Hao Dong", "title": "End-to-End Object Detection with Adaptive Clustering Transformer", "comments": "technique report", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  End-to-end Object Detection with Transformer (DETR)proposes to perform object\ndetection with Transformer and achieve comparable performance with two-stage\nobject detection like Faster-RCNN. However, DETR needs huge computational\nresources for training and inference due to the high-resolution spatial input.\nIn this paper, a novel variant of transformer named Adaptive Clustering\nTransformer(ACT) has been proposed to reduce the computation cost for\nhigh-resolution input. ACT cluster the query features adaptively using Locality\nSensitive Hashing (LSH) and ap-proximate the query-key interaction using the\nprototype-key interaction. ACT can reduce the quadratic O(N2) complexity inside\nself-attention into O(NK) where K is the number of prototypes in each layer.\nACT can be a drop-in module replacing the original self-attention module\nwithout any training. ACT achieves a good balance between accuracy and\ncomputation cost (FLOPs). The code is available as supplementary for the ease\nof experiment replication and verification.\n", "versions": [{"version": "v1", "created": "Wed, 18 Nov 2020 14:36:37 GMT"}], "update_date": "2020-11-19", "authors_parsed": [["Zheng", "Minghang", ""], ["Gao", "Peng", ""], ["Wang", "Xiaogang", ""], ["Li", "Hongsheng", ""], ["Dong", "Hao", ""]]}, {"id": "2011.09330", "submitter": "Taewon Kang", "authors": "Taewon Kang, Soohyun Kim, Sunwoo Kim, Seungryong Kim", "title": "Online Exemplar Fine-Tuning for Image-to-Image Translation", "comments": "10 pages, 13 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing techniques to solve exemplar-based image-to-image translation within\ndeep convolutional neural networks (CNNs) generally require a training phase to\noptimize the network parameters on domain-specific and task-specific\nbenchmarks, thus having limited applicability and generalization ability. In\nthis paper, we propose a novel framework, for the first time, to solve\nexemplar-based translation through an online optimization given an input image\npair, called online exemplar fine-tuning (OEFT), in which we fine-tune the\noff-the-shelf and general-purpose networks to the input image pair themselves.\nWe design two sub-networks, namely correspondence fine-tuning and multiple GAN\ninversion, and optimize these network parameters and latent codes, starting\nfrom the pre-trained ones, with well-defined loss functions. Our framework does\nnot require the off-line training phase, which has been the main challenge of\nexisting methods, but the pre-trained networks to enable optimization in\nonline. Experimental results prove that our framework is effective in having a\ngeneralization power to unseen image pairs and clearly even outperforms the\nstate-of-the-arts needing the intensive training phase.\n", "versions": [{"version": "v1", "created": "Wed, 18 Nov 2020 15:13:16 GMT"}], "update_date": "2020-11-19", "authors_parsed": [["Kang", "Taewon", ""], ["Kim", "Soohyun", ""], ["Kim", "Sunwoo", ""], ["Kim", "Seungryong", ""]]}, {"id": "2011.09364", "submitter": "Hossein Aboutalebi", "authors": "Hossein Aboutalebi, Mohammad Javad Shafiee Alexander Wong", "title": "Self-Gradient Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The incredible effectiveness of adversarial attacks on fooling deep neural\nnetworks poses a tremendous hurdle in the widespread adoption of deep learning\nin safety and security-critical domains. While adversarial defense mechanisms\nhave been proposed since the discovery of the adversarial vulnerability issue\nof deep neural networks, there is a long path to fully understand and address\nthis issue. In this study, we hypothesize that part of the reason for the\nincredible effectiveness of adversarial attacks is their ability to implicitly\ntap into and exploit the gradient flow of a deep neural network. This innate\nability to exploit gradient flow makes defending against such attacks quite\nchallenging. Motivated by this hypothesis we argue that if a deep neural\nnetwork architecture can explicitly tap into its own gradient flow during the\ntraining, it can boost its defense capability significantly. Inspired by this\nfact, we introduce the concept of self-gradient networks, a novel deep neural\nnetwork architecture designed to be more robust against adversarial\nperturbations. Gradient flow information is leveraged within self-gradient\nnetworks to achieve greater perturbation stability beyond what can be achieved\nin the standard training process. We conduct a theoretical analysis to gain\nbetter insights into the behaviour of the proposed self-gradient networks to\nillustrate the efficacy of leverage this additional gradient flow information.\nThe proposed self-gradient network architecture enables much more efficient and\neffective adversarial training, leading to faster convergence towards an\nadversarially robust solution by at least 10X. Experimental results demonstrate\nthe effectiveness of self-gradient networks when compared with state-of-the-art\nadversarial learning strategies, with 10% improvement on the CIFAR10 dataset\nunder PGD and CW adversarial perturbations.\n", "versions": [{"version": "v1", "created": "Wed, 18 Nov 2020 16:04:05 GMT"}, {"version": "v2", "created": "Thu, 19 Nov 2020 04:16:05 GMT"}], "update_date": "2020-11-20", "authors_parsed": [["Aboutalebi", "Hossein", ""], ["Wong", "Mohammad Javad Shafiee Alexander", ""]]}, {"id": "2011.09369", "submitter": "Feng Gao", "authors": "Feng Gao, Jincheng Yu, Hao Shen, Yu Wang, Huazhong Yang", "title": "Attentional Separation-and-Aggregation Network for Self-supervised\n  Depth-Pose Learning in Dynamic Scenes", "comments": "accepted by CoRL2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.RO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Learning depth and ego-motion from unlabeled videos via self-supervision from\nepipolar projection can improve the robustness and accuracy of the 3D\nperception and localization of vision-based robots. However, the rigid\nprojection computed by ego-motion cannot represent all scene points, such as\npoints on moving objects, leading to false guidance in these regions. To\naddress this problem, we propose an Attentional Separation-and-Aggregation\nNetwork (ASANet), which can learn to distinguish and extract the scene's static\nand dynamic characteristics via the attention mechanism. We further propose a\nnovel MotionNet with an ASANet as the encoder, followed by two separate\ndecoders, to estimate the camera's ego-motion and the scene's dynamic motion\nfield. Then, we introduce an auto-selecting approach to detect the moving\nobjects for dynamic-aware learning automatically. Empirical experiments\ndemonstrate that our method can achieve the state-of-the-art performance on the\nKITTI benchmark.\n", "versions": [{"version": "v1", "created": "Wed, 18 Nov 2020 16:07:30 GMT"}], "update_date": "2020-11-19", "authors_parsed": [["Gao", "Feng", ""], ["Yu", "Jincheng", ""], ["Shen", "Hao", ""], ["Wang", "Yu", ""], ["Yang", "Huazhong", ""]]}, {"id": "2011.09390", "submitter": "Bradley Saund", "authors": "Brad Saund and Dmitry Berenson", "title": "Diverse Plausible Shape Completions from Ambiguous Depth Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose PSSNet, a network architecture for generating diverse plausible 3D\nreconstructions from a single 2.5D depth image. Existing methods tend to\nproduce only small variations on a single shape, even when multiple shapes are\nconsistent with an observation. To obtain diversity we alter a Variational Auto\nEncoder by providing a learned shape bounding box feature as side information\nduring training. Since these features are known during training, we are able to\nadd a supervised loss to the encoder and noiseless values to the decoder. To\nevaluate, we sample a set of completions from a network, construct a set of\nplausible shape matches for each test observation, and compare using our\nplausible diversity metric defined over sets of shapes. We perform experiments\nusing Shapenet mugs and partially-occluded YCB objects and find that our method\nperforms comparably in datasets with little ambiguity, and outperforms existing\nmethods when many shapes plausibly fit an observed depth image. We demonstrate\none use for PSSNet on a physical robot when grasping objects in occlusion and\nclutter.\n", "versions": [{"version": "v1", "created": "Wed, 18 Nov 2020 16:42:51 GMT"}], "update_date": "2020-11-19", "authors_parsed": [["Saund", "Brad", ""], ["Berenson", "Dmitry", ""]]}, {"id": "2011.09414", "submitter": "Burhaneddin Yaman", "authors": "Burhaneddin Yaman, Chetan Shenoy, Zilin Deng, Steen Moeller, Hossam\n  El-Rewaidy, Reza Nezafat, and Mehmet Ak\\c{c}akaya", "title": "Self-Supervised Physics-Guided Deep Learning Reconstruction For\n  High-Resolution 3D LGE CMR", "comments": null, "journal-ref": "Proceedings of IEEE ISBI, 2021", "doi": "10.1109/ISBI48211.2021.9434054", "report-no": null, "categories": "eess.IV cs.CV cs.LG eess.SP physics.med-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Late gadolinium enhancement (LGE) cardiac MRI (CMR) is the clinical standard\nfor diagnosis of myocardial scar. 3D isotropic LGE CMR provides improved\ncoverage and resolution compared to 2D imaging. However, image acceleration is\nrequired due to long scan times and contrast washout. Physics-guided deep\nlearning (PG-DL) approaches have recently emerged as an improved accelerated\nMRI strategy. Training of PG-DL methods is typically performed in supervised\nmanner requiring fully-sampled data as reference, which is challenging in 3D\nLGE CMR. Recently, a self-supervised learning approach was proposed to enable\ntraining PG-DL techniques without fully-sampled data. In this work, we extend\nthis self-supervised learning approach to 3D imaging, while tackling challenges\nrelated to small training database sizes of 3D volumes. Results and a reader\nstudy on prospectively accelerated 3D LGE show that the proposed approach at\n6-fold acceleration outperforms the clinically utilized compressed sensing\napproach at 3-fold acceleration.\n", "versions": [{"version": "v1", "created": "Wed, 18 Nov 2020 17:22:21 GMT"}], "update_date": "2021-06-02", "authors_parsed": [["Yaman", "Burhaneddin", ""], ["Shenoy", "Chetan", ""], ["Deng", "Zilin", ""], ["Moeller", "Steen", ""], ["El-Rewaidy", "Hossam", ""], ["Nezafat", "Reza", ""], ["Ak\u00e7akaya", "Mehmet", ""]]}, {"id": "2011.09420", "submitter": "Sanjaya Herath Mr", "authors": "Yasiru Ranasinghe, Sanjaya Herath, Kavinga Weerasooriya, Mevan\n  Ekanayake, Roshan Godaliyadda, Parakrama Ekanayake, Vijitha Herath", "title": "Convolutional Autoencoder for Blind Hyperspectral Image Unmixing", "comments": "7 pages, 4 figures, conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In the remote sensing context spectral unmixing is a technique to decompose a\nmixed pixel into two fundamental representatives: endmembers and abundances. In\nthis paper, a novel architecture is proposed to perform blind unmixing on\nhyperspectral images. The proposed architecture consists of convolutional\nlayers followed by an autoencoder. The encoder transforms the feature space\nproduced through convolutional layers to a latent space representation. Then,\nfrom these latent characteristics the decoder reconstructs the roll-out image\nof the monochrome image which is at the input of the architecture; and each\nsingle-band image is fed sequentially. Experimental results on real\nhyperspectral data concludes that the proposed algorithm outperforms existing\nunmixing methods at abundance estimation and generates competitive results for\nendmember extraction with RMSE and SAD as the metrics, respectively.\n", "versions": [{"version": "v1", "created": "Wed, 18 Nov 2020 17:41:31 GMT"}], "update_date": "2020-11-19", "authors_parsed": [["Ranasinghe", "Yasiru", ""], ["Herath", "Sanjaya", ""], ["Weerasooriya", "Kavinga", ""], ["Ekanayake", "Mevan", ""], ["Godaliyadda", "Roshan", ""], ["Ekanayake", "Parakrama", ""], ["Herath", "Vijitha", ""]]}, {"id": "2011.09427", "submitter": "Anthony Bisulco", "authors": "Anthony Bisulco, Fernando Cladera Ojeda, Volkan Isler, Daniel D. Lee", "title": "Fast Motion Understanding with Spatiotemporal Neural Networks and\n  Dynamic Vision Sensors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a Dynamic Vision Sensor (DVS) based system for reasoning\nabout high speed motion. As a representative scenario, we consider the case of\na robot at rest reacting to a small, fast approaching object at speeds higher\nthan 15m/s. Since conventional image sensors at typical frame rates observe\nsuch an object for only a few frames, estimating the underlying motion presents\na considerable challenge for standard computer vision systems and algorithms.\nIn this paper we present a method motivated by how animals such as insects\nsolve this problem with their relatively simple vision systems.\n  Our solution takes the event stream from a DVS and first encodes the temporal\nevents with a set of causal exponential filters across multiple time scales. We\ncouple these filters with a Convolutional Neural Network (CNN) to efficiently\nextract relevant spatiotemporal features. The combined network learns to output\nboth the expected time to collision of the object, as well as the predicted\ncollision point on a discretized polar grid. These critical estimates are\ncomputed with minimal delay by the network in order to react appropriately to\nthe incoming object. We highlight the results of our system to a toy dart\nmoving at 23.4m/s with a 24.73{\\deg} error in ${\\theta}$, 18.4mm average\ndiscretized radius prediction error, and 25.03% median time to collision\nprediction error.\n", "versions": [{"version": "v1", "created": "Wed, 18 Nov 2020 17:55:07 GMT"}], "update_date": "2020-11-19", "authors_parsed": [["Bisulco", "Anthony", ""], ["Ojeda", "Fernando Cladera", ""], ["Isler", "Volkan", ""], ["Lee", "Daniel D.", ""]]}, {"id": "2011.09471", "submitter": "Leslie Smith", "authors": "Helena E. Liu and Leslie N. Smith", "title": "FROST: Faster and more Robust One-shot Semi-supervised Training", "comments": "Withdrawn because the results reported were due to an error in our\n  code", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV eess.IV stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Recent advances in one-shot semi-supervised learning have lowered the barrier\nfor deep learning of new applications. However, the state-of-the-art for\nsemi-supervised learning is slow to train and the performance is sensitive to\nthe choices of the labeled data and hyper-parameter values. In this paper, we\npresent a one-shot semi-supervised learning method that trains up to an order\nof magnitude faster and is more robust than state-of-the-art methods.\nSpecifically, we show that by combining semi-supervised learning with a\none-stage, single network version of self-training, our FROST methodology\ntrains faster and is more robust to choices for the labeled samples and changes\nin hyper-parameters. Our experiments demonstrate FROST's capability to perform\nwell when the composition of the unlabeled data is unknown; that is when the\nunlabeled data contain unequal numbers of each class and can contain\nout-of-distribution examples that don't belong to any of the training classes.\nHigh performance, speed of training, and insensitivity to hyper-parameters make\nFROST the most practical method for one-shot semi-supervised training. Our code\nis available at https://github.com/HelenaELiu/FROST.\n", "versions": [{"version": "v1", "created": "Wed, 18 Nov 2020 18:56:03 GMT"}, {"version": "v2", "created": "Fri, 20 Nov 2020 11:29:58 GMT"}, {"version": "v3", "created": "Sun, 29 Nov 2020 23:45:56 GMT"}, {"version": "v4", "created": "Fri, 4 Dec 2020 14:04:18 GMT"}], "update_date": "2020-12-07", "authors_parsed": [["Liu", "Helena E.", ""], ["Smith", "Leslie N.", ""]]}, {"id": "2011.09473", "submitter": "Brian Dolhansky", "authors": "Brian Dolhansky, Cristian Canton Ferrer", "title": "Adversarial collision attacks on image hashing functions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hashing images with a perceptual algorithm is a common approach to solving\nduplicate image detection problems. However, perceptual image hashing\nalgorithms are differentiable, and are thus vulnerable to gradient-based\nadversarial attacks. We demonstrate that not only is it possible to modify an\nimage to produce an unrelated hash, but an exact image hash collision between a\nsource and target image can be produced via minuscule adversarial\nperturbations. In a white box setting, these collisions can be replicated\nacross nearly every image pair and hash type (including both deep and\nnon-learned hashes). Furthermore, by attacking points other than the output of\na hashing function, an attacker can avoid having to know the details of a\nparticular algorithm, resulting in collisions that transfer across different\nhash sizes or model architectures. Using these techniques, an adversary can\npoison the image lookup table of a duplicate image detection service, resulting\nin undefined or unwanted behavior. Finally, we offer several potential\nmitigations to gradient-based image hash attacks.\n", "versions": [{"version": "v1", "created": "Wed, 18 Nov 2020 18:59:02 GMT"}], "update_date": "2020-11-19", "authors_parsed": [["Dolhansky", "Brian", ""], ["Ferrer", "Cristian Canton", ""]]}, {"id": "2011.09517", "submitter": "Tanveer Syeda-Mahmood", "authors": "Tanveer Syeda-Mahmood, Ph.D, K.C.L Wong, Ph.D, Joy T. Wu, M.D., M.P.H,\n  Ashutosh Jadhav, Ph.D, Orest Boyko, M.D. Ph.D", "title": "Extracting and Learning Fine-Grained Labels from Chest Radiographs", "comments": "This paper won the Homer R. Warner Award at AMIA 2020 awarded to a\n  paper that best describes approaches to improving computerized information\n  acquisition, knowledge data acquisition and management, and experimental\n  results documenting the value of these approaches. The paper shows a\n  combination of textual and visual processing to automatically recognize\n  complex findings in chest X-rays", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Chest radiographs are the most common diagnostic exam in emergency rooms and\nintensive care units today. Recently, a number of researchers have begun\nworking on large chest X-ray datasets to develop deep learning models for\nrecognition of a handful of coarse finding classes such as opacities, masses\nand nodules. In this paper, we focus on extracting and learning fine-grained\nlabels for chest X-ray images. Specifically we develop a new method of\nextracting fine-grained labels from radiology reports by combining\nvocabulary-driven concept extraction with phrasal grouping in dependency parse\ntrees for association of modifiers with findings. A total of 457 fine-grained\nlabels depicting the largest spectrum of findings to date were selected and\nsufficiently large datasets acquired to train a new deep learning model\ndesigned for fine-grained classification. We show results that indicate a\nhighly accurate label extraction process and a reliable learning of\nfine-grained labels. The resulting network, to our knowledge, is the first to\nrecognize fine-grained descriptions of findings in images covering over nine\nmodifiers including laterality, location, severity, size and appearance.\n", "versions": [{"version": "v1", "created": "Wed, 18 Nov 2020 19:56:08 GMT"}], "update_date": "2020-11-20", "authors_parsed": [["Syeda-Mahmood", "Tanveer", ""], ["D", "Ph.", ""], ["Wong", "K. C. L", ""], ["D", "Ph.", ""], ["Wu", "Joy T.", ""], ["D.", "M.", ""], ["H", "M. P.", ""], ["Jadhav", "Ashutosh", ""], ["D", "Ph.", ""], ["Boyko", "Orest", ""], ["D", "M. D. Ph.", ""]]}, {"id": "2011.09524", "submitter": "Hasan Saribas", "authors": "Hasan Saribas, Hakan Cevikalp, Okan K\\\"op\\\"ukl\\\"u, Bedirhan Uzun", "title": "TRAT: Tracking by Attention Using Spatio-Temporal Features", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Robust object tracking requires knowledge of tracked objects' appearance,\nmotion and their evolution over time. Although motion provides distinctive and\ncomplementary information especially for fast moving objects, most of the\nrecent tracking architectures primarily focus on the objects' appearance\ninformation. In this paper, we propose a two-stream deep neural network tracker\nthat uses both spatial and temporal features. Our architecture is developed\nover ATOM tracker and contains two backbones: (i) 2D-CNN network to capture\nappearance features and (ii) 3D-CNN network to capture motion features. The\nfeatures returned by the two networks are then fused with attention based\nFeature Aggregation Module (FAM). Since the whole architecture is unified, it\ncan be trained end-to-end. The experimental results show that the proposed\ntracker TRAT (TRacking by ATtention) achieves state-of-the-art performance on\nmost of the benchmarks and it significantly outperforms the baseline ATOM\ntracker.\n", "versions": [{"version": "v1", "created": "Wed, 18 Nov 2020 20:11:12 GMT"}], "update_date": "2020-11-20", "authors_parsed": [["Saribas", "Hasan", ""], ["Cevikalp", "Hakan", ""], ["K\u00f6p\u00fckl\u00fc", "Okan", ""], ["Uzun", "Bedirhan", ""]]}, {"id": "2011.09526", "submitter": "Aiswarya Akumalla", "authors": "Aiswarya Akumalla, Seth Haney, Maksim Bazhenov", "title": "Contextual Fusion For Adversarial Robustness", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Mammalian brains handle complex reasoning tasks in a gestalt manner by\nintegrating information from regions of the brain that are specialised to\nindividual sensory modalities. This allows for improved robustness and better\ngeneralisation ability. In contrast, deep neural networks are usually designed\nto process one particular information stream and susceptible to various types\nof adversarial perturbations. While many methods exist for detecting and\ndefending against adversarial attacks, they do not generalise across a range of\nattacks and negatively affect performance on clean, unperturbed data. We\ndeveloped a fusion model using a combination of background and foreground\nfeatures extracted in parallel from Places-CNN and Imagenet-CNN. We tested the\nbenefits of the fusion approach on preserving adversarial robustness for human\nperceivable (e.g., Gaussian blur) and network perceivable (e.g.,\ngradient-based) attacks for CIFAR-10 and MS COCO data sets. For gradient based\nattacks, our results show that fusion allows for significant improvements in\nclassification without decreasing performance on unperturbed data and without\nneed to perform adversarial retraining. Our fused model revealed improvements\nfor Gaussian blur type perturbations as well. The increase in performance from\nfusion approach depended on the variability of the image contexts; larger\nincreases were seen for classes of images with larger differences in their\ncontexts. We also demonstrate the effect of regularization to bias the\nclassifier decision in the presence of a known adversary. We propose that this\nbiologically inspired approach to integrate information across multiple\nmodalities provides a new way to improve adversarial robustness that can be\ncomplementary to current state of the art approaches.\n", "versions": [{"version": "v1", "created": "Wed, 18 Nov 2020 20:13:23 GMT"}], "update_date": "2020-11-20", "authors_parsed": [["Akumalla", "Aiswarya", ""], ["Haney", "Seth", ""], ["Bazhenov", "Maksim", ""]]}, {"id": "2011.09530", "submitter": "Hassan Akbari", "authors": "Hassan Akbari, Hamid Palangi, Jianwei Yang, Sudha Rao, Asli\n  Celikyilmaz, Roland Fernandez, Paul Smolensky, Jianfeng Gao, Shih-Fu Chang", "title": "Neuro-Symbolic Representations for Video Captioning: A Case for\n  Leveraging Inductive Biases for Vision and Language", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neuro-symbolic representations have proved effective in learning structure\ninformation in vision and language. In this paper, we propose a new model\narchitecture for learning multi-modal neuro-symbolic representations for video\ncaptioning. Our approach uses a dictionary learning-based method of learning\nrelations between videos and their paired text descriptions. We refer to these\nrelations as relative roles and leverage them to make each token role-aware\nusing attention. This results in a more structured and interpretable\narchitecture that incorporates modality-specific inductive biases for the\ncaptioning task. Intuitively, the model is able to learn spatial, temporal, and\ncross-modal relations in a given pair of video and text. The disentanglement\nachieved by our proposal gives the model more capacity to capture multi-modal\nstructures which result in captions with higher quality for videos. Our\nexperiments on two established video captioning datasets verifies the\neffectiveness of the proposed approach based on automatic metrics. We further\nconduct a human evaluation to measure the grounding and relevance of the\ngenerated captions and observe consistent improvement for the proposed model.\nThe codes and trained models can be found at\nhttps://github.com/hassanhub/R3Transformer\n", "versions": [{"version": "v1", "created": "Wed, 18 Nov 2020 20:21:19 GMT"}], "update_date": "2020-11-20", "authors_parsed": [["Akbari", "Hassan", ""], ["Palangi", "Hamid", ""], ["Yang", "Jianwei", ""], ["Rao", "Sudha", ""], ["Celikyilmaz", "Asli", ""], ["Fernandez", "Roland", ""], ["Smolensky", "Paul", ""], ["Gao", "Jianfeng", ""], ["Chang", "Shih-Fu", ""]]}, {"id": "2011.09540", "submitter": "Satish Kumar", "authors": "Satish Kumar, A S M Iftekhar, Michael Goebel, Tom Bullock, Mary H.\n  MacLean, Michael B. Miller, Tyler Santander, Barry Giesbrecht, Scott T.\n  Grafton, B.S. Manjunath", "title": "StressNet: Detecting Stress in Thermal Videos", "comments": "11 pages, 10 figues, 2 tables, Conference WACV2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Precise measurement of physiological signals is critical for the effective\nmonitoring of human vital signs. Recent developments in computer vision have\ndemonstrated that signals such as pulse rate and respiration rate can be\nextracted from digital video of humans, increasing the possibility of\ncontact-less monitoring. This paper presents a novel approach to obtaining\nphysiological signals and classifying stress states from thermal video. The\nproposed network--\"StressNet\"--features a hybrid emission representation model\nthat models the direct emission and absorption of heat by the skin and\nunderlying blood vessels. This results in an information-rich feature\nrepresentation of the face, which is used by spatio-temporal network for\nreconstructing the ISTI ( Initial Systolic Time Interval: a measure of change\nin cardiac sympathetic activity that is considered to be a quantitative index\nof stress in humans ). The reconstructed ISTI signal is fed into a\nstress-detection model to detect and classify the individual's stress state (\ni.e. stress or no stress ). A detailed evaluation demonstrates that StressNet\nachieves estimated the ISTI signal with 95% accuracy and detect stress with\naverage precision of 0.842. The source code is available on Github.\n", "versions": [{"version": "v1", "created": "Wed, 18 Nov 2020 20:47:23 GMT"}, {"version": "v2", "created": "Mon, 23 Nov 2020 18:38:23 GMT"}], "update_date": "2020-11-24", "authors_parsed": [["Kumar", "Satish", ""], ["Iftekhar", "A S M", ""], ["Goebel", "Michael", ""], ["Bullock", "Tom", ""], ["MacLean", "Mary H.", ""], ["Miller", "Michael B.", ""], ["Santander", "Tyler", ""], ["Giesbrecht", "Barry", ""], ["Grafton", "Scott T.", ""], ["Manjunath", "B. S.", ""]]}, {"id": "2011.09556", "submitter": "Jungseok Hong", "authors": "Jungseok Hong, Sadman Sakib Enan, Christopher Morse, Junaed Sattar", "title": "Visual Diver Face Recognition for Underwater Human-Robot Interaction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper presents a deep-learned facial recognition method for underwater\nrobots to identify scuba divers. Specifically, the proposed method is able to\nrecognize divers underwater with faces heavily obscured by scuba masks and\nbreathing apparatus. Our contribution in this research is towards robust facial\nidentification of individuals under significant occlusion of facial features\nand image degradation from underwater optical distortions. With the ability to\ncorrectly recognize divers, autonomous underwater vehicles (AUV) will be able\nto engage in collaborative tasks with the correct person in human-robot teams\nand ensure that instructions are accepted from only those authorized to command\nthe robots. We demonstrate that our proposed framework is able to learn\ndiscriminative features from real-world diver faces through different data\naugmentation and generation techniques. Experimental evaluations show that this\nframework achieves a 3-fold increase in prediction accuracy compared to the\nstate-of-the-art (SOTA) algorithms and is well-suited for embedded inference on\nrobotic platforms.\n", "versions": [{"version": "v1", "created": "Wed, 18 Nov 2020 21:57:09 GMT"}], "update_date": "2020-11-20", "authors_parsed": [["Hong", "Jungseok", ""], ["Enan", "Sadman Sakib", ""], ["Morse", "Christopher", ""], ["Sattar", "Junaed", ""]]}, {"id": "2011.09563", "submitter": "Hanqing Chao", "authors": "Jiajin Zhang, Hanqing Chao, Pingkun Yan", "title": "Robustified Domain Adaptation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unsupervised domain adaptation (UDA) is widely used to transfer knowledge\nfrom a labeled source domain to an unlabeled target domain with different data\ndistribution. While extensive studies attested that deep learning models are\nvulnerable to adversarial attacks, the adversarial robustness of models in\ndomain adaptation application has largely been overlooked. This paper points\nout that the inevitable domain distribution deviation in UDA is a critical\nbarrier to model robustness on the target domain. To address the problem, we\npropose a novel Class-consistent Unsupervised Robust Domain Adaptation (CURDA)\nframework for training robust UDA models. With the introduced contrastive\nrobust training and source anchored adversarial contrastive losses, our\nproposed CURDA framework can effectively robustify UDA models by simultaneously\nminimizing the data distribution deviation and the distance between target\ndomain clean-adversarial pairs without creating classification confusion.\nExperiments on several public benchmarks show that CURDA can significantly\nimprove model robustness in the target domain with only minor cost of accuracy\non the clean samples.\n", "versions": [{"version": "v1", "created": "Wed, 18 Nov 2020 22:21:54 GMT"}, {"version": "v2", "created": "Wed, 24 Mar 2021 21:18:25 GMT"}], "update_date": "2021-03-26", "authors_parsed": [["Zhang", "Jiajin", ""], ["Chao", "Hanqing", ""], ["Yan", "Pingkun", ""]]}, {"id": "2011.09577", "submitter": "Sadra Naddaf", "authors": "Sadra Naddaf-Sh, M-Mahdi Naddaf-Sh, Amir R. Kashani and Hassan\n  Zargarzadeh", "title": "An Efficient and Scalable Deep Learning Approach for Road Damage\n  Detection", "comments": "removed redundant postscripts", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Pavement condition evaluation is essential to time the preventative or\nrehabilitative actions and control distress propagation. Failing to conduct\ntimely evaluations can lead to severe structural and financial loss of the\ninfrastructure and complete reconstructions. Automated computer-aided surveying\nmeasures can provide a database of road damage patterns and their locations.\nThis database can be utilized for timely road repairs to gain the minimum cost\nof maintenance and the asphalt's maximum durability. This paper introduces a\ndeep learning-based surveying scheme to analyze the image-based distress data\nin real-time. A database consisting of a diverse population of crack distress\ntypes such as longitudinal, transverse, and alligator cracks, photographed\nusing mobile-device is used. Then, a family of efficient and scalable models\nthat are tuned for pavement crack detection is trained, and various\naugmentation policies are explored. Proposed models, resulted in F1-scores,\nranging from 52% to 56%, and average inference time from 178-10 images per\nsecond. Finally, the performance of the object detectors are examined, and\nerror analysis is reported against various images. The source code is available\nat https://github.com/mahdi65/roadDamageDetection2020.\n", "versions": [{"version": "v1", "created": "Wed, 18 Nov 2020 23:05:41 GMT"}, {"version": "v2", "created": "Wed, 25 Nov 2020 18:58:18 GMT"}, {"version": "v3", "created": "Thu, 17 Dec 2020 17:58:08 GMT"}], "update_date": "2020-12-18", "authors_parsed": [["Naddaf-Sh", "Sadra", ""], ["Naddaf-Sh", "M-Mahdi", ""], ["Kashani", "Amir R.", ""], ["Zargarzadeh", "Hassan", ""]]}, {"id": "2011.09581", "submitter": "Tharindu Fernando", "authors": "Theekshana Dissanayake, Tharindu Fernando, Simon Denman, Sridha\n  Sridharan, Clinton Fookes", "title": "Patient-independent Epileptic Seizure Prediction using Deep Learning\n  Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Objective: Epilepsy is one of the most prevalent neurological diseases among\nhumans and can lead to severe brain injuries, strokes, and brain tumors. Early\ndetection of seizures can help to mitigate injuries, and can be used to aid the\ntreatment of patients with epilepsy. The purpose of a seizure prediction system\nis to successfully identify the pre-ictal brain stage, which occurs before a\nseizure event. Patient-independent seizure prediction models are designed to\noffer accurate performance across multiple subjects within a dataset, and have\nbeen identified as a real-world solution to the seizure prediction problem.\nHowever, little attention has been given for designing such models to adapt to\nthe high inter-subject variability in EEG data. Methods: We propose two\npatient-independent deep learning architectures with different learning\nstrategies that can learn a global function utilizing data from multiple\nsubjects. Results: Proposed models achieve state-of-the-art performance for\nseizure prediction on the CHB-MIT-EEG dataset, demonstrating 88.81% and 91.54%\naccuracy respectively. Conclusions: The Siamese model trained on the proposed\nlearning strategy is able to learn patterns related to patient variations in\ndata while predicting seizures. Significance: Our models show superior\nperformance for patient-independent seizure prediction, and the same\narchitecture can be used as a patient-specific classifier after model\nadaptation. We are the first study that employs model interpretation to\nunderstand classifier behavior for the task for seizure prediction, and we also\nshow that the MFCC feature map utilized by our models contains predictive\nbiomarkers related to interictal and pre-ictal brain states.\n", "versions": [{"version": "v1", "created": "Wed, 18 Nov 2020 23:13:48 GMT"}], "update_date": "2020-11-20", "authors_parsed": [["Dissanayake", "Theekshana", ""], ["Fernando", "Tharindu", ""], ["Denman", "Simon", ""], ["Sridharan", "Sridha", ""], ["Fookes", "Clinton", ""]]}, {"id": "2011.09584", "submitter": "Clemens Eppner", "authors": "Clemens Eppner, Arsalan Mousavian, Dieter Fox", "title": "ACRONYM: A Large-Scale Grasp Dataset Based on Simulation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce ACRONYM, a dataset for robot grasp planning based on physics\nsimulation. The dataset contains 17.7M parallel-jaw grasps, spanning 8872\nobjects from 262 different categories, each labeled with the grasp result\nobtained from a physics simulator. We show the value of this large and diverse\ndataset by using it to train two state-of-the-art learning-based grasp planning\nalgorithms. Grasp performance improves significantly when compared to the\noriginal smaller dataset. Data and tools can be accessed at\nhttps://sites.google.com/nvidia.com/graspdataset.\n", "versions": [{"version": "v1", "created": "Wed, 18 Nov 2020 23:24:00 GMT"}], "update_date": "2020-11-20", "authors_parsed": [["Eppner", "Clemens", ""], ["Mousavian", "Arsalan", ""], ["Fox", "Dieter", ""]]}, {"id": "2011.09594", "submitter": "Tong Ke", "authors": "Tong Ke, Tien Do, Khiem Vuong, Kourosh Sartipi, and Stergios I.\n  Roumeliotis", "title": "Deep Multi-view Depth Estimation with Predicted Uncertainty", "comments": "IEEE International Conference on Robotics and Automation (ICRA 2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we address the problem of estimating dense depth from a\nsequence of images using deep neural networks. Specifically, we employ a\ndense-optical-flow network to compute correspondences and then triangulate the\npoint cloud to obtain an initial depth map.Parts of the point cloud, however,\nmay be less accurate than others due to lack of common observations or small\nparallax. To further increase the triangulation accuracy, we introduce a\ndepth-refinement network (DRN) that optimizes the initial depth map based on\nthe image's contextual cues. In particular, the DRN contains an iterative\nrefinement module (IRM) that improves the depth accuracy over iterations by\nrefining the deep features. Lastly, the DRN also predicts the uncertainty in\nthe refined depths, which is desirable in applications such as measurement\nselection for scene reconstruction. We show experimentally that our algorithm\noutperforms state-of-the-art approaches in terms of depth accuracy, and verify\nthat our predicted uncertainty is highly correlated to the actual depth error.\n", "versions": [{"version": "v1", "created": "Thu, 19 Nov 2020 00:22:09 GMT"}, {"version": "v2", "created": "Sat, 27 Mar 2021 14:33:40 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Ke", "Tong", ""], ["Do", "Tien", ""], ["Vuong", "Khiem", ""], ["Sartipi", "Kourosh", ""], ["Roumeliotis", "Stergios I.", ""]]}, {"id": "2011.09608", "submitter": "Soopil Kim", "authors": "Soopil Kim, Sion An, Philip Chikontwe, Sang Hyun Park", "title": "Bidirectional RNN-based Few Shot Learning for 3D Medical Image\n  Segmentation", "comments": "Submitted to AAAI21", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Segmentation of organs of interest in 3D medical images is necessary for\naccurate diagnosis and longitudinal studies. Though recent advances using deep\nlearning have shown success for many segmentation tasks, large datasets are\nrequired for high performance and the annotation process is both time consuming\nand labor intensive. In this paper, we propose a 3D few shot segmentation\nframework for accurate organ segmentation using limited training samples of the\ntarget organ annotation. To achieve this, a U-Net like network is designed to\npredict segmentation by learning the relationship between 2D slices of support\ndata and a query image, including a bidirectional gated recurrent unit (GRU)\nthat learns consistency of encoded features between adjacent slices. Also, we\nintroduce a transfer learning method to adapt the characteristics of the target\nimage and organ by updating the model before testing with arbitrary support and\nquery data sampled from the support data. We evaluate our proposed model using\nthree 3D CT datasets with annotations of different organs. Our model yielded\nsignificantly improved performance over state-of-the-art few shot segmentation\nmodels and was comparable to a fully supervised model trained with more target\ntraining data.\n", "versions": [{"version": "v1", "created": "Thu, 19 Nov 2020 01:44:55 GMT"}], "update_date": "2020-11-20", "authors_parsed": [["Kim", "Soopil", ""], ["An", "Sion", ""], ["Chikontwe", "Philip", ""], ["Park", "Sang Hyun", ""]]}, {"id": "2011.09619", "submitter": "Soroush Ziaeinejad", "authors": "Ali Atghaei, Soroush Ziaeinejad, Mohammad Rahmati", "title": "Abnormal Event Detection in Urban Surveillance Videos Using GAN and\n  Transfer Learning", "comments": "7 pages, 9 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Abnormal event detection (AED) in urban surveillance videos has multiple\nchallenges. Unlike other computer vision problems, the AED is not solely\ndependent on the content of frames. It also depends on the appearance of the\nobjects and their movements in the scene. Various methods have been proposed to\naddress the AED problem. Among those, deep learning based methods show the best\nresults. This paper is based on deep learning methods and provides an effective\nway to detect and locate abnormal events in videos by handling spatio temporal\ndata. This paper uses generative adversarial networks (GANs) and performs\ntransfer learning algorithms on pre trained convolutional neural network (CNN)\nwhich result in an accurate and efficient model. The efficiency of the model is\nfurther improved by processing the optical flow information of the video. This\npaper runs experiments on two benchmark datasets for AED problem (UCSD Peds1\nand UCSD Peds2) and compares the results with other previous methods. The\ncomparisons are based on various criteria such as area under curve (AUC) and\ntrue positive rate (TPR). Experimental results show that the proposed method\ncan effectively detect and locate abnormal events in crowd scenes.\n", "versions": [{"version": "v1", "created": "Thu, 19 Nov 2020 02:39:35 GMT"}], "update_date": "2020-11-20", "authors_parsed": [["Atghaei", "Ali", ""], ["Ziaeinejad", "Soroush", ""], ["Rahmati", "Mohammad", ""]]}, {"id": "2011.09634", "submitter": "Yujie Zhong", "authors": "Yujie Zhong, Linhai Xie, Sen Wang, Lucia Specia, Yishu Miao", "title": "Watch and Learn: Mapping Language and Noisy Real-world Videos with\n  Self-supervision", "comments": "NeurIPS 2020 Self-Supervised Learning Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we teach machines to understand visuals and natural language\nby learning the mapping between sentences and noisy video snippets without\nexplicit annotations. Firstly, we define a self-supervised learning framework\nthat captures the cross-modal information. A novel adversarial learning module\nis then introduced to explicitly handle the noises in the natural videos, where\nthe subtitle sentences are not guaranteed to be strongly corresponded to the\nvideo snippets. For training and evaluation, we contribute a new dataset\n`ApartmenTour' that contains a large number of online videos and subtitles. We\ncarry out experiments on the bidirectional retrieval tasks between sentences\nand videos, and the results demonstrate that our proposed model achieves the\nstate-of-the-art performance on both retrieval tasks and exceeds several strong\nbaselines. The dataset can be downloaded at https://github.com/zyj-13/WAL.\n", "versions": [{"version": "v1", "created": "Thu, 19 Nov 2020 03:43:56 GMT"}, {"version": "v2", "created": "Mon, 11 Jan 2021 09:52:47 GMT"}], "update_date": "2021-01-12", "authors_parsed": [["Zhong", "Yujie", ""], ["Xie", "Linhai", ""], ["Wang", "Sen", ""], ["Specia", "Lucia", ""], ["Miao", "Yishu", ""]]}, {"id": "2011.09654", "submitter": "Suihanjin Yu", "authors": "Suihanjin Yu, Youmin Zhang, Chen Wang, Xiao Bai, Liang Zhang, Edwin R.\n  Hancock", "title": "HMFlow: Hybrid Matching Optical Flow Network for Small and Fast-Moving\n  Objects", "comments": "8 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In optical flow estimation task, coarse-to-fine (C2F) warping strategy is\nwidely used to deal with the large displacement problem and provides efficiency\nand speed. However, limited by the small search range between the first images\nand warped second images, current coarse-to-fine optical flow networks fail to\ncapture small and fast-moving objects which disappear at coarse resolution\nlevels. To address this problem, we introduce a lightweight but effective\nGlobal Matching Component (GMC) to grab global matching features. We propose a\nnew Hybrid Matching Optical Flow Network (HMFlow) by integrating GMC into\nexisting coarse-to-fine networks seamlessly. Besides keeping in high accuracy\nand small model size, our proposed HMFlow can apply global matching features to\nguide the network to discover the small and fast-moving objects mismatched by\nlocal matching features. We also build a new dataset, named Small and\nFast-Moving Chairs (SFChairs), for evaluation. The experimental results show\nthat our proposed network achieves considerable performance, especially at\nregions with small and fast-moving objects.\n", "versions": [{"version": "v1", "created": "Thu, 19 Nov 2020 04:58:13 GMT"}], "update_date": "2020-11-20", "authors_parsed": [["Yu", "Suihanjin", ""], ["Zhang", "Youmin", ""], ["Wang", "Chen", ""], ["Bai", "Xiao", ""], ["Zhang", "Liang", ""], ["Hancock", "Edwin R.", ""]]}, {"id": "2011.09663", "submitter": "Ziad Al-Halah", "authors": "Ziad Al-Halah, Kristen Grauman", "title": "Modeling Fashion Influence from Photos", "comments": "To appear in the IEEE Transactions on Multimedia, 2020. Project page:\n  https://www.cs.utexas.edu/~ziad/influence_from_photos.html. arXiv admin note:\n  substantial text overlap with arXiv:2004.01316", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The evolution of clothing styles and their migration across the world is\nintriguing, yet difficult to describe quantitatively. We propose to discover\nand quantify fashion influences from catalog and social media photos. We\nexplore fashion influence along two channels: geolocation and fashion brands.\nWe introduce an approach that detects which of these entities influence which\nother entities in terms of propagating their styles. We then leverage the\ndiscovered influence patterns to inform a novel forecasting model that predicts\nthe future popularity of any given style within any given city or brand. To\ndemonstrate our idea, we leverage public large-scale datasets of 7.7M Instagram\nphotos from 44 major world cities (where styles are worn with variable\nfrequency) as well as 41K Amazon product photos (where styles are purchased\nwith variable frequency). Our model learns directly from the image data how\nstyles move between locations and how certain brands affect each other's\ndesigns in a predictable way. The discovered influence relationships reveal how\nboth cities and brands exert and receive fashion influence for an array of\nvisual styles inferred from the images. Furthermore, the proposed forecasting\nmodel achieves state-of-the-art results for challenging style forecasting\ntasks. Our results indicate the advantage of grounding visual style evolution\nboth spatially and temporally, and for the first time, they quantify the\npropagation of inter-brand and inter-city influences.\n", "versions": [{"version": "v1", "created": "Tue, 17 Nov 2020 20:24:03 GMT"}], "update_date": "2020-11-20", "authors_parsed": [["Al-Halah", "Ziad", ""], ["Grauman", "Kristen", ""]]}, {"id": "2011.09670", "submitter": "Xue Yang", "authors": "Xue Yang, Liping Hou, Yue Zhou, Wentao Wang, Junchi Yan", "title": "Dense Label Encoding for Boundary Discontinuity Free Rotation Detection", "comments": "12 pages, 6 figures, 9 tables, accepted by CVPR21", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Rotation detection serves as a fundamental building block in many visual\napplications involving aerial image, scene text, and face etc. Differing from\nthe dominant regression-based approaches for orientation estimation, this paper\nexplores a relatively less-studied methodology based on classification. The\nhope is to inherently dismiss the boundary discontinuity issue as encountered\nby the regression-based detectors. We propose new techniques to push its\nfrontier in two aspects: i) new encoding mechanism: the design of two Densely\nCoded Labels (DCL) for angle classification, to replace the Sparsely Coded\nLabel (SCL) in existing classification-based detectors, leading to three times\ntraining speed increase as empirically observed across benchmarks, further with\nnotable improvement in detection accuracy; ii) loss re-weighting: we propose\nAngle Distance and Aspect Ratio Sensitive Weighting (ADARSW), which improves\nthe detection accuracy especially for square-like objects, by making DCL-based\ndetectors sensitive to angular distance and object's aspect ratio. Extensive\nexperiments and visual analysis on large-scale public datasets for aerial\nimages i.e. DOTA, UCAS-AOD, HRSC2016, as well as scene text dataset ICDAR2015\nand MLT, show the effectiveness of our approach. The source code is available\nat https://github.com/Thinklab-SJTU/DCL_RetinaNet_Tensorflow and is also\nintegrated in our open source rotation detection benchmark:\nhttps://github.com/yangxue0827/RotationDetection.\n", "versions": [{"version": "v1", "created": "Thu, 19 Nov 2020 05:42:02 GMT"}, {"version": "v2", "created": "Wed, 3 Mar 2021 03:31:54 GMT"}, {"version": "v3", "created": "Sun, 9 May 2021 02:50:43 GMT"}, {"version": "v4", "created": "Tue, 25 May 2021 08:54:16 GMT"}], "update_date": "2021-05-26", "authors_parsed": [["Yang", "Xue", ""], ["Hou", "Liping", ""], ["Zhou", "Yue", ""], ["Wang", "Wentao", ""], ["Yan", "Junchi", ""]]}, {"id": "2011.09677", "submitter": "Ming Qian", "authors": "Ming Qian and Min Xia and Chunyi Sun and Zhiwei Wang and Liguo Weng", "title": "Defocus Blur Detection via Salient Region Detection Prior", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Defocus blur always occurred in photos when people take photos by Digital\nSingle Lens Reflex Camera(DSLR), giving salient region and aesthetic pleasure.\nDefocus blur Detection aims to separate the out-of-focus and depth-of-field\nareas in photos, which is an important work in computer vision. Current works\nfor defocus blur detection mainly focus on the designing of networks, the\noptimizing of the loss function, and the application of multi-stream strategy,\nmeanwhile, these works do not pay attention to the shortage of training data.\nIn this work, to address the above data-shortage problem, we turn to rethink\nthe relationship between two tasks: defocus blur detection and salient region\ndetection. In an image with bokeh effect, it is obvious that the salient region\nand the depth-of-field area overlap in most cases. So we first train our\nnetwork on the salient region detection tasks, then transfer the pre-trained\nmodel to the defocus blur detection tasks. Besides, we propose a novel network\nfor defocus blur detection. Experiments show that our transfer strategy works\nwell on many current models, and demonstrate the superiority of our network.\n", "versions": [{"version": "v1", "created": "Thu, 19 Nov 2020 05:56:11 GMT"}], "update_date": "2020-11-20", "authors_parsed": [["Qian", "Ming", ""], ["Xia", "Min", ""], ["Sun", "Chunyi", ""], ["Wang", "Zhiwei", ""], ["Weng", "Liguo", ""]]}, {"id": "2011.09695", "submitter": "Anushikha Singh", "authors": "Anushikha Singh, Brejesh Lall, B. K. Panigrahi, Anjali Agrawal, Anurag\n  Agrawal, DJ Christopher, Balamugesh Thangakunam", "title": "Deep LF-Net: Semantic Lung Segmentation from Indian Chest Radiographs\n  Including Severely Unhealthy Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A chest radiograph, commonly called chest x-ray (CxR), plays a vital role in\nthe diagnosis of various lung diseases, such as lung cancer, tuberculosis,\npneumonia, and many more. Automated segmentation of the lungs is an important\nstep to design a computer-aided diagnostic tool for examination of a CxR.\nPrecise lung segmentation is considered extremely challenging because of\nvariance in the shape of the lung caused by health issues, age, and gender. The\nproposed work investigates the use of an efficient deep convolutional neural\nnetwork for accurate segmentation of lungs from CxR. We attempt an end to end\nDeepLabv3+ network which integrates DeepLab architecture, encoder-decoder, and\ndilated convolution for semantic lung segmentation with fast training and high\naccuracy. We experimented with the different pre-trained base networks:\nResnet18 and Mobilenetv2, associated with the Deeplabv3+ model for performance\nanalysis. The proposed approach does not require any pre-processing technique\non chest x-ray images before being fed to a neural network. Morphological\noperations were used to remove false positives that occurred during semantic\nsegmentation. We construct a CxR dataset of the Indian population that contain\nhealthy and unhealthy CxRs of clinically confirmed patients of tuberculosis,\nchronic obstructive pulmonary disease, interstitial lung disease, pleural\neffusion, and lung cancer. The proposed method is tested on 688 images of our\nIndian CxR dataset including images with severe abnormal findings to validate\nits robustness. We also experimented on commonly used benchmark datasets such\nas Japanese Society of Radiological Technology; Montgomery County, USA; and\nShenzhen, China for state-of-the-art comparison. The performance of our method\nis tested against techniques described in the literature and achieved the\nhighest accuracy for lung segmentation on Indian and public datasets.\n", "versions": [{"version": "v1", "created": "Thu, 19 Nov 2020 07:21:02 GMT"}], "update_date": "2020-11-20", "authors_parsed": [["Singh", "Anushikha", ""], ["Lall", "Brejesh", ""], ["Panigrahi", "B. K.", ""], ["Agrawal", "Anjali", ""], ["Agrawal", "Anurag", ""], ["Christopher", "DJ", ""], ["Thangakunam", "Balamugesh", ""]]}, {"id": "2011.09697", "submitter": "Muhammad Kashif Ali Mr.", "authors": "Muhammad Kashif Ali, Sangjoon Yu, Tae Hyun Kim", "title": "Learning Deep Video Stabilization without Optical Flow", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Learning the necessary high-level reasoning for video stabilization without\nthe help of optical flow has proved to be one of the most challenging tasks in\nthe field of computer vision. In this work, we present an iterative frame\ninterpolation strategy to generate a novel dataset that is diverse enough to\nformulate video stabilization as a supervised learning problem unassisted by\noptical flow. A major benefit of treating video stabilization as a pure RGB\nbased generative task over the conventional optical flow assisted approaches is\nthe preservation of content and resolution, which is usually obstructed in the\nlatter approaches. To do so, we provide a new video stabilization dataset and\ntrain an efficient network that can produce competitive stabilization results\nin a fraction of the time taken to do the same with the recent iterative frame\ninterpolation schema. Our method provides qualitatively and quantitatively\nbetter results than those generated through state-of-the-art video\nstabilization methods. To the best of our knowledge, this is the only work that\ndemonstrates the importance of perspective in formulating video stabilization\nas a deep learning problem instead of replacing it with an inter-frame motion\nmeasure\n", "versions": [{"version": "v1", "created": "Thu, 19 Nov 2020 07:26:06 GMT"}], "update_date": "2020-11-20", "authors_parsed": [["Ali", "Muhammad Kashif", ""], ["Yu", "Sangjoon", ""], ["Kim", "Tae Hyun", ""]]}, {"id": "2011.09699", "submitter": "Yunfan Liu", "authors": "Yunfan Liu, Qi Li, Zhenan Sun, Tieniu Tan", "title": "Style Intervention: How to Achieve Spatial Disentanglement with\n  Style-based Generators?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Generative Adversarial Networks (GANs) with style-based generators (e.g.\nStyleGAN) successfully enable semantic control over image synthesis, and recent\nstudies have also revealed that interpretable image translations could be\nobtained by modifying the latent code. However, in terms of the low-level image\ncontent, traveling in the latent space would lead to `spatially entangled\nchanges' in corresponding images, which is undesirable in many real-world\napplications where local editing is required. To solve this problem, we analyze\nproperties of the 'style space' and explore the possibility of controlling the\nlocal translation with pre-trained style-based generators. Concretely, we\npropose 'Style Intervention', a lightweight optimization-based algorithm which\ncould adapt to arbitrary input images and render natural translation effects\nunder flexible objectives. We verify the performance of the proposed framework\nin facial attribute editing on high-resolution images, where both photo-realism\nand consistency are required. Extensive qualitative results demonstrate the\neffectiveness of our method, and quantitative measurements also show that the\nproposed algorithm outperforms state-of-the-art benchmarks in various aspects.\n", "versions": [{"version": "v1", "created": "Thu, 19 Nov 2020 07:37:31 GMT"}], "update_date": "2020-11-20", "authors_parsed": [["Liu", "Yunfan", ""], ["Li", "Qi", ""], ["Sun", "Zhenan", ""], ["Tan", "Tieniu", ""]]}, {"id": "2011.09701", "submitter": "Jiang He", "authors": "Jiang He, Jie Li, Qiangqiang Yuan, Huanfeng Shen, and Liangpei Zhang", "title": "Spectral Response Function Guided Deep Optimization-driven Network for\n  Spectral Super-resolution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Hyperspectral images are crucial for many research works. Spectral\nsuper-resolution (SSR) is a method used to obtain high spatial resolution (HR)\nhyperspectral images from HR multispectral images. Traditional SSR methods\ninclude model-driven algorithms and deep learning. By unfolding a variational\nmethod, this paper proposes an optimization-driven convolutional neural network\n(CNN) with a deep spatial-spectral prior, resulting in physically interpretable\nnetworks. Unlike the fully data-driven CNN, auxiliary spectral response\nfunction (SRF) is utilized to guide CNNs to group the bands with spectral\nrelevance. In addition, the channel attention module (CAM) and reformulated\nspectral angle mapper loss function are applied to achieve an effective\nreconstruction model. Finally, experiments on two types of datasets, including\nnatural and remote sensing images, demonstrate the spectral enhancement effect\nof the proposed method. And the classification results on the remote sensing\ndataset also verified the validity of the information enhanced by the proposed\nmethod.\n", "versions": [{"version": "v1", "created": "Thu, 19 Nov 2020 07:52:45 GMT"}, {"version": "v2", "created": "Tue, 8 Dec 2020 13:38:53 GMT"}], "update_date": "2020-12-09", "authors_parsed": [["He", "Jiang", ""], ["Li", "Jie", ""], ["Yuan", "Qiangqiang", ""], ["Shen", "Huanfeng", ""], ["Zhang", "Liangpei", ""]]}, {"id": "2011.09704", "submitter": "Zongyu Guo", "authors": "Zongyu Guo, Zhizheng Zhang, Runsen Feng, Zhibo Chen", "title": "Causal Contextual Prediction for Learned Image Compression", "comments": "13 pages", "journal-ref": null, "doi": "10.1109/TCSVT.2021.3089491", "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Over the past several years, we have witnessed impressive progress in the\nfield of learned image compression. Recent learned image codecs are commonly\nbased on autoencoders, that first encode an image into low-dimensional latent\nrepresentations and then decode them for reconstruction purposes. To capture\nspatial dependencies in the latent space, prior works exploit hyperprior and\nspatial context model to build an entropy model, which estimates the bit-rate\nfor end-to-end rate-distortion optimization. However, such an entropy model is\nsuboptimal from two aspects: (1) It fails to capture spatially global\ncorrelations among the latents. (2) Cross-channel relationships of the latents\nare still underexplored. In this paper, we propose the concept of separate\nentropy coding to leverage a serial decoding process for causal contextual\nentropy prediction in the latent space. A causal context model is proposed that\nseparates the latents across channels and makes use of cross-channel\nrelationships to generate highly informative contexts. Furthermore, we propose\na causal global prediction model, which is able to find global reference points\nfor accurate predictions of unknown points. Both these two models facilitate\nentropy estimation without the transmission of overhead. In addition, we\nfurther adopt a new separate attention module to build more powerful transform\nnetworks. Experimental results demonstrate that our full image compression\nmodel outperforms standard VVC/H.266 codec on Kodak dataset in terms of both\nPSNR and MS-SSIM, yielding the state-of-the-art rate-distortion performance.\n", "versions": [{"version": "v1", "created": "Thu, 19 Nov 2020 08:15:10 GMT"}, {"version": "v2", "created": "Mon, 21 Dec 2020 14:51:59 GMT"}, {"version": "v3", "created": "Mon, 4 Jan 2021 03:05:34 GMT"}, {"version": "v4", "created": "Thu, 17 Jun 2021 03:51:28 GMT"}], "update_date": "2021-06-18", "authors_parsed": [["Guo", "Zongyu", ""], ["Zhang", "Zhizheng", ""], ["Feng", "Runsen", ""], ["Chen", "Zhibo", ""]]}, {"id": "2011.09737", "submitter": "Xiangyu Zhu", "authors": "Xiangyu Zhu, Hao Wang, Hongyan Fei, Zhen Lei, Stan Z. Li", "title": "Face Forgery Detection by 3D Decomposition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Detecting digital face manipulation has attracted extensive attention due to\nfake media's potential harms to the public. However, recent advances have been\nable to reduce the forgery signals to a low magnitude. Decomposition, which\nreversibly decomposes an image into several constituent elements, is a\npromising way to highlight the hidden forgery details. In this paper, we\nconsider a face image as the production of the intervention of the underlying\n3D geometry and the lighting environment, and decompose it in a computer\ngraphics view. Specifically, by disentangling the face image into 3D shape,\ncommon texture, identity texture, ambient light, and direct light, we find the\ndevil lies in the direct light and the identity texture. Based on this\nobservation, we propose to utilize facial detail, which is the combination of\ndirect light and identity texture, as the clue to detect the subtle forgery\npatterns. Besides, we highlight the manipulated region with a supervised\nattention mechanism and introduce a two-stream structure to exploit both face\nimage and facial detail together as a multi-modality task. Extensive\nexperiments indicate the effectiveness of the extra features extracted from the\nfacial detail, and our method achieves the state-of-the-art performance.\n", "versions": [{"version": "v1", "created": "Thu, 19 Nov 2020 09:25:44 GMT"}], "update_date": "2020-11-20", "authors_parsed": [["Zhu", "Xiangyu", ""], ["Wang", "Hao", ""], ["Fei", "Hongyan", ""], ["Lei", "Zhen", ""], ["Li", "Stan Z.", ""]]}, {"id": "2011.09763", "submitter": "Tim Prangemeier", "authors": "Tim Prangemeier, Christoph Reich, Heinz Koeppl", "title": "Attention-Based Transformers for Instance Segmentation of Cells in\n  Microstructures", "comments": "IEEE BIBM 2020 (accepted)", "journal-ref": null, "doi": "10.1109/BIBM49941.2020.9313305", "report-no": null, "categories": "cs.CV eess.SP physics.ins-det q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Detecting and segmenting object instances is a common task in biomedical\napplications. Examples range from detecting lesions on functional magnetic\nresonance images, to the detection of tumours in histopathological images and\nextracting quantitative single-cell information from microscopy imagery, where\ncell segmentation is a major bottleneck. Attention-based transformers are\nstate-of-the-art in a range of deep learning fields. They have recently been\nproposed for segmentation tasks where they are beginning to outperforming other\nmethods. We present a novel attention-based cell detection transformer\n(Cell-DETR) for direct end-to-end instance segmentation. While the segmentation\nperformance is on par with a state-of-the-art instance segmentation method,\nCell-DETR is simpler and faster. We showcase the method's contribution in a the\ntypical use case of segmenting yeast in microstructured environments, commonly\nemployed in systems or synthetic biology. For the specific use case, the\nproposed method surpasses the state-of-the-art tools for semantic segmentation\nand additionally predicts the individual object instances. The fast and\naccurate instance segmentation performance increases the experimental\ninformation yield for a posteriori data processing and makes online monitoring\nof experiments and closed-loop optimal experimental design feasible.\n", "versions": [{"version": "v1", "created": "Thu, 19 Nov 2020 10:49:56 GMT"}, {"version": "v2", "created": "Fri, 20 Nov 2020 08:04:27 GMT"}], "update_date": "2021-04-12", "authors_parsed": [["Prangemeier", "Tim", ""], ["Reich", "Christoph", ""], ["Koeppl", "Heinz", ""]]}, {"id": "2011.09766", "submitter": "Zhuo Zheng", "authors": "Zhuo Zheng, Yanfei Zhong, Junjue Wang, Ailong Ma", "title": "Foreground-Aware Relation Network for Geospatial Object Segmentation in\n  High Spatial Resolution Remote Sensing Imagery", "comments": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\n  Recognition(CVPR). 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Geospatial object segmentation, as a particular semantic segmentation task,\nalways faces with larger-scale variation, larger intra-class variance of\nbackground, and foreground-background imbalance in the high spatial resolution\n(HSR) remote sensing imagery. However, general semantic segmentation methods\nmainly focus on scale variation in the natural scene, with inadequate\nconsideration of the other two problems that usually happen in the large area\nearth observation scene. In this paper, we argue that the problems lie on the\nlack of foreground modeling and propose a foreground-aware relation network\n(FarSeg) from the perspectives of relation-based and optimization-based\nforeground modeling, to alleviate the above two problems. From perspective of\nrelation, FarSeg enhances the discrimination of foreground features via\nforeground-correlated contexts associated by learning foreground-scene\nrelation. Meanwhile, from perspective of optimization, a foreground-aware\noptimization is proposed to focus on foreground examples and hard examples of\nbackground during training for a balanced optimization. The experimental\nresults obtained using a large scale dataset suggest that the proposed method\nis superior to the state-of-the-art general semantic segmentation methods and\nachieves a better trade-off between speed and accuracy. Code has been made\navailable at: \\url{https://github.com/Z-Zheng/FarSeg}.\n", "versions": [{"version": "v1", "created": "Thu, 19 Nov 2020 10:57:43 GMT"}], "update_date": "2020-11-20", "authors_parsed": [["Zheng", "Zhuo", ""], ["Zhong", "Yanfei", ""], ["Wang", "Junjue", ""], ["Ma", "Ailong", ""]]}, {"id": "2011.09768", "submitter": "Weize Quan", "authors": "Xuewei Bian, Chaoqun Wang, Weize Quan, Juntao Ye, Xiaopeng Zhang,\n  Dong-Ming Yan", "title": "Scene text removal via cascaded text stroke detection and erasing", "comments": "14 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent learning-based approaches show promising performance improvement for\nscene text removal task. However, these methods usually leave some remnants of\ntext and obtain visually unpleasant results. In this work, we propose a novel\n\"end-to-end\" framework based on accurate text stroke detection. Specifically,\nwe decouple the text removal problem into text stroke detection and stroke\nremoval. We design a text stroke detection network and a text removal\ngeneration network to solve these two sub-problems separately. Then, we combine\nthese two networks as a processing unit, and cascade this unit to obtain the\nfinal model for text removal. Experimental results demonstrate that the\nproposed method significantly outperforms the state-of-the-art approaches for\nlocating and erasing scene text. Since current publicly available datasets are\nall synthetic and cannot properly measure the performance of different methods,\nwe therefore construct a new real-world dataset, which will be released to\nfacilitate the relevant research.\n", "versions": [{"version": "v1", "created": "Thu, 19 Nov 2020 11:05:13 GMT"}], "update_date": "2020-11-20", "authors_parsed": [["Bian", "Xuewei", ""], ["Wang", "Chaoqun", ""], ["Quan", "Weize", ""], ["Ye", "Juntao", ""], ["Zhang", "Xiaopeng", ""], ["Yan", "Dong-Ming", ""]]}, {"id": "2011.09778", "submitter": "Anushikha Singh", "authors": "Anushikha Singh, Brejesh Lall, B.K. Panigrahi, Anjali Agrawal, Anurag\n  Agrawal, Balamugesh Thangakunam, DJ Christopher", "title": "Deep Learning for Automated Screening of Tuberculosis from Indian Chest\n  X-rays: Analysis and Update", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Background and Objective: Tuberculosis (TB) is a significant public health\nissue and a leading cause of death worldwide. Millions of deaths can be averted\nby early diagnosis and successful treatment of TB patients. Automated diagnosis\nof TB holds vast potential to assist medical experts in expediting and\nimproving its diagnosis, especially in developing countries like India, where\nthere is a shortage of trained medical experts and radiologists. To date,\nseveral deep learning based methods for automated detection of TB from chest\nradiographs have been proposed. However, the performance of a few of these\nmethods on the Indian chest radiograph data set has been suboptimal, possibly\ndue to different texture of the lungs on chest radiographs of Indian subjects\ncompared to other countries. Thus deep learning for accurate and automated\ndiagnosis of TB on Indian datasets remains an important subject of research.\nMethods: The proposed work explores the performance of convolutional neural\nnetworks (CNNs) for the diagnosis of TB in Indian chest x-ray images. Three\ndifferent pre-trained neural network models, AlexNet, GoogLenet, and ResNet are\nused to classify chest x-ray images into healthy or TB infected. The proposed\napproach does not require any pre-processing technique. Also, other works use\npre-trained NNs as a tool for crafting features and then apply standard\nclassification techniques. However, we attempt an end to end NN model based\ndiagnosis of TB from chest x-rays. The proposed visualization tool can also be\nused by radiologists in the screening of large datasets. Results: The proposed\nmethod achieved 93.40% accuracy with 98.60% sensitivity to diagnose TB for the\nIndian population. Conclusions: The performance of the proposed method is also\ntested against techniques described in the literature. The proposed method\noutperforms the state of art on Indian and Shenzhen datasets.\n", "versions": [{"version": "v1", "created": "Thu, 19 Nov 2020 11:34:27 GMT"}], "update_date": "2020-11-20", "authors_parsed": [["Singh", "Anushikha", ""], ["Lall", "Brejesh", ""], ["Panigrahi", "B. K.", ""], ["Agrawal", "Anjali", ""], ["Agrawal", "Anurag", ""], ["Thangakunam", "Balamugesh", ""], ["Christopher", "DJ", ""]]}, {"id": "2011.09781", "submitter": "Yuanqiang Cai", "authors": "Yuanqiang Cai, Chang Liu, Weiqiang Wang, Qixiang Ye", "title": "Towards Spatio-Temporal Video Scene Text Detection via Temporal\n  Clustering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  With only bounding-box annotations in the spatial domain, existing video\nscene text detection (VSTD) benchmarks lack temporal relation of text instances\namong video frames, which hinders the development of video text-related\napplications. In this paper, we systematically introduce a new large-scale\nbenchmark, named as STVText4, a well-designed spatial-temporal detection metric\n(STDM), and a novel clustering-based baseline method, referred to as Temporal\nClustering (TC). STVText4 opens a challenging yet promising direction of VSTD,\ntermed as ST-VSTD, which targets at simultaneously detecting video scene texts\nin both spatial and temporal domains. STVText4 contains more than 1.4 million\ntext instances from 161,347 video frames of 106 videos, where each instance is\nannotated with not only spatial bounding box and temporal range but also four\nintrinsic attributes, including legibility, density, scale, and lifecycle, to\nfacilitate the community. With continuous propagation of identical texts in the\nvideo sequence, TC can accurately output the spatial quadrilateral and temporal\nrange of the texts, which sets a strong baseline for ST-VSTD. Experiments\ndemonstrate the efficacy of our method and the great academic and practical\nvalue of the STVText4. The dataset and code will be available soon.\n", "versions": [{"version": "v1", "created": "Thu, 19 Nov 2020 11:40:35 GMT"}], "update_date": "2020-11-20", "authors_parsed": [["Cai", "Yuanqiang", ""], ["Liu", "Chang", ""], ["Wang", "Weiqiang", ""], ["Ye", "Qixiang", ""]]}, {"id": "2011.09783", "submitter": "Henrik Pedersen", "authors": "S{\\o}ren Rasmussen, Karsten {\\O}stergaard Noe, Oliver Gyldenberg\n  Hjermitslev and Henrik Pedersen", "title": "DeepMorph: A System for Hiding Bitstrings in Morphable Vector Drawings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We introduce DeepMorph, an information embedding technique for vector\ndrawings. Provided a vector drawing, such as a Scalable Vector Graphics (SVG)\nfile, our method embeds bitstrings in the image by perturbing the drawing\nprimitives (lines, circles, etc.). This results in a morphed image that can be\ndecoded to recover the original bitstring. The use-case is similar to that of\nthe well-known QR code, but our solution provides creatives with artistic\nfreedom to transfer digital information via drawings of their own design. The\nmethod comprises two neural networks, which are trained jointly: an encoder\nnetwork that transforms a bitstring into a perturbation of the drawing\nprimitives, and a decoder network that recovers the bitstring from an image of\nthe morphed drawing. To enable end-to-end training via back propagation, we\nintroduce a soft rasterizer, which is differentiable with respect to\nperturbations of the drawing primitives. In order to add robustness towards\nreal-world image capture conditions, image corruptions are injected between the\nsoft rasterizer and the decoder. Further, the addition of an object detection\nand camera pose estimation system enables decoding of drawings in complex\nscenes as well as use of the drawings as markers for use in augmented reality\napplications. We demonstrate that our method reliably recovers bitstrings from\nreal-world photos of printed drawings, thereby providing a novel solution for\ncreatives to transfer digital information via artistic imagery.\n", "versions": [{"version": "v1", "created": "Thu, 19 Nov 2020 11:55:39 GMT"}], "update_date": "2020-11-20", "authors_parsed": [["Rasmussen", "S\u00f8ren", ""], ["Noe", "Karsten \u00d8stergaard", ""], ["Hjermitslev", "Oliver Gyldenberg", ""], ["Pedersen", "Henrik", ""]]}, {"id": "2011.09789", "submitter": "Shangxi Wu", "authors": "Shangxi Wu and Jitao Sang and Xian Zhao and Lizhang Chen", "title": "An Experimental Study of Semantic Continuity for Deep Learning Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning models suffer from the problem of semantic discontinuity: small\nperturbations in the input space tend to cause semantic-level interference to\nthe model output. We argue that the semantic discontinuity results from these\ninappropriate training targets and contributes to notorious issues such as\nadversarial robustness, interpretability, etc. We first conduct data analysis\nto provide evidence of semantic discontinuity in existing deep learning models,\nand then design a simple semantic continuity constraint which theoretically\nenables models to obtain smooth gradients and learn semantic-oriented features.\nQualitative and quantitative experiments prove that semantically continuous\nmodels successfully reduce the use of non-semantic information, which further\ncontributes to the improvement in adversarial robustness, interpretability,\nmodel transfer, and machine bias.\n", "versions": [{"version": "v1", "created": "Thu, 19 Nov 2020 12:23:28 GMT"}], "update_date": "2020-11-20", "authors_parsed": [["Wu", "Shangxi", ""], ["Sang", "Jitao", ""], ["Zhao", "Xian", ""], ["Chen", "Lizhang", ""]]}, {"id": "2011.09796", "submitter": "Chunhua Shen", "authors": "Hao Chen, Chunhua Shen, Zhi Tian", "title": "Unifying Instance and Panoptic Segmentation with Dynamic Rank-1\n  Convolutions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Recently, fully-convolutional one-stage networks have shown superior\nperformance comparing to two-stage frameworks for instance segmentation as\ntypically they can generate higher-quality mask predictions with less\ncomputation. In addition, their simple design opens up new opportunities for\njoint multi-task learning. In this paper, we demonstrate that adding a single\nclassification layer for semantic segmentation, fully-convolutional instance\nsegmentation networks can achieve state-of-the-art panoptic segmentation\nquality. This is made possible by our novel dynamic rank-1 convolution\n(DR1Conv), a novel dynamic module that can efficiently merge high-level context\ninformation with low-level detailed features which is beneficial for both\nsemantic and instance segmentation. Importantly, the proposed new method,\ntermed DR1Mask, can perform panoptic segmentation by adding a single layer. To\nour knowledge, DR1Mask is the first panoptic segmentation framework that\nexploits a shared feature map for both instance and semantic segmentation by\nconsidering both efficacy and efficiency. Not only our framework is much more\nefficient -- twice as fast as previous best two-branch approaches, but also the\nunified framework opens up opportunities for using the same context module to\nimprove the performance for both tasks. As a byproduct, when performing\ninstance segmentation alone, DR1Mask is 10% faster and 1 point in mAP more\naccurate than previous state-of-the-art instance segmentation network\nBlendMask. Code is available at: https://git.io/AdelaiDet\n", "versions": [{"version": "v1", "created": "Thu, 19 Nov 2020 12:42:10 GMT"}], "update_date": "2020-11-20", "authors_parsed": [["Chen", "Hao", ""], ["Shen", "Chunhua", ""], ["Tian", "Zhi", ""]]}, {"id": "2011.09804", "submitter": "Manuel Sam Ribeiro", "authors": "Manuel Sam Ribeiro, Jennifer Sanger, Jing-Xuan Zhang, Aciel Eshky,\n  Alan Wrench, Korin Richmond, Steve Renals", "title": "TaL: a synchronised multi-speaker corpus of ultrasound tongue imaging,\n  audio, and lip videos", "comments": "8 pages, 4 figures, Accepted to SLT2021, IEEE Spoken Language\n  Technology Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.CL cs.CV cs.SD eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the Tongue and Lips corpus (TaL), a multi-speaker corpus of audio,\nultrasound tongue imaging, and lip videos. TaL consists of two parts: TaL1 is a\nset of six recording sessions of one professional voice talent, a male native\nspeaker of English; TaL80 is a set of recording sessions of 81 native speakers\nof English without voice talent experience. Overall, the corpus contains 24\nhours of parallel ultrasound, video, and audio data, of which approximately\n13.5 hours are speech. This paper describes the corpus and presents benchmark\nresults for the tasks of speech recognition, speech synthesis\n(articulatory-to-acoustic mapping), and automatic synchronisation of ultrasound\nto audio. The TaL corpus is publicly available under the CC BY-NC 4.0 license.\n", "versions": [{"version": "v1", "created": "Thu, 19 Nov 2020 13:11:46 GMT"}], "update_date": "2020-11-20", "authors_parsed": [["Ribeiro", "Manuel Sam", ""], ["Sanger", "Jennifer", ""], ["Zhang", "Jing-Xuan", ""], ["Eshky", "Aciel", ""], ["Wrench", "Alan", ""], ["Richmond", "Korin", ""], ["Renals", "Steve", ""]]}, {"id": "2011.09808", "submitter": "Linxi Huan", "authors": "Linxi Huan, Nan Xue, Xianwei Zheng, Wei He, Jianya Gong, Gui-Song Xia", "title": "Unmixing Convolutional Features for Crisp Edge Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a context-aware tracing strategy (CATS) for crisp edge\ndetection with deep edge detectors, based on an observation that the\nlocalization ambiguity of deep edge detectors is mainly caused by the mixing\nphenomenon of convolutional neural networks: feature mixing in edge\nclassification and side mixing during fusing side predictions. The CATS\nconsists of two modules: a novel tracing loss that performs feature unmixing by\ntracing boundaries for better side edge learning, and a context-aware fusion\nblock that tackles the side mixing by aggregating the complementary merits of\nlearned side edges. Experiments demonstrate that the proposed CATS can be\nintegrated into modern deep edge detectors to improve localization accuracy.\nWith the vanilla VGG16 backbone, in terms of BSDS500 dataset, our CATS improves\nthe F-measure (ODS) of the RCF and BDCN deep edge detectors by 12% and 6%\nrespectively when evaluating without using the morphological non-maximal\nsuppression scheme for edge detection.\n", "versions": [{"version": "v1", "created": "Thu, 19 Nov 2020 13:30:15 GMT"}, {"version": "v2", "created": "Tue, 29 Jun 2021 13:01:08 GMT"}], "update_date": "2021-06-30", "authors_parsed": [["Huan", "Linxi", ""], ["Xue", "Nan", ""], ["Zheng", "Xianwei", ""], ["He", "Wei", ""], ["Gong", "Jianya", ""], ["Xia", "Gui-Song", ""]]}, {"id": "2011.09831", "submitter": "Javier Fumanal-Idocin Mr.", "authors": "Javier Fumanal-Idocin, Zdenko Tak\\'a\\v{c}, Javier Fern\\'andez Jose\n  Antonio Sanz, Harkaitz Goyena, Ching-Teng Lin, Yu-Kai Wang, Humberto Bustince", "title": "Interval-valued aggregation functions based on moderate deviations\n  applied to Motor-Imagery-Based Brain Computer Interface", "comments": null, "journal-ref": null, "doi": "10.1109/TFUZZ.2021.3092824", "report-no": null, "categories": "cs.HC cs.CV cs.NA math.NA", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this work we study the use of moderate deviation functions to measure\nsimilarity and dissimilarity among a set of given interval-valued data. To do\nso, we introduce the notion of interval-valued moderate deviation function and\nwe study in particular those interval-valued moderate deviation functions which\npreserve the width of the input intervals. Then, we study how to apply these\nfunctions to construct interval-valued aggregation functions. We have applied\nthem in the decision making phase of two Motor-Imagery Brain Computer Interface\nframeworks, obtaining better results than those obtained using other numerical\nand intervalar aggregations.\n", "versions": [{"version": "v1", "created": "Thu, 19 Nov 2020 14:10:29 GMT"}, {"version": "v2", "created": "Thu, 1 Jul 2021 07:09:17 GMT"}], "update_date": "2021-07-02", "authors_parsed": [["Fumanal-Idocin", "Javier", ""], ["Tak\u00e1\u010d", "Zdenko", ""], ["Sanz", "Javier Fern\u00e1ndez Jose Antonio", ""], ["Goyena", "Harkaitz", ""], ["Lin", "Ching-Teng", ""], ["Wang", "Yu-Kai", ""], ["Bustince", "Humberto", ""]]}, {"id": "2011.09832", "submitter": "Edgar Riba Pi", "authors": "Jian Shi, Edgar Riba, Dmytro Mishkin, Francesc Moreno and Anguelos\n  Nicolaou", "title": "Differentiable Data Augmentation with Kornia", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper we present a review of the Kornia differentiable data\naugmentation (DDA) module for both for spatial (2D) and volumetric (3D)\ntensors. This module leverages differentiable computer vision solutions from\nKornia, with an aim of integrating data augmentation (DA) pipelines and\nstrategies to existing PyTorch components (e.g. autograd for differentiability,\noptim for optimization). In addition, we provide a benchmark comparing\ndifferent DA frameworks and a short review for a number of approaches that make\nuse of Kornia DDA.\n", "versions": [{"version": "v1", "created": "Thu, 19 Nov 2020 14:15:21 GMT"}], "update_date": "2020-11-20", "authors_parsed": [["Shi", "Jian", ""], ["Riba", "Edgar", ""], ["Mishkin", "Dmytro", ""], ["Moreno", "Francesc", ""], ["Nicolaou", "Anguelos", ""]]}, {"id": "2011.09846", "submitter": "Ben Saunders", "authors": "Ben Saunders, Necati Cihan Camgoz, Richard Bowden", "title": "Everybody Sign Now: Translating Spoken Language to Photo Realistic Sign\n  Language Video", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To be truly understandable and accepted by Deaf communities, an automatic\nSign Language Production (SLP) system must generate a photo-realistic signer.\nPrior approaches based on graphical avatars have proven unpopular, whereas\nrecent neural SLP works that produce skeleton pose sequences have been shown to\nbe not understandable to Deaf viewers.\n  In this paper, we propose SignGAN, the first SLP model to produce\nphoto-realistic continuous sign language videos directly from spoken language.\nWe employ a transformer architecture with a Mixture Density Network (MDN)\nformulation to handle the translation from spoken language to skeletal pose. A\npose-conditioned human synthesis model is then introduced to generate a\nphoto-realistic sign language video from the skeletal pose sequence. This\nallows the photo-realistic production of sign videos directly translated from\nwritten text.\n  We further propose a novel keypoint-based loss function, which significantly\nimproves the quality of synthesized hand images, operating in the keypoint\nspace to avoid issues caused by motion blur. In addition, we introduce a method\nfor controllable video generation, enabling training on large, diverse sign\nlanguage datasets and providing the ability to control the signer appearance at\ninference.\n  Using a dataset of eight different sign language interpreters extracted from\nbroadcast footage, we show that SignGAN significantly outperforms all baseline\nmethods for quantitative metrics and human perceptual studies.\n", "versions": [{"version": "v1", "created": "Thu, 19 Nov 2020 14:31:06 GMT"}, {"version": "v2", "created": "Fri, 20 Nov 2020 10:16:24 GMT"}, {"version": "v3", "created": "Mon, 23 Nov 2020 13:31:28 GMT"}, {"version": "v4", "created": "Thu, 26 Nov 2020 19:00:34 GMT"}], "update_date": "2020-11-30", "authors_parsed": [["Saunders", "Ben", ""], ["Camgoz", "Necati Cihan", ""], ["Bowden", "Richard", ""]]}, {"id": "2011.09855", "submitter": "Pasquale Cascarano", "authors": "Pasquale Cascarano, Maria Colomba Comes, Arianna Mencattini, Maria\n  Carla Parrini, Elena Loli Piccolomini, Eugenio Martinelli", "title": "Recursive Deep Prior Video: a Super Resolution algorithm for Time-Lapse\n  Microscopy of organ-on-chip experiments", "comments": "Paper submitted to a peer-reviewed journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Biological experiments based on organ-on-chips (OOCs) exploit light\nTime-Lapse Microscopy (TLM) for a direct observation of cell movement that is\nan observable signature of underlying biological processes. A high spatial\nresolution is essential to capture cell dynamics and interactions from recorded\nexperiments by TLM. Unfortunately, due to physical and cost limitations,\nacquiring high resolution videos is not always possible. To overcome the\nproblem, we present here a new deep learning-based algorithm that extends the\nwell known Deep Image Prior (DIP) to TLM Video Super Resolution (SR) without\nrequiring any training. The proposed Recursive Deep Prior Video (RDPV) method\nintroduces some novelties. The weights of the DIP network architecture are\ninitialized for each of the frames according to a new recursive updating rule\ncombined with an efficient early stopping criterion. Moreover, the DIP loss\nfunction is penalized by two different Total Variation (TV) based terms. The\nmethod has been validated on synthetic, i.e., artificially generated, as well\nas real videos from OOC experiments related to tumor-immune interaction.\nAchieved results are compared with several state-of-the-art trained deep\nlearning SR algorithms showing outstanding performances.\n", "versions": [{"version": "v1", "created": "Thu, 19 Nov 2020 14:36:33 GMT"}], "update_date": "2020-11-20", "authors_parsed": [["Cascarano", "Pasquale", ""], ["Comes", "Maria Colomba", ""], ["Mencattini", "Arianna", ""], ["Parrini", "Maria Carla", ""], ["Piccolomini", "Elena Loli", ""], ["Martinelli", "Eugenio", ""]]}, {"id": "2011.09876", "submitter": "Xing Shen", "authors": "Xing Shen, Jirui Yang, Chunbo Wei, Bing Deng, Jianqiang Huang,\n  Xiansheng Hua, Xiaoliang Cheng, Kewei Liang", "title": "DCT-Mask: Discrete Cosine Transform Mask Representation for Instance\n  Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Binary grid mask representation is broadly used in instance segmentation. A\nrepresentative instantiation is Mask R-CNN which predicts masks on a $28\\times\n28$ binary grid. Generally, a low-resolution grid is not sufficient to capture\nthe details, while a high-resolution grid dramatically increases the training\ncomplexity. In this paper, we propose a new mask representation by applying the\ndiscrete cosine transform(DCT) to encode the high-resolution binary grid mask\ninto a compact vector. Our method, termed DCT-Mask, could be easily integrated\ninto most pixel-based instance segmentation methods. Without any bells and\nwhistles, DCT-Mask yields significant gains on different frameworks, backbones,\ndatasets, and training schedules. It does not require any pre-processing or\npre-training, and almost no harm to the running speed. Especially, for\nhigher-quality annotations and more complex backbones, our method has a greater\nimprovement. Moreover, we analyze the performance of our method from the\nperspective of the quality of mask representation. The main reason why DCT-Mask\nworks well is that it obtains a high-quality mask representation with low\ncomplexity. Code is available at https://github.com/aliyun/DCT-Mask.git.\n", "versions": [{"version": "v1", "created": "Thu, 19 Nov 2020 15:00:21 GMT"}, {"version": "v2", "created": "Wed, 14 Apr 2021 11:46:21 GMT"}, {"version": "v3", "created": "Tue, 27 Apr 2021 13:53:02 GMT"}], "update_date": "2021-04-28", "authors_parsed": [["Shen", "Xing", ""], ["Yang", "Jirui", ""], ["Wei", "Chunbo", ""], ["Deng", "Bing", ""], ["Huang", "Jianqiang", ""], ["Hua", "Xiansheng", ""], ["Cheng", "Xiaoliang", ""], ["Liang", "Kewei", ""]]}, {"id": "2011.09884", "submitter": "Qing Guo", "authors": "Bing Yu and Hua Qi and Qing Guo and Felix Juefei-Xu and Xiaofei Xie\n  and Lei Ma and Jianjun Zhao", "title": "DeepRepair: Style-Guided Repairing for DNNs in the Real-world\n  Operational Environment", "comments": "14 pages; 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks (DNNs) are being widely applied for various real-world\napplications across domains due to their high performance (e.g., high accuracy\non image classification). Nevertheless, a well-trained DNN after deployment\ncould oftentimes raise errors during practical use in the operational\nenvironment due to the mismatching between distributions of the training\ndataset and the potential unknown noise factors in the operational environment,\ne.g., weather, blur, noise etc. Hence, it poses a rather important problem for\nthe DNNs' real-world applications: how to repair the deployed DNNs for\ncorrecting the failure samples (i.e., incorrect prediction) under the deployed\noperational environment while not harming their capability of handling normal\nor clean data. The number of failure samples we can collect in practice, caused\nby the noise factors in the operational environment, is often limited.\nTherefore, It is rather challenging how to repair more similar failures based\non the limited failure samples we can collect.\n  In this paper, we propose a style-guided data augmentation for repairing DNN\nin the operational environment. We propose a style transfer method to learn and\nintroduce the unknown failure patterns within the failure data into the\ntraining data via data augmentation. Moreover, we further propose the\nclustering-based failure data generation for much more effective style-guided\ndata augmentation. We conduct a large-scale evaluation with fifteen degradation\nfactors that may happen in the real world and compare with four\nstate-of-the-art data augmentation methods and two DNN repairing methods,\ndemonstrating that our method can significantly enhance the deployed DNNs on\nthe corrupted data in the operational environment, and with even better\naccuracy on clean datasets.\n", "versions": [{"version": "v1", "created": "Thu, 19 Nov 2020 15:09:44 GMT"}], "update_date": "2020-11-20", "authors_parsed": [["Yu", "Bing", ""], ["Qi", "Hua", ""], ["Guo", "Qing", ""], ["Juefei-Xu", "Felix", ""], ["Xie", "Xiaofei", ""], ["Ma", "Lei", ""], ["Zhao", "Jianjun", ""]]}, {"id": "2011.09899", "submitter": "Yuhang Li", "authors": "Yuhang Li, Feng Zhu, Ruihao Gong, Mingzhu Shen, Xin Dong, Fengwei Yu,\n  Shaoqing Lu, Shi Gu", "title": "MixMix: All You Need for Data-Free Compression Are Feature and Data\n  Mixing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  User data confidentiality protection is becoming a rising challenge in the\npresent deep learning research. Without access to data, conventional\ndata-driven model compression faces a higher risk of performance degradation.\nRecently, some works propose to generate images from a specific pretrained\nmodel to serve as training data. However, the inversion process only utilizes\nbiased feature statistics stored in one model and is from low-dimension to\nhigh-dimension. As a consequence, it inevitably encounters the difficulties of\ngeneralizability and inexact inversion, which leads to unsatisfactory\nperformance. To address these problems, we propose MixMix based on two simple\nyet effective techniques: (1) Feature Mixing: utilizes various models to\nconstruct a universal feature space for generalized inversion; (2) Data Mixing:\nmixes the synthesized images and labels to generate exact label information. We\nprove the effectiveness of MixMix from both theoretical and empirical\nperspectives. Extensive experiments show that MixMix outperforms existing\nmethods on the mainstream compression tasks, including quantization, knowledge\ndistillation, and pruning. Specifically, MixMix achieves up to 4% and 20%\naccuracy uplift on quantization and pruning, respectively, compared to existing\ndata-free compression work.\n", "versions": [{"version": "v1", "created": "Thu, 19 Nov 2020 15:33:43 GMT"}, {"version": "v2", "created": "Wed, 16 Jun 2021 07:22:41 GMT"}], "update_date": "2021-06-17", "authors_parsed": [["Li", "Yuhang", ""], ["Zhu", "Feng", ""], ["Gong", "Ruihao", ""], ["Shen", "Mingzhu", ""], ["Dong", "Xin", ""], ["Yu", "Fengwei", ""], ["Lu", "Shaoqing", ""], ["Gu", "Shi", ""]]}, {"id": "2011.09908", "submitter": "Kunbo Zhang", "authors": "Kunbo Zhang, Zhenteng Shen, Yunlong Wang, Zhenan Sun", "title": "All-in-Focus Iris Camera With a Great Capture Volume", "comments": "to be published in International Joint Conference on Biometrics 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Imaging volume of an iris recognition system has been restricting the\nthroughput and cooperation convenience in biometric applications. Numerous\nimprovement trials are still impractical to supersede the dominant fixed-focus\nlens in stand-off iris recognition due to incremental performance increase and\ncomplicated optical design. In this study, we develop a novel all-in-focus iris\nimaging system using a focus-tunable lens and a 2D steering mirror to greatly\nextend capture volume by spatiotemporal multiplexing method. Our iris imaging\ndepth of field extension system requires no mechanical motion and is capable to\nadjust the focal plane at extremely high speed. In addition, the motorized\nreflection mirror adaptively steers the light beam to extend the horizontal and\nvertical field of views in an active manner. The proposed all-in-focus iris\ncamera increases the depth of field up to 3.9 m which is a factor of 37.5\ncompared with conventional long focal lens. We also experimentally demonstrate\nthe capability of this 3D light beam steering imaging system in real-time\nmulti-person iris refocusing using dynamic focal stacks and the potential of\ncontinuous iris recognition for moving participants.\n", "versions": [{"version": "v1", "created": "Thu, 19 Nov 2020 15:39:45 GMT"}], "update_date": "2020-11-20", "authors_parsed": [["Zhang", "Kunbo", ""], ["Shen", "Zhenteng", ""], ["Wang", "Yunlong", ""], ["Sun", "Zhenan", ""]]}, {"id": "2011.09928", "submitter": "Haoyu Dong", "authors": "Haoyu Dong, Ze Wang, Qiang Qiu, and Guillermo Sapiro", "title": "Using Text to Teach Image Retrieval", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image retrieval relies heavily on the quality of the data modeling and the\ndistance measurement in the feature space. Building on the concept of image\nmanifold, we first propose to represent the feature space of images, learned\nvia neural networks, as a graph. Neighborhoods in the feature space are now\ndefined by the geodesic distance between images, represented as graph vertices\nor manifold samples. When limited images are available, this manifold is\nsparsely sampled, making the geodesic computation and the corresponding\nretrieval harder. To address this, we augment the manifold samples with\ngeometrically aligned text, thereby using a plethora of sentences to teach us\nabout images. In addition to extensive results on standard datasets\nillustrating the power of text to help in image retrieval, a new public dataset\nbased on CLEVR is introduced to quantify the semantic similarity between visual\ndata and text data. The experimental results show that the joint embedding\nmanifold is a robust representation, allowing it to be a better basis to\nperform image retrieval given only an image and a textual instruction on the\ndesired modifications over the image\n", "versions": [{"version": "v1", "created": "Thu, 19 Nov 2020 16:09:14 GMT"}], "update_date": "2020-11-20", "authors_parsed": [["Dong", "Haoyu", ""], ["Wang", "Ze", ""], ["Qiu", "Qiang", ""], ["Sapiro", "Guillermo", ""]]}, {"id": "2011.09941", "submitter": "Lingxi Xie", "authors": "Xinyue Huo, Lingxi Xie, Longhui Wei, Xiaopeng Zhang, Hao Li, Zijie\n  Yang, Wengang Zhou, Houqiang Li, Qi Tian", "title": "Heterogeneous Contrastive Learning: Encoding Spatial Information for\n  Compact Visual Representations", "comments": "10 pages, 4 figures, 6 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Contrastive learning has achieved great success in self-supervised visual\nrepresentation learning, but existing approaches mostly ignored spatial\ninformation which is often crucial for visual representation. This paper\npresents heterogeneous contrastive learning (HCL), an effective approach that\nadds spatial information to the encoding stage to alleviate the learning\ninconsistency between the contrastive objective and strong data augmentation\noperations. We demonstrate the effectiveness of HCL by showing that (i) it\nachieves higher accuracy in instance discrimination and (ii) it surpasses\nexisting pre-training methods in a series of downstream tasks while shrinking\nthe pre-training costs by half. More importantly, we show that our approach\nachieves higher efficiency in visual representations, and thus delivers a key\nmessage to inspire the future research of self-supervised visual representation\nlearning.\n", "versions": [{"version": "v1", "created": "Thu, 19 Nov 2020 16:26:25 GMT"}], "update_date": "2020-11-20", "authors_parsed": [["Huo", "Xinyue", ""], ["Xie", "Lingxi", ""], ["Wei", "Longhui", ""], ["Zhang", "Xiaopeng", ""], ["Li", "Hao", ""], ["Yang", "Zijie", ""], ["Zhou", "Wengang", ""], ["Li", "Houqiang", ""], ["Tian", "Qi", ""]]}, {"id": "2011.09944", "submitter": "Xianping Li", "authors": "Xianping Li, Teresa Wu", "title": "A Preliminary Comparison Between Compressive Sampling and Anisotropic\n  Mesh-based Image Representation", "comments": "9 pages, 4 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Compressed sensing (CS) has become a popular field in the last two decades to\nrepresent and reconstruct a sparse signal with much fewer samples than the\nsignal itself. Although regular images are not sparse on their own, many can be\nsparsely represented in wavelet transform domain. Therefore, CS has also been\nwidely applied to represent digital images. However, an alternative approach,\nadaptive sampling such as mesh-based image representation (MbIR), has not\nattracted as much attention. MbIR works directly on image pixels and represents\nthe image with fewer points using a triangular mesh. In this paper, we perform\na preliminary comparison between the CS and a recently developed MbIR method,\nAMA representation. The results demonstrate that, at the same sample density,\nAMA representation can provide better reconstruction quality than CS based on\nthe tested algorithms. Further investigation with recent algorithms is needed\nto perform a thorough comparison.\n", "versions": [{"version": "v1", "created": "Thu, 19 Nov 2020 16:38:02 GMT"}, {"version": "v2", "created": "Mon, 14 Dec 2020 03:14:04 GMT"}], "update_date": "2020-12-15", "authors_parsed": [["Li", "Xianping", ""], ["Wu", "Teresa", ""]]}, {"id": "2011.09957", "submitter": "Cristian Canton Ferrer", "authors": "Paarth Neekhara, Brian Dolhansky, Joanna Bitton, Cristian Canton\n  Ferrer", "title": "Adversarial Threats to DeepFake Detection: A Practical Perspective", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Facially manipulated images and videos or DeepFakes can be used maliciously\nto fuel misinformation or defame individuals. Therefore, detecting DeepFakes is\ncrucial to increase the credibility of social media platforms and other media\nsharing web sites. State-of-the art DeepFake detection techniques rely on\nneural network based classification models which are known to be vulnerable to\nadversarial examples. In this work, we study the vulnerabilities of\nstate-of-the-art DeepFake detection methods from a practical stand point. We\nperform adversarial attacks on DeepFake detectors in a black box setting where\nthe adversary does not have complete knowledge of the classification models. We\nstudy the extent to which adversarial perturbations transfer across different\nmodels and propose techniques to improve the transferability of adversarial\nexamples. We also create more accessible attacks using Universal Adversarial\nPerturbations which pose a very feasible attack scenario since they can be\neasily shared amongst attackers. We perform our evaluations on the winning\nentries of the DeepFake Detection Challenge (DFDC) and demonstrate that they\ncan be easily bypassed in a practical attack scenario by designing transferable\nand accessible adversarial attacks.\n", "versions": [{"version": "v1", "created": "Thu, 19 Nov 2020 16:53:38 GMT"}], "update_date": "2020-11-20", "authors_parsed": [["Neekhara", "Paarth", ""], ["Dolhansky", "Brian", ""], ["Bitton", "Joanna", ""], ["Ferrer", "Cristian Canton", ""]]}, {"id": "2011.09977", "submitter": "Jihao Andreas Lin", "authors": "Jihao Andreas Lin, Jakob Br\\\"unker, Daniel F\\\"ahrmann", "title": "Learning to Predict the 3D Layout of a Scene", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  While 2D object detection has improved significantly over the past, real\nworld applications of computer vision often require an understanding of the 3D\nlayout of a scene. Many recent approaches to 3D detection use LiDAR point\nclouds for prediction. We propose a method that only uses a single RGB image,\nthus enabling applications in devices or vehicles that do not have LiDAR\nsensors. By using an RGB image, we can leverage the maturity and success of\nrecent 2D object detectors, by extending a 2D detector with a 3D detection\nhead. In this paper we discuss different approaches and experiments, including\nboth regression and classification methods, for designing this 3D detection\nhead. Furthermore, we evaluate how subproblems and implementation details\nimpact the overall prediction result. We use the KITTI dataset for training,\nwhich consists of street traffic scenes with class labels, 2D bounding boxes\nand 3D annotations with seven degrees of freedom. Our final architecture is\nbased on Faster R-CNN. The outputs of the convolutional backbone are fixed\nsized feature maps for every region of interest. Fully connected layers within\nthe network head then propose an object class and perform 2D bounding box\nregression. We extend the network head by a 3D detection head, which predicts\nevery degree of freedom of a 3D bounding box via classification. We achieve a\nmean average precision of 47.3% for moderately difficult data, measured at a 3D\nintersection over union threshold of 70%, as required by the official KITTI\nbenchmark; outperforming previous state-of-the-art single RGB only methods by a\nlarge margin.\n", "versions": [{"version": "v1", "created": "Thu, 19 Nov 2020 17:23:30 GMT"}], "update_date": "2020-11-20", "authors_parsed": [["Lin", "Jihao Andreas", ""], ["Br\u00fcnker", "Jakob", ""], ["F\u00e4hrmann", "Daniel", ""]]}, {"id": "2011.09980", "submitter": "Burak Uzkent", "authors": "Kumar Ayush, Burak Uzkent, Chenlin Meng, Kumar Tanmay, Marshall Burke,\n  David Lobell, Stefano Ermon", "title": "Geography-Aware Self-Supervised Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Contrastive learning methods have significantly narrowed the gap between\nsupervised and unsupervised learning on computer vision tasks. In this paper,\nwe explore their application to remote sensing, where unlabeled data is often\nabundant but labeled data is scarce. We first show that due to their different\ncharacteristics, a non-trivial gap persists between contrastive and supervised\nlearning on standard benchmarks. To close the gap, we propose novel training\nmethods that exploit the spatiotemporal structure of remote sensing data. We\nleverage spatially aligned images over time to construct temporal positive\npairs in contrastive learning and geo-location to design pre-text tasks. Our\nexperiments show that our proposed method closes the gap between contrastive\nand supervised learning on image classification, object detection and semantic\nsegmentation for remote sensing and other geo-tagged image datasets.\n", "versions": [{"version": "v1", "created": "Thu, 19 Nov 2020 17:29:13 GMT"}, {"version": "v2", "created": "Sat, 21 Nov 2020 21:10:35 GMT"}, {"version": "v3", "created": "Tue, 24 Nov 2020 17:18:32 GMT"}, {"version": "v4", "created": "Sat, 28 Nov 2020 03:26:05 GMT"}, {"version": "v5", "created": "Wed, 2 Dec 2020 19:28:56 GMT"}], "update_date": "2020-12-04", "authors_parsed": [["Ayush", "Kumar", ""], ["Uzkent", "Burak", ""], ["Meng", "Chenlin", ""], ["Tanmay", "Kumar", ""], ["Burke", "Marshall", ""], ["Lobell", "David", ""], ["Ermon", "Stefano", ""]]}, {"id": "2011.10007", "submitter": "Jiayuan Mao", "authors": "Yikai Li, Jiayuan Mao, Xiuming Zhang, William T. Freeman, Joshua B.\n  Tenenbaum, Noah Snavely, Jiajun Wu", "title": "Multi-Plane Program Induction with 3D Box Priors", "comments": "NeurIPS 2020. First two authors contributed equally. Project page:\n  http://bpi.csail.mit.edu", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider two important aspects in understanding and editing images:\nmodeling regular, program-like texture or patterns in 2D planes, and 3D posing\nof these planes in the scene. Unlike prior work on image-based program\nsynthesis, which assumes the image contains a single visible 2D plane, we\npresent Box Program Induction (BPI), which infers a program-like scene\nrepresentation that simultaneously models repeated structure on multiple 2D\nplanes, the 3D position and orientation of the planes, and camera parameters,\nall from a single image. Our model assumes a box prior, i.e., that the image\ncaptures either an inner view or an outer view of a box in 3D. It uses neural\nnetworks to infer visual cues such as vanishing points, wireframe lines to\nguide a search-based algorithm to find the program that best explains the\nimage. Such a holistic, structured scene representation enables 3D-aware\ninteractive image editing operations such as inpainting missing pixels,\nchanging camera parameters, and extrapolate the image contents.\n", "versions": [{"version": "v1", "created": "Thu, 19 Nov 2020 18:07:46 GMT"}, {"version": "v2", "created": "Sun, 22 Nov 2020 19:13:03 GMT"}], "update_date": "2020-11-24", "authors_parsed": [["Li", "Yikai", ""], ["Mao", "Jiayuan", ""], ["Zhang", "Xiuming", ""], ["Freeman", "William T.", ""], ["Tenenbaum", "Joshua B.", ""], ["Snavely", "Noah", ""], ["Wu", "Jiajun", ""]]}, {"id": "2011.10028", "submitter": "Egor Ershov I", "authors": "Egor Ershov, Alex Savchik, Illya Semenkov, Nikola Bani\\'c, Alexander\n  Belokopytov, Daria Senshina, Karlo Koscevi\\'c, Marko Suba\\v{s}i\\'c, Sven\n  Lon\\v{c}ari\\'c", "title": "The Cube++ Illumination Estimation Dataset", "comments": null, "journal-ref": null, "doi": "10.1109/ACCESS.2020.3045066", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Computational color constancy has the important task of reducing the\ninfluence of the scene illumination on the object colors. As such, it is an\nessential part of the image processing pipelines of most digital cameras. One\nof the important parts of the computational color constancy is illumination\nestimation, i.e. estimating the illumination color. When an illumination\nestimation method is proposed, its accuracy is usually reported by providing\nthe values of error metrics obtained on the images of publicly available\ndatasets. However, over time it has been shown that many of these datasets have\nproblems such as too few images, inappropriate image quality, lack of scene\ndiversity, absence of version tracking, violation of various assumptions, GDPR\nregulation violation, lack of additional shooting procedure info, etc. In this\npaper, a new illumination estimation dataset is proposed that aims to alleviate\nmany of the mentioned problems and to help the illumination estimation\nresearch. It consists of 4890 images with known illumination colors as well as\nwith additional semantic data that can further make the learning process more\naccurate. Due to the usage of the SpyderCube color target, for every image\nthere are two ground-truth illumination records covering different directions.\nBecause of that, the dataset can be used for training and testing of methods\nthat perform single or two-illuminant estimation. This makes it superior to\nmany similar existing datasets. The datasets, it's smaller version\nSimpleCube++, and the accompanying code are available at\nhttps://github.com/Visillect/CubePlusPlus/.\n", "versions": [{"version": "v1", "created": "Thu, 19 Nov 2020 18:50:08 GMT"}], "update_date": "2020-12-29", "authors_parsed": [["Ershov", "Egor", ""], ["Savchik", "Alex", ""], ["Semenkov", "Illya", ""], ["Bani\u0107", "Nikola", ""], ["Belokopytov", "Alexander", ""], ["Senshina", "Daria", ""], ["Koscevi\u0107", "Karlo", ""], ["Suba\u0161i\u0107", "Marko", ""], ["Lon\u010dari\u0107", "Sven", ""]]}, {"id": "2011.10033", "submitter": "Xinge Zhu", "authors": "Xinge Zhu, Hui Zhou, Tai Wang, Fangzhou Hong, Yuexin Ma, Wei Li,\n  Hongsheng Li, Dahua Lin", "title": "Cylindrical and Asymmetrical 3D Convolution Networks for LiDAR\n  Segmentation", "comments": "This work achieves the 1st place in the leaderboard of SemanticKITTI\n  (until CVPR DDL) and based on this work, we also achieve the 1st place in the\n  leaderboard of SemanticKITTI panoptic segmentation; Code at\n  https://github.com/xinge008/Cylinder3D", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  State-of-the-art methods for large-scale driving-scene LiDAR segmentation\noften project the point clouds to 2D space and then process them via 2D\nconvolution. Although this corporation shows the competitiveness in the point\ncloud, it inevitably alters and abandons the 3D topology and geometric\nrelations. A natural remedy is to utilize the3D voxelization and 3D convolution\nnetwork. However, we found that in the outdoor point cloud, the improvement\nobtained in this way is quite limited. An important reason is the property of\nthe outdoor point cloud, namely sparsity and varying density. Motivated by this\ninvestigation, we propose a new framework for the outdoor LiDAR segmentation,\nwhere cylindrical partition and asymmetrical 3D convolution networks are\ndesigned to explore the 3D geometric pat-tern while maintaining these inherent\nproperties. Moreover, a point-wise refinement module is introduced to alleviate\nthe interference of lossy voxel-based label encoding. We evaluate the proposed\nmodel on two large-scale datasets, i.e., SemanticKITTI and nuScenes. Our method\nachieves the 1st place in the leaderboard of SemanticKITTI and outperforms\nexisting methods on nuScenes with a noticeable margin, about 4%. Furthermore,\nthe proposed 3D framework also generalizes well to LiDAR panoptic segmentation\nand LiDAR 3D detection.\n", "versions": [{"version": "v1", "created": "Thu, 19 Nov 2020 18:53:11 GMT"}], "update_date": "2020-11-20", "authors_parsed": [["Zhu", "Xinge", ""], ["Zhou", "Hui", ""], ["Wang", "Tai", ""], ["Hong", "Fangzhou", ""], ["Ma", "Yuexin", ""], ["Li", "Wei", ""], ["Li", "Hongsheng", ""], ["Lin", "Dahua", ""]]}, {"id": "2011.10039", "submitter": "Songwei Ge", "authors": "Songwei Ge, Vedanuj Goswami, C. Lawrence Zitnick and Devi Parikh", "title": "Creative Sketch Generation", "comments": "Published as a conference paper at ICLR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Sketching or doodling is a popular creative activity that people engage in.\nHowever, most existing work in automatic sketch understanding or generation has\nfocused on sketches that are quite mundane. In this work, we introduce two\ndatasets of creative sketches -- Creative Birds and Creative Creatures --\ncontaining 10k sketches each along with part annotations. We propose DoodlerGAN\n-- a part-based Generative Adversarial Network (GAN) -- to generate unseen\ncompositions of novel part appearances. Quantitative evaluations as well as\nhuman studies demonstrate that sketches generated by our approach are more\ncreative and of higher quality than existing approaches. In fact, in Creative\nBirds, subjects prefer sketches generated by DoodlerGAN over those drawn by\nhumans! Our code can be found at https://github.com/facebookresearch/DoodlerGAN\nand a demo can be found at http://doodlergan.cloudcv.org.\n", "versions": [{"version": "v1", "created": "Thu, 19 Nov 2020 18:57:00 GMT"}, {"version": "v2", "created": "Wed, 3 Mar 2021 20:01:54 GMT"}], "update_date": "2021-03-05", "authors_parsed": [["Ge", "Songwei", ""], ["Goswami", "Vedanuj", ""], ["Zitnick", "C. Lawrence", ""], ["Parikh", "Devi", ""]]}, {"id": "2011.10043", "submitter": "Han Hu", "authors": "Zhenda Xie and Yutong Lin and Zheng Zhang and Yue Cao and Stephen Lin\n  and Han Hu", "title": "Propagate Yourself: Exploring Pixel-Level Consistency for Unsupervised\n  Visual Representation Learning", "comments": "Accepted in CVPR2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Contrastive learning methods for unsupervised visual representation learning\nhave reached remarkable levels of transfer performance. We argue that the power\nof contrastive learning has yet to be fully unleashed, as current methods are\ntrained only on instance-level pretext tasks, leading to representations that\nmay be sub-optimal for downstream tasks requiring dense pixel predictions. In\nthis paper, we introduce pixel-level pretext tasks for learning dense feature\nrepresentations. The first task directly applies contrastive learning at the\npixel level. We additionally propose a pixel-to-propagation consistency task\nthat produces better results, even surpassing the state-of-the-art approaches\nby a large margin. Specifically, it achieves 60.2 AP, 41.4 / 40.5 mAP and 77.2\nmIoU when transferred to Pascal VOC object detection (C4), COCO object\ndetection (FPN / C4) and Cityscapes semantic segmentation using a ResNet-50\nbackbone network, which are 2.6 AP, 0.8 / 1.0 mAP and 1.0 mIoU better than the\nprevious best methods built on instance-level contrastive learning. Moreover,\nthe pixel-level pretext tasks are found to be effective for pre-training not\nonly regular backbone networks but also head networks used for dense downstream\ntasks, and are complementary to instance-level contrastive methods. These\nresults demonstrate the strong potential of defining pretext tasks at the pixel\nlevel, and suggest a new path forward in unsupervised visual representation\nlearning. Code is available at \\url{https://github.com/zdaxie/PixPro}.\n", "versions": [{"version": "v1", "created": "Thu, 19 Nov 2020 18:59:45 GMT"}, {"version": "v2", "created": "Tue, 9 Mar 2021 14:29:39 GMT"}], "update_date": "2021-03-10", "authors_parsed": [["Xie", "Zhenda", ""], ["Lin", "Yutong", ""], ["Zhang", "Zheng", ""], ["Cao", "Yue", ""], ["Lin", "Stephen", ""], ["Hu", "Han", ""]]}, {"id": "2011.10063", "submitter": "Kwonjoon Lee", "authors": "Gaurav Parmar, Dacheng Li, Kwonjoon Lee, Zhuowen Tu", "title": "Dual Contradistinctive Generative Autoencoder", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new generative autoencoder model with dual contradistinctive\nlosses to improve generative autoencoder that performs simultaneous inference\n(reconstruction) and synthesis (sampling). Our model, named dual\ncontradistinctive generative autoencoder (DC-VAE), integrates an instance-level\ndiscriminative loss (maintaining the instance-level fidelity for the\nreconstruction/synthesis) with a set-level adversarial loss (encouraging the\nset-level fidelity for there construction/synthesis), both being\ncontradistinctive. Extensive experimental results by DC-VAE across different\nresolutions including 32x32, 64x64, 128x128, and 512x512 are reported. The two\ncontradistinctive losses in VAE work harmoniously in DC-VAE leading to a\nsignificant qualitative and quantitative performance enhancement over the\nbaseline VAEs without architectural changes. State-of-the-art or competitive\nresults among generative autoencoders for image reconstruction, image\nsynthesis, image interpolation, and representation learning are observed.\nDC-VAE is a general-purpose VAE model, applicable to a wide variety of\ndownstream tasks in computer vision and machine learning.\n", "versions": [{"version": "v1", "created": "Thu, 19 Nov 2020 19:00:53 GMT"}], "update_date": "2020-11-23", "authors_parsed": [["Parmar", "Gaurav", ""], ["Li", "Dacheng", ""], ["Lee", "Kwonjoon", ""], ["Tu", "Zhuowen", ""]]}, {"id": "2011.10077", "submitter": "Songzhu Zheng", "authors": "Songzhu Zheng, Pengxiang Wu, Aman Goswami, Mayank Goswami, Dimitris\n  Metaxas, Chao Chen", "title": "Error-Bounded Correction of Noisy Labels", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  To collect large scale annotated data, it is inevitable to introduce label\nnoise, i.e., incorrect class labels. To be robust against label noise, many\nsuccessful methods rely on the noisy classifiers (i.e., models trained on the\nnoisy training data) to determine whether a label is trustworthy. However, it\nremains unknown why this heuristic works well in practice. In this paper, we\nprovide the first theoretical explanation for these methods. We prove that the\nprediction of a noisy classifier can indeed be a good indicator of whether the\nlabel of a training data is clean. Based on the theoretical result, we propose\na novel algorithm that corrects the labels based on the noisy classifier\nprediction. The corrected labels are consistent with the true Bayesian optimal\nclassifier with high probability. We incorporate our label correction algorithm\ninto the training of deep neural networks and train models that achieve\nsuperior testing performance on multiple public datasets.\n", "versions": [{"version": "v1", "created": "Thu, 19 Nov 2020 19:23:23 GMT"}], "update_date": "2020-11-23", "authors_parsed": [["Zheng", "Songzhu", ""], ["Wu", "Pengxiang", ""], ["Goswami", "Aman", ""], ["Goswami", "Mayank", ""], ["Metaxas", "Dimitris", ""], ["Chen", "Chao", ""]]}, {"id": "2011.10082", "submitter": "Meng Ye", "authors": "Meng Ye, Xiao Lin, Giedrius Burachas, Ajay Divakaran, Yi Yao", "title": "Hybrid Consistency Training with Prototype Adaptation for Few-Shot\n  Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Few-Shot Learning (FSL) aims to improve a model's generalization capability\nin low data regimes. Recent FSL works have made steady progress via metric\nlearning, meta learning, representation learning, etc. However, FSL remains\nchallenging due to the following longstanding difficulties. 1) The seen and\nunseen classes are disjoint, resulting in a distribution shift between training\nand testing. 2) During testing, labeled data of previously unseen classes is\nsparse, making it difficult to reliably extrapolate from labeled support\nexamples to unlabeled query examples. To tackle the first challenge, we\nintroduce Hybrid Consistency Training to jointly leverage interpolation\nconsistency, including interpolating hidden features, that imposes linear\nbehavior locally and data augmentation consistency that learns robust\nembeddings against sample variations. As for the second challenge, we use\nunlabeled examples to iteratively normalize features and adapt prototypes, as\nopposed to commonly used one-time update, for more reliable prototype-based\ntransductive inference. We show that our method generates a 2% to 5%\nimprovement over the state-of-the-art methods with similar backbones on five\nFSL datasets and, more notably, a 7% to 8% improvement for more challenging\ncross-domain FSL.\n", "versions": [{"version": "v1", "created": "Thu, 19 Nov 2020 19:51:33 GMT"}], "update_date": "2020-11-23", "authors_parsed": [["Ye", "Meng", ""], ["Lin", "Xiao", ""], ["Burachas", "Giedrius", ""], ["Divakaran", "Ajay", ""], ["Yao", "Yi", ""]]}, {"id": "2011.10084", "submitter": "Sahand Sharifzadeh", "authors": "Sahand Sharifzadeh, Sina Moayed Baharlou, Volker Tresp", "title": "Classification by Attention: Scene Graph Classification with Prior\n  Knowledge", "comments": "Thirty-Fifth AAAI Conference on Artificial Intelligence (AAAI-2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A major challenge in scene graph classification is that the appearance of\nobjects and relations can be significantly different from one image to another.\nPrevious works have addressed this by relational reasoning over all objects in\nan image or incorporating prior knowledge into classification. Unlike previous\nworks, we do not consider separate models for perception and prior knowledge.\nInstead, we take a multi-task learning approach, where we implement the\nclassification as an attention layer. This allows for the prior knowledge to\nemerge and propagate within the perception model. By enforcing the model also\nto represent the prior, we achieve a strong inductive bias. We show that our\nmodel can accurately generate commonsense knowledge and that the iterative\ninjection of this knowledge to scene representations leads to significantly\nhigher classification performance. Additionally, our model can be fine-tuned on\nexternal knowledge given as triples. When combined with self-supervised\nlearning and with 1% of annotated images only, this gives more than 3%\nimprovement in object classification, 26% in scene graph classification, and\n36% in predicate prediction accuracy.\n", "versions": [{"version": "v1", "created": "Thu, 19 Nov 2020 19:54:04 GMT"}, {"version": "v2", "created": "Thu, 17 Dec 2020 11:52:05 GMT"}], "update_date": "2020-12-18", "authors_parsed": [["Sharifzadeh", "Sahand", ""], ["Baharlou", "Sina Moayed", ""], ["Tresp", "Volker", ""]]}, {"id": "2011.10094", "submitter": "Anh Cat Le Ngo", "authors": "Anh-Cat Le-Ngo, Truyen Tran, Santu Rana, Sunil Gupta, Svetha Venkatesh", "title": "Logically Consistent Loss for Visual Question Answering", "comments": "10 pages, 6 figure, 9 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given an image, a back-ground knowledge, and a set of questions about an\nobject, human learners answer the questions very consistently regardless of\nquestion forms and semantic tasks. The current advancement in neural-network\nbased Visual Question Answering (VQA), despite their impressive performance,\ncannot ensure such consistency due to identically distribution (i.i.d.)\nassumption. We propose a new model-agnostic logic constraint to tackle this\nissue by formulating a logically consistent loss in the multi-task learning\nframework as well as a data organisation called family-batch and hybrid-batch.\nTo demonstrate usefulness of this proposal, we train and evaluate MAC-net based\nVQA machines with and without the proposed logically consistent loss and the\nproposed data organization. The experiments confirm that the proposed loss\nformulae and introduction of hybrid-batch leads to more consistency as well as\nbetter performance. Though the proposed approach is tested with MAC-net, it can\nbe utilised in any other QA methods whenever the logical consistency between\nanswers exist.\n", "versions": [{"version": "v1", "created": "Thu, 19 Nov 2020 20:31:05 GMT"}], "update_date": "2020-11-23", "authors_parsed": [["Le-Ngo", "Anh-Cat", ""], ["Tran", "Truyen", ""], ["Rana", "Santu", ""], ["Gupta", "Sunil", ""], ["Venkatesh", "Svetha", ""]]}, {"id": "2011.10111", "submitter": "Mohammadjavad Abbaspour", "authors": "Mohammadjavad Abbaspour and Mohammad Ali Masnadi-Shirazi", "title": "Online Multi-Object Tracking with delta-GLMB Filter based on Occlusion\n  and Identity Switch Handling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In this paper, we propose an online multi-object tracking (MOT) method in a\ndelta Generalized Labeled Multi-Bernoulli (delta-GLMB) filter framework to\naddress occlusion and miss-detection issues, reduce false alarms, and recover\nidentity switch (ID switch). To handle occlusion and miss-detection issues, we\npropose a measurement-to-disappeared track association method based on one-step\ndelta-GLMB filter, so it is possible to manage these difficulties by jointly\nprocessing occluded or miss-detected objects. This part of proposed method is\nbased on a proposed similarity metric which is responsible for defining the\nweight of hypothesized reappeared tracks. We also extend the delta-GLMB filter\nto efficiently recover switched IDs using the cardinality density, size and\ncolor features of the hypothesized tracks. We also propose a novel birth model\nto achieve more effective clutter removal performance. In both\nocclusion/miss-detection handler and newly-birthed object detector sections of\nthe proposed method, unassigned measurements play a significant role, since\nthey are used as the candidates for reappeared or birth objects. In addition,\nwe perform an ablation study which confirms the effectiveness of our\ncontributions in comparison with the baseline method. We evaluate the proposed\nmethod on well-known and publicly available MOT15 and MOT17 test datasets which\nare focused on pedestrian tracking. Experimental results show that the proposed\ntracker performs better or at least at the same level of the state-of-the-art\nonline and offline MOT methods. It effectively handles the occlusion and ID\nswitch issues and reduces false alarms as well.\n", "versions": [{"version": "v1", "created": "Thu, 19 Nov 2020 21:38:40 GMT"}, {"version": "v2", "created": "Mon, 26 Apr 2021 08:31:57 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Abbaspour", "Mohammadjavad", ""], ["Masnadi-Shirazi", "Mohammad Ali", ""]]}, {"id": "2011.10118", "submitter": "Rogerio Bonatti", "authors": "Rogerio Bonatti, Arthur Bucker, Sebastian Scherer, Mustafa Mukadam and\n  Jessica Hodgins", "title": "Batteries, camera, action! Learning a semantic control space for\n  expressive robot cinematography", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.GR cs.HC cs.RO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Aerial vehicles are revolutionizing the way film-makers can capture shots of\nactors by composing novel aerial and dynamic viewpoints. However, despite great\nadvancements in autonomous flight technology, generating expressive camera\nbehaviors is still a challenge and requires non-technical users to edit a large\nnumber of unintuitive control parameters. In this work, we develop a\ndata-driven framework that enables editing of these complex camera positioning\nparameters in a semantic space (e.g. calm, enjoyable, establishing). First, we\ngenerate a database of video clips with a diverse range of shots in a\nphoto-realistic simulator, and use hundreds of participants in a crowd-sourcing\nframework to obtain scores for a set of semantic descriptors for each clip.\nNext, we analyze correlations between descriptors and build a semantic control\nspace based on cinematography guidelines and human perception studies. Finally,\nwe learn a generative model that can map a set of desired semantic video\ndescriptors into low-level camera trajectory parameters. We evaluate our system\nby demonstrating that our model successfully generates shots that are rated by\nparticipants as having the expected degrees of expression for each descriptor.\nWe also show that our models generalize to different scenes in both simulation\nand real-world experiments. Data and video found at:\nhttps://sites.google.com/view/robotcam.\n", "versions": [{"version": "v1", "created": "Thu, 19 Nov 2020 21:56:53 GMT"}, {"version": "v2", "created": "Wed, 31 Mar 2021 21:15:21 GMT"}], "update_date": "2021-04-02", "authors_parsed": [["Bonatti", "Rogerio", ""], ["Bucker", "Arthur", ""], ["Scherer", "Sebastian", ""], ["Mukadam", "Mustafa", ""], ["Hodgins", "Jessica", ""]]}, {"id": "2011.10132", "submitter": "Sally Sisi Qu", "authors": "Sisi Qu, Mattia Soldan, Mengmeng Xu, Jesper Tegner, Bernard Ghanem", "title": "VLG-Net: Video-Language Graph Matching Network for Video Grounding", "comments": "12 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Grounding language queries in videos aims at identifying the time interval\n(or moment) semantically relevant to a language query. The solution to this\nchallenging task demands the understanding of videos' and queries' semantic\ncontent and the fine-grained reasoning about their multi-modal interactions.\nOur key idea is to recast this challenge into an algorithmic graph matching\nproblem. Fueled by recent advances in Graph Neural Networks, we propose to\nleverage Graph Convolutional Networks to model video and textual information as\nwell as their semantic alignment. To enable the mutual exchange of information\nacross the domains, we design a novel Video-Language Graph Matching Network\n(VLG-Net) to match video and query graphs. Core ingredients include\nrepresentation graphs, built on top of video snippets and query tokens\nseparately, which are used for modeling the intra-modality relationships. A\nGraph Matching layer is adopted for cross-modal context modeling and\nmulti-modal fusion. Finally, moment candidates are created using masked moment\nattention pooling by fusing the moment's enriched snippet features. We\ndemonstrate superior performance over state-of-the-art grounding methods on\nthree widely used datasets for temporal localization of moments in videos with\nnatural language queries: ActivityNet-Captions, TACoS, and DiDeMo.\n", "versions": [{"version": "v1", "created": "Thu, 19 Nov 2020 22:32:03 GMT"}], "update_date": "2020-11-23", "authors_parsed": [["Qu", "Sisi", ""], ["Soldan", "Mattia", ""], ["Xu", "Mengmeng", ""], ["Tegner", "Jesper", ""], ["Ghanem", "Bernard", ""]]}, {"id": "2011.10142", "submitter": "Weilin Zhang", "authors": "Weilin Zhang, Yu-Xiong Wang, David A. Forsyth", "title": "Cooperating RPN's Improve Few-Shot Object Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Learning to detect an object in an image from very few training examples -\nfew-shot object detection - is challenging, because the classifier that sees\nproposal boxes has very little training data. A particularly challenging\ntraining regime occurs when there are one or two training examples. In this\ncase, if the region proposal network (RPN) misses even one high\nintersection-over-union (IOU) training box, the classifier's model of how\nobject appearance varies can be severely impacted. We use multiple distinct yet\ncooperating RPN's. Our RPN's are trained to be different, but not too\ndifferent; doing so yields significant performance improvements over state of\nthe art for COCO and PASCAL VOC in the very few-shot setting. This effect\nappears to be independent of the choice of classifier or dataset.\n", "versions": [{"version": "v1", "created": "Thu, 19 Nov 2020 23:03:22 GMT"}], "update_date": "2020-11-23", "authors_parsed": [["Zhang", "Weilin", ""], ["Wang", "Yu-Xiong", ""], ["Forsyth", "David A.", ""]]}, {"id": "2011.10147", "submitter": "Yair Kittenplon", "authors": "Yair Kittenplon, Yonina C. Eldar, Dan Raviv", "title": "FlowStep3D: Model Unrolling for Self-Supervised Scene Flow Estimation", "comments": "CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimating the 3D motion of points in a scene, known as scene flow, is a core\nproblem in computer vision. Traditional learning-based methods designed to\nlearn end-to-end 3D flow often suffer from poor generalization. Here we present\na recurrent architecture that learns a single step of an unrolled iterative\nalignment procedure for refining scene flow predictions. Inspired by classical\nalgorithms, we demonstrate iterative convergence toward the solution using\nstrong regularization. The proposed method can handle sizeable temporal\ndeformations and suggests a slimmer architecture than competitive all-to-all\ncorrelation approaches. Trained on FlyingThings3D synthetic data only, our\nnetwork successfully generalizes to real scans, outperforming all existing\nmethods by a large margin on the KITTI self-supervised benchmark.\n", "versions": [{"version": "v1", "created": "Thu, 19 Nov 2020 23:23:48 GMT"}, {"version": "v2", "created": "Sun, 4 Apr 2021 14:19:35 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Kittenplon", "Yair", ""], ["Eldar", "Yonina C.", ""], ["Raviv", "Dan", ""]]}, {"id": "2011.10170", "submitter": "Dingwen Tao", "authors": "Chengming Zhang, Geng Yuan, Wei Niu, Jiannan Tian, Sian Jin, Donglin\n  Zhuang, Zhe Jiang, Yanzhi Wang, Bin Ren, Shuaiwen Leon Song, Dingwen Tao", "title": "ClickTrain: Efficient and Accurate End-to-End Deep Learning Training via\n  Fine-Grained Architecture-Preserving Pruning", "comments": "12 pages, 15 figures, 2 tables, published by ICS'21", "journal-ref": null, "doi": "10.1145/3447818.3459988", "report-no": null, "categories": "cs.CV cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Convolutional neural networks (CNNs) are becoming increasingly deeper, wider,\nand non-linear because of the growing demand on prediction accuracy and\nanalysis quality. The wide and deep CNNs, however, require a large amount of\ncomputing resources and processing time. Many previous works have studied model\npruning to improve inference performance, but little work has been done for\neffectively reducing training cost. In this paper, we propose ClickTrain: an\nefficient and accurate end-to-end training and pruning framework for CNNs.\nDifferent from the existing pruning-during-training work, ClickTrain provides\nhigher model accuracy and compression ratio via fine-grained\narchitecture-preserving pruning. By leveraging pattern-based pruning with our\nproposed novel accurate weight importance estimation, dynamic pattern\ngeneration and selection, and compiler-assisted computation optimizations,\nClickTrain generates highly accurate and fast pruned CNN models for direct\ndeployment without any extra time overhead, compared with the baseline\ntraining. ClickTrain also reduces the end-to-end time cost of the\npruning-after-training method by up to 2.3X with comparable accuracy and\ncompression ratio. Moreover, compared with the state-of-the-art\npruning-during-training approach, ClickTrain provides significant improvements\nboth accuracy and compression ratio on the tested CNN models and datasets,\nunder similar limited training time.\n", "versions": [{"version": "v1", "created": "Fri, 20 Nov 2020 01:46:56 GMT"}, {"version": "v2", "created": "Thu, 1 Apr 2021 00:43:17 GMT"}, {"version": "v3", "created": "Fri, 2 Apr 2021 18:55:15 GMT"}, {"version": "v4", "created": "Sat, 1 May 2021 03:33:27 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Zhang", "Chengming", ""], ["Yuan", "Geng", ""], ["Niu", "Wei", ""], ["Tian", "Jiannan", ""], ["Jin", "Sian", ""], ["Zhuang", "Donglin", ""], ["Jiang", "Zhe", ""], ["Wang", "Yanzhi", ""], ["Ren", "Bin", ""], ["Song", "Shuaiwen Leon", ""], ["Tao", "Dingwen", ""]]}, {"id": "2011.10174", "submitter": "Tai Wang", "authors": "Tai Wang, Conghui He, Zhe Wang, Jianping Shi, Dahua Lin", "title": "FLAVA: Find, Localize, Adjust and Verify to Annotate LiDAR-Based Point\n  Clouds", "comments": "Full technical report for the UIST 2020 Poster version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV cs.HC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Recent years have witnessed the rapid progress of perception algorithms on\ntop of LiDAR, a widely adopted sensor for autonomous driving systems. These\nLiDAR-based solutions are typically data hungry, requiring a large amount of\ndata to be labeled for training and evaluation. However, annotating this kind\nof data is very challenging due to the sparsity and irregularity of point\nclouds and more complex interaction involved in this procedure. To tackle this\nproblem, we propose FLAVA, a systematic approach to minimizing human\ninteraction in the annotation process. Specifically, we divide the annotation\npipeline into four parts: find, localize, adjust and verify. In addition, we\ncarefully design the UI for different stages of the annotation procedure, thus\nkeeping the annotators to focus on the aspects that are most important to each\nstage. Furthermore, our system also greatly reduces the amount of interaction\nby introducing a light-weight yet effective mechanism to propagate the\nannotation results. Experimental results show that our method can remarkably\naccelerate the procedure and improve the annotation quality.\n", "versions": [{"version": "v1", "created": "Fri, 20 Nov 2020 02:22:36 GMT"}], "update_date": "2020-11-23", "authors_parsed": [["Wang", "Tai", ""], ["He", "Conghui", ""], ["Wang", "Zhe", ""], ["Shi", "Jianping", ""], ["Lin", "Dahua", ""]]}, {"id": "2011.10185", "submitter": "Zhouyong Liu", "authors": "Zhouyong Liu, Shun Luo, Wubin Li, Jingben Lu, Yufan Wu, Shilei Sun,\n  Chunguo Li, Luxi Yang", "title": "ConvTransformer: A Convolutional Transformer Network for Video Frame\n  Synthesis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Convolutional Neural Networks (CNNs) are powerful models that have\nachieved excellent performance on difficult computer vision tasks. Although\nCNNs perform well whenever large labeled training samples are available, they\nwork badly on video frame synthesis due to objects deforming and moving, scene\nlighting changes, and cameras moving in video sequence. In this paper, we\npresent a novel and general end-to-end architecture, called convolutional\nTransformer or ConvTransformer, for video frame sequence learning and video\nframe synthesis. The core ingredient of ConvTransformer is the proposed\nattention layer, i.e., multi-head convolutional self-attention layer, that\nlearns the sequential dependence of video sequence. ConvTransformer uses an\nencoder, built upon multi-head convolutional self-attention layer, to encode\nthe sequential dependence between the input frames, and then a decoder decodes\nthe long-term dependence between the target synthesized frames and the input\nframes. Experiments on video future frame extrapolation task show\nConvTransformer to be superior in quality while being more parallelizable to\nrecent approaches built upon convolutional LSTM (ConvLSTM). To the best of our\nknowledge, this is the first time that ConvTransformer architecture is proposed\nand applied to video frame synthesis.\n", "versions": [{"version": "v1", "created": "Fri, 20 Nov 2020 02:52:53 GMT"}, {"version": "v2", "created": "Tue, 1 Jun 2021 09:01:10 GMT"}], "update_date": "2021-06-03", "authors_parsed": [["Liu", "Zhouyong", ""], ["Luo", "Shun", ""], ["Li", "Wubin", ""], ["Lu", "Jingben", ""], ["Wu", "Yufan", ""], ["Sun", "Shilei", ""], ["Li", "Chunguo", ""], ["Yang", "Luxi", ""]]}, {"id": "2011.10188", "submitter": "Naimul Mefraz Khan", "authors": "Nicolas Ewen and Naimul Khan", "title": "Targeted Self Supervision for Classification on a Small COVID-19 CT Scan\n  Dataset", "comments": "Submitted to ISBI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditionally, convolutional neural networks need large amounts of data\nlabelled by humans to train. Self supervision has been proposed as a method of\ndealing with small amounts of labelled data. The aim of this study is to\ndetermine whether self supervision can increase classification performance on a\nsmall COVID-19 CT scan dataset. This study also aims to determine whether the\nproposed self supervision strategy, targeted self supervision, is a viable\noption for a COVID-19 imaging dataset. A total of 10 experiments are run\ncomparing the classification performance of the proposed method of self\nsupervision with different amounts of data. The experiments run with the\nproposed self supervision strategy perform significantly better than their\nnon-self supervised counterparts. We get almost 8% increase in accuracy with\nfull self supervision when compared to no self supervision. The results suggest\nthat self supervision can improve classification performance on a small\nCOVID-19 CT scan dataset. Code for targeted self supervision can be found at\nthis link:\nhttps://github.com/Mewtwo/Targeted-Self-Supervision/tree/main/COVID-CT\n", "versions": [{"version": "v1", "created": "Fri, 20 Nov 2020 03:07:17 GMT"}], "update_date": "2020-11-23", "authors_parsed": [["Ewen", "Nicolas", ""], ["Khan", "Naimul", ""]]}, {"id": "2011.10189", "submitter": "Yekai Wang", "authors": "Yekai Wang", "title": "MobileDepth: Efficient Monocular Depth Prediction on Mobile Devices", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Depth prediction is fundamental for many useful applications on computer\nvision and robotic systems. On mobile phones, the performance of some useful\napplications such as augmented reality, autofocus and so on could be enhanced\nby accurate depth prediction. In this work, an efficient fully convolutional\nnetwork architecture for depth prediction has been proposed, which uses RegNetY\n06 as the encoder and split-concatenate shuffle blocks as decoder. At the same\ntime, an appropriate combination of data augmentation, hyper-parameters and\nloss functions to efficiently train the lightweight network has been provided.\nAlso, an Android application has been developed which can load CNN models to\npredict depth map by the monocular images captured from the mobile camera and\nevaluate the average latency and frame per second of the models. As a result,\nthe network achieves 82.7% {\\delta}1 accuracy on NYU Depth v2 dataset and at\nthe same time, have only 62ms latency on ARM A76 CPUs so that it can predict\nthe depth map from the mobile camera in real-time.\n", "versions": [{"version": "v1", "created": "Fri, 20 Nov 2020 03:08:54 GMT"}], "update_date": "2020-11-23", "authors_parsed": [["Wang", "Yekai", ""]]}, {"id": "2011.10190", "submitter": "Reza Ghoddoosian", "authors": "Reza Ghoddoosian, Saif Sayed, Vassilis Athitsos", "title": "Action Duration Prediction for Segment-Level Alignment of Weakly-Labeled\n  Videos", "comments": "Accepted to WACV 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper focuses on weakly-supervised action alignment, where only the\nordered sequence of video-level actions is available for training. We propose a\nnovel Duration Network, which captures a short temporal window of the video and\nlearns to predict the remaining duration of a given action at any point in time\nwith a level of granularity based on the type of that action. Further, we\nintroduce a Segment-Level Beam Search to obtain the best alignment, that\nmaximizes our posterior probability. Segment-Level Beam Search efficiently\naligns actions by considering only a selected set of frames that have more\nconfident predictions. The experimental results show that our alignments for\nlong videos are more robust than existing models. Moreover, the proposed method\nachieves state of the art results in certain cases on the popular Breakfast and\nHollywood Extended datasets.\n", "versions": [{"version": "v1", "created": "Fri, 20 Nov 2020 03:16:53 GMT"}], "update_date": "2020-11-23", "authors_parsed": [["Ghoddoosian", "Reza", ""], ["Sayed", "Saif", ""], ["Athitsos", "Vassilis", ""]]}, {"id": "2011.10201", "submitter": "Sokratis Makrogiannis", "authors": "Sokratis Makrogiannis and Chelsea E. Harris and Keni Zheng", "title": "Discriminative Localized Sparse Representations for Breast Cancer\n  Screening", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Breast cancer is the most common cancer among women both in developed and\ndeveloping countries. Early detection and diagnosis of breast cancer may reduce\nits mortality and improve the quality of life. Computer-aided detection (CADx)\nand computer-aided diagnosis (CAD) techniques have shown promise for reducing\nthe burden of human expert reading and improve the accuracy and reproducibility\nof results. Sparse analysis techniques have produced relevant results for\nrepresenting and recognizing imaging patterns. In this work we propose a method\nfor Label Consistent Spatially Localized Ensemble Sparse Analysis (LC-SLESA).\nIn this work we apply dictionary learning to our block based sparse analysis\nmethod to classify breast lesions as benign or malignant. The performance of\nour method in conjunction with LC-KSVD dictionary learning is evaluated using\n10-, 20-, and 30-fold cross validation on the MIAS dataset. Our results\nindicate that the proposed sparse analyses may be a useful component for breast\ncancer screening applications.\n", "versions": [{"version": "v1", "created": "Fri, 20 Nov 2020 04:15:17 GMT"}], "update_date": "2020-11-23", "authors_parsed": [["Makrogiannis", "Sokratis", ""], ["Harris", "Chelsea E.", ""], ["Zheng", "Keni", ""]]}, {"id": "2011.10202", "submitter": "Parker Lusk", "authors": "Parker C. Lusk, Kaveh Fathian, Jonathan P. How", "title": "CLIPPER: A Graph-Theoretic Framework for Robust Data Association", "comments": "accepted ICRA'21", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present CLIPPER (Consistent LInking, Pruning, and Pairwise Error\nRectification), a framework for robust data association in the presence of\nnoise and outliers. We formulate the problem in a graph-theoretic framework\nusing the notion of geometric consistency. State-of-the-art techniques that use\nthis framework utilize either combinatorial optimization techniques that do not\nscale well to large-sized problems, or use heuristic approximations that yield\nlow accuracy in high-noise, high-outlier regimes. In contrast, CLIPPER uses a\nrelaxation of the combinatorial problem and returns solutions that are\nguaranteed to correspond to the optima of the original problem. Low time\ncomplexity is achieved with an efficient projected gradient ascent approach.\nExperiments indicate that CLIPPER maintains a consistently low runtime of 15 ms\nwhere exact methods can require up to 24 s at their peak, even on small-sized\nproblems with 200 associations. When evaluated on noisy point cloud\nregistration problems, CLIPPER achieves 100% precision and 98% recall in 90%\noutlier regimes while competing algorithms begin degrading by 70% outliers. In\nan instance of associating noisy points of the Stanford Bunny with 990 outlier\nassociations and only 10 inlier associations, CLIPPER successfully returns 8\ninlier associations with 100% precision in 138 ms. Code is available at\nhttps://mit-acl.github.io/clipper.\n", "versions": [{"version": "v1", "created": "Fri, 20 Nov 2020 04:15:54 GMT"}, {"version": "v2", "created": "Fri, 9 Apr 2021 12:57:02 GMT"}], "update_date": "2021-04-12", "authors_parsed": [["Lusk", "Parker C.", ""], ["Fathian", "Kaveh", ""], ["How", "Jonathan P.", ""]]}, {"id": "2011.10217", "submitter": "Chunhua Shen", "authors": "Jianpeng Zhang, Yutong Xie, Yong Xia, Chunhua Shen", "title": "DoDNet: Learning to segment multi-organ and tumors from multiple\n  partially labeled datasets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Due to the intensive cost of labor and expertise in annotating 3D medical\nimages at a voxel level, most benchmark datasets are equipped with the\nannotations of only one type of organs and/or tumors, resulting in the\nso-called partially labeling issue. To address this, we propose a dynamic\non-demand network (DoDNet) that learns to segment multiple organs and tumors on\npartially labeled datasets. DoDNet consists of a shared encoder-decoder\narchitecture, a task encoding module, a controller for generating dynamic\nconvolution filters, and a single but dynamic segmentation head. The\ninformation of the current segmentation task is encoded as a task-aware prior\nto tell the model what the task is expected to solve. Different from existing\napproaches which fix kernels after training, the kernels in dynamic head are\ngenerated adaptively by the controller, conditioned on both input image and\nassigned task. Thus, DoDNet is able to segment multiple organs and tumors, as\ndone by multiple networks or a multi-head network, in a much efficient and\nflexible manner. We have created a large-scale partially labeled dataset,\ntermed MOTS, and demonstrated the superior performance of our DoDNet over other\ncompetitors on seven organ and tumor segmentation tasks. We also transferred\nthe weights pre-trained on MOTS to a downstream multi-organ segmentation task\nand achieved state-of-the-art performance. This study provides a general 3D\nmedical image segmentation model that has been pre-trained on a large-scale\npartially labelled dataset and can be extended (after fine-tuning) to\ndownstream volumetric medical data segmentation tasks. The dataset and code\nareavailableat: https://git.io/DoDNet\n", "versions": [{"version": "v1", "created": "Fri, 20 Nov 2020 04:56:39 GMT"}], "update_date": "2020-11-23", "authors_parsed": [["Zhang", "Jianpeng", ""], ["Xie", "Yutong", ""], ["Xia", "Yong", ""], ["Shen", "Chunhua", ""]]}, {"id": "2011.10223", "submitter": "Jayadeva", "authors": "Himanshu Pant, Jayadeva and Sumit Soman", "title": "Complexity Controlled Generative Adversarial Networks", "comments": "11 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the issues faced in training Generative Adversarial Nets (GANs) and\ntheir variants is the problem of mode collapse, wherein the training stability\nin terms of the generative loss increases as more training data is used. In\nthis paper, we propose an alternative architecture via the Low-Complexity\nNeural Network (LCNN), which attempts to learn models with low complexity. The\nmotivation is that controlling model complexity leads to models that do not\noverfit the training data. We incorporate the LCNN loss function for GANs, Deep\nConvolutional GANs (DCGANs) and Spectral Normalized GANs (SNGANs), in order to\ndevelop hybrid architectures called the LCNN-GAN, LCNN-DCGAN and LCNN-SNGAN\nrespectively. On various large benchmark image datasets, we show that the use\nof our proposed models results in stable training while avoiding the problem of\nmode collapse, resulting in better training stability. We also show how the\nlearning behavior can be controlled by a hyperparameter in the LCNN functional,\nwhich also provides an improved inception score.\n", "versions": [{"version": "v1", "created": "Fri, 20 Nov 2020 05:35:55 GMT"}], "update_date": "2020-11-23", "authors_parsed": [["Pant", "Himanshu", ""], ["Jayadeva", "", ""], ["Soman", "Sumit", ""]]}, {"id": "2011.10231", "submitter": "Burak Uzkent", "authors": "Shuvam Chakraborty, Burak Uzkent, Kumar Ayush, Kumar Tanmay, Evan\n  Sheehan, Stefano Ermon", "title": "Efficient Conditional Pre-training for Transfer Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Almost all the state-of-the-art neural networks for computer vision tasks are\ntrained by (1) pre-training on a large-scale dataset and (2) finetuning on the\ntarget dataset. This strategy helps reduce dependence on the target dataset and\nimproves convergence rate and generalization on the target task. Although\npre-training on large-scale datasets is very useful, its foremost disadvantage\nis high training cost. To address this, we propose efficient filtering methods\nto select relevant subsets from the pre-training dataset. Additionally, we\ndiscover that lowering image resolutions in the pre-training step offers a\ngreat trade-off between cost and performance. We validate our techniques by\npre-training on ImageNet in both the unsupervised and supervised settings and\nfinetuning on a diverse collection of target datasets and tasks. Our proposed\nmethods drastically reduce pre-training cost and provide strong performance\nboosts. Finally, we improve standard ImageNet pre-training by 1-3% by tuning\navailable models on our subsets and pre-training on a dataset filtered from a\nlarger scale dataset.\n", "versions": [{"version": "v1", "created": "Fri, 20 Nov 2020 06:16:15 GMT"}, {"version": "v2", "created": "Thu, 10 Dec 2020 08:36:12 GMT"}, {"version": "v3", "created": "Thu, 24 Dec 2020 01:04:42 GMT"}, {"version": "v4", "created": "Fri, 9 Apr 2021 22:52:18 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Chakraborty", "Shuvam", ""], ["Uzkent", "Burak", ""], ["Ayush", "Kumar", ""], ["Tanmay", "Kumar", ""], ["Sheehan", "Evan", ""], ["Ermon", "Stefano", ""]]}, {"id": "2011.10232", "submitter": "Yusuke Monno", "authors": "Takeru Suda, Masayuki Tanaka, Yusuke Monno, Masatoshi Okutomi", "title": "Deep Snapshot HDR Imaging Using Multi-Exposure Color Filter Array", "comments": "Accepted at ACCV2020 (Oral). Project page:\n  http://www.ok.sc.e.titech.ac.jp/res/DSHDR/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a deep snapshot high dynamic range (HDR) imaging\nframework that can effectively reconstruct an HDR image from the RAW data\ncaptured using a multi-exposure color filter array (ME-CFA), which consists of\na mosaic pattern of RGB filters with different exposure levels. To effectively\nlearn the HDR image reconstruction network, we introduce the idea of luminance\nnormalization that simultaneously enables effective loss computation and input\ndata normalization by considering relative local contrasts in the\n\"normalized-by-luminance\" HDR domain. This idea makes it possible to equally\nhandle the errors in both bright and dark areas regardless of absolute\nluminance levels, which significantly improves the visual image quality in a\ntone-mapped domain. Experimental results using two public HDR image datasets\ndemonstrate that our framework outperforms other snapshot methods and produces\nhigh-quality HDR images with fewer visual artifacts.\n", "versions": [{"version": "v1", "created": "Fri, 20 Nov 2020 06:31:37 GMT"}], "update_date": "2020-11-23", "authors_parsed": [["Suda", "Takeru", ""], ["Tanaka", "Masayuki", ""], ["Monno", "Yusuke", ""], ["Okutomi", "Masatoshi", ""]]}, {"id": "2011.10239", "submitter": "Fangrui Liu", "authors": "Fangrui Liu, Zheng Liu", "title": "Shuffle and Learn: Minimizing Mutual Information for Unsupervised\n  Hashing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Unsupervised binary representation allows fast data retrieval without any\nannotations, enabling practical application like fast person re-identification\nand multimedia retrieval. It is argued that conflicts in binary space are one\nof the major barriers to high-performance unsupervised hashing as current\nmethods failed to capture the precise code conflicts in the full domain. A\nnovel relaxation method called Shuffle and Learn is proposed to tackle code\nconflicts in the unsupervised hash. Approximated derivatives for joint\nprobability and the gradients for the binary layer are introduced to bridge the\nupdate from the hash to the input. Proof on $\\epsilon$-Convergence of joint\nprobability with approximated derivatives is provided to guarantee the\npreciseness on update applied on the mutual information. The proposed algorithm\nis carried out with iterative global updates to minimize mutual information,\ndiverging the code before regular unsupervised optimization. Experiments\nsuggest that the proposed method can relax the code optimization from local\noptimum and help to generate binary representations that are more\ndiscriminative and informative without any annotations. Performance benchmarks\non image retrieval with the unsupervised binary code are conducted on three\nopen datasets, and the model achieves state-of-the-art accuracy on image\nretrieval task for all those datasets. Datasets and reproducible code are\nprovided.\n", "versions": [{"version": "v1", "created": "Fri, 20 Nov 2020 07:14:55 GMT"}], "update_date": "2020-11-23", "authors_parsed": [["Liu", "Fangrui", ""], ["Liu", "Zheng", ""]]}, {"id": "2011.10241", "submitter": "Jia Peng", "authors": "Peng Jia, Mingyang Ma, Dongmei Cai, Weihua Wang, Juanjuan Li, Can Li", "title": "Compressive Shack-Hartmann Wavefront Sensor based on Deep Neural\n  Networks", "comments": "To appear in the MNRAS and the complete code can be downloaded from:\n  https://nadc.china-vo.org/article/20200722160959?id=101045", "journal-ref": null, "doi": "10.1093/mnras/staa4045", "report-no": null, "categories": "astro-ph.IM astro-ph.GA astro-ph.SR cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The Shack-Hartmann wavefront sensor is widely used to measure aberrations\ninduced by atmospheric turbulence in adaptive optics systems. However if there\nexists strong atmospheric turbulence or the brightness of guide stars is low,\nthe accuracy of wavefront measurements will be affected. In this paper, we\npropose a compressive Shack-Hartmann wavefront sensing method. Instead of\nreconstructing wavefronts with slope measurements of all sub-apertures, our\nmethod reconstructs wavefronts with slope measurements of sub-apertures which\nhave spot images with high signal to noise ratio. Besides, we further propose\nto use a deep neural network to accelerate wavefront reconstruction speed.\nDuring the training stage of the deep neural network, we propose to add a\ndrop-out layer to simulate the compressive sensing process, which could\nincrease development speed of our method. After training, the compressive\nShack-Hartmann wavefront sensing method can reconstruct wavefronts in high\nspatial resolution with slope measurements from only a small amount of\nsub-apertures. We integrate the straightforward compressive Shack-Hartmann\nwavefront sensing method with image deconvolution algorithm to develop a\nhigh-order image restoration method. We use images restored by the high-order\nimage restoration method to test the performance of our the compressive\nShack-Hartmann wavefront sensing method. The results show that our method can\nimprove the accuracy of wavefront measurements and is suitable for real-time\napplications.\n", "versions": [{"version": "v1", "created": "Fri, 20 Nov 2020 07:18:21 GMT"}, {"version": "v2", "created": "Thu, 31 Dec 2020 09:04:38 GMT"}], "update_date": "2021-04-14", "authors_parsed": [["Jia", "Peng", ""], ["Ma", "Mingyang", ""], ["Cai", "Dongmei", ""], ["Wang", "Weihua", ""], ["Li", "Juanjuan", ""], ["Li", "Can", ""]]}, {"id": "2011.10243", "submitter": "Jia Peng", "authors": "Peng Jia, Xuebo Wu, Zhengyang Li, Bo Li, Weihua Wang, Qiang Liu, Adam\n  Popowicz", "title": "Point Spread Function Estimation for Wide Field Small Aperture\n  Telescopes with Deep Neural Networks and Calibration Data", "comments": "Accepted by the MNRAS", "journal-ref": null, "doi": "10.1093/mnras/stab1461", "report-no": null, "categories": "astro-ph.IM astro-ph.GA astro-ph.SR cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The point spread function (PSF) reflects states of a telescope and plays an\nimportant role in development of data processing methods, such as PSF based\nastrometry, photometry and image restoration. However, for wide field small\naperture telescopes (WFSATs), estimating PSF in any position of the whole field\nof view is hard, because aberrations induced by the optical system are quite\ncomplex and the signal to noise ratio of star images is often too low for PSF\nestimation. In this paper, we further develop our deep neural network (DNN)\nbased PSF modelling method and show its applications in PSF estimation. During\nthe telescope alignment and testing stage, our method collects system\ncalibration data through modification of optical elements within engineering\ntolerances (tilting and decentering). Then we use these data to train a DNN\n(Tel--Net). After training, the Tel--Net can estimate PSF in any field of view\nfrom several discretely sampled star images. We use both simulated and\nexperimental data to test performance of our method. The results show that the\nTel--Net can successfully reconstruct PSFs of WFSATs of any states and in any\npositions of the FoV. Its results are significantly more precise than results\nobtained by the compared classic method - Inverse Distance Weight (IDW)\ninterpolation. Our method provides foundations for developing of deep neural\nnetwork based data processing methods for WFSATs, which require strong prior\ninformation of PSFs.\n", "versions": [{"version": "v1", "created": "Fri, 20 Nov 2020 07:26:02 GMT"}, {"version": "v2", "created": "Tue, 18 May 2021 11:48:57 GMT"}], "update_date": "2021-07-07", "authors_parsed": [["Jia", "Peng", ""], ["Wu", "Xuebo", ""], ["Li", "Zhengyang", ""], ["Li", "Bo", ""], ["Wang", "Weihua", ""], ["Liu", "Qiang", ""], ["Popowicz", "Adam", ""]]}, {"id": "2011.10250", "submitter": "Zhenhua Wang", "authors": "Zhenhua Wang, Jiajun Meng, Dongyan Guo, Jianhua Zhang, Javen Qinfeng\n  Shi, Shengyong Chen", "title": "Consistency-Aware Graph Network for Human Interaction Understanding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Compared with the progress made on human activity classification, much less\nsuccess has been achieved on human interaction understanding (HIU). Apart from\nthe latter task is much more challenging, the main cause is that recent\napproaches learn human interactive relations via shallow graphical models,\nwhich is inadequate to model complicated human interactions. In this paper, we\npropose a consistency-aware graph network, which combines the representative\nability of graph network and the consistency-aware reasoning to facilitate the\nHIU task. Our network consists of three components, a backbone CNN to extract\nimage features, a factor graph network to learn third-order interactive\nrelations among participants, and a consistency-aware reasoning module to\nenforce labeling and grouping consistencies. Our key observation is that the\nconsistency-aware-reasoning bias for HIU can be embedded into an energy\nfunction, minimizing which delivers consistent predictions. An efficient\nmean-field inference algorithm is proposed, such that all modules of our\nnetwork could be trained jointly in an end-to-end manner. Experimental results\nshow that our approach achieves leading performance on three benchmarks.\n", "versions": [{"version": "v1", "created": "Fri, 20 Nov 2020 07:49:21 GMT"}, {"version": "v2", "created": "Thu, 26 Nov 2020 08:20:21 GMT"}, {"version": "v3", "created": "Tue, 23 Mar 2021 15:32:12 GMT"}], "update_date": "2021-03-24", "authors_parsed": [["Wang", "Zhenhua", ""], ["Meng", "Jiajun", ""], ["Guo", "Dongyan", ""], ["Zhang", "Jianhua", ""], ["Shi", "Javen Qinfeng", ""], ["Chen", "Shengyong", ""]]}, {"id": "2011.10251", "submitter": "Dhruval Jain", "authors": "Dhruval Jain, Arun D Prabhu, Gopi Ramena, Manoj Goyal, Debi Prasanna\n  Mohanty, Sukumar Moharana, Naresh Purre", "title": "On-Device Text Image Super Resolution", "comments": "Accepted to the International Conference on Pattern\n  Recognition(ICPR), 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Recent research on super-resolution (SR) has witnessed major developments\nwith the advancements of deep convolutional neural networks. There is a need\nfor information extraction from scenic text images or even document images on\ndevice, most of which are low-resolution (LR) images. Therefore, SR becomes an\nessential pre-processing step as Bicubic Upsampling, which is conventionally\npresent in smartphones, performs poorly on LR images. To give the user more\ncontrol over his privacy, and to reduce the carbon footprint by reducing the\noverhead of cloud computing and hours of GPU usage, executing SR models on the\nedge is a necessity in the recent times. There are various challenges in\nrunning and optimizing a model on resource-constrained platforms like\nsmartphones. In this paper, we present a novel deep neural network that\nreconstructs sharper character edges and thus boosts OCR confidence. The\nproposed architecture not only achieves significant improvement in PSNR over\nbicubic upsampling on various benchmark datasets but also runs with an average\ninference time of 11.7 ms per image. We have outperformed state-of-the-art on\nthe Text330 dataset. We also achieve an OCR accuracy of 75.89% on the ICDAR\n2015 TextSR dataset, where ground truth has an accuracy of 78.10%.\n", "versions": [{"version": "v1", "created": "Fri, 20 Nov 2020 07:49:48 GMT"}], "update_date": "2020-11-23", "authors_parsed": [["Jain", "Dhruval", ""], ["Prabhu", "Arun D", ""], ["Ramena", "Gopi", ""], ["Goyal", "Manoj", ""], ["Mohanty", "Debi Prasanna", ""], ["Moharana", "Sukumar", ""], ["Purre", "Naresh", ""]]}, {"id": "2011.10258", "submitter": "Wenlong Gao", "authors": "Wenlong Gao and Ying Chen and Yong Peng", "title": "Cascade Attentive Dropout for Weakly Supervised Object Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Weakly supervised object detection (WSOD) aims to classify and locate objects\nwith only image-level supervision. Many WSOD approaches adopt multiple instance\nlearning as the initial model, which is prone to converge to the most\ndiscriminative object regions while ignoring the whole object, and therefore\nreduce the model detection performance. In this paper, a novel cascade\nattentive dropout strategy is proposed to alleviate the part domination\nproblem, together with an improved global context module. We purposely discard\nattentive elements in both channel and space dimensions, and capture the\ninter-pixel and inter-channel dependencies to induce the model to better\nunderstand the global context. Extensive experiments have been conducted on the\nchallenging PASCAL VOC 2007 benchmarks, which achieve 49.8% mAP and 66.0%\nCorLoc, outperforming state-of-the-arts.\n", "versions": [{"version": "v1", "created": "Fri, 20 Nov 2020 08:08:13 GMT"}], "update_date": "2020-11-23", "authors_parsed": [["Gao", "Wenlong", ""], ["Chen", "Ying", ""], ["Peng", "Yong", ""]]}, {"id": "2011.10260", "submitter": "Jie Chen", "authors": "Tingting Zhang (1), Jie Chen (1), Caiying Wu (1), Zhifei He (1),\n  Tieyong Zeng (2) and Qiyu Jin (1) ((1) School of Mathematical Science, Inner\n  Mongolia University, Hohhot, China (2) Department of Mathematics, The Chinese\n  University of Hong Kong, Shatin, Hong Kong, China)", "title": "Edge Adaptive Hybrid Regularization Model For Image Deblurring", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The parameter selection is crucial to regularization based image restoration\nmethods. Generally speaking, a spatially fixed parameter for regularization\nitem in the whole image does not perform well for both edge and smooth areas. A\nlarger parameter of regularization item reduces noise better in smooth areas\nbut blurs edge regions, while a small parameter sharpens edge but causes\nresidual noise. In this paper, an automated spatially adaptive regularization\nmodel, which combines the harmonic and TV models, is proposed for\nreconstruction of noisy and blurred images. In the proposed model, it detects\nthe edges and then spatially adjusts the parameters of Tikhonov and TV\nregularization terms for each pixel according to the edge information.\nAccordingly, the edge information matrix will be also dynamically updated\nduring the iterations. Computationally, the newly-established model is convex,\nwhich can be solved by the semi-proximal alternating direction method of\nmultipliers (sPADMM) with a linear-rate convergence rate. Numerical simulation\nresults demonstrate that the proposed model effectively reserves the image\nedges and eliminates the noise and blur at the same time. In comparison to\nstate-of-the-art algorithms, it outperforms other methods in terms of PSNR,\nSSIM and visual quality.\n", "versions": [{"version": "v1", "created": "Fri, 20 Nov 2020 08:12:23 GMT"}, {"version": "v2", "created": "Tue, 6 Apr 2021 09:02:24 GMT"}], "update_date": "2021-04-07", "authors_parsed": [["Zhang", "Tingting", ""], ["Chen", "Jie", ""], ["Wu", "Caiying", ""], ["He", "Zhifei", ""], ["Zeng", "Tieyong", ""], ["Jin", "Qiyu", ""]]}, {"id": "2011.10269", "submitter": "Jiali Duan", "authors": "Jiali Duan, Yen-Liang Lin, Son Tran, Larry S. Davis and C.-C. Jay Kuo", "title": "SLADE: A Self-Training Framework For Distance Metric Learning", "comments": "Accepted by CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most existing distance metric learning approaches use fully labeled data to\nlearn the sample similarities in an embedding space. We present a self-training\nframework, SLADE, to improve retrieval performance by leveraging additional\nunlabeled data. We first train a teacher model on the labeled data and use it\nto generate pseudo labels for the unlabeled data. We then train a student model\non both labels and pseudo labels to generate final feature embeddings. We use\nself-supervised representation learning to initialize the teacher model. To\nbetter deal with noisy pseudo labels generated by the teacher network, we\ndesign a new feature basis learning component for the student network, which\nlearns basis functions of feature representations for unlabeled data. The\nlearned basis vectors better measure the pairwise similarity and are used to\nselect high-confident samples for training the student network. We evaluate our\nmethod on standard retrieval benchmarks: CUB-200, Cars-196 and In-shop.\nExperimental results demonstrate that our approach significantly improves the\nperformance over the state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Fri, 20 Nov 2020 08:26:10 GMT"}, {"version": "v2", "created": "Mon, 29 Mar 2021 20:15:39 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Duan", "Jiali", ""], ["Lin", "Yen-Liang", ""], ["Tran", "Son", ""], ["Davis", "Larry S.", ""], ["Kuo", "C. -C. Jay", ""]]}, {"id": "2011.10274", "submitter": "Boris Chidlovskii", "authors": "Maxime Pietrantoni, Boris Chidlovskii, Tomi Silander", "title": "Learning Synthetic to Real Transfer for Localization and Navigational\n  Tasks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Autonomous navigation consists in an agent being able to navigate without\nhuman intervention or supervision, it affects both high level planning and low\nlevel control. Navigation is at the crossroad of multiple disciplines, it\ncombines notions of computer vision, robotics and control. This work aimed at\ncreating, in a simulation, a navigation pipeline whose transfer to the real\nworld could be done with as few efforts as possible. Given the limited time and\nthe wide range of problematic to be tackled, absolute navigation performances\nwhile important was not the main objective. The emphasis was rather put on\nstudying the sim2real gap which is one the major bottlenecks of modern robotics\nand autonomous navigation. To design the navigation pipeline four main\nchallenges arise; environment, localization, navigation and planning. The\niGibson simulator is picked for its photo-realistic textures and physics\nengine. A topological approach to tackle space representation was picked over\nmetric approaches because they generalize better to new environments and are\nless sensitive to change of conditions. The navigation pipeline is decomposed\nas a localization module, a planning module and a local navigation module.\nThese modules utilize three different networks, an image representation\nextractor, a passage detector and a local policy. The laters are trained on\nspecifically tailored tasks with some associated datasets created for those\nspecific tasks. Localization is the ability for the agent to localize itself\nagainst a specific space representation. It must be reliable, repeatable and\nrobust to a wide variety of transformations. Localization is tackled as an\nimage retrieval task using a deep neural network trained on an auxiliary task\nas a feature descriptor extractor. The local policy is trained with behavioral\ncloning from expert trajectories gathered with ROS navigation stack.\n", "versions": [{"version": "v1", "created": "Fri, 20 Nov 2020 08:37:03 GMT"}, {"version": "v2", "created": "Mon, 23 Nov 2020 16:43:22 GMT"}], "update_date": "2020-11-24", "authors_parsed": [["Pietrantoni", "Maxime", ""], ["Chidlovskii", "Boris", ""], ["Silander", "Tomi", ""]]}, {"id": "2011.10278", "submitter": "Junho Koh", "authors": "Junho Koh, Jaekyum Kim, Younji Shin, Byeongwon Lee, Seungji Yang and\n  Jun Won Choi", "title": "Joint Representation of Temporal Image Sequences and Object Motion for\n  Video Object Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we propose a new video object detector (VoD) method referred\nto as temporal feature aggregation and motion-aware VoD (TM-VoD), which\nproduces a joint representation of temporal image sequences and object motion.\nThe proposed TM-VoD aggregates visual feature maps extracted by convolutional\nneural networks applying the temporal attention gating and spatial feature\nalignment. This temporal feature aggregation is performed in two stages in a\nhierarchical fashion. In the first stage, the visual feature maps are fused at\na pixel level via gated attention model. In the second stage, the proposed\nmethod aggregates the features after aligning the object features using\ntemporal box offset calibration and weights them according to the cosine\nsimilarity measure. The proposed TM-VoD also finds the representation of the\nmotion of objects in two successive steps. The pixel-level motion features are\nfirst computed based on the incremental changes between the adjacent visual\nfeature maps. Then, box-level motion features are obtained from both the region\nof interest (RoI)-aligned pixel-level motion features and the sequential\nchanges of the box coordinates. Finally, all these features are concatenated to\nproduce a joint representation of the objects for VoD. The experiments\nconducted on the ImageNet VID dataset demonstrate that the proposed method\noutperforms existing VoD methods and achieves a performance comparable to that\nof state-of-the-art VoDs.\n", "versions": [{"version": "v1", "created": "Fri, 20 Nov 2020 08:46:12 GMT"}], "update_date": "2020-11-23", "authors_parsed": [["Koh", "Junho", ""], ["Kim", "Jaekyum", ""], ["Shin", "Younji", ""], ["Lee", "Byeongwon", ""], ["Yang", "Seungji", ""], ["Choi", "Jun Won", ""]]}, {"id": "2011.10284", "submitter": "Nils Thuerey", "authors": "Marie-Lena Eckert, Kiwon Um, Nils Thuerey", "title": "ScalarFlow: A Large-Scale Volumetric Data Set of Real-world Scalar\n  Transport Flows for Computer Animation and Machine Learning", "comments": "Details and data at:\n  https://ge.in.tum.de/publications/2019-scalarflow-eckert/", "journal-ref": null, "doi": "10.1145/3355089.3356545", "report-no": null, "categories": "cs.GR cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present ScalarFlow, a first large-scale data set of\nreconstructions of real-world smoke plumes. We additionally propose a framework\nfor accurate physics-based reconstructions from a small number of video\nstreams. Central components of our algorithm are a novel estimation of unseen\ninflow regions and an efficient regularization scheme. Our data set includes a\nlarge number of complex and natural buoyancy-driven flows. The flows transition\nto turbulent flows and contain observable scalar transport processes. As such,\nthe ScalarFlow data set is tailored towards computer graphics, vision, and\nlearning applications. The published data set will contain volumetric\nreconstructions of velocity and density, input image sequences, together with\ncalibration data, code, and instructions how to recreate the commodity hardware\ncapture setup. We further demonstrate one of the many potential application\nareas: a first perceptual evaluation study, which reveals that the complexity\nof the captured flows requires a huge simulation resolution for regular solvers\nin order to recreate at least parts of the natural complexity contained in the\ncaptured data.\n", "versions": [{"version": "v1", "created": "Fri, 20 Nov 2020 08:55:00 GMT"}], "update_date": "2020-11-23", "authors_parsed": [["Eckert", "Marie-Lena", ""], ["Um", "Kiwon", ""], ["Thuerey", "Nils", ""]]}, {"id": "2011.10287", "submitter": "Sindy L\\\"owe", "authors": "Sindy L\\\"owe, Klaus Greff, Rico Jonschkowski, Alexey Dosovitskiy,\n  Thomas Kipf", "title": "Learning Object-Centric Video Models by Contrasting Sets", "comments": "NeurIPS 2020 Workshop on Object Representations for Learning and\n  Reasoning", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Contrastive, self-supervised learning of object representations recently\nemerged as an attractive alternative to reconstruction-based training. Prior\napproaches focus on contrasting individual object representations (slots)\nagainst one another. However, a fundamental problem with this approach is that\nthe overall contrastive loss is the same for (i) representing a different\nobject in each slot, as it is for (ii) (re-)representing the same object in all\nslots. Thus, this objective does not inherently push towards the emergence of\nobject-centric representations in the slots. We address this problem by\nintroducing a global, set-based contrastive loss: instead of contrasting\nindividual slot representations against one another, we aggregate the\nrepresentations and contrast the joined sets against one another. Additionally,\nwe introduce attention-based encoders to this contrastive setup which\nsimplifies training and provides interpretable object masks. Our results on two\nsynthetic video datasets suggest that this approach compares favorably against\nprevious contrastive methods in terms of reconstruction, future prediction and\nobject separation performance.\n", "versions": [{"version": "v1", "created": "Fri, 20 Nov 2020 09:13:42 GMT"}], "update_date": "2020-11-23", "authors_parsed": [["L\u00f6we", "Sindy", ""], ["Greff", "Klaus", ""], ["Jonschkowski", "Rico", ""], ["Dosovitskiy", "Alexey", ""], ["Kipf", "Thomas", ""]]}, {"id": "2011.10290", "submitter": "Jing Guo", "authors": "Jing Guo (1), Shuping Wang (1), Chen Luo (1), Qiyu Jin (1), Michael\n  Kwok-Po Ng (2) ((1) School of Mathematical Science, Inner Mongolia\n  University, Hohhot, China, (2) Department of Mathematics, University of Hong\n  Kong, Pokfulam, Hong Kong, China)", "title": "Image Denoising by Gaussian Patch Mixture Model and Low Rank Patches", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Non-local self-similarity based low rank algorithms are the state-of-the-art\nmethods for image denoising. In this paper, a new method is proposed by solving\ntwo issues: how to improve similar patches matching accuracy and build an\nappropriate low rank matrix approximation model for Gaussian noise. For the\nfirst issue, similar patches can be found locally or globally. Local patch\nmatching is to find similar patches in a large neighborhood which can alleviate\nnoise effect, but the number of patches may be insufficient. Global patch\nmatching is to determine enough similar patches but the error rate of patch\nmatching may be higher. Based on this, we first use local patch matching method\nto reduce noise and then use Gaussian patch mixture model to achieve global\npatch matching. The second issue is that there is no low rank matrix\napproximation model to adapt to Gaussian noise. We build a new model according\nto the characteristics of Gaussian noise, then prove that there is a globally\noptimal solution of the model. By solving the two issues, experimental results\nare reported to show that the proposed approach outperforms the\nstate-of-the-art denoising methods includes several deep learning ones in both\nPSNR / SSIM values and visual quality.\n", "versions": [{"version": "v1", "created": "Fri, 20 Nov 2020 09:28:04 GMT"}], "update_date": "2020-11-23", "authors_parsed": [["Guo", "Jing", ""], ["Wang", "Shuping", ""], ["Luo", "Chen", ""], ["Jin", "Qiyu", ""], ["Ng", "Michael Kwok-Po", ""]]}, {"id": "2011.10313", "submitter": "Peng Peng", "authors": "Peng Peng and Jiugen Wang", "title": "Segmentation overlapping wear particles with few labelled data and\n  imbalance sample", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Ferrograph image segmentation is of significance for obtaining features of\nwear particles. However, wear particles are usually overlapped in the form of\ndebris chains, which makes challenges to segment wear debris. An overlapping\nwear particle segmentation network (OWPSNet) is proposed in this study to\nsegment the overlapped debris chains. The proposed deep learning model includes\nthree parts: a region segmentation network, an edge detection network and a\nfeature refine module. The region segmentation network is an improved U shape\nnetwork, and it is applied to separate the wear debris form background of\nferrograph image. The edge detection network is used to detect the edges of\nwear particles. Then, the feature refine module combines low-level features and\nhigh-level semantic features to obtain the final results. In order to solve the\nproblem of sample imbalance, we proposed a square dice loss function to\noptimize the model. Finally, extensive experiments have been carried out on a\nferrograph image dataset. Results show that the proposed model is capable of\nseparating overlapping wear particles. Moreover, the proposed square dice loss\nfunction can improve the segmentation results, especially for the segmentation\nresults of wear particle edge.\n", "versions": [{"version": "v1", "created": "Fri, 20 Nov 2020 10:04:16 GMT"}], "update_date": "2020-11-23", "authors_parsed": [["Peng", "Peng", ""], ["Wang", "Jiugen", ""]]}, {"id": "2011.10328", "submitter": "Vitus Benson", "authors": "Vitus Benson and Alexander Ecker", "title": "Assessing out-of-domain generalization for robust building damage\n  detection", "comments": "Published at NeurIPS 2020 Workshop on Artificial Intelligence for\n  Humanitarian Assistance and Disaster Response (AI+HADR 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  An important step for limiting the negative impact of natural disasters is\nrapid damage assessment after a disaster occurred. For instance, building\ndamage detection can be automated by applying computer vision techniques to\nsatellite imagery. Such models operate in a multi-domain setting: every\ndisaster is inherently different (new geolocation, unique circumstances), and\nmodels must be robust to a shift in distribution between disaster imagery\navailable for training and the images of the new event. Accordingly, estimating\nreal-world performance requires an out-of-domain (OOD) test set. However,\nbuilding damage detection models have so far been evaluated mostly in the\nsimpler yet unrealistic in-distribution (IID) test setting. Here we argue that\nfuture work should focus on the OOD regime instead. We assess OOD performance\nof two competitive damage detection models and find that existing\nstate-of-the-art models show a substantial generalization gap: their\nperformance drops when evaluated OOD on new disasters not used during training.\nMoreover, IID performance is not predictive of OOD performance, rendering\ncurrent benchmarks uninformative about real-world performance. Code and model\nweights are available at https://github.com/ecker-lab/robust-bdd.\n", "versions": [{"version": "v1", "created": "Fri, 20 Nov 2020 10:30:43 GMT"}], "update_date": "2020-11-23", "authors_parsed": [["Benson", "Vitus", ""], ["Ecker", "Alexander", ""]]}, {"id": "2011.10331", "submitter": "Xiang Fang", "authors": "Xiang Fang, Yuchong Hu, Pan Zhou, and Dapeng Oliver Wu", "title": "ANIMC: A Soft Framework for Auto-weighted Noisy and Incomplete\n  Multi-view Clustering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-view clustering has wide applications in many image processing\nscenarios. In these scenarios, original image data often contain missing\ninstances and noises, which is ignored by most multi-view clustering methods.\nHowever, missing instances may make these methods difficult to use directly and\nnoises will lead to unreliable clustering results. In this paper, we propose a\nnovel Auto-weighted Noisy and Incomplete Multi-view Clustering framework\n(ANIMC) via a soft auto-weighted strategy and a doubly soft regular regression\nmodel. Firstly, by designing adaptive semi-regularized nonnegative matrix\nfactorization (adaptive semi-RNMF), the soft auto-weighted strategy assigns a\nproper weight to each view and adds a soft boundary to balance the influence of\nnoises and incompleteness. Secondly, by proposing{\\theta}-norm, the doubly soft\nregularized regression model adjusts the sparsity of our model by choosing\ndifferent{\\theta}. Compared with existing methods, ANIMC has three unique\nadvantages: 1) it is a soft algorithm to adjust our framework in different\nscenarios, thereby improving its generalization ability; 2) it automatically\nlearns a proper weight for each view, thereby reducing the influence of noises;\n3) it performs doubly soft regularized regression that aligns the same\ninstances in different views, thereby decreasing the impact of missing\ninstances. Extensive experimental results demonstrate its superior advantages\nover other state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Fri, 20 Nov 2020 10:37:27 GMT"}, {"version": "v2", "created": "Wed, 28 Apr 2021 12:17:22 GMT"}], "update_date": "2021-04-29", "authors_parsed": [["Fang", "Xiang", ""], ["Hu", "Yuchong", ""], ["Zhou", "Pan", ""], ["Wu", "Dapeng Oliver", ""]]}, {"id": "2011.10336", "submitter": "Samuel Hurault", "authors": "Samuel Hurault, Coloma Ballester, Gloria Haro", "title": "Self-Supervised Small Soccer Player Detection and Tracking", "comments": "In Proceedings of the 3rd International Workshop on Multimedia\n  Content Analysis in Sports (MMSports '20)", "journal-ref": "Proceedings of the 3rd International Workshop on Multimedia\n  Content Analysis in Sports (2020) 9-18", "doi": "10.1145/3422844.3423054", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In a soccer game, the information provided by detecting and tracking brings\ncrucial clues to further analyze and understand some tactical aspects of the\ngame, including individual and team actions. State-of-the-art tracking\nalgorithms achieve impressive results in scenarios on which they have been\ntrained for, but they fail in challenging ones such as soccer games. This is\nfrequently due to the player small relative size and the similar appearance\namong players of the same team. Although a straightforward solution would be to\nretrain these models by using a more specific dataset, the lack of such\npublicly available annotated datasets entails searching for other effective\nsolutions. In this work, we propose a self-supervised pipeline which is able to\ndetect and track low-resolution soccer players under different recording\nconditions without any need of ground-truth data. Extensive quantitative and\nqualitative experimental results are presented evaluating its performance. We\nalso present a comparison to several state-of-the-art methods showing that both\nthe proposed detector and the proposed tracker achieve top-tier results, in\nparticular in the presence of small players.\n", "versions": [{"version": "v1", "created": "Fri, 20 Nov 2020 10:57:18 GMT"}], "update_date": "2020-11-23", "authors_parsed": [["Hurault", "Samuel", ""], ["Ballester", "Coloma", ""], ["Haro", "Gloria", ""]]}, {"id": "2011.10359", "submitter": "Benjamin Graham", "authors": "Benjamin Graham, David Novotny", "title": "RidgeSfM: Structure from Motion via Robust Pairwise Matching Under Depth\n  Uncertainty", "comments": "Presenting at 3DV 2020. Source code released at\n  https://github.com/facebookresearch/RidgeSfM", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of simultaneously estimating a dense depth map and\ncamera pose for a large set of images of an indoor scene. While classical SfM\npipelines rely on a two-step approach where cameras are first estimated using a\nbundle adjustment in order to ground the ensuing multi-view stereo stage, both\nour poses and dense reconstructions are a direct output of an altered bundle\nadjuster. To this end, we parametrize each depth map with a linear combination\nof a limited number of basis \"depth-planes\" predicted in a monocular fashion by\na deep net. Using a set of high-quality sparse keypoint matches, we optimize\nover the per-frame linear combinations of depth planes and camera poses to form\na geometrically consistent cloud of keypoints. Although our bundle adjustment\nonly considers sparse keypoints, the inferred linear coefficients of the basis\nplanes immediately give us dense depth maps. RidgeSfM is able to collectively\nalign hundreds of frames, which is its main advantage over recent memory-heavy\ndeep alternatives that can align at most 10 frames. Quantitative comparisons\nreveal performance superior to a state-of-the-art large-scale SfM pipeline.\n", "versions": [{"version": "v1", "created": "Fri, 20 Nov 2020 11:59:20 GMT"}], "update_date": "2020-11-23", "authors_parsed": [["Graham", "Benjamin", ""], ["Novotny", "David", ""]]}, {"id": "2011.10379", "submitter": "Julian Ost", "authors": "Julian Ost, Fahim Mannan, Nils Thuerey, Julian Knodt, Felix Heide", "title": "Neural Scene Graphs for Dynamic Scenes", "comments": "Updated Project Page http://light.princeton.edu/neural-scene-graphs", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recent implicit neural rendering methods have demonstrated that it is\npossible to learn accurate view synthesis for complex scenes by predicting\ntheir volumetric density and color supervised solely by a set of RGB images.\nHowever, existing methods are restricted to learning efficient representations\nof static scenes that encode all scene objects into a single neural network,\nand lack the ability to represent dynamic scenes and decompositions into\nindividual scene objects. In this work, we present the first neural rendering\nmethod that decomposes dynamic scenes into scene graphs. We propose a learned\nscene graph representation, which encodes object transformation and radiance,\nto efficiently render novel arrangements and views of the scene. To this end,\nwe learn implicitly encoded scenes, combined with a jointly learned latent\nrepresentation to describe objects with a single implicit function. We assess\nthe proposed method on synthetic and real automotive data, validating that our\napproach learns dynamic scenes -- only by observing a video of this scene --\nand allows for rendering novel photo-realistic views of novel scene\ncompositions with unseen sets of objects at unseen poses.\n", "versions": [{"version": "v1", "created": "Fri, 20 Nov 2020 12:37:10 GMT"}, {"version": "v2", "created": "Mon, 1 Mar 2021 11:20:52 GMT"}, {"version": "v3", "created": "Fri, 5 Mar 2021 16:21:16 GMT"}], "update_date": "2021-03-08", "authors_parsed": [["Ost", "Julian", ""], ["Mannan", "Fahim", ""], ["Thuerey", "Nils", ""], ["Knodt", "Julian", ""], ["Heide", "Felix", ""]]}, {"id": "2011.10381", "submitter": "Oh Kwanseok", "authors": "Kwanseok Oh, Jee Seok Yoon, Heung-Il Suk", "title": "Born Identity Network: Multi-way Counterfactual Map Generation to\n  Explain a Classifier's Decision", "comments": "17 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There exists an apparent negative correlation between performance and\ninterpretability of deep learning models. In an effort to reduce this negative\ncorrelation, we propose a Born Identity Network (BIN), which is a post-hoc\napproach for producing multi-way counterfactual maps. A counterfactual map\ntransforms an input sample to be conditioned and classified as a target label,\nwhich is similar to how humans process knowledge through counterfactual\nthinking. For example, a counterfactual map can localize hypothetical\nabnormalities from a normal brain image that may cause it to be diagnosed with\na disease. Specifically, our proposed BIN consists of two core components:\nCounterfactual Map Generator and Target Attribution Network. The Counterfactual\nMap Generator is a variation of conditional GAN which can synthesize a\ncounterfactual map conditioned on an arbitrary target label. The Target\nAttribution Network provides adequate assistance for generating synthesized\nmaps by conditioning a target label into the Counterfactual Map Generator. We\nhave validated our proposed BIN in qualitative and quantitative analysis on\nMNIST, 3D Shapes, and ADNI datasets, and showed the comprehensibility and\nfidelity of our method from various ablation studies.\n", "versions": [{"version": "v1", "created": "Fri, 20 Nov 2020 12:43:08 GMT"}, {"version": "v2", "created": "Tue, 24 Nov 2020 06:09:36 GMT"}, {"version": "v3", "created": "Wed, 6 Jan 2021 03:53:20 GMT"}, {"version": "v4", "created": "Thu, 8 Apr 2021 05:24:34 GMT"}], "update_date": "2021-04-09", "authors_parsed": [["Oh", "Kwanseok", ""], ["Yoon", "Jee Seok", ""], ["Suk", "Heung-Il", ""]]}, {"id": "2011.10407", "submitter": "Jia Peng", "authors": "Peng Jia, Qiang Liu, Yongyang Sun, Yitian Zheng, Wenbo Liu, Yifei Zhao", "title": "Smart obervation method with wide field small aperture telescopes for\n  real time transient detection", "comments": "To appear in Proc. of SPIE 2020, Paper Number (11449-80), Comments\n  are welcome", "journal-ref": null, "doi": null, "report-no": null, "categories": "astro-ph.IM astro-ph.GA astro-ph.SR cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Wide field small aperture telescopes (WFSATs) are commonly used for fast sky\nsurvey. Telescope arrays composed by several WFSATs are capable to scan sky\nseveral times per night. Huge amount of data would be obtained by them and\nthese data need to be processed immediately. In this paper, we propose ARGUS\n(Astronomical taRGets detection framework for Unified telescopes) for real-time\ntransit detection. The ARGUS uses a deep learning based astronomical detection\nalgorithm implemented in embedded devices in each WFSATs to detect astronomical\ntargets. The position and probability of a detection being an astronomical\ntargets will be sent to a trained ensemble learning algorithm to output\ninformation of celestial sources. After matching these sources with star\ncatalog, ARGUS will directly output type and positions of transient candidates.\nWe use simulated data to test the performance of ARGUS and find that ARGUS can\nincrease the performance of WFSATs in transient detection tasks robustly.\n", "versions": [{"version": "v1", "created": "Fri, 20 Nov 2020 13:48:32 GMT"}], "update_date": "2020-11-23", "authors_parsed": [["Jia", "Peng", ""], ["Liu", "Qiang", ""], ["Sun", "Yongyang", ""], ["Zheng", "Yitian", ""], ["Liu", "Wenbo", ""], ["Zhao", "Yifei", ""]]}, {"id": "2011.10432", "submitter": "George Pantazis", "authors": "George Pantazis, George Dimas and Dimitris K. Iakovidis", "title": "SalSum: Saliency-based Video Summarization using Generative Adversarial\n  Networks", "comments": "18 pages, 5 figures. Submitted to Multimedia Tools and Applications", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The huge amount of video data produced daily by camera-based systems, such as\nsurveilance, medical and telecommunication systems, emerges the need for\neffective video summarization (VS) methods. These methods should be capable of\ncreating an overview of the video content. In this paper, we propose a novel VS\nmethod based on a Generative Adversarial Network (GAN) model pre-trained with\nhuman eye fixations. The main contribution of the proposed method is that it\ncan provide perceptually compatible video summaries by combining both perceived\ncolor and spatiotemporal visual attention cues in a unsupervised scheme.\nSeveral fusion approaches are considered for robustness under uncertainty, and\npersonalization. The proposed method is evaluated in comparison to\nstate-of-the-art VS approaches on the benchmark dataset VSUMM. The experimental\nresults conclude that SalSum outperforms the state-of-the-art approaches by\nproviding the highest f-measure score on the VSUMM benchmark.\n", "versions": [{"version": "v1", "created": "Fri, 20 Nov 2020 14:53:08 GMT"}], "update_date": "2020-11-24", "authors_parsed": [["Pantazis", "George", ""], ["Dimas", "George", ""], ["Iakovidis", "Dimitris K.", ""]]}, {"id": "2011.10433", "submitter": "Veronika Cheplygina", "authors": "Veronika Cheplygina and Adria Perez-Rovira and Wieying Kuo and Harm A.\n  W. M. Tiddens and Marleen de Bruijne", "title": "Crowdsourcing Airway Annotations in Chest Computed Tomography Images", "comments": null, "journal-ref": null, "doi": "10.1371/journal.pone.0249580", "report-no": null, "categories": "cs.CV cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Measuring airways in chest computed tomography (CT) scans is important for\ncharacterizing diseases such as cystic fibrosis, yet very time-consuming to\nperform manually. Machine learning algorithms offer an alternative, but need\nlarge sets of annotated scans for good performance. We investigate whether\ncrowdsourcing can be used to gather airway annotations. We generate image\nslices at known locations of airways in 24 subjects and request the crowd\nworkers to outline the airway lumen and airway wall. After combining multiple\ncrowd workers, we compare the measurements to those made by the experts in the\noriginal scans. Similar to our preliminary study, a large portion of the\nannotations were excluded, possibly due to workers misunderstanding the\ninstructions. After excluding such annotations, moderate to strong correlations\nwith the expert can be observed, although these correlations are slightly lower\nthan inter-expert correlations. Furthermore, the results across subjects in\nthis study are quite variable. Although the crowd has potential in annotating\nairways, further development is needed for it to be robust enough for gathering\nannotations in practice. For reproducibility, data and code are available\nonline: \\url{http://github.com/adriapr/crowdairway.git}.\n", "versions": [{"version": "v1", "created": "Fri, 20 Nov 2020 14:54:32 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Cheplygina", "Veronika", ""], ["Perez-Rovira", "Adria", ""], ["Kuo", "Wieying", ""], ["Tiddens", "Harm A. W. M.", ""], ["de Bruijne", "Marleen", ""]]}, {"id": "2011.10452", "submitter": "Zachary Ravichandran", "authors": "Zachary Ravichandran, J. Daniel Griffith, Benjamin Smith, and Costas\n  Frost", "title": "Bridging Scene Understanding and Task Execution with Flexible Simulation\n  Environments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Significant progress has been made in scene understanding which seeks to\nbuild 3D, metric and object-oriented representations of the world.\nConcurrently, reinforcement learning has made impressive strides largely\nenabled by advances in simulation. Comparatively, there has been less focus in\nsimulation for perception algorithms. Simulation is becoming increasingly vital\nas sophisticated perception approaches such as metric-semantic mapping or 3D\ndynamic scene graph generation require precise 3D, 2D, and inertial information\nin an interactive environment. To that end, we present TESSE (Task Execution\nwith Semantic Segmentation Environments), an open source simulator for\ndeveloping scene understanding and task execution algorithms. TESSE has been\nused to develop state-of-the-art solutions for metric-semantic mapping and 3D\ndynamic scene graph generation. Additionally, TESSE served as the platform for\nthe GOSEEK Challenge at the International Conference of Robotics and Automation\n(ICRA) 2020, an object search competition with an emphasis on reinforcement\nlearning. Code for TESSE is available at https://github.com/MIT-TESSE.\n", "versions": [{"version": "v1", "created": "Fri, 20 Nov 2020 15:29:23 GMT"}], "update_date": "2020-11-23", "authors_parsed": [["Ravichandran", "Zachary", ""], ["Griffith", "J. Daniel", ""], ["Smith", "Benjamin", ""], ["Frost", "Costas", ""]]}, {"id": "2011.10465", "submitter": "Chen Zuge", "authors": "Wu Kehe, Chen Zuge, Zhang Xiaoliang, Li Wei", "title": "Improvement of Classification in One-Stage Detector", "comments": "17 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  RetinaNet proposed Focal Loss for classification task and improved one-stage\ndetectors greatly. However, there is still a gap between it and two-stage\ndetectors. We analyze the prediction of RetinaNet and find that the\nmisalignment of classification and localization is the main factor. Most of\npredicted boxes, whose IoU with ground-truth boxes are greater than 0.5, while\ntheir classification scores are lower than 0.5, which shows that the\nclassification task still needs to be optimized. In this paper we proposed an\nobject confidence task for this problem, and it shares features with\nclassification task. This task uses IoUs between samples and ground-truth boxes\nas targets, and it only uses losses of positive samples in training, which can\nincrease loss weight of positive samples in classification task training. Also\nthe joint of classification score and object confidence will be used to guide\nNMS. Our method can not only improve classification task, but also ease\nmisalignment of classification and localization. To evaluate the effectiveness\nof this method, we show our experiments on MS COCO 2017 dataset. Without\nwhistles and bells, our method can improve AP by 0.7% and 1.0% on COCO\nvalidation dataset with ResNet50 and ResNet101 respectively at same training\nconfigs, and it can achieve 38.4% AP with two times training time. Code is at:\nhttp://github.com/chenzuge1/RetinaNet-Conf.git.\n", "versions": [{"version": "v1", "created": "Fri, 20 Nov 2020 15:56:44 GMT"}], "update_date": "2020-11-23", "authors_parsed": [["Kehe", "Wu", ""], ["Zuge", "Chen", ""], ["Xiaoliang", "Zhang", ""], ["Wei", "Li", ""]]}, {"id": "2011.10471", "submitter": "Yorai Shaoul", "authors": "Yorai Shaoul, Katherine Liu, Kyel Ok and Nicholas Roy", "title": "Online Descriptor Enhancement via Self-Labelling Triplets for Visual\n  Data Association", "comments": "Under review for the 2021 IEEE/RSJ International Conference on\n  Intelligent Robots and Systems (IROS 2021). This work has been submitted to\n  the IEEE for possible publication. Copyright may be transferred without\n  notice, after which this version may no longer be accessible", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Object-level data association is central to robotic applications such as\ntracking-by-detection and object-level simultaneous localization and mapping.\nWhile current learned visual data association methods outperform hand-crafted\nalgorithms, many rely on large collections of domain-specific training examples\nthat can be difficult to obtain without prior knowledge. Additionally, such\nmethods often remain fixed during inference-time and do not harness observed\ninformation to better their performance. We propose a self-supervised method\nfor incrementally refining visual descriptors to improve performance in the\ntask of object-level visual data association. Our method optimizes deep\ndescriptor generators online, by continuously training a widely available image\nclassification network pre-trained with domain-independent data. We show that\nearlier layers in the network outperform later-stage layers for the data\nassociation task while also allowing for a 94% reduction in the number of\nparameters, enabling the online optimization. We show that self-labelling\nchallenging triplets--choosing positive examples separated by large temporal\ndistances and negative examples close in the descriptor space--improves the\nquality of the learned descriptors for the multi-object tracking task. Finally,\nwe demonstrate that our approach surpasses other visual data-association\nmethods applied to a tracking-by-detection task, and show that it provides\nbetter performance-gains when compared to other methods that attempt to adapt\nto observed information.\n", "versions": [{"version": "v1", "created": "Fri, 6 Nov 2020 17:42:04 GMT"}, {"version": "v2", "created": "Thu, 3 Jun 2021 21:12:07 GMT"}], "update_date": "2021-06-07", "authors_parsed": [["Shaoul", "Yorai", ""], ["Liu", "Katherine", ""], ["Ok", "Kyel", ""], ["Roy", "Nicholas", ""]]}, {"id": "2011.10472", "submitter": "Sharadha Srinivasan", "authors": "Sharadha Srinivasan, Madan Musuvathi", "title": "GenderRobustness: Robustness of Gender Detection in Facial Recognition\n  Systems with variation in Image Properties", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  In recent times, there have been increasing accusations on artificial\nintelligence systems and algorithms of computer vision of possessing implicit\nbiases. Even though these conversations are more prevalent now and systems are\nimproving by performing extensive testing and broadening their horizon, biases\nstill do exist. One such class of systems where bias is said to exist is facial\nrecognition systems, where bias has been observed on the basis of gender,\nethnicity, skin tone and other facial attributes. This is even more disturbing,\ngiven the fact that these systems are used in practically every sector of the\nindustries today. From as critical as criminal identification to as simple as\ngetting your attendance registered, these systems have gained a huge market,\nespecially in recent years. That in itself is a good enough reason for\ndevelopers of these systems to ensure that the bias is kept to a bare minimum\nor ideally non-existent, to avoid major issues like favoring a particular\ngender, race, or class of people or rather making a class of people susceptible\nto false accusations due to inability of these systems to correctly recognize\nthose people.\n", "versions": [{"version": "v1", "created": "Wed, 18 Nov 2020 18:13:23 GMT"}, {"version": "v2", "created": "Thu, 26 Nov 2020 22:18:15 GMT"}], "update_date": "2020-11-30", "authors_parsed": [["Srinivasan", "Sharadha", ""], ["Musuvathi", "Madan", ""]]}, {"id": "2011.10475", "submitter": "Jong Chul Ye", "authors": "Eunju Cha, Chanseok Lee, Mooseok Jang, and Jong Chul Ye", "title": "DeepPhaseCut: Deep Relaxation in Phase for Unsupervised Fourier Phase\n  Retrieval", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fourier phase retrieval is a classical problem of restoring a signal only\nfrom the measured magnitude of its Fourier transform. Although Fienup-type\nalgorithms, which use prior knowledge in both spatial and Fourier domains, have\nbeen widely used in practice, they can often stall in local minima. Modern\nmethods such as PhaseLift and PhaseCut may offer performance guarantees with\nthe help of convex relaxation. However, these algorithms are usually\ncomputationally intensive for practical use. To address this problem, we\npropose a novel, unsupervised, feed-forward neural network for Fourier phase\nretrieval which enables immediate high quality reconstruction. Unlike the\nexisting deep learning approaches that use a neural network as a regularization\nterm or an end-to-end blackbox model for supervised training, our algorithm is\na feed-forward neural network implementation of PhaseCut algorithm in an\nunsupervised learning framework. Specifically, our network is composed of two\ngenerators: one for the phase estimation using PhaseCut loss, followed by\nanother generator for image reconstruction, all of which are trained\nsimultaneously using a cycleGAN framework without matched data. The link to the\nclassical Fienup-type algorithms and the recent symmetry-breaking learning\napproach is also revealed. Extensive experiments demonstrate that the proposed\nmethod outperforms all existing approaches in Fourier phase retrieval problems.\n", "versions": [{"version": "v1", "created": "Fri, 20 Nov 2020 16:10:08 GMT"}], "update_date": "2020-11-23", "authors_parsed": [["Cha", "Eunju", ""], ["Lee", "Chanseok", ""], ["Jang", "Mooseok", ""], ["Ye", "Jong Chul", ""]]}, {"id": "2011.10486", "submitter": "\\\"Ozg\\\"un \\c{C}i\\c{c}ek", "authors": "\\\"Ozg\\\"un \\c{C}i\\c{c}ek, Yassine Marrakchi, Enoch Boasiako Antwi,\n  Barbara Di Ventura and Thomas Brox", "title": "Recovering the Imperfect: Cell Segmentation in the Presence of\n  Dynamically Localized Proteins", "comments": "Accepted at MICCAI Workshop on Medical Image Learning with Less\n  Labels and Imperfect Data, 2020", "journal-ref": null, "doi": "10.1007/978-3-030-61166-8_9", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deploying off-the-shelf segmentation networks on biomedical data has become\ncommon practice, yet if structures of interest in an image sequence are visible\nonly temporarily, existing frame-by-frame methods fail. In this paper, we\nprovide a solution to segmentation of imperfect data through time based on\ntemporal propagation and uncertainty estimation. We integrate uncertainty\nestimation into Mask R-CNN network and propagate motion-corrected segmentation\nmasks from frames with low uncertainty to those frames with high uncertainty to\nhandle temporary loss of signal for segmentation. We demonstrate the value of\nthis approach over frame-by-frame segmentation and regular temporal propagation\non data from human embryonic kidney (HEK293T) cells transiently transfected\nwith a fluorescent protein that moves in and out of the nucleus over time. The\nmethod presented here will empower microscopic experiments aimed at\nunderstanding molecular and cellular function.\n", "versions": [{"version": "v1", "created": "Fri, 20 Nov 2020 16:30:55 GMT"}], "update_date": "2020-11-23", "authors_parsed": [["\u00c7i\u00e7ek", "\u00d6zg\u00fcn", ""], ["Marrakchi", "Yassine", ""], ["Antwi", "Enoch Boasiako", ""], ["Di Ventura", "Barbara", ""], ["Brox", "Thomas", ""]]}, {"id": "2011.10505", "submitter": "Leonid Mill", "authors": "Leonid Mill, David Wolff, Nele Gerrits, Patrick Philipp, Lasse Kling,\n  Florian Vollnhals, Andrew Ignatenko, Christian Jaremenko, Yixing Huang,\n  Olivier De Castro, Jean-Nicolas Audinot, Inge Nelissen, Tom Wirtz, Andreas\n  Maier, Silke Christiansen", "title": "Synthetic Image Rendering Solves Annotation Problem in Deep Learning\n  Nanoparticle Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cond-mat.mtrl-sci cs.CV eess.IV physics.app-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nanoparticles occur in various environments as a consequence of man-made\nprocesses, which raises concerns about their impact on the environment and\nhuman health. To allow for proper risk assessment, a precise and statistically\nrelevant analysis of particle characteristics (such as e.g. size, shape and\ncomposition) is required that would greatly benefit from automated image\nanalysis procedures. While deep learning shows impressive results in object\ndetection tasks, its applicability is limited by the amount of representative,\nexperimentally collected and manually annotated training data. Here, we present\nan elegant, flexible and versatile method to bypass this costly and tedious\ndata acquisition process. We show that using a rendering software allows to\ngenerate realistic, synthetic training data to train a state-of-the art deep\nneural network. Using this approach, we derive a segmentation accuracy that is\ncomparable to man-made annotations for toxicologically relevant metal-oxide\nnanoparticle ensembles which we chose as examples. Our study paves the way\ntowards the use of deep learning for automated, high-throughput particle\ndetection in a variety of imaging techniques such as microscopies and\nspectroscopies, for a wide variety of studies and applications, including the\ndetection of plastic micro- and nanoparticles.\n", "versions": [{"version": "v1", "created": "Fri, 20 Nov 2020 17:05:36 GMT"}], "update_date": "2020-11-23", "authors_parsed": [["Mill", "Leonid", ""], ["Wolff", "David", ""], ["Gerrits", "Nele", ""], ["Philipp", "Patrick", ""], ["Kling", "Lasse", ""], ["Vollnhals", "Florian", ""], ["Ignatenko", "Andrew", ""], ["Jaremenko", "Christian", ""], ["Huang", "Yixing", ""], ["De Castro", "Olivier", ""], ["Audinot", "Jean-Nicolas", ""], ["Nelissen", "Inge", ""], ["Wirtz", "Tom", ""], ["Maier", "Andreas", ""], ["Christiansen", "Silke", ""]]}, {"id": "2011.10510", "submitter": "M Quamer Nasim", "authors": "M Quamer Nasim, Tannistha Maiti, Ayush Srivastava, Tarry Singh, and\n  Jie Mei", "title": "Seismic Facies Analysis: A Deep Domain Adaptation Approach", "comments": "18 pages, 10 figures, 5 tables, and supplementary material included\n  in the end of the paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.geo-ph cs.AI cs.CV cs.LG eess.IV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Deep neural networks (DNNs) can learn accurately from large quantities of\nlabeled input data, but DNNs sometimes fail to generalize to test data sampled\nfrom different input distributions. Unsupervised Deep Domain Adaptation (DDA)\nproves useful when no input labels are available, and distribution shifts are\nobserved in the target domain (TD). Experiments are performed on seismic images\nof the F3 block 3D dataset from offshore Netherlands (source domain; SD) and\nPenobscot 3D survey data from Canada (target domain; TD). Three geological\nclasses from SD and TD that have similar reflection patterns are considered. In\nthe present study, an improved deep neural network architecture named\nEarthAdaptNet (EAN) is proposed to semantically segment the seismic images. We\nspecifically use a transposed residual unit to replace the traditional dilated\nconvolution in the decoder block. The EAN achieved a pixel-level accuracy >84%\nand an accuracy of ~70% for the minority classes, showing improved performance\ncompared to existing architectures. In addition, we introduced the CORAL\n(Correlation Alignment) method to the EAN to create an unsupervised deep domain\nadaptation network (EAN-DDA) for the classification of seismic reflections\nfromF3 and Penobscot. Maximum class accuracy achieved was ~99% for class 2 of\nPenobscot with >50% overall accuracy. Taken together, EAN-DDA has the potential\nto classify target domain seismic facies classes with high accuracy.\n", "versions": [{"version": "v1", "created": "Fri, 20 Nov 2020 17:09:06 GMT"}, {"version": "v2", "created": "Fri, 25 Jun 2021 18:27:06 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Nasim", "M Quamer", ""], ["Maiti", "Tannistha", ""], ["Srivastava", "Ayush", ""], ["Singh", "Tarry", ""], ["Mei", "Jie", ""]]}, {"id": "2011.10512", "submitter": "David Forsyth", "authors": "D. A. Forsyth and Jason J. Rock", "title": "Intrinsic Image Decomposition using Paradigms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Intrinsic image decomposition is the classical task of mapping image to\nalbedo. The WHDR dataset allows methods to be evaluated by comparing\npredictions to human judgements (\"lighter\", \"same as\", \"darker\"). The best\nmodern intrinsic image methods learn a map from image to albedo using rendered\nmodels and human judgements. This is convenient for practical methods, but\ncannot explain how a visual agent without geometric, surface and illumination\nmodels and a renderer could learn to recover intrinsic images.\n  This paper describes a method that learns intrinsic image decomposition\nwithout seeing WHDR annotations, rendered data, or ground truth data. The\nmethod relies on paradigms - fake albedos and fake shading fields - together\nwith a novel smoothing procedure that ensures good behavior at short scales on\nreal images. Long scale error is controlled by averaging. Our method achieves\nWHDR scores competitive with those of strong recent methods allowed to see\ntraining WHDR annotations, rendered data, and ground truth data. Because our\nmethod is unsupervised, we can compute estimates of the test/train variance of\nWHDR scores; these are quite large, and it is unsafe to rely small differences\nin reported WHDR.\n", "versions": [{"version": "v1", "created": "Fri, 20 Nov 2020 17:10:12 GMT"}], "update_date": "2020-11-23", "authors_parsed": [["Forsyth", "D. A.", ""], ["Rock", "Jason J.", ""]]}, {"id": "2011.10566", "submitter": "Xinlei Chen", "authors": "Xinlei Chen and Kaiming He", "title": "Exploring Simple Siamese Representation Learning", "comments": "Technical report, 10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Siamese networks have become a common structure in various recent models for\nunsupervised visual representation learning. These models maximize the\nsimilarity between two augmentations of one image, subject to certain\nconditions for avoiding collapsing solutions. In this paper, we report\nsurprising empirical results that simple Siamese networks can learn meaningful\nrepresentations even using none of the following: (i) negative sample pairs,\n(ii) large batches, (iii) momentum encoders. Our experiments show that\ncollapsing solutions do exist for the loss and structure, but a stop-gradient\noperation plays an essential role in preventing collapsing. We provide a\nhypothesis on the implication of stop-gradient, and further show\nproof-of-concept experiments verifying it. Our \"SimSiam\" method achieves\ncompetitive results on ImageNet and downstream tasks. We hope this simple\nbaseline will motivate people to rethink the roles of Siamese architectures for\nunsupervised representation learning. Code will be made available.\n", "versions": [{"version": "v1", "created": "Fri, 20 Nov 2020 18:59:33 GMT"}], "update_date": "2020-11-23", "authors_parsed": [["Chen", "Xinlei", ""], ["He", "Kaiming", ""]]}, {"id": "2011.10600", "submitter": "Yasser Dahou", "authors": "Yasser Dahou, Marouane Tliba, Kevin McGuinness, Noel O'Connor", "title": "ATSal: An Attention Based Architecture for Saliency Prediction in 360\n  Videos", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The spherical domain representation of 360 video/image presents many\nchallenges related to the storage, processing, transmission and rendering of\nomnidirectional videos (ODV). Models of human visual attention can be used so\nthat only a single viewport is rendered at a time, which is important when\ndeveloping systems that allow users to explore ODV with head mounted displays\n(HMD). Accordingly, researchers have proposed various saliency models for 360\nvideo/images. This paper proposes ATSal, a novel attention based (head-eye)\nsaliency model for 360\\degree videos. The attention mechanism explicitly\nencodes global static visual attention allowing expert models to focus on\nlearning the saliency on local patches throughout consecutive frames. We\ncompare the proposed approach to other state-of-the-art saliency models on two\ndatasets: Salient360! and VR-EyeTracking. Experimental results on over 80 ODV\nvideos (75K+ frames) show that the proposed method outperforms the existing\nstate-of-the-art.\n", "versions": [{"version": "v1", "created": "Fri, 20 Nov 2020 19:19:48 GMT"}], "update_date": "2020-11-24", "authors_parsed": [["Dahou", "Yasser", ""], ["Tliba", "Marouane", ""], ["McGuinness", "Kevin", ""], ["O'Connor", "Noel", ""]]}, {"id": "2011.10608", "submitter": "Rameswar Panda", "authors": "Ulrich Finkler, Michele Merler, Rameswar Panda, Mayoore S. Jaiswal,\n  Hui Wu, Kandan Ramakrishnan, Chun-Fu Chen, Minsik Cho, David Kung, Rogerio\n  Feris, and Bishwaranjan Bhattacharjee", "title": "Large Scale Neural Architecture Search with Polyharmonic Splines", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Neural Architecture Search (NAS) is a powerful tool to automatically design\ndeep neural networks for many tasks, including image classification. Due to the\nsignificant computational burden of the search phase, most NAS methods have\nfocused so far on small, balanced datasets. All attempts at conducting NAS at\nlarge scale have employed small proxy sets, and then transferred the learned\narchitectures to larger datasets by replicating or stacking the searched cells.\nWe propose a NAS method based on polyharmonic splines that can perform search\ndirectly on large scale, imbalanced target datasets. We demonstrate the\neffectiveness of our method on the ImageNet22K benchmark[16], which contains 14\nmillion images distributed in a highly imbalanced manner over 21,841\ncategories. By exploring the search space of the ResNet [23] and Big-Little Net\nResNext [11] architectures directly on ImageNet22K, our polyharmonic splines\nNAS method designed a model which achieved a top-1 accuracy of 40.03% on\nImageNet22K, an absolute improvement of 3.13% over the state of the art with\nsimilar global batch size [15].\n", "versions": [{"version": "v1", "created": "Fri, 20 Nov 2020 19:50:35 GMT"}], "update_date": "2020-11-24", "authors_parsed": [["Finkler", "Ulrich", ""], ["Merler", "Michele", ""], ["Panda", "Rameswar", ""], ["Jaiswal", "Mayoore S.", ""], ["Wu", "Hui", ""], ["Ramakrishnan", "Kandan", ""], ["Chen", "Chun-Fu", ""], ["Cho", "Minsik", ""], ["Kung", "David", ""], ["Feris", "Rogerio", ""], ["Bhattacharjee", "Bishwaranjan", ""]]}, {"id": "2011.10625", "submitter": "Zhentian Qian", "authors": "Zhentian Qian, Kartik Patath, Jie Fu, Jing Xiao", "title": "Semantic SLAM with Autonomous Object-Level Data Association", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is often desirable to capture and map semantic information of an\nenvironment during simultaneous localization and mapping (SLAM). Such semantic\ninformation can enable a robot to better distinguish places with similar\nlow-level geometric and visual features and perform high-level tasks that use\nsemantic information about objects to be manipulated and environments to be\nnavigated. While semantic SLAM has gained increasing attention, there is little\nresearch on semanticlevel data association based on semantic objects, i.e.,\nobject-level data association. In this paper, we propose a novel object-level\ndata association algorithm based on bag of words algorithm, formulated as a\nmaximum weighted bipartite matching problem. With object-level data association\nsolved, we develop a quadratic-programming-based semantic object initialization\nscheme using dual quadric and introduce additional constraints to improve the\nsuccess rate of object initialization. The integrated semantic-level SLAM\nsystem can achieve high-accuracy object-level data association and real-time\nsemantic mapping as demonstrated in the experiments. The online semantic map\nbuilding and semantic-level localization capabilities facilitate semantic-level\nmapping and task planning in a priori unknown environment.\n", "versions": [{"version": "v1", "created": "Fri, 20 Nov 2020 20:33:39 GMT"}], "update_date": "2020-11-24", "authors_parsed": [["Qian", "Zhentian", ""], ["Patath", "Kartik", ""], ["Fu", "Jie", ""], ["Xiao", "Jing", ""]]}, {"id": "2011.10650", "submitter": "Rewon Child", "authors": "Rewon Child", "title": "Very Deep VAEs Generalize Autoregressive Models and Can Outperform Them\n  on Images", "comments": "17 pages, 14 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a hierarchical VAE that, for the first time, generates samples\nquickly while outperforming the PixelCNN in log-likelihood on all natural image\nbenchmarks. We begin by observing that, in theory, VAEs can actually represent\nautoregressive models, as well as faster, better models if they exist, when\nmade sufficiently deep. Despite this, autoregressive models have historically\noutperformed VAEs in log-likelihood. We test if insufficient depth explains why\nby scaling a VAE to greater stochastic depth than previously explored and\nevaluating it CIFAR-10, ImageNet, and FFHQ. In comparison to the PixelCNN,\nthese very deep VAEs achieve higher likelihoods, use fewer parameters, generate\nsamples thousands of times faster, and are more easily applied to\nhigh-resolution images. Qualitative studies suggest this is because the VAE\nlearns efficient hierarchical visual representations. We release our source\ncode and models at https://github.com/openai/vdvae.\n", "versions": [{"version": "v1", "created": "Fri, 20 Nov 2020 21:35:31 GMT"}, {"version": "v2", "created": "Tue, 16 Mar 2021 18:33:19 GMT"}], "update_date": "2021-03-18", "authors_parsed": [["Child", "Rewon", ""]]}, {"id": "2011.10654", "submitter": "Soumick Chatterjee", "authors": "Dhanunjaya Mitta, Soumick Chatterjee, Oliver Speck and Andreas\n  N\\\"urnberger", "title": "Upgraded W-Net with Attention Gates and its Application in Unsupervised\n  3D Liver Segmentation", "comments": null, "journal-ref": "Proceedings of the 10th International Conference on Pattern\n  Recognition Applications and Methods 2021 - Volume 1", "doi": "10.5220/0010221504880494", "report-no": "ICPRAM, ISBN 978-989-758-486-2, pages 488-494", "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Segmentation of biomedical images can assist radiologists to make a better\ndiagnosis and take decisions faster by helping in the detection of\nabnormalities, such as tumors. Manual or semi-automated segmentation, however,\ncan be a time-consuming task. Most deep learning based automated segmentation\nmethods are supervised and rely on manually segmented ground-truth. A possible\nsolution for the problem would be an unsupervised deep learning based approach\nfor automated segmentation, which this research work tries to address. We use a\nW-Net architecture and modified it, such that it can be applied to 3D volumes.\nIn addition, to suppress noise in the segmentation we added attention gates to\nthe skip connections. The loss for the segmentation output was calculated using\nsoft N-Cuts and for the reconstruction output using SSIM. Conditional Random\nFields were used as a post-processing step to fine-tune the results. The\nproposed method has shown promising results, with a dice coefficient of 0.88\nfor the liver segmentation compared against manual segmentation.\n", "versions": [{"version": "v1", "created": "Fri, 20 Nov 2020 21:45:28 GMT"}], "update_date": "2021-02-11", "authors_parsed": [["Mitta", "Dhanunjaya", ""], ["Chatterjee", "Soumick", ""], ["Speck", "Oliver", ""], ["N\u00fcrnberger", "Andreas", ""]]}, {"id": "2011.10655", "submitter": "Renu Sharma", "authors": "Renu Sharma and Arun Ross", "title": "Viability of Optical Coherence Tomography for Iris Presentation Attack\n  Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In this paper, we propose the use of Optical Coherence Tomography (OCT)\nimaging for the problem of iris presentation attack (PA) detection. We assess\nits viability by comparing its performance with respect to traditional iris\nimaging modalities, viz., near-infrared (NIR) and visible spectrum. OCT imaging\nprovides a cross-sectional view of an eye, whereas traditional imaging provides\n2D iris textural information. PA detection is performed using three\nstate-of-the-art deep architectures (VGG19, ResNet50 and DenseNet121) to\ndifferentiate between bonafide and PA samples for each of the three imaging\nmodalities. Experiments are performed on a dataset of 2,169 bonafide, 177 Van\nDyke eyes and 360 cosmetic contact images acquired using all three imaging\nmodalities under intra-attack (known PAs) and cross-attack (unknown PAs)\nscenarios. We observe promising results demonstrating OCT as a viable solution\nfor iris presentation attack detection.\n", "versions": [{"version": "v1", "created": "Thu, 22 Oct 2020 18:00:51 GMT"}], "update_date": "2020-11-24", "authors_parsed": [["Sharma", "Renu", ""], ["Ross", "Arun", ""]]}, {"id": "2011.10670", "submitter": "Junwei Liang", "authors": "Junwei Liang", "title": "From Recognition to Prediction: Analysis of Human Action and Trajectory\n  Prediction in Video", "comments": "Ph.D. Thesis. Version 2: Defense. See here:\n  https://junweiliang.github.io/thesis/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  With the advancement in computer vision deep learning, systems now are able\nto analyze an unprecedented amount of rich visual information from videos to\nenable applications such as autonomous driving, socially-aware robot assistant\nand public safety monitoring. Deciphering human behaviors to predict their\nfuture paths/trajectories and what they would do from videos is important in\nthese applications. However, human trajectory prediction still remains a\nchallenging task, as scene semantics and human intent are difficult to model.\nMany systems do not provide high-level semantic attributes to reason about\npedestrian future. This design hinders prediction performance in video data\nfrom diverse domains and unseen scenarios. To enable optimal future human\nbehavioral forecasting, it is crucial for the system to be able to detect and\nanalyze human activities as well as scene semantics, passing informative\nfeatures to the subsequent prediction module for context understanding.\n", "versions": [{"version": "v1", "created": "Fri, 20 Nov 2020 22:23:34 GMT"}, {"version": "v2", "created": "Thu, 1 Jul 2021 16:22:59 GMT"}, {"version": "v3", "created": "Fri, 16 Jul 2021 13:45:43 GMT"}], "update_date": "2021-07-19", "authors_parsed": [["Liang", "Junwei", ""]]}, {"id": "2011.10671", "submitter": "Di Feng", "authors": "Di Feng, Ali Harakeh, Steven Waslander, Klaus Dietmayer", "title": "A Review and Comparative Study on Probabilistic Object Detection in\n  Autonomous Driving", "comments": "Accepted in the IEEE Transactions on Intelligent Transportation\n  Systems", "journal-ref": null, "doi": "10.1109/TITS.2021.3096854", "report-no": null, "categories": "cs.CV cs.RO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Capturing uncertainty in object detection is indispensable for safe\nautonomous driving. In recent years, deep learning has become the de-facto\napproach for object detection, and many probabilistic object detectors have\nbeen proposed. However, there is no summary on uncertainty estimation in deep\nobject detection, and existing methods are not only built with different\nnetwork architectures and uncertainty estimation methods, but also evaluated on\ndifferent datasets with a wide range of evaluation metrics. As a result, a\ncomparison among methods remains challenging, as does the selection of a model\nthat best suits a particular application. This paper aims to alleviate this\nproblem by providing a review and comparative study on existing probabilistic\nobject detection methods for autonomous driving applications. First, we provide\nan overview of generic uncertainty estimation in deep learning, and then\nsystematically survey existing methods and evaluation metrics for probabilistic\nobject detection. Next, we present a strict comparative study for probabilistic\nobject detection based on an image detector and three public autonomous driving\ndatasets. Finally, we present a discussion of the remaining challenges and\nfuture works. Code has been made available at\nhttps://github.com/asharakeh/pod_compare.git\n", "versions": [{"version": "v1", "created": "Fri, 20 Nov 2020 22:30:36 GMT"}, {"version": "v2", "created": "Sun, 11 Jul 2021 07:04:10 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Feng", "Di", ""], ["Harakeh", "Ali", ""], ["Waslander", "Steven", ""], ["Dietmayer", "Klaus", ""]]}, {"id": "2011.10675", "submitter": "Cristina Vasconcelos", "authors": "Cristina Vasconcelos, Hugo Larochelle, Vincent Dumoulin, Nicolas Le\n  Roux, Ross Goroshin", "title": "An Effective Anti-Aliasing Approach for Residual Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image pre-processing in the frequency domain has traditionally played a vital\nrole in computer vision and was even part of the standard pipeline in the early\ndays of deep learning. However, with the advent of large datasets, many\npractitioners concluded that this was unnecessary due to the belief that these\npriors can be learned from the data itself. Frequency aliasing is a phenomenon\nthat may occur when sub-sampling any signal, such as an image or feature map,\ncausing distortion in the sub-sampled output. We show that we can mitigate this\neffect by placing non-trainable blur filters and using smooth activation\nfunctions at key locations, particularly where networks lack the capacity to\nlearn them. These simple architectural changes lead to substantial improvements\nin out-of-distribution generalization on both image classification under\nnatural corruptions on ImageNet-C [10] and few-shot learning on Meta-Dataset\n[17], without introducing additional trainable parameters and using the default\nhyper-parameters of open source codebases.\n", "versions": [{"version": "v1", "created": "Fri, 20 Nov 2020 22:55:57 GMT"}], "update_date": "2020-11-24", "authors_parsed": [["Vasconcelos", "Cristina", ""], ["Larochelle", "Hugo", ""], ["Dumoulin", "Vincent", ""], ["Roux", "Nicolas Le", ""], ["Goroshin", "Ross", ""]]}, {"id": "2011.10678", "submitter": "Alireza Zareian", "authors": "Alireza Zareian, Kevin Dela Rosa, Derek Hao Hu, Shih-Fu Chang", "title": "Open-Vocabulary Object Detection Using Captions", "comments": "To be presented at CVPR 2021 (oral paper)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Despite the remarkable accuracy of deep neural networks in object detection,\nthey are costly to train and scale due to supervision requirements.\nParticularly, learning more object categories typically requires proportionally\nmore bounding box annotations. Weakly supervised and zero-shot learning\ntechniques have been explored to scale object detectors to more categories with\nless supervision, but they have not been as successful and widely adopted as\nsupervised models. In this paper, we put forth a novel formulation of the\nobject detection problem, namely open-vocabulary object detection, which is\nmore general, more practical, and more effective than weakly supervised and\nzero-shot approaches. We propose a new method to train object detectors using\nbounding box annotations for a limited set of object categories, as well as\nimage-caption pairs that cover a larger variety of objects at a significantly\nlower cost. We show that the proposed method can detect and localize objects\nfor which no bounding box annotation is provided during training, at a\nsignificantly higher accuracy than zero-shot approaches. Meanwhile, objects\nwith bounding box annotation can be detected almost as accurately as supervised\nmethods, which is significantly better than weakly supervised baselines.\nAccordingly, we establish a new state of the art for scalable object detection.\n", "versions": [{"version": "v1", "created": "Fri, 20 Nov 2020 23:05:46 GMT"}, {"version": "v2", "created": "Sun, 14 Mar 2021 18:45:04 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Zareian", "Alireza", ""], ["Rosa", "Kevin Dela", ""], ["Hu", "Derek Hao", ""], ["Chang", "Shih-Fu", ""]]}, {"id": "2011.10680", "submitter": "Amir Gholami", "authors": "Zhewei Yao, Zhen Dong, Zhangcheng Zheng, Amir Gholami, Jiali Yu, Eric\n  Tan, Leyuan Wang, Qijing Huang, Yida Wang, Michael W. Mahoney, Kurt Keutzer", "title": "HAWQV3: Dyadic Neural Network Quantization", "comments": null, "journal-ref": "ICML 2021", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current low-precision quantization algorithms often have the hidden cost of\nconversion back and forth from floating point to quantized integer values. This\nhidden cost limits the latency improvement realized by quantizing Neural\nNetworks. To address this, we present HAWQV3, a novel mixed-precision\ninteger-only quantization framework. The contributions of HAWQV3 are the\nfollowing: (i) An integer-only inference where the entire computational graph\nis performed only with integer multiplication, addition, and bit shifting,\nwithout any floating point operations or even integer division; (ii) A novel\nhardware-aware mixed-precision quantization method where the bit-precision is\ncalculated by solving an integer linear programming problem that balances the\ntrade-off between model perturbation and other constraints, e.g., memory\nfootprint and latency; (iii) Direct hardware deployment and open source\ncontribution for 4-bit uniform/mixed-precision quantization in TVM, achieving\nan average speed up of $1.45\\times$ for uniform 4-bit, as compared to uniform\n8-bit for ResNet50 on T4 GPUs; and (iv) extensive evaluation of the proposed\nmethods on ResNet18/50 and InceptionV3, for various model compression levels\nwith/without mixed precision. For ResNet50, our INT8 quantization achieves an\naccuracy of $77.58\\%$, which is $2.68\\%$ higher than prior integer-only work,\nand our mixed-precision INT4/8 quantization can reduce INT8 latency by $23\\%$\nand still achieve $76.73\\%$ accuracy. Our framework and the TVM implementation\nhave been open sourced.\n", "versions": [{"version": "v1", "created": "Fri, 20 Nov 2020 23:51:43 GMT"}, {"version": "v2", "created": "Mon, 15 Feb 2021 06:43:10 GMT"}, {"version": "v3", "created": "Wed, 23 Jun 2021 07:49:12 GMT"}], "update_date": "2021-06-24", "authors_parsed": [["Yao", "Zhewei", ""], ["Dong", "Zhen", ""], ["Zheng", "Zhangcheng", ""], ["Gholami", "Amir", ""], ["Yu", "Jiali", ""], ["Tan", "Eric", ""], ["Wang", "Leyuan", ""], ["Huang", "Qijing", ""], ["Wang", "Yida", ""], ["Mahoney", "Michael W.", ""], ["Keutzer", "Kurt", ""]]}, {"id": "2011.10687", "submitter": "Gowri Somanath", "authors": "Gowri Somanath and Daniel Kurz", "title": "HDR Environment Map Estimation for Real-Time Augmented Reality", "comments": "Supplementary video at\n  https://docs-assets.developer.apple.com/ml-research/papers/hdr-environment-map.mp4\n  Code at https://github.com/apple/ml-envmapnet Accepted to CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a method to estimate an HDR environment map from a narrow\nfield-of-view LDR camera image in real-time. This enables perceptually\nappealing reflections and shading on virtual objects of any material finish,\nfrom mirror to diffuse, rendered into a real physical environment using\naugmented reality. Our method is based on our efficient convolutional neural\nnetwork architecture, EnvMapNet, trained end-to-end with two novel losses,\nProjectionLoss for the generated image, and ClusterLoss for adversarial\ntraining. Through qualitative and quantitative comparison to state-of-the-art\nmethods, we demonstrate that our algorithm reduces the directional error of\nestimated light sources by more than 50%, and achieves 3.7 times lower Frechet\nInception Distance (FID). We further showcase a mobile application that is able\nto run our neural network model in under 9 ms on an iPhone XS, and render in\nreal-time, visually coherent virtual objects in previously unseen real-world\nenvironments.\n", "versions": [{"version": "v1", "created": "Sat, 21 Nov 2020 01:01:53 GMT"}, {"version": "v2", "created": "Fri, 26 Mar 2021 19:32:32 GMT"}, {"version": "v3", "created": "Fri, 14 May 2021 21:32:49 GMT"}, {"version": "v4", "created": "Tue, 22 Jun 2021 22:15:31 GMT"}, {"version": "v5", "created": "Tue, 27 Jul 2021 20:48:22 GMT"}], "update_date": "2021-07-29", "authors_parsed": [["Somanath", "Gowri", ""], ["Kurz", "Daniel", ""]]}, {"id": "2011.10688", "submitter": "Xinwei Yao", "authors": "Xinwei Yao, Ohad Fried, Kayvon Fatahalian, Maneesh Agrawala", "title": "Iterative Text-based Editing of Talking-heads Using Neural Retargeting", "comments": "Project Website is https://davidyao.me/projects/text2vid", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.MM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present a text-based tool for editing talking-head video that enables an\niterative editing workflow. On each iteration users can edit the wording of the\nspeech, further refine mouth motions if necessary to reduce artifacts and\nmanipulate non-verbal aspects of the performance by inserting mouth gestures\n(e.g. a smile) or changing the overall performance style (e.g. energetic,\nmumble). Our tool requires only 2-3 minutes of the target actor video and it\nsynthesizes the video for each iteration in about 40 seconds, allowing users to\nquickly explore many editing possibilities as they iterate. Our approach is\nbased on two key ideas. (1) We develop a fast phoneme search algorithm that can\nquickly identify phoneme-level subsequences of the source repository video that\nbest match a desired edit. This enables our fast iteration loop. (2) We\nleverage a large repository of video of a source actor and develop a new\nself-supervised neural retargeting technique for transferring the mouth motions\nof the source actor to the target actor. This allows us to work with relatively\nshort target actor videos, making our approach applicable in many real-world\nediting scenarios. Finally, our refinement and performance controls give users\nthe ability to further fine-tune the synthesized results.\n", "versions": [{"version": "v1", "created": "Sat, 21 Nov 2020 01:05:55 GMT"}], "update_date": "2020-11-24", "authors_parsed": [["Yao", "Xinwei", ""], ["Fried", "Ohad", ""], ["Fatahalian", "Kayvon", ""], ["Agrawala", "Maneesh", ""]]}, {"id": "2011.10697", "submitter": "Mahdi Elhousni", "authors": "Elhousni Mahdi, Huang Xinming and Zhang Ziming", "title": "Aerial Height Prediction and Refinement Neural Networks with Semantic\n  and Geometric Guidance", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning provides a powerful new approach to many computer vision tasks.\nHeight prediction from aerial images is one of those tasks that benefited\ngreatly from the deployment of deep learning which replaced old multi-view\ngeometry techniques. This letter proposes a two-stage approach, where first a\nmulti-task neural network is used to predict the height map resulting from a\nsingle RGB aerial input image. We also include a second refinement step, where\na denoising autoencoder is used to produce higher quality height maps.\nExperiments on two publicly available datasets show that our method is capable\nof producing state-of-the-art results. Code is available at\nhttps://github.com/melhousni/DSMNet.\n", "versions": [{"version": "v1", "created": "Sat, 21 Nov 2020 01:39:37 GMT"}, {"version": "v2", "created": "Mon, 15 Feb 2021 19:00:15 GMT"}, {"version": "v3", "created": "Mon, 12 Apr 2021 16:58:27 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Mahdi", "Elhousni", ""], ["Xinming", "Huang", ""], ["Ziming", "Zhang", ""]]}, {"id": "2011.10698", "submitter": "Shihong Fang", "authors": "Shihong Fang, Anna Choromanska", "title": "Backdoor Attacks on the DNN Interpretation System", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.AI cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Interpretability is crucial to understand the inner workings of deep neural\nnetworks (DNNs) and many interpretation methods generate saliency maps that\nhighlight parts of the input image that contribute the most to the prediction\nmade by the DNN. In this paper we design a backdoor attack that alters the\nsaliency map produced by the network for an input image only with injected\ntrigger that is invisible to the naked eye while maintaining the prediction\naccuracy. The attack relies on injecting poisoned data with a trigger into the\ntraining data set. The saliency maps are incorporated in the penalty term of\nthe objective function that is used to train a deep model and its influence on\nmodel training is conditioned upon the presence of a trigger. We design two\ntypes of attacks: targeted attack that enforces a specific modification of the\nsaliency map and untargeted attack when the importance scores of the top pixels\nfrom the original saliency map are significantly reduced. We perform empirical\nevaluation of the proposed backdoor attacks on gradient-based and gradient-free\ninterpretation methods for a variety of deep learning architectures. We show\nthat our attacks constitute a serious security threat when deploying deep\nlearning models developed by untrusty sources. Finally, in the Supplement we\ndemonstrate that the proposed methodology can be used in an inverted setting,\nwhere the correct saliency map can be obtained only in the presence of a\ntrigger (key), effectively making the interpretation system available only to\nselected users.\n", "versions": [{"version": "v1", "created": "Sat, 21 Nov 2020 01:54:45 GMT"}, {"version": "v2", "created": "Thu, 3 Dec 2020 01:49:42 GMT"}], "update_date": "2020-12-04", "authors_parsed": [["Fang", "Shihong", ""], ["Choromanska", "Anna", ""]]}, {"id": "2011.10702", "submitter": "Alexander Wong", "authors": "James Ren Hou Lee, Maya Pavlova, Mahmoud Famouri, and Alexander Wong", "title": "CancerNet-SCa: Tailored Deep Neural Network Designs for Detection of\n  Skin Cancer from Dermoscopy Images", "comments": "8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Skin cancer continues to be the most frequently diagnosed form of cancer in\nthe U.S., with not only significant effects on health and well-being but also\nsignificant economic costs associated with treatment. A crucial step to the\ntreatment and management of skin cancer is effective skin cancer detection due\nto strong prognosis when treated at an early stage, with one of the key\nscreening approaches being dermoscopy examination. Motivated by the advances of\ndeep learning and inspired by the open source initiatives in the research\ncommunity, in this study we introduce CancerNet-SCa, a suite of deep neural\nnetwork designs tailored for the detection of skin cancer from dermoscopy\nimages that is open source and available to the general public as part of the\nCancer-Net initiative. To the best of the authors' knowledge, CancerNet-SCa\ncomprises of the first machine-designed deep neural network architecture\ndesigns tailored specifically for skin cancer detection, one of which\npossessing a self-attention architecture design with attention condensers.\nFurthermore, we investigate and audit the behaviour of CancerNet-SCa in a\nresponsible and transparent manner via explainability-driven model auditing.\nWhile CancerNet-SCa is not a production-ready screening solution, the hope is\nthat the release of CancerNet-SCa in open source, open access form will\nencourage researchers, clinicians, and citizen data scientists alike to\nleverage and build upon them.\n", "versions": [{"version": "v1", "created": "Sat, 21 Nov 2020 02:17:59 GMT"}], "update_date": "2020-11-24", "authors_parsed": [["Lee", "James Ren Hou", ""], ["Pavlova", "Maya", ""], ["Famouri", "Mahmoud", ""], ["Wong", "Alexander", ""]]}, {"id": "2011.10704", "submitter": "Weixin Liang", "authors": "Weixin Liang, James Zou", "title": "Neural Group Testing to Accelerate Deep Learning", "comments": "ISIT 2021. Code & data available at\n  https://github.com/Weixin-Liang/NeuralGroupTesting", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CL cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in deep learning have made the use of large, deep neural\nnetworks with tens of millions of parameters. The sheer size of these networks\nimposes a challenging computational burden during inference. Existing work\nfocuses primarily on accelerating each forward pass of a neural network.\nInspired by the group testing strategy for efficient disease testing, we\npropose neural group testing, which accelerates by testing a group of samples\nin one forward pass. Groups of samples that test negative are ruled out. If a\ngroup tests positive, samples in that group are then retested adaptively. A key\nchallenge of neural group testing is to modify a deep neural network so that it\ncould test multiple samples in one forward pass. We propose three designs to\nachieve this without introducing any new parameters and evaluate their\nperformances. We applied neural group testing in an image moderation task to\ndetect rare but inappropriate images. We found that neural group testing can\ngroup up to 16 images in one forward pass and reduce the overall computation\ncost by over 73% while improving detection performance.\n", "versions": [{"version": "v1", "created": "Sat, 21 Nov 2020 02:23:54 GMT"}, {"version": "v2", "created": "Sun, 9 May 2021 23:03:47 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Liang", "Weixin", ""], ["Zou", "James", ""]]}, {"id": "2011.10726", "submitter": "Michael Danielczuk", "authors": "Michael Danielczuk, Arsalan Mousavian, Clemens Eppner, Dieter Fox", "title": "Object Rearrangement Using Learned Implicit Collision Functions", "comments": "First two authors contributed equally. 2021 IEEE International\n  Conference on Robotics and Automation. 8 pages, 4 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Robotic object rearrangement combines the skills of picking and placing\nobjects. When object models are unavailable, typical collision-checking models\nmay be unable to predict collisions in partial point clouds with occlusions,\nmaking generation of collision-free grasping or placement trajectories\nchallenging. We propose a learned collision model that accepts scene and query\nobject point clouds and predicts collisions for 6DOF object poses within the\nscene. We train the model on a synthetic set of 1 million scene/object point\ncloud pairs and 2 billion collision queries. We leverage the learned collision\nmodel as part of a model predictive path integral (MPPI) policy in a tabletop\nrearrangement task and show that the policy can plan collision-free grasps and\nplacements for objects unseen in training in both simulated and physical\ncluttered scenes with a Franka Panda robot. The learned model outperforms both\ntraditional pipelines and learned ablations by 9.8% in accuracy on a dataset of\nsimulated collision queries and is 75x faster than the best-performing\nbaseline. Videos and supplementary material are available at\nhttps://research.nvidia.com/publication/2021-03_Object-Rearrangement-Using.\n", "versions": [{"version": "v1", "created": "Sat, 21 Nov 2020 05:36:06 GMT"}, {"version": "v2", "created": "Fri, 26 Mar 2021 07:38:35 GMT"}], "update_date": "2021-03-29", "authors_parsed": [["Danielczuk", "Michael", ""], ["Mousavian", "Arsalan", ""], ["Eppner", "Clemens", ""], ["Fox", "Dieter", ""]]}, {"id": "2011.10727", "submitter": "Ravindra Yadav", "authors": "Ravindra Yadav, Ashish Sardana, Vinay P Namboodiri, Rajesh M Hegde", "title": "Stochastic Talking Face Generation Using Latent Distribution Matching", "comments": "InterSpeech 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  The ability to envisage the visual of a talking face based just on hearing a\nvoice is a unique human capability. There have been a number of works that have\nsolved for this ability recently. We differ from these approaches by enabling a\nvariety of talking face generations based on single audio input. Indeed, just\nhaving the ability to generate a single talking face would make a system almost\nrobotic in nature. In contrast, our unsupervised stochastic audio-to-video\ngeneration model allows for diverse generations from a single audio input.\nParticularly, we present an unsupervised stochastic audio-to-video generation\nmodel that can capture multiple modes of the video distribution. We ensure that\nall the diverse generations are plausible. We do so through a principled\nmulti-modal variational autoencoder framework. We demonstrate its efficacy on\nthe challenging LRW and GRID datasets and demonstrate performance better than\nthe baseline, while having the ability to generate multiple diverse lip\nsynchronized videos.\n", "versions": [{"version": "v1", "created": "Sat, 21 Nov 2020 06:05:24 GMT"}], "update_date": "2020-11-24", "authors_parsed": [["Yadav", "Ravindra", ""], ["Sardana", "Ashish", ""], ["Namboodiri", "Vinay P", ""], ["Hegde", "Rajesh M", ""]]}, {"id": "2011.10731", "submitter": "Weixin Liang", "authors": "Weixin Liang, Feiyang Niu, Aishwarya Reganti, Govind Thattai, Gokhan\n  Tur", "title": "LRTA: A Transparent Neural-Symbolic Reasoning Framework with Modular\n  Supervision for Visual Question Answering", "comments": "NeurIPS KR2ML 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The predominant approach to visual question answering (VQA) relies on\nencoding the image and question with a \"black-box\" neural encoder and decoding\na single token as the answer like \"yes\" or \"no\". Despite this approach's strong\nquantitative results, it struggles to come up with intuitive, human-readable\nforms of justification for the prediction process. To address this\ninsufficiency, we reformulate VQA as a full answer generation task, which\nrequires the model to justify its predictions in natural language. We propose\nLRTA [Look, Read, Think, Answer], a transparent neural-symbolic reasoning\nframework for visual question answering that solves the problem step-by-step\nlike humans and provides human-readable form of justification at each step.\nSpecifically, LRTA learns to first convert an image into a scene graph and\nparse a question into multiple reasoning instructions. It then executes the\nreasoning instructions one at a time by traversing the scene graph using a\nrecurrent neural-symbolic execution module. Finally, it generates a full answer\nto the given question with natural language justifications. Our experiments on\nGQA dataset show that LRTA outperforms the state-of-the-art model by a large\nmargin (43.1% v.s. 28.0%) on the full answer generation task. We also create a\nperturbed GQA test set by removing linguistic cues (attributes and relations)\nin the questions for analyzing whether a model is having a smart guess with\nsuperficial data correlations. We show that LRTA makes a step towards truly\nunderstanding the question while the state-of-the-art model tends to learn\nsuperficial correlations from the training data.\n", "versions": [{"version": "v1", "created": "Sat, 21 Nov 2020 06:39:42 GMT"}], "update_date": "2020-11-24", "authors_parsed": [["Liang", "Weixin", ""], ["Niu", "Feiyang", ""], ["Reganti", "Aishwarya", ""], ["Thattai", "Govind", ""], ["Tur", "Gokhan", ""]]}, {"id": "2011.10752", "submitter": "Baudouin Denis de Senneville PhD", "authors": "Baudouin Denis de Senneville, Mario Ries, Wilbert Bartels, Chrit\n  Moonen", "title": "MRI-Guided High Intensity Focused Ultrasound of Liver and Kidney", "comments": "27 pages, 5 figures", "journal-ref": null, "doi": "10.1007/174_2011_394", "report-no": "Interventional Magnetic Resonance Imaging, 2012", "categories": "physics.med-ph cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High Intensity Focused Ultrasound (HIFU) can be used to achieve a local\ntemperature increase deep inside the human body in a non-invasive way. MRI\nguidance of the procedure allows in situ target definition. In addition, MRI\ncan be used to provide continuous temperature mapping during HIFU for spatial\nand temporal control of the heating procedure and prediction of the final\nlesion based on the received thermal dose. Temperature mapping of mobile organs\nas kidney and liver is challenging, as well as real-time processing methods for\nfeedback control of the HIFU procedure. In this paper, recent technological\nadvances are reviewed in MR temperature mapping of these organs, in motion\ncompensation of the HIFU beam, in intercostal HIFU sonication, and in\nvolumetric ablation and feedback control strategies. Recent pre-clinical\nstudies have demonstrated the feasibility of each of these novel methods. The\nperspectives to translate those advances into the clinic are addressed. It can\nbe concluded that MR guided HIFU for ablation in liver and kidney appears\nfeasible but requires further work on integration of technologically advanced\nmethods.\n", "versions": [{"version": "v1", "created": "Sat, 21 Nov 2020 09:37:06 GMT"}], "update_date": "2020-11-24", "authors_parsed": [["de Senneville", "Baudouin Denis", ""], ["Ries", "Mario", ""], ["Bartels", "Wilbert", ""], ["Moonen", "Chrit", ""]]}, {"id": "2011.10759", "submitter": "Faizaan Sakib", "authors": "Faizaan Sakib and Tilo Burghardt", "title": "Visual Recognition of Great Ape Behaviours in the Wild", "comments": "4 pages, 4 figures, to be published in the proceedings of ICPR 2020\n  at the Visual observation and analysis of Vertebrate And Insect Behaviour\n  (VAIB) workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose a first great ape-specific visual behaviour recognition system\nutilising deep learning that is capable of detecting nine core ape behaviours.\n", "versions": [{"version": "v1", "created": "Sat, 21 Nov 2020 10:27:21 GMT"}], "update_date": "2020-11-24", "authors_parsed": [["Sakib", "Faizaan", ""], ["Burghardt", "Tilo", ""]]}, {"id": "2011.10772", "submitter": "Kemal Oksuz", "authors": "Kemal Oksuz and Baris Can Cam and Sinan Kalkan and Emre Akbas", "title": "One Metric to Measure them All: Localisation Recall Precision (LRP) for\n  Evaluating Visual Detection Tasks", "comments": "Under review at TPAMI; revised version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Despite being widely used as a performance measure for visual detection\ntasks, Average Precision (AP) is limited in reflecting localisation quality,\n(ii) interpretability and (iii) robustness to the design choices regarding its\ncomputation, and its applicability to outputs without confidence scores.\nPanoptic Quality (PQ), a measure proposed for evaluating panoptic segmentation\n(Kirillov et al., 2019), does not suffer from these limitations but is limited\nto panoptic segmentation. In this paper, we propose Localisation Recall\nPrecision (LRP) Error as the performance measure for all visual detection\ntasks. LRP Error, initially proposed only for object detection by Oksuz et al.\n(2018), does not suffer from the aforementioned limitations and is applicable\nto all visual detection tasks. We also introduce Optimal LRP (oLRP) Error as\nthe minimum LRP error obtained over confidence scores to evaluate visual\ndetectors and obtain optimal thresholds for deployment. We provide a detailed\ncomparative analysis of LRP with AP and PQ, and use nearly 100 state-of-the-art\nvisual detectors from seven visual detection tasks (i.e. object detection,\nkeypoint detection, instance segmentation, panoptic segmentation, visual\nrelationship detection, zero-shot detection and generalised zero-shot\ndetection) using ten datasets (i.e. different COCO variants, LVIS, Open Images,\nPascal, ILSVRC) to empirically show that LRP provides richer and more\ndiscriminative information than its counterparts. Code available at:\nhttps://github.com/kemaloksuz/LRP-Error\n", "versions": [{"version": "v1", "created": "Sat, 21 Nov 2020 11:20:42 GMT"}, {"version": "v2", "created": "Tue, 13 Jul 2021 06:24:35 GMT"}], "update_date": "2021-07-14", "authors_parsed": [["Oksuz", "Kemal", ""], ["Cam", "Baris Can", ""], ["Kalkan", "Sinan", ""], ["Akbas", "Emre", ""]]}, {"id": "2011.10776", "submitter": "Suping Wu", "authors": "Lei Li, Suping Wu", "title": "DmifNet:3D Shape Reconstruction Based on Dynamic Multi-Branch\n  Information Fusion", "comments": "ICPR 2020 (Oral)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  3D object reconstruction from a single-view image is a long-standing\nchallenging problem. Previous work was difficult to accurately reconstruct 3D\nshapes with a complex topology which has rich details at the edges and corners.\nMoreover, previous works used synthetic data to train their network, but domain\nadaptation problems occurred when tested on real data. In this paper, we\npropose a Dynamic Multi-branch Information Fusion Network (DmifNet) which can\nrecover a high-fidelity 3D shape of arbitrary topology from a 2D image.\nSpecifically, we design several side branches from the intermediate layers to\nmake the network produce more diverse representations to improve the\ngeneralization ability of network. In addition, we utilize DoG (Difference of\nGaussians) to extract edge geometry and corners information from input images.\nThen, we use a separate side branch network to process the extracted data to\nbetter capture edge geometry and corners feature information. Finally, we\ndynamically fuse the information of all branches to gain final predicted\nprobability. Extensive qualitative and quantitative experiments on a\nlarge-scale publicly available dataset demonstrate the validity and efficiency\nof our method. Code and models are publicly available at\nhttps://github.com/leilimaster/DmifNet.\n", "versions": [{"version": "v1", "created": "Sat, 21 Nov 2020 11:31:27 GMT"}], "update_date": "2020-11-24", "authors_parsed": [["Li", "Lei", ""], ["Wu", "Suping", ""]]}, {"id": "2011.10812", "submitter": "Fan Lu", "authors": "Fan Lu, Guang Chen, Yinlong Liu, Zhijun Li, Sanqing Qu, Tianpei Zou", "title": "MoNet: Motion-based Point Cloud Prediction Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Predicting the future can significantly improve the safety of intelligent\nvehicles, which is a key component in autonomous driving. 3D point clouds\naccurately model 3D information of surrounding environment and are crucial for\nintelligent vehicles to perceive the scene. Therefore, prediction of 3D point\nclouds has great significance for intelligent vehicles, which can be utilized\nfor numerous further applications. However, due to point clouds are unordered\nand unstructured, point cloud prediction is challenging and has not been deeply\nexplored in current literature. In this paper, we propose a novel motion-based\nneural network named MoNet. The key idea of the proposed MoNet is to integrate\nmotion features between two consecutive point clouds into the prediction\npipeline. The introduction of motion features enables the model to more\naccurately capture the variations of motion information across frames and thus\nmake better predictions for future motion. In addition, content features are\nintroduced to model the spatial content of individual point clouds. A recurrent\nneural network named MotionRNN is proposed to capture the temporal correlations\nof both features. Besides, we propose an attention-based motion align module to\naddress the problem of missing motion features in the inference pipeline.\nExtensive experiments on two large scale outdoor LiDAR datasets demonstrate the\nperformance of the proposed MoNet. Moreover, we perform experiments on\napplications using the predicted point clouds and the results indicate the\ngreat application potential of the proposed method.\n", "versions": [{"version": "v1", "created": "Sat, 21 Nov 2020 15:43:31 GMT"}], "update_date": "2020-11-24", "authors_parsed": [["Lu", "Fan", ""], ["Chen", "Guang", ""], ["Liu", "Yinlong", ""], ["Li", "Zhijun", ""], ["Qu", "Sanqing", ""], ["Zou", "Tianpei", ""]]}, {"id": "2011.10823", "submitter": "Pitchayagan Temniranrat", "authors": "Pitchayagan Temniranrat, Kantip Kiratiratanapruk, Apichon Kitvimonrat,\n  Wasin Sinthupinyo and Sujin Patarapuwadol", "title": "A System for Automatic Rice Disease Detection from Rice Paddy Images\n  Serviced via a Chatbot", "comments": "19 pages, 6 figures", "journal-ref": "Computers and Electronics in Agriculture, Volume 185, June 2021,\n  106156", "doi": "10.1016/j.compag.2021.106156", "report-no": null, "categories": "eess.SY cs.CV cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A LINE Bot System to diagnose rice diseases from actual paddy field images\nwas developed and presented in this paper. It was easy-to-use and automatic\nsystem designed to help rice farmers improve the rice yield and quality. The\ntargeted images were taken from the actual paddy environment without special\nsample preparation. We used a deep learning neural networks technique to detect\nrice diseases from the images. We developed an object detection model training\nand refinement process to improve the performance of our previous research on\nrice leave diseases detection. The process was based on analyzing the model's\npredictive results and could be repeatedly used to improve the quality of the\ndatabase in the next training of the model. The deployment model for our LINE\nBot system was created from the selected best performance technique in our\nprevious paper, YOLOv3, trained by refined training data set. The performance\nof the deployment model was measured on 5 target classes and found that the\nAverage True Positive Point improved from 91.1% in the previous paper to 95.6%\nin this study. Therefore, we used this deployment model for Rice Disease LINE\nBot system. Our system worked automatically real-time to suggest primary\ndiagnosis results to the users in the LINE group, which included rice farmers\nand rice disease specialists. They could communicate freely via chat. In the\nreal LINE Bot deployment, the model's performance was measured by our own\ndefined measurement Average True Positive Point and was found to be an average\nof 78.86%. The system was fast and took only 2-3 s for detection process in our\nsystem server.\n", "versions": [{"version": "v1", "created": "Sat, 21 Nov 2020 16:45:02 GMT"}, {"version": "v2", "created": "Wed, 23 Jun 2021 08:49:24 GMT"}], "update_date": "2021-06-24", "authors_parsed": [["Temniranrat", "Pitchayagan", ""], ["Kiratiratanapruk", "Kantip", ""], ["Kitvimonrat", "Apichon", ""], ["Sinthupinyo", "Wasin", ""], ["Patarapuwadol", "Sujin", ""]]}, {"id": "2011.10830", "submitter": "Mengmeng Xu", "authors": "Mengmeng Xu, Juan-Manuel Perez-Rua, Victor Escorcia, Brais Martinez,\n  Xiatian Zhu, Li Zhang, Bernard Ghanem, Tao Xiang", "title": "Boundary-sensitive Pre-training for Temporal Localization in Videos", "comments": "11 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many video analysis tasks require temporal localization thus detection of\ncontent changes. However, most existing models developed for these tasks are\npre-trained on general video action classification tasks. This is because large\nscale annotation of temporal boundaries in untrimmed videos is expensive.\nTherefore no suitable datasets exist for temporal boundary-sensitive\npre-training. In this paper for the first time, we investigate model\npre-training for temporal localization by introducing a novel\nboundary-sensitive pretext (BSP) task. Instead of relying on costly manual\nannotations of temporal boundaries, we propose to synthesize temporal\nboundaries in existing video action classification datasets. With the\nsynthesized boundaries, BSP can be simply conducted via classifying the\nboundary types. This enables the learning of video representations that are\nmuch more transferable to downstream temporal localization tasks. Extensive\nexperiments show that the proposed BSP is superior and complementary to the\nexisting action classification based pre-training counterpart, and achieves new\nstate-of-the-art performance on several temporal localization tasks.\n", "versions": [{"version": "v1", "created": "Sat, 21 Nov 2020 17:46:24 GMT"}, {"version": "v2", "created": "Tue, 24 Nov 2020 13:47:46 GMT"}, {"version": "v3", "created": "Fri, 26 Mar 2021 11:01:35 GMT"}], "update_date": "2021-03-29", "authors_parsed": [["Xu", "Mengmeng", ""], ["Perez-Rua", "Juan-Manuel", ""], ["Escorcia", "Victor", ""], ["Martinez", "Brais", ""], ["Zhu", "Xiatian", ""], ["Zhang", "Li", ""], ["Ghanem", "Bernard", ""], ["Xiang", "Tao", ""]]}, {"id": "2011.10839", "submitter": "Marco Scarpetta", "authors": "Nicola Giaquinto, Marco Scarpetta, Maurizio Spadavecchia, Gregorio\n  Andria", "title": "Deep Learning-Based Computer Vision for Real Time Intravenous Drip\n  Infusion Monitoring", "comments": "7 pages, 9 figures", "journal-ref": null, "doi": "10.1109/JSEN.2020.3039009", "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper explores the use of deep learning-based computer vision for\nreal-time monitoring of the flow in intravenous (IV) infusions. IV infusions\nare among the most common therapies in hospitalized patients and, given that\nboth over-infusion and under-infusion can cause severe damages, monitoring the\nflow rate of the fluid being administered to patients is very important for\ntheir safety. The proposed system uses a camera to film the IV drip infusion\nkit and a deep learning-based algorithm to classify acquired frames into two\ndifferent states: frames with a drop that has just begun to take shape and\nframes with a well-formed drop. The alternation of these two states is used to\ncount drops and derive a measurement of the flow rate of the drip. The usage of\na camera as sensing element makes the proposed system safe in medical\nenvironments and easier to be integrated into current health facilities.\nExperimental results are reported in the paper that confirm the accuracy of the\nsystem and its capability to produce real-time estimates. The proposed method\ncan be therefore effectively adopted to implement IV infusion monitoring and\ncontrol systems.\n", "versions": [{"version": "v1", "created": "Sat, 21 Nov 2020 18:26:44 GMT"}], "update_date": "2020-11-24", "authors_parsed": [["Giaquinto", "Nicola", ""], ["Scarpetta", "Marco", ""], ["Spadavecchia", "Maurizio", ""], ["Andria", "Gregorio", ""]]}, {"id": "2011.10850", "submitter": "Honglei Zhang", "authors": "Honglei Zhang, Hu Wang, Yuanzhouhan Cao, Chunhua Shen, Yidong Li", "title": "Robust Watermarking Using Inverse Gradient Attention", "comments": "9 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Watermarking is the procedure of encoding desired information into an image\nto resist potential noises while ensuring the embedded image has little\nperceptual perturbations from the original image. Recently, with the tremendous\nsuccesses gained by deep neural networks in various fields, digital\nwatermarking has attracted increasing number of attentions. The neglect of\nconsidering the pixel importance within the cover image of deep neural models\nwill inevitably affect the model robustness for information hiding. Targeting\nat the problem, in this paper, we propose a novel deep watermarking scheme with\nInverse Gradient Attention (IGA), combing the ideas of adversarial learning and\nattention mechanism to endow different importance to different pixels. With the\nproposed method, the model is able to spotlight pixels with more robustness for\nembedding data. Besides, from an orthogonal point of view, in order to increase\nthe model embedding capacity, we propose a complementary message coding module.\nEmpirically, extensive experiments show that the proposed model outperforms the\nstate-of-the-art methods on two prevalent datasets under multiple settings.\n", "versions": [{"version": "v1", "created": "Sat, 21 Nov 2020 19:08:23 GMT"}, {"version": "v2", "created": "Fri, 5 Feb 2021 02:51:09 GMT"}], "update_date": "2021-02-08", "authors_parsed": [["Zhang", "Honglei", ""], ["Wang", "Hu", ""], ["Cao", "Yuanzhouhan", ""], ["Shen", "Chunhua", ""], ["Li", "Yidong", ""]]}, {"id": "2011.10857", "submitter": "Mahdi Biparva", "authors": "Mahdi Biparva, John Tsotsos", "title": "Contextual Interference Reduction by Selective Fine-Tuning of Neural\n  Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Feature disentanglement of the foreground target objects and the background\nsurrounding context has not been yet fully accomplished. The lack of network\ninterpretability prevents advancing for feature disentanglement and better\ngeneralization robustness. We study the role of the context on interfering with\na disentangled foreground target object representation in this work. We\nhypothesize that the representation of the surrounding context is heavily tied\nwith the foreground object due to the dense hierarchical parametrization of\nconvolutional networks with under-constrained learning algorithms. Working on a\nframework that benefits from the bottom-up and top-down processing paradigms,\nwe investigate a systematic approach to shift learned representations in\nfeedforward networks from the emphasis on the irrelevant context to the\nforeground objects. The top-down processing provides importance maps as the\nmeans of the network internal self-interpretation that will guide the learning\nalgorithm to focus on the relevant foreground regions towards achieving a more\nrobust representations. We define an experimental evaluation setup with the\nrole of context emphasized using the MNIST dataset. The experimental results\nreveal not only that the label prediction accuracy is improved but also a\nhigher degree of robustness to the background perturbation using various noise\ngeneration methods is obtained.\n", "versions": [{"version": "v1", "created": "Sat, 21 Nov 2020 20:11:12 GMT"}], "update_date": "2020-11-24", "authors_parsed": [["Biparva", "Mahdi", ""], ["Tsotsos", "John", ""]]}, {"id": "2011.10875", "submitter": "Heng Fan", "authors": "Heng Fan, Halady Akhilesha Miththanthaya, Harshit, Siranjiv Ramana\n  Rajan, Xiaoqiong Liu, Zhilin Zou, Yuewei Lin, Haibin Ling", "title": "Transparent Object Tracking Benchmark", "comments": "tech. report", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual tracking has achieved considerable progress in recent years. However,\ncurrent research in the field mainly focuses on tracking of opaque objects,\nwhile little attention is paid to transparent object tracking. In this paper,\nwe make the first attempt in exploring this problem by proposing a Transparent\nObject Tracking Benchmark (TOTB). Specifically, TOTB consists of 225 videos\n(86K frames) from 15 diverse transparent object categories. Each sequence is\nmanually labeled with axis-aligned bounding boxes. To the best of our\nknowledge, TOTB is the first benchmark dedicated to transparent object\ntracking. In order to understand how existing trackers perform and to provide\ncomparison for future research on TOTB, we extensively evaluate 25\nstate-of-the-art tracking algorithms. The evaluation results exhibit that more\nefforts are needed to improve transparent object tracking. Besides, we observe\nsome nontrivial findings from the evaluation that are discrepant with some\ncommon beliefs in opaque object tracking. For example, we find that deep(er)\nfeatures are not always good for improvements. Moreover, to encourage future\nresearch, we introduce a novel tracker, named TransATOM, which leverages\ntransparency features for tracking and surpasses all 25 evaluated approaches by\na large margin. By releasing TOTB, we expect to facilitate future research and\napplication of transparent object tracking in both the academia and industry.\nThe TOTB and evaluation results as well as TransATOM will be made available at\nhttps://hengfan2010.github.io/projects/TOTB.\n", "versions": [{"version": "v1", "created": "Sat, 21 Nov 2020 21:39:43 GMT"}], "update_date": "2020-11-24", "authors_parsed": [["Fan", "Heng", ""], ["Miththanthaya", "Halady Akhilesha", ""], ["Harshit", "", ""], ["Rajan", "Siranjiv Ramana", ""], ["Liu", "Xiaoqiong", ""], ["Zou", "Zhilin", ""], ["Lin", "Yuewei", ""], ["Ling", "Haibin", ""]]}, {"id": "2011.10881", "submitter": "Zhiqing Sun", "authors": "Zhiqing Sun, Shengcao Cao, Yiming Yang, Kris Kitani", "title": "Rethinking Transformer-based Set Prediction for Object Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  DETR is a recently proposed Transformer-based method which views object\ndetection as a set prediction problem and achieves state-of-the-art performance\nbut demands extra-long training time to converge. In this paper, we investigate\nthe causes of the optimization difficulty in the training of DETR. Our\nexaminations reveal several factors contributing to the slow convergence of\nDETR, primarily the issues with the Hungarian loss and the Transformer cross\nattention mechanism. To overcome these issues we propose two solutions, namely,\nTSP-FCOS (Transformer-based Set Prediction with FCOS) and TSP-RCNN\n(Transformer-based Set Prediction with RCNN). Experimental results show that\nthe proposed methods not only converge much faster than the original DETR, but\nalso significantly outperform DETR and other baselines in terms of detection\naccuracy.\n", "versions": [{"version": "v1", "created": "Sat, 21 Nov 2020 21:59:42 GMT"}], "update_date": "2020-11-24", "authors_parsed": [["Sun", "Zhiqing", ""], ["Cao", "Shengcao", ""], ["Yang", "Yiming", ""], ["Kitani", "Kris", ""]]}, {"id": "2011.10889", "submitter": "Karan Sikka", "authors": "Karan Sikka, Jihua Huang, Andrew Silberfarb, Prateeth Nayak, Luke\n  Rohrer, Pritish Sahu, John Byrnes, Ajay Divakaran, Richard Rohwer", "title": "Zero-Shot Learning with Knowledge Enhanced Visual Semantic Embeddings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We improve zero-shot learning (ZSL) by incorporating common-sense knowledge\nin DNNs. We propose Common-Sense based Neuro-Symbolic Loss (CSNL) that\nformulates prior knowledge as novel neuro-symbolic loss functions that\nregularize visual-semantic embedding. CSNL forces visual features in the VSE to\nobey common-sense rules relating to hypernyms and attributes. We introduce two\nkey novelties for improved learning: (1) enforcement of rules for a group\ninstead of a single concept to take into account class-wise relationships, and\n(2) confidence margins inside logical operators that enable implicit curriculum\nlearning and prevent premature overfitting. We evaluate the advantages of\nincorporating each knowledge source and show consistent gains over prior\nstate-of-art methods in both conventional and generalized ZSL e.g. 11.5%, 5.5%,\nand 11.6% improvements on AWA2, CUB, and Kinetics respectively.\n", "versions": [{"version": "v1", "created": "Sat, 21 Nov 2020 22:58:38 GMT"}], "update_date": "2020-11-24", "authors_parsed": [["Sikka", "Karan", ""], ["Huang", "Jihua", ""], ["Silberfarb", "Andrew", ""], ["Nayak", "Prateeth", ""], ["Rohrer", "Luke", ""], ["Sahu", "Pritish", ""], ["Byrnes", "John", ""], ["Divakaran", "Ajay", ""], ["Rohwer", "Richard", ""]]}, {"id": "2011.10893", "submitter": "Hossein Talebi", "authors": "Hossein Talebi, Ehsan Amid, Peyman Milanfar, and Manfred K. Warmuth", "title": "Rank-smoothed Pairwise Learning In Perceptual Quality Assessment", "comments": null, "journal-ref": "IEEE International Conference on Image Processing (ICIP) 2020", "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conducting pairwise comparisons is a widely used approach in curating human\nperceptual preference data. Typically raters are instructed to make their\nchoices according to a specific set of rules that address certain dimensions of\nimage quality and aesthetics. The outcome of this process is a dataset of\nsampled image pairs with their associated empirical preference probabilities.\nTraining a model on these pairwise preferences is a common deep learning\napproach. However, optimizing by gradient descent through mini-batch learning\nmeans that the \"global\" ranking of the images is not explicitly taken into\naccount. In other words, each step of the gradient descent relies only on a\nlimited number of pairwise comparisons. In this work, we demonstrate that\nregularizing the pairwise empirical probabilities with aggregated rankwise\nprobabilities leads to a more reliable training loss. We show that training a\ndeep image quality assessment model with our rank-smoothed loss consistently\nimproves the accuracy of predicting human preferences.\n", "versions": [{"version": "v1", "created": "Sat, 21 Nov 2020 23:33:14 GMT"}], "update_date": "2020-11-24", "authors_parsed": [["Talebi", "Hossein", ""], ["Amid", "Ehsan", ""], ["Milanfar", "Peyman", ""], ["Warmuth", "Manfred K.", ""]]}, {"id": "2011.10904", "submitter": "Yuanzheng Ci", "authors": "Yuanzheng Ci, Chen Lin, Ming Sun, Boyu Chen, Hongwen Zhang, Wanli\n  Ouyang", "title": "Evolving Search Space for Neural Architecture Search", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The automation of neural architecture design has been a coveted alternative\nto human experts. Recent works have small search space, which is easier to\noptimize but has a limited upper bound of the optimal solution. Extra human\ndesign is needed for those methods to propose a more suitable space with\nrespect to the specific task and algorithm capacity. To further enhance the\ndegree of automation for neural architecture search, we present a Neural\nSearch-space Evolution (NSE) scheme that iteratively amplifies the results from\nthe previous effort by maintaining an optimized search space subset. This\ndesign minimizes the necessity of a well-designed search space. We further\nextend the flexibility of obtainable architectures by introducing a learnable\nmulti-branch setting. By employing the proposed method, a consistent\nperformance gain is achieved during a progressive search over upcoming search\nspaces. We achieve 77.3% top-1 retrain accuracy on ImageNet with 333M FLOPs,\nwhich yielded a state-of-the-art performance among previous auto-generated\narchitectures that do not involve knowledge distillation or weight pruning.\nWhen the latency constraint is adopted, our result also performs better than\nthe previous best-performing mobile models with a 77.9% Top-1 retrain accuracy.\n", "versions": [{"version": "v1", "created": "Sun, 22 Nov 2020 01:11:19 GMT"}], "update_date": "2020-11-24", "authors_parsed": [["Ci", "Yuanzheng", ""], ["Lin", "Chen", ""], ["Sun", "Ming", ""], ["Chen", "Boyu", ""], ["Zhang", "Hongwen", ""], ["Ouyang", "Wanli", ""]]}, {"id": "2011.10909", "submitter": "Prashanth Vijayaraghavan", "authors": "Prashanth Vijayaraghavan, Deb Roy", "title": "Video SemNet: Memory-Augmented Video Semantic Network", "comments": "6 pages, NIPS 2017 Workshop Visually-Grounded Interaction and\n  Language (ViGIL)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Stories are a very compelling medium to convey ideas, experiences, social and\ncultural values. Narrative is a specific manifestation of the story that turns\nit into knowledge for the audience. In this paper, we propose a machine\nlearning approach to capture the narrative elements in movies by bridging the\ngap between the low-level data representations and semantic aspects of the\nvisual medium. We present a Memory-Augmented Video Semantic Network, called\nVideo SemNet, to encode the semantic descriptors and learn an embedding for the\nvideo. The model employs two main components: (i) a neural semantic learner\nthat learns latent embeddings of semantic descriptors and (ii) a memory module\nthat retains and memorizes specific semantic patterns from the video. We\nevaluate the video representations obtained from variants of our model on two\ntasks: (a) genre prediction and (b) IMDB Rating prediction. We demonstrate that\nour model is able to predict genres and IMDB ratings with a weighted F-1 score\nof 0.72 and 0.63 respectively. The results are indicative of the\nrepresentational power of our model and the ability of such representations to\nmeasure audience engagement.\n", "versions": [{"version": "v1", "created": "Sun, 22 Nov 2020 01:36:37 GMT"}], "update_date": "2020-11-24", "authors_parsed": [["Vijayaraghavan", "Prashanth", ""], ["Roy", "Deb", ""]]}, {"id": "2011.10916", "submitter": "Kunjal Panchal", "authors": "Kunjal Panchal", "title": "Hierachical Delta-Attention Method for Multimodal Fusion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In vision and linguistics; the main input modalities are facial expressions,\nspeech patterns, and the words uttered. The issue with analysis of any one mode\nof expression (Visual, Verbal or Vocal) is that lot of contextual information\ncan get lost. This asks researchers to inspect multiple modalities to get a\nthorough understanding of the cross-modal dependencies and temporal context of\nthe situation to analyze the expression. This work attempts at preserving the\nlong-range dependencies within and across different modalities, which would be\nbottle-necked by the use of recurrent networks and adds the concept of\ndelta-attention to focus on local differences per modality to capture the\nidiosyncrasy of different people. We explore a cross-attention fusion technique\nto get the global view of the emotion expressed through these\ndelta-self-attended modalities, in order to fuse all the local nuances and\nglobal context together. The addition of attention is new to the multi-modal\nfusion field and currently being scrutinized for on what stage the attention\nmechanism should be used, this work achieves competitive accuracy for overall\nand per-class classification which is close to the current state-of-the-art\nwith almost half number of parameters.\n", "versions": [{"version": "v1", "created": "Sun, 22 Nov 2020 02:45:52 GMT"}], "update_date": "2020-11-24", "authors_parsed": [["Panchal", "Kunjal", ""]]}, {"id": "2011.10925", "submitter": "Benyamin Ghojogh", "authors": "Benyamin Ghojogh, Ali Ghodsi, Fakhri Karray, Mark Crowley", "title": "Locally Linear Embedding and its Variants: Tutorial and Survey", "comments": "To appear as a part of an upcoming textbook on dimensionality\n  reduction and manifold learning", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This is a tutorial and survey paper for Locally Linear Embedding (LLE) and\nits variants. The idea of LLE is fitting the local structure of manifold in the\nembedding space. In this paper, we first cover LLE, kernel LLE, inverse LLE,\nand feature fusion with LLE. Then, we cover out-of-sample embedding using\nlinear reconstruction, eigenfunctions, and kernel mapping. Incremental LLE is\nexplained for embedding streaming data. Landmark LLE methods using the Nystrom\napproximation and locally linear landmarks are explained for big data\nembedding. We introduce the methods for parameter selection of number of\nneighbors using residual variance, Procrustes statistics, preservation\nneighborhood error, and local neighborhood selection. Afterwards, Supervised\nLLE (SLLE), enhanced SLLE, SLLE projection, probabilistic SLLE, supervised\nguided LLE (using Hilbert-Schmidt independence criterion), and semi-supervised\nLLE are explained for supervised and semi-supervised embedding. Robust LLE\nmethods using least squares problem and penalty functions are also introduced\nfor embedding in the presence of outliers and noise. Then, we introduce fusion\nof LLE with other manifold learning methods including Isomap (i.e., ISOLLE),\nprincipal component analysis, Fisher discriminant analysis, discriminant LLE,\nand Isotop. Finally, we explain weighted LLE in which the distances,\nreconstruction weights, or the embeddings are adjusted for better embedding; we\ncover weighted LLE for deformed distributed data, weighted LLE using\nprobability of occurrence, SLLE by adjusting weights, modified LLE, and\niterative LLE.\n", "versions": [{"version": "v1", "created": "Sun, 22 Nov 2020 03:44:45 GMT"}], "update_date": "2020-11-24", "authors_parsed": [["Ghojogh", "Benyamin", ""], ["Ghodsi", "Ali", ""], ["Karray", "Fakhri", ""], ["Crowley", "Mark", ""]]}, {"id": "2011.10927", "submitter": "Aayush Rana", "authors": "Aayush J Rana, Yogesh S Rawat", "title": "We don't Need Thousand Proposals$\\colon$ Single Shot Actor-Action\n  Detection in Videos", "comments": "8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We propose SSA2D, a simple yet effective end-to-end deep network for\nactor-action detection in videos. The existing methods take a top-down approach\nbased on region-proposals (RPN), where the action is estimated based on the\ndetected proposals followed by post-processing such as non-maximal suppression.\nWhile effective in terms of performance, these methods pose limitations in\nscalability for dense video scenes with a high memory requirement for thousands\nof proposals. We propose to solve this problem from a different perspective\nwhere we don't need any proposals. SSA2D is a unified network, which performs\npixel level joint actor-action detection in a single-shot, where every pixel of\nthe detected actor is assigned an action label. SSA2D has two main advantages:\n1) It is a fully convolutional network which does not require any proposals and\npost-processing making it memory as well as time efficient, 2) It is easily\nscalable to dense video scenes as its memory requirement is independent of the\nnumber of actors present in the scene. We evaluate the proposed method on the\nActor-Action dataset (A2D) and Video Object Relation (VidOR) dataset,\ndemonstrating its effectiveness in multiple actors and action detection in a\nvideo. SSA2D is 11x faster during inference with comparable (sometimes better)\nperformance and fewer network parameters when compared with the prior works.\n", "versions": [{"version": "v1", "created": "Sun, 22 Nov 2020 03:53:40 GMT"}], "update_date": "2020-11-24", "authors_parsed": [["Rana", "Aayush J", ""], ["Rawat", "Yogesh S", ""]]}, {"id": "2011.10934", "submitter": "Yiyuan Pan", "authors": "Yiyuan Pan, Xuecheng Xu, Weijie Li, Yunxiang Cui, Yue Wang, Rong Xiong", "title": "CORAL: Colored structural representation for bi-modal place recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Place recognition is indispensable for a drift-free localization system. Due\nto the variations of the environment, place recognition using single-modality\nhas limitations. In this paper, we propose a bi-modal place recognition method,\nwhich can extract a compound global descriptor from the two modalities, vision\nand LiDAR. Specifically, we first build the elevation image generated from 3D\npoints as a structural representation. Then, we derive the correspondences\nbetween 3D points and image pixels that are further used in merging the\npixel-wise visual features into the elevation map grids. In this way, we fuse\nthe structural features and visual features in the consistent bird-eye view\nframe, yielding a semantic representation, namely CORAL. And the whole network\nis called CORAL-VLAD. Comparisons on the Oxford RobotCar show that CORAL-VLAD\nhas superior performance against other state-of-the-art methods. We also\ndemonstrate that our network can be generalized to other scenes and sensor\nconfigurations on cross-city datasets.\n", "versions": [{"version": "v1", "created": "Sun, 22 Nov 2020 04:51:40 GMT"}, {"version": "v2", "created": "Mon, 19 Jul 2021 11:56:04 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Pan", "Yiyuan", ""], ["Xu", "Xuecheng", ""], ["Li", "Weijie", ""], ["Cui", "Yunxiang", ""], ["Wang", "Yue", ""], ["Xiong", "Rong", ""]]}, {"id": "2011.10944", "submitter": "Haizhou Shi", "authors": "Haizhou Shi, Dongliang Luo, Siliang Tang, Jian Wang, Yueting Zhuang", "title": "Run Away From your Teacher: Understanding BYOL by a Novel\n  Self-Supervised Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, a newly proposed self-supervised framework Bootstrap Your Own\nLatent (BYOL) seriously challenges the necessity of negative samples in\ncontrastive learning frameworks. BYOL works like a charm despite the fact that\nit discards the negative samples completely and there is no measure to prevent\ncollapse in its training objective. In this paper, we suggest understanding\nBYOL from the view of our proposed interpretable self-supervised learning\nframework, Run Away From your Teacher (RAFT). RAFT optimizes two objectives at\nthe same time: (i) aligning two views of the same data to similar\nrepresentations and (ii) running away from the model's Mean Teacher (MT, the\nexponential moving average of the history models) instead of BYOL's running\ntowards it. The second term of RAFT explicitly prevents the representation\ncollapse and thus makes RAFT a more conceptually reliable framework. We provide\nbasic benchmarks of RAFT on CIFAR10 to validate the effectiveness of our\nmethod. Furthermore, we prove that BYOL is equivalent to RAFT under certain\nconditions, providing solid reasoning for BYOL's counter-intuitive success.\n", "versions": [{"version": "v1", "created": "Sun, 22 Nov 2020 05:49:50 GMT"}], "update_date": "2020-11-24", "authors_parsed": [["Shi", "Haizhou", ""], ["Luo", "Dongliang", ""], ["Tang", "Siliang", ""], ["Wang", "Jian", ""], ["Zhuang", "Yueting", ""]]}, {"id": "2011.10949", "submitter": "Zhicheng Yan", "authors": "Zhicheng Yan, Xiaoliang Dai, Peizhao Zhang, Yuandong Tian, Bichen Wu,\n  Matt Feiszli", "title": "FP-NAS: Fast Probabilistic Neural Architecture Search", "comments": "CVPR 2021 camera-ready version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Differential Neural Architecture Search (NAS) requires all layer choices to\nbe held in memory simultaneously; this limits the size of both search space and\nfinal architecture. In contrast, Probabilistic NAS, such as PARSEC, learns a\ndistribution over high-performing architectures, and uses only as much memory\nas needed to train a single model. Nevertheless, it needs to sample many\narchitectures, making it computationally expensive for searching in an\nextensive space. To solve these problems, we propose a sampling method adaptive\nto the distribution entropy, drawing more samples to encourage explorations at\nthe beginning, and reducing samples as learning proceeds. Furthermore, to\nsearch fast in the multi-variate space, we propose a coarse-to-fine strategy by\nusing a factorized distribution at the beginning which can reduce the number of\narchitecture parameters by over an order of magnitude. We call this method Fast\nProbabilistic NAS (FP-NAS). Compared with PARSEC, it can sample 64% fewer\narchitectures and search 2.1x faster. Compared with FBNetV2, FP-NAS is 1.9x -\n3.5x faster, and the searched models outperform FBNetV2 models on ImageNet.\nFP-NAS allows us to expand the giant FBNetV2 space to be wider (i.e. larger\nchannel choices) and deeper (i.e. more blocks), while adding Split-Attention\nblock and enabling the search over the number of splits. When searching a model\nof size 0.4G FLOPS, FP-NAS is 132x faster than EfficientNet, and the searched\nFP-NAS-L0 model outperforms EfficientNet-B0 by 0.7% accuracy. Without using any\narchitecture surrogate or scaling tricks, we directly search large models up to\n1.0G FLOPS. Our FP-NAS-L2 model with simple distillation outperforms BigNAS-XL\nwith advanced in-place distillation by 0.7% accuracy using similar FLOPS.\n", "versions": [{"version": "v1", "created": "Sun, 22 Nov 2020 06:10:05 GMT"}, {"version": "v2", "created": "Tue, 24 Nov 2020 06:48:03 GMT"}, {"version": "v3", "created": "Wed, 31 Mar 2021 17:21:49 GMT"}], "update_date": "2021-04-01", "authors_parsed": [["Yan", "Zhicheng", ""], ["Dai", "Xiaoliang", ""], ["Zhang", "Peizhao", ""], ["Tian", "Yuandong", ""], ["Wu", "Bichen", ""], ["Feiszli", "Matt", ""]]}, {"id": "2011.10972", "submitter": "Weixia Zhang", "authors": "Weixia Zhang, Chao Ma, Qi Wu and Xiaokang Yang", "title": "Language-guided Navigation via Cross-Modal Grounding and Alternate\n  Adversarial Learning", "comments": "Accepted to IEEE TCSVT", "journal-ref": null, "doi": "10.1109/TCSVT.2020.3039522", "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The emerging vision-and-language navigation (VLN) problem aims at learning to\nnavigate an agent to the target location in unseen photo-realistic environments\naccording to the given language instruction. The main challenges of VLN arise\nmainly from two aspects: first, the agent needs to attend to the meaningful\nparagraphs of the language instruction corresponding to the dynamically-varying\nvisual environments; second, during the training process, the agent usually\nimitate the shortest-path to the target location. Due to the discrepancy of\naction selection between training and inference, the agent solely on the basis\nof imitation learning does not perform well. Sampling the next action from its\npredicted probability distribution during the training process allows the agent\nto explore diverse routes from the environments, yielding higher success rates.\nNevertheless, without being presented with the shortest navigation paths during\nthe training process, the agent may arrive at the target location through an\nunexpected longer route. To overcome these challenges, we design a cross-modal\ngrounding module, which is composed of two complementary attention mechanisms,\nto equip the agent with a better ability to track the correspondence between\nthe textual and visual modalities. We then propose to recursively alternate the\nlearning schemes of imitation and exploration to narrow the discrepancy between\ntraining and inference. We further exploit the advantages of both these two\nlearning schemes via adversarial learning. Extensive experimental results on\nthe Room-to-Room (R2R) benchmark dataset demonstrate that the proposed learning\nscheme is generalized and complementary to prior arts. Our method performs well\nagainst state-of-the-art approaches in terms of effectiveness and efficiency.\n", "versions": [{"version": "v1", "created": "Sun, 22 Nov 2020 09:13:46 GMT"}], "update_date": "2020-11-24", "authors_parsed": [["Zhang", "Weixia", ""], ["Ma", "Chao", ""], ["Wu", "Qi", ""], ["Yang", "Xiaokang", ""]]}, {"id": "2011.10974", "submitter": "Shuyang Gu", "authors": "Shuyang Gu, Jianmin Bao, Dong Chen", "title": "Learnable Sampling 3D Convolution for Video Enhancement and Action\n  Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A key challenge in video enhancement and action recognition is to fuse useful\ninformation from neighboring frames. Recent works suggest establishing accurate\ncorrespondences between neighboring frames before fusing temporal information.\nHowever, the generated results heavily depend on the quality of correspondence\nestimation. In this paper, we propose a more robust solution: \\emph{sampling\nand fusing multi-level features} across neighborhood frames to generate the\nresults. Based on this idea, we introduce a new module to improve the\ncapability of 3D convolution, namely, learnable sampling 3D convolution\n(\\emph{LS3D-Conv}). We add learnable 2D offsets to 3D convolution which aims to\nsample locations on spatial feature maps across frames. The offsets can be\nlearned for specific tasks. The \\emph{LS3D-Conv} can flexibly replace 3D\nconvolution layers in existing 3D networks and get new architectures, which\nlearns the sampling at multiple feature levels. The experiments on video\ninterpolation, video super-resolution, video denoising, and action recognition\ndemonstrate the effectiveness of our approach.\n", "versions": [{"version": "v1", "created": "Sun, 22 Nov 2020 09:20:49 GMT"}], "update_date": "2020-11-24", "authors_parsed": [["Gu", "Shuyang", ""], ["Bao", "Jianmin", ""], ["Chen", "Dong", ""]]}, {"id": "2011.11005", "submitter": "Xinzheng Zhang Prof.", "authors": "Xinzheng Zhang, Hang Su, Ce Zhang, Xiaowei Gu, Xiaoheng Tan, Peter M.\n  Atkinson", "title": "Robust Unsupervised Small Area Change Detection from SAR Imagery Using\n  Deep Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Small area change detection from synthetic aperture radar (SAR) is a highly\nchallenging task. In this paper, a robust unsupervised approach is proposed for\nsmall area change detection from multi-temporal SAR images using deep learning.\nFirst, a multi-scale superpixel reconstruction method is developed to generate\na difference image (DI), which can suppress the speckle noise effectively and\nenhance edges by exploiting local, spatially homogeneous information. Second, a\ntwo-stage centre-constrained fuzzy c-means clustering algorithm is proposed to\ndivide the pixels of the DI into changed, unchanged and intermediate classes\nwith a parallel clustering strategy. Image patches belonging to the first two\nclasses are then constructed as pseudo-label training samples, and image\npatches of the intermediate class are treated as testing samples. Finally, a\nconvolutional wavelet neural network (CWNN) is designed and trained to classify\ntesting samples into changed or unchanged classes, coupled with a deep\nconvolutional generative adversarial network (DCGAN) to increase the number of\nchanged class within the pseudo-label training samples. Numerical experiments\non four real SAR datasets demonstrate the validity and robustness of the\nproposed approach, achieving up to 99.61% accuracy for small area change\ndetection.\n", "versions": [{"version": "v1", "created": "Sun, 22 Nov 2020 12:50:08 GMT"}], "update_date": "2020-11-24", "authors_parsed": [["Zhang", "Xinzheng", ""], ["Su", "Hang", ""], ["Zhang", "Ce", ""], ["Gu", "Xiaowei", ""], ["Tan", "Xiaoheng", ""], ["Atkinson", "Peter M.", ""]]}, {"id": "2011.11007", "submitter": "Mehdi Khoshboresh-Masouleh", "authors": "Mohammad R. Bayanlou, Mehdi Khoshboresh-Masouleh", "title": "SAMA-VTOL: A new unmanned aircraft system for remotely sensed data\n  collection", "comments": "12 pages, 6 figures", "journal-ref": "Proceedings Volume 11525, SPIE Future Sensing Technologies;\n  115250V (2020)", "doi": "10.1117/12.2580533", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  In recent years, unmanned aircraft systems (UASs) are frequently used in many\ndifferent applications of photogrammetry such as building damage monitoring,\narchaeological mapping and vegetation monitoring. In this paper, a new\nstate-of-the-art vertical take-off and landing fixed-wing UAS is proposed to\nrobust photogrammetry missions, called SAMA-VTOL. In this study, the capability\nof SAMA-VTOL is investigated for generating orthophoto. The major stages are\nincluding designing, building and experimental scenario. First, a brief\ndescription of design and build is introduced. Next, an experiment was done to\ngenerate accurate orthophoto with minimum ground control points requirements.\nThe processing step, which includes automatic aerial triangulation with camera\ncalibration and model generation. In this regard, the Pix4Dmapper software was\nused to orientate the images, produce point clouds, creating digital surface\nmodel and generating orthophoto mosaic. Experimental results based on the test\narea covering 26.3 hectares indicate that our SAMA-VTOL performs well in the\northophoto mosaic task.\n", "versions": [{"version": "v1", "created": "Sun, 22 Nov 2020 12:55:16 GMT"}], "update_date": "2020-11-24", "authors_parsed": [["Bayanlou", "Mohammad R.", ""], ["Khoshboresh-Masouleh", "Mehdi", ""]]}, {"id": "2011.11015", "submitter": "Brett Roads", "authors": "Brett D. Roads, Bradley C. Love", "title": "Enriching ImageNet with Human Similarity Judgments and Psychological\n  Embeddings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Advances in object recognition flourished in part because of the availability\nof high-quality datasets and associated benchmarks. However, these\nbenchmarks---such as ILSVRC---are relatively task-specific, focusing\npredominately on predicting class labels. We introduce a publicly-available\ndataset that embodies the task-general capabilities of human perception and\nreasoning. The Human Similarity Judgments extension to ImageNet (ImageNet-HSJ)\nis composed of human similarity judgments that supplement the ILSVRC validation\nset. The new dataset supports a range of task and performance metrics,\nincluding the evaluation of unsupervised learning algorithms. We demonstrate\ntwo methods of assessment: using the similarity judgments directly and using a\npsychological embedding trained on the similarity judgments. This embedding\nspace contains an order of magnitude more points (i.e., images) than previous\nefforts based on human judgments. Scaling to the full 50,000 image set was made\npossible through a selective sampling process that used variational Bayesian\ninference and model ensembles to sample aspects of the embedding space that\nwere most uncertain. This methodological innovation not only enables scaling,\nbut should also improve the quality of solutions by focusing sampling where it\nis needed. To demonstrate the utility of ImageNet-HSJ, we used the similarity\nratings and the embedding space to evaluate how well several popular models\nconform to human similarity judgments. One finding is that more complex models\nthat perform better on task-specific benchmarks do not better conform to human\nsemantic judgments. In addition to the human similarity judgments, pre-trained\npsychological embeddings and code for inferring variational embeddings are made\npublicly available. Collectively, ImageNet-HSJ assets support the appraisal of\ninternal representations and the development of more human-like models.\n", "versions": [{"version": "v1", "created": "Sun, 22 Nov 2020 13:41:54 GMT"}], "update_date": "2020-11-24", "authors_parsed": [["Roads", "Brett D.", ""], ["Love", "Bradley C.", ""]]}, {"id": "2011.11020", "submitter": "Alberto Bartesaghi", "authors": "Qinwen Huang, Ye Zhou, Xiaochen Du, Reed Chen, Jianyou Wang, Cynthia\n  Rudin, Alberto Bartesaghi", "title": "Cryo-ZSSR: multiple-image super-resolution based on deep internal\n  learning", "comments": "11 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG q-bio.BM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Single-particle cryo-electron microscopy (cryo-EM) is an emerging imaging\nmodality capable of visualizing proteins and macro-molecular complexes at\nnear-atomic resolution. The low electron-doses used to prevent sample radiation\ndamage, result in images where the power of the noise is 100 times greater than\nthe power of the signal. To overcome the low-SNRs, hundreds of thousands of\nparticle projections acquired over several days of data collection are averaged\nin 3D to determine the structure of interest. Meanwhile, recent image\nsuper-resolution (SR) techniques based on neural networks have shown state of\nthe art performance on natural images. Building on these advances, we present a\nmultiple-image SR algorithm based on deep internal learning designed\nspecifically to work under low-SNR conditions. Our approach leverages the\ninternal image statistics of cryo-EM movies and does not require training on\nground-truth data. When applied to a single-particle dataset of apoferritin, we\nshow that the resolution of 3D structures obtained from SR micrographs can\nsurpass the limits imposed by the imaging system. Our results indicate that the\ncombination of low magnification imaging with image SR has the potential to\naccelerate cryo-EM data collection without sacrificing resolution.\n", "versions": [{"version": "v1", "created": "Sun, 22 Nov 2020 14:04:54 GMT"}], "update_date": "2020-11-24", "authors_parsed": [["Huang", "Qinwen", ""], ["Zhou", "Ye", ""], ["Du", "Xiaochen", ""], ["Chen", "Reed", ""], ["Wang", "Jianyou", ""], ["Rudin", "Cynthia", ""], ["Bartesaghi", "Alberto", ""]]}, {"id": "2011.11036", "submitter": "Jinjin Gu", "authors": "Jinjin Gu, Chao Dong", "title": "Interpreting Super-Resolution Networks with Local Attribution Maps", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Image super-resolution (SR) techniques have been developing rapidly,\nbenefiting from the invention of deep networks and its successive\nbreakthroughs. However, it is acknowledged that deep learning and deep neural\nnetworks are difficult to interpret. SR networks inherit this mysterious nature\nand little works make attempt to understand them. In this paper, we perform\nattribution analysis of SR networks, which aims at finding the input pixels\nthat strongly influence the SR results. We propose a novel attribution approach\ncalled local attribution map (LAM), which inherits the integral gradient method\nyet with two unique features. One is to use the blurred image as the baseline\ninput, and the other is to adopt the progressive blurring function as the path\nfunction. Based on LAM, we show that: (1) SR networks with a wider range of\ninvolved input pixels could achieve better performance. (2) Attention networks\nand non-local networks extract features from a wider range of input pixels. (3)\nComparing with the range that actually contributes, the receptive field is\nlarge enough for most deep networks. (4) For SR networks, textures with regular\nstripes or grids are more likely to be noticed, while complex semantics are\ndifficult to utilize. Our work opens new directions for designing SR networks\nand interpreting low-level vision deep models.\n", "versions": [{"version": "v1", "created": "Sun, 22 Nov 2020 15:11:00 GMT"}], "update_date": "2020-11-24", "authors_parsed": [["Gu", "Jinjin", ""], ["Dong", "Chao", ""]]}, {"id": "2011.11052", "submitter": "Ahror Belaid", "authors": "Hicham Messaoudi, Ahror Belaid, Mohamed Lamine Allaoui, Ahcene Zetout,\n  Mohand Said Allili, Souhil Tliba, Douraied Ben Salem, Pierre-Henri Conze", "title": "Efficient embedding network for 3D brain tumor segmentation", "comments": "Multimodal Brain Tumor Segmentation Challenge 2020", "journal-ref": "Multimodal Brain Tumor Segmentation Challenge 2020 (BRATS)\n  BrainLes 2020", "doi": null, "report-no": "30", "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  3D medical image processing with deep learning greatly suffers from a lack of\ndata. Thus, studies carried out in this field are limited compared to works\nrelated to 2D natural image analysis, where very large datasets exist. As a\nresult, powerful and efficient 2D convolutional neural networks have been\ndeveloped and trained. In this paper, we investigate a way to transfer the\nperformance of a two-dimensional classiffication network for the purpose of\nthree-dimensional semantic segmentation of brain tumors. We propose an\nasymmetric U-Net network by incorporating the EfficientNet model as part of the\nencoding branch. As the input data is in 3D, the first layers of the encoder\nare devoted to the reduction of the third dimension in order to fit the input\nof the EfficientNet network. Experimental results on validation and test data\nfrom the BraTS 2020 challenge demonstrate that the proposed method achieve\npromising performance.\n", "versions": [{"version": "v1", "created": "Sun, 22 Nov 2020 16:17:29 GMT"}], "update_date": "2020-11-24", "authors_parsed": [["Messaoudi", "Hicham", ""], ["Belaid", "Ahror", ""], ["Allaoui", "Mohamed Lamine", ""], ["Zetout", "Ahcene", ""], ["Allili", "Mohand Said", ""], ["Tliba", "Souhil", ""], ["Salem", "Douraied Ben", ""], ["Conze", "Pierre-Henri", ""]]}, {"id": "2011.11058", "submitter": "Shashi Kant Gupta", "authors": "Shivi Gupta, Shashi Kant Gupta", "title": "Investigating Emotion-Color Association in Deep Neural Networks", "comments": "8 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It has been found that representations learned by Deep Neural Networks (DNNs)\ncorrelate very well to neural responses measured in primates' brains and\npsychological representations exhibited by human similarity judgment. On\nanother hand, past studies have shown that particular colors can be associated\nwith specific emotion arousal in humans. Do deep neural networks also learn\nthis behavior? In this study, we investigate if DNNs can learn implicit\nassociations in stimuli, particularly, an emotion-color association between\nimage stimuli. Our study was conducted in two parts. First, we collected human\nresponses on a forced-choice decision task in which subjects were asked to\nselect a color for a specified emotion-inducing image. Next, we modeled this\ndecision task on neural networks using the similarity between deep\nrepresentation (extracted using DNNs trained on object classification tasks) of\nthe images and images of colors used in the task. We found that our model\nshowed a fuzzy linear relationship between the two decision probabilities. This\nresults in two interesting findings, 1. The representations learned by deep\nneural networks can indeed show an emotion-color association 2. The\nemotion-color association is not just random but involves some cognitive\nphenomena. Finally, we also show that this method can help us in the emotion\nclassification task, specifically when there are very few examples to train the\nmodel. This analysis can be relevant to psychologists studying emotion-color\nassociations and artificial intelligence researchers modeling emotional\nintelligence in machines or studying representations learned by deep neural\nnetworks.\n", "versions": [{"version": "v1", "created": "Sun, 22 Nov 2020 16:48:02 GMT"}], "update_date": "2020-11-24", "authors_parsed": [["Gupta", "Shivi", ""], ["Gupta", "Shashi Kant", ""]]}, {"id": "2011.11060", "submitter": "Oleg Lobachev", "authors": "Oleg Lobachev, Takuya Funatomi, Alexander Pfaffenroth, Reinhold\n  F\\\"orster, Lars Knudsen, Christoph Wrede, Michael Guthe, David Haberth\\\"ur,\n  Ruslan Hlushchuk, Thomas Salaets, Jaan Toelen, Simone Gaffling, Christian\n  M\\\"uhlfeld, Roman Grothausmann", "title": "Registration of serial sections: An evaluation method based on\n  distortions of the ground truths", "comments": "Supplemental data available under https://zenodo.org/record/4282448", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Registration of histological serial sections is a challenging task. Serial\nsections exhibit distortions and damage from sectioning. Missing information on\nhow the tissue looked before cutting makes a realistic validation of 2D\nregistrations extremely difficult.\n  This work proposes methods for ground-truth-based evaluation of\nregistrations. Firstly, we present a methodology to generate test data for\nregistrations. We distort an innately registered image stack in the manner\nsimilar to the cutting distortion of serial sections. Test cases are generated\nfrom existing 3D data sets, thus the ground truth is known. Secondly, our test\ncase generation premises evaluation of the registrations with known ground\ntruths. Our methodology for such an evaluation technique distinguishes this\nwork from other approaches. Both under- and over-registration become evident in\nour evaluations. We also survey existing validation efforts.\n  We present a full-series evaluation across six different registration methods\napplied to our distorted 3D data sets of animal lungs. Our distorted and ground\ntruth data sets are made publicly available.\n", "versions": [{"version": "v1", "created": "Sun, 22 Nov 2020 16:50:52 GMT"}, {"version": "v2", "created": "Sun, 6 Jun 2021 18:01:02 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Lobachev", "Oleg", ""], ["Funatomi", "Takuya", ""], ["Pfaffenroth", "Alexander", ""], ["F\u00f6rster", "Reinhold", ""], ["Knudsen", "Lars", ""], ["Wrede", "Christoph", ""], ["Guthe", "Michael", ""], ["Haberth\u00fcr", "David", ""], ["Hlushchuk", "Ruslan", ""], ["Salaets", "Thomas", ""], ["Toelen", "Jaan", ""], ["Gaffling", "Simone", ""], ["M\u00fchlfeld", "Christian", ""], ["Grothausmann", "Roman", ""]]}, {"id": "2011.11067", "submitter": "Pratik Mazumder", "authors": "Pratik Mazumder, Pravendra Singh, Vinay P. Namboodiri", "title": "RNNP: A Robust Few-Shot Learning Approach", "comments": "Accepted in WACV 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning from a few examples is an important practical aspect of training\nclassifiers. Various works have examined this aspect quite well. However, all\nexisting approaches assume that the few examples provided are always correctly\nlabeled. This is a strong assumption, especially if one considers the current\ntechniques for labeling using crowd-based labeling services. We address this\nissue by proposing a novel robust few-shot learning approach. Our method relies\non generating robust prototypes from a set of few examples. Specifically, our\nmethod refines the class prototypes by producing hybrid features from the\nsupport examples of each class. The refined prototypes help to classify the\nquery images better. Our method can replace the evaluation phase of any\nfew-shot learning method that uses a nearest neighbor prototype-based\nevaluation procedure to make them robust. We evaluate our method on standard\nmini-ImageNet and tiered-ImageNet datasets. We perform experiments with various\nlabel corruption rates in the support examples of the few-shot classes. We\nobtain significant improvement over widely used few-shot learning methods that\nsuffer significant performance degeneration in the presence of label noise. We\nfinally provide extensive ablation experiments to validate our method.\n", "versions": [{"version": "v1", "created": "Sun, 22 Nov 2020 17:23:08 GMT"}], "update_date": "2020-11-24", "authors_parsed": [["Mazumder", "Pratik", ""], ["Singh", "Pravendra", ""], ["Namboodiri", "Vinay P.", ""]]}, {"id": "2011.11071", "submitter": "Andreea-Maria Oncescu", "authors": "Andreea-Maria Oncescu, Jo\\~ao F. Henriques, Yang Liu, Andrew\n  Zisserman, Samuel Albanie", "title": "QuerYD: A video dataset with high-quality text and audio narrations", "comments": "5 pages, 4 figures, accepted at ICASSP 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce QuerYD, a new large-scale dataset for retrieval and event\nlocalisation in video. A unique feature of our dataset is the availability of\ntwo audio tracks for each video: the original audio, and a high-quality spoken\ndescription of the visual content. The dataset is based on YouDescribe, a\nvolunteer project that assists visually-impaired people by attaching voiced\nnarrations to existing YouTube videos. This ever-growing collection of videos\ncontains highly detailed, temporally aligned audio and text annotations. The\ncontent descriptions are more relevant than dialogue, and more detailed than\nprevious description attempts, which can be observed to contain many\nsuperficial or uninformative descriptions. To demonstrate the utility of the\nQuerYD dataset, we show that it can be used to train and benchmark strong\nmodels for retrieval and event localisation. Data, code and models are made\npublicly available, and we hope that QuerYD inspires further research on video\nunderstanding with written and spoken natural language.\n", "versions": [{"version": "v1", "created": "Sun, 22 Nov 2020 17:33:44 GMT"}, {"version": "v2", "created": "Wed, 17 Feb 2021 13:38:19 GMT"}], "update_date": "2021-02-18", "authors_parsed": [["Oncescu", "Andreea-Maria", ""], ["Henriques", "Jo\u00e3o F.", ""], ["Liu", "Yang", ""], ["Zisserman", "Andrew", ""], ["Albanie", "Samuel", ""]]}, {"id": "2011.11078", "submitter": "Anshul Gupta", "authors": "Anshul Gupta, Joydeep Medhi, Aratrik Chattopadhyay, Vikram Gupta", "title": "End-to-End Differentiable 6DoF Object Pose Estimation with Local and\n  Global Constraints", "comments": "Accepted at the Workshop on Differentiable vision, graphics, and\n  physics applied to machine learning at Neurips 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inferring the 6DoF pose of an object from a single RGB image is an important\nbut challenging task, especially under heavy occlusion. While recent approaches\nimprove upon the two stage approaches by training an end-to-end pipeline, they\ndo not leverage local and global constraints. In this paper, we propose\npairwise feature extraction to integrate local constraints, and triplet\nregularization to integrate global constraints for improved 6DoF object pose\nestimation. Coupled with better augmentation, our approach achieves state of\nthe art results on the challenging Occlusion Linemod dataset, with a 9%\nimprovement over the previous state of the art, and achieves competitive\nresults on the Linemod dataset.\n", "versions": [{"version": "v1", "created": "Sun, 22 Nov 2020 18:02:25 GMT"}], "update_date": "2020-11-24", "authors_parsed": [["Gupta", "Anshul", ""], ["Medhi", "Joydeep", ""], ["Chattopadhyay", "Aratrik", ""], ["Gupta", "Vikram", ""]]}, {"id": "2011.11081", "submitter": "Limin Yu", "authors": "Junli Cao, B.S., Junyan Wu, M.S., Jing W. Zhang, M.D., Ph.D., Jay J.\n  Ye, M.D., Ph.D., Limin Yu, M.D., M.S", "title": "Deep learning model trained on mobile phone-acquired frozen section\n  images effectively detects basal cell carcinoma", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Background: Margin assessment of basal cell carcinoma using the frozen\nsection is a common task of pathology intraoperative consultation. Although\nfrequently straight-forward, the determination of the presence or absence of\nbasal cell carcinoma on the tissue sections can sometimes be challenging. We\nexplore if a deep learning model trained on mobile phone-acquired frozen\nsection images can have adequate performance for future deployment. Materials\nand Methods: One thousand two hundred and forty-one (1241) images of frozen\nsections performed for basal cell carcinoma margin status were acquired using\nmobile phones. The photos were taken at 100x magnification (10x objective). The\nimages were downscaled from a 4032 x 3024 pixel resolution to 576 x 432 pixel\nresolution. Semantic segmentation algorithm Deeplab V3 with Xception backbone\nwas used for model training. Results: The model uses an image as input and\nproduces a 2-dimensional black and white output of prediction of the same\ndimension; the areas determined to be basal cell carcinoma were displayed with\nwhite color, in a black background. Any output with the number of white pixels\nexceeding 0.5% of the total number of pixels is deemed positive for basal cell\ncarcinoma. On the test set, the model achieves area under curve of 0.99 for\nreceiver operator curve and 0.97 for precision-recall curve at the pixel level.\nThe accuracy of classification at the slide level is 96%. Conclusions: The deep\nlearning model trained with mobile phone images shows satisfactory performance\ncharacteristics, and thus demonstrates the potential for deploying as a mobile\nphone app to assist in frozen section interpretation in real time.\n", "versions": [{"version": "v1", "created": "Sun, 22 Nov 2020 18:30:23 GMT"}], "update_date": "2020-11-24", "authors_parsed": [["Cao", "Junli", ""], ["S.", "B.", ""], ["Wu", "Junyan", ""], ["S.", "M.", ""], ["Zhang", "Jing W.", ""], ["D.", "M.", ""], ["D.", "Ph.", ""], ["Ye", "Jay J.", ""], ["D.", "M.", ""], ["D.", "Ph.", ""], ["Yu", "Limin", ""], ["D.", "M.", ""], ["S", "M.", ""]]}, {"id": "2011.11094", "submitter": "Matej Grcic", "authors": "Matej Grci\\'c, Petra Bevandi\\'c and Sini\\v{s}a \\v{S}egvi\\'c", "title": "Dense open-set recognition with synthetic outliers generated by Real NVP", "comments": "Accepted to VISAPP 2021 conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Today's deep models are often unable to detect inputs which do not belong to\nthe training distribution. This gives rise to confident incorrect predictions\nwhich could lead to devastating consequences in many important application\nfields such as healthcare and autonomous driving. Interestingly, both\ndiscriminative and generative models appear to be equally affected.\nConsequently, this vulnerability represents an important research challenge. We\nconsider an outlier detection approach based on discriminative training with\njointly learned synthetic outliers. We obtain the synthetic outliers by\nsampling an RNVP model which is jointly trained to generate datapoints at the\nborder of the training distribution. We show that this approach can be adapted\nfor simultaneous semantic segmentation and dense outlier detection. We present\nimage classification experiments on CIFAR-10, as well as semantic segmentation\nexperiments on three existing datasets (StreetHazards, WD-Pascal, Fishyscapes\nLost & Found), and one contributed dataset. Our models perform competitively\nwith respect to the state of the art despite producing predictions with only\none forward pass.\n", "versions": [{"version": "v1", "created": "Sun, 22 Nov 2020 19:40:26 GMT"}], "update_date": "2020-11-24", "authors_parsed": [["Grci\u0107", "Matej", ""], ["Bevandi\u0107", "Petra", ""], ["\u0160egvi\u0107", "Sini\u0161a", ""]]}, {"id": "2011.11108", "submitter": "Mohammadreza Salehi Dehnavi", "authors": "Mohammadreza Salehi, Niousha Sadjadi, Soroosh Baselizadeh, Mohammad\n  Hossein Rohban, Hamid R. Rabiee", "title": "Multiresolution Knowledge Distillation for Anomaly Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unsupervised representation learning has proved to be a critical component of\nanomaly detection/localization in images. The challenges to learn such a\nrepresentation are two-fold. Firstly, the sample size is not often large enough\nto learn a rich generalizable representation through conventional techniques.\nSecondly, while only normal samples are available at training, the learned\nfeatures should be discriminative of normal and anomalous samples. Here, we\npropose to use the \"distillation\" of features at various layers of an expert\nnetwork, pre-trained on ImageNet, into a simpler cloner network to tackle both\nissues. We detect and localize anomalies using the discrepancy between the\nexpert and cloner networks' intermediate activation values given the input\ndata. We show that considering multiple intermediate hints in distillation\nleads to better exploiting the expert's knowledge and more distinctive\ndiscrepancy compared to solely utilizing the last layer activation values.\nNotably, previous methods either fail in precise anomaly localization or need\nexpensive region-based training. In contrast, with no need for any special or\nintensive training procedure, we incorporate interpretability algorithms in our\nnovel framework for the localization of anomalous regions. Despite the striking\ncontrast between some test datasets and ImageNet, we achieve competitive or\nsignificantly superior results compared to the SOTA methods on MNIST, F-MNIST,\nCIFAR-10, MVTecAD, Retinal-OCT, and two Medical datasets on both anomaly\ndetection and localization.\n", "versions": [{"version": "v1", "created": "Sun, 22 Nov 2020 21:16:35 GMT"}], "update_date": "2020-11-24", "authors_parsed": [["Salehi", "Mohammadreza", ""], ["Sadjadi", "Niousha", ""], ["Baselizadeh", "Soroosh", ""], ["Rohban", "Mohammad Hossein", ""], ["Rabiee", "Hamid R.", ""]]}, {"id": "2011.11155", "submitter": "Hao Zhu", "authors": "Hao Zhu, Yang Yuan, Guosheng Hu, Xiang Wu, Neil Robertson", "title": "Imbalance Robust Softmax for Deep Embeeding Learning", "comments": "has been accepted by ACCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Deep embedding learning is expected to learn a metric space in which features\nhave smaller maximal intra-class distance than minimal inter-class distance. In\nrecent years, one research focus is to solve the open-set problem by\ndiscriminative deep embedding learning in the field of face recognition (FR)\nand person re-identification (re-ID). Apart from open-set problem, we find that\nimbalanced training data is another main factor causing the performance\ndegradation of FR and re-ID, and data imbalance widely exists in the real\napplications. However, very little research explores why and how data imbalance\ninfluences the performance of FR and re-ID with softmax or its variants. In\nthis work, we deeply investigate data imbalance in the perspective of neural\nnetwork optimisation and feature distribution about softmax. We find one main\nreason of performance degradation caused by data imbalance is that the weights\n(from the penultimate fully-connected layer) are far from their class centers\nin feature space. Based on this investigation, we propose a unified framework,\nImbalance-Robust Softmax (IR-Softmax), which can simultaneously solve the\nopen-set problem and reduce the influence of data imbalance. IR-Softmax can\ngeneralise to any softmax and its variants (which are discriminative for\nopen-set problem) by directly setting the weights as their class centers,\nnaturally solving the data imbalance problem. In this work, we explicitly\nre-formulate two discriminative softmax (A-Softmax and AM-Softmax) under the\nframework of IR-Softmax. We conduct extensive experiments on FR databases (LFW,\nMegaFace) and re-ID database (Market-1501, Duke), and IR-Softmax outperforms\nmany state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Mon, 23 Nov 2020 00:43:07 GMT"}], "update_date": "2020-11-24", "authors_parsed": [["Zhu", "Hao", ""], ["Yuan", "Yang", ""], ["Hu", "Guosheng", ""], ["Wu", "Xiang", ""], ["Robertson", "Neil", ""]]}, {"id": "2011.11156", "submitter": "Divya Shanmugam", "authors": "Divya Shanmugam, Davis Blalock, Guha Balakrishnan, John Guttag", "title": "When and Why Test-Time Augmentation Works", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Test-time augmentation (TTA)---the aggregation of predictions across\ntransformed versions of a test input---is a common practice in image\nclassification. In this paper, we present theoretical and experimental analyses\nthat shed light on 1) when test time augmentation is likely to be helpful and\n2) when to use various test-time augmentation policies. A key finding is that\neven when TTA produces a net improvement in accuracy, it can change many\ncorrect predictions into incorrect predictions. We delve into when and why\ntest-time augmentation changes a prediction from being correct to incorrect and\nvice versa. Our analysis suggests that the nature and amount of training data,\nthe model architecture, and the augmentation policy all matter. Building on\nthese insights, we present a learning-based method for aggregating test-time\naugmentations. Experiments across a diverse set of models, datasets, and\naugmentations show that our method delivers consistent improvements over\nexisting approaches.\n", "versions": [{"version": "v1", "created": "Mon, 23 Nov 2020 00:46:00 GMT"}], "update_date": "2020-11-24", "authors_parsed": [["Shanmugam", "Divya", ""], ["Blalock", "Davis", ""], ["Balakrishnan", "Guha", ""], ["Guttag", "John", ""]]}, {"id": "2011.11164", "submitter": "Cui Jiequan", "authors": "Jiequan Cui, Shu Liu, Liwei Wang, Jiaya Jia", "title": "Learnable Boundary Guided Adversarial Training", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Previous adversarial training raises model robustness under the compromise of\naccuracy on natural data. In this paper, our target is to reduce natural\naccuracy degradation. We use the model logits from one clean model\n$\\mathcal{M}^{natural}$ to guide learning of the robust model\n$\\mathcal{M}^{robust}$, taking into consideration that logits from the well\ntrained clean model $\\mathcal{M}^{natural}$ embed the most discriminative\nfeatures of natural data, {\\it e.g.}, generalizable classifier boundary. Our\nsolution is to constrain logits from the robust model $\\mathcal{M}^{robust}$\nthat takes adversarial examples as input and make it similar to those from a\nclean model $\\mathcal{M}^{natural}$ fed with corresponding natural data. It\nlets $\\mathcal{M}^{robust}$ inherit the classifier boundary of\n$\\mathcal{M}^{natural}$. Thus, we name our method Boundary Guided Adversarial\nTraining (BGAT). Moreover, we generalize BGAT to Learnable Boundary Guided\nAdversarial Training (LBGAT) by training $\\mathcal{M}^{natural}$ and\n$\\mathcal{M}^{robust}$ simultaneously and collaboratively to learn one most\nrobustness-friendly classifier boundary for the strongest robustness. Extensive\nexperiments are conducted on CIFAR-10, CIFAR-100, and challenging Tiny ImageNet\ndatasets. Along with other state-of-the-art adversarial training approaches,\n{\\it e.g.}, Adversarial Logit Pairing (ALP) and TRADES, the performance is\nfurther enhanced.\n", "versions": [{"version": "v1", "created": "Mon, 23 Nov 2020 01:36:05 GMT"}], "update_date": "2020-11-24", "authors_parsed": [["Cui", "Jiequan", ""], ["Liu", "Shu", ""], ["Wang", "Liwei", ""], ["Jia", "Jiaya", ""]]}, {"id": "2011.11167", "submitter": "Edward Kim", "authors": "Edward Kim, Maryam Daniali, Jocelyn Rego, Garrett T. Kenyon", "title": "The Selectivity and Competition of the Mind's Eye in Visual Perception", "comments": "8 pages, under review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Research has shown that neurons within the brain are selective to certain\nstimuli. For example, the fusiform face area (FFA) region is known by\nneuroscientists to selectively activate when people see faces over non-face\nobjects. However, the mechanisms by which the primary visual system directs\ninformation to the correct higher levels of the brain are currently unknown. In\nour work, we mimic several high-level neural mechanisms of perception by\ncreating a novel computational model that incorporates lateral and top down\nfeedback in the form of hierarchical competition. Not only do we show that\nthese elements can help explain the information flow and selectivity of high\nlevel areas within the brain, we also demonstrate that these neural mechanisms\nprovide the foundation of a novel classification framework that rivals\ntraditional supervised learning in computer vision. Additionally, we present\nboth quantitative and qualitative results that demonstrate that our generative\nframework is consistent with neurological themes and enables simple, yet robust\ncategory level classification.\n", "versions": [{"version": "v1", "created": "Mon, 23 Nov 2020 01:55:46 GMT"}, {"version": "v2", "created": "Tue, 23 Mar 2021 15:43:27 GMT"}], "update_date": "2021-03-24", "authors_parsed": [["Kim", "Edward", ""], ["Daniali", "Maryam", ""], ["Rego", "Jocelyn", ""], ["Kenyon", "Garrett T.", ""]]}, {"id": "2011.11183", "submitter": "Junnan Li Dr", "authors": "Junnan Li, Caiming Xiong, Steven Hoi", "title": "CoMatch: Semi-supervised Learning with Contrastive Graph Regularization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semi-supervised learning has been an effective paradigm for leveraging\nunlabeled data to reduce the reliance on labeled data. We propose CoMatch, a\nnew semi-supervised learning method that unifies dominant approaches and\naddresses their limitations. CoMatch jointly learns two representations of the\ntraining data, their class probabilities and low-dimensional embeddings. The\ntwo representations interact with each other to jointly evolve. The embeddings\nimpose a smoothness constraint on the class probabilities to improve the\npseudo-labels, whereas the pseudo-labels regularize the structure of the\nembeddings through graph-based contrastive learning. CoMatch achieves\nstate-of-the-art performance on multiple datasets. It achieves substantial\naccuracy improvements on the label-scarce CIFAR-10 and STL-10. On ImageNet with\n1% labels, CoMatch achieves a top-1 accuracy of 66.0%, outperforming FixMatch\nby 12.6%. Furthermore, CoMatch achieves better representation learning\nperformance on downstream tasks, outperforming both supervised learning and\nself-supervised learning. Code and pre-trained models are available at\nhttps://github.com/salesforce/CoMatch.\n", "versions": [{"version": "v1", "created": "Mon, 23 Nov 2020 02:54:57 GMT"}, {"version": "v2", "created": "Wed, 3 Mar 2021 01:58:15 GMT"}], "update_date": "2021-03-04", "authors_parsed": [["Li", "Junnan", ""], ["Xiong", "Caiming", ""], ["Hoi", "Steven", ""]]}, {"id": "2011.11186", "submitter": "Ziliang Zhong", "authors": "Ziliang Zhong, Muhang Zheng, Huafeng Mai, Jianan Zhao, Xinyi Liu", "title": "Cancer image classification based on DenseNet model", "comments": null, "journal-ref": "2004-present Journal of Physics: Conference Series", "doi": "10.1088/1742-6596/1651/1/012143", "report-no": null, "categories": "cs.CV stat.AP stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Computer-aided diagnosis establishes methods for robust assessment of medical\nimage-based examination. Image processing introduced a promising strategy to\nfacilitate disease classification and detection while diminishing unnecessary\nexpenses. In this paper, we propose a novel metastatic cancer image\nclassification model based on DenseNet Block, which can effectively identify\nmetastatic cancer in small image patches taken from larger digital pathology\nscans. We evaluate the proposed approach to the slightly modified version of\nthe PatchCamelyon (PCam) benchmark dataset. The dataset is the slightly\nmodified version of the PatchCamelyon (PCam) benchmark dataset provided by\nKaggle competition, which packs the clinically-relevant task of metastasis\ndetection into a straight-forward binary image classification task. The\nexperiments indicated that our model outperformed other classical methods like\nResnet34, Vgg19. Moreover, we also conducted data augmentation experiment and\nstudy the relationship between Batches processed and loss value during the\ntraining and validation process.\n", "versions": [{"version": "v1", "created": "Mon, 23 Nov 2020 03:05:42 GMT"}], "update_date": "2020-12-30", "authors_parsed": [["Zhong", "Ziliang", ""], ["Zheng", "Muhang", ""], ["Mai", "Huafeng", ""], ["Zhao", "Jianan", ""], ["Liu", "Xinyi", ""]]}, {"id": "2011.11190", "submitter": "Kunming Li", "authors": "Kunming Li, Stuart Eiffert, Mao Shan, Francisco Gomez-Donoso, Stewart\n  Worrall and Eduardo Nebot", "title": "Attentional-GCNN: Adaptive Pedestrian Trajectory Prediction towards\n  Generic Autonomous Vehicle Use Cases", "comments": "8 pages, 5 figures, submitted to ICRA 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Autonomous vehicle navigation in shared pedestrian environments requires the\nability to predict future crowd motion both accurately and with minimal delay.\nUnderstanding the uncertainty of the prediction is also crucial. Most existing\napproaches however can only estimate uncertainty through repeated sampling of\ngenerative models. Additionally, most current predictive models are trained on\ndatasets that assume complete observability of the crowd using an aerial view.\nThese are generally not representative of real-world usage from a vehicle\nperspective, and can lead to the underestimation of uncertainty bounds when the\non-board sensors are occluded. Inspired by prior work in motion prediction\nusing spatio-temporal graphs, we propose a novel Graph Convolutional Neural\nNetwork (GCNN)-based approach, Attentional-GCNN, which aggregates information\nof implicit interaction between pedestrians in a crowd by assigning attention\nweight in edges of the graph. Our model can be trained to either output a\nprobabilistic distribution or faster deterministic prediction, demonstrating\napplicability to autonomous vehicle use cases where either speed or accuracy\nwith uncertainty bounds are required. To further improve the training of\npredictive models, we propose an automatically labelled pedestrian dataset\ncollected from an intelligent vehicle platform representative of real-world\nuse. Through experiments on a number of datasets, we show our proposed method\nachieves an improvement over the state of art by 10% Average Displacement Error\n(ADE) and 12% Final Displacement Error (FDE) with fast inference speeds.\n", "versions": [{"version": "v1", "created": "Mon, 23 Nov 2020 03:13:26 GMT"}], "update_date": "2020-11-24", "authors_parsed": [["Li", "Kunming", ""], ["Eiffert", "Stuart", ""], ["Shan", "Mao", ""], ["Gomez-Donoso", "Francisco", ""], ["Worrall", "Stewart", ""], ["Nebot", "Eduardo", ""]]}, {"id": "2011.11193", "submitter": "Mohammad Golbabaee", "authors": "Mohammad Golbabaee and Clarice Poon", "title": "An off-the-grid approach to multi-compartment magnetic resonance\n  fingerprinting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.NA math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel numerical approach to separate multiple tissue\ncompartments in image voxels and to estimate quantitatively their nuclear\nmagnetic resonance (NMR) properties and mixture fractions, given magnetic\nresonance fingerprinting (MRF) measurements. The number of tissues, their types\nor quantitative properties are not a-priori known, but the image is assumed to\nbe composed of sparse compartments with linearly mixed Bloch magnetisation\nresponses within voxels. Fine-grid discretisation of the multi-dimensional NMR\nproperties creates large and highly coherent MRF dictionaries that can\nchallenge scalability and precision of the numerical methods for (discrete)\nsparse approximation. To overcome these issues, we propose an off-the-grid\napproach equipped with an extended notion of the sparse group lasso\nregularisation for sparse approximation using continuous (non-discretised)\nBloch response models. Further, the nonlinear and non-analytical Bloch\nresponses are approximated by a neural network, enabling efficient\nback-propagation of the gradients through the proposed algorithm. Tested on\nsimulated and in-vivo healthy brain MRF data, we demonstrate effectiveness of\nthe proposed scheme compared to the baseline multicompartment MRF methods.\n", "versions": [{"version": "v1", "created": "Mon, 23 Nov 2020 03:16:55 GMT"}], "update_date": "2020-11-24", "authors_parsed": [["Golbabaee", "Mohammad", ""], ["Poon", "Clarice", ""]]}, {"id": "2011.11194", "submitter": "Xiang Fang", "authors": "Xiang Fang, Yuchong Hu, Pan Zhou, Dapeng Oliver Wu", "title": "V3H: View Variation and View Heredity for Incomplete Multi-view\n  Clustering", "comments": "Publisheded in IEEE Transactions on Artificial Intelligence", "journal-ref": "IEEE Transactions on Artificial Intelligence 2020", "doi": "10.1109/TAI.2021.3052425", "report-no": "vol. 1, no. 3, pp. 233-247, Dec. 2020", "categories": "cs.LG cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Real data often appear in the form of multiple incomplete views. Incomplete\nmulti-view clustering is an effective method to integrate these incomplete\nviews. Previous methods only learn the consistent information between different\nviews and ignore the unique information of each view, which limits their\nclustering performance and generalizations. To overcome this limitation, we\npropose a novel View Variation and View Heredity approach (V3H). Inspired by\nthe variation and the heredity in genetics, V3H first decomposes each subspace\ninto a variation matrix for the corresponding view and a heredity matrix for\nall the views to represent the unique information and the consistent\ninformation respectively. Then, by aligning different views based on their\ncluster indicator matrices, V3H integrates the unique information from\ndifferent views to improve the clustering performance. Finally, with the help\nof the adjustable low-rank representation based on the heredity matrix, V3H\nrecovers the underlying true data structure to reduce the influence of the\nlarge incompleteness. More importantly, V3H presents possibly the first work to\nintroduce genetics to clustering algorithms for learning simultaneously the\nconsistent information and the unique information from incomplete multi-view\ndata. Extensive experimental results on fifteen benchmark datasets validate its\nsuperiority over other state-of-the-arts.\n", "versions": [{"version": "v1", "created": "Mon, 23 Nov 2020 03:24:48 GMT"}, {"version": "v2", "created": "Thu, 14 Jan 2021 16:47:52 GMT"}, {"version": "v3", "created": "Fri, 30 Apr 2021 08:34:39 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Fang", "Xiang", ""], ["Hu", "Yuchong", ""], ["Zhou", "Pan", ""], ["Wu", "Dapeng Oliver", ""]]}, {"id": "2011.11198", "submitter": "Kien Nguyen Thanh", "authors": "Kien Nguyen, Clinton Fookes, Sridha Sridharan, Arun Ross", "title": "Complex-valued Iris Recognition Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In this work, we design a complex-valued neural network for the task of iris\nrecognition. Unlike the problem of general object recognition, where\nreal-valued neural networks can be used to extract pertinent features, iris\nrecognition depends on the extraction of both phase and amplitude information\nfrom the input iris texture in order to better represent its stochastic\ncontent. This necessitates the extraction and processing of phase information\nthat cannot be effectively handled by a real-valued neural network. In this\nregard, we design a complex-valued neural network that can better capture the\nmulti-scale, multi-resolution, and multi-orientation phase and amplitude\nfeatures of the iris texture. We show a strong correspondence of the proposed\ncomplex-valued iris recognition network with Gabor wavelets that are used to\ngenerate the classical IrisCode; however, the proposed method enables automatic\ncomplex-valued feature learning that is tailored for iris recognition.\nExperiments conducted on three benchmark datasets - ND-CrossSensor-2013,\nCASIA-Iris-Thousand and UBIRIS.v2 - show the benefit of the proposed network\nfor the task of iris recognition. Further, the generalization capability of the\nproposed network is demonstrated by training and testing it across different\ndatasets. Finally, visualization schemes are used to convey the type of\nfeatures being extracted by the complex-valued network in comparison to\nclassical real-valued networks. The results of this work are likely to be\napplicable in other domains, where complex Gabor filters are used for texture\nmodeling.\n", "versions": [{"version": "v1", "created": "Mon, 23 Nov 2020 03:36:18 GMT"}], "update_date": "2020-11-24", "authors_parsed": [["Nguyen", "Kien", ""], ["Fookes", "Clinton", ""], ["Sridharan", "Sridha", ""], ["Ross", "Arun", ""]]}, {"id": "2011.11200", "submitter": "Yandong Li", "authors": "Yandong Li, Xuhui Jia, Ruoxin Sang, Yukun Zhu, Bradley Green, Liqiang\n  Wang, Boqing Gong", "title": "Ranking Neural Checkpoints", "comments": "Accepted to CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper is concerned with ranking many pre-trained deep neural networks\n(DNNs), called checkpoints, for the transfer learning to a downstream task.\nThanks to the broad use of DNNs, we may easily collect hundreds of checkpoints\nfrom various sources. Which of them transfers the best to our downstream task\nof interest? Striving to answer this question thoroughly, we establish a neural\ncheckpoint ranking benchmark (NeuCRaB) and study some intuitive ranking\nmeasures. These measures are generic, applying to the checkpoints of different\noutput types without knowing how the checkpoints are pre-trained on which\ndataset. They also incur low computation cost, making them practically\nmeaningful. Our results suggest that the linear separability of the features\nextracted by the checkpoints is a strong indicator of transferability. We also\narrive at a new ranking measure, NLEEP, which gives rise to the best\nperformance in the experiments.\n", "versions": [{"version": "v1", "created": "Mon, 23 Nov 2020 04:05:46 GMT"}, {"version": "v2", "created": "Tue, 30 Mar 2021 15:21:40 GMT"}, {"version": "v3", "created": "Wed, 31 Mar 2021 15:39:25 GMT"}], "update_date": "2021-04-02", "authors_parsed": [["Li", "Yandong", ""], ["Jia", "Xuhui", ""], ["Sang", "Ruoxin", ""], ["Zhu", "Yukun", ""], ["Green", "Bradley", ""], ["Wang", "Liqiang", ""], ["Gong", "Boqing", ""]]}, {"id": "2011.11201", "submitter": "Wei Yu", "authors": "Wei Yu, Wenxin Chen, Songhenh Yin, Steve Easterbrook, Animesh Garg", "title": "Concept Grounding with Modular Action-Capsules in Semantic Video\n  Prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Recent works in video prediction have mainly focused on passive forecasting\nand low-level action-conditional prediction, which sidesteps the learning of\ninteraction between agents and objects. We introduce the task of semantic\naction-conditional video prediction, which uses semantic action labels to\ndescribe those interactions and can be regarded as an inverse problem of action\nrecognition. The challenge of this new task primarily lies in how to\neffectively inform the model of semantic action information. To bridge vision\nand language, we utilize the idea of capsule and propose a novel video\nprediction model, Modular Action Capsule Network (MAC). Our method is evaluated\non two newly designed synthetic datasets, CLEVR-Building-Blocks and\nSapien-Kitchen, and one real-world dataset called TowerCreation. Experiments\nshow that given different action labels, MAC can correctly condition on\ninstructions and generate corresponding future frames without need of bounding\nboxes. We further demonstrate that the trained model can make\nout-of-distribution generalization, be quickly adapted to new object categories\nand exploit its learnt features for object detection, showing the progression\ntowards higher-level cognitive abilities.\n", "versions": [{"version": "v1", "created": "Mon, 23 Nov 2020 04:12:22 GMT"}, {"version": "v2", "created": "Wed, 21 Apr 2021 17:56:03 GMT"}], "update_date": "2021-04-22", "authors_parsed": [["Yu", "Wei", ""], ["Chen", "Wenxin", ""], ["Yin", "Songhenh", ""], ["Easterbrook", "Steve", ""], ["Garg", "Animesh", ""]]}, {"id": "2011.11204", "submitter": "Dongyan Guo", "authors": "Dongyan Guo, Yanyan Shao, Ying Cui, Zhenhua Wang, Liyan Zhang, Chunhua\n  Shen", "title": "Graph Attention Tracking", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Siamese network based trackers formulate the visual tracking task as a\nsimilarity matching problem. Almost all popular Siamese trackers realize the\nsimilarity learning via convolutional feature cross-correlation between a\ntarget branch and a search branch. However, since the size of target feature\nregion needs to be pre-fixed, these cross-correlation base methods suffer from\neither reserving much adverse background information or missing a great deal of\nforeground information. Moreover, the global matching between the target and\nsearch region also largely neglects the target structure and part-level\ninformation.\n  In this paper, to solve the above issues, we propose a simple target-aware\nSiamese graph attention network for general object tracking. We propose to\nestablish part-to-part correspondence between the target and the search region\nwith a complete bipartite graph, and apply the graph attention mechanism to\npropagate target information from the template feature to the search feature.\nFurther, instead of using the pre-fixed region cropping for\ntemplate-feature-area selection, we investigate a target-aware area selection\nmechanism to fit the size and aspect ratio variations of different objects.\nExperiments on challenging benchmarks including GOT-10k, UAV123, OTB-100 and\nLaSOT demonstrate that the proposed SiamGAT outperforms many state-of-the-art\ntrackers and achieves leading performance. Code is available at:\nhttps://git.io/SiamGAT\n", "versions": [{"version": "v1", "created": "Mon, 23 Nov 2020 04:26:45 GMT"}], "update_date": "2020-11-24", "authors_parsed": [["Guo", "Dongyan", ""], ["Shao", "Yanyan", ""], ["Cui", "Ying", ""], ["Wang", "Zhenhua", ""], ["Zhang", "Liyan", ""], ["Shen", "Chunhua", ""]]}, {"id": "2011.11210", "submitter": "Han Hu", "authors": "Qing Zhu and Qisen Shang and Han Hu and Haojia Yu and Ruofei Zhong", "title": "Structure-Aware Completion of Photogrammetric Meshes in Urban Road\n  Environment", "comments": null, "journal-ref": null, "doi": "10.1016/j.isprsjprs.2021.02.010", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Photogrammetric mesh models obtained from aerial oblique images have been\nwidely used for urban reconstruction. However, the photogrammetric meshes also\nsuffer from severe texture problems, especially on the road areas due to\nocclusion. This paper proposes a structure-aware completion approach to improve\nthe quality of meshes by removing undesired vehicles on the road seamlessly.\nSpecifically, the discontinuous texture atlas is first integrated to a\ncontinuous screen space through rendering by the graphics pipeline; the\nrendering also records necessary mapping for deintegration to the original\ntexture atlas after editing. Vehicle regions are masked by a standard object\ndetection approach, e.g. Faster RCNN. Then, the masked regions are completed\nguided by the linear structures and regularities in the road region, which is\nimplemented based on Patch Match. Finally, the completed rendered image is\ndeintegrated to the original texture atlas and the triangles for the vehicles\nare also flattened for improved meshes. Experimental evaluations and analyses\nare conducted against three datasets, which are captured with different sensors\nand ground sample distances. The results reveal that the proposed method can\nquite realistic meshes after removing the vehicles. The structure-aware\ncompletion approach for road regions outperforms popular image completion\nmethods and ablation study further confirms the effectiveness of the linear\nguidance. It should be noted that the proposed method is also capable to handle\ntiled mesh models for large-scale scenes. Dataset and code are available at\nvrlab.org.cn/~hanhu/projects/mesh.\n", "versions": [{"version": "v1", "created": "Mon, 23 Nov 2020 05:04:28 GMT"}, {"version": "v2", "created": "Tue, 24 Nov 2020 05:54:14 GMT"}, {"version": "v3", "created": "Wed, 10 Feb 2021 03:45:29 GMT"}], "update_date": "2021-03-12", "authors_parsed": [["Zhu", "Qing", ""], ["Shang", "Qisen", ""], ["Hu", "Han", ""], ["Yu", "Haojia", ""], ["Zhong", "Ruofei", ""]]}, {"id": "2011.11221", "submitter": "Xianjin Chao", "authors": "Xianjin Chao, Yanrui Bin, Wenqing Chu, Xuan Cao, Yanhao Ge, Chengjie\n  Wang, Jilin Li, Feiyue Huang, Howard Leung", "title": "Adversarial Refinement Network for Human Motion Prediction", "comments": "Accepted by ACCV 2020(Oral)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Human motion prediction aims to predict future 3D skeletal sequences by\ngiving a limited human motion as inputs. Two popular methods, recurrent neural\nnetworks and feed-forward deep networks, are able to predict rough motion\ntrend, but motion details such as limb movement may be lost. To predict more\naccurate future human motion, we propose an Adversarial Refinement Network\n(ARNet) following a simple yet effective coarse-to-fine mechanism with novel\nadversarial error augmentation. Specifically, we take both the historical\nmotion sequences and coarse prediction as input of our cascaded refinement\nnetwork to predict refined human motion and strengthen the refinement network\nwith adversarial error augmentation. During training, we deliberately introduce\nthe error distribution by learning through the adversarial mechanism among\ndifferent subjects. In testing, our cascaded refinement network alleviates the\nprediction error from the coarse predictor resulting in a finer prediction\nrobustly. This adversarial error augmentation provides rich error cases as\ninput to our refinement network, leading to better generalization performance\non the testing dataset. We conduct extensive experiments on three standard\nbenchmark datasets and show that our proposed ARNet outperforms other\nstate-of-the-art methods, especially on challenging aperiodic actions in both\nshort-term and long-term predictions.\n", "versions": [{"version": "v1", "created": "Mon, 23 Nov 2020 05:42:20 GMT"}, {"version": "v2", "created": "Tue, 24 Nov 2020 02:16:10 GMT"}], "update_date": "2020-11-25", "authors_parsed": [["Chao", "Xianjin", ""], ["Bin", "Yanrui", ""], ["Chu", "Wenqing", ""], ["Cao", "Xuan", ""], ["Ge", "Yanhao", ""], ["Wang", "Chengjie", ""], ["Li", "Jilin", ""], ["Huang", "Feiyue", ""], ["Leung", "Howard", ""]]}, {"id": "2011.11232", "submitter": "Gyeongsik Moon", "authors": "Gyeongsik Moon and Kyoung Mu Lee", "title": "NeuralAnnot: Neural Annotator for in-the-wild Expressive 3D Human Pose\n  and Mesh Training Sets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recovering expressive 3D human pose and mesh from in-the-wild images is\ngreatly challenging due to the absence of the training data. Several\noptimization-based methods have been used to obtain 3D human model fits from GT\n2D poses, which serve as pseudo-groundtruth (GT) 3D poses and meshes. However,\nthey often suffer from severe depth ambiguity while requiring long running time\nbecause of their per-sample optimization that only uses 2D supervisions and\npriors. The per-sample optimization optimizes a 3D human model on each sample\nindependently; therefore, running it on a large number of samples consumes long\nrunning time. In addition, the absence of the 3D supervisions makes their\nframework suffer from depth ambiguity. To overcome the limitations, we present\nNeuralAnnot, a neural annotator that learns to construct in-the-wild expressive\n3D human pose and mesh training sets. Our NeuralAnnot is trained on entire\ndatasets by considering multiple samples together with additional 3D\nsupervisions from auxiliary datasets; therefore, it produces far better 3D\npseudo-GT fits much faster. We show that the newly obtained training set brings\ngreat performance gain, which will be publicly released with codes.\n", "versions": [{"version": "v1", "created": "Mon, 23 Nov 2020 06:33:39 GMT"}, {"version": "v2", "created": "Sat, 28 Nov 2020 03:32:19 GMT"}, {"version": "v3", "created": "Thu, 1 Apr 2021 00:39:08 GMT"}, {"version": "v4", "created": "Sun, 18 Apr 2021 07:41:26 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Moon", "Gyeongsik", ""], ["Lee", "Kyoung Mu", ""]]}, {"id": "2011.11233", "submitter": "Xiangxiang Chu", "authors": "Xiaoxing Wang and Xiangxiang Chu and Yuda Fan and Zhexi Zhang and\n  Xiaolin Wei and Junchi Yan and Xiaokang Yang", "title": "ROME: Robustifying Memory-Efficient NAS via Topology Disentanglement and\n  Gradients Accumulation", "comments": "Observe new collapse in memory efficient NAS and address it using\n  ROME", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Single-path based differentiable neural architecture search has great\nstrengths for its low computational cost and memory-friendly nature. However,\nwe surprisingly discover that it suffers from severe searching instability\nwhich has been primarily ignored, posing a potential weakness for a wider\napplication. In this paper, we delve into its performance collapse issue and\npropose a new algorithm called RObustifying Memory-Efficient NAS (ROME).\nSpecifically, 1) for consistent topology in the search and evaluation stage, we\ninvolve separate parameters to disentangle the topology from the operations of\nthe architecture. In such a way, we can independently sample connections and\noperations without interference; 2) to discount sampling unfairness and\nvariance, we enforce fair sampling for weight update and apply a gradient\naccumulation mechanism for architecture parameters. Extensive experiments\ndemonstrate that our proposed method has strong performance and robustness,\nwhere it mostly achieves state-of-the-art results on a large number of standard\nbenchmarks.\n", "versions": [{"version": "v1", "created": "Mon, 23 Nov 2020 06:34:07 GMT"}], "update_date": "2020-11-24", "authors_parsed": [["Wang", "Xiaoxing", ""], ["Chu", "Xiangxiang", ""], ["Fan", "Yuda", ""], ["Zhang", "Zhexi", ""], ["Wei", "Xiaolin", ""], ["Yan", "Junchi", ""], ["Yang", "Xiaokang", ""]]}, {"id": "2011.11245", "submitter": "Jinlu Liu", "authors": "Jinlu Liu and Liang Song and Yongqiang Qin", "title": "BiOpt: Bi-Level Optimization for Few-Shot Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Few-shot segmentation is a challenging task that aims to segment objects of\nnew classes given scarce support images. In the inductive setting, existing\nprototype-based methods focus on extracting prototypes from the support images;\nhowever, they fail to utilize semantic information of the query images. In this\npaper, we propose Bi-level Optimization (BiOpt), which succeeds to compute\nclass prototypes from the query images under inductive setting. The learning\nprocedure of BiOpt is decomposed into two nested loops: inner and outer loop.\nOn each task, the inner loop aims to learn optimized prototypes from the query\nimages. An init step is conducted to fully exploit knowledge from both support\nand query features, so as to give reasonable initialized prototypes into the\ninner loop. The outer loop aims to learn a discriminative embedding space\nacross different tasks. Extensive experiments on two benchmarks verify the\nsuperiority of our proposed BiOpt algorithm. In particular, we consistently\nachieve the state-of-the-art performance on 5-shot PASCAL-$5^i$ and 1-shot\nCOCO-$20^i$.\n", "versions": [{"version": "v1", "created": "Mon, 23 Nov 2020 07:09:48 GMT"}], "update_date": "2020-11-24", "authors_parsed": [["Liu", "Jinlu", ""], ["Song", "Liang", ""], ["Qin", "Yongqiang", ""]]}, {"id": "2011.11257", "submitter": "Lars Ankile", "authors": "Lars Lien Ankile, Morgan Feet Heggland, Kjartan Krange", "title": "Application of Facial Recognition using Convolutional Neural Networks\n  for Entry Access Control", "comments": "10 pages, 7 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The purpose of this paper is to design a solution to the problem of facial\nrecognition by use of convolutional neural networks, with the intention of\napplying the solution in a camera-based home-entry access control system. More\nspecifically, the paper focuses on solving the supervised classification\nproblem of taking images of people as input and classifying the person in the\nimage as one of the authors or not. Two approaches are proposed: (1) building\nand training a neural network called WoodNet from scratch and (2) leveraging\ntransfer learning by utilizing a network pre-trained on the ImageNet database\nand adapting it to this project's data and classes. In order to train the\nmodels to recognize the authors, a dataset containing more than 150 000 images\nhas been created, balanced over the authors and others. Image extraction from\nvideos and image augmentation techniques were instrumental for dataset\ncreation. The results are two models classifying the individuals in the dataset\nwith high accuracy, achieving over 99% accuracy on held-out test data. The\npre-trained model fitted significantly faster than WoodNet, and seems to\ngeneralize better. However, these results come with a few caveats. Because of\nthe way the dataset was compiled, as well as the high accuracy, one has reason\nto believe the models over-fitted to the data to some degree. An added\nconsequence of the data compilation method is that the test dataset may not be\nsufficiently different from the training data, limiting its ability to validate\ngeneralization of the models. However, utilizing the models in a web-cam based\nsystem, classifying faces in real-time, shows promising results and indicates\nthat the models generalized fairly well for at least some of the classes (see\nthe accompanying video).\n", "versions": [{"version": "v1", "created": "Mon, 23 Nov 2020 07:55:24 GMT"}], "update_date": "2020-11-24", "authors_parsed": [["Ankile", "Lars Lien", ""], ["Heggland", "Morgan Feet", ""], ["Krange", "Kjartan", ""]]}, {"id": "2011.11260", "submitter": "Zheng Dang", "authors": "Zheng Dang and Fei Wang and Mathieu Salzmann", "title": "3D Registration for Self-Occluded Objects in Context", "comments": "8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While much progress has been made on the task of 3D point cloud registration,\nthere still exists no learning-based method able to estimate the 6D pose of an\nobject observed by a 2.5D sensor in a scene. The challenges of this scenario\ninclude the fact that most measurements are outliers depicting the object's\nsurrounding context, and the mismatch between the complete 3D object model and\nits self-occluded observations.\n  We introduce the first deep learning framework capable of effectively\nhandling this scenario. Our method consists of an instance segmentation module\nfollowed by a pose estimation one. It allows us to perform 3D registration in a\none-shot manner, without requiring an expensive iterative procedure. We further\ndevelop an on-the-fly rendering-based training strategy that is both time- and\nmemory-efficient. Our experiments evidence the superiority of our approach over\nthe state-of-the-art traditional and learning-based 3D registration methods.\n", "versions": [{"version": "v1", "created": "Mon, 23 Nov 2020 08:05:28 GMT"}], "update_date": "2020-11-24", "authors_parsed": [["Dang", "Zheng", ""], ["Wang", "Fei", ""], ["Salzmann", "Mathieu", ""]]}, {"id": "2011.11261", "submitter": "Zehua Zhang", "authors": "Zehua Zhang and David Crandall", "title": "Hierarchically Decoupled Spatial-Temporal Contrast for Self-supervised\n  Video Representation Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We present a novel way for self-supervised video representation learning by:\n(a) decoupling the learning objective into two contrastive subtasks\nrespectively emphasizing spatial and temporal features, and (b) performing it\nhierarchically to encourage multi-scale understanding. Motivated by their\neffectiveness in supervised learning, we first introduce spatial-temporal\nfeature learning decoupling and hierarchical learning to the context of\nunsupervised video learning. In particular, our method directs the network to\nseparately capture spatial and temporal features on the basis of contrastive\nlearning via manipulating augmentations as regularization, and further solve\nsuch proxy tasks hierarchically by optimizing towards a compound contrastive\nloss. Experiments show that our proposed Hierarchically Decoupled\nSpatial-Temporal Contrast (HDC) achieves substantial gains over directly\nlearning spatial-temporal features as a whole and significantly outperforms\nother state-of-the-art unsupervised methods on downstream action recognition\nbenchmarks on UCF101 and HMDB51. We will release our code and pretrained\nweights.\n", "versions": [{"version": "v1", "created": "Mon, 23 Nov 2020 08:05:39 GMT"}], "update_date": "2020-11-24", "authors_parsed": [["Zhang", "Zehua", ""], ["Crandall", "David", ""]]}, {"id": "2011.11286", "submitter": "Ekraam Sabir", "authors": "Ekraam Sabir, Ayush Jaiswal, Wael AbdAlmageed, Prem Natarajan", "title": "MEG: Multi-Evidence GNN for Multimodal Semantic Forensics", "comments": "To be published at ICPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fake news often involves semantic manipulations across modalities such as\nimage, text, location etc and requires the development of multimodal semantic\nforensics for its detection. Recent research has centered the problem around\nimages, calling it image repurposing -- where a digitally unmanipulated image\nis semantically misrepresented by means of its accompanying multimodal metadata\nsuch as captions, location, etc. The image and metadata together comprise a\nmultimedia package. The problem setup requires algorithms to perform multimodal\nsemantic forensics to authenticate a query multimedia package using a reference\ndataset of potentially related packages as evidences. Existing methods are\nlimited to using a single evidence (retrieved package), which ignores potential\nperformance improvement from the use of multiple evidences. In this work, we\nintroduce a novel graph neural network based model for multimodal semantic\nforensics, which effectively utilizes multiple retrieved packages as evidences\nand is scalable with the number of evidences. We compare the scalability and\nperformance of our model against existing methods. Experimental results show\nthat the proposed model outperforms existing state-of-the-art algorithms with\nan error reduction of up to 25%.\n", "versions": [{"version": "v1", "created": "Mon, 23 Nov 2020 09:01:28 GMT"}], "update_date": "2020-11-24", "authors_parsed": [["Sabir", "Ekraam", ""], ["Jaiswal", "Ayush", ""], ["AbdAlmageed", "Wael", ""], ["Natarajan", "Prem", ""]]}, {"id": "2011.11305", "submitter": "Ioannis Apostolopoulos", "authors": "Ioannis D. Apostolopoulos, Mpesiana Tzani", "title": "Industrial object, machine part and defect recognition towards fully\n  automated industrial monitoring employing deep learning. The case of\n  multilevel VGG19", "comments": "17 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern industry requires modern solutions for monitoring the automatic\nproduction of goods. Smart monitoring of the functionality of the mechanical\nparts of technology systems or machines is mandatory for a fully automatic\nproduction process. Although Deep Learning has been advancing, allowing for\nreal-time object detection and other tasks, little has been investigated about\nthe effectiveness of specially designed Convolutional Neural Networks for\ndefect detection and industrial object recognition. In the particular study, we\nemployed six publically available industrial-related datasets containing defect\nmaterials and industrial tools or engine parts, aiming to develop a specialized\nmodel for pattern recognition. Motivated by the recent success of the Virtual\nGeometry Group (VGG) network, we propose a modified version of it, called\nMultipath VGG19, which allows for more local and global feature extraction,\nwhile the extra features are fused via concatenation. The experiments verified\nthe effectiveness of MVGG19 over the traditional VGG19. Specifically, top\nclassification performance was achieved in five of the six image datasets,\nwhile the average classification improvement was 6.95%.\n", "versions": [{"version": "v1", "created": "Mon, 23 Nov 2020 10:05:50 GMT"}], "update_date": "2020-11-24", "authors_parsed": [["Apostolopoulos", "Ioannis D.", ""], ["Tzani", "Mpesiana", ""]]}, {"id": "2011.11309", "submitter": "Yuzhi Zhao", "authors": "Zhao Yuzhi, Po Lai-Man, Wang Xuehui, Liu Kangcheng, Zhang Yujia, Yu\n  Wing-Yin, Xian Pengfei, Xiong Jingjing", "title": "Legacy Photo Editing with Learned Noise Prior", "comments": "accepted by IEEE WACV 2021, 2nd round submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There are quite a number of photographs captured under undesirable conditions\nin the last century. Thus, they are often noisy, regionally incomplete, and\ngrayscale formatted. Conventional approaches mainly focus on one point so that\nthose restoration results are not perceptually sharp or clean enough. To solve\nthese problems, we propose a noise prior learner NEGAN to simulate the noise\ndistribution of real legacy photos using unpaired images. It mainly focuses on\nmatching high-frequency parts of noisy images through discrete wavelet\ntransform (DWT) since they include most of noise statistics. We also create a\nlarge legacy photo dataset for learning noise prior. Using learned noise prior,\nwe can easily build valid training pairs by degrading clean images. Then, we\npropose an IEGAN framework performing image editing including joint denoising,\ninpainting and colorization based on the estimated noise prior. We evaluate the\nproposed system and compare it with state-of-the-art image enhancement methods.\nThe experimental results demonstrate that it achieves the best perceptual\nquality.\nhttps://github.com/zhaoyuzhi/Legacy-Photo-Editing-with-Learned-Noise-Prior for\nthe codes and the proposed LP dataset.\n", "versions": [{"version": "v1", "created": "Mon, 23 Nov 2020 10:18:01 GMT"}, {"version": "v2", "created": "Tue, 24 Nov 2020 08:27:30 GMT"}], "update_date": "2020-11-25", "authors_parsed": [["Yuzhi", "Zhao", ""], ["Lai-Man", "Po", ""], ["Xuehui", "Wang", ""], ["Kangcheng", "Liu", ""], ["Yujia", "Zhang", ""], ["Wing-Yin", "Yu", ""], ["Pengfei", "Xian", ""], ["Jingjing", "Xiong", ""]]}, {"id": "2011.11311", "submitter": "Ines Rieger", "authors": "Jessica Deuschel, Bettina Finzel, Ines Rieger", "title": "Uncovering the Bias in Facial Expressions", "comments": "Accepted at the colloquium \"Forschende Frauen\", University of\n  Bamberg, 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Over the past decades the machine and deep learning community has celebrated\ngreat achievements in challenging tasks such as image classification. The deep\narchitecture of artificial neural networks together with the plenitude of\navailable data makes it possible to describe highly complex relations. Yet, it\nis still impossible to fully capture what the deep learning model has learned\nand to verify that it operates fairly and without creating bias, especially in\ncritical tasks, for instance those arising in the medical field. One example\nfor such a task is the detection of distinct facial expressions, called Action\nUnits, in facial images. Considering this specific task, our research aims to\nprovide transparency regarding bias, specifically in relation to gender and\nskin color. We train a neural network for Action Unit classification and\nanalyze its performance quantitatively based on its accuracy and qualitatively\nbased on heatmaps. A structured review of our results indicates that we are\nable to detect bias. Even though we cannot conclude from our results that lower\nclassification performance emerged solely from gender and skin color bias,\nthese biases must be addressed, which is why we end by giving suggestions on\nhow the detected bias can be avoided.\n", "versions": [{"version": "v1", "created": "Mon, 23 Nov 2020 10:20:10 GMT"}], "update_date": "2020-11-24", "authors_parsed": [["Deuschel", "Jessica", ""], ["Finzel", "Bettina", ""], ["Rieger", "Ines", ""]]}, {"id": "2011.11314", "submitter": "Gerald Baier", "authors": "Gerald Baier and Antonin Deschemps and Michael Schmitt and Naoto\n  Yokoya", "title": "Synthesizing Optical and SAR Imagery From Land Cover Maps and Auxiliary\n  Raster Data", "comments": null, "journal-ref": null, "doi": "10.1109/TGRS.2021.3068532", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We synthesize both optical RGB and synthetic aperture radar (SAR) remote\nsensing images from land cover maps and auxiliary raster data using generative\nadversarial networks (GANs). In remote sensing, many types of data, such as\ndigital elevation models (DEMs) or precipitation maps, are often not reflected\nin land cover maps but still influence image content or structure. Including\nsuch data in the synthesis process increases the quality of the generated\nimages and exerts more control on their characteristics. Spatially adaptive\nnormalization layers fuse both inputs and are applied to a full-blown generator\narchitecture consisting of encoder and decoder to take full advantage of the\ninformation content in the auxiliary raster data. Our method successfully\nsynthesizes medium (10 m) and high (1 m) resolution images when trained with\nthe corresponding data set. We show the advantage of data fusion of land cover\nmaps and auxiliary information using mean intersection over unions (mIoUs),\npixel accuracy, and Fr\\'echet inception distances (FIDs) using pretrained U-Net\nsegmentation models. Handpicked images exemplify how fusing information avoids\nambiguities in the synthesized images. By slightly editing the input, our\nmethod can be used to synthesize realistic changes, i.e., raising the water\nlevels. The source code is available at https://github.com/gbaier/rs_img_synth\nand we published the newly created high-resolution dataset at\nhttps://ieee-dataport.org/open-access/geonrw.\n", "versions": [{"version": "v1", "created": "Mon, 23 Nov 2020 10:28:10 GMT"}, {"version": "v2", "created": "Tue, 25 May 2021 13:25:48 GMT"}], "update_date": "2021-05-26", "authors_parsed": [["Baier", "Gerald", ""], ["Deschemps", "Antonin", ""], ["Schmitt", "Michael", ""], ["Yokoya", "Naoto", ""]]}, {"id": "2011.11344", "submitter": "Michael Mommert", "authors": "Michael Mommert, Mario Sigel, Marcel Neuhausler, Linus Scheibenreif,\n  Damian Borth", "title": "Characterization of Industrial Smoke Plumes from Remote Sensing Data", "comments": "To be presented at the \"Tackling Climate Change with Machine\n  Learning\" workshop at NeurIPS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The major driver of global warming has been identified as the anthropogenic\nrelease of greenhouse gas (GHG) emissions from industrial activities. The\nquantitative monitoring of these emissions is mandatory to fully understand\ntheir effect on the Earth's climate and to enforce emission regulations on a\nlarge scale. In this work, we investigate the possibility to detect and\nquantify industrial smoke plumes from globally and freely available multi-band\nimage data from ESA's Sentinel-2 satellites. Using a modified ResNet-50, we can\ndetect smoke plumes of different sizes with an accuracy of 94.3%. The model\ncorrectly ignores natural clouds and focuses on those imaging channels that are\nrelated to the spectral absorption from aerosols and water vapor, enabling the\nlocalization of smoke. We exploit this localization ability and train a U-Net\nsegmentation model on a labeled sub-sample of our data, resulting in an\nIntersection-over-Union (IoU) metric of 0.608 and an overall accuracy for the\ndetection of any smoke plume of 94.0%; on average, our model can reproduce the\narea covered by smoke in an image to within 5.6%. The performance of our model\nis mostly limited by occasional confusion with surface objects, the inability\nto identify semi-transparent smoke, and human limitations to properly identify\nsmoke based on RGB-only images. Nevertheless, our results enable us to reliably\ndetect and qualitatively estimate the level of smoke activity in order to\nmonitor activity in industrial plants across the globe. Our data set and code\nbase are publicly available.\n", "versions": [{"version": "v1", "created": "Mon, 23 Nov 2020 11:54:32 GMT"}], "update_date": "2020-11-24", "authors_parsed": [["Mommert", "Michael", ""], ["Sigel", "Mario", ""], ["Neuhausler", "Marcel", ""], ["Scheibenreif", "Linus", ""], ["Borth", "Damian", ""]]}, {"id": "2011.11365", "submitter": "Jia Wang", "authors": "Jia Wang, Ping Wang, Biao Li, Yinghui Gao, and Siyi Zhao", "title": "A Learning-based Optimization Algorithm:Image Registration Optimizer\n  Network", "comments": "6pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Remote sensing image registration is valuable for image-based navigation\nsystem despite posing many challenges. As the search space of registration is\nusually non-convex, the optimization algorithm, which aims to search the best\ntransformation parameters, is a challenging step. Conventional optimization\nalgorithms can hardly reconcile the contradiction of simultaneous rapid\nconvergence and the global optimization. In this paper, a novel learning-based\noptimization algorithm named Image Registration Optimizer Network (IRON) is\nproposed, which can predict the global optimum after single iteration. The IRON\nis trained by a 3D tensor (9x9x9), which consists of similar metric values. The\nelements of the 3D tensor correspond to the 9x9x9 neighbors of the initial\nparameters in the search space. Then, the tensor's label is a vector that\npoints to the global optimal parameters from the initial parameters. Because of\nthe special architecture, the IRON could predict the global optimum directly\nfor any initialization. The experimental results demonstrate that the proposed\nalgorithm performs better than other classical optimization algorithms as it\nhas higher accuracy, lower root of mean square error (RMSE), and more\nefficiency. Our IRON codes are available for further\nstudy.https://www.github.com/jaxwangkd04/IRON\n", "versions": [{"version": "v1", "created": "Mon, 23 Nov 2020 12:44:52 GMT"}], "update_date": "2020-11-24", "authors_parsed": [["Wang", "Jia", ""], ["Wang", "Ping", ""], ["Li", "Biao", ""], ["Gao", "Yinghui", ""], ["Zhao", "Siyi", ""]]}, {"id": "2011.11377", "submitter": "Yuzhi Zhao", "authors": "Yuzhi Zhao, Lai-Man Po, Kwok-Wai Cheung, Wing-Yin Yu, Yasar Abbas Ur\n  Rehman", "title": "SCGAN: Saliency Map-guided Colorization with Generative Adversarial\n  Network", "comments": "accepted by IEEE Transactions on Circuits and Systems for Video\n  Technology", "journal-ref": null, "doi": "10.1109/TCSVT.2020.3037688", "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a grayscale photograph, the colorization system estimates a visually\nplausible colorful image. Conventional methods often use semantics to colorize\ngrayscale images. However, in these methods, only classification semantic\ninformation is embedded, resulting in semantic confusion and color bleeding in\nthe final colorized image. To address these issues, we propose a fully\nautomatic Saliency Map-guided Colorization with Generative Adversarial Network\n(SCGAN) framework. It jointly predicts the colorization and saliency map to\nminimize semantic confusion and color bleeding in the colorized image. Since\nthe global features from pre-trained VGG-16-Gray network are embedded to the\ncolorization encoder, the proposed SCGAN can be trained with much less data\nthan state-of-the-art methods to achieve perceptually reasonable colorization.\nIn addition, we propose a novel saliency map-based guidance method. Branches of\nthe colorization decoder are used to predict the saliency map as a proxy\ntarget. Moreover, two hierarchical discriminators are utilized for the\ngenerated colorization and saliency map, respectively, in order to strengthen\nvisual perception performance. The proposed system is evaluated on ImageNet\nvalidation set. Experimental results show that SCGAN can generate more\nreasonable colorized images than state-of-the-art techniques.\n", "versions": [{"version": "v1", "created": "Mon, 23 Nov 2020 13:06:54 GMT"}], "update_date": "2020-11-24", "authors_parsed": [["Zhao", "Yuzhi", ""], ["Po", "Lai-Man", ""], ["Cheung", "Kwok-Wai", ""], ["Yu", "Wing-Yin", ""], ["Rehman", "Yasar Abbas Ur", ""]]}, {"id": "2011.11378", "submitter": "Shih-Lun Wu", "authors": "Shih-Lun Wu, Hsiao-Yen Tung, Yu-Lun Hsu", "title": "Deep Learning for Automatic Quality Grading of Mangoes: Methods and\n  Insights", "comments": "Accepted to ICMLA 2020; 8 pages, 8 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The quality grading of mangoes is a crucial task for mango growers as it\nvastly affects their profit. However, until today, this process still relies on\nlaborious efforts of humans, who are prone to fatigue and errors. To remedy\nthis, the paper approaches the grading task with various convolutional neural\nnetworks (CNN), a tried-and-tested deep learning technology in computer vision.\nThe models involved include Mask R-CNN (for background removal), the numerous\npast winners of the ImageNet challenge, namely AlexNet, VGGs, and ResNets; and,\na family of self-defined convolutional autoencoder-classifiers (ConvAE-Clfs)\ninspired by the claimed benefit of multi-task learning in classification tasks.\nTransfer learning is also adopted in this work via utilizing the ImageNet\npretrained weights. Besides elaborating on the preprocessing techniques,\ntraining details, and the resulting performance, we go one step further to\nprovide explainable insights into the model's working with the help of saliency\nmaps and principal component analysis (PCA). These insights provide a succinct,\nmeaningful glimpse into the intricate deep learning black box, fostering trust,\nand can also be presented to humans in real-world use cases for reviewing the\ngrading results.\n", "versions": [{"version": "v1", "created": "Mon, 23 Nov 2020 13:09:47 GMT"}], "update_date": "2020-11-24", "authors_parsed": [["Wu", "Shih-Lun", ""], ["Tung", "Hsiao-Yen", ""], ["Hsu", "Yu-Lun", ""]]}, {"id": "2011.11383", "submitter": "Atis Elsts", "authors": "Maksims Ivanovs, Roberts Kadikis, Martins Lulla, Aleksejs Rutkovskis,\n  and Atis Elsts", "title": "Automated Quality Assessment of Hand Washing Using Deep Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Washing hands is one of the most important ways to prevent infectious\ndiseases, including COVID-19. Unfortunately, medical staff does not always\nfollow the World Health Organization (WHO) hand washing guidelines in their\neveryday work. To this end, we present neural networks for automatically\nrecognizing the different washing movements defined by the WHO. We train the\nneural network on a part of a large (2000+ videos) real-world labeled dataset\nwith the different washing movements. The preliminary results show that using\npre-trained neural network models such as MobileNetV2 and Xception for the\ntask, it is possible to achieve >64 % accuracy in recognizing the different\nwashing movements. We also describe the collection and the structure of the\nabove open-access dataset created as part of this work. Finally, we describe\nhow the neural network can be used to construct a mobile phone application for\nautomatic quality control and real-time feedback for medical professionals.\n", "versions": [{"version": "v1", "created": "Mon, 23 Nov 2020 13:22:53 GMT"}, {"version": "v2", "created": "Tue, 1 Dec 2020 16:05:27 GMT"}], "update_date": "2020-12-02", "authors_parsed": [["Ivanovs", "Maksims", ""], ["Kadikis", "Roberts", ""], ["Lulla", "Martins", ""], ["Rutkovskis", "Aleksejs", ""], ["Elsts", "Atis", ""]]}, {"id": "2011.11390", "submitter": "Arthur Douillard", "authors": "Arthur Douillard and Yifu Chen and Arnaud Dapogny and Matthieu Cord", "title": "PLOP: Learning without Forgetting for Continual Semantic Segmentation", "comments": "Accepted at CVPR 2021, code:\n  https://github.com/arthurdouillard/CVPR2021_PLOP", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Deep learning approaches are nowadays ubiquitously used to tackle computer\nvision tasks such as semantic segmentation, requiring large datasets and\nsubstantial computational power. Continual learning for semantic segmentation\n(CSS) is an emerging trend that consists in updating an old model by\nsequentially adding new classes. However, continual learning methods are\nusually prone to catastrophic forgetting. This issue is further aggravated in\nCSS where, at each step, old classes from previous iterations are collapsed\ninto the background. In this paper, we propose Local POD, a multi-scale pooling\ndistillation scheme that preserves long- and short-range spatial relationships\nat feature level. Furthermore, we design an entropy-based pseudo-labelling of\nthe background w.r.t. classes predicted by the old model to deal with\nbackground shift and avoid catastrophic forgetting of the old classes. Our\napproach, called PLOP, significantly outperforms state-of-the-art methods in\nexisting CSS scenarios, as well as in newly proposed challenging benchmarks.\n", "versions": [{"version": "v1", "created": "Mon, 23 Nov 2020 13:35:03 GMT"}, {"version": "v2", "created": "Tue, 24 Nov 2020 16:39:00 GMT"}, {"version": "v3", "created": "Thu, 11 Mar 2021 09:43:29 GMT"}], "update_date": "2021-03-12", "authors_parsed": [["Douillard", "Arthur", ""], ["Chen", "Yifu", ""], ["Dapogny", "Arnaud", ""], ["Cord", "Matthieu", ""]]}, {"id": "2011.11444", "submitter": "Alice Ruget", "authors": "Alice Ruget, Stephen McLaughlin, Robert K. Henderson, Istvan Gyongy,\n  Abderrahim Halimi and Jonathan Leach", "title": "Robust super-resolution depth imaging via a multi-feature fusion deep\n  network", "comments": null, "journal-ref": null, "doi": "10.1364/OE.415563", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Three-dimensional imaging plays an important role in imaging applications\nwhere it is necessary to record depth. The number of applications that use\ndepth imaging is increasing rapidly, and examples include self-driving\nautonomous vehicles and auto-focus assist on smartphone cameras. Light\ndetection and ranging (LIDAR) via single-photon sensitive detector (SPAD)\narrays is an emerging technology that enables the acquisition of depth images\nat high frame rates. However, the spatial resolution of this technology is\ntypically low in comparison to the intensity images recorded by conventional\ncameras. To increase the native resolution of depth images from a SPAD camera,\nwe develop a deep network built specifically to take advantage of the multiple\nfeatures that can be extracted from a camera's histogram data. The network is\ndesigned for a SPAD camera operating in a dual-mode such that it captures\nalternate low resolution depth and high resolution intensity images at high\nframe rates, thus the system does not require any additional sensor to provide\nintensity images. The network then uses the intensity images and multiple\nfeatures extracted from downsampled histograms to guide the upsampling of the\ndepth. Our network provides significant image resolution enhancement and image\ndenoising across a wide range of signal-to-noise ratios and photon levels. We\napply the network to a range of 3D data, demonstrating denoising and a\nfour-fold resolution enhancement of depth.\n", "versions": [{"version": "v1", "created": "Fri, 20 Nov 2020 14:24:12 GMT"}, {"version": "v2", "created": "Mon, 1 Feb 2021 11:25:46 GMT"}], "update_date": "2021-04-21", "authors_parsed": [["Ruget", "Alice", ""], ["McLaughlin", "Stephen", ""], ["Henderson", "Robert K.", ""], ["Gyongy", "Istvan", ""], ["Halimi", "Abderrahim", ""], ["Leach", "Jonathan", ""]]}, {"id": "2011.11452", "submitter": "Xiaoxiang Zhu", "authors": "Chunping Qiu, Lukas Liebel, Lloyd H. Hughes, Michael Schmitt, Marco\n  K\\\"orner, and Xiao Xiang Zhu", "title": "Multi-task Learning for Human Settlement Extent Regression and Local\n  Climate Zone Classification", "comments": "This work has been accepted by IEEE GRSL for publication. Copyright\n  may be transferred without notice, after which this version may no longer be\n  accessible", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human Settlement Extent (HSE) and Local Climate Zone (LCZ) maps are both\nessential sources, e.g., for sustainable urban development and Urban Heat\nIsland (UHI) studies. Remote sensing (RS)- and deep learning (DL)-based\nclassification approaches play a significant role by providing the potential\nfor global mapping. However, most of the efforts only focus on one of the two\nschemes, usually on a specific scale. This leads to unnecessary redundancies,\nsince the learned features could be leveraged for both of these related tasks.\nIn this letter, the concept of multi-task learning (MTL) is introduced to HSE\nregression and LCZ classification for the first time. We propose a MTL\nframework and develop an end-to-end Convolutional Neural Network (CNN), which\nconsists of a backbone network for shared feature learning, attention modules\nfor task-specific feature learning, and a weighting strategy for balancing the\ntwo tasks. We additionally propose to exploit HSE predictions as a prior for\nLCZ classification to enhance the accuracy. The MTL approach was extensively\ntested with Sentinel-2 data of 13 cities across the world. The results\ndemonstrate that the framework is able to provide a competitive solution for\nboth tasks.\n", "versions": [{"version": "v1", "created": "Mon, 23 Nov 2020 14:54:13 GMT"}], "update_date": "2020-11-24", "authors_parsed": [["Qiu", "Chunping", ""], ["Liebel", "Lukas", ""], ["Hughes", "Lloyd H.", ""], ["Schmitt", "Michael", ""], ["K\u00f6rner", "Marco", ""], ["Zhu", "Xiao Xiang", ""]]}, {"id": "2011.11459", "submitter": "Matias Valdenegro-Toro", "authors": "Lauren Michelle Pfeifer and Matias Valdenegro-Toro", "title": "Automatic Detection and Classification of Tick-borne Skin Lesions using\n  Deep Learning", "comments": "2 pages, 8 figures, with appendix", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Around the globe, ticks are the culprit of transmitting a variety of\nbacterial, viral and parasitic diseases. The incidence of tick-borne diseases\nhas drastically increased within the last decade, with annual cases of Lyme\ndisease soaring to an estimated 300,000 in the United States alone. As a\nresult, more efforts in improving lesion identification approaches and\ndiagnostics for tick-borne illnesses is critical. The objective for this study\nis to build upon the approach used by Burlina et al. by using a variety of\nconvolutional neural network models to detect tick-borne skin lesions. We\nexpanded the data inputs by acquiring images from Google in seven different\nlanguages to test if this would diversify training data and improve the\naccuracy of skin lesion detection. The final dataset included nearly 6,080\nimages and was trained on a combination of architectures (ResNet 34, ResNet 50,\nVGG 19, and Dense Net 121). We obtained an accuracy of 80.72% with our model\ntrained on the DenseNet 121 architecture.\n", "versions": [{"version": "v1", "created": "Mon, 23 Nov 2020 15:16:14 GMT"}], "update_date": "2020-11-24", "authors_parsed": [["Pfeifer", "Lauren Michelle", ""], ["Valdenegro-Toro", "Matias", ""]]}, {"id": "2011.11461", "submitter": "Matias Valdenegro-Toro", "authors": "Octavio Arriaga and Matias Valdenegro-Toro", "title": "Unsupervised Difficulty Estimation with Action Scores", "comments": "2 pages, 6 figures, with appendix", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Evaluating difficulty and biases in machine learning models has become of\nextreme importance as current models are now being applied in real-world\nsituations. In this paper we present a simple method for calculating a\ndifficulty score based on the accumulation of losses for each sample during\ntraining. We call this the action score. Our proposed method does not require\nany modification of the model neither any external supervision, as it can be\nimplemented as callback that gathers information from the training process. We\ntest and analyze our approach in two different settings: image classification,\nand object detection, and we show that in both settings the action score can\nprovide insights about model and dataset biases.\n", "versions": [{"version": "v1", "created": "Mon, 23 Nov 2020 15:18:44 GMT"}], "update_date": "2020-11-24", "authors_parsed": [["Arriaga", "Octavio", ""], ["Valdenegro-Toro", "Matias", ""]]}, {"id": "2011.11479", "submitter": "Humam Alwassel", "authors": "Humam Alwassel, Silvio Giancola, Bernard Ghanem", "title": "TSP: Temporally-Sensitive Pretraining of Video Encoders for Localization\n  Tasks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to the large memory footprint of untrimmed videos, current\nstate-of-the-art video localization methods operate atop precomputed video clip\nfeatures. These features are extracted from video encoders typically trained\nfor trimmed action classification tasks, making such features not necessarily\nsuitable for temporal localization. In this work, we propose a novel supervised\npretraining paradigm for clip features that not only trains to classify\nactivities but also considers background clips and global video information to\nimprove temporal sensitivity. Extensive experiments show that using features\ntrained with our novel pretraining strategy significantly improves the\nperformance of recent state-of-the-art methods on three tasks: Temporal Action\nLocalization, Action Proposal Generation, and Dense Video Captioning. We also\nshow that our pretraining approach is effective across three encoder\narchitectures and two pretraining datasets. We believe video feature encoding\nis an important building block for localization algorithms, and extracting\ntemporally-sensitive features should be of paramount importance in building\nmore accurate models. The code and pretrained models are available on our\nproject website.\n", "versions": [{"version": "v1", "created": "Mon, 23 Nov 2020 15:40:15 GMT"}, {"version": "v2", "created": "Mon, 29 Mar 2021 15:27:30 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Alwassel", "Humam", ""], ["Giancola", "Silvio", ""], ["Ghanem", "Bernard", ""]]}, {"id": "2011.11498", "submitter": "Cheng Sun", "authors": "Cheng Sun, Min Sun, Hwann-Tzong Chen", "title": "HoHoNet: 360 Indoor Holistic Understanding with Latent Horizontal\n  Features", "comments": "preprint", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present HoHoNet, a versatile and efficient framework for holistic\nunderstanding of an indoor 360-degree panorama using a Latent Horizontal\nFeature (LHFeat). The compact LHFeat flattens the features along the vertical\ndirection and has shown success in modeling per-column modality for room layout\nreconstruction. HoHoNet advances in two important aspects. First, the deep\narchitecture is redesigned to run faster with improved accuracy. Second, we\npropose a novel horizon-to-dense module, which relaxes the per-column output\nshape constraint, allowing per-pixel dense prediction from LHFeat. HoHoNet is\nfast: It runs at 52 FPS and 110 FPS with ResNet-50 and ResNet-34 backbones\nrespectively, for modeling dense modalities from a high-resolution $512 \\times\n1024$ panorama. HoHoNet is also accurate. On the tasks of layout estimation and\nsemantic segmentation, HoHoNet achieves results on par with current\nstate-of-the-art. On dense depth estimation, HoHoNet outperforms all the prior\narts by a large margin.\n", "versions": [{"version": "v1", "created": "Mon, 23 Nov 2020 15:59:41 GMT"}, {"version": "v2", "created": "Tue, 24 Nov 2020 03:58:30 GMT"}], "update_date": "2020-11-25", "authors_parsed": [["Sun", "Cheng", ""], ["Sun", "Min", ""], ["Chen", "Hwann-Tzong", ""]]}, {"id": "2011.11506", "submitter": "Zheng Wang", "authors": "Zheng Wang, Xin Yuan, Toshihiko Yamasaki, Yutian Lin, Xin Xu, Wenjun\n  Zeng", "title": "Re-identification = Retrieval + Verification: Back to Essence and\n  Forward with a New Metric", "comments": "10 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Re-identification (re-ID) is currently investigated as a closed-world image\nretrieval task, and evaluated by retrieval based metrics. The algorithms return\nranking lists to users, but cannot tell which images are the true target. In\nessence, current re-ID overemphasizes the importance of retrieval but\nunderemphasizes that of verification, \\textit{i.e.}, all returned images are\nconsidered as the target. On the other hand, re-ID should also include the\nscenario that the query identity does not appear in the gallery. To this end,\nwe go back to the essence of re-ID, \\textit{i.e.}, a combination of retrieval\nand verification in an open-set setting, and put forward a new metric, namely,\nGenuine Open-set re-ID Metric (GOM).\n  GOM explicitly balances the effect of performing retrieval and verification\ninto a single unified metric. It can also be decomposed into a family of\nsub-metrics, enabling a clear analysis of re-ID performance. We evaluate the\neffectiveness of GOM on the re-ID benchmarks, showing its ability to capture\nimportant aspects of re-ID performance that have not been taken into account by\nestablished metrics so far. Furthermore, we show GOM scores excellent in\naligning with human visual evaluation of re-ID performance. Related codes are\navailable at https://github.com/YuanXinCherry/Person-reID-Evaluation\n", "versions": [{"version": "v1", "created": "Mon, 23 Nov 2020 16:11:19 GMT"}], "update_date": "2020-11-24", "authors_parsed": [["Wang", "Zheng", ""], ["Yuan", "Xin", ""], ["Yamasaki", "Toshihiko", ""], ["Lin", "Yutian", ""], ["Xu", "Xin", ""], ["Zeng", "Wenjun", ""]]}, {"id": "2011.11528", "submitter": "Yikai Wang", "authors": "Yikai Wang, Wenbing Huang, Bin Fang, Fuchun Sun", "title": "Elastic Interaction of Particles for Robotic Tactile Simulation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tactile sensing plays an important role in robotic perception and\nmanipulation. To overcome the real-world limitations of data collection,\nsimulating tactile response in virtual environment comes as a desire direction\nof robotic research. Most existing works model the tactile sensor as a rigid\nmulti-body, which is incapable of reflecting the elastic property of the\ntactile sensor as well as characterizing the fine-grained physical interaction\nbetween two objects. In this paper, we propose Elastic Interaction of Particles\n(EIP), a novel framework for tactile emulation. At its core, EIP models the\ntactile sensor as a group of coordinated particles, and the elastic theory is\napplied to regulate the deformation of particles during the contact process.\nThe implementation of EIP is conducted from scratch, without resorting to any\nexisting physics engine. Experiments to verify the effectiveness of our method\nhave been carried out on two applications: robotic perception with tactile data\nand 3D geometric reconstruction by tactile-visual fusion. It is possible to\nopen up a new vein for robotic tactile simulation, and contribute to various\ndownstream robotic tasks.\n", "versions": [{"version": "v1", "created": "Mon, 23 Nov 2020 16:37:00 GMT"}], "update_date": "2020-11-24", "authors_parsed": [["Wang", "Yikai", ""], ["Huang", "Wenbing", ""], ["Fang", "Bin", ""], ["Sun", "Fuchun", ""]]}, {"id": "2011.11534", "submitter": "Gyeongsik Moon", "authors": "Gyeongsik Moon and Kyoung Mu Lee", "title": "Pose2Pose: 3D Positional Pose-Guided 3D Rotational Pose Prediction for\n  Expressive 3D Human Pose and Mesh Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Previous expressive 3D human pose and mesh estimation methods mostly rely on\na single image feature vector to predict 3D rotations of human joints (i.e., 3D\nrotational pose) from an input image. However, the single image feature vector\nlacks human joint-level features. To resolve the limitation, we present\nPose2Pose, a 3D positional pose-guided 3D rotational pose prediction framework\nfor expressive 3D human pose and mesh estimation. Pose2Pose extracts the\njoint-level features on the position of human joints (i.e., positional pose)\nusing a positional pose-guided pooling, and the joint-level features are used\nfor the 3D rotational pose prediction. Our Pose2Pose is trained in an\nend-to-end manner and largely outperforms previous expressive methods. The\ncodes will be publicly available.\n", "versions": [{"version": "v1", "created": "Mon, 23 Nov 2020 16:48:35 GMT"}, {"version": "v2", "created": "Sat, 28 Nov 2020 03:30:54 GMT"}, {"version": "v3", "created": "Thu, 1 Apr 2021 00:11:39 GMT"}], "update_date": "2021-04-02", "authors_parsed": [["Moon", "Gyeongsik", ""], ["Lee", "Kyoung Mu", ""]]}, {"id": "2011.11557", "submitter": "Martin Kolarik", "authors": "Martin Kolarik, Radim Burget, Carlos M. Travieso-Gonzalez, Jan Kocica", "title": "Planar 3D Transfer Learning for End to End Unimodal MRI Unbalanced Data\n  Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel approach of 2D to 3D transfer learning based on mapping\npre-trained 2D convolutional neural network weights into planar 3D kernels. The\nmethod is validated by the proposed planar 3D res-u-net network with encoder\ntransferred from the 2D VGG-16, which is applied for a single-stage unbalanced\n3D image data segmentation. In particular, we evaluate the method on the MICCAI\n2016 MS lesion segmentation challenge dataset utilizing solely fluid-attenuated\ninversion recovery (FLAIR) sequence without brain extraction for training and\ninference to simulate real medical praxis. The planar 3D res-u-net network\nperformed the best both in sensitivity and Dice score amongst end to end\nmethods processing raw MRI scans and achieved comparable Dice score to a\nstate-of-the-art unimodal not end to end approach. Complete source code was\nreleased under the open-source license, and this paper complies with the\nMachine learning reproducibility checklist. By implementing practical transfer\nlearning for 3D data representation, we could segment heavily unbalanced data\nwithout selective sampling and achieved more reliable results using less\ntraining data in a single modality. From a medical perspective, the unimodal\napproach gives an advantage in real praxis as it does not require\nco-registration nor additional scanning time during an examination. Although\nmodern medical imaging methods capture high-resolution 3D anatomy scans\nsuitable for computer-aided detection system processing, deployment of\nautomatic systems for interpretation of radiology imaging is still rather\ntheoretical in many medical areas. Our work aims to bridge the gap by offering\na solution for partial research questions.\n", "versions": [{"version": "v1", "created": "Mon, 23 Nov 2020 17:11:50 GMT"}], "update_date": "2020-11-24", "authors_parsed": [["Kolarik", "Martin", ""], ["Burget", "Radim", ""], ["Travieso-Gonzalez", "Carlos M.", ""], ["Kocica", "Jan", ""]]}, {"id": "2011.11559", "submitter": "Martin Kolarik", "authors": "Martin Kolarik, Radim Burget, Kamil Riha", "title": "Comparing Normalization Methods for Limited Batch Size Segmentation\n  Neural Networks", "comments": null, "journal-ref": "2020 43rd International Conference on Telecommunications and\n  Signal Processing (TSP)", "doi": "10.1109/TSP49548.2020.9163397", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The widespread use of Batch Normalization has enabled training deeper neural\nnetworks with more stable and faster results. However, the Batch Normalization\nworks best using large batch size during training and as the state-of-the-art\nsegmentation convolutional neural network architectures are very memory\ndemanding, large batch size is often impossible to achieve on current hardware.\nWe evaluate the alternative normalization methods proposed to solve this issue\non a problem of binary spine segmentation from 3D CT scan. Our results show the\neffectiveness of Instance Normalization in the limited batch size neural\nnetwork training environment. Out of all the compared methods the Instance\nNormalization achieved the highest result with Dice coefficient = 0.96 which is\ncomparable to our previous results achieved by deeper network with longer\ntraining time. We also show that the Instance Normalization implementation used\nin this experiment is computational time efficient when compared to the network\nwithout any normalization method.\n", "versions": [{"version": "v1", "created": "Mon, 23 Nov 2020 17:13:24 GMT"}], "update_date": "2020-11-24", "authors_parsed": [["Kolarik", "Martin", ""], ["Burget", "Radim", ""], ["Riha", "Kamil", ""]]}, {"id": "2011.11567", "submitter": "Shaifali Parashar", "authors": "Shaifali Parashar, Yuxuan Long, Mathieu Salzmann and Pascal Fua", "title": "A Closed-Form Solution to Local Non-Rigid Structure-from-Motion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A recent trend in Non-Rigid Structure-from-Motion (NRSfM) is to express\nlocal, differential constraints between pairs of images, from which the surface\nnormal at any point can be obtained by solving a system of polynomial\nequations. The systems of equations derived in previous work, however, are of\nhigh degree, having up to five real solutions, thus requiring a computationally\nexpensive strategy to select a unique solution. Furthermore, they suffer from\ndegeneracies that make the resulting estimates unreliable, without any\nmechanism to identify this situation.\n  In this paper, we show that, under widely applicable assumptions, we can\nderive a new system of equation in terms of the surface normals whose two\nsolutions can be obtained in closed-form and can easily be disambiguated\nlocally. Our formalism further allows us to assess how reliable the estimated\nlocal normals are and, hence, to discard them if they are not. Our experiments\nshow that our reconstructions, obtained from two or more views, are\nsignificantly more accurate than those of state-of-the-art methods, while also\nbeing faster.\n", "versions": [{"version": "v1", "created": "Mon, 23 Nov 2020 17:26:19 GMT"}, {"version": "v2", "created": "Tue, 13 Jul 2021 12:59:01 GMT"}], "update_date": "2021-07-14", "authors_parsed": [["Parashar", "Shaifali", ""], ["Long", "Yuxuan", ""], ["Salzmann", "Mathieu", ""], ["Fua", "Pascal", ""]]}, {"id": "2011.11572", "submitter": "Saeid Asgari Taghanaki", "authors": "Saeid Asgari Taghanaki, Jieliang Luo, Ran Zhang, Ye Wang, Pradeep\n  Kumar Jayaraman, Krishna Murthy Jatavallabhula", "title": "RobustPointSet: A Dataset for Benchmarking Robustness of Point Cloud\n  Classifiers", "comments": "Published at the Robust and Reliable Machine Learning in the Real\n  World Workshop, ICLR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  The 3D deep learning community has seen significant strides in pointcloud\nprocessing over the last few years. However, the datasets on which deep models\nhave been trained have largely remained the same. Most datasets comprise clean,\nclutter-free pointclouds canonicalized for pose. Models trained on these\ndatasets fail in uninterpretible and unintuitive ways when presented with data\nthat contains transformations \"unseen\" at train time. While data augmentation\nenables models to be robust to \"previously seen\" input transformations, 1) we\nshow that this does not work for unseen transformations during inference, and\n2) data augmentation makes it difficult to analyze a model's inherent\nrobustness to transformations. To this end, we create a publicly available\ndataset for robustness analysis of point cloud classification models\n(independent of data augmentation) to input transformations, called\nRobustPointSet. Our experiments indicate that despite all the progress in the\npoint cloud classification, there is no single architecture that consistently\nperforms better -- several fail drastically -- when evaluated on transformed\ntest sets. We also find that robustness to unseen transformations cannot be\nbrought about merely by extensive data augmentation. RobustPointSet can be\naccessed through https://github.com/AutodeskAILab/RobustPointSet.\n", "versions": [{"version": "v1", "created": "Mon, 23 Nov 2020 17:33:53 GMT"}, {"version": "v2", "created": "Tue, 24 Nov 2020 18:34:46 GMT"}, {"version": "v3", "created": "Wed, 25 Nov 2020 01:58:25 GMT"}, {"version": "v4", "created": "Sun, 28 Mar 2021 08:17:56 GMT"}, {"version": "v5", "created": "Fri, 16 Apr 2021 09:01:08 GMT"}], "update_date": "2021-04-19", "authors_parsed": [["Taghanaki", "Saeid Asgari", ""], ["Luo", "Jieliang", ""], ["Zhang", "Ran", ""], ["Wang", "Ye", ""], ["Jayaraman", "Pradeep Kumar", ""], ["Jatavallabhula", "Krishna Murthy", ""]]}, {"id": "2011.11586", "submitter": "Angel Villar-Corrales", "authors": "Angel Villar-Corrales and Veniamin I. Morgenshtern", "title": "Scattering Transform Based Image Clustering using Projection onto\n  Orthogonal Complement", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In the last few years, large improvements in image clustering have been\ndriven by the recent advances in deep learning. However, due to the\narchitectural complexity of deep neural networks, there is no mathematical\ntheory that explains the success of deep clustering techniques. In this work we\nintroduce Projected-Scattering Spectral Clustering (PSSC), a state-of-the-art,\nstable, and fast algorithm for image clustering, which is also mathematically\ninterpretable. PSSC includes a novel method to exploit the geometric structure\nof the scattering transform of small images. This method is inspired by the\nobservation that, in the scattering transform domain, the subspaces formed by\nthe eigenvectors corresponding to the few largest eigenvalues of the data\nmatrices of individual classes are nearly shared among different classes.\nTherefore, projecting out those shared subspaces reduces the intra-class\nvariability, substantially increasing the clustering performance. We call this\nmethod Projection onto Orthogonal Complement (POC). Our experiments demonstrate\nthat PSSC obtains the best results among all shallow clustering algorithms.\nMoreover, it achieves comparable clustering performance to that of recent\nstate-of-the-art clustering techniques, while reducing the execution time by\nmore than one order of magnitude. In the spirit of reproducible research, we\npublish a high quality code repository along with the paper.\n", "versions": [{"version": "v1", "created": "Mon, 23 Nov 2020 17:59:03 GMT"}, {"version": "v2", "created": "Tue, 24 Nov 2020 19:16:39 GMT"}], "update_date": "2020-11-26", "authors_parsed": [["Villar-Corrales", "Angel", ""], ["Morgenshtern", "Veniamin I.", ""]]}, {"id": "2011.11597", "submitter": "Alon Zvirin", "authors": "Sagi Levanon, Oshry Markovich, Itamar Gozlan, Ortal Bakhshian, Alon\n  Zvirin, Yaron Honen, and Ron Kimmel", "title": "Abiotic Stress Prediction from RGB-T Images of Banana Plantlets", "comments": "Accepted paper at ECCV 2020 Workshop on Computer Vision Problems in\n  Plant Phenotyping", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Prediction of stress conditions is important for monitoring plant growth\nstages, disease detection, and assessment of crop yields. Multi-modal data,\nacquired from a variety of sensors, offers diverse perspectives and is expected\nto benefit the prediction process. We present several methods and strategies\nfor abiotic stress prediction in banana plantlets, on a dataset acquired during\na two and a half weeks period, of plantlets subject to four separate water and\nfertilizer treatments. The dataset consists of RGB and thermal images, taken\nonce daily of each plant. Results are encouraging, in the sense that neural\nnetworks exhibit high prediction rates (over $90\\%$ amongst four classes), in\ncases where there are hardly any noticeable features distinguishing the\ntreatments, much higher than field experts can supply.\n", "versions": [{"version": "v1", "created": "Mon, 23 Nov 2020 18:15:33 GMT"}], "update_date": "2020-11-24", "authors_parsed": [["Levanon", "Sagi", ""], ["Markovich", "Oshry", ""], ["Gozlan", "Itamar", ""], ["Bakhshian", "Ortal", ""], ["Zvirin", "Alon", ""], ["Honen", "Yaron", ""], ["Kimmel", "Ron", ""]]}, {"id": "2011.11600", "submitter": "Vitor Fortes Rey", "authors": "Vitor Fortes Rey, Kamalveer Kaur Garewal, Paul Lukowicz", "title": "Yet it moves: Learning from Generic Motions to Generate IMU data from\n  YouTube videos", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Human activity recognition (HAR) using wearable sensors has benefited much\nless from recent advances in Machine Learning than fields such as computer\nvision and natural language processing. This is to a large extent due to the\nlack of large scale repositories of labeled training data. In our research we\naim to facilitate the use of online videos, which exists in ample quantity for\nmost activities and are much easier to label than sensor data, to simulate\nlabeled wearable motion sensor data. In previous work we already demonstrate\nsome preliminary results in this direction focusing on very simple, activity\nspecific simulation models and a single sensor modality (acceleration\nnorm)\\cite{10.1145/3341162.3345590}. In this paper we show how we can train a\nregression model on generic motions for both accelerometer and gyro signals and\nthen apply it to videos of the target activities to generate synthetic IMU data\n(acceleration and gyro norms) that can be used to train and/or improve HAR\nmodels. We demonstrate that systems trained on simulated data generated by our\nregression model can come to within around 10% of the mean F1 score of a system\ntrained on real sensor data. Furthermore we show that by either including a\nsmall amount of real sensor data for model calibration or simply leveraging the\nfact that (in general) we can easily generate much more simulated data from\nvideo than we can collect in terms of real sensor data the advantage of real\nsensor data can be eventually equalized.\n", "versions": [{"version": "v1", "created": "Mon, 23 Nov 2020 18:16:46 GMT"}], "update_date": "2020-11-24", "authors_parsed": [["Rey", "Vitor Fortes", ""], ["Garewal", "Kamalveer Kaur", ""], ["Lukowicz", "Paul", ""]]}, {"id": "2011.11602", "submitter": "Anthony Rhodes", "authors": "Anthony D. Rhodes, Manan Goel", "title": "High Fidelity Interactive Video Segmentation Using Tensor Decomposition\n  Boundary Loss Convolutional Tessellations and Context Aware Skip Connections", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We provide a high fidelity deep learning algorithm (HyperSeg) for interactive\nvideo segmentation tasks using a convolutional network with context-aware skip\nconnections, and compressed, hypercolumn image features combined with a\nconvolutional tessellation procedure. In order to maintain high output\nfidelity, our model crucially processes and renders all image features in high\nresolution, without utilizing downsampling or pooling procedures. We maintain\nthis consistent, high grade fidelity efficiently in our model chiefly through\ntwo means: (1) We use a statistically-principled tensor decomposition procedure\nto modulate the number of hypercolumn features and (2) We render these features\nin their native resolution using a convolutional tessellation technique. For\nimproved pixel level segmentation results, we introduce a boundary loss\nfunction; for improved temporal coherence in video data, we include temporal\nimage information in our model. Through experiments, we demonstrate the\nimproved accuracy of our model against baseline models for interactive\nsegmentation tasks using high resolution video data. We also introduce a\nbenchmark video segmentation dataset, the VFX Segmentation Dataset, which\ncontains over 27,046 high resolution video frames, including greenscreen and\nvarious composited scenes with corresponding, hand crafted, pixel level\nsegmentations. Our work presents an extension to improvement to state of the\nart segmentation fidelity with high resolution data and can be used across a\nbroad range of application domains, including VFX pipelines and medical imaging\ndisciplines.\n", "versions": [{"version": "v1", "created": "Mon, 23 Nov 2020 18:21:42 GMT"}], "update_date": "2020-11-24", "authors_parsed": [["Rhodes", "Anthony D.", ""], ["Goel", "Manan", ""]]}, {"id": "2011.11603", "submitter": "Zhonghao Wang", "authors": "Zhonghao Wang, Mo Yu, Kai Wang, Jinjun Xiong, Wen-mei Hwu, Mark\n  Hasegawa-Johnson, Humphrey Shi", "title": "Interpretable Visual Reasoning via Induced Symbolic Space", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of concept induction in visual reasoning, i.e.,\nidentifying concepts and their hierarchical relationships from question-answer\npairs associated with images; and achieve an interpretable model via working on\nthe induced symbolic concept space. To this end, we first design a new\nframework named object-centric compositional attention model (OCCAM) to perform\nthe visual reasoning task with object-level visual features. Then, we come up\nwith a method to induce concepts of objects and relations using clues from the\nattention patterns between objects' visual features and question words.\nFinally, we achieve a higher level of interpretability by imposing OCCAM on the\nobjects represented in the induced symbolic concept space. Experiments on the\nCLEVR dataset demonstrate: 1) our OCCAM achieves a new state of the art without\nhuman-annotated functional programs; 2) our induced concepts are both accurate\nand sufficient as OCCAM achieves an on-par performance on objects represented\neither in visual features or in the induced symbolic concept space.\n", "versions": [{"version": "v1", "created": "Mon, 23 Nov 2020 18:21:49 GMT"}], "update_date": "2020-11-24", "authors_parsed": [["Wang", "Zhonghao", ""], ["Yu", "Mo", ""], ["Wang", "Kai", ""], ["Xiong", "Jinjun", ""], ["Hwu", "Wen-mei", ""], ["Hasegawa-Johnson", "Mark", ""], ["Shi", "Humphrey", ""]]}, {"id": "2011.11610", "submitter": "Rutwik Palaskar", "authors": "Rutwik Palaskar, Renu Vyas, Vilas Khedekar, Sangeeta Palaskar, Pranjal\n  Sahu", "title": "Transfer Learning for Oral Cancer Detection using Microscopic Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Oral cancer has more than 83% survival rate if detected in its early stages,\nhowever, only 29% of cases are currently detected early. Deep learning\ntechniques can detect patterns of oral cancer cells and can aid in its early\ndetection. In this work, we present the first results of neural networks for\noral cancer detection using microscopic images. We compare numerous\nstate-of-the-art models via transfer learning approach and collect and release\nan augmented dataset of high-quality microscopic images of oral cancer. We\npresent a comprehensive study of different models and report their performance\non this type of data. Overall, we obtain a 10-15% absolute improvement with\ntransfer learning methods compared to a simple Convolutional Neural Network\nbaseline. Ablation studies show the added benefit of data augmentation\ntechniques with finetuning for this task.\n", "versions": [{"version": "v1", "created": "Mon, 23 Nov 2020 18:35:59 GMT"}, {"version": "v2", "created": "Fri, 9 Apr 2021 18:41:26 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Palaskar", "Rutwik", ""], ["Vyas", "Renu", ""], ["Khedekar", "Vilas", ""], ["Palaskar", "Sangeeta", ""], ["Sahu", "Pranjal", ""]]}, {"id": "2011.11627", "submitter": "J. De Curt\\'o I D\\'iAz", "authors": "J. de Curt\\'o and R. Duvall", "title": "Cycle-consistent Generative Adversarial Networks for Neural Style\n  Transfer using data from Chang'E-4", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative Adversarial Networks (GANs) have had tremendous applications in\nComputer Vision. Yet, in the context of space science and planetary exploration\nthe door is open for major advances. We introduce tools to handle planetary\ndata from the mission Chang'E-4 and present a framework for Neural Style\nTransfer using Cycle-consistency from rendered images. The experiments are\nconducted in the context of the Iris Lunar Rover, a nano-rover that will be\ndeployed in lunar terrain in 2021 as the flagship of Carnegie Mellon, being the\nfirst unmanned rover of America to be on the Moon.\n", "versions": [{"version": "v1", "created": "Mon, 23 Nov 2020 18:57:27 GMT"}], "update_date": "2020-11-24", "authors_parsed": [["de Curt\u00f3", "J.", ""], ["Duvall", "R.", ""]]}, {"id": "2011.11630", "submitter": "Hala Lamdouar", "authors": "Hala Lamdouar, Charig Yang, Weidi Xie, Andrew Zisserman", "title": "Betrayed by Motion: Camouflaged Object Discovery via Motion Segmentation", "comments": "ACCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The objective of this paper is to design a computational architecture that\ndiscovers camouflaged objects in videos, specifically by exploiting motion\ninformation to perform object segmentation. We make the following three\ncontributions: (i) We propose a novel architecture that consists of two\nessential components for breaking camouflage, namely, a differentiable\nregistration module to align consecutive frames based on the background, which\neffectively emphasises the object boundary in the difference image, and a\nmotion segmentation module with memory that discovers the moving objects, while\nmaintaining the object permanence even when motion is absent at some point.\n(ii) We collect the first large-scale Moving Camouflaged Animals (MoCA) video\ndataset, which consists of over 140 clips across a diverse range of animals (67\ncategories). (iii) We demonstrate the effectiveness of the proposed model on\nMoCA, and achieve competitive performance on the unsupervised segmentation\nprotocol on DAVIS2016 by only relying on motion.\n", "versions": [{"version": "v1", "created": "Mon, 23 Nov 2020 18:59:08 GMT"}], "update_date": "2020-11-24", "authors_parsed": [["Lamdouar", "Hala", ""], ["Yang", "Charig", ""], ["Xie", "Weidi", ""], ["Zisserman", "Andrew", ""]]}, {"id": "2011.11637", "submitter": "Ilia Shumailov", "authors": "Yiren Zhao, Ilia Shumailov, Robert Mullins and Ross Anderson", "title": "Nudge Attacks on Point-Cloud DNNs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.AI cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  The wide adaption of 3D point-cloud data in safety-critical applications such\nas autonomous driving makes adversarial samples a real threat. Existing\nadversarial attacks on point clouds achieve high success rates but modify a\nlarge number of points, which is usually difficult to do in real-life\nscenarios. In this paper, we explore a family of attacks that only perturb a\nfew points of an input point cloud, and name them nudge attacks. We demonstrate\nthat nudge attacks can successfully flip the results of modern point-cloud\nDNNs. We present two variants, gradient-based and decision-based, showing their\neffectiveness in white-box and grey-box scenarios. Our extensive experiments\nshow nudge attacks are effective at generating both targeted and untargeted\nadversarial point clouds, by changing a few points or even a single point from\nthe entire point-cloud input. We find that with a single point we can reliably\nthwart predictions in 12--80% of cases, whereas 10 points allow us to further\nincrease this to 37--95%. Finally, we discuss the possible defenses against\nsuch attacks, and explore their limitations.\n", "versions": [{"version": "v1", "created": "Sun, 22 Nov 2020 18:04:02 GMT"}], "update_date": "2020-11-25", "authors_parsed": [["Zhao", "Yiren", ""], ["Shumailov", "Ilia", ""], ["Mullins", "Robert", ""], ["Anderson", "Ross", ""]]}, {"id": "2011.11674", "submitter": "Mozhdeh Rouhsedaghat", "authors": "Mozhdeh Rouhsedaghat and Yifan Wang and Shuowen Hu and Suya You and\n  C.-C. Jay Kuo", "title": "Low-Resolution Face Recognition In Resource-Constrained Environments", "comments": "11 pages, 5 figures, under consideration at Pattern Recognition\n  Letters", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A non-parametric low-resolution face recognition model for\nresource-constrained environments with limited networking and computing is\nproposed in this work. Such environments often demand a small model capable of\nbeing effectively trained on a small number of labeled data samples, with low\ntraining complexity, and low-resolution input images. To address these\nchallenges, we adopt an emerging explainable machine learning methodology\ncalled successive subspace learning (SSL).SSL offers an explainable\nnon-parametric model that flexibly trades the model size for verification\nperformance. Its training complexity is significantly lower since its model is\ntrained in a one-pass feedforward manner without backpropagation. Furthermore,\nactive learning can be conveniently incorporated to reduce the labeling cost.\nThe effectiveness of the proposed model is demonstrated by experiments on the\nLFW and the CMU Multi-PIE datasets.\n", "versions": [{"version": "v1", "created": "Mon, 23 Nov 2020 19:14:02 GMT"}], "update_date": "2020-11-25", "authors_parsed": [["Rouhsedaghat", "Mozhdeh", ""], ["Wang", "Yifan", ""], ["Hu", "Shuowen", ""], ["You", "Suya", ""], ["Kuo", "C. -C. Jay", ""]]}, {"id": "2011.11675", "submitter": "Liang-Chieh Chen", "authors": "Liang-Chieh Chen, Huiyu Wang, Siyuan Qiao", "title": "Scaling Wide Residual Networks for Panoptic Segmentation", "comments": "Update experimental results", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Wide Residual Networks (Wide-ResNets), a shallow but wide model variant\nof the Residual Networks (ResNets) by stacking a small number of residual\nblocks with large channel sizes, have demonstrated outstanding performance on\nmultiple dense prediction tasks. However, since proposed, the Wide-ResNet\narchitecture has barely evolved over the years. In this work, we revisit its\narchitecture design for the recent challenging panoptic segmentation task,\nwhich aims to unify semantic segmentation and instance segmentation. A baseline\nmodel is obtained by incorporating the simple and effective\nSqueeze-and-Excitation and Switchable Atrous Convolution to the Wide-ResNets.\nIts network capacity is further scaled up or down by adjusting the width (i.e.,\nchannel size) and depth (i.e., number of layers), resulting in a family of\nSWideRNets (short for Scaling Wide Residual Networks). We demonstrate that such\na simple scaling scheme, coupled with grid search, identifies several\nSWideRNets that significantly advance state-of-the-art performance on panoptic\nsegmentation datasets in both the fast model regime and strong model regime.\n", "versions": [{"version": "v1", "created": "Mon, 23 Nov 2020 19:14:11 GMT"}, {"version": "v2", "created": "Mon, 8 Feb 2021 04:07:27 GMT"}], "update_date": "2021-02-09", "authors_parsed": [["Chen", "Liang-Chieh", ""], ["Wang", "Huiyu", ""], ["Qiao", "Siyuan", ""]]}, {"id": "2011.11693", "submitter": "Rika Antonova", "authors": "Rika Antonova, Anastasiia Varava, Peiyang Shi, J. Frederico Carvalho,\n  Danica Kragic", "title": "Sequential Topological Representations for Predictive Models of\n  Deformable Objects", "comments": "To appear in PMLR (Proceedings of Machine Learning Research) as part\n  of L4DC (Learning for Dynamics and Control) conference proceedings", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Deformable objects present a formidable challenge for robotic manipulation\ndue to the lack of canonical low-dimensional representations and the difficulty\nof capturing, predicting, and controlling such objects. We construct compact\ntopological representations to capture the state of highly deformable objects\nthat are topologically nontrivial. We develop an approach that tracks the\nevolution of this topological state through time. Under several mild\nassumptions, we prove that the topology of the scene and its evolution can be\nrecovered from point clouds representing the scene. Our further contribution is\na method to learn predictive models that take a sequence of past point cloud\nobservations as input and predict a sequence of topological states, conditioned\non target/future control actions. Our experiments with highly deformable\nobjects in simulation show that the proposed multistep predictive models yield\nmore precise results than those obtained from computational topology libraries.\nThese models can leverage patterns inferred across various objects and offer\nfast multistep predictions suitable for real-time applications.\n", "versions": [{"version": "v1", "created": "Mon, 23 Nov 2020 19:45:15 GMT"}, {"version": "v2", "created": "Mon, 10 May 2021 20:31:42 GMT"}], "update_date": "2021-05-12", "authors_parsed": [["Antonova", "Rika", ""], ["Varava", "Anastasiia", ""], ["Shi", "Peiyang", ""], ["Carvalho", "J. Frederico", ""], ["Kragic", "Danica", ""]]}, {"id": "2011.11719", "submitter": "Abel D\\'iaz Berenguer", "authors": "Abel D\\'iaz Berenguer, Hichem Sahli, Boris Joukovsky, Maryna\n  Kvasnytsia, Ine Dirks, Mitchel Alioscha-Perez, Nikos Deligiannis, Panagiotis\n  Gonidakis, Sebasti\\'an Amador S\\'anchez, Redona Brahimetaj, Evgenia\n  Papavasileiou, Jonathan Cheung-Wai Chana, Fei Li, Shangzhen Song, Yixin Yang,\n  Sofie Tilborghs, Siri Willems, Tom Eelbode, Jeroen Bertels, Dirk\n  Vandermeulen, Frederik Maes, Paul Suetens, Lucas Fidon, Tom Vercauteren,\n  David Robben, Arne Brys, Dirk Smeets, Bart Ilsen, Nico Buls, Nina Watt\\'e,\n  Johan de Mey, Annemiek Snoeckx, Paul M. Parizel, Julien Guiot, Louis Deprez,\n  Paul Meunier, Stefaan Gryspeerdt, Kristof De Smet, Bart Jansen, Jef\n  Vandemeulebroucke", "title": "Explainable-by-design Semi-Supervised Representation Learning for\n  COVID-19 Diagnosis from CT Imaging", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Our motivating application is a real-world problem: COVID-19 classification\nfrom CT imaging, for which we present an explainable Deep Learning approach\nbased on a semi-supervised classification pipeline that employs variational\nautoencoders to extract efficient feature embedding. We have optimized the\narchitecture of two different networks for CT images: (i) a novel conditional\nvariational autoencoder (CVAE) with a specific architecture that integrates the\nclass labels inside the encoder layers and uses side information with shared\nattention layers for the encoder, which make the most of the contextual clues\nfor representation learning, and (ii) a downstream convolutional neural network\nfor supervised classification using the encoder structure of the CVAE. With the\nexplainable classification results, the proposed diagnosis system is very\neffective for COVID-19 classification. Based on the promising results obtained\nqualitatively and quantitatively, we envisage a wide deployment of our\ndeveloped technique in large-scale clinical studies.Code is available at\nhttps://git.etrovub.be/AVSP/ct-based-covid-19-diagnostic-tool.git.\n", "versions": [{"version": "v1", "created": "Mon, 23 Nov 2020 20:51:22 GMT"}, {"version": "v2", "created": "Wed, 2 Dec 2020 12:40:15 GMT"}], "update_date": "2020-12-03", "authors_parsed": [["Berenguer", "Abel D\u00edaz", ""], ["Sahli", "Hichem", ""], ["Joukovsky", "Boris", ""], ["Kvasnytsia", "Maryna", ""], ["Dirks", "Ine", ""], ["Alioscha-Perez", "Mitchel", ""], ["Deligiannis", "Nikos", ""], ["Gonidakis", "Panagiotis", ""], ["S\u00e1nchez", "Sebasti\u00e1n Amador", ""], ["Brahimetaj", "Redona", ""], ["Papavasileiou", "Evgenia", ""], ["Chana", "Jonathan Cheung-Wai", ""], ["Li", "Fei", ""], ["Song", "Shangzhen", ""], ["Yang", "Yixin", ""], ["Tilborghs", "Sofie", ""], ["Willems", "Siri", ""], ["Eelbode", "Tom", ""], ["Bertels", "Jeroen", ""], ["Vandermeulen", "Dirk", ""], ["Maes", "Frederik", ""], ["Suetens", "Paul", ""], ["Fidon", "Lucas", ""], ["Vercauteren", "Tom", ""], ["Robben", "David", ""], ["Brys", "Arne", ""], ["Smeets", "Dirk", ""], ["Ilsen", "Bart", ""], ["Buls", "Nico", ""], ["Watt\u00e9", "Nina", ""], ["de Mey", "Johan", ""], ["Snoeckx", "Annemiek", ""], ["Parizel", "Paul M.", ""], ["Guiot", "Julien", ""], ["Deprez", "Louis", ""], ["Meunier", "Paul", ""], ["Gryspeerdt", "Stefaan", ""], ["De Smet", "Kristof", ""], ["Jansen", "Bart", ""], ["Vandemeulebroucke", "Jef", ""]]}, {"id": "2011.11721", "submitter": "Christen Maximilian Filtenborg", "authors": "Maximilian Filtenborg, Efstratios Gavves, Deepak Gupta", "title": "Siamese Tracking with Lingual Object Constraints", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Classically, visual object tracking involves following a target object\nthroughout a given video, and it provides us the motion trajectory of the\nobject. However, for many practical applications, this output is often\ninsufficient since additional semantic information is required to act on the\nvideo material. Example applications of this are surveillance and\ntarget-specific video summarization, where the target needs to be monitored\nwith respect to certain predefined constraints, e.g., 'when standing near a\nyellow car'. This paper explores, tracking visual objects subjected to\nadditional lingual constraints. Differently from Li et al., we impose\nadditional lingual constraints upon tracking, which enables new applications of\ntracking. Whereas in their work the goal is to improve and extend upon tracking\nitself. To perform benchmarks and experiments, we contribute two datasets:\nc-MOT16 and c-LaSOT, curated through appending additional constraints to the\nframes of the original LaSOT and MOT16 datasets. We also experiment with two\ndeep models SiamCT-DFG and SiamCT-CA, obtained through extending a recent\nstate-of-the-art Siamese tracking method and adding modules inspired from the\nfields of natural language processing and visual question answering. Through\nexperimental results, we show that the proposed model SiamCT-CA can\nsignificantly outperform its counterparts. Furthermore, our method enables the\nselective compression of videos, based on the validity of the constraint.\n", "versions": [{"version": "v1", "created": "Mon, 23 Nov 2020 20:55:08 GMT"}], "update_date": "2020-11-25", "authors_parsed": [["Filtenborg", "Maximilian", ""], ["Gavves", "Efstratios", ""], ["Gupta", "Deepak", ""]]}, {"id": "2011.11722", "submitter": "Deepali Jain", "authors": "Deepali Jain, Atil Iscen, Ken Caluwaerts", "title": "From Pixels to Legs: Hierarchical Learning of Quadruped Locomotion", "comments": null, "journal-ref": "4th Conference on Robot Learning (CoRL 2020), Cambridge MA, USA", "doi": null, "report-no": null, "categories": "cs.RO cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Legged robots navigating crowded scenes and complex terrains in the real\nworld are required to execute dynamic leg movements while processing visual\ninput for obstacle avoidance and path planning. We show that a quadruped robot\ncan acquire both of these skills by means of hierarchical reinforcement\nlearning (HRL). By virtue of their hierarchical structure, our policies learn\nto implicitly break down this joint problem by concurrently learning High Level\n(HL) and Low Level (LL) neural network policies. These two levels are connected\nby a low dimensional hidden layer, which we call latent command. HL receives a\nfirst-person camera view, whereas LL receives the latent command from HL and\nthe robot's on-board sensors to control its actuators. We train policies to\nwalk in two different environments: a curved cliff and a maze. We show that\nhierarchical policies can concurrently learn to locomote and navigate in these\nenvironments, and show they are more efficient than non-hierarchical neural\nnetwork policies. This architecture also allows for knowledge reuse across\ntasks. LL networks trained on one task can be transferred to a new task in a\nnew environment. Finally HL, which processes camera images, can be evaluated at\nmuch lower and varying frequencies compared to LL, thus reducing computation\ntimes and bandwidth requirements.\n", "versions": [{"version": "v1", "created": "Mon, 23 Nov 2020 20:55:54 GMT"}], "update_date": "2020-12-01", "authors_parsed": [["Jain", "Deepali", ""], ["Iscen", "Atil", ""], ["Caluwaerts", "Ken", ""]]}, {"id": "2011.11724", "submitter": "Seong Hun Lee", "authors": "Seong Hun Lee, Javier Civera", "title": "Rotation-Only Bundle Adjustment", "comments": "Accepted to CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel method for estimating the global rotations of the cameras\nindependently of their positions and the scene structure. When two calibrated\ncameras observe five or more of the same points, their relative rotation can be\nrecovered independently of the translation. We extend this idea to multiple\nviews, thereby decoupling the rotation estimation from the translation and\nstructure estimation. Our approach provides several benefits such as complete\nimmunity to inaccurate translations and structure, and the accuracy improvement\nwhen used with rotation averaging. We perform extensive evaluations on both\nsynthetic and real datasets, demonstrating consistent and significant gains in\naccuracy when used with the state-of-the-art rotation averaging method.\n", "versions": [{"version": "v1", "created": "Mon, 23 Nov 2020 20:57:11 GMT"}, {"version": "v2", "created": "Sat, 27 Mar 2021 23:23:05 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Lee", "Seong Hun", ""], ["Civera", "Javier", ""]]}, {"id": "2011.11728", "submitter": "Yuhao Zhu", "authors": "Carlos Mauricio Villegas Burgos, Tianqi Yang, Nick Vamivakas, Yuhao\n  Zhu", "title": "End-to-End Framework for Efficient Deep Learning Using Metasurfaces\n  Optics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Deep learning using Convolutional Neural Networks (CNNs) has been shown to\nsignificantly out-performed many conventional vision algorithms. Despite\nefforts to increase the CNN efficiency both algorithmically and with\nspecialized hardware, deep learning remains difficult to deploy in\nresource-constrained environments. In this paper, we propose an end-to-end\nframework to explore optically compute the CNNs in free-space, much like a\ncomputational camera. Compared to existing free-space optics-based approaches\nwhich are limited to processing single-channel (i.e., grayscale) inputs, we\npropose the first general approach, based on nanoscale meta-surface optics,\nthat can process RGB data directly from the natural scenes. Our system achieves\nup to an order of magnitude energy saving, simplifies the sensor design, all\nthe while sacrificing little network accuracy.\n", "versions": [{"version": "v1", "created": "Mon, 23 Nov 2020 21:06:04 GMT"}, {"version": "v2", "created": "Fri, 21 May 2021 13:22:00 GMT"}], "update_date": "2021-05-24", "authors_parsed": [["Burgos", "Carlos Mauricio Villegas", ""], ["Yang", "Tianqi", ""], ["Vamivakas", "Nick", ""], ["Zhu", "Yuhao", ""]]}, {"id": "2011.11730", "submitter": "Tong Ke", "authors": "Tong Ke, Kejian J. Wu, and Stergios I. Roumeliotis", "title": "RISE-SLAM: A Resource-aware Inverse Schmidt Estimator for SLAM", "comments": "IROS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present the RISE-SLAM algorithm for performing\nvisual-inertial simultaneous localization and mapping (SLAM), while improving\nestimation consistency. Specifically, in order to achieve real-time operation,\nexisting approaches often assume previously-estimated states to be perfectly\nknown, which leads to inconsistent estimates. Instead, based on the idea of the\nSchmidt-Kalman filter, which has processing cost linear in the size of the\nstate vector but quadratic memory requirements, we derive a new consistent\napproximate method in the information domain, which has linear memory\nrequirements and adjustable (constant to linear) processing cost. In\nparticular, this method, the resource-aware inverse Schmidt estimator (RISE),\nallows trading estimation accuracy for computational efficiency. Furthermore,\nand in order to better address the requirements of a SLAM system during an\nexploration vs. a relocalization phase, we employ different configurations of\nRISE (in terms of the number and order of states updated) to maximize accuracy\nwhile preserving efficiency. Lastly, we evaluate the proposed RISE-SLAM\nalgorithm on publicly-available datasets and demonstrate its superiority, both\nin terms of accuracy and efficiency, as compared to alternative visual-inertial\nSLAM systems.\n", "versions": [{"version": "v1", "created": "Mon, 23 Nov 2020 21:10:32 GMT"}], "update_date": "2020-11-25", "authors_parsed": [["Ke", "Tong", ""], ["Wu", "Kejian J.", ""], ["Roumeliotis", "Stergios I.", ""]]}, {"id": "2011.11731", "submitter": "Mahmoud Afifi", "authors": "Mahmoud Afifi, Marcus A. Brubaker, Michael S. Brown", "title": "HistoGAN: Controlling Colors of GAN-Generated and Real Images via Color\n  Histograms", "comments": "CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While generative adversarial networks (GANs) can successfully produce\nhigh-quality images, they can be challenging to control. Simplifying GAN-based\nimage generation is critical for their adoption in graphic design and artistic\nwork. This goal has led to significant interest in methods that can intuitively\ncontrol the appearance of images generated by GANs. In this paper, we present\nHistoGAN, a color histogram-based method for controlling GAN-generated images'\ncolors. We focus on color histograms as they provide an intuitive way to\ndescribe image color while remaining decoupled from domain-specific semantics.\nSpecifically, we introduce an effective modification of the recent StyleGAN\narchitecture to control the colors of GAN-generated images specified by a\ntarget color histogram feature. We then describe how to expand HistoGAN to\nrecolor real images. For image recoloring, we jointly train an encoder network\nalong with HistoGAN. The recoloring model, ReHistoGAN, is an unsupervised\napproach trained to encourage the network to keep the original image's content\nwhile changing the colors based on the given target histogram. We show that\nthis histogram-based approach offers a better way to control GAN-generated and\nreal images' colors while producing more compelling results compared to\nexisting alternative strategies.\n", "versions": [{"version": "v1", "created": "Mon, 23 Nov 2020 21:14:19 GMT"}, {"version": "v2", "created": "Sat, 27 Mar 2021 02:23:03 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Afifi", "Mahmoud", ""], ["Brubaker", "Marcus A.", ""], ["Brown", "Michael S.", ""]]}, {"id": "2011.11732", "submitter": "Yun Liu", "authors": "Boris Babenko, Akinori Mitani, Ilana Traynis, Naho Kitade, Preeti\n  Singh, April Maa, Jorge Cuadros, Greg S. Corrado, Lily Peng, Dale R. Webster,\n  Avinash Varadarajan, Naama Hammel, Yun Liu", "title": "Detecting hidden signs of diabetes in external eye photographs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Diabetes-related retinal conditions can be detected by examining the\nposterior of the eye. By contrast, examining the anterior of the eye can reveal\nconditions affecting the front of the eye, such as changes to the eyelids,\ncornea, or crystalline lens. In this work, we studied whether external\nphotographs of the front of the eye can reveal insights into both diabetic\nretinal diseases and blood glucose control. We developed a deep learning system\n(DLS) using external eye photographs of 145,832 patients with diabetes from 301\ndiabetic retinopathy (DR) screening sites in one US state, and evaluated the\nDLS on three validation sets containing images from 198 sites in 18 other US\nstates. In validation set A (n=27,415 patients, all undilated), the DLS\ndetected poor blood glucose control (HbA1c > 9%) with an area under receiver\noperating characteristic curve (AUC) of 70.2; moderate-or-worse DR with an AUC\nof 75.3; diabetic macular edema with an AUC of 78.0; and vision-threatening DR\nwith an AUC of 79.4. For all 4 prediction tasks, the DLS's AUC was higher\n(p<0.001) than using available self-reported baseline characteristics (age,\nsex, race/ethnicity, years with diabetes). In terms of positive predictive\nvalue, the predicted top 5% of patients had a 67% chance of having HbA1c > 9%,\nand a 20% chance of having vision threatening diabetic retinopathy. The results\ngeneralized to dilated pupils (validation set B, 5,058 patients) and to a\ndifferent screening service (validation set C, 10,402 patients). Our results\nindicate that external eye photographs contain information useful for\nhealthcare providers managing patients with diabetes, and may help prioritize\npatients for in-person screening. Further work is needed to validate these\nfindings on different devices and patient populations (those without diabetes)\nto evaluate its utility for remote diagnosis and management.\n", "versions": [{"version": "v1", "created": "Mon, 23 Nov 2020 21:14:34 GMT"}], "update_date": "2020-11-25", "authors_parsed": [["Babenko", "Boris", ""], ["Mitani", "Akinori", ""], ["Traynis", "Ilana", ""], ["Kitade", "Naho", ""], ["Singh", "Preeti", ""], ["Maa", "April", ""], ["Cuadros", "Jorge", ""], ["Corrado", "Greg S.", ""], ["Peng", "Lily", ""], ["Webster", "Dale R.", ""], ["Varadarajan", "Avinash", ""], ["Hammel", "Naama", ""], ["Liu", "Yun", ""]]}, {"id": "2011.11734", "submitter": "Felix Richards Mr", "authors": "Felix Richards, Adeline Paiement, Xianghua Xie, Pierre-Alain Duc", "title": "Learnable Gabor modulated complex-valued networks for orientation\n  robustness", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Robustness to transformation is desirable in many computer vision tasks,\ngiven that input data often exhibits pose variance within classes. While\ntranslation invariance and equivariance is a documented phenomenon of CNNs,\nsensitivity to other transformations is typically encouraged through data\naugmentation. We investigate the modulation of complex valued convolutional\nweights with learned Gabor filters to enable orientation robustness. With Gabor\nmodulation, the designed network is able to generate orientation dependent\nfeatures free of interpolation with a single set of rotation-governing\nparameters. Moreover, by learning rotation parameters alongside traditional\nconvolutional weights, the representation space is not constrained and may\nadapt to the exact input transformation. We present Learnable Convolutional\nGabor Networks (LCGNs), that are parameter-efficient and offer increased model\ncomplexity while keeping backpropagation simple. We demonstrate that learned\nGabor modulation utilising an end-to-end complex architecture enables rotation\ninvariance and equivariance on MNIST and a new dataset of simulated images of\ngalactic cirri.\n", "versions": [{"version": "v1", "created": "Mon, 23 Nov 2020 21:22:27 GMT"}], "update_date": "2020-11-25", "authors_parsed": [["Richards", "Felix", ""], ["Paiement", "Adeline", ""], ["Xie", "Xianghua", ""], ["Duc", "Pierre-Alain", ""]]}, {"id": "2011.11735", "submitter": "Varnith Chordia", "authors": "Varnith Chordia, Vijay Kumar BG", "title": "Large Scale Multimodal Classification Using an Ensemble of Transformer\n  Models and Co-Attention", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurate and efficient product classification is significant for E-commerce\napplications, as it enables various downstream tasks such as recommendation,\nretrieval, and pricing. Items often contain textual and visual information, and\nutilizing both modalities usually outperforms classification utilizing either\nmode alone. In this paper we describe our methodology and results for the SIGIR\neCom Rakuten Data Challenge. We employ a dual attention technique to model\nimage-text relationships using pretrained language and image embeddings. While\ndual attention has been widely used for Visual Question Answering(VQA) tasks,\nours is the first attempt to apply the concept for multimodal classification.\n", "versions": [{"version": "v1", "created": "Mon, 23 Nov 2020 21:22:54 GMT"}], "update_date": "2020-11-25", "authors_parsed": [["Chordia", "Varnith", ""], ["BG", "Vijay Kumar", ""]]}, {"id": "2011.11736", "submitter": "Hamed Dashti", "authors": "Rassa Ghavami Modegh (1,2), Mehrab Hamidi (1,2), Saeed Masoudian (1),\n  Amir Mohseni (1), Hamzeh Lotfalinezhad (1), Mohammad Ali Kazemi (3), Behnaz\n  Moradi (3), Mahyar Ghafoori (4), Omid Motamedi (4), Omid Pournik (4), Kiara\n  Rezaei-Kalantari (5), Amirreza Manteghinezhad (6,7), Shaghayegh Haghjooy\n  Javanmard (6,7), Fateme Abdoli Nezhad (8), Ahmad Enhesari (8), Mohammad Saeed\n  Kheyrkhah (9), Razieh Eghtesadi (10), Javid Azadbakht (11), Akbar\n  Aliasgharzadeh (10), Mohammad Reza Sharif (12), Ali Khaleghi (13), Abbas\n  Foroutan (14), Hossein Ghanaati (3), Hamed Dashti (1), Hamid R. Rabiee (1,2)\n  ((1) AI-Med Group, AI Innovation Center, Sharif University of Technology,\n  Tehran, Iran, (2) DML Lab, Department of Computer Engineering, Sharif\n  University of Technology, Tehran, Iran, (3) Department of Radiology, Tehran\n  University of Medical Sciences, Tehran, Iran, (4) Preventive Medicine and\n  Public Health Research Center, Psychosocial Health Research Institute,\n  Community and Family Medicine Department, School of Medicine, Iran University\n  of Medical Sciences, Tehran, Iran, (5) Cardiovascular Medical and Research\n  Center, Iran University of Medical Sciences, Tehran, Iran, (6) Applied\n  Physiology Research Center, Isfahan Cardiovascular Research Institute,\n  Isfahan, Iran, (7) University of Medical Science, Isfahan, Iran, (8) Kerman\n  University of Medical Sciences, Kerman, Iran, (9) Research Institute of\n  Animal Embryo Technology, Shahrekord University, Shahrekord, Iran, (10)\n  Kashan University of Medical Sciences, Kashan, Iran, (11) Department of\n  Radiology, Kashan University of Medical Sciences, Kashan, Iran, (12)\n  Department of Pediatrics, Kashan University of Medical Sciences, Kashan,\n  Iran, (13) Department of Computer Engineering, Imam Khomeini International\n  University, Qazvin, Iran, (14) Shaheed Beheshti University of Medical\n  Sciences, Medical Academy of Science, Tehran, Iran)", "title": "Accurate and Rapid Diagnosis of COVID-19 Pneumonia with Batch Effect\n  Removal of Chest CT-Scans and Interpretable Artificial Intelligence", "comments": "27 pages, 4 figures. Some minor changes have been applied to the\n  text, some fomulae are added to help the descriptions become more clear, two\n  names and two names are corrected (The full version of the names are\n  included)", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  COVID-19 is a virus with high transmission rate that demands rapid\nidentification of the infected patients to reduce the spread of the disease.\nThe current gold-standard test, Reverse-Transcription Polymerase Chain Reaction\n(RT-PCR), has a high rate of false negatives. Diagnosing from CT-scan images as\na more accurate alternative has the challenge of distinguishing COVID-19 from\nother pneumonia diseases. Artificial intelligence can help radiologists and\nphysicians to accelerate the process of diagnosis, increase its accuracy, and\nmeasure the severity of the disease. We designed a new interpretable deep\nneural network to distinguish healthy people, patients with COVID-19, and\npatients with other pneumonia diseases from axial lung CT-scan images. Our\nmodel also detects the infected areas and calculates the percentage of the\ninfected lung volume. We first preprocessed the images to eliminate the batch\neffects of different devices, and then adopted a weakly supervised method to\ntrain the model without having any tags for the infected parts. We trained and\nevaluated the model on a large dataset of 3359 samples from 6 different medical\ncenters. The model reached sensitivities of 97.75% and 98.15%, and\nspecificities of 87% and 81.03% in separating healthy people from the diseased\nand COVID-19 from other diseases, respectively. It also demonstrated similar\nperformance for 1435 samples from 6 different medical centers which proves its\ngeneralizability. The performance of the model on a large diverse dataset, its\ngeneralizability, and interpretability makes it suitable to be used as a\nreliable diagnostic system.\n", "versions": [{"version": "v1", "created": "Mon, 23 Nov 2020 21:23:55 GMT"}, {"version": "v2", "created": "Fri, 8 Jan 2021 07:08:00 GMT"}], "update_date": "2021-01-11", "authors_parsed": [["Modegh", "Rassa Ghavami", ""], ["Hamidi", "Mehrab", ""], ["Masoudian", "Saeed", ""], ["Mohseni", "Amir", ""], ["Lotfalinezhad", "Hamzeh", ""], ["Kazemi", "Mohammad Ali", ""], ["Moradi", "Behnaz", ""], ["Ghafoori", "Mahyar", ""], ["Motamedi", "Omid", ""], ["Pournik", "Omid", ""], ["Rezaei-Kalantari", "Kiara", ""], ["Manteghinezhad", "Amirreza", ""], ["Javanmard", "Shaghayegh Haghjooy", ""], ["Nezhad", "Fateme Abdoli", ""], ["Enhesari", "Ahmad", ""], ["Kheyrkhah", "Mohammad Saeed", ""], ["Eghtesadi", "Razieh", ""], ["Azadbakht", "Javid", ""], ["Aliasgharzadeh", "Akbar", ""], ["Sharif", "Mohammad Reza", ""], ["Khaleghi", "Ali", ""], ["Foroutan", "Abbas", ""], ["Ghanaati", "Hossein", ""], ["Dashti", "Hamed", ""], ["Rabiee", "Hamid R.", ""]]}, {"id": "2011.11750", "submitter": "Ziyue Xu", "authors": "Dong Yang, Ziyue Xu, Wenqi Li, Andriy Myronenko, Holger R. Roth,\n  Stephanie Harmon, Sheng Xu, Baris Turkbey, Evrim Turkbey, Xiaosong Wang,\n  Wentao Zhu, Gianpaolo Carrafiello, Francesca Patella, Maurizio Cariati,\n  Hirofumi Obinata, Hitoshi Mori, Kaku Tamura, Peng An, Bradford J. Wood,\n  Daguang Xu", "title": "Federated Semi-Supervised Learning for COVID Region Segmentation in\n  Chest CT using Multi-National Data from China, Italy, Japan", "comments": "Accepted with minor revision to Medical Image Analysis", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The recent outbreak of COVID-19 has led to urgent needs for reliable\ndiagnosis and management of SARS-CoV-2 infection. As a complimentary tool,\nchest CT has been shown to be able to reveal visual patterns characteristic for\nCOVID-19, which has definite value at several stages during the disease course.\nTo facilitate CT analysis, recent efforts have focused on computer-aided\ncharacterization and diagnosis, which has shown promising results. However,\ndomain shift of data across clinical data centers poses a serious challenge\nwhen deploying learning-based models. In this work, we attempt to find a\nsolution for this challenge via federated and semi-supervised learning. A\nmulti-national database consisting of 1704 scans from three countries is\nadopted to study the performance gap, when training a model with one dataset\nand applying it to another. Expert radiologists manually delineated 945 scans\nfor COVID-19 findings. In handling the variability in both the data and\nannotations, a novel federated semi-supervised learning technique is proposed\nto fully utilize all available data (with or without annotations). Federated\nlearning avoids the need for sensitive data-sharing, which makes it favorable\nfor institutions and nations with strict regulatory policy on data privacy.\nMoreover, semi-supervision potentially reduces the annotation burden under a\ndistributed setting. The proposed framework is shown to be effective compared\nto fully supervised scenarios with conventional data sharing instead of model\nweight sharing.\n", "versions": [{"version": "v1", "created": "Mon, 23 Nov 2020 21:51:26 GMT"}], "update_date": "2020-11-25", "authors_parsed": [["Yang", "Dong", ""], ["Xu", "Ziyue", ""], ["Li", "Wenqi", ""], ["Myronenko", "Andriy", ""], ["Roth", "Holger R.", ""], ["Harmon", "Stephanie", ""], ["Xu", "Sheng", ""], ["Turkbey", "Baris", ""], ["Turkbey", "Evrim", ""], ["Wang", "Xiaosong", ""], ["Zhu", "Wentao", ""], ["Carrafiello", "Gianpaolo", ""], ["Patella", "Francesca", ""], ["Cariati", "Maurizio", ""], ["Obinata", "Hirofumi", ""], ["Mori", "Hitoshi", ""], ["Tamura", "Kaku", ""], ["An", "Peng", ""], ["Wood", "Bradford J.", ""], ["Xu", "Daguang", ""]]}, {"id": "2011.11757", "submitter": "Valerio Biscione", "authors": "Valerio Biscione, Jeffrey Bowers", "title": "Learning Translation Invariance in CNNs", "comments": "NeurIPS 2020 Workshop SVRHM", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When seeing a new object, humans can immediately recognize it across\ndifferent retinal locations: we say that the internal object representation is\ninvariant to translation. It is commonly believed that Convolutional Neural\nNetworks (CNNs) are architecturally invariant to translation thanks to the\nconvolution and/or pooling operations they are endowed with. In fact, several\nworks have found that these networks systematically fail to recognise new\nobjects on untrained locations. In this work we show how, even though CNNs are\nnot 'architecturally invariant' to translation, they can indeed 'learn' to be\ninvariant to translation. We verified that this can be achieved by pretraining\non ImageNet, and we found that it is also possible with much simpler datasets\nin which the items are fully translated across the input canvas. We\ninvestigated how this pretraining affected the internal network\nrepresentations, finding that the invariance was almost always acquired, even\nthough it was some times disrupted by further training due to catastrophic\nforgetting/interference. These experiments show how pretraining a network on an\nenvironment with the right 'latent' characteristics (a more naturalistic\nenvironment) can result in the network learning deep perceptual rules which\nwould dramatically improve subsequent generalization.\n", "versions": [{"version": "v1", "created": "Fri, 6 Nov 2020 09:39:27 GMT"}], "update_date": "2020-11-25", "authors_parsed": [["Biscione", "Valerio", ""], ["Bowers", "Jeffrey", ""]]}, {"id": "2011.11758", "submitter": "Keemin Sohn", "authors": "Seungyun Jeong and Keemin Sohn", "title": "Multi-regime analysis for computer vision-based traffic surveillance\n  using a change-point detection algorithm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.SY eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As a result of significant advances in deep learning, computer vision\ntechnology has been widely adopted in the field of traffic surveillance.\nNonetheless, it is difficult to find a universal model that can measure traffic\nparameters irrespective of ambient conditions such as times of the day,\nweather, or shadows. These conditions vary recurrently, but the exact points of\nchange are inconsistent and unpredictable. Thus, the application of a\nmulti-regime method would be problematic, even when separate sets of model\nparameters are prepared in advance. In the present study we devised a robust\napproach that facilitates multi-regime analysis. This approach employs an\nonline parametric algorithm to determine the change-points for ambient\nconditions. An autoencoder was used to reduce the dimensions of input images,\nand reduced feature vectors were used to implement the online change-point\nalgorithm. Seven separate periods were tagged with typical times in a given\nday. Multi-regime analysis was then performed so that the traffic density could\nbe separately measured for each period. To train and test models for vehicle\ncounting, 1,100 video images were randomly chosen for each period and labeled\nwith traffic counts. The measurement accuracy of multi-regime analysis was much\nhigher than that of an integrated model trained on all data.\n", "versions": [{"version": "v1", "created": "Sat, 7 Nov 2020 13:54:36 GMT"}], "update_date": "2020-11-25", "authors_parsed": [["Jeong", "Seungyun", ""], ["Sohn", "Keemin", ""]]}, {"id": "2011.11759", "submitter": "Baudouin Denis de Senneville PhD", "authors": "Luc Lafitte, R\\'emi Giraud, Cornel Zachiu, Mario Ries, Olivier Sutter,\n  Antoine Petit, Olivier Seror, Clair Poignard, Baudouin Denis de Senneville", "title": "Patch-based field-of-view matching in multi-modal images for\n  electroporation-based ablations", "comments": "22 pages, 9 figures", "journal-ref": "Computerized Medical Imaging and Graphics (2020)", "doi": "10.1016/j.compmedimag.2020.101750", "report-no": null, "categories": "eess.IV cs.CV cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Various multi-modal imaging sensors are currently involved at different steps\nof an interventional therapeutic work-flow. Cone beam computed tomography\n(CBCT), computed tomography (CT) or Magnetic Resonance (MR) images thereby\nprovides complementary functional and/or structural information of the targeted\nregion and organs at risk. Merging this information relies on a correct spatial\nalignment of the observed anatomy between the acquired images. This can be\nachieved by the means of multi-modal deformable image registration (DIR),\ndemonstrated to be capable of estimating dense and elastic deformations between\nimages acquired by multiple imaging devices. However, due to the typically\ndifferent field-of-view (FOV) sampled across the various imaging modalities,\nsuch algorithms may severely fail in finding a satisfactory solution.\n  In the current study we propose a new fast method to align the FOV in\nmulti-modal 3D medical images. To this end, a patch-based approach is\nintroduced and combined with a state-of-the-art multi-modal image similarity\nmetric in order to cope with multi-modal medical images. The occurrence of\nestimated patch shifts is computed for each spatial direction and the shift\nvalue with maximum occurrence is selected and used to adjust the image\nfield-of-view.\n  We show that a regional registration approach using voxel patches provides a\ngood structural compromise between the voxel-wise and \"global shifts\"\napproaches. The method was thereby beneficial for CT to CBCT and MRI to CBCT\nregistration tasks, especially when highly different image FOVs are involved.\nBesides, the benefit of the method for CT to CBCT and MRI to CBCT image\nregistration is analyzed, including the impact of artifacts generated by\npercutaneous needle insertions. Additionally, the computational needs are\ndemonstrated to be compatible with clinical constraints in the practical case\nof on-line procedures.\n", "versions": [{"version": "v1", "created": "Mon, 9 Nov 2020 11:27:45 GMT"}], "update_date": "2020-11-25", "authors_parsed": [["Lafitte", "Luc", ""], ["Giraud", "R\u00e9mi", ""], ["Zachiu", "Cornel", ""], ["Ries", "Mario", ""], ["Sutter", "Olivier", ""], ["Petit", "Antoine", ""], ["Seror", "Olivier", ""], ["Poignard", "Clair", ""], ["de Senneville", "Baudouin Denis", ""]]}, {"id": "2011.11760", "submitter": "Gabriel Huang", "authors": "Gabriel Huang, Bo Pang, Zhenhai Zhu, Clara Rivera, Radu Soricut", "title": "Multimodal Pretraining for Dense Video Captioning", "comments": "AACL-IJCNLP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning specific hands-on skills such as cooking, car maintenance, and home\nrepairs increasingly happens via instructional videos. The user experience with\nsuch videos is known to be improved by meta-information such as time-stamped\nannotations for the main steps involved. Generating such annotations\nautomatically is challenging, and we describe here two relevant contributions.\nFirst, we construct and release a new dense video captioning dataset, Video\nTimeline Tags (ViTT), featuring a variety of instructional videos together with\ntime-stamped annotations. Second, we explore several multimodal\nsequence-to-sequence pretraining strategies that leverage large unsupervised\ndatasets of videos and caption-like texts. We pretrain and subsequently\nfinetune dense video captioning models using both YouCook2 and ViTT. We show\nthat such models generalize well and are robust over a wide variety of\ninstructional videos.\n", "versions": [{"version": "v1", "created": "Tue, 10 Nov 2020 21:49:14 GMT"}], "update_date": "2020-11-25", "authors_parsed": [["Huang", "Gabriel", ""], ["Pang", "Bo", ""], ["Zhu", "Zhenhai", ""], ["Rivera", "Clara", ""], ["Soricut", "Radu", ""]]}, {"id": "2011.11765", "submitter": "Tri Huynh", "authors": "Tri Huynh, Simon Kornblith, Matthew R. Walter, Michael Maire, Maryam\n  Khademi", "title": "Boosting Contrastive Self-Supervised Learning with False Negative\n  Cancellation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Self-supervised representation learning has witnessed significant leaps\nfueled by recent progress in Contrastive learning, which seeks to learn\ntransformations that embed positive input pairs nearby, while pushing negative\npairs far apart. While positive pairs can be generated reliably (e.g., as\ndifferent views of the same image), it is difficult to accurately establish\nnegative pairs, defined as samples from different images regardless of their\nsemantic content or visual features. A fundamental problem in contrastive\nlearning is mitigating the effects of false negatives. Contrasting false\nnegatives induces two critical issues in representation learning: discarding\nsemantic information and slow convergence. In this paper, we study this problem\nin detail and propose novel approaches to mitigate the effects of false\nnegatives. The proposed methods exhibit consistent and significant improvements\nover existing contrastive learning-based models. They achieve new\nstate-of-the-art performance on ImageNet evaluations, achieving 5.8% absolute\nimprovement in top-1 accuracy over the previous state-of-the-art when\nfinetuning with 1% labels, as well as transferring to downstream tasks.\n", "versions": [{"version": "v1", "created": "Mon, 23 Nov 2020 22:17:21 GMT"}], "update_date": "2020-11-25", "authors_parsed": [["Huynh", "Tri", ""], ["Kornblith", "Simon", ""], ["Walter", "Matthew R.", ""], ["Maire", "Michael", ""], ["Khademi", "Maryam", ""]]}, {"id": "2011.11777", "submitter": "Mostafa Jahanifar", "authors": "Mostafa Jahanifar, Neda Zamani Tajeddin, Meisam Hasani, Babak\n  Shekarchi, Kamran Azema", "title": "Automatic Recognition of the Supraspinatus Tendinopathy from Ultrasound\n  Images using Convolutional Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Tendon injuries like tendinopathies, full and partial thickness tears are\nprevalent, and the supraspinatus tendon (SST) is the most vulnerable ones in\nthe rotator cuff. Early diagnosis of SST tendinopathies is of high importance\nand hard to achieve using ultrasound imaging. In this paper, an automatic\ntendinopathy recognition framework based on convolutional neural networks has\nbeen proposed to assist the diagnosis. This framework has two essential parts\nof tendon segmentation and classification. Tendon segmentation is done through\na novel network, NASUNet, which follows an encoder-decoder architecture\nparadigm and utilizes a multi-scale Enlarging cell. Moreover, a general\nclassification pipeline has been proposed for tendinopathy recognition, which\nsupports different base models as the feature extractor engine. Two feature\nmaps comprising positional information of the tendon region have been\nintroduced as the network input to make the classification network\nspatial-aware. To evaluate the tendinopathy recognition system, a data set\nconsisting of 100 SST ultrasound images have been acquired, in which\ntendinopathy cases are double-verified by magnetic resonance imaging. In both\nsegmentation and classification tasks, lack of training data has been\ncompensated by incorporating knowledge transferring, transfer learning, and\ndata augmentation techniques. In cross-validation experiments, the proposed\ntendinopathy recognition model achieves 91% accuracy, 86.67% sensitivity, and\n92.86% specificity, showing state-of-the-art performance against other models.\n", "versions": [{"version": "v1", "created": "Mon, 23 Nov 2020 22:41:41 GMT"}], "update_date": "2020-11-25", "authors_parsed": [["Jahanifar", "Mostafa", ""], ["Tajeddin", "Neda Zamani", ""], ["Hasani", "Meisam", ""], ["Shekarchi", "Babak", ""], ["Azema", "Kamran", ""]]}, {"id": "2011.11778", "submitter": "Chengyue Gong", "authors": "Chengyue Gong, Dilin Wang, Meng Li, Vikas Chandra, Qiang Liu", "title": "KeepAugment: A Simple Information-Preserving Data Augmentation Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data augmentation (DA) is an essential technique for training\nstate-of-the-art deep learning systems. In this paper, we empirically show data\naugmentation might introduce noisy augmented examples and consequently hurt the\nperformance on unaugmented data during inference. To alleviate this issue, we\npropose a simple yet highly effective approach, dubbed \\emph{KeepAugment}, to\nincrease augmented images fidelity. The idea is first to use the saliency map\nto detect important regions on the original images and then preserve these\ninformative regions during augmentation. This information-preserving strategy\nallows us to generate more faithful training examples. Empirically, we\ndemonstrate our method significantly improves on a number of prior art data\naugmentation schemes, e.g. AutoAugment, Cutout, random erasing, achieving\npromising results on image classification, semi-supervised image\nclassification, multi-view multi-camera tracking and object detection.\n", "versions": [{"version": "v1", "created": "Mon, 23 Nov 2020 22:43:04 GMT"}], "update_date": "2020-11-25", "authors_parsed": [["Gong", "Chengyue", ""], ["Wang", "Dilin", ""], ["Li", "Meng", ""], ["Chandra", "Vikas", ""], ["Liu", "Qiang", ""]]}, {"id": "2011.11779", "submitter": "Chengyue Gong", "authors": "Chengyue Gong, Dilin Wang, Qiang Liu", "title": "AlphaMatch: Improving Consistency for Semi-supervised Learning with\n  Alpha-divergence", "comments": "preprint", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Semi-supervised learning (SSL) is a key approach toward more data-efficient\nmachine learning by jointly leverage both labeled and unlabeled data. We\npropose AlphaMatch, an efficient SSL method that leverages data augmentations,\nby efficiently enforcing the label consistency between the data points and the\naugmented data derived from them. Our key technical contribution lies on: 1)\nusing alpha-divergence to prioritize the regularization on data with high\nconfidence, achieving a similar effect as FixMatch but in a more flexible\nfashion, and 2) proposing an optimization-based, EM-like algorithm to enforce\nthe consistency, which enjoys better convergence than iterative regularization\nprocedures used in recent SSL methods such as FixMatch, UDA, and MixMatch.\nAlphaMatch is simple and easy to implement, and consistently outperforms prior\narts on standard benchmarks, e.g. CIFAR-10, SVHN, CIFAR-100, STL-10.\nSpecifically, we achieve 91.3% test accuracy on CIFAR-10 with just 4 labelled\ndata per class, substantially improving over the previously best 88.7% accuracy\nachieved by FixMatch.\n", "versions": [{"version": "v1", "created": "Mon, 23 Nov 2020 22:43:45 GMT"}], "update_date": "2020-11-25", "authors_parsed": [["Gong", "Chengyue", ""], ["Wang", "Dilin", ""], ["Liu", "Qiang", ""]]}, {"id": "2011.11784", "submitter": "Charles Herrmann", "authors": "Charles Herrmann and Chen Wang and Richard Strong Bowen and Emil\n  Keyder and Michael Krainin and Ce Liu and Ramin Zabih", "title": "Robust image stitching with multiple registrations", "comments": "ECCV 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Panorama creation is one of the most widely deployed techniques in computer\nvision. In addition to industry applications such as Google Street View, it is\nalso used by millions of consumers in smartphones and other cameras.\nTraditionally, the problem is decomposed into three phases: registration, which\npicks a single transformation of each source image to align it to the other\ninputs, seam finding, which selects a source image for each pixel in the final\nresult, and blending, which fixes minor visual artifacts. Here, we observe that\nthe use of a single registration often leads to errors, especially in scenes\nwith significant depth variation or object motion. We propose instead the use\nof multiple registrations, permitting regions of the image at different depths\nto be captured with greater accuracy. MRF inference techniques naturally extend\nto seam finding over multiple registrations, and we show here that their energy\nfunctions can be readily modified with new terms that discourage duplication\nand tearing, common problems that are exacerbated by the use of multiple\nregistrations. Our techniques are closely related to layer-based stereo, and\nmove image stitching closer to explicit scene modeling. Experimental evidence\ndemonstrates that our techniques often generate significantly better panoramas\nwhen there is substantial motion or parallax.\n", "versions": [{"version": "v1", "created": "Mon, 23 Nov 2020 23:08:39 GMT"}], "update_date": "2020-11-25", "authors_parsed": [["Herrmann", "Charles", ""], ["Wang", "Chen", ""], ["Bowen", "Richard Strong", ""], ["Keyder", "Emil", ""], ["Krainin", "Michael", ""], ["Liu", "Ce", ""], ["Zabih", "Ramin", ""]]}, {"id": "2011.11787", "submitter": "David Biertimpel", "authors": "David Biertimpel, Sindi Shkodrani, Anil S. Baslamisli and N\\'ora Baka", "title": "Prior to Segment: Foreground Cues for Weakly Annotated Classes in\n  Partially Supervised Instance Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Instance segmentation methods require large datasets with expensive and thus\nlimited instance-level mask labels. Partially supervised instance segmentation\naims to improve mask prediction with limited mask labels by utilizing the more\nabundant weak box labels. In this work, we show that a class agnostic mask\nhead, commonly used in partially supervised instance segmentation, has\ndifficulties learning a general concept of foreground for the weakly annotated\nclasses using box supervision only. To resolve this problem we introduce an\nobject mask prior (OMP) that provides the mask head with the general concept of\nforeground implicitly learned by the box classification head under the\nsupervision of all classes. This helps the class agnostic mask head to focus on\nthe primary object in a region of interest (RoI) and improves generalization to\nthe weakly annotated classes. We test our approach on the COCO dataset using\ndifferent splits of strongly and weakly supervised classes. Our approach\nsignificantly improves over the Mask R-CNN baseline and obtains competitive\nperformance with the state-of-the-art, while offering a much simpler\narchitecture.\n", "versions": [{"version": "v1", "created": "Mon, 23 Nov 2020 23:15:06 GMT"}, {"version": "v2", "created": "Sat, 10 Apr 2021 22:29:26 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Biertimpel", "David", ""], ["Shkodrani", "Sindi", ""], ["Baslamisli", "Anil S.", ""], ["Baka", "N\u00f3ra", ""]]}, {"id": "2011.11789", "submitter": "Charles Herrmann", "authors": "Charles Herrmann and Chen Wang and Richard Strong Bowen and Emil\n  Keyder and Ramin Zabih", "title": "Object-centered image stitching", "comments": "ECCV 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image stitching is typically decomposed into three phases: registration,\nwhich aligns the source images with a common target image; seam finding, which\ndetermines for each target pixel the source image it should come from; and\nblending, which smooths transitions over the seams. As described in [1], the\nseam finding phase attempts to place seams between pixels where the transition\nbetween source images is not noticeable. Here, we observe that the most\nproblematic failures of this approach occur when objects are cropped, omitted,\nor duplicated. We therefore take an object-centered approach to the problem,\nleveraging recent advances in object detection [2,3,4]. We penalize candidate\nsolutions with this class of error by modifying the energy function used in the\nseam finding stage. This produces substantially more realistic stitching\nresults on challenging imagery. In addition, these methods can be used to\ndetermine when there is non-recoverable occlusion in the input data, and also\nsuggest a simple evaluation metric that can be used to evaluate the output of\nstitching algorithms.\n", "versions": [{"version": "v1", "created": "Mon, 23 Nov 2020 23:20:09 GMT"}], "update_date": "2020-11-25", "authors_parsed": [["Herrmann", "Charles", ""], ["Wang", "Chen", ""], ["Bowen", "Richard Strong", ""], ["Keyder", "Emil", ""], ["Zabih", "Ramin", ""]]}, {"id": "2011.11805", "submitter": "Andrew O'Brien", "authors": "Edward Kim, Connor Onweller, Andrew O'Brien, Kathleen McCoy", "title": "The Interpretable Dictionary in Sparse Coding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Artificial neural networks (ANNs), specifically deep learning networks, have\noften been labeled as black boxes due to the fact that the internal\nrepresentation of the data is not easily interpretable. In our work, we\nillustrate that an ANN, trained using sparse coding under specific sparsity\nconstraints, yields a more interpretable model than the standard deep learning\nmodel. The dictionary learned by sparse coding can be more easily understood\nand the activations of these elements creates a selective feature output. We\ncompare and contrast our sparse coding model with an equivalent feed forward\nconvolutional autoencoder trained on the same data. Our results show both\nqualitative and quantitative benefits in the interpretation of the learned\nsparse coding dictionary as well as the internal activation representations.\n", "versions": [{"version": "v1", "created": "Tue, 24 Nov 2020 00:26:40 GMT"}], "update_date": "2020-11-25", "authors_parsed": [["Kim", "Edward", ""], ["Onweller", "Connor", ""], ["O'Brien", "Andrew", ""], ["McCoy", "Kathleen", ""]]}, {"id": "2011.11814", "submitter": "Felix Wimbauer", "authors": "Felix Wimbauer, Nan Yang, Lukas von Stumberg, Niclas Zeller, Daniel\n  Cremers", "title": "MonoRec: Semi-Supervised Dense Reconstruction in Dynamic Environments\n  from a Single Moving Camera", "comments": "CVPR 2021, Project page with video can be found under\n  https://vision.in.tum.de/research/monorec. 14 pages, 10 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose MonoRec, a semi-supervised monocular dense\nreconstruction architecture that predicts depth maps from a single moving\ncamera in dynamic environments. MonoRec is based on a multi-view stereo setting\nwhich encodes the information of multiple consecutive images in a cost volume.\nTo deal with dynamic objects in the scene, we introduce a MaskModule that\npredicts moving object masks by leveraging the photometric inconsistencies\nencoded in the cost volumes. Unlike other multi-view stereo methods, MonoRec is\nable to reconstruct both static and moving objects by leveraging the predicted\nmasks. Furthermore, we present a novel multi-stage training scheme with a\nsemi-supervised loss formulation that does not require LiDAR depth values. We\ncarefully evaluate MonoRec on the KITTI dataset and show that it achieves\nstate-of-the-art performance compared to both multi-view and single-view\nmethods. With the model trained on KITTI, we further demonstrate that MonoRec\nis able to generalize well to both the Oxford RobotCar dataset and the more\nchallenging TUM-Mono dataset recorded by a handheld camera. Code and related\nmaterials will be available at https://vision.in.tum.de/research/monorec.\n", "versions": [{"version": "v1", "created": "Tue, 24 Nov 2020 00:40:36 GMT"}, {"version": "v2", "created": "Mon, 1 Mar 2021 22:01:43 GMT"}, {"version": "v3", "created": "Thu, 6 May 2021 09:04:06 GMT"}], "update_date": "2021-05-07", "authors_parsed": [["Wimbauer", "Felix", ""], ["Yang", "Nan", ""], ["von Stumberg", "Lukas", ""], ["Zeller", "Niclas", ""], ["Cremers", "Daniel", ""]]}, {"id": "2011.11831", "submitter": "Basile Van Hoorick", "authors": "Basile Van Hoorick, Carl Vondrick", "title": "Dissecting Image Crops", "comments": "Update: Minor rewrite of two introduction sentences", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The elementary operation of cropping underpins nearly every computer vision\nsystem, ranging from data augmentation and translation invariance to\ncomputational photography and representation learning. This paper investigates\nthe subtle traces introduced by this operation. For example, despite\nrefinements to camera optics, lenses will leave behind certain clues, notably\nchromatic aberration and vignetting. Photographers also leave behind other\nclues relating to image aesthetics and scene composition. We study how to\ndetect these traces, and investigate the impact that cropping has on the image\ndistribution. While our aim is to dissect the fundamental impact of spatial\ncrops, there are also a number of practical implications to our work, such as\ndetecting image manipulations and equipping neural network researchers with a\nbetter understanding of shortcut learning. Code is available at\nhttps://github.com/basilevh/dissecting-image-crops.\n", "versions": [{"version": "v1", "created": "Tue, 24 Nov 2020 01:33:47 GMT"}, {"version": "v2", "created": "Wed, 25 Nov 2020 02:15:07 GMT"}, {"version": "v3", "created": "Thu, 26 Nov 2020 15:39:30 GMT"}], "update_date": "2020-11-30", "authors_parsed": [["Van Hoorick", "Basile", ""], ["Vondrick", "Carl", ""]]}, {"id": "2011.11834", "submitter": "Loris Nanni", "authors": "Loris Nanni, Alessandra Lumini, Stefano Ghidoni and Gianluca Maguolo", "title": "Comparisons among different stochastic selection of activation layers\n  for convolutional neural networks for healthcare", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Classification of biological images is an important task with crucial\napplication in many fields, such as cell phenotypes recognition, detection of\ncell organelles and histopathological classification, and it might help in\nearly medical diagnosis, allowing automatic disease classification without the\nneed of a human expert. In this paper we classify biomedical images using\nensembles of neural networks. We create this ensemble using a ResNet50\narchitecture and modifying its activation layers by substituting ReLUs with\nother functions. We select our activations among the following ones: ReLU,\nleaky ReLU, Parametric ReLU, ELU, Adaptive Piecewice Linear Unit, S-Shaped\nReLU, Swish , Mish, Mexican Linear Unit, Gaussian Linear Unit, Parametric\nDeformable Linear Unit, Soft Root Sign (SRS) and others.\n  As a baseline, we used an ensemble of neural networks that only use ReLU\nactivations. We tested our networks on several small and medium sized\nbiomedical image datasets. Our results prove that our best ensemble obtains a\nbetter performance than the ones of the naive approaches. In order to encourage\nthe reproducibility of this work, the MATLAB code of all the experiments will\nbe shared at https://github.com/LorisNanni.\n", "versions": [{"version": "v1", "created": "Tue, 24 Nov 2020 01:53:39 GMT"}], "update_date": "2020-11-25", "authors_parsed": [["Nanni", "Loris", ""], ["Lumini", "Alessandra", ""], ["Ghidoni", "Stefano", ""], ["Maguolo", "Gianluca", ""]]}, {"id": "2011.11840", "submitter": "Omobayode Fagbohungbe", "authors": "Omobayode Fagbohungbe, Lijun Qian", "title": "Benchmarking Inference Performance of Deep Learning Models on Analog\n  Devices", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.AR cs.CV cs.PF", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Analog hardware implemented deep learning models are promising for\ncomputation and energy constrained systems such as edge computing devices.\nHowever, the analog nature of the device and the associated many noise sources\nwill cause changes to the value of the weights in the trained deep learning\nmodels deployed on such devices. In this study, systematic evaluation of the\ninference performance of trained popular deep learning models for image\nclassification deployed on analog devices has been carried out, where additive\nwhite Gaussian noise has been added to the weights of the trained models during\ninference. It is observed that deeper models and models with more redundancy in\ndesign such as VGG are more robust to the noise in general. However, the\nperformance is also affected by the design philosophy of the model, the\ndetailed structure of the model, the exact machine learning task, as well as\nthe datasets.\n", "versions": [{"version": "v1", "created": "Tue, 24 Nov 2020 02:14:39 GMT"}, {"version": "v2", "created": "Wed, 16 Dec 2020 22:04:52 GMT"}], "update_date": "2020-12-18", "authors_parsed": [["Fagbohungbe", "Omobayode", ""], ["Qian", "Lijun", ""]]}, {"id": "2011.11842", "submitter": "Yu-Ding Lu", "authors": "Yu-Ding Lu, Hsin-Ying Lee, Hung-Yu Tseng, Ming-Hsuan Yang", "title": "Unsupervised Discovery of Disentangled Manifolds in GANs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  As recent generative models can generate photo-realistic images, people seek\nto understand the mechanism behind the generation process. Interpretable\ngeneration process is beneficial to various image editing applications. In this\nwork, we propose a framework to discover interpretable directions in the latent\nspace given arbitrary pre-trained generative adversarial networks. We propose\nto learn the transformation from prior one-hot vectors representing different\nattributes to the latent space used by pre-trained models. Furthermore, we\napply a centroid loss function to improve consistency and smoothness while\ntraversing through different directions. We demonstrate the efficacy of the\nproposed framework on a wide range of datasets. The discovered direction\nvectors are shown to be visually corresponding to various distinct attributes\nand thus enable attribute editing.\n", "versions": [{"version": "v1", "created": "Tue, 24 Nov 2020 02:18:08 GMT"}, {"version": "v2", "created": "Sun, 29 Nov 2020 18:56:50 GMT"}], "update_date": "2020-12-01", "authors_parsed": [["Lu", "Yu-Ding", ""], ["Lee", "Hsin-Ying", ""], ["Tseng", "Hung-Yu", ""], ["Yang", "Ming-Hsuan", ""]]}, {"id": "2011.11844", "submitter": "Naoya Takahashi", "authors": "Naoya Takahashi, Yuki Mitsufuji", "title": "Densely connected multidilated convolutional networks for dense\n  prediction tasks", "comments": "Accepted to CVPR 2021. arXiv admin note: text overlap with\n  arXiv:2010.01733", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Tasks that involve high-resolution dense prediction require a modeling of\nboth local and global patterns in a large input field. Although the local and\nglobal structures often depend on each other and their simultaneous modeling is\nimportant, many convolutional neural network (CNN)-based approaches interchange\nrepresentations in different resolutions only a few times. In this paper, we\nclaim the importance of a dense simultaneous modeling of multiresolution\nrepresentation and propose a novel CNN architecture called densely connected\nmultidilated DenseNet (D3Net). D3Net involves a novel multidilated convolution\nthat has different dilation factors in a single layer to model different\nresolutions simultaneously. By combining the multidilated convolution with the\nDenseNet architecture, D3Net incorporates multiresolution learning with an\nexponentially growing receptive field in almost all layers, while avoiding the\naliasing problem that occurs when we naively incorporate the dilated\nconvolution in DenseNet. Experiments on the image semantic segmentation task\nusing Cityscapes and the audio source separation task using MUSDB18 show that\nthe proposed method has superior performance over state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Sat, 21 Nov 2020 05:15:12 GMT"}, {"version": "v2", "created": "Wed, 9 Jun 2021 00:31:49 GMT"}], "update_date": "2021-06-10", "authors_parsed": [["Takahashi", "Naoya", ""], ["Mitsufuji", "Yuki", ""]]}, {"id": "2011.11857", "submitter": "J\\'er\\^ome Rony", "authors": "J\\'er\\^ome Rony, Eric Granger, Marco Pedersoli, Ismail Ben Ayed", "title": "Augmented Lagrangian Adversarial Attacks", "comments": "Code available at\n  https://github.com/jeromerony/augmented_lagrangian_adversarial_attacks", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adversarial attack algorithms are dominated by penalty methods, which are\nslow in practice, or more efficient distance-customized methods, which are\nheavily tailored to the properties of the considered distance. We propose a\nwhite-box attack algorithm to generate minimally perturbed adversarial examples\nbased on Augmented Lagrangian principles. We bring several non-trivial\nalgorithmic modifications, which have a crucial effect on performance. Our\nattack enjoys the generality of penalty methods and the computational\nefficiency of distance-customized algorithms, and can be readily used for a\nwide set of distances. We compare our attack to state-of-the-art methods on\nthree datasets and several models, and consistently obtain competitive\nperformances with similar or lower computational complexity.\n", "versions": [{"version": "v1", "created": "Tue, 24 Nov 2020 02:51:08 GMT"}], "update_date": "2020-11-25", "authors_parsed": [["Rony", "J\u00e9r\u00f4me", ""], ["Granger", "Eric", ""], ["Pedersoli", "Marco", ""], ["Ayed", "Ismail Ben", ""]]}, {"id": "2011.11858", "submitter": "Hexin Bai", "authors": "Hexin Bai, Wensheng Cheng, Peng Chu, Juehuan Liu, Kai Zhang, Haibin\n  Ling", "title": "GMOT-40: A Benchmark for Generic Multiple Object Tracking", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multiple Object Tracking (MOT) has witnessed remarkable advances in recent\nyears. However, existing studies dominantly request prior knowledge of the\ntracking target, and hence may not generalize well to unseen categories. In\ncontrast, Generic Multiple Object Tracking (GMOT), which requires little prior\ninformation about the target, is largely under-explored. In this paper, we make\ncontributions to boost the study of GMOT in three aspects. First, we construct\nthe first public GMOT dataset, dubbed GMOT-40, which contains 40 carefully\nannotated sequences evenly distributed among 10 object categories. In addition,\ntwo tracking protocols are adopted to evaluate different characteristics of\ntracking algorithms. Second, by noting the lack of devoted tracking algorithms,\nwe have designed a series of baseline GMOT algorithms. Third, we perform a\nthorough evaluation on GMOT-40, involving popular MOT algorithms (with\nnecessary modifications) and the proposed baselines. We will release the\nGMOT-40 benchmark, the evaluation results, as well as the baseline algorithm to\nthe public upon the publication of the paper.\n", "versions": [{"version": "v1", "created": "Tue, 24 Nov 2020 02:51:46 GMT"}, {"version": "v2", "created": "Wed, 2 Dec 2020 17:40:03 GMT"}, {"version": "v3", "created": "Wed, 7 Apr 2021 19:13:00 GMT"}], "update_date": "2021-04-09", "authors_parsed": [["Bai", "Hexin", ""], ["Cheng", "Wensheng", ""], ["Chu", "Peng", ""], ["Liu", "Juehuan", ""], ["Zhang", "Kai", ""], ["Ling", "Haibin", ""]]}, {"id": "2011.11865", "submitter": "Chuhua Xian", "authors": "Chuhua Xian, Kun Qian, Zitian Zhang, and Charlie C.L. Wang", "title": "Multi-Scale Progressive Fusion Learning for Depth Map Super-Resolution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Limited by the cost and technology, the resolution of depth map collected by\ndepth camera is often lower than that of its associated RGB camera. Although\nthere have been many researches on RGB image super-resolution (SR), a major\nproblem with depth map super-resolution is that there will be obvious jagged\nedges and excessive loss of details. To tackle these difficulties, in this\nwork, we propose a multi-scale progressive fusion network for depth map SR,\nwhich possess an asymptotic structure to integrate hierarchical features in\ndifferent domains. Given a low-resolution (LR) depth map and its associated\nhigh-resolution (HR) color image, We utilize two different branches to achieve\nmulti-scale feature learning. Next, we propose a step-wise fusion strategy to\nrestore the HR depth map. Finally, a multi-dimensional loss is introduced to\nconstrain clear boundaries and details. Extensive experiments show that our\nproposed method produces improved results against state-of-the-art methods both\nqualitatively and quantitatively.\n", "versions": [{"version": "v1", "created": "Tue, 24 Nov 2020 03:03:07 GMT"}], "update_date": "2020-11-25", "authors_parsed": [["Xian", "Chuhua", ""], ["Qian", "Kun", ""], ["Zhang", "Zitian", ""], ["Wang", "Charlie C. L.", ""]]}, {"id": "2011.11872", "submitter": "Arman Afrasiyabi", "authors": "Arman Afrasiyabi, Jean-Fran\\c{c}ois Lalonde, Christian Gagn\\'e", "title": "Persistent Mixture Model Networks for Few-Shot Image Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce Persistent Mixture Model (PMM) networks for representation\nlearning in the few-shot image classification context. While previous methods\nrepresent classes with a single centroid or rely on post hoc clustering\nmethods, our method learns a mixture model for each base class jointly with the\ndata representation in an end-to-end manner. The PMM training algorithm is\norganized into two main stages: 1) initial training and 2) progressive\nfollowing. First, the initial estimate for multi-component mixtures is learned\nfor each class in the base domain using a combination of two loss functions\n(competitive and collaborative). The resulting network is then progressively\nrefined through a leader-follower learning procedure, which uses the current\nestimate of the learner as a fixed \"target\" network. This target network is\nused to make a consistent assignment of instances to mixture components, in\norder to increase performance while stabilizing the training. The effectiveness\nof our joint representation/mixture learning approach is demonstrated with\nextensive experiments on four standard datasets and four backbones. In\nparticular, we demonstrate that when we combine our robust representation with\nrecent alignment- and margin-based approaches, we achieve new state-of-the-art\nresults in the inductive setting, with an absolute accuracy for 5-shot\nclassification of 82.45% on miniImageNet, 88.20% with tieredImageNet, and\n60.70% in FC100, all using the ResNet-12 backbone.\n", "versions": [{"version": "v1", "created": "Tue, 24 Nov 2020 03:16:27 GMT"}], "update_date": "2020-11-25", "authors_parsed": [["Afrasiyabi", "Arman", ""], ["Lalonde", "Jean-Fran\u00e7ois", ""], ["Gagn\u00e9", "Christian", ""]]}, {"id": "2011.11879", "submitter": "Cheng Jiang", "authors": "Cheng Jiang (1), Jun Liao (1), Pei Dong (1), Zhaoxuan Ma (1), De Cai\n  (1), Guoan Zheng (2), Yueping Liu (3), Hong Bu (4 and 5) and Jianhua Yao (1)\n  ((1) Tencent AI Lab, Shenzhen, China,(2) Department of Biomedical\n  Engineering, University of Connecticut, Storrs, CT, USA,(3) Department of\n  Pathology, The Fourth Hospital of Hebei Medical University, Hebei, China,(4)\n  Department of Pathology, West China Hospital, Sichuan University, Chengdu,\n  China,(5) Laboratory of Pathology, Clinical Research Centre for Breast, West\n  China Hospital, Sichuan University, Chengdu, China.)", "title": "Blind deblurring for microscopic pathology images using deep learning\n  networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Artificial Intelligence (AI)-powered pathology is a revolutionary step in the\nworld of digital pathology and shows great promise to increase both diagnosis\naccuracy and efficiency. However, defocus and motion blur can obscure tissue or\ncell characteristics hence compromising AI algorithms'accuracy and robustness\nin analyzing the images. In this paper, we demonstrate a deep-learning-based\napproach that can alleviate the defocus and motion blur of a microscopic image\nand output a sharper and cleaner image with retrieved fine details without\nprior knowledge of the blur type, blur extent and pathological stain. In this\napproach, a deep learning classifier is first trained to identify the image\nblur type. Then, two encoder-decoder networks are trained and used alone or in\ncombination to deblur the input image. It is an end-to-end approach and\nintroduces no corrugated artifacts as traditional blind deconvolution methods\ndo. We test our approach on different types of pathology specimens and\ndemonstrate great performance on image blur correction and the subsequent\nimprovement on the diagnosis outcome of AI algorithms.\n", "versions": [{"version": "v1", "created": "Tue, 24 Nov 2020 03:52:45 GMT"}], "update_date": "2020-11-25", "authors_parsed": [["Jiang", "Cheng", "", "4 and 5"], ["Liao", "Jun", "", "4 and 5"], ["Dong", "Pei", "", "4 and 5"], ["Ma", "Zhaoxuan", "", "4 and 5"], ["Cai", "De", "", "4 and 5"], ["Zheng", "Guoan", "", "4 and 5"], ["Liu", "Yueping", "", "4 and 5"], ["Bu", "Hong", "", "4 and 5"], ["Yao", "Jianhua", ""]]}, {"id": "2011.11890", "submitter": "Mahmoud Afifi", "authors": "Mahmoud Afifi, Jonathan T. Barron, Chloe LeGendre, Yun-Ta Tsai,\n  Francois Bleibel", "title": "Cross-Camera Convolutional Color Constancy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present \"Cross-Camera Convolutional Color Constancy\" (C5), a\nlearning-based method, trained on images from multiple cameras, that accurately\nestimates a scene's illuminant color from raw images captured by a new camera\npreviously unseen during training. C5 is a hypernetwork-like extension of the\nconvolutional color constancy (CCC) approach: C5 learns to generate the weights\nof a CCC model that is then evaluated on the input image, with the CCC weights\ndynamically adapted to different input content. Unlike prior cross-camera color\nconstancy models, which are usually designed to be agnostic to the spectral\nproperties of test-set images from unobserved cameras, C5 approaches this\nproblem through the lens of transductive inference: additional unlabeled images\nare provided as input to the model at test time, which allows the model to\ncalibrate itself to the spectral properties of the test-set camera during\ninference. C5 achieves state-of-the-art accuracy for cross-camera color\nconstancy on several datasets, is fast to evaluate (~7 and ~90 ms per image on\na GPU or CPU, respectively), and requires little memory (~2 MB), and, thus, is\na practical solution to the problem of calibration-free automatic white balance\nfor mobile photography.\n", "versions": [{"version": "v1", "created": "Tue, 24 Nov 2020 04:37:22 GMT"}], "update_date": "2020-11-25", "authors_parsed": [["Afifi", "Mahmoud", ""], ["Barron", "Jonathan T.", ""], ["LeGendre", "Chloe", ""], ["Tsai", "Yun-Ta", ""], ["Bleibel", "Francois", ""]]}, {"id": "2011.11893", "submitter": "Baifeng Shi", "authors": "Baifeng Shi, Qi Dai, Judy Hoffman, Kate Saenko, Trevor Darrell,\n  Huijuan Xu", "title": "Temporal Action Detection with Multi-level Supervision", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Training temporal action detection in videos requires large amounts of\nlabeled data, yet such annotation is expensive to collect. Incorporating\nunlabeled or weakly-labeled data to train action detection model could help\nreduce annotation cost. In this work, we first introduce the Semi-supervised\nAction Detection (SSAD) task with a mixture of labeled and unlabeled data and\nanalyze different types of errors in the proposed SSAD baselines which are\ndirectly adapted from the semi-supervised classification task. To alleviate the\nmain error of action incompleteness (i.e., missing parts of actions) in SSAD\nbaselines, we further design an unsupervised foreground attention (UFA) module\nutilizing the \"independence\" between foreground and background motion. Then we\nincorporate weakly-labeled data into SSAD and propose Omni-supervised Action\nDetection (OSAD) with three levels of supervision. An information bottleneck\n(IB) suppressing the scene information in non-action frames while preserving\nthe action information is designed to help overcome the accompanying\naction-context confusion problem in OSAD baselines. We extensively benchmark\nagainst the baselines for SSAD and OSAD on our created data splits in THUMOS14\nand ActivityNet1.2, and demonstrate the effectiveness of the proposed UFA and\nIB methods. Lastly, the benefit of our full OSAD-IB model under limited\nannotation budgets is shown by exploring the optimal annotation strategy for\nlabeled, unlabeled and weakly-labeled data.\n", "versions": [{"version": "v1", "created": "Tue, 24 Nov 2020 04:45:17 GMT"}, {"version": "v2", "created": "Thu, 18 Feb 2021 10:23:05 GMT"}], "update_date": "2021-02-19", "authors_parsed": [["Shi", "Baifeng", ""], ["Dai", "Qi", ""], ["Hoffman", "Judy", ""], ["Saenko", "Kate", ""], ["Darrell", "Trevor", ""], ["Xu", "Huijuan", ""]]}, {"id": "2011.11900", "submitter": "Jeong-Gi Kwak", "authors": "Jeong-gi Kwak, David K. Han, Hanseok Ko", "title": "CAFE-GAN: Arbitrary Face Attribute Editing with Complementary Attention\n  Feature", "comments": null, "journal-ref": "European Conference on Computer Vision (ECCV), 2020", "doi": "10.1007/978-3-030-58568-6_31", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The goal of face attribute editing is altering a facial image according to\ngiven target attributes such as hair color, mustache, gender, etc. It belongs\nto the image-to-image domain transfer problem with a set of attributes\nconsidered as a distinctive domain. There have been some works in multi-domain\ntransfer problem focusing on facial attribute editing employing Generative\nAdversarial Network (GAN). These methods have reported some successes but they\nalso result in unintended changes in facial regions - meaning the generator\nalters regions unrelated to the specified attributes. To address this\nunintended altering problem, we propose a novel GAN model which is designed to\nedit only the parts of a face pertinent to the target attributes by the concept\nof Complementary Attention Feature (CAFE). CAFE identifies the facial regions\nto be transformed by considering both target attributes as well as\ncomplementary attributes, which we define as those attributes absent in the\ninput facial image. In addition, we introduce a complementary feature matching\nto help in training the generator for utilizing the spatial information of\nattributes. Effectiveness of the proposed method is demonstrated by analysis\nand comparison study with state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Tue, 24 Nov 2020 05:21:03 GMT"}], "update_date": "2020-11-25", "authors_parsed": [["Kwak", "Jeong-gi", ""], ["Han", "David K.", ""], ["Ko", "Hanseok", ""]]}, {"id": "2011.11906", "submitter": "Chong Chen", "authors": "Chong Chen", "title": "Spatiotemporal Imaging with Diffeomorphic Optimal Transportation", "comments": "39 pages, 7 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.CV physics.med-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a variational model with diffeomorphic optimal transportation for\njoint image reconstruction and motion estimation. The proposed model is a\nproduction of assembling the Wasserstein distance with the Benamou--Brenier\nformula in optimal transportation and the flow of diffeomorphisms involved in\nlarge deformation diffeomorphic metric mapping, which is suitable for the\nscenario of spatiotemporal imaging with large diffeomorphic and mass-preserving\ndeformations. Specifically, we first use the Benamou--Brenier formula to\ncharacterize the optimal transport cost among the flow of mass-preserving\nimages, and restrict the velocity field into the admissible Hilbert space to\nguarantee the generated deformation flow being diffeomorphic. We then gain the\nODE-constrained equivalent formulation for Benamou--Brenier formula. We finally\nobtain the proposed model with ODE constraint following the framework that\npresented in our previous work. We further get the equivalent PDE-constrained\noptimal control formulation. The proposed model is compared against several\nexisting alternatives theoretically. The alternating minimization algorithm is\npresented for solving the time-discretized version of the proposed model with\nODE constraint. Several important issues on the proposed model and associated\nalgorithms are also discussed. Particularly, we present several potential\nmodels based on the proposed diffeomorphic optimal transportation. Under\nappropriate conditions, the proposed algorithm also provides a new scheme to\nsolve the models using quadratic Wasserstein distance. The performance is\nfinally evaluated by several numerical experiments in space-time tomography,\nwhere the data is measured from the concerned sequential images with sparse\nviews and/or various noise levels.\n", "versions": [{"version": "v1", "created": "Tue, 24 Nov 2020 05:55:25 GMT"}], "update_date": "2020-11-25", "authors_parsed": [["Chen", "Chong", ""]]}, {"id": "2011.11912", "submitter": "Noriaki Hirose", "authors": "Noriaki Hirose, Shun Taguchi, Keisuke Kawano, Satoshi Koide", "title": "Variational Monocular Depth Estimation for Reliability Prediction", "comments": "17 pages, 11 figures, 7 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Self-supervised learning for monocular depth estimation is widely\ninvestigated as an alternative to supervised learning approach, that requires a\nlot of ground truths. Previous works have successfully improved the accuracy of\ndepth estimation by modifying the model structure, adding objectives, and\nmasking dynamic objects and occluded area. However, when using such estimated\ndepth image in applications, such as autonomous vehicles, and robots, we have\nto uniformly believe the estimated depth at each pixel position. This could\nlead to fatal errors in performing the tasks, because estimated depth at some\npixels may make a bigger mistake. In this paper, we theoretically formulate a\nvariational model for the monocular depth estimation to predict the reliability\nof the estimated depth image. Based on the results, we can exclude the\nestimated depths with low reliability or refine them for actual use. The\neffectiveness of the proposed method is quantitatively and qualitatively\ndemonstrated using the KITTI benchmark and Make3D dataset.\n", "versions": [{"version": "v1", "created": "Tue, 24 Nov 2020 06:23:51 GMT"}], "update_date": "2020-11-25", "authors_parsed": [["Hirose", "Noriaki", ""], ["Taguchi", "Shun", ""], ["Kawano", "Keisuke", ""], ["Koide", "Satoshi", ""]]}, {"id": "2011.11946", "submitter": "Martin Humenberger", "authors": "No\\'e Pion, Martin Humenberger, Gabriela Csurka, Yohann Cabon, Torsten\n  Sattler", "title": "Benchmarking Image Retrieval for Visual Localization", "comments": "International Conference on 3D Vision, 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Visual localization, i.e., camera pose estimation in a known scene, is a core\ncomponent of technologies such as autonomous driving and augmented reality.\nState-of-the-art localization approaches often rely on image retrieval\ntechniques for one of two tasks: (1) provide an approximate pose estimate or\n(2) determine which parts of the scene are potentially visible in a given query\nimage. It is common practice to use state-of-the-art image retrieval algorithms\nfor these tasks. These algorithms are often trained for the goal of retrieving\nthe same landmark under a large range of viewpoint changes. However, robustness\nto viewpoint changes is not necessarily desirable in the context of visual\nlocalization. This paper focuses on understanding the role of image retrieval\nfor multiple visual localization tasks. We introduce a benchmark setup and\ncompare state-of-the-art retrieval representations on multiple datasets. We\nshow that retrieval performance on classical landmark retrieval/recognition\ntasks correlates only for some but not all tasks to localization performance.\nThis indicates a need for retrieval approaches specifically designed for\nlocalization tasks. Our benchmark and evaluation protocols are available at\nhttps://github.com/naver/kapture-localization.\n", "versions": [{"version": "v1", "created": "Tue, 24 Nov 2020 07:59:52 GMT"}, {"version": "v2", "created": "Tue, 1 Dec 2020 07:19:03 GMT"}], "update_date": "2020-12-02", "authors_parsed": [["Pion", "No\u00e9", ""], ["Humenberger", "Martin", ""], ["Csurka", "Gabriela", ""], ["Cabon", "Yohann", ""], ["Sattler", "Torsten", ""]]}, {"id": "2011.11952", "submitter": "Hao Zheng", "authors": "Hao Zheng, Yulei Qin, Yun Gu, Fangfang Xie, Jie Yang, Jiayuan Sun,\n  Guang-zhong Yang", "title": "Alleviating Class-wise Gradient Imbalance for Pulmonary Airway\n  Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automated airway segmentation is a prerequisite for pre-operative diagnosis\nand intra-operative navigation for pulmonary intervention. Due to the small\nsize and scattered spatial distribution of peripheral bronchi, this is hampered\nby severe class imbalance between foreground and background regions, which\nmakes it challenging for CNN-based methods to parse distal small airways. In\nthis paper, we demonstrate that this problem is arisen by gradient erosion and\ndilation of the neighborhood voxels. During back-propagation, if the ratio of\nthe foreground gradient to background gradient is small while the class\nimbalance is local, the foreground gradients can be eroded by their\nneighborhoods. This process cumulatively increases the noise information\nincluded in the gradient flow from top layers to the bottom ones, limiting the\nlearning of small structures in CNNs. To alleviate this problem, we use group\nsupervision and the corresponding WingsNet to provide complementary gradient\nflows to enhance the training of shallow layers. To further address the\nintra-class imbalance between large and small airways, we design a General\nUnion loss function which obviates the impact of airway size by distance-based\nweights and adaptively tunes the gradient ratio based on the learning process.\nExtensive experiments on public datasets demonstrate that the proposed method\ncan predict the airway structures with higher accuracy and better morphological\ncompleteness than the baselines.\n", "versions": [{"version": "v1", "created": "Tue, 24 Nov 2020 08:14:38 GMT"}, {"version": "v2", "created": "Thu, 29 Apr 2021 10:15:37 GMT"}], "update_date": "2021-04-30", "authors_parsed": [["Zheng", "Hao", ""], ["Qin", "Yulei", ""], ["Gu", "Yun", ""], ["Xie", "Fangfang", ""], ["Yang", "Jie", ""], ["Sun", "Jiayuan", ""], ["Yang", "Guang-zhong", ""]]}, {"id": "2011.11953", "submitter": "Wenhao Wang", "authors": "Wenhao Wang, Shengcai Liao, Fang Zhao, Cuicui Kang, Ling Shao", "title": "DomainMix: Learning Generalizable Person Re-Identification Without Human\n  Annotations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Existing person re-identification methods often have low generalizability,\nwhich is mostly due to the limited availability of large-scale labeled training\ndata. However, labeling large-scale training data is very expensive and\ntime-consuming. To address this, this paper presents a solution, called\nDomainMix, which can learn a person re-identification model from both synthetic\nand real-world data, for the first time, completely without human annotations.\nThis way, the proposed method enjoys the cheap availability of large-scale\ntraining data, and benefiting from its scalability and diversity, the learned\nmodel is able to generalize well on unseen domains. Specifically, inspired from\na recent work generating large-scale synthetic data for effective person\nre-identification training, in each epoch, the proposed method firstly clusters\nthe unlabeled real-world images and select the reliable clusters according to\nthree criteria, i.e. independence, compactness, and quantity. Then, the\nclassification layer is initialized adaptively using the generated features of\nreal-world images. When training, to address the large domain gap between two\ndomains, a domain-invariant feature learning method is proposed, which designs\nan adversarial learning between domain-invariant feature learning and domain\ndiscrimination, and meanwhile learns a discriminative feature for person\nre-identification. This way, the domain gap between synthetic and real-world\ndata is much reduced, and the learned feature is generalizable thanks to the\nlarge-scale and diverse training data. Experimental results show that the\nproposed annotation-free method is more or less comparable to the counterpart\ntrained with full human annotations, which is quite promising. In addition, it\nachieves the current state of the art on several person re-identification\ndatasets under direct cross-dataset evaluation.\n", "versions": [{"version": "v1", "created": "Tue, 24 Nov 2020 08:15:53 GMT"}, {"version": "v2", "created": "Sun, 21 Mar 2021 01:53:47 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Wang", "Wenhao", ""], ["Liao", "Shengcai", ""], ["Zhao", "Fang", ""], ["Kang", "Cuicui", ""], ["Shao", "Ling", ""]]}, {"id": "2011.11954", "submitter": "Fredrik Westling", "authors": "Fredrik Westling, Mitch Bryson, James Underwood", "title": "SimTreeLS: Simulating aerial and terrestrial laser scans of trees", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There are numerous emerging applications for digitizing trees using\nterrestrial and aerial laser scanning, particularly in the fields of\nagriculture and forestry. Interpretation of LiDAR point clouds is increasingly\nrelying on data-driven methods (such as supervised machine learning) that rely\non large quantities of hand-labelled data. As this data is potentially\nexpensive to capture, and difficult to clearly visualise and label manually, a\nmeans of supplementing real LiDAR scans with simulated data is becoming a\nnecessary step in realising the potential of these methods. We present an open\nsource tool, SimTreeLS (Simulated Tree Laser Scans), for generating point\nclouds which simulate scanning with user-defined sensor, trajectory, tree shape\nand layout parameters. Upon simulation, material classification is kept in a\npointwise fashion so leaf and woody matter are perfectly known, and unique\nidentifiers separate individual trees, foregoing post-simulation labelling.\nThis allows for an endless supply of procedurally generated data with similar\ncharacteristics to real LiDAR captures, which can then be used for development\nof data processing techniques or training of machine learning algorithms. To\nvalidate our method, we compare the characteristics of a simulated scan with a\nreal scan using similar trees and the same sensor and trajectory parameters.\nResults suggest the simulated data is significantly more similar to real data\nthan a sample-based control. We also demonstrate application of SimTreeLS on\ncontexts beyond the real data available, simulating scans of new tree shapes,\nnew trajectories and new layouts, with results presenting well. SimTreeLS is\navailable as an open source resource built on publicly available libraries.\n", "versions": [{"version": "v1", "created": "Tue, 24 Nov 2020 08:25:42 GMT"}], "update_date": "2020-11-25", "authors_parsed": [["Westling", "Fredrik", ""], ["Bryson", "Mitch", ""], ["Underwood", "James", ""]]}, {"id": "2011.11956", "submitter": "Alex Ling Yu Hung", "authors": "Alex Ling Yu Hung, Wanwen Chen, John Galeotti", "title": "Ultrasound Confidence Maps of Intensity and Structure Based on Directed\n  Acyclic Graphs and Artifact Models", "comments": "5 pages, conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ultrasound imaging has been improving, but continues to suffer from inherent\nartifacts that are challenging to model, such as attenuation, shadowing,\ndiffraction, speckle, etc. These artifacts can potentially confuse image\nanalysis algorithms unless an attempt is made to assess the certainty of\nindividual pixel values. Our novel confidence algorithms analyze pixel values\nusing a directed acyclic graph based on acoustic physical properties of\nultrasound imaging. We demonstrate unique capabilities of our approach and\ncompare it against previous confidence-measurement algorithms for\nshadow-detection and image-compounding tasks.\n", "versions": [{"version": "v1", "created": "Tue, 24 Nov 2020 08:32:56 GMT"}, {"version": "v2", "created": "Mon, 22 Mar 2021 16:17:28 GMT"}, {"version": "v3", "created": "Wed, 14 Apr 2021 05:06:47 GMT"}, {"version": "v4", "created": "Tue, 27 Apr 2021 18:52:10 GMT"}], "update_date": "2021-04-29", "authors_parsed": [["Hung", "Alex Ling Yu", ""], ["Chen", "Wanwen", ""], ["Galeotti", "John", ""]]}, {"id": "2011.11957", "submitter": "Yingpeng Deng", "authors": "Yingpeng Deng and Lina J. Karam", "title": "Towards Imperceptible Universal Attacks on Texture Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Although deep neural networks (DNNs) have been shown to be susceptible to\nimage-agnostic adversarial attacks on natural image classification problems,\nthe effects of such attacks on DNN-based texture recognition have yet to be\nexplored. As part of our work, we find that limiting the perturbation's $l_p$\nnorm in the spatial domain may not be a suitable way to restrict the\nperceptibility of universal adversarial perturbations for texture images. Based\non the fact that human perception is affected by local visual frequency\ncharacteristics, we propose a frequency-tuned universal attack method to\ncompute universal perturbations in the frequency domain. Our experiments\nindicate that our proposed method can produce less perceptible perturbations\nyet with a similar or higher white-box fooling rates on various DNN texture\nclassifiers and texture datasets as compared to existing universal attack\ntechniques. We also demonstrate that our approach can improve the attack\nrobustness against defended models as well as the cross-dataset transferability\nfor texture recognition problems.\n", "versions": [{"version": "v1", "created": "Tue, 24 Nov 2020 08:33:59 GMT"}], "update_date": "2020-11-25", "authors_parsed": [["Deng", "Yingpeng", ""], ["Karam", "Lina J.", ""]]}, {"id": "2011.11958", "submitter": "Alex Ling Yu Hung", "authors": "Alex Ling Yu Hung, Edward Chen, John Galeotti", "title": "Weakly- and Semi-Supervised Probabilistic Segmentation and\n  Quantification of Ultrasound Needle-Reverberation Artifacts to Allow Better\n  AI Understanding of Tissue Beneath Needles", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ultrasound image quality has continually been improving. However, when\nneedles or other metallic objects are operating inside the tissue, the\nresulting reverberation artifacts can severely corrupt the surrounding image\nquality. Such effects are challenging for existing computer vision algorithms\nfor medical image analysis. Needle reverberation artifacts can be hard to\nidentify at times and affect various pixel values to different degrees. The\nboundaries of such artifacts are ambiguous, leading to disagreement among human\nexperts labeling the artifacts. We propose a weakly- and semi-supervised,\nprobabilistic needle-and-reverberation-artifact segmentation algorithm to\nseparate the desired tissue-based pixel values from the superimposed artifacts.\nOur method models the intensity decay of artifact intensities and is designed\nto minimize the human labeling error. We demonstrate the applicability of the\napproach and compare it against other segmentation algorithms. Our method is\ncapable of differentiating between the reverberations from artifact-free\npatches as well as of modeling the intensity fall-off in the artifacts. Our\nmethod matches state-of-the-art artifact segmentation performance and sets a\nnew standard in estimating the per-pixel contributions of artifact vs\nunderlying anatomy, especially in the immediately adjacent regions between\nreverberation lines. Our algorithm is also able to improve the performance\ndownstream image analysis algorithms.\n", "versions": [{"version": "v1", "created": "Tue, 24 Nov 2020 08:34:38 GMT"}, {"version": "v2", "created": "Thu, 3 Jun 2021 18:57:04 GMT"}], "update_date": "2021-06-07", "authors_parsed": [["Hung", "Alex Ling Yu", ""], ["Chen", "Edward", ""], ["Galeotti", "John", ""]]}, {"id": "2011.11961", "submitter": "Zhanghan Ke", "authors": "Zhanghan Ke, Kaican Li, Yurou Zhou, Qiuhua Wu, Xiangyu Mao, Qiong Yan,\n  Rynson W.H. Lau", "title": "Is a Green Screen Really Necessary for Real-Time Portrait Matting?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For portrait matting without the green screen, existing works either require\nauxiliary inputs that are costly to obtain or use multiple models that are\ncomputationally expensive. Consequently, they are unavailable in real-time\napplications. In contrast, we present a light-weight matting objective\ndecomposition network (MODNet), which can process portrait matting from a\nsingle input image in real time. The design of MODNet benefits from optimizing\na series of correlated sub-objectives simultaneously via explicit constraints.\nMoreover, since trimap-free methods usually suffer from the domain shift\nproblem in practice, we introduce (1) a self-supervised strategy based on\nsub-objectives consistency to adapt MODNet to real-world data and (2) a\none-frame delay trick to smooth the results when applying MODNet to portrait\nvideo sequence.\n  MODNet is easy to be trained in an end-to-end style. It is much faster than\ncontemporaneous matting methods and runs at 63 frames per second. On a\ncarefully designed portrait matting benchmark newly proposed in this work,\nMODNet greatly outperforms prior trimap-free methods. More importantly, our\nmethod achieves remarkable results in daily photos and videos. Now, do you\nreally need a green screen for real-time portrait matting?\n", "versions": [{"version": "v1", "created": "Tue, 24 Nov 2020 08:38:36 GMT"}, {"version": "v2", "created": "Sun, 29 Nov 2020 03:27:58 GMT"}], "update_date": "2020-12-01", "authors_parsed": [["Ke", "Zhanghan", ""], ["Li", "Kaican", ""], ["Zhou", "Yurou", ""], ["Wu", "Qiuhua", ""], ["Mao", "Xiangyu", ""], ["Yan", "Qiong", ""], ["Lau", "Rynson W. H.", ""]]}, {"id": "2011.11962", "submitter": "Alex Ling Yu Hung", "authors": "Alex Ling Yu Hung, John Galeotti", "title": "Good and Bad Boundaries in Ultrasound Compounding: Preserving Anatomic\n  Boundaries While Suppressing Artifacts", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ultrasound 3D compounding is important for volumetric reconstruction, but as\nof yet there is no consensus on best practices for compounding. Ultrasound\nimages depend on probe direction and the path sound waves pass through, so when\nmultiple intersecting B-scans of the same spot from different perspectives\nyield different pixel values, there is not a single, ideal representation for\ncompounding (i.e. combining) the overlapping pixel values. Current popular\nmethods inevitably suppress or altogether leave out bright or dark regions that\nare useful, and potentially introduce new artifacts. In this work, we establish\na new algorithm to compound the overlapping pixels from different view points\nin ultrasound. We uniquely leverage Laplacian and Gaussian Pyramids to preserve\nthe maximum boundary contrast without overemphasizing noise and speckle. We\nevaluate our algorithm by comparing ours with previous algorithms, and we show\nthat our approach not only preserves both light and dark details, but also\nsomewhat suppresses artifacts, rather than amplifying them.\n", "versions": [{"version": "v1", "created": "Tue, 24 Nov 2020 08:41:51 GMT"}, {"version": "v2", "created": "Wed, 14 Apr 2021 05:26:34 GMT"}], "update_date": "2021-04-15", "authors_parsed": [["Hung", "Alex Ling Yu", ""], ["Galeotti", "John", ""]]}, {"id": "2011.11964", "submitter": "Fangzhou Hong", "authors": "Fangzhou Hong, Hui Zhou, Xinge Zhu, Hongsheng Li, Ziwei Liu", "title": "LiDAR-based Panoptic Segmentation via Dynamic Shifting Network", "comments": "Rank 1st place in the leaderboard of SemanticKITTI Panoptic\n  Segmentation (accessed at 2020-11-16); Codes at\n  https://github.com/hongfz16/DS-Net", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  With the rapid advances of autonomous driving, it becomes critical to equip\nits sensing system with more holistic 3D perception. However, existing works\nfocus on parsing either the objects (e.g. cars and pedestrians) or scenes (e.g.\ntrees and buildings) from the LiDAR sensor. In this work, we address the task\nof LiDAR-based panoptic segmentation, which aims to parse both objects and\nscenes in a unified manner. As one of the first endeavors towards this new\nchallenging task, we propose the Dynamic Shifting Network (DS-Net), which\nserves as an effective panoptic segmentation framework in the point cloud\nrealm. In particular, DS-Net has three appealing properties: 1) strong backbone\ndesign. DS-Net adopts the cylinder convolution that is specifically designed\nfor LiDAR point clouds. The extracted features are shared by the semantic\nbranch and the instance branch which operates in a bottom-up clustering style.\n2) Dynamic Shifting for complex point distributions. We observe that\ncommonly-used clustering algorithms like BFS or DBSCAN are incapable of\nhandling complex autonomous driving scenes with non-uniform point cloud\ndistributions and varying instance sizes. Thus, we present an efficient\nlearnable clustering module, dynamic shifting, which adapts kernel functions\non-the-fly for different instances. 3) Consensus-driven Fusion. Finally,\nconsensus-driven fusion is used to deal with the disagreement between semantic\nand instance predictions. To comprehensively evaluate the performance of\nLiDAR-based panoptic segmentation, we construct and curate benchmarks from two\nlarge-scale autonomous driving LiDAR datasets, SemanticKITTI and nuScenes.\nExtensive experiments demonstrate that our proposed DS-Net achieves superior\naccuracies over current state-of-the-art methods. Notably, we achieve 1st place\non the public leaderboard of SemanticKITTI, outperforming 2nd place by 2.6% in\nterms of the PQ metric.\n", "versions": [{"version": "v1", "created": "Tue, 24 Nov 2020 08:44:46 GMT"}, {"version": "v2", "created": "Tue, 1 Dec 2020 05:49:08 GMT"}], "update_date": "2020-12-02", "authors_parsed": [["Hong", "Fangzhou", ""], ["Zhou", "Hui", ""], ["Zhu", "Xinge", ""], ["Li", "Hongsheng", ""], ["Liu", "Ziwei", ""]]}, {"id": "2011.11974", "submitter": "Yang You", "authors": "Yang You, Wenhai Liu, Yong-Lu Li, Weiming Wang, Cewu Lu", "title": "UKPGAN: Unsupervised KeyPoint GANeration", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Keypoint detection is an essential component for the object registration and\nalignment. However, previous works mainly focused on how to register keypoints\nunder arbitrary rigid transformations. Differently, in this work, we reckon\nkeypoints under an information compression scheme to represent the whole\nobject. Based on this, we propose UKPGAN, an unsupervised 3D keypoint detector\nwhere keypoints are detected so that they could reconstruct the original object\nshape. Two modules: GAN-based keypoint sparsity control and salient information\ndistillation modules are proposed to locate those important keypoints.\nExtensive experiments show that our keypoints preserve the semantic information\nof objects and align well with human annotated part and keypoint labels.\nFurthermore, we show that UKPGAN can be applied to either rigid objects or\nnon-rigid SMPL human bodies under arbitrary pose deformations. As a keypoint\ndetector, our model is stable under both rigid and non-rigid transformations,\nwith local reference frame estimation. Our code is available on\nhttps://github.com/qq456cvb/UKPGAN.\n", "versions": [{"version": "v1", "created": "Tue, 24 Nov 2020 09:08:21 GMT"}, {"version": "v2", "created": "Thu, 28 Jan 2021 04:04:19 GMT"}], "update_date": "2021-01-29", "authors_parsed": [["You", "Yang", ""], ["Liu", "Wenhai", ""], ["Li", "Yong-Lu", ""], ["Wang", "Weiming", ""], ["Lu", "Cewu", ""]]}, {"id": "2011.11986", "submitter": "Dmytro Mishkin", "authors": "Daniel Barath, Dmytro Mishkin, Ivan Eichhardt, Ilia Shipachev, Jiri\n  Matas", "title": "Efficient Initial Pose-graph Generation for Global SfM", "comments": "Added supplementary material", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose ways to speed up the initial pose-graph generation for global\nStructure-from-Motion algorithms. To avoid forming tentative point\ncorrespondences by FLANN and geometric verification by RANSAC, which are the\nmost time-consuming steps of the pose-graph creation, we propose two new\nmethods - built on the fact that image pairs usually are matched consecutively.\nThus, candidate relative poses can be recovered from paths in the partly-built\npose-graph. We propose a heuristic for the A* traversal, considering global\nsimilarity of images and the quality of the pose-graph edges. Given a relative\npose from a path, descriptor-based feature matching is made \"light-weight\" by\nexploiting the known epipolar geometry. To speed up PROSAC-based sampling when\nRANSAC is applied, we propose a third method to order the correspondences by\ntheir inlier probabilities from previous estimations. The algorithms are tested\non 402130 image pairs from the 1DSfM dataset and they speed up the feature\nmatching 17 times and pose estimation 5 times.\n", "versions": [{"version": "v1", "created": "Tue, 24 Nov 2020 09:32:03 GMT"}, {"version": "v2", "created": "Thu, 26 Nov 2020 16:11:55 GMT"}], "update_date": "2020-11-30", "authors_parsed": [["Barath", "Daniel", ""], ["Mishkin", "Dmytro", ""], ["Eichhardt", "Ivan", ""], ["Shipachev", "Ilia", ""], ["Matas", "Jiri", ""]]}, {"id": "2011.12001", "submitter": "Yang You", "authors": "Yang You, Zelin Ye, Yujing Lou, Chengkun Li, Yong-Lu Li, Lizhuang Ma,\n  Weiming Wang, Cewu Lu", "title": "Canonical Voting: Towards Robust Oriented Bounding Box Detection in 3D\n  Scenes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  3D object detection has attracted much attention thanks to the advances in\nsensors and deep learning methods for point clouds. Current state-of-the-art\nmethods like VoteNet regress direct offset towards object centers and box\norientations with an additional Multi-Layer-Perceptron network. Both their\noffset and orientation predictions are not accurate due to the fundamental\ndifficulty in rotation classification. In the work, we disentangle the direct\noffset into Local Canonical Coordinates (LCC), box scales and box orientations.\nOnly LCC and box scales are regressed while box orientations are generated by a\ncanonical voting scheme. Finally, a LCC-aware back-projection checking\nalgorithm iteratively cuts out bounding boxes from the generated vote maps,\nwith the elimination of false positives. Our model achieves state-of-the-art\nperformance on challenging large-scale datasets of real point cloud scans:\nScanNet, SceneNN with 8.8 and 5.1 mAP improvement respectively. Code is\navailable on https://github.com/qq456cvb/CanonicalVoting.\n", "versions": [{"version": "v1", "created": "Tue, 24 Nov 2020 10:03:24 GMT"}, {"version": "v2", "created": "Wed, 17 Mar 2021 03:02:13 GMT"}], "update_date": "2021-03-18", "authors_parsed": [["You", "Yang", ""], ["Ye", "Zelin", ""], ["Lou", "Yujing", ""], ["Li", "Chengkun", ""], ["Li", "Yong-Lu", ""], ["Ma", "Lizhuang", ""], ["Wang", "Weiming", ""], ["Lu", "Cewu", ""]]}, {"id": "2011.12004", "submitter": "Hassen Drira", "authors": "Racha Friji, Hassen Drira, Faten Chaieb, Sebastian Kurtek, Hamza Kchok", "title": "KShapeNet: Riemannian network on Kendall shape space for Skeleton based\n  Action Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Learning architectures, albeit successful in most computer vision tasks,\nwere designed for data with an underlying Euclidean structure, which is not\nusually fulfilled since pre-processed data may lie on a non-linear space. In\nthis paper, we propose a geometry aware deep learning approach for\nskeleton-based action recognition. Skeleton sequences are first modeled as\ntrajectories on Kendall's shape space and then mapped to the linear tangent\nspace. The resulting structured data are then fed to a deep learning\narchitecture, which includes a layer that optimizes over rigid and non rigid\ntransformations of the 3D skeletons, followed by a CNN-LSTM network. The\nassessment on two large scale skeleton datasets, namely NTU-RGB+D and NTU-RGB+D\n120, has proven that proposed approach outperforms existing geometric deep\nlearning methods and is competitive with respect to recently published\napproaches.\n", "versions": [{"version": "v1", "created": "Tue, 24 Nov 2020 10:14:07 GMT"}], "update_date": "2020-11-25", "authors_parsed": [["Friji", "Racha", ""], ["Drira", "Hassen", ""], ["Chaieb", "Faten", ""], ["Kurtek", "Sebastian", ""], ["Kchok", "Hamza", ""]]}, {"id": "2011.12024", "submitter": "Haoxi Ran", "authors": "Haoxi Ran, Guangfu Wang, Li Lu", "title": "RIN: Textured Human Model Recovery and Imitation with a Single Image", "comments": "The paper is not well-organized and have to be modified later. Hence\n  authors decide to withdraw it", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human imitation has become topical recently, driven by GAN's ability to\ndisentangle human pose and body content. However, the latest methods hardly\nfocus on 3D information, and to avoid self-occlusion, a massive amount of input\nimages are needed. In this paper, we propose RIN, a novel volume-based\nframework for reconstructing a textured 3D model from a single picture and\nimitating a subject with the generated model. Specifically, to estimate most of\nthe human texture, we propose a U-Net-like front-to-back translation network.\nWith both front and back images input, the textured volume recovery module\nallows us to color a volumetric human. A sequence of 3D poses then guides the\ncolored volume via Flowable Disentangle Networks as a volume-to-volume\ntranslation task. To project volumes to a 2D plane during training, we design a\ndifferentiable depth-aware renderer. Our experiments demonstrate that our\nvolume-based model is adequate for human imitation, and the back view can be\nestimated reliably using our network. While prior works based on either 2D pose\nor semantic map often fail for the unstable appearance of a human, our\nframework can still produce concrete results, which are competitive to those\nimagined from multi-view input.\n", "versions": [{"version": "v1", "created": "Tue, 24 Nov 2020 11:04:35 GMT"}, {"version": "v2", "created": "Sun, 29 Nov 2020 05:03:23 GMT"}, {"version": "v3", "created": "Thu, 4 Mar 2021 07:17:18 GMT"}], "update_date": "2021-03-05", "authors_parsed": [["Ran", "Haoxi", ""], ["Wang", "Guangfu", ""], ["Lu", "Li", ""]]}, {"id": "2011.12025", "submitter": "Thomas Verelst", "authors": "Thomas Verelst and Tinne Tuytelaars", "title": "SegBlocks: Block-Based Dynamic Resolution Networks for Real-Time\n  Segmentation", "comments": "long version, 11 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  SegBlocks reduces the computational cost of existing neural networks, by\ndynamically adjusting the processing resolution of image regions based on their\ncomplexity. Our method splits an image into blocks and downsamples blocks of\nlow complexity, reducing the number of operations and memory consumption. A\nlightweight policy network, selecting the complex regions, is trained using\nreinforcement learning. In addition, we introduce several modules implemented\nin CUDA to process images in blocks. Most important, our novel BlockPad module\nprevents the feature discontinuities at block borders of which existing methods\nsuffer, while keeping memory consumption under control. Our experiments on\nCityscapes and Mapillary Vistas semantic segmentation show that dynamically\nprocessing images offers a better accuracy versus complexity trade-off compared\nto static baselines of similar complexity. For instance, our method reduces the\nnumber of floating-point operations of SwiftNet-RN18 by 60% and increases the\ninference speed by 50%, with only 0.3% decrease in mIoU accuracy on Cityscapes.\n", "versions": [{"version": "v1", "created": "Tue, 24 Nov 2020 11:05:07 GMT"}], "update_date": "2020-11-25", "authors_parsed": [["Verelst", "Thomas", ""], ["Tuytelaars", "Tinne", ""]]}, {"id": "2011.12026", "submitter": "Ivan Skorokhodov", "authors": "Ivan Skorokhodov, Savva Ignatyev, Mohamed Elhoseiny", "title": "Adversarial Generation of Continuous Images", "comments": "19 pages, 17 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In most existing learning systems, images are typically viewed as 2D pixel\narrays. However, in another paradigm gaining popularity, a 2D image is\nrepresented as an implicit neural representation (INR) - an MLP that predicts\nan RGB pixel value given its (x,y) coordinate. In this paper, we propose two\nnovel architectural techniques for building INR-based image decoders:\nfactorized multiplicative modulation and multi-scale INRs, and use them to\nbuild a state-of-the-art continuous image GAN. Previous attempts to adapt INRs\nfor image generation were limited to MNIST-like datasets and do not scale to\ncomplex real-world data. Our proposed INR-GAN architecture improves the\nperformance of continuous image generators by several times, greatly reducing\nthe gap between continuous image GANs and pixel-based ones. Apart from that, we\nexplore several exciting properties of the INR-based decoders, like\nout-of-the-box superresolution, meaningful image-space interpolation,\naccelerated inference of low-resolution images, an ability to extrapolate\noutside of image boundaries, and strong geometric prior. The project page is\nlocated at https://universome.github.io/inr-gan.\n", "versions": [{"version": "v1", "created": "Tue, 24 Nov 2020 11:06:40 GMT"}, {"version": "v2", "created": "Mon, 28 Jun 2021 09:00:05 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Skorokhodov", "Ivan", ""], ["Ignatyev", "Savva", ""], ["Elhoseiny", "Mohamed", ""]]}, {"id": "2011.12032", "submitter": "Zitong Yu", "authors": "Zitong Yu, Xiaobai Li, Jingang Shi, Zhaoqiang Xia, Guoying Zhao", "title": "Revisiting Pixel-Wise Supervision for Face Anti-Spoofing", "comments": "submitted to IEEE Transactions on Biometrics, Behavior and Identity\n  Science", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Face anti-spoofing (FAS) plays a vital role in securing face recognition\nsystems from the presentation attacks (PAs). As more and more realistic PAs\nwith novel types spring up, it is necessary to develop robust algorithms for\ndetecting unknown attacks even in unseen scenarios. However, deep models\nsupervised by traditional binary loss (e.g., `0' for bonafide vs. `1' for PAs)\nare weak in describing intrinsic and discriminative spoofing patterns.\nRecently, pixel-wise supervision has been proposed for the FAS task, intending\nto provide more fine-grained pixel/patch-level cues. In this paper, we firstly\ngive a comprehensive review and analysis about the existing pixel-wise\nsupervision methods for FAS. Then we propose a novel pyramid supervision, which\nguides deep models to learn both local details and global semantics from\nmulti-scale spatial context. Extensive experiments are performed on five FAS\nbenchmark datasets to show that, without bells and whistles, the proposed\npyramid supervision could not only improve the performance beyond existing\npixel-wise supervision frameworks, but also enhance the model's\ninterpretability (i.e., locating the patch-level positions of PAs more\nreasonably). Furthermore, elaborate studies are conducted for exploring the\nefficacy of different architecture configurations with two kinds of pixel-wise\nsupervisions (binary mask and depth map supervisions), which provides\ninspirable insights for future architecture/supervision design.\n", "versions": [{"version": "v1", "created": "Tue, 24 Nov 2020 11:25:58 GMT"}], "update_date": "2020-11-25", "authors_parsed": [["Yu", "Zitong", ""], ["Li", "Xiaobai", ""], ["Shi", "Jingang", ""], ["Xia", "Zhaoqiang", ""], ["Zhao", "Guoying", ""]]}, {"id": "2011.12059", "submitter": "Fan Wang", "authors": "Fan Wang", "title": "Infrared small target detection based on isotropic constraint under\n  complex background", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Infrared search and tracking (IRST) system has been widely concerned and\napplied in the area of national defence. Small target detection under complex\nbackground is a very challenging task in the development of system algorithm.\nLow signal-to-clutter ratio (SCR) of target and the interference caused by\nirregular background clutter make it difficult to get an accurate result. In\nthis paper, small targets are considered to have two characteristics of high\ncontrast and isotropy, and we propose a multilayer gray difference (MGD) method\nconstrained by isotropy. Firstly, the suspected regions are obtained through\nMGD, and then the eigenvalues of the original image's Hessian matrix are\ncalculated to obtain the isotropy parameter of each region. Finally, those\nregions do not meet the isotropic constraint condition are suppressed.\nExperiments show that the proposed method is effective and superior to several\ncommon methods in terms of signal-to-clutter ratio gain (SCRG) and receiver\noperating characteristic (ROC) curve.\n", "versions": [{"version": "v1", "created": "Tue, 24 Nov 2020 12:25:05 GMT"}], "update_date": "2020-11-25", "authors_parsed": [["Wang", "Fan", ""]]}, {"id": "2011.12077", "submitter": "Muhammad Zaigham Zaheer", "authors": "Muhammad Zaigham Zaheer, Arif Mahmood, Marcella Astrid, Seung-Ik Lee", "title": "CLAWS: Clustering Assisted Weakly Supervised Learning with Normalcy\n  Suppression for Anomalous Event Detection", "comments": "Presented in the European Conference on Computer Vision ECCV 2020.\n  (Changes from actual paper: 1) Recently published methods have been added in\n  ShanghaiTech and UCF Crime comparison tabs. 2) Due to some error in arxiv\n  compilation, few references are exceeding the paragraph. Also, word\n  'normalcy' in the title is misspelling despite being correct in the code.\n  (Contents are intact)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Learning to detect real-world anomalous events through video-level labels is\na challenging task due to the rare occurrence of anomalies as well as noise in\nthe labels. In this work, we propose a weakly supervised anomaly detection\nmethod which has manifold contributions including1) a random batch based\ntraining procedure to reduce inter-batch correlation, 2) a normalcy suppression\nmechanism to minimize anomaly scores of the normal regions of a video by taking\ninto account the overall information available in one training batch, and 3) a\nclustering distance based loss to contribute towards mitigating the label noise\nand to produce better anomaly representations by encouraging our model to\ngenerate distinct normal and anomalous clusters. The proposed method\nobtains83.03% and 89.67% frame-level AUC performance on the UCF Crime and\nShanghaiTech datasets respectively, demonstrating its superiority over the\nexisting state-of-the-art algorithms.\n", "versions": [{"version": "v1", "created": "Tue, 24 Nov 2020 13:27:40 GMT"}, {"version": "v2", "created": "Wed, 25 Nov 2020 04:11:27 GMT"}, {"version": "v3", "created": "Wed, 16 Jun 2021 16:08:00 GMT"}], "update_date": "2021-06-17", "authors_parsed": [["Zaheer", "Muhammad Zaigham", ""], ["Mahmood", "Arif", ""], ["Astrid", "Marcella", ""], ["Lee", "Seung-Ik", ""]]}, {"id": "2011.12079", "submitter": "Xiang Liu", "authors": "Hongyuan Wang, Xiang Liu, Wen Kang, Zhiqiang Yan, Bingwen Wang,\n  Qianhao Ning", "title": "Multi-Features Guidance Network for partial-to-partial point cloud\n  registration", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  To eliminate the problems of large dimensional differences, big semantic gap,\nand mutual interference caused by hybrid features, in this paper, we propose a\nnovel Multi-Features Guidance Network for partial-to-partial point cloud\nregistration(MFG). The proposed network mainly includes four parts: keypoints'\nfeature extraction, correspondences searching, correspondences credibility\ncomputation, and SVD, among which correspondences searching and correspondence\ncredibility computation are the cores of the network. Unlike the previous work,\nwe utilize the shape features and the spatial coordinates to guide\ncorrespondences search independently and fusing the matching results to obtain\nthe final matching matrix. In the correspondences credibility computation\nmodule, based on the conflicted relationship between the features matching\nmatrix and the coordinates matching matrix, we score the reliability for each\ncorrespondence, which can reduce the impact of mismatched or non-matched\npoints. Experimental results show that our network outperforms the current\nstate-of-the-art while maintaining computational efficiency.\n", "versions": [{"version": "v1", "created": "Tue, 24 Nov 2020 13:31:58 GMT"}], "update_date": "2020-11-25", "authors_parsed": [["Wang", "Hongyuan", ""], ["Liu", "Xiang", ""], ["Kang", "Wen", ""], ["Yan", "Zhiqiang", ""], ["Wang", "Bingwen", ""], ["Ning", "Qianhao", ""]]}, {"id": "2011.12082", "submitter": "Kejun Wang", "authors": "Jing Chen, Chenhui Wang, Kejun Wang, Meichen Liu", "title": "Computational efficient deep neural network with difference attention\n  maps for facial action unit detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we propose a computational efficient end-to-end training deep\nneural network (CEDNN) model and spatial attention maps based on difference\nimages. Firstly, the difference image is generated by image processing. Then\nfive binary images of difference images are obtained using different\nthresholds, which are used as spatial attention maps. We use group convolution\nto reduce model complexity. Skip connection and $\\text{1}\\times \\text{1}$\nconvolution are used to ensure good performance even if the network model is\nnot deep. As an input, spatial attention map can be selectively fed into the\ninput of each block. The feature maps tend to focus on the parts that are\nrelated to the target task better. In addition, we only need to adjust the\nparameters of classifier to train different numbers of AU. It can be easily\nextended to varying datasets without increasing too much computation. A large\nnumber of experimental results show that the proposed CEDNN is obviously better\nthan the traditional deep learning method on DISFA+ and CK+ datasets. After\nadding spatial attention maps, the result is better than the most advanced AU\ndetection method. At the same time, the scale of the network is small, the\nrunning speed is fast, and the requirement for experimental equipment is low.\n", "versions": [{"version": "v1", "created": "Tue, 24 Nov 2020 13:34:58 GMT"}, {"version": "v2", "created": "Fri, 27 Nov 2020 03:23:50 GMT"}], "update_date": "2020-11-30", "authors_parsed": [["Chen", "Jing", ""], ["Wang", "Chenhui", ""], ["Wang", "Kejun", ""], ["Liu", "Meichen", ""]]}, {"id": "2011.12091", "submitter": "Xirong Li", "authors": "Xirong Li and Fangming Zhou and Chaoxi Xu and Jiaqi Ji and Gang Yang", "title": "SEA: Sentence Encoder Assembly for Video Retrieval by Textual Queries", "comments": "accepted for publication as a REGULAR paper in the IEEE Transactions\n  on Multimedia", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG cs.MM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Retrieving unlabeled videos by textual queries, known as Ad-hoc Video Search\n(AVS), is a core theme in multimedia data management and retrieval. The success\nof AVS counts on cross-modal representation learning that encodes both query\nsentences and videos into common spaces for semantic similarity computation.\nInspired by the initial success of previously few works in combining multiple\nsentence encoders, this paper takes a step forward by developing a new and\ngeneral method for effectively exploiting diverse sentence encoders. The\nnovelty of the proposed method, which we term Sentence Encoder Assembly (SEA),\nis two-fold. First, different from prior art that use only a single common\nspace, SEA supports text-video matching in multiple encoder-specific common\nspaces. Such a property prevents the matching from being dominated by a\nspecific encoder that produces an encoding vector much longer than other\nencoders. Second, in order to explore complementarities among the individual\ncommon spaces, we propose multi-space multi-loss learning. As extensive\nexperiments on four benchmarks (MSR-VTT, TRECVID AVS 2016-2019, TGIF and MSVD)\nshow, SEA surpasses the state-of-the-art. In addition, SEA is extremely ease to\nimplement. All this makes SEA an appealing solution for AVS and promising for\ncontinuously advancing the task by harvesting new sentence encoders.\n", "versions": [{"version": "v1", "created": "Tue, 24 Nov 2020 13:54:28 GMT"}], "update_date": "2020-11-25", "authors_parsed": [["Li", "Xirong", ""], ["Zhou", "Fangming", ""], ["Xu", "Chaoxi", ""], ["Ji", "Jiaqi", ""], ["Yang", "Gang", ""]]}, {"id": "2011.12097", "submitter": "Shuyang Sun", "authors": "Shuyang Sun, Liang Chen, Gregory Slabaugh, Philip Torr", "title": "Learning to Sample the Most Useful Training Patches from Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Some image restoration tasks like demosaicing require difficult training\nsamples to learn effective models. Existing methods attempt to address this\ndata training problem by manually collecting a new training dataset that\ncontains adequate hard samples, however, there are still hard and simple areas\neven within one single image. In this paper, we present a data-driven approach\ncalled PatchNet that learns to select the most useful patches from an image to\nconstruct a new training set instead of manual or random selection. We show\nthat our simple idea automatically selects informative samples out from a\nlarge-scale dataset, leading to a surprising 2.35dB generalisation gain in\nterms of PSNR. In addition to its remarkable effectiveness, PatchNet is also\nresource-friendly as it is applied only during training and therefore does not\nrequire any additional computational cost during inference.\n", "versions": [{"version": "v1", "created": "Tue, 24 Nov 2020 14:06:50 GMT"}], "update_date": "2020-11-25", "authors_parsed": [["Sun", "Shuyang", ""], ["Chen", "Liang", ""], ["Slabaugh", "Gregory", ""], ["Torr", "Philip", ""]]}, {"id": "2011.12100", "submitter": "Michael Niemeyer", "authors": "Michael Niemeyer, Andreas Geiger", "title": "GIRAFFE: Representing Scenes as Compositional Generative Neural Feature\n  Fields", "comments": "Accepted to CVPR 2021 (oral). Project page:\n  http://bit.ly/giraffe-project", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep generative models allow for photorealistic image synthesis at high\nresolutions. But for many applications, this is not enough: content creation\nalso needs to be controllable. While several recent works investigate how to\ndisentangle underlying factors of variation in the data, most of them operate\nin 2D and hence ignore that our world is three-dimensional. Further, only few\nworks consider the compositional nature of scenes. Our key hypothesis is that\nincorporating a compositional 3D scene representation into the generative model\nleads to more controllable image synthesis. Representing scenes as\ncompositional generative neural feature fields allows us to disentangle one or\nmultiple objects from the background as well as individual objects' shapes and\nappearances while learning from unstructured and unposed image collections\nwithout any additional supervision. Combining this scene representation with a\nneural rendering pipeline yields a fast and realistic image synthesis model. As\nevidenced by our experiments, our model is able to disentangle individual\nobjects and allows for translating and rotating them in the scene as well as\nchanging the camera pose.\n", "versions": [{"version": "v1", "created": "Tue, 24 Nov 2020 14:14:15 GMT"}, {"version": "v2", "created": "Thu, 29 Apr 2021 14:46:36 GMT"}], "update_date": "2021-04-30", "authors_parsed": [["Niemeyer", "Michael", ""], ["Geiger", "Andreas", ""]]}, {"id": "2011.12102", "submitter": "Mingtao Pei", "authors": "Qing Gao, Mingtao Pei, Hongyu Shen", "title": "Do You Live a Healthy Life? Analyzing Lifestyle by Visual Life Logging", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A healthy lifestyle is the key to better health and happiness and has a\nconsiderable effect on quality of life and disease prevention. Current\nlifelogging/egocentric datasets are not suitable for lifestyle analysis;\nconsequently, there is no research on lifestyle analysis in the field of\ncomputer vision. In this work, we investigate the problem of lifestyle analysis\nand build a visual lifelogging dataset for lifestyle analysis (VLDLA). The\nVLDLA contains images captured by a wearable camera every 3 seconds from 8:00\nam to 6:00 pm for seven days. In contrast to current lifelogging/egocentric\ndatasets, our dataset is suitable for lifestyle analysis as images are taken\nwith short intervals to capture activities of short duration; moreover, images\nare taken continuously from morning to evening to record all the activities\nperformed by a user. Based on our dataset, we classify the user activities in\neach frame and use three latent fluents of the user, which change over time and\nare associated with activities, to measure the healthy degree of the user's\nlifestyle. The scores for the three latent fluents are computed based on\nrecognized activities, and the healthy degree of the lifestyle for the day is\ndetermined based on the scores for the latent fluents. Experimental results\nshow that our method can be used to analyze the healthiness of users'\nlifestyles.\n", "versions": [{"version": "v1", "created": "Tue, 24 Nov 2020 14:20:12 GMT"}], "update_date": "2020-11-25", "authors_parsed": [["Gao", "Qing", ""], ["Pei", "Mingtao", ""], ["Shen", "Hongyu", ""]]}, {"id": "2011.12104", "submitter": "Wanquan Feng", "authors": "Wanquan Feng, Juyong Zhang, Hongrui Cai, Haofei Xu, Junhui Hou and\n  Hujun Bao", "title": "Recurrent Multi-view Alignment Network for Unsupervised Surface\n  Registration", "comments": "Accepted to CVPR2021. The source codes are available at\n  https://github.com/WanquanF/RMA-Net", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning non-rigid registration in an end-to-end manner is challenging due to\nthe inherent high degrees of freedom and the lack of labeled training data. In\nthis paper, we resolve these two challenges simultaneously. First, we propose\nto represent the non-rigid transformation with a point-wise combination of\nseveral rigid transformations. This representation not only makes the solution\nspace well-constrained but also enables our method to be solved iteratively\nwith a recurrent framework, which greatly reduces the difficulty of learning.\nSecond, we introduce a differentiable loss function that measures the 3D shape\nsimilarity on the projected multi-view 2D depth images so that our full\nframework can be trained end-to-end without ground truth supervision. Extensive\nexperiments on several different datasets demonstrate that our proposed method\noutperforms the previous state-of-the-art by a large margin. The source codes\nare available at https://github.com/WanquanF/RMA-Net.\n", "versions": [{"version": "v1", "created": "Tue, 24 Nov 2020 14:22:42 GMT"}, {"version": "v2", "created": "Tue, 13 Apr 2021 08:07:17 GMT"}], "update_date": "2021-04-14", "authors_parsed": [["Feng", "Wanquan", ""], ["Zhang", "Juyong", ""], ["Cai", "Hongrui", ""], ["Xu", "Haofei", ""], ["Hou", "Junhui", ""], ["Bao", "Hujun", ""]]}, {"id": "2011.12105", "submitter": "Guangming Wang", "authors": "Minjian Xin, Guangming Wang, Zhe Liu, Hesheng Wang", "title": "Achieving Sample-Efficient Learning of Long-Horizon Sparse-Reward\n  Robotic Tasks with Base Controllers", "comments": "7 pages, 5 figures, under review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Application of Deep Reinforcement Learning (DRL) algorithms in robotic tasks\nfaces many challenges. On the one hand, reward-shaping for complex tasks that\ninvolve multiple sequences is difficult and may result in sub-optimal\nperformances. On the other hand, a sparse-reward setting renders exploration\ninefficient, and exploration using physical robots is of high-cost and unsafe.\nIn this paper we propose a method of learning long-horizon sparse-reward tasks\nutilizing one or more existing controllers. Built upon Deep Deterministic\nPolicy Gradients (DDPG), our algorithm incorporates the controllers into stages\nof exploration, policy update, and most importantly, learning a heuristic value\nfunction that naturally interpolates along task trajectories. Through\nexperiments ranging from stacking blocks to cups, we present a straightforward\nway of synthesizing these controllers, and show that the learned state-based or\nimage-based policies steadily outperform them. Compared to previous works of\nlearning from demonstrations, our method improves sample efficiency by orders\nof magnitude. Overall, our method bears the potential of leveraging existing\nindustrial robot manipulation systems to build more flexible and intelligent\ncontrollers.\n", "versions": [{"version": "v1", "created": "Tue, 24 Nov 2020 14:23:57 GMT"}, {"version": "v2", "created": "Tue, 23 Mar 2021 10:43:25 GMT"}], "update_date": "2021-03-24", "authors_parsed": [["Xin", "Minjian", ""], ["Wang", "Guangming", ""], ["Liu", "Zhe", ""], ["Wang", "Hesheng", ""]]}, {"id": "2011.12108", "submitter": "Jinlong Fan", "authors": "Jinlong Fan and Jing Zhang and Stephen J. Maybank and Dacheng Tao", "title": "Wide-angle Image Rectification: A Survey", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Wide field-of-view (FOV) cameras, which capture a larger scene area than\nnarrow FOV cameras, are used in many applications including 3D reconstruction,\nautonomous driving, and video surveillance. However, wide-angle images contain\ndistortions that violate the assumptions underlying pinhole camera models,\nresulting in object distortion, difficulties in estimating scene distance,\narea, and direction, and preventing the use of off-the-shelf deep models\ntrained on undistorted images for downstream computer vision tasks. Image\nrectification, which aims to correct these distortions, can solve these\nproblems. In this paper, we comprehensively survey progress in wide-angle image\nrectification from transformation models to rectification methods.\nSpecifically, we first present a detailed description and discussion of the\ncamera models used in different approaches. Then, we summarize several\ndistortion models including radial distortion and projection distortion. Next,\nwe review both traditional geometry-based image rectification methods and deep\nlearning-based methods, where the former formulate distortion parameter\nestimation as an optimization problem and the latter treat it as a regression\nproblem by leveraging the power of deep neural networks. We evaluate the\nperformance of state-of-the-art methods on public datasets and show that\nalthough both kinds of methods can achieve good results, these methods only\nwork well for specific camera models and distortion types. We also provide a\nstrong baseline model and carry out an empirical study of different distortion\nmodels on synthetic datasets and real-world wide-angle images. Finally, we\ndiscuss several potential research directions that are expected to further\nadvance this area in the future.\n", "versions": [{"version": "v1", "created": "Fri, 30 Oct 2020 17:28:40 GMT"}], "update_date": "2020-11-25", "authors_parsed": [["Fan", "Jinlong", ""], ["Zhang", "Jing", ""], ["Maybank", "Stephen J.", ""], ["Tao", "Dacheng", ""]]}, {"id": "2011.12111", "submitter": "Vincent Talbo", "authors": "Vincent Talbo, Mehdi Haddab, Derek Aubert, Redha Moulla", "title": "Wavelet-based clustering for time-series trend detection", "comments": "10 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we introduce a method performing clustering of time-series on\nthe basis of their trend (increasing, stagnating/decreasing, and seasonal\nbehavior). The clustering is performed using $k$-means method on a selection of\ncoefficients obtained by discrete wavelet transform, reducing drastically the\ndimensionality. The method is applied on an use case for the clustering of a\n864 daily sales revenue time-series for 61 retail shops. The results are\npresented for different mother wavelets. The importance of each wavelet\ncoefficient and its level is discussed thanks to a principal component analysis\nalong with a reconstruction of the signal from the selected wavelet\ncoefficients.\n", "versions": [{"version": "v1", "created": "Tue, 17 Nov 2020 09:41:49 GMT"}], "update_date": "2020-11-25", "authors_parsed": [["Talbo", "Vincent", ""], ["Haddab", "Mehdi", ""], ["Aubert", "Derek", ""], ["Moulla", "Redha", ""]]}, {"id": "2011.12143", "submitter": "Lukun Zheng", "authors": "Yuhang Jiang, Lukun Zheng", "title": "Deep learning for video game genre classification", "comments": "21 pages, 6 figures, 3 tables. arXiv admin note: substantial text\n  overlap with arXiv:2011.07658", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video game genre classification based on its cover and textual description\nwould be utterly beneficial to many modern identification, collocation, and\nretrieval systems. At the same time, it is also an extremely challenging task\ndue to the following reasons: First, there exists a wide variety of video game\ngenres, many of which are not concretely defined. Second, video game covers\nvary in many different ways such as colors, styles, textual information, etc,\neven for games of the same genre. Third, cover designs and textual descriptions\nmay vary due to many external factors such as country, culture, target reader\npopulations, etc. With the growing competitiveness in the video game industry,\nthe cover designers and typographers push the cover designs to its limit in the\nhope of attracting sales. The computer-based automatic video game genre\nclassification systems become a particularly exciting research topic in recent\nyears. In this paper, we propose a multi-modal deep learning framework to solve\nthis problem. The contribution of this paper is four-fold. First, we compiles a\nlarge dataset consisting of 50,000 video games from 21 genres made of cover\nimages, description text, and title text and the genre information. Second,\nimage-based and text-based, state-of-the-art models are evaluated thoroughly\nfor the task of genre classification for video games. Third, we developed an\nefficient and salable multi-modal framework based on both images and texts.\nFourth, a thorough analysis of the experimental results is given and future\nworks to improve the performance is suggested. The results show that the\nmulti-modal framework outperforms the current state-of-the-art image-based or\ntext-based models. Several challenges are outlined for this task. More efforts\nand resources are needed for this classification task in order to reach a\nsatisfactory level.\n", "versions": [{"version": "v1", "created": "Sat, 21 Nov 2020 22:31:43 GMT"}], "update_date": "2020-11-25", "authors_parsed": [["Jiang", "Yuhang", ""], ["Zheng", "Lukun", ""]]}, {"id": "2011.12149", "submitter": "Sheng Ao", "authors": "Sheng Ao, Qingyong Hu, Bo Yang, Andrew Markham, Yulan Guo", "title": "SpinNet: Learning a General Surface Descriptor for 3D Point Cloud\n  Registration", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Extracting robust and general 3D local features is key to downstream tasks\nsuch as point cloud registration and reconstruction. Existing learning-based\nlocal descriptors are either sensitive to rotation transformations, or rely on\nclassical handcrafted features which are neither general nor representative. In\nthis paper, we introduce a new, yet conceptually simple, neural architecture,\ntermed SpinNet, to extract local features which are rotationally invariant\nwhilst sufficiently informative to enable accurate registration. A Spatial\nPoint Transformer is first introduced to map the input local surface into a\ncarefully designed cylindrical space, enabling end-to-end optimization with\nSO(2) equivariant representation. A Neural Feature Extractor which leverages\nthe powerful point-based and 3D cylindrical convolutional neural layers is then\nutilized to derive a compact and representative descriptor for matching.\nExtensive experiments on both indoor and outdoor datasets demonstrate that\nSpinNet outperforms existing state-of-the-art techniques by a large margin.\nMore critically, it has the best generalization ability across unseen scenarios\nwith different sensor modalities. The code is available at\nhttps://github.com/QingyongHu/SpinNet.\n", "versions": [{"version": "v1", "created": "Tue, 24 Nov 2020 15:00:56 GMT"}, {"version": "v2", "created": "Fri, 9 Apr 2021 16:42:19 GMT"}], "update_date": "2021-04-12", "authors_parsed": [["Ao", "Sheng", ""], ["Hu", "Qingyong", ""], ["Yang", "Bo", ""], ["Markham", "Andrew", ""], ["Guo", "Yulan", ""]]}, {"id": "2011.12172", "submitter": "Chen Chen", "authors": "Sijie Zhu and Taojiannan Yang and Chen Chen", "title": "VIGOR: Cross-View Image Geo-localization beyond One-to-one Retrieval", "comments": "CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cross-view image geo-localization aims to determine the locations of\nstreet-view query images by matching with GPS-tagged reference images from\naerial view. Recent works have achieved surprisingly high retrieval accuracy on\ncity-scale datasets. However, these results rely on the assumption that there\nexists a reference image exactly centered at the location of any query image,\nwhich is not applicable for practical scenarios. In this paper, we redefine\nthis problem with a more realistic assumption that the query image can be\narbitrary in the area of interest and the reference images are captured before\nthe queries emerge. This assumption breaks the one-to-one retrieval setting of\nexisting datasets as the queries and reference images are not perfectly aligned\npairs, and there may be multiple reference images covering one query location.\nTo bridge the gap between this realistic setting and existing datasets, we\npropose a new large-scale benchmark -- VIGOR -- for cross-View Image\nGeo-localization beyond One-to-one Retrieval. We benchmark existing\nstate-of-the-art methods and propose a novel end-to-end framework to localize\nthe query in a coarse-to-fine manner. Apart from the image-level retrieval\naccuracy, we also evaluate the localization accuracy in terms of the actual\ndistance (meters) using the raw GPS data. Extensive experiments are conducted\nunder different application scenarios to validate the effectiveness of the\nproposed method. The results indicate that cross-view geo-localization in this\nrealistic setting is still challenging, fostering new research in this\ndirection. Our dataset and code will be released at\n\\url{https://github.com/Jeff-Zilence/VIGOR}\n", "versions": [{"version": "v1", "created": "Tue, 24 Nov 2020 15:50:54 GMT"}, {"version": "v2", "created": "Mon, 22 Mar 2021 04:01:54 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Zhu", "Sijie", ""], ["Yang", "Taojiannan", ""], ["Chen", "Chen", ""]]}, {"id": "2011.12222", "submitter": "Peirong Liu", "authors": "Peirong Liu and Lin Tian and Yubo Zhang and Stephen R. Aylward and\n  Yueh Z. Lee and Marc Niethammer", "title": "Discovering Hidden Physics Behind Transport Dynamics", "comments": "Accepted as ORAL at CVPR 2021 (20 pages, 13 figures)", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transport processes are ubiquitous. They are, for example, at the heart of\noptical flow approaches; or of perfusion imaging, where blood transport is\nassessed, most commonly by injecting a tracer. An advection-diffusion equation\nis widely used to describe these transport phenomena. Our goal is estimating\nthe underlying physics of advection-diffusion equations, expressed as velocity\nand diffusion tensor fields. We propose a learning framework (YETI) building on\nan auto-encoder structure between 2D and 3D image time-series, which\nincorporates the advection-diffusion model. To help with identifiability, we\ndevelop an advection-diffusion simulator which allows pre-training of our model\nby supervised learning using the velocity and diffusion tensor fields. Instead\nof directly learning these velocity and diffusion tensor fields, we introduce\nrepresentations that assure incompressible flow and symmetric positive\nsemi-definite diffusion fields and demonstrate the additional benefits of these\nrepresentations on improving estimation accuracy. We further use transfer\nlearning to apply YETI on a public brain magnetic resonance (MR) perfusion\ndataset of stroke patients and show its ability to successfully distinguish\nstroke lesions from normal brain regions via the estimated velocity and\ndiffusion tensor fields.\n", "versions": [{"version": "v1", "created": "Tue, 24 Nov 2020 17:19:25 GMT"}, {"version": "v2", "created": "Mon, 29 Mar 2021 14:21:08 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Liu", "Peirong", ""], ["Tian", "Lin", ""], ["Zhang", "Yubo", ""], ["Aylward", "Stephen R.", ""], ["Lee", "Yueh Z.", ""], ["Niethammer", "Marc", ""]]}, {"id": "2011.12236", "submitter": "Ariel Ruiz-Garcia", "authors": "Ariel Ruiz-Garcia, Ibrahim Almakky, Vasile Palade, Luke Hicks", "title": "Generative Adversarial Stacked Autoencoders", "comments": "arXiv admin note: text overlap with arXiv:2007.09790", "journal-ref": "Proceedings of the LatinX in AI Research Workshop at at the 34th\n  Conference on Neural Information Processing Systems (NeurIPS 2020),\n  Vancouver, Canada", "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Generative Adversarial Networks (GANs) have become predominant in image\ngeneration tasks. Their success is attributed to the training regime which\nemploys two models: a generator G and discriminator D that compete in a minimax\nzero sum game. Nonetheless, GANs are difficult to train due to their\nsensitivity to hyperparameter and parameter initialisation, which often leads\nto vanishing gradients, non-convergence, or mode collapse, where the generator\nis unable to create samples with different variations. In this work, we propose\na novel Generative Adversarial Stacked Convolutional Autoencoder(GASCA) model\nand a generative adversarial gradual greedy layer-wise learning algorithm\nde-signed to train Adversarial Autoencoders in an efficient and incremental\nmanner. Our training approach produces images with significantly lower\nreconstruction error than vanilla joint training.\n", "versions": [{"version": "v1", "created": "Sun, 22 Nov 2020 17:51:59 GMT"}], "update_date": "2020-11-25", "authors_parsed": [["Ruiz-Garcia", "Ariel", ""], ["Almakky", "Ibrahim", ""], ["Palade", "Vasile", ""], ["Hicks", "Luke", ""]]}, {"id": "2011.12256", "submitter": "Ali Babolhavaeji", "authors": "Ali Babolhavaeji and Mohammad Fanaei", "title": "Multi-Stage CNN-Based Monocular 3D Vehicle Localization and Orientation\n  Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper aims to design a 3D object detection model from 2D images taken by\nmonocular cameras by combining the estimated bird's-eye view elevation map and\nthe deep representation of object features. The proposed model has a\npre-trained ResNet-50 network as its backend network and three more branches.\nThe model first builds a bird's-eye view elevation map to estimate the depth of\nthe object in the scene and by using that estimates the object's 3D bounding\nboxes. We have trained and evaluate it on two major datasets: a syntactic\ndataset and the KIITI dataset.\n", "versions": [{"version": "v1", "created": "Tue, 24 Nov 2020 18:01:57 GMT"}], "update_date": "2020-11-25", "authors_parsed": [["Babolhavaeji", "Ali", ""], ["Fanaei", "Mohammad", ""]]}, {"id": "2011.12263", "submitter": "Matteo Dunnhofer", "authors": "Matteo Dunnhofer, Antonino Furnari, Giovanni Maria Farinella,\n  Christian Micheloni", "title": "Is First Person Vision Challenging for Object Tracking? The TREK-100\n  Benchmark Dataset", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding human-object interactions is fundamental in First Person Vision\n(FPV). Tracking algorithms which follow the objects manipulated by the camera\nwearer can provide useful information to effectively model such interactions.\nDespite a few previous attempts to exploit trackers in FPV applications, a\nsystematic analysis of the performance of state-of-the-art trackers in this\ndomain is still missing. On the other hand, the visual tracking solutions\navailable in the computer vision literature have significantly improved their\nperformance in the last years for a large variety of target objects and\ntracking scenarios. To fill the gap, in this paper, we present TREK-100, the\nfirst benchmark dataset for visual object tracking in FPV. The dataset is\ncomposed of 100 video sequences densely annotated with 60K bounding boxes, 17\nsequence attributes, 13 action verb attributes and 29 target object attributes.\nAlong with the dataset, we present an extensive analysis of the performance of\n30 among the best and most recent visual trackers. Our results show that object\ntracking in FPV is challenging, which suggests that more research efforts\nshould be devoted to this problem.\n", "versions": [{"version": "v1", "created": "Tue, 24 Nov 2020 18:18:15 GMT"}], "update_date": "2020-11-25", "authors_parsed": [["Dunnhofer", "Matteo", ""], ["Furnari", "Antonino", ""], ["Farinella", "Giovanni Maria", ""], ["Micheloni", "Christian", ""]]}, {"id": "2011.12267", "submitter": "Nori Uday Kiran", "authors": "Hirak Doshi, N. Uday Kiran", "title": "Constraint Based Refinement of Optical Flow", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV math.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The goal of this paper is to formulate a general framework for a\nconstraint-based refinement of the optical flow using variational methods. We\ndemonstrate that for a particular choice of the constraint, formulated as a\nminimization problem with the quadratic regularization, our results are close\nto the continuity equation based fluid flow. This closeness to the continuity\nmodel is theoretically justified through a modified augmented Lagrangian method\nand validated numerically. Further, along with the continuity constraint, our\nmodel can include geometric constraints as well. The correctness of our process\nis studied in the Hilbert space setting. Moreover, a special feature of our\nsystem is the possibility of a diagonalization by the Cauchy-Riemann operator\nand transforming it to a diffusion process on the curl and the divergence of\nthe flow. Using the theory of semigroups on the decoupled system, we show that\nour process preserves the spatial characteristics of the divergence and the\nvorticities. We perform several numerical experiments and show the results on\ndifferent datasets.\n", "versions": [{"version": "v1", "created": "Tue, 24 Nov 2020 18:23:39 GMT"}], "update_date": "2020-11-25", "authors_parsed": [["Doshi", "Hirak", ""], ["Kiran", "N. Uday", ""]]}, {"id": "2011.12276", "submitter": "Hubert Lin", "authors": "Hubert Lin, Mitchell Van Zuijlen, Maarten W.A. Wijntjes, Sylvia C.\n  Pont, Kavita Bala", "title": "Insights From A Large-Scale Database of Material Depictions In Paintings", "comments": "International Workshop on Fine Art Pattern Extraction and\n  Recognition, ICPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Deep learning has paved the way for strong recognition systems which are\noften both trained on and applied to natural images. In this paper, we examine\nthe give-and-take relationship between such visual recognition systems and the\nrich information available in the fine arts. First, we find that visual\nrecognition systems designed for natural images can work surprisingly well on\npaintings. In particular, we find that interactive segmentation tools can be\nused to cleanly annotate polygonal segments within paintings, a task which is\ntime consuming to undertake by hand. We also find that FasterRCNN, a model\nwhich has been designed for object recognition in natural scenes, can be\nquickly repurposed for detection of materials in paintings. Second, we show\nthat learning from paintings can be beneficial for neural networks that are\nintended to be used on natural images. We find that training on paintings\ninstead of natural images can improve the quality of learned features and we\nfurther find that a large number of paintings can be a valuable source of test\ndata for evaluating domain adaptation algorithms. Our experiments are based on\na novel large-scale annotated database of material depictions in paintings\nwhich we detail in a separate manuscript.\n", "versions": [{"version": "v1", "created": "Tue, 24 Nov 2020 18:42:58 GMT"}], "update_date": "2020-11-25", "authors_parsed": [["Lin", "Hubert", ""], ["Van Zuijlen", "Mitchell", ""], ["Wijntjes", "Maarten W. A.", ""], ["Pont", "Sylvia C.", ""], ["Bala", "Kavita", ""]]}, {"id": "2011.12289", "submitter": "Dongdong Chen", "authors": "Yunsheng Li and Yinpeng Chen and Xiyang Dai and Dongdong Chen and\n  Mengchen Liu and Lu Yuan and Zicheng Liu and Lei Zhang and Nuno Vasconcelos", "title": "MicroNet: Towards Image Recognition with Extremely Low FLOPs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present MicroNet, which is an efficient convolutional\nneural network using extremely low computational cost (e.g. 6 MFLOPs on\nImageNet classification). Such a low cost network is highly desired on edge\ndevices, yet usually suffers from a significant performance degradation. We\nhandle the extremely low FLOPs based upon two design principles: (a) avoiding\nthe reduction of network width by lowering the node connectivity, and (b)\ncompensating for the reduction of network depth by introducing more complex\nnon-linearity per layer. Firstly, we propose Micro-Factorized convolution to\nfactorize both pointwise and depthwise convolutions into low rank matrices for\na good tradeoff between the number of channels and input/output connectivity.\nSecondly, we propose a new activation function, named Dynamic Shift-Max, to\nimprove the non-linearity via maxing out multiple dynamic fusions between an\ninput feature map and its circular channel shift. The fusions are dynamic as\ntheir parameters are adapted to the input. Building upon Micro-Factorized\nconvolution and dynamic Shift-Max, a family of MicroNets achieve a significant\nperformance gain over the state-of-the-art in the low FLOP regime. For\ninstance, MicroNet-M1 achieves 61.1% top-1 accuracy on ImageNet classification\nwith 12 MFLOPs, outperforming MobileNetV3 by 11.3%.\n", "versions": [{"version": "v1", "created": "Tue, 24 Nov 2020 18:59:39 GMT"}], "update_date": "2020-11-25", "authors_parsed": [["Li", "Yunsheng", ""], ["Chen", "Yinpeng", ""], ["Dai", "Xiyang", ""], ["Chen", "Dongdong", ""], ["Liu", "Mengchen", ""], ["Yuan", "Lu", ""], ["Liu", "Zicheng", ""], ["Zhang", "Lei", ""], ["Vasconcelos", "Nuno", ""]]}, {"id": "2011.12372", "submitter": "Will Price", "authors": "Will Price and Dima Damen", "title": "Play Fair: Frame Attributions in Video Models", "comments": "Code available at: https://github.com/willprice/play-fair/ and\n  supporting website at: https://play-fair.willprice.dev/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce an attribution method for explaining action\nrecognition models. Such models fuse information from multiple frames within a\nvideo, through score aggregation or relational reasoning. We break down a\nmodel's class score into the sum of contributions from each frame, fairly. Our\nmethod adapts an axiomatic solution to fair reward distribution in cooperative\ngames, known as the Shapley value, for elements in a variable-length sequence,\nwhich we call the Element Shapley Value (ESV). Critically, we propose a\ntractable approximation of ESV that scales linearly with the number of frames\nin the sequence. We employ ESV to explain two action recognition models (TRN\nand TSN) on the fine-grained dataset Something-Something. We offer detailed\nanalysis of supporting/distracting frames, and the relationships of ESVs to the\nframe's position, class prediction, and sequence length. We compare ESV to\nnaive baselines and two commonly used feature attribution methods: Grad-CAM and\nIntegrated-Gradients.\n", "versions": [{"version": "v1", "created": "Tue, 24 Nov 2020 20:45:29 GMT"}], "update_date": "2020-11-26", "authors_parsed": [["Price", "Will", ""], ["Damen", "Dima", ""]]}, {"id": "2011.12384", "submitter": "Chen Chen", "authors": "Sijie Zhu and Taojiannan Yang and Matias Mendieta and Chen Chen", "title": "A3D: Adaptive 3D Networks for Video Action Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents A3D, an adaptive 3D network that can infer at a wide\nrange of computational constraints with one-time training. Instead of training\nmultiple models in a grid-search manner, it generates good configurations by\ntrading off between network width and spatio-temporal resolution. Furthermore,\nthe computation cost can be adapted after the model is deployed to meet\nvariable constraints, for example, on edge devices. Even under the same\ncomputational constraints, the performance of our adaptive networks can be\nsignificantly boosted over the baseline counterparts by the mutual training\nalong three dimensions. When a multiple pathway framework, e.g. SlowFast, is\nadopted, our adaptive method encourages a better trade-off between pathways\nthan manual designs. Extensive experiments on the Kinetics dataset show the\neffectiveness of the proposed framework. The performance gain is also verified\nto transfer well between datasets and tasks. Code will be made available.\n", "versions": [{"version": "v1", "created": "Tue, 24 Nov 2020 21:01:11 GMT"}], "update_date": "2020-11-26", "authors_parsed": [["Zhu", "Sijie", ""], ["Yang", "Taojiannan", ""], ["Mendieta", "Matias", ""], ["Chen", "Chen", ""]]}, {"id": "2011.12398", "submitter": "Anthony Kelly", "authors": "Anthony Kelly", "title": "Distribution Conditional Denoising: A Flexible Discriminative Image\n  Denoiser", "comments": "10 pages, 8 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  A flexible discriminative image denoiser is introduced in which multi-task\nlearning methods are applied to a densoising FCN based on U-Net. The\nactivations of the U-Net model are modified by affine transforms that are a\nlearned function of conditioning inputs. The learning procedure for multiple\nnoise types and levels involves applying a distribution of noise parameters\nduring training to the conditioning inputs, with the same noise parameters\napplied to a noise generating layer at the input (similar to the approach taken\nin a denoising autoencoder). It is shown that this flexible denoising model\nachieves state of the art performance on images corrupted with Gaussian and\nPoisson noise. It has also been shown that this conditional training method can\ngeneralise a fixed noise level U-Net denoiser to a variety of noise levels.\n", "versions": [{"version": "v1", "created": "Tue, 24 Nov 2020 21:27:18 GMT"}], "update_date": "2020-11-26", "authors_parsed": [["Kelly", "Anthony", ""]]}, {"id": "2011.12408", "submitter": "Sahand Hajifar", "authors": "Sahand Hajifar and Hongyue Sun", "title": "Online Domain Adaptation for Continuous Cross-Subject Liver Viability\n  Evaluation Based on Irregular Thermal Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Accurate evaluation of liver viability during its procurement is a\nchallenging issue and has traditionally been addressed by taking invasive\nbiopsy on liver. Recently, people have started to investigate on the\nnon-invasive evaluation of liver viability during its procurement using the\nliver surface thermal images. However, existing works include the background\nnoise in the thermal images and do not consider the cross-subject heterogeneity\nof livers, thus the viability evaluation accuracy can be affected. In this\npaper, we propose to use the irregular thermal data of the pure liver region,\nand the cross-subject liver evaluation information (i.e., the available\nviability label information in cross-subject livers), for the real-time\nevaluation of a new liver's viability. To achieve this objective, we extract\nfeatures of irregular thermal data based on tools from graph signal processing\n(GSP), and propose an online domain adaptation (DA) and classification\nframework using the GSP features of cross-subject livers. A multiconvex block\ncoordinate descent based algorithm is designed to jointly learn the\ndomain-invariant features during online DA and learn the classifier. Our\nproposed framework is applied to the liver procurement data, and classifies the\nliver viability accurately.\n", "versions": [{"version": "v1", "created": "Tue, 24 Nov 2020 21:42:19 GMT"}], "update_date": "2020-11-26", "authors_parsed": [["Hajifar", "Sahand", ""], ["Sun", "Hongyue", ""]]}, {"id": "2011.12423", "submitter": "Hatem Hajri", "authors": "Manon C\\'esaire, Hatem Hajri, Sylvain Lamprier, and Patrick Gallinari", "title": "Stochastic sparse adversarial attacks", "comments": "The link to the codes is given:\n  https://github.com/SSAA3/stochastic-sparse-adv-attacks", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces stochastic sparse adversarial attacks (SSAA), simple,\nfast and purely noise-based targeted and untargeted $L_0$ attacks of neural\nnetwork classifiers (NNC). SSAA are devised by exploiting a simple small-time\nexpansion idea widely used for Markov processes and offer new examples of $L_0$\nattacks whose studies have been limited. They are designed to solve the known\nscalability issue of the family of Jacobian-based saliency maps attacks to\nlarge datasets and they succeed in solving it. Experiments on small and large\ndatasets (CIFAR-10 and ImageNet) illustrate further advantages of SSAA in\ncomparison with the-state-of-the-art methods. For instance, in the untargeted\ncase, our method called Voting Folded Gaussian Attack (VFGA) scales efficiently\nto ImageNet and achieves a significantly lower $L_0$ score than SparseFool (up\nto $\\frac{2}{5}$ lower) while being faster. Moreover, VFGA achieves better\n$L_0$ scores on ImageNet than Sparse-RS when both attacks are fully successful\non a large number of samples. Codes are publicly available through the link\nhttps://github.com/SSAA3/stochastic-sparse-adv-attacks\n", "versions": [{"version": "v1", "created": "Tue, 24 Nov 2020 22:07:51 GMT"}, {"version": "v2", "created": "Sun, 18 Apr 2021 22:02:27 GMT"}, {"version": "v3", "created": "Tue, 27 Jul 2021 10:51:29 GMT"}], "update_date": "2021-07-28", "authors_parsed": [["C\u00e9saire", "Manon", ""], ["Hajri", "Hatem", ""], ["Lamprier", "Sylvain", ""], ["Gallinari", "Patrick", ""]]}, {"id": "2011.12427", "submitter": "Luiz A. Zanlorensi", "authors": "Luiz A. Zanlorensi and Rayson Laroca and Diego R. Lucio and Lucas R.\n  Santos and Alceu S. Britto Jr. and David Menotti", "title": "UFPR-Periocular: A Periocular Dataset Collected by Mobile Devices in\n  Unconstrained Scenarios", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, ocular biometrics in unconstrained environments using images\nobtained at visible wavelength have gained the researchers' attention,\nespecially with images captured by mobile devices. Periocular recognition has\nbeen demonstrated to be an alternative when the iris trait is not available due\nto occlusions or low image resolution. However, the periocular trait does not\nhave the high uniqueness presented in the iris trait. Thus, the use of datasets\ncontaining many subjects is essential to assess biometric systems' capacity to\nextract discriminating information from the periocular region. Also, to address\nthe within-class variability caused by lighting and attributes in the\nperiocular region, it is of paramount importance to use datasets with images of\nthe same subject captured in distinct sessions. As the datasets available in\nthe literature do not present all these factors, in this work, we present a new\nperiocular dataset containing samples from 1,122 subjects, acquired in 3\nsessions by 196 different mobile devices. The images were captured under\nunconstrained environments with just a single instruction to the participants:\nto place their eyes on a region of interest. We also performed an extensive\nbenchmark with several Convolutional Neural Network (CNN) architectures and\nmodels that have been employed in state-of-the-art approaches based on\nMulti-class Classification, Multitask Learning, Pairwise Filters Network, and\nSiamese Network. The results achieved in the closed- and open-world protocol,\nconsidering the identification and verification tasks, show that this area\nstill needs research and development.\n", "versions": [{"version": "v1", "created": "Tue, 24 Nov 2020 22:20:37 GMT"}], "update_date": "2020-11-26", "authors_parsed": [["Zanlorensi", "Luiz A.", ""], ["Laroca", "Rayson", ""], ["Lucio", "Diego R.", ""], ["Santos", "Lucas R.", ""], ["Britto", "Alceu S.", "Jr."], ["Menotti", "David", ""]]}, {"id": "2011.12429", "submitter": "Zeynettin Akkus", "authors": "Mohamed Y. Elwazir, Zeynettin Akkus, Didem Oguz, Jae K. Oh", "title": "Fully Automated Mitral Inflow Doppler Analysis Using Deep Learning", "comments": null, "journal-ref": "IEEE BIBE 2020 Proceedings", "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Echocardiography (echo) is an indispensable tool in a cardiologist's\ndiagnostic armamentarium. To date, almost all echocardiographic parameters\nrequire time-consuming manual labeling and measurements by an experienced\nechocardiographer and exhibit significant variability, owing to the noisy and\nartifact-laden nature of echo images. For example, mitral inflow (MI) Doppler\nis used to assess left ventricular (LV) diastolic function, which is of\nparamount clinical importance to distinguish between different cardiac\ndiseases. In the current work we present a fully automated workflow which\nleverages deep learning to a) label MI Doppler images acquired in an echo\nstudy, b) detect the envelope of MI Doppler signal, c) extract early and late\nfiling (E and A wave) flow velocities and E-wave deceleration time from the\nenvelope. We trained a variety of convolutional neural networks (CNN) models on\n5544 images of 140 patients for predicting 24 image classes including MI\nDoppler images and obtained overall accuracy of 0.97 on 1737 images of 40\npatients. Automated E and A wave velocity showed excellent correlation (Pearson\nR 0.99 and 0.98 respectively) and Bland Altman agreement (mean difference 0.06\nand 0.05 m/s respectively and SD 0.03 for both) with the operator measurements.\nDeceleration time also showed good but lower correlation (Pearson R 0.82) and\nBland-Altman agreement (mean difference: 34.1ms, SD: 30.9ms). These results\ndemonstrate feasibility of Doppler echocardiography measurement automation and\nthe promise of a fully automated echocardiography measurement package.\n", "versions": [{"version": "v1", "created": "Tue, 24 Nov 2020 22:27:14 GMT"}], "update_date": "2020-11-26", "authors_parsed": [["Elwazir", "Mohamed Y.", ""], ["Akkus", "Zeynettin", ""], ["Oguz", "Didem", ""], ["Oh", "Jae K.", ""]]}, {"id": "2011.12430", "submitter": "Yan Xia", "authors": "Yan Xia, Yusheng Xu, Shuang Li, Rui Wang, Juan Du, Daniel Cremers, Uwe\n  Stilla", "title": "SOE-Net: A Self-Attention and Orientation Encoding Network for Point\n  Cloud based Place Recognition", "comments": "Accepted by CVPR2021 (Oral)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We tackle the problem of place recognition from point cloud data and\nintroduce a self-attention and orientation encoding network (SOE-Net) that\nfully explores the relationship between points and incorporates long-range\ncontext into point-wise local descriptors. Local information of each point from\neight orientations is captured in a PointOE module, whereas long-range feature\ndependencies among local descriptors are captured with a self-attention unit.\nMoreover, we propose a novel loss function called Hard Positive Hard Negative\nquadruplet loss (HPHN quadruplet), that achieves better performance than the\ncommonly used metric learning loss. Experiments on various benchmark datasets\ndemonstrate superior performance of the proposed network over the current\nstate-of-the-art approaches. Our code is released publicly at\nhttps://github.com/Yan-Xia/SOE-Net.\n", "versions": [{"version": "v1", "created": "Tue, 24 Nov 2020 22:28:25 GMT"}, {"version": "v2", "created": "Sun, 23 May 2021 11:37:49 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Xia", "Yan", ""], ["Xu", "Yusheng", ""], ["Li", "Shuang", ""], ["Wang", "Rui", ""], ["Du", "Juan", ""], ["Cremers", "Daniel", ""], ["Stilla", "Uwe", ""]]}, {"id": "2011.12436", "submitter": "Shane Gilroy", "authors": "Shane Gilroy, John O'Dwyer and Lucas Bortoleto", "title": "Characterisation of CMOS Image Sensor Performance in Low Light\n  Automotive Applications", "comments": null, "journal-ref": "Irish Machine Vision and Image Processing Conference Proceedings\n  2017", "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The applications of automotive cameras in Advanced Driver-Assistance Systems\n(ADAS) are growing rapidly as automotive manufacturers strive to provide 360\ndegree protection for their customers. Vision systems must capture high quality\nimages in both daytime and night-time scenarios in order to produce the large\ninformational content required for software analysis in applications such as\nlane departure, pedestrian detection and collision detection. The challenge in\nproducing high quality images in low light scenarios is that the signal to\nnoise ratio is greatly reduced. This can result in noise becoming the dominant\nfactor in a captured image thereby making these safety systems less effective\nat night. This paper outlines a systematic method for characterisation of state\nof the art image sensor performance in response to noise, so as to improve the\ndesign and performance of automotive cameras in low light scenarios. The\nexperiment outlined in this paper demonstrates how this method can be used to\ncharacterise the performance of CMOS image sensors in response to electrical\nnoise on the power supply lines.\n", "versions": [{"version": "v1", "created": "Tue, 24 Nov 2020 22:49:54 GMT"}], "update_date": "2020-11-26", "authors_parsed": [["Gilroy", "Shane", ""], ["O'Dwyer", "John", ""], ["Bortoleto", "Lucas", ""]]}, {"id": "2011.12437", "submitter": "Dimitrios Tanoglidis", "authors": "Dimitrios Tanoglidis, Aleksandra \\'Ciprijanovi\\'c, Alex Drlica-Wagner", "title": "DeepShadows: Separating Low Surface Brightness Galaxies from Artifacts\n  using Deep Learning", "comments": "22 pages, 11 figures. Code and data related to this work can be found\n  at: https://github.com/dtanoglidis/DeepShadows", "journal-ref": null, "doi": null, "report-no": null, "categories": "astro-ph.GA astro-ph.CO astro-ph.IM cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Searches for low-surface-brightness galaxies (LSBGs) in galaxy surveys are\nplagued by the presence of a large number of artifacts (e.g., objects blended\nin the diffuse light from stars and galaxies, Galactic cirrus, star-forming\nregions in the arms of spiral galaxies, etc.) that have to be rejected through\ntime consuming visual inspection. In future surveys, which are expected to\ncollect hundreds of petabytes of data and detect billions of objects, such an\napproach will not be feasible. We investigate the use of convolutional neural\nnetworks (CNNs) for the problem of separating LSBGs from artifacts in survey\nimages. We take advantage of the fact that, for the first time, we have\navailable a large number of labeled LSBGs and artifacts from the Dark Energy\nSurvey, that we use to train, validate, and test a CNN model. That model, which\nwe call DeepShadows, achieves a test accuracy of $92.0 \\%$, a significant\nimprovement relative to feature-based machine learning models. We also study\nthe ability to use transfer learning to adapt this model to classify objects\nfrom the deeper Hyper-Suprime-Cam survey, and we show that after the model is\nretrained on a very small sample from the new survey, it can reach an accuracy\nof $87.6\\%$. These results demonstrate that CNNs offer a very promising path in\nthe quest to study the low-surface-brightness universe.\n", "versions": [{"version": "v1", "created": "Tue, 24 Nov 2020 22:51:08 GMT"}], "update_date": "2020-11-26", "authors_parsed": [["Tanoglidis", "Dimitrios", ""], ["\u0106iprijanovi\u0107", "Aleksandra", ""], ["Drlica-Wagner", "Alex", ""]]}, {"id": "2011.12438", "submitter": "Natalia Neverova", "authors": "Natalia Neverova, David Novotny, Vasil Khalidov, Marc Szafraniec,\n  Patrick Labatut, Andrea Vedaldi", "title": "Continuous Surface Embeddings", "comments": "NeurIPS, 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this work, we focus on the task of learning and representing dense\ncorrespondences in deformable object categories. While this problem has been\nconsidered before, solutions so far have been rather ad-hoc for specific object\ntypes (i.e., humans), often with significant manual work involved. However,\nscaling the geometry understanding to all objects in nature requires more\nautomated approaches that can also express correspondences between related, but\ngeometrically different objects. To this end, we propose a new, learnable\nimage-based representation of dense correspondences. Our model predicts, for\neach pixel in a 2D image, an embedding vector of the corresponding vertex in\nthe object mesh, therefore establishing dense correspondences between image\npixels and 3D object geometry. We demonstrate that the proposed approach\nperforms on par or better than the state-of-the-art methods for dense pose\nestimation for humans, while being conceptually simpler. We also collect a new\nin-the-wild dataset of dense correspondences for animal classes and demonstrate\nthat our framework scales naturally to the new deformable object categories.\n", "versions": [{"version": "v1", "created": "Tue, 24 Nov 2020 22:52:15 GMT"}], "update_date": "2020-11-26", "authors_parsed": [["Neverova", "Natalia", ""], ["Novotny", "David", ""], ["Khalidov", "Vasil", ""], ["Szafraniec", "Marc", ""], ["Labatut", "Patrick", ""], ["Vedaldi", "Andrea", ""]]}, {"id": "2011.12440", "submitter": "Bernhard Egger", "authors": "Skylar Sutherland and Bernhard Egger and Joshua Tenenbaum", "title": "Building 3D Morphable Models from a Single Scan", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a method for constructing generative models of 3D objects from a\nsingle 3D mesh. Our method produces a 3D morphable model that represents shape\nand albedo in terms of Gaussian processes. We define the shape deformations in\nphysical (3D) space and the albedo deformations as a combination of\nphysical-space and color-space deformations. Whereas previous approaches have\ntypically built 3D morphable models from multiple high-quality 3D scans through\nprincipal component analysis, we build 3D morphable models from a single scan\nor template. We demonstrate the utility of these models in the domain of face\nmodeling through inverse rendering and registration tasks. Specifically, we\nshow that our approach can be used to perform face recognition using only a\nsingle 3D scan (one scan total, not one per person), and further demonstrate\nhow multiple scans can be incorporated to improve performance without requiring\ndense correspondence. Our approach enables the synthesis of 3D morphable models\nfor 3D object categories where dense correspondence between multiple scans is\nunavailable. We demonstrate this by constructing additional 3D morphable models\nfor fish and birds and use them to perform simple inverse rendering tasks.\n", "versions": [{"version": "v1", "created": "Tue, 24 Nov 2020 23:08:14 GMT"}], "update_date": "2020-11-26", "authors_parsed": [["Sutherland", "Skylar", ""], ["Egger", "Bernhard", ""], ["Tenenbaum", "Joshua", ""]]}, {"id": "2011.12450", "submitter": "Peize Sun", "authors": "Peize Sun, Rufeng Zhang, Yi Jiang, Tao Kong, Chenfeng Xu, Wei Zhan,\n  Masayoshi Tomizuka, Lei Li, Zehuan Yuan, Changhu Wang, Ping Luo", "title": "Sparse R-CNN: End-to-End Object Detection with Learnable Proposals", "comments": "add test-dev; add crowdhuman", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present Sparse R-CNN, a purely sparse method for object detection in\nimages. Existing works on object detection heavily rely on dense object\ncandidates, such as $k$ anchor boxes pre-defined on all grids of image feature\nmap of size $H\\times W$. In our method, however, a fixed sparse set of learned\nobject proposals, total length of $N$, are provided to object recognition head\nto perform classification and location. By eliminating $HWk$ (up to hundreds of\nthousands) hand-designed object candidates to $N$ (e.g. 100) learnable\nproposals, Sparse R-CNN completely avoids all efforts related to object\ncandidates design and many-to-one label assignment. More importantly, final\npredictions are directly output without non-maximum suppression post-procedure.\nSparse R-CNN demonstrates accuracy, run-time and training convergence\nperformance on par with the well-established detector baselines on the\nchallenging COCO dataset, e.g., achieving 45.0 AP in standard $3\\times$\ntraining schedule and running at 22 fps using ResNet-50 FPN model. We hope our\nwork could inspire re-thinking the convention of dense prior in object\ndetectors. The code is available at: https://github.com/PeizeSun/SparseR-CNN.\n", "versions": [{"version": "v1", "created": "Wed, 25 Nov 2020 00:01:28 GMT"}, {"version": "v2", "created": "Mon, 26 Apr 2021 14:20:03 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Sun", "Peize", ""], ["Zhang", "Rufeng", ""], ["Jiang", "Yi", ""], ["Kong", "Tao", ""], ["Xu", "Chenfeng", ""], ["Zhan", "Wei", ""], ["Tomizuka", "Masayoshi", ""], ["Li", "Lei", ""], ["Yuan", "Zehuan", ""], ["Wang", "Changhu", ""], ["Luo", "Ping", ""]]}, {"id": "2011.12454", "submitter": "Zidi Xiu", "authors": "Zidi Xiu, Junya Chen, Ricardo Henao, Benjamin Goldstein, Lawrence\n  Carin, Chenyang Tao", "title": "Supercharging Imbalanced Data Learning With Energy-based Contrastive\n  Representation Transfer", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Dealing with severe class imbalance poses a major challenge for real-world\napplications, especially when the accurate classification and generalization of\nminority classes is of primary interest. In computer vision, learning from long\ntailed datasets is a recurring theme, especially for natural image datasets.\nWhile existing solutions mostly appeal to sampling or weighting adjustments to\nalleviate the pathological imbalance, or imposing inductive bias to prioritize\nnon-spurious associations, we take novel perspectives to promote sample\nefficiency and model generalization based on the invariance principles of\ncausality. Our proposal posits a meta-distributional scenario, where the data\ngenerating mechanism is invariant across the label-conditional feature\ndistributions. Such causal assumption enables efficient knowledge transfer from\nthe dominant classes to their under-represented counterparts, even if the\nrespective feature distributions show apparent disparities. This allows us to\nleverage a causal data inflation procedure to enlarge the representation of\nminority classes. Our development is orthogonal to the existing extreme\nclassification techniques thus can be seamlessly integrated. The utility of our\nproposal is validated with an extensive set of synthetic and real-world\ncomputer vision tasks against SOTA solutions.\n", "versions": [{"version": "v1", "created": "Wed, 25 Nov 2020 00:13:11 GMT"}, {"version": "v2", "created": "Thu, 18 Mar 2021 20:14:16 GMT"}, {"version": "v3", "created": "Sat, 5 Jun 2021 02:44:06 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Xiu", "Zidi", ""], ["Chen", "Junya", ""], ["Henao", "Ricardo", ""], ["Goldstein", "Benjamin", ""], ["Carin", "Lawrence", ""], ["Tao", "Chenyang", ""]]}, {"id": "2011.12470", "submitter": "Sicheng Zhao", "authors": "Sicheng Zhao, Xuanbai Chen, Xiangyu Yue, Chuang Lin, Pengfei Xu, Ravi\n  Krishna, Jufeng Yang, Guiguang Ding, Alberto L. Sangiovanni-Vincentelli, Kurt\n  Keutzer", "title": "Emotional Semantics-Preserved and Feature-Aligned CycleGAN for Visual\n  Emotion Adaptation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Thanks to large-scale labeled training data, deep neural networks (DNNs) have\nobtained remarkable success in many vision and multimedia tasks. However,\nbecause of the presence of domain shift, the learned knowledge of the\nwell-trained DNNs cannot be well generalized to new domains or datasets that\nhave few labels. Unsupervised domain adaptation (UDA) studies the problem of\ntransferring models trained on one labeled source domain to another unlabeled\ntarget domain. In this paper, we focus on UDA in visual emotion analysis for\nboth emotion distribution learning and dominant emotion classification.\nSpecifically, we design a novel end-to-end cycle-consistent adversarial model,\ntermed CycleEmotionGAN++. First, we generate an adapted domain to align the\nsource and target domains on the pixel-level by improving CycleGAN with a\nmulti-scale structured cycle-consistency loss. During the image translation, we\npropose a dynamic emotional semantic consistency loss to preserve the emotion\nlabels of the source images. Second, we train a transferable task classifier on\nthe adapted domain with feature-level alignment between the adapted and target\ndomains. We conduct extensive UDA experiments on the Flickr-LDL & Twitter-LDL\ndatasets for distribution learning and ArtPhoto & FI datasets for emotion\nclassification. The results demonstrate the significant improvements yielded by\nthe proposed CycleEmotionGAN++ as compared to state-of-the-art UDA approaches.\n", "versions": [{"version": "v1", "created": "Wed, 25 Nov 2020 01:31:01 GMT"}], "update_date": "2020-11-26", "authors_parsed": [["Zhao", "Sicheng", ""], ["Chen", "Xuanbai", ""], ["Yue", "Xiangyu", ""], ["Lin", "Chuang", ""], ["Xu", "Pengfei", ""], ["Krishna", "Ravi", ""], ["Yang", "Jufeng", ""], ["Ding", "Guiguang", ""], ["Sangiovanni-Vincentelli", "Alberto L.", ""], ["Keutzer", "Kurt", ""]]}, {"id": "2011.12482", "submitter": "Luca D'Alessio", "authors": "Luca D'Alessio and Mehrtash Babadi", "title": "CellSegmenter: unsupervised representation learning and instance\n  segmentation of modular images", "comments": "9 + 9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We introduce CellSegmenter, a structured deep generative model and an\namortized inference framework for unsupervised representation learning and\ninstance segmentation tasks. The proposed inference algorithm is convolutional\nand parallelized, without any recurrent mechanisms, and is able to resolve\nobject-object occlusion while simultaneously treating distant non-occluding\nobjects independently. This leads to extremely fast training times while\nallowing extrapolation to arbitrary number of instances. We further introduce a\ntransparent posterior regularization strategy that encourages scene\nreconstructions with fewest localized objects and a low-complexity background.\nWe evaluate our method on a challenging synthetic multi-MNIST dataset with a\nstructured background and achieve nearly perfect accuracy with only a few\nhundred training epochs. Finally, we show segmentation results obtained for a\ncell nuclei imaging dataset, demonstrating the ability of our method to provide\nhigh-quality segmentations while also handling realistic use cases involving\nlarge number of instances.\n", "versions": [{"version": "v1", "created": "Wed, 25 Nov 2020 02:10:58 GMT"}], "update_date": "2020-11-26", "authors_parsed": [["D'Alessio", "Luca", ""], ["Babadi", "Mehrtash", ""]]}, {"id": "2011.12483", "submitter": "Heng Fan", "authors": "Heng Fan, Haibin Ling", "title": "CRACT: Cascaded Regression-Align-Classification for Robust Visual\n  Tracking", "comments": "tech. report", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High quality object proposals are crucial in visual tracking algorithms that\nutilize region proposal network (RPN). Refinement of these proposals, typically\nby box regression and classification in parallel, has been popularly adopted to\nboost tracking performance. However, it still meets problems when dealing with\ncomplex and dynamic background. Thus motivated, in this paper we introduce an\nimproved proposal refinement module, Cascaded Regression-Align-Classification\n(CRAC), which yields new state-of-the-art performances on many benchmarks.\n  First, having observed that the offsets from box regression can serve as\nguidance for proposal feature refinement, we design CRAC as a cascade of box\nregression, feature alignment and box classification. The key is to bridge box\nregression and classification via an alignment step, which leads to more\naccurate features for proposal classification with improved robustness. To\naddress the variation in object appearance, we introduce an\nidentification-discrimination component for box classification, which leverages\noffline reliable fine-grained template and online rich background information\nto distinguish the target from background. Moreover, we present pyramid\nRoIAlign that benefits CRAC by exploiting both the local and global cues of\nproposals. During inference, tracking proceeds by ranking all refined proposals\nand selecting the best one. In experiments on seven benchmarks including\nOTB-2015, UAV123, NfS, VOT-2018, TrackingNet, GOT-10k and LaSOT, our CRACT\nexhibits very promising results in comparison with state-of-the-art competitors\nand runs in real-time.\n", "versions": [{"version": "v1", "created": "Wed, 25 Nov 2020 02:18:33 GMT"}], "update_date": "2020-11-26", "authors_parsed": [["Fan", "Heng", ""], ["Ling", "Haibin", ""]]}, {"id": "2011.12485", "submitter": "Yicheng Wu", "authors": "Yicheng Wu, Qiurui He, Tianfan Xue, Rahul Garg, Jiawen Chen, Ashok\n  Veeraraghavan, Jonathan T. Barron", "title": "How to Train Neural Networks for Flare Removal", "comments": "A new version paper is uploaded", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When a camera is pointed at a strong light source, the resulting photograph\nmay contain lens flare artifacts. Flares appear in a wide variety of patterns\n(halos, streaks, color bleeding, haze, etc.) and this diversity in appearance\nmakes flare removal challenging. Existing analytical solutions make strong\nassumptions about the artifact's geometry or brightness, and therefore only\nwork well on a small subset of flares. Machine learning techniques have shown\nsuccess in removing other types of artifacts, like reflections, but have not\nbeen widely applied to flare removal due to the lack of training data. To solve\nthis problem, we explicitly model the optical causes of flare either\nempirically or using wave optics, and generate semi-synthetic pairs of\nflare-corrupted and clean images. This enables us to train neural networks to\nremove lens flare for the first time. Experiments show our data synthesis\napproach is critical for accurate flare removal, and that models trained with\nour technique generalize well to real lens flares across different scenes,\nlighting conditions, and cameras.\n", "versions": [{"version": "v1", "created": "Wed, 25 Nov 2020 02:23:50 GMT"}, {"version": "v2", "created": "Thu, 26 Nov 2020 03:02:42 GMT"}, {"version": "v3", "created": "Tue, 30 Mar 2021 13:55:52 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Wu", "Yicheng", ""], ["He", "Qiurui", ""], ["Xue", "Tianfan", ""], ["Garg", "Rahul", ""], ["Chen", "Jiawen", ""], ["Veeraraghavan", "Ashok", ""], ["Barron", "Jonathan T.", ""]]}, {"id": "2011.12486", "submitter": "Woohyeon Shim", "authors": "Woohyeon Shim and Minsu Cho", "title": "CircleGAN: Generative Adversarial Learning across Spherical Circles", "comments": "16 pages, 8 figures", "journal-ref": "34th Conference on Neural Information Processing Systems (NeurIPS\n  2020), Vancouver, Canada", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel discriminator for GANs that improves realness and\ndiversity of generated samples by learning a structured hypersphere embedding\nspace using spherical circles. The proposed discriminator learns to populate\nrealistic samples around the longest spherical circle, i.e., a great circle,\nwhile pushing unrealistic samples toward the poles perpendicular to the great\ncircle. Since longer circles occupy larger area on the hypersphere, they\nencourage more diversity in representation learning, and vice versa.\nDiscriminating samples based on their corresponding spherical circles can thus\nnaturally induce diversity to generated samples. We also extend the proposed\nmethod for conditional settings with class labels by creating a hypersphere for\neach category and performing class-wise discrimination and update. In\nexperiments, we validate the effectiveness for both unconditional and\nconditional generation on standard benchmarks, achieving the state of the art.\n", "versions": [{"version": "v1", "created": "Wed, 25 Nov 2020 02:27:20 GMT"}, {"version": "v2", "created": "Tue, 5 Jan 2021 02:52:52 GMT"}], "update_date": "2021-01-06", "authors_parsed": [["Shim", "Woohyeon", ""], ["Cho", "Minsu", ""]]}, {"id": "2011.12490", "submitter": "Daniel Rebain", "authors": "Daniel Rebain, Wei Jiang, Soroosh Yazdani, Ke Li, Kwang Moo Yi, Andrea\n  Tagliasacchi", "title": "DeRF: Decomposed Radiance Fields", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the advent of Neural Radiance Fields (NeRF), neural networks can now\nrender novel views of a 3D scene with quality that fools the human eye. Yet,\ngenerating these images is very computationally intensive, limiting their\napplicability in practical scenarios. In this paper, we propose a technique\nbased on spatial decomposition capable of mitigating this issue. Our key\nobservation is that there are diminishing returns in employing larger (deeper\nand/or wider) networks. Hence, we propose to spatially decompose a scene and\ndedicate smaller networks for each decomposed part. When working together,\nthese networks can render the whole scene. This allows us near-constant\ninference time regardless of the number of decomposed parts. Moreover, we show\nthat a Voronoi spatial decomposition is preferable for this purpose, as it is\nprovably compatible with the Painter's Algorithm for efficient and GPU-friendly\nrendering. Our experiments show that for real-world scenes, our method provides\nup to 3x more efficient inference than NeRF (with the same rendering quality),\nor an improvement of up to 1.0~dB in PSNR (for the same inference cost).\n", "versions": [{"version": "v1", "created": "Wed, 25 Nov 2020 02:47:16 GMT"}], "update_date": "2020-11-26", "authors_parsed": [["Rebain", "Daniel", ""], ["Jiang", "Wei", ""], ["Yazdani", "Soroosh", ""], ["Li", "Ke", ""], ["Yi", "Kwang Moo", ""], ["Tagliasacchi", "Andrea", ""]]}, {"id": "2011.12492", "submitter": "Qinyan Huang", "authors": "Qinyan Huang and Weiwen Zhou and Minjie Wan and Xin Chen and Qian Chen\n  and Guohua Gu", "title": "Multi-feature driven active contour segmentation model for infrared\n  image with intensity inhomogeneity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Infrared (IR) image segmentation is essential in many urban defence\napplications, such as pedestrian surveillance, vehicle counting, security\nmonitoring, etc. Active contour model (ACM) is one of the most widely used\nimage segmentation tools at present, but the existing methods only utilize the\nlocal or global single feature information of image to minimize the energy\nfunction, which is easy to cause false segmentations in IR images. In this\npaper, we propose a multi-feature driven active contour segmentation model to\nhandle IR images with intensity inhomogeneity. Firstly, an especially-designed\nsigned pressure force (SPF) function is constructed by combining the global\ninformation calculated by global average gray information and the local\nmulti-feature information calculated by local entropy, local standard deviation\nand gradient information. Then, we draw upon adaptive weight coefficient\ncalculated by local range to adjust the afore-mentioned global term and local\nterm. Next, the SPF function is substituted into the level set formulation\n(LSF) for further evolution. Finally, the LSF converges after a finite number\nof iterations, and the IR image segmentation result is obtained from the\ncorresponding convergence result. Experimental results demonstrate that the\npresented method outperforms the state-of-the-art models in terms of precision\nrate and overlapping rate in IR test images.\n", "versions": [{"version": "v1", "created": "Wed, 25 Nov 2020 02:51:25 GMT"}], "update_date": "2020-11-26", "authors_parsed": [["Huang", "Qinyan", ""], ["Zhou", "Weiwen", ""], ["Wan", "Minjie", ""], ["Chen", "Xin", ""], ["Chen", "Qian", ""], ["Gu", "Guohua", ""]]}, {"id": "2011.12498", "submitter": "Chunyu Wang", "authors": "Rongchang Xie and Chunyu Wang and Wenjun Zeng and Yizhou Wang", "title": "Humble Teacher and Eager Student: Dual Network Learning for\n  Semi-supervised 2D Human Pose Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semi-supervised learning aims to boost the accuracy of a model by exploring\nunlabeled images. The state-of-the-art methods are consistency-based which\nlearn about unlabeled images by encouraging the model to give consistent\npredictions for images under different augmentations. However, when applied to\npose estimation, the methods degenerate and predict every pixel in unlabeled\nimages as background. This is because contradictory predictions are gradually\npushed to the background class due to highly imbalanced class distribution. But\nthis is not an issue in supervised learning because it has accurate labels.\nThis inspires us to stabilize the training by obtaining reliable pseudo labels.\nSpecifically, we learn two networks to mutually teach each other. In\nparticular, for each image, we compose an easy-hard pair by applying different\naugmentations and feed them to both networks. The more reliable predictions on\neasy images in each network are used to teach the other network to learn about\nthe corresponding hard images. The approach successfully avoids degeneration\nand achieves promising results on public datasets. The source code will be\nreleased.\n", "versions": [{"version": "v1", "created": "Wed, 25 Nov 2020 03:29:52 GMT"}], "update_date": "2020-11-26", "authors_parsed": [["Xie", "Rongchang", ""], ["Wang", "Chunyu", ""], ["Zeng", "Wenjun", ""], ["Wang", "Yizhou", ""]]}, {"id": "2011.12505", "submitter": "Wei Gao", "authors": "Wei Gao, Shangwei Guo, Tianwei Zhang, Han Qiu, Yonggang Wen, Yang Liu", "title": "Privacy-preserving Collaborative Learning with Automatic Transformation\n  Search", "comments": "17 pages, 16 figures, CVPR2021 oral", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Collaborative learning has gained great popularity due to its benefit of data\nprivacy protection: participants can jointly train a Deep Learning model\nwithout sharing their training sets. However, recent works discovered that an\nadversary can fully recover the sensitive training samples from the shared\ngradients. Such reconstruction attacks pose severe threats to collaborative\nlearning. Hence, effective mitigation solutions are urgently desired.\n  In this paper, we propose to leverage data augmentation to defeat\nreconstruction attacks: by preprocessing sensitive images with\ncarefully-selected transformation policies, it becomes infeasible for the\nadversary to extract any useful information from the corresponding gradients.\nWe design a novel search method to automatically discover qualified policies.\nWe adopt two new metrics to quantify the impacts of transformations on data\nprivacy and model usability, which can significantly accelerate the search\nspeed. Comprehensive evaluations demonstrate that the policies discovered by\nour method can defeat existing reconstruction attacks in collaborative\nlearning, with high efficiency and negligible impact on the model performance.\n", "versions": [{"version": "v1", "created": "Wed, 25 Nov 2020 03:56:54 GMT"}, {"version": "v2", "created": "Sun, 28 Mar 2021 14:30:50 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Gao", "Wei", ""], ["Guo", "Shangwei", ""], ["Zhang", "Tianwei", ""], ["Qiu", "Han", ""], ["Wen", "Yonggang", ""], ["Liu", "Yang", ""]]}, {"id": "2011.12506", "submitter": "Yan Han", "authors": "Yan Han, Chongyan Chen, Liyan Tang, Mingquan Lin, Ajay Jaiswal, Song\n  Wang, Ahmed Tewfik, George Shih, Ying Ding, Yifan Peng", "title": "Using Radiomics as Prior Knowledge for Thorax Disease Classification and\n  Localization in Chest X-rays", "comments": "Accepted by AMIA 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Chest X-ray becomes one of the most common medical diagnoses due to its\nnoninvasiveness. The number of chest X-ray images has skyrocketed, but reading\nchest X-rays still have been manually performed by radiologists, which creates\nhuge burnouts and delays. Traditionally, radiomics, as a subfield of radiology\nthat can extract a large number of quantitative features from medical images,\ndemonstrates its potential to facilitate medical imaging diagnosis before the\ndeep learning era. In this paper, we develop an end-to-end framework,\nChexRadiNet, that can utilize the radiomics features to improve the abnormality\nclassification performance. Specifically, ChexRadiNet first applies a\nlight-weight but efficient triplet-attention mechanism to classify the chest\nX-rays and highlight the abnormal regions. Then it uses the generated class\nactivation map to extract radiomic features, which further guides our model to\nlearn more robust image features. After a number of iterations and with the\nhelp of radiomic features, our framework can converge to more accurate image\nregions. We evaluate the ChexRadiNet framework using three public datasets: NIH\nChestX-ray, CheXpert, and MIMIC-CXR. We find that ChexRadiNet outperforms the\nstate-of-the-art on both disease detection (0.843 in AUC) and localization\n(0.679 in T(IoU) = 0.1). We will make the code publicly available at\nhttps://github.com/bionlplab/lung_disease_detection_amia2021, with the hope\nthat this method can facilitate the development of automatic systems with a\nhigher-level understanding of the radiological world.\n", "versions": [{"version": "v1", "created": "Wed, 25 Nov 2020 04:16:38 GMT"}, {"version": "v2", "created": "Tue, 19 Jan 2021 01:37:38 GMT"}, {"version": "v3", "created": "Fri, 9 Jul 2021 20:29:44 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Han", "Yan", ""], ["Chen", "Chongyan", ""], ["Tang", "Liyan", ""], ["Lin", "Mingquan", ""], ["Jaiswal", "Ajay", ""], ["Wang", "Song", ""], ["Tewfik", "Ahmed", ""], ["Shih", "George", ""], ["Ding", "Ying", ""], ["Peng", "Yifan", ""]]}, {"id": "2011.12527", "submitter": "Liangzhi Li", "authors": "Bowen Wang, Liangzhi Li, Manisha Verma, Yuta Nakashima, Ryo Kawasaki,\n  Hajime Nagahara", "title": "Match Them Up: Visually Explainable Few-shot Image Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Few-shot learning (FSL) approaches are usually based on an assumption that\nthe pre-trained knowledge can be obtained from base (seen) categories and can\nbe well transferred to novel (unseen) categories. However, there is no\nguarantee, especially for the latter part. This issue leads to the unknown\nnature of the inference process in most FSL methods, which hampers its\napplication in some risk-sensitive areas. In this paper, we reveal a new way to\nperform FSL for image classification, using visual representations from the\nbackbone model and weights generated by a newly-emerged explainable classifier.\nThe weighted representations only include a minimum number of distinguishable\nfeatures and the visualized weights can serve as an informative hint for the\nFSL process. Finally, a discriminator will compare the representations of each\npair of the images in the support set and the query set. Pairs with the highest\nscores will decide the classification results. Experimental results prove that\nthe proposed method can achieve both good accuracy and satisfactory\nexplainability on three mainstream datasets.\n", "versions": [{"version": "v1", "created": "Wed, 25 Nov 2020 05:47:35 GMT"}], "update_date": "2020-11-26", "authors_parsed": [["Wang", "Bowen", ""], ["Li", "Liangzhi", ""], ["Verma", "Manisha", ""], ["Nakashima", "Yuta", ""], ["Kawasaki", "Ryo", ""], ["Nagahara", "Hajime", ""]]}, {"id": "2011.12528", "submitter": "Naofumi Akimoto", "authors": "Naofumi Akimoto, Akio Hayakawa, Andrew Shin, Takuya Narihira", "title": "Reference-Based Video Colorization with Spatiotemporal Correspondence", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel reference-based video colorization framework with\nspatiotemporal correspondence. Reference-based methods colorize grayscale\nframes referencing a user input color frame. Existing methods suffer from the\ncolor leakage between objects and the emergence of average colors, derived from\nnon-local semantic correspondence in space. To address this issue, we warp\ncolors only from the regions on the reference frame restricted by\ncorrespondence in time. We propagate masks as temporal correspondences, using\ntwo complementary tracking approaches: off-the-shelf instance tracking for high\nperformance segmentation, and newly proposed dense tracking to track various\ntypes of objects. By restricting temporally-related regions for referencing\ncolors, our approach propagates faithful colors throughout the video.\nExperiments demonstrate that our method outperforms state-of-the-art methods\nquantitatively and qualitatively.\n", "versions": [{"version": "v1", "created": "Wed, 25 Nov 2020 05:47:38 GMT"}], "update_date": "2020-11-26", "authors_parsed": [["Akimoto", "Naofumi", ""], ["Hayakawa", "Akio", ""], ["Shin", "Andrew", ""], ["Narihira", "Takuya", ""]]}, {"id": "2011.12550", "submitter": "Xizhe Xue", "authors": "Xizhe Xue and Ying Li and Qiang Shen", "title": "Robust Correlation Tracking via Multi-channel Fused Features and\n  Reliable Response Map", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Benefiting from its ability to efficiently learn how an object is changing,\ncorrelation filters have recently demonstrated excellent performance for\nrapidly tracking objects. Designing effective features and handling model\ndrifts are two important aspects for online visual tracking. This paper tackles\nthese challenges by proposing a robust correlation tracking algorithm (RCT)\nbased on two ideas: First, we propose a method to fuse features in order to\nmore naturally describe the gradient and color information of the tracked\nobject, and introduce the fused features into a background aware correlation\nfilter to obtain the response map. Second, we present a novel strategy to\nsignificantly reduce noise in the response map and therefore ease the problem\nof model drift. Systematic comparative evaluations performed over multiple\ntracking benchmarks demonstrate the efficacy of the proposed approach.\n", "versions": [{"version": "v1", "created": "Wed, 25 Nov 2020 07:15:03 GMT"}], "update_date": "2020-11-26", "authors_parsed": [["Xue", "Xizhe", ""], ["Li", "Ying", ""], ["Shen", "Qiang", ""]]}, {"id": "2011.12562", "submitter": "Chang-Bin Zhang", "authors": "Chang-Bin Zhang, Peng-Tao Jiang, Qibin Hou, Yunchao Wei, Qi Han, Zhen\n  Li, Ming-Ming Cheng", "title": "Delving Deep into Label Smoothing", "comments": "12 pages, 7 figures, 12 tables", "journal-ref": "IEEE Transactions on Image Processing, 2021", "doi": "10.1109/TIP.2021.3089942", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Label smoothing is an effective regularization tool for deep neural networks\n(DNNs), which generates soft labels by applying a weighted average between the\nuniform distribution and the hard label. It is often used to reduce the\noverfitting problem of training DNNs and further improve classification\nperformance. In this paper, we aim to investigate how to generate more reliable\nsoft labels. We present an Online Label Smoothing (OLS) strategy, which\ngenerates soft labels based on the statistics of the model prediction for the\ntarget category. The proposed OLS constructs a more reasonable probability\ndistribution between the target categories and non-target categories to\nsupervise DNNs. Experiments demonstrate that based on the same classification\nmodels, the proposed approach can effectively improve the classification\nperformance on CIFAR-100, ImageNet, and fine-grained datasets. Additionally,\nthe proposed method can significantly improve the robustness of DNN models to\nnoisy labels compared to current label smoothing approaches.\n", "versions": [{"version": "v1", "created": "Wed, 25 Nov 2020 08:03:11 GMT"}, {"version": "v2", "created": "Thu, 22 Jul 2021 08:32:54 GMT"}], "update_date": "2021-07-23", "authors_parsed": [["Zhang", "Chang-Bin", ""], ["Jiang", "Peng-Tao", ""], ["Hou", "Qibin", ""], ["Wei", "Yunchao", ""], ["Han", "Qi", ""], ["Li", "Zhen", ""], ["Cheng", "Ming-Ming", ""]]}, {"id": "2011.12563", "submitter": "Shan Lin", "authors": "Shan Lin, Chang-Tsun Li, Alex C. Kot", "title": "Multi-Domain Adversarial Feature Generalization for Person\n  Re-Identification", "comments": "TIP (Accept with Mandatory Minor Revisions)", "journal-ref": null, "doi": "10.1109/TIP.2020.3046864", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the assistance of sophisticated training methods applied to single\nlabeled datasets, the performance of fully-supervised person re-identification\n(Person Re-ID) has been improved significantly in recent years. However, these\nmodels trained on a single dataset usually suffer from considerable performance\ndegradation when applied to videos of a different camera network. To make\nPerson Re-ID systems more practical and scalable, several cross-dataset domain\nadaptation methods have been proposed, which achieve high performance without\nthe labeled data from the target domain. However, these approaches still\nrequire the unlabeled data of the target domain during the training process,\nmaking them impractical. A practical Person Re-ID system pre-trained on other\ndatasets should start running immediately after deployment on a new site\nwithout having to wait until sufficient images or videos are collected and the\npre-trained model is tuned. To serve this purpose, in this paper, we\nreformulate person re-identification as a multi-dataset domain generalization\nproblem. We propose a multi-dataset feature generalization network (MMFA-AAE),\nwhich is capable of learning a universal domain-invariant feature\nrepresentation from multiple labeled datasets and generalizing it to `unseen'\ncamera systems. The network is based on an adversarial auto-encoder to learn a\ngeneralized domain-invariant latent feature representation with the Maximum\nMean Discrepancy (MMD) measure to align the distributions across multiple\ndomains. Extensive experiments demonstrate the effectiveness of the proposed\nmethod. Our MMFA-AAE approach not only outperforms most of the domain\ngeneralization Person Re-ID methods, but also surpasses many state-of-the-art\nsupervised methods and unsupervised domain adaptation methods by a large\nmargin.\n", "versions": [{"version": "v1", "created": "Wed, 25 Nov 2020 08:03:15 GMT"}], "update_date": "2021-02-03", "authors_parsed": [["Lin", "Shan", ""], ["Li", "Chang-Tsun", ""], ["Kot", "Alex C.", ""]]}, {"id": "2011.12589", "submitter": "Jaskirat Singh", "authors": "Jaskirat Singh and Liang Zheng", "title": "Combining Semantic Guidance and Deep Reinforcement Learning For\n  Generating Human Level Paintings", "comments": null, "journal-ref": "Proceedings of the IEEE/CVF Conference on Computer Vision and\n  Pattern Recognition (CVPR), 2021, pp. 16387-16396", "doi": null, "report-no": null, "categories": "cs.CV cs.CG cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Generation of stroke-based non-photorealistic imagery, is an important\nproblem in the computer vision community. As an endeavor in this direction,\nsubstantial recent research efforts have been focused on teaching machines \"how\nto paint\", in a manner similar to a human painter. However, the applicability\nof previous methods has been limited to datasets with little variation in\nposition, scale and saliency of the foreground object. As a consequence, we\nfind that these methods struggle to cover the granularity and diversity\npossessed by real world images. To this end, we propose a Semantic Guidance\npipeline with 1) a bi-level painting procedure for learning the distinction\nbetween foreground and background brush strokes at training time. 2) We also\nintroduce invariance to the position and scale of the foreground object through\na neural alignment model, which combines object localization and spatial\ntransformer networks in an end to end manner, to zoom into a particular\nsemantic instance. 3) The distinguishing features of the in-focus object are\nthen amplified by maximizing a novel guided backpropagation based focus reward.\nThe proposed agent does not require any supervision on human stroke-data and\nsuccessfully handles variations in foreground object attributes, thus,\nproducing much higher quality canvases for the CUB-200 Birds and Stanford\nCars-196 datasets. Finally, we demonstrate the further efficacy of our method\non complex datasets with multiple foreground object instances by evaluating an\nextension of our method on the challenging Virtual-KITTI dataset. Source code\nand models are available at https://github.com/1jsingh/semantic-guidance.\n", "versions": [{"version": "v1", "created": "Wed, 25 Nov 2020 09:00:04 GMT"}, {"version": "v2", "created": "Tue, 15 Jun 2021 00:39:15 GMT"}], "update_date": "2021-06-16", "authors_parsed": [["Singh", "Jaskirat", ""], ["Zheng", "Liang", ""]]}, {"id": "2011.12610", "submitter": "Xiahai Zhuang", "authors": "Shangqi Gao and Xiahai Zhuang", "title": "Rank-One Network: An Effective Framework for Image Restoration", "comments": null, "journal-ref": "IEEE Transactions on Pattern Analysis and Machine Intelligence\n  2021", "doi": "10.1109/TPAMI.2020.3046476", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The principal rank-one (RO) components of an image represent the\nself-similarity of the image, which is an important property for image\nrestoration. However, the RO components of a corrupted image could be decimated\nby the procedure of image denoising. We suggest that the RO property should be\nutilized and the decimation should be avoided in image restoration. To achieve\nthis, we propose a new framework comprised of two modules, i.e., the RO\ndecomposition and RO reconstruction. The RO decomposition is developed to\ndecompose a corrupted image into the RO components and residual. This is\nachieved by successively applying RO projections to the image or its residuals\nto extract the RO components. The RO projections, based on neural networks,\nextract the closest RO component of an image. The RO reconstruction is aimed to\nreconstruct the important information, respectively from the RO components and\nresidual, as well as to restore the image from this reconstructed information.\nExperimental results on four tasks, i.e., noise-free image super-resolution\n(SR), realistic image SR, gray-scale image denoising, and color image\ndenoising, show that the method is effective and efficient for image\nrestoration, and it delivers superior performance for realistic image SR and\ncolor image denoising.\n", "versions": [{"version": "v1", "created": "Wed, 25 Nov 2020 09:39:24 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Gao", "Shangqi", ""], ["Zhuang", "Xiahai", ""]]}, {"id": "2011.12616", "submitter": "Marco Toldo", "authors": "Marco Toldo, Umberto Michieli, Pietro Zanuttigh", "title": "Unsupervised Domain Adaptation in Semantic Segmentation via Orthogonal\n  and Clustered Embeddings", "comments": "Accepted at WACV 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning frameworks allowed for a remarkable advancement in semantic\nsegmentation, but the data hungry nature of convolutional networks has rapidly\nraised the demand for adaptation techniques able to transfer learned knowledge\nfrom label-abundant domains to unlabeled ones. In this paper we propose an\neffective Unsupervised Domain Adaptation (UDA) strategy, based on a feature\nclustering method that captures the different semantic modes of the feature\ndistribution and groups features of the same class into tight and\nwell-separated clusters. Furthermore, we introduce two novel learning\nobjectives to enhance the discriminative clustering performance: an\northogonality loss forces spaced out individual representations to be\northogonal, while a sparsity loss reduces class-wise the number of active\nfeature channels. The joint effect of these modules is to regularize the\nstructure of the feature space. Extensive evaluations in the synthetic-to-real\nscenario show that we achieve state-of-the-art performance.\n", "versions": [{"version": "v1", "created": "Wed, 25 Nov 2020 10:06:22 GMT"}], "update_date": "2020-11-26", "authors_parsed": [["Toldo", "Marco", ""], ["Michieli", "Umberto", ""], ["Zanuttigh", "Pietro", ""]]}, {"id": "2011.12618", "submitter": "John Chen", "authors": "John Chen, Samarth Sinha, Anastasios Kyrillidis", "title": "StackMix: A complementary Mix algorithm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Techniques combining multiple images as input/output have proven to be\neffective data augmentations for training convolutional neural networks. In\nthis paper, we present StackMix: Each input is presented as a concatenation of\ntwo images, and the label is the mean of the two one-hot labels. On its own,\nStackMix rivals other widely used methods in the \"Mix\" line of work. More\nimportantly, unlike previous work, significant gains across a variety of\nbenchmarks are achieved by combining StackMix with existing Mix augmentation,\neffectively mixing more than two images. E.g., by combining StackMix with\nCutMix, test error in the supervised setting is improved across a variety of\nsettings over CutMix, including 0.8\\% on ImageNet, 3\\% on Tiny ImageNet, 2\\% on\nCIFAR-100, 0.5\\% on CIFAR-10, and 1.5\\% on STL-10. Similar results are achieved\nwith Mixup.We further show that gains hold for robustness to common input\ncorruptions and perturbations at varying severities with a 0.7\\% improvement on\nCIFAR-100-C, by combining StackMix with AugMix over AugMix. On its own,\nimprovements with StackMix hold across different number of labeled samples on\nCIFAR-100, maintaining approximately a 2\\% gap in test accuracy -- down to\nusing only 5\\% of the whole dataset -- and is effective in the semi-supervised\nsetting with a 2\\% improvement with the standard benchmark $\\Pi$-model.\nFinally, we perform an extensive ablation study to better understand the\nproposed algorithm.\n", "versions": [{"version": "v1", "created": "Wed, 25 Nov 2020 10:15:24 GMT"}, {"version": "v2", "created": "Wed, 17 Mar 2021 16:49:41 GMT"}], "update_date": "2021-03-18", "authors_parsed": [["Chen", "John", ""], ["Sinha", "Samarth", ""], ["Kyrillidis", "Anastasios", ""]]}, {"id": "2011.12619", "submitter": "Zhe Chen", "authors": "Jack Humphreys, Zhe Chen, and Dacheng Tao", "title": "Recent Progress in Appearance-based Action Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Action recognition, which is formulated as a task to identify various human\nactions in a video, has attracted increasing interest from computer vision\nresearchers due to its importance in various applications. Recently,\nappearance-based methods have achieved promising progress towards accurate\naction recognition. In general, these methods mainly fulfill the task by\napplying various schemes to model spatial and temporal visual information\neffectively. To better understand the current progress of appearance-based\naction recognition, we provide a comprehensive review of recent achievements in\nthis area. In particular, we summarise and discuss several dozens of related\nresearch papers, which can be roughly divided into four categories according to\ndifferent appearance modelling strategies. The obtained categories include 2D\nconvolutional methods, 3D convolutional methods, motion representation-based\nmethods, and context representation-based methods. We analyse and discuss\nrepresentative methods from each category, comprehensively. Empirical results\nare also summarised to better illustrate cutting-edge algorithms. We conclude\nby identifying important areas for future research gleaned from our\ncategorisation.\n", "versions": [{"version": "v1", "created": "Wed, 25 Nov 2020 10:18:12 GMT"}], "update_date": "2020-11-26", "authors_parsed": [["Humphreys", "Jack", ""], ["Chen", "Zhe", ""], ["Tao", "Dacheng", ""]]}, {"id": "2011.12636", "submitter": "Prateek Katiyar Dr.", "authors": "Prateek Katiyar, Anna Khoreva", "title": "Improving Augmentation and Evaluation Schemes for Semantic Image\n  Synthesis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CG cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite data augmentation being a de facto technique for boosting the\nperformance of deep neural networks, little attention has been paid to\ndeveloping augmentation strategies for generative adversarial networks (GANs).\nTo this end, we introduce a novel augmentation scheme designed specifically for\nGAN-based semantic image synthesis models. We propose to randomly warp object\nshapes in the semantic label maps used as an input to the generator. The local\nshape discrepancies between the warped and non-warped label maps and images\nenable the GAN to learn better the structural and geometric details of the\nscene and thus to improve the quality of generated images. While benchmarking\nthe augmented GAN models against their vanilla counterparts, we discover that\nthe quantification metrics reported in the previous semantic image synthesis\nstudies are strongly biased towards specific semantic classes as they are\nderived via an external pre-trained segmentation network. We therefore propose\nto improve the established semantic image synthesis evaluation scheme by\nanalyzing separately the performance of generated images on the biased and\nunbiased classes for the given segmentation network. Finally, we show strong\nquantitative and qualitative improvements obtained with our augmentation\nscheme, on both class splits, using state-of-the-art semantic image synthesis\nmodels across three different datasets. On average across COCO-Stuff, ADE20K\nand Cityscapes datasets, the augmented models outperform their vanilla\ncounterparts by ~3 mIoU and ~10 FID points.\n", "versions": [{"version": "v1", "created": "Wed, 25 Nov 2020 10:55:26 GMT"}, {"version": "v2", "created": "Tue, 8 Dec 2020 16:22:06 GMT"}, {"version": "v3", "created": "Sat, 30 Jan 2021 09:43:15 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["Katiyar", "Prateek", ""], ["Khoreva", "Anna", ""]]}, {"id": "2011.12640", "submitter": "Yutong Xie", "authors": "Yutong Xie, Jianpeng Zhang, Zehui Liao, Yong Xia, and Chunhua Shen", "title": "PGL: Prior-Guided Local Self-supervised Learning for 3D Medical Image\n  Segmentation", "comments": "13 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  It has been widely recognized that the success of deep learning in image\nsegmentation relies overwhelmingly on a myriad amount of densely annotated\ntraining data, which, however, are difficult to obtain due to the tremendous\nlabor and expertise required, particularly for annotating 3D medical images.\nAlthough self-supervised learning (SSL) has shown great potential to address\nthis issue, most SSL approaches focus only on image-level global consistency,\nbut ignore the local consistency which plays a pivotal role in capturing\nstructural information for dense prediction tasks such as segmentation. In this\npaper, we propose a PriorGuided Local (PGL) self-supervised model that learns\nthe region-wise local consistency in the latent feature space. Specifically, we\nuse the spatial transformations, which produce different augmented views of the\nsame image, as a prior to deduce the location relation between two views, which\nis then used to align the feature maps of the same local region but being\nextracted on two views. Next, we construct a local consistency loss to minimize\nthe voxel-wise discrepancy between the aligned feature maps. Thus, our PGL\nmodel learns the distinctive representations of local regions, and hence is\nable to retain structural information. This ability is conducive to downstream\nsegmentation tasks. We conducted an extensive evaluation on four public\ncomputerized tomography (CT) datasets that cover 11 kinds of major human organs\nand two tumors. The results indicate that using pre-trained PGL model to\ninitialize a downstream network leads to a substantial performance improvement\nover both random initialization and the initialization with global\nconsistency-based models. Code and pre-trained weights will be made available\nat: https://git.io/PGL.\n", "versions": [{"version": "v1", "created": "Wed, 25 Nov 2020 11:03:11 GMT"}], "update_date": "2020-11-26", "authors_parsed": [["Xie", "Yutong", ""], ["Zhang", "Jianpeng", ""], ["Liao", "Zehui", ""], ["Xia", "Yong", ""], ["Shen", "Chunhua", ""]]}, {"id": "2011.12641", "submitter": "Sixing Yu", "authors": "Sixing Yu, Arya Mazaheri, Ali Jannesari", "title": "Auto Graph Encoder-Decoder for Model Compression and Network\n  Acceleration", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Model compression aims to deploy deep neural networks (DNN) to mobile devices\nwith limited computing power and storage resource. However, most of the\nexisting model compression methods rely on manually defined rules, which\nrequires domain expertise. In this paper, we propose an Auto Graph\nencoder-decoder Model Compression (AGMC) method combined with graph neural\nnetworks (GNN) and reinforcement learning (RL) to find the best compression\npolicy. We model the target DNN as a graph and use GNN to learn the embeddings\nof the DNN automatically. In our experiments, we first compared our method with\nrule-based DNN embedding methods to show the graph auto encoder-decoder's\neffectiveness. Our learning-based DNN embedding achieved better performance and\na higher compression ratio with fewer search steps. Moreover, we evaluated the\nAGMC on CIFAR-10 and ILSVRC-2012 datasets and compared handcrafted and\nlearning-based model compression approaches. Our method outperformed\nhandcrafted and learning-based methods on ResNet-56 with 3.6% and 1.8% higher\naccuracy, respectively. Furthermore, we achieved a higher compression ratio\nthan state-of-the-art methods on MobileNet-V2 with just 0.93% accuracy loss.\n", "versions": [{"version": "v1", "created": "Wed, 25 Nov 2020 11:05:21 GMT"}, {"version": "v2", "created": "Thu, 31 Dec 2020 04:20:20 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["Yu", "Sixing", ""], ["Mazaheri", "Arya", ""], ["Jannesari", "Ali", ""]]}, {"id": "2011.12643", "submitter": "Christian Wallraven", "authors": "Bj\\\"orn Browatzki, J\\\"orn-Philipp Lies, Christian Wallraven", "title": "The Unreasonable Effectiveness of Encoder-Decoder Networks for Retinal\n  Vessel Segmentation", "comments": null, "journal-ref": "In: Fu H., Garvin M.K., MacGillivray T., Xu Y., Zheng Y. (eds)\n  Ophthalmic Medical Image Analysis. OMIA 2020. Lecture Notes in Computer\n  Science, vol 12069. Springer, Cham", "doi": "10.1007/978-3-030-63419-3_5", "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  We propose an encoder-decoder framework for the segmentation of blood vessels\nin retinal images that relies on the extraction of large-scale patches at\nmultiple image-scales during training. Experiments on three fundus image\ndatasets demonstrate that this approach achieves state-of-the-art results and\ncan be implemented using a simple and efficient fully-convolutional network\nwith a parameter count of less than 0.8M. Furthermore, we show that this\nframework - called VLight - avoids overfitting to specific training images and\ngeneralizes well across different datasets, which makes it highly suitable for\nreal-world applications where robustness, accuracy as well as low inference\ntime on high-resolution fundus images is required.\n", "versions": [{"version": "v1", "created": "Wed, 25 Nov 2020 11:10:37 GMT"}], "update_date": "2020-11-26", "authors_parsed": [["Browatzki", "Bj\u00f6rn", ""], ["Lies", "J\u00f6rn-Philipp", ""], ["Wallraven", "Christian", ""]]}, {"id": "2011.12646", "submitter": "Guillaume Jaume", "authors": "Guillaume Jaume and Pushpak Pati and Behzad Bozorgtabar and Antonio\n  Foncubierta-Rodr\\'iguez and Florinda Feroce and Anna Maria Anniciello and\n  Tilman Rau and Jean-Philippe Thiran and Maria Gabrani and Orcun Goksel", "title": "Quantifying Explainers of Graph Neural Networks in Computational\n  Pathology", "comments": "CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Explainability of deep learning methods is imperative to facilitate their\nclinical adoption in digital pathology. However, popular deep learning methods\nand explainability techniques (explainers) based on pixel-wise processing\ndisregard biological entities' notion, thus complicating comprehension by\npathologists. In this work, we address this by adopting biological entity-based\ngraph processing and graph explainers enabling explanations accessible to\npathologists. In this context, a major challenge becomes to discern meaningful\nexplainers, particularly in a standardized and quantifiable fashion. To this\nend, we propose herein a set of novel quantitative metrics based on statistics\nof class separability using pathologically measurable concepts to characterize\ngraph explainers. We employ the proposed metrics to evaluate three types of\ngraph explainers, namely the layer-wise relevance propagation, gradient-based\nsaliency, and graph pruning approaches, to explain Cell-Graph representations\nfor Breast Cancer Subtyping. The proposed metrics are also applicable in other\ndomains by using domain-specific intuitive concepts. We validate the\nqualitative and quantitative findings on the BRACS dataset, a large cohort of\nbreast cancer RoIs, by expert pathologists.\n", "versions": [{"version": "v1", "created": "Wed, 25 Nov 2020 11:13:01 GMT"}, {"version": "v2", "created": "Fri, 14 May 2021 16:44:46 GMT"}], "update_date": "2021-05-17", "authors_parsed": [["Jaume", "Guillaume", ""], ["Pati", "Pushpak", ""], ["Bozorgtabar", "Behzad", ""], ["Foncubierta-Rodr\u00edguez", "Antonio", ""], ["Feroce", "Florinda", ""], ["Anniciello", "Anna Maria", ""], ["Rau", "Tilman", ""], ["Thiran", "Jean-Philippe", ""], ["Gabrani", "Maria", ""], ["Goksel", "Orcun", ""]]}, {"id": "2011.12652", "submitter": "Giuliana Ramella", "authors": "Giuliana Ramella", "title": "Evaluation of quality measures for color quantization", "comments": "Preprint", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Visual quality evaluation is one of the challenging basic problems in image\nprocessing. It also plays a central role in the shaping, implementation,\noptimization, and testing of many methods. The existing image quality\nassessment methods focused on images corrupted by common degradation types\nwhile little attention was paid to color quantization. This in spite there is a\nwide range of applications requiring color quantization assessment being used\nas a preprocessing step when color-based tasks are more efficiently\naccomplished on a reduced number of colors. In this paper, we propose and\ncarry-out a quantitative performance evaluation of nine well-known and commonly\nused full-reference image quality assessment measures. The evaluation is done\nby using two publicly available and subjectively rated image quality databases\nfor color quantization degradation and by considering suitable combinations or\nsubparts of them. The results indicate the quality measures that have closer\nperformances in terms of their correlation to the subjective human rating and\nshow that the evaluation of the statistical performance of the quality measures\nfor color quantization is significantly impacted by the selected image quality\ndatabase while maintaining a similar trend on each database. The detected\nstrong similarity both on individual databases and on databases obtained by\nintegration provides the ability to validate the integration process and to\nconsider the quantitative performance evaluation on each database as an\nindicator for performance on the other databases. The experimental results are\nuseful to address the choice of suitable quality measures for color\nquantization and to improve their future employment.\n", "versions": [{"version": "v1", "created": "Wed, 25 Nov 2020 11:25:54 GMT"}], "update_date": "2020-11-26", "authors_parsed": [["Ramella", "Giuliana", ""]]}, {"id": "2011.12661", "submitter": "Jay Santokhi", "authors": "Jay Santokhi, Pankaj Daga, Joned Sarwar, Anna Jordan, Emil Hewage", "title": "Temporal Autoencoder with U-Net Style Skip-Connections for Frame\n  Prediction", "comments": "7 pages, 3 figures, 3 tables, 4 equations", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Finding sustainable and novel solutions to predict city-wide mobility\nbehaviour is an ever-growing problem given increased urban complexity and\ngrowing populations. This paper seeks to address this by describing a traffic\nframe prediction approach that uses Convolutional LSTMs to create a Temporal\nAutoencoder with U-Net style skip-connections that marry together recurrent and\ntraditional computer vision techniques to capture spatio-temporal dependencies\nat different scales without losing topological details of a given city.\nUtilisation of Cyclical Learning Rates is also presented, improving training\nefficiency by achieving lower loss scores in fewer epochs than standard\napproaches.\n", "versions": [{"version": "v1", "created": "Wed, 25 Nov 2020 11:41:36 GMT"}], "update_date": "2020-12-01", "authors_parsed": [["Santokhi", "Jay", ""], ["Daga", "Pankaj", ""], ["Sarwar", "Joned", ""], ["Jordan", "Anna", ""], ["Hewage", "Emil", ""]]}, {"id": "2011.12663", "submitter": "Frederik Warburg", "authors": "Frederik Warburg, Martin J{\\o}rgensen, Javier Civera, S{\\o}ren Hauberg", "title": "Bayesian Triplet Loss: Uncertainty Quantification in Image Retrieval", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Uncertainty quantification in image retrieval is crucial for downstream\ndecisions, yet it remains a challenging and largely unexplored problem. Current\nmethods for estimating uncertainties are poorly calibrated, computationally\nexpensive, or based on heuristics. We present a new method that views image\nembeddings as stochastic features rather than deterministic features. Our two\nmain contributions are (1) a likelihood that matches the triplet constraint and\nthat evaluates the probability of an anchor being closer to a positive than a\nnegative; and (2) a prior over the feature space that justifies the\nconventional l2 normalization. To ensure computational efficiency, we derive a\nvariational approximation of the posterior, called the Bayesian triplet loss,\nthat produces state-of-the-art uncertainty estimates and matches the predictive\nperformance of current state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Wed, 25 Nov 2020 11:47:33 GMT"}, {"version": "v2", "created": "Wed, 9 Jun 2021 10:16:21 GMT"}], "update_date": "2021-06-10", "authors_parsed": [["Warburg", "Frederik", ""], ["J\u00f8rgensen", "Martin", ""], ["Civera", "Javier", ""], ["Hauberg", "S\u00f8ren", ""]]}, {"id": "2011.12672", "submitter": "Mattia Segu", "authors": "Mattia Segu, Alessio Tonioni, Federico Tombari", "title": "Batch Normalization Embeddings for Deep Domain Generalization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Domain generalization aims at training machine learning models to perform\nrobustly across different and unseen domains. Several recent methods use\nmultiple datasets to train models to extract domain-invariant features, hoping\nto generalize to unseen domains. Instead, first we explicitly train\ndomain-dependant representations by using ad-hoc batch normalization layers to\ncollect independent domain's statistics. Then, we propose to use these\nstatistics to map domains in a shared latent space, where membership to a\ndomain can be measured by means of a distance function. At test time, we\nproject samples from an unknown domain into the same space and infer properties\nof their domain as a linear combination of the known ones. We apply the same\nmapping strategy at training and test time, learning both a latent\nrepresentation and a powerful but lightweight ensemble model. We show a\nsignificant increase in classification accuracy over current state-of-the-art\ntechniques on popular domain generalization benchmarks: PACS, Office-31 and\nOffice-Caltech.\n", "versions": [{"version": "v1", "created": "Wed, 25 Nov 2020 12:02:57 GMT"}, {"version": "v2", "created": "Thu, 26 Nov 2020 17:05:19 GMT"}, {"version": "v3", "created": "Tue, 18 May 2021 09:58:12 GMT"}], "update_date": "2021-05-19", "authors_parsed": [["Segu", "Mattia", ""], ["Tonioni", "Alessio", ""], ["Tombari", "Federico", ""]]}, {"id": "2011.12680", "submitter": "Morgan Frearson Mr", "authors": "Morgan Frearson, Kien Nguyen", "title": "Adversarial Attack on Facial Recognition using Visible Light", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The use of deep learning for human identification and object detection is\nbecoming ever more prevalent in the surveillance industry. These systems have\nbeen trained to identify human body's or faces with a high degree of accuracy.\nHowever, there have been successful attempts to fool these systems with\ndifferent techniques called adversarial attacks. This paper presents a final\nreport for an adversarial attack using visible light on facial recognition\nsystems. The relevance of this research is to exploit the physical downfalls of\ndeep neural networks. This demonstration of weakness within these systems are\nin hopes that this research will be used in the future to improve the training\nmodels for object recognition. As results were gathered the project objectives\nwere adjusted to fit the outcomes. Because of this the following paper\ninitially explores an adversarial attack using infrared light before\nreadjusting to a visible light attack. A research outline on infrared light and\nfacial recognition are presented within. A detailed analyzation of the current\nfindings and possible future recommendations of the project are presented. The\nchallenges encountered are evaluated and a final solution is delivered. The\nprojects final outcome exhibits the ability to effectively fool recognition\nsystems using light.\n", "versions": [{"version": "v1", "created": "Wed, 25 Nov 2020 12:20:23 GMT"}], "update_date": "2020-11-26", "authors_parsed": [["Frearson", "Morgan", ""], ["Nguyen", "Kien", ""]]}, {"id": "2011.12688", "submitter": "Hui Yuan", "authors": "Qi Liu, Hui Yuan, Raouf Hamzaoui, Honglei Su, Junhui Hou, Huan Yang", "title": "Reduced Reference Perceptual Quality Model and Application to Rate\n  Control for 3D Point Cloud Compression", "comments": "14 figures and 7 tables, submitted to IEEE T IP", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In rate-distortion optimization, the encoder settings are determined by\nmaximizing a reconstruction quality measure subject to a constraint on the bit\nrate. One of the main challenges of this approach is to define a quality\nmeasure that can be computed with low computational cost and which correlates\nwell with perceptual quality. While several quality measures that fulfil these\ntwo criteria have been developed for images and video, no such one exists for\n3D point clouds. We address this limitation for the video-based point cloud\ncompression (V-PCC) standard by proposing a linear perceptual quality model\nwhose variables are the V-PCC geometry and color quantization parameters and\nwhose coefficients can easily be computed from two features extracted from the\noriginal 3D point cloud. Subjective quality tests with 400 compressed 3D point\nclouds show that the proposed model correlates well with the mean opinion\nscore, outperforming state-of-the-art full reference objective measures in\nterms of Spearman rank-order and Pearsons linear correlation coefficient.\nMoreover, we show that for the same target bit rate, ratedistortion\noptimization based on the proposed model offers higher perceptual quality than\nrate-distortion optimization based on exhaustive search with a point-to-point\nobjective quality metric.\n", "versions": [{"version": "v1", "created": "Wed, 25 Nov 2020 12:42:02 GMT"}], "update_date": "2020-12-01", "authors_parsed": [["Liu", "Qi", ""], ["Yuan", "Hui", ""], ["Hamzaoui", "Raouf", ""], ["Su", "Honglei", ""], ["Hou", "Junhui", ""], ["Yang", "Huan", ""]]}, {"id": "2011.12690", "submitter": "Bas van der Heijden", "authors": "Bas van der Heijden, Laura Ferranti, Jens Kober, Robert Babuska", "title": "DeepKoCo: Efficient latent planning with a robust Koopman representation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.RO cs.SY eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents DeepKoCo, a novel model-based agent that learns a latent\nKoopman representation from images. This representation allows DeepKoCo to plan\nefficiently using linear control methods, such as linear model predictive\ncontrol. Compared to traditional agents, DeepKoCo is robust to task-irrelevant\ndynamics, thanks to the use of a tailored lossy autoencoder network that allows\nDeepKoCo to learn latent dynamics that reconstruct and predict only observed\ncosts, rather than all observed dynamics. As our results show, DeepKoCo\nachieves a similar final performance as traditional model-free methods on\ncomplex control tasks, while being considerably more robust to distractor\ndynamics, making the proposed agent more amenable for real-life applications.\n", "versions": [{"version": "v1", "created": "Wed, 25 Nov 2020 12:46:55 GMT"}, {"version": "v2", "created": "Tue, 15 Jun 2021 07:52:13 GMT"}], "update_date": "2021-06-16", "authors_parsed": [["van der Heijden", "Bas", ""], ["Ferranti", "Laura", ""], ["Kober", "Jens", ""], ["Babuska", "Robert", ""]]}, {"id": "2011.12722", "submitter": "Bing Liu", "authors": "Anzhu Yu, Wenyue Guo, Bing Liu, Xin Chen, Xin Wang, Xuefeng Cao,\n  Bingchuan Jiang", "title": "Attention Aware Cost Volume Pyramid Based Multi-view Stereo Network for\n  3D Reconstruction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present an efficient multi-view stereo (MVS) network for 3D reconstruction\nfrom multiview images. While previous learning based reconstruction approaches\nperformed quite well, most of them estimate depth maps at a fixed resolution\nusing plane sweep volumes with a fixed depth hypothesis at each plane, which\nrequires densely sampled planes for desired accuracy and therefore is difficult\nto achieve high resolution depth maps. In this paper we introduce a\ncoarseto-fine depth inference strategy to achieve high resolution depth. This\nstrategy estimates the depth map at coarsest level, while the depth maps at\nfiner levels are considered as the upsampled depth map from previous level with\npixel-wise depth residual. Thus, we narrow the depth searching range with\npriori information from previous level and construct new cost volumes from the\npixel-wise depth residual to perform depth map refinement. Then the final depth\nmap could be achieved iteratively since all the parameters are shared between\ndifferent levels. At each level, the self-attention layer is introduced to the\nfeature extraction block for capturing the long range dependencies for depth\ninference task, and the cost volume is generated using similarity measurement\ninstead of the variance based methods used in previous work. Experiments were\nconducted on both the DTU benchmark dataset and recently released BlendedMVS\ndataset. The results demonstrated that our model could outperform most\nstate-of-the-arts (SOTA) methods. The codebase of this project is at\nhttps://github.com/ArthasMil/AACVP-MVSNet.\n", "versions": [{"version": "v1", "created": "Wed, 25 Nov 2020 13:34:11 GMT"}], "update_date": "2020-12-01", "authors_parsed": [["Yu", "Anzhu", ""], ["Guo", "Wenyue", ""], ["Liu", "Bing", ""], ["Chen", "Xin", ""], ["Wang", "Xin", ""], ["Cao", "Xuefeng", ""], ["Jiang", "Bingchuan", ""]]}, {"id": "2011.12735", "submitter": "Victor Saase", "authors": "Victor Saase, Holger Wenz, Thomas Ganslandt, Christoph Groden,\n  M\\'at\\'e E. Maros", "title": "Simple statistical methods for unsupervised brain anomaly detection on\n  MRI are competitive to deep learning methods", "comments": "20 pages, 7 figures, to be submitted to Medical Image Analysis", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Statistical analysis of magnetic resonance imaging (MRI) can help\nradiologists to detect pathologies that are otherwise likely to be missed. Deep\nlearning (DL) has shown promise in modeling complex spatial data for brain\nanomaly detection. However, DL models have major deficiencies: they need large\namounts of high-quality training data, are difficult to design and train and\nare sensitive to subtle changes in scanning protocols and hardware. Here, we\nshow that also simple statistical methods such as voxel-wise (baseline and\ncovariance) models and a linear projection method using spatial patterns can\nachieve DL-equivalent (3D convolutional autoencoder) performance in\nunsupervised pathology detection. All methods were trained (N=395) and compared\n(N=44) on a novel, expert-curated multiparametric (8 sequences) head MRI\ndataset of healthy and pathological cases, respectively. We show that these\nsimple methods can be more accurate in detecting small lesions and are\nconsiderably easier to train and comprehend. The methods were quantitatively\ncompared using AUC and average precision and evaluated qualitatively on\nclinical use cases comprising brain atrophy, tumors (small metastases) and\nmovement artefacts. Our results demonstrate that while DL methods may be\nuseful, they should show a sufficiently large performance improvement over\nsimpler methods to justify their usage. Thus, simple statistical methods should\nprovide the baseline for benchmarks. Source code and trained models are\navailable on GitHub (https://github.com/vsaase/simpleBAD).\n", "versions": [{"version": "v1", "created": "Wed, 25 Nov 2020 13:45:11 GMT"}], "update_date": "2020-11-26", "authors_parsed": [["Saase", "Victor", ""], ["Wenz", "Holger", ""], ["Ganslandt", "Thomas", ""], ["Groden", "Christoph", ""], ["Maros", "M\u00e1t\u00e9 E.", ""]]}, {"id": "2011.12745", "submitter": "Yue Qian", "authors": "Yue Qian, Junhui Hou, Sam Kwong and Ying He", "title": "Deep Magnification-Flexible Upsampling over 3D Point Clouds", "comments": "13 pages, 14 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper addresses the problem of generating dense point clouds from given\nsparse point clouds to model the underlying geometric structures of\nobjects/scenes. To tackle this challenging issue, we propose a novel end-to-end\nlearning-based framework. Specifically, by taking advantage of the linear\napproximation theorem, we first formulate the problem explicitly, which boils\ndown to determining the interpolation weights and high-order approximation\nerrors. Then, we design a lightweight neural network to adaptively learn\nunified and sorted interpolation weights as well as the high-order refinements,\nby analyzing the local geometry of the input point cloud. The proposed method\ncan be interpreted by the explicit formulation, and thus is more\nmemory-efficient than existing ones. In sharp contrast to the existing methods\nthat work only for a pre-defined and fixed upsampling factor, the proposed\nframework only requires a single neural network with one-time training to\nhandle various upsampling factors, which is highly desired in real-world\napplications. In addition, we propose a simple yet effective training strategy\nto drive such a flexible ability. In addition, our method can handle\nnon-uniformly distributed and noisy data well. Extensive experiments on both\nsynthetic and real-world data demonstrate the superiority of the proposed\nmethod over state-of-the-art methods both quantitatively and qualitatively.\n", "versions": [{"version": "v1", "created": "Wed, 25 Nov 2020 14:00:18 GMT"}, {"version": "v2", "created": "Tue, 18 May 2021 07:44:11 GMT"}], "update_date": "2021-05-19", "authors_parsed": [["Qian", "Yue", ""], ["Hou", "Junhui", ""], ["Kwong", "Sam", ""], ["He", "Ying", ""]]}, {"id": "2011.12762", "submitter": "Samuel Rivera", "authors": "Samuel Rivera, Olga Mendoza-Schrock, Ashley Diehl", "title": "Transfer Learning for Aided Target Recognition: Comparing Deep Learning\n  to other Machine Learning Approaches", "comments": null, "journal-ref": null, "doi": "10.1117/12.2514753", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Aided target recognition (AiTR), the problem of classifying objects from\nsensor data, is an important problem with applications across industry and\ndefense. While classification algorithms continue to improve, they often\nrequire more training data than is available or they do not transfer well to\nsettings not represented in the training set. These problems are mitigated by\ntransfer learning (TL), where knowledge gained in a well-understood source\ndomain is transferred to a target domain of interest. In this context, the\ntarget domain could represents a poorly-labeled dataset, a different sensor, or\nan altogether new set of classes to identify.\n  While TL for classification has been an active area of machine learning (ML)\nresearch for decades, transfer learning within a deep learning framework\nremains a relatively new area of research. Although deep learning (DL) provides\nexceptional modeling flexibility and accuracy on recent real world problems,\nopen questions remain regarding how much transfer benefit is gained by using DL\nversus other ML architectures. Our goal is to address this shortcoming by\ncomparing transfer learning within a DL framework to other ML approaches across\ntransfer tasks and datasets. Our main contributions are: 1) an empirical\nanalysis of DL and ML algorithms on several transfer tasks and domains\nincluding gene expressions and satellite imagery, and 2) a discussion of the\nlimitations and assumptions of TL for aided target recognition -- both for DL\nand ML in general. We close with a discussion of future directions for DL\ntransfer.\n", "versions": [{"version": "v1", "created": "Wed, 25 Nov 2020 14:25:49 GMT"}], "update_date": "2020-11-26", "authors_parsed": [["Rivera", "Samuel", ""], ["Mendoza-Schrock", "Olga", ""], ["Diehl", "Ashley", ""]]}, {"id": "2011.12786", "submitter": "Vinayak Elangovan", "authors": "Kavan Adeshara and Vinayak Elangovan", "title": "Face recognition using PCA integrated with Delaunay triangulation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Face Recognition is most used for biometric user authentication that\nidentifies a user based on his or her facial features. The system is in high\ndemand, as it is used by many businesses and employed in many devices such as\nsmartphones and surveillance cameras. However, one frequent problem that is\nstill observed in this user-verification method is its accuracy rate. Numerous\napproaches and algorithms have been experimented to improve the stated flaw of\nthe system. This research develops one such algorithm that utilizes a\ncombination of two different approaches. Using the concepts from Linear Algebra\nand computational geometry, the research examines the integration of Principal\nComponent Analysis with Delaunay Triangulation; the method triangulates a set\nof face landmark points and obtains eigenfaces of the provided images. It\ncompares the algorithm with traditional PCA and discusses the inclusion of\ndifferent face landmark points to deliver an effective recognition rate.\n", "versions": [{"version": "v1", "created": "Wed, 25 Nov 2020 14:46:08 GMT"}], "update_date": "2020-11-26", "authors_parsed": [["Adeshara", "Kavan", ""], ["Elangovan", "Vinayak", ""]]}, {"id": "2011.12790", "submitter": "Federico Ceola", "authors": "Federico Ceola, Elisa Maiettini, Giulia Pasquale, Lorenzo Rosasco and\n  Lorenzo Natale", "title": "Fast Region Proposal Learning for Object Detection for Robotics", "comments": "Manuscript submitted to 2021 IEEE/RSJ International Conference on\n  Intelligent Robots and Systems (IROS)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Object detection is a fundamental task for robots to operate in unstructured\nenvironments. Today, there are several deep learning algorithms that solve this\ntask with remarkable performance. Unfortunately, training such systems requires\nseveral hours of GPU time. For robots, to successfully adapt to changes in the\nenvironment or learning new objects, it is also important that object detectors\ncan be re-trained in a short amount of time. A recent method [1] proposes an\narchitecture that leverages on the powerful representation of deep learning\ndescriptors, while permitting fast adaptation time. Leveraging on the natural\ndecomposition of the task in (i) regions candidate generation, (ii) feature\nextraction and (iii) regions classification, this method performs fast\nadaptation of the detector, by only re-training the classification layer. This\nshortens training time while maintaining state-of-the-art performance. In this\npaper, we firstly demonstrate that a further boost in accuracy can be obtained\nby adapting, in addition, the regions candidate generation on the task at hand.\nSecondly, we extend the object detection system presented in [1] with the\nproposed fast learning approach, showing experimental evidence on the\nimprovement provided in terms of speed and accuracy on two different robotics\ndatasets. The code to reproduce the experiments is publicly available on\nGitHub.\n", "versions": [{"version": "v1", "created": "Wed, 25 Nov 2020 14:51:49 GMT"}, {"version": "v2", "created": "Fri, 9 Apr 2021 12:18:08 GMT"}], "update_date": "2021-06-30", "authors_parsed": [["Ceola", "Federico", ""], ["Maiettini", "Elisa", ""], ["Pasquale", "Giulia", ""], ["Rosasco", "Lorenzo", ""], ["Natale", "Lorenzo", ""]]}, {"id": "2011.12799", "submitter": "Dani Lischinski", "authors": "Zongze Wu, Dani Lischinski, Eli Shechtman", "title": "StyleSpace Analysis: Disentangled Controls for StyleGAN Image Generation", "comments": "25 pages, 21 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We explore and analyze the latent style space of StyleGAN2, a\nstate-of-the-art architecture for image generation, using models pretrained on\nseveral different datasets. We first show that StyleSpace, the space of\nchannel-wise style parameters, is significantly more disentangled than the\nother intermediate latent spaces explored by previous works. Next, we describe\na method for discovering a large collection of style channels, each of which is\nshown to control a distinct visual attribute in a highly localized and\ndisentangled manner. Third, we propose a simple method for identifying style\nchannels that control a specific attribute, using a pretrained classifier or a\nsmall number of example images. Manipulation of visual attributes via these\nStyleSpace controls is shown to be better disentangled than via those proposed\nin previous works. To show this, we make use of a newly proposed Attribute\nDependency metric. Finally, we demonstrate the applicability of StyleSpace\ncontrols to the manipulation of real images. Our findings pave the way to\nsemantically meaningful and well-disentangled image manipulations via simple\nand intuitive interfaces.\n", "versions": [{"version": "v1", "created": "Wed, 25 Nov 2020 15:00:33 GMT"}, {"version": "v2", "created": "Thu, 3 Dec 2020 17:30:00 GMT"}], "update_date": "2020-12-04", "authors_parsed": [["Wu", "Zongze", ""], ["Lischinski", "Dani", ""], ["Shechtman", "Eli", ""]]}, {"id": "2011.12805", "submitter": "Federico Ceola", "authors": "Federico Ceola, Elisa Maiettini, Giulia Pasquale, Lorenzo Rosasco and\n  Lorenzo Natale", "title": "Fast Object Segmentation Learning with Kernel-based Methods for Robotics", "comments": "Manuscript accepted to 2021 IEEE International Conference on Robotics\n  and Automation (ICRA)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Object segmentation is a key component in the visual system of a robot that\nperforms tasks like grasping and object manipulation, especially in presence of\nocclusions. Like many other computer vision tasks, the adoption of deep\narchitectures has made available algorithms that perform this task with\nremarkable performance. However, adoption of such algorithms in robotics is\nhampered by the fact that training requires large amount of computing time and\nit cannot be performed on-line. In this work, we propose a novel architecture\nfor object segmentation, that overcomes this problem and provides comparable\nperformance in a fraction of the time required by the state-of-the-art methods.\nOur approach is based on a pre-trained Mask R-CNN, in which various layers have\nbeen replaced with a set of classifiers and regressors that are re-trained for\na new task. We employ an efficient Kernel-based method that allows for fast\ntraining on large scale problems. Our approach is validated on the YCB-Video\ndataset which is widely adopted in the computer vision and robotics community,\ndemonstrating that we can achieve and even surpass performance of the\nstate-of-the-art, with a significant reduction (${\\sim}6\\times$) of the\ntraining time. The code to reproduce the experiments is publicly available on\nGitHub.\n", "versions": [{"version": "v1", "created": "Wed, 25 Nov 2020 15:07:39 GMT"}, {"version": "v2", "created": "Fri, 9 Apr 2021 11:55:42 GMT"}], "update_date": "2021-04-12", "authors_parsed": [["Ceola", "Federico", ""], ["Maiettini", "Elisa", ""], ["Pasquale", "Giulia", ""], ["Rosasco", "Lorenzo", ""], ["Natale", "Lorenzo", ""]]}, {"id": "2011.12807", "submitter": "Thibault Maho", "authors": "Thibault Maho, Teddy Furon, Erwan Le Merrer", "title": "SurFree: a fast surrogate-free black-box attack", "comments": "8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.AI cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning classifiers are critically prone to evasion attacks.\nAdversarial examples are slightly modified inputs that are then misclassified,\nwhile remaining perceptively close to their originals. Last couple of years\nhave witnessed a striking decrease in the amount of queries a black box attack\nsubmits to the target classifier, in order to forge adversarials. This\nparticularly concerns the black-box score-based setup, where the attacker has\naccess to top predicted probabilites: the amount of queries went from to\nmillions of to less than a thousand. This paper presents SurFree, a geometrical\napproach that achieves a similar drastic reduction in the amount of queries in\nthe hardest setup: black box decision-based attacks (only the top-1 label is\navailable). We first highlight that the most recent attacks in that setup,\nHSJA, QEBA and GeoDA all perform costly gradient surrogate estimations. SurFree\nproposes to bypass these, by instead focusing on careful trials along diverse\ndirections, guided by precise indications of geometrical properties of the\nclassifier decision boundaries. We motivate this geometric approach before\nperforming a head-to-head comparison with previous attacks with the amount of\nqueries as a first class citizen. We exhibit a faster distortion decay under\nlow query amounts (few hundreds to a thousand), while remaining competitive at\nhigher query budgets.\n", "versions": [{"version": "v1", "created": "Wed, 25 Nov 2020 15:08:19 GMT"}], "update_date": "2020-11-26", "authors_parsed": [["Maho", "Thibault", ""], ["Furon", "Teddy", ""], ["Merrer", "Erwan Le", ""]]}, {"id": "2011.12815", "submitter": "Tianlin Liu", "authors": "Tianlin Liu, Anadi Chaman, David Belius, and Ivan Dokmani\\'c", "title": "Interpreting U-Nets via Task-Driven Multiscale Dictionary Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  U-Nets have been tremendously successful in many imaging inverse problems. In\nan effort to understand the source of this success, we show that one can reduce\na U-Net to a tractable, well-understood sparsity-driven dictionary model while\nretaining its strong empirical performance. We achieve this by extracting a\ncertain multiscale convolutional dictionary from the standard U-Net. This\ndictionary imitates the structure of the U-Net in its convolution,\nscale-separation, and skip connection aspects, while doing away with the\nnonlinear parts. We show that this model can be trained in a task-driven\ndictionary learning framework and yield comparable results to standard U-Nets\non a number of relevant tasks, including CT and MRI reconstruction. These\nresults suggest that the success of the U-Net may be explained mainly by its\nmultiscale architecture and the induced sparse representation.\n", "versions": [{"version": "v1", "created": "Wed, 25 Nov 2020 15:18:00 GMT"}], "update_date": "2020-11-26", "authors_parsed": [["Liu", "Tianlin", ""], ["Chaman", "Anadi", ""], ["Belius", "David", ""], ["Dokmani\u0107", "Ivan", ""]]}, {"id": "2011.12833", "submitter": "Wonwoong Cho", "authors": "Wonwoong Cho, Inyeop Lee, David Inouye", "title": "Enhanced 3DMM Attribute Control via Synthetic Dataset Creation Pipeline", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While facial attribute manipulation of 2D images via Generative Adversarial\nNetworks (GANs) has become common in computer vision and graphics due to its\nmany practical uses, research on 3D attribute manipulation is relatively\nundeveloped. Existing 3D attribute manipulation methods are limited because the\nsame semantic changes are applied to every 3D face. The key challenge for\ndeveloping better 3D attribute control methods is the lack of paired training\ndata in which one attribute is changed while other attributes are held fixed --\ne.g., a pair of 3D faces where one is male and the other is female but all\nother attributes, such as race and expression, are the same. To overcome this\nchallenge, we design a novel pipeline for generating paired 3D faces by\nharnessing the power of GANs. On top of this pipeline, we then propose an\nenhanced non-linear 3D conditional attribute controller that increases the\nprecision and diversity of 3D attribute control compared to existing methods.\nWe demonstrate the validity of our dataset creation pipeline and the superior\nperformance of our conditional attribute controller via quantitative and\nqualitative evaluations.\n", "versions": [{"version": "v1", "created": "Wed, 25 Nov 2020 15:43:24 GMT"}, {"version": "v2", "created": "Fri, 11 Dec 2020 04:47:46 GMT"}], "update_date": "2020-12-14", "authors_parsed": [["Cho", "Wonwoong", ""], ["Lee", "Inyeop", ""], ["Inouye", "David", ""]]}, {"id": "2011.12835", "submitter": "Pierre-Marc Jodoin", "authors": "Bach Ngoc Kim, Jose Dolz, Christian Desrosiers, Pierre-Marc Jodoin", "title": "Privacy Preserving for Medical Image Analysis via Non-Linear Deformation\n  Proxy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a client-server system which allows for the analysis of\nmulti-centric medical images while preserving patient identity. In our\napproach, the client protects the patient identity by applying a pseudo-random\nnon-linear deformation to the input image. This results into a proxy image\nwhich is sent to the server for processing. The server then returns back the\ndeformed processed image which the client reverts to a canonical form. Our\nsystem has three components: 1) a flow-field generator which produces a\npseudo-random deformation function, 2) a Siamese discriminator that learns the\npatient identity from the processed image, 3) a medical image processing\nnetwork that analyzes the content of the proxy images. The system is trained\nend-to-end in an adversarial manner. By fooling the discriminator, the\nflow-field generator learns to produce a bi-directional non-linear deformation\nwhich allows to remove and recover the identity of the subject from both the\ninput image and output result. After end-to-end training, the flow-field\ngenerator is deployed on the client side and the segmentation network is\ndeployed on the server side. The proposed method is validated on the task of\nMRI brain segmentation using images from two different datasets. Results show\nthat the segmentation accuracy of our method is similar to a system trained on\nnon-encoded images, while considerably reducing the ability to recover subject\nidentity.\n", "versions": [{"version": "v1", "created": "Wed, 25 Nov 2020 15:44:12 GMT"}, {"version": "v2", "created": "Thu, 26 Nov 2020 15:51:12 GMT"}], "update_date": "2020-11-30", "authors_parsed": [["Kim", "Bach Ngoc", ""], ["Dolz", "Jose", ""], ["Desrosiers", "Christian", ""], ["Jodoin", "Pierre-Marc", ""]]}, {"id": "2011.12836", "submitter": "Yu Zeng", "authors": "Yu Zeng, Zhe Lin, Huchuan Lu, Vishal M. Patel", "title": "CR-Fill: Generative Image Inpainting with Auxiliary Contexutal\n  Reconstruction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent deep generative inpainting methods use attention layers to allow the\ngenerator to explicitly borrow feature patches from the known region to\ncomplete a missing region. Due to the lack of supervision signals for the\ncorrespondence between missing regions and known regions, it may fail to find\nproper reference features, which often leads to artifacts in the results. Also,\nit computes pair-wise similarity across the entire feature map during inference\nbringing a significant computational overhead. To address this issue, we\npropose to teach such patch-borrowing behavior to an attention-free generator\nby joint training of an auxiliary contextual reconstruction task, which\nencourages the generated output to be plausible even when reconstructed by\nsurrounding regions. The auxiliary branch can be seen as a learnable loss\nfunction, i.e. named as contextual reconstruction (CR) loss, where\nquery-reference feature similarity and reference-based reconstructor are\njointly optimized with the inpainting generator. The auxiliary branch (i.e. CR\nloss) is required only during training, and only the inpainting generator is\nrequired during the inference. Experimental results demonstrate that the\nproposed inpainting model compares favourably against the state-of-the-art in\nterms of quantitative and visual performance.\n", "versions": [{"version": "v1", "created": "Wed, 25 Nov 2020 15:45:12 GMT"}, {"version": "v2", "created": "Wed, 31 Mar 2021 11:47:51 GMT"}], "update_date": "2021-04-01", "authors_parsed": [["Zeng", "Yu", ""], ["Lin", "Zhe", ""], ["Lu", "Huchuan", ""], ["Patel", "Vishal M.", ""]]}, {"id": "2011.12844", "submitter": "Cian M. Scannell", "authors": "Rudolf L.M. van Herten and Amedeo Chiribiri and Marcel Breeuwer and\n  Mitko Veta and Cian M. Scannell", "title": "Physics-informed neural networks for myocardial perfusion MRI\n  quantification", "comments": "Submitted to Medical Image Analysis", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV physics.med-ph", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Tracer-kinetic models allow for the quantification of kinetic parameters such\nas blood flow from dynamic contrast-enhanced magnetic resonance (MR) images.\nFitting the observed data with multi-compartment exchange models is desirable,\nas they are physiologically plausible and resolve directly for blood flow and\nmicrovascular function. However, the reliability of model fitting is limited by\nthe low signal-to-noise ratio, temporal resolution, and acquisition length.\nThis may result in inaccurate parameter estimates.\n  This study introduces physics-informed neural networks (PINNs) as a means to\nperform myocardial perfusion MR quantification, which provides a versatile\nscheme for the inference of kinetic parameters. These neural networks can be\ntrained to fit the observed perfusion MR data while respecting the underlying\nphysical conservation laws described by a multi-compartment exchange model.\nHere, we provide a framework for the implementation of PINNs in myocardial\nperfusion MR.\n  The approach is validated both in silico and in vivo. In the in silico study,\nan overall reduction in mean-squared error with the ground-truth parameters was\nobserved compared to a standard non-linear least squares fitting approach. The\nin vivo study demonstrates that the method produces parameter values comparable\nto those previously found in literature, as well as providing parameter maps\nwhich match the clinical diagnosis of patients.\n", "versions": [{"version": "v1", "created": "Wed, 25 Nov 2020 16:02:52 GMT"}, {"version": "v2", "created": "Mon, 7 Dec 2020 17:11:04 GMT"}], "update_date": "2020-12-08", "authors_parsed": [["van Herten", "Rudolf L. M.", ""], ["Chiribiri", "Amedeo", ""], ["Breeuwer", "Marcel", ""], ["Veta", "Mitko", ""], ["Scannell", "Cian M.", ""]]}, {"id": "2011.12847", "submitter": "Anis Sarker", "authors": "Qianwei Cheng, AKM Mahbubur Rahman, Anis Sarker, Abu Bakar Siddik\n  Nayem, Ovi Paul, Amin Ahsan Ali, M Ashraful Amin, Ryosuke Shibasaki and\n  Moinul Zaber", "title": "Deep-learning coupled with novel classification method to classify the\n  urban environment of the developing world", "comments": "Accepted paper at 2nd International Conference on Signal Processing\n  and Machine Learning (SIGML 2021); 20 pages, 7 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Rapid globalization and the interdependence of humanity that engender\ntremendous in-flow of human migration towards the urban spaces. With advent of\nhigh definition satellite images, high resolution data, computational methods\nsuch as deep neural network, capable hardware; urban planning is seeing a\nparadigm shift. Legacy data on urban environments are now being complemented\nwith high-volume, high-frequency data. In this paper we propose a novel\nclassification method that is readily usable for machine analysis and show\napplicability of the methodology on a developing world setting. The\nstate-of-the-art is mostly dominated by classification of building structures,\nbuilding types etc. and largely represents the developed world which are\ninsufficient for developing countries such as Bangladesh where the surrounding\nis crucial for the classification. Moreover, the traditional methods propose\nsmall-scale classifications, which give limited information with poor\nscalability and are slow to compute. We categorize the urban area in terms of\ninformal and formal spaces taking the surroundings into account. 50 km x 50 km\nGoogle Earth image of Dhaka, Bangladesh was visually annotated and categorized\nby an expert. The classification is based broadly on two dimensions:\nurbanization and the architectural form of urban environment. Consequently, the\nurban space is divided into four classes: 1) highly informal; 2) moderately\ninformal; 3) moderately formal; and 4) highly formal areas. In total 16\nsub-classes were identified. For semantic segmentation, Google's DeeplabV3+\nmodel was used which increases the field of view of the filters to incorporate\nlarger context. Image encompassing 70% of the urban space was used for training\nand the remaining 30% was used for testing and validation. The model is able to\nsegment with 75% accuracy and 60% Mean IoU.\n", "versions": [{"version": "v1", "created": "Wed, 25 Nov 2020 16:08:07 GMT"}, {"version": "v2", "created": "Thu, 7 Jan 2021 18:19:44 GMT"}], "update_date": "2021-01-08", "authors_parsed": [["Cheng", "Qianwei", ""], ["Rahman", "AKM Mahbubur", ""], ["Sarker", "Anis", ""], ["Nayem", "Abu Bakar Siddik", ""], ["Paul", "Ovi", ""], ["Ali", "Amin Ahsan", ""], ["Amin", "M Ashraful", ""], ["Shibasaki", "Ryosuke", ""], ["Zaber", "Moinul", ""]]}, {"id": "2011.12850", "submitter": "Can Chen", "authors": "Can Chen, Luca Zanotti Fragonara and Antonios Tsourdos", "title": "Relation3DMOT: Exploiting Deep Affinity for 3D Multi-Object Tracking\n  from View Aggregation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Autonomous systems need to localize and track surrounding objects in 3D space\nfor safe motion planning. As a result, 3D multi-object tracking (MOT) plays a\nvital role in autonomous navigation. Most MOT methods use a\ntracking-by-detection pipeline, which includes object detection and data\nassociation processing. However, many approaches detect objects in 2D RGB\nsequences for tracking, which is lack of reliability when localizing objects in\n3D space. Furthermore, it is still challenging to learn discriminative features\nfor temporally-consistent detection in different frames, and the affinity\nmatrix is normally learned from independent object features without considering\nthe feature interaction between detected objects in the different frames. To\nsettle these problems, We firstly employ a joint feature extractor to fuse the\n2D and 3D appearance features captured from both 2D RGB images and 3D point\nclouds respectively, and then propose a novel convolutional operation, named\nRelationConv, to better exploit the correlation between each pair of objects in\nthe adjacent frames, and learn a deep affinity matrix for further data\nassociation. We finally provide extensive evaluation to reveal that our\nproposed model achieves state-of-the-art performance on KITTI tracking\nbenchmark.\n", "versions": [{"version": "v1", "created": "Wed, 25 Nov 2020 16:14:40 GMT"}], "update_date": "2020-11-26", "authors_parsed": [["Chen", "Can", ""], ["Fragonara", "Luca Zanotti", ""], ["Tsourdos", "Antonios", ""]]}, {"id": "2011.12857", "submitter": "Christian Schiffer", "authors": "Christian Schiffer, Hannah Spitzer, Kai Kiwitz, Nina Unger, Konrad\n  Wagstyl, Alan C. Evans, Stefan Harmeling, Katrin Amunts, Timo Dickscheid", "title": "Convolutional Neural Networks for cytoarchitectonic brain mapping at\n  large scale", "comments": "Preprint submitted to NeuroImage", "journal-ref": null, "doi": "10.1016/j.neuroimage.2021.118327", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human brain atlases provide spatial reference systems for data characterizing\nbrain organization at different levels, coming from different brains.\nCytoarchitecture is a basic principle of the microstructural organization of\nthe brain, as regional differences in the arrangement and composition of\nneuronal cells are indicators of changes in connectivity and function.\nAutomated scanning procedures and observer-independent methods are\nprerequisites to reliably identify cytoarchitectonic areas, and to achieve\nreproducible models of brain segregation. Time becomes a key factor when moving\nfrom the analysis of single regions of interest towards high-throughput\nscanning of large series of whole-brain sections. Here we present a new\nworkflow for mapping cytoarchitectonic areas in large series of cell-body\nstained histological sections of human postmortem brains. It is based on a Deep\nConvolutional Neural Network (CNN), which is trained on a pair of section\nimages with annotations, with a large number of un-annotated sections in\nbetween. The model learns to create all missing annotations in between with\nhigh accuracy, and faster than our previous workflow based on\nobserver-independent mapping. The new workflow does not require preceding\n3D-reconstruction of sections, and is robust against histological artefacts. It\nprocesses large data sets with sizes in the order of multiple Terabytes\nefficiently. The workflow was integrated into a web interface, to allow access\nwithout expertise in deep learning and batch computing. Applying deep neural\nnetworks for cytoarchitectonic mapping opens new perspectives to enable\nhigh-resolution models of brain areas, introducing CNNs to identify borders of\nbrain areas.\n", "versions": [{"version": "v1", "created": "Wed, 25 Nov 2020 16:25:13 GMT"}], "update_date": "2021-07-07", "authors_parsed": [["Schiffer", "Christian", ""], ["Spitzer", "Hannah", ""], ["Kiwitz", "Kai", ""], ["Unger", "Nina", ""], ["Wagstyl", "Konrad", ""], ["Evans", "Alan C.", ""], ["Harmeling", "Stefan", ""], ["Amunts", "Katrin", ""], ["Dickscheid", "Timo", ""]]}, {"id": "2011.12859", "submitter": "Elena Sizikova", "authors": "Omkar Kumbhar, Elena Sizikova, Najib Majaj, Denis G. Pelli", "title": "Anytime Prediction as a Model of Human Reaction Time", "comments": "7 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CV q-bio.NC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Neural networks today often recognize objects as well as people do, and thus\nmight serve as models of the human recognition process. However, most such\nnetworks provide their answer after a fixed computational effort, whereas human\nreaction time varies, e.g. from 0.2 to 10 s, depending on the properties of\nstimulus and task. To model the effect of difficulty on human reaction time, we\nconsidered a classification network that uses early-exit classifiers to make\nanytime predictions. Comparing human and MSDNet accuracy in classifying\nCIFAR-10 images in added Gaussian noise, we find that the network equivalent\ninput noise SD is 15 times higher than human, and that human efficiency is only\n0.6\\% that of the network. When appropriate amounts of noise are present to\nbring the two observers (human and network) into the same accuracy range, they\nshow very similar dependence on duration or FLOPS, i.e. very similar\nspeed-accuracy tradeoff. We conclude that Anytime classification (i.e. early\nexits) is a promising model for human reaction time in recognition tasks.\n", "versions": [{"version": "v1", "created": "Wed, 25 Nov 2020 16:30:52 GMT"}], "update_date": "2020-11-26", "authors_parsed": [["Kumbhar", "Omkar", ""], ["Sizikova", "Elena", ""], ["Majaj", "Najib", ""], ["Pelli", "Denis G.", ""]]}, {"id": "2011.12865", "submitter": "Christian Schiffer", "authors": "Christian Schiffer, Katrin Amunts, Stefan Harmeling, Timo Dickscheid", "title": "Contrastive Representation Learning for Whole Brain Cytoarchitectonic\n  Mapping in Histological Human Brain Sections", "comments": "Accepted to ISBI 2021", "journal-ref": null, "doi": "10.1109/ISBI48211.2021.9433986", "report-no": null, "categories": "eess.IV cs.CV q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cytoarchitectonic maps provide microstructural reference parcellations of the\nbrain, describing its organization in terms of the spatial arrangement of\nneuronal cell bodies as measured from histological tissue sections. Recent work\nprovided the first automatic segmentations of cytoarchitectonic areas in the\nvisual system using Convolutional Neural Networks. We aim to extend this\napproach to become applicable to a wider range of brain areas, envisioning a\nsolution for mapping the complete human brain. Inspired by recent success in\nimage classification, we propose a contrastive learning objective for encoding\nmicroscopic image patches into robust microstructural features, which are\nefficient for cytoarchitectonic area classification. We show that a model\npre-trained using this learning task outperforms a model trained from scratch,\nas well as a model pre-trained on a recently proposed auxiliary task. We\nperform cluster analysis in the feature space to show that the learned\nrepresentations form anatomically meaningful groups.\n", "versions": [{"version": "v1", "created": "Wed, 25 Nov 2020 16:44:23 GMT"}, {"version": "v2", "created": "Thu, 28 Jan 2021 10:17:01 GMT"}], "update_date": "2021-06-02", "authors_parsed": [["Schiffer", "Christian", ""], ["Amunts", "Katrin", ""], ["Harmeling", "Stefan", ""], ["Dickscheid", "Timo", ""]]}, {"id": "2011.12866", "submitter": "Yue Li", "authors": "Yue Li, Marc Habermann, Bernhard Thomaszewski, Stelian Coros, Thabo\n  Beeler, Christian Theobalt", "title": "Deep Physics-aware Inference of Cloth Deformation for Monocular Human\n  Performance Capture", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recent monocular human performance capture approaches have shown compelling\ndense tracking results of the full body from a single RGB camera. However,\nexisting methods either do not estimate clothing at all or model cloth\ndeformation with simple geometric priors instead of taking into account the\nunderlying physical principles. This leads to noticeable artifacts in their\nreconstructions, such as baked-in wrinkles, implausible deformations that\nseemingly defy gravity, and intersections between cloth and body. To address\nthese problems, we propose a person-specific, learning-based method that\nintegrates a finite element-based simulation layer into the training process to\nprovide for the first time physics supervision in the context of\nweakly-supervised deep monocular human performance capture. We show how\nintegrating physics into the training process improves the learned cloth\ndeformations, allows modeling clothing as a separate piece of geometry, and\nlargely reduces cloth-body intersections. Relying only on weak 2D multi-view\nsupervision during training, our approach leads to a significant improvement\nover current state-of-the-art methods and is thus a clear step towards\nrealistic monocular capture of the entire deforming surface of a clothed human.\n", "versions": [{"version": "v1", "created": "Wed, 25 Nov 2020 16:46:00 GMT"}], "update_date": "2020-11-26", "authors_parsed": [["Li", "Yue", ""], ["Habermann", "Marc", ""], ["Thomaszewski", "Bernhard", ""], ["Coros", "Stelian", ""], ["Beeler", "Thabo", ""], ["Theobalt", "Christian", ""]]}, {"id": "2011.12870", "submitter": "Yi Zhou", "authors": "Yi Zhou, Zhenhao Chen", "title": "Multimodal Learning for Hateful Memes Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Memes are used for spreading ideas through social networks. Although most\nmemes are created for humor, some memes become hateful under the combination of\npictures and text. Automatically detecting the hateful memes can help reduce\ntheir harmful social impact. Unlike the conventional multimodal tasks, where\nthe visual and textual information is semantically aligned, the challenge of\nhateful memes detection lies in its unique multimodal information. The image\nand text in memes are weakly aligned or even irrelevant, which requires the\nmodel to understand the content and perform reasoning over multiple modalities.\nIn this paper, we focus on multimodal hateful memes detection and propose a\nnovel method that incorporates the image captioning process into the memes\ndetection process. We conduct extensive experiments on multimodal meme datasets\nand illustrated the effectiveness of our approach. Our model achieves promising\nresults on the Hateful Memes Detection Challenge.\n", "versions": [{"version": "v1", "created": "Wed, 25 Nov 2020 16:49:15 GMT"}, {"version": "v2", "created": "Thu, 26 Nov 2020 03:57:32 GMT"}, {"version": "v3", "created": "Sun, 6 Dec 2020 22:16:30 GMT"}], "update_date": "2020-12-08", "authors_parsed": [["Zhou", "Yi", ""], ["Chen", "Zhenhao", ""]]}, {"id": "2011.12885", "submitter": "Xiang Li", "authors": "Xiang Li, Wenhai Wang, Xiaolin Hu, Jun Li, Jinhui Tang, Jian Yang", "title": "Generalized Focal Loss V2: Learning Reliable Localization Quality\n  Estimation for Dense Object Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Localization Quality Estimation (LQE) is crucial and popular in the recent\nadvancement of dense object detectors since it can provide accurate ranking\nscores that benefit the Non-Maximum Suppression processing and improve\ndetection performance. As a common practice, most existing methods predict LQE\nscores through vanilla convolutional features shared with object classification\nor bounding box regression. In this paper, we explore a completely novel and\ndifferent perspective to perform LQE -- based on the learned distributions of\nthe four parameters of the bounding box. The bounding box distributions are\ninspired and introduced as \"General Distribution\" in GFLV1, which describes the\nuncertainty of the predicted bounding boxes well. Such a property makes the\ndistribution statistics of a bounding box highly correlated to its real\nlocalization quality. Specifically, a bounding box distribution with a sharp\npeak usually corresponds to high localization quality, and vice versa. By\nleveraging the close correlation between distribution statistics and the real\nlocalization quality, we develop a considerably lightweight Distribution-Guided\nQuality Predictor (DGQP) for reliable LQE based on GFLV1, thus producing GFLV2.\nTo our best knowledge, it is the first attempt in object detection to use a\nhighly relevant, statistical representation to facilitate LQE. Extensive\nexperiments demonstrate the effectiveness of our method. Notably, GFLV2\n(ResNet-101) achieves 46.2 AP at 14.6 FPS, surpassing the previous\nstate-of-the-art ATSS baseline (43.6 AP at 14.6 FPS) by absolute 2.6 AP on COCO\n{\\tt test-dev}, without sacrificing the efficiency both in training and\ninference. Code will be available at https://github.com/implus/GFocalV2.\n", "versions": [{"version": "v1", "created": "Wed, 25 Nov 2020 17:06:37 GMT"}], "update_date": "2020-11-26", "authors_parsed": [["Li", "Xiang", ""], ["Wang", "Wenhai", ""], ["Hu", "Xiaolin", ""], ["Li", "Jun", ""], ["Tang", "Jinhui", ""], ["Yang", "Jian", ""]]}, {"id": "2011.12888", "submitter": "Ignacio Sarasua", "authors": "Ignacio Sarasua, Sebastian Poelsterl, Christian Wachinger", "title": "Recalibration of Neural Networks for Point Cloud Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spatial and channel re-calibration have become powerful concepts in computer\nvision. Their ability to capture long-range dependencies is especially useful\nfor those networks that extract local features, such as CNNs. While\nre-calibration has been widely studied for image analysis, it has not yet been\nused on shape representations. In this work, we introduce re-calibration\nmodules on deep neural networks for 3D point clouds. We propose a set of\nre-calibration blocks that extend Squeeze and Excitation blocks and that can be\nadded to any network for 3D point cloud analysis that builds a global\ndescriptor by hierarchically combining features from multiple local\nneighborhoods. We run two sets of experiments to validate our approach. First,\nwe demonstrate the benefit and versatility of our proposed modules by\nincorporating them into three state-of-the-art networks for 3D point cloud\nanalysis: PointNet++, DGCNN, and RSCNN. We evaluate each network on two tasks:\nobject classification on ModelNet40, and object part segmentation on ShapeNet.\nOur results show an improvement of up to 1% in accuracy for ModelNet40 compared\nto the baseline method. In the second set of experiments, we investigate the\nbenefits of re-calibration blocks on Alzheimer's Disease (AD) diagnosis. Our\nresults demonstrate that our proposed methods yield a 2% increase in accuracy\nfor diagnosing AD and a 2.3% increase in concordance index for predicting AD\nonset with time-to-event analysis. Concluding, re-calibration improves the\naccuracy of point cloud architectures, while only minimally increasing the\nnumber of parameters.\n", "versions": [{"version": "v1", "created": "Wed, 25 Nov 2020 17:14:34 GMT"}], "update_date": "2020-11-26", "authors_parsed": [["Sarasua", "Ignacio", ""], ["Poelsterl", "Sebastian", ""], ["Wachinger", "Christian", ""]]}, {"id": "2011.12893", "submitter": "Wonwoong Cho", "authors": "Myunggi Lee, Wonwoong Cho, Moonheum Kim, David Inouye, Nojun Kwak", "title": "StyleUV: Diverse and High-fidelity UV Map Generative Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reconstructing 3D human faces in the wild with the 3D Morphable Model (3DMM)\nhas become popular in recent years. While most prior work focuses on estimating\nmore robust and accurate geometry, relatively little attention has been paid to\nimproving the quality of the texture model. Meanwhile, with the advent of\nGenerative Adversarial Networks (GANs), there has been great progress in\nreconstructing realistic 2D images. Recent work demonstrates that GANs trained\nwith abundant high-quality UV maps can produce high-fidelity textures superior\nto those produced by existing methods. However, acquiring such high-quality UV\nmaps is difficult because they are expensive to acquire, requiring laborious\nprocesses to refine. In this work, we present a novel UV map generative model\nthat learns to generate diverse and realistic synthetic UV maps without\nrequiring high-quality UV maps for training. Our proposed framework can be\ntrained solely with in-the-wild images (i.e., UV maps are not required) by\nleveraging a combination of GANs and a differentiable renderer. Both\nquantitative and qualitative evaluations demonstrate that our proposed texture\nmodel produces more diverse and higher fidelity textures compared to existing\nmethods.\n", "versions": [{"version": "v1", "created": "Wed, 25 Nov 2020 17:19:44 GMT"}], "update_date": "2020-11-26", "authors_parsed": [["Lee", "Myunggi", ""], ["Cho", "Wonwoong", ""], ["Kim", "Moonheum", ""], ["Inouye", "David", ""], ["Kwak", "Nojun", ""]]}, {"id": "2011.12902", "submitter": "Ivan Evtimov", "authors": "Ivan Evtimov, Russel Howes, Brian Dolhansky, Hamed Firooz, Cristian\n  Canton Ferrer", "title": "Adversarial Evaluation of Multimodal Models under Realistic Gray Box\n  Assumption", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work examines the vulnerability of multimodal (image + text) models to\nadversarial threats similar to those discussed in previous literature on\nunimodal (image- or text-only) models. We introduce realistic assumptions of\npartial model knowledge and access, and discuss how these assumptions differ\nfrom the standard \"black-box\"/\"white-box\" dichotomy common in current\nliterature on adversarial attacks. Working under various levels of these\n\"gray-box\" assumptions, we develop new attack methodologies unique to\nmultimodal classification and evaluate them on the Hateful Memes Challenge\nclassification task. We find that attacking multiple modalities yields stronger\nattacks than unimodal attacks alone (inducing errors in up to 73% of cases),\nand that the unimodal image attacks on multimodal classifiers we explored were\nstronger than character-based text augmentation attacks (inducing errors on\naverage in 45% and 30% of cases, respectively).\n", "versions": [{"version": "v1", "created": "Wed, 25 Nov 2020 17:37:40 GMT"}, {"version": "v2", "created": "Thu, 26 Nov 2020 09:03:45 GMT"}, {"version": "v3", "created": "Wed, 9 Jun 2021 16:23:04 GMT"}], "update_date": "2021-06-10", "authors_parsed": [["Evtimov", "Ivan", ""], ["Howes", "Russel", ""], ["Dolhansky", "Brian", ""], ["Firooz", "Hamed", ""], ["Ferrer", "Cristian Canton", ""]]}, {"id": "2011.12906", "submitter": "Mohsen Jafarzadeh", "authors": "Mohsen Jafarzadeh, Akshay Raj Dhamija, Steve Cruz, Chunchun Li,\n  Touqeer Ahmad, Terrance E. Boult", "title": "Open-World Learning Without Labels", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG eess.IV eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Open-world learning is a problem where an autonomous agent detects things\nthat it does not know and learns them over time from a non-stationary and\nnever-ending stream of data; in an open-world environment, the training data\nand objective criteria are never available at once. The agent should grasp new\nknowledge from learning without forgetting acquired prior knowledge.\nResearchers proposed a few open-world learning agents for image classification\ntasks that operate in complex scenarios. However, all prior work on open-world\nlearning has all labeled data to learn the new classes from the stream of\nimages. In scenarios where autonomous agents should respond in near real-time\nor work in areas with limited communication infrastructure, human labeling of\ndata is not possible. Therefore, supervised open-world learning agents are not\nscalable solutions for such applications. Herein, we propose a new framework\nthat enables agents to learn new classes from a stream of unlabeled data in an\nunsupervised manner. Also, we study the robustness and learning speed of such\nagents with supervised and unsupervised feature representation. We also\nintroduce a new metric for open-world learning without labels. We anticipate\nour theories and method to be a starting point for developing autonomous true\nopen-world never-ending learning agents.\n", "versions": [{"version": "v1", "created": "Wed, 25 Nov 2020 17:41:03 GMT"}, {"version": "v2", "created": "Mon, 14 Dec 2020 01:39:54 GMT"}], "update_date": "2020-12-15", "authors_parsed": [["Jafarzadeh", "Mohsen", ""], ["Dhamija", "Akshay Raj", ""], ["Cruz", "Steve", ""], ["Li", "Chunchun", ""], ["Ahmad", "Touqeer", ""], ["Boult", "Terrance E.", ""]]}, {"id": "2011.12912", "submitter": "Rahul Sajnani", "authors": "Rahul Sajnani, AadilMehdi Sanchawala, Krishna Murthy Jatavallabhula,\n  Srinath Sridhar, K. Madhava Krishna", "title": "DRACO: Weakly Supervised Dense Reconstruction And Canonicalization of\n  Objects", "comments": "Preprint. For project page and code, see\n  https://aadilmehdis.github.io/DRACO-Project-Page/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present DRACO, a method for Dense Reconstruction And Canonicalization of\nObject shape from one or more RGB images. Canonical shape reconstruction,\nestimating 3D object shape in a coordinate space canonicalized for scale,\nrotation, and translation parameters, is an emerging paradigm that holds\npromise for a multitude of robotic applications. Prior approaches either rely\non painstakingly gathered dense 3D supervision, or produce only sparse\ncanonical representations, limiting real-world applicability. DRACO performs\ndense canonicalization using only weak supervision in the form of camera poses\nand semantic keypoints at train time. During inference, DRACO predicts dense\nobject-centric depth maps in a canonical coordinate-space, solely using one or\nmore RGB images of an object. Extensive experiments on canonical shape\nreconstruction and pose estimation show that DRACO is competitive or superior\nto fully-supervised methods.\n", "versions": [{"version": "v1", "created": "Wed, 25 Nov 2020 17:50:56 GMT"}], "update_date": "2020-11-26", "authors_parsed": [["Sajnani", "Rahul", ""], ["Sanchawala", "AadilMehdi", ""], ["Jatavallabhula", "Krishna Murthy", ""], ["Sridhar", "Srinath", ""], ["Krishna", "K. Madhava", ""]]}, {"id": "2011.12913", "submitter": "Yoshitomo Matsubara", "authors": "Yoshitomo Matsubara", "title": "torchdistill: A Modular, Configuration-Driven Framework for Knowledge\n  Distillation", "comments": "Accepted to the 3rd Workshop on Reproducible Research in Pattern\n  Recognition at ICPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While knowledge distillation (transfer) has been attracting attentions from\nthe research community, the recent development in the fields has heightened the\nneed for reproducible studies and highly generalized frameworks to lower\nbarriers to such high-quality, reproducible deep learning research. Several\nresearchers voluntarily published frameworks used in their knowledge\ndistillation studies to help other interested researchers reproduce their\noriginal work. Such frameworks, however, are usually neither well generalized\nnor maintained, thus researchers are still required to write a lot of code to\nrefactor/build on the frameworks for introducing new methods, models, datasets\nand designing experiments. In this paper, we present our developed open-source\nframework built on PyTorch and dedicated for knowledge distillation studies.\nThe framework is designed to enable users to design experiments by declarative\nPyYAML configuration files, and helps researchers complete the recently\nproposed ML Code Completeness Checklist. Using the developed framework, we\ndemonstrate its various efficient training strategies, and implement a variety\nof knowledge distillation methods. We also reproduce some of their original\nexperimental results on the ImageNet and COCO datasets presented at major\nmachine learning conferences such as ICLR, NeurIPS, CVPR and ECCV, including\nrecent state-of-the-art methods. All the source code, configurations, log files\nand trained model weights are publicly available at\nhttps://github.com/yoshitomo-matsubara/torchdistill .\n", "versions": [{"version": "v1", "created": "Wed, 25 Nov 2020 17:51:30 GMT"}, {"version": "v2", "created": "Wed, 27 Jan 2021 19:13:21 GMT"}], "update_date": "2021-01-29", "authors_parsed": [["Matsubara", "Yoshitomo", ""]]}, {"id": "2011.12930", "submitter": "Anand Gopalakrishnan", "authors": "Anand Gopalakrishnan, Sjoerd van Steenkiste, J\\\"urgen Schmidhuber", "title": "Unsupervised Object Keypoint Learning using Local Spatial Predictability", "comments": "Accepted to ICLR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG cs.NE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose PermaKey, a novel approach to representation learning based on\nobject keypoints. It leverages the predictability of local image regions from\nspatial neighborhoods to identify salient regions that correspond to object\nparts, which are then converted to keypoints. Unlike prior approaches, it\nutilizes predictability to discover object keypoints, an intrinsic property of\nobjects. This ensures that it does not overly bias keypoints to focus on\ncharacteristics that are not unique to objects, such as movement, shape, colour\netc. We demonstrate the efficacy of PermaKey on Atari where it learns keypoints\ncorresponding to the most salient object parts and is robust to certain visual\ndistractors. Further, on downstream RL tasks in the Atari domain we demonstrate\nhow agents equipped with our keypoints outperform those using competing\nalternatives, even on challenging environments with moving backgrounds or\ndistractor objects.\n", "versions": [{"version": "v1", "created": "Wed, 25 Nov 2020 18:27:05 GMT"}, {"version": "v2", "created": "Mon, 8 Mar 2021 15:10:29 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Gopalakrishnan", "Anand", ""], ["van Steenkiste", "Sjoerd", ""], ["Schmidhuber", "J\u00fcrgen", ""]]}, {"id": "2011.12942", "submitter": "Itamar Winter", "authors": "Itamar Winter, Daphna Weinshall", "title": "Multiclass non-Adversarial Image Synthesis, with Application to\n  Classification from Very Small Sample", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The generation of synthetic images is currently being dominated by Generative\nAdversarial Networks (GANs). Despite their outstanding success in generating\nrealistic looking images, they still suffer from major drawbacks, including an\nunstable and highly sensitive training procedure, mode-collapse and\nmode-mixture, and dependency on large training sets. In this work we present a\nnovel non-adversarial generative method - Clustered Optimization of LAtent\nspace (COLA), which overcomes some of the limitations of GANs, and outperforms\nGANs when training data is scarce. In the full data regime, our method is\ncapable of generating diverse multi-class images with no supervision,\nsurpassing previous non-adversarial methods in terms of image quality and\ndiversity. In the small-data regime, where only a small sample of labeled\nimages is available for training with no access to additional unlabeled data,\nour results surpass state-of-the-art GAN models trained on the same amount of\ndata. Finally, when utilizing our model to augment small datasets, we surpass\nthe state-of-the-art performance in small-sample classification tasks on\nchallenging datasets, including CIFAR-10, CIFAR-100, STL-10 and Tiny-ImageNet.\nA theoretical analysis supporting the essence of the method is presented.\n", "versions": [{"version": "v1", "created": "Wed, 25 Nov 2020 18:47:27 GMT"}, {"version": "v2", "created": "Tue, 1 Dec 2020 10:29:21 GMT"}], "update_date": "2020-12-02", "authors_parsed": [["Winter", "Itamar", ""], ["Weinshall", "Daphna", ""]]}, {"id": "2011.12945", "submitter": "Nimit Sohoni", "authors": "Nimit S. Sohoni, Jared A. Dunnmon, Geoffrey Angus, Albert Gu,\n  Christopher R\\'e", "title": "No Subclass Left Behind: Fine-Grained Robustness in Coarse-Grained\n  Classification Problems", "comments": "39 pages. Accepted as a conference paper at NeurIPS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In real-world classification tasks, each class often comprises multiple\nfiner-grained \"subclasses.\" As the subclass labels are frequently unavailable,\nmodels trained using only the coarser-grained class labels often exhibit highly\nvariable performance across different subclasses. This phenomenon, known as\nhidden stratification, has important consequences for models deployed in\nsafety-critical applications such as medicine. We propose GEORGE, a method to\nboth measure and mitigate hidden stratification even when subclass labels are\nunknown. We first observe that unlabeled subclasses are often separable in the\nfeature space of deep models, and exploit this fact to estimate subclass labels\nfor the training data via clustering techniques. We then use these approximate\nsubclass labels as a form of noisy supervision in a distributionally robust\noptimization objective. We theoretically characterize the performance of GEORGE\nin terms of the worst-case generalization error across any subclass. We\nempirically validate GEORGE on a mix of real-world and benchmark image\nclassification datasets, and show that our approach boosts worst-case subclass\naccuracy by up to 22 percentage points compared to standard training\ntechniques, without requiring any information about the subclasses.\n", "versions": [{"version": "v1", "created": "Wed, 25 Nov 2020 18:50:32 GMT"}], "update_date": "2020-12-01", "authors_parsed": [["Sohoni", "Nimit S.", ""], ["Dunnmon", "Jared A.", ""], ["Angus", "Geoffrey", ""], ["Gu", "Albert", ""], ["R\u00e9", "Christopher", ""]]}, {"id": "2011.12948", "submitter": "Keunhong Park", "authors": "Keunhong Park, Utkarsh Sinha, Jonathan T. Barron, Sofien Bouaziz, Dan\n  B Goldman, Steven M. Seitz, Ricardo Martin-Brualla", "title": "Nerfies: Deformable Neural Radiance Fields", "comments": "Project page with videos: https://nerfies.github.io/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the first method capable of photorealistically reconstructing\ndeformable scenes using photos/videos captured casually from mobile phones. Our\napproach augments neural radiance fields (NeRF) by optimizing an additional\ncontinuous volumetric deformation field that warps each observed point into a\ncanonical 5D NeRF. We observe that these NeRF-like deformation fields are prone\nto local minima, and propose a coarse-to-fine optimization method for\ncoordinate-based models that allows for more robust optimization. By adapting\nprinciples from geometry processing and physical simulation to NeRF-like\nmodels, we propose an elastic regularization of the deformation field that\nfurther improves robustness. We show that our method can turn casually captured\nselfie photos/videos into deformable NeRF models that allow for photorealistic\nrenderings of the subject from arbitrary viewpoints, which we dub \"nerfies.\" We\nevaluate our method by collecting time-synchronized data using a rig with two\nmobile phones, yielding train/validation images of the same pose at different\nviewpoints. We show that our method faithfully reconstructs non-rigidly\ndeforming scenes and reproduces unseen views with high fidelity.\n", "versions": [{"version": "v1", "created": "Wed, 25 Nov 2020 18:55:04 GMT"}, {"version": "v2", "created": "Thu, 26 Nov 2020 01:52:45 GMT"}, {"version": "v3", "created": "Tue, 11 May 2021 04:42:44 GMT"}, {"version": "v4", "created": "Fri, 14 May 2021 00:30:29 GMT"}], "update_date": "2021-05-17", "authors_parsed": [["Park", "Keunhong", ""], ["Sinha", "Utkarsh", ""], ["Barron", "Jonathan T.", ""], ["Bouaziz", "Sofien", ""], ["Goldman", "Dan B", ""], ["Seitz", "Steven M.", ""], ["Martin-Brualla", "Ricardo", ""]]}, {"id": "2011.12950", "submitter": "Jia-Bin Huang", "authors": "Wenqi Xian, Jia-Bin Huang, Johannes Kopf, Changil Kim", "title": "Space-time Neural Irradiance Fields for Free-Viewpoint Video", "comments": "Project website: https://video-nerf.github.io/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present a method that learns a spatiotemporal neural irradiance field for\ndynamic scenes from a single video. Our learned representation enables\nfree-viewpoint rendering of the input video. Our method builds upon recent\nadvances in implicit representations. Learning a spatiotemporal irradiance\nfield from a single video poses significant challenges because the video\ncontains only one observation of the scene at any point in time. The 3D\ngeometry of a scene can be legitimately represented in numerous ways since\nvarying geometry (motion) can be explained with varying appearance and vice\nversa. We address this ambiguity by constraining the time-varying geometry of\nour dynamic scene representation using the scene depth estimated from video\ndepth estimation methods, aggregating contents from individual frames into a\nsingle global representation. We provide an extensive quantitative evaluation\nand demonstrate compelling free-viewpoint rendering results.\n", "versions": [{"version": "v1", "created": "Wed, 25 Nov 2020 18:59:28 GMT"}, {"version": "v2", "created": "Fri, 18 Jun 2021 20:42:30 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Xian", "Wenqi", ""], ["Huang", "Jia-Bin", ""], ["Kopf", "Johannes", ""], ["Kim", "Changil", ""]]}, {"id": "2011.12953", "submitter": "Jifeng Dai", "authors": "Hao Tian, Yuntao Chen, Jifeng Dai, Zhaoxiang Zhang, Xizhou Zhu", "title": "Unsupervised Object Detection with LiDAR Clues", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the importance of unsupervised object detection, to the best of our\nknowledge, there is no previous work addressing this problem. One main issue,\nwidely known to the community, is that object boundaries derived only from 2D\nimage appearance are ambiguous and unreliable. To address this, we exploit\nLiDAR clues to aid unsupervised object detection. By exploiting the 3D scene\nstructure, the issue of localization can be considerably mitigated. We further\nidentify another major issue, seldom noticed by the community, that the\nlong-tailed and open-ended (sub-)category distribution should be accommodated.\nIn this paper, we present the first practical method for unsupervised object\ndetection with the aid of LiDAR clues. In our approach, candidate object\nsegments based on 3D point clouds are firstly generated. Then, an iterative\nsegment labeling process is conducted to assign segment labels and to train a\nsegment labeling network, which is based on features from both 2D images and 3D\npoint clouds. The labeling process is carefully designed so as to mitigate the\nissue of long-tailed and open-ended distribution. The final segment labels are\nset as pseudo annotations for object detection network training. Extensive\nexperiments on the large-scale Waymo Open dataset suggest that the derived\nunsupervised object detection method achieves reasonable accuracy compared with\nthat of strong supervision within the LiDAR visible range. Code shall be\nreleased.\n", "versions": [{"version": "v1", "created": "Wed, 25 Nov 2020 18:59:54 GMT"}, {"version": "v2", "created": "Fri, 27 Nov 2020 03:22:18 GMT"}, {"version": "v3", "created": "Mon, 19 Apr 2021 07:46:00 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Tian", "Hao", ""], ["Chen", "Yuntao", ""], ["Dai", "Jifeng", ""], ["Zhang", "Zhaoxiang", ""], ["Zhu", "Xizhou", ""]]}, {"id": "2011.12954", "submitter": "Peng Jiang", "authors": "Peng Jiang, Philip Osteen, Maggie Wigness, Srikanth Saripalli", "title": "RELLIS-3D Dataset: Data, Benchmarks and Analysis", "comments": "7 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Semantic scene understanding is crucial for robust and safe autonomous\nnavigation, particularly so in off-road environments. Recent deep learning\nadvances for 3D semantic segmentation rely heavily on large sets of training\ndata, however existing autonomy datasets either represent urban environments or\nlack multimodal off-road data. We fill this gap with RELLIS-3D, a multimodal\ndataset collected in an off-road environment, which contains annotations for\n13,556 LiDAR scans and 6,235 images. The data was collected on the Rellis\nCampus of Texas A\\&M University and presents challenges to existing algorithms\nrelated to class imbalance and environmental topography. Additionally, we\nevaluate the current state-of-the-art deep learning semantic segmentation\nmodels on this dataset. Experimental results show that RELLIS-3D presents\nchallenges for algorithms designed for segmentation in urban environments. This\nnovel dataset provides the resources needed by researchers to continue to\ndevelop more advanced algorithms and investigate new research directions to\nenhance autonomous navigation in off-road environments. RELLIS-3D is available\nat https://github.com/unmannedlab/RELLIS-3D\n", "versions": [{"version": "v1", "created": "Tue, 17 Nov 2020 18:28:01 GMT"}, {"version": "v2", "created": "Thu, 3 Dec 2020 02:45:03 GMT"}, {"version": "v3", "created": "Mon, 26 Apr 2021 19:44:12 GMT"}], "update_date": "2021-04-28", "authors_parsed": [["Jiang", "Peng", ""], ["Osteen", "Philip", ""], ["Wigness", "Maggie", ""], ["Saripalli", "Srikanth", ""]]}, {"id": "2011.12957", "submitter": "Ahmed Mohammed", "authors": "A. Mohammed, I. Farup, M. Pedersen, S. Yildirim, and {\\O} Hovde", "title": "PS-DeVCEM: Pathology-sensitive deep learning model for video capsule\n  endoscopy based on weakly labeled data", "comments": null, "journal-ref": "Computer Vision and Image Understanding 201 (2020): 103062", "doi": "10.1016/j.cviu.2020.103062", "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose a novel pathology-sensitive deep learning model (PS-DeVCEM) for\nframe-level anomaly detection and multi-label classification of different colon\ndiseases in video capsule endoscopy (VCE) data. Our proposed model is capable\nof coping with the key challenge of colon apparent heterogeneity caused by\nseveral types of diseases. Our model is driven by attention-based deep multiple\ninstance learning and is trained end-to-end on weakly labeled data using video\nlabels instead of detailed frame-by-frame annotation. The spatial and temporal\nfeatures are obtained through ResNet50 and residual Long short-term memory\n(residual LSTM) blocks, respectively. Additionally, the learned temporal\nattention module provides the importance of each frame to the final label\nprediction. Moreover, we developed a self-supervision method to maximize the\ndistance between classes of pathologies. We demonstrate through qualitative and\nquantitative experiments that our proposed weakly supervised learning model\ngives superior precision and F1-score reaching, 61.6% and 55.1%, as compared to\nthree state-of-the-art video analysis methods respectively. We also show our\nmodel's ability to temporally localize frames with pathologies, without frame\nannotation information during training. Furthermore, we collected and annotated\nthe first and largest VCE dataset with only video labels. The dataset contains\n455 short video segments with 28,304 frames and 14 classes of colorectal\ndiseases and artifacts. Dataset and code supporting this publication will be\nmade available on our home page.\n", "versions": [{"version": "v1", "created": "Sun, 22 Nov 2020 15:33:37 GMT"}], "update_date": "2020-11-30", "authors_parsed": [["Mohammed", "A.", ""], ["Farup", "I.", ""], ["Pedersen", "M.", ""], ["Yildirim", "S.", ""], ["Hovde", "\u00d8", ""]]}, {"id": "2011.12960", "submitter": "Lars Ankile", "authors": "Lars Lien Ankile, Morgan Feet Heggland, Kjartan Krange", "title": "Deep Convolutional Neural Networks: A survey of the foundations,\n  selected improvements, and some current applications", "comments": "10 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Within the world of machine learning there exists a wide range of different\nmethods with respective advantages and applications. This paper seeks to\npresent and discuss one such method, namely Convolutional Neural Networks\n(CNNs). CNNs are deep neural networks that use a special linear operation\ncalled convolution. This operation represents a key and distinctive element of\nCNNs, and will therefore be the focus of this method paper. The discussion\nstarts with the theoretical foundations that underlie convolutions and CNNs.\nThen, the discussion proceeds to discuss some improvements and augmentations\nthat can be made to adapt the method to estimate a wider set of function\nclasses. The paper mainly investigates two ways of improving the method: by\nusing locally connected layers, which can make the network less invariant to\ntranslation, and tiled convolution, which allows for the learning of more\ncomplex invariances than standard convolution. Furthermore, the use of the Fast\nFourier Transform can improve the computational efficiency of convolution.\nSubsequently, this paper discusses two applications of convolution that have\nproven to be very effective in practice. First, the YOLO architecture is a\nstate of the art neural network for image object classification, which\naccurately predicts bounding boxes around objects in images. Second, tumor\ndetection in mammography may be performed using CNNs, accomplishing 7.2% higher\nspecificity than actual doctors with only .3% less sensitivity. Finally, the\ninvention of technology that outperforms humans in different fields also raises\ncertain ethical and regulatory questions that are briefly discussed.\n", "versions": [{"version": "v1", "created": "Wed, 25 Nov 2020 19:03:23 GMT"}], "update_date": "2020-11-30", "authors_parsed": [["Ankile", "Lars Lien", ""], ["Heggland", "Morgan Feet", ""], ["Krange", "Kjartan", ""]]}, {"id": "2011.12982", "submitter": "Hugo Touvron", "authors": "Hugo Touvron, Alexandre Sablayrolles, Matthijs Douze, Matthieu Cord,\n  Herv\\'e J\\'egou", "title": "Grafit: Learning fine-grained image representations with coarse labels", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper tackles the problem of learning a finer representation than the\none provided by training labels. This enables fine-grained category retrieval\nof images in a collection annotated with coarse labels only.\n  Our network is learned with a nearest-neighbor classifier objective, and an\ninstance loss inspired by self-supervised learning. By jointly leveraging the\ncoarse labels and the underlying fine-grained latent space, it significantly\nimproves the accuracy of category-level retrieval methods.\n  Our strategy outperforms all competing methods for retrieving or classifying\nimages at a finer granularity than that available at train time. It also\nimproves the accuracy for transfer learning tasks to fine-grained datasets,\nthereby establishing the new state of the art on five public benchmarks, like\niNaturalist-2018.\n", "versions": [{"version": "v1", "created": "Wed, 25 Nov 2020 19:06:26 GMT"}], "update_date": "2020-11-30", "authors_parsed": [["Touvron", "Hugo", ""], ["Sablayrolles", "Alexandre", ""], ["Douze", "Matthijs", ""], ["Cord", "Matthieu", ""], ["J\u00e9gou", "Herv\u00e9", ""]]}, {"id": "2011.12986", "submitter": "Katrin Renz", "authors": "Katrin Renz, Nicolaj C. Stache, Samuel Albanie, G\\\"ul Varol", "title": "Sign language segmentation with temporal convolutional networks", "comments": "Appears in: 2021 IEEE International Conference on Acoustics, Speech\n  and Signal Processing (ICASSP'21). 5 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The objective of this work is to determine the location of temporal\nboundaries between signs in continuous sign language videos. Our approach\nemploys 3D convolutional neural network representations with iterative temporal\nsegment refinement to resolve ambiguities between sign boundary cues. We\ndemonstrate the effectiveness of our approach on the BSLCORPUS, PHOENIX14 and\nBSL-1K datasets, showing considerable improvement over the prior state of the\nart and the ability to generalise to new signers, languages and domains.\n", "versions": [{"version": "v1", "created": "Wed, 25 Nov 2020 19:11:48 GMT"}, {"version": "v2", "created": "Fri, 12 Feb 2021 17:16:41 GMT"}], "update_date": "2021-02-15", "authors_parsed": [["Renz", "Katrin", ""], ["Stache", "Nicolaj C.", ""], ["Albanie", "Samuel", ""], ["Varol", "G\u00fcl", ""]]}, {"id": "2011.12999", "submitter": "Jo\\~ao Pedro Moreira Ferreira", "authors": "Jo\\~ao P. Ferreira, Thiago M. Coutinho, Thiago L. Gomes, Jos\\'e F.\n  Neto, Rafael Azevedo, Renato Martins, Erickson R. Nascimento", "title": "Learning to dance: A graph convolutional adversarial network to generate\n  realistic dance motions from audio", "comments": "Accepted at the Elsevier Computers & Graphics (C&G) 2020", "journal-ref": null, "doi": "10.1016/j.cag.2020.09.009", "report-no": null, "categories": "cs.GR cs.CV cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Synthesizing human motion through learning techniques is becoming an\nincreasingly popular approach to alleviating the requirement of new data\ncapture to produce animations. Learning to move naturally from music, i.e., to\ndance, is one of the more complex motions humans often perform effortlessly.\nEach dance movement is unique, yet such movements maintain the core\ncharacteristics of the dance style. Most approaches addressing this problem\nwith classical convolutional and recursive neural models undergo training and\nvariability issues due to the non-Euclidean geometry of the motion manifold\nstructure.In this paper, we design a novel method based on graph convolutional\nnetworks to tackle the problem of automatic dance generation from audio\ninformation. Our method uses an adversarial learning scheme conditioned on the\ninput music audios to create natural motions preserving the key movements of\ndifferent music styles. We evaluate our method with three quantitative metrics\nof generative methods and a user study. The results suggest that the proposed\nGCN model outperforms the state-of-the-art dance generation method conditioned\non music in different experiments. Moreover, our graph-convolutional approach\nis simpler, easier to be trained, and capable of generating more realistic\nmotion styles regarding qualitative and different quantitative metrics. It also\npresented a visual movement perceptual quality comparable to real motion data.\n", "versions": [{"version": "v1", "created": "Wed, 25 Nov 2020 19:53:53 GMT"}, {"version": "v2", "created": "Mon, 30 Nov 2020 17:59:15 GMT"}], "update_date": "2020-12-01", "authors_parsed": [["Ferreira", "Jo\u00e3o P.", ""], ["Coutinho", "Thiago M.", ""], ["Gomes", "Thiago L.", ""], ["Neto", "Jos\u00e9 F.", ""], ["Azevedo", "Rafael", ""], ["Martins", "Renato", ""], ["Nascimento", "Erickson R.", ""]]}, {"id": "2011.13000", "submitter": "Reena Elangovan", "authors": "Reena Elangovan, Shubham Jain, Anand Raghunathan", "title": "Ax-BxP: Approximate Blocked Computation for Precision-Reconfigurable\n  Deep Neural Network Acceleration", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Precision scaling has emerged as a popular technique to optimize the compute\nand storage requirements of Deep Neural Networks (DNNs). Efforts toward\ncreating ultra-low-precision (sub-8-bit) DNNs suggest that the minimum\nprecision required to achieve a given network-level accuracy varies\nconsiderably across networks, and even across layers within a network,\nrequiring support for variable precision in DNN hardware. Previous proposals\nsuch as bit-serial hardware incur high overheads, significantly diminishing the\nbenefits of lower precision. To efficiently support precision\nre-configurability in DNN accelerators, we introduce an approximate computing\nmethod wherein DNN computations are performed block-wise (a block is a group of\nbits) and re-configurability is supported at the granularity of blocks. Results\nof block-wise computations are composed in an approximate manner to enable\nefficient re-configurability. We design a DNN accelerator that embodies\napproximate blocked computation and propose a method to determine a suitable\napproximation configuration for a given DNN. By varying the approximation\nconfigurations across DNNs, we achieve 1.17x-1.73x and 1.02x-2.04x improvement\nin system energy and performance respectively, over an 8-bit fixed-point (FxP8)\nbaseline, with negligible loss in classification accuracy. Further, by varying\nthe approximation configurations across layers and data-structures within DNNs,\nwe achieve 1.25x-2.42x and 1.07x-2.95x improvement in system energy and\nperformance respectively, with negligible accuracy loss.\n", "versions": [{"version": "v1", "created": "Wed, 25 Nov 2020 20:00:38 GMT"}, {"version": "v2", "created": "Fri, 23 Jul 2021 15:35:56 GMT"}], "update_date": "2021-07-26", "authors_parsed": [["Elangovan", "Reena", ""], ["Jain", "Shubham", ""], ["Raghunathan", "Anand", ""]]}, {"id": "2011.13005", "submitter": "Shengyu Huang", "authors": "Shengyu Huang, Zan Gojcic, Mikhail Usvyatsov, Andreas Wieser, Konrad\n  Schindler", "title": "PREDATOR: Registration of 3D Point Clouds with Low Overlap", "comments": "CVPR 2021 (Oral) - Camera ready. The first two authors contributed\n  equally", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We introduce PREDATOR, a model for pairwise point-cloud registration with\ndeep attention to the overlap region. Different from previous work, our model\nis specifically designed to handle (also) point-cloud pairs with low overlap.\nIts key novelty is an overlap-attention block for early information exchange\nbetween the latent encodings of the two point clouds. In this way the\nsubsequent decoding of the latent representations into per-point features is\nconditioned on the respective other point cloud, and thus can predict which\npoints are not only salient, but also lie in the overlap region between the two\npoint clouds. The ability to focus on points that are relevant for matching\ngreatly improves performance: PREDATOR raises the rate of successful\nregistrations by more than 20% in the low-overlap scenario, and also sets a new\nstate of the art for the 3DMatch benchmark with 89% registration recall.\n", "versions": [{"version": "v1", "created": "Wed, 25 Nov 2020 20:25:03 GMT"}, {"version": "v2", "created": "Fri, 19 Mar 2021 13:48:09 GMT"}], "update_date": "2021-03-22", "authors_parsed": [["Huang", "Shengyu", ""], ["Gojcic", "Zan", ""], ["Usvyatsov", "Mikhail", ""], ["Wieser", "Andreas", ""], ["Schindler", "Konrad", ""]]}, {"id": "2011.13011", "submitter": "Tianyu Han", "authors": "Tianyu Han, Sven Nebelung, Federico Pedersoli, Markus Zimmermann,\n  Maximilian Schulze-Hagen, Michael Ho, Christoph Haarburger, Fabian Kiessling,\n  Christiane Kuhl, Volkmar Schulz, Daniel Truhn", "title": "Advancing diagnostic performance and clinical usability of neural\n  networks via adversarial training and dual batch normalization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Unmasking the decision-making process of machine learning models is essential\nfor implementing diagnostic support systems in clinical practice. Here, we\ndemonstrate that adversarially trained models can significantly enhance the\nusability of pathology detection as compared to their standard counterparts. We\nlet six experienced radiologists rate the interpretability of saliency maps in\ndatasets of X-rays, computed tomography, and magnetic resonance imaging scans.\nSignificant improvements were found for our adversarial models, which could be\nfurther improved by the application of dual batch normalization. Contrary to\nprevious research on adversarially trained models, we found that the accuracy\nof such models was equal to standard models when sufficiently large datasets\nand dual batch norm training were used. To ensure transferability, we\nadditionally validated our results on an external test set of 22,433 X-rays.\nThese findings elucidate that different paths for adversarial and real images\nare needed during training to achieve state of the art results with superior\nclinical interpretability.\n", "versions": [{"version": "v1", "created": "Wed, 25 Nov 2020 20:41:01 GMT"}], "update_date": "2020-11-30", "authors_parsed": [["Han", "Tianyu", ""], ["Nebelung", "Sven", ""], ["Pedersoli", "Federico", ""], ["Zimmermann", "Markus", ""], ["Schulze-Hagen", "Maximilian", ""], ["Ho", "Michael", ""], ["Haarburger", "Christoph", ""], ["Kiessling", "Fabian", ""], ["Kuhl", "Christiane", ""], ["Schulz", "Volkmar", ""], ["Truhn", "Daniel", ""]]}, {"id": "2011.13026", "submitter": "Davis Wertheimer", "authors": "Davis Wertheimer, Omid Poursaeed and Bharath Hariharan", "title": "Augmentation-Interpolative AutoEncoders for Unsupervised Few-Shot Image\n  Generation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We aim to build image generation models that generalize to new domains from\nfew examples. To this end, we first investigate the generalization properties\nof classic image generators, and discover that autoencoders generalize\nextremely well to new domains, even when trained on highly constrained data. We\nleverage this insight to produce a robust, unsupervised few-shot image\ngeneration algorithm, and introduce a novel training procedure based on\nrecovering an image from data augmentations. Our Augmentation-Interpolative\nAutoEncoders synthesize realistic images of novel objects from only a few\nreference images, and outperform both prior interpolative models and supervised\nfew-shot image generators. Our procedure is simple and lightweight, generalizes\nbroadly, and requires no category labels or other supervision during training.\n", "versions": [{"version": "v1", "created": "Wed, 25 Nov 2020 21:18:55 GMT"}], "update_date": "2020-11-30", "authors_parsed": [["Wertheimer", "Davis", ""], ["Poursaeed", "Omid", ""], ["Hariharan", "Bharath", ""]]}, {"id": "2011.13045", "submitter": "Homer Walke", "authors": "Homer Walke, R. Kenny Jones, Daniel Ritchie", "title": "Learning to Infer Shape Programs Using Latent Execution Self Training", "comments": "15 pages, 16 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inferring programs which generate 2D and 3D shapes is important for reverse\nengineering, enabling shape editing, and more. Supervised learning is hard to\napply to this problem, as paired (program, shape) data rarely exists. Recent\napproaches use supervised pre-training with randomly-generated programs and\nthen refine using self-supervised learning. But self-supervised learning either\nrequires that the program execution process be differentiable or relies on\nreinforcement learning, which is unstable and slow to converge. In this paper,\nwe present a new approach for learning to infer shape programs, which we call\nlatent execution self training (LEST). As with recent prior work, LEST starts\nby training on randomly-generated (program, shape) pairs. As its name implies,\nit is based on the idea of self-training: running a model on unlabeled input\nshapes, treating the predicted programs as ground truth latent labels, and\ntraining again. Self-training is known to be susceptible to local minima. LEST\ncircumvents this problem by leveraging the fact that predicted latent programs\nare executable: for a given shape $\\mathbf{x}^* \\in S^*$ and its predicted\nprogram $\\mathbf{z} \\in P$, we execute $\\mathbf{z}$ to obtain a shape\n$\\mathbf{x} \\in S$ and train on $(\\mathbf{z} \\in P, \\mathbf{x} \\in S)$ pairs,\nrather than $(\\mathbf{z} \\in P, \\mathbf{x}^* \\in S^*)$ pairs. Experiments show\nthat the distribution of executed shapes $S$ converges toward the distribution\nof real shapes $S^*$. We establish connections between LEST and algorithms for\nlearning generative models, including variational Bayes, wake sleep, and\nexpectation maximization. For constructive solid geometry and assembly-based\nmodeling, LEST's inferred programs converge to high reconstruction accuracy\nsignificantly faster than those of reinforcement learning.\n", "versions": [{"version": "v1", "created": "Wed, 25 Nov 2020 22:10:32 GMT"}], "update_date": "2020-11-30", "authors_parsed": [["Walke", "Homer", ""], ["Jones", "R. Kenny", ""], ["Ritchie", "Daniel", ""]]}, {"id": "2011.13046", "submitter": "Yutong Bai", "authors": "Yutong Bai, Haoqi Fan, Ishan Misra, Ganesh Venkatesh, Yongyi Lu, Yuyin\n  Zhou, Qihang Yu, Vikas Chandra, Alan Yuille", "title": "Can Temporal Information Help with Contrastive Self-Supervised Learning?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Leveraging temporal information has been regarded as essential for developing\nvideo understanding models. However, how to properly incorporate temporal\ninformation into the recent successful instance discrimination based\ncontrastive self-supervised learning (CSL) framework remains unclear. As an\nintuitive solution, we find that directly applying temporal augmentations does\nnot help, or even impair video CSL in general. This counter-intuitive\nobservation motivates us to re-design existing video CSL frameworks, for better\nintegration of temporal knowledge.\n  To this end, we present Temporal-aware Contrastive self-supervised\nlearningTaCo, as a general paradigm to enhance video CSL. Specifically, TaCo\nselects a set of temporal transformations not only as strong data augmentation\nbut also to constitute extra self-supervision for video understanding. By\njointly contrasting instances with enriched temporal transformations and\nlearning these transformations as self-supervised signals, TaCo can\nsignificantly enhance unsupervised video representation learning. For instance,\nTaCo demonstrates consistent improvement in downstream classification tasks\nover a list of backbones and CSL approaches. Our best model achieves 85.1%\n(UCF-101) and 51.6% (HMDB-51) top-1 accuracy, which is a 3% and 2.4% relative\nimprovement over the previous state-of-the-art.\n", "versions": [{"version": "v1", "created": "Wed, 25 Nov 2020 22:14:08 GMT"}], "update_date": "2020-11-30", "authors_parsed": [["Bai", "Yutong", ""], ["Fan", "Haoqi", ""], ["Misra", "Ishan", ""], ["Venkatesh", "Ganesh", ""], ["Lu", "Yongyi", ""], ["Zhou", "Yuyin", ""], ["Yu", "Qihang", ""], ["Chandra", "Vikas", ""], ["Yuille", "Alan", ""]]}, {"id": "2011.13055", "submitter": "Sameera Ramasinghe Mr.", "authors": "Sameera Ramasinghe, Moshiur Farazi, Salman Khan, Nick Barnes, Stephen\n  Gould", "title": "Rethinking conditional GAN training: An approach using geometrically\n  structured latent manifolds", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conditional GANs (cGAN), in their rudimentary form, suffer from critical\ndrawbacks such as the lack of diversity in generated outputs and distortion\nbetween the latent and output manifolds. Although efforts have been made to\nimprove results, they can suffer from unpleasant side-effects such as the\ntopology mismatch between latent and output spaces. In contrast, we tackle this\nproblem from a geometrical perspective and propose a novel training mechanism\nthat increases both the diversity and the visual quality of a vanilla cGAN, by\nsystematically encouraging a bi-lipschitz mapping between the latent and the\noutput manifolds. We validate the efficacy of our solution on a baseline cGAN\n(i.e., Pix2Pix) which lacks diversity, and show that by only modifying its\ntraining mechanism (i.e., with our proposed Pix2Pix-Geo), one can achieve more\ndiverse and realistic outputs on a broad set of image-to-image translation\ntasks. Codes are available at https://github.com/samgregoost/Rethinking-CGANs.\n", "versions": [{"version": "v1", "created": "Wed, 25 Nov 2020 22:54:11 GMT"}, {"version": "v2", "created": "Mon, 30 Nov 2020 06:00:21 GMT"}, {"version": "v3", "created": "Wed, 2 Jun 2021 11:50:46 GMT"}], "update_date": "2021-06-03", "authors_parsed": [["Ramasinghe", "Sameera", ""], ["Farazi", "Moshiur", ""], ["Khan", "Salman", ""], ["Barnes", "Nick", ""], ["Gould", "Stephen", ""]]}, {"id": "2011.13066", "submitter": "Li Liu", "authors": "Yixiong Chen, Chunhui Zhang, Li Liu, Cheng Feng, Changfeng Dong,\n  Yongfang Luo, Xiang Wan", "title": "Effective Sample Pair Generation for Ultrasound Video Contrastive\n  Representation Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Most deep neural networks (DNNs) based ultrasound (US) medical image analysis\nmodels use pretrained backbones (e.g., ImageNet) for better model\ngeneralization. However, the domain gap between natural and medical images\ncauses an inevitable performance bottleneck when applying to US image analysis.\nOur idea is to pretrain DNNs on US images directly to avoid this bottleneck.\nDue to the lack of annotated large-scale datasets of US images, we first\nconstruct a new large-scale US video-based image dataset named US-4, containing\nover 23,000 high-resolution images from four US video sub-datasets, where two\nsub-datasets are newly collected by our local experienced doctors. To make full\nuse of this dataset, we then innovatively propose an US semi-supervised\ncontrastive learning (USCL) method to effectively learn feature representations\nof US images, with a new sample pair generation (SPG) scheme to tackle the\nproblem that US images extracted from videos have high similarities. Moreover,\nthe USCL treats contrastive loss as a consistent regularization, which boosts\nthe performance of pretrained backbones by combining the supervised loss in a\nmutually reinforcing way. Extensive experiments on down-stream tasks'\nfine-tuning show the superiority of our approach against ImageNet pretraining\nand pretraining using previous state-of-the-art semi-supervised learning\napproaches. In particular, our pretrained backbone gets fine-tuning accuracy of\nover 94%, which is 9% higher than 85% of the ImageNet pretrained model on the\nwidely used POCUS dataset. The constructed US-4 dataset and source codes of\nthis work will be made public.\n", "versions": [{"version": "v1", "created": "Wed, 25 Nov 2020 23:44:38 GMT"}], "update_date": "2020-11-30", "authors_parsed": [["Chen", "Yixiong", ""], ["Zhang", "Chunhui", ""], ["Liu", "Li", ""], ["Feng", "Cheng", ""], ["Dong", "Changfeng", ""], ["Luo", "Yongfang", ""], ["Wan", "Xiang", ""]]}, {"id": "2011.13074", "submitter": "Peng Zhou", "authors": "Peng Zhou, Lingxi Xie, Bingbing Ni, Cong Geng, Qi Tian", "title": "Omni-GAN: On the Secrets of cGANs and Beyond", "comments": "Introducing Omni-INR-GAN, which can extrapolate low-resolution images\n  to arbitrary resolution", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The conditional generative adversarial network (cGAN) is a powerful tool of\ngenerating high-quality images, but existing approaches mostly suffer\nunsatisfying performance or the risk of mode collapse. This paper presents\nOmni-GAN, a variant of cGAN that reveals the devil in designing a proper\ndiscriminator for training the model. The key is to ensure that the\ndiscriminator receives strong supervision to perceive the concepts and moderate\nregularization to avoid collapse. Omni-GAN is easily implemented and freely\nintegrated with off-the-shelf encoding methods (e.g., implicit neural\nrepresentation, INR). Experiments validate the superior performance of Omni-GAN\nand Omni-INR-GAN in a wide range of image generation and restoration tasks. In\nparticular, Omni-INR-GAN sets new records on the ImageNet dataset with\nimpressive Inception scores of 262.85 and 343.22 for the image sizes of 128 and\n256, respectively, surpassing the previous records by 100+ points. Moreover,\nleveraging the generator prior, Omni-INR-GAN can extrapolate low-resolution\nimages to arbitrary resolution, even up to x60+ higher resolution. Code is\navailable.\n", "versions": [{"version": "v1", "created": "Thu, 26 Nov 2020 00:30:20 GMT"}, {"version": "v2", "created": "Fri, 19 Feb 2021 05:33:05 GMT"}, {"version": "v3", "created": "Mon, 29 Mar 2021 03:00:44 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Zhou", "Peng", ""], ["Xie", "Lingxi", ""], ["Ni", "Bingbing", ""], ["Geng", "Cong", ""], ["Tian", "Qi", ""]]}, {"id": "2011.13075", "submitter": "Haoxin Wang", "authors": "Haoxin Wang, BaekGyu Kim, Jiang Xie and Zhu Han", "title": "Energy Drain of the Object Detection Processing Pipeline for Mobile\n  Devices: Analysis and Implications", "comments": "This is a personal copy of the authors. Not for redistribution. The\n  final version of this paper was accepted by IEEE Transactions on Green\n  Communications and Networking", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF cs.CV cs.MM cs.NI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Applying deep learning to object detection provides the capability to\naccurately detect and classify complex objects in the real world. However,\ncurrently, few mobile applications use deep learning because such technology is\ncomputation-intensive and energy-consuming. This paper, to the best of our\nknowledge, presents the first detailed experimental study of a mobile augmented\nreality (AR) client's energy consumption and the detection latency of executing\nConvolutional Neural Networks (CNN) based object detection, either locally on\nthe smartphone or remotely on an edge server. In order to accurately measure\nthe energy consumption on the smartphone and obtain the breakdown of energy\nconsumed by each phase of the object detection processing pipeline, we propose\na new measurement strategy. Our detailed measurements refine the energy\nanalysis of mobile AR clients and reveal several interesting perspectives\nregarding the energy consumption of executing CNN-based object detection.\nFurthermore, several insights and research opportunities are proposed based on\nour experimental results. These findings from our experimental study will guide\nthe design of energy-efficient processing pipeline of CNN-based object\ndetection.\n", "versions": [{"version": "v1", "created": "Thu, 26 Nov 2020 00:32:07 GMT"}], "update_date": "2020-11-30", "authors_parsed": [["Wang", "Haoxin", ""], ["Kim", "BaekGyu", ""], ["Xie", "Jiang", ""], ["Han", "Zhu", ""]]}, {"id": "2011.13076", "submitter": "Or Litany", "authors": "Or Litany, Emanuele Rodol\\`a, Alex Bronstein, Michael Bronstein,\n  Daniel Cremers", "title": "Non-Rigid Puzzles", "comments": null, "journal-ref": "Computer Graphics Forum, Volume 35, Issue 5, August 2016", "doi": "10.1111/cgf.12970", "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Shape correspondence is a fundamental problem in computer graphics and\nvision, with applications in various problems including animation, texture\nmapping, robotic vision, medical imaging, archaeology and many more. In\nsettings where the shapes are allowed to undergo non-rigid deformations and\nonly partial views are available, the problem becomes very challenging. To this\nend, we present a non-rigid multi-part shape matching algorithm. We assume to\nbe given a reference shape and its multiple parts undergoing a non-rigid\ndeformation. Each of these query parts can be additionally contaminated by\nclutter, may overlap with other parts, and there might be missing parts or\nredundant ones. Our method simultaneously solves for the segmentation of the\nreference model, and for a dense correspondence to (subsets of) the parts.\nExperimental results on synthetic as well as real scans demonstrate the\neffectiveness of our method in dealing with this challenging matching scenario.\n", "versions": [{"version": "v1", "created": "Thu, 26 Nov 2020 00:32:30 GMT"}], "update_date": "2020-11-30", "authors_parsed": [["Litany", "Or", ""], ["Rodol\u00e0", "Emanuele", ""], ["Bronstein", "Alex", ""], ["Bronstein", "Michael", ""], ["Cremers", "Daniel", ""]]}, {"id": "2011.13080", "submitter": "Bolin Pan", "authors": "Bolin Pan, Simon R. Arridge, Felix Lucka, Ben T. Cox, Nam Huynh, Paul\n  C. Beard, Edward Z. Zhang, Marta M. Betcke", "title": "Photoacoustic Reconstruction Using Sparsity in Curvelet Frame", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.NA math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We compare two approaches to photoacoustic image reconstruction from\ncompressed/subsampled photoacoustic data based on assumption of sparsity in the\nCurvelet frame: DR, a two step approach based on the recovery of the complete\nvolume of the photoacoustic data from the subsampled data followed by the\nacoustic inversion, and p0R, a one step approach where the photoacoustic image\n(the initial pressure, p0) is directly recovered from the subsampled data. For\nrepresentation of the photoacoustic data, we propose a modification of the\nCurvelet transform corresponding to the restriction to the range of the\nphotoacoustic forward operator. Both recovery problems are formulated in a\nvariational framework. As the Curvelet frame is heavily overdetermined, we use\nreweighted l1 norm penalties to enhance the sparsity of the solution. The data\nreconstruction problem DR is a standard compressed sensing recovery problem,\nwhich we solve using an ADMM-type algorithm, SALSA. Subsequently, the initial\npressure is recovered using time reversal as implemented in the k-Wave Toolbox.\nThe p0 reconstruction problem, p0R, aims to recover the photoacoustic image\ndirectly via FISTA, or ADMM when in addition including a non-negativity\nconstraint. We compare and discuss the relative merits of the two approaches\nand illustrate them on 2D simulated and 3D real data.\n", "versions": [{"version": "v1", "created": "Thu, 26 Nov 2020 00:51:39 GMT"}], "update_date": "2020-11-30", "authors_parsed": [["Pan", "Bolin", ""], ["Arridge", "Simon R.", ""], ["Lucka", "Felix", ""], ["Cox", "Ben T.", ""], ["Huynh", "Nam", ""], ["Beard", "Paul C.", ""], ["Zhang", "Edward Z.", ""], ["Betcke", "Marta M.", ""]]}, {"id": "2011.13084", "submitter": "Zhengqi Li", "authors": "Zhengqi Li, Simon Niklaus, Noah Snavely, Oliver Wang", "title": "Neural Scene Flow Fields for Space-Time View Synthesis of Dynamic Scenes", "comments": "CVPR 2021, Project Website: http://www.cs.cornell.edu/~zl548/NSFF/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a method to perform novel view and time synthesis of dynamic\nscenes, requiring only a monocular video with known camera poses as input. To\ndo this, we introduce Neural Scene Flow Fields, a new representation that\nmodels the dynamic scene as a time-variant continuous function of appearance,\ngeometry, and 3D scene motion. Our representation is optimized through a neural\nnetwork to fit the observed input views. We show that our representation can be\nused for complex dynamic scenes, including thin structures, view-dependent\neffects, and natural degrees of motion. We conduct a number of experiments that\ndemonstrate our approach significantly outperforms recent monocular view\nsynthesis methods, and show qualitative results of space-time view synthesis on\na variety of real-world videos.\n", "versions": [{"version": "v1", "created": "Thu, 26 Nov 2020 01:23:44 GMT"}, {"version": "v2", "created": "Fri, 2 Apr 2021 02:48:29 GMT"}, {"version": "v3", "created": "Wed, 21 Apr 2021 02:11:44 GMT"}], "update_date": "2021-04-22", "authors_parsed": [["Li", "Zhengqi", ""], ["Niklaus", "Simon", ""], ["Snavely", "Noah", ""], ["Wang", "Oliver", ""]]}, {"id": "2011.13096", "submitter": "Sibo Qiao", "authors": "Sibo Qiao, Shanchen Pang, Gang Luo, Silin Pan, Xun Wang, Min Wang, Xue\n  Zhai, Taotao Chen", "title": "Automatic Detection of Cardiac Chambers Using an Attention-based YOLOv4\n  Framework from Four-chamber View of Fetal Echocardiography", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Echocardiography is a powerful prenatal examination tool for early diagnosis\nof fetal congenital heart diseases (CHDs). The four-chamber (FC) view is a\ncrucial and easily accessible ultrasound (US) image among echocardiography\nimages. Automatic analysis of FC views contributes significantly to the early\ndiagnosis of CHDs. The first step to automatically analyze fetal FC views is\nlocating the fetal four crucial chambers of heart in a US image. However, it is\na greatly challenging task due to several key factors, such as numerous\nspeckles in US images, the fetal cardiac chambers with small size and unfixed\npositions, and category indistinction caused by the similarity of cardiac\nchambers. These factors hinder the process of capturing robust and\ndiscriminative features, hence destroying fetal cardiac anatomical chambers\nprecise localization. Therefore, we first propose a multistage residual hybrid\nattention module (MRHAM) to improve the feature learning. Then, we present an\nimproved YOLOv4 detection model, namely MRHAM-YOLOv4-Slim. Specially, the\nresidual identity mapping is replaced with the MRHAM in the backbone of\nMRHAM-YOLOv4-Slim, accurately locating the four important chambers in fetal FC\nviews. Extensive experiments demonstrate that our proposed method outperforms\ncurrent state-of-the-art, including the precision of 0.919, the recall of\n0.971, the F1 score of 0.944, the mAP of 0.953, and the frames per second (FPS)\nof 43.\n", "versions": [{"version": "v1", "created": "Thu, 26 Nov 2020 02:28:24 GMT"}, {"version": "v2", "created": "Sun, 13 Dec 2020 07:45:12 GMT"}], "update_date": "2020-12-15", "authors_parsed": [["Qiao", "Sibo", ""], ["Pang", "Shanchen", ""], ["Luo", "Gang", ""], ["Pan", "Silin", ""], ["Wang", "Xun", ""], ["Wang", "Min", ""], ["Zhai", "Xue", ""], ["Chen", "Taotao", ""]]}, {"id": "2011.13117", "submitter": "Seung-Hwan Baek", "authors": "Seung-Hwan Baek, Felix Heide", "title": "Polka Lines: Learning Structured Illumination and Reconstruction for\n  Active Stereo", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Active stereo cameras that recover depth from structured light captures have\nbecome a cornerstone sensor modality for 3D scene reconstruction and\nunderstanding tasks across application domains. Existing active stereo cameras\nproject a pseudo-random dot pattern on object surfaces to extract disparity\nindependently of object texture. Such hand-crafted patterns are designed in\nisolation from the scene statistics, ambient illumination conditions, and the\nreconstruction method. In this work, we propose the first method to jointly\nlearn structured illumination and reconstruction, parameterized by a\ndiffractive optical element and a neural network, in an end-to-end fashion. To\nthis end, we introduce a novel differentiable image formation model for active\nstereo, relying on both wave and geometric optics, and a novel trinocular\nreconstruction network. The jointly optimized pattern, which we dub \"Polka\nLines,\" together with the reconstruction network, achieve state-of-the-art\nactive-stereo depth estimates across imaging conditions. We validate the\nproposed method in simulation and on a hardware prototype, and show that our\nmethod outperforms existing active stereo systems.\n", "versions": [{"version": "v1", "created": "Thu, 26 Nov 2020 04:02:43 GMT"}, {"version": "v2", "created": "Wed, 26 May 2021 00:09:24 GMT"}], "update_date": "2021-05-27", "authors_parsed": [["Baek", "Seung-Hwan", ""], ["Heide", "Felix", ""]]}, {"id": "2011.13118", "submitter": "Xiaoxiao Long", "authors": "Xiaoxiao Long, Lingjie Liu, Wei Li, Christian Theobalt, Wenping Wang", "title": "Multi-view Depth Estimation using Epipolar Spatio-Temporal Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel method for multi-view depth estimation from a single\nvideo, which is a critical task in various applications, such as perception,\nreconstruction and robot navigation. Although previous learning-based methods\nhave demonstrated compelling results, most works estimate depth maps of\nindividual video frames independently, without taking into consideration the\nstrong geometric and temporal coherence among the frames. Moreover, current\nstate-of-the-art (SOTA) models mostly adopt a fully 3D convolution network for\ncost regularization and therefore require high computational cost, thus\nlimiting their deployment in real-world applications. Our method achieves\ntemporally coherent depth estimation results by using a novel Epipolar\nSpatio-Temporal (EST) transformer to explicitly associate geometric and\ntemporal correlation with multiple estimated depth maps. Furthermore, to reduce\nthe computational cost, inspired by recent Mixture-of-Experts models, we design\na compact hybrid network consisting of a 2D context-aware network and a 3D\nmatching network which learn 2D context information and 3D disparity cues\nseparately. Extensive experiments demonstrate that our method achieves higher\naccuracy in depth estimation and significant speedup than the SOTA methods.\n", "versions": [{"version": "v1", "created": "Thu, 26 Nov 2020 04:04:21 GMT"}, {"version": "v2", "created": "Tue, 1 Dec 2020 02:55:11 GMT"}, {"version": "v3", "created": "Mon, 12 Jul 2021 16:02:54 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Long", "Xiaoxiao", ""], ["Liu", "Lingjie", ""], ["Li", "Wei", ""], ["Theobalt", "Christian", ""], ["Wang", "Wenping", ""]]}, {"id": "2011.13126", "submitter": "Yichun Shi", "authors": "Yichun Shi, Divyansh Aggarwal, Anil K. Jain", "title": "Lifting 2D StyleGAN for 3D-Aware Face Generation", "comments": "in CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a framework, called LiftedGAN, that disentangles and lifts a\npre-trained StyleGAN2 for 3D-aware face generation. Our model is \"3D-aware\" in\nthe sense that it is able to (1) disentangle the latent space of StyleGAN2 into\ntexture, shape, viewpoint, lighting and (2) generate 3D components for\nrendering synthetic images. Unlike most previous methods, our method is\ncompletely self-supervised, i.e. it neither requires any manual annotation nor\n3DMM model for training. Instead, it learns to generate images as well as their\n3D components by distilling the prior knowledge in StyleGAN2 with a\ndifferentiable renderer. The proposed model is able to output both the 3D shape\nand texture, allowing explicit pose and lighting control over generated images.\nQualitative and quantitative results show the superiority of our approach over\nexisting methods on 3D-controllable GANs in content controllability while\ngenerating realistic high quality images.\n", "versions": [{"version": "v1", "created": "Thu, 26 Nov 2020 05:02:09 GMT"}, {"version": "v2", "created": "Sun, 18 Apr 2021 09:43:49 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Shi", "Yichun", ""], ["Aggarwal", "Divyansh", ""], ["Jain", "Anil K.", ""]]}, {"id": "2011.13140", "submitter": "Weixin Huang", "authors": "Weixin Huang, Huawei Liang, Linglong Lin, Zhiling Wang, Shaobo Wang,\n  Biao Yu, Runxin Niu", "title": "A Fast Point Cloud Ground Segmentation Approach Based on Coarse-To-Fine\n  Markov Random Field", "comments": "16 page,22 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ground segmentation is an important preprocessing task for autonomous\nvehicles (AVs) with 3D LiDARs. To solve the problem of existing ground\nsegmentation methods being very difficult to balance accuracy and computational\ncomplexity, a fast point cloud ground segmentation approach based on a\ncoarse-to-fine Markov random field (MRF) method is proposed. The method uses an\nimproved elevation map for ground coarse segmentation, and then uses\nspatiotemporal adjacent points to optimize the segmentation results. The\nprocessed point cloud is classified into high-confidence obstacle points,\nground points, and unknown classification points to initialize an MRF model.\nThe graph cut method is then used to solve the model to achieve fine\nsegmentation. Experiments on datasets showed that our method improves on other\nalgorithms in terms of ground segmentation accuracy and is faster than other\ngraph-based algorithms, which require only a single core of an I7-3770 CPU to\nprocess a frame of Velodyne HDL-64E data (in 39.77 ms, on average). Field tests\nwere also conducted to demonstrate the effectiveness of the proposed method.\n", "versions": [{"version": "v1", "created": "Thu, 26 Nov 2020 06:07:24 GMT"}, {"version": "v2", "created": "Fri, 29 Jan 2021 07:32:04 GMT"}], "update_date": "2021-02-01", "authors_parsed": [["Huang", "Weixin", ""], ["Liang", "Huawei", ""], ["Lin", "Linglong", ""], ["Wang", "Zhiling", ""], ["Wang", "Shaobo", ""], ["Yu", "Biao", ""], ["Niu", "Runxin", ""]]}, {"id": "2011.13144", "submitter": "Runmin Cong", "authors": "Qijian Zhang, Runmin Cong, Chongyi Li, Ming-Ming Cheng, Yuming Fang,\n  Xiaochun Cao, Yao Zhao, and Sam Kwong", "title": "Dense Attention Fluid Network for Salient Object Detection in Optical\n  Remote Sensing Images", "comments": "Accepted by IEEE Transactions on Image Processing, EORSSD dataset:\n  https://github.com/rmcong/EORSSD-dataset", "journal-ref": null, "doi": "10.1109/TIP.2020.3042084", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Despite the remarkable advances in visual saliency analysis for natural scene\nimages (NSIs), salient object detection (SOD) for optical remote sensing images\n(RSIs) still remains an open and challenging problem. In this paper, we propose\nan end-to-end Dense Attention Fluid Network (DAFNet) for SOD in optical RSIs. A\nGlobal Context-aware Attention (GCA) module is proposed to adaptively capture\nlong-range semantic context relationships, and is further embedded in a Dense\nAttention Fluid (DAF) structure that enables shallow attention cues flow into\ndeep layers to guide the generation of high-level feature attention maps.\nSpecifically, the GCA module is composed of two key components, where the\nglobal feature aggregation module achieves mutual reinforcement of salient\nfeature embeddings from any two spatial locations, and the cascaded pyramid\nattention module tackles the scale variation issue by building up a cascaded\npyramid framework to progressively refine the attention map in a coarse-to-fine\nmanner. In addition, we construct a new and challenging optical RSI dataset for\nSOD that contains 2,000 images with pixel-wise saliency annotations, which is\ncurrently the largest publicly available benchmark. Extensive experiments\ndemonstrate that our proposed DAFNet significantly outperforms the existing\nstate-of-the-art SOD competitors. https://github.com/rmcong/DAFNet_TIP20\n", "versions": [{"version": "v1", "created": "Thu, 26 Nov 2020 06:14:10 GMT"}], "update_date": "2021-02-03", "authors_parsed": [["Zhang", "Qijian", ""], ["Cong", "Runmin", ""], ["Li", "Chongyi", ""], ["Cheng", "Ming-Ming", ""], ["Fang", "Yuming", ""], ["Cao", "Xiaochun", ""], ["Zhao", "Yao", ""], ["Kwong", "Sam", ""]]}, {"id": "2011.13150", "submitter": "Jong Chul Ye", "authors": "Serin Yang, Eung Yeop Kim, and Jong Chul Ye", "title": "Continuous Conversion of CT Kernel using Switchable CycleGAN with AdaIN", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  X-ray computed tomography (CT) uses different filter kernels to highlight\ndifferent structures. Since the raw sinogram data is usually removed after the\nreconstruction, in case there are additional need for other types of kernel\nimages that were not previously generated, the patient may need to be scanned\nagain. Accordingly, there exists increasing demand for post-hoc image domain\nconversion from one kernel to another without sacrificing the image quality. In\nthis paper, we propose a novel unsupervised continuous kernel conversion method\nusing cycle-consistent generative adversarial network (cycleGAN) with adaptive\ninstance normalization (AdaIN). Even without paired training data, not only can\nour network translate the images between two different kernels, but it can also\nconvert images along the interpolation path between the two kernel domains. We\nalso show that the quality of generated images can be further improved if\nintermediate kernel domain images are available. Experimental results confirm\nthat our method not only enables accurate kernel conversion that is comparable\nto supervised learning methods, but also generates intermediate kernel images\nin the unseen domain that are useful for hypopharyngeal cancer diagnosis.\n", "versions": [{"version": "v1", "created": "Thu, 26 Nov 2020 06:35:57 GMT"}, {"version": "v2", "created": "Mon, 26 Apr 2021 00:51:33 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Yang", "Serin", ""], ["Kim", "Eung Yeop", ""], ["Ye", "Jong Chul", ""]]}, {"id": "2011.13160", "submitter": "Xin Hong", "authors": "Xin Hong, Yanyan Lan, Liang Pang, Jiafeng Guo and Xueqi Cheng", "title": "Transformation Driven Visual Reasoning", "comments": "Accepted to CVPR 2021. Resources including the TRANCE dataset and the\n  code can be found at our homepage https://hongxin2019.github.io/TVR", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper defines a new visual reasoning paradigm by introducing an\nimportant factor, i.e.~transformation. The motivation comes from the fact that\nmost existing visual reasoning tasks, such as CLEVR in VQA, are solely defined\nto test how well the machine understands the concepts and relations within\nstatic settings, like one image. We argue that this kind of \\textbf{state\ndriven visual reasoning} approach has limitations in reflecting whether the\nmachine has the ability to infer the dynamics between different states, which\nhas been shown as important as state-level reasoning for human cognition in\nPiaget's theory. To tackle this problem, we propose a novel\n\\textbf{transformation driven visual reasoning} task. Given both the initial\nand final states, the target is to infer the corresponding single-step or\nmulti-step transformation, represented as a triplet (object, attribute, value)\nor a sequence of triplets, respectively. Following this definition, a new\ndataset namely TRANCE is constructed on the basis of CLEVR, including three\nlevels of settings, i.e.~Basic (single-step transformation), Event (multi-step\ntransformation), and View (multi-step transformation with variant views).\nExperimental results show that the state-of-the-art visual reasoning models\nperform well on Basic, but are still far from human-level intelligence on Event\nand View. We believe the proposed new paradigm will boost the development of\nmachine visual reasoning. More advanced methods and real data need to be\ninvestigated in this direction. The resource of TVR is available at\nhttps://hongxin2019.github.io/TVR.\n", "versions": [{"version": "v1", "created": "Thu, 26 Nov 2020 07:11:31 GMT"}, {"version": "v2", "created": "Fri, 2 Apr 2021 06:25:46 GMT"}], "update_date": "2021-04-05", "authors_parsed": [["Hong", "Xin", ""], ["Lan", "Yanyan", ""], ["Pang", "Liang", ""], ["Guo", "Jiafeng", ""], ["Cheng", "Xueqi", ""]]}, {"id": "2011.13179", "submitter": "Giuliana Ramella", "authors": "Giuliana Ramella", "title": "Saliency-based segmentation of dermoscopic images using color\n  information", "comments": "Preprint", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Skin lesion segmentation is one of the crucial steps for an efficient\nnon-invasive computer-aided early diagnosis of melanoma. In this paper, we\ninvestigate how saliency and color information can be usefully employed to\ndetermine the lesion region. Unlike most existing saliency-based methods, to\ndiscriminate against the skin lesion from the surrounding regions we enucleate\nsome properties related to saliency and color information and we propose a\nnovel segmentation process using binarization coupled with new perceptual\ncriteria based on these properties. To refine the accuracy of the proposed\nmethod, the segmentation step is preceded by a pre-processing aimed at reducing\nthe computation burden, removing artifacts, and improving contrast. We have\nassessed the method on two public databases including 1497 dermoscopic images\nand compared its performance with that of classical saliency-based methods and\nwith that of some more recent saliency-based methods specifically applied to\ndermoscopic images. Results of qualitative and quantitative evaluations of the\nproposed method are promising as the obtained skin lesion segmentation is\naccurate and the method performs satisfactorily in comparison to other existing\nsaliency-based segmentation methods.\n", "versions": [{"version": "v1", "created": "Thu, 26 Nov 2020 08:47:10 GMT"}], "update_date": "2020-11-30", "authors_parsed": [["Ramella", "Giuliana", ""]]}, {"id": "2011.13181", "submitter": "Genki Osada", "authors": "Genki Osada, Budrul Ahsan, Revoti Prasad Bora, Takashi Nishide", "title": "Regularization with Latent Space Virtual Adversarial Training", "comments": "Accepted at ECCV 2020 (Oral)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Virtual Adversarial Training (VAT) has shown impressive results among\nrecently developed regularization methods called consistency regularization.\nVAT utilizes adversarial samples, generated by injecting perturbation in the\ninput space, for training and thereby enhances the generalization ability of a\nclassifier. However, such adversarial samples can be generated only within a\nvery small area around the input data point, which limits the adversarial\neffectiveness of such samples. To address this problem we propose LVAT (Latent\nspace VAT), which injects perturbation in the latent space instead of the input\nspace. LVAT can generate adversarial samples flexibly, resulting in more\nadverse effects and thus more effective regularization. The latent space is\nbuilt by a generative model, and in this paper, we examine two different type\nof models: variational auto-encoder and normalizing flow, specifically Glow. We\nevaluated the performance of our method in both supervised and semi-supervised\nlearning scenarios for an image classification task using SVHN and CIFAR-10\ndatasets. In our evaluation, we found that our method outperforms VAT and other\nstate-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Thu, 26 Nov 2020 08:51:38 GMT"}], "update_date": "2020-11-30", "authors_parsed": [["Osada", "Genki", ""], ["Ahsan", "Budrul", ""], ["Bora", "Revoti Prasad", ""], ["Nishide", "Takashi", ""]]}, {"id": "2011.13183", "submitter": "Yanjia Zhu", "authors": "Yanjia Zhu, Hongxiang Cai, Shuhan Zhang, Chenhao Wang, Yichao Xiong", "title": "TinaFace: Strong but Simple Baseline for Face Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Face detection has received intensive attention in recent years. Many works\npresent lots of special methods for face detection from different perspectives\nlike model architecture, data augmentation, label assignment and etc., which\nmake the overall algorithm and system become more and more complex. In this\npaper, we point out that \\textbf{there is no gap between face detection and\ngeneric object detection}. Then we provide a strong but simple baseline method\nto deal with face detection named TinaFace. We use ResNet-50 \\cite{he2016deep}\nas backbone, and all modules and techniques in TinaFace are constructed on\nexisting modules, easily implemented and based on generic object detection. On\nthe hard test set of the most popular and challenging face detection benchmark\nWIDER FACE \\cite{yang2016wider}, with single-model and single-scale, our\nTinaFace achieves 92.1\\% average precision (AP), which exceeds most of the\nrecent face detectors with larger backbone. And after using test time\naugmentation (TTA), our TinaFace outperforms the current state-of-the-art\nmethod and achieves 92.4\\% AP. The code will be available at\n\\url{https://github.com/Media-Smart/vedadet}.\n", "versions": [{"version": "v1", "created": "Thu, 26 Nov 2020 08:54:19 GMT"}, {"version": "v2", "created": "Wed, 2 Dec 2020 11:31:34 GMT"}, {"version": "v3", "created": "Fri, 22 Jan 2021 08:05:39 GMT"}], "update_date": "2021-01-25", "authors_parsed": [["Zhu", "Yanjia", ""], ["Cai", "Hongxiang", ""], ["Zhang", "Shuhan", ""], ["Wang", "Chenhao", ""], ["Xiong", "Yichao", ""]]}, {"id": "2011.13202", "submitter": "Soroosh Poorgholi", "authors": "Soroosh Poorgholi, Osman Semih Kayhan and Jan C. van Gemert", "title": "t-EVA: Time-Efficient t-SNE Video Annotation", "comments": "ICPR 2020 (HCAU)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG eess.IV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Video understanding has received more attention in the past few years due to\nthe availability of several large-scale video datasets. However, annotating\nlarge-scale video datasets are cost-intensive. In this work, we propose a\ntime-efficient video annotation method using spatio-temporal feature similarity\nand t-SNE dimensionality reduction to speed up the annotation process\nmassively. Placing the same actions from different videos near each other in\nthe two-dimensional space based on feature similarity helps the annotator to\ngroup-label video clips. We evaluate our method on two subsets of the\nActivityNet (v1.3) and a subset of the Sports-1M dataset. We show that t-EVA\ncan outperform other video annotation tools while maintaining test accuracy on\nvideo classification.\n", "versions": [{"version": "v1", "created": "Thu, 26 Nov 2020 09:56:54 GMT"}], "update_date": "2020-11-30", "authors_parsed": [["Poorgholi", "Soroosh", ""], ["Kayhan", "Osman Semih", ""], ["van Gemert", "Jan C.", ""]]}, {"id": "2011.13209", "submitter": "Udo Frese", "authors": "Jesse Richter-Klug and Udo Frese", "title": "Handling Object Symmetries in CNN-based Pose Estimation", "comments": "This work has been accepte at ICRA 2021. Copyright may be transferred\n  without notice, after which this version may no longer be accessible", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we investigate the problems that Convolutional Neural Networks\n(CNN)-based pose estimators have with symmetric objects. We considered the\nvalue of the CNN's output representation when continuously rotating the object\nand found that it has to form a closed loop after each step of symmetry.\nOtherwise, the CNN (which is itself a continuous function) has to replicate an\nuncontinuous function. On a 1-DOF toy example we show that commonly used\nrepresentations do not fulfill this demand and analyze the problems caused\nthereby. In particular, we find that the popular min-over-symmetries approach\nfor creating a symmetry-aware loss tends not to work well with gradient-based\noptimization, i.e. deep learning.\n  We propose a representation called \"closed symmetry loop\" (csl) from these\ninsights, where the angle of relevant vectors is multiplied by the symmetry\norder and then generalize it to 6-DOF. The representation extends our algorithm\nfrom [Richter-Klug, ICVS, 2019] including a method to disambiguate symmetric\nequivalents during the final pose estimation. The algorithm handles continuous\nrotational symmetry (e.g. a bottle) and discrete rotational symmetry (e.g. a\n4-fold symmetric box). It is evaluated on the T-LESS dataset, where it reaches\nstate-of-the-art for unrefining RGB-based methods.\n", "versions": [{"version": "v1", "created": "Thu, 26 Nov 2020 10:10:25 GMT"}, {"version": "v2", "created": "Tue, 27 Apr 2021 07:24:24 GMT"}], "update_date": "2021-04-28", "authors_parsed": [["Richter-Klug", "Jesse", ""], ["Frese", "Udo", ""]]}, {"id": "2011.13226", "submitter": "Han Hu", "authors": "Qing Zhu and Shengzhi Huang and Han Hu and Haifeng Li and Min Chen and\n  Ruofei Zhong", "title": "Depth-Enhanced Feature Pyramid Network for Occlusion-Aware Verification\n  of Buildings from Oblique Images", "comments": null, "journal-ref": null, "doi": "10.1016/j.isprsjprs.2021.01.025", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Detecting the changes of buildings in urban environments is essential.\nExisting methods that use only nadir images suffer from severe problems of\nambiguous features and occlusions between buildings and other regions.\nFurthermore, buildings in urban environments vary significantly in scale, which\nleads to performance issues when using single-scale features. To solve these\nissues, this paper proposes a fused feature pyramid network, which utilizes\nboth color and depth data for the 3D verification of existing buildings 2D\nfootprints from oblique images. First, the color data of oblique images are\nenriched with the depth information rendered from 3D mesh models. Second,\nmultiscale features are fused in the feature pyramid network to convolve both\nthe color and depth data. Finally, multi-view information from both the nadir\nand oblique images is used in a robust voting procedure to label changes in\nexisting buildings. Experimental evaluations using both the ISPRS benchmark\ndatasets and Shenzhen datasets reveal that the proposed method outperforms the\nResNet and EfficientNet networks by 5\\% and 2\\%, respectively, in terms of\nrecall rate and precision. We demonstrate that the proposed method can\nsuccessfully detect all changed buildings; therefore, only those marked as\nchanged need to be manually checked during the pipeline updating procedure;\nthis significantly reduces the manual quality control requirements. Moreover,\nablation studies indicate that using depth data, feature pyramid modules, and\nmulti-view voting strategies can lead to clear and progressive improvements.\n", "versions": [{"version": "v1", "created": "Thu, 26 Nov 2020 10:51:36 GMT"}, {"version": "v2", "created": "Sat, 23 Jan 2021 08:46:50 GMT"}], "update_date": "2021-02-25", "authors_parsed": [["Zhu", "Qing", ""], ["Huang", "Shengzhi", ""], ["Hu", "Han", ""], ["Li", "Haifeng", ""], ["Chen", "Min", ""], ["Zhong", "Ruofei", ""]]}, {"id": "2011.13228", "submitter": "Florin Walter", "authors": "Florin C. Walter, Sebastian Damrich, Fred A. Hamprecht", "title": "MultiStar: Instance Segmentation of Overlapping Objects with Star-Convex\n  Polygons", "comments": "Accepted for ISBI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Instance segmentation of overlapping objects in biomedical images remains a\nlargely unsolved problem. We take up this challenge and present MultiStar, an\nextension to the popular instance segmentation method StarDist. The key novelty\nof our method is that we identify pixels at which objects overlap and use this\ninformation to improve proposal sampling and to avoid suppressing proposals of\ntruly overlapping objects. This allows us to apply the ideas of StarDist to\nimages with overlapping objects, while incurring only a small overhead compared\nto the established method. MultiStar shows promising results on two datasets\nand has the advantage of using a simple and easy to train network architecture.\n", "versions": [{"version": "v1", "created": "Thu, 26 Nov 2020 10:52:33 GMT"}, {"version": "v2", "created": "Thu, 14 Jan 2021 10:19:15 GMT"}], "update_date": "2021-01-15", "authors_parsed": [["Walter", "Florin C.", ""], ["Damrich", "Sebastian", ""], ["Hamprecht", "Fred A.", ""]]}, {"id": "2011.13241", "submitter": "Kim Myungchul", "authors": "Myungchul Kim, Sanghyun Woo, Dahun Kim, and In So Kweon", "title": "The Devil is in the Boundary: Exploiting Boundary Representation for\n  Basis-based Instance Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pursuing a more coherent scene understanding towards real-time vision\napplications, single-stage instance segmentation has recently gained\npopularity, achieving a simpler and more efficient design than its two-stage\ncounterparts. Besides, its global mask representation often leads to superior\naccuracy to the two-stage Mask R-CNN which has been dominant thus far. Despite\nthe promising advances in single-stage methods, finer delineation of instance\nboundaries still remains unexcavated. Indeed, boundary information provides a\nstrong shape representation that can operate in synergy with the\nfully-convolutional mask features of the single-stage segmenter. In this work,\nwe propose Boundary Basis based Instance Segmentation(B2Inst) to learn a global\nboundary representation that can complement existing global-mask-based methods\nthat are often lacking high-frequency details. Besides, we devise a unified\nquality measure of both mask and boundary and introduce a network block that\nlearns to score the per-instance predictions of itself. When applied to the\nstrongest baselines in single-stage instance segmentation, our B2Inst leads to\nconsistent improvements and accurately parse out the instance boundaries in a\nscene. Regardless of being single-stage or two-stage frameworks, we outperform\nthe existing state-of-the-art methods on the COCO dataset with the same\nResNet-50 and ResNet-101 backbones.\n", "versions": [{"version": "v1", "created": "Thu, 26 Nov 2020 11:26:06 GMT"}], "update_date": "2020-11-30", "authors_parsed": [["Kim", "Myungchul", ""], ["Woo", "Sanghyun", ""], ["Kim", "Dahun", ""], ["Kweon", "In So", ""]]}, {"id": "2011.13244", "submitter": "Abdullah Hamdi", "authors": "Abdullah Hamdi, Silvio Giancola, Bernard Ghanem", "title": "MVTN: Multi-View Transformation Network for 3D Shape Recognition", "comments": "preprint", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Multi-view projection methods have demonstrated their ability to reach\nstate-of-the-art performance on 3D shape recognition. Those methods learn\ndifferent ways to aggregate information from multiple views. However, the\ncamera view-points for those views tend to be heuristically set and fixed for\nall shapes. To circumvent the lack of dynamism of current multi-view methods,\nwe propose to learn those view-points. In particular, we introduce the\nMulti-View Transformation Network (MVTN) that regresses optimal view-points for\n3D shape recognition, building upon advances in differentiable rendering. As a\nresult, MVTN can be trained end-to-end along with any multi-view network for 3D\nshape classification. We integrate MVTN in a novel adaptive multi-view pipeline\nthat can render either 3D meshes or point clouds. MVTN exhibits clear\nperformance gains in the tasks of 3D shape classification and 3D shape\nretrieval without the need for extra training supervision. In these tasks, MVTN\nachieves state-of-the-art performance on ModelNet40, ShapeNet Core55, and the\nmost recent and realistic ScanObjectNN dataset (up to 6% improvement).\nInterestingly, we also show that MVTN can provide network robustness against\nrotation and occlusion in the 3D domain.\n", "versions": [{"version": "v1", "created": "Thu, 26 Nov 2020 11:33:53 GMT"}, {"version": "v2", "created": "Sat, 17 Apr 2021 23:50:13 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Hamdi", "Abdullah", ""], ["Giancola", "Silvio", ""], ["Ghanem", "Bernard", ""]]}, {"id": "2011.13246", "submitter": "Dawood Al Chanti", "authors": "Dawood Al Chanti, Vanessa Gonzalez Duque, Marion Crouzier, Antoine\n  Nordez, Lilian Lacourpaille, and Diana Mateus", "title": "IFSS-Net: Interactive Few-Shot Siamese Network for Faster Muscle\n  Segmentation and Propagation in Volumetric Ultrasound", "comments": "14 pages, 18 figures, 10 Tables", "journal-ref": null, "doi": "10.1109/TMI.2021.3058303", "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present an accurate, fast and efficient method for segmentation and muscle\nmask propagation in 3D freehand ultrasound data, towards accurate volume\nquantification. A deep Siamese 3D Encoder-Decoder network that captures the\nevolution of the muscle appearance and shape for contiguous slices is deployed.\nWe uses it to propagate a reference mask annotated by a clinical expert. To\nhandle longer changes of the muscle shape over the entire volume and to provide\nan accurate propagation, we devise a Bidirectional Long Short Term Memory\nmodule. Also, to train our model with a minimal amount of training samples, we\npropose a strategy combining learning from few annotated 2D ultrasound slices\nwith sequential pseudo-labeling of the unannotated slices. We introduce a\ndecremental update of the objective function to guide the model convergence in\nthe absence of large amounts of annotated data. After training with a small\nnumber of volumes, the decremental update transitions from a weakly-supervised\ntraining to a few-shot setting. Finally, to handle the class-imbalance between\nforeground and background muscle pixels, we propose a parametric Tversky loss\nfunction that learns to adaptively penalize false positives and false\nnegatives. We validate our approach for the segmentation, label propagation,\nand volume computation of the three low-limb muscles on a dataset of 61600\nimages from 44 subjects. We achieve a Dice score coefficient of over $95~\\%$\nand a volumetric error \\textcolor{black}{of} $1.6035 \\pm 0.587~\\%$.\n", "versions": [{"version": "v1", "created": "Thu, 26 Nov 2020 11:37:25 GMT"}, {"version": "v2", "created": "Sat, 30 Jan 2021 11:40:48 GMT"}], "update_date": "2021-02-09", "authors_parsed": [["Chanti", "Dawood Al", ""], ["Duque", "Vanessa Gonzalez", ""], ["Crouzier", "Marion", ""], ["Nordez", "Antoine", ""], ["Lacourpaille", "Lilian", ""], ["Mateus", "Diana", ""]]}, {"id": "2011.13256", "submitter": "Chunhua Shen", "authors": "Changyong Shu, Yifan Liu, Jianfei Gao, Lin Xu, Chunhua Shen", "title": "Channel-wise Distillation for Semantic Segmentation", "comments": "Included more results on Object Detection. Code is available at:\n  https://git.io/ChannelDis", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Knowledge distillation (KD) has been proven to be a simple and effective tool\nfor training compact models. Almost all KD variants for semantic segmentation\nalign the student and teacher networks' feature maps in the spatial domain,\ntypically by minimizing point-wise and/or pair-wise discrepancy. Observing that\nin semantic segmentation, some layers' feature activations of each channel tend\nto encode saliency of scene categories (analogue to class activation mapping),\nwe propose to align features channel-wise between the student and teacher\nnetworks. To this end, we first transform the feature map of each channel into\na distribution using softmax normalization, and then minimize the\nKullback-Leibler (KL) divergence of the corresponding channels of the two\nnetworks. By doing so, our method focuses on mimicking the soft distributions\nof channels between networks. In particular, the KL divergence enables learning\nto pay more attention to the most salient regions of the channel-wise maps,\npresumably corresponding to the most useful signals for semantic segmentation.\nExperiments demonstrate that our channel-wise distillation outperforms almost\nall existing spatial distillation methods for semantic segmentation\nconsiderably, and requires less computational cost during training. We\nconsistently achieve superior performance on three benchmarks with various\nnetwork structures. Code is available at: https://git.io/ChannelDis\n", "versions": [{"version": "v1", "created": "Thu, 26 Nov 2020 12:00:38 GMT"}, {"version": "v2", "created": "Fri, 22 Jan 2021 01:59:32 GMT"}], "update_date": "2021-01-25", "authors_parsed": [["Shu", "Changyong", ""], ["Liu", "Yifan", ""], ["Gao", "Jianfei", ""], ["Xu", "Lin", ""], ["Shen", "Chunhua", ""]]}, {"id": "2011.13265", "submitter": "Sandesh Ramesh", "authors": "Sandesh Ramesh, Anirudh Hebbar, Varun Yadav, Thulasiram Gunta, and A\n  Balachandra", "title": "CYPUR-NN: Crop Yield Prediction Using Regression and Neural Networks", "comments": "Advances in Intelligent Systems and Computing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Our recent study using historic data of paddy yield and associated conditions\ninclude humidity, luminescence, and temperature. By incorporating regression\nmodels and neural networks (NN), one can produce highly satisfactory\nforecasting of paddy yield. Simulations indicate that our model can predict\npaddy yield with high accuracy while concurrently detecting diseases that may\nexist and are oblivious to the human eye. Crop Yield Prediction Using\nRegression and Neural Networks (CYPUR-NN) is developed here as a system that\nwill facilitate agriculturists and farmers to predict yield from a picture or\nby entering values via a web interface. CYPUR-NN has been tested on stock\nimages and the experimental results are promising.\n", "versions": [{"version": "v1", "created": "Thu, 26 Nov 2020 12:50:58 GMT"}], "update_date": "2020-11-30", "authors_parsed": [["Ramesh", "Sandesh", ""], ["Hebbar", "Anirudh", ""], ["Yadav", "Varun", ""], ["Gunta", "Thulasiram", ""], ["Balachandra", "A", ""]]}, {"id": "2011.13273", "submitter": "Tingtian Li", "authors": "Tingtian Li, Zixun Sun, Xiao Chen", "title": "Group-Skeleton-Based Human Action Recognition in Complex Events", "comments": "accpeted by ACM MM 2020", "journal-ref": null, "doi": "10.1145/3394171.3416280", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human action recognition as an important application of computer vision has\nbeen studied for decades. Among various approaches, skeleton-based methods\nrecently attract increasing attention due to their robust and superior\nperformance. However, existing skeleton-based methods ignore the potential\naction relationships between different persons, while the action of a person is\nhighly likely to be impacted by another person especially in complex events. In\nthis paper, we propose a novel group-skeleton-based human action recognition\nmethod in complex events. This method first utilizes multi-scale\nspatial-temporal graph convolutional networks (MS-G3Ds) to extract skeleton\nfeatures from multiple persons. In addition to the traditional key point\ncoordinates, we also input the key point speed values to the networks for\nbetter performance. Then we use multilayer perceptrons (MLPs) to embed the\ndistance values between the reference person and other persons into the\nextracted features. Lastly, all the features are fed into another MS-G3D for\nfeature fusion and classification. For avoiding class imbalance problems, the\nnetworks are trained with a focal loss. The proposed algorithm is also our\nsolution for the Large-scale Human-centric Video Analysis in Complex Events\nChallenge. Results on the HiEve dataset show that our method can give superior\nperformance compared to other state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Thu, 26 Nov 2020 13:19:14 GMT"}, {"version": "v2", "created": "Thu, 25 Feb 2021 03:42:32 GMT"}], "update_date": "2021-02-26", "authors_parsed": [["Li", "Tingtian", ""], ["Sun", "Zixun", ""], ["Chen", "Xiao", ""]]}, {"id": "2011.13307", "submitter": "Wu Weijia", "authors": "Weijia Wu, Enze Xie, Ruimao Zhang, Wenhai Wang, Guan Pang, Zhen Li,\n  Hong Zhou, Ping Luo", "title": "SelfText Beyond Polygon: Unconstrained Text Detection with Box\n  Supervision and Dynamic Self-Training", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Although a polygon is a more accurate representation than an upright bounding\nbox for text detection, the annotations of polygons are extremely expensive and\nchallenging. Unlike existing works that employ fully-supervised training with\npolygon annotations, we propose a novel text detection system termed SelfText\nBeyond Polygon (SBP) with Bounding Box Supervision (BBS) and Dynamic Self\nTraining (DST), where training a polygon-based text detector with only a\nlimited set of upright bounding box annotations. For BBS, we firstly utilize\nthe synthetic data with character-level annotations to train a Skeleton\nAttention Segmentation Network (SASN). Then the box-level annotations are\nadopted to guide the generation of high-quality polygon-liked pseudo labels,\nwhich can be used to train any detectors. In this way, our method achieves the\nsame performance as text detectors trained with polygon annotations (i.e., both\nare 85.0% F-score for PSENet on ICDAR2015 ). For DST, through dynamically\nremoving the false alarms, it is able to leverage limited labeled data as well\nas massive unlabeled data to further outperform the expensive baseline. We hope\nSBP can provide a new perspective for text detection to save huge labeling\ncosts. Code is available at: github.com/weijiawu/SBP.\n", "versions": [{"version": "v1", "created": "Thu, 26 Nov 2020 14:19:33 GMT"}, {"version": "v2", "created": "Sat, 5 Dec 2020 07:58:55 GMT"}], "update_date": "2020-12-08", "authors_parsed": [["Wu", "Weijia", ""], ["Xie", "Enze", ""], ["Zhang", "Ruimao", ""], ["Wang", "Wenhai", ""], ["Pang", "Guan", ""], ["Li", "Zhen", ""], ["Zhou", "Hong", ""], ["Luo", "Ping", ""]]}, {"id": "2011.13313", "submitter": "Kailun Yang", "authors": "Kaite Xiang, Kailun Yang and Kaiwei Wang", "title": "Polarization-driven Semantic Segmentation via Efficient\n  Attention-bridged Fusion", "comments": "Accepted by Optics Express. 18 pages, 16 figures, 3 tables, 9\n  equations. Code will be made publicly available at\n  https://github.com/Katexiang/EAFNet", "journal-ref": null, "doi": "10.1364/OE.416130", "report-no": null, "categories": "cs.CV cs.RO eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Semantic Segmentation (SS) is promising for outdoor scene perception in\nsafety-critical applications like autonomous vehicles, assisted navigation and\nso on. However, traditional SS is primarily based on RGB images, which limits\nthe reliability of SS in complex outdoor scenes, where RGB images lack\nnecessary information dimensions to fully perceive unconstrained environments.\nAs preliminary investigation, we examine SS in an unexpected obstacle detection\nscenario, which demonstrates the necessity of multimodal fusion. Thereby, in\nthis work, we present EAFNet, an Efficient Attention-bridged Fusion Network to\nexploit complementary information coming from different optical sensors.\nSpecifically, we incorporate polarization sensing to obtain supplementary\ninformation, considering its optical characteristics for robust representation\nof diverse materials. By using a single-shot polarization sensor, we build the\nfirst RGB-P dataset which consists of 394 annotated pixel-aligned\nRGB-Polarization images. A comprehensive variety of experiments shows the\neffectiveness of EAFNet to fuse polarization and RGB information, as well as\nthe flexibility to be adapted to other sensor combination scenarios.\n", "versions": [{"version": "v1", "created": "Thu, 26 Nov 2020 14:32:42 GMT"}, {"version": "v2", "created": "Fri, 22 Jan 2021 22:06:59 GMT"}], "update_date": "2021-02-24", "authors_parsed": [["Xiang", "Kaite", ""], ["Yang", "Kailun", ""], ["Wang", "Kaiwei", ""]]}, {"id": "2011.13317", "submitter": "Diogo Luvizon", "authors": "Diogo C. Luvizon, Gustavo Sutter P. Carvalho, Andreza A. dos Santos,\n  Jhonatas S. Conceicao, Jose L. Flores-Campana, Luis G. L. Decker, Marcos R.\n  Souza, Helio Pedrini, Antonio Joia, Otavio A. B. Penatti", "title": "Adaptive Multiplane Image Generation from a Single Internet Picture", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the last few years, several works have tackled the problem of novel view\nsynthesis from stereo images or even from a single picture. However, previous\nmethods are computationally expensive, specially for high-resolution images. In\nthis paper, we address the problem of generating a multiplane image (MPI) from\na single high-resolution picture. We present the adaptive-MPI representation,\nwhich allows rendering novel views with low computational requirements. To this\nend, we propose an adaptive slicing algorithm that produces an MPI with a\nvariable number of image planes. We present a new lightweight CNN for depth\nestimation, which is learned by knowledge distillation from a larger network.\nOccluded regions in the adaptive-MPI are inpainted also by a lightweight CNN.\nWe show that our method is capable of producing high-quality predictions with\none order of magnitude less parameters compared to previous approaches. The\nrobustness of our method is evidenced on challenging pictures from the\nInternet.\n", "versions": [{"version": "v1", "created": "Thu, 26 Nov 2020 14:35:05 GMT"}], "update_date": "2020-11-30", "authors_parsed": [["Luvizon", "Diogo C.", ""], ["Carvalho", "Gustavo Sutter P.", ""], ["Santos", "Andreza A. dos", ""], ["Conceicao", "Jhonatas S.", ""], ["Flores-Campana", "Jose L.", ""], ["Decker", "Luis G. L.", ""], ["Souza", "Marcos R.", ""], ["Pedrini", "Helio", ""], ["Joia", "Antonio", ""], ["Penatti", "Otavio A. B.", ""]]}, {"id": "2011.13322", "submitter": "Zhen Huang", "authors": "Zhen Huang, Xu Shen, Xinmei Tian, Houqiang Li, Jianqiang Huang and\n  Xian-Sheng Hua", "title": "Spatio-Temporal Inception Graph Convolutional Networks for\n  Skeleton-Based Action Recognition", "comments": "ACMMM 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG cs.MM", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Skeleton-based human action recognition has attracted much attention with the\nprevalence of accessible depth sensors. Recently, graph convolutional networks\n(GCNs) have been widely used for this task due to their powerful capability to\nmodel graph data. The topology of the adjacency graph is a key factor for\nmodeling the correlations of the input skeletons. Thus, previous methods mainly\nfocus on the design/learning of the graph topology. But once the topology is\nlearned, only a single-scale feature and one transformation exist in each layer\nof the networks. Many insights, such as multi-scale information and multiple\nsets of transformations, that have been proven to be very effective in\nconvolutional neural networks (CNNs), have not been investigated in GCNs. The\nreason is that, due to the gap between graph-structured skeleton data and\nconventional image/video data, it is very challenging to embed these insights\ninto GCNs. To overcome this gap, we reinvent the split-transform-merge strategy\nin GCNs for skeleton sequence processing. Specifically, we design a simple and\nhighly modularized graph convolutional network architecture for skeleton-based\naction recognition. Our network is constructed by repeating a building block\nthat aggregates multi-granularity information from both the spatial and\ntemporal paths. Extensive experiments demonstrate that our network outperforms\nstate-of-the-art methods by a significant margin with only 1/5 of the\nparameters and 1/10 of the FLOPs.\n", "versions": [{"version": "v1", "created": "Thu, 26 Nov 2020 14:43:04 GMT"}], "update_date": "2020-11-30", "authors_parsed": [["Huang", "Zhen", ""], ["Shen", "Xu", ""], ["Tian", "Xinmei", ""], ["Li", "Houqiang", ""], ["Huang", "Jianqiang", ""], ["Hua", "Xian-Sheng", ""]]}, {"id": "2011.13327", "submitter": "Dennis Klinkhammer", "authors": "Dennis Klinkhammer", "title": "Analysing Social Media Network Data with R: Semi-Automated Screening of\n  Users, Comments and Communication Patterns", "comments": "14 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.CV stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Communication on social media platforms is not only culturally and\npolitically relevant, it is also increasingly widespread across societies.\nUsers not only communicate via social media platforms, but also search\nspecifically for information, disseminate it or post information themselves.\nHowever, fake news, hate speech and even radicalizing elements are part of this\nmodern form of communication: Sometimes with far-reaching effects on\nindividuals and societies. A basic understanding of these mechanisms and\ncommunication patterns could help to counteract negative forms of\ncommunication, e.g. bullying among children or extreme political points of\nview. To this end, a method will be presented in order to break down the\nunderlying communication patterns, to trace individual users and to inspect\ntheir comments and range on social media platforms; Or to contrast them later\non via qualitative research. This approeach can identify particularly active\nusers with an accuracy of 100 percent, if the framing social networks as well\nas the topics are taken into account. However, methodological as well as\ncounteracting approaches must be even more dynamic and flexible to ensure\nsensitivity and specifity regarding users who spread hate speech, fake news and\nradicalizing elements.\n", "versions": [{"version": "v1", "created": "Thu, 26 Nov 2020 14:52:01 GMT"}], "update_date": "2020-11-30", "authors_parsed": [["Klinkhammer", "Dennis", ""]]}, {"id": "2011.13328", "submitter": "Chunhua Shen", "authors": "Tong He, Chunhua Shen, Anton van den Hengel", "title": "DyCo3D: Robust Instance Segmentation of 3D Point Clouds through Dynamic\n  Convolution", "comments": "Appearing in IEEE Conf. Computer Vision and Pattern Recognition\n  (CVPR), 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Previous top-performing approaches for point cloud instance segmentation\ninvolve a bottom-up strategy, which often includes inefficient operations or\ncomplex pipelines, such as grouping over-segmented components, introducing\nadditional steps for refining, or designing complicated loss functions. The\ninevitable variation in the instance scales can lead bottom-up methods to\nbecome particularly sensitive to hyper-parameter values. To this end, we\npropose instead a dynamic, proposal-free, data-driven approach that generates\nthe appropriate convolution kernels to apply in response to the nature of the\ninstances. To make the kernels discriminative, we explore a large context by\ngathering homogeneous points that share identical semantic categories and have\nclose votes for the geometric centroids. Instances are then decoded by several\nsimple convolutional layers. Due to the limited receptive field introduced by\nthe sparse convolution, a small light-weight transformer is also devised to\ncapture the long-range dependencies and high-level interactions among point\nsamples. The proposed method achieves promising results on both ScanetNetV2 and\nS3DIS, and this performance is robust to the particular hyper-parameter values\nchosen. It also improves inference speed by more than 25% over the current\nstate-of-the-art. Code is available at: https://git.io/DyCo3D\n", "versions": [{"version": "v1", "created": "Thu, 26 Nov 2020 14:56:57 GMT"}, {"version": "v2", "created": "Sat, 6 Mar 2021 00:41:22 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["He", "Tong", ""], ["Shen", "Chunhua", ""], ["Hengel", "Anton van den", ""]]}, {"id": "2011.13341", "submitter": "Miao Liu", "authors": "Miao Liu, Dexin Yang, Yan Zhang, Zhaopeng Cui, James M. Rehg, Siyu\n  Tang", "title": "4D Human Body Capture from Egocentric Video via 3D Scene Grounding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To understand human daily social interaction from egocentric perspective, we\nintroduce a novel task of reconstructing a time series of second-person 3D\nhuman body meshes from monocular egocentric videos. The unique viewpoint and\nrapid embodied camera motion of egocentric videos raise additional technical\nbarriers for human body capture. To address those challenges, we propose a\nnovel optimization-based approach that leverages 2D observations of the entire\nvideo sequence and human-scene interaction constraint to estimate second-person\nhuman poses, shapes and global motion that are grounded on the 3D environment\ncaptured from the egocentric view. We conduct detailed ablation studies to\nvalidate our design choice. Moreover, we compare our method with previous\nstate-of-the-art method on human motion capture from monocular video, and show\nthat our method estimates more accurate human-body poses and shapes under the\nchallenging egocentric setting. In addition, we demonstrate that our approach\nproduces more realistic human-scene interaction. Our project page is available\nat: https://aptx4869lm.github.io/4DEgocentricBodyCapture/\n", "versions": [{"version": "v1", "created": "Thu, 26 Nov 2020 15:17:16 GMT"}], "update_date": "2020-11-30", "authors_parsed": [["Liu", "Miao", ""], ["Yang", "Dexin", ""], ["Zhang", "Yan", ""], ["Cui", "Zhaopeng", ""], ["Rehg", "James M.", ""], ["Tang", "Siyu", ""]]}, {"id": "2011.13356", "submitter": "Xiangxiang Chu", "authors": "Xiangxiang Chu and Xiaohang Zhan and Xiaolin Wei", "title": "Beyond Single Instance Multi-view Unsupervised Representation Learning", "comments": "A plug-in approach with minimal modification to existing methods\n  based on instance discrimination", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent unsupervised contrastive representation learning follows a Single\nInstance Multi-view (SIM) paradigm where positive pairs are usually constructed\nwith intra-image data augmentation. In this paper, we propose an effective\napproach called Beyond Single Instance Multi-view (BSIM). Specifically, we\nimpose more accurate instance discrimination capability by measuring the joint\nsimilarity between two randomly sampled instances and their mixture, namely\nspurious-positive pairs. We believe that learning joint similarity helps to\nimprove the performance when encoded features are distributed more evenly in\nthe latent space. We apply it as an orthogonal improvement for unsupervised\ncontrastive representation learning, including current outstanding methods\nSimCLR, MoCo, and BYOL. We evaluate our learned representations on many\ndownstream benchmarks like linear classification on ImageNet-1k and PASCAL VOC\n2007, object detection on MS COCO 2017 and VOC, etc. We obtain substantial\ngains with a large margin almost on all these tasks compared with prior arts.\n", "versions": [{"version": "v1", "created": "Thu, 26 Nov 2020 15:43:27 GMT"}], "update_date": "2020-11-30", "authors_parsed": [["Chu", "Xiangxiang", ""], ["Zhan", "Xiaohang", ""], ["Wei", "Xiaolin", ""]]}, {"id": "2011.13360", "submitter": "Samadhi Poornima Kumarasinghe Wickrama Arachchilage", "authors": "S. W. Arachchilage, E. Izquierdo", "title": "ClusterFace: Joint Clustering and Classification for Set-Based Face\n  Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Deep learning technology has enabled successful modeling of complex facial\nfeatures when high quality images are available. Nonetheless, accurate modeling\nand recognition of human faces in real world scenarios `on the wild' or under\nadverse conditions remains an open problem. When unconstrained faces are mapped\ninto deep features, variations such as illumination, pose, occlusion, etc., can\ncreate inconsistencies in the resultant feature space. Hence, deriving\nconclusions based on direct associations could lead to degraded performance.\nThis rises the requirement for a basic feature space analysis prior to face\nrecognition. This paper devises a joint clustering and classification scheme\nwhich learns deep face associations in an easy-to-hard way. Our method is based\non hierarchical clustering where the early iterations tend to preserve high\nreliability. The rationale of our method is that a reliable clustering result\ncan provide insights on the distribution of the feature space, that can guide\nthe classification that follows. Experimental evaluations on three tasks, face\nverification, face identification and rank-order search, demonstrates better or\ncompetitive performance compared to the state-of-the-art, on all three\nexperiments.\n", "versions": [{"version": "v1", "created": "Thu, 26 Nov 2020 15:55:27 GMT"}], "update_date": "2020-11-30", "authors_parsed": [["Arachchilage", "S. W.", ""], ["Izquierdo", "E.", ""]]}, {"id": "2011.13361", "submitter": "Samadhi Poornima Kumarasinghe Wickrama Arachchilage", "authors": "S. W. Arachchilage, E. Izquierdo", "title": "SSDL: Self-Supervised Domain Learning for Improved Face Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Face recognition in unconstrained environments is challenging due to\nvariations in illumination, quality of sensing, motion blur and etc. An\nindividual's face appearance can vary drastically under different conditions\ncreating a gap between train (source) and varying test (target) data. The\ndomain gap could cause decreased performance levels in direct knowledge\ntransfer from source to target. Despite fine-tuning with domain specific data\ncould be an effective solution, collecting and annotating data for all domains\nis extremely expensive. To this end, we propose a self-supervised domain\nlearning (SSDL) scheme that trains on triplets mined from unlabelled data. A\nkey factor in effective discriminative learning, is selecting informative\ntriplets. Building on most confident predictions, we follow an \"easy-to-hard\"\nscheme of alternate triplet mining and self-learning. Comprehensive experiments\non four different benchmarks show that SSDL generalizes well on different\ndomains.\n", "versions": [{"version": "v1", "created": "Thu, 26 Nov 2020 15:55:59 GMT"}], "update_date": "2020-11-30", "authors_parsed": [["Arachchilage", "S. W.", ""], ["Izquierdo", "E.", ""]]}, {"id": "2011.13367", "submitter": "Adrien Deli\\`ege Mr", "authors": "Adrien Deli\\`ege, Anthony Cioppa, Silvio Giancola, Meisam J.\n  Seikavandi, Jacob V. Dueholm, Kamal Nasrollahi, Bernard Ghanem, Thomas B.\n  Moeslund, Marc Van Droogenbroeck", "title": "SoccerNet-v2: A Dataset and Benchmarks for Holistic Understanding of\n  Broadcast Soccer Videos", "comments": "Paper accepted for the CVsports workshop at CVPR2021. This document\n  contains 8 pages + references + supplementary material", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding broadcast videos is a challenging task in computer vision, as\nit requires generic reasoning capabilities to appreciate the content offered by\nthe video editing. In this work, we propose SoccerNet-v2, a novel large-scale\ncorpus of manual annotations for the SoccerNet video dataset, along with open\nchallenges to encourage more research in soccer understanding and broadcast\nproduction. Specifically, we release around 300k annotations within SoccerNet's\n500 untrimmed broadcast soccer videos. We extend current tasks in the realm of\nsoccer to include action spotting, camera shot segmentation with boundary\ndetection, and we define a novel replay grounding task. For each task, we\nprovide and discuss benchmark results, reproducible with our open-source\nadapted implementations of the most relevant works in the field. SoccerNet-v2\nis presented to the broader research community to help push computer vision\ncloser to automatic solutions for more general video understanding and\nproduction purposes.\n", "versions": [{"version": "v1", "created": "Thu, 26 Nov 2020 16:10:16 GMT"}, {"version": "v2", "created": "Wed, 17 Mar 2021 14:55:54 GMT"}, {"version": "v3", "created": "Mon, 19 Apr 2021 15:03:26 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Deli\u00e8ge", "Adrien", ""], ["Cioppa", "Anthony", ""], ["Giancola", "Silvio", ""], ["Seikavandi", "Meisam J.", ""], ["Dueholm", "Jacob V.", ""], ["Nasrollahi", "Kamal", ""], ["Ghanem", "Bernard", ""], ["Moeslund", "Thomas B.", ""], ["Van Droogenbroeck", "Marc", ""]]}, {"id": "2011.13371", "submitter": "Luojie Huang", "authors": "Luojie Huang, Gregory N. McKay, Nicholas J. Durr", "title": "A Deep Learning Bidirectional Temporal Tracking Algorithm for Automated\n  Blood Cell Counting from Non-invasive Capillaroscopy Videos", "comments": "10 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Oblique back-illumination capillaroscopy has recently been introduced as a\nmethod for high-quality, non-invasive blood cell imaging in human capillaries.\nTo make this technique practical for clinical blood cell counting, solutions\nfor automatic processing of acquired videos are needed. Here, we take the first\nstep towards this goal, by introducing a deep learning multi-cell tracking\nmodel, named CycleTrack, which achieves accurate blood cell counting from\ncapillaroscopic videos. CycleTrack combines two simple online tracking models,\nSORT and CenterTrack, and is tailored to features of capillary blood cell flow.\nBlood cells are tracked by displacement vectors in two opposing temporal\ndirections (forward- and backward-tracking) between consecutive frames. This\napproach yields accurate tracking despite rapidly moving and deforming blood\ncells. The proposed model outperforms other baseline trackers, achieving 65.57%\nMultiple Object Tracking Accuracy and 73.95% ID F1 score on test videos.\nCompared to manual blood cell counting, CycleTrack achieves 96.58 $\\pm$ 2.43%\ncell counting accuracy among 8 test videos with 1000 frames each compared to\n93.45% and 77.02% accuracy for independent CenterTrack and SORT almost without\nadditional time expense. It takes 800s to track and count approximately 8000\nblood cells from 9,600 frames captured in a typical one-minute video. Moreover,\nthe blood cell velocity measured by CycleTrack demonstrates a consistent,\npulsatile pattern within the physiological range of heart rate. Lastly, we\ndiscuss future improvements for the CycleTrack framework, which would enable\nclinical translation of the oblique back-illumination microscope towards a\nreal-time and non-invasive point-of-care blood cell counting and analyzing\ntechnology.\n", "versions": [{"version": "v1", "created": "Thu, 26 Nov 2020 16:27:13 GMT"}, {"version": "v2", "created": "Wed, 9 Dec 2020 18:34:39 GMT"}, {"version": "v3", "created": "Mon, 12 Apr 2021 12:47:35 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Huang", "Luojie", ""], ["McKay", "Gregory N.", ""], ["Durr", "Nicholas J.", ""]]}, {"id": "2011.13375", "submitter": "Ashish Hooda", "authors": "Athena Sayles, Ashish Hooda, Mohit Gupta, Rahul Chatterjee, Earlence\n  Fernandes", "title": "Invisible Perturbations: Physical Adversarial Examples Exploiting the\n  Rolling Shutter Effect", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CR cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Physical adversarial examples for camera-based computer vision have so far\nbeen achieved through visible artifacts -- a sticker on a Stop sign, colorful\nborders around eyeglasses or a 3D printed object with a colorful texture. An\nimplicit assumption here is that the perturbations must be visible so that a\ncamera can sense them. By contrast, we contribute a procedure to generate, for\nthe first time, physical adversarial examples that are invisible to human eyes.\nRather than modifying the victim object with visible artifacts, we modify light\nthat illuminates the object. We demonstrate how an attacker can craft a\nmodulated light signal that adversarially illuminates a scene and causes\ntargeted misclassifications on a state-of-the-art ImageNet deep learning model.\nConcretely, we exploit the radiometric rolling shutter effect in commodity\ncameras to create precise striping patterns that appear on images. To human\neyes, it appears like the object is illuminated, but the camera creates an\nimage with stripes that will cause ML models to output the attacker-desired\nclassification. We conduct a range of simulation and physical experiments with\nLEDs, demonstrating targeted attack rates up to 84%.\n", "versions": [{"version": "v1", "created": "Thu, 26 Nov 2020 16:34:47 GMT"}, {"version": "v2", "created": "Mon, 30 Nov 2020 02:36:24 GMT"}, {"version": "v3", "created": "Sun, 18 Apr 2021 16:21:42 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Sayles", "Athena", ""], ["Hooda", "Ashish", ""], ["Gupta", "Mohit", ""], ["Chatterjee", "Rahul", ""], ["Fernandes", "Earlence", ""]]}, {"id": "2011.13377", "submitter": "Linus Ericsson", "authors": "Linus Ericsson, Henry Gouk and Timothy M. Hospedales", "title": "How Well Do Self-Supervised Models Transfer?", "comments": "CVPR 2021. Code available at\n  https://github.com/linusericsson/ssl-transfer", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Self-supervised visual representation learning has seen huge progress\nrecently, but no large scale evaluation has compared the many models now\navailable. We evaluate the transfer performance of 13 top self-supervised\nmodels on 40 downstream tasks, including many-shot and few-shot recognition,\nobject detection, and dense prediction. We compare their performance to a\nsupervised baseline and show that on most tasks the best self-supervised models\noutperform supervision, confirming the recently observed trend in the\nliterature. We find ImageNet Top-1 accuracy to be highly correlated with\ntransfer to many-shot recognition, but increasingly less so for few-shot,\nobject detection and dense prediction. No single self-supervised method\ndominates overall, suggesting that universal pre-training is still unsolved.\nOur analysis of features suggests that top self-supervised learners fail to\npreserve colour information as well as supervised alternatives, but tend to\ninduce better classifier calibration, and less attentive overfitting than\nsupervised learners.\n", "versions": [{"version": "v1", "created": "Thu, 26 Nov 2020 16:38:39 GMT"}, {"version": "v2", "created": "Mon, 29 Mar 2021 13:20:03 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Ericsson", "Linus", ""], ["Gouk", "Henry", ""], ["Hospedales", "Timothy M.", ""]]}, {"id": "2011.13388", "submitter": "Mattia Segu", "authors": "Mattia Segu, Margarita Grinvald, Roland Siegwart, Federico Tombari", "title": "3DSNet: Unsupervised Shape-to-Shape 3D Style Transfer", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transferring the style from one image onto another is a popular and widely\nstudied task in computer vision. Yet, style transfer in the 3D setting remains\na largely unexplored problem. To our knowledge, we propose the first\nlearning-based approach for style transfer between 3D objects based on\ndisentangled content and style representations. The proposed method can\nsynthesize new 3D shapes both in the form of point clouds and meshes, combining\nthe content and style of a source and target 3D model to generate a novel shape\nthat resembles in style the target while retaining the source content.\nFurthermore, we extend our technique to implicitly learn the multimodal style\ndistribution of the chosen domains. By sampling style codes from the learned\ndistributions, we increase the variety of styles that our model can confer to\nan input shape. Experimental results validate the effectiveness of the proposed\n3D style transfer method on a number of benchmarks. The implementation of our\nframework will be released upon acceptance.\n", "versions": [{"version": "v1", "created": "Thu, 26 Nov 2020 16:59:12 GMT"}, {"version": "v2", "created": "Mon, 30 Nov 2020 08:45:55 GMT"}, {"version": "v3", "created": "Thu, 14 Jan 2021 09:29:34 GMT"}, {"version": "v4", "created": "Tue, 18 May 2021 09:17:13 GMT"}], "update_date": "2021-05-19", "authors_parsed": [["Segu", "Mattia", ""], ["Grinvald", "Margarita", ""], ["Siegwart", "Roland", ""], ["Tombari", "Federico", ""]]}, {"id": "2011.13391", "submitter": "Ulugbek Kamilov", "authors": "Mingyang Xie, Yu Sun, Jiaming Liu, Brendt Wohlberg, and Ulugbek S.\n  Kamilov", "title": "Joint Reconstruction and Calibration using Regularization by Denoising", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Regularization by denoising (RED) is a broadly applicable framework for\nsolving inverse problems by using priors specified as denoisers. While RED has\nbeen shown to provide state-of-the-art performance in a number of applications,\nexisting RED algorithms require exact knowledge of the measurement operator\ncharacterizing the imaging system, limiting their applicability in problems\nwhere the measurement operator has parametric uncertainties. We propose a new\nmethod, called Calibrated RED (Cal-RED), that enables joint calibration of the\nmeasurement operator along with reconstruction of the unknown image. Cal-RED\nextends the traditional RED methodology to imaging problems that require the\ncalibration of the measurement operator. We validate Cal-RED on the problem of\nimage reconstruction in computerized tomography (CT) under perturbed projection\nangles. Our results corroborate the effectiveness of Cal-RED for joint\ncalibration and reconstruction using pre-trained deep denoisers as image\npriors.\n", "versions": [{"version": "v1", "created": "Thu, 26 Nov 2020 17:05:56 GMT"}], "update_date": "2020-11-30", "authors_parsed": [["Xie", "Mingyang", ""], ["Sun", "Yu", ""], ["Liu", "Jiaming", ""], ["Wohlberg", "Brendt", ""], ["Kamilov", "Ulugbek S.", ""]]}, {"id": "2011.13392", "submitter": "Abhishek Moitra", "authors": "Abhishek Moitra and Priyadarshini Panda", "title": "Exposing the Robustness and Vulnerability of Hybrid 8T-6T SRAM Memory\n  Architectures to Adversarial Attacks in Deep Neural Networks", "comments": "11 pages, 13 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Learning is able to solve a plethora of once impossible problems.\nHowever, they are vulnerable to input adversarial attacks preventing them from\nbeing autonomously deployed in critical applications. Several\nalgorithm-centered works have discussed methods to cause adversarial attacks\nand improve adversarial robustness of a Deep Neural Network (DNN). In this\nwork, we elicit the advantages and vulnerabilities of hybrid 6T-8T memories to\nimprove the adversarial robustness and cause adversarial attacks on DNNs. We\nshow that bit-error noise in hybrid memories due to erroneous 6T-SRAM cells\nhave deterministic behaviour based on the hybrid memory configurations (V_DD,\n8T-6T ratio). This controlled noise (surgical noise) can be strategically\nintroduced into specific DNN layers to improve the adversarial accuracy of\nDNNs. At the same time, surgical noise can be carefully injected into the DNN\nparameters stored in hybrid memory to cause adversarial attacks. To improve the\nadversarial robustness of DNNs using surgical noise, we propose a methodology\nto select appropriate DNN layers and their corresponding hybrid memory\nconfigurations to introduce the required surgical noise. Using this, we achieve\n2-8% higher adversarial accuracy without re-training against white-box attacks\nlike FGSM, than the baseline models (with no surgical noise introduced). To\ndemonstrate adversarial attacks using surgical noise, we design a novel,\nwhite-box attack on DNN parameters stored in hybrid memory banks that causes\nthe DNN inference accuracy to drop by more than 60% with over 90% confidence\nvalue. We support our claims with experiments, performed using benchmark\ndatasets-CIFAR10 and CIFAR100 on VGG19 and ResNet18 networks.\n", "versions": [{"version": "v1", "created": "Thu, 26 Nov 2020 17:08:06 GMT"}], "update_date": "2020-11-30", "authors_parsed": [["Moitra", "Abhishek", ""], ["Panda", "Priyadarshini", ""]]}, {"id": "2011.13399", "submitter": "Mattia Segu", "authors": "Mattia Segu, Federico Pirovano, Gianmario Fumagalli, Amedeo Fabris", "title": "Depth-Aware Action Recognition: Pose-Motion Encoding through Temporal\n  Heatmaps", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most state-of-the-art methods for action recognition rely only on 2D spatial\nfeatures encoding appearance, motion or pose. However, 2D data lacks the depth\ninformation, which is crucial for recognizing fine-grained actions. In this\npaper, we propose a depth-aware volumetric descriptor that encodes pose and\nmotion information in a unified representation for action classification\nin-the-wild. Our framework is robust to many challenges inherent to action\nrecognition, e.g. variation in viewpoint, scene, clothing and body shape. The\nkey component of our method is the Depth-Aware Pose Motion representation\n(DA-PoTion), a new video descriptor that encodes the 3D movement of semantic\nkeypoints of the human body. Given a video, we produce human joint heatmaps for\neach frame using a state-of-the-art 3D human pose regressor and we give each of\nthem a unique color code according to the relative time in the clip. Then, we\naggregate such 3D time-encoded heatmaps for all human joints to obtain a\nfixed-size descriptor (DA-PoTion), which is suitable for classifying actions\nusing a shallow 3D convolutional neural network (CNN). The DA-PoTion alone\ndefines a new state-of-the-art on the Penn Action Dataset. Moreover, we\nleverage the intrinsic complementarity of our pose motion descriptor with\nappearance based approaches by combining it with Inflated 3D ConvNet (I3D) to\ndefine a new state-of-the-art on the JHMDB Dataset.\n", "versions": [{"version": "v1", "created": "Thu, 26 Nov 2020 17:26:42 GMT"}], "update_date": "2020-11-30", "authors_parsed": [["Segu", "Mattia", ""], ["Pirovano", "Federico", ""], ["Fumagalli", "Gianmario", ""], ["Fabris", "Amedeo", ""]]}, {"id": "2011.13406", "submitter": "Spencer Whitehead", "authors": "Spencer Whitehead, Hui Wu, Yi Ren Fung, Heng Ji, Rogerio Feris, Kate\n  Saenko", "title": "Learning from Lexical Perturbations for Consistent Visual Question\n  Answering", "comments": "14 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing Visual Question Answering (VQA) models are often fragile and\nsensitive to input variations. In this paper, we propose a novel approach to\naddress this issue based on modular networks, which creates two questions\nrelated by linguistic perturbations and regularizes the visual reasoning\nprocess between them to be consistent during training. We show that our\nframework markedly improves consistency and generalization ability,\ndemonstrating the value of controlled linguistic perturbations as a useful and\ncurrently underutilized training and regularization tool for VQA models. We\nalso present VQA Perturbed Pairings (VQA P2), a new, low-cost benchmark and\naugmentation pipeline to create controllable linguistic variations of VQA\nquestions. Our benchmark uniquely draws from large-scale linguistic resources,\navoiding human annotation effort while maintaining data quality compared to\ngenerative approaches. We benchmark existing VQA models using VQA P2 and\nprovide robustness analysis on each type of linguistic variation.\n", "versions": [{"version": "v1", "created": "Thu, 26 Nov 2020 17:38:03 GMT"}, {"version": "v2", "created": "Wed, 23 Dec 2020 00:29:27 GMT"}], "update_date": "2020-12-24", "authors_parsed": [["Whitehead", "Spencer", ""], ["Wu", "Hui", ""], ["Fung", "Yi Ren", ""], ["Ji", "Heng", ""], ["Feris", "Rogerio", ""], ["Saenko", "Kate", ""]]}, {"id": "2011.13417", "submitter": "Paul Guerrero", "authors": "Wamiq Para, Paul Guerrero, Tom Kelly, Leonidas Guibas, Peter Wonka", "title": "Generative Layout Modeling using Constraint Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new generative model for layout generation. We generate layouts\nin three steps. First, we generate the layout elements as nodes in a layout\ngraph. Second, we compute constraints between layout elements as edges in the\nlayout graph. Third, we solve for the final layout using constrained\noptimization. For the first two steps, we build on recent transformer\narchitectures. The layout optimization implements the constraints efficiently.\nWe show three practical contributions compared to the state of the art: our\nwork requires no user input, produces higher quality layouts, and enables many\nnovel capabilities for conditional layout generation.\n", "versions": [{"version": "v1", "created": "Thu, 26 Nov 2020 18:18:37 GMT"}], "update_date": "2020-11-30", "authors_parsed": [["Para", "Wamiq", ""], ["Guerrero", "Paul", ""], ["Kelly", "Tom", ""], ["Guibas", "Leonidas", ""], ["Wonka", "Peter", ""]]}, {"id": "2011.13427", "submitter": "Soyong Shin", "authors": "Soyong Shin, Eni Halilaj", "title": "Multi-view Human Pose and Shape Estimation Using Learnable Volumetric\n  Aggregation", "comments": "code will be released soon", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Human pose and shape estimation from RGB images is a highly sought after\nalternative to marker-based motion capture, which is laborious, requires\nexpensive equipment, and constrains capture to laboratory environments.\nMonocular vision-based algorithms, however, still suffer from rotational\nambiguities and are not ready for translation in healthcare applications, where\nhigh accuracy is paramount. While fusion of data from multiple viewpoints could\novercome these challenges, current algorithms require further improvement to\nobtain clinically acceptable accuracies. In this paper, we propose a learnable\nvolumetric aggregation approach to reconstruct 3D human body pose and shape\nfrom calibrated multi-view images. We use a parametric representation of the\nhuman body, which makes our approach directly applicable to medical\napplications. Compared to previous approaches, our framework shows higher\naccuracy and greater promise for real-time prediction, given its cost\nefficiency.\n", "versions": [{"version": "v1", "created": "Thu, 26 Nov 2020 18:33:35 GMT"}], "update_date": "2020-11-30", "authors_parsed": [["Shin", "Soyong", ""], ["Halilaj", "Eni", ""]]}, {"id": "2011.13429", "submitter": "Ihsan Ullah", "authors": "hsan Ullah, Andre Rios, Vaibhav Gala and Susan Mckeever", "title": "Explaining Deep Learning Models for Structured Data using Layer-Wise\n  Relevance Propagation", "comments": "13 pages, 5 figures, 6 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Trust and credibility in machine learning models is bolstered by the ability\nof a model to explain itsdecisions. While explainability of deep learning\nmodels is a well-known challenge, a further chal-lenge is clarity of the\nexplanation itself, which must be interpreted by downstream users.\nLayer-wiseRelevance Propagation (LRP), an established explainability technique\ndeveloped for deep models incomputer vision, provides intuitive human-readable\nheat maps of input images. We present the novelapplication of LRP for the first\ntime with structured datasets using a deep neural network (1D-CNN),for Credit\nCard Fraud detection and Telecom Customer Churn prediction datasets. We show\nhow LRPis more effective than traditional explainability concepts of Local\nInterpretable Model-agnostic Ex-planations (LIME) and Shapley Additive\nExplanations (SHAP) for explainability. This effectivenessis both local to a\nsample level and holistic over the whole testing set. We also discuss the\nsignificantcomputational time advantage of LRP (1-2s) over LIME (22s) and SHAP\n(108s), and thus its poten-tial for real time application scenarios. In\naddition, our validation of LRP has highlighted features forenhancing model\nperformance, thus opening up a new area of research of using XAI as an\napproachfor feature subset selection\n", "versions": [{"version": "v1", "created": "Thu, 26 Nov 2020 18:34:21 GMT"}], "update_date": "2020-11-30", "authors_parsed": [["Ullah", "hsan", ""], ["Rios", "Andre", ""], ["Gala", "Vaibhav", ""], ["Mckeever", "Susan", ""]]}, {"id": "2011.13475", "submitter": "Priyank Pathak", "authors": "Priyank Pathak", "title": "Fine-Grained Re-Identification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Research into the task of re-identification (ReID) is picking up momentum in\ncomputer vision for its many use cases and zero-shot learning nature. This\npaper proposes a computationally efficient fine-grained ReID model, FGReID,\nwhich is among the first models to unify image and video ReID while keeping the\nnumber of training parameters minimal. FGReID takes advantage of video-based\npre-training and spatial feature attention to improve performance on both video\nand image ReID tasks. FGReID achieves state-of-the-art (SOTA) on MARS,\niLIDS-VID, and PRID-2011 video person ReID benchmarks. Eliminating temporal\npooling yields an image ReID model that surpasses SOTA on CUHK01 and Market1501\nimage person ReID benchmarks. The FGReID achieves near SOTA performance on the\nvehicle ReID dataset VeRi as well, demonstrating its ability to generalize.\nAdditionally we do an ablation study analyzing the key features influencing\nmodel performance on ReID tasks. Finally, we discuss the moral dilemmas related\nto ReID tasks, including the potential for misuse. Code for this work is\npublicly available at https:\n//github.com/ppriyank/Fine-grained-ReIdentification.\n", "versions": [{"version": "v1", "created": "Thu, 26 Nov 2020 21:04:17 GMT"}, {"version": "v2", "created": "Mon, 22 Mar 2021 17:06:19 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Pathak", "Priyank", ""]]}, {"id": "2011.13484", "submitter": "Matthias Wilms", "authors": "Matthias Wilms and Jordan J. Bannister and Pauline Mouches and M.\n  Ethan MacDonald and Deepthi Rajashekar and S\\\"onke Langner and Nils D.\n  Forkert", "title": "Bidirectional Modeling and Analysis of Brain Aging with Normalizing\n  Flows", "comments": "Presented at 3rd International Workshop on Machine Learning in\n  Clinical Neuroimaging - in conjunction with MICCAI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Brain aging is a widely studied longitudinal process throughout which the\nbrain undergoes considerable morphological changes and various machine learning\napproaches have been proposed to analyze it. Within this context, brain age\nprediction from structural MR images and age-specific brain morphology template\ngeneration are two problems that have attracted much attention. While most\napproaches tackle these tasks independently, we assume that they are inverse\ndirections of the same functional bidirectional relationship between a brain's\nmorphology and an age variable. In this paper, we propose to model this\nrelationship with a single conditional normalizing flow, which unifies brain\nage prediction and age-conditioned generative modeling in a novel way. In an\ninitial evaluation of this idea, we show that our normalizing flow brain aging\nmodel can accurately predict brain age while also being able to generate\nage-specific brain morphology templates that realistically represent the\ntypical aging trend in a healthy population. This work is a step towards\nunified modeling of functional relationships between 3D brain morphology and\nclinical variables of interest with powerful normalizing flows.\n", "versions": [{"version": "v1", "created": "Thu, 26 Nov 2020 22:23:48 GMT"}], "update_date": "2020-11-30", "authors_parsed": [["Wilms", "Matthias", ""], ["Bannister", "Jordan J.", ""], ["Mouches", "Pauline", ""], ["MacDonald", "M. Ethan", ""], ["Rajashekar", "Deepthi", ""], ["Langner", "S\u00f6nke", ""], ["Forkert", "Nils D.", ""]]}, {"id": "2011.13495", "submitter": "Zhizhong Han", "authors": "Baorui Ma and Zhizhong Han and Yu-Shen Liu and Matthias Zwicker", "title": "Neural-Pull: Learning Signed Distance Functions from Point Clouds by\n  Learning to Pull Space onto Surfaces", "comments": "To appear at ICML2021. Code and data are available at\n  https://github.com/mabaorui/NeuralPull", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reconstructing continuous surfaces from 3D point clouds is a fundamental\noperation in 3D geometry processing. Several recent state-of-the-art methods\naddress this problem using neural networks to learn signed distance functions\n(SDFs). In this paper, we introduce \\textit{Neural-Pull}, a new approach that\nis simple and leads to high quality SDFs. Specifically, we train a neural\nnetwork to pull query 3D locations to their closest points on the surface using\nthe predicted signed distance values and the gradient at the query locations,\nboth of which are computed by the network itself. The pulling operation moves\neach query location with a stride given by the distance predicted by the\nnetwork. Based on the sign of the distance, this may move the query location\nalong or against the direction of the gradient of the SDF. This is a\ndifferentiable operation that allows us to update the signed distance value and\nthe gradient simultaneously during training. Our outperforming results under\nwidely used benchmarks demonstrate that we can learn SDFs more accurately and\nflexibly for surface reconstruction and single image reconstruction than the\nstate-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Thu, 26 Nov 2020 23:18:10 GMT"}, {"version": "v2", "created": "Sun, 23 May 2021 17:54:34 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Ma", "Baorui", ""], ["Han", "Zhizhong", ""], ["Liu", "Yu-Shen", ""], ["Zwicker", "Matthias", ""]]}, {"id": "2011.13509", "submitter": "Junghyo Jo", "authors": "Juno Hwang and Wonseok Hwang and Junghyo Jo", "title": "Tractable loss function and color image generation of multinary\n  restricted Boltzmann machine", "comments": "NueRIPS 2020 DiffCVGP workshop paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The restricted Boltzmann machine (RBM) is a representative generative model\nbased on the concept of statistical mechanics. In spite of the strong merit of\ninterpretability, unavailability of backpropagation makes it less competitive\nthan other generative models. Here we derive differentiable loss functions for\nboth binary and multinary RBMs. Then we demonstrate their learnability and\nperformance by generating colored face images.\n", "versions": [{"version": "v1", "created": "Fri, 27 Nov 2020 00:50:59 GMT"}], "update_date": "2020-11-30", "authors_parsed": [["Hwang", "Juno", ""], ["Hwang", "Wonseok", ""], ["Jo", "Junghyo", ""]]}, {"id": "2011.13526", "submitter": "Meng Shen", "authors": "Meng Shen, Hao Yu, Liehuang Zhu, Ke Xu, Qi Li, Xiaojiang Du", "title": "Robust Attacks on Deep Learning Face Recognition in the Physical World", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks (DNNs) have been increasingly used in face recognition\n(FR) systems. Recent studies, however, show that DNNs are vulnerable to\nadversarial examples, which can potentially mislead the FR systems using DNNs\nin the physical world. Existing attacks on these systems either generate\nperturbations working merely in the digital world, or rely on customized\nequipments to generate perturbations and are not robust in varying physical\nenvironments. In this paper, we propose FaceAdv, a physical-world attack that\ncrafts adversarial stickers to deceive FR systems. It mainly consists of a\nsticker generator and a transformer, where the former can craft several\nstickers with different shapes and the latter transformer aims to digitally\nattach stickers to human faces and provide feedbacks to the generator to\nimprove the effectiveness of stickers. We conduct extensive experiments to\nevaluate the effectiveness of FaceAdv on attacking 3 typical FR systems (i.e.,\nArcFace, CosFace and FaceNet). The results show that compared with a\nstate-of-the-art attack, FaceAdv can significantly improve success rate of both\ndodging and impersonating attacks. We also conduct comprehensive evaluations to\ndemonstrate the robustness of FaceAdv.\n", "versions": [{"version": "v1", "created": "Fri, 27 Nov 2020 02:24:43 GMT"}], "update_date": "2020-11-30", "authors_parsed": [["Shen", "Meng", ""], ["Yu", "Hao", ""], ["Zhu", "Liehuang", ""], ["Xu", "Ke", ""], ["Li", "Qi", ""], ["Du", "Xiaojiang", ""]]}, {"id": "2011.13528", "submitter": "Hongli Song", "authors": "Lichao Wang, Lanxin Lei, Hongli Song, Weibao Wang", "title": "The NEOLIX Open Dataset for Autonomous Driving", "comments": "7 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  With the gradual maturity of 5G technology,autonomous driving technology has\nattracted moreand more attention among the research commu-nity. Autonomous\ndriving vehicles rely on the co-operation of artificial intelligence, visual\ncomput-ing, radar, monitoring equipment and GPS, whichenables computers to\noperate motor vehicles auto-matically and safely without human\ninterference.However, the large-scale dataset for training andsystem evaluation\nis still a hot potato in the devel-opment of robust perception models. In this\npaper,we present the NEOLIX dataset and its applica-tions in the autonomous\ndriving area. Our datasetincludes about 30,000 frames with point cloud la-bels,\nand more than 600k 3D bounding boxes withannotations. The data collection\ncovers multipleregions, and various driving conditions, includingday, night,\ndawn, dusk and sunny day. In orderto label this complete dataset, we developed\nvari-ous tools and algorithms specified for each task tospeed up the labelling\nprocess. It is expected thatour dataset and related algorithms can support\nandmotivate researchers for the further developmentof autonomous driving in the\nfield of computer vi-sion.\n", "versions": [{"version": "v1", "created": "Fri, 27 Nov 2020 02:27:39 GMT"}, {"version": "v2", "created": "Thu, 28 Jan 2021 06:41:15 GMT"}], "update_date": "2021-01-29", "authors_parsed": [["Wang", "Lichao", ""], ["Lei", "Lanxin", ""], ["Song", "Hongli", ""], ["Wang", "Weibao", ""]]}, {"id": "2011.13529", "submitter": "Huang Zhuo", "authors": "Zhuo Huang, Ying Tai, Chengjie Wang, Jian Yang, Chen Gong", "title": "They are Not Completely Useless: Towards Recycling Transferable\n  Unlabeled Data for Class-Mismatched Semi-Supervised Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semi-Supervised Learning (SSL) with mismatched classes deals with the problem\nthat the classes-of-interests in the limited labeled data is only a subset of\nthe classes in massive unlabeled data. As a result, the classes only possessed\nby the unlabeled data may mislead the classifier training and thus hindering\nthe realistic landing of various SSL methods. To solve this problem, existing\nmethods usually divide unlabeled data to in-distribution (ID) data and\nout-of-distribution (OOD) data, and directly discard or weaken the OOD data to\navoid their adverse impact. In other words, they treat OOD data as completely\nuseless and thus the potential valuable information for classification\ncontained by them is totally ignored. To remedy this defect, this paper\nproposes a \"Transferable OOD data Recycling\" (TOOR) method which properly\nutilizes ID data as well as the \"recyclable\" OOD data to enrich the information\nfor conducting class-mismatched SSL. Specifically, TOOR firstly attributes all\nunlabeled data to ID data or OOD data, among which the ID data are directly\nused for training. Then we treat the OOD data that have a close relationship\nwith ID data and labeled data as recyclable, and employ adversarial domain\nadaptation to project them to the space of ID data and labeled data. In other\nwords, the recyclability of an OOD datum is evaluated by its transferability,\nand the recyclable OOD data are transferred so that they are compatible with\nthe distribution of known classes-of-interests. Consequently, our TOOR method\nextracts more information from unlabeled data than existing approaches, so it\ncan achieve the improved performance which is demonstrated by the experiments\non typical benchmark datasets.\n", "versions": [{"version": "v1", "created": "Fri, 27 Nov 2020 02:29:35 GMT"}, {"version": "v2", "created": "Wed, 6 Jan 2021 08:59:19 GMT"}, {"version": "v3", "created": "Tue, 30 Mar 2021 01:28:31 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Huang", "Zhuo", ""], ["Tai", "Ying", ""], ["Wang", "Chengjie", ""], ["Yang", "Jian", ""], ["Gong", "Chen", ""]]}, {"id": "2011.13534", "submitter": "Nishant Subramani", "authors": "Nishant Subramani and Alexandre Matton and Malcolm Greaves and Adrian\n  Lam", "title": "A Survey of Deep Learning Approaches for OCR and Document Understanding", "comments": "Accepted to the ML-RSA Workshop at NeurIPS2020. 15 pages (10 +\n  References)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV cs.IR cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Documents are a core part of many businesses in many fields such as law,\nfinance, and technology among others. Automatic understanding of documents such\nas invoices, contracts, and resumes is lucrative, opening up many new avenues\nof business. The fields of natural language processing and computer vision have\nseen tremendous progress through the development of deep learning such that\nthese methods have started to become infused in contemporary document\nunderstanding systems. In this survey paper, we review different techniques for\ndocument understanding for documents written in English and consolidate\nmethodologies present in literature to act as a jumping-off point for\nresearchers exploring this area.\n", "versions": [{"version": "v1", "created": "Fri, 27 Nov 2020 03:05:59 GMT"}, {"version": "v2", "created": "Thu, 4 Feb 2021 23:48:39 GMT"}], "update_date": "2021-02-08", "authors_parsed": [["Subramani", "Nishant", ""], ["Matton", "Alexandre", ""], ["Greaves", "Malcolm", ""], ["Lam", "Adrian", ""]]}, {"id": "2011.13544", "submitter": "Zhenqiang Ying", "authors": "Zhenqiang Ying (1), Maniratnam Mandal (1), Deepti Ghadiyaram (2), Alan\n  Bovik (1) ((1) University of Texas at Austin, (2) Facebook AI)", "title": "Patch-VQ: 'Patching Up' the Video Quality Problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  No-reference (NR) perceptual video quality assessment (VQA) is a complex,\nunsolved, and important problem to social and streaming media applications.\nEfficient and accurate video quality predictors are needed to monitor and guide\nthe processing of billions of shared, often imperfect, user-generated content\n(UGC). Unfortunately, current NR models are limited in their prediction\ncapabilities on real-world, \"in-the-wild\" UGC video data. To advance progress\non this problem, we created the largest (by far) subjective video quality\ndataset, containing 39, 000 realworld distorted videos and 117, 000 space-time\nlocalized video patches ('v-patches'), and 5.5M human perceptual quality\nannotations. Using this, we created two unique NR-VQA models: (a) a\nlocal-to-global region-based NR VQA architecture (called PVQ) that learns to\npredict global video quality and achieves state-of-the-art performance on 3 UGC\ndatasets, and (b) a first-of-a-kind space-time video quality mapping engine\n(called PVQ Mapper) that helps localize and visualize perceptual distortions in\nspace and time. We will make the new database and prediction models available\nimmediately following the review process.\n", "versions": [{"version": "v1", "created": "Fri, 27 Nov 2020 03:46:44 GMT"}], "update_date": "2020-11-30", "authors_parsed": [["Ying", "Zhenqiang", "", "University of Texas at Austin"], ["Mandal", "Maniratnam", "", "University of Texas at Austin"], ["Ghadiyaram", "Deepti", "", "Facebook AI"], ["Bovik", "Alan", "", "University of Texas at Austin"]]}, {"id": "2011.13553", "submitter": "Yi Gu", "authors": "Yi Gu, Jie Li, Yuting Gao, Ruoxin Chen, Chentao Wu, Feiyang Cai, Chao\n  Wang, Zirui Zhang", "title": "Association: Remind Your GAN not to Forget", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural networks are susceptible to catastrophic forgetting. They fail to\npreserve previously acquired knowledge when adapting to new tasks. Inspired by\nhuman associative memory system, we propose a brain-like approach that imitates\nthe associative learning process to achieve continual learning. We design a\nheuristics mechanism to potentiatively stimulate the model, which guides the\nmodel to recall the historical episodes based on the current circumstance and\nobtained association experience. Besides, a distillation measure is added to\ndepressively alter the efficacy of synaptic transmission, which dampens the\nfeature reconstruction learning for new task. The framework is mediated by\npotentiation and depression stimulation that play opposing roles in directing\nsynaptic and behavioral plasticity. It requires no access to the original data\nand is more similar to human cognitive process. Experiments demonstrate the\neffectiveness of our method in alleviating catastrophic forgetting on\nimage-to-image translation tasks.\n", "versions": [{"version": "v1", "created": "Fri, 27 Nov 2020 04:43:15 GMT"}, {"version": "v2", "created": "Thu, 25 Mar 2021 09:35:00 GMT"}], "update_date": "2021-03-26", "authors_parsed": [["Gu", "Yi", ""], ["Li", "Jie", ""], ["Gao", "Yuting", ""], ["Chen", "Ruoxin", ""], ["Wu", "Chentao", ""], ["Cai", "Feiyang", ""], ["Wang", "Chao", ""], ["Zhang", "Zirui", ""]]}, {"id": "2011.13560", "submitter": "Mingfu Xue", "authors": "Mingfu Xue, Shichang Sun, Zhiyu Wu, Can He, Jian Wang, Weiqiang Liu", "title": "SocialGuard: An Adversarial Example Based Privacy-Preserving Technique\n  for Social Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The popularity of various social platforms has prompted more people to share\ntheir routine photos online. However, undesirable privacy leakages occur due to\nsuch online photo sharing behaviors. Advanced deep neural network (DNN) based\nobject detectors can easily steal users' personal information exposed in shared\nphotos. In this paper, we propose a novel adversarial example based\nprivacy-preserving technique for social images against object detectors based\nprivacy stealing. Specifically, we develop an Object Disappearance Algorithm to\ncraft two kinds of adversarial social images. One can hide all objects in the\nsocial images from being detected by an object detector, and the other can make\nthe customized sensitive objects be incorrectly classified by the object\ndetector. The Object Disappearance Algorithm constructs perturbation on a clean\nsocial image. After being injected with the perturbation, the social image can\neasily fool the object detector, while its visual quality will not be degraded.\nWe use two metrics, privacy-preserving success rate and privacy leakage rate,\nto evaluate the effectiveness of the proposed method. Experimental results show\nthat, the proposed method can effectively protect the privacy of social images.\nThe privacy-preserving success rates of the proposed method on MS-COCO and\nPASCAL VOC 2007 datasets are high up to 96.1% and 99.3%, respectively, and the\nprivacy leakage rates on these two datasets are as low as 0.57% and 0.07%,\nrespectively. In addition, compared with existing image processing methods (low\nbrightness, noise, blur, mosaic and JPEG compression), the proposed method can\nachieve much better performance in privacy protection and image visual quality\nmaintenance.\n", "versions": [{"version": "v1", "created": "Fri, 27 Nov 2020 05:12:47 GMT"}], "update_date": "2020-11-30", "authors_parsed": [["Xue", "Mingfu", ""], ["Sun", "Shichang", ""], ["Wu", "Zhiyu", ""], ["He", "Can", ""], ["Wang", "Jian", ""], ["Liu", "Weiqiang", ""]]}, {"id": "2011.13580", "submitter": "Chuan-Shen Hu", "authors": "Chuan-Shen Hu, Yu-Min Chung", "title": "A Sheaf and Topology Approach to Generating Local Branch Numbers in\n  Digital Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV math.AT", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  This paper concerns a theoretical approach that combines topological data\nanalysis (TDA) and sheaf theory. Topological data analysis, a rising field in\nmathematics and computer science, concerns the shape of the data and has been\nproven effective in many scientific disciplines. Sheaf theory, a mathematics\nsubject in algebraic geometry, provides a framework for describing the local\nconsistency in geometric objects. Persistent homology (PH) is one of the main\ndriving forces in TDA, and the idea is to track changes of geometric objects at\ndifferent scales. The persistence diagram (PD) summarizes the information of PH\nin the form of a multi-set. While PD provides useful information about the\nunderlying objects, it lacks fine relations about the local consistency of\nspecific pairs of generators in PD, such as the merging relation between two\nconnected components in the PH. The sheaf structure provides a novel point of\nview for describing the merging relation of local objects in PH. It is the goal\nof this paper to establish a theoretic framework that utilizes the sheaf theory\nto uncover finer information from the PH. We also show that the proposed theory\ncan be applied to identify the branch numbers of local objects in digital\nimages.\n", "versions": [{"version": "v1", "created": "Fri, 27 Nov 2020 06:50:59 GMT"}, {"version": "v2", "created": "Thu, 3 Dec 2020 02:43:38 GMT"}], "update_date": "2020-12-04", "authors_parsed": [["Hu", "Chuan-Shen", ""], ["Chung", "Yu-Min", ""]]}, {"id": "2011.13583", "submitter": "Apoorv Khandelwal", "authors": "Margot Hanley, Apoorv Khandelwal, Hadar Averbuch-Elor, Noah Snavely\n  and Helen Nissenbaum", "title": "An Ethical Highlighter for People-Centric Dataset Creation", "comments": "Part of the Navigating the Broader Impacts of AI Research Workshop at\n  NeurIPS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.CV cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Important ethical concerns arising from computer vision datasets of people\nhave been receiving significant attention, and a number of datasets have been\nwithdrawn as a result. To meet the academic need for people-centric datasets,\nwe propose an analytical framework to guide ethical evaluation of existing\ndatasets and to serve future dataset creators in avoiding missteps. Our work is\ninformed by a review and analysis of prior works and highlights where such\nethical challenges arise.\n", "versions": [{"version": "v1", "created": "Fri, 27 Nov 2020 07:18:44 GMT"}], "update_date": "2020-11-30", "authors_parsed": [["Hanley", "Margot", ""], ["Khandelwal", "Apoorv", ""], ["Averbuch-Elor", "Hadar", ""], ["Snavely", "Noah", ""], ["Nissenbaum", "Helen", ""]]}, {"id": "2011.13588", "submitter": "Yafu Tian", "authors": "Yafu Tian, Alexander Carballo, Ruifeng Li and Kazuya Takeda", "title": "Road Scene Graph: A Semantic Graph-Based Scene Representation Dataset\n  for Intelligent Vehicles", "comments": "8 pages, 8 figures, under review ICRA 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Rich semantic information extraction plays a vital role on next-generation\nintelligent vehicles. Currently there is great amount of research focusing on\nfundamental applications such as 6D pose detection, road scene semantic\nsegmentation, etc. And this provides us a great opportunity to think about how\nshall these data be organized and exploited.\n  In this paper we propose road scene graph,a special scene-graph for\nintelligent vehicles. Different to classical data representation, this graph\nprovides not only object proposals but also their pair-wise relationships. By\norganizing them in a topological graph, these data are explainable,\nfully-connected, and could be easily processed by GCNs (Graph Convolutional\nNetworks). Here we apply scene graph on roads using our Road Scene Graph\ndataset, including the basic graph prediction model. This work also includes\nexperimental evaluations using the proposed model.\n", "versions": [{"version": "v1", "created": "Fri, 27 Nov 2020 07:33:11 GMT"}], "update_date": "2020-11-30", "authors_parsed": [["Tian", "Yafu", ""], ["Carballo", "Alexander", ""], ["Li", "Ruifeng", ""], ["Takeda", "Kazuya", ""]]}, {"id": "2011.13591", "submitter": "Shengran Hu", "authors": "Shengran Hu, Ran Cheng, Cheng He, Zhichao Lu", "title": "Multi-objective Neural Architecture Search with Almost No Training", "comments": "EMO 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In the recent past, neural architecture search (NAS) has attracted increasing\nattention from both academia and industries. Despite the steady stream of\nimpressive empirical results, most existing NAS algorithms are computationally\nprohibitive to execute due to the costly iterations of stochastic gradient\ndescent (SGD) training. In this work, we propose an effective alternative,\ndubbed Random-Weight Evaluation (RWE), to rapidly estimate the performance of\nnetwork architectures. By just training the last linear classification layer,\nRWE reduces the computational cost of evaluating an architecture from hours to\nseconds. When integrated within an evolutionary multi-objective algorithm, RWE\nobtains a set of efficient architectures with state-of-the-art performance on\nCIFAR-10 with less than two hours' searching on a single GPU card. Ablation\nstudies on rank-order correlations and transfer learning experiments to\nImageNet have further validated the effectiveness of RWE.\n", "versions": [{"version": "v1", "created": "Fri, 27 Nov 2020 07:39:17 GMT"}], "update_date": "2020-11-30", "authors_parsed": [["Hu", "Shengran", ""], ["Cheng", "Ran", ""], ["He", "Cheng", ""], ["Lu", "Zhichao", ""]]}, {"id": "2011.13607", "submitter": "Frank Yu", "authors": "Frank Yu, Mathieu Salzmann, Pascal Fua, Helge Rhodin", "title": "PCLs: Geometry-aware Neural Reconstruction of 3D Pose with Perspective\n  Crop Layers", "comments": "CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Local processing is an essential feature of CNNs and other neural network\narchitectures - it is one of the reasons why they work so well on images where\nrelevant information is, to a large extent, local. However, perspective effects\nstemming from the projection in a conventional camera vary for different global\npositions in the image. We introduce Perspective Crop Layers (PCLs) - a form of\nperspective crop of the region of interest based on the camera geometry - and\nshow that accounting for the perspective consistently improves the accuracy of\nstate-of-the-art 3D pose reconstruction methods. PCLs are modular neural\nnetwork layers, which, when inserted into existing CNN and MLP architectures,\ndeterministically remove the location-dependent perspective effects while\nleaving end-to-end training and the number of parameters of the underlying\nneural network unchanged. We demonstrate that PCL leads to improved 3D human\npose reconstruction accuracy for CNN architectures that use cropping\noperations, such as spatial transformer networks (STN), and, somewhat\nsurprisingly, MLPs used for 2D-to-3D keypoint lifting. Our conclusion is that\nit is important to utilize camera calibration information when available, for\nclassical and deep-learning-based computer vision alike. PCL offers an easy way\nto improve the accuracy of existing 3D reconstruction networks by making them\ngeometry aware. Our code is publicly available at\ngithub.com/yu-frank/PerspectiveCropLayers.\n", "versions": [{"version": "v1", "created": "Fri, 27 Nov 2020 08:48:43 GMT"}, {"version": "v2", "created": "Thu, 15 Apr 2021 17:39:07 GMT"}], "update_date": "2021-04-16", "authors_parsed": [["Yu", "Frank", ""], ["Salzmann", "Mathieu", ""], ["Fua", "Pascal", ""], ["Rhodin", "Helge", ""]]}, {"id": "2011.13611", "submitter": "Mu Cai", "authors": "Mu Cai, Hong Zhang, Huijuan Huang, Qichuan Geng, Gao Huang", "title": "Frequency Domain Image Translation: More Photo-realistic, Better\n  Identity-preserving", "comments": "13 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image-to-image translation aims at translating a particular style of an image\nto another. The synthesized images can be more photo-realistic and\nidentity-preserving by decomposing the image into content and style in a\ndisentangled manner. While existing models focus on designing specialized\nnetwork architecture to separate the two components, this paper investigates\nhow to explicitly constrain the content and style statistics of images. We\nachieve this goal by transforming the input image into high frequency and low\nfrequency information, which correspond to the content and style, respectively.\nWe regulate the frequency distribution from two aspects: a) a spatial level\nrestriction to locally restrict the frequency distribution of images; b) a\nspectral level regulation to enhance the global consistency among images. On\nmultiple datasets we show that the proposed approach consistently leads to\nsignificant improvements on top of various state-of-the-art image translation\nmodels.\n", "versions": [{"version": "v1", "created": "Fri, 27 Nov 2020 08:58:56 GMT"}, {"version": "v2", "created": "Tue, 1 Dec 2020 04:19:17 GMT"}], "update_date": "2020-12-02", "authors_parsed": [["Cai", "Mu", ""], ["Zhang", "Hong", ""], ["Huang", "Huijuan", ""], ["Geng", "Qichuan", ""], ["Huang", "Gao", ""]]}, {"id": "2011.13614", "submitter": "Shanshan Wang", "authors": "Kehan Qi, Yu Gong, Xinfeng Liu, Xin Liu, Hairong Zheng, Shanshan Wang", "title": "Multi-task MR Imaging with Iterative Teacher Forcing and Re-weighted\n  Deep Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Noises, artifacts, and loss of information caused by the magnetic resonance\n(MR) reconstruction may compromise the final performance of the downstream\napplications. In this paper, we develop a re-weighted multi-task deep learning\nmethod to learn prior knowledge from the existing big dataset and then utilize\nthem to assist simultaneous MR reconstruction and segmentation from the\nunder-sampled k-space data. The multi-task deep learning framework is equipped\nwith two network sub-modules, which are integrated and trained by our designed\niterative teacher forcing scheme (ITFS) under the dynamic re-weighted loss\nconstraint (DRLC). The ITFS is designed to avoid error accumulation by\ninjecting the fully-sampled data into the training process. The DRLC is\nproposed to dynamically balance the contributions from the reconstruction and\nsegmentation sub-modules so as to co-prompt the multi-task accuracy. The\nproposed method has been evaluated on two open datasets and one in vivo\nin-house dataset and compared to six state-of-the-art methods. Results show\nthat the proposed method possesses encouraging capabilities for simultaneous\nand accurate MR reconstruction and segmentation.\n", "versions": [{"version": "v1", "created": "Fri, 27 Nov 2020 09:08:05 GMT"}], "update_date": "2020-11-30", "authors_parsed": [["Qi", "Kehan", ""], ["Gong", "Yu", ""], ["Liu", "Xinfeng", ""], ["Liu", "Xin", ""], ["Zheng", "Hairong", ""], ["Wang", "Shanshan", ""]]}, {"id": "2011.13615", "submitter": "Siyu Liu", "authors": "Siyu Liu, Jason A. Dowling, Craig Engstrom, Peter B. Greer, Stuart\n  Crozier, Shekhar S. Chandra", "title": "Manipulating Medical Image Translation with Manifold Disentanglement", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Medical image translation (e.g. CT to MR) is a challenging task as it\nrequires I) faithful translation of domain-invariant features (e.g. shape\ninformation of anatomical structures) and II) realistic synthesis of\ntarget-domain features (e.g. tissue appearance in MR). In this work, we propose\nManifold Disentanglement Generative Adversarial Network (MDGAN), a novel image\ntranslation framework that explicitly models these two types of features. It\nemploys a fully convolutional generator to model domain-invariant features, and\nit uses style codes to separately model target-domain features as a manifold.\nThis design aims to explicitly disentangle domain-invariant features and\ndomain-specific features while gaining individual control of both. The image\ntranslation process is formulated as a stylisation task, where the input is\n\"stylised\" (translated) into diverse target-domain images based on style codes\nsampled from the learnt manifold. We test MDGAN for multi-modal medical image\ntranslation, where we create two domain-specific manifold clusters on the\nmanifold to translate segmentation maps into pseudo-CT and pseudo-MR images,\nrespectively. We show that by traversing a path across the MR manifold cluster,\nthe target output can be manipulated while still retaining the shape\ninformation from the input.\n", "versions": [{"version": "v1", "created": "Fri, 27 Nov 2020 09:11:52 GMT"}], "update_date": "2020-11-30", "authors_parsed": [["Liu", "Siyu", ""], ["Dowling", "Jason A.", ""], ["Engstrom", "Craig", ""], ["Greer", "Peter B.", ""], ["Crozier", "Stuart", ""], ["Chandra", "Shekhar S.", ""]]}, {"id": "2011.13628", "submitter": "Xiao Song", "authors": "Zhenxun Yuan, Xiao Song, Lei Bai, Wengang Zhou, Zhe Wang, Wanli Ouyang", "title": "Temporal-Channel Transformer for 3D Lidar-Based Video Object Detection\n  in Autonomous Driving", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The strong demand of autonomous driving in the industry has lead to strong\ninterest in 3D object detection and resulted in many excellent 3D object\ndetection algorithms. However, the vast majority of algorithms only model\nsingle-frame data, ignoring the temporal information of the sequence of data.\nIn this work, we propose a new transformer, called Temporal-Channel\nTransformer, to model the spatial-temporal domain and channel domain\nrelationships for video object detecting from Lidar data. As a special design\nof this transformer, the information encoded in the encoder is different from\nthat in the decoder, i.e. the encoder encodes temporal-channel information of\nmultiple frames while the decoder decodes the spatial-channel information for\nthe current frame in a voxel-wise manner. Specifically, the temporal-channel\nencoder of the transformer is designed to encode the information of different\nchannels and frames by utilizing the correlation among features from different\nchannels and frames. On the other hand, the spatial decoder of the transformer\nwill decode the information for each location of the current frame. Before\nconducting the object detection with detection head, the gate mechanism is\ndeployed for re-calibrating the features of current frame, which filters out\nthe object irrelevant information by repetitively refine the representation of\ntarget frame along with the up-sampling process. Experimental results show that\nwe achieve the state-of-the-art performance in grid voxel-based 3D object\ndetection on the nuScenes benchmark.\n", "versions": [{"version": "v1", "created": "Fri, 27 Nov 2020 09:35:39 GMT"}], "update_date": "2020-11-30", "authors_parsed": [["Yuan", "Zhenxun", ""], ["Song", "Xiao", ""], ["Bai", "Lei", ""], ["Zhou", "Wengang", ""], ["Wang", "Zhe", ""], ["Ouyang", "Wanli", ""]]}, {"id": "2011.13649", "submitter": "Fumio Okura", "authors": "Takuma Doi, Fumio Okura, Toshiki Nagahara, Yasuyuki Matsushita,\n  Yasushi Yagi", "title": "Descriptor-Free Multi-View Region Matching for Instance-Wise 3D\n  Reconstruction", "comments": "ACCV2020 Oral", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper proposes a multi-view extension of instance segmentation without\nrelying on texture or shape descriptor matching. Multi-view instance\nsegmentation becomes challenging for scenes with repetitive textures and\nshapes, e.g., plant leaves, due to the difficulty of multi-view matching using\ntexture or shape descriptors. To this end, we propose a multi-view region\nmatching method based on epipolar geometry, which does not rely on any feature\ndescriptors. We further show that the epipolar region matching can be easily\nintegrated into instance segmentation and effective for instance-wise 3D\nreconstruction. Experiments demonstrate the improved accuracy of multi-view\ninstance matching and the 3D reconstruction compared to the baseline methods.\n", "versions": [{"version": "v1", "created": "Fri, 27 Nov 2020 10:45:18 GMT"}], "update_date": "2020-11-30", "authors_parsed": [["Doi", "Takuma", ""], ["Okura", "Fumio", ""], ["Nagahara", "Toshiki", ""], ["Matsushita", "Yasuyuki", ""], ["Yagi", "Yasushi", ""]]}, {"id": "2011.13650", "submitter": "Yu Deng", "authors": "Yu Deng, Jiaolong Yang, Xin Tong", "title": "Deformed Implicit Field: Modeling 3D Shapes with Learned Dense\n  Correspondence", "comments": "CVPR21 camera ready version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel Deformed Implicit Field (DIF) representation for modeling\n3D shapes of a category and generating dense correspondences among shapes. With\nDIF, a 3D shape is represented by a template implicit field shared across the\ncategory, together with a 3D deformation field and a correction field dedicated\nfor each shape instance. Shape correspondences can be easily established using\ntheir deformation fields. Our neural network, dubbed DIF-Net, jointly learns a\nshape latent space and these fields for 3D objects belonging to a category\nwithout using any correspondence or part label. The learned DIF-Net can also\nprovides reliable correspondence uncertainty measurement reflecting shape\nstructure discrepancy. Experiments show that DIF-Net not only produces\nhigh-fidelity 3D shapes but also builds high-quality dense correspondences\nacross different shapes. We also demonstrate several applications such as\ntexture transfer and shape editing, where our method achieves compelling\nresults that cannot be achieved by previous methods.\n", "versions": [{"version": "v1", "created": "Fri, 27 Nov 2020 10:45:26 GMT"}, {"version": "v2", "created": "Mon, 30 Nov 2020 04:37:02 GMT"}, {"version": "v3", "created": "Mon, 29 Mar 2021 10:46:14 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Deng", "Yu", ""], ["Yang", "Jiaolong", ""], ["Tong", "Xin", ""]]}, {"id": "2011.13669", "submitter": "Marlon Marcon", "authors": "Marlon Marcon, Olga Regina Pereira Bellon and Luciano Silva", "title": "Towards real-time object recognition and pose estimation in point clouds", "comments": "Accepted as Full paper at VISAPP2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Object recognition and 6DoF pose estimation are quite challenging tasks in\ncomputer vision applications. Despite efficiency in such tasks, standard\nmethods deliver far from real-time processing rates. This paper presents a\nnovel pipeline to estimate a fine 6DoF pose of objects, applied to realistic\nscenarios in real-time. We split our proposal into three main parts. Firstly, a\nColor feature classification leverages the use of pre-trained CNN color\nfeatures trained on the ImageNet for object detection. A Feature-based\nregistration module conducts a coarse pose estimation, and finally, a\nFine-adjustment step performs an ICP-based dense registration. Our proposal\nachieves, in the best case, an accuracy performance of almost 83\\% on the RGB-D\nScenes dataset. Regarding processing time, the object detection task is done at\na frame processing rate up to 90 FPS, and the pose estimation at almost 14 FPS\nin a full execution strategy. We discuss that due to the proposal's modularity,\nwe could let the full execution occurs only when necessary and perform a\nscheduled execution that unlocks real-time processing, even for multitask\nsituations.\n", "versions": [{"version": "v1", "created": "Fri, 27 Nov 2020 11:10:46 GMT"}], "update_date": "2020-11-30", "authors_parsed": [["Marcon", "Marlon", ""], ["Bellon", "Olga Regina Pereira", ""], ["Silva", "Luciano", ""]]}, {"id": "2011.13675", "submitter": "Yang Zhao", "authors": "Yang Zhao, Wei Jia, Ronggang Wang, Xiaoping Liu, Xuesong Gao, Weiqiang\n  Chen, and Wen Gao", "title": "Deinterlacing Network for Early Interlaced Videos", "comments": "10 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  With the rapid development of image restoration techniques, high-definition\nreconstruction of early videos has achieved impressive results. However, there\nare few studies about the interlacing artifacts that often appear in early\nvideos and significantly affect visual perception. Traditional deinterlacing\napproaches are mainly focused on early interlacing scanning systems and thus\ncannot handle the complex and complicated artifacts in real-world early\ninterlaced videos. Hence, this paper proposes a specific deinterlacing network\n(DIN), which is motivated by the traditional deinterlacing strategy. The\nproposed DIN consists of two stages, i.e., a cooperative vertical interpolation\nstage for split fields, and a merging stage that is applied to perceive\nmovements and remove ghost artifacts. Experimental results demonstrate that the\nproposed method can effectively remove complex artifacts in early interlaced\nvideos.\n", "versions": [{"version": "v1", "created": "Fri, 27 Nov 2020 11:24:36 GMT"}], "update_date": "2020-11-30", "authors_parsed": [["Zhao", "Yang", ""], ["Jia", "Wei", ""], ["Wang", "Ronggang", ""], ["Liu", "Xiaoping", ""], ["Gao", "Xuesong", ""], ["Chen", "Weiqiang", ""], ["Gao", "Wen", ""]]}, {"id": "2011.13677", "submitter": "Songtao Liu", "authors": "Songtao Liu, Zeming Li, Jian Sun", "title": "Self-EMD: Self-Supervised Object Detection without ImageNet", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a novel self-supervised representation learning\nmethod, Self-EMD, for object detection. Our method directly trained on\nunlabeled non-iconic image dataset like COCO, instead of commonly used\niconic-object image dataset like ImageNet. We keep the convolutional feature\nmaps as the image embedding to preserve spatial structures and adopt Earth\nMover's Distance (EMD) to compute the similarity between two embeddings. Our\nFaster R-CNN (ResNet50-FPN) baseline achieves 39.8% mAP on COCO, which is on\npar with the state of the art self-supervised methods pre-trained on ImageNet.\nMore importantly, it can be further improved to 40.4% mAP with more unlabeled\nimages, showing its great potential for leveraging more easily obtained\nunlabeled data. Code will be made available.\n", "versions": [{"version": "v1", "created": "Fri, 27 Nov 2020 11:27:19 GMT"}, {"version": "v2", "created": "Tue, 8 Dec 2020 15:06:16 GMT"}, {"version": "v3", "created": "Mon, 22 Mar 2021 09:41:15 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Liu", "Songtao", ""], ["Li", "Zeming", ""], ["Sun", "Jian", ""]]}, {"id": "2011.13681", "submitter": "Arjun Mani", "authors": "Arjun Mani, Nobline Yoo, Will Hinthorn, Olga Russakovsky", "title": "Point and Ask: Incorporating Pointing into Visual Question Answering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual Question Answering (VQA) has become one of the key benchmarks of\nvisual recognition progress. Multiple VQA extensions have been explored to\nbetter simulate real-world settings: different question formulations, changing\ntraining and test distributions, conversational consistency in dialogues, and\nexplanation-based answering. In this work, we further expand this space by\nconsidering visual questions that include a spatial point of reference.\nPointing is a nearly universal gesture among humans, and real-world VQA is\nlikely to involve a gesture towards the target region.\n  Concretely, we (1) introduce and motivate point-input questions as an\nextension of VQA, (2) define three novel classes of questions within this\nspace, and (3) for each class, introduce both a benchmark dataset and a series\nof baseline models to handle its unique challenges. There are two key\ndistinctions from prior work. First, we explicitly design the benchmarks to\nrequire the point input, i.e., we ensure that the visual question cannot be\nanswered accurately without the spatial reference. Second, we explicitly\nexplore the more realistic point spatial input rather than the standard but\nunnatural bounding box input. Through our exploration we uncover and address\nseveral visual recognition challenges, including the ability to infer human\nintent, reason both locally and globally about the image, and effectively\ncombine visual, language and spatial inputs. Code is available at:\nhttps://github.com/princetonvisualai/pointingqa .\n", "versions": [{"version": "v1", "created": "Fri, 27 Nov 2020 11:43:45 GMT"}, {"version": "v2", "created": "Wed, 16 Jun 2021 16:54:24 GMT"}, {"version": "v3", "created": "Thu, 17 Jun 2021 06:33:25 GMT"}], "update_date": "2021-06-18", "authors_parsed": [["Mani", "Arjun", ""], ["Yoo", "Nobline", ""], ["Hinthorn", "Will", ""], ["Russakovsky", "Olga", ""]]}, {"id": "2011.13688", "submitter": "Chenyan Wu", "authors": "Chenyan Wu, Yukun Chen, Jiajia Luo, Che-Chun Su, Anuja Dawane,\n  Bikramjot Hanzra, Zhuo Deng, Bilan Liu, James Wang, Cheng-Hao Kuo", "title": "MEBOW: Monocular Estimation of Body Orientation In the Wild", "comments": "Accepted by CVPR2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Body orientation estimation provides crucial visual cues in many\napplications, including robotics and autonomous driving. It is particularly\ndesirable when 3-D pose estimation is difficult to infer due to poor image\nresolution, occlusion or indistinguishable body parts. We present COCO-MEBOW\n(Monocular Estimation of Body Orientation in the Wild), a new large-scale\ndataset for orientation estimation from a single in-the-wild image. The\nbody-orientation labels for around 130K human bodies within 55K images from the\nCOCO dataset have been collected using an efficient and high-precision\nannotation pipeline. We also validated the benefits of the dataset. First, we\nshow that our dataset can substantially improve the performance and the\nrobustness of a human body orientation estimation model, the development of\nwhich was previously limited by the scale and diversity of the available\ntraining data. Additionally, we present a novel triple-source solution for 3-D\nhuman pose estimation, where 3-D pose labels, 2-D pose labels, and our\nbody-orientation labels are all used in joint training. Our model significantly\noutperforms state-of-the-art dual-source solutions for monocular 3-D human pose\nestimation, where training only uses 3-D pose labels and 2-D pose labels. This\nsubstantiates an important advantage of MEBOW for 3-D human pose estimation,\nwhich is particularly appealing because the per-instance labeling cost for body\norientations is far less than that for 3-D poses. The work demonstrates high\npotential of MEBOW in addressing real-world challenges involving understanding\nhuman behaviors. Further information of this work is available at\nhttps://chenyanwu.github.io/MEBOW/.\n", "versions": [{"version": "v1", "created": "Fri, 27 Nov 2020 11:56:13 GMT"}], "update_date": "2020-11-30", "authors_parsed": [["Wu", "Chenyan", ""], ["Chen", "Yukun", ""], ["Luo", "Jiajia", ""], ["Su", "Che-Chun", ""], ["Dawane", "Anuja", ""], ["Hanzra", "Bikramjot", ""], ["Deng", "Zhuo", ""], ["Liu", "Bilan", ""], ["Wang", "James", ""], ["Kuo", "Cheng-Hao", ""]]}, {"id": "2011.13692", "submitter": "Mingfu Xue", "authors": "Mingfu Xue, Chengxiang Yuan, Can He, Jian Wang, Weiqiang Liu", "title": "NaturalAE: Natural and Robust Physical Adversarial Examples for Object\n  Detectors", "comments": null, "journal-ref": "Journal of Information Security and Applications 57 (2021) 102694,\n  1-12", "doi": "10.1016/j.jisa.2020.102694", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a natural and robust physical adversarial example\nattack method targeting object detectors under real-world conditions. The\ngenerated adversarial examples are robust to various physical constraints and\nvisually look similar to the original images, thus these adversarial examples\nare natural to humans and will not cause any suspicions. First, to ensure the\nrobustness of the adversarial examples in real-world conditions, the proposed\nmethod exploits different image transformation functions, to simulate various\nphysical changes during the iterative optimization of the adversarial examples\ngeneration. Second, to construct natural adversarial examples, the proposed\nmethod uses an adaptive mask to constrain the area and intensities of the added\nperturbations, and utilizes the real-world perturbation score (RPS) to make the\nperturbations be similar to those real noises in physical world. Compared with\nexisting studies, our generated adversarial examples can achieve a high success\nrate with less conspicuous perturbations. Experimental results demonstrate\nthat, the generated adversarial examples are robust under various indoor and\noutdoor physical conditions, including different distances, angles,\nilluminations, and photographing. Specifically, the attack success rate of\ngenerated adversarial examples indoors and outdoors is high up to 73.33% and\n82.22%, respectively. Meanwhile, the proposed method ensures the naturalness of\nthe generated adversarial example, and the size of added perturbations is much\nsmaller than the perturbations in the existing works. Further, the proposed\nphysical adversarial attack method can be transferred from the white-box models\nto other object detection models.\n", "versions": [{"version": "v1", "created": "Fri, 27 Nov 2020 12:03:53 GMT"}, {"version": "v2", "created": "Wed, 17 Mar 2021 08:47:26 GMT"}], "update_date": "2021-03-18", "authors_parsed": [["Xue", "Mingfu", ""], ["Yuan", "Chengxiang", ""], ["He", "Can", ""], ["Wang", "Jian", ""], ["Liu", "Weiqiang", ""]]}, {"id": "2011.13698", "submitter": "Mickael Tardy", "authors": "Mickael Tardy, Diana Mateus", "title": "Lightweight U-Net for High-Resolution Breast Imaging", "comments": "in Proceedings of iTWIST'20, Paper-ID: 30, Nantes, France, December,\n  2-4, 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We study the fully convolutional neural networks in the context of malignancy\ndetection for breast cancer screening. We work on a supervised segmentation\ntask looking for an acceptable compromise between the precision of the network\nand the computational complexity.\n", "versions": [{"version": "v1", "created": "Fri, 27 Nov 2020 12:25:24 GMT"}], "update_date": "2020-11-30", "authors_parsed": [["Tardy", "Mickael", ""], ["Mateus", "Diana", ""]]}, {"id": "2011.13705", "submitter": "Mingfu Xue", "authors": "Mingfu Xue, Can He, Zhiyu Wu, Jian Wang, Zhe Liu, Weiqiang Liu", "title": "3D Invisible Cloak", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a novel physical stealth attack against the person\ndetectors in real world. The proposed method generates an adversarial patch,\nand prints it on real clothes to make a three dimensional (3D) invisible cloak.\nAnyone wearing the cloak can evade the detection of person detectors and\nachieve stealth. We consider the impacts of those 3D physical constraints\n(i.e., radian, wrinkle, occlusion, angle, etc.) on person stealth attacks, and\npropose 3D transformations to generate 3D invisible cloak. We launch the person\nstealth attacks in 3D physical space instead of 2D plane by printing the\nadversarial patches on real clothes under challenging and complex 3D physical\nscenarios. The conventional and 3D transformations are performed on the patch\nduring its optimization process. Further, we study how to generate the optimal\n3D invisible cloak. Specifically, we explore how to choose input images with\nspecific shapes and colors to generate the optimal 3D invisible cloak. Besides,\nafter successfully making the object detector misjudge the person as other\nobjects, we explore how to make a person completely disappeared, i.e., the\nperson will not be detected as any objects. Finally, we present a systematic\nevaluation framework to methodically evaluate the performance of the proposed\nattack in digital domain and physical world. Experimental results in various\nindoor and outdoor physical scenarios show that, the proposed person stealth\nattack method is robust and effective even under those complex and challenging\nphysical conditions, such as the cloak is wrinkled, obscured, curved, and from\ndifferent angles. The attack success rate in digital domain (Inria data set) is\n86.56%, while the static and dynamic stealth attack performance in physical\nworld is 100% and 77%, respectively, which are significantly better than\nexisting works.\n", "versions": [{"version": "v1", "created": "Fri, 27 Nov 2020 12:43:04 GMT"}], "update_date": "2020-11-30", "authors_parsed": [["Xue", "Mingfu", ""], ["He", "Can", ""], ["Wu", "Zhiyu", ""], ["Wang", "Jian", ""], ["Liu", "Zhe", ""], ["Liu", "Weiqiang", ""]]}, {"id": "2011.13714", "submitter": "Aishwarya Jadhav", "authors": "Aishwarya Jadhav", "title": "Detection of Malaria Vector Breeding Habitats using Topographic Models", "comments": "Presented at NeurIPS 2020 Workshop on Machine Learning for the\n  Developing World", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Treatment of stagnant water bodies that act as a breeding site for malarial\nvectors is a fundamental step in most malaria elimination campaigns. However,\nidentification of such water bodies over large areas is expensive,\nlabour-intensive and time-consuming and hence, challenging in countries with\nlimited resources. Practical models that can efficiently locate water bodies\ncan target the limited resources by greatly reducing the area that needs to be\nscanned by the field workers. To this end, we propose a practical topographic\nmodel based on easily available, global, high-resolution DEM data to predict\nlocations of potential vector-breeding water sites. We surveyed the Obuasi\nregion of Ghana to assess the impact of various topographic features on\ndifferent types of water bodies and uncover the features that significantly\ninfluence the formation of aquatic habitats. We further evaluate the\neffectiveness of multiple models. Our best model significantly outperforms\nearlier attempts that employ topographic variables for detection of small water\nsites, even the ones that utilize additional satellite imagery data and\ndemonstrates robustness across different settings.\n", "versions": [{"version": "v1", "created": "Fri, 27 Nov 2020 12:59:55 GMT"}], "update_date": "2020-11-30", "authors_parsed": [["Jadhav", "Aishwarya", ""]]}, {"id": "2011.13728", "submitter": "Niladri Shekhar Dutt", "authors": "Niladri Shekhar Dutt, Sunil Patel", "title": "A study of traits that affect learnability in GANs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative Adversarial Networks GANs are algorithmic architectures that use\ntwo neural networks, pitting one against the opposite so as to come up with\nnew, synthetic instances of data that can pass for real data. Training a GAN is\na challenging problem which requires us to apply advanced techniques like\nhyperparameter tuning, architecture engineering etc. Many different losses,\nregularization and normalization schemes, network architectures have been\nproposed to solve this challenging problem for different types of datasets. It\nbecomes necessary to understand the experimental observations and deduce a\nsimple theory for it. In this paper, we perform empirical experiments using\nparameterized synthetic datasets to probe what traits affect learnability.\n", "versions": [{"version": "v1", "created": "Fri, 27 Nov 2020 13:31:37 GMT"}], "update_date": "2020-11-30", "authors_parsed": [["Dutt", "Niladri Shekhar", ""], ["Patel", "Sunil", ""]]}, {"id": "2011.13741", "submitter": "Aditya Jyoti Paul", "authors": "Aditya Jyoti Paul, Puranjay Mohan, Stuti Sehgal", "title": "Rethinking Generalization in American Sign Language Prediction for Edge\n  Devices with Extremely Low Memory Footprint", "comments": "6 pages, Published in IEEE RAICS 2020, see https://raics.in", "journal-ref": "2020 IEEE Recent Advances in Intelligent Computational Systems\n  (RAICS), 2020, pp. 147-152", "doi": "10.1109/RAICS51191.2020.9332480", "report-no": null, "categories": "cs.LG cs.AI cs.CL cs.CV cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to the boom in technical compute in the last few years, the world has\nseen massive advances in artificially intelligent systems solving diverse\nreal-world problems. But a major roadblock in the ubiquitous acceptance of\nthese models is their enormous computational complexity and memory footprint.\nHence efficient architectures and training techniques are required for\ndeployment on extremely low resource inference endpoints. This paper proposes\nan architecture for detection of alphabets in American Sign Language on an ARM\nCortex-M7 microcontroller having just 496 KB of framebuffer RAM. Leveraging\nparameter quantization is a common technique that might cause varying drops in\ntest accuracy. This paper proposes using interpolation as augmentation amongst\nother techniques as an efficient method of reducing this drop, which also helps\nthe model generalize well to previously unseen noisy data. The proposed model\nis about 185 KB post-quantization and inference speed is 20 frames per second.\n", "versions": [{"version": "v1", "created": "Fri, 27 Nov 2020 14:05:42 GMT"}, {"version": "v2", "created": "Sat, 13 Feb 2021 10:24:01 GMT"}], "update_date": "2021-02-16", "authors_parsed": [["Paul", "Aditya Jyoti", ""], ["Mohan", "Puranjay", ""], ["Sehgal", "Stuti", ""]]}, {"id": "2011.13761", "submitter": "Lyujie Chen", "authors": "Lyujie Chen, Xiaming Yuan, Yao Xiao, Yiding Zhang and Jihong Zhu", "title": "Robust Autonomous Landing of UAV in Non-Cooperative Environments based\n  on Dynamic Time Camera-LiDAR Fusion", "comments": "Submitted to ICRA 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Selecting safe landing sites in non-cooperative environments is a key step\ntowards the full autonomy of UAVs. However, the existing methods have the\ncommon problems of poor generalization ability and robustness. Their\nperformance in unknown environments is significantly degraded and the error\ncannot be self-detected and corrected. In this paper, we construct a UAV system\nequipped with low-cost LiDAR and binocular cameras to realize autonomous\nlanding in non-cooperative environments by detecting the flat and safe ground\narea. Taking advantage of the non-repetitive scanning and high FOV coverage\ncharacteristics of LiDAR, we come up with a dynamic time depth completion\nalgorithm. In conjunction with the proposed self-evaluation method of the depth\nmap, our model can dynamically select the LiDAR accumulation time at the\ninference phase to ensure an accurate prediction result. Based on the depth\nmap, the high-level terrain information such as slope, roughness, and the size\nof the safe area are derived. We have conducted extensive autonomous landing\nexperiments in a variety of familiar or completely unknown environments,\nverifying that our model can adaptively balance the accuracy and speed, and the\nUAV can robustly select a safe landing site.\n", "versions": [{"version": "v1", "created": "Fri, 27 Nov 2020 14:47:02 GMT"}], "update_date": "2020-11-30", "authors_parsed": [["Chen", "Lyujie", ""], ["Yuan", "Xiaming", ""], ["Xiao", "Yao", ""], ["Zhang", "Yiding", ""], ["Zhu", "Jihong", ""]]}, {"id": "2011.13775", "submitter": "Denis Korzhenkov", "authors": "Ivan Anokhin, Kirill Demochkin, Taras Khakhulin, Gleb Sterkin, Victor\n  Lempitsky, Denis Korzhenkov", "title": "Image Generators with Conditionally-Independent Pixel Synthesis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Existing image generator networks rely heavily on spatial convolutions and,\noptionally, self-attention blocks in order to gradually synthesize images in a\ncoarse-to-fine manner. Here, we present a new architecture for image\ngenerators, where the color value at each pixel is computed independently given\nthe value of a random latent vector and the coordinate of that pixel. No\nspatial convolutions or similar operations that propagate information across\npixels are involved during the synthesis. We analyze the modeling capabilities\nof such generators when trained in an adversarial fashion, and observe the new\ngenerators to achieve similar generation quality to state-of-the-art\nconvolutional generators. We also investigate several interesting properties\nunique to the new architecture.\n", "versions": [{"version": "v1", "created": "Fri, 27 Nov 2020 15:16:11 GMT"}], "update_date": "2020-11-30", "authors_parsed": [["Anokhin", "Ivan", ""], ["Demochkin", "Kirill", ""], ["Khakhulin", "Taras", ""], ["Sterkin", "Gleb", ""], ["Lempitsky", "Victor", ""], ["Korzhenkov", "Denis", ""]]}, {"id": "2011.13776", "submitter": "Hao Chen", "authors": "Hao Chen, Benoit Lagadec, Francois Bremond", "title": "Enhancing Diversity in Teacher-Student Networks via Asymmetric branches\n  for Unsupervised Person Re-identification", "comments": "WACV 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The objective of unsupervised person re-identification (Re-ID) is to learn\ndiscriminative features without labor-intensive identity annotations.\nState-of-the-art unsupervised Re-ID methods assign pseudo labels to unlabeled\nimages in the target domain and learn from these noisy pseudo labels. Recently\nintroduced Mean Teacher Model is a promising way to mitigate the label noise.\nHowever, during the training, self-ensembled teacher-student networks quickly\nconverge to a consensus which leads to a local minimum. We explore the\npossibility of using an asymmetric structure inside neural network to address\nthis problem. First, asymmetric branches are proposed to extract features in\ndifferent manners, which enhances the feature diversity in appearance\nsignatures. Then, our proposed cross-branch supervision allows one branch to\nget supervision from the other branch, which transfers distinct knowledge and\nenhances the weight diversity between teacher and student networks. Extensive\nexperiments show that our proposed method can significantly surpass the\nperformance of previous work on both unsupervised domain adaptation and fully\nunsupervised Re-ID tasks.\n", "versions": [{"version": "v1", "created": "Fri, 27 Nov 2020 15:17:10 GMT"}], "update_date": "2020-11-30", "authors_parsed": [["Chen", "Hao", ""], ["Lagadec", "Benoit", ""], ["Bremond", "Francois", ""]]}, {"id": "2011.13784", "submitter": "Guangming Wang", "authors": "Guangming Wang, Yehui Yang, Huixin Zhang, Zhe Liu, and Hesheng Wang", "title": "Spherical Interpolated Convolutional Network with Distance-Feature\n  Density for 3D Semantic Segmentation of Point Clouds", "comments": "10 pages, 10 figures, under review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The semantic segmentation of point clouds is an important part of the\nenvironment perception for robots. However, it is difficult to directly adopt\nthe traditional 3D convolution kernel to extract features from raw 3D point\nclouds because of the unstructured property of point clouds. In this paper, a\nspherical interpolated convolution operator is proposed to replace the\ntraditional grid-shaped 3D convolution operator. This newly proposed feature\nextraction operator improves the accuracy of the network and reduces the\nparameters of the network. In addition, this paper analyzes the defect of point\ncloud interpolation methods based on the distance as the interpolation weight\nand proposes the self-learned distance-feature density by combining the\ndistance and the feature correlation. The proposed method makes the feature\nextraction of spherical interpolated convolution network more rational and\neffective. The effectiveness of the proposed network is demonstrated on the 3D\nsemantic segmentation task of point clouds. Experiments show that the proposed\nmethod achieves good performance on the ScanNet dataset and Paris-Lille-3D\ndataset.\n", "versions": [{"version": "v1", "created": "Fri, 27 Nov 2020 15:35:12 GMT"}], "update_date": "2020-11-30", "authors_parsed": [["Wang", "Guangming", ""], ["Yang", "Yehui", ""], ["Zhang", "Huixin", ""], ["Liu", "Zhe", ""], ["Wang", "Hesheng", ""]]}, {"id": "2011.13786", "submitter": "Andrey Voynov", "authors": "Anton Cherepkov, Andrey Voynov, Artem Babenko", "title": "Navigating the GAN Parameter Space for Semantic Image Editing", "comments": "Supplementary code: https://github.com/yandex-research/navigan", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative Adversarial Networks (GANs) are currently an indispensable tool\nfor visual editing, being a standard component of image-to-image translation\nand image restoration pipelines. Furthermore, GANs are especially useful for\ncontrollable generation since their latent spaces contain a wide range of\ninterpretable directions, well suited for semantic editing operations. By\ngradually changing latent codes along these directions, one can produce\nimpressive visual effects, unattainable without GANs.\n  In this paper, we significantly expand the range of visual effects achievable\nwith the state-of-the-art models, like StyleGAN2. In contrast to existing\nworks, which mostly operate by latent codes, we discover interpretable\ndirections in the space of the generator parameters. By several simple methods,\nwe explore this space and demonstrate that it also contains a plethora of\ninterpretable directions, which are an excellent source of non-trivial semantic\nmanipulations. The discovered manipulations cannot be achieved by transforming\nthe latent codes and can be used to edit both synthetic and real images. We\nrelease our code and models and hope they will serve as a handy tool for\nfurther efforts on GAN-based image editing.\n", "versions": [{"version": "v1", "created": "Fri, 27 Nov 2020 15:38:56 GMT"}, {"version": "v2", "created": "Tue, 1 Dec 2020 12:34:59 GMT"}, {"version": "v3", "created": "Wed, 21 Apr 2021 12:45:11 GMT"}], "update_date": "2021-04-22", "authors_parsed": [["Cherepkov", "Anton", ""], ["Voynov", "Andrey", ""], ["Babenko", "Artem", ""]]}, {"id": "2011.13816", "submitter": "Lie Ju", "authors": "Lie Ju, Xin Wang, Xin Zhao, Paul Bonnington, Tom Drummond, Zongyuan Ge", "title": "Leveraging Regular Fundus Images for Training UWF Fundus Diagnosis\n  Models via Adversarial Learning and Pseudo-Labeling", "comments": null, "journal-ref": null, "doi": "10.1109/TMI.2021.3056395", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, ultra-widefield (UWF) 200\\degree~fundus imaging by Optos cameras\nhas gradually been introduced because of its broader insights for detecting\nmore information on the fundus than regular 30 degree - 60 degree fundus\ncameras. Compared with UWF fundus images, regular fundus images contain a large\namount of high-quality and well-annotated data. Due to the domain gap, models\ntrained by regular fundus images to recognize UWF fundus images perform poorly.\nHence, given that annotating medical data is labor intensive and time\nconsuming, in this paper, we explore how to leverage regular fundus images to\nimprove the limited UWF fundus data and annotations for more efficient\ntraining. We propose the use of a modified cycle generative adversarial network\n(CycleGAN) model to bridge the gap between regular and UWF fundus and generate\nadditional UWF fundus images for training. A consistency regularization term is\nproposed in the loss of the GAN to improve and regulate the quality of the\ngenerated data. Our method does not require that images from the two domains be\npaired or even that the semantic labels be the same, which provides great\nconvenience for data collection. Furthermore, we show that our method is robust\nto noise and errors introduced by the generated unlabeled data with the\npseudo-labeling technique. We evaluated the effectiveness of our methods on\nseveral common fundus diseases and tasks, such as diabetic retinopathy (DR)\nclassification, lesion detection and tessellated fundus segmentation. The\nexperimental results demonstrate that our proposed method simultaneously\nachieves superior generalizability of the learned representations and\nperformance improvements in multiple tasks.\n", "versions": [{"version": "v1", "created": "Fri, 27 Nov 2020 16:25:30 GMT"}, {"version": "v2", "created": "Tue, 2 Feb 2021 03:38:41 GMT"}], "update_date": "2021-02-03", "authors_parsed": [["Ju", "Lie", ""], ["Wang", "Xin", ""], ["Zhao", "Xin", ""], ["Bonnington", "Paul", ""], ["Drummond", "Tom", ""], ["Ge", "Zongyuan", ""]]}, {"id": "2011.13817", "submitter": "Victor Fragoso", "authors": "Victor Fragoso, Sudipta Sinha", "title": "Generalized Pose-and-Scale Estimation using 4-Point Congruence\n  Constraints", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present gP4Pc, a new method for computing the absolute pose of a\ngeneralized camera with unknown internal scale from four corresponding 3D\npoint-and-ray pairs. Unlike most pose-and-scale methods, gP4Pc is based on\nconstraints arising from the congruence of shapes defined by two sets of four\npoints related by an unknown similarity transformation. By choosing a novel\nparametrization for the problem, we derive a system of four quadratic equations\nin four scalar variables. The variables represent the distances of 3D points\nalong the rays from the camera centers. After solving this system via Groebner\nbasis-based automatic polynomial solvers, we compute the similarity\ntransformation using an efficient 3D point-point alignment method. We also\npropose a specialized variant of our solver for the case of coplanar points,\nwhich is computationally very efficient and about 3x faster than the fastest\nexisting solver. Our experiments on real and synthetic datasets, demonstrate\nthat gP4Pc is among the fastest methods in terms of total running time when\nused within a RANSAC framework, while achieving competitive numerical\nstability, accuracy, and robustness to noise.\n", "versions": [{"version": "v1", "created": "Fri, 27 Nov 2020 16:30:19 GMT"}], "update_date": "2020-11-30", "authors_parsed": [["Fragoso", "Victor", ""], ["Sinha", "Sudipta", ""]]}, {"id": "2011.13843", "submitter": "Elena Burceanu", "authors": "Elena Burceanu", "title": "SFTrack++: A Fast Learnable Spectral Segmentation Approach for\n  Space-Time Consistent Tracking", "comments": "Accepted at Neural Information Processing Systems (NeurIPS) 2020 -\n  Pre-registration Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an object tracking method, SFTrack++, that smoothly learns to\npreserve the tracked object consistency over space and time dimensions by\ntaking a spectral clustering approach over the graph of pixels from the video,\nusing a fast 3D filtering formulation for finding the principal eigenvector of\nthis graph's adjacency matrix. To better capture complex aspects of the tracked\nobject, we enrich our formulation to multi-channel inputs, which permit\ndifferent points of view for the same input. The channel inputs could be, like\nin our experiments, the output of multiple tracking methods or other feature\nmaps. After extracting and combining those feature maps, instead of relying\nonly on hidden layers representations to predict a good tracking bounding box,\nwe explicitly learn an intermediate, more refined one, namely the segmentation\nmap of the tracked object. This prevents the rough common bounding box approach\nto introduce noise and distractors in the learning process. We test our method,\nSFTrack++, on seven tracking benchmarks: VOT2018, LaSOT, TrackingNet, GOT10k,\nNFS, OTB-100, and UAV123.\n", "versions": [{"version": "v1", "created": "Fri, 27 Nov 2020 17:15:20 GMT"}, {"version": "v2", "created": "Mon, 30 Nov 2020 11:25:56 GMT"}], "update_date": "2020-12-01", "authors_parsed": [["Burceanu", "Elena", ""]]}, {"id": "2011.13849", "submitter": "Reza Maalek", "authors": "Reza Maalek and Derek Lichti", "title": "Robust Detection of Non-overlapping Ellipses from Points with\n  Applications to Circular Target Extraction in Images and Cylinder Detection\n  in Point Clouds", "comments": null, "journal-ref": null, "doi": "10.1016/j.isprsjprs.2021.04.010", "report-no": null, "categories": "cs.CV cs.CG cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This manuscript provides a collection of new methods for the automated\ndetection of non-overlapping ellipses from edge points. The methods introduce\nnew developments in: (i) robust Monte Carlo-based ellipse fitting to\n2-dimensional (2D) points in the presence of outliers; (ii) detection of\nnon-overlapping ellipse from 2D edge points; and (iii) extraction of cylinder\nfrom 3D point clouds. The proposed methods were thoroughly compared with\nestablished state-of-the-art methods, using simulated and real-world datasets,\nthrough the design of four sets of original experiments. It was found that the\nproposed robust ellipse detection was superior to four reliable robust methods,\nincluding the popular least median of squares, in both simulated and real-world\ndatasets. The proposed process for detecting non-overlapping ellipses achieved\nF-measure of 99.3% on real images, compared to F-measures of 42.4%, 65.6%, and\n59.2%, obtained using the methods of Fornaciari, Patraucean, and Panagiotakis,\nrespectively. The proposed cylinder extraction method identified all detectable\nmechanical pipes in two real-world point clouds, obtained under laboratory, and\nindustrial construction site conditions. The results of this investigation show\npromise for the application of the proposed methods for automatic extraction of\ncircular targets from images and pipes from point clouds.\n", "versions": [{"version": "v1", "created": "Thu, 19 Nov 2020 21:56:02 GMT"}, {"version": "v2", "created": "Sun, 20 Dec 2020 15:07:21 GMT"}, {"version": "v3", "created": "Tue, 30 Mar 2021 17:56:30 GMT"}], "update_date": "2021-05-03", "authors_parsed": [["Maalek", "Reza", ""], ["Lichti", "Derek", ""]]}, {"id": "2011.13851", "submitter": "Mahdi Rezaei", "authors": "Soheil Khatibi, Meisam Teimouri, Mahdi Rezaei", "title": "Real-time Active Vision for a Humanoid Soccer Robot Using Deep\n  Reinforcement Learning", "comments": "The paper has been accepted in ICAART 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.CV cs.LG eess.IV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In this paper, we present an active vision method using a deep reinforcement\nlearning approach for a humanoid soccer-playing robot. The proposed method\nadaptively optimises the viewpoint of the robot to acquire the most useful\nlandmarks for self-localisation while keeping the ball into its viewpoint.\nActive vision is critical for humanoid decision-maker robots with a limited\nfield of view. To deal with an active vision problem, several probabilistic\nentropy-based approaches have previously been proposed which are highly\ndependent on the accuracy of the self-localisation model. However, in this\nresearch, we formulate the problem as an episodic reinforcement learning\nproblem and employ a Deep Q-learning method to solve it. The proposed network\nonly requires the raw images of the camera to move the robot's head toward the\nbest viewpoint. The model shows a very competitive rate of 80% success rate in\nachieving the best viewpoint. We implemented the proposed method on a humanoid\nrobot simulated in Webots simulator. Our evaluations and experimental results\nshow that the proposed method outperforms the entropy-based methods in the\nRoboCup context, in cases with high self-localisation errors.\n", "versions": [{"version": "v1", "created": "Fri, 27 Nov 2020 17:29:48 GMT"}], "update_date": "2020-11-30", "authors_parsed": [["Khatibi", "Soheil", ""], ["Teimouri", "Meisam", ""], ["Rezaei", "Mahdi", ""]]}, {"id": "2011.13866", "submitter": "Dor Verbin", "authors": "Dor Verbin and Todd Zickler", "title": "Field of Junctions: Extracting Boundary Structure at Low SNR", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a bottom-up model for simultaneously finding many boundary\nelements in an image, including contours, corners and junctions. The model\nexplains boundary shape in each small patch using a 'generalized M-junction'\ncomprising M angles and a freely-moving vertex. Images are analyzed using\nnon-convex optimization to cooperatively find M+2 junction values at every\nlocation, with spatial consistency being enforced by a novel regularizer that\nreduces curvature while preserving corners and junctions. The resulting 'field\nof junctions' is simultaneously a contour detector, corner/junction detector,\nand boundary-aware smoothing of regional appearance. Notably, its unified\nanalysis of contours, corners, junctions and uniform regions allows it to\nsucceed at high noise levels, where other methods for segmentation and boundary\ndetection fail.\n", "versions": [{"version": "v1", "created": "Fri, 27 Nov 2020 17:46:08 GMT"}, {"version": "v2", "created": "Wed, 28 Apr 2021 19:46:12 GMT"}], "update_date": "2021-04-30", "authors_parsed": [["Verbin", "Dor", ""], ["Zickler", "Todd", ""]]}, {"id": "2011.13894", "submitter": "Victor Fragoso", "authors": "Marcela Mera-Trujillo, Benjamin Smith, Victor Fragoso", "title": "Efficient Scene Compression for Visual-based Localization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Estimating the pose of a camera with respect to a 3D reconstruction or scene\nrepresentation is a crucial step for many mixed reality and robotics\napplications. Given the vast amount of available data nowadays, many\napplications constrain storage and/or bandwidth to work efficiently. To satisfy\nthese constraints, many applications compress a scene representation by\nreducing its number of 3D points. While state-of-the-art methods use\n$K$-cover-based algorithms to compress a scene, they are slow and hard to tune.\nTo enhance speed and facilitate parameter tuning, this work introduces a novel\napproach that compresses a scene representation by means of a constrained\nquadratic program (QP). Because this QP resembles a one-class support vector\nmachine, we derive a variant of the sequential minimal optimization to solve\nit. Our approach uses the points corresponding to the support vectors as the\nsubset of points to represent a scene. We also present an efficient\ninitialization method that allows our method to converge quickly. Our\nexperiments on publicly available datasets show that our approach compresses a\nscene representation quickly while delivering accurate pose estimates.\n", "versions": [{"version": "v1", "created": "Fri, 27 Nov 2020 18:36:06 GMT"}], "update_date": "2020-11-30", "authors_parsed": [["Mera-Trujillo", "Marcela", ""], ["Smith", "Benjamin", ""], ["Fragoso", "Victor", ""]]}, {"id": "2011.13913", "submitter": "Mahmut Yurt", "authors": "Mahmut Yurt, Muzaffer \\\"Ozbey, Salman Ul Hassan Dar, Berk T{\\i}naz,\n  Kader Karl{\\i} O\\u{g}uz, Tolga \\c{C}ukur", "title": "Progressively Volumetrized Deep Generative Models for Data-Efficient\n  Contextual Learning of MR Image Recovery", "comments": "Fixed a typo", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Magnetic resonance imaging (MRI) offers the flexibility to image a given\nanatomic volume under a multitude of tissue contrasts. Yet, scan time\nconsiderations put stringent limits on the quality and diversity of MRI data.\nThe gold-standard approach to alleviate this limitation is to recover\nhigh-quality images from data undersampled across various dimensions such as\nthe Fourier domain or contrast sets. A central divide among recovery methods is\nwhether the anatomy is processed per volume or per cross-section. Volumetric\nmodels offer enhanced capture of global contextual information, but they can\nsuffer from suboptimal learning due to elevated model complexity.\nCross-sectional models with lower complexity offer improved learning behavior,\nyet they ignore contextual information across the longitudinal dimension of the\nvolume. Here, we introduce a novel data-efficient progressively volumetrized\ngenerative model (ProvoGAN) that decomposes complex volumetric image recovery\ntasks into a series of simpler cross-sectional tasks across individual\nrectilinear dimensions. ProvoGAN effectively captures global context and\nrecovers fine-structural details across all dimensions, while maintaining low\nmodel complexity and data-efficiency advantages of cross-sectional models.\nComprehensive demonstrations on mainstream MRI reconstruction and synthesis\ntasks show that ProvoGAN yields superior performance to state-of-the-art\nvolumetric and cross-sectional models.\n", "versions": [{"version": "v1", "created": "Fri, 27 Nov 2020 18:55:56 GMT"}, {"version": "v2", "created": "Thu, 3 Dec 2020 17:48:15 GMT"}], "update_date": "2020-12-04", "authors_parsed": [["Yurt", "Mahmut", ""], ["\u00d6zbey", "Muzaffer", ""], ["Dar", "Salman Ul Hassan", ""], ["T\u0131naz", "Berk", ""], ["O\u011fuz", "Kader Karl\u0131", ""], ["\u00c7ukur", "Tolga", ""]]}, {"id": "2011.13917", "submitter": "Jennifer J. Sun", "authors": "Jennifer J. Sun, Ann Kennedy, Eric Zhan, David J. Anderson, Yisong\n  Yue, Pietro Perona", "title": "Task Programming: Learning Data Efficient Behavior Representations", "comments": "To appear in as an Oral in CVPR 2021. Code:\n  https://github.com/neuroethology/TREBA. Project page:\n  https://sites.google.com/view/task-programming", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Specialized domain knowledge is often necessary to accurately annotate\ntraining sets for in-depth analysis, but can be burdensome and time-consuming\nto acquire from domain experts. This issue arises prominently in automated\nbehavior analysis, in which agent movements or actions of interest are detected\nfrom video tracking data. To reduce annotation effort, we present TREBA: a\nmethod to learn annotation-sample efficient trajectory embedding for behavior\nanalysis, based on multi-task self-supervised learning. The tasks in our method\ncan be efficiently engineered by domain experts through a process we call \"task\nprogramming\", which uses programs to explicitly encode structured knowledge\nfrom domain experts. Total domain expert effort can be reduced by exchanging\ndata annotation time for the construction of a small number of programmed\ntasks. We evaluate this trade-off using data from behavioral neuroscience, in\nwhich specialized domain knowledge is used to identify behaviors. We present\nexperimental results in three datasets across two domains: mice and fruit\nflies. Using embeddings from TREBA, we reduce annotation burden by up to a\nfactor of 10 without compromising accuracy compared to state-of-the-art\nfeatures. Our results thus suggest that task programming and self-supervision\ncan be an effective way to reduce annotation effort for domain experts.\n", "versions": [{"version": "v1", "created": "Fri, 27 Nov 2020 18:58:32 GMT"}, {"version": "v2", "created": "Mon, 29 Mar 2021 17:59:47 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Sun", "Jennifer J.", ""], ["Kennedy", "Ann", ""], ["Zhan", "Eric", ""], ["Anderson", "David J.", ""], ["Yue", "Yisong", ""], ["Perona", "Pietro", ""]]}, {"id": "2011.13920", "submitter": "Sara Sabour", "authors": "Sara Sabour, Andrea Tagliasacchi, Soroosh Yazdani, Geoffrey E. Hinton,\n  David J. Fleet", "title": "Unsupervised part representation by Flow Capsules", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Capsule networks aim to parse images into a hierarchy of objects, parts and\nrelations. While promising, they remain limited by an inability to learn\neffective low level part descriptions. To address this issue we propose a way\nto learn primary capsule encoders that detect atomic parts from a single image.\nDuring training we exploit motion as a powerful perceptual cue for part\ndefinition, with an expressive decoder for part generation within a layered\nimage model with occlusion. Experiments demonstrate robust part discovery in\nthe presence of multiple objects, cluttered backgrounds, and occlusion. The\npart decoder infers the underlying shape masks, effectively filling in occluded\nregions of the detected shapes. We evaluate FlowCapsules on unsupervised part\nsegmentation and unsupervised image classification.\n", "versions": [{"version": "v1", "created": "Fri, 27 Nov 2020 18:59:42 GMT"}, {"version": "v2", "created": "Fri, 19 Feb 2021 18:07:46 GMT"}], "update_date": "2021-02-22", "authors_parsed": [["Sabour", "Sara", ""], ["Tagliasacchi", "Andrea", ""], ["Yazdani", "Soroosh", ""], ["Hinton", "Geoffrey E.", ""], ["Fleet", "David J.", ""]]}, {"id": "2011.13922", "submitter": "Yicong Hong", "authors": "Yicong Hong, Qi Wu, Yuankai Qi, Cristian Rodriguez-Opazo, Stephen\n  Gould", "title": "A Recurrent Vision-and-Language BERT for Navigation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accuracy of many visiolinguistic tasks has benefited significantly from the\napplication of vision-and-language(V&L) BERT. However, its application for the\ntask of vision-and-language navigation (VLN) remains limited. One reason for\nthis is the difficulty adapting the BERT architecture to the partially\nobservable Markov decision process present in VLN, requiring history-dependent\nattention and decision making. In this paper we propose a recurrent BERT model\nthat is time-aware for use in VLN. Specifically, we equip the BERT model with a\nrecurrent function that maintains cross-modal state information for the agent.\nThrough extensive experiments on R2R and REVERIE we demonstrate that our model\ncan replace more complex encoder-decoder models to achieve state-of-the-art\nresults. Moreover, our approach can be generalised to other transformer-based\narchitectures, supports pre-training, and is capable of solving navigation and\nreferring expression tasks simultaneously.\n", "versions": [{"version": "v1", "created": "Thu, 26 Nov 2020 00:23:00 GMT"}, {"version": "v2", "created": "Sun, 28 Mar 2021 11:45:58 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Hong", "Yicong", ""], ["Wu", "Qi", ""], ["Qi", "Yuankai", ""], ["Rodriguez-Opazo", "Cristian", ""], ["Gould", "Stephen", ""]]}, {"id": "2011.13927", "submitter": "Kevin Raina", "authors": "Kevin Raina", "title": "Modelling brain lesion volume in patches with CNN-based Poisson\n  Regression", "comments": null, "journal-ref": "In BIOIMAGING (pp. 172-176) 2020", "doi": "10.5220/0009102701720176", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Monitoring the progression of lesions is important for clinical response.\nSummary statistics such as lesion volume are objective and easy to interpret,\nwhich can help clinicians assess lesion growth or decay. CNNs are commonly used\nin medical image segmentation for their ability to produce useful features\nwithin large contexts and their associated efficient iterative patch-based\ntraining. Many CNN architectures require hundreds of thousands parameters to\nyield a good segmentation. In this work, an efficient, computationally\ninexpensive CNN is implemented to estimate the number of lesion voxels in a\npredefined patch size from magnetic resonance (MR) images. The output of the\nCNN is interpreted as the conditional Poisson parameter over the patch,\nallowing standard mini-batch gradient descent to be employed. The ISLES2015\n(SISS) data is used to train and evaluate the model, which by estimating lesion\nvolume from raw features, accurately identified the lesion image with the\nlarger lesion volume for 86% of paired sample patches. An argument for the\ndevelopment and use of estimating lesion volumes to also aid in model selection\nfor segmentation is made.\n", "versions": [{"version": "v1", "created": "Thu, 26 Nov 2020 21:11:15 GMT"}], "update_date": "2020-12-01", "authors_parsed": [["Raina", "Kevin", ""]]}, {"id": "2011.13961", "submitter": "Albert Pumarola", "authors": "Albert Pumarola, Enric Corona, Gerard Pons-Moll, Francesc\n  Moreno-Noguer", "title": "D-NeRF: Neural Radiance Fields for Dynamic Scenes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural rendering techniques combining machine learning with geometric\nreasoning have arisen as one of the most promising approaches for synthesizing\nnovel views of a scene from a sparse set of images. Among these, stands out the\nNeural radiance fields (NeRF), which trains a deep network to map 5D input\ncoordinates (representing spatial location and viewing direction) into a volume\ndensity and view-dependent emitted radiance. However, despite achieving an\nunprecedented level of photorealism on the generated images, NeRF is only\napplicable to static scenes, where the same spatial location can be queried\nfrom different images. In this paper we introduce D-NeRF, a method that extends\nneural radiance fields to a dynamic domain, allowing to reconstruct and render\nnovel images of objects under rigid and non-rigid motions from a \\emph{single}\ncamera moving around the scene. For this purpose we consider time as an\nadditional input to the system, and split the learning process in two main\nstages: one that encodes the scene into a canonical space and another that maps\nthis canonical representation into the deformed scene at a particular time.\nBoth mappings are simultaneously learned using fully-connected networks. Once\nthe networks are trained, D-NeRF can render novel images, controlling both the\ncamera view and the time variable, and thus, the object movement. We\ndemonstrate the effectiveness of our approach on scenes with objects under\nrigid, articulated and non-rigid motions. Code, model weights and the dynamic\nscenes dataset will be released.\n", "versions": [{"version": "v1", "created": "Fri, 27 Nov 2020 19:06:50 GMT"}], "update_date": "2020-12-01", "authors_parsed": [["Pumarola", "Albert", ""], ["Corona", "Enric", ""], ["Pons-Moll", "Gerard", ""], ["Moreno-Noguer", "Francesc", ""]]}, {"id": "2011.13971", "submitter": "Ozan Ciga", "authors": "Ozan Ciga, Anne L. Martel, Tony Xu", "title": "Self supervised contrastive learning for digital histopathology", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unsupervised learning has been a long-standing goal of machine learning and\nis especially important for medical image analysis, where the learning can\ncompensate for the scarcity of labeled datasets. A promising subclass of\nunsupervised learning is self-supervised learning, which aims to learn salient\nfeatures using the raw input as the learning signal. In this paper, we use a\ncontrastive self-supervised learning method Chen et al. (2020a) that achieved\nstate-of-the-art results on natural-scene images, and apply this method to\ndigital histopathology by collecting and training on 60 histopathology datasets\nwithout any labels. We find that combining multiple multi-organ datasets with\ndifferent types of staining and resolution properties improves the quality of\nthe learned features. Furthermore, we find drastically subsampling a dataset\n(e.g., using ? 1% of the available image patches) does not negatively impact\nthe learned representations, unlike training on natural-scene images. Linear\nclassifiers trained on top of the learned features show that networks\npretrained on digital histopathology datasets perform better than ImageNet\npretrained networks, boosting task performances up to 7.5% in accuracy and 8.9%\nin F1. These findings may also be useful when applying newer contrastive\ntechniques to histopathology data. Pretrained PyTorch models are made publicly\navailable at https://github.com/ozanciga/self-supervised-histopathology.\n", "versions": [{"version": "v1", "created": "Fri, 27 Nov 2020 19:18:45 GMT"}], "update_date": "2020-12-01", "authors_parsed": [["Ciga", "Ozan", ""], ["Martel", "Anne L.", ""], ["Xu", "Tony", ""]]}, {"id": "2011.13974", "submitter": "Sidike Paheding", "authors": "Uzair Khan, Paheding Sidike, Colin Elkin and Vijay Devabhaktuni", "title": "Trends in deep learning for medical hyperspectral image analysis", "comments": null, "journal-ref": "in IEEE Access, vol. 9, pp. 79534-79548, 2021", "doi": "10.1109/ACCESS.2021.3068392", "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning algorithms have seen acute growth of interest in their\napplications throughout several fields of interest in the last decade, with\nmedical hyperspectral imaging being a particularly promising domain. So far, to\nthe best of our knowledge, there is no review paper that discusses the\nimplementation of deep learning for medical hyperspectral imaging, which is\nwhat this review paper aims to accomplish by examining publications that\ncurrently utilize deep learning to perform effective analysis of medical\nhyperspectral imagery. This paper discusses deep learning concepts that are\nrelevant and applicable to medical hyperspectral imaging analysis, several of\nwhich have been implemented since the boom in deep learning. This will comprise\nof reviewing the use of deep learning for classification, segmentation, and\ndetection in order to investigate the analysis of medical hyperspectral\nimaging. Lastly, we discuss the current and future challenges pertaining to\nthis discipline and the possible efforts to overcome such trials.\n", "versions": [{"version": "v1", "created": "Fri, 27 Nov 2020 19:42:06 GMT"}], "update_date": "2021-06-10", "authors_parsed": [["Khan", "Uzair", ""], ["Sidike", "Paheding", ""], ["Elkin", "Colin", ""], ["Devabhaktuni", "Vijay", ""]]}, {"id": "2011.14004", "submitter": "Jihyeon Lee", "authors": "Jihyeon Lee, Joseph Z. Xu, Kihyuk Sohn, Wenhan Lu, David Berthelot,\n  Izzeddin Gur, Pranav Khaitan, Ke-Wei (Fiona) Huang, Kyriacos Koupparis,\n  Bernhard Kowatsch", "title": "Assessing Post-Disaster Damage from Satellite Imagery using\n  Semi-Supervised Learning Techniques", "comments": "NeurIPS 2020 Artificial Intelligence for Humanitarian Assistance and\n  Disaster Response Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  To respond to disasters such as earthquakes, wildfires, and armed conflicts,\nhumanitarian organizations require accurate and timely data in the form of\ndamage assessments, which indicate what buildings and population centers have\nbeen most affected. Recent research combines machine learning with remote\nsensing to automatically extract such information from satellite imagery,\nreducing manual labor and turn-around time. A major impediment to using machine\nlearning methods in real disaster response scenarios is the difficulty of\nobtaining a sufficient amount of labeled data to train a model for an unfolding\ndisaster. This paper shows a novel application of semi-supervised learning\n(SSL) to train models for damage assessment with a minimal amount of labeled\ndata and large amount of unlabeled data. We compare the performance of\nstate-of-the-art SSL methods, including MixMatch and FixMatch, to a supervised\nbaseline for the 2010 Haiti earthquake, 2017 Santa Rosa wildfire, and 2016\narmed conflict in Syria. We show how models trained with SSL methods can reach\nfully supervised performance despite using only a fraction of labeled data and\nidentify areas for further improvements.\n", "versions": [{"version": "v1", "created": "Tue, 24 Nov 2020 22:26:14 GMT"}], "update_date": "2020-12-01", "authors_parsed": [["Lee", "Jihyeon", "", "Fiona"], ["Xu", "Joseph Z.", "", "Fiona"], ["Sohn", "Kihyuk", "", "Fiona"], ["Lu", "Wenhan", "", "Fiona"], ["Berthelot", "David", "", "Fiona"], ["Gur", "Izzeddin", "", "Fiona"], ["Khaitan", "Pranav", "", "Fiona"], ["Ke-Wei", "", "", "Fiona"], ["Huang", "", ""], ["Koupparis", "Kyriacos", ""], ["Kowatsch", "Bernhard", ""]]}, {"id": "2011.14005", "submitter": "Naga Karthik Enamundram", "authors": "Enamundram M. V. Naga Karthik, Catherine Laporte, Farida Cheriet", "title": "Three-dimensional Segmentation of the Scoliotic Spine from MRI using\n  Unsupervised Volume-based MR-CT Synthesis", "comments": "To appear in the Proceedings of the SPIE Medical Imaging Conference\n  2021, San Diego, CA. 9 pages, 4 figures in total", "journal-ref": "Proceedings Volume 11596, SPIE Medical Imaging 2021: Image\n  Processing; 115961H", "doi": "10.1117/12.2580677", "report-no": null, "categories": "eess.IV cs.CV stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Vertebral bone segmentation from magnetic resonance (MR) images is a\nchallenging task. Due to the inherent nature of the modality to emphasize soft\ntissues of the body, common thresholding algorithms are ineffective in\ndetecting bones in MR images. On the other hand, it is relatively easier to\nsegment bones from CT images because of the high contrast between bones and the\nsurrounding regions. For this reason, we perform a cross-modality synthesis\nbetween MR and CT domains for simple thresholding-based segmentation of the\nvertebral bones. However, this implicitly assumes the availability of paired\nMR-CT data, which is rare, especially in the case of scoliotic patients. In\nthis paper, we present a completely unsupervised, fully three-dimensional (3D)\ncross-modality synthesis method for segmenting scoliotic spines. A 3D CycleGAN\nmodel is trained for an unpaired volume-to-volume translation across MR and CT\ndomains. Then, the Otsu thresholding algorithm is applied to the synthesized CT\nvolumes for easy segmentation of the vertebral bones. The resulting\nsegmentation is used to reconstruct a 3D model of the spine. We validate our\nmethod on 28 scoliotic vertebrae in 3 patients by computing the\npoint-to-surface mean distance between the landmark points for each vertebra\nobtained from pre-operative X-rays and the surface of the segmented vertebra.\nOur study results in a mean error of 3.41 $\\pm$ 1.06 mm. Based on qualitative\nand quantitative results, we conclude that our method is able to obtain a good\nsegmentation and 3D reconstruction of scoliotic spines, all after training from\nunpaired data in an unsupervised manner.\n", "versions": [{"version": "v1", "created": "Wed, 25 Nov 2020 18:34:52 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Karthik", "Enamundram M. V. Naga", ""], ["Laporte", "Catherine", ""], ["Cheriet", "Farida", ""]]}, {"id": "2011.14015", "submitter": "Udai Nagpal", "authors": "Udai G. Nagpal, David A Knowles", "title": "Active Learning in CNNs via Expected Improvement Maximization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning models such as Convolutional Neural Networks (CNNs) have\ndemonstrated high levels of effectiveness in a variety of domains, including\ncomputer vision and more recently, computational biology. However, training\neffective models often requires assembling and/or labeling large datasets,\nwhich may be prohibitively time-consuming or costly. Pool-based active learning\ntechniques have the potential to mitigate these issues, leveraging models\ntrained on limited data to selectively query unlabeled data points from a pool\nin an attempt to expedite the learning process. Here we present \"Dropout-based\nExpected IMprOvementS\" (DEIMOS), a flexible and computationally-efficient\napproach to active learning that queries points that are expected to maximize\nthe model's improvement across a representative sample of points. The proposed\nframework enables us to maintain a prediction covariance matrix capturing model\nuncertainty, and to dynamically update this matrix in order to generate diverse\nbatches of points in the batch-mode setting. Our active learning results\ndemonstrate that DEIMOS outperforms several existing baselines across multiple\nregression and classification tasks taken from computer vision and genomics.\n", "versions": [{"version": "v1", "created": "Fri, 27 Nov 2020 22:06:52 GMT"}], "update_date": "2020-12-01", "authors_parsed": [["Nagpal", "Udai G.", ""], ["Knowles", "David A", ""]]}, {"id": "2011.14021", "submitter": "Xingqian Xu", "authors": "Xingqian Xu, Zhifei Zhang, Zhaowen Wang, Brian Price, Zhonghao Wang,\n  Humphrey Shi", "title": "Rethinking Text Segmentation: A Novel Dataset and A Text-Specific\n  Refinement Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Text segmentation is a prerequisite in many real-world text-related tasks,\ne.g., text style transfer, and scene text removal. However, facing the lack of\nhigh-quality datasets and dedicated investigations, this critical prerequisite\nhas been left as an assumption in many works, and has been largely overlooked\nby current research. To bridge this gap, we proposed TextSeg, a large-scale\nfine-annotated text dataset with six types of annotations: word- and\ncharacter-wise bounding polygons, masks and transcriptions. We also introduce\nText Refinement Network (TexRNet), a novel text segmentation approach that\nadapts to the unique properties of text, e.g. non-convex boundary, diverse\ntexture, etc., which often impose burdens on traditional segmentation models.\nIn our TexRNet, we propose text specific network designs to address such\nchallenges, including key features pooling and attention-based similarity\nchecking. We also introduce trimap and discriminator losses that show\nsignificant improvement on text segmentation. Extensive experiments are carried\nout on both our TextSeg dataset and other existing datasets. We demonstrate\nthat TexRNet consistently improves text segmentation performance by nearly 2%\ncompared to other state-of-the-art segmentation methods. Our dataset and code\nwill be made available at\nhttps://github.com/SHI-Labs/Rethinking-Text-Segmentation.\n", "versions": [{"version": "v1", "created": "Fri, 27 Nov 2020 22:50:09 GMT"}], "update_date": "2020-12-01", "authors_parsed": [["Xu", "Xingqian", ""], ["Zhang", "Zhifei", ""], ["Wang", "Zhaowen", ""], ["Price", "Brian", ""], ["Wang", "Zhonghao", ""], ["Shi", "Humphrey", ""]]}, {"id": "2011.14027", "submitter": "Jack Lanchantin", "authors": "Jack Lanchantin, Tianlu Wang, Vicente Ordonez, Yanjun Qi", "title": "General Multi-label Image Classification with Transformers", "comments": "13 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Multi-label image classification is the task of predicting a set of labels\ncorresponding to objects, attributes or other entities present in an image. In\nthis work we propose the Classification Transformer (C-Tran), a general\nframework for multi-label image classification that leverages Transformers to\nexploit the complex dependencies among visual features and labels. Our approach\nconsists of a Transformer encoder trained to predict a set of target labels\ngiven an input set of masked labels, and visual features from a convolutional\nneural network. A key ingredient of our method is a label mask training\nobjective that uses a ternary encoding scheme to represent the state of the\nlabels as positive, negative, or unknown during training. Our model shows\nstate-of-the-art performance on challenging datasets such as COCO and Visual\nGenome. Moreover, because our model explicitly represents the uncertainty of\nlabels during training, it is more general by allowing us to produce improved\nresults for images with partial or extra label annotations during inference. We\ndemonstrate this additional capability in the COCO, Visual Genome, News500, and\nCUB image datasets.\n", "versions": [{"version": "v1", "created": "Fri, 27 Nov 2020 23:20:35 GMT"}], "update_date": "2020-12-01", "authors_parsed": [["Lanchantin", "Jack", ""], ["Wang", "Tianlu", ""], ["Ordonez", "Vicente", ""], ["Qi", "Yanjun", ""]]}, {"id": "2011.14031", "submitter": "Fnu Devvrit", "authors": "Devvrit, Minhao Cheng, Cho-Jui Hsieh, Inderjit Dhillon", "title": "Voting based ensemble improves robustness of defensive models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Developing robust models against adversarial perturbations has been an active\narea of research and many algorithms have been proposed to train individual\nrobust models. Taking these pretrained robust models, we aim to study whether\nit is possible to create an ensemble to further improve robustness. Several\nprevious attempts tackled this problem by ensembling the soft-label prediction\nand have been proved vulnerable based on the latest attack methods. In this\npaper, we show that if the robust training loss is diverse enough, a simple\nhard-label based voting ensemble can boost the robust error over each\nindividual model. Furthermore, given a pool of robust models, we develop a\nprincipled way to select which models to ensemble. Finally, to verify the\nimproved robustness, we conduct extensive experiments to study how to attack a\nvoting-based ensemble and develop several new white-box attacks. On CIFAR-10\ndataset, by ensembling several state-of-the-art pre-trained defense models, our\nmethod can achieve a 59.8% robust accuracy, outperforming all the existing\ndefensive models without using additional data.\n", "versions": [{"version": "v1", "created": "Sat, 28 Nov 2020 00:08:45 GMT"}], "update_date": "2020-12-01", "authors_parsed": [["Devvrit", "", ""], ["Cheng", "Minhao", ""], ["Hsieh", "Cho-Jui", ""], ["Dhillon", "Inderjit", ""]]}, {"id": "2011.14035", "submitter": "Constantinos Chamzas", "authors": "Dimitrios Chamzas, Constantinos Chamzas and Konstantinos Moustakas", "title": "cMinMax: A Fast Algorithm to Find the Corners of an N-dimensional Convex\n  Polytope", "comments": "Accepted in GRAPP 2021", "journal-ref": "GRAPP, 2021, 229-236", "doi": "10.5220/0010259002290236", "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  During the last years, the emerging field of Augmented & Virtual Reality\n(AR-VR) has seen tremendousgrowth. At the same time there is a trend to develop\nlow cost high-quality AR systems where computing poweris in demand. Feature\npoints are extensively used in these real-time frame-rate and 3D applications,\nthereforeefficient high-speed feature detectors are necessary. Corners are such\nspecial features and often are used as thefirst step in the marker alignment in\nAugmented Reality (AR). Corners are also used in image registration\nandrecognition, tracking, SLAM, robot path finding and 2D or 3D object\ndetection and retrieval. Therefore thereis a large number of corner detection\nalgorithms but most of them are too computationally intensive for use\ninreal-time applications of any complexity. Many times the border of the image\nis a convex polygon. For thisspecial, but quite common case, we have developed\na specific algorithm, cMinMax. The proposed algorithmis faster, approximately\nby a factor of 5 compared to the widely used Harris Corner Detection algorithm.\nInaddition is highly parallelizable. The algorithm is suitable for the fast\nregistration of markers in augmentedreality systems and in applications where a\ncomputationally efficient real time feature detector is necessary.The algorithm\ncan also be extended to N-dimensional polyhedrons.\n", "versions": [{"version": "v1", "created": "Sat, 28 Nov 2020 00:32:11 GMT"}, {"version": "v2", "created": "Wed, 24 Mar 2021 15:11:00 GMT"}], "update_date": "2021-03-25", "authors_parsed": [["Chamzas", "Dimitrios", ""], ["Chamzas", "Constantinos", ""], ["Moustakas", "Konstantinos", ""]]}, {"id": "2011.14036", "submitter": "Taro Makino", "authors": "Taro Makino, Stanislaw Jastrzebski, Witold Oleszkiewicz, Celin Chacko,\n  Robin Ehrenpreis, Naziya Samreen, Chloe Chhor, Eric Kim, Jiyon Lee, Kristine\n  Pysarenko, Beatriu Reig, Hildegard Toth, Divya Awal, Linda Du, Alice Kim,\n  James Park, Daniel K. Sodickson, Laura Heacock, Linda Moy, Kyunghyun Cho,\n  Krzysztof J. Geras", "title": "Differences between human and machine perception in medical diagnosis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.CY cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks (DNNs) show promise in image-based medical diagnosis,\nbut cannot be fully trusted since their performance can be severely degraded by\ndataset shifts to which human perception remains invariant. If we can better\nunderstand the differences between human and machine perception, we can\npotentially characterize and mitigate this effect. We therefore propose a\nframework for comparing human and machine perception in medical diagnosis. The\ntwo are compared with respect to their sensitivity to the removal of clinically\nmeaningful information, and to the regions of an image deemed most suspicious.\nDrawing inspiration from the natural image domain, we frame both comparisons in\nterms of perturbation robustness. The novelty of our framework is that separate\nanalyses are performed for subgroups with clinically meaningful differences. We\nargue that this is necessary in order to avert Simpson's paradox and draw\ncorrect conclusions. We demonstrate our framework with a case study in breast\ncancer screening, and reveal significant differences between radiologists and\nDNNs. We compare the two with respect to their robustness to Gaussian low-pass\nfiltering, performing a subgroup analysis on microcalcifications and soft\ntissue lesions. For microcalcifications, DNNs use a separate set of high\nfrequency components than radiologists, some of which lie outside the image\nregions considered most suspicious by radiologists. These features run the risk\nof being spurious, but if not, could represent potential new biomarkers. For\nsoft tissue lesions, the divergence between radiologists and DNNs is even\nstarker, with DNNs relying heavily on spurious high frequency components\nignored by radiologists. Importantly, this deviation in soft tissue lesions was\nonly observable through subgroup analysis, which highlights the importance of\nincorporating medical domain knowledge into our comparison framework.\n", "versions": [{"version": "v1", "created": "Sat, 28 Nov 2020 00:32:17 GMT"}], "update_date": "2020-12-01", "authors_parsed": [["Makino", "Taro", ""], ["Jastrzebski", "Stanislaw", ""], ["Oleszkiewicz", "Witold", ""], ["Chacko", "Celin", ""], ["Ehrenpreis", "Robin", ""], ["Samreen", "Naziya", ""], ["Chhor", "Chloe", ""], ["Kim", "Eric", ""], ["Lee", "Jiyon", ""], ["Pysarenko", "Kristine", ""], ["Reig", "Beatriu", ""], ["Toth", "Hildegard", ""], ["Awal", "Divya", ""], ["Du", "Linda", ""], ["Kim", "Alice", ""], ["Park", "James", ""], ["Sodickson", "Daniel K.", ""], ["Heacock", "Laura", ""], ["Moy", "Linda", ""], ["Cho", "Kyunghyun", ""], ["Geras", "Krzysztof J.", ""]]}, {"id": "2011.14054", "submitter": "Junru Wu", "authors": "Junru Wu, Xiang Yu, Buyu Liu, Zhangyang Wang, Manmohan Chandraker", "title": "Uncertainty-Aware Physically-Guided Proxy Tasks for Unseen Domain Face\n  Anti-spoofing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Face anti-spoofing (FAS) seeks to discriminate genuine faces from fake ones\narising from any type of spoofing attack. Due to the wide varieties of attacks,\nit is implausible to obtain training data that spans all attack types. We\npropose to leverage physical cues to attain better generalization on unseen\ndomains. As a specific demonstration, we use physically guided proxy cues such\nas depth, reflection, and material to complement our main anti-spoofing (a.k.a\nliveness detection) task, with the intuition that genuine faces across domains\nhave consistent face-like geometry, minimal reflection, and skin material. We\nintroduce a novel uncertainty-aware attention scheme that independently learns\nto weigh the relative contributions of the main and proxy tasks, preventing the\nover-confident issue with traditional attention modules. Further, we propose\nattribute-assisted hard negative mining to disentangle liveness-irrelevant\nfeatures with liveness features during learning. We evaluate extensively on\npublic benchmarks with intra-dataset and inter-dataset protocols. Our method\nachieves the superior performance especially in unseen domain generalization\nfor FAS.\n", "versions": [{"version": "v1", "created": "Sat, 28 Nov 2020 03:22:26 GMT"}], "update_date": "2020-12-01", "authors_parsed": [["Wu", "Junru", ""], ["Yu", "Xiang", ""], ["Liu", "Buyu", ""], ["Wang", "Zhangyang", ""], ["Chandraker", "Manmohan", ""]]}, {"id": "2011.14058", "submitter": "Wei He", "authors": "Zhongzhan Huang, Senwei Liang, Mingfu Liang, Wei He, Haizhao Yang", "title": "Efficient Attention Network: Accelerate Attention by Searching Where to\n  Plug", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, many plug-and-play self-attention modules are proposed to enhance\nthe model generalization by exploiting the internal information of deep\nconvolutional neural networks (CNNs). Previous works lay an emphasis on the\ndesign of attention module for specific functionality, e.g., light-weighted or\ntask-oriented attention. However, they ignore the importance of where to plug\nin the attention module since they connect the modules individually with each\nblock of the entire CNN backbone for granted, leading to incremental\ncomputational cost and number of parameters with the growth of network depth.\nThus, we propose a framework called Efficient Attention Network (EAN) to\nimprove the efficiency for the existing attention modules. In EAN, we leverage\nthe sharing mechanism (Huang et al. 2020) to share the attention module within\nthe backbone and search where to connect the shared attention module via\nreinforcement learning. Finally, we obtain the attention network with sparse\nconnections between the backbone and modules, while (1) maintaining accuracy\n(2) reducing extra parameter increment and (3) accelerating inference.\nExtensive experiments on widely-used benchmarks and popular attention networks\nshow the effectiveness of EAN. Furthermore, we empirically illustrate that our\nEAN has the capacity of transferring to other tasks and capturing the\ninformative features. The code is available at\nhttps://github.com/gbup-group/EAN-efficient-attention-network.\n", "versions": [{"version": "v1", "created": "Sat, 28 Nov 2020 03:31:08 GMT"}, {"version": "v2", "created": "Sun, 11 Jul 2021 12:44:58 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Huang", "Zhongzhan", ""], ["Liang", "Senwei", ""], ["Liang", "Mingfu", ""], ["He", "Wei", ""], ["Yang", "Haizhao", ""]]}, {"id": "2011.14070", "submitter": "Declan McIntosh", "authors": "Declan McIntosh, Tunai Porto Marques, Alexandra Branzan Albu, Rodney\n  Rountree, Fabio De Leo", "title": "Movement Tracks for the Automatic Detection of Fish Behavior in Videos", "comments": "8 pages, To be published in NeurIPS 2020 Workshop Tackling Climate\n  Change with Machine Learning", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Global warming is predicted to profoundly impact ocean ecosystems. Fish\nbehavior is an important indicator of changes in such marine environments.\nThus, the automatic identification of key fish behavior in videos represents a\nmuch needed tool for marine researchers, enabling them to study climate\nchange-related phenomena. We offer a dataset of sablefish (Anoplopoma fimbria)\nstartle behaviors in underwater videos, and investigate the use of deep\nlearning (DL) methods for behavior detection on it. Our proposed detection\nsystem identifies fish instances using DL-based frameworks, determines\ntrajectory tracks, derives novel behavior-specific features, and employs Long\nShort-Term Memory (LSTM) networks to identify startle behavior in sablefish.\nIts performance is studied by comparing it with a state-of-the-art DL-based\nvideo event detector.\n", "versions": [{"version": "v1", "created": "Sat, 28 Nov 2020 05:51:19 GMT"}], "update_date": "2020-12-01", "authors_parsed": [["McIntosh", "Declan", ""], ["Marques", "Tunai Porto", ""], ["Albu", "Alexandra Branzan", ""], ["Rountree", "Rodney", ""], ["De Leo", "Fabio", ""]]}, {"id": "2011.14076", "submitter": "Aaron Babier", "authors": "Aaron Babier, Binghao Zhang, Rafid Mahmood, Kevin L. Moore, Thomas G.\n  Purdie, Andrea L. McNiven, Timothy C. Y. Chan", "title": "OpenKBP: The open-access knowledge-based planning grand challenge", "comments": "26 pages, 6 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.med-ph cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The purpose of this work is to advance fair and consistent comparisons of\ndose prediction methods for knowledge-based planning (KBP) in radiation therapy\nresearch. We hosted OpenKBP, a 2020 AAPM Grand Challenge, and challenged\nparticipants to develop the best method for predicting the dose of contoured CT\nimages. The models were evaluated according to two separate scores: (1) dose\nscore, which evaluates the full 3D dose distributions, and (2) dose-volume\nhistogram (DVH) score, which evaluates a set DVH metrics. Participants were\ngiven the data of 340 patients who were treated for head-and-neck cancer with\nradiation therapy. The data was partitioned into training (n=200), validation\n(n=40), and testing (n=100) datasets. All participants performed training and\nvalidation with the corresponding datasets during the validation phase of the\nChallenge, and we ranked the models in the testing phase based on out-of-sample\nperformance. The Challenge attracted 195 participants from 28 countries, and 73\nof those participants formed 44 teams in the validation phase, which received a\ntotal of 1750 submissions. The testing phase garnered submissions from 28\nteams. On average, over the course of the validation phase, participants\nimproved the dose and DVH scores of their models by a factor of 2.7 and 5.7,\nrespectively. In the testing phase one model achieved significantly better dose\nand DVH score than the runner-up models. Lastly, many of the top performing\nteams reported using generalizable techniques (e.g., ensembles) to achieve\nhigher performance than their competition. This is the first competition for\nknowledge-based planning research, and it helped launch the first platform for\ncomparing KBP prediction methods fairly and consistently. The OpenKBP datasets\nare available publicly to help benchmark future KBP research, which has also\ndemocratized KBP research by making it accessible to everyone.\n", "versions": [{"version": "v1", "created": "Sat, 28 Nov 2020 06:45:06 GMT"}, {"version": "v2", "created": "Wed, 13 Jan 2021 19:46:18 GMT"}], "update_date": "2021-01-15", "authors_parsed": [["Babier", "Aaron", ""], ["Zhang", "Binghao", ""], ["Mahmood", "Rafid", ""], ["Moore", "Kevin L.", ""], ["Purdie", "Thomas G.", ""], ["McNiven", "Andrea L.", ""], ["Chan", "Timothy C. Y.", ""]]}, {"id": "2011.14087", "submitter": "Paul Wimmer", "authors": "Paul Wimmer, Jens Mehnert and Alexandru Condurache", "title": "FreezeNet: Full Performance by Reduced Storage Costs", "comments": "Conference Paper of the Asian Conference on Computer Vision (ACCV)\n  2020", "journal-ref": "ACCV (6) 2020: 685-701", "doi": "10.1007/978-3-030-69544-6\\_41", "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pruning generates sparse networks by setting parameters to zero. In this work\nwe improve one-shot pruning methods, applied before training, without adding\nany additional storage costs while preserving the sparse gradient computations.\nThe main difference to pruning is that we do not sparsify the network's weights\nbut learn just a few key parameters and keep the other ones fixed at their\nrandom initialized value. This mechanism is called freezing the parameters.\nThose frozen weights can be stored efficiently with a single 32bit random seed\nnumber. The parameters to be frozen are determined one-shot by a single for-\nand backward pass applied before training starts. We call the introduced method\nFreezeNet. In our experiments we show that FreezeNets achieve good results,\nespecially for extreme freezing rates. Freezing weights preserves the gradient\nflow throughout the network and consequently, FreezeNets train better and have\nan increased capacity compared to their pruned counterparts. On the\nclassification tasks MNIST and CIFAR-10/100 we outperform SNIP, in this setting\nthe best reported one-shot pruning method, applied before training. On MNIST,\nFreezeNet achieves 99.2% performance of the baseline LeNet-5-Caffe\narchitecture, while compressing the number of trained and stored parameters by\na factor of x 157.\n", "versions": [{"version": "v1", "created": "Sat, 28 Nov 2020 08:32:44 GMT"}], "update_date": "2021-07-28", "authors_parsed": [["Wimmer", "Paul", ""], ["Mehnert", "Jens", ""], ["Condurache", "Alexandru", ""]]}, {"id": "2011.14097", "submitter": "Shohreh Deldari", "authors": "Shohreh Deldari, Daniel V. Smith, Hao Xue, Flora D. Salim", "title": "Time Series Change Point Detection with Self-Supervised Contrastive\n  Predictive Coding", "comments": "Accepted at The WEB Conference 2021 (WWW'21)", "journal-ref": null, "doi": "10.1145/3442381.3449903", "report-no": null, "categories": "cs.LG cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Change Point Detection (CPD) methods identify the times associated with\nchanges in the trends and properties of time series data in order to describe\nthe underlying behaviour of the system. For instance, detecting the changes and\nanomalies associated with web service usage, application usage or human\nbehaviour can provide valuable insights for downstream modelling tasks. We\npropose a novel approach for self-supervised Time Series Change Point detection\nmethod based onContrastivePredictive coding (TS-CP^2). TS-CP^2 is the first\napproach to employ a contrastive learning strategy for CPD by learning an\nembedded representation that separates pairs of embeddings of time adjacent\nintervals from pairs of interval embeddings separated across time. Through\nextensive experiments on three diverse, widely used time series datasets, we\ndemonstrate that our method outperforms five state-of-the-art CPD methods,\nwhich include unsupervised and semi-supervisedapproaches. TS-CP^2 is shown to\nimprove the performance of methods that use either handcrafted statistical or\ntemporal features by 79.4% and deep learning-based methods by 17.0% with\nrespect to the F1-score averaged across the three datasets.\n", "versions": [{"version": "v1", "created": "Sat, 28 Nov 2020 09:36:18 GMT"}, {"version": "v2", "created": "Tue, 1 Dec 2020 23:21:33 GMT"}, {"version": "v3", "created": "Mon, 8 Feb 2021 22:26:19 GMT"}, {"version": "v4", "created": "Fri, 26 Feb 2021 09:20:41 GMT"}, {"version": "v5", "created": "Fri, 5 Mar 2021 00:24:56 GMT"}], "update_date": "2021-03-08", "authors_parsed": [["Deldari", "Shohreh", ""], ["Smith", "Daniel V.", ""], ["Xue", "Hao", ""], ["Salim", "Flora D.", ""]]}, {"id": "2011.14101", "submitter": "Florian Dubost", "authors": "Florian Dubost, Erin Hong, Daniel Y Fu, Nandita Bhaskhar, Siyi Tang,\n  Khaled Saab, Jared Dunnmon, Daniel Rubin, Christopher Lee-Messer", "title": "Semi-Supervised Learning for Sparsely-Labeled Sequential Data:\n  Application to Healthcare Video Processing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Labeled data is a critical resource for training and evaluating machine\nlearning models. However, many real-life datasets are only partially labeled.\nWe propose a semi-supervised machine learning training strategy to improve\nevent detection performance on sequential data, such as video recordings, when\nonly sparse labels are available, such as event start times without their\ncorresponding end times. Our method uses noisy guesses of the events' end times\nto train event detection models. Depending on how conservative these guesses\nare, mislabeled false positives may be introduced into the training set (i.e.,\nnegative sequences mislabeled as positives). We further propose a mathematical\nmodel for estimating how many inaccurate labels a model is exposed to, based on\nhow noisy the end time guesses are. Finally, we show that neural networks can\nimprove their detection performance by leveraging more training data with less\nconservative approximations despite the higher proportion of incorrect labels.\nWe adapt sequential versions of MNIST and CIFAR-10 to empirically evaluate our\nmethod, and find that our risk-tolerant strategy outperforms conservative\nestimates by 12 points of mean average precision for MNIST, and 3.5 points for\nCIFAR. Then, we leverage the proposed training strategy to tackle a real-life\napplication: processing continuous video recordings of epilepsy patients to\nimprove seizure detection, and show that our method outperforms baseline\nlabeling methods by 10 points of average precision.\n", "versions": [{"version": "v1", "created": "Sat, 28 Nov 2020 09:54:44 GMT"}, {"version": "v2", "created": "Tue, 1 Dec 2020 09:00:37 GMT"}, {"version": "v3", "created": "Thu, 25 Mar 2021 20:48:00 GMT"}], "update_date": "2021-03-29", "authors_parsed": [["Dubost", "Florian", ""], ["Hong", "Erin", ""], ["Fu", "Daniel Y", ""], ["Bhaskhar", "Nandita", ""], ["Tang", "Siyi", ""], ["Saab", "Khaled", ""], ["Dunnmon", "Jared", ""], ["Rubin", "Daniel", ""], ["Lee-Messer", "Christopher", ""]]}, {"id": "2011.14107", "submitter": "Hui-Po Wang", "authors": "Hui-Po Wang, Ning Yu, Mario Fritz", "title": "Hijack-GAN: Unintended-Use of Pretrained, Black-Box GANs", "comments": "Accepted by CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While Generative Adversarial Networks (GANs) show increasing performance and\nthe level of realism is becoming indistinguishable from natural images, this\nalso comes with high demands on data and computation. We show that\nstate-of-the-art GAN models -- such as they are being publicly released by\nresearchers and industry -- can be used for a range of applications beyond\nunconditional image generation. We achieve this by an iterative scheme that\nalso allows gaining control over the image generation process despite the\nhighly non-linear latent spaces of the latest GAN models. We demonstrate that\nthis opens up the possibility to re-use state-of-the-art, difficult to train,\npre-trained GANs with a high level of control even if only black-box access is\ngranted. Our work also raises concerns and awareness that the use cases of a\npublished GAN model may well reach beyond the creators' intention, which needs\nto be taken into account before a full public release. Code is available at\nhttps://github.com/a514514772/hijackgan.\n", "versions": [{"version": "v1", "created": "Sat, 28 Nov 2020 11:07:36 GMT"}, {"version": "v2", "created": "Mon, 29 Mar 2021 13:20:00 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Wang", "Hui-Po", ""], ["Yu", "Ning", ""], ["Fritz", "Mario", ""]]}, {"id": "2011.14123", "submitter": "Zhe Chu", "authors": "Zhe Chu, Mengkai Hu, Xiangyu Chen", "title": "Robotic grasp detection using a novel two-stage approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, deep learning has been successfully applied to robotic grasp\ndetection. Based on convolutional neural networks (CNNs), there have been lots\nof end-to-end detection approaches. But end-to-end approaches have strict\nrequirements for the dataset used for training the neural network models and\nit's hard to achieve in practical use. Therefore, we proposed a two-stage\napproach using particle swarm optimizer (PSO) candidate estimator and CNN to\ndetect the most likely grasp. Our approach achieved an accuracy of 92.8% on the\nCornell Grasp Dataset, which leaped into the front ranks of the existing\napproaches and is able to run at real-time speeds. After a small change of the\napproach, we can predict multiple grasps per object in the meantime so that an\nobject can be grasped in a variety of ways.\n", "versions": [{"version": "v1", "created": "Sat, 28 Nov 2020 12:26:35 GMT"}], "update_date": "2020-12-01", "authors_parsed": [["Chu", "Zhe", ""], ["Hu", "Mengkai", ""], ["Chen", "Xiangyu", ""]]}, {"id": "2011.14132", "submitter": "Quan Huu Cap", "authors": "Quan Huu Cap and Hitoshi Iyatomi and Atsushi Fukuda", "title": "MIINet: An Image Quality Improvement Framework for Supporting Medical\n  Diagnosis", "comments": "Accepted at the ICPR2020 Workshops", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Medical images have been indispensable and useful tools for supporting\nmedical experts in making diagnostic decisions. However, taken medical images\nespecially throat and endoscopy images are normally hazy, lack of focus, or\nuneven illumination. Thus, these could difficult the diagnosis process for\ndoctors. In this paper, we propose MIINet, a novel image-to-image translation\nnetwork for improving quality of medical images by unsupervised translating\nlow-quality images to the high-quality clean version. Our MIINet is not only\ncapable of generating high-resolution clean images, but also preserving the\nattributes of original images, making the diagnostic more favorable for\ndoctors. Experiments on dehazing 100 practical throat images show that our\nMIINet largely improves the mean doctor opinion score (MDOS), which assesses\nthe quality and the reproducibility of the images from the baseline of 2.36 to\n4.11, while dehazed images by CycleGAN got lower score of 3.83. The MIINet is\nconfirmed by three physicians to be satisfying in supporting throat disease\ndiagnostic from original low-quality images.\n", "versions": [{"version": "v1", "created": "Sat, 28 Nov 2020 13:44:42 GMT"}], "update_date": "2020-12-01", "authors_parsed": [["Cap", "Quan Huu", ""], ["Iyatomi", "Hitoshi", ""], ["Fukuda", "Atsushi", ""]]}, {"id": "2011.14133", "submitter": "Mohit Lamba", "authors": "Mohit Lamba and Atul Balaji and Kaushik Mitra", "title": "Towards Fast and Light-Weight Restoration of Dark Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The ability to capture good quality images in the dark and near-zero lux\nconditions has been a long-standing pursuit of the computer vision community.\nThe seminal work by Chen et al. [5] has especially caused renewed interest in\nthis area, resulting in methods that build on top of their work in a bid to\nimprove the reconstruction. However, for practical utility and deployment of\nlow-light enhancement algorithms on edge devices such as embedded systems,\nsurveillance cameras, autonomous robots and smartphones, the solution must\nrespect additional constraints such as limited GPU memory and processing power.\nWith this in mind, we propose a deep neural network architecture that aims to\nstrike a balance between the network latency, memory utilization, model\nparameters, and reconstruction quality. The key idea is to forbid computations\nin the High-Resolution (HR) space and limit them to a Low-Resolution (LR)\nspace. However, doing the bulk of computations in the LR space causes artifacts\nin the restored image. We thus propose Pack and UnPack operations, which allow\nus to effectively transit between the HR and LR spaces without incurring much\nartifacts in the restored image. We show that we can enhance a full resolution,\n2848 x 4256, extremely dark single-image in the ballpark of 3 seconds even on a\nCPU. We achieve this with 2 - 7x fewer model parameters, 2 - 3x lower memory\nutilization, 5 - 20x speed up and yet maintain a competitive image\nreconstruction quality compared to the state-of-the-art algorithms.\n", "versions": [{"version": "v1", "created": "Sat, 28 Nov 2020 13:53:50 GMT"}], "update_date": "2020-12-01", "authors_parsed": [["Lamba", "Mohit", ""], ["Balaji", "Atul", ""], ["Mitra", "Kaushik", ""]]}, {"id": "2011.14134", "submitter": "Soumick Chatterjee", "authors": "Soumick Chatterjee, Alessandro Sciarra, Max D\\\"unnwald, Steffen\n  Oeltze-Jafra, Andreas N\\\"urnberger and Oliver Speck", "title": "Retrospective Motion Correction of MR Images using Prior-Assisted Deep\n  Learning", "comments": null, "journal-ref": "Medical Imaging Meets NeurIPS 2020", "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In MRI, motion artefacts are among the most common types of artefacts. They\ncan degrade images and render them unusable for accurate diagnosis. Traditional\nmethods, such as prospective or retrospective motion correction, have been\nproposed to avoid or alleviate motion artefacts. Recently, several other\nmethods based on deep learning approaches have been proposed to solve this\nproblem. This work proposes to enhance the performance of existing deep\nlearning models by the inclusion of additional information present as image\npriors. The proposed approach has shown promising results and will be further\ninvestigated for clinical validity.\n", "versions": [{"version": "v1", "created": "Sat, 28 Nov 2020 14:03:59 GMT"}], "update_date": "2020-12-01", "authors_parsed": [["Chatterjee", "Soumick", ""], ["Sciarra", "Alessandro", ""], ["D\u00fcnnwald", "Max", ""], ["Oeltze-Jafra", "Steffen", ""], ["N\u00fcrnberger", "Andreas", ""], ["Speck", "Oliver", ""]]}, {"id": "2011.14141", "submitter": "Shariq Bhat", "authors": "Shariq Farooq Bhat, Ibraheem Alhashim, Peter Wonka", "title": "AdaBins: Depth Estimation using Adaptive Bins", "comments": "13 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We address the problem of estimating a high quality dense depth map from a\nsingle RGB input image. We start out with a baseline encoder-decoder\nconvolutional neural network architecture and pose the question of how the\nglobal processing of information can help improve overall depth estimation. To\nthis end, we propose a transformer-based architecture block that divides the\ndepth range into bins whose center value is estimated adaptively per image. The\nfinal depth values are estimated as linear combinations of the bin centers. We\ncall our new building block AdaBins. Our results show a decisive improvement\nover the state-of-the-art on several popular depth datasets across all metrics.\nWe also validate the effectiveness of the proposed block with an ablation study\nand provide the code and corresponding pre-trained weights of the new\nstate-of-the-art model.\n", "versions": [{"version": "v1", "created": "Sat, 28 Nov 2020 14:40:45 GMT"}], "update_date": "2020-12-01", "authors_parsed": [["Bhat", "Shariq Farooq", ""], ["Alhashim", "Ibraheem", ""], ["Wonka", "Peter", ""]]}, {"id": "2011.14143", "submitter": "Tarun Yenamandra", "authors": "Tarun Yenamandra, Ayush Tewari, Florian Bernard, Hans-Peter Seidel,\n  Mohamed Elgharib, Daniel Cremers, Christian Theobalt", "title": "i3DMM: Deep Implicit 3D Morphable Model of Human Heads", "comments": "Project page: http://gvv.mpi-inf.mpg.de/projects/i3DMM/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the first deep implicit 3D morphable model (i3DMM) of full heads.\nUnlike earlier morphable face models it not only captures identity-specific\ngeometry, texture, and expressions of the frontal face, but also models the\nentire head, including hair. We collect a new dataset consisting of 64 people\nwith different expressions and hairstyles to train i3DMM. Our approach has the\nfollowing favorable properties: (i) It is the first full head morphable model\nthat includes hair. (ii) In contrast to mesh-based models it can be trained on\nmerely rigidly aligned scans, without requiring difficult non-rigid\nregistration. (iii) We design a novel architecture to decouple the shape model\ninto an implicit reference shape and a deformation of this reference shape.\nWith that, dense correspondences between shapes can be learned implicitly. (iv)\nThis architecture allows us to semantically disentangle the geometry and color\ncomponents, as color is learned in the reference space. Geometry is further\ndisentangled as identity, expressions, and hairstyle, while color is\ndisentangled as identity and hairstyle components. We show the merits of i3DMM\nusing ablation studies, comparisons to state-of-the-art models, and\napplications such as semantic head editing and texture transfer. We will make\nour model publicly available.\n", "versions": [{"version": "v1", "created": "Sat, 28 Nov 2020 15:01:53 GMT"}], "update_date": "2020-12-01", "authors_parsed": [["Yenamandra", "Tarun", ""], ["Tewari", "Ayush", ""], ["Bernard", "Florian", ""], ["Seidel", "Hans-Peter", ""], ["Elgharib", "Mohamed", ""], ["Cremers", "Daniel", ""], ["Theobalt", "Christian", ""]]}, {"id": "2011.14150", "submitter": "Yuhui Xu", "authors": "Yuhui Xu, Lingxi Xie, Cihang Xie, Jieru Mei, Siyuan Qiao, Wei Shen,\n  Hongkai Xiong, Alan Yuille", "title": "Batch Normalization with Enhanced Linear Transformation", "comments": "12 pages. The code is available at\n  https://github.com/yuhuixu1993/BNET", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Batch normalization (BN) is a fundamental unit in modern deep networks, in\nwhich a linear transformation module was designed for improving BN's\nflexibility of fitting complex data distributions. In this paper, we\ndemonstrate properly enhancing this linear transformation module can\neffectively improve the ability of BN. Specifically, rather than using a single\nneuron, we propose to additionally consider each neuron's neighborhood for\ncalculating the outputs of the linear transformation. Our method, named BNET,\ncan be implemented with 2-3 lines of code in most deep learning libraries.\nDespite the simplicity, BNET brings consistent performance gains over a wide\nrange of backbones and visual benchmarks. Moreover, we verify that BNET\naccelerates the convergence of network training and enhances spatial\ninformation by assigning the important neurons with larger weights accordingly.\nThe code is available at https://github.com/yuhuixu1993/BNET.\n", "versions": [{"version": "v1", "created": "Sat, 28 Nov 2020 15:42:36 GMT"}], "update_date": "2020-12-01", "authors_parsed": [["Xu", "Yuhui", ""], ["Xie", "Lingxi", ""], ["Xie", "Cihang", ""], ["Mei", "Jieru", ""], ["Qiao", "Siyuan", ""], ["Shen", "Wei", ""], ["Xiong", "Hongkai", ""], ["Yuille", "Alan", ""]]}, {"id": "2011.14155", "submitter": "Yutong Xie", "authors": "Jianpeng Zhang, Yutong Xie, Yan Wang, Yong Xia", "title": "Inter-slice Context Residual Learning for 3D Medical Image Segmentation", "comments": "Accpeted by IEEE-TMI", "journal-ref": "IEEE Trans. Med. Imaging (2020)", "doi": "10.1109/TMI.2020.3034995", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Automated and accurate 3D medical image segmentation plays an essential role\nin assisting medical professionals to evaluate disease progresses and make fast\ntherapeutic schedules. Although deep convolutional neural networks (DCNNs) have\nwidely applied to this task, the accuracy of these models still need to be\nfurther improved mainly due to their limited ability to 3D context perception.\nIn this paper, we propose the 3D context residual network (ConResNet) for the\naccurate segmentation of 3D medical images. This model consists of an encoder,\na segmentation decoder, and a context residual decoder. We design the context\nresidual module and use it to bridge both decoders at each scale. Each context\nresidual module contains both context residual mapping and context attention\nmapping, the formal aims to explicitly learn the inter-slice context\ninformation and the latter uses such context as a kind of attention to boost\nthe segmentation accuracy. We evaluated this model on the MICCAI 2018 Brain\nTumor Segmentation (BraTS) dataset and NIH Pancreas Segmentation (Pancreas-CT)\ndataset. Our results not only demonstrate the effectiveness of the proposed 3D\ncontext residual learning scheme but also indicate that the proposed ConResNet\nis more accurate than six top-ranking methods in brain tumor segmentation and\nseven top-ranking methods in pancreas segmentation. Code is available at\nhttps://git.io/ConResNet\n", "versions": [{"version": "v1", "created": "Sat, 28 Nov 2020 16:03:39 GMT"}], "update_date": "2020-12-01", "authors_parsed": [["Zhang", "Jianpeng", ""], ["Xie", "Yutong", ""], ["Wang", "Yan", ""], ["Xia", "Yong", ""]]}, {"id": "2011.14164", "submitter": "Nanqing Dong", "authors": "Nanqing Dong, Michael Kampffmeyer, Xiaodan Liang, Min Xu, Irina\n  Voiculescu, Eric P. Xing", "title": "Towards Robust Medical Image Segmentation on Small-Scale Data with\n  Incomplete Labels", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The data-driven nature of deep learning models for semantic segmentation\nrequires a large number of pixel-level annotations. However, large-scale and\nfully labeled medical datasets are often unavailable for practical tasks.\nRecently, partially supervised methods have been proposed to utilize images\nwith incomplete labels to mitigate the data scarcity problem in the medical\ndomain. As an emerging research area, the breakthroughs made by existing\nmethods rely on either large-scale data or complex model design, which makes\nthem 1) less practical for certain real-life tasks and 2) less robust for\nsmall-scale data. It is time to step back and think about the robustness of\npartially supervised methods and how to maximally utilize small-scale and\npartially labeled data for medical image segmentation tasks. To bridge the\nmethodological gaps in label-efficient deep learning with partial supervision,\nwe propose RAMP, a simple yet efficient data augmentation framework for\npartially supervised medical image segmentation by exploiting the assumption\nthat patients share anatomical similarities. We systematically evaluate RAMP\nand the previous methods in various controlled multi-structure segmentation\ntasks. Compared to the mainstream approaches, RAMP consistently improves the\nperformance of traditional segmentation networks on small-scale partially\nlabeled data and utilize additional image-wise weak annotations.\n", "versions": [{"version": "v1", "created": "Sat, 28 Nov 2020 16:31:00 GMT"}], "update_date": "2020-12-01", "authors_parsed": [["Dong", "Nanqing", ""], ["Kampffmeyer", "Michael", ""], ["Liang", "Xiaodan", ""], ["Xu", "Min", ""], ["Voiculescu", "Irina", ""], ["Xing", "Eric P.", ""]]}, {"id": "2011.14196", "submitter": "Seyed Mohsen Hosseini", "authors": "Seyed Mohsen Hosseini", "title": "Lattice Fusion Networks for Image Denoising", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A novel method for feature fusion in convolutional neural networks is\nproposed in this paper. Different feature fusion techniques are suggested to\nfacilitate the flow of information and improve the training of deep neural\nnetworks. Some of these techniques as well as the proposed network can be\nconsidered a type of Directed Acyclic Graph (DAG) Network, where a layer can\nreceive inputs from other layers and have outputs to other layers. In the\nproposed general framework of Lattice Fusion Network (LFNet), feature maps of\neach convolutional layer are passed to other layers based on a lattice graph\nstructure, where nodes are convolutional layers. To evaluate the performance of\nthe proposed architecture, different designs based on the general framework of\nLFNet are implemented for the task of image denoising. This task is used as an\nexample where training deep convolutional networks is needed. Results are\ncompared with state of the art methods. The proposed network is able to achieve\nbetter results with far fewer learnable parameters, which shows the\neffectiveness of LFNets for training of deep neural networks.\n", "versions": [{"version": "v1", "created": "Sat, 28 Nov 2020 18:57:54 GMT"}, {"version": "v2", "created": "Fri, 12 Feb 2021 04:29:24 GMT"}, {"version": "v3", "created": "Thu, 1 Jul 2021 17:27:18 GMT"}], "update_date": "2021-07-02", "authors_parsed": [["Hosseini", "Seyed Mohsen", ""]]}, {"id": "2011.14200", "submitter": "Sandesh Ramesh", "authors": "Sandesh Ramesh, Manoj Kumar M V, and Sanjay H A", "title": "E-Pro: Euler Angle and Probabilistic Model for Face Detection and\n  Recognition", "comments": "4th International Conference on Inventive Systems and Control\n  (ICISC), 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  It is human nature to give prime importance to facial appearances. Often, to\nlook good is to feel good. Also, facial features are unique to every individual\non this planet, which means it is a source of vital information. This work\nproposes a framework named E-Pro for the detection and recognition of faces by\ntaking facial images as inputs. E-Pro has its potential application in various\ndomains, namely attendance, surveillance, crowd monitoring, biometric-based\nauthentication etc. E-Pro is developed here as a mobile application that aims\nto aid lecturers to mark attendance in a classroom by detecting and recognizing\nthe faces of students from a picture clicked through the app. E-Pro has been\ndeveloped using Google Firebase Face Recognition APIs, which uses Euler Angles,\nand Probabilistic Model. E-Pro has been tested on stock images and the\nexperimental results are promising.\n", "versions": [{"version": "v1", "created": "Sat, 28 Nov 2020 19:12:39 GMT"}], "update_date": "2020-12-01", "authors_parsed": [["Ramesh", "Sandesh", ""], ["M", "Manoj Kumar", "V"], ["A", "Sanjay H", ""]]}, {"id": "2011.14204", "submitter": "Ayush Jaiswal", "authors": "Ayush Jaiswal, Yue Wu, Pradeep Natarajan, Premkumar Natarajan", "title": "Class-agnostic Object Detection", "comments": "To appear in Proceedings of WACV 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Object detection models perform well at localizing and classifying objects\nthat they are shown during training. However, due to the difficulty and cost\nassociated with creating and annotating detection datasets, trained models\ndetect a limited number of object types with unknown objects treated as\nbackground content. This hinders the adoption of conventional detectors in\nreal-world applications like large-scale object matching, visual grounding,\nvisual relation prediction, obstacle detection (where it is more important to\ndetermine the presence and location of objects than to find specific types),\netc. We propose class-agnostic object detection as a new problem that focuses\non detecting objects irrespective of their object-classes. Specifically, the\ngoal is to predict bounding boxes for all objects in an image but not their\nobject-classes. The predicted boxes can then be consumed by another system to\nperform application-specific classification, retrieval, etc. We propose\ntraining and evaluation protocols for benchmarking class-agnostic detectors to\nadvance future research in this domain. Finally, we propose (1) baseline\nmethods and (2) a new adversarial learning framework for class-agnostic\ndetection that forces the model to exclude class-specific information from\nfeatures used for predictions. Experimental results show that adversarial\nlearning improves class-agnostic detection efficacy.\n", "versions": [{"version": "v1", "created": "Sat, 28 Nov 2020 19:22:38 GMT"}], "update_date": "2020-12-01", "authors_parsed": [["Jaiswal", "Ayush", ""], ["Wu", "Yue", ""], ["Natarajan", "Pradeep", ""], ["Natarajan", "Premkumar", ""]]}, {"id": "2011.14211", "submitter": "Bingzhe Wei", "authors": "Hongbin Pei, Bingzhe Wei, Kevin Chen-Chuan Chang, Chunxu Zhang, Bo\n  Yang", "title": "Curvature Regularization to Prevent Distortion in Graph Embedding", "comments": "Published as a conference paper at NeurIPS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent research on graph embedding has achieved success in various\napplications. Most graph embedding methods preserve the proximity in a graph\ninto a manifold in an embedding space. We argue an important but neglected\nproblem about this proximity-preserving strategy: Graph topology patterns,\nwhile preserved well into an embedding manifold by preserving proximity, may\ndistort in the ambient embedding Euclidean space, and hence to detect them\nbecomes difficult for machine learning models. To address the problem, we\npropose curvature regularization, to enforce flatness for embedding manifolds,\nthereby preventing the distortion. We present a novel angle-based sectional\ncurvature, termed ABS curvature, and accordingly three kinds of curvature\nregularization to induce flat embedding manifolds during graph embedding. We\nintegrate curvature regularization into five popular proximity-preserving\nembedding methods, and empirical results in two applications show significant\nimprovements on a wide range of open graph datasets.\n", "versions": [{"version": "v1", "created": "Sat, 28 Nov 2020 20:16:24 GMT"}], "update_date": "2020-12-01", "authors_parsed": [["Pei", "Hongbin", ""], ["Wei", "Bingzhe", ""], ["Chang", "Kevin Chen-Chuan", ""], ["Zhang", "Chunxu", ""], ["Yang", "Bo", ""]]}, {"id": "2011.14214", "submitter": "Anadi Chaman", "authors": "Anadi Chaman (1), Ivan Dokmani\\'c (2) ((1) University of Illinois at\n  Urbana-Champaign, (2) University of Basel)", "title": "Truly shift-invariant convolutional neural networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Thanks to the use of convolution and pooling layers, convolutional neural\nnetworks were for a long time thought to be shift-invariant. However, recent\nworks have shown that the output of a CNN can change significantly with small\nshifts in input: a problem caused by the presence of downsampling (stride)\nlayers. The existing solutions rely either on data augmentation or on\nanti-aliasing, both of which have limitations and neither of which enables\nperfect shift invariance. Additionally, the gains obtained from these methods\ndo not extend to image patterns not seen during training. To address these\nchallenges, we propose adaptive polyphase sampling (APS), a simple sub-sampling\nscheme that allows convolutional neural networks to achieve 100% consistency in\nclassification performance under shifts, without any loss in accuracy. With\nAPS, the networks exhibit perfect consistency to shifts even before training,\nmaking it the first approach that makes convolutional neural networks truly\nshift-invariant.\n", "versions": [{"version": "v1", "created": "Sat, 28 Nov 2020 20:57:35 GMT"}, {"version": "v2", "created": "Tue, 1 Dec 2020 12:46:12 GMT"}, {"version": "v3", "created": "Fri, 4 Dec 2020 12:18:15 GMT"}, {"version": "v4", "created": "Tue, 30 Mar 2021 19:47:57 GMT"}], "update_date": "2021-04-01", "authors_parsed": [["Chaman", "Anadi", ""], ["Dokmani\u0107", "Ivan", ""]]}, {"id": "2011.14218", "submitter": "Debayan Deb", "authors": "Debayan Deb, Xiaoming Liu, Anil K. Jain", "title": "FaceGuard: A Self-Supervised Defense Against Adversarial Face Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Prevailing defense mechanisms against adversarial face images tend to overfit\nto the adversarial perturbations in the training set and fail to generalize to\nunseen adversarial attacks. We propose a new self-supervised adversarial\ndefense framework, namely FaceGuard, that can automatically detect, localize,\nand purify a wide variety of adversarial faces without utilizing pre-computed\nadversarial training samples. During training, FaceGuard automatically\nsynthesizes challenging and diverse adversarial attacks, enabling a classifier\nto learn to distinguish them from real faces and a purifier attempts to remove\nthe adversarial perturbations in the image space. Experimental results on LFW\ndataset show that FaceGuard can achieve 99.81% detection accuracy on six unseen\nadversarial attack types. In addition, the proposed method can enhance the face\nrecognition performance of ArcFace from 34.27% TAR @ 0.1% FAR under no defense\nto 77.46% TAR @ 0.1% FAR.\n", "versions": [{"version": "v1", "created": "Sat, 28 Nov 2020 21:18:46 GMT"}, {"version": "v2", "created": "Mon, 5 Apr 2021 20:37:56 GMT"}], "update_date": "2021-04-07", "authors_parsed": [["Deb", "Debayan", ""], ["Liu", "Xiaoming", ""], ["Jain", "Anil K.", ""]]}, {"id": "2011.14229", "submitter": "Jian Wang", "authors": "Jian Wang, Miaomiao Zhang", "title": "Deep Learning for Regularization Prediction in Diffeomorphic Image\n  Registration", "comments": "17 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a predictive model for estimating regularization\nparameters of diffeomorphic image registration. We introduce a novel framework\nthat automatically determines the parameters controlling the smoothness of\ndiffeomorphic transformations. Our method significantly reduces the effort of\nparameter tuning, which is time and labor-consuming. To achieve the goal, we\ndevelop a predictive model based on deep convolutional neural networks (CNN)\nthat learns the mapping between pairwise images and the regularization\nparameter of image registration. In contrast to previous methods that estimate\nsuch parameters in a high-dimensional image space, our model is built in an\nefficient bandlimited space with much lower dimensions. We demonstrate the\neffectiveness of our model on both 2D synthetic data and 3D real brain images.\nExperimental results show that our model not only predicts appropriate\nregularization parameters for image registration, but also improving the\nnetwork training in terms of time and memory efficiency.\n", "versions": [{"version": "v1", "created": "Sat, 28 Nov 2020 22:56:44 GMT"}], "update_date": "2020-12-01", "authors_parsed": [["Wang", "Jian", ""], ["Zhang", "Miaomiao", ""]]}, {"id": "2011.14259", "submitter": "Laureano Moro-Velazquez", "authors": "Julian D. Arias-Londo\\~no, Jorge A. Gomez-Garcia, Laureano\n  Moro-Velazquez, Juan I. Godino-Llorente", "title": "Artificial Intelligence applied to chest X-Ray images for the automatic\n  detection of COVID-19. A thoughtful evaluation approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current standard protocols used in the clinic for diagnosing COVID-19 include\nmolecular or antigen tests, generally complemented by a plain chest X-Ray. The\ncombined analysis aims to reduce the significant number of false negatives of\nthese tests, but also to provide complementary evidence about the presence and\nseverity of the disease. However, the procedure is not free of errors, and the\ninterpretation of the chest X-Ray is only restricted to radiologists due to its\ncomplexity. With the long term goal to provide new evidence for the diagnosis,\nthis paper presents an evaluation of different methods based on a deep neural\nnetwork. These are the first steps to develop an automatic COVID-19 diagnosis\ntool using chest X-Ray images, that would additionally differentiate between\ncontrols, pneumonia or COVID-19 groups. The paper describes the process\nfollowed to train a Convolutional Neural Network with a dataset of more than\n79,500 X-Ray images compiled from different sources, including more than 8,500\nCOVID-19 examples. For the sake of evaluation and comparison of the models\ndeveloped, three different experiments were carried out following three\npreprocessing schemes. The aim is to evaluate how preprocessing the data\naffects the results and improves its explainability. Likewise, a critical\nanalysis is carried out about different variability issues that might\ncompromise the system and the effects on the performance. With the employed\nmethodology, a 91.5% classification accuracy is obtained, with a 87.4% average\nrecall for the worst but most explainable experiment, which requires a previous\nautomatic segmentation of the lungs region.\n", "versions": [{"version": "v1", "created": "Sun, 29 Nov 2020 02:48:39 GMT"}], "update_date": "2020-12-01", "authors_parsed": [["Arias-Londo\u00f1o", "Julian D.", ""], ["Gomez-Garcia", "Jorge A.", ""], ["Moro-Velazquez", "Laureano", ""], ["Godino-Llorente", "Juan I.", ""]]}, {"id": "2011.14265", "submitter": "Chunhua Shen", "authors": "Hu Wang, Peng Chen, Bohan Zhuang, Chunhua Shen", "title": "Fully Quantized Image Super-Resolution Networks", "comments": "Results updated", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  With the rising popularity of intelligent mobile devices, it is of great\npractical significance to develop accurate, realtime and energy-efficient image\nSuper-Resolution (SR) inference methods. A prevailing method for improving the\ninference efficiency is model quantization, which allows for replacing the\nexpensive floating-point operations with efficient fixed-point or bitwise\narithmetic. To date, it is still challenging for quantized SR frameworks to\ndeliver feasible accuracy-efficiency trade-off. Here, we propose a Fully\nQuantized image Super-Resolution framework (FQSR) to jointly optimize\nefficiency and accuracy. In particular, we target on obtaining end-to-end\nquantized models for all layers, especially including skip connections, which\nwas rarely addressed in the literature. We further identify training obstacles\nfaced by low-bit SR networks and propose two novel methods accordingly. The two\ndifficulites are caused by 1) activation and weight distributions being vastly\ndistinctive in different layers; 2) the inaccurate approximation of the\nquantization. We apply our quantization scheme on multiple mainstream\nsuper-resolution architectures, including SRResNet, SRGAN and EDSR.\nExperimental results show that our FQSR using low bits quantization can achieve\non par performance compared with the full-precision counterparts on five\nbenchmark datasets and surpass state-of-the-art quantized SR methods with\nsignificantly reduced computational cost and memory consumption.\n", "versions": [{"version": "v1", "created": "Sun, 29 Nov 2020 03:53:49 GMT"}, {"version": "v2", "created": "Mon, 19 Apr 2021 03:38:50 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Wang", "Hu", ""], ["Chen", "Peng", ""], ["Zhuang", "Bohan", ""], ["Shen", "Chunhua", ""]]}, {"id": "2011.14272", "submitter": "Chongzhen Zhang", "authors": "Chongzhen Zhang, Yang Tang, Chaoqiang Zhao, Qiyu Sun, Zhencheng Ye and\n  J\\\"urgen Kurths", "title": "Multi-task GANs for Semantic Segmentation and Depth Completion with\n  Cycle Consistency", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Semantic segmentation and depth completion are two challenging tasks in scene\nunderstanding, and they are widely used in robotics and autonomous driving.\nAlthough several works are proposed to jointly train these two tasks using some\nsmall modifications, like changing the last layer, the result of one task is\nnot utilized to improve the performance of the other one despite that there are\nsome similarities between these two tasks. In this paper, we propose multi-task\ngenerative adversarial networks (Multi-task GANs), which are not only competent\nin semantic segmentation and depth completion, but also improve the accuracy of\ndepth completion through generated semantic images. In addition, we improve the\ndetails of generated semantic images based on CycleGAN by introducing\nmulti-scale spatial pooling blocks and the structural similarity reconstruction\nloss. Furthermore, considering the inner consistency between semantic and\ngeometric structures, we develop a semantic-guided smoothness loss to improve\ndepth completion results. Extensive experiments on Cityscapes dataset and KITTI\ndepth completion benchmark show that the Multi-task GANs are capable of\nachieving competitive performance for both semantic segmentation and depth\ncompletion tasks.\n", "versions": [{"version": "v1", "created": "Sun, 29 Nov 2020 04:12:16 GMT"}], "update_date": "2020-12-01", "authors_parsed": [["Zhang", "Chongzhen", ""], ["Tang", "Yang", ""], ["Zhao", "Chaoqiang", ""], ["Sun", "Qiyu", ""], ["Ye", "Zhencheng", ""], ["Kurths", "J\u00fcrgen", ""]]}, {"id": "2011.14284", "submitter": "Ujjwal Verma", "authors": "Girisha S, Ujjwal Verma, Manohara Pai M M and Radhika Pai", "title": "UVid-Net: Enhanced Semantic Segmentation of UAV Aerial Videos by\n  Embedding Temporal Information", "comments": "Includes additional discussions/results and comparison with SOTA\n  methods. Published in IEEE JSTARS", "journal-ref": "Published in IEEE Journal of Selected Topics in Applied Earth\n  Observations and Remote Sensing, vol. 14, pp. 4115-4127, 2021", "doi": "10.1109/JSTARS.2021.3069909", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semantic segmentation of aerial videos has been extensively used for decision\nmaking in monitoring environmental changes, urban planning, and disaster\nmanagement. The reliability of these decision support systems is dependent on\nthe accuracy of the video semantic segmentation algorithms. The existing CNN\nbased video semantic segmentation methods have enhanced the image semantic\nsegmentation methods by incorporating an additional module such as LSTM or\noptical flow for computing temporal dynamics of the video which is a\ncomputational overhead. The proposed research work modifies the CNN\narchitecture by incorporating temporal information to improve the efficiency of\nvideo semantic segmentation.\n  In this work, an enhanced encoder-decoder based CNN architecture (UVid-Net)\nis proposed for UAV video semantic segmentation. The encoder of the proposed\narchitecture embeds temporal information for temporally consistent labelling.\nThe decoder is enhanced by introducing the feature-refiner module, which aids\nin accurate localization of the class labels. The proposed UVid-Net\narchitecture for UAV video semantic segmentation is quantitatively evaluated on\nextended ManipalUAVid dataset. The performance metric mIoU of 0.79 has been\nobserved which is significantly greater than the other state-of-the-art\nalgorithms. Further, the proposed work produced promising results even for the\npre-trained model of UVid-Net on urban street scene with fine tuning the final\nlayer on UAV aerial videos.\n", "versions": [{"version": "v1", "created": "Sun, 29 Nov 2020 05:01:39 GMT"}, {"version": "v2", "created": "Thu, 27 May 2021 13:04:56 GMT"}], "update_date": "2021-05-28", "authors_parsed": [["S", "Girisha", ""], ["Verma", "Ujjwal", ""], ["M", "Manohara Pai M", ""], ["Pai", "Radhika", ""]]}, {"id": "2011.14285", "submitter": "Haoxi Ran", "authors": "Haoxi Ran, Li Lu", "title": "Deeper or Wider Networks of Point Clouds with Self-attention?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Prevalence of deeper networks driven by self-attention is in stark contrast\nto underexplored point-based methods. In this paper, we propose groupwise\nself-attention as the basic block to construct our network: SepNet. Our\nproposed module can effectively capture both local and global dependencies.\nThis module computes the features of a group based on the summation of the\nweighted features of any point within the group. For convenience, we generalize\ngroupwise operations to assemble this module. To further facilitate our\nnetworks, we deepen and widen SepNet on the tasks of segmentation and\nclassification respectively, and verify its practicality. Specifically, SepNet\nachieves state-of-the-art for the tasks of classification and segmentation on\nmost of the datasets. We show empirical evidence that SepNet can obtain extra\naccuracy in classification or segmentation from increased width or depth,\nrespectively.\n", "versions": [{"version": "v1", "created": "Sun, 29 Nov 2020 05:03:06 GMT"}], "update_date": "2020-12-01", "authors_parsed": [["Ran", "Haoxi", ""], ["Lu", "Li", ""]]}, {"id": "2011.14288", "submitter": "Chunhua Shen", "authors": "Yutong Dai, Hao Lu, Chunhua Shen", "title": "Learning Affinity-Aware Upsampling for Deep Image Matting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  We show that learning affinity in upsampling provides an effective and\nefficient approach to exploit pairwise interactions in deep networks.\nSecond-order features are commonly used in dense prediction to build adjacent\nrelations with a learnable module after upsampling such as non-local blocks.\nSince upsampling is essential, learning affinity in upsampling can avoid\nadditional propagation layers, offering the potential for building compact\nmodels. By looking at existing upsampling operators from a unified mathematical\nperspective, we generalize them into a second-order form and introduce\nAffinity-Aware Upsampling (A2U) where upsampling kernels are generated using a\nlight-weight lowrank bilinear model and are conditioned on second-order\nfeatures. Our upsampling operator can also be extended to downsampling. We\ndiscuss alternative implementations of A2U and verify their effectiveness on\ntwo detail-sensitive tasks: image reconstruction on a toy dataset; and a\nlargescale image matting task where affinity-based ideas constitute mainstream\nmatting approaches. In particular, results on the Composition-1k matting\ndataset show that A2U achieves a 14% relative improvement in the SAD metric\nagainst a strong baseline with negligible increase of parameters (<0.5%).\nCompared with the state-of-the-art matting network, we achieve 8% higher\nperformance with only 40% model complexity.\n", "versions": [{"version": "v1", "created": "Sun, 29 Nov 2020 05:09:43 GMT"}], "update_date": "2020-12-01", "authors_parsed": [["Dai", "Yutong", ""], ["Lu", "Hao", ""], ["Shen", "Chunhua", ""]]}, {"id": "2011.14289", "submitter": "Lei Wang", "authors": "Lei Wang, Yuchun Huang, Pengjie Tao, Yaolin Hou, Yuxuan Liu", "title": "Learning geometry-image representation for 3D point cloud generation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We study the problem of generating point clouds of 3D objects. Instead of\ndiscretizing the object into 3D voxels with huge computational cost and\nresolution limitations, we propose a novel geometry image based generator (GIG)\nto convert the 3D point cloud generation problem to a 2D geometry image\ngeneration problem. Since the geometry image is a completely regular 2D array\nthat contains the surface points of the 3D object, it leverages both the\nregularity of the 2D array and the geodesic neighborhood of the 3D surface.\nThus, one significant benefit of our GIG is that it allows us to directly\ngenerate the 3D point clouds using efficient 2D image generation networks.\nExperiments on both rigid and non-rigid 3D object datasets have demonstrated\nthe promising performance of our method to not only create plausible and novel\n3D objects, but also learn a probabilistic latent space that well supports the\nshape editing like interpolation and arithmetic.\n", "versions": [{"version": "v1", "created": "Sun, 29 Nov 2020 05:21:10 GMT"}], "update_date": "2020-12-01", "authors_parsed": [["Wang", "Lei", ""], ["Huang", "Yuchun", ""], ["Tao", "Pengjie", ""], ["Hou", "Yaolin", ""], ["Liu", "Yuxuan", ""]]}, {"id": "2011.14298", "submitter": "Alphin J Thottupattu", "authors": "Alphin J. Thottupattu, Jayanthi Sivaswamy, Venkateswaran P. Krishnan", "title": "A method for large diffeomorphic registration via broken geodesics", "comments": "18 pages and 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Anatomical variabilities seen in longitudinal data or inter-subject data is\nusually described by the underlying deformation, captured by non-rigid\nregistration of these images. Stationary Velocity Field (SVF) based non-rigid\nregistration algorithms are widely used for registration. SVF based methods\nform a metric-free framework which captures a finite dimensional submanifold of\ndeformations embedded in the infinite dimensional smooth manifold of\ndiffeomorphisms. However, these methods cover only a limited degree of\ndeformations. In this paper, we address this limitation and define an\napproximate metric space for the manifold of diffeomorphisms $\\mathcal{G}$. We\npropose a method to break down the large deformation into finite compositions\nof small deformations. This results in a broken geodesic path on $\\mathcal{G}$\nand its length now forms an approximate registration metric. We illustrate the\nmethod using a simple, intensity-based, log-demon implementation. Validation\nresults of the proposed method show that it can capture large and complex\ndeformations while producing qualitatively better results than the\nstate-of-the-art methods. The results also demonstrate that the proposed\nregistration metric is a good indicator of the degree of deformation.\n", "versions": [{"version": "v1", "created": "Sun, 29 Nov 2020 06:14:53 GMT"}, {"version": "v2", "created": "Sun, 3 Jan 2021 05:49:37 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Thottupattu", "Alphin J.", ""], ["Sivaswamy", "Jayanthi", ""], ["Krishnan", "Venkateswaran P.", ""]]}, {"id": "2011.14301", "submitter": "Haotian Xie", "authors": "Haotian Xie, Yong Zhang, Jun Wang, Jingjing Zhang, Yifan Ma, Zhaogang\n  Yang", "title": "Automated Prostate Cancer Diagnosis Based on Gleason Grading Using\n  Convolutional Neural Network", "comments": "This article has been removed by arXiv administrators because the\n  submitter did not have the authority to grant the license applied at the time\n  of submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Gleason grading system using histological images is the most powerful\ndiagnostic and prognostic predictor of prostate cancer. The current standard\ninspection is evaluating Gleason H&E-stained histopathology images by\npathologists. However, it is complicated, time-consuming, and subject to\nobservers. Deep learning (DL) based-methods that automatically learn image\nfeatures and achieve higher generalization ability have attracted significant\nattention. However, challenges remain especially using DL to train the whole\nslide image (WSI), a predominant clinical source in the current diagnostic\nsetting, containing billions of pixels, morphological heterogeneity, and\nartifacts. Hence, we proposed a convolutional neural network (CNN)-based\nautomatic classification method for accurate grading of PCa using whole slide\nhistopathology images. In this paper, a data augmentation method named\nPatch-Based Image Reconstruction (PBIR) was proposed to reduce the high\nresolution and increase the diversity of WSIs. In addition, a distribution\ncorrection (DC) module was developed to enhance the adaption of pretrained\nmodel to the target dataset by adjusting the data distribution. Besides, a\nQuadratic Weighted Mean Square Error (QWMSE) function was presented to reduce\nthe misdiagnosis caused by equal Euclidean distances. Our experiments indicated\nthe combination of PBIR, DC, and QWMSE function was necessary for achieving\nsuperior expert-level performance, leading to the best results (0.8885\nquadratic-weighted kappa coefficient).\n", "versions": [{"version": "v1", "created": "Sun, 29 Nov 2020 06:42:08 GMT"}], "update_date": "2020-12-10", "authors_parsed": [["Xie", "Haotian", ""], ["Zhang", "Yong", ""], ["Wang", "Jun", ""], ["Zhang", "Jingjing", ""], ["Ma", "Yifan", ""], ["Yang", "Zhaogang", ""]]}, {"id": "2011.14302", "submitter": "Li Rui", "authors": "Rui Li, Shunyi Zheng, Chenxi Duan, Jianlin Su, and Ce Zhang", "title": "Multi-stage Attention ResU-Net for Semantic Segmentation of\n  Fine-Resolution Remote Sensing Images", "comments": "arXiv admin note: substantial text overlap with arXiv:2007.14902,\n  arXiv:2009.02130", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The attention mechanism can refine the extracted feature maps and boost the\nclassification performance of the deep network, which has become an essential\ntechnique in computer vision and natural language processing. However, the\nmemory and computational costs of the dot-product attention mechanism increase\nquadratically with the spatio-temporal size of the input. Such growth hinders\nthe usage of attention mechanisms considerably in application scenarios with\nlarge-scale inputs. In this Letter, we propose a Linear Attention Mechanism\n(LAM) to address this issue, which is approximately equivalent to dot-product\nattention with computational efficiency. Such a design makes the incorporation\nbetween attention mechanisms and deep networks much more flexible and\nversatile. Based on the proposed LAM, we re-factor the skip connections in the\nraw U-Net and design a Multi-stage Attention ResU-Net (MAResU-Net) for semantic\nsegmentation from fine-resolution remote sensing images. Experiments conducted\non the Vaihingen dataset demonstrated the effectiveness and efficiency of our\nMAResU-Net. Open-source code is available at\nhttps://github.com/lironui/Multistage-Attention-ResU-Net.\n", "versions": [{"version": "v1", "created": "Sun, 29 Nov 2020 07:24:21 GMT"}, {"version": "v2", "created": "Tue, 1 Dec 2020 06:25:01 GMT"}], "update_date": "2020-12-02", "authors_parsed": [["Li", "Rui", ""], ["Zheng", "Shunyi", ""], ["Duan", "Chenxi", ""], ["Su", "Jianlin", ""], ["Zhang", "Ce", ""]]}, {"id": "2011.14306", "submitter": "Terumasa Tokunaga Dr", "authors": "Ryoya Katafuchi, Terumasa Tokunaga", "title": "Image-based Plant Disease Diagnosis with Unsupervised Anomaly Detection\n  Based on Reconstructability of Colors", "comments": "14 pages, 7 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes an unsupervised anomaly detection technique for\nimage-based plant disease diagnosis. The construction of large and publicly\navailable datasets containing labeled images of healthy and diseased crop\nplants led to growing interest in computer vision techniques for automatic\nplant disease diagnosis. Although supervised image classifiers based on deep\nlearning can be a powerful tool for plant disease diagnosis, they require a\nhuge amount of labeled data. The data mining technique of anomaly detection\nincludes unsupervised approaches that do not require rare samples for training\nclassifiers. We propose an unsupervised anomaly detection technique for\nimage-based plant disease diagnosis that is based on the reconstructability of\ncolors; a deep encoder-decoder network trained to reconstruct the colors of\n\\textit{healthy} plant images should fail to reconstruct colors of symptomatic\nregions. Our proposed method includes a new image-based framework for plant\ndisease detection that utilizes a conditional adversarial network called\npix2pix and a new anomaly score based on CIEDE2000 color difference.\nExperiments with PlantVillage dataset demonstrated the superiority of our\nproposed method compared to an existing anomaly detector at identifying\ndiseased crop images in terms of accuracy, interpretability and computational\nefficiency.\n", "versions": [{"version": "v1", "created": "Sun, 29 Nov 2020 07:44:05 GMT"}, {"version": "v2", "created": "Mon, 21 Dec 2020 13:47:44 GMT"}, {"version": "v3", "created": "Tue, 22 Dec 2020 17:27:41 GMT"}, {"version": "v4", "created": "Mon, 22 Feb 2021 16:44:55 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["Katafuchi", "Ryoya", ""], ["Tokunaga", "Terumasa", ""]]}, {"id": "2011.14311", "submitter": "Xiaoxu Li", "authors": "Xiaoxu Li, Jijie Wu, Zhuo Sun, Zhanyu Ma, Jie Cao, Jing-Hao Xue", "title": "BSNet: Bi-Similarity Network for Few-shot Fine-grained Image\n  Classification", "comments": "IEEE TIP 2020", "journal-ref": null, "doi": "10.1109/TIP.2020.3043128", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Few-shot learning for fine-grained image classification has gained recent\nattention in computer vision. Among the approaches for few-shot learning, due\nto the simplicity and effectiveness, metric-based methods are favorably\nstate-of-the-art on many tasks. Most of the metric-based methods assume a\nsingle similarity measure and thus obtain a single feature space. However, if\nsamples can simultaneously be well classified via two distinct similarity\nmeasures, the samples within a class can distribute more compactly in a smaller\nfeature space, producing more discriminative feature maps. Motivated by this,\nwe propose a so-called \\textit{Bi-Similarity Network} (\\textit{BSNet}) that\nconsists of a single embedding module and a bi-similarity module of two\nsimilarity measures. After the support images and the query images pass through\nthe convolution-based embedding module, the bi-similarity module learns feature\nmaps according to two similarity measures of diverse characteristics. In this\nway, the model is enabled to learn more discriminative and less\nsimilarity-biased features from few shots of fine-grained images, such that the\nmodel generalization ability can be significantly improved. Through extensive\nexperiments by slightly modifying established metric/similarity based networks,\nwe show that the proposed approach produces a substantial improvement on\nseveral fine-grained image benchmark datasets. Codes are available at:\nhttps://github.com/spraise/BSNet\n", "versions": [{"version": "v1", "created": "Sun, 29 Nov 2020 08:38:17 GMT"}], "update_date": "2021-02-03", "authors_parsed": [["Li", "Xiaoxu", ""], ["Wu", "Jijie", ""], ["Sun", "Zhuo", ""], ["Ma", "Zhanyu", ""], ["Cao", "Jie", ""], ["Xue", "Jing-Hao", ""]]}, {"id": "2011.14326", "submitter": "Roland Molontay", "authors": "Kate Barnes, Tiernon Riesenmy, Minh Duc Trinh, Eli Lleshi, N\\'ora\n  Balogh, Roland Molontay", "title": "Dank or Not? -- Analyzing and Predicting the Popularity of Memes on\n  Reddit", "comments": "23 pages, 12 figures", "journal-ref": null, "doi": "10.1007/s41109-021-00358-7", "report-no": null, "categories": "cs.SI cs.CL cs.CV cs.CY physics.soc-ph", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Internet memes have become an increasingly pervasive form of contemporary\nsocial communication that attracted a lot of research interest recently. In\nthis paper, we analyze the data of 129,326 memes collected from Reddit in the\nmiddle of March, 2020, when the most serious coronavirus restrictions were\nbeing introduced around the world. This article not only provides a looking\nglass into the thoughts of Internet users during the COVID-19 pandemic but we\nalso perform a content-based predictive analysis of what makes a meme go viral.\nUsing machine learning methods, we also study what incremental predictive power\nimage related attributes have over textual attributes on meme popularity. We\nfind that the success of a meme can be predicted based on its content alone\nmoderately well, our best performing machine learning model predicts viral\nmemes with AUC=0.68. We also find that both image related and textual\nattributes have significant incremental predictive power over each other.\n", "versions": [{"version": "v1", "created": "Sun, 29 Nov 2020 09:57:17 GMT"}, {"version": "v2", "created": "Fri, 22 Jan 2021 08:31:42 GMT"}], "update_date": "2021-03-11", "authors_parsed": [["Barnes", "Kate", ""], ["Riesenmy", "Tiernon", ""], ["Trinh", "Minh Duc", ""], ["Lleshi", "Eli", ""], ["Balogh", "N\u00f3ra", ""], ["Molontay", "Roland", ""]]}, {"id": "2011.14329", "submitter": "Ruskin Raj Manku", "authors": "Ruskin Raj Manku and Ayush Sharma and Anand Panchbhai", "title": "Malaria Detection and Classificaiton", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Malaria is a disease of global concern according to the World Health\nOrganization. Billions of people in the world are at risk of Malaria today.\nMicroscopy is considered the gold standard for Malaria diagnosis. Microscopic\nassessment of blood samples requires the need of trained professionals who at\ntimes are not available in rural areas where Malaria is a problem. Full\nautomation of Malaria diagnosis is a challenging task. In this work, we put\nforward a framework for diagnosis of malaria. We adopt a two layer approach,\nwhere we detect infected cells using a Faster-RCNN in the first layer, crop\nthem out, and feed the cropped cells to a seperate neural network for\nclassification. The proposed methodology was tested on an openly available\ndataset, this will serve as a baseline for the future methods as currently\nthere is no common dataset on which results are reported for Malaria Diagnosis.\n", "versions": [{"version": "v1", "created": "Sun, 29 Nov 2020 10:04:01 GMT"}], "update_date": "2020-12-01", "authors_parsed": [["Manku", "Ruskin Raj", ""], ["Sharma", "Ayush", ""], ["Panchbhai", "Anand", ""]]}, {"id": "2011.14334", "submitter": "Peng Zhang", "authors": "Peng Zhang, Jiaming Xu, Jing shi, Yunzhe Hao, Bo Xu", "title": "Audio-visual Speech Separation with Adversarially Disentangled Visual\n  Representation", "comments": "5 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.CV eess.AS", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Speech separation aims to separate individual voice from an audio mixture of\nmultiple simultaneous talkers. Although audio-only approaches achieve\nsatisfactory performance, they build on a strategy to handle the predefined\nconditions, limiting their application in the complex auditory scene. Towards\nthe cocktail party problem, we propose a novel audio-visual speech separation\nmodel. In our model, we use the face detector to detect the number of speakers\nin the scene and use visual information to avoid the permutation problem. To\nimprove our model's generalization ability to unknown speakers, we extract\nspeech-related visual features from visual inputs explicitly by the\nadversarially disentangled method, and use this feature to assist speech\nseparation. Besides, the time-domain approach is adopted, which could avoid the\nphase reconstruction problem existing in the time-frequency domain models. To\ncompare our model's performance with other models, we create two benchmark\ndatasets of 2-speaker mixture from GRID and TCDTIMIT audio-visual datasets.\nThrough a series of experiments, our proposed model is shown to outperform the\nstate-of-the-art audio-only model and three audio-visual models.\n", "versions": [{"version": "v1", "created": "Sun, 29 Nov 2020 10:48:42 GMT"}], "update_date": "2020-12-01", "authors_parsed": [["Zhang", "Peng", ""], ["Xu", "Jiaming", ""], ["shi", "Jing", ""], ["Hao", "Yunzhe", ""], ["Xu", "Bo", ""]]}, {"id": "2011.14340", "submitter": "Dawid Rymarczyk", "authors": "Dawid Rymarczyk, {\\L}ukasz Struski, Jacek Tabor, Bartosz Zieli\\'nski", "title": "ProtoPShare: Prototype Sharing for Interpretable Image Classification\n  and Similarity Discovery", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we introduce ProtoPShare, a self-explained method that\nincorporates the paradigm of prototypical parts to explain its predictions. The\nmain novelty of the ProtoPShare is its ability to efficiently share\nprototypical parts between the classes thanks to our data-dependent\nmerge-pruning. Moreover, the prototypes are more consistent and the model is\nmore robust to image perturbations than the state of the art method ProtoPNet.\nWe verify our findings on two datasets, the CUB-200-2011 and the Stanford Cars.\n", "versions": [{"version": "v1", "created": "Sun, 29 Nov 2020 11:23:05 GMT"}], "update_date": "2020-12-01", "authors_parsed": [["Rymarczyk", "Dawid", ""], ["Struski", "\u0141ukasz", ""], ["Tabor", "Jacek", ""], ["Zieli\u0144ski", "Bartosz", ""]]}, {"id": "2011.14347", "submitter": "Mahmut Yurt", "authors": "Mahmut Yurt, Salman Ul Hassan Dar, Muzaffer \\\"Ozbey, Berk T{\\i}naz,\n  Kader Karl{\\i} O\\u{g}uz, Tolga \\c{C}ukur", "title": "Semi-Supervised Learning of Mutually Accelerated MRI Synthesis without\n  Fully-Sampled Ground Truths", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning-based synthetic multi-contrast MRI commonly involves deep models\ntrained using high-quality images of source and target contrasts, regardless of\nwhether source and target domain samples are paired or unpaired. This results\nin undesirable reliance on fully-sampled acquisitions of all MRI contrasts,\nwhich might prove impractical due to limitations on scan costs and time. Here,\nwe propose a novel semi-supervised deep generative model that instead learns to\nrecover high-quality target images directly from accelerated acquisitions of\nsource and target contrasts. To achieve this, the proposed model introduces\nnovel multi-coil tensor losses in image, k-space and adversarial domains. These\nselective losses are based only on acquired k-space samples, and randomized\nsampling masks are used across subjects to capture relationships among acquired\nand non-acquired k-space regions. Comprehensive experiments on multi-contrast\nneuroimaging datasets demonstrate that our semi-supervised approach yields\nequivalent performance to gold-standard fully-supervised models, while\noutperforming a cascaded approach that learns to synthesize based on\nreconstructions of undersampled data. Therefore, the proposed approach holds\ngreat promise to improve the feasibility and utility of accelerated MRI\nacquisitions mutually undersampled across both contrast sets and k-space.\n", "versions": [{"version": "v1", "created": "Sun, 29 Nov 2020 11:56:37 GMT"}, {"version": "v2", "created": "Wed, 12 May 2021 10:51:11 GMT"}], "update_date": "2021-05-13", "authors_parsed": [["Yurt", "Mahmut", ""], ["Dar", "Salman Ul Hassan", ""], ["\u00d6zbey", "Muzaffer", ""], ["T\u0131naz", "Berk", ""], ["O\u011fuz", "Kader Karl\u0131", ""], ["\u00c7ukur", "Tolga", ""]]}, {"id": "2011.14356", "submitter": "Pengtao Xu", "authors": "Pengtao Xu, Jian Cao, Fanhua Shang, Wenyu Sun, Pu Li", "title": "Layer Pruning via Fusible Residual Convolutional Block for Deep Neural\n  Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In order to deploy deep convolutional neural networks (CNNs) on\nresource-limited devices, many model pruning methods for filters and weights\nhave been developed, while only a few to layer pruning. However, compared with\nfilter pruning and weight pruning, the compact model obtained by layer pruning\nhas less inference time and run-time memory usage when the same FLOPs and\nnumber of parameters are pruned because of less data moving in memory. In this\npaper, we propose a simple layer pruning method using fusible residual\nconvolutional block (ResConv), which is implemented by inserting shortcut\nconnection with a trainable information control parameter into a single\nconvolutional layer. Using ResConv structures in training can improve network\naccuracy and train deep plain networks, and adds no additional computation\nduring inference process because ResConv is fused to be an ordinary\nconvolutional layer after training. For layer pruning, we convert convolutional\nlayers of network into ResConv with a layer scaling factor. In the training\nprocess, the L1 regularization is adopted to make the scaling factors sparse,\nso that unimportant layers are automatically identified and then removed,\nresulting in a model of layer reduction. Our pruning method achieves excellent\nperformance of compression and acceleration over the state-of-the-arts on\ndifferent datasets, and needs no retraining in the case of low pruning rate.\nFor example, with ResNet-110, we achieve a 65.5%-FLOPs reduction by removing\n55.5% of the parameters, with only a small loss of 0.13% in top-1 accuracy on\nCIFAR-10.\n", "versions": [{"version": "v1", "created": "Sun, 29 Nov 2020 12:51:16 GMT"}], "update_date": "2020-12-01", "authors_parsed": [["Xu", "Pengtao", ""], ["Cao", "Jian", ""], ["Shang", "Fanhua", ""], ["Sun", "Wenyu", ""], ["Li", "Pu", ""]]}, {"id": "2011.14358", "submitter": "Saqib Ali Khan", "authors": "Saqib Ali Khan, Yilei Shi, Muhammad Shahzad, Xiao Xiang Zhu", "title": "Exploring Deep 3D Spatial Encodings for Large-Scale 3D Scene\n  Understanding", "comments": "Submitted to IEEE Geoscience and Remote Sensing Letters (GRSL)\n  Journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semantic segmentation of raw 3D point clouds is an essential component in 3D\nscene analysis, but it poses several challenges, primarily due to the\nnon-Euclidean nature of 3D point clouds. Although, several deep learning based\napproaches have been proposed to address this task, but almost all of them\nemphasized on using the latent (global) feature representations from\ntraditional convolutional neural networks (CNN), resulting in severe loss of\nspatial information, thus failing to model the geometry of the underlying 3D\nobjects, that plays an important role in remote sensing 3D scenes. In this\nletter, we have proposed an alternative approach to overcome the limitations of\nCNN based approaches by encoding the spatial features of raw 3D point clouds\ninto undirected symmetrical graph models. These encodings are then combined\nwith a high-dimensional feature vector extracted from a traditional CNN into a\nlocalized graph convolution operator that outputs the required 3D segmentation\nmap. We have performed experiments on two standard benchmark datasets\n(including an outdoor aerial remote sensing dataset and an indoor synthetic\ndataset). The proposed method achieves on par state-of-the-art accuracy with\nimproved training time and model stability thus indicating strong potential for\nfurther research towards a generalized state-of-the-art method for 3D scene\nunderstanding.\n", "versions": [{"version": "v1", "created": "Sun, 29 Nov 2020 12:56:19 GMT"}], "update_date": "2020-12-01", "authors_parsed": [["Khan", "Saqib Ali", ""], ["Shi", "Yilei", ""], ["Shahzad", "Muhammad", ""], ["Zhu", "Xiao Xiang", ""]]}, {"id": "2011.14370", "submitter": "Sidhartha Narayan S", "authors": "Sarah, S.Sidhartha Narayan, Irfaan Arif, Hrithwik Shalu, Juned\n  Kadiwala", "title": "A smartphone based multi input workflow for non-invasive estimation of\n  haemoglobin levels using machine learning techniques", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.HC eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We suggest a low cost, non invasive healthcare system that measures\nhaemoglobin levels in patients and can be used as a preliminary diagnostic test\nfor anaemia. A combination of image processing, machine learning and deep\nlearning techniques are employed to develop predictive models to measure\nhaemoglobin levels. This is achieved through the color analysis of the\nfingernail beds, palpebral conjunctiva and tongue of the patients. This\npredictive model is then encapsulated in a healthcare application. This\napplication expedites data collection and facilitates active learning of the\nmodel. It also incorporates personalized calibration of the model for each\npatient, assisting in the continual monitoring of the haemoglobin levels of the\npatient. Upon validating this framework using data, it can serve as a highly\naccurate preliminary diagnostic test for anaemia.\n", "versions": [{"version": "v1", "created": "Sun, 29 Nov 2020 13:57:09 GMT"}], "update_date": "2020-12-01", "authors_parsed": [["Sarah", "", ""], ["Narayan", "S. Sidhartha", ""], ["Arif", "Irfaan", ""], ["Shalu", "Hrithwik", ""], ["Kadiwala", "Juned", ""]]}, {"id": "2011.14372", "submitter": "Alessa Hering", "authors": "Alessa Hering, Stephanie H\\\"ager, Jan Moltz, Nikolas Lessmann, Stefan\n  Heldmann and Bram van Ginneken", "title": "CNN-based Lung CT Registration with Multiple Anatomical Constraints", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Deep-learning-based registration methods emerged as a fast alternative to\nconventional registration methods. However, these methods often still cannot\nachieve the same performance as conventional registration methods because they\nare either limited to small deformation or they fail to handle a superposition\nof large and small deformations without producing implausible deformation\nfields with foldings inside.\n  In this paper, we identify important strategies of conventional registration\nmethods for lung registration and successfully developed the deep-learning\ncounterpart. We employ a Gaussian-pyramid-based multilevel framework that can\nsolve the image registration optimization in a coarse-to-fine fashion.\nFurthermore, we prevent foldings of the deformation field and restrict the\ndeterminant of the Jacobian to physiologically meaningful values by combining a\nvolume change penalty with a curvature regularizer in the loss function.\nKeypoint correspondences are integrated to focus on the alignment of smaller\nstructures.\n  We perform an extensive evaluation to assess the accuracy, the robustness,\nthe plausibility of the estimated deformation fields, and the transferability\nof our registration approach. We show that it achieves state-of-the-art results\non the COPDGene dataset compared to conventional registration method with much\nshorter execution time. In our experiments on the DIRLab exhale to inhale lung\nregistration, we demonstrate substantial improvements (TRE below $1.2$ mm) over\nother deep learning methods. Our algorithm is publicly available at\nhttps://grand-challenge.org/algorithms/deep-learning-based-ct-lung-registration/.\n", "versions": [{"version": "v1", "created": "Sun, 29 Nov 2020 14:09:31 GMT"}, {"version": "v2", "created": "Mon, 14 Jun 2021 10:43:23 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Hering", "Alessa", ""], ["H\u00e4ger", "Stephanie", ""], ["Moltz", "Jan", ""], ["Lessmann", "Nikolas", ""], ["Heldmann", "Stefan", ""], ["van Ginneken", "Bram", ""]]}, {"id": "2011.14380", "submitter": "Shreya Roy", "authors": "Shreya Roy, Anirban Chakraborty (Indian Institute of Science,\n  Bangalore)", "title": "Single Image Super-resolution with a Switch Guided Hybrid Network for\n  Satellite Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The major drawbacks with Satellite Images are low resolution, Low resolution\nmakes it difficult to identify the objects present in Satellite images. We have\nexperimented with several deep models available for Single Image\nSuperresolution on the SpaceNet dataset and have evaluated the performance of\neach of them on the satellite image data. We will dive into the recent\nevolution of the deep models in the context of SISR over the past few years and\nwill present a comparative study between these models. The entire Satellite\nimage of an area is divided into equal-sized patches. Each patch will be used\nindependently for training. These patches will differ in nature. Say, for\nexample, the patches over urban areas have non-homogeneous backgrounds because\nof different types of objects like vehicles, buildings, roads, etc. On the\nother hand, patches over jungles will be more homogeneous in nature. Hence,\ndifferent deep models will fit on different kinds of patches. In this study, we\nwill try to explore this further with the help of a Switching Convolution\nNetwork. The idea is to train a switch classifier that will automatically\nclassify a patch into one category of models best suited for it.\n", "versions": [{"version": "v1", "created": "Sun, 29 Nov 2020 14:47:23 GMT"}], "update_date": "2020-12-01", "authors_parsed": [["Roy", "Shreya", "", "Indian Institute of Science,\n  Bangalore"], ["Chakraborty", "Anirban", "", "Indian Institute of Science,\n  Bangalore"]]}, {"id": "2011.14387", "submitter": "Marija Vella", "authors": "Marija Vella, Jo\\~ao F. C. Mota", "title": "Overcoming Measurement Inconsistency in Deep Learning for Linear Inverse\n  Problems: Applications in Medical Imaging", "comments": "Accepted for publication at ICASSP 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV eess.SP math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The remarkable performance of deep neural networks (DNNs) currently makes\nthem the method of choice for solving linear inverse problems. They have been\napplied to super-resolve and restore images, as well as to reconstruct MR and\nCT images. In these applications, DNNs invert a forward operator by finding,\nvia training data, a map between the measurements and the input images. It is\nthen expected that the map is still valid for the test data. This framework,\nhowever, introduces measurement inconsistency during testing. We show that such\ninconsistency, which can be critical in domains like medical imaging or\ndefense, is intimately related to the generalization error. We then propose a\nframework that post-processes the output of DNNs with an optimization algorithm\nthat enforces measurement consistency. Experiments on MR images show that\nenforcing measurement consistency via our method can lead to large gains in\nreconstruction performance.\n", "versions": [{"version": "v1", "created": "Sun, 29 Nov 2020 15:19:41 GMT"}, {"version": "v2", "created": "Mon, 31 May 2021 10:28:45 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Vella", "Marija", ""], ["Mota", "Jo\u00e3o F. C.", ""]]}, {"id": "2011.14389", "submitter": "Robert Weston Mr", "authors": "Rob Weston, Oiwi Parker Jones and Ingmar Posner", "title": "There and Back Again: Learning to Simulate Radar Data for Real-World\n  Applications", "comments": "6 pages + 2 references", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV cs.LG eess.SP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Simulating realistic radar data has the potential to significantly accelerate\nthe development of data-driven approaches to radar processing. However, it is\nfraught with difficulty due to the notoriously complex image formation process.\nHere we propose to learn a radar sensor model capable of synthesising faithful\nradar observations based on simulated elevation maps. In particular, we adopt\nan adversarial approach to learning a forward sensor model from unaligned radar\nexamples. In addition, modelling the backward model encourages the output to\nremain aligned to the world state through a cyclical consistency criterion. The\nbackward model is further constrained to predict elevation maps from real radar\ndata that are grounded by partial measurements obtained from corresponding\nlidar scans. Both models are trained in a joint optimisation. We demonstrate\nthe efficacy of our approach by evaluating a down-stream segmentation model\ntrained purely on simulated data in a real-world deployment. This achieves\nperformance within four percentage points of the same model trained entirely on\nreal data.\n", "versions": [{"version": "v1", "created": "Sun, 29 Nov 2020 15:49:23 GMT"}], "update_date": "2020-12-01", "authors_parsed": [["Weston", "Rob", ""], ["Jones", "Oiwi Parker", ""], ["Posner", "Ingmar", ""]]}, {"id": "2011.14398", "submitter": "Phong Nguyen-Ha", "authors": "Phong Nguyen, Animesh Karnewar, Lam Huynh, Esa Rahtu, Jiri Matas,\n  Janne Heikkila", "title": "RGBD-Net: Predicting color and depth images for novel views synthesis", "comments": "19 pages, 15 figures. Code will be available at:\n  https://github.com/phongnhhn92/RGBDNet", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new cascaded architecture for novel view synthesis, called\nRGBD-Net, which consists of two core components: a hierarchical depth\nregression network and a depth-aware generator network. The former one predicts\ndepth maps of the target views by using adaptive depth scaling, while the\nlatter one leverages the predicted depths and renders spatially and temporally\nconsistent target images. In the experimental evaluation on standard datasets,\nRGBD-Net not only outperforms the state-of-the-art by a clear margin, but it\nalso generalizes well to new scenes without per-scene optimization. Moreover,\nwe show that RGBD-Net can be optionally trained without depth supervision while\nstill retaining high-quality rendering. Thanks to the depth regression network,\nRGBD-Net can be also used for creating dense 3D point clouds that are more\naccurate than those produced by some state-of-the-art multi-view stereo\nmethods.\n", "versions": [{"version": "v1", "created": "Sun, 29 Nov 2020 16:42:53 GMT"}, {"version": "v2", "created": "Fri, 9 Jul 2021 15:06:24 GMT"}], "update_date": "2021-07-12", "authors_parsed": [["Nguyen", "Phong", ""], ["Karnewar", "Animesh", ""], ["Huynh", "Lam", ""], ["Rahtu", "Esa", ""], ["Matas", "Jiri", ""], ["Heikkila", "Janne", ""]]}, {"id": "2011.14416", "submitter": "Juan Isern MSc", "authors": "Juan Isern, Francisco Barranco, Daniel Deniz, Juho Lesonen, Jari\n  Hannuksela, Richard R. Carrillo", "title": "Reconfigurable Cyber-Physical System for Critical Infrastructure\n  Protection in Smart Cities via Smart Video-Surveillance", "comments": "13 pages, 8 figures and 5 tables", "journal-ref": "Pattern Recognition Letters Volume 140, December 2020, Pages\n  303-309", "doi": "10.1016/j.patrec.2020.11.004", "report-no": null, "categories": "cs.CV cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automated surveillance is essential for the protection of Critical\nInfrastructures (CIs) in future Smart Cities. The dynamic environments and\nbandwidth requirements demand systems that adapt themselves to react when\nevents of interest occur. We present a reconfigurable Cyber Physical System for\nthe protection of CIs using distributed cloud-edge smart video surveillance.\nOur local edge nodes perform people detection via Deep Learning. Processing is\nembedded in high performance SoCs (System-on-Chip) achieving real-time\nperformance ($\\approx$ 100 fps - frames per second) which enables efficiently\nmanaging video streams of more cameras source at lower frame rate. Cloud server\ngathers results from nodes to carry out biometric facial identification,\ntracking, and perimeter monitoring. A Quality and Resource Management module\nmonitors data bandwidth and triggers reconfiguration adapting the transmitted\nvideo resolution. This also enables a flexible use of the network by multiple\ncameras while maintaining the accuracy of biometric identification. A\nreal-world example shows a reduction of $\\approx$ 75\\% bandwidth use with\nrespect to the no-reconfiguration scenario.\n", "versions": [{"version": "v1", "created": "Sun, 29 Nov 2020 18:43:25 GMT"}], "update_date": "2020-12-01", "authors_parsed": [["Isern", "Juan", ""], ["Barranco", "Francisco", ""], ["Deniz", "Daniel", ""], ["Lesonen", "Juho", ""], ["Hannuksela", "Jari", ""], ["Carrillo", "Richard R.", ""]]}, {"id": "2011.14417", "submitter": "Abu Md Niamul Taufique", "authors": "Abu Md Niamul Taufique and Andreas Savakis", "title": "LABNet: Local Graph Aggregation Network with Class Balanced Loss for\n  Vehicle Re-Identification", "comments": "12 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Vehicle re-identification is an important computer vision task where the\nobjective is to identify a specific vehicle among a set of vehicles seen at\nvarious viewpoints. Recent methods based on deep learning utilize a global\naverage pooling layer after the backbone feature extractor, however, this\nignores any spatial reasoning on the feature map. In this paper, we propose\nlocal graph aggregation on the backbone feature map, to learn associations of\nlocal information and hence improve feature learning as well as reduce the\neffects of partial occlusion and background clutter. Our local graph\naggregation network considers spatial regions of the feature map as nodes and\nbuilds a local neighborhood graph that performs local feature aggregation\nbefore the global average pooling layer. We further utilize a batch\nnormalization layer to improve the system effectiveness. Additionally, we\nintroduce a class balanced loss to compensate for the imbalance in the sample\ndistributions found in the most widely used vehicle re-identification datasets.\nFinally, we evaluate our method in three popular benchmarks and show that our\napproach outperforms many state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Sun, 29 Nov 2020 18:43:30 GMT"}, {"version": "v2", "created": "Sun, 31 Jan 2021 04:47:03 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["Taufique", "Abu Md Niamul", ""], ["Savakis", "Andreas", ""]]}, {"id": "2011.14427", "submitter": "George Cazenavette V", "authors": "George Cazenavette, Calvin Murdock, Simon Lucey", "title": "Architectural Adversarial Robustness: The Case for Deep Pursuit", "comments": "11 pages, 14 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite their unmatched performance, deep neural networks remain susceptible\nto targeted attacks by nearly imperceptible levels of adversarial noise. While\nthe underlying cause of this sensitivity is not well understood, theoretical\nanalyses can be simplified by reframing each layer of a feed-forward network as\nan approximate solution to a sparse coding problem. Iterative solutions using\nbasis pursuit are theoretically more stable and have improved adversarial\nrobustness. However, cascading layer-wise pursuit implementations suffer from\nerror accumulation in deeper networks. In contrast, our new method of deep\npursuit approximates the activations of all layers as a single global\noptimization problem, allowing us to consider deeper, real-world architectures\nwith skip connections such as residual networks. Experimentally, our approach\ndemonstrates improved robustness to adversarial noise.\n", "versions": [{"version": "v1", "created": "Sun, 29 Nov 2020 19:39:23 GMT"}], "update_date": "2020-12-01", "authors_parsed": [["Cazenavette", "George", ""], ["Murdock", "Calvin", ""], ["Lucey", "Simon", ""]]}, {"id": "2011.14447", "submitter": "Sagnik Das", "authors": "Sagnik Das, Hassan Ahmed Sial, Ke Ma, Ramon Baldrich, Maria Vanrell,\n  Dimitris Samaras", "title": "Intrinsic Decomposition of Document Images In-the-Wild", "comments": "This a modified version of the BMVC 2020 accepted manuscript", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Automatic document content processing is affected by artifacts caused by the\nshape of the paper, non-uniform and diverse color of lighting conditions.\nFully-supervised methods on real data are impossible due to the large amount of\ndata needed. Hence, the current state of the art deep learning models are\ntrained on fully or partially synthetic images. However, document shadow or\nshading removal results still suffer because: (a) prior methods rely on\nuniformity of local color statistics, which limit their application on\nreal-scenarios with complex document shapes and textures and; (b) synthetic or\nhybrid datasets with non-realistic, simulated lighting conditions are used to\ntrain the models. In this paper we tackle these problems with our two main\ncontributions. First, a physically constrained learning-based method that\ndirectly estimates document reflectance based on intrinsic image formation\nwhich generalizes to challenging illumination conditions. Second, a new dataset\nthat clearly improves previous synthetic ones, by adding a large range of\nrealistic shading and diverse multi-illuminant conditions, uniquely customized\nto deal with documents in-the-wild. The proposed architecture works in a\nself-supervised manner where only the synthetic texture is used as a weak\ntraining signal (obviating the need for very costly ground truth with\ndisentangled versions of shading and reflectance). The proposed approach leads\nto a significant generalization of document reflectance estimation in real\nscenes with challenging illumination. We extensively evaluate on the real\nbenchmark datasets available for intrinsic image decomposition and document\nshadow removal tasks. Our reflectance estimation scheme, when used as a\npre-processing step of an OCR pipeline, shows a 26% improvement of character\nerror rate (CER), thus, proving the practical applicability.\n", "versions": [{"version": "v1", "created": "Sun, 29 Nov 2020 21:39:58 GMT"}], "update_date": "2020-12-01", "authors_parsed": [["Das", "Sagnik", ""], ["Sial", "Hassan Ahmed", ""], ["Ma", "Ke", ""], ["Baldrich", "Ramon", ""], ["Vanrell", "Maria", ""], ["Samaras", "Dimitris", ""]]}, {"id": "2011.14448", "submitter": "Mohamed Sayed", "authors": "Mohamed Sayed, Gabriel Brostow", "title": "Improved Handling of Motion Blur in Online Object Detection", "comments": "Mirroring accepted CVPR paper. Added results for other real-world\n  blur datasets. Main paper: 8 pages + 3 references. Supplemental: 9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We wish to detect specific categories of objects, for online vision systems\nthat will run in the real world. Object detection is already very challenging.\nIt is even harder when the images are blurred, from the camera being in a car\nor a hand-held phone. Most existing efforts either focused on sharp images,\nwith easy to label ground truth, or they have treated motion blur as one of\nmany generic corruptions.\n  Instead, we focus especially on the details of egomotion induced blur. We\nexplore five classes of remedies, where each targets different potential causes\nfor the performance gap between sharp and blurred images. For example, first\ndeblurring an image changes its human interpretability, but at present, only\npartly improves object detection. The other four classes of remedies address\nmulti-scale texture, out-of-distribution testing, label generation, and\nconditioning by blur-type. Surprisingly, we discover that custom label\ngeneration aimed at resolving spatial ambiguity, ahead of all others, markedly\nimproves object detection. Also, in contrast to findings from classification,\nwe see a noteworthy boost by conditioning our model on bespoke categories of\nmotion blur.\n  We validate and cross-breed the different remedies experimentally on blurred\nCOCO images and real-world blur datasets, producing an easy and practical\nfavorite model with superior detection rates.\n", "versions": [{"version": "v1", "created": "Sun, 29 Nov 2020 21:58:26 GMT"}, {"version": "v2", "created": "Tue, 30 Mar 2021 14:34:38 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Sayed", "Mohamed", ""], ["Brostow", "Gabriel", ""]]}, {"id": "2011.14462", "submitter": "Rui Fan", "authors": "Ellen Yi-Ge, Rui Fan, Zechun Liu, Zhiqiang Shen", "title": "Conditional Link Prediction of Category-Implicit Keypoint Detection", "comments": "WACV 2021 paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Keypoints of objects reflect their concise abstractions, while the\ncorresponding connection links (CL) build the skeleton by detecting the\nintrinsic relations between keypoints. Existing approaches are typically\ncomputationally-intensive, inapplicable for instances belonging to multiple\nclasses, and/or infeasible to simultaneously encode connection information. To\naddress the aforementioned issues, we propose an end-to-end category-implicit\nKeypoint and Link Prediction Network (KLPNet), which is the first approach for\nsimultaneous semantic keypoint detection (for multi-class instances) and CL\nrejuvenation. In our KLPNet, a novel Conditional Link Prediction Graph is\nproposed for link prediction among keypoints that are contingent on a\npredefined category. Furthermore, a Cross-stage Keypoint Localization Module\n(CKLM) is introduced to explore feature aggregation for coarse-to-fine keypoint\nlocalization. Comprehensive experiments conducted on three publicly available\nbenchmarks demonstrate that our KLPNet consistently outperforms all other\nstate-of-the-art approaches. Furthermore, the experimental results of CL\nprediction also show the effectiveness of our KLPNet with respect to occlusion\nproblems.\n", "versions": [{"version": "v1", "created": "Sun, 29 Nov 2020 23:00:37 GMT"}], "update_date": "2020-12-01", "authors_parsed": [["Yi-Ge", "Ellen", ""], ["Fan", "Rui", ""], ["Liu", "Zechun", ""], ["Shen", "Zhiqiang", ""]]}, {"id": "2011.14477", "submitter": "Hubert Lin", "authors": "Hubert Lin, Mitchell van Zuijlen, Sylvia C. Pont, Maarten W.A.\n  Wijntjes, Kavita Bala", "title": "What Can Style Transfer and Paintings Do For Model Robustness?", "comments": "CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A common strategy for improving model robustness is through data\naugmentations. Data augmentations encourage models to learn desired\ninvariances, such as invariance to horizontal flipping or small changes in\ncolor. Recent work has shown that arbitrary style transfer can be used as a\nform of data augmentation to encourage invariance to textures by creating\npainting-like images from photographs. However, a stylized photograph is not\nquite the same as an artist-created painting. Artists depict perceptually\nmeaningful cues in paintings so that humans can recognize salient components in\nscenes, an emphasis which is not enforced in style transfer. Therefore, we\nstudy how style transfer and paintings differ in their impact on model\nrobustness. First, we investigate the role of paintings as style images for\nstylization-based data augmentation. We find that style transfer functions well\neven without paintings as style images. Second, we show that learning from\npaintings as a form of perceptual data augmentation can improve model\nrobustness. Finally, we investigate the invariances learned from stylization\nand from paintings, and show that models learn different invariances from these\ndiffering forms of data. Our results provide insights into how stylization\nimproves model robustness, and provide evidence that artist-created paintings\ncan be a valuable source of data for model robustness.\n", "versions": [{"version": "v1", "created": "Mon, 30 Nov 2020 00:25:04 GMT"}, {"version": "v2", "created": "Thu, 27 May 2021 11:31:36 GMT"}], "update_date": "2021-05-28", "authors_parsed": [["Lin", "Hubert", ""], ["van Zuijlen", "Mitchell", ""], ["Pont", "Sylvia C.", ""], ["Wijntjes", "Maarten W. A.", ""], ["Bala", "Kavita", ""]]}, {"id": "2011.14478", "submitter": "Yixiong Zou", "authors": "Yixiong Zou, Shanghang Zhang, Guangyao Chen, Yonghong Tian, Kurt\n  Keutzer, Jos\\'e M. F. Moura", "title": "Annotation-Efficient Untrimmed Video Action Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning has achieved great success in recognizing video actions, but\nthe collection and annotation of training data are still quite laborious, which\nmainly lies in two aspects: (1) the amount of required annotated data is large;\n(2) temporally annotating the location of each action is time-consuming. Works\nsuch as few-shot learning or untrimmed video recognition have been proposed to\nhandle either one aspect or the other. However, very few existing works can\nhandle both issues simultaneously. In this paper, we target a new problem,\nAnnotation-Efficient Video Recognition, to reduce the requirement of\nannotations for both large amount of samples and the action location. Such\nproblem is challenging due to two aspects: (1) the untrimmed videos only have\nweak supervision; (2) video segments not relevant to current actions of\ninterests (background, BG) could contain actions of interests (foreground, FG)\nin novel classes, which is a widely existing phenomenon but has rarely been\nstudied in few-shot untrimmed video recognition. To achieve this goal, by\nanalyzing the property of BG, we categorize BG into informative BG (IBG) and\nnon-informative BG (NBG), and we propose (1) an open-set detection based method\nto find the NBG and FG, (2) a contrastive learning method to learn IBG and\ndistinguish NBG in a self-supervised way, and (3) a self-weighting mechanism\nfor the better distinguishing of IBG and FG. Extensive experiments on\nActivityNet v1.2 and ActivityNet v1.3 verify the rationale and effectiveness of\nthe proposed methods.\n", "versions": [{"version": "v1", "created": "Mon, 30 Nov 2020 00:26:58 GMT"}, {"version": "v2", "created": "Tue, 20 Apr 2021 08:51:46 GMT"}], "update_date": "2021-04-21", "authors_parsed": [["Zou", "Yixiong", ""], ["Zhang", "Shanghang", ""], ["Chen", "Guangyao", ""], ["Tian", "Yonghong", ""], ["Keutzer", "Kurt", ""], ["Moura", "Jos\u00e9 M. F.", ""]]}, {"id": "2011.14479", "submitter": "Huaxiong Li", "authors": "Haoxing Chen and Huaxiong Li and Yaohui Li and Chunlin Chen", "title": "Multi-scale Adaptive Task Attention Network for Few-Shot Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The goal of few-shot learning is to classify unseen categories with few\nlabeled samples. Recently, the low-level information metric-learning based\nmethods have achieved satisfying performance, since local representations (LRs)\nare more consistent between seen and unseen classes. However, most of these\nmethods deal with each category in the support set independently, which is not\nsufficient to measure the relation between features, especially in a certain\ntask. Moreover, the low-level information-based metric learning method suffers\nwhen dominant objects of different scales exist in a complex background. To\naddress these issues, this paper proposes a novel Multi-scale Adaptive Task\nAttention Network (MATANet) for few-shot learning. Specifically, we first use a\nmulti-scale feature generator to generate multiple features at different\nscales. Then, an adaptive task attention module is proposed to select the most\nimportant LRs among the entire task. Afterwards, a similarity-to-class module\nand a fusion layer are utilized to calculate a joint multi-scale similarity\nbetween the query image and the support set. Extensive experiments on popular\nbenchmarks clearly show the effectiveness of the proposed MATANet compared with\nstate-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Mon, 30 Nov 2020 00:36:01 GMT"}], "update_date": "2020-12-01", "authors_parsed": [["Chen", "Haoxing", ""], ["Li", "Huaxiong", ""], ["Li", "Yaohui", ""], ["Chen", "Chunlin", ""]]}, {"id": "2011.14488", "submitter": "Marc Law", "authors": "Aayush Prakash, Shoubhik Debnath, Jean-Francois Lafleche, Eric\n  Cameracci, Gavriel State, Marc T. Law", "title": "Sim2SG: Sim-to-Real Scene Graph Generation for Transfer Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scene graph (SG) generation has been gaining a lot of traction recently.\nCurrent SG generation techniques, however, rely on the availability of\nexpensive and limited number of labeled datasets. Synthetic data offers a\nviable alternative as labels are essentially free. However, neural network\nmodels trained on synthetic data, do not perform well on real data because of\nthe domain gap. To overcome this challenge, we propose Sim2SG, a scalable\ntechnique for sim-to-real transfer for scene graph generation. Sim2SG addresses\nthe domain gap by decomposing it into appearance, label and prediction\ndiscrepancies between the two domains. We handle these discrepancies by\nintroducing pseudo statistic based self-learning and adversarial techniques.\nSim2SG does not require costly supervision from the real-world dataset. Our\nexperiments demonstrate significant improvements over baselines in reducing the\ndomain gap both qualitatively and quantitatively. We validate our approach on\ntoy simulators, as well as realistic simulators evaluated on real-world data.\n", "versions": [{"version": "v1", "created": "Mon, 30 Nov 2020 01:21:55 GMT"}], "update_date": "2020-12-01", "authors_parsed": [["Prakash", "Aayush", ""], ["Debnath", "Shoubhik", ""], ["Lafleche", "Jean-Francois", ""], ["Cameracci", "Eric", ""], ["State", "Gavriel", ""], ["Law", "Marc T.", ""]]}, {"id": "2011.14503", "submitter": "Yuqing Wang", "authors": "Yuqing Wang, Zhaoliang Xu, Xinlong Wang, Chunhua Shen, Baoshan Cheng,\n  Hao Shen, Huaxia Xia", "title": "End-to-End Video Instance Segmentation with Transformers", "comments": "CVPR2021 Oral", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Video instance segmentation (VIS) is the task that requires simultaneously\nclassifying, segmenting and tracking object instances of interest in video.\nRecent methods typically develop sophisticated pipelines to tackle this task.\nHere, we propose a new video instance segmentation framework built upon\nTransformers, termed VisTR, which views the VIS task as a direct end-to-end\nparallel sequence decoding/prediction problem. Given a video clip consisting of\nmultiple image frames as input, VisTR outputs the sequence of masks for each\ninstance in the video in order directly. At the core is a new, effective\ninstance sequence matching and segmentation strategy, which supervises and\nsegments instances at the sequence level as a whole. VisTR frames the instance\nsegmentation and tracking in the same perspective of similarity learning, thus\nconsiderably simplifying the overall pipeline and is significantly different\nfrom existing approaches. Without bells and whistles, VisTR achieves the\nhighest speed among all existing VIS models, and achieves the best result among\nmethods using single model on the YouTube-VIS dataset. For the first time, we\ndemonstrate a much simpler and faster video instance segmentation framework\nbuilt upon Transformers, achieving competitive accuracy. We hope that VisTR can\nmotivate future research for more video understanding tasks.\n", "versions": [{"version": "v1", "created": "Mon, 30 Nov 2020 02:03:50 GMT"}, {"version": "v2", "created": "Fri, 4 Dec 2020 10:45:41 GMT"}, {"version": "v3", "created": "Wed, 24 Mar 2021 06:45:17 GMT"}, {"version": "v4", "created": "Sun, 25 Apr 2021 09:43:28 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Wang", "Yuqing", ""], ["Xu", "Zhaoliang", ""], ["Wang", "Xinlong", ""], ["Shen", "Chunhua", ""], ["Cheng", "Baoshan", ""], ["Shen", "Hao", ""], ["Xia", "Huaxia", ""]]}, {"id": "2011.14504", "submitter": "Jiayi Yang", "authors": "Jiayi Yang, Lei Deng, Yukuan Yang, Yuan Xie, Guoqi Li", "title": "Training and Inference for Integer-Based Semantic Segmentation Network", "comments": "17 page, 12 figures, submitted to Neurocomputing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Semantic segmentation has been a major topic in research and industry in\nrecent years. However, due to the computation complexity of pixel-wise\nprediction and backpropagation algorithm, semantic segmentation has been\ndemanding in computation resources, resulting in slow training and inference\nspeed and large storage space to store models. Existing schemes that speed up\nsegmentation network change the network structure and come with noticeable\naccuracy degradation. However, neural network quantization can be used to\nreduce computation load while maintaining comparable accuracy and original\nnetwork structure. Semantic segmentation networks are different from\ntraditional deep convolutional neural networks (DCNNs) in many ways, and this\ntopic has not been thoroughly explored in existing works. In this paper, we\npropose a new quantization framework for training and inference of segmentation\nnetworks, where parameters and operations are constrained to 8-bit\ninteger-based values for the first time. Full quantization of the data flow and\nthe removal of square and root operations in batch normalization give our\nframework the ability to perform inference on fixed-point devices. Our proposed\nframework is evaluated on mainstream semantic segmentation networks like\nFCN-VGG16 and DeepLabv3-ResNet50, achieving comparable accuracy against\nfloating-point framework on ADE20K dataset and PASCAL VOC 2012 dataset.\n", "versions": [{"version": "v1", "created": "Mon, 30 Nov 2020 02:07:07 GMT"}], "update_date": "2020-12-01", "authors_parsed": [["Yang", "Jiayi", ""], ["Deng", "Lei", ""], ["Yang", "Yukuan", ""], ["Xie", "Yuan", ""], ["Li", "Guoqi", ""]]}, {"id": "2011.14512", "submitter": "Huangxing Lin", "authors": "Huangxing Lin, Yihong Zhuang, Yue Huang, Xinghao Ding, Yizhou Yu,\n  Xiaoqing Liu and John Paisley", "title": "Adaptive noise imitation for image denoising", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  The effectiveness of existing denoising algorithms typically relies on\naccurate pre-defined noise statistics or plenty of paired data, which limits\ntheir practicality. In this work, we focus on denoising in the more common case\nwhere noise statistics and paired data are unavailable. Considering that\ndenoising CNNs require supervision, we develop a new \\textbf{adaptive noise\nimitation (ADANI)} algorithm that can synthesize noisy data from naturally\nnoisy images. To produce realistic noise, a noise generator takes unpaired\nnoisy/clean images as input, where the noisy image is a guide for noise\ngeneration. By imposing explicit constraints on the type, level and gradient of\nnoise, the output noise of ADANI will be similar to the guided noise, while\nkeeping the original clean background of the image. Coupling the noisy data\noutput from ADANI with the corresponding ground-truth, a denoising CNN is then\ntrained in a fully-supervised manner. Experiments show that the noisy data\nproduced by ADANI are visually and statistically similar to real ones so that\nthe denoising CNN in our method is competitive to other networks trained with\nexternal paired data.\n", "versions": [{"version": "v1", "created": "Mon, 30 Nov 2020 02:49:36 GMT"}], "update_date": "2020-12-01", "authors_parsed": [["Lin", "Huangxing", ""], ["Zhuang", "Yihong", ""], ["Huang", "Yue", ""], ["Ding", "Xinghao", ""], ["Yu", "Yizhou", ""], ["Liu", "Xiaoqing", ""], ["Paisley", "John", ""]]}, {"id": "2011.14525", "submitter": "Benteng Ma", "authors": "Benteng Ma, Jing Zhang, Yong Xia, Dacheng Tao", "title": "Inter-layer Transition in Neural Architecture Search", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Differential Neural Architecture Search (NAS) methods represent the network\narchitecture as a repetitive proxy directed acyclic graph (DAG) and optimize\nthe network weights and architecture weights alternatively in a differential\nmanner. However, existing methods model the architecture weights on each edge\n(i.e., a layer in the network) as statistically independent variables, ignoring\nthe dependency between edges in DAG induced by their directed topological\nconnections. In this paper, we make the first attempt to investigate such\ndependency by proposing a novel Inter-layer Transition NAS method. It casts the\narchitecture optimization into a sequential decision process where the\ndependency between the architecture weights of connected edges is explicitly\nmodeled. Specifically, edges are divided into inner and outer groups according\nto whether or not their predecessor edges are in the same cell. While the\narchitecture weights of outer edges are optimized independently, those of inner\nedges are derived sequentially based on the architecture weights of their\npredecessor edges and the learnable transition matrices in an attentive\nprobability transition manner. Experiments on five benchmarks confirm the value\nof modeling inter-layer dependency and demonstrate the proposed method\noutperforms state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Mon, 30 Nov 2020 03:33:52 GMT"}], "update_date": "2020-12-01", "authors_parsed": [["Ma", "Benteng", ""], ["Zhang", "Jing", ""], ["Xia", "Yong", ""], ["Tao", "Dacheng", ""]]}, {"id": "2011.14540", "submitter": "Shuhao Cui", "authors": "Shuhao Cui, Xuan Jin, Shuhui Wang, Yuan He, Qingming Huang", "title": "Heuristic Domain Adaptation", "comments": null, "journal-ref": "NeurIPS 2020", "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  In visual domain adaptation (DA), separating the domain-specific\ncharacteristics from the domain-invariant representations is an ill-posed\nproblem. Existing methods apply different kinds of priors or directly minimize\nthe domain discrepancy to address this problem, which lack flexibility in\nhandling real-world situations. Another research pipeline expresses the\ndomain-specific information as a gradual transferring process, which tends to\nbe suboptimal in accurately removing the domain-specific properties. In this\npaper, we address the modeling of domain-invariant and domain-specific\ninformation from the heuristic search perspective. We identify the\ncharacteristics in the existing representations that lead to larger domain\ndiscrepancy as the heuristic representations. With the guidance of heuristic\nrepresentations, we formulate a principled framework of Heuristic Domain\nAdaptation (HDA) with well-founded theoretical guarantees. To perform HDA, the\ncosine similarity scores and independence measurements between domain-invariant\nand domain-specific representations are cast into the constraints at the\ninitial and final states during the learning procedure. Similar to the final\ncondition of heuristic search, we further derive a constraint enforcing the\nfinal range of heuristic network output to be small. Accordingly, we propose\nHeuristic Domain Adaptation Network (HDAN), which explicitly learns the\ndomain-invariant and domain-specific representations with the above mentioned\nconstraints. Extensive experiments show that HDAN has exceeded state-of-the-art\non unsupervised DA, multi-source DA and semi-supervised DA. The code is\navailable at https://github.com/cuishuhao/HDA.\n", "versions": [{"version": "v1", "created": "Mon, 30 Nov 2020 04:21:35 GMT"}], "update_date": "2020-12-01", "authors_parsed": [["Cui", "Shuhao", ""], ["Jin", "Xuan", ""], ["Wang", "Shuhui", ""], ["He", "Yuan", ""], ["Huang", "Qingming", ""]]}, {"id": "2011.14563", "submitter": "Yuan Liu", "authors": "Yuan Liu, Lingjie Liu, Cheng Lin, Zhen Dong, Wenping Wang", "title": "Learnable Motion Coherence for Correspondence Pruning", "comments": "10 pages, 7 figures, project page:\n  https://liuyuan-pal.github.io/LMCNet/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motion coherence is an important clue for distinguishing true correspondences\nfrom false ones. Modeling motion coherence on sparse putative correspondences\nis challenging due to their sparsity and uneven distributions. Existing works\non motion coherence are sensitive to parameter settings and have difficulty in\ndealing with complex motion patterns. In this paper, we introduce a network\ncalled Laplacian Motion Coherence Network (LMCNet) to learn motion coherence\nproperty for correspondence pruning. We propose a novel formulation of fitting\ncoherent motions with a smooth function on a graph of correspondences and show\nthat this formulation allows a closed-form solution by graph Laplacian. This\nclosed-form solution enables us to design a differentiable layer in a learning\nframework to capture global motion coherence from putative correspondences. The\nglobal motion coherence is further combined with local coherence extracted by\nanother local layer to robustly detect inlier correspondences. Experiments\ndemonstrate that LMCNet has superior performances to the state of the art in\nrelative camera pose estimation and correspondences pruning of dynamic scenes.\n", "versions": [{"version": "v1", "created": "Mon, 30 Nov 2020 05:57:25 GMT"}], "update_date": "2020-12-01", "authors_parsed": [["Liu", "Yuan", ""], ["Liu", "Lingjie", ""], ["Lin", "Cheng", ""], ["Dong", "Zhen", ""], ["Wang", "Wenping", ""]]}, {"id": "2011.14565", "submitter": "Zerong Zheng", "authors": "Zerong Zheng, Tao Yu, Qionghai Dai, Yebin Liu", "title": "Deep Implicit Templates for 3D Shape Representation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep implicit functions (DIFs), as a kind of 3D shape representation, are\nbecoming more and more popular in the 3D vision community due to their\ncompactness and strong representation power. However, unlike polygon mesh-based\ntemplates, it remains a challenge to reason dense correspondences or other\nsemantic relationships across shapes represented by DIFs, which limits its\napplications in texture transfer, shape analysis and so on. To overcome this\nlimitation and also make DIFs more interpretable, we propose Deep Implicit\nTemplates, a new 3D shape representation that supports explicit correspondence\nreasoning in deep implicit representations. Our key idea is to formulate DIFs\nas conditional deformations of a template implicit function. To this end, we\npropose Spatial Warping LSTM, which decomposes the conditional spatial\ntransformation into multiple affine transformations and guarantees\ngeneralization capability. Moreover, the training loss is carefully designed in\norder to achieve high reconstruction accuracy while learning a plausible\ntemplate with accurate correspondences in an unsupervised manner. Experiments\nshow that our method can not only learn a common implicit template for a\ncollection of shapes, but also establish dense correspondences across all the\nshapes simultaneously without any supervision.\n", "versions": [{"version": "v1", "created": "Mon, 30 Nov 2020 06:01:49 GMT"}, {"version": "v2", "created": "Thu, 13 May 2021 09:22:32 GMT"}], "update_date": "2021-05-14", "authors_parsed": [["Zheng", "Zerong", ""], ["Yu", "Tao", ""], ["Dai", "Qionghai", ""], ["Liu", "Yebin", ""]]}, {"id": "2011.14574", "submitter": "Yufei Xu", "authors": "Yufei Xu, Jing Zhang, Stephen J. Maybank, Dacheng Tao", "title": "DUT: Learning Video Stabilization by Simply Watching Unstable Videos", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a Deep Unsupervised Trajectory-based stabilization framework (DUT)\nin this paper. Traditional stabilizers focus on trajectory-based smoothing,\nwhich is controllable but fragile in occluded and textureless cases regarding\nthe usage of hand-crafted features. On the other hand, previous deep video\nstabilizers directly generate stable videos in a supervised manner without\nexplicit trajectory estimation, which is robust but less controllable and the\nappropriate paired data are hard to obtain. To construct a controllable and\nrobust stabilizer, DUT makes the first attempt to stabilize unstable videos by\nexplicitly estimating and smoothing trajectories in an unsupervised deep\nlearning manner, which is composed of a DNN-based keypoint detector and motion\nestimator to generate grid-based trajectories, and a DNN-based trajectory\nsmoother to stabilize videos. We exploit both the nature of continuity in\nmotion and the consistency of keypoints and grid vertices before and after\nstabilization for unsupervised training. Experiment results on public\nbenchmarks show that DUT outperforms representative state-of-the-art methods\nboth qualitatively and quantitatively.\n", "versions": [{"version": "v1", "created": "Mon, 30 Nov 2020 06:48:20 GMT"}, {"version": "v2", "created": "Tue, 1 Dec 2020 02:40:19 GMT"}], "update_date": "2020-12-02", "authors_parsed": [["Xu", "Yufei", ""], ["Zhang", "Jing", ""], ["Maybank", "Stephen J.", ""], ["Tao", "Dacheng", ""]]}, {"id": "2011.14578", "submitter": "Stone Yun", "authors": "Stone Yun and Alexander Wong", "title": "Where Should We Begin? A Low-Level Exploration of Weight Initialization\n  Impact on Quantized Behaviour of Deep Neural Networks", "comments": "Accepted for publication at the 6th Annual Conference on Computer\n  Vision and Intelligent Systems (CVIS2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  With the proliferation of deep convolutional neural network (CNN) algorithms\nfor mobile processing, limited precision quantization has become an essential\ntool for CNN efficiency. Consequently, various works have sought to design\nfixed precision quantization algorithms and quantization-focused optimization\ntechniques that minimize quantization induced performance degradation. However,\nthere is little concrete understanding of how various CNN design decisions/best\npractices affect quantized inference behaviour. Weight initialization\nstrategies are often associated with solving issues such as vanishing/exploding\ngradients but an often-overlooked aspect is their impact on the final trained\ndistributions of each layer. We present an in-depth, fine-grained ablation\nstudy of the effect of different weights initializations on the final\ndistributions of weights and activations of different CNN architectures. The\nfine-grained, layerwise analysis enables us to gain deep insights on how\ninitial weights distributions will affect final accuracy and quantized\nbehaviour. To our best knowledge, we are the first to perform such a low-level,\nin-depth quantitative analysis of weights initialization and its effect on\nquantized behaviour.\n", "versions": [{"version": "v1", "created": "Mon, 30 Nov 2020 06:54:28 GMT"}], "update_date": "2020-12-01", "authors_parsed": [["Yun", "Stone", ""], ["Wong", "Alexander", ""]]}, {"id": "2011.14579", "submitter": "Zhijian Qiao", "authors": "Zhijian Qiao, Huanshu Wei, Zhe Liu, Chuanzhe Suo, Hesheng Wang", "title": "End-to-End 3D Point Cloud Learning for Registration Task Using Virtual\n  Correspondences", "comments": "Accepted to IROS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  3D Point cloud registration is still a very challenging topic due to the\ndifficulty in finding the rigid transformation between two point clouds with\npartial correspondences, and it's even harder in the absence of any initial\nestimation information. In this paper, we present an end-to-end deep-learning\nbased approach to resolve the point cloud registration problem. Firstly, the\nrevised LPD-Net is introduced to extract features and aggregate them with the\ngraph network. Secondly, the self-attention mechanism is utilized to enhance\nthe structure information in the point cloud and the cross-attention mechanism\nis designed to enhance the corresponding information between the two input\npoint clouds. Based on which, the virtual corresponding points can be generated\nby a soft pointer based method, and finally, the point cloud registration\nproblem can be solved by implementing the SVD method. Comparison results in\nModelNet40 dataset validate that the proposed approach reaches the\nstate-of-the-art in point cloud registration tasks and experiment resutls in\nKITTI dataset validate the effectiveness of the proposed approach in real\napplications.Our source code is available at\n\\url{https://github.com/qiaozhijian/VCR-Net.git}\n", "versions": [{"version": "v1", "created": "Mon, 30 Nov 2020 06:55:05 GMT"}, {"version": "v2", "created": "Fri, 18 Jun 2021 02:06:59 GMT"}], "update_date": "2021-06-21", "authors_parsed": [["Qiao", "Zhijian", ""], ["Wei", "Huanshu", ""], ["Liu", "Zhe", ""], ["Suo", "Chuanzhe", ""], ["Wang", "Hesheng", ""]]}, {"id": "2011.14584", "submitter": "Hsin-Pai Cheng", "authors": "Hsin-Pai Cheng, Feng Liang, Meng Li, Bowen Cheng, Feng Yan, Hai Li,\n  Vikas Chandra, Yiran Chen", "title": "ScaleNAS: One-Shot Learning of Scale-Aware Representations for Visual\n  Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Scale variance among different sizes of body parts and objects is a\nchallenging problem for visual recognition tasks. Existing works usually design\ndedicated backbone or apply Neural architecture Search(NAS) for each task to\ntackle this challenge. However, existing works impose significant limitations\non the design or search space. To solve these problems, we present ScaleNAS, a\none-shot learning method for exploring scale-aware representations. ScaleNAS\nsolves multiple tasks at a time by searching multi-scale feature aggregation.\nScaleNAS adopts a flexible search space that allows an arbitrary number of\nblocks and cross-scale feature fusions. To cope with the high search cost\nincurred by the flexible space, ScaleNAS employs one-shot learning for\nmulti-scale supernet driven by grouped sampling and evolutionary search.\nWithout further retraining, ScaleNet can be directly deployed for different\nvisual recognition tasks with superior performance. We use ScaleNAS to create\nhigh-resolution models for two different tasks, ScaleNet-P for human pose\nestimation and ScaleNet-S for semantic segmentation. ScaleNet-P and ScaleNet-S\noutperform existing manually crafted and NAS-based methods in both tasks. When\napplying ScaleNet-P to bottom-up human pose estimation, it surpasses the\nstate-of-the-art HigherHRNet. In particular, ScaleNet-P4 achieves 71.6% AP on\nCOCO test-dev, achieving new state-of-the-art result.\n", "versions": [{"version": "v1", "created": "Mon, 30 Nov 2020 07:11:11 GMT"}], "update_date": "2020-12-01", "authors_parsed": [["Cheng", "Hsin-Pai", ""], ["Liang", "Feng", ""], ["Li", "Meng", ""], ["Cheng", "Bowen", ""], ["Yan", "Feng", ""], ["Li", "Hai", ""], ["Chandra", "Vikas", ""], ["Chen", "Yiran", ""]]}, {"id": "2011.14585", "submitter": "Jaehui Hwang", "authors": "Jaehui Hwang, Jun-Hyuk Kim, Jun-Ho Choi, and Jong-Seok Lee", "title": "Just One Moment: Inconspicuous One Frame Attack on Deep Action\n  Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The video-based action recognition task has been extensively studied in\nrecent years. In this paper, we study the vulnerability of deep learning-based\naction recognition methods against the adversarial attack using a new one frame\nattack that adds an inconspicuous perturbation to only a single frame of a\ngiven video clip. We investigate the effectiveness of our one frame attack on\nstate-of-the-art action recognition models, along with thorough analysis of the\nvulnerability in terms of their model structure and perceivability of the\nperturbation. Our method shows high fooling rates and produces hardly\nperceivable perturbation to human observers, which is evaluated by a subjective\ntest. In addition, we present a video-agnostic approach that finds a universal\nperturbation.\n", "versions": [{"version": "v1", "created": "Mon, 30 Nov 2020 07:11:56 GMT"}], "update_date": "2020-12-01", "authors_parsed": [["Hwang", "Jaehui", ""], ["Kim", "Jun-Hyuk", ""], ["Choi", "Jun-Ho", ""], ["Lee", "Jong-Seok", ""]]}, {"id": "2011.14586", "submitter": "Stone Yun", "authors": "Stone Yun and Alexander Wong", "title": "FactorizeNet: Progressive Depth Factorization for Efficient Network\n  Architecture Exploration Under Quantization Constraints", "comments": "Accepted for Publication at the 2020 Workshop on Energy Efficient\n  Machine Learning and Cognitive Computing (EMC2 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Depth factorization and quantization have emerged as two of the principal\nstrategies for designing efficient deep convolutional neural network (CNN)\narchitectures tailored for low-power inference on the edge. However, there is\nstill little detailed understanding of how different depth factorization\nchoices affect the final, trained distributions of each layer in a CNN,\nparticularly in the situation of quantized weights and activations. In this\nstudy, we introduce a progressive depth factorization strategy for efficient\nCNN architecture exploration under quantization constraints. By algorithmically\nincreasing the granularity of depth factorization in a progressive manner, the\nproposed strategy enables a fine-grained, low-level analysis of layer-wise\ndistributions. Thus enabling the gain of in-depth, layer-level insights on\nefficiency-accuracy tradeoffs under fixed-precision quantization. Such a\nprogressive depth factorization strategy also enables efficient identification\nof the optimal depth-factorized macroarchitecture design (which we will refer\nto here as FactorizeNet) based on the desired efficiency-accuracy requirements.\n", "versions": [{"version": "v1", "created": "Mon, 30 Nov 2020 07:12:26 GMT"}], "update_date": "2020-12-01", "authors_parsed": [["Yun", "Stone", ""], ["Wong", "Alexander", ""]]}, {"id": "2011.14589", "submitter": "Tianze Gao", "authors": "Tianze Gao, Huihui Pan, Huijun Gao", "title": "Monocular 3D Object Detection with Sequential Feature Association and\n  Depth Hint Augmentation", "comments": "11 pages, 11 figures. This work has been submitted to the IEEE for\n  possible publication. Copyright may be transferred without notice, after\n  which this version may no longer be accessible", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Monocular 3D object detection is a promising research topic for the\nintelligent perception systems of autonomous driving. In this work, a\nsingle-stage keypoint-based network, named as FADNet, is presented to address\nthe task of monocular 3D object detection. In contrast to previous\nkeypoint-based methods which adopt identical layouts for output branches, we\npropose to divide the output modalities into different groups according to the\nestimating difficulty, whereby different groups are treated differently by\nsequential feature association. Another contribution of this work is the\nstrategy of depth hint augmentation. To provide characterized depth patterns as\nhints for depth estimation, a dedicated depth hint module is designed to\ngenerate row-wise features named as depth hints, which are explicitly\nsupervised in a bin-wise manner. In the training stage, the regression outputs\nare uniformly encoded to enable loss disentanglement. The 2D loss term is\nfurther adapted to be depth-aware for improving the detection accuracy of small\nobjects. The contributions of this work are validated by conducting experiments\nand ablation study on the KITTI benchmark. Without utilizing depth priors, post\noptimization, or other refinement modules, our network performs competitively\nagainst state-of-the-art methods while maintaining a decent running speed.\n", "versions": [{"version": "v1", "created": "Mon, 30 Nov 2020 07:19:14 GMT"}, {"version": "v2", "created": "Tue, 1 Dec 2020 12:51:55 GMT"}, {"version": "v3", "created": "Wed, 2 Dec 2020 03:07:22 GMT"}], "update_date": "2020-12-03", "authors_parsed": [["Gao", "Tianze", ""], ["Pan", "Huihui", ""], ["Gao", "Huijun", ""]]}, {"id": "2011.14593", "submitter": "Christina Baek", "authors": "Ziyang Wu, Christina Baek, Chong You, Yi Ma", "title": "Incremental Learning via Rate Reduction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current deep learning architectures suffer from catastrophic forgetting, a\nfailure to retain knowledge of previously learned classes when incrementally\ntrained on new classes. The fundamental roadblock faced by deep learning\nmethods is that deep learning models are optimized as \"black boxes,\" making it\ndifficult to properly adjust the model parameters to preserve knowledge about\npreviously seen data. To overcome the problem of catastrophic forgetting, we\npropose utilizing an alternative \"white box\" architecture derived from the\nprinciple of rate reduction, where each layer of the network is explicitly\ncomputed without back propagation. Under this paradigm, we demonstrate that,\ngiven a pre-trained network and new data classes, our approach can provably\nconstruct a new network that emulates joint training with all past and new\nclasses. Finally, our experiments show that our proposed learning algorithm\nobserves significantly less decay in classification performance, outperforming\nstate of the art methods on MNIST and CIFAR-10 by a large margin and justifying\nthe use of \"white box\" algorithms for incremental learning even for\nsufficiently complex image data.\n", "versions": [{"version": "v1", "created": "Mon, 30 Nov 2020 07:23:55 GMT"}], "update_date": "2020-12-01", "authors_parsed": [["Wu", "Ziyang", ""], ["Baek", "Christina", ""], ["You", "Chong", ""], ["Ma", "Yi", ""]]}, {"id": "2011.14594", "submitter": "Tianze Gao", "authors": "Tianze Gao, Huihui Pan, Zidong Wang, Huijun Gao", "title": "A CRF-based Framework for Tracklet Inactivation in Online Multi-Object\n  Tracking", "comments": "13 pages, 8 figures. This work has been accepted by IEEE Transactions\n  on Multimedia. (DOI: 10.1109/TMM.2021.3062489)", "journal-ref": null, "doi": "10.1109/TMM.2021.3062489", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Online multi-object tracking (MOT) is an active research topic in the domain\nof computer vision. Although many previously proposed algorithms have exhibited\ndecent results, the issue of tracklet inactivation has not been sufficiently\nstudied. Simple strategies such as using a fixed threshold on classification\nscores are adopted, yielding undesirable tracking mistakes and limiting the\noverall performance. In this paper, a conditional random field (CRF) based\nframework is put forward to tackle the tracklet inactivation issue in online\nMOT problems. A discrete CRF which exploits the intra-frame relationship\nbetween tracking hypotheses is developed to improve the robustness of tracklet\ninactivation. Separate sets of feature functions are designed for the unary and\nbinary terms in the CRF, which take into account various tracking challenges in\npractical scenarios. To handle the problem of varying CRF nodes in the MOT\ncontext, two strategies named as hypothesis filtering and dummy nodes are\nemployed. In the proposed framework, the inference stage is conducted by using\nthe loopy belief propagation algorithm, and the CRF parameters are determined\nby utilizing the maximum likelihood estimation method followed by slight manual\nadjustment. Experimental results show that the tracker combined with the\nCRF-based framework outperforms the baseline on the MOT16 and MOT17 benchmarks.\nThe extensibility of the proposed framework is further validated by an\nextensive experiment.\n", "versions": [{"version": "v1", "created": "Mon, 30 Nov 2020 07:28:52 GMT"}, {"version": "v2", "created": "Fri, 23 Apr 2021 07:49:35 GMT"}], "update_date": "2021-04-26", "authors_parsed": [["Gao", "Tianze", ""], ["Pan", "Huihui", ""], ["Wang", "Zidong", ""], ["Gao", "Huijun", ""]]}, {"id": "2011.14598", "submitter": "Chen Zhao", "authors": "Chen Zhao, Ali Thabet, Bernard Ghanem", "title": "Video Self-Stitching Graph Network for Temporal Action Localization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Temporal action localization (TAL) in videos is a challenging task,\nespecially due to the large variation in action temporal scales. Short actions\nusually occupy the major proportion in the data, but have the lowest\nperformance with all current methods. In this paper, we confront the challenge\nof short actions and propose a multi-level cross-scale solution dubbed as video\nself-stitching graph network (VSGN). We have two key components in VSGN: video\nself-stitching (VSS) and cross-scale graph pyramid network (xGPN). In VSS, we\nfocus on a short period of a video and magnify it along the temporal dimension\nto obtain a larger scale. We stitch the original clip and its magnified\ncounterpart in one input sequence to take advantage of the complementary\nproperties of both scales. The xGPN component further exploits the cross-scale\ncorrelations by a pyramid of cross-scale graph networks, each containing a\nhybrid module to aggregate features from across scales as well as within the\nsame scale. Our VSGN not only enhances the feature representations, but also\ngenerates more positive anchors for short actions and more short training\nsamples. Experiments demonstrate that VSGN obviously improves the localization\nperformance of short actions as well as achieving the state-of-the-art overall\nperformance on THUMOS-14 and ActivityNet-v1.3.\n", "versions": [{"version": "v1", "created": "Mon, 30 Nov 2020 07:44:52 GMT"}, {"version": "v2", "created": "Sun, 13 Dec 2020 07:04:05 GMT"}, {"version": "v3", "created": "Tue, 30 Mar 2021 05:08:55 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Zhao", "Chen", ""], ["Thabet", "Ali", ""], ["Ghanem", "Bernard", ""]]}, {"id": "2011.14603", "submitter": "Sandesh Ramesh", "authors": "Sandesh Ramesh, Manoj Kumar M V, and K Aditya Shastry", "title": "REaL: Real-time Face Detection and Recognition Using Euclidean Space and\n  Likelihood Estimation", "comments": "International Journal of System Assurance Engineering and Management", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Detecting and recognizing faces accurately has always been a challenge.\nDifferentiating facial features, training images, and producing quick results\nrequire a lot of computation. The REaL system we have proposed in this paper\ndiscusses its functioning and ways in which computations can be carried out in\na short period. REaL experiments are carried out on live images and the\nrecognition rates are promising. The system is also successful in removing\nnon-human objects from its calculations. The system uses a local database to\nstore captured images and feeds the neural network frequently. The captured\nimages are cropped automatically to remove unwanted noise. The system\ncalculates the Euler angles and the probability of whether the face is smiling,\nhas its left eye, and right eyes open or not.\n", "versions": [{"version": "v1", "created": "Mon, 30 Nov 2020 08:03:04 GMT"}], "update_date": "2020-12-01", "authors_parsed": [["Ramesh", "Sandesh", ""], ["M", "Manoj Kumar", "V"], ["Shastry", "K Aditya", ""]]}, {"id": "2011.14607", "submitter": "Jae-Yeong Lee", "authors": "Jae-Yeong Lee", "title": "Zero-Shot Calibration of Fisheye Cameras", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  In this paper, we present a novel zero-shot camera calibration method that\nestimates camera parameters with no calibration image. It is common sense that\nwe need at least one or more pattern images for camera calibration. However,\nthe proposed method estimates camera parameters from the horizontal and\nvertical field of view information of the camera without any image acquisition.\nThe proposed method is particularly useful for wide-angle or fisheye cameras\nthat have large image distortion. Image distortion is modeled in the way\nfisheye lenses are designed and estimated based on the square pixel assumption\nof the image sensors. The calibration accuracy of the proposed method is\nevaluated on eight different commercial cameras qualitatively and\nquantitatively, and compared with conventional calibration methods. The\nexperimental results show that the calibration accuracy of the zero-shot method\nis comparable to conventional full calibration results. The method can be used\nas a practical alternative in real applications where individual calibration is\ndifficult or impractical, and in most field applications where calibration\naccuracy is less critical. Moreover, the estimated camera parameters by the\nmethod can also be used to provide proper initialization of any existing\ncalibration methods, making them to converge more stably and avoid local\nminima.\n", "versions": [{"version": "v1", "created": "Mon, 30 Nov 2020 08:10:24 GMT"}], "update_date": "2020-12-01", "authors_parsed": [["Lee", "Jae-Yeong", ""]]}, {"id": "2011.14611", "submitter": "Jinlong Fan", "authors": "Jinlong Fan and Jing Zhang and Dacheng Tao", "title": "SIR: Self-supervised Image Rectification via Seeing the Same Scene from\n  Multiple Different Lenses", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning has demonstrated its power in image rectification by leveraging\nthe representation capacity of deep neural networks via supervised training\nbased on a large-scale synthetic dataset. However, the model may overfit the\nsynthetic images and generalize not well on real-world fisheye images due to\nthe limited universality of a specific distortion model and the lack of\nexplicitly modeling the distortion and rectification process. In this paper, we\npropose a novel self-supervised image rectification (SIR) method based on an\nimportant insight that the rectified results of distorted images of a same\nscene from different lens should be the same. Specifically, we devise a new\nnetwork architecture with a shared encoder and several prediction heads, each\nof which predicts the distortion parameter of a specific distortion model. We\nfurther leverage a differentiable warping module to generate the rectified\nimages and re-distorted images from the distortion parameters and exploit the\nintra- and inter-model consistency between them during training, thereby\nleading to a self-supervised learning scheme without the need for ground-truth\ndistortion parameters or normal images. Experiments on synthetic dataset and\nreal-world fisheye images demonstrate that our method achieves comparable or\neven better performance than the supervised baseline method and representative\nstate-of-the-art methods. Self-supervised learning also improves the\nuniversality of distortion models while keeping their self-consistency.\n", "versions": [{"version": "v1", "created": "Mon, 30 Nov 2020 08:23:25 GMT"}, {"version": "v2", "created": "Fri, 18 Jun 2021 07:26:29 GMT"}], "update_date": "2021-06-21", "authors_parsed": [["Fan", "Jinlong", ""], ["Zhang", "Jing", ""], ["Tao", "Dacheng", ""]]}, {"id": "2011.14619", "submitter": "Zhaoqi Su", "authors": "Zhaoqi Su and Tao Yu and Yangang Wang and Yipeng Li and Yebin Liu", "title": "DeepCloth: Neural Garment Representation for Shape and Style Editing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Garment representation, animation and editing is a challenging topic in the\narea of computer vision and graphics. Existing methods cannot perform smooth\nand reasonable garment transition under different shape styles and topologies.\nIn this work, we introduce a novel method, termed as DeepCloth, to establish a\nunified garment representation framework enabling free and smooth garment style\ntransition. Our key idea is to represent garment geometry by a \"UV-position map\nwith mask\", which potentially allows the description of various garments with\ndifferent shapes and topologies. Furthermore, we learn a continuous feature\nspace mapped from the above UV space, enabling garment shape editing and\ntransition by controlling the garment features. Finally, we demonstrate\napplications of garment animation, reconstruction and editing based on our\nneural garment representation and encoding method. To conclude, with the\nproposed DeepCloth, we move a step forward on establishing a more flexible and\ngeneral 3D garment digitization framework. Experiments demonstrate that our\nmethod can achieve the state-of-the-art garment modeling results compared with\nthe previous methods.\n", "versions": [{"version": "v1", "created": "Mon, 30 Nov 2020 08:42:38 GMT"}], "update_date": "2020-12-01", "authors_parsed": [["Su", "Zhaoqi", ""], ["Yu", "Tao", ""], ["Wang", "Yangang", ""], ["Li", "Yipeng", ""], ["Liu", "Yebin", ""]]}, {"id": "2011.14627", "submitter": "Qianqian Zhang", "authors": "Qianqian Zhang and Ruizhi Sun", "title": "SAR Image Despeckling Based on Convolutional Denoising Autoencoder", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In Synthetic Aperture Radar (SAR) imaging, despeckling is very important for\nimage analysis,whereas speckle is known as a kind of multiplicative noise\ncaused by the coherent imaging system. During the past three decades, various\nalgorithms have been proposed to denoise the SAR image. Generally, the BM3D is\nconsidered as the state of art technique to despeckle the speckle noise with\nexcellent performance. More recently, deep learning make a success in image\ndenoising and achieved a improvement over conventional method where large train\ndataset is required. Unlike most of the images SAR image despeckling approach,\nthe proposed approach learns the speckle from corrupted images directly. In\nthis paper, the limited scale of dataset make a efficient exploration by using\nconvolutioal denoising autoencoder (C-DAE) to reconstruct the speckle-free SAR\nimages. Batch normalization strategy is integrated with C- DAE to speed up the\ntrain time. Moreover, we compute image quality in standard metrics, PSNR and\nSSIM. It is revealed that our approach perform well than some others.\n", "versions": [{"version": "v1", "created": "Mon, 30 Nov 2020 09:02:25 GMT"}], "update_date": "2020-12-01", "authors_parsed": [["Zhang", "Qianqian", ""], ["Sun", "Ruizhi", ""]]}, {"id": "2011.14631", "submitter": "Yuemei Zhou", "authors": "Yuemei Zhou, Gaochang Wu, Ying Fu, Kun Li, Yebin Liu", "title": "Cross-MPI: Cross-scale Stereo for Image Super-Resolution using\n  Multiplane Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Various combinations of cameras enrich computational photography, among which\nreference-based superresolution (RefSR) plays a critical role in multiscale\nimaging systems. However, existing RefSR approaches fail to accomplish\nhigh-fidelity super-resolution under a large resolution gap, e.g., 8x\nupscaling, due to the lower consideration of the underlying scene structure. In\nthis paper, we aim to solve the RefSR problem in actual multiscale camera\nsystems inspired by multiplane image (MPI) representation. Specifically, we\npropose Cross-MPI, an end-to-end RefSR network composed of a novel plane-aware\nattention-based MPI mechanism, a multiscale guided upsampling module as well as\na super-resolution (SR) synthesis and fusion module. Instead of using a direct\nand exhaustive matching between the cross-scale stereo, the proposed\nplane-aware attention mechanism fully utilizes the concealed scene structure\nfor efficient attention-based correspondence searching. Further combined with a\ngentle coarse-to-fine guided upsampling strategy, the proposed Cross-MPI can\nachieve a robust and accurate detail transmission. Experimental results on both\ndigitally synthesized and optical zoom cross-scale data show that the Cross-MPI\nframework can achieve superior performance against the existing RefSR methods\nand is a real fit for actual multiscale camera systems even with large-scale\ndifferences.\n", "versions": [{"version": "v1", "created": "Mon, 30 Nov 2020 09:14:07 GMT"}, {"version": "v2", "created": "Mon, 29 Mar 2021 13:58:19 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Zhou", "Yuemei", ""], ["Wu", "Gaochang", ""], ["Fu", "Ying", ""], ["Li", "Kun", ""], ["Liu", "Yebin", ""]]}, {"id": "2011.14642", "submitter": "XiaoChen Zhao", "authors": "Xiaochen Zhao, Zerong Zheng, Chaonan Ji, Zhenyi Liu, Siyou Lin, Tao\n  Yu, Jinli Suo, Yebin Liu", "title": "Vehicle Reconstruction and Texture Estimation Using Deep Implicit\n  Semantic Template Mapping", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce VERTEX, an effective solution to recover 3D shape and intrinsic\ntexture of vehicles from uncalibrated monocular input in real-world street\nenvironments. To fully utilize the template prior of vehicles, we propose a\nnovel geometry and texture joint representation, based on implicit semantic\ntemplate mapping. Compared to existing representations which infer 3D texture\ndistribution, our method explicitly constrains the texture distribution on the\n2D surface of the template as well as avoids limitations of fixed resolution\nand topology. Moreover, by fusing the global and local features together, our\napproach is capable to generate consistent and detailed texture in both visible\nand invisible areas. We also contribute a new synthetic dataset containing 830\nelaborate textured car models labeled with sparse key points and rendered using\nPhysically Based Rendering (PBRT) system with measured HDRI skymaps to obtain\nhighly realistic images. Experiments demonstrate the superior performance of\nour approach on both testing dataset and in-the-wild images. Furthermore, the\npresented technique enables additional applications such as 3D vehicle texture\ntransfer and material identification.\n", "versions": [{"version": "v1", "created": "Mon, 30 Nov 2020 09:27:10 GMT"}, {"version": "v2", "created": "Mon, 29 Mar 2021 05:32:08 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Zhao", "Xiaochen", ""], ["Zheng", "Zerong", ""], ["Ji", "Chaonan", ""], ["Liu", "Zhenyi", ""], ["Lin", "Siyou", ""], ["Yu", "Tao", ""], ["Suo", "Jinli", ""], ["Liu", "Yebin", ""]]}, {"id": "2011.14660", "submitter": "Shuai Zhao", "authors": "Shuai Zhao, Liguang Zhou, Wenxiao Wang, Deng Cai, Tin Lun Lam,\n  Yangsheng Xu", "title": "Towards Better Accuracy-efficiency Trade-offs: Divide and Co-training", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The width of a neural network matters since increasing the width will\nnecessarily increase the model capacity. However, the performance of a network\ndoes not improve linearly with the width and soon gets saturated. In this case,\nwe argue that increasing the number of networks (ensemble) can achieve better\naccuracy-efficiency trade-offs than purely increasing the width. To prove it,\none large network is divided into several small ones regarding its parameters\nand regularization components. Each of these small networks has a fraction of\nthe original one's parameters. We then train these small networks together and\nmake them see various views of the same data to increase their diversity.\nDuring this co-training process, networks can also learn from each other. As a\nresult, small networks can achieve better ensemble performance than the large\none with few or no extra parameters or FLOPs. Small networks can also achieve\nfaster inference speed than the large one by concurrent running on different\ndevices. We validate our argument with 8 different neural architectures on\ncommon benchmarks through extensive experiments. The code is available at\n\\url{https://github.com/mzhaoshuai/Divide-and-Co-training}.\n", "versions": [{"version": "v1", "created": "Mon, 30 Nov 2020 10:03:34 GMT"}, {"version": "v2", "created": "Tue, 29 Dec 2020 02:41:21 GMT"}, {"version": "v3", "created": "Sat, 20 Mar 2021 14:03:54 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Zhao", "Shuai", ""], ["Zhou", "Liguang", ""], ["Wang", "Wenxiao", ""], ["Cai", "Deng", ""], ["Lam", "Tin Lun", ""], ["Xu", "Yangsheng", ""]]}, {"id": "2011.14661", "submitter": "Yusuke Kawamoto", "authors": "Seira Hidano, Takao Murakami, Yusuke Kawamoto", "title": "TransMIA: Membership Inference Attacks Using Transfer Shadow Training", "comments": "IJCNN 2021 conference paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Transfer learning has been widely studied and gained increasing popularity to\nimprove the accuracy of machine learning models by transferring some knowledge\nacquired in different training. However, no prior work has pointed out that\ntransfer learning can strengthen privacy attacks on machine learning models. In\nthis paper, we propose TransMIA (Transfer learning-based Membership Inference\nAttacks), which use transfer learning to perform membership inference attacks\non the source model when the adversary is able to access the parameters of the\ntransferred model. In particular, we propose a transfer shadow training\ntechnique, where an adversary employs the parameters of the transferred model\nto construct shadow models, to significantly improve the performance of\nmembership inference when a limited amount of shadow training data is available\nto the adversary. We evaluate our attacks using two real datasets, and show\nthat our attacks outperform the state-of-the-art that does not use our transfer\nshadow training technique. We also compare four combinations of the\nlearning-based/entropy-based approach and the fine-tuning/freezing approach,\nall of which employ our transfer shadow training technique. Then we examine the\nperformance of these four approaches based on the distributions of confidence\nvalues, and discuss possible countermeasures against our attacks.\n", "versions": [{"version": "v1", "created": "Mon, 30 Nov 2020 10:03:43 GMT"}, {"version": "v2", "created": "Wed, 10 Feb 2021 13:20:40 GMT"}, {"version": "v3", "created": "Fri, 23 Apr 2021 14:50:44 GMT"}], "update_date": "2021-04-26", "authors_parsed": [["Hidano", "Seira", ""], ["Murakami", "Takao", ""], ["Kawamoto", "Yusuke", ""]]}, {"id": "2011.14663", "submitter": "Lu Han", "authors": "Han-Jia Ye, Lu Han, De-Chuan Zhan", "title": "Revisiting Unsupervised Meta-Learning: Amplifying or Compensating for\n  the Characteristics of Few-Shot Tasks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Meta-learning becomes a practical approach towards few-shot image\nclassification, where a visual recognition system is constructed with limited\nannotated data. Inductive bias such as embedding is learned from a base class\nset with ample labeled examples and then generalizes to few-shot tasks with\nnovel classes. Surprisingly, we find that the base class set labels are not\nnecessary, and discriminative embeddings could be meta-learned in an\nunsupervised manner. Comprehensive analyses indicate two modifications -- the\nsemi-normalized distance metric and the sufficient sampling -- improves\nunsupervised meta-learning (UML) significantly. Based on the modified baseline,\nwe further amplify or compensate for the characteristic of tasks when training\na UML model. First, mixed embeddings are incorporated to increase the\ndifficulty of few-shot tasks. Next, we utilize a task-specific embedding\ntransformation to deal with the specific properties among tasks, maintaining\nthe generalization ability into the vanilla embeddings. Experiments on few-shot\nlearning benchmarks verify that our approaches outperform previous UML methods\nby a 4-10% performance gap, and embeddings learned with our UML achieve\ncomparable or even better performance than its supervised variants.\n", "versions": [{"version": "v1", "created": "Mon, 30 Nov 2020 10:08:35 GMT"}, {"version": "v2", "created": "Tue, 1 Dec 2020 03:38:16 GMT"}], "update_date": "2020-12-02", "authors_parsed": [["Ye", "Han-Jia", ""], ["Han", "Lu", ""], ["Zhan", "De-Chuan", ""]]}, {"id": "2011.14665", "submitter": "Isma Hadji", "authors": "Isma Hadji and Richard P. Wildes", "title": "Why Convolutional Networks Learn Oriented Bandpass Filters: Theory and\n  Empirical Support", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  It has been repeatedly observed that convolutional architectures when applied\nto image understanding tasks learn oriented bandpass filters. A standard\nexplanation of this result is that these filters reflect the structure of the\nimages that they have been exposed to during training: Natural images typically\nare locally composed of oriented contours at various scales and oriented\nbandpass filters are matched to such structure. We offer an alternative\nexplanation based not on the structure of images, but rather on the structure\nof convolutional architectures. In particular, complex exponentials are the\neigenfunctions of convolution. These eigenfunctions are defined globally;\nhowever, convolutional architectures operate locally. To enforce locality, one\ncan apply a windowing function to the eigenfunctions, which leads to oriented\nbandpass filters as the natural operators to be learned with convolutional\narchitectures. From a representational point of view, these filters allow for a\nlocal systematic way to characterize and operate on an image or other signal.\nWe offer empirical support for the hypothesis that convolutional networks learn\nsuch filters at all of their convolutional layers. While previous research has\nshown evidence of filters having oriented bandpass characteristics at early\nlayers, ours appears to be the first study to document the predominance of such\nfilter characteristics at all layers. Previous studies have missed this\nobservation because they have concentrated on the cumulative compositional\neffects of filtering across layers, while we examine the filter characteristics\nthat are present at each layer.\n", "versions": [{"version": "v1", "created": "Mon, 30 Nov 2020 10:10:44 GMT"}], "update_date": "2020-12-01", "authors_parsed": [["Hadji", "Isma", ""], ["Wildes", "Richard P.", ""]]}, {"id": "2011.14667", "submitter": "Longyao Liu", "authors": "Longyao Liu, Bo Ma, Yulin Zhang, Xin Yi, Haozhi Li", "title": "AFD-Net: Adaptive Fully-Dual Network for Few-Shot Object Detection", "comments": "9 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Few-shot object detection (FSOD) aims at learning a detector that can fast\nadapt to previously unseen objects with scarce annotated examples, which is\nchallenging and demanding. Existing methods solve this problem by performing\nsubtasks of classification and localization utilizing a shared component (e.g.,\nRoI head) in the detector, yet few of them take the distinct preferences of two\nsubtasks towards feature embedding into consideration. In this paper, we\ncarefully analyze the characteristics of FSOD, and present that a general\nfew-shot detector should consider the explicit decomposition of two subtasks,\nas well as leveraging information from both of them to enhance feature\nrepresentations. To the end, we propose a simple yet effective Adaptive\nFully-Dual Network (AFD-Net). Specifically, we extend Faster R-CNN by\nintroducing Dual Query Encoder and Dual Attention Generator for separate\nfeature extraction, and Dual Aggregator for separate model reweighting.\nSpontaneously, separate state estimation is achieved by the R-CNN detector.\nBesides, for the acquisition of enhanced feature representations, we further\nintroduce Adaptive Fusion Mechanism to adaptively perform feature fusion in\ndifferent subtasks. Extensive experiments on PASCAL VOC and MS COCO in various\nsettings show that, our method achieves new state-of-the-art performance by a\nlarge margin, demonstrating its effectiveness and generalization ability.\n", "versions": [{"version": "v1", "created": "Mon, 30 Nov 2020 10:21:32 GMT"}, {"version": "v2", "created": "Mon, 19 Apr 2021 07:29:07 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Liu", "Longyao", ""], ["Ma", "Bo", ""], ["Zhang", "Yulin", ""], ["Yi", "Xin", ""], ["Li", "Haozhi", ""]]}, {"id": "2011.14669", "submitter": "Yiming Wang", "authors": "Yiming Wang and Alessio Del Bue", "title": "Where to Explore Next? ExHistCNN for History-aware Autonomous 3D\n  Exploration", "comments": "published on European Conference on Computer Vision, 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we address the problem of autonomous 3D exploration of an\nunknown indoor environment using a depth camera. We cast the problem as the\nestimation of the Next Best View (NBV) that maximises the coverage of the\nunknown area. We do this by re-formulating NBV estimation as a classification\nproblem and we propose a novel learning-based metric that encodes both, the\ncurrent 3D observation (a depth frame) and the history of the ongoing\nreconstruction. One of the major contributions of this work is about\nintroducing a new representation for the 3D reconstruction history as an\nauxiliary utility map which is efficiently coupled with the current depth\nobservation. With both pieces of information, we train a light-weight CNN,\nnamed ExHistCNN, that estimates the NBV as a set of directions towards which\nthe depth sensor finds most unexplored areas. We perform extensive evaluation\non both synthetic and real room scans demonstrating that the proposed ExHistCNN\nis able to approach the exploration performance of an oracle using the complete\nknowledge of the 3D environment.\n", "versions": [{"version": "v1", "created": "Mon, 30 Nov 2020 10:29:29 GMT"}], "update_date": "2020-12-01", "authors_parsed": [["Wang", "Yiming", ""], ["Del Bue", "Alessio", ""]]}, {"id": "2011.14670", "submitter": "Seokeon Choi", "authors": "Seokeon Choi, Taekyung Kim, Minki Jeong, Hyoungseob Park, Changick Kim", "title": "Meta Batch-Instance Normalization for Generalizable Person\n  Re-Identification", "comments": "CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although supervised person re-identification (Re-ID) methods have shown\nimpressive performance, they suffer from a poor generalization capability on\nunseen domains. Therefore, generalizable Re-ID has recently attracted growing\nattention. Many existing methods have employed an instance normalization\ntechnique to reduce style variations, but the loss of discriminative\ninformation could not be avoided. In this paper, we propose a novel\ngeneralizable Re-ID framework, named Meta Batch-Instance Normalization\n(MetaBIN). Our main idea is to generalize normalization layers by simulating\nunsuccessful generalization scenarios beforehand in the meta-learning pipeline.\nTo this end, we combine learnable batch-instance normalization layers with\nmeta-learning and investigate the challenging cases caused by both batch and\ninstance normalization layers. Moreover, we diversify the virtual simulations\nvia our meta-train loss accompanied by a cyclic inner-updating manner to boost\ngeneralization capability. After all, the MetaBIN framework prevents our model\nfrom overfitting to the given source styles and improves the generalization\ncapability to unseen domains without additional data augmentation or\ncomplicated network design. Extensive experimental results show that our model\noutperforms the state-of-the-art methods on the large-scale domain\ngeneralization Re-ID benchmark and the cross-domain Re-ID problem. The source\ncode is available at: https://github.com/bismex/MetaBIN.\n", "versions": [{"version": "v1", "created": "Mon, 30 Nov 2020 10:31:03 GMT"}, {"version": "v2", "created": "Mon, 29 Mar 2021 17:38:27 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Choi", "Seokeon", ""], ["Kim", "Taekyung", ""], ["Jeong", "Minki", ""], ["Park", "Hyoungseob", ""], ["Kim", "Changick", ""]]}, {"id": "2011.14672", "submitter": "Jiefeng Li", "authors": "Jiefeng Li, Chao Xu, Zhicun Chen, Siyuan Bian, Lixin Yang, Cewu Lu", "title": "HybrIK: A Hybrid Analytical-Neural Inverse Kinematics Solution for 3D\n  Human Pose and Shape Estimation", "comments": "To appear in CVPR2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Model-based 3D pose and shape estimation methods reconstruct a full 3D mesh\nfor the human body by estimating several parameters. However, learning the\nabstract parameters is a highly non-linear process and suffers from image-model\nmisalignment, leading to mediocre model performance. In contrast, 3D keypoint\nestimation methods combine deep CNN network with the volumetric representation\nto achieve pixel-level localization accuracy but may predict unrealistic body\nstructure. In this paper, we address the above issues by bridging the gap\nbetween body mesh estimation and 3D keypoint estimation. We propose a novel\nhybrid inverse kinematics solution (HybrIK). HybrIK directly transforms\naccurate 3D joints to relative body-part rotations for 3D body mesh\nreconstruction, via the twist-and-swing decomposition. The swing rotation is\nanalytically solved with 3D joints, and the twist rotation is derived from the\nvisual cues through the neural network. We show that HybrIK preserves both the\naccuracy of 3D pose and the realistic body structure of the parametric human\nmodel, leading to a pixel-aligned 3D body mesh and a more accurate 3D pose than\nthe pure 3D keypoint estimation methods. Without bells and whistles, the\nproposed method surpasses the state-of-the-art methods by a large margin on\nvarious 3D human pose and shape benchmarks. As an illustrative example, HybrIK\noutperforms all the previous methods by 13.2 mm MPJPE and 21.9 mm PVE on 3DPW\ndataset. Our code is available at https://github.com/Jeff-sjtu/HybrIK.\n", "versions": [{"version": "v1", "created": "Mon, 30 Nov 2020 10:32:30 GMT"}, {"version": "v2", "created": "Sat, 6 Mar 2021 03:05:24 GMT"}, {"version": "v3", "created": "Mon, 5 Apr 2021 13:57:49 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Li", "Jiefeng", ""], ["Xu", "Chao", ""], ["Chen", "Zhicun", ""], ["Bian", "Siyuan", ""], ["Yang", "Lixin", ""], ["Lu", "Cewu", ""]]}, {"id": "2011.14679", "submitter": "Bastian Wandt", "authors": "Bastian Wandt, Marco Rudolph, Petrissa Zell, Helge Rhodin, Bodo\n  Rosenhahn", "title": "CanonPose: Self-Supervised Monocular 3D Human Pose Estimation in the\n  Wild", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human pose estimation from single images is a challenging problem in computer\nvision that requires large amounts of labeled training data to be solved\naccurately. Unfortunately, for many human activities (\\eg outdoor sports) such\ntraining data does not exist and is hard or even impossible to acquire with\ntraditional motion capture systems. We propose a self-supervised approach that\nlearns a single image 3D pose estimator from unlabeled multi-view data. To this\nend, we exploit multi-view consistency constraints to disentangle the observed\n2D pose into the underlying 3D pose and camera rotation. In contrast to most\nexisting methods, we do not require calibrated cameras and can therefore learn\nfrom moving cameras. Nevertheless, in the case of a static camera setup, we\npresent an optional extension to include constant relative camera rotations\nover multiple views into our framework. Key to the success are new, unbiased\nreconstruction objectives that mix information across views and training\nsamples. The proposed approach is evaluated on two benchmark datasets\n(Human3.6M and MPII-INF-3DHP) and on the in-the-wild SkiPose dataset.\n", "versions": [{"version": "v1", "created": "Mon, 30 Nov 2020 10:42:27 GMT"}], "update_date": "2020-12-01", "authors_parsed": [["Wandt", "Bastian", ""], ["Rudolph", "Marco", ""], ["Zell", "Petrissa", ""], ["Rhodin", "Helge", ""], ["Rosenhahn", "Bodo", ""]]}, {"id": "2011.14695", "submitter": "Risheng Huang", "authors": "Risheng Huang, Li Shen, Xuan Wang, Cheng Lin, Hao-Zhi Huang", "title": "Adaptive Compact Attention For Few-shot Video-to-video Translation", "comments": "Video available: https://youtu.be/1OCFbUrypKQ", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes an adaptive compact attention model for few-shot\nvideo-to-video translation. Existing works in this domain only use features\nfrom pixel-wise attention without considering the correlations among multiple\nreference images, which leads to heavy computation but limited performance.\nTherefore, we introduce a novel adaptive compact attention mechanism to\nefficiently extract contextual features jointly from multiple reference images,\nof which encoded view-dependent and motion-dependent information can\nsignificantly benefit the synthesis of realistic videos. Our core idea is to\nextract compact basis sets from all the reference images as higher-level\nrepresentations. To further improve the reliability, in the inference phase, we\nalso propose a novel method based on the Delaunay Triangulation algorithm to\nautomatically select the resourceful references according to the input label.\nWe extensively evaluate our method on a large-scale talking-head video dataset\nand a human dancing dataset; the experimental results show the superior\nperformance of our method for producing photorealistic and temporally\nconsistent videos, and considerable improvements over the state-of-the-art\nmethod.\n", "versions": [{"version": "v1", "created": "Mon, 30 Nov 2020 11:19:12 GMT"}], "update_date": "2020-12-01", "authors_parsed": [["Huang", "Risheng", ""], ["Shen", "Li", ""], ["Wang", "Xuan", ""], ["Lin", "Cheng", ""], ["Huang", "Hao-Zhi", ""]]}, {"id": "2011.14696", "submitter": "Akshay L Chandra", "authors": "Akshay L Chandra, Sai Vikas Desai, Chaitanya Devaguptapu, Vineeth N\n  Balasubramanian", "title": "On Initial Pools for Deep Active Learning", "comments": "Accepted at NeurIPS 2020 Preregistration Workshop and included in\n  PMLR v148. 19 pages, 9 figures", "journal-ref": "Proceedings of Machine Learning Research. 148 (2021) 14-32", "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Active Learning (AL) techniques aim to minimize the training data required to\ntrain a model for a given task. Pool-based AL techniques start with a small\ninitial labeled pool and then iteratively pick batches of the most informative\nsamples for labeling. Generally, the initial pool is sampled randomly and\nlabeled to seed the AL iterations. While recent studies have focused on\nevaluating the robustness of various query functions in AL, little to no\nattention has been given to the design of the initial labeled pool for deep\nactive learning. Given the recent successes of learning representations in\nself-supervised/unsupervised ways, we study if an intelligently sampled initial\nlabeled pool can improve deep AL performance. We investigate the effect of\nintelligently sampled initial labeled pools, including the use of\nself-supervised and unsupervised strategies, on deep AL methods. The setup,\nhypotheses, methodology, and implementation details were evaluated by peer\nreview before experiments were conducted. Experimental results could not\nconclusively prove that intelligently sampled initial pools are better for AL\nthan random initial pools in the long run, although a Variational\nAutoencoder-based initial pool sampling strategy showed interesting trends that\nmerit deeper investigation.\n", "versions": [{"version": "v1", "created": "Mon, 30 Nov 2020 11:22:31 GMT"}, {"version": "v2", "created": "Wed, 14 Jul 2021 11:14:09 GMT"}], "update_date": "2021-07-15", "authors_parsed": [["Chandra", "Akshay L", ""], ["Desai", "Sai Vikas", ""], ["Devaguptapu", "Chaitanya", ""], ["Balasubramanian", "Vineeth N", ""]]}, {"id": "2011.14700", "submitter": "Dat Nguyen Thanh", "authors": "Dat Thanh Nguyen, Maurice Quach, Giuseppe Valenzise, Pierre Duhamel", "title": "Learning-based lossless compression of 3D point cloud geometry", "comments": "5 pages, accepted paper at ICASSP 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a learning-based, lossless compression method for static\npoint cloud geometry, based on context-adaptive arithmetic coding. Unlike most\nexisting methods working in the octree domain, our encoder operates in a hybrid\nmode, mixing octree and voxel-based coding. We adaptively partition the point\ncloud into multi-resolution voxel blocks according to the point cloud\nstructure, and use octree to signal the partitioning. On the one hand, octree\nrepresentation can eliminate the sparsity in the point cloud. On the other\nhand, in the voxel domain, convolutions can be naturally expressed, and\ngeometric information (i.e., planes, surfaces, etc.) is explicitly processed by\na neural network. Our context model benefits from these properties and learns a\nprobability distribution of the voxels using a deep convolutional neural\nnetwork with masked filters, called VoxelDNN. Experiments show that our method\noutperforms the state-of-the-art MPEG G-PCC standard with average rate savings\nof 28% on a diverse set of point clouds from the Microsoft Voxelized Upper\nBodies (MVUB) and MPEG. The implementation is available at\nhttps://github.com/Weafre/VoxelDNN.\n", "versions": [{"version": "v1", "created": "Mon, 30 Nov 2020 11:27:16 GMT"}, {"version": "v2", "created": "Tue, 20 Apr 2021 09:29:28 GMT"}], "update_date": "2021-04-21", "authors_parsed": [["Nguyen", "Dat Thanh", ""], ["Quach", "Maurice", ""], ["Valenzise", "Giuseppe", ""], ["Duhamel", "Pierre", ""]]}, {"id": "2011.14714", "submitter": "Chuang Yang", "authors": "Chuang Yang, Zhitong Xiong, Mulin Chen, Qi Wang, and Xuelong Li", "title": "BOTD: Bold Outline Text Detector", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, text detection has attracted sufficient attention in the field of\ncomputer vision and artificial intelligence. Among the existing approaches,\nregression-based models are limited to handle the texts with arbitrary shapes,\nwhile segmentation-based algorithms have high computational costs and suffer\nfrom the text adhesion problem. In this paper, we propose a new one-stage text\ndetector, termed as Bold Outline Text Detector (BOTD), which is able to process\nthe arbitrary-shaped text with low model complexity. Different from previous\nworks, BOTD utilizes the Polar Minimum Distance (PMD) to encode the shortest\ndistance between the center point and the contour of the text instance, and\ngenerates a Center Mask (CM) for each text instance. After learning the PMD\nheat map and CM map, the final results can be obtained with a simple Text\nReconstruction Module (TRM). Since the CM resides within the text box exactly,\nthe text adhesion problem is avoided naturally. Meanwhile, all the points on\nthe text contour share the same PMD, so the complexity of BOTD is much lower\nthan existing segmentation-based methods. Experimental results on three\nreal-world benchmarks show the state-of-the-art performance of BOTD.\n", "versions": [{"version": "v1", "created": "Mon, 30 Nov 2020 11:54:14 GMT"}, {"version": "v2", "created": "Tue, 1 Dec 2020 09:05:09 GMT"}, {"version": "v3", "created": "Sun, 20 Dec 2020 05:56:29 GMT"}, {"version": "v4", "created": "Thu, 18 Mar 2021 00:34:47 GMT"}, {"version": "v5", "created": "Tue, 11 May 2021 13:29:01 GMT"}, {"version": "v6", "created": "Fri, 21 May 2021 10:47:02 GMT"}], "update_date": "2021-05-24", "authors_parsed": [["Yang", "Chuang", ""], ["Xiong", "Zhitong", ""], ["Chen", "Mulin", ""], ["Wang", "Qi", ""], ["Li", "Xuelong", ""]]}, {"id": "2011.14723", "submitter": "Dvir Ginzburg", "authors": "Dvir Ginzburg and Dan Raviv", "title": "Dual Geometric Graph Network (DG2N) -- Iterative network for deformable\n  shape alignment", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide a novel new approach for aligning geometric models using a dual\ngraph structure where local features are mapping probabilities. Alignment of\nnon-rigid structures is one of the most challenging computer vision tasks due\nto the high number of unknowns needed to model the correspondence. We have seen\na leap forward using DNN models in template alignment and functional maps, but\nthose methods fail for inter-class alignment where nonisometric deformations\nexist. Here we propose to rethink this task and use unrolling concepts on a\ndual graph structure - one for a forward map and one for a backward map, where\nthe features are pulled back matching probabilities from the target into the\nsource. We report state of the art results on stretchable domains alignment in\na rapid and stable solution for meshes and cloud of points.\n", "versions": [{"version": "v1", "created": "Mon, 30 Nov 2020 12:03:28 GMT"}, {"version": "v2", "created": "Sat, 27 Mar 2021 06:23:32 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Ginzburg", "Dvir", ""], ["Raviv", "Dan", ""]]}, {"id": "2011.14733", "submitter": "Farzan Shenavarmasouleh", "authors": "Farzan Shenavarmasouleh, Farid Ghareh Mohammadi, M. Hadi Amini, Hamid\n  R. Arabnia", "title": "DRDr II: Detecting the Severity Level of Diabetic Retinopathy Using Mask\n  RCNN and Transfer Learning", "comments": "The 2020 International Conference on Computational Science and\n  Computational Intelligence (CSCI'2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  DRDr II is a hybrid of machine learning and deep learning worlds. It builds\non the successes of its antecedent, namely, DRDr, that was trained to detect,\nlocate, and create segmentation masks for two types of lesions (exudates and\nmicroaneurysms) that can be found in the eyes of the Diabetic Retinopathy (DR)\npatients; and uses the entire model as a solid feature extractor in the core of\nits pipeline to detect the severity level of the DR cases. We employ a big\ndataset with over 35 thousand fundus images collected from around the globe and\nafter 2 phases of preprocessing alongside feature extraction, we succeed in\npredicting the correct severity levels with over 92% accuracy.\n", "versions": [{"version": "v1", "created": "Mon, 30 Nov 2020 12:23:22 GMT"}], "update_date": "2020-12-01", "authors_parsed": [["Shenavarmasouleh", "Farzan", ""], ["Mohammadi", "Farid Ghareh", ""], ["Amini", "M. Hadi", ""], ["Arabnia", "Hamid R.", ""]]}, {"id": "2011.14744", "submitter": "Yinyu Nie", "authors": "Yinyu Nie, Ji Hou, Xiaoguang Han, Matthias Nie{\\ss}ner", "title": "RfD-Net: Point Scene Understanding by Semantic Instance Reconstruction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semantic scene understanding from point clouds is particularly challenging as\nthe points reflect only a sparse set of the underlying 3D geometry. Previous\nworks often convert point cloud into regular grids (e.g. voxels or bird-eye\nview images), and resort to grid-based convolutions for scene understanding. In\nthis work, we introduce RfD-Net that jointly detects and reconstructs dense\nobject surfaces directly from raw point clouds. Instead of representing scenes\nwith regular grids, our method leverages the sparsity of point cloud data and\nfocuses on predicting shapes that are recognized with high objectness. With\nthis design, we decouple the instance reconstruction into global object\nlocalization and local shape prediction. It not only eases the difficulty of\nlearning 2-D manifold surfaces from sparse 3D space, the point clouds in each\nobject proposal convey shape details that support implicit function learning to\nreconstruct any high-resolution surfaces. Our experiments indicate that\ninstance detection and reconstruction present complementary effects, where the\nshape prediction head shows consistent effects on improving object detection\nwith modern 3D proposal network backbones. The qualitative and quantitative\nevaluations further demonstrate that our approach consistently outperforms the\nstate-of-the-arts and improves over 11 of mesh IoU in object reconstruction.\n", "versions": [{"version": "v1", "created": "Mon, 30 Nov 2020 12:58:05 GMT"}], "update_date": "2020-12-01", "authors_parsed": [["Nie", "Yinyu", ""], ["Hou", "Ji", ""], ["Han", "Xiaoguang", ""], ["Nie\u00dfner", "Matthias", ""]]}, {"id": "2011.14752", "submitter": "Thoudam Doren Singh", "authors": "Alok Singh, Thoudam Doren Singh, Sivaji Bandyopadhyay", "title": "A Comprehensive Review on Recent Methods and Challenges of Video\n  Description", "comments": "Paper of 35 pages submitted to ACM Computing Surveys", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video description involves the generation of the natural language description\nof actions, events, and objects in the video. There are various applications of\nvideo description by filling the gap between languages and vision for visually\nimpaired people, generating automatic title suggestion based on content,\nbrowsing of the video based on the content and video-guided machine translation\n[86] etc.In the past decade, several works had been done in this field in terms\nof approaches/methods for video description, evaluation metrics,and datasets.\nFor analyzing the progress in the video description task, a comprehensive\nsurvey is needed that covers all the phases of video description approaches\nwith a special focus on recent deep learning approaches. In this work, we\nreport a comprehensive survey on the phases of video description approaches,\nthe dataset for video description, evaluation metrics, open competitions for\nmotivating the research on the video description, open challenges in this\nfield, and future research directions. In this survey, we cover the\nstate-of-the-art approaches proposed for each and every dataset with their pros\nand cons. For the growth of this research domain,the availability of numerous\nbenchmark dataset is a basic need. Further, we categorize all the dataset into\ntwo classes: open domain dataset and domain-specific dataset. From our survey,\nwe observe that the work in this field is in fast-paced development since the\ntask of video description falls in the intersection of computer vision and\nnatural language processing. But still, the work in the video description is\nfar from saturation stage due to various challenges like the redundancy due to\nsimilar frames which affect the quality of visual features, the availability of\ndataset containing more diverse content and availability of an effective\nevaluation metric.\n", "versions": [{"version": "v1", "created": "Mon, 30 Nov 2020 13:08:45 GMT"}], "update_date": "2020-12-01", "authors_parsed": [["Singh", "Alok", ""], ["Singh", "Thoudam Doren", ""], ["Bandyopadhyay", "Sivaji", ""]]}, {"id": "2011.14759", "submitter": "Tony Lindeberg", "authors": "Tony Lindeberg", "title": "Scale-covariant and scale-invariant Gaussian derivative networks", "comments": "27 pages, 10 figures", "journal-ref": "SSVM 2021: Scale Space and Variational Methods in Computer Vision,\n  Springer LNCS 12679: 3-14, 2021", "doi": "10.1007/978-3-030-75549-2_1", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a hybrid approach between scale-space theory and deep\nlearning, where a deep learning architecture is constructed by coupling\nparameterized scale-space operations in cascade. By sharing the learnt\nparameters between multiple scale channels, and by using the transformation\nproperties of the scale-space primitives under scaling transformations, the\nresulting network becomes provably scale covariant. By in addition performing\nmax pooling over the multiple scale channels, a resulting network architecture\nfor image classification also becomes provably scale invariant. We investigate\nthe performance of such networks on the MNISTLargeScale dataset, which contains\nrescaled images from original MNIST over a factor of 4 concerning training data\nand over a factor of 16 concerning testing data. It is demonstrated that the\nresulting approach allows for scale generalization, enabling good performance\nfor classifying patterns at scales not present in the training data.\n", "versions": [{"version": "v1", "created": "Mon, 30 Nov 2020 13:15:10 GMT"}, {"version": "v2", "created": "Thu, 17 Dec 2020 14:33:17 GMT"}, {"version": "v3", "created": "Fri, 18 Dec 2020 10:11:23 GMT"}, {"version": "v4", "created": "Thu, 31 Dec 2020 15:13:17 GMT"}, {"version": "v5", "created": "Tue, 26 Jan 2021 17:24:45 GMT"}, {"version": "v6", "created": "Fri, 5 Feb 2021 14:37:32 GMT"}, {"version": "v7", "created": "Thu, 11 Feb 2021 12:32:20 GMT"}, {"version": "v8", "created": "Thu, 8 Apr 2021 09:26:33 GMT"}], "update_date": "2021-04-30", "authors_parsed": [["Lindeberg", "Tony", ""]]}, {"id": "2011.14761", "submitter": "Aleksandr Safin", "authors": "Oleg Voynov, Aleksandr Safin, Savva Ignatyev and Evgeny Burnaev", "title": "How Good MVSNets Are at Depth Fusion", "comments": "7 pages, 6 figures, 1 table. Accepted to ICMV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the effects of the additional input to deep multi-view stereo\nmethods in the form of low-quality sensor depth. We modify two state-of-the-art\ndeep multi-view stereo methods for using with the input depth. We show that the\nadditional input depth may improve the quality of deep multi-view stereo.\n", "versions": [{"version": "v1", "created": "Mon, 30 Nov 2020 13:15:51 GMT"}], "update_date": "2020-12-01", "authors_parsed": [["Voynov", "Oleg", ""], ["Safin", "Aleksandr", ""], ["Ignatyev", "Savva", ""], ["Burnaev", "Evgeny", ""]]}, {"id": "2011.14773", "submitter": "Gregorio Bernabe G.", "authors": "Jes\\'us M. Rodr\\'iguez-de-Vera and Josefa Gonz\\'alez-Carrillo and\n  Jos\\'e M. Garc\\'ia and Gregorio Bernab\\'e", "title": "Deep learning approach to left ventricular non-compaction measurement", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Left ventricular non-compaction (LVNC) is a rare cardiomyopathy characterized\nby abnormal trabeculations in the left ventricle cavity. Although traditional\ncomputer vision approaches exist for LVNC diagnosis, deep learning-based tools\ncould not be found in the literature. In this paper, a first approach using\nconvolutional neural networks (CNNs) is presented. Four CNNs are trained to\nautomatically segment the compacted and trabecular areas of the left ventricle\nfor a population of patients diagnosed with Hypertrophic cardiomyopathy.\nInference results confirm that deep learning-based approaches can achieve\nexcellent results in the diagnosis and measurement of LVNC. The two best CNNs\n(U-Net and Efficient U-Net B1) perform image segmentation in less than 0.2 s on\na CPU and in less than 0.01 s on a GPU. Additionally, a subjective evaluation\nof the output images with the identified zones is performed by expert\ncardiologists, with a perfect visual agreement for all the slices,\noutperforming already existing automatic tools.\n", "versions": [{"version": "v1", "created": "Mon, 30 Nov 2020 13:32:44 GMT"}], "update_date": "2020-12-01", "authors_parsed": [["Rodr\u00edguez-de-Vera", "Jes\u00fas M.", ""], ["Gonz\u00e1lez-Carrillo", "Josefa", ""], ["Garc\u00eda", "Jos\u00e9 M.", ""], ["Bernab\u00e9", "Gregorio", ""]]}, {"id": "2011.14785", "submitter": "Yan Yang", "authors": "Yan Yang and Md Zakir Hossain and Tom Gedeon and Shafin Rahman", "title": "S2FGAN: Semantically Aware Interactive Sketch-to-Face Translation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Interactive facial image manipulation attempts to edit single and multiple\nface attributes using a photo-realistic face and/or semantic mask as input. In\nthe absence of the photo-realistic image (only sketch/mask available), previous\nmethods only retrieve the original face but ignore the potential of aiding\nmodel controllability and diversity in the translation process. This paper\nproposes a sketch-to-image generation framework called S2FGAN, aiming to\nimprove users' ability to interpret and flexibility of face attribute editing\nfrom a simple sketch. The proposed framework modifies the constrained latent\nspace semantics trained on Generative Adversarial Networks (GANs). We employ\ntwo latent spaces to control the face appearance and adjust the desired\nattributes of the generated face. Instead of constraining the translation\nprocess by using a reference image, the users can command the model to retouch\nthe generated images by involving the semantic information in the generation\nprocess. In this way, our method can manipulate single or multiple face\nattributes by only specifying attributes to be changed. Extensive experimental\nresults on CelebAMask-HQ dataset empirically shows our superior performance and\neffectiveness on this task. Our method successfully outperforms\nstate-of-the-art methods on attribute manipulation by exploiting greater\ncontrol of attribute intensity.\n", "versions": [{"version": "v1", "created": "Mon, 30 Nov 2020 13:42:39 GMT"}, {"version": "v2", "created": "Mon, 5 Jul 2021 23:34:09 GMT"}], "update_date": "2021-07-07", "authors_parsed": [["Yang", "Yan", ""], ["Hossain", "Md Zakir", ""], ["Gedeon", "Tom", ""], ["Rahman", "Shafin", ""]]}, {"id": "2011.14787", "submitter": "Michal P\\'andy", "authors": "Michal P\\'andy, Daniel Lenton, Ronald Clark", "title": "Unsupervised Path Regression Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We demonstrate that challenging shortest path problems can be solved via\ndirect spline regression from a neural network, trained in an unsupervised\nmanner (i.e. without requiring ground truth optimal paths for training). To\nachieve this, we derive a geometry-dependent optimal cost function whose minima\nguarantees collision-free solutions. Our method beats state-of-the-art\nsupervised learning baselines for shortest path planning, with a much more\nscalable training pipeline, and a significant speedup in inference time.\n", "versions": [{"version": "v1", "created": "Mon, 30 Nov 2020 13:45:55 GMT"}, {"version": "v2", "created": "Tue, 9 Mar 2021 11:38:55 GMT"}], "update_date": "2021-03-10", "authors_parsed": [["P\u00e1ndy", "Michal", ""], ["Lenton", "Daniel", ""], ["Clark", "Ronald", ""]]}, {"id": "2011.14791", "submitter": "Silvan Weder", "authors": "Silvan Weder, Johannes L. Sch\\\"onberger, Marc Pollefeys, Martin R.\n  Oswald", "title": "NeuralFusion: Online Depth Fusion in Latent Space", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  We present a novel online depth map fusion approach that learns depth map\naggregation in a latent feature space. While previous fusion methods use an\nexplicit scene representation like signed distance functions (SDFs), we propose\na learned feature representation for the fusion. The key idea is a separation\nbetween the scene representation used for the fusion and the output scene\nrepresentation, via an additional translator network. Our neural network\narchitecture consists of two main parts: a depth and feature fusion\nsub-network, which is followed by a translator sub-network to produce the final\nsurface representation (e.g. TSDF) for visualization or other tasks. Our\napproach is an online process, handles high noise levels, and is particularly\nable to deal with gross outliers common for photometric stereo-based depth\nmaps. Experiments on real and synthetic data demonstrate improved results\ncompared to the state of the art, especially in challenging scenarios with\nlarge amounts of noise and outliers.\n", "versions": [{"version": "v1", "created": "Mon, 30 Nov 2020 13:50:59 GMT"}, {"version": "v2", "created": "Tue, 8 Jun 2021 12:28:27 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Weder", "Silvan", ""], ["Sch\u00f6nberger", "Johannes L.", ""], ["Pollefeys", "Marc", ""], ["Oswald", "Martin R.", ""]]}, {"id": "2011.14814", "submitter": "Gal Lifshitz", "authors": "Gal Lifshitz and Dan Raviv", "title": "Unsupervised Optical Flow Using Cost Function Unrolling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Analyzing motion between two consecutive images is one of the fundamental\ntasks in computer vision. In the lack of labeled data, the loss functions are\nsplit into consistency and smoothness, allowing for self-supervised training.\nThis paper focuses on the cost function derivation and presents an unrolling\niterative approach, transferring the hard L1 smoothness constraint into a\nsofter multi-layer iterative scheme. More accurate gradients, especially near\nnon-differential positions, improve the network's convergence, providing\nsuperior results on tested scenarios. We report state-of-the-art results on\nboth MPI Sintel and KITTI 2015 unsupervised optical flow benchmarks. The\nprovided approach can be used to enhance various architectures and not limited\njust to the presented pipeline.\n", "versions": [{"version": "v1", "created": "Mon, 30 Nov 2020 14:10:03 GMT"}], "update_date": "2020-12-01", "authors_parsed": [["Lifshitz", "Gal", ""], ["Raviv", "Dan", ""]]}, {"id": "2011.14842", "submitter": "Wail Mustafa", "authors": "Wail Mustafa, Christian Kehl, Ulrik Lund Olsen, S{\\o}ren Kimmer Schou\n  Gregersen, David Malmgren-Hansen, Jan Kehres, Anders Bjorholm Dahl", "title": "Sparse-View Spectral CT Reconstruction Using Deep Learning", "comments": "13 pages, 9 figures, submitted to The IEEE Transactions on\n  Computational Imaging", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spectral computed tomography (CT) is an emerging technology capable of\nproviding high chemical specificity, which is crucial for many applications\nsuch as detecting threats in luggage. This type of application requires both\nfast and high-quality image reconstruction and is often based on sparse-view\n(few) projections. The conventional filtered back projection (FBP) method is\nfast but it produces low-quality images dominated by noise and artifacts in\nsparse-view CT. Iterative methods with, e.g., total variation regularizers can\ncircumvent that but they are computationally expensive, as the computational\nload proportionally increases with the number of spectral channels. Instead, we\npropose an approach for fast reconstruction of sparse-view spectral CT data\nusing a U-Net convolutional neural network architecture with multi-channel\ninput and output. The network is trained to output high-quality CT images from\nFBP input image reconstructions. Our method is fast at run-time and because the\ninternal convolutions are shared between the channels, the computational load\nincreases only at the first and last layers, making it an efficient approach to\nprocess spectral data with a large number of channels. We have validated our\napproach using real CT scans. Our results show qualitatively and quantitatively\nthat our approach outperforms the state-of-the-art iterative methods.\nFurthermore, the results indicate that the network can exploit the coupling of\nthe channels to enhance the overall quality and robustness.\n", "versions": [{"version": "v1", "created": "Mon, 30 Nov 2020 14:36:23 GMT"}, {"version": "v2", "created": "Fri, 26 Mar 2021 23:49:03 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Mustafa", "Wail", ""], ["Kehl", "Christian", ""], ["Olsen", "Ulrik Lund", ""], ["Gregersen", "S\u00f8ren Kimmer Schou", ""], ["Malmgren-Hansen", "David", ""], ["Kehres", "Jan", ""], ["Dahl", "Anders Bjorholm", ""]]}, {"id": "2011.14858", "submitter": "Aditya Jyoti Paul", "authors": "Puranjay Mohan, Aditya Jyoti Paul, Abhay Chirania", "title": "A Tiny CNN Architecture for Medical Face Mask Detection for\n  Resource-Constrained Endpoints", "comments": "11 pages, Published in Springer LNEE at\n  http://link.springer.com/chapter/10.1007%2F978-981-16-0749-3_52", "journal-ref": "Innovations in Electrical and Electronic Engineering. Lecture\n  Notes in Electrical Engineering, vol 756, pp 657-670, Springer, Singapore,\n  2021", "doi": "10.1007/978-981-16-0749-3_52", "report-no": null, "categories": "cs.CV cs.CY cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The world is going through one of the most dangerous pandemics of all time\nwith the rapid spread of the novel coronavirus (COVID-19). According to the\nWorld Health Organisation, the most effective way to thwart the transmission of\ncoronavirus is to wear medical face masks. Monitoring the use of face masks in\npublic places has been a challenge because manual monitoring could be unsafe.\nThis paper proposes an architecture for detecting medical face masks for\ndeployment on resource-constrained endpoints having extremely low memory\nfootprints. A small development board with an ARM Cortex-M7 microcontroller\nclocked at 480 Mhz and having just 496 KB of framebuffer RAM, has been used for\nthe deployment of the model. Using the TensorFlow Lite framework, the model is\nquantized to further reduce its size. The proposed model is 138 KB post\nquantization and runs at the inference speed of 30 FPS.\n", "versions": [{"version": "v1", "created": "Mon, 30 Nov 2020 14:56:23 GMT"}, {"version": "v2", "created": "Tue, 22 Dec 2020 18:52:33 GMT"}, {"version": "v3", "created": "Thu, 3 Jun 2021 12:55:21 GMT"}], "update_date": "2021-06-04", "authors_parsed": [["Mohan", "Puranjay", ""], ["Paul", "Aditya Jyoti", ""], ["Chirania", "Abhay", ""]]}, {"id": "2011.14859", "submitter": "Derek Lim", "authors": "Derek Lim, Ren\\'e Vidal, Benjamin D. Haeffele", "title": "Doubly Stochastic Subspace Clustering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many state-of-the-art subspace clustering methods follow a two-step process\nby first constructing an affinity matrix between data points and then applying\nspectral clustering to this affinity. Most of the research into these methods\nfocuses on the first step of generating the affinity, which often exploits the\nself-expressive property of linear subspaces, with little consideration\ntypically given to the spectral clustering step that produces the final\nclustering. Moreover, existing methods often obtain the final affinity that is\nused in the spectral clustering step by applying ad-hoc or arbitrarily chosen\npostprocessing steps to the affinity generated by a self-expressive clustering\nformulation, which can have a significant impact on the overall clustering\nperformance. In this work, we unify these two steps by learning both a\nself-expressive representation of the data and an affinity matrix that is\nwell-normalized for spectral clustering. In our proposed models, we constrain\nthe affinity matrix to be doubly stochastic, which results in a principled\nmethod for affinity matrix normalization while also exploiting known benefits\nof doubly stochastic normalization in spectral clustering. We develop a general\nframework and derive two models: one that jointly learns the self-expressive\nrepresentation along with the doubly stochastic affinity, and one that\nsequentially solves for one then the other. Furthermore, we leverage sparsity\nin the problem to develop a fast active-set method for the sequential solver\nthat enables efficient computation on large datasets. Experiments show that our\nmethod achieves state-of-the-art subspace clustering performance on many common\ndatasets in computer vision.\n", "versions": [{"version": "v1", "created": "Mon, 30 Nov 2020 14:56:54 GMT"}, {"version": "v2", "created": "Mon, 19 Apr 2021 23:50:41 GMT"}], "update_date": "2021-04-21", "authors_parsed": [["Lim", "Derek", ""], ["Vidal", "Ren\u00e9", ""], ["Haeffele", "Benjamin D.", ""]]}, {"id": "2011.14870", "submitter": "Luis Felipe M\\\"uller de Oliveira Henriques", "authors": "Luis Felipe M.O. Henriques, Eduardo Morgan, Sergio Colcher, Ruy Luiz\n  Milidi\\'u", "title": "Prior Flow Variational Autoencoder: A density estimation model for\n  Non-Intrusive Load Monitoring", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Non-Intrusive Load Monitoring (NILM) is a computational technique to estimate\nthe power loads' appliance-by-appliance from the whole consumption measured by\na single meter. In this paper, we propose a conditional density estimation\nmodel, based on deep neural networks, that joins a Conditional Variational\nAutoencoder with a Conditional Invertible Normalizing Flow model to estimate\nthe individual appliance's power demand. The resulting model is called Prior\nFlow Variational Autoencoder or, for simplicity PFVAE. Thus, instead of having\none model per appliance, the resulting model is responsible for estimating the\npower demand, appliance-by-appliance, at once. We train and evaluate our\nproposed model in a publicly available dataset composed of power demand\nmeasures from a poultry feed factory located in Brazil. The proposed model's\nquality is evaluated by comparing the obtained normalized disaggregation error\n(NDE) and signal aggregated error (SAE) with the previous work values on the\nsame dataset. Our proposal achieves highly competitive results, and for six of\nthe eight machines belonging to the dataset, we observe consistent improvements\nthat go from 28% up to 81% in NDE and from 27% up to 86% in SAE.\n", "versions": [{"version": "v1", "created": "Mon, 30 Nov 2020 15:05:59 GMT"}, {"version": "v2", "created": "Sun, 27 Jun 2021 22:51:43 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Henriques", "Luis Felipe M. O.", ""], ["Morgan", "Eduardo", ""], ["Colcher", "Sergio", ""], ["Milidi\u00fa", "Ruy Luiz", ""]]}, {"id": "2011.14871", "submitter": "Sahithya Ravi", "authors": "Sahithya Ravi, Samaneh Khoshrou, Mykola Pechenizkiy", "title": "ViDi: Descriptive Visual Data Clustering as Radiologist Assistant in\n  COVID-19 Streamline Diagnostic", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the light of the COVID-19 pandemic, deep learning methods have been widely\ninvestigated in detecting COVID-19 from chest X-rays. However, a more pragmatic\napproach to applying AI methods to a medical diagnosis is designing a framework\nthat facilitates human-machine interaction and expert decision making. Studies\nhave shown that categorization can play an essential rule in accelerating\nreal-world decision making. Inspired by descriptive document clustering, we\npropose a domain-independent explanatory clustering framework to group\ncontextually related instances and support radiologists' decision making. While\nmost descriptive clustering approaches employ domain-specific characteristics\nto form meaningful clusters, we focus on model-level explanation as a more\ngeneral-purpose element of every learning process to achieve cluster\nhomogeneity. We employ DeepSHAP to generate homogeneous clusters in terms of\ndisease severity and describe the clusters using favorable and unfavorable\nsaliency maps, which visualize the class discriminating regions of an image.\nThese human-interpretable maps complement radiologist knowledge to investigate\nthe whole cluster at once. Besides, as part of this study, we evaluate a model\nbased on VGG-19, which can identify COVID and pneumonia cases with a positive\npredictive value of 95% and 97%, respectively, comparable to the recent\nexplainable approaches for COVID diagnosis.\n", "versions": [{"version": "v1", "created": "Mon, 30 Nov 2020 15:06:08 GMT"}], "update_date": "2020-12-01", "authors_parsed": [["Ravi", "Sahithya", ""], ["Khoshrou", "Samaneh", ""], ["Pechenizkiy", "Mykola", ""]]}, {"id": "2011.14873", "submitter": "Ti Bai", "authors": "Ti Bai, Biling Wang, Dan Nguyen, Bao Wang, Bin Dong, Wenxiang Cong,\n  Mannudeep K. Kalra, and Steve Jiang", "title": "Deep Interactive Denoiser (DID) for X-Ray Computed Tomography", "comments": "under review", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Low dose computed tomography (LDCT) is desirable for both diagnostic imaging\nand image guided interventions. Denoisers are openly used to improve the\nquality of LDCT. Deep learning (DL)-based denoisers have shown state-of-the-art\nperformance and are becoming one of the mainstream methods. However, there\nexists two challenges regarding the DL-based denoisers: 1) a trained model\ntypically does not generate different image candidates with different\nnoise-resolution tradeoffs which sometimes are needed for different clinical\ntasks; 2) the model generalizability might be an issue when the noise level in\nthe testing images is different from that in the training dataset. To address\nthese two challenges, in this work, we introduce a lightweight optimization\nprocess at the testing phase on top of any existing DL-based denoisers to\ngenerate multiple image candidates with different noise-resolution tradeoffs\nsuitable for different clinical tasks in real-time. Consequently, our method\nallows the users to interact with the denoiser to efficiently review various\nimage candidates and quickly pick up the desired one, and thereby was termed as\ndeep interactive denoiser (DID). Experimental results demonstrated that DID can\ndeliver multiple image candidates with different noise-resolution tradeoffs,\nand shows great generalizability regarding various network architectures, as\nwell as training and testing datasets with various noise levels.\n", "versions": [{"version": "v1", "created": "Mon, 30 Nov 2020 15:08:32 GMT"}, {"version": "v2", "created": "Sun, 6 Dec 2020 17:53:20 GMT"}], "update_date": "2020-12-08", "authors_parsed": [["Bai", "Ti", ""], ["Wang", "Biling", ""], ["Nguyen", "Dan", ""], ["Wang", "Bao", ""], ["Dong", "Bin", ""], ["Cong", "Wenxiang", ""], ["Kalra", "Mannudeep K.", ""], ["Jiang", "Steve", ""]]}, {"id": "2011.14880", "submitter": "Bojun Ouyang", "authors": "Bojun Ouyang, Dan Raviv", "title": "Occlusion Guided Scene Flow Estimation on 3D Point Clouds", "comments": "Aaccepted at CVPR 2021 Workshop on Autonomous Driving", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  3D scene flow estimation is a vital tool in perceiving our environment given\ndepth or range sensors. Unlike optical flow, the data is usually sparse and in\nmost cases partially occluded in between two temporal samplings. Here we\npropose a new scene flow architecture called OGSF-Net which tightly couples the\nlearning for both flow and occlusions between frames. Their coupled symbiosis\nresults in a more accurate prediction of flow in space. Unlike a traditional\nmulti-action network, our unified approach is fused throughout the network,\nboosting performances for both occlusion detection and flow estimation. Our\narchitecture is the first to gauge the occlusion in 3D scene flow estimation on\npoint clouds. In key datasets such as Flyingthings3D and KITTI, we achieve the\nstate-of-the-art results.\n", "versions": [{"version": "v1", "created": "Mon, 30 Nov 2020 15:22:03 GMT"}, {"version": "v2", "created": "Mon, 19 Apr 2021 16:49:36 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Ouyang", "Bojun", ""], ["Raviv", "Dan", ""]]}, {"id": "2011.14894", "submitter": "Juan E Arco", "authors": "Juan E. Arco, A. Ortiz, J.Ramirez, F.J. Martinez-Murcia, Yu-Dong\n  Zhang, Juan M. Gorriz", "title": "Uncertainty-driven ensembles of deep architectures for multiclass\n  classification. Application to COVID-19 diagnosis in chest X-ray images", "comments": "1 Table, 7 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV stat.ML", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Respiratory diseases kill million of people each year. Diagnosis of these\npathologies is a manual, time-consuming process that has inter and\nintra-observer variability, delaying diagnosis and treatment. The recent\nCOVID-19 pandemic has demonstrated the need of developing systems to automatize\nthe diagnosis of pneumonia, whilst Convolutional Neural Network (CNNs) have\nproved to be an excellent option for the automatic classification of medical\nimages. However, given the need of providing a confidence classification in\nthis context it is crucial to quantify the reliability of the model's\npredictions. In this work, we propose a multi-level ensemble classification\nsystem based on a Bayesian Deep Learning approach in order to maximize\nperformance while quantifying the uncertainty of each classification decision.\nThis tool combines the information extracted from different architectures by\nweighting their results according to the uncertainty of their predictions.\nPerformance of the Bayesian network is evaluated in a real scenario where\nsimultaneously differentiating between four different pathologies: control vs\nbacterial pneumonia vs viral pneumonia vs COVID-19 pneumonia. A three-level\ndecision tree is employed to divide the 4-class classification into three\nbinary classifications, yielding an accuracy of 98.06% and overcoming the\nresults obtained by recent literature. The reduced preprocessing needed for\nobtaining this high performance, in addition to the information provided about\nthe reliability of the predictions evidence the applicability of the system to\nbe used as an aid for clinicians.\n", "versions": [{"version": "v1", "created": "Fri, 27 Nov 2020 14:06:25 GMT"}], "update_date": "2020-12-01", "authors_parsed": [["Arco", "Juan E.", ""], ["Ortiz", "A.", ""], ["Ramirez", "J.", ""], ["Martinez-Murcia", "F. J.", ""], ["Zhang", "Yu-Dong", ""], ["Gorriz", "Juan M.", ""]]}, {"id": "2011.14901", "submitter": "Annika Lindh", "authors": "Annika Lindh, Robert J. Ross, John D. Kelleher", "title": "Language-Driven Region Pointer Advancement for Controllable Image\n  Captioning", "comments": "Accepted to COLING 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV cs.LG cs.NE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Controllable Image Captioning is a recent sub-field in the multi-modal task\nof Image Captioning wherein constraints are placed on which regions in an image\nshould be described in the generated natural language caption. This puts a\nstronger focus on producing more detailed descriptions, and opens the door for\nmore end-user control over results. A vital component of the Controllable Image\nCaptioning architecture is the mechanism that decides the timing of attending\nto each region through the advancement of a region pointer. In this paper, we\npropose a novel method for predicting the timing of region pointer advancement\nby treating the advancement step as a natural part of the language structure\nvia a NEXT-token, motivated by a strong correlation to the sentence structure\nin the training data. We find that our timing agrees with the ground-truth\ntiming in the Flickr30k Entities test data with a precision of 86.55% and a\nrecall of 97.92%. Our model implementing this technique improves the\nstate-of-the-art on standard captioning metrics while additionally\ndemonstrating a considerably larger effective vocabulary size.\n", "versions": [{"version": "v1", "created": "Mon, 30 Nov 2020 15:34:59 GMT"}], "update_date": "2020-12-01", "authors_parsed": [["Lindh", "Annika", ""], ["Ross", "Robert J.", ""], ["Kelleher", "John D.", ""]]}, {"id": "2011.14906", "submitter": "Julio Cezar Silveira Jacques Junior", "authors": "Julio C. S. Jacques Junior, Agata Lapedriza, Cristina Palmero, Xavier\n  Bar\\'o and Sergio Escalera", "title": "Person Perception Biases Exposed: Revisiting the First Impressions\n  Dataset", "comments": "accepted on 11th International Workshop on Human Behavior\n  Understanding (HBU), organized as part of WACV 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work revisits the ChaLearn First Impressions database, annotated for\npersonality perception using pairwise comparisons via crowdsourcing. We analyse\nfor the first time the original pairwise annotations, and reveal existing\nperson perception biases associated to perceived attributes like gender,\nethnicity, age and face attractiveness. We show how person perception bias can\ninfluence data labelling of a subjective task, which has received little\nattention from the computer vision and machine learning communities by now. We\nfurther show that the mechanism used to convert pairwise annotations to\ncontinuous values may magnify the biases if no special treatment is considered.\nThe findings of this study are relevant for the computer vision community that\nis still creating new datasets on subjective tasks, and using them for\npractical applications, ignoring these perceptual biases.\n", "versions": [{"version": "v1", "created": "Mon, 30 Nov 2020 15:41:27 GMT"}], "update_date": "2020-12-01", "authors_parsed": [["Junior", "Julio C. S. Jacques", ""], ["Lapedriza", "Agata", ""], ["Palmero", "Cristina", ""], ["Bar\u00f3", "Xavier", ""], ["Escalera", "Sergio", ""]]}, {"id": "2011.14908", "submitter": "Seyed Mohsen Hosseini", "authors": "Seyed Mohsen Hosseini", "title": "Image Denoising for Strong Gaussian Noises With Specialized CNNs for\n  Different Frequency Components", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In machine learning approach to image denoising a network is trained to\nrecover a clean image from a noisy one. In this paper a novel structure is\nproposed based on training multiple specialized networks as opposed to existing\nstructures that are base on a single network. The proposed model is an\nalternative for training a very deep network to avoid issues like vanishing or\nexploding gradient. By dividing a very deep network into two smaller networks\nthe same number of learnable parameters will be available, but two smaller\nnetworks should be trained which are easier to train. Over smoothing and waxy\nartifacts are major problems with existing methods; because the network tries\nto keep the Mean Square Error (MSE) low for general structures and details,\nwhich leads to overlooking of details. This problem is more severe in the\npresence of strong noise. To reduce this problem, in the proposed structure,\nthe image is decomposed into its low and high frequency components and each\ncomponent is used to train a separate denoising convolutional neural network.\nOne network is specialized to reconstruct the general structure of the image\nand the other one is specialized to reconstruct the details. Results of the\nproposed method show higher peak signal to noise ratio (PSNR), and structural\nsimilarity index (SSIM) compared to a popular state of the art denoising method\nin the presence of strong noises.\n", "versions": [{"version": "v1", "created": "Thu, 26 Nov 2020 23:20:25 GMT"}], "update_date": "2020-12-01", "authors_parsed": [["Hosseini", "Seyed Mohsen", ""]]}, {"id": "2011.14910", "submitter": "Jonathan Francis", "authors": "Manoj Bhat, Jonathan Francis, Jean Oh", "title": "Trajformer: Trajectory Prediction with Local Self-Attentive Contexts for\n  Autonomous Driving", "comments": "Accepted: Machine Learning for Autonomous Driving @ NeurIPS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Effective feature-extraction is critical to models' contextual understanding,\nparticularly for applications to robotics and autonomous driving, such as\nmultimodal trajectory prediction. However, state-of-the-art generative methods\nface limitations in representing the scene context, leading to predictions of\ninadmissible futures. We alleviate these limitations through the use of\nself-attention, which enables better control over representing the agent's\nsocial context; we propose a local feature-extraction pipeline that produces\nmore salient information downstream, with improved parameter efficiency. We\nshow improvements on standard metrics (minADE, minFDE, DAO, DAC) over various\nbaselines on the Argoverse dataset. We release our code at:\nhttps://github.com/Manojbhat09/Trajformer\n", "versions": [{"version": "v1", "created": "Mon, 30 Nov 2020 15:42:15 GMT"}], "update_date": "2020-12-01", "authors_parsed": [["Bhat", "Manoj", ""], ["Francis", "Jonathan", ""], ["Oh", "Jean", ""]]}, {"id": "2011.14922", "submitter": "Hanwen Miao", "authors": "Hanwen Miao, Shengan Zhang, Carol Flannagan", "title": "Driver Behavior Extraction from Videos in Naturalistic Driving Datasets\n  with 3D ConvNets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Naturalistic driving data (NDD) is an important source of information to\nunderstand crash causation and human factors and to further develop crash\navoidance countermeasures. Videos recorded while driving are often included in\nsuch datasets. While there is often a large amount of video data in NDD, only a\nsmall portion of them can be annotated by human coders and used for research,\nwhich underuses all video data. In this paper, we explored a computer vision\nmethod to automatically extract the information we need from videos. More\nspecifically, we developed a 3D ConvNet algorithm to automatically extract\ncell-phone-related behaviors from videos. The experiments show that our method\ncan extract chunks from videos, most of which (~79%) contain the automatically\nlabeled cell phone behaviors. In conjunction with human review of the extracted\nchunks, this approach can find cell-phone-related driver behaviors much more\nefficiently than simply viewing video.\n", "versions": [{"version": "v1", "created": "Mon, 30 Nov 2020 15:53:15 GMT"}], "update_date": "2020-12-01", "authors_parsed": [["Miao", "Hanwen", ""], ["Zhang", "Shengan", ""], ["Flannagan", "Carol", ""]]}, {"id": "2011.14936", "submitter": "Yi Wang", "authors": "Yi Wang, Zhen-Peng Bian, Yunhao Zhou, Lap-Pui Chau", "title": "Rethinking and Designing a High-performing Automatic License Plate\n  Recognition Approach", "comments": "13 pages. Accepted for Publication at IEEE Transactions on\n  Intelligent Transportation Systems", "journal-ref": null, "doi": "10.1109/TITS.2021.3087158", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a real-time and accurate automatic license plate\nrecognition (ALPR) approach. Our study illustrates the outstanding design of\nALPR with four insights: (1) the resampling-based cascaded framework is\nbeneficial to both speed and accuracy; (2) the highly efficient license plate\nrecognition should abundant additional character segmentation and recurrent\nneural network (RNN), but adopt a plain convolutional neural network (CNN); (3)\nin the case of CNN, taking advantage of vertex information on license plates\nimproves the recognition performance; and (4) the weight-sharing character\nclassifier addresses the lack of training images in small-scale datasets. Based\non these insights, we propose a novel ALPR approach, termed VSNet.\nSpecifically, VSNet includes two CNNs, i.e., VertexNet for license plate\ndetection and SCR-Net for license plate recognition, integrated in a\nresampling-based cascaded manner. In VertexNet, we propose an efficient\nintegration block to extract the spatial features of license plates. With\nvertex supervisory information, we propose a vertex-estimation branch in\nVertexNet such that license plates can be rectified as the input images of\nSCR-Net. In SCR-Net, we introduce a horizontal encoding technique for\nleft-to-right feature extraction and propose a weight-sharing classifier for\ncharacter recognition. Experimental results show that the proposed VSNet\noutperforms state-of-the-art methods by more than 50% relative improvement on\nerror rate, achieving > 99% recognition accuracy on CCPD and AOLP datasets with\n149 FPS inference speed. Moreover, our method illustrates an outstanding\ngeneralization capability when evaluated on the unseen PKUData and CLPD\ndatasets.\n", "versions": [{"version": "v1", "created": "Mon, 30 Nov 2020 16:03:57 GMT"}, {"version": "v2", "created": "Thu, 17 Jun 2021 07:05:00 GMT"}], "update_date": "2021-06-18", "authors_parsed": [["Wang", "Yi", ""], ["Bian", "Zhen-Peng", ""], ["Zhou", "Yunhao", ""], ["Chau", "Lap-Pui", ""]]}, {"id": "2011.14943", "submitter": "Kashif Ahmad", "authors": "Naina Said, Kashif Ahmad, Asma Gul, Nasir Ahmad, Ala Al-Fuqaha", "title": "Floods Detection in Twitter Text and Images", "comments": "3 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we present our methods for the MediaEval 2020 Flood Related\nMultimedia task, which aims to analyze and combine textual and visual content\nfrom social media for the detection of real-world flooding events. The task\nmainly focuses on identifying floods related tweets relevant to a specific\narea. We propose several schemes to address the challenge. For text-based flood\nevents detection, we use three different methods, relying on Bog of Words (BOW)\nand an Italian Version of Bert individually and in combination, achieving an\nF1-score of 0.77%, 0.68%, and 0.70% on the development set, respectively. For\nthe visual analysis, we rely on features extracted via multiple\nstate-of-the-art deep models pre-trained on ImageNet. The extracted features\nare then used to train multiple individual classifiers whose scores are then\ncombined in a late fusion manner achieving an F1-score of 0.75%. For our\nmandatory multi-modal run, we combine the classification scores obtained with\nthe best textual and visual schemes in a late fusion manner. Overall, better\nresults are obtained with the multimodal scheme achieving an F1-score of 0.80%\non the development set.\n", "versions": [{"version": "v1", "created": "Mon, 30 Nov 2020 16:08:19 GMT"}], "update_date": "2020-12-01", "authors_parsed": [["Said", "Naina", ""], ["Ahmad", "Kashif", ""], ["Gul", "Asma", ""], ["Ahmad", "Nasir", ""], ["Al-Fuqaha", "Ala", ""]]}, {"id": "2011.14944", "submitter": "Kashif Ahmad", "authors": "Firoj Alam, Zohaib Hassan, Kashif Ahmad, Asma Gul, Michael Reiglar,\n  Nicola Conci, Ala AL-Fuqaha", "title": "Flood Detection via Twitter Streams using Textual and Visual Features", "comments": "3 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The paper presents our proposed solutions for the MediaEval 2020\nFlood-Related Multimedia Task, which aims to analyze and detect flooding events\nin multimedia content shared over Twitter. In total, we proposed four different\nsolutions including a multi-modal solution combining textual and visual\ninformation for the mandatory run, and three single modal image and text-based\nsolutions as optional runs. In the multimodal method, we rely on a supervised\nmultimodal bitransformer model that combines textual and visual features in an\nearly fusion, achieving a micro F1-score of .859 on the development data set.\nFor the text-based flood events detection, we use a transformer network (i.e.,\npretrained Italian BERT model) achieving an F1-score of .853. For image-based\nsolutions, we employed multiple deep models, pre-trained on both, the ImageNet\nand places data sets, individually and combined in an early fusion achieving\nF1-scores of .816 and .805 on the development set, respectively.\n", "versions": [{"version": "v1", "created": "Mon, 30 Nov 2020 16:09:11 GMT"}], "update_date": "2020-12-01", "authors_parsed": [["Alam", "Firoj", ""], ["Hassan", "Zohaib", ""], ["Ahmad", "Kashif", ""], ["Gul", "Asma", ""], ["Reiglar", "Michael", ""], ["Conci", "Nicola", ""], ["AL-Fuqaha", "Ala", ""]]}, {"id": "2011.14956", "submitter": "Yongquan Yang", "authors": "Yongquan Yang, Yiming Yang, Jie Chen, Jiayi Zheng, Zhongxi Zheng", "title": "Handling Noisy Labels via One-Step Abductive Multi-Target Learning", "comments": "35 pages, 17 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning from noisy labels is an important concern because of the lack of\naccurate ground-truth labels in plenty of real-world scenarios. In practice,\nvarious approaches for this concern first make corrections corresponding to\npotentially noisy-labeled instances, and then update predictive model with\ninformation of the made corrections. However, in specific areas, such as\nmedical histopathology whole slide image analysis (MHWSIA), it is often\ndifficult or even impossible for experts to manually achieve the noisy-free\nground-truth labels which leads to labels with heavy noise. This situation\nraises two more difficult problems: 1) the methodology of approaches making\ncorrections corresponding to potentially noisy-labeled instances has\nlimitations due to the heavy noise existing in labels; and 2) the appropriate\nevaluation strategy for validation/testing is unclear because of the great\ndifficulty in collecting the noisy-free ground-truth labels. In this paper, we\nfocus on alleviating these two problems. For the problem 1), we present a\none-step abductive multi-target learning framework (OSAMTLF) that imposes a\none-step logical reasoning upon machine learning via a multi-target learning\nprocedure to abduct the predictions of the learning model to be subject to our\nprior knowledge. For the problem 2), we propose a logical assessment formula\n(LAF) that evaluates the logical rationality of the outputs of an approach by\nestimating the consistencies between the predictions of the learning model and\nthe logical facts narrated from the results of the one-step logical reasoning\nof OSAMTLF. Applying OSAMTLF and LAF to the Helicobacter pylori (H. pylori)\nsegmentation task in MHWSIA, we show that OSAMTLF is able to abduct the machine\nlearning model achieving logically more rational predictions, which is beyond\nthe capability of various state-of-the-art approaches for learning from noisy\nlabels.\n", "versions": [{"version": "v1", "created": "Wed, 25 Nov 2020 09:40:34 GMT"}], "update_date": "2020-12-01", "authors_parsed": [["Yang", "Yongquan", ""], ["Yang", "Yiming", ""], ["Chen", "Jie", ""], ["Zheng", "Jiayi", ""], ["Zheng", "Zhongxi", ""]]}, {"id": "2011.14959", "submitter": "Ti Bai", "authors": "Ti Bai, Biling Wang, Dan Nguyen, Steve Jiang", "title": "Deep Dose Plugin Towards Real-time Monte Carlo Dose Calculation Through\n  a Deep Learning based Denoising Algorithm", "comments": "under review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Monte Carlo (MC) simulation is considered the gold standard method for\nradiotherapy dose calculation. However, achieving high precision requires a\nlarge number of simulation histories, which is time consuming. The use of\ncomputer graphics processing units (GPUs) has greatly accelerated MC simulation\nand allows dose calculation within a few minutes for a typical radiotherapy\ntreatment plan. However, some clinical applications demand real time efficiency\nfor MC dose calculation. To tackle this problem, we have developed a real time,\ndeep learning based dose denoiser that can be plugged into a current GPU based\nMC dose engine to enable real time MC dose calculation. We used two different\nacceleration strategies to achieve this goal: 1) we applied voxel unshuffle and\nvoxel shuffle operators to decrease the input and output sizes without any\ninformation loss, and 2) we decoupled the 3D volumetric convolution into a 2D\naxial convolution and a 1D slice convolution. In addition, we used a weakly\nsupervised learning framework to train the network, which greatly reduces the\nsize of the required training dataset and thus enables fast fine tuning based\nadaptation of the trained model to different radiation beams. Experimental\nresults show that the proposed denoiser can run in as little as 39 ms, which is\naround 11.6 times faster than the baseline model. As a result, the whole MC\ndose calculation pipeline can be finished within 0.15 seconds, including both\nGPU MC dose calculation and deep learning based denoising, achieving the real\ntime efficiency needed for some radiotherapy applications, such as online\nadaptive radiotherapy.\n", "versions": [{"version": "v1", "created": "Mon, 30 Nov 2020 16:33:51 GMT"}, {"version": "v2", "created": "Wed, 23 Dec 2020 16:57:15 GMT"}], "update_date": "2020-12-24", "authors_parsed": [["Bai", "Ti", ""], ["Wang", "Biling", ""], ["Nguyen", "Dan", ""], ["Jiang", "Steve", ""]]}, {"id": "2011.14969", "submitter": "Gaurang Sriramanan", "authors": "Gaurang Sriramanan, Sravanti Addepalli, Arya Baburaj, R. Venkatesh\n  Babu", "title": "Guided Adversarial Attack for Evaluating and Enhancing Adversarial\n  Defenses", "comments": "NeurIPS 2020 (Spotlight)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Advances in the development of adversarial attacks have been fundamental to\nthe progress of adversarial defense research. Efficient and effective attacks\nare crucial for reliable evaluation of defenses, and also for developing robust\nmodels. Adversarial attacks are often generated by maximizing standard losses\nsuch as the cross-entropy loss or maximum-margin loss within a constraint set\nusing Projected Gradient Descent (PGD). In this work, we introduce a relaxation\nterm to the standard loss, that finds more suitable gradient-directions,\nincreases attack efficacy and leads to more efficient adversarial training. We\npropose Guided Adversarial Margin Attack (GAMA), which utilizes function\nmapping of the clean image to guide the generation of adversaries, thereby\nresulting in stronger attacks. We evaluate our attack against multiple defenses\nand show improved performance when compared to existing attacks. Further, we\npropose Guided Adversarial Training (GAT), which achieves state-of-the-art\nperformance amongst single-step defenses by utilizing the proposed relaxation\nterm for both attack generation and training.\n", "versions": [{"version": "v1", "created": "Mon, 30 Nov 2020 16:39:39 GMT"}], "update_date": "2020-12-01", "authors_parsed": [["Sriramanan", "Gaurang", ""], ["Addepalli", "Sravanti", ""], ["Baburaj", "Arya", ""], ["Babu", "R. Venkatesh", ""]]}, {"id": "2011.14983", "submitter": "Douglas Gomes PhD", "authors": "Douglas P. S. Gomes, Michael J. Horry, Anwaar Ulhaq, Manoranjan Paul,\n  Subrata Chakraborty, Manash Saha, Tanmoy Debnath, D.M. Motiur Rahaman", "title": "MAVIDH Score: A COVID-19 Severity Scoring using Chest X-Ray Pathology\n  Features", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The application of computer vision for COVID-19 diagnosis is complex and\nchallenging, given the risks associated with patient misclassifications.\nArguably, the primary value of medical imaging for COVID-19 lies rather on\npatient prognosis. Radiological images can guide physicians assessing the\nseverity of the disease, and a series of images from the same patient at\ndifferent stages can help to gauge disease progression. Hence, a simple method\nbased on lung-pathology interpretable features for scoring disease severity\nfrom Chest X-rays is proposed here. As the primary contribution, this method\ncorrelates well to patient severity in different stages of disease progression\nwith competitive results compared to other existing, more complex methods. An\noriginal data selection approach is also proposed, allowing the simple model to\nlearn the severity-related features. It is hypothesized that the resulting\ncompetitive performance presented here is related to the method being\nfeature-based rather than reliant on lung involvement or opacity as others in\nthe literature. A second contribution comes from the validation of the results,\nconceptualized as the scoring of patients groups from different stages of the\ndisease. Besides performing such validation on an independent data set, the\nresults were also compared with other proposed scoring methods in the\nliterature. The results show that there is a significant correlation between\nthe scoring system (MAVIDH) and patient outcome, which could potentially help\nphysicians rating and following disease progression in COVID-19 patients.\n", "versions": [{"version": "v1", "created": "Mon, 30 Nov 2020 16:55:28 GMT"}, {"version": "v2", "created": "Tue, 1 Dec 2020 20:52:56 GMT"}, {"version": "v3", "created": "Tue, 2 Feb 2021 13:01:09 GMT"}], "update_date": "2021-02-03", "authors_parsed": [["Gomes", "Douglas P. S.", ""], ["Horry", "Michael J.", ""], ["Ulhaq", "Anwaar", ""], ["Paul", "Manoranjan", ""], ["Chakraborty", "Subrata", ""], ["Saha", "Manash", ""], ["Debnath", "Tanmoy", ""], ["Rahaman", "D. M. Motiur", ""]]}, {"id": "2011.15000", "submitter": "Abhijeet Patil", "authors": "Abhijeet Patil, Mohd. Talha, Aniket Bhatia, Nikhil Cherian Kurian,\n  Sammed Mangale, Sunil Patel, Amit Sethi", "title": "Fast, Self Supervised, Fully Convolutional Color Normalization of H&E\n  Stained Images", "comments": "--", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Performance of deep learning algorithms decreases drastically if the data\ndistributions of the training and testing sets are different. Due to variations\nin staining protocols, reagent brands, and habits of technicians, color\nvariation in digital histopathology images is quite common. Color variation\ncauses problems for the deployment of deep learning-based solutions for\nautomatic diagnosis system in histopathology. Previously proposed color\nnormalization methods consider a small patch as a reference for normalization,\nwhich creates artifacts on out-of-distribution source images. These methods are\nalso slow as most of the computation is performed on CPUs instead of the GPUs.\nWe propose a color normalization technique, which is fast during its\nself-supervised training as well as inference. Our method is based on a\nlightweight fully-convolutional neural network and can be easily attached to a\ndeep learning-based pipeline as a pre-processing block. For classification and\nsegmentation tasks on CAMELYON17 and MoNuSeg datasets respectively, the\nproposed method is faster and gives a greater increase in accuracy than the\nstate of the art methods.\n", "versions": [{"version": "v1", "created": "Mon, 30 Nov 2020 17:05:58 GMT"}], "update_date": "2020-12-01", "authors_parsed": [["Patil", "Abhijeet", ""], ["Talha", "Mohd.", ""], ["Bhatia", "Aniket", ""], ["Kurian", "Nikhil Cherian", ""], ["Mangale", "Sammed", ""], ["Patel", "Sunil", ""], ["Sethi", "Amit", ""]]}, {"id": "2011.15002", "submitter": "Jinjin Gu", "authors": "Jinjin Gu, Haoming Cai, Haoyu Chen, Xiaoxing Ye, Jimmy Ren, Chao Dong", "title": "Image Quality Assessment for Perceptual Image Restoration: A New\n  Dataset, Benchmark and Metric", "comments": "arXiv admin note: substantial text overlap with arXiv:2007.12142", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Image quality assessment (IQA) is the key factor for the fast development of\nimage restoration (IR) algorithms. The most recent perceptual IR algorithms\nbased on generative adversarial networks (GANs) have brought in significant\nimprovement on visual performance, but also pose great challenges for\nquantitative evaluation. Notably, we observe an increasing inconsistency\nbetween perceptual quality and the evaluation results. We present two\nquestions: Can existing IQA methods objectively evaluate recent IR algorithms?\nWith the focus on beating current benchmarks, are we getting better IR\nalgorithms? To answer the questions and promote the development of IQA methods,\nwe contribute a large-scale IQA dataset, called Perceptual Image Processing\nALgorithms (PIPAL) dataset. Especially, this dataset includes the results of\nGAN-based IR algorithms, which are missing in previous datasets. We collect\nmore than 1.13 million human judgments to assign subjective scores for PIPAL\nimages using the more reliable Elo system. Based on PIPAL, we present new\nbenchmarks for both IQA and SR methods. Our results indicate that existing IQA\nmethods cannot fairly evaluate GAN-based IR algorithms. While using appropriate\nevaluation methods is important, IQA methods should also be updated along with\nthe development of IR algorithms. At last, we shed light on how to improve the\nIQA performance on GAN-based distortion. Inspired by the find that the existing\nIQA methods have an unsatisfactory performance on the GAN-based distortion\npartially because of their low tolerance to spatial misalignment, we propose to\nimprove the performance of an IQA network on GAN-based distortion by explicitly\nconsidering this misalignment. We propose the Space Warping Difference Network,\nwhich includes the novel l_2 pooling layers and Space Warping Difference\nlayers. Experiments demonstrate the effectiveness of the proposed method.\n", "versions": [{"version": "v1", "created": "Mon, 30 Nov 2020 17:06:46 GMT"}], "update_date": "2020-12-01", "authors_parsed": [["Gu", "Jinjin", ""], ["Cai", "Haoming", ""], ["Chen", "Haoyu", ""], ["Ye", "Xiaoxing", ""], ["Ren", "Jimmy", ""], ["Dong", "Chao", ""]]}, {"id": "2011.15045", "submitter": "Sreyas Mohan", "authors": "Dev Yashpal Sheth, Sreyas Mohan, Joshua L. Vincent, Ramon Manzorro,\n  Peter A. Crozier, Mitesh M. Khapra, Eero P. Simoncelli, Carlos\n  Fernandez-Granda", "title": "Unsupervised Deep Video Denoising", "comments": "Dev and Sreyas contributed equally", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep convolutional neural networks (CNNs) currently achieve state-of-the-art\nperformance in denoising videos. They are typically trained with supervision,\nminimizing the error between the network output and ground-truth clean videos.\nHowever, in many applications, such as microscopy, noiseless videos are not\navailable. To address these cases, we build on recent advances in unsupervised\nstill image denoising to develop an Unsupervised Deep Video Denoiser (UDVD).\nUDVD is shown to perform competitively with current state-of-the-art supervised\nmethods on benchmark datasets, even when trained only on a single short noisy\nvideo sequence. Experiments on fluorescence-microscopy and electron-microscopy\ndata illustrate the promise of our approach for imaging modalities where\nground-truth clean data is generally not available. In addition, we study the\nmechanisms used by trained CNNs to perform video denoising. An analysis of the\ngradient of the network output with respect to its input reveals that these\nnetworks perform spatio-temporal filtering that is adapted to the particular\nspatial structures and motion of the underlying content. We interpret this as\nan implicit and highly effective form of motion compensation, a widely used\nparadigm in traditional video denoising, compression, and analysis. Code and\niPython notebooks for our analysis are available in\nhttps://sreyas-mohan.github.io/udvd/ .\n", "versions": [{"version": "v1", "created": "Mon, 30 Nov 2020 17:45:08 GMT"}, {"version": "v2", "created": "Tue, 1 Dec 2020 04:25:50 GMT"}], "update_date": "2020-12-02", "authors_parsed": [["Sheth", "Dev Yashpal", ""], ["Mohan", "Sreyas", ""], ["Vincent", "Joshua L.", ""], ["Manzorro", "Ramon", ""], ["Crozier", "Peter A.", ""], ["Khapra", "Mitesh M.", ""], ["Simoncelli", "Eero P.", ""], ["Fernandez-Granda", "Carlos", ""]]}, {"id": "2011.15049", "submitter": "Vinicius Vianna", "authors": "Vinicius Pavanelli Vianna and Luiz Otavio Murta Jr", "title": "Long-range medical image registration through generalized mutual\n  information (GMI): toward a fully automatic volumetric alignment", "comments": "13 pages, 8 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image registration is a key operation in medical image processing, allowing a\nplethora of applications. Mutual information (MI) is consolidated as a robust\nsimilarity metric often used for medical image registration. Although MI\nprovides a robust medical image registration, it usually fails when the needed\nimage transform is too big due to MI local maxima traps. In this paper, we\npropose and evaluate a generalized parametric MI as an affine registration cost\nfunction. We assessed the generalized MI (GMI) functions for separable affine\ntransforms and exhaustively evaluated the GMI mathematical image seeking the\nmaximum registration range through a gradient descent simulation. We also\nemployed Monte Carlo simulation essays for testing translation registering of\nrandomized T1 versus T2 images. GMI functions showed to have smooth isosurfaces\ndriving the algorithm to the global maxima. Results show significantly\nprolonged registration ranges, avoiding the traps of local maxima. We evaluated\na range of [-150mm,150mm] for translations, [-180{\\deg},180{\\deg}] for\nrotations, [0.5,2] for scales, and [-1,1] for skew with a success rate of\n99.99%, 97.58%, 99.99%, and 99.99% respectively for the transforms in the\nsimulated gradient descent. We also obtained 99.75% success in Monte Carlo\nsimulation from 2,000 randomized translations trials with 1,113 subjects T1 and\nT2 MRI images. The findings point towards the reliability of GMI for long-range\nregistration with enhanced speed performance\n", "versions": [{"version": "v1", "created": "Mon, 30 Nov 2020 17:48:28 GMT"}], "update_date": "2020-12-01", "authors_parsed": [["Vianna", "Vinicius Pavanelli", ""], ["Murta", "Luiz Otavio", "Jr"]]}, {"id": "2011.15079", "submitter": "Christian Diller", "authors": "Christian Diller, Thomas Funkhouser, Angela Dai", "title": "Forecasting Characteristic 3D Poses of Human Actions", "comments": "Paper Video: https://youtu.be/vSxJg9z7cAM Project Page:\n  https://charposes.christian-diller.de/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose the task of forecasting characteristic 3D poses: from a monocular\nvideo observation of a person, to predict a future 3D pose of that person in a\nlikely action-defining, characteristic pose - for instance, from observing a\nperson reaching for a banana, predict the pose of the person eating the banana.\nPrior work on human motion prediction estimates future poses at fixed time\nintervals. Although easy to define, this frame-by-frame formulation confounds\ntemporal and intentional aspects of human action. Instead, we define a\nsemantically meaningful pose prediction task that decouples the predicted pose\nfrom time, taking inspiration from goal-directed behavior. To predict\ncharacteristic poses, we propose a probabilistic approach that first models the\npossible multi-modality in the distribution of likely characteristic poses. It\nthen samples future pose hypotheses from the predicted distribution in an\nautoregressive fashion to model dependencies between joints and finally\noptimizes the resulting pose with bone length and angle constraints. To\nevaluate our method, we construct a dataset of manually annotated\ncharacteristic 3D poses. Our experiments with this dataset suggest that our\nproposed probabilistic approach outperforms state-of-the-art methods by 22% on\naverage.\n", "versions": [{"version": "v1", "created": "Mon, 30 Nov 2020 18:20:17 GMT"}, {"version": "v2", "created": "Wed, 7 Apr 2021 17:58:08 GMT"}], "update_date": "2021-04-08", "authors_parsed": [["Diller", "Christian", ""], ["Funkhouser", "Thomas", ""], ["Dai", "Angela", ""]]}, {"id": "2011.15081", "submitter": "Albert Matveev", "authors": "Albert Matveev, Alexey Artemov, Ruslan Rakhimov, Gleb Bobrovskikh,\n  Daniele Panozzo, Denis Zorin, Evgeny Burnaev", "title": "DEF: Deep Estimation of Sharp Geometric Features in 3D Shapes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sharp feature lines carry essential information about human-made objects,\nenabling compact 3D shape representations, high-quality surface reconstruction,\nand are a signal source for mesh processing. While extracting high-quality\nlines from noisy and undersampled data is challenging for traditional methods,\ndeep learning-powered algorithms can leverage global and semantic information\nfrom the training data to aid in the process. We propose Deep Estimators of\nFeatures (DEFs), a learning-based framework for predicting sharp geometric\nfeatures in sampled 3D shapes. Differently from existing data-driven methods,\nwhich reduce this problem to feature classification, we propose to regress a\nscalar field representing the distance from point samples to the closest\nfeature line on local patches. By fusing the result of individual patches, we\ncan process large 3D models, which are impossible to process for existing\ndata-driven methods due to their size and complexity. Extensive experimental\nevaluation of DEFs is implemented on synthetic and real-world 3D shape datasets\nand suggests advantages of our image- and point-based estimators over\ncompetitor methods, as well as improved noise robustness and scalability of our\napproach.\n", "versions": [{"version": "v1", "created": "Mon, 30 Nov 2020 18:21:00 GMT"}], "update_date": "2020-12-01", "authors_parsed": [["Matveev", "Albert", ""], ["Artemov", "Alexey", ""], ["Rakhimov", "Ruslan", ""], ["Bobrovskikh", "Gleb", ""], ["Panozzo", "Daniele", ""], ["Zorin", "Denis", ""], ["Burnaev", "Evgeny", ""]]}, {"id": "2011.15084", "submitter": "Yecheng Ma", "authors": "Yecheng Jason Ma, Jeevana Priya Inala, Dinesh Jayaraman, Osbert\n  Bastani", "title": "Diverse Sampling for Normalizing Flow Based Trajectory Forecasting", "comments": "Technical report, 18 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  For autonomous cars to drive safely and effectively, they must anticipate the\nstochastic future trajectories of other agents in the scene, such as\npedestrians and other cars. Forecasting such complex multi-modal distributions\nrequires powerful probabilistic approaches. Normalizing flows have recently\nemerged as an attractive tool to model such distributions. However, when\ngenerating trajectory predictions from a flow model, a key drawback is that\nindependent samples often do not adequately capture all the modes in the\nunderlying distribution. We propose Diversity Sampling for Flow (DSF), a method\nfor improving the quality and the diversity of trajectory samples from a\npre-trained flow model. Rather than producing individual samples, DSF produces\na set of trajectories in one shot. Given a pre-trained forecasting flow model,\nwe train DSF using gradients from the model, to optimize an objective function\nthat rewards high likelihood for individual trajectories in the predicted set,\ntogether with high spatial separation between trajectories. DSF is easy to\nimplement, and we show that it offers a simple plug-in improvement for several\nexisting flow-based forecasting models, achieving state-of-art results on two\nchallenging vehicle and pedestrian forecasting benchmarks.\n", "versions": [{"version": "v1", "created": "Mon, 30 Nov 2020 18:23:29 GMT"}], "update_date": "2020-12-01", "authors_parsed": [["Ma", "Yecheng Jason", ""], ["Inala", "Jeevana Priya", ""], ["Jayaraman", "Dinesh", ""], ["Bastani", "Osbert", ""]]}, {"id": "2011.15093", "submitter": "Ahmed Fetit", "authors": "Seoin Chai, Daniel Rueckert, Ahmed E. Fetit", "title": "Reducing Textural Bias Improves Robustness of Deep Segmentation Models", "comments": "To appear in MIUA 2021 (accepted version)", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Despite advances in deep learning, robustness under domain shift remains a\nmajor bottleneck in medical imaging settings. Findings on natural images\nsuggest that deep neural models can show a strong textural bias when carrying\nout image classification tasks. In this thorough empirical study, we draw\ninspiration from findings on natural images and investigate ways in which\naddressing the textural bias phenomenon could bring up the robustness of deep\nsegmentation models when applied to three-dimensional (3D) medical data. To\nachieve this, publicly available MRI scans from the Developing Human Connectome\nProject are used to study ways in which simulating textural noise can help\ntrain robust models in a complex semantic segmentation task. We contribute an\nextensive empirical investigation consisting of 176 experiments and illustrate\nhow applying specific types of simulated textural noise prior to training can\nlead to texture invariant models, resulting in improved robustness when\nsegmenting scans corrupted by previously unseen noise types and levels.\n", "versions": [{"version": "v1", "created": "Mon, 30 Nov 2020 18:29:53 GMT"}, {"version": "v2", "created": "Tue, 22 Jun 2021 22:17:35 GMT"}, {"version": "v3", "created": "Sun, 27 Jun 2021 21:15:40 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Chai", "Seoin", ""], ["Rueckert", "Daniel", ""], ["Fetit", "Ahmed E.", ""]]}, {"id": "2011.15102", "submitter": "Pengtao Xie", "authors": "Xuefeng Du, Haochen Zhang, Pengtao Xie", "title": "Learning by Passing Tests, with Application to Neural Architecture\n  Search", "comments": "arXiv admin note: substantial text overlap with arXiv:2012.04863,\n  arXiv:2012.12502, arXiv:2012.12899", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Learning through tests is a broadly used methodology in human learning and\nshows great effectiveness in improving learning outcome: a sequence of tests\nare made with increasing levels of difficulty; the learner takes these tests to\nidentify his/her weak points in learning and continuously addresses these weak\npoints to successfully pass these tests. We are interested in investigating\nwhether this powerful learning technique can be borrowed from humans to improve\nthe learning abilities of machines. We propose a novel learning approach called\nlearning by passing tests (LPT). In our approach, a tester model creates\nincreasingly more-difficult tests to evaluate a learner model. The learner\ntries to continuously improve its learning ability so that it can successfully\npass however difficult tests created by the tester. We propose a multi-level\noptimization framework to formulate LPT, where the tester learns to create\ndifficult and meaningful tests and the learner learns to pass these tests. We\ndevelop an efficient algorithm to solve the LPT problem. Our method is applied\nfor neural architecture search and achieves significant improvement over\nstate-of-the-art baselines on CIFAR-100, CIFAR-10, and ImageNet.\n", "versions": [{"version": "v1", "created": "Mon, 30 Nov 2020 18:33:34 GMT"}, {"version": "v2", "created": "Fri, 12 Mar 2021 03:43:01 GMT"}], "update_date": "2021-03-15", "authors_parsed": [["Du", "Xuefeng", ""], ["Zhang", "Haochen", ""], ["Xie", "Pengtao", ""]]}, {"id": "2011.15103", "submitter": "Parmida Davarmanesh", "authors": "Parmida Davarmanesh, Kuanhao Jiang, Tingting Ou, Artem Vysogorets,\n  Stanislav Ivashkevich, Max Kiehn, Shantanu H. Joshi, Nicholas Malaya", "title": "Automating Artifact Detection in Video Games", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In spite of advances in gaming hardware and software, gameplay is often\ntainted with graphics errors, glitches, and screen artifacts. This proof of\nconcept study presents a machine learning approach for automated detection of\ngraphics corruptions in video games. Based on a sample of representative screen\ncorruption examples, the model was able to identify 10 of the most commonly\noccurring screen artifacts with reasonable accuracy. Feature representation of\nthe data included discrete Fourier transforms, histograms of oriented\ngradients, and graph Laplacians. Various combinations of these features were\nused to train machine learning models that identify individual classes of\ngraphics corruptions and that later were assembled into a single mixed experts\n\"ensemble\" classifier. The ensemble classifier was tested on heldout test sets,\nand produced an accuracy of 84% on the games it had seen before, and 69% on\ngames it had never seen before.\n", "versions": [{"version": "v1", "created": "Mon, 30 Nov 2020 18:34:40 GMT"}], "update_date": "2020-12-01", "authors_parsed": [["Davarmanesh", "Parmida", ""], ["Jiang", "Kuanhao", ""], ["Ou", "Tingting", ""], ["Vysogorets", "Artem", ""], ["Ivashkevich", "Stanislav", ""], ["Kiehn", "Max", ""], ["Joshi", "Shantanu H.", ""], ["Malaya", "Nicholas", ""]]}, {"id": "2011.15119", "submitter": "Tingwu Wang", "authors": "Tingwu Wang, Yunrong Guo, Maria Shugrina, Sanja Fidler", "title": "UniCon: Universal Neural Controller For Physics-based Character Motion", "comments": "15 pages, 15 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.CV cs.LG cs.RO", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The field of physics-based animation is gaining importance due to the\nincreasing demand for realism in video games and films, and has recently seen\nwide adoption of data-driven techniques, such as deep reinforcement learning\n(RL), which learn control from (human) demonstrations. While RL has shown\nimpressive results at reproducing individual motions and interactive\nlocomotion, existing methods are limited in their ability to generalize to new\nmotions and their ability to compose a complex motion sequence interactively.\nIn this paper, we propose a physics-based universal neural controller (UniCon)\nthat learns to master thousands of motions with different styles by learning on\nlarge-scale motion datasets. UniCon is a two-level framework that consists of a\nhigh-level motion scheduler and an RL-powered low-level motion executor, which\nis our key innovation. By systematically analyzing existing multi-motion RL\nframeworks, we introduce a novel objective function and training techniques\nwhich make a significant leap in performance. Once trained, our motion executor\ncan be combined with different high-level schedulers without the need for\nretraining, enabling a variety of real-time interactive applications. We show\nthat UniCon can support keyboard-driven control, compose motion sequences drawn\nfrom a large pool of locomotion and acrobatics skills and teleport a person\ncaptured on video to a physics-based virtual avatar. Numerical and qualitative\nresults demonstrate a significant improvement in efficiency, robustness and\ngeneralizability of UniCon over prior state-of-the-art, showcasing\ntransferability to unseen motions, unseen humanoid models and unseen\nperturbation.\n", "versions": [{"version": "v1", "created": "Mon, 30 Nov 2020 18:51:16 GMT"}], "update_date": "2020-12-01", "authors_parsed": [["Wang", "Tingwu", ""], ["Guo", "Yunrong", ""], ["Shugrina", "Maria", ""], ["Fidler", "Sanja", ""]]}, {"id": "2011.15124", "submitter": "Emanuele Bugliarello", "authors": "Emanuele Bugliarello, Ryan Cotterell, Naoaki Okazaki, Desmond Elliott", "title": "Multimodal Pretraining Unmasked: A Meta-Analysis and a Unified Framework\n  of Vision-and-Language BERTs", "comments": "To appear in TACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large-scale pretraining and task-specific fine-tuning is now the standard\nmethodology for many tasks in computer vision and natural language processing.\nRecently, a multitude of methods have been proposed for pretraining vision and\nlanguage BERTs to tackle challenges at the intersection of these two key areas\nof AI. These models can be categorised into either single-stream or dual-stream\nencoders. We study the differences between these two categories, and show how\nthey can be unified under a single theoretical framework. We then conduct\ncontrolled experiments to discern the empirical differences between five V&L\nBERTs. Our experiments show that training data and hyperparameters are\nresponsible for most of the differences between the reported results, but they\nalso reveal that the embedding layer plays a crucial role in these massive\nmodels.\n", "versions": [{"version": "v1", "created": "Mon, 30 Nov 2020 18:55:24 GMT"}, {"version": "v2", "created": "Sun, 30 May 2021 23:37:58 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Bugliarello", "Emanuele", ""], ["Cotterell", "Ryan", ""], ["Okazaki", "Naoaki", ""], ["Elliott", "Desmond", ""]]}, {"id": "2011.15126", "submitter": "Ting-Chun Wang", "authors": "Ting-Chun Wang, Arun Mallya, Ming-Yu Liu", "title": "One-Shot Free-View Neural Talking-Head Synthesis for Video Conferencing", "comments": "CVPR 2021 camera ready (oral). Our project page can be found at\n  https://nvlabs.github.io/face-vid2vid", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a neural talking-head video synthesis model and demonstrate its\napplication to video conferencing. Our model learns to synthesize a\ntalking-head video using a source image containing the target person's\nappearance and a driving video that dictates the motion in the output. Our\nmotion is encoded based on a novel keypoint representation, where the\nidentity-specific and motion-related information is decomposed unsupervisedly.\nExtensive experimental validation shows that our model outperforms competing\nmethods on benchmark datasets. Moreover, our compact keypoint representation\nenables a video conferencing system that achieves the same visual quality as\nthe commercial H.264 standard while only using one-tenth of the bandwidth.\nBesides, we show our keypoint representation allows the user to rotate the head\nduring synthesis, which is useful for simulating face-to-face video\nconferencing experiences.\n", "versions": [{"version": "v1", "created": "Mon, 30 Nov 2020 18:56:35 GMT"}, {"version": "v2", "created": "Tue, 30 Mar 2021 17:54:33 GMT"}, {"version": "v3", "created": "Fri, 2 Apr 2021 23:37:06 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Wang", "Ting-Chun", ""], ["Mallya", "Arun", ""], ["Liu", "Ming-Yu", ""]]}, {"id": "2011.15128", "submitter": "Aleksander Holynski", "authors": "Aleksander Holynski, Brian Curless, Steven M. Seitz, Richard Szeliski", "title": "Animating Pictures with Eulerian Motion Fields", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we demonstrate a fully automatic method for converting a still\nimage into a realistic animated looping video. We target scenes with continuous\nfluid motion, such as flowing water and billowing smoke. Our method relies on\nthe observation that this type of natural motion can be convincingly reproduced\nfrom a static Eulerian motion description, i.e. a single, temporally constant\nflow field that defines the immediate motion of a particle at a given 2D\nlocation. We use an image-to-image translation network to encode motion priors\nof natural scenes collected from online videos, so that for a new photo, we can\nsynthesize a corresponding motion field. The image is then animated using the\ngenerated motion through a deep warping technique: pixels are encoded as deep\nfeatures, those features are warped via Eulerian motion, and the resulting\nwarped feature maps are decoded as images. In order to produce continuous,\nseamlessly looping video textures, we propose a novel video looping technique\nthat flows features both forward and backward in time and then blends the\nresults. We demonstrate the effectiveness and robustness of our method by\napplying it to a large collection of examples including beaches, waterfalls,\nand flowing rivers.\n", "versions": [{"version": "v1", "created": "Mon, 30 Nov 2020 18:59:06 GMT"}], "update_date": "2020-12-01", "authors_parsed": [["Holynski", "Aleksander", ""], ["Curless", "Brian", ""], ["Seitz", "Steven M.", ""], ["Szeliski", "Richard", ""]]}]