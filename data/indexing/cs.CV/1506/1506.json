[{"id": "1506.00051", "submitter": "Jurandy Almeida", "authors": "Leonardo A. Duarte, Ot\\'avio A. B. Penatti, and Jurandy Almeida", "title": "Bag of Genres for Video Retrieval", "comments": null, "journal-ref": "in 2016 29th SIBGRAPI Conference on Graphics, Patterns and Images\n  (SIBGRAPI), S\\~{a}o Jos\\'{e} dos Campos, Brazil, 2016, pp. 257-264", "doi": "10.1109/SIBGRAPI.2016.043", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Often, videos are composed of multiple concepts or even genres. For instance,\nnews videos may contain sports, action, nature, etc. Therefore, encoding the\ndistribution of such concepts/genres in a compact and effective representation\nis a challenging task. In this sense, we propose the Bag of Genres\nrepresentation, which is based on a visual dictionary defined by a genre\nclassifier. Each visual word corresponds to a region in the classification\nspace. The Bag of Genres video vector contains a summary of the activations of\neach genre in the video content. We evaluate the proposed method for video\ngenre retrieval using the dataset of MediaEval Tagging Task of 2012 and for\nvideo event retrieval using the EVVE dataset. Results show that the proposed\nmethod achieves results comparable or superior to state-of-the-art methods,\nwith the advantage of providing a much more compact representation than\nexisting features.\n", "versions": [{"version": "v1", "created": "Sat, 30 May 2015 00:35:28 GMT"}, {"version": "v2", "created": "Sat, 26 Dec 2020 14:17:03 GMT"}], "update_date": "2020-12-29", "authors_parsed": [["Duarte", "Leonardo A.", ""], ["Penatti", "Ot\u00e1vio A. B.", ""], ["Almeida", "Jurandy", ""]]}, {"id": "1506.00060", "submitter": "Xiaohao Cai", "authors": "Xiaohao Cai, Raymond Chan, Mila Nikolova, and Tieyong Zeng", "title": "A Three-stage Approach for Segmenting Degraded Color Images: Smoothing,\n  Lifting and Thresholding (SLaT)", "comments": "19 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a SLaT (Smoothing, Lifting and Thresholding) method\nwith three stages for multiphase segmentation of color images corrupted by\ndifferent degradations: noise, information loss, and blur. At the first stage,\na convex variant of the Mumford-Shah model is applied to each channel to obtain\na smooth image. We show that the model has unique solution under the different\ndegradations. In order to properly handle the color information, the second\nstage is dimension lifting where we consider a new vector-valued image composed\nof the restored image and its transform in the secondary color space with\nadditional information. This ensures that even if the first color space has\nhighly correlated channels, we can still have enough information to give good\nsegmentation results. In the last stage, we apply multichannel thresholding to\nthe combined vector-valued image to find the segmentation. The number of phases\nis only required in the last stage, so users can choose or change it all\nwithout the need of solving the previous stages again. Experiments demonstrate\nthat our SLaT method gives excellent results in terms of segmentation quality\nand CPU time in comparison with other state-of-the-art segmentation methods.\n", "versions": [{"version": "v1", "created": "Sat, 30 May 2015 02:53:18 GMT"}], "update_date": "2015-06-02", "authors_parsed": [["Cai", "Xiaohao", ""], ["Chan", "Raymond", ""], ["Nikolova", "Mila", ""], ["Zeng", "Tieyong", ""]]}, {"id": "1506.00097", "submitter": "Alex James Dr", "authors": "Alex Pappachen James, Belur Dasarathy", "title": "A Review of Feature and Data Fusion with Medical Images", "comments": "Multisensor Data Fusion: From Algorithm and Architecture Design to\n  Applications, CRC Press, 2015. arXiv admin note: substantial text overlap\n  with arXiv:1401.0166", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  The fusion techniques that utilize multiple feature sets to form new features\nthat are often more robust and contain useful information for future processing\nare referred to as feature fusion. The term data fusion is applied to the class\nof techniques used for combining decisions obtained from multiple feature sets\nto form global decisions. Feature and data fusion interchangeably represent two\nimportant classes of techniques that have proved to be of practical importance\nin a wide range of medical imaging problems\n", "versions": [{"version": "v1", "created": "Sat, 30 May 2015 09:50:19 GMT"}], "update_date": "2015-06-02", "authors_parsed": [["James", "Alex Pappachen", ""], ["Dasarathy", "Belur", ""]]}, {"id": "1506.00168", "submitter": "Sagi Eppel", "authors": "Sagi Eppel", "title": "Using curvature to distinguish between surface reflections and vessel\n  contents in computer vision based recognition of materials in transparent\n  vessels", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The recognition of materials and objects inside transparent containers using\ncomputer vision has a wide range of applications, ranging from industrial\nbottles filling to the automation of chemistry laboratory. One of the main\nchallenges in such recognition is the ability to distinguish between image\nfeatures resulting from the vessels surface and image features resulting from\nthe material inside the vessel. Reflections and the functional parts of a\nvessels surface can create strong edges that can be mistakenly identified as\ncorresponding to the vessel contents, and cause recognition errors. The ability\nto evaluate whether a specific edge in an image stems from the vessels surface\nor from its contents can considerably improve the ability to identify materials\ninside transparent vessels. This work will suggest a method for such\nevaluation, based on the following two assumptions: 1) Areas of high curvature\non the vessel surface are likely to cause strong edges due to changes in\nreflectivity, as is the appearance of functional parts (e.g. corks or valves).\n2) Most transparent vessels (bottles, glasses) have high symmetry\n(cylindrical). As a result the curvature angle of the vessels surface at each\npoint of the image is similar to the curvature angle of the contour line of the\nvessel in the same row in the image. These assumptions, allow the\nidentification of image regions with strong edges corresponding to the vessel\nsurface reflections. Combining this method with existing image analysis methods\nfor detecting materials inside transparent containers allows considerable\nimprovement in accuracy.\n", "versions": [{"version": "v1", "created": "Sat, 30 May 2015 21:34:41 GMT"}], "update_date": "2015-06-02", "authors_parsed": [["Eppel", "Sagi", ""]]}, {"id": "1506.00176", "submitter": "Lianwen Jin", "authors": "Liquan Qiu, Lianwen Jin, Ruifen Dai, Yuxiang Zhang, Lei Li", "title": "An Open Source Testing Tool for Evaluating Handwriting Input Methods", "comments": "5 pages, 3 figures, 11 tables. Accepted to appear at ICDAR 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents an open source tool for testing the recognition accuracy\nof Chinese handwriting input methods. The tool consists of two modules, namely\nthe PC and Android mobile client. The PC client reads handwritten samples in\nthe computer, and transfers them individually to the Android client in\naccordance with the socket communication protocol. After the Android client\nreceives the data, it simulates the handwriting on screen of client device, and\ntriggers the corresponding handwriting recognition method. The recognition\naccuracy is recorded by the Android client. We present the design principles\nand describe the implementation of the test platform. We construct several test\ndatasets for evaluating different handwriting recognition systems, and conduct\nan objective and comprehensive test using six Chinese handwriting input methods\nwith five datasets. The test results for the recognition accuracy are then\ncompared and analyzed.\n", "versions": [{"version": "v1", "created": "Sat, 30 May 2015 22:35:55 GMT"}], "update_date": "2015-06-02", "authors_parsed": [["Qiu", "Liquan", ""], ["Jin", "Lianwen", ""], ["Dai", "Ruifen", ""], ["Zhang", "Yuxiang", ""], ["Li", "Lei", ""]]}, {"id": "1506.00278", "submitter": "Licheng Yu", "authors": "Licheng Yu, Eunbyung Park, Alexander C. Berg, and Tamara L. Berg", "title": "Visual Madlibs: Fill in the blank Image Generation and Question\n  Answering", "comments": "10 pages; 8 figures; 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce a new dataset consisting of 360,001 focused\nnatural language descriptions for 10,738 images. This dataset, the Visual\nMadlibs dataset, is collected using automatically produced fill-in-the-blank\ntemplates designed to gather targeted descriptions about: people and objects,\ntheir appearances, activities, and interactions, as well as inferences about\nthe general scene or its broader context. We provide several analyses of the\nVisual Madlibs dataset and demonstrate its applicability to two new description\ngeneration tasks: focused description generation, and multiple-choice\nquestion-answering for images. Experiments using joint-embedding and deep\nlearning methods show promising results on these tasks.\n", "versions": [{"version": "v1", "created": "Sun, 31 May 2015 19:39:44 GMT"}], "update_date": "2015-06-02", "authors_parsed": [["Yu", "Licheng", ""], ["Park", "Eunbyung", ""], ["Berg", "Alexander C.", ""], ["Berg", "Tamara L.", ""]]}, {"id": "1506.00333", "submitter": "Lin Ma", "authors": "Lin Ma, Zhengdong Lu, Hang Li", "title": "Learning to Answer Questions From Image Using Convolutional Neural\n  Network", "comments": "7 pages, 4 figures. Accepted by AAAI 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose to employ the convolutional neural network (CNN)\nfor the image question answering (QA). Our proposed CNN provides an end-to-end\nframework with convolutional architectures for learning not only the image and\nquestion representations, but also their inter-modal interactions to produce\nthe answer. More specifically, our model consists of three CNNs: one image CNN\nto encode the image content, one sentence CNN to compose the words of the\nquestion, and one multimodal convolution layer to learn their joint\nrepresentation for the classification in the space of candidate answer words.\nWe demonstrate the efficacy of our proposed model on the DAQUAR and COCO-QA\ndatasets, which are two benchmark datasets for the image QA, with the\nperformances significantly outperforming the state-of-the-art.\n", "versions": [{"version": "v1", "created": "Mon, 1 Jun 2015 03:09:49 GMT"}, {"version": "v2", "created": "Fri, 13 Nov 2015 09:54:59 GMT"}], "update_date": "2015-11-16", "authors_parsed": [["Ma", "Lin", ""], ["Lu", "Zhengdong", ""], ["Li", "Hang", ""]]}, {"id": "1506.00368", "submitter": "Thanh The Van", "authors": "Thanh The Van, Thanh Manh Le", "title": "RBIR using Interest Regions and Binary Signatures", "comments": "14 pages, 8 figures", "journal-ref": "Annales Univ. Sci. Budapest, Sect. Comp. 43 (2014), pp. 89-103", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce an approach to overcome the low accuracy of the\nContent-Based Image Retrieval (CBIR) (when using the global features). To\nincrease the accuracy, we use Harris-Laplace detector to identify the interest\nregions of image. Then, we build the Region-Based Image Retrieval (RBIR). For\nthe efficient image storage and retrieval, we encode images into binary\nsignatures. The binary signature of a image is created from its interest\nregions. Furthermore, this paper also provides an algorithm for image retrieval\non S-tree by comparing the images' signatures on a metric similarly to EMD\n(earth mover's distance). Finally, we evaluate the created models on COREL's\nimages.\n", "versions": [{"version": "v1", "created": "Mon, 1 Jun 2015 07:27:42 GMT"}], "update_date": "2015-06-03", "authors_parsed": [["Van", "Thanh The", ""], ["Le", "Thanh Manh", ""]]}, {"id": "1506.00395", "submitter": "Andrea Fusiello", "authors": "Roberto Toldo, Riccardo Gherardi, Michela Farenzena and Andrea\n  Fusiello", "title": "Hierarchical structure-and-motion recovery from uncalibrated images", "comments": "Accepted for publication in CVIU", "journal-ref": "Computer Vision and Image Understanding, Volume 140, November\n  2015, Pages 127-143", "doi": "10.1016/j.cviu.2015.05.011", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the structure-and-motion problem, that requires to find\ncamera motion and 3D struc- ture from point matches. A new pipeline, dubbed\nSamantha, is presented, that departs from the prevailing sequential paradigm\nand embraces instead a hierarchical approach. This method has several\nadvantages, like a provably lower computational complexity, which is necessary\nto achieve true scalability, and better error containment, leading to more\nstability and less drift. Moreover, a practical autocalibration procedure\nallows to process images without ancillary information. Experiments with real\ndata assess the accuracy and the computational efficiency of the method.\n", "versions": [{"version": "v1", "created": "Mon, 1 Jun 2015 08:55:54 GMT"}], "update_date": "2015-10-30", "authors_parsed": [["Toldo", "Roberto", ""], ["Gherardi", "Riccardo", ""], ["Farenzena", "Michela", ""], ["Fusiello", "Andrea", ""]]}, {"id": "1506.00473", "submitter": "Patrick Heas", "authors": "Patrick H\\'eas, Ang\\'elique Dr\\'emeau, C\\'edric Herzet", "title": "An Efficient Algorithm for Video Super-Resolution Based On a Sequential\n  Model", "comments": "37 pages, SIAM Journal on Imaging Sciences, 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we propose a novel procedure for video super-resolution, that\nis the recovery of a sequence of high-resolution images from its low-resolution\ncounterpart. Our approach is based on a \"sequential\" model (i.e., each\nhigh-resolution frame is supposed to be a displaced version of the preceding\none) and considers the use of sparsity-enforcing priors. Both the recovery of\nthe high-resolution images and the motion fields relating them is tackled. This\nleads to a large-dimensional, non-convex and non-smooth problem. We propose an\nalgorithmic framework to address the latter. Our approach relies on fast\ngradient evaluation methods and modern optimization techniques for\nnon-differentiable/non-convex problems. Unlike some other previous works, we\nshow that there exists a provably-convergent method with a complexity linear in\nthe problem dimensions. We assess the proposed optimization method on {several\nvideo benchmarks and emphasize its good performance with respect to the state\nof the art.}\n", "versions": [{"version": "v1", "created": "Mon, 1 Jun 2015 12:33:41 GMT"}, {"version": "v2", "created": "Tue, 27 Oct 2015 09:54:38 GMT"}, {"version": "v3", "created": "Mon, 15 Feb 2016 08:22:26 GMT"}], "update_date": "2016-02-16", "authors_parsed": [["H\u00e9as", "Patrick", ""], ["Dr\u00e9meau", "Ang\u00e9lique", ""], ["Herzet", "C\u00e9dric", ""]]}, {"id": "1506.00481", "submitter": "Weilin Huang", "authors": "Weilin Huang and Hujun Yin", "title": "Robust Face Recognition with Structural Binary Gradient Patterns", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a computationally efficient yet powerful binary framework\nfor robust facial representation based on image gradients. It is termed as\nstructural binary gradient patterns (SBGP). To discover underlying local\nstructures in the gradient domain, we compute image gradients from multiple\ndirections and simplify them into a set of binary strings. The SBGP is derived\nfrom certain types of these binary strings that have meaningful local\nstructures and are capable of resembling fundamental textural information. They\ndetect micro orientational edges and possess strong orientation and locality\ncapabilities, thus enabling great discrimination. The SBGP also benefits from\nthe advantages of the gradient domain and exhibits profound robustness against\nillumination variations. The binary strategy realized by pixel correlations in\na small neighborhood substantially simplifies the computational complexity and\nachieves extremely efficient processing with only 0.0032s in Matlab for a\ntypical face image. Furthermore, the discrimination power of the SBGP can be\nenhanced on a set of defined orientational image gradient magnitudes, further\nenforcing locality and orientation. Results of extensive experiments on various\nbenchmark databases illustrate significant improvements of the SBGP based\nrepresentations over the existing state-of-the-art local descriptors in the\nterms of discrimination, robustness and complexity. Codes for the SBGP methods\nwill be available at\nhttp://www.eee.manchester.ac.uk/research/groups/sisp/software/.\n", "versions": [{"version": "v1", "created": "Mon, 1 Jun 2015 12:57:07 GMT"}], "update_date": "2015-06-02", "authors_parsed": [["Huang", "Weilin", ""], ["Yin", "Hujun", ""]]}, {"id": "1506.00511", "submitter": "Jimmy Ba", "authors": "Jimmy Ba, Kevin Swersky, Sanja Fidler and Ruslan Salakhutdinov", "title": "Predicting Deep Zero-Shot Convolutional Neural Networks using Textual\n  Descriptions", "comments": "Correct the typos in table 1 regarding [5]. To appear in ICCV 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the main challenges in Zero-Shot Learning of visual categories is\ngathering semantic attributes to accompany images. Recent work has shown that\nlearning from textual descriptions, such as Wikipedia articles, avoids the\nproblem of having to explicitly define these attributes. We present a new model\nthat can classify unseen categories from their textual description.\nSpecifically, we use text features to predict the output weights of both the\nconvolutional and the fully connected layers in a deep convolutional neural\nnetwork (CNN). We take advantage of the architecture of CNNs and learn features\nat different layers, rather than just learning an embedding space for both\nmodalities, as is common with existing approaches. The proposed model also\nallows us to automatically generate a list of pseudo- attributes for each\nvisual category consisting of words from Wikipedia articles. We train our\nmodels end-to-end us- ing the Caltech-UCSD bird and flower datasets and\nevaluate both ROC and Precision-Recall curves. Our empirical results show that\nthe proposed model significantly outperforms previous methods.\n", "versions": [{"version": "v1", "created": "Mon, 1 Jun 2015 14:37:06 GMT"}, {"version": "v2", "created": "Fri, 25 Sep 2015 16:20:44 GMT"}], "update_date": "2015-09-28", "authors_parsed": [["Ba", "Jimmy", ""], ["Swersky", "Kevin", ""], ["Fidler", "Sanja", ""], ["Salakhutdinov", "Ruslan", ""]]}, {"id": "1506.00527", "submitter": "Gianluigi Ciocca", "authors": "Simone Bianco, Gianluigi Ciocca", "title": "User Preferences Modeling and Learning for Pleasing Photo Collage\n  Generation", "comments": "To be published in ACM Transactions on Multimedia Computing,\n  Communications, and Applications (TOMM)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.CV cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we consider how to automatically create pleasing photo collages\ncreated by placing a set of images on a limited canvas area. The task is\nformulated as an optimization problem. Differently from existing\nstate-of-the-art approaches, we here exploit subjective experiments to model\nand learn pleasantness from user preferences. To this end, we design an\nexperimental framework for the identification of the criteria that need to be\ntaken into account to generate a pleasing photo collage. Five different\nthematic photo datasets are used to create collages using state-of-the-art\ncriteria. A first subjective experiment where several subjects evaluated the\ncollages, emphasizes that different criteria are involved in the subjective\ndefinition of pleasantness. We then identify new global and local criteria and\ndesign algorithms to quantify them. The relative importance of these criteria\nare automatically learned by exploiting the user preferences, and new collages\nare generated. To validate our framework, we performed several psycho-visual\nexperiments involving different users. The results shows that the proposed\nframework allows to learn a novel computational model which effectively encodes\nan inter-user definition of pleasantness. The learned definition of\npleasantness generalizes well to new photo datasets of different themes and\nsizes not used in the learning. Moreover, compared with two state of the art\napproaches, the collages created using our framework are preferred by the\nmajority of the users.\n", "versions": [{"version": "v1", "created": "Mon, 1 Jun 2015 15:20:29 GMT"}], "update_date": "2015-06-02", "authors_parsed": [["Bianco", "Simone", ""], ["Ciocca", "Gianluigi", ""]]}, {"id": "1506.00575", "submitter": "Nicolas Boumal", "authors": "Nicolas Boumal", "title": "A Riemannian low-rank method for optimization over semidefinite matrices\n  with block-diagonal constraints", "comments": "37 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.CV stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new algorithm to solve optimization problems of the form $\\min\nf(X)$ for a smooth function $f$ under the constraints that $X$ is positive\nsemidefinite and the diagonal blocks of $X$ are small identity matrices. Such\nproblems often arise as the result of relaxing a rank constraint (lifting). In\nparticular, many estimation tasks involving phases, rotations, orthonormal\nbases or permutations fit in this framework, and so do certain relaxations of\ncombinatorial problems such as Max-Cut. The proposed algorithm exploits the\nfacts that (1) such formulations admit low-rank solutions, and (2) their\nrank-restricted versions are smooth optimization problems on a Riemannian\nmanifold. Combining insights from both the Riemannian and the convex geometries\nof the problem, we characterize when second-order critical points of the smooth\nproblem reveal KKT points of the semidefinite problem. We compare against state\nof the art, mature software and find that, on certain interesting problem\ninstances, what we call the staircase method is orders of magnitude faster, is\nmore accurate and scales better. Code is available.\n", "versions": [{"version": "v1", "created": "Mon, 1 Jun 2015 17:17:49 GMT"}, {"version": "v2", "created": "Wed, 6 Jan 2016 15:01:39 GMT"}], "update_date": "2016-01-07", "authors_parsed": [["Boumal", "Nicolas", ""]]}, {"id": "1506.00711", "submitter": "Babak Saleh", "authors": "Ahmed Elgammal and Babak Saleh", "title": "Quantifying Creativity in Art Networks", "comments": "This paper will be published in the sixth International Conference on\n  Computational Creativity (ICCC) June 29-July 2nd 2015, Park City, Utah, USA.\n  This arXiv version is an extended version of the conference paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CV cs.CY cs.MM cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Can we develop a computer algorithm that assesses the creativity of a\npainting given its context within art history? This paper proposes a novel\ncomputational framework for assessing the creativity of creative products, such\nas paintings, sculptures, poetry, etc. We use the most common definition of\ncreativity, which emphasizes the originality of the product and its influential\nvalue. The proposed computational framework is based on constructing a network\nbetween creative products and using this network to infer about the originality\nand influence of its nodes. Through a series of transformations, we construct a\nCreativity Implication Network. We show that inference about creativity in this\nnetwork reduces to a variant of network centrality problems which can be solved\nefficiently. We apply the proposed framework to the task of quantifying\ncreativity of paintings (and sculptures). We experimented on two datasets with\nover 62K paintings to illustrate the behavior of the proposed framework. We\nalso propose a methodology for quantitatively validating the results of the\nproposed algorithm, which we call the \"time machine experiment\".\n", "versions": [{"version": "v1", "created": "Tue, 2 Jun 2015 00:20:54 GMT"}], "update_date": "2015-07-31", "authors_parsed": [["Elgammal", "Ahmed", ""], ["Saleh", "Babak", ""]]}, {"id": "1506.00752", "submitter": "Ira Kemelmacher-Shlizerman", "authors": "Supasorn Suwajanakorn, Ira Kemelmacher-Shlizerman, Steve Seitz", "title": "What Makes Kevin Spacey Look Like Kevin Spacey", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We reconstruct a controllable model of a person from a large photo collection\nthat captures his or her {\\em persona}, i.e., physical appearance and behavior.\nThe ability to operate on unstructured photo collections enables modeling a\nhuge number of people, including celebrities and other well photographed people\nwithout requiring them to be scanned. Moreover, we show the ability to drive or\n{\\em puppeteer} the captured person B using any other video of a different\nperson A. In this scenario, B acts out the role of person A, but retains\nhis/her own personality and character. Our system is based on a novel\ncombination of 3D face reconstruction, tracking, alignment, and multi-texture\nmodeling, applied to the puppeteering problem. We demonstrate convincing\nresults on a large variety of celebrities derived from Internet imagery and\nvideo.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jun 2015 05:06:45 GMT"}], "update_date": "2015-06-03", "authors_parsed": [["Suwajanakorn", "Supasorn", ""], ["Kemelmacher-Shlizerman", "Ira", ""], ["Seitz", "Steve", ""]]}, {"id": "1506.00761", "submitter": "Thanh The Van", "authors": "Thanh The Van, Thanh Manh Le", "title": "Image Retrieval Based on Binary Signature ang S-kGraph", "comments": "17 pages, 9 figures", "journal-ref": "Annales Univ. Sci. Budapest, Sect. Comp., 43(2014), pp.105-122", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce an optimum approach for querying similar images\non large digital-image databases. Our work is based on RBIR (region-based image\nretrieval) method which uses multiple regions as the key to retrieval images.\nThis method significantly improves the accuracy of queries. However, this also\nincreases the cost of computing. To reduce this expensive computational cost,\nwe implement binary signature encoder which maps an image to its identification\nin binary. In order to fasten the lookup, binary signatures of images are\nclassified by the help of S-kGraph. Finally, our work is evaluated on COREL's\nimages.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jun 2015 06:12:41 GMT"}], "update_date": "2015-06-03", "authors_parsed": [["Van", "Thanh The", ""], ["Le", "Thanh Manh", ""]]}, {"id": "1506.00768", "submitter": "Madhu Khurana", "authors": "Madhu Khurana and Vikas Saxena", "title": "Soft Computing Techniques for Change Detection in remotely sensed images\n  : A Review", "comments": "9 pages, 1 table, 1 figure", "journal-ref": "International Journal of Computer Science Issues, Volume 12, Issue\n  2, March 2015, pp 245-253", "doi": null, "report-no": null, "categories": "cs.NE cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the advent of remote sensing satellites, a huge repository of remotely\nsensed images is available. Change detection in remotely sensed images has been\nan active research area as it helps us understand the transitions that are\ntaking place on the Earths surface. This paper discusses the methods and their\nclassifications proposed by various researchers for change detection. Since use\nof soft computing based techniques are now very popular among research\ncommunity, this paper also presents a classification based on learning\ntechniques used in soft-computing methods for change detection.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jun 2015 06:38:54 GMT"}, {"version": "v2", "created": "Tue, 25 Sep 2018 07:45:40 GMT"}], "update_date": "2018-09-26", "authors_parsed": [["Khurana", "Madhu", ""], ["Saxena", "Vikas", ""]]}, {"id": "1506.00815", "submitter": "Yuhuang Hu", "authors": "Yuhuang Hu, M.S. Ishwarya, Chu Kiong Loo", "title": "Classify Images with Conceptor Network", "comments": "This paper has been withdrawn by the author due to a crucial sign\n  error in experiments", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article demonstrates a new conceptor network based classifier in\nclassifying images. Mathematical descriptions and analysis are presented.\nVarious tests are experimented using three benchmark datasets: MNIST, CIFAR-10\nand CIFAR-100. The experiments displayed that conceptor network can offer\nsuperior results and flexible configurations than conventional classifiers such\nas Softmax Regression and Support Vector Machine (SVM).\n", "versions": [{"version": "v1", "created": "Tue, 2 Jun 2015 09:49:45 GMT"}, {"version": "v2", "created": "Wed, 3 Jun 2015 13:57:14 GMT"}, {"version": "v3", "created": "Sat, 6 Jun 2015 16:58:41 GMT"}, {"version": "v4", "created": "Fri, 12 Jun 2015 01:13:06 GMT"}], "update_date": "2015-06-15", "authors_parsed": [["Hu", "Yuhuang", ""], ["Ishwarya", "M. S.", ""], ["Loo", "Chu Kiong", ""]]}, {"id": "1506.00925", "submitter": "Catarina Runa Miranda M.D.", "authors": "Catarina Runa Miranda, Pedro Mendes, Pedro Coelho, Xenxo Alvarez,\n  Jo\\~ao Freitas, Miguel Sales Dias, Ver\\'onica Costa Orvalho", "title": "Facial Expressions Tracking and Recognition: Database Protocols for\n  Systems Validation and Evaluation", "comments": "10 pages, 6 images, Computers & Graphics", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Each human face is unique. It has its own shape, topology, and distinguishing\nfeatures. As such, developing and testing facial tracking systems are\nchallenging tasks. The existing face recognition and tracking algorithms in\nComputer Vision mainly specify concrete situations according to particular\ngoals and applications, requiring validation methodologies with data that fits\ntheir purposes. However, a database that covers all possible variations of\nexternal and factors does not exist, increasing researchers' work in acquiring\ntheir own data or compiling groups of databases.\n  To address this shortcoming, we propose a methodology for facial data\nacquisition through definition of fundamental variables, such as subject\ncharacteristics, acquisition hardware, and performance parameters. Following\nthis methodology, we also propose two protocols that allow the capturing of\nfacial behaviors under uncontrolled and real-life situations. As validation, we\nexecuted both protocols which lead to creation of two sample databases: FdMiee\n(Facial database with Multi input, expressions, and environments) and FACIA\n(Facial Multimodal database driven by emotional induced acting).\n  Using different types of hardware, FdMiee captures facial information under\nenvironmental and facial behaviors variations. FACIA is an extension of FdMiee\nintroducing a pipeline to acquire additional facial behaviors and speech using\nan emotion-acting method. Therefore, this work eases the creation of adaptable\ndatabase according to algorithm's requirements and applications, leading to\nsimplified validation and testing processes.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jun 2015 15:33:10 GMT"}], "update_date": "2015-06-03", "authors_parsed": [["Miranda", "Catarina Runa", ""], ["Mendes", "Pedro", ""], ["Coelho", "Pedro", ""], ["Alvarez", "Xenxo", ""], ["Freitas", "Jo\u00e3o", ""], ["Dias", "Miguel Sales", ""], ["Orvalho", "Ver\u00f3nica Costa", ""]]}, {"id": "1506.01072", "submitter": "Xinyu Wu", "authors": "Xinyu Wu, Vishal Saxena, Kehan Zhu", "title": "Homogeneous Spiking Neuromorphic System for Real-World Pattern\n  Recognition", "comments": "This is a preprint of an article accepted for publication in IEEE\n  Journal on Emerging and Selected Topics in Circuits and Systems, vol 5, no.\n  2, June 2015", "journal-ref": "IEEE Journal on Emerging and Selected Topics in Circuits and\n  Systems, vol 5, no. 2, June 2015", "doi": "10.1109/JETCAS.2015.2433552", "report-no": null, "categories": "cs.NE cs.AI cs.CV cs.ET", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A neuromorphic chip that combines CMOS analog spiking neurons and memristive\nsynapses offers a promising solution to brain-inspired computing, as it can\nprovide massive neural network parallelism and density. Previous hybrid analog\nCMOS-memristor approaches required extensive CMOS circuitry for training, and\nthus eliminated most of the density advantages gained by the adoption of\nmemristor synapses. Further, they used different waveforms for pre and\npost-synaptic spikes that added undesirable circuit overhead. Here we describe\na hardware architecture that can feature a large number of memristor synapses\nto learn real-world patterns. We present a versatile CMOS neuron that combines\nintegrate-and-fire behavior, drives passive memristors and implements\ncompetitive learning in a compact circuit module, and enables in-situ\nplasticity in the memristor synapses. We demonstrate handwritten-digits\nrecognition using the proposed architecture using transistor-level circuit\nsimulations. As the described neuromorphic architecture is homogeneous, it\nrealizes a fundamental building block for large-scale energy-efficient\nbrain-inspired silicon chips that could lead to next-generation cognitive\ncomputing.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jun 2015 21:35:51 GMT"}, {"version": "v2", "created": "Mon, 8 Jun 2015 20:32:49 GMT"}], "update_date": "2015-06-11", "authors_parsed": [["Wu", "Xinyu", ""], ["Saxena", "Vishal", ""], ["Zhu", "Kehan", ""]]}, {"id": "1506.01092", "submitter": "Seungjin Choi", "authors": "Saehoon Kim and Seungjin Choi", "title": "Bilinear Random Projections for Locality-Sensitive Binary Codes", "comments": "11 pages, 23 figures, CVPR-2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Locality-sensitive hashing (LSH) is a popular data-independent indexing\nmethod for approximate similarity search, where random projections followed by\nquantization hash the points from the database so as to ensure that the\nprobability of collision is much higher for objects that are close to each\nother than for those that are far apart. Most of high-dimensional visual\ndescriptors for images exhibit a natural matrix structure. When visual\ndescriptors are represented by high-dimensional feature vectors and long binary\ncodes are assigned, a random projection matrix requires expensive complexities\nin both space and time. In this paper we analyze a bilinear random projection\nmethod where feature matrices are transformed to binary codes by two smaller\nrandom projection matrices. We base our theoretical analysis on extending\nRaginsky and Lazebnik's result where random Fourier features are composed with\nrandom binary quantizers to form locality sensitive binary codes. To this end,\nwe answer the following two questions: (1) whether a bilinear random projection\nalso yields similarity-preserving binary codes; (2) whether a bilinear random\nprojection yields performance gain or loss, compared to a large linear\nprojection. Regarding the first question, we present upper and lower bounds on\nthe expected Hamming distance between binary codes produced by bilinear random\nprojections. In regards to the second question, we analyze the upper and lower\nbounds on covariance between two bits of binary codes, showing that the\ncorrelation between two bits is small. Numerical experiments on MNIST and\nFlickr45K datasets confirm the validity of our method.\n", "versions": [{"version": "v1", "created": "Wed, 3 Jun 2015 00:30:26 GMT"}], "update_date": "2015-06-04", "authors_parsed": [["Kim", "Saehoon", ""], ["Choi", "Seungjin", ""]]}, {"id": "1506.01115", "submitter": "Alexandros-Stavros Iliopoulos", "authors": "Alexandros-Stavros Iliopoulos, Tiancheng Liu, Xiaobai Sun", "title": "Hyperspectral Image Classification and Clutter Detection via Multiple\n  Structural Embeddings and Dimension Reductions", "comments": "13 pages, 6 figures (30 images), submitted to International\n  Conference on Computer Vision (ICCV) 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new and effective approach for Hyperspectral Image (HSI)\nclassification and clutter detection, overcoming a few long-standing challenges\npresented by HSI data characteristics. Residing in a high-dimensional spectral\nattribute space, HSI data samples are known to be strongly correlated in their\nspectral signatures, exhibit nonlinear structure due to several physical laws,\nand contain uncertainty and noise from multiple sources. In the presented\napproach, we generate an adaptive, structurally enriched representation\nenvironment, and employ the locally linear embedding (LLE) in it. There are two\nstructure layers external to LLE. One is feature space embedding: the HSI data\nattributes are embedded into a discriminatory feature space where\nspatio-spectral coherence and distinctive structures are distilled and\nexploited to mitigate various difficulties encountered in the native\nhyperspectral attribute space. The other structure layer encloses the ranges of\nalgorithmic parameters for LLE and feature embedding, and supports a\nmultiplexing and integrating scheme for contending with multi-source\nuncertainty. Experiments on two commonly used HSI datasets with a small number\nof learning samples have rendered remarkably high-accuracy classification\nresults, as well as distinctive maps of detected clutter regions.\n", "versions": [{"version": "v1", "created": "Wed, 3 Jun 2015 04:04:43 GMT"}], "update_date": "2015-06-04", "authors_parsed": [["Iliopoulos", "Alexandros-Stavros", ""], ["Liu", "Tiancheng", ""], ["Sun", "Xiaobai", ""]]}, {"id": "1506.01125", "submitter": "Zhun Zhong", "authors": "Zhun Zhong, Zongmin Li, Runlin Li, Xiaoxia Sun", "title": "Unsupervised domain adaption dictionary learning for visual recognition", "comments": "5 pages, 3 figures, ICIP 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  Over the last years, dictionary learning method has been extensively applied\nto deal with various computer vision recognition applications, and produced\nstate-of-the-art results. However, when the data instances of a target domain\nhave a different distribution than that of a source domain, the dictionary\nlearning method may fail to perform well. In this paper, we address the\ncross-domain visual recognition problem and propose a simple but effective\nunsupervised domain adaption approach, where labeled data are only from source\ndomain. In order to bring the original data in source and target domain into\nthe same distribution, the proposed method forcing nearest coupled data between\nsource and target domain to have identical sparse representations while jointly\nlearning dictionaries for each domain, where the learned dictionaries can\nreconstruct original data in source and target domain respectively. So that\nsparse representations of original data can be used to perform visual\nrecognition tasks. We demonstrate the effectiveness of our approach on standard\ndatasets. Our method performs on par or better than competitive\nstate-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Wed, 3 Jun 2015 05:21:37 GMT"}], "update_date": "2015-06-04", "authors_parsed": [["Zhong", "Zhun", ""], ["Li", "Zongmin", ""], ["Li", "Runlin", ""], ["Sun", "Xiaoxia", ""]]}, {"id": "1506.01144", "submitter": "Chunhua Shen", "authors": "Qi Wu, Chunhua Shen, Lingqiao Liu, Anthony Dick, Anton van den Hengel", "title": "What value do explicit high level concepts have in vision to language\n  problems?", "comments": "Accepted to IEEE Conf. Computer Vision and Pattern Recognition 2016.\n  Fixed title", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Much of the recent progress in Vision-to-Language (V2L) problems has been\nachieved through a combination of Convolutional Neural Networks (CNNs) and\nRecurrent Neural Networks (RNNs). This approach does not explicitly represent\nhigh-level semantic concepts, but rather seeks to progress directly from image\nfeatures to text. We propose here a method of incorporating high-level concepts\ninto the very successful CNN-RNN approach, and show that it achieves a\nsignificant improvement on the state-of-the-art performance in both image\ncaptioning and visual question answering. We also show that the same mechanism\ncan be used to introduce external semantic information and that doing so\nfurther improves performance. In doing so we provide an analysis of the value\nof high level semantic information in V2L problems.\n", "versions": [{"version": "v1", "created": "Wed, 3 Jun 2015 07:06:11 GMT"}, {"version": "v2", "created": "Sat, 6 Jun 2015 03:16:41 GMT"}, {"version": "v3", "created": "Sun, 4 Oct 2015 03:41:04 GMT"}, {"version": "v4", "created": "Fri, 9 Oct 2015 03:21:37 GMT"}, {"version": "v5", "created": "Thu, 14 Apr 2016 08:05:03 GMT"}, {"version": "v6", "created": "Thu, 28 Apr 2016 04:59:36 GMT"}], "update_date": "2016-04-29", "authors_parsed": [["Wu", "Qi", ""], ["Shen", "Chunhua", ""], ["Liu", "Lingqiao", ""], ["Dick", "Anthony", ""], ["Hengel", "Anton van den", ""]]}, {"id": "1506.01151", "submitter": "Mathieu Aubry", "authors": "Mathieu Aubry and Bryan Russell", "title": "Understanding deep features with computer-generated imagery", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce an approach for analyzing the variation of features generated by\nconvolutional neural networks (CNNs) with respect to scene factors that occur\nin natural images. Such factors may include object style, 3D viewpoint, color,\nand scene lighting configuration. Our approach analyzes CNN feature responses\ncorresponding to different scene factors by controlling for them via rendering\nusing a large database of 3D CAD models. The rendered images are presented to a\ntrained CNN and responses for different layers are studied with respect to the\ninput scene factors. We perform a decomposition of the responses based on\nknowledge of the input scene factors and analyze the resulting components. In\nparticular, we quantify their relative importance in the CNN responses and\nvisualize them using principal component analysis. We show qualitative and\nquantitative results of our study on three CNNs trained on large image\ndatasets: AlexNet, Places, and Oxford VGG. We observe important differences\nacross the networks and CNN layers for different scene factors and object\ncategories. Finally, we demonstrate that our analysis based on\ncomputer-generated imagery translates to the network representation of natural\nimages.\n", "versions": [{"version": "v1", "created": "Wed, 3 Jun 2015 07:41:14 GMT"}], "update_date": "2015-06-04", "authors_parsed": [["Aubry", "Mathieu", ""], ["Russell", "Bryan", ""]]}, {"id": "1506.01165", "submitter": "Thanh The Van", "authors": "Thanh Manh Le, Thanh The Van", "title": "Image Retrieval System Base on EMD Similarity Measure and S-Tree", "comments": "14 pages, 3 figures, Appendix", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper approaches the binary signature for each image based on the\npercentage of the pixels in each color images, at the same time the paper\nbuilds a similar measure between images based on EMD (Earth Mover's Distance).\nBesides, the paper proceeded to create the S-tree based on the similar measure\nEMD to store the image's binary signatures to quickly query image signature\ndata. From there, the paper build an image retrieval algorithm and CBIR\n(Content-Based Image Retrieval) based on a similar measure EMD and S-tree.\nBased on this theory, the paper proceeded to build application and experimental\nassessment of the process of querying image on the database system which have\nover 10,000 images.\n", "versions": [{"version": "v1", "created": "Wed, 3 Jun 2015 08:50:13 GMT"}], "update_date": "2015-06-04", "authors_parsed": [["Le", "Thanh Manh", ""], ["Van", "Thanh The", ""]]}, {"id": "1506.01166", "submitter": "Thanh The Van", "authors": "Thanh The Van, Thanh Manh Le", "title": "Color Image Retrieval Using Fuzzy Measure Hamming and S-Tree", "comments": "9 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This chapter approaches the image retrieval system on the base of the colors\nof image. It creates fuzzy signature to describe the color of image on color\nspace HSV and builds fuzzy Hamming distance (FHD) to evaluate the similarity\nbetween the images. In order to reduce the storage space and speed up the\nsearch of similar images, it aims to create S-tree to store fuzzy signature\nrelies on FHD and builds image retrieval algorithm on S-tree. Then, it provides\nthe content-based image retrieval (CBIR) and an image retrieval method on FHD\nand S-tree. Last but not least, based on this theory, it also presents an\napplication and experimental assessment of the process of querying similar\nimage on the database system over 10,000 images.\n", "versions": [{"version": "v1", "created": "Wed, 3 Jun 2015 08:55:12 GMT"}], "update_date": "2015-06-04", "authors_parsed": [["Van", "Thanh The", ""], ["Le", "Thanh Manh", ""]]}, {"id": "1506.01186", "submitter": "Leslie Smith", "authors": "Leslie N. Smith", "title": "Cyclical Learning Rates for Training Neural Networks", "comments": "Presented at WACV 2017; see https://github.com/bckenstler/CLR for\n  instructions to implement CLR in Keras", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is known that the learning rate is the most important hyper-parameter to\ntune for training deep neural networks. This paper describes a new method for\nsetting the learning rate, named cyclical learning rates, which practically\neliminates the need to experimentally find the best values and schedule for the\nglobal learning rates. Instead of monotonically decreasing the learning rate,\nthis method lets the learning rate cyclically vary between reasonable boundary\nvalues. Training with cyclical learning rates instead of fixed values achieves\nimproved classification accuracy without a need to tune and often in fewer\niterations. This paper also describes a simple way to estimate \"reasonable\nbounds\" -- linearly increasing the learning rate of the network for a few\nepochs. In addition, cyclical learning rates are demonstrated on the CIFAR-10\nand CIFAR-100 datasets with ResNets, Stochastic Depth networks, and DenseNets,\nand the ImageNet dataset with the AlexNet and GoogLeNet architectures. These\nare practical tools for everyone who trains neural networks.\n", "versions": [{"version": "v1", "created": "Wed, 3 Jun 2015 09:54:31 GMT"}, {"version": "v2", "created": "Fri, 5 Jun 2015 20:40:18 GMT"}, {"version": "v3", "created": "Wed, 26 Oct 2016 19:07:58 GMT"}, {"version": "v4", "created": "Thu, 29 Dec 2016 15:20:01 GMT"}, {"version": "v5", "created": "Thu, 23 Mar 2017 11:38:19 GMT"}, {"version": "v6", "created": "Tue, 4 Apr 2017 11:34:46 GMT"}], "update_date": "2017-04-05", "authors_parsed": [["Smith", "Leslie N.", ""]]}, {"id": "1506.01195", "submitter": "Tianyi Liu", "authors": "Tianyi Liu, Shuangsang Fang, Yuehui Zhao, Peng Wang, Jun Zhang", "title": "Implementation of Training Convolutional Neural Networks", "comments": "10 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning refers to the shining branch of machine learning that is based\non learning levels of representations. Convolutional Neural Networks (CNN) is\none kind of deep neural network. It can study concurrently. In this article, we\ngave a detailed analysis of the process of CNN algorithm both the forward\nprocess and back propagation. Then we applied the particular convolutional\nneural network to implement the typical face recognition problem by java. Then,\na parallel strategy was proposed in section4. In addition, by measuring the\nactual time of forward and backward computing, we analysed the maximal speed up\nand parallel efficiency theoretically.\n", "versions": [{"version": "v1", "created": "Wed, 3 Jun 2015 10:18:49 GMT"}, {"version": "v2", "created": "Thu, 4 Jun 2015 02:10:39 GMT"}], "update_date": "2015-06-05", "authors_parsed": [["Liu", "Tianyi", ""], ["Fang", "Shuangsang", ""], ["Zhao", "Yuehui", ""], ["Wang", "Peng", ""], ["Zhang", "Jun", ""]]}, {"id": "1506.01342", "submitter": "Aruni RoyChowdhury", "authors": "Aruni RoyChowdhury, Tsung-Yu Lin, Subhransu Maji, Erik Learned-Miller", "title": "One-to-many face recognition with bilinear CNNs", "comments": "Published version at WACV 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The recent explosive growth in convolutional neural network (CNN) research\nhas produced a variety of new architectures for deep learning. One intriguing\nnew architecture is the bilinear CNN (B-CNN), which has shown dramatic\nperformance gains on certain fine-grained recognition problems [15]. We apply\nthis new CNN to the challenging new face recognition benchmark, the IARPA Janus\nBenchmark A (IJB-A) [12]. It features faces from a large number of identities\nin challenging real-world conditions. Because the face images were not\nidentified automatically using a computerized face detection system, it does\nnot have the bias inherent in such a database. We demonstrate the performance\nof the B-CNN model beginning from an AlexNet-style network pre-trained on\nImageNet. We then show results for fine-tuning using a moderate-sized and\npublic external database, FaceScrub [17]. We also present results with\nadditional fine-tuning on the limited training data provided by the protocol.\nIn each case, the fine-tuned bilinear model shows substantial improvements over\nthe standard CNN. Finally, we demonstrate how a standard CNN pre-trained on a\nlarge face database, the recently released VGG-Face model [20], can be\nconverted into a B-CNN without any additional feature training. This B-CNN\nimproves upon the CNN performance on the IJB-A benchmark, achieving 89.5%\nrank-1 recall.\n", "versions": [{"version": "v1", "created": "Wed, 3 Jun 2015 18:34:41 GMT"}, {"version": "v2", "created": "Thu, 4 Jun 2015 00:41:33 GMT"}, {"version": "v3", "created": "Fri, 5 Jun 2015 21:27:50 GMT"}, {"version": "v4", "created": "Wed, 23 Sep 2015 20:05:39 GMT"}, {"version": "v5", "created": "Mon, 28 Mar 2016 21:32:33 GMT"}], "update_date": "2016-03-30", "authors_parsed": [["RoyChowdhury", "Aruni", ""], ["Lin", "Tsung-Yu", ""], ["Maji", "Subhransu", ""], ["Learned-Miller", "Erik", ""]]}, {"id": "1506.01398", "submitter": "Jismy Alphonse", "authors": "Jismy Alphonse, Biju V. G.", "title": "Recognition of Changes in SAR Images Based on Gauss-Log Ratio and MRFFCM", "comments": "7 pages, 7 figures, 2 tables in International Journal of advanced\n  studies in Computer Science and Engineering (IJASCSE), ISSN : 2278 7917,\n  Volume 4 Issue 5, 2015, www.ijascse.org", "journal-ref": null, "doi": null, "report-no": "page 65-71", "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A modified version of MRFFCM (Markov Random Field Fuzzy C means) based SAR\n(Synthetic aperture Radar) image change detection method is proposed in this\npaper. It involves three steps: Difference Image (DI) generation by using\nGauss-log ratio operator, speckle noise reduction by SRAD (Speckle Reducing\nAnisotropic Diffusion), and the detection of changed regions by using MRFFCM.\nThe proposed method is compared with existing methods such as FCM and MRFFCM\nusing simulated and real SAR images. The measures used for evaluation includes\nOverall Error (OE), Percentage Correct Classification (PCC), Kappa Coefficient\n(KC), Root Mean Square Error (RMSE), and Peak Signal to Noise Ratio (PSNR). The\nresults show that the proposed method is better compared to FCM and MRFFCM\nbased change detection method.\n", "versions": [{"version": "v1", "created": "Wed, 3 Jun 2015 20:38:37 GMT"}], "update_date": "2015-06-05", "authors_parsed": [["Alphonse", "Jismy", ""], ["G.", "Biju V.", ""]]}, {"id": "1506.01437", "submitter": "Paul Hand", "authors": "Paul Hand and Choongbum Lee and Vladislav Voroninski", "title": "ShapeFit: Exact location recovery from corrupted pairwise directions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IT math.CO math.IT math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Let $t_1,\\ldots,t_n \\in \\mathbb{R}^d$ and consider the location recovery\nproblem: given a subset of pairwise direction observations $\\{(t_i - t_j) /\n\\|t_i - t_j\\|_2\\}_{i<j \\in [n] \\times [n]}$, where a constant fraction of these\nobservations are arbitrarily corrupted, find $\\{t_i\\}_{i=1}^n$ up to a global\ntranslation and scale. We propose a novel algorithm for the location recovery\nproblem, which consists of a simple convex program over $dn$ real variables. We\nprove that this program recovers a set of $n$ i.i.d. Gaussian locations exactly\nand with high probability if the observations are given by an \\erdosrenyi\ngraph, $d$ is large enough, and provided that at most a constant fraction of\nobservations involving any particular location are adversarially corrupted. We\nalso prove that the program exactly recovers Gaussian locations for $d=3$ if\nthe fraction of corrupted observations at each location is, up to\npoly-logarithmic factors, at most a constant. Both of these recovery theorems\nare based on a set of deterministic conditions that we prove are sufficient for\nexact recovery.\n", "versions": [{"version": "v1", "created": "Thu, 4 Jun 2015 00:16:20 GMT"}, {"version": "v2", "created": "Sat, 4 Jul 2015 04:51:53 GMT"}], "update_date": "2015-07-07", "authors_parsed": [["Hand", "Paul", ""], ["Lee", "Choongbum", ""], ["Voroninski", "Vladislav", ""]]}, {"id": "1506.01472", "submitter": "Dibya Jyoti Bora", "authors": "Dibya Jyoti Bora, Anil Kumar Gupta, Fayaz Ahmad Khan", "title": "Comparing the Performance of L*A*B* and HSV Color Spaces with Respect to\n  Color Image Segmentation", "comments": "11 pages, 19 figures in International Journal of Emerging Technology\n  and Advanced Engineering,Volume 5, Issue 2, February 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Color image segmentation is a very emerging topic for image processing\nresearch. Since it has the ability to present the result in a way that is much\nmore close to the human yes perceive, so todays more research is going on this\narea. Choosing a proper color space is a very important issue for color image\nsegmentation process. Generally LAB and HSV are the two frequently chosen color\nspaces. In this paper a comparative analysis is performed between these two\ncolor spaces with respect to color image segmentation. For measuring their\nperformance, we consider the parameters: mse and psnr . It is found that HSV\ncolor space is performing better than LAB.\n", "versions": [{"version": "v1", "created": "Thu, 4 Jun 2015 06:37:14 GMT"}], "update_date": "2015-06-05", "authors_parsed": [["Bora", "Dibya Jyoti", ""], ["Gupta", "Anil Kumar", ""], ["Khan", "Fayaz Ahmad", ""]]}, {"id": "1506.01497", "submitter": "Kaiming He", "authors": "Shaoqing Ren, Kaiming He, Ross Girshick, Jian Sun", "title": "Faster R-CNN: Towards Real-Time Object Detection with Region Proposal\n  Networks", "comments": "Extended tech report", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  State-of-the-art object detection networks depend on region proposal\nalgorithms to hypothesize object locations. Advances like SPPnet and Fast R-CNN\nhave reduced the running time of these detection networks, exposing region\nproposal computation as a bottleneck. In this work, we introduce a Region\nProposal Network (RPN) that shares full-image convolutional features with the\ndetection network, thus enabling nearly cost-free region proposals. An RPN is a\nfully convolutional network that simultaneously predicts object bounds and\nobjectness scores at each position. The RPN is trained end-to-end to generate\nhigh-quality region proposals, which are used by Fast R-CNN for detection. We\nfurther merge RPN and Fast R-CNN into a single network by sharing their\nconvolutional features---using the recently popular terminology of neural\nnetworks with 'attention' mechanisms, the RPN component tells the unified\nnetwork where to look. For the very deep VGG-16 model, our detection system has\na frame rate of 5fps (including all steps) on a GPU, while achieving\nstate-of-the-art object detection accuracy on PASCAL VOC 2007, 2012, and MS\nCOCO datasets with only 300 proposals per image. In ILSVRC and COCO 2015\ncompetitions, Faster R-CNN and RPN are the foundations of the 1st-place winning\nentries in several tracks. Code has been made publicly available.\n", "versions": [{"version": "v1", "created": "Thu, 4 Jun 2015 07:58:34 GMT"}, {"version": "v2", "created": "Sun, 13 Sep 2015 07:54:00 GMT"}, {"version": "v3", "created": "Wed, 6 Jan 2016 06:30:17 GMT"}], "update_date": "2016-01-07", "authors_parsed": [["Ren", "Shaoqing", ""], ["He", "Kaiming", ""], ["Girshick", "Ross", ""], ["Sun", "Jian", ""]]}, {"id": "1506.01596", "submitter": "Roozbeh Rajabi", "authors": "Roozbeh Rajabi, Hassan Ghassemian", "title": "Multilayer Structured NMF for Spectral Unmixing of Hyperspectral Images", "comments": "4 pages, conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the challenges in hyperspectral data analysis is the presence of mixed\npixels. Mixed pixels are the result of low spatial resolution of hyperspectral\nsensors. Spectral unmixing methods decompose a mixed pixel into a set of\nendmembers and abundance fractions. Due to nonnegativity constraint on\nabundance fraction values, NMF based methods are well suited to this problem.\nIn this paper multilayer NMF has been used to improve the results of NMF\nmethods for spectral unmixing of hyperspectral data under the linear mixing\nframework. Sparseness constraint on both spectral signatures and abundance\nfractions matrices are used in this paper. Evaluation of the proposed algorithm\nis done using synthetic and real datasets in terms of spectral angle and\nabundance angle distances. Results show that the proposed algorithm outperforms\nother previously proposed methods.\n", "versions": [{"version": "v1", "created": "Thu, 4 Jun 2015 13:53:33 GMT"}], "update_date": "2015-06-05", "authors_parsed": [["Rajabi", "Roozbeh", ""], ["Ghassemian", "Hassan", ""]]}, {"id": "1506.01698", "submitter": "Anna Rohrbach", "authors": "Anna Rohrbach and Marcus Rohrbach and Bernt Schiele", "title": "The Long-Short Story of Movie Description", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generating descriptions for videos has many applications including assisting\nblind people and human-robot interaction. The recent advances in image\ncaptioning as well as the release of large-scale movie description datasets\nsuch as MPII Movie Description allow to study this task in more depth. Many of\nthe proposed methods for image captioning rely on pre-trained object classifier\nCNNs and Long-Short Term Memory recurrent networks (LSTMs) for generating\ndescriptions. While image description focuses on objects, we argue that it is\nimportant to distinguish verbs, objects, and places in the challenging setting\nof movie description. In this work we show how to learn robust visual\nclassifiers from the weak annotations of the sentence descriptions. Based on\nthese visual classifiers we learn how to generate a description using an LSTM.\nWe explore different design choices to build and train the LSTM and achieve the\nbest performance to date on the challenging MPII-MD dataset. We compare and\nanalyze our approach and prior work along various dimensions to better\nunderstand the key challenges of the movie description task.\n", "versions": [{"version": "v1", "created": "Thu, 4 Jun 2015 19:45:36 GMT"}], "update_date": "2015-06-05", "authors_parsed": [["Rohrbach", "Anna", ""], ["Rohrbach", "Marcus", ""], ["Schiele", "Bernt", ""]]}, {"id": "1506.01710", "submitter": "Dibya Jyoti Bora", "authors": "Dibya Jyoti Bora, Anil Kumar Gupta", "title": "A Novel Approach Towards Clustering Based Image Segmentation", "comments": "5 pages, 7 figures, 1 table in International Journal of Emerging\n  Science and Engineering, Volume-2 Issue-11, September 2014. arXiv admin note:\n  text overlap with arXiv:1506.01472", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In computer vision, image segmentation is always selected as a major research\ntopic by researchers. Due to its vital rule in image processing, there always\narises the need of a better image segmentation method. Clustering is an\nunsupervised study with its application in almost every field of science and\nengineering. Many researchers used clustering in image segmentation process.\nBut still there requires improvement of such approaches. In this paper, a novel\napproach for clustering based image segmentation is proposed. Here, we give\nimportance on color space and choose lab for this task. The famous hard\nclustering algorithm K-means is used, but as its performance is dependent on\nchoosing a proper distance measure, so, we go for cosine distance measure. Then\nthe segmented image is filtered with sobel filter. The filtered image is\nanalyzed with marker watershed algorithm to have the final segmented result of\nour original image. The MSE and PSNR values are evaluated to observe the\nperformance.\n", "versions": [{"version": "v1", "created": "Thu, 4 Jun 2015 06:44:47 GMT"}], "update_date": "2015-06-08", "authors_parsed": [["Bora", "Dibya Jyoti", ""], ["Gupta", "Anil Kumar", ""]]}, {"id": "1506.01732", "submitter": "Sudeep Pillai", "authors": "Sudeep Pillai, John Leonard", "title": "Monocular SLAM Supported Object Recognition", "comments": "Accepted to appear at Robotics: Science and Systems 2015, Rome, Italy", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we develop a monocular SLAM-aware object recognition system\nthat is able to achieve considerably stronger recognition performance, as\ncompared to classical object recognition systems that function on a\nframe-by-frame basis. By incorporating several key ideas including multi-view\nobject proposals and efficient feature encoding methods, our proposed system is\nable to detect and robustly recognize objects in its environment using a single\nRGB camera in near-constant time. Through experiments, we illustrate the\nutility of using such a system to effectively detect and recognize objects,\nincorporating multiple object viewpoint detections into a unified prediction\nhypothesis. The performance of the proposed recognition system is evaluated on\nthe UW RGB-D Dataset, showing strong recognition performance and scalable\nrun-time performance compared to current state-of-the-art recognition systems.\n", "versions": [{"version": "v1", "created": "Thu, 4 Jun 2015 21:07:56 GMT"}], "update_date": "2015-06-08", "authors_parsed": [["Pillai", "Sudeep", ""], ["Leonard", "John", ""]]}, {"id": "1506.01911", "submitter": "Lionel Pigou", "authors": "Lionel Pigou, A\\\"aron van den Oord, Sander Dieleman, Mieke Van\n  Herreweghe, Joni Dambre", "title": "Beyond Temporal Pooling: Recurrence and Temporal Convolutions for\n  Gesture Recognition in Video", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent studies have demonstrated the power of recurrent neural networks for\nmachine translation, image captioning and speech recognition. For the task of\ncapturing temporal structure in video, however, there still remain numerous\nopen research questions. Current research suggests using a simple temporal\nfeature pooling strategy to take into account the temporal aspect of video. We\ndemonstrate that this method is not sufficient for gesture recognition, where\ntemporal information is more discriminative compared to general video\nclassification tasks. We explore deep architectures for gesture recognition in\nvideo and propose a new end-to-end trainable neural network architecture\nincorporating temporal convolutions and bidirectional recurrence. Our main\ncontributions are twofold; first, we show that recurrence is crucial for this\ntask; second, we show that adding temporal convolutions leads to significant\nimprovements. We evaluate the different approaches on the Montalbano gesture\nrecognition dataset, where we achieve state-of-the-art results.\n", "versions": [{"version": "v1", "created": "Fri, 5 Jun 2015 13:43:01 GMT"}, {"version": "v2", "created": "Mon, 9 Nov 2015 16:20:26 GMT"}, {"version": "v3", "created": "Wed, 10 Feb 2016 16:50:29 GMT"}], "update_date": "2016-02-11", "authors_parsed": [["Pigou", "Lionel", ""], ["Oord", "A\u00e4ron van den", ""], ["Dieleman", "Sander", ""], ["Van Herreweghe", "Mieke", ""], ["Dambre", "Joni", ""]]}, {"id": "1506.01929", "submitter": "Philippe Weinzaepfel", "authors": "Philippe Weinzaepfel, Zaid Harchaoui, Cordelia Schmid", "title": "Learning to track for spatio-temporal action localization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an effective approach for spatio-temporal action localization in\nrealistic videos. The approach first detects proposals at the frame-level and\nscores them with a combination of static and motion CNN features. It then\ntracks high-scoring proposals throughout the video using a\ntracking-by-detection approach. Our tracker relies simultaneously on\ninstance-level and class-level detectors. The tracks are scored using a\nspatio-temporal motion histogram, a descriptor at the track level, in\ncombination with the CNN features. Finally, we perform temporal localization of\nthe action using a sliding-window approach at the track level. We present\nexperimental results for spatio-temporal localization on the UCF-Sports, J-HMDB\nand UCF-101 action localization datasets, where our approach outperforms the\nstate of the art with a margin of 15%, 7% and 12% respectively in mAP.\n", "versions": [{"version": "v1", "created": "Fri, 5 Jun 2015 14:48:46 GMT"}, {"version": "v2", "created": "Sun, 27 Sep 2015 11:21:16 GMT"}], "update_date": "2015-09-29", "authors_parsed": [["Weinzaepfel", "Philippe", ""], ["Harchaoui", "Zaid", ""], ["Schmid", "Cordelia", ""]]}, {"id": "1506.01939", "submitter": "Abdelmajid  Mansour", "authors": "Abdelmajid Hassan Mansour, Gafar Zen Alabdeen Salh, Ali Shaif Alhalemi", "title": "Facial Expressions recognition Based on Principal Component Analysis\n  (PCA)", "comments": "6 pages, 13 figures, 9 tables, Volume 18 Number 5; Dec 2014; pp.\n  188-193", "journal-ref": null, "doi": "10.14445/22312803/IJCTT-V18P143", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The facial expression recognition is an ocular task that can be performed\nwithout human discomfort, is really a speedily growing on the computer research\nfield. There are many applications and programs uses facial expression to\nevaluate human character, judgment, feelings, and viewpoint. The process of\nrecognizing facial expression is a hard task due to the several circumstances\nsuch as facial occlusions, face shape, illumination, face colors, and etc. This\npaper present a PCA methodology to distinguish expressions of faces under\ndifferent circumstances and identifying it. Relies on Eigen faces technique\nusing standard Data base images. So as to overcome the problem of difficulty to\ncomputers to identify the features and expressions of persons.\n", "versions": [{"version": "v1", "created": "Tue, 23 Dec 2014 15:55:41 GMT"}], "update_date": "2015-06-08", "authors_parsed": [["Mansour", "Abdelmajid Hassan", ""], ["Salh", "Gafar Zen Alabdeen", ""], ["Alhalemi", "Ali Shaif", ""]]}, {"id": "1506.02025", "submitter": "Max Jaderberg", "authors": "Max Jaderberg, Karen Simonyan, Andrew Zisserman, Koray Kavukcuoglu", "title": "Spatial Transformer Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional Neural Networks define an exceptionally powerful class of\nmodels, but are still limited by the lack of ability to be spatially invariant\nto the input data in a computationally and parameter efficient manner. In this\nwork we introduce a new learnable module, the Spatial Transformer, which\nexplicitly allows the spatial manipulation of data within the network. This\ndifferentiable module can be inserted into existing convolutional\narchitectures, giving neural networks the ability to actively spatially\ntransform feature maps, conditional on the feature map itself, without any\nextra training supervision or modification to the optimisation process. We show\nthat the use of spatial transformers results in models which learn invariance\nto translation, scale, rotation and more generic warping, resulting in\nstate-of-the-art performance on several benchmarks, and for a number of classes\nof transformations.\n", "versions": [{"version": "v1", "created": "Fri, 5 Jun 2015 19:54:26 GMT"}, {"version": "v2", "created": "Thu, 3 Dec 2015 16:13:08 GMT"}, {"version": "v3", "created": "Thu, 4 Feb 2016 18:08:46 GMT"}], "update_date": "2016-02-05", "authors_parsed": [["Jaderberg", "Max", ""], ["Simonyan", "Karen", ""], ["Zisserman", "Andrew", ""], ["Kavukcuoglu", "Koray", ""]]}, {"id": "1506.02059", "submitter": "Haonan Yu", "authors": "Haonan Yu and Jeffrey Mark Siskind", "title": "Sentence Directed Video Object Codetection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We tackle the problem of video object codetection by leveraging the weak\nsemantic constraint implied by sentences that describe the video content.\nUnlike most existing work that focuses on codetecting large objects which are\nusually salient both in size and appearance, we can codetect objects that are\nsmall or medium sized. Our method assumes no human pose or depth information\nsuch as is required by the most recent state-of-the-art method. We employ weak\nsemantic constraint on the codetection process by pairing the video with\nsentences. Although the semantic information is usually simple and weak, it can\ngreatly boost the performance of our codetection framework by reducing the\nsearch space of the hypothesized object detections. Our experiment demonstrates\nan average IoU score of 0.423 on a new challenging dataset which contains 15\nobject classes and 150 videos with 12,509 frames in total, and an average IoU\nscore of 0.373 on a subset of an existing dataset, originally intended for\nactivity recognition, which contains 5 object classes and 75 videos with 8,854\nframes in total.\n", "versions": [{"version": "v1", "created": "Fri, 5 Jun 2015 20:34:12 GMT"}, {"version": "v2", "created": "Tue, 26 Jan 2016 20:38:42 GMT"}], "update_date": "2016-01-27", "authors_parsed": [["Yu", "Haonan", ""], ["Siskind", "Jeffrey Mark", ""]]}, {"id": "1506.02083", "submitter": "Min Xu", "authors": "Min Xu", "title": "Automatic tracking of protein vesicles", "comments": "Author's master thesis (University of Southern California, May 2009).\n  Adviser: Sergey Lototsky. ISBN: 9781109140439", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the advance of fluorescence imaging technologies, recently cell\nbiologists are able to record the movement of protein vesicles within a living\ncell. Automatic tracking of the movements of these vesicles become key for\nqualitative analysis of dynamics of theses vesicles. In this thesis, we\nformulate such tracking problem as video object tracking problem, and design a\ndynamic programming method for tracking single object. Our experiments on\nsimulation data show that the method can identify a track with high accuracy\nwhich is robust to the choose of tracking parameters and presence of high level\nnoise. We then extend this method to the tracking multiple objects using the\ntrack elimination strategy. In multiple object tracking, the above approach\noften fails to correctly identify a track when two tracks cross. We solve this\nproblem by incorporating the Kalman filter into the dynamic programming\nframework. Our experiments on simulated data show that the tracking accuracy is\nsignificantly improved.\n", "versions": [{"version": "v1", "created": "Fri, 5 Jun 2015 22:59:47 GMT"}], "update_date": "2015-06-09", "authors_parsed": [["Xu", "Min", ""]]}, {"id": "1506.02106", "submitter": "Amy Bearman", "authors": "Amy Bearman, Olga Russakovsky, Vittorio Ferrari, Li Fei-Fei", "title": "What's the Point: Semantic Segmentation with Point Supervision", "comments": "ECCV (2016) submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The semantic image segmentation task presents a trade-off between test time\naccuracy and training-time annotation cost. Detailed per-pixel annotations\nenable training accurate models but are very time-consuming to obtain,\nimage-level class labels are an order of magnitude cheaper but result in less\naccurate models. We take a natural step from image-level annotation towards\nstronger supervision: we ask annotators to point to an object if one exists. We\nincorporate this point supervision along with a novel objectness potential in\nthe training loss function of a CNN model. Experimental results on the PASCAL\nVOC 2012 benchmark reveal that the combined effect of point-level supervision\nand objectness potential yields an improvement of 12.9% mIOU over image-level\nsupervision. Further, we demonstrate that models trained with point-level\nsupervision are more accurate than models trained with image-level,\nsquiggle-level or full supervision given a fixed annotation budget.\n", "versions": [{"version": "v1", "created": "Sat, 6 Jun 2015 02:45:48 GMT"}, {"version": "v2", "created": "Wed, 10 Jun 2015 15:45:20 GMT"}, {"version": "v3", "created": "Wed, 23 Sep 2015 18:52:24 GMT"}, {"version": "v4", "created": "Tue, 10 Nov 2015 20:19:47 GMT"}, {"version": "v5", "created": "Sat, 23 Jul 2016 17:41:43 GMT"}], "update_date": "2016-07-26", "authors_parsed": [["Bearman", "Amy", ""], ["Russakovsky", "Olga", ""], ["Ferrari", "Vittorio", ""], ["Fei-Fei", "Li", ""]]}, {"id": "1506.02108", "submitter": "Chunhua Shen", "authors": "Guosheng Lin, Chunhua Shen, Ian Reid, Anton van den Hengel", "title": "Deeply Learning the Messages in Message Passing Inference", "comments": "11 pages. Appearing in Proc. The Twenty-ninth Annual Conference on\n  Neural Information Processing Systems (NIPS), 2015, Montreal, Canada", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep structured output learning shows great promise in tasks like semantic\nimage segmentation. We proffer a new, efficient deep structured model learning\nscheme, in which we show how deep Convolutional Neural Networks (CNNs) can be\nused to estimate the messages in message passing inference for structured\nprediction with Conditional Random Fields (CRFs). With such CNN message\nestimators, we obviate the need to learn or evaluate potential functions for\nmessage calculation. This confers significant efficiency for learning, since\notherwise when performing structured learning for a CRF with CNN potentials it\nis necessary to undertake expensive inference for every stochastic gradient\niteration. The network output dimension for message estimation is the same as\nthe number of classes, in contrast to the network output for general CNN\npotential functions in CRFs, which is exponential in the order of the\npotentials. Hence CNN message learning has fewer network parameters and is more\nscalable for cases that a large number of classes are involved. We apply our\nmethod to semantic image segmentation on the PASCAL VOC 2012 dataset. We\nachieve an intersection-over-union score of 73.4 on its test set, which is the\nbest reported result for methods using the VOC training images alone. This\nimpressive performance demonstrates the effectiveness and usefulness of our CNN\nmessage learning method.\n", "versions": [{"version": "v1", "created": "Sat, 6 Jun 2015 02:52:38 GMT"}, {"version": "v2", "created": "Wed, 10 Jun 2015 06:49:06 GMT"}, {"version": "v3", "created": "Tue, 8 Sep 2015 04:29:45 GMT"}], "update_date": "2015-09-09", "authors_parsed": [["Lin", "Guosheng", ""], ["Shen", "Chunhua", ""], ["Reid", "Ian", ""], ["Hengel", "Anton van den", ""]]}, {"id": "1506.02167", "submitter": "Ayan Chakrabarti", "authors": "Ayan Chakrabarti", "title": "Color Constancy by Learning to Predict Chromaticity from Luminance", "comments": "Appears in Advances in Neural Information Processing Systems 28 (NIPS\n  2015)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Color constancy is the recovery of true surface color from observed color,\nand requires estimating the chromaticity of scene illumination to correct for\nthe bias it induces. In this paper, we show that the per-pixel color statistics\nof natural scenes---without any spatial or semantic context---can by themselves\nbe a powerful cue for color constancy. Specifically, we describe an illuminant\nestimation method that is built around a \"classifier\" for identifying the true\nchromaticity of a pixel given its luminance (absolute brightness across color\nchannels). During inference, each pixel's observed color restricts its true\nchromaticity to those values that can be explained by one of a candidate set of\nilluminants, and applying the classifier over these values yields a\ndistribution over the corresponding illuminants. A global estimate for the\nscene illuminant is computed through a simple aggregation of these\ndistributions across all pixels. We begin by simply defining the\nluminance-to-chromaticity classifier by computing empirical histograms over\ndiscretized chromaticity and luminance values from a training set of natural\nimages. These histograms reflect a preference for hues corresponding to smooth\nreflectance functions, and for achromatic colors in brighter pixels. Despite\nits simplicity, the resulting estimation algorithm outperforms current\nstate-of-the-art color constancy methods. Next, we propose a method to learn\nthe luminance-to-chromaticity classifier \"end-to-end\". Using stochastic\ngradient descent, we set chromaticity-luminance likelihoods to minimize errors\nin the final scene illuminant estimates on a training set. This leads to\nfurther improvements in accuracy, most significantly in the tail of the error\ndistribution.\n", "versions": [{"version": "v1", "created": "Sat, 6 Jun 2015 16:13:34 GMT"}, {"version": "v2", "created": "Mon, 7 Dec 2015 18:49:15 GMT"}], "update_date": "2015-12-08", "authors_parsed": [["Chakrabarti", "Ayan", ""]]}, {"id": "1506.02178", "submitter": "Dimitrios Tzionas", "authors": "Dimitrios Tzionas, Luca Ballan, Abhilash Srikantha, Pablo Aponte, Marc\n  Pollefeys, Juergen Gall", "title": "Capturing Hands in Action using Discriminative Salient Points and\n  Physics Simulation", "comments": "Accepted for publication by the International Journal of Computer\n  Vision (IJCV) on 16.02.2016 (submitted on 17.10.14). A combination into a\n  single framework of an ECCV'12 multicamera-RGB and a monocular-RGBD GCPR'14\n  hand tracking paper with several extensions, additional experiments and\n  details", "journal-ref": null, "doi": "10.1007/s11263-016-0895-4", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hand motion capture is a popular research field, recently gaining more\nattention due to the ubiquity of RGB-D sensors. However, even most recent\napproaches focus on the case of a single isolated hand. In this work, we focus\non hands that interact with other hands or objects and present a framework that\nsuccessfully captures motion in such interaction scenarios for both rigid and\narticulated objects. Our framework combines a generative model with\ndiscriminatively trained salient points to achieve a low tracking error and\nwith collision detection and physics simulation to achieve physically plausible\nestimates even in case of occlusions and missing visual data. Since all\ncomponents are unified in a single objective function which is almost\neverywhere differentiable, it can be optimized with standard optimization\ntechniques. Our approach works for monocular RGB-D sequences as well as setups\nwith multiple synchronized RGB cameras. For a qualitative and quantitative\nevaluation, we captured 29 sequences with a large variety of interactions and\nup to 150 degrees of freedom.\n", "versions": [{"version": "v1", "created": "Sat, 6 Jun 2015 18:08:56 GMT"}, {"version": "v2", "created": "Thu, 1 Oct 2015 17:07:24 GMT"}, {"version": "v3", "created": "Fri, 19 Feb 2016 21:24:01 GMT"}, {"version": "v4", "created": "Mon, 7 Mar 2016 18:09:59 GMT"}], "update_date": "2016-03-29", "authors_parsed": [["Tzionas", "Dimitrios", ""], ["Ballan", "Luca", ""], ["Srikantha", "Abhilash", ""], ["Aponte", "Pablo", ""], ["Pollefeys", "Marc", ""], ["Gall", "Juergen", ""]]}, {"id": "1506.02184", "submitter": "Jun Ye", "authors": "Jun Ye, Hao Hu, Kai Li, Guo-Jun Qi and Kien A. Hua", "title": "First-Take-All: Temporal Order-Preserving Hashing for 3D Action Videos", "comments": "9 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the prevalence of the commodity depth cameras, the new paradigm of user\ninterfaces based on 3D motion capturing and recognition have dramatically\nchanged the way of interactions between human and computers. Human action\nrecognition, as one of the key components in these devices, plays an important\nrole to guarantee the quality of user experience. Although the model-driven\nmethods have achieved huge success, they cannot provide a scalable solution for\nefficiently storing, retrieving and recognizing actions in the large-scale\napplications. These models are also vulnerable to the temporal translation and\nwarping, as well as the variations in motion scales and execution rates. To\naddress these challenges, we propose to treat the 3D human action recognition\nas a video-level hashing problem and propose a novel First-Take-All (FTA)\nHashing algorithm capable of hashing the entire video into hash codes of fixed\nlength. We demonstrate that this FTA algorithm produces a compact\nrepresentation of the video invariant to the above mentioned variations,\nthrough which action recognition can be solved by an efficient nearest neighbor\nsearch by the Hamming distance between the FTA hash codes. Experiments on the\npublic 3D human action datasets shows that the FTA algorithm can reach a\nrecognition accuracy higher than 80%, with about 15 bits per frame considering\nthere are 65 frames per video over the datasets.\n", "versions": [{"version": "v1", "created": "Sat, 6 Jun 2015 19:36:11 GMT"}], "update_date": "2015-06-09", "authors_parsed": [["Ye", "Jun", ""], ["Hu", "Hao", ""], ["Li", "Kai", ""], ["Qi", "Guo-Jun", ""], ["Hua", "Kien A.", ""]]}, {"id": "1506.02203", "submitter": "Matteo Ruggero Ronchi", "authors": "Matteo Ruggero Ronchi and Pietro Perona", "title": "Describing Common Human Visual Actions in Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Which common human actions and interactions are recognizable in monocular\nstill images? Which involve objects and/or other people? How many is a person\nperforming at a time? We address these questions by exploring the actions and\ninteractions that are detectable in the images of the MS COCO dataset. We make\ntwo main contributions. First, a list of 140 common `visual actions', obtained\nby analyzing the largest on-line verb lexicon currently available for English\n(VerbNet) and human sentences used to describe images in MS COCO. Second, a\ncomplete set of annotations for those `visual actions', composed of\nsubject-object and associated verb, which we call COCO-a (a for `actions').\nCOCO-a is larger than existing action datasets in terms of number of actions\nand instances of these actions, and is unique because it is data-driven, rather\nthan experimenter-biased. Other unique features are that it is exhaustive, and\nthat all subjects and objects are localized. A statistical analysis of the\naccuracy of our annotations and of each action, interaction and subject-object\ncombination is provided.\n", "versions": [{"version": "v1", "created": "Sun, 7 Jun 2015 00:33:23 GMT"}], "update_date": "2015-06-09", "authors_parsed": [["Ronchi", "Matteo Ruggero", ""], ["Perona", "Pietro", ""]]}, {"id": "1506.02211", "submitter": "Chao Dong", "authors": "Chao Dong and Ximei Zhu and Yubin Deng and Chen Change Loy and Yu Qiao", "title": "Boosting Optical Character Recognition: A Super-Resolution Approach", "comments": "5 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Text image super-resolution is a challenging yet open research problem in the\ncomputer vision community. In particular, low-resolution images hamper the\nperformance of typical optical character recognition (OCR) systems. In this\narticle, we summarize our entry to the ICDAR2015 Competition on Text Image\nSuper-Resolution. Experiments are based on the provided ICDAR2015 TextSR\ndataset and the released Tesseract-OCR 3.02 system. We report that our winning\nentry of text image super-resolution framework has largely improved the OCR\nperformance with low-resolution images used as input, reaching an OCR accuracy\nscore of 77.19%, which is comparable with that of using the original\nhigh-resolution images 78.80%.\n", "versions": [{"version": "v1", "created": "Sun, 7 Jun 2015 02:29:45 GMT"}], "update_date": "2015-06-09", "authors_parsed": [["Dong", "Chao", ""], ["Zhu", "Ximei", ""], ["Deng", "Yubin", ""], ["Loy", "Chen Change", ""], ["Qiao", "Yu", ""]]}, {"id": "1506.02247", "submitter": "Gonzalo Galiano", "authors": "Gonzalo Galiano, Emanuele Schiavi, Juli\\'an Velasco", "title": "Well-posedness of a nonlinear integro-differential problem and its\n  rearranged formulation", "comments": "Final version. To appear in Nolinear Analysis Real World Applications\n  (2016)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the existence and uniqueness of solutions of a nonlinear\nintegro-differential problem which we reformulate introducing the notion of the\ndecreasing rearrangement of the solution. A dimensional reduction of the\nproblem is obtained and a detailed analysis of the properties of the solutions\nof the model is provided. Finally, a fast numerical method is devised and\nimplemented to show the performance of the model when typical image processing\ntasks such as filtering and segmentation are performed.\n", "versions": [{"version": "v1", "created": "Sun, 7 Jun 2015 10:23:58 GMT"}, {"version": "v2", "created": "Fri, 8 Apr 2016 10:11:18 GMT"}], "update_date": "2016-04-11", "authors_parsed": [["Galiano", "Gonzalo", ""], ["Schiavi", "Emanuele", ""], ["Velasco", "Juli\u00e1n", ""]]}, {"id": "1506.02264", "submitter": "Shmuel Peleg", "authors": "Yedid Hoshen, Shmuel Peleg", "title": "Visual Learning of Arithmetic Operations", "comments": "To appear in AAAI 2016", "journal-ref": "Proc. AAAI'16, Phoenix, Feb. 2016, pp. 3733-3739", "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A simple Neural Network model is presented for end-to-end visual learning of\narithmetic operations from pictures of numbers. The input consists of two\npictures, each showing a 7-digit number. The output, also a picture, displays\nthe number showing the result of an arithmetic operation (e.g., addition or\nsubtraction) on the two input numbers. The concepts of a number, or of an\noperator, are not explicitly introduced. This indicates that addition is a\nsimple cognitive task, which can be learned visually using a very small number\nof neurons.\n  Other operations, e.g., multiplication, were not learnable using this\narchitecture. Some tasks were not learnable end-to-end (e.g., addition with\nRoman numerals), but were easily learnable once broken into two separate\nsub-tasks: a perceptual \\textit{Character Recognition} and cognitive\n\\textit{Arithmetic} sub-tasks. This indicates that while some tasks may be\neasily learnable end-to-end, other may need to be broken into sub-tasks.\n", "versions": [{"version": "v1", "created": "Sun, 7 Jun 2015 13:44:15 GMT"}, {"version": "v2", "created": "Fri, 27 Nov 2015 12:18:48 GMT"}], "update_date": "2017-01-06", "authors_parsed": [["Hoshen", "Yedid", ""], ["Peleg", "Shmuel", ""]]}, {"id": "1506.02265", "submitter": "Yilun Wang", "authors": "Yilun Wang, Sheng Zhang, Junjie Zheng, Heng Chen, and Huafu Chen", "title": "Randomized Structural Sparsity based Support Identification with\n  Applications to Locating Activated or Discriminative Brain Areas: A\n  Multi-center Reproducibility Study", "comments": "arXiv admin note: text overlap with arXiv:1410.4650", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we focus on how to locate the relevant or discriminative brain\nregions related with external stimulus or certain mental decease, which is also\ncalled support identification, based on the neuroimaging data. The main\ndifficulty lies in the extremely high dimensional voxel space and relatively\nfew training samples, easily resulting in an unstable brain region discovery\n(or called feature selection in context of pattern recognition). When the\ntraining samples are from different centers and have betweencenter variations,\nit will be even harder to obtain a reliable and consistent result.\nCorresponding, we revisit our recently proposed algorithm based on stability\nselection and structural sparsity. It is applied to the multi-center MRI data\nanalysis for the first time. A consistent and stable result is achieved across\ndifferent centers despite the between-center data variation while many other\nstate-of-the-art methods such as two sample t-test fail. Moreover, we have\nempirically showed that the performance of this algorithm is robust and\ninsensitive to several of its key parameters. In addition, the support\nidentification results on both functional MRI and structural MRI are\ninterpretable and can be the potential biomarkers.\n", "versions": [{"version": "v1", "created": "Sun, 7 Jun 2015 13:53:54 GMT"}], "update_date": "2015-06-09", "authors_parsed": [["Wang", "Yilun", ""], ["Zhang", "Sheng", ""], ["Zheng", "Junjie", ""], ["Chen", "Heng", ""], ["Chen", "Huafu", ""]]}, {"id": "1506.02328", "submitter": "Dong Liu", "authors": "Guangnan Ye, Yitong Li, Hongliang Xu, Dong Liu and Shih-Fu Chang", "title": "EventNet: A Large Scale Structured Concept Library for Complex Event\n  Detection in Video", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Event-specific concepts are the semantic concepts designed for the events of\ninterest, which can be used as a mid-level representation of complex events in\nvideos. Existing methods only focus on defining event-specific concepts for a\nsmall number of predefined events, but cannot handle novel unseen events. This\nmotivates us to build a large scale event-specific concept library that covers\nas many real-world events and their concepts as possible. Specifically, we\nchoose WikiHow, an online forum containing a large number of how-to articles on\nhuman daily life events. We perform a coarse-to-fine event discovery process\nand discover 500 events from WikiHow articles. Then we use each event name as\nquery to search YouTube and discover event-specific concepts from the tags of\nreturned videos. After an automatic filter process, we end up with 95,321\nvideos and 4,490 concepts. We train a Convolutional Neural Network (CNN) model\non the 95,321 videos over the 500 events, and use the model to extract deep\nlearning feature from video content. With the learned deep learning feature, we\ntrain 4,490 binary SVM classifiers as the event-specific concept library. The\nconcepts and events are further organized in a hierarchical structure defined\nby WikiHow, and the resultant concept library is called EventNet. Finally, the\nEventNet concept library is used to generate concept based representation of\nevent videos. To the best of our knowledge, EventNet represents the first video\nevent ontology that organizes events and their concepts into a semantic\nstructure. It offers great potential for event retrieval and browsing.\nExtensive experiments over the zero-shot event retrieval task when no training\nsamples are available show that the EventNet concept library consistently and\nsignificantly outperforms the state-of-the-art (such as the 20K ImageNet\nconcepts trained with CNN) by a large margin up to 207%.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jun 2015 00:34:51 GMT"}, {"version": "v2", "created": "Mon, 17 Aug 2015 17:13:23 GMT"}], "update_date": "2015-08-18", "authors_parsed": [["Ye", "Guangnan", ""], ["Li", "Yitong", ""], ["Xu", "Hongliang", ""], ["Liu", "Dong", ""], ["Chang", "Shih-Fu", ""]]}, {"id": "1506.02345", "submitter": "Vladimir Saveljev", "authors": "Vladimir Saveljev", "title": "Wavelets and continuous wavelet transform for autostereoscopic multiview\n  images", "comments": "4 pages, 10 figures", "journal-ref": "Applied Optics, Vol. 55, Issue 23, pp. 6275-6284 (2016)", "doi": "10.1364/AO.55.006275", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, the reference functions for the synthesis and analysis of the\nautostereoscopic multiview and integral images in three-dimensional displays we\nintroduced. In the current paper, we propose the wavelets to analyze such\nimages. The wavelets are built on the reference functions as on the scaling\nfunctions of the wavelet analysis. The continuous wavelet transform was\nsuccessfully applied to the testing wireframe binary objects. The restored\nlocations correspond to the structure of the testing wireframe binary objects.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jun 2015 03:47:17 GMT"}], "update_date": "2016-10-13", "authors_parsed": [["Saveljev", "Vladimir", ""]]}, {"id": "1506.02432", "submitter": "Craig Henderson", "authors": "Craig Henderson and Ebroul Izquierdo", "title": "Reflection Invariance: an important consideration of image orientation", "comments": "7 pages, 4 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this position paper, we consider the state of computer vision research\nwith respect to invariance to the horizontal orientation of an image -- what we\nterm reflection invariance. We describe why we consider reflection invariance\nto be an important property and provide evidence where the absence of this\ninvariance produces surprising inconsistencies in state-of-the-art systems. We\ndemonstrate inconsistencies in methods of object detection and scene\nclassification when they are presented with images and the horizontal mirror of\nthose images. Finally, we examine where some of the invariance is exhibited in\nfeature detection and descriptors, and make a case for future consideration of\nreflection invariance as a measure of quality in computer vision algorithms.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jun 2015 10:44:36 GMT"}], "update_date": "2015-06-09", "authors_parsed": [["Henderson", "Craig", ""], ["Izquierdo", "Ebroul", ""]]}, {"id": "1506.02509", "submitter": "Lei Zhang", "authors": "Lei Zhang and David Zhang", "title": "SVM and ELM: Who Wins? Object Recognition with Deep Convolutional\n  Features from ImageNet", "comments": "7 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning with a convolutional neural network (CNN) has been proved to be\nvery effective in feature extraction and representation of images. For image\nclassification problems, this work aim at finding which classifier is more\ncompetitive based on high-level deep features of images. In this report, we\nhave discussed the nearest neighbor, support vector machines and extreme\nlearning machines for image classification under deep convolutional activation\nfeature representation. Specifically, we adopt the benchmark object recognition\ndataset from multiple sources with domain bias for evaluating different\nclassifiers. The deep features of the object dataset are obtained by a\nwell-trained CNN with five convolutional layers and three fully-connected\nlayers on the challenging ImageNet. Experiments demonstrate that the ELMs\noutperform SVMs in cross-domain recognition tasks. In particular,\nstate-of-the-art results are obtained by kernel ELM which outperforms SVMs with\nabout 4% of the average accuracy. The features and codes are available in\nhttp://www.escience.cn/people/lei/index.html\n", "versions": [{"version": "v1", "created": "Mon, 8 Jun 2015 13:58:01 GMT"}], "update_date": "2015-06-09", "authors_parsed": [["Zhang", "Lei", ""], ["Zhang", "David", ""]]}, {"id": "1506.02515", "submitter": "Vadim Lebedev", "authors": "Vadim Lebedev, Victor Lempitsky", "title": "Fast ConvNets Using Group-wise Brain Damage", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We revisit the idea of brain damage, i.e. the pruning of the coefficients of\na neural network, and suggest how brain damage can be modified and used to\nspeedup convolutional layers. The approach uses the fact that many efficient\nimplementations reduce generalized convolutions to matrix multiplications. The\nsuggested brain damage process prunes the convolutional kernel tensor in a\ngroup-wise fashion by adding group-sparsity regularization to the standard\ntraining process. After such group-wise pruning, convolutions can be reduced to\nmultiplications of thinned dense matrices, which leads to speedup. In the\ncomparison on AlexNet, the method achieves very competitive performance.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jun 2015 14:20:37 GMT"}, {"version": "v2", "created": "Mon, 7 Dec 2015 18:11:36 GMT"}], "update_date": "2015-12-08", "authors_parsed": [["Lebedev", "Vadim", ""], ["Lempitsky", "Victor", ""]]}, {"id": "1506.02544", "submitter": "Youssef  Mroueh", "authors": "Youssef Mroueh, Stephen Voinea, Tomaso Poggio", "title": "Learning with Group Invariant Features: A Kernel Perspective", "comments": "NIPS 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We analyze in this paper a random feature map based on a theory of invariance\nI-theory introduced recently. More specifically, a group invariant signal\nsignature is obtained through cumulative distributions of group transformed\nrandom projections. Our analysis bridges invariant feature learning with kernel\nmethods, as we show that this feature map defines an expected Haar integration\nkernel that is invariant to the specified group action. We show how this\nnon-linear random feature map approximates this group invariant kernel\nuniformly on a set of $N$ points. Moreover, we show that it defines a function\nspace that is dense in the equivalent Invariant Reproducing Kernel Hilbert\nSpace. Finally, we quantify error rates of the convergence of the empirical\nrisk minimization, as well as the reduction in the sample complexity of a\nlearning algorithm using such an invariant representation for signal\nclassification, in a classical supervised learning setting.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jun 2015 15:19:30 GMT"}, {"version": "v2", "created": "Fri, 4 Dec 2015 20:49:25 GMT"}], "update_date": "2015-12-07", "authors_parsed": [["Mroueh", "Youssef", ""], ["Voinea", "Stephen", ""], ["Poggio", "Tomaso", ""]]}, {"id": "1506.02565", "submitter": "Seungjin Choi", "authors": "Yong-Deok Kim, Taewoong Jang, Bohyung Han, and Seungjin Choi", "title": "Learning to Select Pre-Trained Deep Representations with Bayesian\n  Evidence Framework", "comments": "Appearing in CVPR-2016 (oral presentation)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a Bayesian evidence framework to facilitate transfer learning from\npre-trained deep convolutional neural networks (CNNs). Our framework is\nformulated on top of a least squares SVM (LS-SVM) classifier, which is simple\nand fast in both training and testing, and achieves competitive performance in\npractice. The regularization parameters in LS-SVM is estimated automatically\nwithout grid search and cross-validation by maximizing evidence, which is a\nuseful measure to select the best performing CNN out of multiple candidates for\ntransfer learning; the evidence is optimized efficiently by employing Aitken's\ndelta-squared process, which accelerates convergence of fixed point update. The\nproposed Bayesian evidence framework also provides a good solution to identify\nthe best ensemble of heterogeneous CNNs through a greedy algorithm. Our\nBayesian evidence framework for transfer learning is tested on 12 visual\nrecognition datasets and illustrates the state-of-the-art performance\nconsistently in terms of prediction accuracy and modeling efficiency.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jun 2015 15:56:26 GMT"}, {"version": "v2", "created": "Tue, 9 Jun 2015 18:57:35 GMT"}, {"version": "v3", "created": "Thu, 24 Dec 2015 03:40:28 GMT"}, {"version": "v4", "created": "Mon, 25 Apr 2016 01:35:31 GMT"}], "update_date": "2016-04-26", "authors_parsed": [["Kim", "Yong-Deok", ""], ["Jang", "Taewoong", ""], ["Han", "Bohyung", ""], ["Choi", "Seungjin", ""]]}, {"id": "1506.02588", "submitter": "Matthijs Douze", "authors": "Matthijs Douze, J\\'er\\^ome Revaud, Jakob Verbeek, Herv\\'e J\\'egou,\n  Cordelia Schmid", "title": "Circulant temporal encoding for video retrieval and temporal alignment", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of specific video event retrieval. Given a query video\nof a specific event, e.g., a concert of Madonna, the goal is to retrieve other\nvideos of the same event that temporally overlap with the query. Our approach\nencodes the frame descriptors of a video to jointly represent their appearance\nand temporal order. It exploits the properties of circulant matrices to\nefficiently compare the videos in the frequency domain. This offers a\nsignificant gain in complexity and accurately localizes the matching parts of\nvideos. The descriptors can be compressed in the frequency domain with a\nproduct quantizer adapted to complex numbers. In this case, video retrieval is\nperformed without decompressing the descriptors. We also consider the temporal\nalignment of a set of videos. We exploit the matching confidence and an\nestimate of the temporal offset computed for all pairs of videos by our\nretrieval approach. Our robust algorithm aligns the videos on a global timeline\nby maximizing the set of temporally consistent matches. The global temporal\nalignment enables synchronous playback of the videos of a given scene.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jun 2015 17:05:53 GMT"}, {"version": "v2", "created": "Mon, 30 Nov 2015 17:00:13 GMT"}], "update_date": "2015-12-01", "authors_parsed": [["Douze", "Matthijs", ""], ["Revaud", "J\u00e9r\u00f4me", ""], ["Verbeek", "Jakob", ""], ["J\u00e9gou", "Herv\u00e9", ""], ["Schmid", "Cordelia", ""]]}, {"id": "1506.02617", "submitter": "Behnam Neyshabur", "authors": "Behnam Neyshabur, Ruslan Salakhutdinov, Nathan Srebro", "title": "Path-SGD: Path-Normalized Optimization in Deep Neural Networks", "comments": "12 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We revisit the choice of SGD for training deep neural networks by\nreconsidering the appropriate geometry in which to optimize the weights. We\nargue for a geometry invariant to rescaling of weights that does not affect the\noutput of the network, and suggest Path-SGD, which is an approximate steepest\ndescent method with respect to a path-wise regularizer related to max-norm\nregularization. Path-SGD is easy and efficient to implement and leads to\nempirical gains over SGD and AdaGrad.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jun 2015 19:01:33 GMT"}], "update_date": "2015-06-09", "authors_parsed": [["Neyshabur", "Behnam", ""], ["Salakhutdinov", "Ruslan", ""], ["Srebro", "Nathan", ""]]}, {"id": "1506.02626", "submitter": "Song Han", "authors": "Song Han, Jeff Pool, John Tran, William J. Dally", "title": "Learning both Weights and Connections for Efficient Neural Networks", "comments": "Published as a conference paper at NIPS 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural networks are both computationally intensive and memory intensive,\nmaking them difficult to deploy on embedded systems. Also, conventional\nnetworks fix the architecture before training starts; as a result, training\ncannot improve the architecture. To address these limitations, we describe a\nmethod to reduce the storage and computation required by neural networks by an\norder of magnitude without affecting their accuracy by learning only the\nimportant connections. Our method prunes redundant connections using a\nthree-step method. First, we train the network to learn which connections are\nimportant. Next, we prune the unimportant connections. Finally, we retrain the\nnetwork to fine tune the weights of the remaining connections. On the ImageNet\ndataset, our method reduced the number of parameters of AlexNet by a factor of\n9x, from 61 million to 6.7 million, without incurring accuracy loss. Similar\nexperiments with VGG-16 found that the number of parameters can be reduced by\n13x, from 138 million to 10.3 million, again with no loss of accuracy.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jun 2015 19:28:43 GMT"}, {"version": "v2", "created": "Wed, 29 Jul 2015 22:27:31 GMT"}, {"version": "v3", "created": "Fri, 30 Oct 2015 23:29:27 GMT"}], "update_date": "2015-11-03", "authors_parsed": [["Han", "Song", ""], ["Pool", "Jeff", ""], ["Tran", "John", ""], ["Dally", "William J.", ""]]}, {"id": "1506.02640", "submitter": "Joseph Redmon", "authors": "Joseph Redmon, Santosh Divvala, Ross Girshick, and Ali Farhadi", "title": "You Only Look Once: Unified, Real-Time Object Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present YOLO, a new approach to object detection. Prior work on object\ndetection repurposes classifiers to perform detection. Instead, we frame object\ndetection as a regression problem to spatially separated bounding boxes and\nassociated class probabilities. A single neural network predicts bounding boxes\nand class probabilities directly from full images in one evaluation. Since the\nwhole detection pipeline is a single network, it can be optimized end-to-end\ndirectly on detection performance.\n  Our unified architecture is extremely fast. Our base YOLO model processes\nimages in real-time at 45 frames per second. A smaller version of the network,\nFast YOLO, processes an astounding 155 frames per second while still achieving\ndouble the mAP of other real-time detectors. Compared to state-of-the-art\ndetection systems, YOLO makes more localization errors but is far less likely\nto predict false detections where nothing exists. Finally, YOLO learns very\ngeneral representations of objects. It outperforms all other detection methods,\nincluding DPM and R-CNN, by a wide margin when generalizing from natural images\nto artwork on both the Picasso Dataset and the People-Art Dataset.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jun 2015 19:52:52 GMT"}, {"version": "v2", "created": "Tue, 9 Jun 2015 07:51:14 GMT"}, {"version": "v3", "created": "Thu, 11 Jun 2015 19:21:47 GMT"}, {"version": "v4", "created": "Thu, 12 Nov 2015 22:53:44 GMT"}, {"version": "v5", "created": "Mon, 9 May 2016 22:22:11 GMT"}], "update_date": "2016-05-11", "authors_parsed": [["Redmon", "Joseph", ""], ["Divvala", "Santosh", ""], ["Girshick", "Ross", ""], ["Farhadi", "Ali", ""]]}, {"id": "1506.02753", "submitter": "Alexey Dosovitskiy", "authors": "Alexey Dosovitskiy and Thomas Brox", "title": "Inverting Visual Representations with Convolutional Networks", "comments": "Version 4 - final version to appear in CVPR-2016. Visually better\n  results obtained with feature similarity and adversarial training are in a\n  different paper - arXiv:1602.02644", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Feature representations, both hand-designed and learned ones, are often hard\nto analyze and interpret, even when they are extracted from visual data. We\npropose a new approach to study image representations by inverting them with an\nup-convolutional neural network. We apply the method to shallow representations\n(HOG, SIFT, LBP), as well as to deep networks. For shallow representations our\napproach provides significantly better reconstructions than existing methods,\nrevealing that there is surprisingly rich information contained in these\nfeatures. Inverting a deep network trained on ImageNet provides several\ninsights into the properties of the feature representation learned by the\nnetwork. Most strikingly, the colors and the rough contours of an image can be\nreconstructed from activations in higher network layers and even from the\npredicted class probabilities.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jun 2015 02:31:40 GMT"}, {"version": "v2", "created": "Thu, 19 Nov 2015 16:35:56 GMT"}, {"version": "v3", "created": "Thu, 3 Dec 2015 18:18:57 GMT"}, {"version": "v4", "created": "Tue, 26 Apr 2016 23:30:11 GMT"}], "update_date": "2016-04-28", "authors_parsed": [["Dosovitskiy", "Alexey", ""], ["Brox", "Thomas", ""]]}, {"id": "1506.02776", "submitter": "Sumith Yd", "authors": "Sumith YD", "title": "Fast Geometric Fit Algorithm for Sphere Using Exact Solution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sphere fitting is a common problem in almost all science and engineering\ndisciplines. Most of methods available are iterative in behavior. This involves\nfitting of the parameters in a least square sense or in a geometric sense. Here\nwe extend the methods of Thomas Chan and Landau who fitted the 2D data using\ncircle. This work closely resemble their work in redefining the error estimate\nand solving the sphere fitting problem exactly. The solutions for center and\nradius of the sphere can be found exactly and the equations can be hard coded\nfor high performance. We have also shown some comparison with other popular\nmethods and how this method behaves.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jun 2015 05:03:47 GMT"}], "update_date": "2015-06-10", "authors_parsed": [["YD", "Sumith", ""]]}, {"id": "1506.02897", "submitter": "Tomas Pfister", "authors": "Tomas Pfister and James Charles and Andrew Zisserman", "title": "Flowing ConvNets for Human Pose Estimation in Videos", "comments": "ICCV'15", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The objective of this work is human pose estimation in videos, where multiple\nframes are available. We investigate a ConvNet architecture that is able to\nbenefit from temporal context by combining information across the multiple\nframes using optical flow.\n  To this end we propose a network architecture with the following novelties:\n(i) a deeper network than previously investigated for regressing heatmaps; (ii)\nspatial fusion layers that learn an implicit spatial model; (iii) optical flow\nis used to align heatmap predictions from neighbouring frames; and (iv) a final\nparametric pooling layer which learns to combine the aligned heatmaps into a\npooled confidence map.\n  We show that this architecture outperforms a number of others, including one\nthat uses optical flow solely at the input layers, one that regresses joint\ncoordinates directly, and one that predicts heatmaps without spatial fusion.\n  The new architecture outperforms the state of the art by a large margin on\nthree video pose estimation datasets, including the very challenging Poses in\nthe Wild dataset, and outperforms other deep methods that don't use a graphical\nmodel on the single-image FLIC benchmark (and also Chen & Yuille and Tompson et\nal. in the high precision region).\n", "versions": [{"version": "v1", "created": "Tue, 9 Jun 2015 13:17:33 GMT"}, {"version": "v2", "created": "Sun, 8 Nov 2015 16:52:59 GMT"}], "update_date": "2015-11-10", "authors_parsed": [["Pfister", "Tomas", ""], ["Charles", "James", ""], ["Zisserman", "Andrew", ""]]}, {"id": "1506.02923", "submitter": "Abdulrahman Ibraheem", "authors": "Abdulrahman Oladipupo Ibraheem", "title": "Compact Shape Trees: A Contribution to the Forest of Shape\n  Correspondences and Matching Methods", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel technique, termed compact shape trees, for computing\ncorrespondences of single-boundary 2-D shapes in O(n2) time. Together with zero\nor more features defined at each of n sample points on the shape's boundary,\nthe compact shape tree of a shape comprises the O(n) collection of vectors\nemanating from any of the sample points on the shape's boundary to the rest of\nthe sample points on the boundary. As it turns out, compact shape trees have a\nnumber of elegant properties both in the spatial and frequency domains. In\nparticular, via a simple vector-algebraic argument, we show that the O(n)\ncollection of vectors in a compact shape tree possesses at least the same\ndiscriminatory power as the O(n2) collection of lines emanating from each\nsample point to every other sample point on a shape's boundary. In addition, we\ndescribe neat approaches for achieving scale and rotation invariance with\ncompact shape trees in the spatial domain; by viewing compact shape trees as\naperiodic discrete signals, we also prove scale and rotation invariance\nproperties for them in the Fourier domain. Towards these, along the way, using\nconcepts from differential geometry and the Calculus, we propose a novel theory\nfor sampling 2-D shape boundaries in a scale and rotation invariant manner.\nFinally, we propose a number of shape recognition experiments to test the\nefficacy of our concept.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jun 2015 14:18:01 GMT"}], "update_date": "2015-06-10", "authors_parsed": [["Ibraheem", "Abdulrahman Oladipupo", ""]]}, {"id": "1506.03011", "submitter": "Rostislav Goroshin", "authors": "Ross Goroshin, Michael Mathieu, Yann LeCun", "title": "Learning to Linearize Under Uncertainty", "comments": "To appear at NIPS 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Training deep feature hierarchies to solve supervised learning tasks has\nachieved state of the art performance on many problems in computer vision.\nHowever, a principled way in which to train such hierarchies in the\nunsupervised setting has remained elusive. In this work we suggest a new\narchitecture and loss for training deep feature hierarchies that linearize the\ntransformations observed in unlabeled natural video sequences. This is done by\ntraining a generative model to predict video frames. We also address the\nproblem of inherent uncertainty in prediction by introducing latent variables\nthat are non-deterministic functions of the input into the network\narchitecture.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jun 2015 17:22:17 GMT"}, {"version": "v2", "created": "Thu, 10 Sep 2015 15:20:38 GMT"}], "update_date": "2015-09-11", "authors_parsed": [["Goroshin", "Ross", ""], ["Mathieu", "Michael", ""], ["LeCun", "Yann", ""]]}, {"id": "1506.03099", "submitter": "Samy Bengio", "authors": "Samy Bengio, Oriol Vinyals, Navdeep Jaitly, Noam Shazeer", "title": "Scheduled Sampling for Sequence Prediction with Recurrent Neural\n  Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recurrent Neural Networks can be trained to produce sequences of tokens given\nsome input, as exemplified by recent results in machine translation and image\ncaptioning. The current approach to training them consists of maximizing the\nlikelihood of each token in the sequence given the current (recurrent) state\nand the previous token. At inference, the unknown previous token is then\nreplaced by a token generated by the model itself. This discrepancy between\ntraining and inference can yield errors that can accumulate quickly along the\ngenerated sequence. We propose a curriculum learning strategy to gently change\nthe training process from a fully guided scheme using the true previous token,\ntowards a less guided scheme which mostly uses the generated token instead.\nExperiments on several sequence prediction tasks show that this approach yields\nsignificant improvements. Moreover, it was used successfully in our winning\nentry to the MSCOCO image captioning challenge, 2015.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jun 2015 20:33:47 GMT"}, {"version": "v2", "created": "Mon, 15 Jun 2015 15:29:22 GMT"}, {"version": "v3", "created": "Wed, 23 Sep 2015 16:35:42 GMT"}], "update_date": "2015-09-24", "authors_parsed": [["Bengio", "Samy", ""], ["Vinyals", "Oriol", ""], ["Jaitly", "Navdeep", ""], ["Shazeer", "Noam", ""]]}, {"id": "1506.03124", "submitter": "Subhamoy Mandal", "authors": "Subhamoy Mandal, Viswanath Pamulakanty Sudarshan, Yeshaswini Nagaraj,\n  Xose Luis Dean Ben, Daniel Razansky", "title": "Multiscale edge detection and parametric shape modeling for boundary\n  delineation in optoacoustic images", "comments": "Engineering in Medicine and Biology Society (EMBC), 2015 37th Annual\n  International Conference of the IEEE (Accepted version)", "journal-ref": "Engineering in Medicine and Biology Society (EMBC), 2015 37th\n  Annual International Conference of the IEEE , vol., no., pp.707-710, 25-29\n  Aug. 2015", "doi": "10.1109/EMBC.2015.7318460", "report-no": null, "categories": "physics.med-ph cs.CV eess.IV", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  In this article, we present a novel scheme for segmenting the image boundary\n(with the background) in optoacoustic small animal in vivo imaging systems. The\nmethod utilizes a multiscale edge detection algorithm to generate a binary edge\nmap. A scale dependent morphological operation is employed to clean spurious\nedges. Thereafter, an ellipse is fitted to the edge map through constrained\nparametric transformations and iterative goodness of fit calculations. The\nmethod delimits the tissue edges through the curve fitting model, which has\nshown high levels of accuracy. Thus, this method enables segmentation of\noptoacoutic images with minimal human intervention, by eliminating need of\nscale selection for multiscale processing and seed point determination for\ncontour mapping.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jun 2015 22:44:26 GMT"}], "update_date": "2019-01-11", "authors_parsed": [["Mandal", "Subhamoy", ""], ["Sudarshan", "Viswanath Pamulakanty", ""], ["Nagaraj", "Yeshaswini", ""], ["Ben", "Xose Luis Dean", ""], ["Razansky", "Daniel", ""]]}, {"id": "1506.03128", "submitter": "Jani Biju Babjan", "authors": "Jani Biju Babjan", "title": "License Plate Recognition System Based on Color Coding Of License Plates", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  License Plate Recognition Systems are used to determine the license plate\nnumber of a vehicle. The current system mainly uses Optical Character\nRecognition to recognize the number plate. There are several problems to this\nsystem. Some of them include interchanging of several letters or numbers\n(letter O with digit 0), difficulty in localizing the license plate, high error\nrate, use of different fonts in license plates etc. So a new system to\nrecognize the license plate number using color coding of license plates is\nproposed in this paper. Easier localization of license plate can be done by\nsearching for the start or stop patters of license plates. An eight segment\ndisplay system along with traditional numbering with the first and last\nsegments left for start or stop patterns is proposed in this paper. Practical\napplications include several areas under Internet of Things (IoT).\n", "versions": [{"version": "v1", "created": "Mon, 8 Jun 2015 15:44:29 GMT"}], "update_date": "2015-06-11", "authors_parsed": [["Babjan", "Jani Biju", ""]]}, {"id": "1506.03184", "submitter": "Cong Yao", "authors": "Xinyu Zhou and Shuchang Zhou and Cong Yao and Zhimin Cao and Qi Yin", "title": "ICDAR 2015 Text Reading in the Wild Competition", "comments": "3 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, text detection and recognition in natural scenes are becoming\nincreasing popular in the computer vision community as well as the document\nanalysis community. However, majority of the existing ideas, algorithms and\nsystems are specifically designed for English. This technical report presents\nthe final results of the ICDAR 2015 Text Reading in the Wild (TRW 2015)\ncompetition, which aims at establishing a benchmark for assessing detection and\nrecognition algorithms devised for both Chinese and English scripts and\nproviding a playground for researchers from the community. In this article, we\ndescribe in detail the dataset, tasks, evaluation protocols and participants of\nthis competition, and report the performance of the participating methods.\nMoreover, promising directions for future research are discussed.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jun 2015 06:46:55 GMT"}], "update_date": "2015-06-11", "authors_parsed": [["Zhou", "Xinyu", ""], ["Zhou", "Shuchang", ""], ["Yao", "Cong", ""], ["Cao", "Zhimin", ""], ["Yin", "Qi", ""]]}, {"id": "1506.03301", "submitter": "Meirav Galun", "authors": "Meirav Galun, Tal Amir, Tal Hassner, Ronen Basri, Yaron Lipman", "title": "Wide baseline stereo matching with convex bounded-distortion constraints", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Finding correspondences in wide baseline setups is a challenging problem.\nExisting approaches have focused largely on developing better feature\ndescriptors for correspondence and on accurate recovery of epipolar line\nconstraints. This paper focuses on the challenging problem of finding\ncorrespondences once approximate epipolar constraints are given. We introduce a\nnovel method that integrates a deformation model. Specifically, we formulate\nthe problem as finding the largest number of corresponding points related by a\nbounded distortion map that obeys the given epipolar constraints. We show that,\nwhile the set of bounded distortion maps is not convex, the subset of maps that\nobey the epipolar line constraints is convex, allowing us to introduce an\nefficient algorithm for matching. We further utilize a robust cost function for\nmatching and employ majorization-minimization for its optimization. Our\nexperiments indicate that our method finds significantly more accurate maps\nthan existing approaches.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jun 2015 13:48:34 GMT"}], "update_date": "2015-06-11", "authors_parsed": [["Galun", "Meirav", ""], ["Amir", "Tal", ""], ["Hassner", "Tal", ""], ["Basri", "Ronen", ""], ["Lipman", "Yaron", ""]]}, {"id": "1506.03358", "submitter": "Lukas F. Lang", "authors": "Lukas F. Lang, Otmar Scherzer", "title": "Optical Flow on Evolving Sphere-Like Surfaces", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we consider optical flow on evolving Riemannian 2-manifolds\nwhich can be parametrised from the 2-sphere. Our main motivation is to estimate\ncell motion in time-lapse volumetric microscopy images depicting fluorescently\nlabelled cells of a live zebrafish embryo. We exploit the fact that the\nrecorded cells float on the surface of the embryo and allow for the extraction\nof an image sequence together with a sphere-like surface. We solve the\nresulting variational problem by means of a Galerkin method based on vector\nspherical harmonics and present numerical results computed from the\naforementioned microscopy data.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jun 2015 15:24:19 GMT"}], "update_date": "2015-06-11", "authors_parsed": [["Lang", "Lukas F.", ""], ["Scherzer", "Otmar", ""]]}, {"id": "1506.03365", "submitter": "Fisher Yu", "authors": "Fisher Yu, Ari Seff, Yinda Zhang, Shuran Song, Thomas Funkhouser,\n  Jianxiong Xiao", "title": "LSUN: Construction of a Large-scale Image Dataset using Deep Learning\n  with Humans in the Loop", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While there has been remarkable progress in the performance of visual\nrecognition algorithms, the state-of-the-art models tend to be exceptionally\ndata-hungry. Large labeled training datasets, expensive and tedious to produce,\nare required to optimize millions of parameters in deep network models. Lagging\nbehind the growth in model capacity, the available datasets are quickly\nbecoming outdated in terms of size and density. To circumvent this bottleneck,\nwe propose to amplify human effort through a partially automated labeling\nscheme, leveraging deep learning with humans in the loop. Starting from a large\nset of candidate images for each category, we iteratively sample a subset, ask\npeople to label them, classify the others with a trained model, split the set\ninto positives, negatives, and unlabeled based on the classification\nconfidence, and then iterate with the unlabeled set. To assess the\neffectiveness of this cascading procedure and enable further progress in visual\nrecognition research, we construct a new image dataset, LSUN. It contains\naround one million labeled images for each of 10 scene categories and 20 object\ncategories. We experiment with training popular convolutional networks and find\nthat they achieve substantial performance gains when trained on this dataset.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jun 2015 15:38:47 GMT"}, {"version": "v2", "created": "Fri, 19 Jun 2015 19:12:05 GMT"}, {"version": "v3", "created": "Sat, 4 Jun 2016 09:51:30 GMT"}], "update_date": "2016-06-07", "authors_parsed": [["Yu", "Fisher", ""], ["Seff", "Ari", ""], ["Zhang", "Yinda", ""], ["Song", "Shuran", ""], ["Funkhouser", "Thomas", ""], ["Xiao", "Jianxiong", ""]]}, {"id": "1506.03412", "submitter": "Vamsi Ithapu", "authors": "Vamsi K. Ithapu, Sathya Ravi, Vikas Singh", "title": "Convergence rates for pretraining and dropout: Guiding learning\n  parameters using network structure", "comments": "This manuscript is now superseded by arXiv:1511.05297 and the\n  corresponding accepted paper in 54th Allerton Conference on Communication,\n  Control and Computing (2017)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unsupervised pretraining and dropout have been well studied, especially with\nrespect to regularization and output consistency. However, our understanding\nabout the explicit convergence rates of the parameter estimates, and their\ndependence on the learning (like denoising and dropout rate) and structural\n(like depth and layer lengths) aspects of the network is less mature. An\ninteresting question in this context is to ask if the network structure could\n\"guide\" the choices of such learning parameters. In this work, we explore these\ngaps between network structure, the learning mechanisms and their interaction\nwith parameter convergence rates. We present a way to address these issues\nbased on the backpropagation convergence rates for general nonconvex objectives\nusing first-order information. We then incorporate two learning mechanisms into\nthis general framework -- denoising autoencoder and dropout, and subsequently\nderive the convergence rates of deep networks. Building upon these bounds, we\nprovide insights into the choices of learning parameters and network sizes that\nachieve certain levels of convergence accuracy. The results derived here\nsupport existing empirical observations, and we also conduct a set of\nexperiments to evaluate them.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jun 2015 17:59:57 GMT"}, {"version": "v2", "created": "Tue, 17 Nov 2015 04:52:53 GMT"}, {"version": "v3", "created": "Wed, 22 Feb 2017 17:32:07 GMT"}], "update_date": "2017-02-23", "authors_parsed": [["Ithapu", "Vamsi K.", ""], ["Ravi", "Sathya", ""], ["Singh", "Vikas", ""]]}, {"id": "1506.03475", "submitter": "Yuqing Hou", "authors": "Yuqing Hou, Zhouchen Lin", "title": "Image Tag Completion and Refinement by Subspace Clustering and Matrix\n  Completion", "comments": "This paper has been withdrawn by the author due to a error in the\n  model formulation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tag-based image retrieval (TBIR) has drawn much attention in recent years due\nto the explosive amount of digital images and crowdsourcing tags. However, the\nTBIR applications still suffer from the deficient and inaccurate tags provided\nby users. Inspired by the subspace clustering methods, we formulate the tag\ncompletion problem in a subspace clustering model which assumes that images are\nsampled from subspaces, and complete the tags using the state-of-the-art Low\nRank Representation (LRR) method. And we propose a matrix completion algorithm\nto further refine the tags. Our empirical results on multiple benchmark\ndatasets for image annotation show that the proposed algorithm outperforms\nstate-of-the-art approaches when handling missing and noisy tags.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jun 2015 20:42:50 GMT"}, {"version": "v2", "created": "Mon, 8 Aug 2016 02:14:37 GMT"}], "update_date": "2016-08-09", "authors_parsed": [["Hou", "Yuqing", ""], ["Lin", "Zhouchen", ""]]}, {"id": "1506.03478", "submitter": "Lucas Theis", "authors": "Lucas Theis and Matthias Bethge", "title": "Generative Image Modeling Using Spatial LSTMs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modeling the distribution of natural images is challenging, partly because of\nstrong statistical dependencies which can extend over hundreds of pixels.\nRecurrent neural networks have been successful in capturing long-range\ndependencies in a number of problems but only recently have found their way\ninto generative image models. We here introduce a recurrent image model based\non multi-dimensional long short-term memory units which are particularly suited\nfor image modeling due to their spatial structure. Our model scales to images\nof arbitrary size and its likelihood is computationally tractable. We find that\nit outperforms the state of the art in quantitative comparisons on several\nimage datasets and produces promising results when used for texture synthesis\nand inpainting.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jun 2015 20:56:14 GMT"}, {"version": "v2", "created": "Fri, 18 Sep 2015 08:06:06 GMT"}], "update_date": "2015-09-21", "authors_parsed": [["Theis", "Lucas", ""], ["Bethge", "Matthias", ""]]}, {"id": "1506.03495", "submitter": "Jose Rodrigues Jr", "authors": "Daniel Y. T. Chino, Letricia P. S. Avalhais, Jose F. Rodrigues Jr.,\n  Agma J. M. Traina", "title": "BoWFire: Detection of Fire in Still Images by Integrating Pixel Color\n  and Texture Analysis", "comments": "8 pages, Proceedings of the 28th SIBGRAPI Conference on Graphics,\n  Patterns and Images, IEEE Press", "journal-ref": null, "doi": "10.1109/SIBGRAPI.2015.19", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Emergency events involving fire are potentially harmful, demanding a fast and\nprecise decision making. The use of crowdsourcing image and videos on crisis\nmanagement systems can aid in these situations by providing more information\nthan verbal/textual descriptions. Due to the usual high volume of data,\nautomatic solutions need to discard non-relevant content without losing\nrelevant information. There are several methods for fire detection on video\nusing color-based models. However, they are not adequate for still image\nprocessing, because they can suffer on high false-positive results. These\nmethods also suffer from parameters with little physical meaning, which makes\nfine tuning a difficult task. In this context, we propose a novel fire\ndetection method for still images that uses classification based on color\nfeatures combined with texture classification on superpixel regions. Our method\nuses a reduced number of parameters if compared to previous works, easing the\nprocess of fine tuning the method. Results show the effectiveness of our method\nof reducing false-positives while its precision remains compatible with the\nstate-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jun 2015 22:16:36 GMT"}], "update_date": "2016-11-17", "authors_parsed": [["Chino", "Daniel Y. T.", ""], ["Avalhais", "Letricia P. S.", ""], ["Rodrigues", "Jose F.", "Jr."], ["Traina", "Agma J. M.", ""]]}, {"id": "1506.03500", "submitter": "Angeliki  Lazaridou", "authors": "Angeliki Lazaridou, Dat Tien Nguyen, Raffaella Bernardi, Marco Baroni", "title": "Unveiling the Dreams of Word Embeddings: Towards Language-Driven Image\n  Generation", "comments": "A 6-page version to appear at the Multimodal Machine Learning NIPS\n  2015 Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce language-driven image generation, the task of generating an\nimage visualizing the semantic contents of a word embedding, e.g., given the\nword embedding of grasshopper, we generate a natural image of a grasshopper. We\nimplement a simple method based on two mapping functions. The first takes as\ninput a word embedding (as produced, e.g., by the word2vec toolkit) and maps it\nonto a high-level visual space (e.g., the space defined by one of the top\nlayers of a Convolutional Neural Network). The second function maps this\nabstract visual representation to pixel space, in order to generate the target\nimage. Several user studies suggest that the current system produces images\nthat capture general visual properties of the concepts encoded in the word\nembedding, such as color or typical environment, and are sufficient to\ndiscriminate between general categories of objects.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jun 2015 22:57:20 GMT"}, {"version": "v2", "created": "Mon, 23 Nov 2015 16:36:48 GMT"}], "update_date": "2015-11-24", "authors_parsed": [["Lazaridou", "Angeliki", ""], ["Nguyen", "Dat Tien", ""], ["Bernardi", "Raffaella", ""], ["Baroni", "Marco", ""]]}, {"id": "1506.03607", "submitter": "Guilhem Ch\\'eron", "authors": "Guilhem Ch\\'eron, Ivan Laptev, Cordelia Schmid", "title": "P-CNN: Pose-based CNN Features for Action Recognition", "comments": "ICCV, December 2015, Santiago, Chile", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work targets human action recognition in video. While recent methods\ntypically represent actions by statistics of local video features, here we\nargue for the importance of a representation derived from human pose. To this\nend we propose a new Pose-based Convolutional Neural Network descriptor (P-CNN)\nfor action recognition. The descriptor aggregates motion and appearance\ninformation along tracks of human body parts. We investigate different schemes\nof temporal aggregation and experiment with P-CNN features obtained both for\nautomatically estimated and manually annotated human poses. We evaluate our\nmethod on the recent and challenging JHMDB and MPII Cooking datasets. For both\ndatasets our method shows consistent improvement over the state of the art.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jun 2015 10:02:03 GMT"}, {"version": "v2", "created": "Wed, 23 Sep 2015 10:48:29 GMT"}], "update_date": "2015-09-24", "authors_parsed": [["Ch\u00e9ron", "Guilhem", ""], ["Laptev", "Ivan", ""], ["Schmid", "Cordelia", ""]]}, {"id": "1506.03648", "submitter": "Deepak Pathak", "authors": "Deepak Pathak, Philipp Kr\\\"ahenb\\\"uhl and Trevor Darrell", "title": "Constrained Convolutional Neural Networks for Weakly Supervised\n  Segmentation", "comments": "12 pages, ICCV 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an approach to learn a dense pixel-wise labeling from image-level\ntags. Each image-level tag imposes constraints on the output labeling of a\nConvolutional Neural Network (CNN) classifier. We propose Constrained CNN\n(CCNN), a method which uses a novel loss function to optimize for any set of\nlinear constraints on the output space (i.e. predicted label distribution) of a\nCNN. Our loss formulation is easy to optimize and can be incorporated directly\ninto standard stochastic gradient descent optimization. The key idea is to\nphrase the training objective as a biconvex optimization for linear models,\nwhich we then relax to nonlinear deep networks. Extensive experiments\ndemonstrate the generality of our new learning framework. The constrained loss\nyields state-of-the-art results on weakly supervised semantic image\nsegmentation. We further demonstrate that adding slightly more supervision can\ngreatly improve the performance of the learning algorithm.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jun 2015 12:30:17 GMT"}, {"version": "v2", "created": "Sun, 18 Oct 2015 23:51:40 GMT"}], "update_date": "2015-10-20", "authors_parsed": [["Pathak", "Deepak", ""], ["Kr\u00e4henb\u00fchl", "Philipp", ""], ["Darrell", "Trevor", ""]]}, {"id": "1506.03799", "submitter": "Amin Jourabloo", "authors": "Amin Jourabloo, Xiaoming Liu", "title": "Pose-Invariant 3D Face Alignment", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Face alignment aims to estimate the locations of a set of landmarks for a\ngiven image. This problem has received much attention as evidenced by the\nrecent advancement in both the methodology and performance. However, most of\nthe existing works neither explicitly handle face images with arbitrary poses,\nnor perform large-scale experiments on non-frontal and profile face images. In\norder to address these limitations, this paper proposes a novel face alignment\nalgorithm that estimates both 2D and 3D landmarks and their 2D visibilities for\na face image with an arbitrary pose. By integrating a 3D deformable model, a\ncascaded coupled-regressor approach is designed to estimate both the camera\nprojection matrix and the 3D landmarks. Furthermore, the 3D model also allows\nus to automatically estimate the 2D landmark visibilities via surface normals.\nWe gather a substantially larger collection of all-pose face images to evaluate\nour algorithm and demonstrate superior performances than the state-of-the-art\nmethods.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jun 2015 19:45:13 GMT"}], "update_date": "2015-06-12", "authors_parsed": [["Jourabloo", "Amin", ""], ["Liu", "Xiaoming", ""]]}, {"id": "1506.03844", "submitter": "Jose Rodrigues Jr", "authors": "Marcos Bedo, Gustavo Blanco, Willian Oliveira, Mirela Cazzolato, Alceu\n  Costa, Jose Rodrigues, Agma Traina and Caetano Traina Jr", "title": "Techniques for effective and efficient fire detection from social media\n  images", "comments": "12 pages, Proceedings of the International Conference on Enterprise\n  Information Systems. Specifically: Marcos Bedo, Gustavo Blanco, Willian\n  Oliveira, Mirela Cazzolato, Alceu Costa, Jose Rodrigues, Agma Traina, Caetano\n  Traina, 2015, Techniques for effective and efficient fire detection from\n  social media images, ICEIS, 34-45", "journal-ref": "Int Conf on Enterp Inf Systems 34-45 SCITEPRESS (2015)", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Social media could provide valuable information to support decision making in\ncrisis management, such as in accidents, explosions and fires. However, much of\nthe data from social media are images, which are uploaded in a rate that makes\nit impossible for human beings to analyze them. Despite the many works on image\nanalysis, there are no fire detection studies on social media. To fill this\ngap, we propose the use and evaluation of a broad set of content-based image\nretrieval and classification techniques for fire detection. Our main\ncontributions are: (i) the development of the Fast-Fire Detection method\n(FFDnR), which combines feature extractor and evaluation functions to support\ninstance-based learning, (ii) the construction of an annotated set of images\nwith ground-truth depicting fire occurrences -- the FlickrFire dataset, and\n(iii) the evaluation of 36 efficient image descriptors for fire detection.\nUsing real data from Flickr, our results showed that FFDnR was able to achieve\na precision for fire detection comparable to that of human annotators.\nTherefore, our work shall provide a solid basis for further developments on\nmonitoring images from social media.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jun 2015 21:23:38 GMT"}, {"version": "v2", "created": "Sat, 4 Jul 2015 20:02:17 GMT"}], "update_date": "2015-07-07", "authors_parsed": [["Bedo", "Marcos", ""], ["Blanco", "Gustavo", ""], ["Oliveira", "Willian", ""], ["Cazzolato", "Mirela", ""], ["Costa", "Alceu", ""], ["Rodrigues", "Jose", ""], ["Traina", "Agma", ""], ["Traina", "Caetano", "Jr"]]}, {"id": "1506.03852", "submitter": "Shell Hu", "authors": "Shell X. Hu, Christopher K. I. Williams and Sinisa Todorovic", "title": "Tree-Cut for Probabilistic Image Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a new probabilistic generative model for image\nsegmentation, i.e. the task of partitioning an image into homogeneous regions.\nOur model is grounded on a mid-level image representation, called a region\ntree, in which regions are recursively split into subregions until superpixels\nare reached. Given the region tree, image segmentation is formalized as\nsampling cuts in the tree from the model. Inference for the cuts is exact, and\nformulated using dynamic programming. Our tree-cut model can be tuned to sample\nsegmentations at a particular scale of interest out of many possible multiscale\nimage segmentations. This generalizes the common notion that there should be\nonly one correct segmentation per image. Also, it allows moving beyond the\nstandard single-scale evaluation, where the segmentation result for an image is\naveraged against the corresponding set of coarse and fine human annotations, to\nconduct a scale-specific evaluation. Our quantitative results are comparable to\nthose of the leading gPb-owt-ucm method, with the notable advantage that we\nadditionally produce a distribution over all possible tree-consistent\nsegmentations of the image.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jun 2015 21:55:06 GMT"}], "update_date": "2015-06-15", "authors_parsed": [["Hu", "Shell X.", ""], ["Williams", "Christopher K. I.", ""], ["Todorovic", "Sinisa", ""]]}, {"id": "1506.03899", "submitter": "Yiyi Liao", "authors": "Yiyi Liao, Sarath Kodagoda, Yue Wang, Lei Shi, Yong Liu", "title": "Place classification with a graph regularized deep neural network model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Place classification is a fundamental ability that a robot should possess to\ncarry out effective human-robot interactions. It is a nontrivial classification\nproblem which has attracted many research. In recent years, there is a high\nexploitation of Artificial Intelligent algorithms in robotics applications.\nInspired by the recent successes of deep learning methods, we propose an\nend-to-end learning approach for the place classification problem. With the\ndeep architectures, this methodology automatically discovers features and\ncontributes in general to higher classification accuracies. The pipeline of our\napproach is composed of three parts. Firstly, we construct multiple layers of\nlaser range data to represent the environment information in different levels\nof granularity. Secondly, each layer of data is fed into a deep neural network\nmodel for classification, where a graph regularization is imposed to the deep\narchitecture for keeping local consistency between adjacent samples. Finally,\nthe predicted labels obtained from all the layers are fused based on confidence\ntrees to maximize the overall confidence. Experimental results validate the\neffective- ness of our end-to-end place classification framework in which both\nthe multi-layer structure and the graph regularization promote the\nclassification performance. Furthermore, results show that the features\nautomatically learned from the raw input range data can achieve competitive\nresults to the features constructed based on statistical and geometrical\ninformation.\n", "versions": [{"version": "v1", "created": "Fri, 12 Jun 2015 05:45:36 GMT"}], "update_date": "2015-06-15", "authors_parsed": [["Liao", "Yiyi", ""], ["Kodagoda", "Sarath", ""], ["Wang", "Yue", ""], ["Shi", "Lei", ""], ["Liu", "Yong", ""]]}, {"id": "1506.03936", "submitter": "Mahshid Majd", "authors": "Mahshid Majd, Farzaneh Shoeleh", "title": "A Novel Hybrid Approach for Cephalometric Landmark Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cephalometric analysis has an important role in dentistry and especially in\northodontics as a treatment planning tool to gauge the size and special\nrelationships of the teeth, jaws and cranium. The first step of using such\nanalyses is localizing some important landmarks known as cephalometric\nlandmarks on craniofacial in x-ray image. The past decade has seen a growing\ninterest in automating this process. In this paper, a novel hybrid approach is\nproposed for automatic detection of cephalometric landmarks. Here, the\nlandmarks are categorized into three main sets according to their anatomical\ncharacteristics and usage in well-known cephalometric analyses. Consequently,\nto have a reliable and accurate detection system, three methods named edge\ntracing, weighted template matching, and analysis based estimation are\ndesigned, each of which is consistent and well-suited for one category. Edge\ntracing method is suggested to predict those landmarks which are located on\nedges. Weighted template matching method is well-suited for landmarks located\nin an obvious and specific structure which can be extracted or searchable in a\ngiven x-ray image. The last but not the least method is named analysis based\nestimation. This method is based on the fact that in cephalometric analyses the\nrelations between landmarks are used and the locations of some landmarks are\nnever used individually. Therefore the third suggested method has a novelty in\nestimating the desired relations directly. The effectiveness of the proposed\napproach is compared with the state of the art methods and the results were\npromising especially in real world applications.\n", "versions": [{"version": "v1", "created": "Fri, 12 Jun 2015 08:40:38 GMT"}], "update_date": "2015-06-15", "authors_parsed": [["Majd", "Mahshid", ""], ["Shoeleh", "Farzaneh", ""]]}, {"id": "1506.03995", "submitter": "Martin Kol\\'a\\v{r}", "authors": "Martin Kol\\'a\\v{r}, Michal Hradi\\v{s}, Pavel Zem\\v{c}\\'ik", "title": "Technical Report: Image Captioning with Semantically Similar Images", "comments": "3 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This report presents our submission to the MS COCO Captioning Challenge 2015.\nThe method uses Convolutional Neural Network activations as an embedding to\nfind semantically similar images. From these images, the most typical caption\nis selected based on unigram frequencies. Although the method received low\nscores with automated evaluation metrics and in human assessed average\ncorrectness, it is competitive in the ratio of captions which pass the Turing\ntest and which are assessed as better or equal to human captions.\n", "versions": [{"version": "v1", "created": "Fri, 12 Jun 2015 11:51:16 GMT"}], "update_date": "2015-06-15", "authors_parsed": [["Kol\u00e1\u0159", "Martin", ""], ["Hradi\u0161", "Michal", ""], ["Zem\u010d\u00edk", "Pavel", ""]]}, {"id": "1506.03998", "submitter": "Sohrab Ferdowsi", "authors": "Sohrab Ferdowsi, Svyatoslav Voloshynovskiy, Dimche Kostadinov", "title": "Sparse Multi-layer Image Approximation: Facial Image Compression", "comments": "Submitted to the MLSP 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IT math.IT", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  We propose a scheme for multi-layer representation of images. The problem is\nfirst treated from an information-theoretic viewpoint where we analyze the\nbehavior of different sources of information under a multi-layer data\ncompression framework and compare it with a single-stage (shallow) structure.\nWe then consider the image data as the source of information and link the\nproposed representation scheme to the problem of multi-layer dictionary\nlearning for visual data. For the current work we focus on the problem of image\ncompression for a special class of images where we report a considerable\nperformance boost in terms of PSNR at high compression ratios in comparison\nwith the JPEG2000 codec.\n", "versions": [{"version": "v1", "created": "Fri, 12 Jun 2015 12:10:57 GMT"}], "update_date": "2015-06-15", "authors_parsed": [["Ferdowsi", "Sohrab", ""], ["Voloshynovskiy", "Svyatoslav", ""], ["Kostadinov", "Dimche", ""]]}, {"id": "1506.04051", "submitter": "Lucia Maddalena", "authors": "Lucia Maddalena and Alfredo Petrosino", "title": "Towards Benchmarking Scene Background Initialization", "comments": "6 pages, SBI dataset, SBMI2015 Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  Given a set of images of a scene taken at different times, the availability\nof an initial background model that describes the scene without foreground\nobjects is the prerequisite for a wide range of applications, ranging from\nvideo surveillance to computational photography. Even though several methods\nhave been proposed for scene background initialization, the lack of a common\ngroundtruthed dataset and of a common set of metrics makes it difficult to\ncompare their performance. To move first steps towards an easy and fair\ncomparison of these methods, we assembled a dataset of sequences frequently\nadopted for background initialization, selected or created ground truths for\nquantitative evaluation through a selected suite of metrics, and compared\nresults obtained by some existing methods, making all the material publicly\navailable.\n", "versions": [{"version": "v1", "created": "Fri, 12 Jun 2015 15:52:46 GMT"}], "update_date": "2015-06-15", "authors_parsed": [["Maddalena", "Lucia", ""], ["Petrosino", "Alfredo", ""]]}, {"id": "1506.04130", "submitter": "Harsh Agrawal", "authors": "Harsh Agrawal, Clint Solomon Mathialagan, Yash Goyal, Neelima Chavali,\n  Prakriti Banik, Akrit Mohapatra, Ahmed Osman, Dhruv Batra", "title": "CloudCV: Large Scale Distributed Computer Vision as a Cloud Service", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We are witnessing a proliferation of massive visual data. Unfortunately\nscaling existing computer vision algorithms to large datasets leaves\nresearchers repeatedly solving the same algorithmic, logistical, and\ninfrastructural problems. Our goal is to democratize computer vision; one\nshould not have to be a computer vision, big data and distributed computing\nexpert to have access to state-of-the-art distributed computer vision\nalgorithms. We present CloudCV, a comprehensive system to provide access to\nstate-of-the-art distributed computer vision algorithms as a cloud service\nthrough a Web Interface and APIs.\n", "versions": [{"version": "v1", "created": "Fri, 12 Jun 2015 19:50:07 GMT"}, {"version": "v2", "created": "Wed, 15 Jun 2016 22:01:51 GMT"}, {"version": "v3", "created": "Mon, 13 Feb 2017 07:30:56 GMT"}], "update_date": "2017-02-14", "authors_parsed": [["Agrawal", "Harsh", ""], ["Mathialagan", "Clint Solomon", ""], ["Goyal", "Yash", ""], ["Chavali", "Neelima", ""], ["Banik", "Prakriti", ""], ["Mohapatra", "Akrit", ""], ["Osman", "Ahmed", ""], ["Batra", "Dhruv", ""]]}, {"id": "1506.04191", "submitter": "Zhiwei Deng", "authors": "Zhiwei Deng, Mengyao Zhai, Lei Chen, Yuhao Liu, Srikanth Muralidharan,\n  Mehrsan Javan Roshtkhari and Greg Mori", "title": "Deep Structured Models For Group Activity Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a deep neural-network-based hierarchical graphical model\nfor individual and group activity recognition in surveillance scenes. Deep\nnetworks are used to recognize the actions of individual people in a scene.\nNext, a neural-network-based hierarchical graphical model refines the predicted\nlabels for each class by considering dependencies between the classes. This\nrefinement step mimics a message-passing step similar to inference in a\nprobabilistic graphical model. We show that this approach can be effective in\ngroup activity recognition, with the deep graphical model improving recognition\nrates over baseline methods.\n", "versions": [{"version": "v1", "created": "Fri, 12 Jun 2015 22:18:08 GMT"}], "update_date": "2015-06-16", "authors_parsed": [["Deng", "Zhiwei", ""], ["Zhai", "Mengyao", ""], ["Chen", "Lei", ""], ["Liu", "Yuhao", ""], ["Muralidharan", "Srikanth", ""], ["Roshtkhari", "Mehrsan Javan", ""], ["Mori", "Greg", ""]]}, {"id": "1506.04214", "submitter": "Xingjian Shi", "authors": "Xingjian Shi, Zhourong Chen, Hao Wang, Dit-Yan Yeung, Wai-kin Wong,\n  Wang-chun Woo", "title": "Convolutional LSTM Network: A Machine Learning Approach for\n  Precipitation Nowcasting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The goal of precipitation nowcasting is to predict the future rainfall\nintensity in a local region over a relatively short period of time. Very few\nprevious studies have examined this crucial and challenging weather forecasting\nproblem from the machine learning perspective. In this paper, we formulate\nprecipitation nowcasting as a spatiotemporal sequence forecasting problem in\nwhich both the input and the prediction target are spatiotemporal sequences. By\nextending the fully connected LSTM (FC-LSTM) to have convolutional structures\nin both the input-to-state and state-to-state transitions, we propose the\nconvolutional LSTM (ConvLSTM) and use it to build an end-to-end trainable model\nfor the precipitation nowcasting problem. Experiments show that our ConvLSTM\nnetwork captures spatiotemporal correlations better and consistently\noutperforms FC-LSTM and the state-of-the-art operational ROVER algorithm for\nprecipitation nowcasting.\n", "versions": [{"version": "v1", "created": "Sat, 13 Jun 2015 03:19:24 GMT"}, {"version": "v2", "created": "Sat, 19 Sep 2015 11:02:03 GMT"}], "update_date": "2015-09-22", "authors_parsed": [["Shi", "Xingjian", ""], ["Chen", "Zhourong", ""], ["Wang", "Hao", ""], ["Yeung", "Dit-Yan", ""], ["Wong", "Wai-kin", ""], ["Woo", "Wang-chun", ""]]}, {"id": "1506.04304", "submitter": "Jeremy Maitin-Shepard", "authors": "Jeremy Maitin-Shepard (1 and 2), Viren Jain (2), Michal Januszewski\n  (2), Peter Li (2), Pieter Abbeel (1) ((1) UC Berkeley, (2) Google)", "title": "Combinatorial Energy Learning for Image Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new machine learning approach for image segmentation that uses\na neural network to model the conditional energy of a segmentation given an\nimage. Our approach, combinatorial energy learning for image segmentation\n(CELIS) places a particular emphasis on modeling the inherent combinatorial\nnature of dense image segmentation problems. We propose efficient algorithms\nfor learning deep neural networks to model the energy function, and for local\noptimization of this energy in the space of supervoxel agglomerations. We\nextensively evaluate our method on a publicly available 3-D microscopy dataset\nwith 25 billion voxels of ground truth data. On an 11 billion voxel test set,\nwe find that our method improves volumetric reconstruction accuracy by more\nthan 20% as compared to two state-of-the-art baseline methods: graph-based\nsegmentation of the output of a 3-D convolutional neural network trained to\npredict boundaries, as well as a random forest classifier trained to\nagglomerate supervoxels that were generated by a 3-D convolutional neural\nnetwork.\n", "versions": [{"version": "v1", "created": "Sat, 13 Jun 2015 18:23:42 GMT"}, {"version": "v2", "created": "Tue, 16 Jun 2015 19:33:20 GMT"}, {"version": "v3", "created": "Fri, 23 Sep 2016 20:47:55 GMT"}], "update_date": "2016-09-27", "authors_parsed": [["Maitin-Shepard", "Jeremy", "", "1 and 2"], ["Jain", "Viren", "", "Google"], ["Januszewski", "Michal", "", "Google"], ["Li", "Peter", "", "Google"], ["Abbeel", "Pieter", "", "UC Berkeley"]]}, {"id": "1506.04338", "submitter": "Wei Yang", "authors": "Wei Yang and Haiting Lin and Sing Bing Kang and Jingyi Yu", "title": "Resolving Scale Ambiguity Via XSlit Aspect Ratio Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In perspective cameras, images of a frontal-parallel 3D object preserve its\naspect ratio invariant to its depth. Such an invariance is useful in\nphotography but is unique to perspective projection. In this paper, we show\nthat alternative non-perspective cameras such as the crossed-slit or XSlit\ncameras exhibit a different depth-dependent aspect ratio (DDAR) property that\ncan be used to 3D recovery. We first conduct a comprehensive analysis to\ncharacterize DDAR, infer object depth from its AR, and model recoverable depth\nrange, sensitivity, and error. We show that repeated shape patterns in real\nManhattan World scenes can be used for 3D reconstruction using a single XSlit\nimage. We also extend our analysis to model slopes of lines. Specifically,\nparallel 3D lines exhibit depth-dependent slopes (DDS) on their images which\ncan also be used to infer their depths. We validate our analyses using real\nXSlit cameras, XSlit panoramas, and catadioptric mirrors. Experiments show that\nDDAR and DDS provide important depth cues and enable effective single-image\nscene reconstruction.\n", "versions": [{"version": "v1", "created": "Sun, 14 Jun 2015 00:07:46 GMT"}], "update_date": "2015-06-16", "authors_parsed": [["Yang", "Wei", ""], ["Lin", "Haiting", ""], ["Kang", "Sing Bing", ""], ["Yu", "Jingyi", ""]]}, {"id": "1506.04340", "submitter": "Yingbo Zhou", "authors": "Rohit Pandey, Yingbo Zhou, Venu Govindaraju", "title": "Deep Secure Encoding: An Application to Face Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present Deep Secure Encoding: a framework for secure\nclassification using deep neural networks, and apply it to the task of\nbiometric template protection for faces. Using deep convolutional neural\nnetworks (CNNs), we learn a robust mapping of face classes to high entropy\nsecure codes. These secure codes are then hashed using standard hash functions\nlike SHA-256 to generate secure face templates. The efficacy of the approach is\nshown on two face databases, namely, CMU-PIE and Extended Yale B, where we\nachieve state of the art matching performance, along with cancelability and\nhigh security with no unrealistic assumptions. Furthermore, the scheme can work\nin both identification and verification modes.\n", "versions": [{"version": "v1", "created": "Sun, 14 Jun 2015 00:39:20 GMT"}], "update_date": "2015-06-16", "authors_parsed": [["Pandey", "Rohit", ""], ["Zhou", "Yingbo", ""], ["Govindaraju", "Venu", ""]]}, {"id": "1506.04356", "submitter": "Milan Rajkovic", "authors": "Milan Rajkovi\\'c and Milo\\v{s} Milovanovi\\'c", "title": "The Artists who Forged Themselves: Detecting Creativity in Art", "comments": "26 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Creativity and the understanding of cognitive processes involved in the\ncreative process are relevant to all of human activities. Comprehension of\ncreativity in the arts is of special interest due to the involvement of many\nscientific and non scientific disciplines. Using digital representation of\npaintings, we show that creative process in painting art may be objectively\nrecognized within the mathematical framework of self organization, a process\ncharacteristic of nonlinear dynamic systems and occurring in natural and social\nsciences. Unlike the artist identification process or the recognition of\nforgery, which presupposes the knowledge of the original work, our method\nrequires no prior knowledge on the originality of the work of art. The original\npaintings are recognized as realizations of the creative process which, in\ngeneral, is shown to correspond to self-organization of texture features which\ndetermine the aesthetic complexity of the painting. The method consists of the\nwavelet based statistical digital image processing and the measure of\nstatistical complexity which represents the minimal (average) information\nnecessary for optimal prediction. The statistical complexity is based on the\nproperly defined causal states with optimal predictive properties. Two\ndifferent time concepts related to the works of art are introduced: the\ninternal time and the artistic time. The internal time of the artwork is\ndetermined by the span of causal dependencies between wavelet coefficients\nwhile the artistic time refers to the internal time during which complexity\nincreases where complexity refers to compositional, aesthetic and structural\narrangement of texture features. The method is illustrated by recognizing the\noriginal paintings from the copies made by the artists themselves, including\nthe works of the famous surrealist painter Ren\\'{e} Magritte.\n", "versions": [{"version": "v1", "created": "Sun, 14 Jun 2015 06:44:34 GMT"}], "update_date": "2015-06-16", "authors_parsed": [["Rajkovi\u0107", "Milan", ""], ["Milovanovi\u0107", "Milo\u0161", ""]]}, {"id": "1506.04395", "submitter": "Weilin Huang", "authors": "Pan He, Weilin Huang, Yu Qiao, Chen Change Loy and Xiaoou Tang", "title": "Reading Scene Text in Deep Convolutional Sequences", "comments": "To appear in the 13th AAAI Conference on Artificial Intelligence\n  (AAAI-16), 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a Deep-Text Recurrent Network (DTRN) that regards scene text\nreading as a sequence labelling problem. We leverage recent advances of deep\nconvolutional neural networks to generate an ordered high-level sequence from a\nwhole word image, avoiding the difficult character segmentation problem. Then a\ndeep recurrent model, building on long short-term memory (LSTM), is developed\nto robustly recognize the generated CNN sequences, departing from most existing\napproaches recognising each character independently. Our model has a number of\nappealing properties in comparison to existing scene text recognition methods:\n(i) It can recognise highly ambiguous words by leveraging meaningful context\ninformation, allowing it to work reliably without either pre- or\npost-processing; (ii) the deep CNN feature is robust to various image\ndistortions; (iii) it retains the explicit order information in word image,\nwhich is essential to discriminate word strings; (iv) the model does not depend\non pre-defined dictionary, and it can process unknown words and arbitrary\nstrings. Codes for the DTRN will be available.\n", "versions": [{"version": "v1", "created": "Sun, 14 Jun 2015 13:34:10 GMT"}, {"version": "v2", "created": "Sun, 20 Dec 2015 21:06:23 GMT"}], "update_date": "2015-12-22", "authors_parsed": [["He", "Pan", ""], ["Huang", "Weilin", ""], ["Qiao", "Yu", ""], ["Loy", "Chen Change", ""], ["Tang", "Xiaoou", ""]]}, {"id": "1506.04449", "submitter": "Wenlin Chen", "authors": "Wenlin Chen, James T. Wilson, Stephen Tyree, Kilian Q. Weinberger,\n  Yixin Chen", "title": "Compressing Convolutional Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional neural networks (CNN) are increasingly used in many areas of\ncomputer vision. They are particularly attractive because of their ability to\n\"absorb\" great quantities of labeled data through millions of parameters.\nHowever, as model sizes increase, so do the storage and memory requirements of\nthe classifiers. We present a novel network architecture, Frequency-Sensitive\nHashed Nets (FreshNets), which exploits inherent redundancy in both\nconvolutional layers and fully-connected layers of a deep learning model,\nleading to dramatic savings in memory and storage consumption. Based on the key\nobservation that the weights of learned convolutional filters are typically\nsmooth and low-frequency, we first convert filter weights to the frequency\ndomain with a discrete cosine transform (DCT) and use a low-cost hash function\nto randomly group frequency parameters into hash buckets. All parameters\nassigned the same hash bucket share a single value learned with standard\nback-propagation. To further reduce model size we allocate fewer hash buckets\nto high-frequency components, which are generally less important. We evaluate\nFreshNets on eight data sets, and show that it leads to drastically better\ncompressed performance than several relevant baselines.\n", "versions": [{"version": "v1", "created": "Sun, 14 Jun 2015 23:16:40 GMT"}], "update_date": "2015-06-16", "authors_parsed": [["Chen", "Wenlin", ""], ["Wilson", "James T.", ""], ["Tyree", "Stephen", ""], ["Weinberger", "Kilian Q.", ""], ["Chen", "Yixin", ""]]}, {"id": "1506.04472", "submitter": "Elham Sagheb", "authors": "Elham Sagheb Hossein Pour", "title": "A Survey of Multithreading Image Analysis", "comments": "6 Pages, 1 Figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Digital image analysis has made a big advance in many areas of enterprise\napplications including medicine, industry, and entertainment by assisting the\nextraction of semantic information from digital images. However, its large\ncomputational complexity has been a trouble to most real-time developments.\nWhile image analysis in general has been studied for a log period in computer\nscience community, the use of multithreading strategy as the most efficient\nimproving computational capacity technique has been limited so far. In this\nsurvey an attempt is made to explain the current knowledge and so far\nprogresses in incorporating image analysis with multithreading approaches. The\npresent work also provides insights and tendencies for the possible future\nenhancement of multithreading image analysis.\n", "versions": [{"version": "v1", "created": "Mon, 15 Jun 2015 03:52:36 GMT"}, {"version": "v2", "created": "Sat, 20 Jun 2015 18:54:22 GMT"}, {"version": "v3", "created": "Sun, 5 Jul 2015 19:04:36 GMT"}, {"version": "v4", "created": "Thu, 9 Jul 2015 21:21:36 GMT"}, {"version": "v5", "created": "Mon, 24 Aug 2015 16:38:45 GMT"}, {"version": "v6", "created": "Wed, 4 Oct 2017 02:55:03 GMT"}], "update_date": "2017-10-05", "authors_parsed": [["Pour", "Elham Sagheb Hossein", ""]]}, {"id": "1506.04500", "submitter": "Yustinus Soelistio Eko", "authors": "Yustinus Eko Soelistio, Eric Postma, Alfons Maes", "title": "Circle-based Eye Center Localization (CECL)", "comments": "Published and presented at The 14th IAPR International Conference on\n  Machine Vision Applications, 2015. http://www.mva-org.jp/mva2015/", "journal-ref": null, "doi": "10.1109/MVA.2015.7153202", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an improved eye center localization method based on the Hough\ntransform, called Circle-based Eye Center Localization (CECL) that is simple,\nrobust, and achieves accuracy on a par with typically more complex\nstate-of-the-art methods. The CECL method relies on color and shape cues that\ndistinguish the iris from other facial structures. The accuracy of the CECL\nmethod is demonstrated through a comparison with 15 state-of-the-art eye center\nlocalization methods against five error thresholds, as reported in the\nliterature. The CECL method achieved an accuracy of 80.8% to 99.4% and ranked\nfirst for 2 of the 5 thresholds. It is concluded that the CECL method offers an\nattractive alternative to existing methods for automatic eye center\nlocalization.\n", "versions": [{"version": "v1", "created": "Mon, 15 Jun 2015 07:54:13 GMT"}, {"version": "v2", "created": "Fri, 21 Aug 2015 01:52:53 GMT"}], "update_date": "2015-08-24", "authors_parsed": [["Soelistio", "Yustinus Eko", ""], ["Postma", "Eric", ""], ["Maes", "Alfons", ""]]}, {"id": "1506.04566", "submitter": "Laurent Hoeltgen", "authors": "Laurent Hoeltgen and Markus Mainberger and Sebastian Hoffmann and\n  Joachim Weickert and Ching Hoo Tang and Simon Setzer and Daniel Johannsen and\n  Frank Neumann and Benjamin Doerr", "title": "Optimising Spatial and Tonal Data for PDE-based Inpainting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Some recent methods for lossy signal and image compression store only a few\nselected pixels and fill in the missing structures by inpainting with a partial\ndifferential equation (PDE). Suitable operators include the Laplacian, the\nbiharmonic operator, and edge-enhancing anisotropic diffusion (EED). The\nquality of such approaches depends substantially on the selection of the data\nthat is kept. Optimising this data in the domain and codomain gives rise to\nchallenging mathematical problems that shall be addressed in our work.\n  In the 1D case, we prove results that provide insights into the difficulty of\nthis problem, and we give evidence that a splitting into spatial and tonal\n(i.e. function value) optimisation does hardly deteriorate the results. In the\n2D setting, we present generic algorithms that achieve a high reconstruction\nquality even if the specified data is very sparse. To optimise the spatial\ndata, we use a probabilistic sparsification, followed by a nonlocal pixel\nexchange that avoids getting trapped in bad local optima. After this spatial\noptimisation we perform a tonal optimisation that modifies the function values\nin order to reduce the global reconstruction error. For homogeneous diffusion\ninpainting, this comes down to a least squares problem for which we prove that\nit has a unique solution. We demonstrate that it can be found efficiently with\na gradient descent approach that is accelerated with fast explicit diffusion\n(FED) cycles. Our framework allows to specify the desired density of the\ninpainting mask a priori. Moreover, is more generic than other data\noptimisation approaches for the sparse inpainting problem, since it can also be\nextended to nonlinear inpainting operators such as EED. This is exploited to\nachieve reconstructions with state-of-the-art quality.\n  We also give an extensive literature survey on PDE-based image compression\nmethods.\n", "versions": [{"version": "v1", "created": "Mon, 15 Jun 2015 12:15:26 GMT"}], "update_date": "2015-06-16", "authors_parsed": [["Hoeltgen", "Laurent", ""], ["Mainberger", "Markus", ""], ["Hoffmann", "Sebastian", ""], ["Weickert", "Joachim", ""], ["Tang", "Ching Hoo", ""], ["Setzer", "Simon", ""], ["Johannsen", "Daniel", ""], ["Neumann", "Frank", ""], ["Doerr", "Benjamin", ""]]}, {"id": "1506.04579", "submitter": "Wei Liu", "authors": "Wei Liu, Andrew Rabinovich, Alexander C. Berg", "title": "ParseNet: Looking Wider to See Better", "comments": "ICLR 2016 submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a technique for adding global context to deep convolutional\nnetworks for semantic segmentation. The approach is simple, using the average\nfeature for a layer to augment the features at each location. In addition, we\nstudy several idiosyncrasies of training, significantly increasing the\nperformance of baseline networks (e.g. from FCN). When we add our proposed\nglobal feature, and a technique for learning normalization parameters, accuracy\nincreases consistently even over our improved versions of the baselines. Our\nproposed approach, ParseNet, achieves state-of-the-art performance on SiftFlow\nand PASCAL-Context with small additional computational cost over baselines, and\nnear current state-of-the-art performance on PASCAL VOC 2012 semantic\nsegmentation with a simple approach. Code is available at\nhttps://github.com/weiliu89/caffe/tree/fcn .\n", "versions": [{"version": "v1", "created": "Mon, 15 Jun 2015 13:00:59 GMT"}, {"version": "v2", "created": "Thu, 19 Nov 2015 22:19:28 GMT"}], "update_date": "2015-11-23", "authors_parsed": [["Liu", "Wei", ""], ["Rabinovich", "Andrew", ""], ["Berg", "Alexander C.", ""]]}, {"id": "1506.04608", "submitter": "Daja Abdul", "authors": "Javairia Nazir, Mehreen Sirshar", "title": "Flow Segmentation in Dense Crowds", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A framework is proposed in this paper that is used to segment flow of dense\ncrowds. The flow field that is generated by the movement in the crowd is\ntreated just like an aperiodic dynamic system. On this flow field a grid of\nparticles is put over for particle advection by the use of a numerical\nintegration scheme. Then flow maps are generated which associates the initial\nposition of the particles with final position. The gradient of the flow maps\ngives the amount of divergence of the neighboring particles. For forward\nintegration and analysis forward Finite time Lyapunov Exponent is calculated\nand backward Finite time Lyapunov Exponent is also calculated it gives the\nLagrangian coherent structures of the flow in crowd. Lagrangian Coherent\nStructures basically divides the flow in crowd into regions and these regions\nhave different dynamics. These regions are then used to get the boundary in the\ndifferent flow segments by using water shed algorithm. The experiment is\nconducted on the crowd dataset of UCF (University of central Florida).\n", "versions": [{"version": "v1", "created": "Mon, 15 Jun 2015 14:14:20 GMT"}], "update_date": "2015-06-16", "authors_parsed": [["Nazir", "Javairia", ""], ["Sirshar", "Mehreen", ""]]}, {"id": "1506.04654", "submitter": "Dmitrii Marin", "authors": "Dmitrii Marin, Yuri Boykov, Yuchen Zhong", "title": "Thin Structure Estimation with Curvature Regularization", "comments": "D. Marin, Y. Zhong, M. Drangova, Y. Boykov. Thin Structure Estimation\n  with Curvature Regularization. International Conference on Computer Vision\n  (ICCV), Santiago, Chili, December 2015, to appear", "journal-ref": "The IEEE International Conference on Computer Vision (ICCV), 2015,\n  pp. 397-405", "doi": "10.1109/ICCV.2015.53", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many applications in vision require estimation of thin structures such as\nboundary edges, surfaces, roads, blood vessels, neurons, etc. Unlike most\nprevious approaches, we simultaneously detect and delineate thin structures\nwith sub-pixel localization and real-valued orientation estimation. This is an\nill-posed problem that requires regularization. We propose an objective\nfunction combining detection likelihoods with a prior minimizing curvature of\nthe center-lines or surfaces. Unlike simple block-coordinate descent, we\ndevelop a novel algorithm that is able to perform joint optimization of\nlocation and detection variables more effectively. Our lower bound optimization\nalgorithm applies to quadratic or absolute curvature. The proposed early vision\nframework is sufficiently general and it can be used in many higher-level\napplications. We illustrate the advantage of our approach on a range of 2D and\n3D examples.\n", "versions": [{"version": "v1", "created": "Mon, 15 Jun 2015 16:28:01 GMT"}, {"version": "v2", "created": "Wed, 16 Sep 2015 15:40:15 GMT"}], "update_date": "2017-05-23", "authors_parsed": [["Marin", "Dmitrii", ""], ["Boykov", "Yuri", ""], ["Zhong", "Yuchen", ""]]}, {"id": "1506.04655", "submitter": "Yang Zhong", "authors": "Yang Zhong, Haibo Li", "title": "Leveraging the Power of Gabor Phase for Face Identification: A Block\n  Matching Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  Different from face verification, face identification is much more demanding.\nTo reach comparable performance, an identifier needs to be roughly N times\nbetter than a verifier. To expect a breakthrough in face identification, we\nneed a fresh look at the fundamental building blocks of face recognition. In\nthis paper we focus on the selection of a suitable signal representation and\nbetter matching strategy for face identification. We demonstrate how Gabor\nphase could be leveraged to improve the performance of face identification by\nusing the Block Matching method. Compared to the existing approaches, the\nproposed method features much lower algorithmic complexity: face images are\nonly filtered by a single-scale Gabor filter pair and the matching is performed\nbetween any pairs of face images at hand without involving any training\nprocess. Benchmark evaluations show that the proposed approach is totally\ncomparable to and even better than state-of-the-art algorithms, which are\ntypically based on more features extracted from a large set of Gabor faces\nand/or rely on heavy training processes.\n", "versions": [{"version": "v1", "created": "Mon, 15 Jun 2015 16:28:03 GMT"}], "update_date": "2015-06-16", "authors_parsed": [["Zhong", "Yang", ""], ["Li", "Haibo", ""]]}, {"id": "1506.04701", "submitter": "Mingming Wang", "authors": "Mingming Wang", "title": "Multi-path Convolutional Neural Networks for Complex Image\n  Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional Neural Networks demonstrate high performance on ImageNet\nLarge-Scale Visual Recognition Challenges contest. Nevertheless, the published\nresults only show the overall performance for all image classes. There is no\nfurther analysis why certain images get worse results and how they could be\nimproved. In this paper, we provide deep performance analysis based on\ndifferent types of images and point out the weaknesses of convolutional neural\nnetworks through experiment. We design a novel multiple paths convolutional\nneural network, which feeds different versions of images into separated paths\nto learn more comprehensive features. This model has better presentation for\nimage than the traditional single path model. We acquire better classification\nresults on complex validation set on both top 1 and top 5 scores than the best\nILSVRC 2013 classification model.\n", "versions": [{"version": "v1", "created": "Mon, 15 Jun 2015 18:51:35 GMT"}, {"version": "v2", "created": "Tue, 16 Jun 2015 19:30:55 GMT"}, {"version": "v3", "created": "Mon, 22 Jun 2015 16:22:23 GMT"}], "update_date": "2015-06-23", "authors_parsed": [["Wang", "Mingming", ""]]}, {"id": "1506.04714", "submitter": "Dinesh Jayaraman", "authors": "Dinesh Jayaraman and Kristen Grauman", "title": "Slow and steady feature analysis: higher order temporal coherence in\n  video", "comments": "in Computer Vision and Pattern Recognition (CVPR) 2016, Las Vegas,\n  NV, June 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  How can unlabeled video augment visual learning? Existing methods perform\n\"slow\" feature analysis, encouraging the representations of temporally close\nframes to exhibit only small differences. While this standard approach captures\nthe fact that high-level visual signals change slowly over time, it fails to\ncapture *how* the visual content changes. We propose to generalize slow feature\nanalysis to \"steady\" feature analysis. The key idea is to impose a prior that\nhigher order derivatives in the learned feature space must be small. To this\nend, we train a convolutional neural network with a regularizer on tuples of\nsequential frames from unlabeled video. It encourages feature changes over time\nto be smooth, i.e., similar to the most recent changes. Using five diverse\ndatasets, including unlabeled YouTube and KITTI videos, we demonstrate our\nmethod's impact on object, scene, and action recognition tasks. We further show\nthat our features learned from unlabeled video can even surpass a standard\nheavily supervised pretraining approach.\n", "versions": [{"version": "v1", "created": "Mon, 15 Jun 2015 19:26:38 GMT"}, {"version": "v2", "created": "Thu, 14 Apr 2016 18:37:33 GMT"}], "update_date": "2016-04-15", "authors_parsed": [["Jayaraman", "Dinesh", ""], ["Grauman", "Kristen", ""]]}, {"id": "1506.04721", "submitter": "Qiaosong Wang", "authors": "Qiaosong Wang, Haiting Lin, Yi Ma, Sing Bing Kang, Jingyi Yu", "title": "Automatic Layer Separation using Light Field Imaging", "comments": "9 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  We propose a novel approach that jointly removes reflection or translucent\nlayer from a scene and estimates scene depth. The input data are captured via\nlight field imaging. The problem is couched as minimizing the rank of the\ntransmitted scene layer via Robust Principle Component Analysis (RPCA). We also\nimpose regularization based on piecewise smoothness, gradient sparsity, and\nlayer independence to simultaneously recover 3D geometry of the transmitted\nlayer. Experimental results on synthetic and real data show that our technique\nis robust and reliable, and can handle a broad range of layer separation\nproblems.\n", "versions": [{"version": "v1", "created": "Mon, 15 Jun 2015 19:36:25 GMT"}], "update_date": "2015-06-16", "authors_parsed": [["Wang", "Qiaosong", ""], ["Lin", "Haiting", ""], ["Ma", "Yi", ""], ["Kang", "Sing Bing", ""], ["Yu", "Jingyi", ""]]}, {"id": "1506.04723", "submitter": "Ming-Yu Liu", "authors": "Ming-Yu Liu, Shuoxin Lin, Srikumar Ramalingam, Oncel Tuzel", "title": "Layered Interpretation of Street View Images", "comments": "The paper will be presented in the 2015 Robotics: Science and Systems\n  Conference (RSS)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a layered street view model to encode both depth and semantic\ninformation on street view images for autonomous driving. Recently, stixels,\nstix-mantics, and tiered scene labeling methods have been proposed to model\nstreet view images. We propose a 4-layer street view model, a compact\nrepresentation over the recently proposed stix-mantics model. Our layers encode\nsemantic classes like ground, pedestrians, vehicles, buildings, and sky in\naddition to the depths. The only input to our algorithm is a pair of stereo\nimages. We use a deep neural network to extract the appearance features for\nsemantic classes. We use a simple and an efficient inference algorithm to\njointly estimate both semantic classes and layered depth values. Our method\noutperforms other competing approaches in Daimler urban scene segmentation\ndataset. Our algorithm is massively parallelizable, allowing a GPU\nimplementation with a processing speed about 9 fps.\n", "versions": [{"version": "v1", "created": "Mon, 15 Jun 2015 19:38:59 GMT"}, {"version": "v2", "created": "Wed, 29 Jul 2015 15:38:28 GMT"}], "update_date": "2015-07-30", "authors_parsed": [["Liu", "Ming-Yu", ""], ["Lin", "Shuoxin", ""], ["Ramalingam", "Srikumar", ""], ["Tuzel", "Oncel", ""]]}, {"id": "1506.04757", "submitter": "Julian McAuley", "authors": "Julian McAuley, Christopher Targett, Qinfeng Shi, Anton van den Hengel", "title": "Image-based Recommendations on Styles and Substitutes", "comments": "11 pages, 10 figures, SIGIR 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Humans inevitably develop a sense of the relationships between objects, some\nof which are based on their appearance. Some pairs of objects might be seen as\nbeing alternatives to each other (such as two pairs of jeans), while others may\nbe seen as being complementary (such as a pair of jeans and a matching shirt).\nThis information guides many of the choices that people make, from buying\nclothes to their interactions with each other. We seek here to model this human\nsense of the relationships between objects based on their appearance. Our\napproach is not based on fine-grained modeling of user annotations but rather\non capturing the largest dataset possible and developing a scalable method for\nuncovering human notions of the visual relationships within. We cast this as a\nnetwork inference problem defined on graphs of related images, and provide a\nlarge-scale dataset for the training and evaluation of the same. The system we\ndevelop is capable of recommending which clothes and accessories will go well\ntogether (and which will not), amongst a host of other applications.\n", "versions": [{"version": "v1", "created": "Mon, 15 Jun 2015 20:01:49 GMT"}], "update_date": "2015-06-17", "authors_parsed": [["McAuley", "Julian", ""], ["Targett", "Christopher", ""], ["Shi", "Qinfeng", ""], ["Hengel", "Anton van den", ""]]}, {"id": "1506.04843", "submitter": "Anirban Dasgupta", "authors": "Anirban Dasgupta, Suvodip Chakrborty, Aritra Chaudhuri, Aurobinda\n  Routray", "title": "Evaluation of Denoising Techniques for EOG signals based on SNR\n  Estimation", "comments": "in IEEE 2016 International Conference on Systems in Medicine and\n  Biology (ICSMB)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper evaluates four algorithms for denoising raw Electrooculography\n(EOG) data based on the Signal to Noise Ratio (SNR). The SNR is computed using\nthe eigenvalue method. The filtering algorithms are a) Finite Impulse Response\n(FIR) bandpass filters, b) Stationary Wavelet Transform, c) Empirical Mode\nDecomposition (EMD) d) FIR Median Hybrid Filters. An EOG dataset has been\nprepared where the subject is asked to perform letter cancelation test on 20\nsubjects.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jun 2015 06:07:21 GMT"}, {"version": "v2", "created": "Thu, 27 Oct 2016 12:27:47 GMT"}], "update_date": "2016-10-28", "authors_parsed": [["Dasgupta", "Anirban", ""], ["Chakrborty", "Suvodip", ""], ["Chaudhuri", "Aritra", ""], ["Routray", "Aurobinda", ""]]}, {"id": "1506.04878", "submitter": "Mykhaylo Andriluka", "authors": "Russell Stewart and Mykhaylo Andriluka", "title": "End-to-end people detection in crowded scenes", "comments": "9 pages, 7 figures. Submitted to NIPS 2015. Supplementary material\n  video: http://www.youtube.com/watch?v=QeWl0h3kQ24", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current people detectors operate either by scanning an image in a sliding\nwindow fashion or by classifying a discrete set of proposals. We propose a\nmodel that is based on decoding an image into a set of people detections. Our\nsystem takes an image as input and directly outputs a set of distinct detection\nhypotheses. Because we generate predictions jointly, common post-processing\nsteps such as non-maximum suppression are unnecessary. We use a recurrent LSTM\nlayer for sequence generation and train our model end-to-end with a new loss\nfunction that operates on sets of detections. We demonstrate the effectiveness\nof our approach on the challenging task of detecting people in crowded scenes.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jun 2015 08:47:16 GMT"}, {"version": "v2", "created": "Wed, 17 Jun 2015 00:19:59 GMT"}, {"version": "v3", "created": "Wed, 8 Jul 2015 21:55:17 GMT"}], "update_date": "2015-07-10", "authors_parsed": [["Stewart", "Russell", ""], ["Andriluka", "Mykhaylo", ""]]}, {"id": "1506.04912", "submitter": "Vahid Abolghasemi", "authors": "Vahid Abolghasemi, Hao Shen, Yaochun Shen, Lu Gan", "title": "Subsampled terahertz data reconstruction based on spatio-temporal\n  dictionary learning", "comments": null, "journal-ref": null, "doi": "10.1016/j.dsp.2015.04.010", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, the problem of terahertz pulsed imaging and reconstruction is\naddressed. It is assumed that an incomplete (subsampled) three dimensional THz\ndata set has been acquired and the aim is to recover all missing samples. A\nsparsity-inducing approach is proposed for this purpose. First, a simple\ninterpolation is applied to incomplete noisy data. Then, we propose a\nspatio-temporal dictionary learning method to obtain an appropriate sparse\nrepresentation of data based on a joint sparse recovery algorithm. Then, using\nthe sparse coefficients and the learned dictionary, the 3D data is effectively\ndenoised by minimizing a simple cost function. We consider two types of\nterahertz data to evaluate the performance of the proposed approach; THz data\nacquired for a model sample with clear layered structures (e.g., a T-shape\nplastic sheet buried in a polythene pellet), and pharmaceutical tablet data\n(with low spatial resolution). The achieved signal-to-noise-ratio for\nreconstruction of T-shape data, from only 5% observation was 19 dB. Moreover,\nthe accuracies of obtained thickness and depth measurements for pharmaceutical\ntablet data after reconstruction from 10% observation were 98.8%, and 99.9%,\nrespectively. These results, along with chemical mapping analysis, presented at\nthe end of this paper, confirm the accuracy of the proposed method.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jun 2015 10:55:46 GMT"}], "update_date": "2015-06-17", "authors_parsed": [["Abolghasemi", "Vahid", ""], ["Shen", "Hao", ""], ["Shen", "Yaochun", ""], ["Gan", "Lu", ""]]}, {"id": "1506.04924", "submitter": "Seunghoon Hong", "authors": "Seunghoon Hong, Hyeonwoo Noh, Bohyung Han", "title": "Decoupled Deep Neural Network for Semi-supervised Semantic Segmentation", "comments": "Added a link to the project page for more comprehensive illustration\n  of results", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel deep neural network architecture for semi-supervised\nsemantic segmentation using heterogeneous annotations. Contrary to existing\napproaches posing semantic segmentation as a single task of region-based\nclassification, our algorithm decouples classification and segmentation, and\nlearns a separate network for each task. In this architecture, labels\nassociated with an image are identified by classification network, and binary\nsegmentation is subsequently performed for each identified label in\nsegmentation network. The decoupled architecture enables us to learn\nclassification and segmentation networks separately based on the training data\nwith image-level and pixel-wise class labels, respectively. It facilitates to\nreduce search space for segmentation effectively by exploiting class-specific\nactivation maps obtained from bridging layers. Our algorithm shows outstanding\nperformance compared to other semi-supervised approaches even with much less\ntraining images with strong annotations in PASCAL VOC dataset.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jun 2015 11:20:04 GMT"}, {"version": "v2", "created": "Wed, 17 Jun 2015 08:38:32 GMT"}], "update_date": "2015-06-18", "authors_parsed": [["Hong", "Seunghoon", ""], ["Noh", "Hyeonwoo", ""], ["Han", "Bohyung", ""]]}, {"id": "1506.04935", "submitter": "St\\'ephanie Gu\\'erit", "authors": "St\\'ephanie Gu\\'erit, Laurent Jacques, Beno\\^it Macq, John A. Lee", "title": "Post-Reconstruction Deconvolution of PET Images by Total Generalized\n  Variation Regularization", "comments": "First published in the Proceedings of the 23rd European Signal\n  Processing Conference (EUSIPCO-2015) in 2015, published by EURASIP", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Improving the quality of positron emission tomography (PET) images, affected\nby low resolution and high level of noise, is a challenging task in nuclear\nmedicine and radiotherapy. This work proposes a restoration method, achieved\nafter tomographic reconstruction of the images and targeting clinical\nsituations where raw data are often not accessible. Based on inverse problem\nmethods, our contribution introduces the recently developed total generalized\nvariation (TGV) norm to regularize PET image deconvolution. Moreover, we\nstabilize this procedure with additional image constraints such as positivity\nand photometry invariance. A criterion for updating and adjusting automatically\nthe regularization parameter in case of Poisson noise is also presented.\nExperiments are conducted on both synthetic data and real patient images.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jun 2015 12:19:04 GMT"}], "update_date": "2015-06-17", "authors_parsed": [["Gu\u00e9rit", "St\u00e9phanie", ""], ["Jacques", "Laurent", ""], ["Macq", "Beno\u00eet", ""], ["Lee", "John A.", ""]]}, {"id": "1506.04954", "submitter": "Sara Soltani", "authors": "Sara Soltani, Misha E. Kilmer, and Per Christian Hansen", "title": "A Tensor-Based Dictionary Learning Approach to Tomographic Image\n  Reconstruction", "comments": "29 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.NA math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider tomographic reconstruction using priors in the form of a\ndictionary learned from training images. The reconstruction has two stages:\nfirst we construct a tensor dictionary prior from our training data, and then\nwe pose the reconstruction problem in terms of recovering the expansion\ncoefficients in that dictionary. Our approach differs from past approaches in\nthat a) we use a third-order tensor representation for our images and b) we\nrecast the reconstruction problem using the tensor formulation. The dictionary\nlearning problem is presented as a non-negative tensor factorization problem\nwith sparsity constraints. The reconstruction problem is formulated in a convex\noptimization framework by looking for a solution with a sparse representation\nin the tensor dictionary. Numerical results show that our tensor formulation\nleads to very sparse representations of both the training images and the\nreconstructions due to the ability of representing repeated features compactly\nin the dictionary.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jun 2015 08:55:28 GMT"}], "update_date": "2015-06-17", "authors_parsed": [["Soltani", "Sara", ""], ["Kilmer", "Misha E.", ""], ["Hansen", "Per Christian", ""]]}, {"id": "1506.05001", "submitter": "Liliana Lo Presti", "authors": "Liliana Lo Presti and Marco La Cascia", "title": "Using Hankel Matrices for Dynamics-based Facial Emotion Recognition and\n  Pain Detection", "comments": "in IEEE Proceedings of Workshop on Analysis and Modeling of Face and\n  Gestures (CVPRW 2015)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a new approach to model the temporal dynamics of a\nsequence of facial expressions. To this purpose, a sequence of Face Image\nDescriptors (FID) is regarded as the output of a Linear Time Invariant (LTI)\nsystem. The temporal dynamics of such sequence of descriptors are represented\nby means of a Hankel matrix. The paper presents different strategies to compute\ndynamics-based representation of a sequence of FID, and reports classification\naccuracy values of the proposed representations within different standard\nclassification frameworks. The representations have been validated in two very\nchallenging application domains: emotion recognition and pain detection.\nExperiments on two publicly available benchmarks and comparison with\nstate-of-the-art approaches demonstrate that the dynamics-based FID\nrepresentation attains competitive performance when off-the-shelf\nclassification tools are adopted.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jun 2015 15:22:46 GMT"}], "update_date": "2015-06-17", "authors_parsed": [["Presti", "Liliana Lo", ""], ["La Cascia", "Marco", ""]]}, {"id": "1506.05011", "submitter": "Theofanis Karaletsos", "authors": "Theofanis Karaletsos, Serge Belongie, Gunnar R\\\"atsch", "title": "Bayesian representation learning with oracle constraints", "comments": "16 pages, publishes in ICLR 16", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Representation learning systems typically rely on massive amounts of labeled\ndata in order to be trained to high accuracy. Recently, high-dimensional\nparametric models like neural networks have succeeded in building rich\nrepresentations using either compressive, reconstructive or supervised\ncriteria. However, the semantic structure inherent in observations is\noftentimes lost in the process. Human perception excels at understanding\nsemantics but cannot always be expressed in terms of labels. Thus,\n\\emph{oracles} or \\emph{human-in-the-loop systems}, for example crowdsourcing,\nare often employed to generate similarity constraints using an implicit\nsimilarity function encoded in human perception. In this work we propose to\ncombine \\emph{generative unsupervised feature learning} with a\n\\emph{probabilistic treatment of oracle information like triplets} in order to\ntransfer implicit privileged oracle knowledge into explicit nonlinear Bayesian\nlatent factor models of the observations. We use a fast variational algorithm\nto learn the joint model and demonstrate applicability to a well-known image\ndataset. We show how implicit triplet information can provide rich information\nto learn representations that outperform previous metric learning approaches as\nwell as generative models without this side-information in a variety of\npredictive tasks. In addition, we illustrate that the proposed approach\ncompartmentalizes the latent spaces semantically which allows interpretation of\nthe latent variables.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jun 2015 15:54:59 GMT"}, {"version": "v2", "created": "Sat, 21 Nov 2015 05:24:01 GMT"}, {"version": "v3", "created": "Fri, 8 Jan 2016 04:47:21 GMT"}, {"version": "v4", "created": "Tue, 1 Mar 2016 23:36:04 GMT"}], "update_date": "2016-03-03", "authors_parsed": [["Karaletsos", "Theofanis", ""], ["Belongie", "Serge", ""], ["R\u00e4tsch", "Gunnar", ""]]}, {"id": "1506.05032", "submitter": "Tiep Vu", "authors": "Tiep Huu Vu, Hojjat Seyed Mousavi, Vishal Monga, Arvind UK Rao, Ganesh\n  Rao", "title": "Histopathological Image Classification using Discriminative\n  Feature-oriented Dictionary Learning", "comments": "Accepted version to Transaction on Medical Imaging, 13 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In histopathological image analysis, feature extraction for classification is\na challenging task due to the diversity of histology features suitable for each\nproblem as well as presence of rich geometrical structures. In this paper, we\npropose an automatic feature discovery framework via learning class-specific\ndictionaries and present a low-complexity method for classification and disease\ngrading in histopathology. Essentially, our Discriminative Feature-oriented\nDictionary Learning (DFDL) method learns class-specific dictionaries such that\nunder a sparsity constraint, the learned dictionaries allow representing a new\nimage sample parsimoniously via the dictionary corresponding to the class\nidentity of the sample. At the same time, the dictionary is designed to be\npoorly capable of representing samples from other classes. Experiments on three\nchallenging real-world image databases: 1) histopathological images of\nintraductal breast lesions, 2) mammalian kidney, lung and spleen images\nprovided by the Animal Diagnostics Lab (ADL) at Pennsylvania State University,\nand 3) brain tumor images from The Cancer Genome Atlas (TCGA) database, reveal\nthe merits of our proposal over state-of-the-art alternatives. {Moreover, we\ndemonstrate that DFDL exhibits a more graceful decay in classification accuracy\nagainst the number of training images which is highly desirable in practice\nwhere generous training is often not available\n", "versions": [{"version": "v1", "created": "Tue, 16 Jun 2015 16:47:44 GMT"}, {"version": "v2", "created": "Tue, 29 Sep 2015 18:11:30 GMT"}, {"version": "v3", "created": "Wed, 30 Sep 2015 01:47:42 GMT"}, {"version": "v4", "created": "Sat, 24 Oct 2015 01:56:24 GMT"}, {"version": "v5", "created": "Tue, 29 Mar 2016 18:09:59 GMT"}], "update_date": "2016-03-30", "authors_parsed": [["Vu", "Tiep Huu", ""], ["Mousavi", "Hojjat Seyed", ""], ["Monga", "Vishal", ""], ["Rao", "Arvind UK", ""], ["Rao", "Ganesh", ""]]}, {"id": "1506.05036", "submitter": "Yael Yankelevsky", "authors": "Yael Yankelevsky, Ishai Shvartz, Tamar Avraham and Alfred M.\n  Bruckstein", "title": "Depth Perception in Autostereograms: 1/f-Noise is Best", "comments": null, "journal-ref": null, "doi": "10.1364/JOSAA.33.000149", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An autostereogram is a single image that encodes depth information that pops\nout when looking at it. The trick is achieved by replicating a vertical strip\nthat sets a basic two-dimensional pattern with disparity shifts that encode a\nthree-dimensional scene. It is of interest to explore the dependency between\nthe ease of perceiving depth in autostereograms and the choice of the basic\npattern used for generating them. In this work we confirm a theory proposed by\nBruckstein et al. to explain the process of autostereographic depth perception,\nproviding a measure for the ease of \"locking into\" the depth profile, based on\nthe spectral properties of the basic pattern used. We report the results of\nthree sets of psychophysical experiments using autostereograms generated from\ntwo-dimensional random noise patterns having power spectra of the form\n$1/f^\\beta$. The experiments were designed to test the ability of human\nsubjects to identify smooth, low resolution surfaces, as well as detail, in the\nform of higher resolution objects in the depth profile, and to determine limits\nin identifying small objects as a function of their size. In accordance with\nthe theory, we discover a significant advantage of the $1/f$ noise pattern\n(pink noise) for fast depth lock-in and fine detail detection, showing that\nsuch patterns are optimal choices for autostereogram design. Validating the\ntheoretical model predictions strengthens its underlying assumptions, and\ncontributes to a better understanding of the visual system's binocular\ndisparity mechanisms.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jun 2015 17:06:19 GMT"}], "update_date": "2016-01-20", "authors_parsed": [["Yankelevsky", "Yael", ""], ["Shvartz", "Ishai", ""], ["Avraham", "Tamar", ""], ["Bruckstein", "Alfred M.", ""]]}, {"id": "1506.05068", "submitter": "Kazuhisa Fujita Dr.", "authors": "Kazuhisa Fujita", "title": "Extract an essential skeleton of a character as a graph from a character\n  image", "comments": null, "journal-ref": "International Journal of Computer Science Issues 10, 5, 35-39,\n  2013", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper aims to make a graph representing an essential skeleton of a\ncharacter from an image that includes a machine printed or a handwritten\ncharacter using the growing neural gas (GNG) method and the relative\nneighborhood graph (RNG) algorithm. The visual system in our brain can\nrecognize printed characters and handwritten characters easily, robustly, and\nprecisely. How can our brains robustly recognize characters? In the visual\nprocessing in our brain, essential features of an object will be used for\nrecognition. The essential features are crosses, corners, junctions and so on.\nThese features may be useful for character recognition by a computer. However,\nextraction of the features is difficult. If the skeleton of a character is\nrepresented as a graph, the features can be more easily extracted. To extract\nthe skeleton of a character as a graph from a character image, we used the GNG\nmethod and the RNG algorithm. We achieved to extract skeleton graphs from\nimages including distorted, noisy, and handwritten characters.\n", "versions": [{"version": "v1", "created": "Sat, 13 Jun 2015 14:25:54 GMT"}], "update_date": "2015-06-17", "authors_parsed": [["Fujita", "Kazuhisa", ""]]}, {"id": "1506.05085", "submitter": "Wenjie Pei", "authors": "Wenjie Pei, Hamdi Dibeklio\\u{g}lu, David M.J. Tax, Laurens van der\n  Maaten", "title": "Time Series Classification using the Hidden-Unit Logistic Model", "comments": "17 pages, 4 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new model for time series classification, called the hidden-unit\nlogistic model, that uses binary stochastic hidden units to model latent\nstructure in the data. The hidden units are connected in a chain structure that\nmodels temporal dependencies in the data. Compared to the prior models for time\nseries classification such as the hidden conditional random field, our model\ncan model very complex decision boundaries because the number of latent states\ngrows exponentially with the number of hidden units. We demonstrate the strong\nperformance of our model in experiments on a variety of (computer vision)\ntasks, including handwritten character recognition, speech recognition, facial\nexpression, and action recognition. We also present a state-of-the-art system\nfor facial action unit detection based on the hidden-unit logistic model.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jun 2015 19:20:00 GMT"}, {"version": "v2", "created": "Tue, 19 Jan 2016 13:33:52 GMT"}], "update_date": "2016-01-20", "authors_parsed": [["Pei", "Wenjie", ""], ["Dibeklio\u011flu", "Hamdi", ""], ["Tax", "David M. J.", ""], ["van der Maaten", "Laurens", ""]]}, {"id": "1506.05163", "submitter": "Mikael Henaff", "authors": "Mikael Henaff, Joan Bruna, Yann LeCun", "title": "Deep Convolutional Networks on Graph-Structured Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Learning's recent successes have mostly relied on Convolutional\nNetworks, which exploit fundamental statistical properties of images, sounds\nand video data: the local stationarity and multi-scale compositional structure,\nthat allows expressing long range interactions in terms of shorter, localized\ninteractions. However, there exist other important examples, such as text\ndocuments or bioinformatic data, that may lack some or all of these strong\nstatistical regularities.\n  In this paper we consider the general question of how to construct deep\narchitectures with small learning complexity on general non-Euclidean domains,\nwhich are typically unknown and need to be estimated from the data. In\nparticular, we develop an extension of Spectral Networks which incorporates a\nGraph Estimation procedure, that we test on large-scale classification\nproblems, matching or improving over Dropout Networks with far less parameters\nto estimate.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jun 2015 22:31:09 GMT"}], "update_date": "2015-06-18", "authors_parsed": [["Henaff", "Mikael", ""], ["Bruna", "Joan", ""], ["LeCun", "Yann", ""]]}, {"id": "1506.05187", "submitter": "Wei Liu", "authors": "Wei Liu, Yijun Li, Xiaogang Chen, Jie Yang, Qiang Wu, Jingyi Yu", "title": "Robust High Quality Image Guided Depth Upsampling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  Time-of-Flight (ToF) depth sensing camera is able to obtain depth maps at a\nhigh frame rate. However, its low resolution and sensitivity to the noise are\nalways a concern. A popular solution is upsampling the obtained noisy low\nresolution depth map with the guidance of the companion high resolution color\nimage. However, due to the constrains in the existing upsampling models, the\nhigh resolution depth map obtained in such way may suffer from either texture\ncopy artifacts or blur of depth discontinuity. In this paper, a novel\noptimization framework is proposed with the brand new data term and smoothness\nterm. The comprehensive experiments using both synthetic data and real data\nshow that the proposed method well tackles the problem of texture copy\nartifacts and blur of depth discontinuity. It also demonstrates sufficient\nrobustness to the noise. Moreover, a data driven scheme is proposed to\nadaptively estimate the parameter in the upsampling optimization framework. The\nencouraging performance is maintained even in the case of large upsampling e.g.\n$8\\times$ and $16\\times$.\n", "versions": [{"version": "v1", "created": "Wed, 17 Jun 2015 02:38:43 GMT"}], "update_date": "2015-06-18", "authors_parsed": [["Liu", "Wei", ""], ["Li", "Yijun", ""], ["Chen", "Xiaogang", ""], ["Yang", "Jie", ""], ["Wu", "Qiang", ""], ["Yu", "Jingyi", ""]]}, {"id": "1506.05196", "submitter": "Salman Khan Mr.", "authors": "Salman H. Khan, Munawar Hayat, Mohammed Bennamoun, Roberto Togneri,\n  and Ferdous Sohel", "title": "A Discriminative Representation of Convolutional Features for Indoor\n  Scene Recognition", "comments": null, "journal-ref": null, "doi": "10.1109/TIP.2016.2567076", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Indoor scene recognition is a multi-faceted and challenging problem due to\nthe diverse intra-class variations and the confusing inter-class similarities.\nThis paper presents a novel approach which exploits rich mid-level\nconvolutional features to categorize indoor scenes. Traditionally used\nconvolutional features preserve the global spatial structure, which is a\ndesirable property for general object recognition. However, we argue that this\nstructuredness is not much helpful when we have large variations in scene\nlayouts, e.g., in indoor scenes. We propose to transform the structured\nconvolutional activations to another highly discriminative feature space. The\nrepresentation in the transformed space not only incorporates the\ndiscriminative aspects of the target dataset, but it also encodes the features\nin terms of the general object categories that are present in indoor scenes. To\nthis end, we introduce a new large-scale dataset of 1300 object categories\nwhich are commonly present in indoor scenes. Our proposed approach achieves a\nsignificant performance boost over previous state of the art approaches on five\nmajor scene classification datasets.\n", "versions": [{"version": "v1", "created": "Wed, 17 Jun 2015 03:55:19 GMT"}], "update_date": "2016-06-29", "authors_parsed": [["Khan", "Salman H.", ""], ["Hayat", "Munawar", ""], ["Bennamoun", "Mohammed", ""], ["Togneri", "Roberto", ""], ["Sohel", "Ferdous", ""]]}, {"id": "1506.05257", "submitter": "Daniel J Mankowitz", "authors": "Daniel J. Mankowitz and Ehud Rivlin", "title": "CFORB: Circular FREAK-ORB Visual Odometry", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel Visual Odometry algorithm entitled Circular FREAK-ORB\n(CFORB). This algorithm detects features using the well-known ORB algorithm\n[12] and computes feature descriptors using the FREAK algorithm [14]. CFORB is\ninvariant to both rotation and scale changes, and is suitable for use in\nenvironments with uneven terrain. Two visual geometric constraints have been\nutilized in order to remove invalid feature descriptor matches. These\nconstraints have not previously been utilized in a Visual Odometry algorithm. A\nvariation to circular matching [16] has also been implemented. This allows\nfeatures to be matched between images without having to be dependent upon the\nepipolar constraint. This algorithm has been run on the KITTI benchmark dataset\nand achieves a competitive average translational error of $3.73 \\%$ and average\nrotational error of $0.0107 deg/m$. CFORB has also been run in an indoor\nenvironment and achieved an average translational error of $3.70 \\%$. After\nrunning CFORB in a highly textured environment with an approximately uniform\nfeature spread across the images, the algorithm achieves an average\ntranslational error of $2.4 \\%$ and an average rotational error of $0.009\ndeg/m$.\n", "versions": [{"version": "v1", "created": "Wed, 17 Jun 2015 09:44:42 GMT"}], "update_date": "2015-06-18", "authors_parsed": [["Mankowitz", "Daniel J.", ""], ["Rivlin", "Ehud", ""]]}, {"id": "1506.05274", "submitter": "Emanuele Rodol\\`a", "authors": "Emanuele Rodol\\`a and Luca Cosmo and Michael M. Bronstein and Andrea\n  Torsello and Daniel Cremers", "title": "Partial Functional Correspondence", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a method for computing partial functional\ncorrespondence between non-rigid shapes. We use perturbation analysis to show\nhow removal of shape parts changes the Laplace-Beltrami eigenfunctions, and\nexploit it as a prior on the spectral representation of the correspondence.\nCorresponding parts are optimization variables in our problem and are used to\nweight the functional correspondence; we are looking for the largest and most\nregular (in the Mumford-Shah sense) parts that minimize correspondence\ndistortion. We show that our approach can cope with very challenging\ncorrespondence settings.\n", "versions": [{"version": "v1", "created": "Wed, 17 Jun 2015 10:47:20 GMT"}, {"version": "v2", "created": "Tue, 22 Dec 2015 12:57:25 GMT"}], "update_date": "2015-12-23", "authors_parsed": [["Rodol\u00e0", "Emanuele", ""], ["Cosmo", "Luca", ""], ["Bronstein", "Michael M.", ""], ["Torsello", "Andrea", ""], ["Cremers", "Daniel", ""]]}, {"id": "1506.05393", "submitter": "Ze Wang", "authors": "Ze Wang", "title": "MRF-ZOOM: A Fast Dictionary Searching Algorithm for Magnetic Resonance\n  Fingerprinting", "comments": "7 figures", "journal-ref": "39th Annual International Conference of the IEEE Engineering in\n  Medicine and Biology Society (EMBC), 3256-3259, 2017", "doi": "10.1109/EMBC.2017.8037551", "report-no": null, "categories": "cs.DS cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Magnetic resonance fingerprinting (MRF) is a new technique for simultaneously\nquantifying multiple MR parameters using one temporally resolved MR scan. But\nits brute-force dictionary generating and searching (DGS) process causes a huge\ndisk space demand and computational burden, prohibiting it from a practical\nmultiple slice high-definition imaging. The purpose of this paper was to\nprovide a fast and space efficient DGS algorithm for MRF. Based on an empirical\nanalysis of properties of the distance function of the acquired MRF signal and\nthe pre-defined MRF dictionary entries, we proposed a parameter separable MRF\nDGS method, which breaks the multiplicative computation complexity into an\nadditive one and enabling a resolution scalable multi-resolution DGS process,\nwhich was dubbed as MRF ZOOM. The evaluation results showed that MRF ZOOM was\nhundreds or thousands of times faster than the original brute-force DGS method.\nThe acceleration was even higher when considering the time difference for\ngenerating the dictionary. Using a high precision quantification, MRF can find\nthe right parameter values for a 64x64 imaging slice in 117 secs. Our data also\nshowed that spatial constraints can be used to further speed up MRF ZOOM.\n", "versions": [{"version": "v1", "created": "Wed, 17 Jun 2015 17:47:18 GMT"}], "update_date": "2017-10-31", "authors_parsed": [["Wang", "Ze", ""]]}, {"id": "1506.05439", "submitter": "Charlie Frogner", "authors": "Charlie Frogner, Chiyuan Zhang, Hossein Mobahi, Mauricio Araya-Polo,\n  Tomaso Poggio", "title": "Learning with a Wasserstein Loss", "comments": "NIPS 2015; v3 updates Algorithm 1 and Equations 6, 8", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning to predict multi-label outputs is challenging, but in many problems\nthere is a natural metric on the outputs that can be used to improve\npredictions. In this paper we develop a loss function for multi-label learning,\nbased on the Wasserstein distance. The Wasserstein distance provides a natural\nnotion of dissimilarity for probability measures. Although optimizing with\nrespect to the exact Wasserstein distance is costly, recent work has described\na regularized approximation that is efficiently computed. We describe an\nefficient learning algorithm based on this regularization, as well as a novel\nextension of the Wasserstein distance from probability measures to unnormalized\nmeasures. We also describe a statistical learning bound for the loss. The\nWasserstein loss can encourage smoothness of the predictions with respect to a\nchosen metric on the output space. We demonstrate this property on a real-data\ntag prediction problem, using the Yahoo Flickr Creative Commons dataset,\noutperforming a baseline that doesn't use the metric.\n", "versions": [{"version": "v1", "created": "Wed, 17 Jun 2015 19:36:41 GMT"}, {"version": "v2", "created": "Fri, 6 Nov 2015 03:46:05 GMT"}, {"version": "v3", "created": "Wed, 30 Dec 2015 01:08:11 GMT"}], "update_date": "2015-12-31", "authors_parsed": [["Frogner", "Charlie", ""], ["Zhang", "Chiyuan", ""], ["Mobahi", "Hossein", ""], ["Araya-Polo", "Mauricio", ""], ["Poggio", "Tomaso", ""]]}, {"id": "1506.05532", "submitter": "Salman Khan Mr.", "authors": "Munawar Hayat, Salman H. Khan, Mohammed Bennamoun, Senjian An", "title": "A Spatial Layout and Scale Invariant Feature Representation for Indoor\n  Scene Classification", "comments": null, "journal-ref": null, "doi": "10.1109/TIP.2016.2599292", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unlike standard object classification, where the image to be classified\ncontains one or multiple instances of the same object, indoor scene\nclassification is quite different since the image consists of multiple distinct\nobjects. Further, these objects can be of varying sizes and are present across\nnumerous spatial locations in different layouts. For automatic indoor scene\ncategorization, large scale spatial layout deformations and scale variations\nare therefore two major challenges and the design of rich feature descriptors\nwhich are robust to these challenges is still an open problem. This paper\nintroduces a new learnable feature descriptor called \"spatial layout and scale\ninvariant convolutional activations\" to deal with these challenges. For this\npurpose, a new Convolutional Neural Network architecture is designed which\nincorporates a novel 'Spatially Unstructured' layer to introduce robustness\nagainst spatial layout deformations. To achieve scale invariance, we present a\npyramidal image representation. For feasible training of the proposed network\nfor images of indoor scenes, the paper proposes a new methodology which\nefficiently adapts a trained network model (on a large scale data) for our task\nwith only a limited amount of available training data. Compared with existing\nstate of the art, the proposed approach achieves a relative performance\nimprovement of 3.2%, 3.8%, 7.0%, 11.9% and 2.1% on MIT-67, Scene-15, Sports-8,\nGraz-02 and NYU datasets respectively.\n", "versions": [{"version": "v1", "created": "Thu, 18 Jun 2015 02:11:37 GMT"}, {"version": "v2", "created": "Fri, 14 Aug 2015 04:01:11 GMT"}], "update_date": "2016-11-03", "authors_parsed": [["Hayat", "Munawar", ""], ["Khan", "Salman H.", ""], ["Bennamoun", "Mohammed", ""], ["An", "Senjian", ""]]}, {"id": "1506.05603", "submitter": "Michael Moeller", "authors": "Emanuele Rodol\\`a, Michael Moeller and Daniel Cremers", "title": "Point-wise Map Recovery and Refinement from Functional Correspondence", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Since their introduction in the shape analysis community, functional maps\nhave met with considerable success due to their ability to compactly represent\ndense correspondences between deformable shapes, with applications ranging from\nshape matching and image segmentation, to exploration of large shape\ncollections. Despite the numerous advantages of such representation, however,\nthe problem of converting a given functional map back to a point-to-point map\nhas received a surprisingly limited interest. In this paper we analyze the\ngeneral problem of point-wise map recovery from arbitrary functional maps. In\ndoing so, we rule out many of the assumptions required by the currently\nestablished approach -- most notably, the limiting requirement of the input\nshapes being nearly-isometric. We devise an efficient recovery process based on\na simple probabilistic model. Experiments confirm that this approach achieves\nremarkable accuracy improvements in very challenging cases.\n", "versions": [{"version": "v1", "created": "Thu, 18 Jun 2015 09:38:47 GMT"}], "update_date": "2015-06-19", "authors_parsed": [["Rodol\u00e0", "Emanuele", ""], ["Moeller", "Michael", ""], ["Cremers", "Daniel", ""]]}, {"id": "1506.05751", "submitter": "Rob  Fergus", "authors": "Emily Denton, Soumith Chintala, Arthur Szlam, Rob Fergus", "title": "Deep Generative Image Models using a Laplacian Pyramid of Adversarial\n  Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we introduce a generative parametric model capable of producing\nhigh quality samples of natural images. Our approach uses a cascade of\nconvolutional networks within a Laplacian pyramid framework to generate images\nin a coarse-to-fine fashion. At each level of the pyramid, a separate\ngenerative convnet model is trained using the Generative Adversarial Nets (GAN)\napproach (Goodfellow et al.). Samples drawn from our model are of significantly\nhigher quality than alternate approaches. In a quantitative assessment by human\nevaluators, our CIFAR10 samples were mistaken for real images around 40% of the\ntime, compared to 10% for samples drawn from a GAN baseline model. We also show\nsamples from models trained on the higher resolution images of the LSUN scene\ndataset.\n", "versions": [{"version": "v1", "created": "Thu, 18 Jun 2015 17:03:54 GMT"}], "update_date": "2015-06-19", "authors_parsed": [["Denton", "Emily", ""], ["Chintala", "Soumith", ""], ["Szlam", "Arthur", ""], ["Fergus", "Rob", ""]]}, {"id": "1506.05870", "submitter": "Kuan-Wen Chen", "authors": "Kuan-Wen Chen, Chun-Hsin Wang, Xiao Wei, Qiao Liang, Ming-Hsuan Yang,\n  Chu-Song Chen, Yi-Ping Hung", "title": "To Know Where We Are: Vision-Based Positioning in Outdoor Environments", "comments": "11 pages, 14 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Augmented reality (AR) displays become more and more popular recently,\nbecause of its high intuitiveness for humans and high-quality head-mounted\ndisplay have rapidly developed. To achieve such displays with augmented\ninformation, highly accurate image registration or ego-positioning are\nrequired, but little attention have been paid for out-door environments. This\npaper presents a method for ego-positioning in outdoor environments with low\ncost monocular cameras. To reduce the computational and memory requirements as\nwell as the communication overheads, we formulate the model compression\nalgorithm as a weighted k-cover problem for better preserving model structures.\nSpecifically for real-world vision-based positioning applications, we consider\nthe issues with large scene change and propose a model update algorithm to\ntackle these problems. A long- term positioning dataset with more than one\nmonth, 106 sessions, and 14,275 images is constructed. Based on both local and\nup-to-date models constructed in our approach, extensive experimental results\nshow that high positioning accuracy (mean ~ 30.9cm, stdev. ~ 15.4cm) can be\nachieved, which outperforms existing vision-based algorithms.\n", "versions": [{"version": "v1", "created": "Fri, 19 Jun 2015 03:11:33 GMT"}], "update_date": "2015-06-22", "authors_parsed": [["Chen", "Kuan-Wen", ""], ["Wang", "Chun-Hsin", ""], ["Wei", "Xiao", ""], ["Liang", "Qiao", ""], ["Yang", "Ming-Hsuan", ""], ["Chen", "Chu-Song", ""], ["Hung", "Yi-Ping", ""]]}, {"id": "1506.05920", "submitter": "Tsuyoshi Kato", "authors": "Tsuyoshi Kato, Raissa Relator, Hayliang Ngouv, Yoshihiro Hirohashi,\n  Tetsuhiro Kakimoto, Kinya Okada", "title": "New Descriptor for Glomerulus Detection in Kidney Microscopy Image", "comments": null, "journal-ref": "BMC Bioinformatics, 16:316, 2015", "doi": "10.1186/s12859-015-0739-1", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Glomerulus detection is a key step in histopathological evaluation of\nmicroscopy images of kidneys. However, the task of automatic detection of\nglomeruli poses challenges due to the disparity in sizes and shapes of\nglomeruli in renal sections. Moreover, extensive variations of their\nintensities due to heterogeneity in immunohistochemistry staining are also\nencountered. Despite being widely recognized as a powerful descriptor for\ngeneral object detection, the rectangular histogram of oriented gradients\n(Rectangular HOG) suffers from many false positives due to the aforementioned\ndifficulties in the context of glomerulus detection.\n  A new descriptor referred to as Segmental HOG is developed to perform a\ncomprehensive detection of hundreds of glomeruli in images of whole kidney\nsections. The new descriptor possesses flexible blocks that can be adaptively\nfitted to input images to acquire robustness to deformations of glomeruli.\nMoreover, the novel segmentation technique employed herewith generates high\nquality segmentation outputs and the algorithm is assured to converge to an\noptimal solution. Consequently, experiments using real world image data reveal\nthat Segmental HOG achieves significant improvements in detection performance\ncompared to Rectangular HOG.\n  The proposed descriptor and method for glomeruli detection present promising\nresults and is expected to be useful in pathological evaluation.\n", "versions": [{"version": "v1", "created": "Fri, 19 Jun 2015 09:03:48 GMT"}], "update_date": "2016-01-08", "authors_parsed": [["Kato", "Tsuyoshi", ""], ["Relator", "Raissa", ""], ["Ngouv", "Hayliang", ""], ["Hirohashi", "Yoshihiro", ""], ["Kakimoto", "Tetsuhiro", ""], ["Okada", "Kinya", ""]]}, {"id": "1506.05929", "submitter": "Nanne Van Noord", "authors": "Nanne van Noord, Eric Postma", "title": "Exploring the influence of scale on artist attribution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Previous work has shown that the artist of an artwork can be identified by\nuse of computational methods that analyse digital images. However, the\ndigitised artworks are often investigated at a coarse scale discarding many of\nthe important details that may define an artist's style. In recent years high\nresolution images of artworks have become available, which, combined with\nincreased processing power and new computational techniques, allow us to\nanalyse digital images of artworks at a very fine scale. In this work we train\nand evaluate a Convolutional Neural Network (CNN) on the task of artist\nattribution using artwork images of varying resolutions. To this end, we\ncombine two existing methods to enable the application of high resolution\nimages to CNNs. By comparing the attribution performances obtained at different\nscales, we find that in most cases finer scales are beneficial to the\nattribution performance, whereas for a minority of the artists, coarser scales\nappear to be preferable. We conclude that artist attribution would benefit from\na multi-scale CNN approach which vastly expands the possibilities for\ncomputational art forensics.\n", "versions": [{"version": "v1", "created": "Fri, 19 Jun 2015 09:27:08 GMT"}], "update_date": "2015-06-22", "authors_parsed": [["van Noord", "Nanne", ""], ["Postma", "Eric", ""]]}, {"id": "1506.05942", "submitter": "Xuehui Wang", "authors": "Xuehui Wang, Jinli Suo, Jingyi Yu, Yongdong Zhang, and Qionghai Dai", "title": "Scene-adaptive Coded Apertures Imaging", "comments": "This paper has been withdrawn by the author due to bad motivation\n  proof and poor experiment performance", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Coded aperture imaging systems have recently shown great success in\nrecovering scene depth and extending the depth-of-field. The ideal pattern,\nhowever, would have to serve two conflicting purposes: 1) be broadband to\nensure robust deconvolution and 2) has sufficient zero-crossings for a high\ndepth discrepancy. This paper presents a simple but effective scene-adaptive\ncoded aperture solution to bridge this gap. We observe that the geometric\nstructures in a natural scene often exhibit only a few edge directions, and the\nsuccessive frames are closely correlated. Therefore we adopt a spatial\npartitioning and temporal propagation scheme. In each frame, we address one\nprincipal direction by applying depth-discriminative codes along it and\nbroadband codes along its orthogonal direction. Since within a frame only the\nregions with edge direction corresponding to its aperture code behaves well, we\nutilize the close among-frame correlation to propagate the high quality single\nframe results temporally to obtain high performance over the whole image\nlattice. To physically implement this scheme, we use a Liquid Crystal on\nSilicon (LCoS) microdisplay that permits fast changing pattern codes. Firstly,\nwe capture the scene with a pinhole and analyze the scene content to determine\nprimary edge orientations. Secondly, we sequentially apply the proposed coding\nscheme with these orientations in the following frames. Experiments on both\nsynthetic and real scenes show that our technique is able to combine advantages\nof the state-of-the-art patterns for recovering better quality depth map and\nall-focus images.\n", "versions": [{"version": "v1", "created": "Fri, 19 Jun 2015 10:09:45 GMT"}, {"version": "v2", "created": "Fri, 18 Dec 2015 02:13:08 GMT"}], "update_date": "2015-12-21", "authors_parsed": [["Wang", "Xuehui", ""], ["Suo", "Jinli", ""], ["Yu", "Jingyi", ""], ["Zhang", "Yongdong", ""], ["Dai", "Qionghai", ""]]}, {"id": "1506.06001", "submitter": "Frederic Devernay", "authors": "Fr\\'ed\\'eric Devernay (INRIA Grenoble Rh\\^one-Alpes / LIG Laboratoire\n  d'Informatique de Grenoble), Paul Beardsley (DRZ)", "title": "Stereoscopic Cinema", "comments": "Published as Ronfard, R\\'emi and Taubin, Gabriel. Image and Geometry\n  Processing for 3-D Cinematography, 5, Springer Berlin Heidelberg, pp.11-51,\n  2010, Geometry and Computing, 978-3-642-12392-4", "journal-ref": null, "doi": "10.1007/978-3-642-12392-4", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stereoscopic cinema has seen a surge of activity in recent years, and for the\nfirst time all of the major Hollywood studios released 3-D movies in 2009. This\nis happening alongside the adoption of 3-D technology for sports broadcasting,\nand the arrival of 3-D TVs for the home. Two previous attempts to introduce 3-D\ncinema in the 1950s and the 1980s failed because the contemporary technology\nwas immature and resulted in viewer discomfort. But current technologies --\nsuch as accurately-adjustable 3-D camera rigs with onboard computers to\nautomatically inform a camera operator of inappropriate stereoscopic shots,\ndigital processing for post-shooting rectification of the 3-D imagery, digital\nprojectors for accurate positioning of the two stereo projections on the cinema\nscreen, and polarized silver screens to reduce cross-talk between the viewers\nleft- and right-eyes -- mean that the viewer experience is at a much higher\nlevel of quality than in the past. Even so, creation of stereoscopic cinema is\nan open, active research area, and there are many challenges from acquisition\nto post-production to automatic adaptation for different-sized display. This\nchapter describes the current state-of-the-art in stereoscopic cinema, and\ndirections of future work.\n", "versions": [{"version": "v1", "created": "Fri, 19 Jun 2015 13:46:23 GMT"}], "update_date": "2015-06-22", "authors_parsed": [["Devernay", "Fr\u00e9d\u00e9ric", "", "INRIA Grenoble Rh\u00f4ne-Alpes / LIG Laboratoire\n  d'Informatique de Grenoble"], ["Beardsley", "Paul", "", "DRZ"]]}, {"id": "1506.06006", "submitter": "Srinivas S S Kruthiventi", "authors": "Srinivas S. S. Kruthiventi and R. Venkatesh Babu", "title": "Crowd Flow Segmentation in Compressed Domain using CRF", "comments": "In IEEE International Conference on Image Processing (ICIP), 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Crowd flow segmentation is an important step in many video surveillance\ntasks. In this work, we propose an algorithm for segmenting flows in H.264\ncompressed videos in a completely unsupervised manner. Our algorithm works on\nmotion vectors which can be obtained by partially decoding the compressed video\nwithout extracting any additional features. Our approach is based on modelling\nthe motion vector field as a Conditional Random Field (CRF) and obtaining\noriented motion segments by finding the optimal labelling which minimises the\nglobal energy of CRF. These oriented motion segments are recursively merged\nbased on gradient across their boundaries to obtain the final flow segments.\nThis work in compressed domain can be easily extended to pixel domain by\nsubstituting motion vectors with motion based features like optical flow. The\nproposed algorithm is experimentally evaluated on a standard crowd flow dataset\nand its superior performance in both accuracy and computational time are\ndemonstrated through quantitative results.\n", "versions": [{"version": "v1", "created": "Fri, 19 Jun 2015 14:01:24 GMT"}], "update_date": "2015-06-22", "authors_parsed": [["Kruthiventi", "Srinivas S. S.", ""], ["Babu", "R. Venkatesh", ""]]}, {"id": "1506.06039", "submitter": "Alexander Dubbs", "authors": "Alexander Dubbs, James Guevara, Darcy S. Peterka, Rafael Yuste", "title": "moco: Fast Motion Correction for Calcium Imaging", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motion correction is the first in a pipeline of algorithms to analyze calcium\nimaging videos and extract biologically relevant information, for example the\nnetwork structure of the neurons therein. Fast motion correction would be\nespecially critical for closed-loop activity triggered stimulation experiments,\nwhere accurate detection and targeting of specific cells in necessary. Our\nalgorithm uses a Fourier-transform approach, and its efficiency derives from a\ncombination of judicious downsampling and the accelerated computation of many\n$L_2$ norms using dynamic programming and two-dimensional, fft-accelerated\nconvolutions. Its accuracy is comparable to that of established community-used\nalgorithms, and it is more stable to large translational motions. It is\nprogrammed in Java and is compatible with ImageJ.\n", "versions": [{"version": "v1", "created": "Fri, 19 Jun 2015 15:03:13 GMT"}], "update_date": "2015-06-22", "authors_parsed": [["Dubbs", "Alexander", ""], ["Guevara", "James", ""], ["Peterka", "Darcy S.", ""], ["Yuste", "Rafael", ""]]}, {"id": "1506.06046", "submitter": "Poonam Yadav Dr", "authors": "Poonam Yadav", "title": "Face Prediction Model for an Automatic Age-invariant Face Recognition\n  System", "comments": "3 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.NE", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  Automated face recognition and identification softwares are becoming part of\nour daily life; it finds its abode not only with Facebook's auto photo tagging,\nApple's iPhoto, Google's Picasa, Microsoft's Kinect, but also in Homeland\nSecurity Department's dedicated biometric face detection systems. Most of these\nautomatic face identification systems fail where the effects of aging come into\nthe picture. Little work exists in the literature on the subject of face\nprediction that accounts for aging, which is a vital part of the computer face\nrecognition systems. In recent years, individual face components' (e.g. eyes,\nnose, mouth) features based matching algorithms have emerged, but these\napproaches are still not efficient. Therefore, in this work we describe a Face\nPrediction Model (FPM), which predicts human face aging or growth related image\nvariation using Principle Component Analysis (PCA) and Artificial Neural\nNetwork (ANN) learning techniques. The FPM captures the facial changes, which\noccur with human aging and predicts the facial image with a few years of gap\nwith an acceptable accuracy of face matching from 76 to 86%.\n", "versions": [{"version": "v1", "created": "Thu, 16 Apr 2015 12:26:21 GMT"}], "update_date": "2015-06-22", "authors_parsed": [["Yadav", "Poonam", ""]]}, {"id": "1506.06068", "submitter": "Teng Qiu", "authors": "Teng Qiu, Yongjie Li", "title": "A general framework for the IT-based clustering methods", "comments": "17 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  Previously, we proposed a physically inspired rule to organize the data\npoints in a sparse yet effective structure, called the in-tree (IT) graph,\nwhich is able to capture a wide class of underlying cluster structures in the\ndatasets, especially for the density-based datasets. Although there are some\nredundant edges or lines between clusters requiring to be removed by computer,\nthis IT graph has a big advantage compared with the k-nearest-neighborhood\n(k-NN) or the minimal spanning tree (MST) graph, in that the redundant edges in\nthe IT graph are much more distinguishable and thus can be easily determined by\nseveral methods previously proposed by us.\n  In this paper, we propose a general framework to re-construct the IT graph,\nbased on an initial neighborhood graph, such as the k-NN or MST, etc, and the\ncorresponding graph distances. For this general framework, our previous way of\nconstructing the IT graph turns out to be a special case of it. This general\nframework 1) can make the IT graph capture a wider class of underlying cluster\nstructures in the datasets, especially for the manifolds, and 2) should be more\neffective to cluster the sparse or graph-based datasets.\n", "versions": [{"version": "v1", "created": "Fri, 19 Jun 2015 16:03:31 GMT"}], "update_date": "2015-06-22", "authors_parsed": [["Qiu", "Teng", ""], ["Li", "Yongjie", ""]]}, {"id": "1506.06096", "submitter": "Dorina Thanou", "authors": "Dorina Thanou, Philip A. Chou, and Pascal Frossard", "title": "Graph-based compression of dynamic 3D point cloud sequences", "comments": null, "journal-ref": null, "doi": "10.1109/TIP.2016.2529506", "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the problem of compression of 3D point cloud sequences\nthat are characterized by moving 3D positions and color attributes. As\ntemporally successive point cloud frames are similar, motion estimation is key\nto effective compression of these sequences. It however remains a challenging\nproblem as the point cloud frames have varying numbers of points without\nexplicit correspondence information. We represent the time-varying geometry of\nthese sequences with a set of graphs, and consider 3D positions and color\nattributes of the points clouds as signals on the vertices of the graphs. We\nthen cast motion estimation as a feature matching problem between successive\ngraphs. The motion is estimated on a sparse set of representative vertices\nusing new spectral graph wavelet descriptors. A dense motion field is\neventually interpolated by solving a graph-based regularization problem. The\nestimated motion is finally used for removing the temporal redundancy in the\npredictive coding of the 3D positions and the color characteristics of the\npoint cloud sequences. Experimental results demonstrate that our method is able\nto accurately estimate the motion between consecutive frames. Moreover, motion\nestimation is shown to bring significant improvement in terms of the overall\ncompression performance of the sequence. To the best of our knowledge, this is\nthe first paper that exploits both the spatial correlation inside each frame\n(through the graph) and the temporal correlation between the frames (through\nthe motion estimation) to compress the color and the geometry of 3D point cloud\nsequences in an efficient way.\n", "versions": [{"version": "v1", "created": "Fri, 19 Jun 2015 17:31:34 GMT"}], "update_date": "2016-08-24", "authors_parsed": [["Thanou", "Dorina", ""], ["Chou", "Philip A.", ""], ["Frossard", "Pascal", ""]]}, {"id": "1506.06155", "submitter": "Mohammad Norouzi", "authors": "Mohammad Norouzi, Maxwell D. Collins, David J. Fleet, Pushmeet Kohli", "title": "CO2 Forest: Improved Random Forest by Continuous Optimization of Oblique\n  Splits", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel algorithm for optimizing multivariate linear threshold\nfunctions as split functions of decision trees to create improved Random Forest\nclassifiers. Standard tree induction methods resort to sampling and exhaustive\nsearch to find good univariate split functions. In contrast, our method\ncomputes a linear combination of the features at each node, and optimizes the\nparameters of the linear combination (oblique) split functions by adopting a\nvariant of latent variable SVM formulation. We develop a convex-concave upper\nbound on the classification loss for a one-level decision tree, and optimize\nthe bound by stochastic gradient descent at each internal node of the tree.\nForests of up to 1000 Continuously Optimized Oblique (CO2) decision trees are\ncreated, which significantly outperform Random Forest with univariate splits\nand previous techniques for constructing oblique trees. Experimental results\nare reported on multi-class classification benchmarks and on Labeled Faces in\nthe Wild (LFW) dataset.\n", "versions": [{"version": "v1", "created": "Fri, 19 Jun 2015 20:42:47 GMT"}, {"version": "v2", "created": "Wed, 24 Jun 2015 21:23:43 GMT"}], "update_date": "2015-06-26", "authors_parsed": [["Norouzi", "Mohammad", ""], ["Collins", "Maxwell D.", ""], ["Fleet", "David J.", ""], ["Kohli", "Pushmeet", ""]]}, {"id": "1506.06204", "submitter": "Pedro O. Pinheiro", "authors": "Pedro O. Pinheiro, Ronan Collobert, Piotr Dollar", "title": "Learning to Segment Object Candidates", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent object detection systems rely on two critical steps: (1) a set of\nobject proposals is predicted as efficiently as possible, and (2) this set of\ncandidate proposals is then passed to an object classifier. Such approaches\nhave been shown they can be fast, while achieving the state of the art in\ndetection performance. In this paper, we propose a new way to generate object\nproposals, introducing an approach based on a discriminative convolutional\nnetwork. Our model is trained jointly with two objectives: given an image\npatch, the first part of the system outputs a class-agnostic segmentation mask,\nwhile the second part of the system outputs the likelihood of the patch being\ncentered on a full object. At test time, the model is efficiently applied on\nthe whole test image and generates a set of segmentation masks, each of them\nbeing assigned with a corresponding object likelihood score. We show that our\nmodel yields significant improvements over state-of-the-art object proposal\nalgorithms. In particular, compared to previous approaches, our model obtains\nsubstantially higher object recall using fewer proposals. We also show that our\nmodel is able to generalize to unseen categories it has not seen during\ntraining. Unlike all previous approaches for generating object masks, we do not\nrely on edges, superpixels, or any other form of low-level segmentation.\n", "versions": [{"version": "v1", "created": "Sat, 20 Jun 2015 06:36:49 GMT"}, {"version": "v2", "created": "Tue, 1 Sep 2015 16:03:01 GMT"}], "update_date": "2015-09-02", "authors_parsed": [["Pinheiro", "Pedro O.", ""], ["Collobert", "Ronan", ""], ["Dollar", "Piotr", ""]]}, {"id": "1506.06272", "submitter": "Fei Sha", "authors": "Junqi Jin, Kun Fu, Runpeng Cui, Fei Sha and Changshui Zhang", "title": "Aligning where to see and what to tell: image caption with region-based\n  attention and scene factorization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent progress on automatic generation of image captions has shown that it\nis possible to describe the most salient information conveyed by images with\naccurate and meaningful sentences. In this paper, we propose an image caption\nsystem that exploits the parallel structures between images and sentences. In\nour model, the process of generating the next word, given the previously\ngenerated ones, is aligned with the visual perception experience where the\nattention shifting among the visual regions imposes a thread of visual\nordering. This alignment characterizes the flow of \"abstract meaning\", encoding\nwhat is semantically shared by both the visual scene and the text description.\nOur system also makes another novel modeling contribution by introducing\nscene-specific contexts that capture higher-level semantic information encoded\nin an image. The contexts adapt language models for word generation to specific\nscene types. We benchmark our system and contrast to published results on\nseveral popular datasets. We show that using either region-based attention or\nscene-specific contexts improves systems without those components. Furthermore,\ncombining these two modeling ingredients attains the state-of-the-art\nperformance.\n", "versions": [{"version": "v1", "created": "Sat, 20 Jun 2015 17:25:38 GMT"}], "update_date": "2015-06-23", "authors_parsed": [["Jin", "Junqi", ""], ["Fu", "Kun", ""], ["Cui", "Runpeng", ""], ["Sha", "Fei", ""], ["Zhang", "Changshui", ""]]}, {"id": "1506.06273", "submitter": "Chuiwen Ma", "authors": "Chuiwen Ma, Liang Shi, Hanlu Huang, Mengyuan Yan", "title": "3D Reconstruction from Full-view Fisheye Camera", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this report, we proposed a 3D reconstruction method for the full-view\nfisheye camera. The camera we used is Ricoh Theta, which captures spherical\nimages and has a wide field of view (FOV). The conventional stereo apporach\nbased on perspective camera model cannot be directly applied and instead we\nused a spherical camera model to depict the relation between 3D point and its\ncorresponding observation in the image. We implemented a system that can\nreconstruct the 3D scene using captures from two or more cameras. A GUI is also\ncreated to allow users to control the view perspective and obtain a better\nintuition of how the scene is rebuilt. Experiments showed that our\nreconstruction results well preserved the structure of the scene in the real\nworld.\n", "versions": [{"version": "v1", "created": "Sat, 20 Jun 2015 17:53:24 GMT"}], "update_date": "2015-06-23", "authors_parsed": [["Ma", "Chuiwen", ""], ["Shi", "Liang", ""], ["Huang", "Hanlu", ""], ["Yan", "Mengyuan", ""]]}, {"id": "1506.06274", "submitter": "Chuiwen Ma", "authors": "Chuiwen Ma, Hao Su, Liang Shi", "title": "Pose Estimation Based on 3D Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we proposed a pose estimation system based on rendered image\ntraining set, which predicts the pose of objects in real image, with knowledge\nof object category and tight bounding box. We developed a patch-based\nmulti-class classification algorithm, and an iterative approach to improve the\naccuracy. We achieved state-of-the-art performance on pose estimation task.\n", "versions": [{"version": "v1", "created": "Sat, 20 Jun 2015 17:55:49 GMT"}], "update_date": "2015-06-23", "authors_parsed": [["Ma", "Chuiwen", ""], ["Su", "Hao", ""], ["Shi", "Liang", ""]]}, {"id": "1506.06289", "submitter": "Manolis Tsakiris", "authors": "Manolis C. Tsakiris and Rene Vidal", "title": "Filtrated Algebraic Subspace Clustering", "comments": null, "journal-ref": "SIAM Journal on Imaging Sciences, Volume 10, Issue 1, pp. 372-415,\n  2017", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Subspace clustering is the problem of clustering data that lie close to a\nunion of linear subspaces. In the abstract form of the problem, where no noise\nor other corruptions are present, the data are assumed to lie in general\nposition inside the algebraic variety of a union of subspaces, and the\nobjective is to decompose the variety into its constituent subspaces. Prior\nalgebraic-geometric approaches to this problem require the subspaces to be of\nequal dimension, or the number of subspaces to be known. Subspaces of arbitrary\ndimensions can still be recovered in closed form, in terms of all homogeneous\npolynomials of degree $m$ that vanish on their union, when an upper bound m on\nthe number of the subspaces is given. In this paper, we propose an alternative,\nprovably correct, algorithm for addressing a union of at most $m$\narbitrary-dimensional subspaces, based on the idea of descending filtrations of\nsubspace arrangements. Our algorithm uses the gradient of a vanishing\npolynomial at a point in the variety to find a hyperplane containing the\nsubspace S passing through that point. By intersecting the variety with this\nhyperplane, we obtain a subvariety that contains S, and recursively applying\nthe procedure until no non-trivial vanishing polynomial exists, our algorithm\neventually identifies S. By repeating this procedure for other points, our\nalgorithm eventually identifies all the subspaces by returning a basis for\ntheir orthogonal complement. Finally, we develop a variant of the abstract\nalgorithm, suitable for computations with noisy data. We show by experiments on\nsynthetic and real data that the proposed algorithm outperforms\nstate-of-the-art methods on several occasions, thus demonstrating the merit of\nthe idea of filtrations.\n", "versions": [{"version": "v1", "created": "Sat, 20 Jun 2015 20:09:25 GMT"}, {"version": "v2", "created": "Sat, 29 Aug 2015 19:50:44 GMT"}, {"version": "v3", "created": "Wed, 28 Oct 2015 03:15:29 GMT"}, {"version": "v4", "created": "Wed, 30 Mar 2016 03:31:42 GMT"}, {"version": "v5", "created": "Wed, 8 Mar 2017 18:49:41 GMT"}], "update_date": "2020-02-13", "authors_parsed": [["Tsakiris", "Manolis C.", ""], ["Vidal", "Rene", ""]]}, {"id": "1506.06343", "submitter": "Chunhua Shen", "authors": "Yao Li, Lingqiao Liu, Chunhua Shen, Anton van den Hengel", "title": "Mining Mid-level Visual Patterns with Deep CNN Activations", "comments": "20 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The purpose of mid-level visual element discovery is to find clusters of\nimage patches that are both representative and discriminative. Here we study\nthis problem from the prospective of pattern mining while relying on the\nrecently popularized Convolutional Neural Networks (CNNs). We observe that a\nfully-connected CNN activation extracted from an image patch typically\npossesses two appealing properties that enable its seamless integration with\npattern mining techniques. The marriage between CNN activations and association\nrule mining, a well-known pattern mining technique in the literature, leads to\nfast and effective discovery of representative and discriminative patterns from\na huge number of image patches. When we retrieve and visualize image patches\nwith the same pattern, surprisingly, they are not only visually similar but\nalso semantically consistent, and thus give rise to a mid-level visual element\nin our work. Given the patterns and retrieved mid-level visual elements, we\npropose two methods to generate image feature representations for each. The\nfirst method is to use the patterns as codewords in a dictionary, similar to\nthe Bag-of-Visual-Words model, we compute a Bag-of-Patterns representation. The\nsecond one relies on the retrieved mid-level visual elements to construct a\nBag-of-Elements representation. We evaluate the two encoding methods on scene\nand object classification tasks, and demonstrate that our approach outperforms\nor matches recent works using CNN activations for these tasks.\n", "versions": [{"version": "v1", "created": "Sun, 21 Jun 2015 10:22:47 GMT"}, {"version": "v2", "created": "Fri, 26 Jun 2015 07:19:40 GMT"}, {"version": "v3", "created": "Sun, 29 May 2016 07:52:17 GMT"}], "update_date": "2016-05-31", "authors_parsed": [["Li", "Yao", ""], ["Liu", "Lingqiao", ""], ["Shen", "Chunhua", ""], ["Hengel", "Anton van den", ""]]}, {"id": "1506.06448", "submitter": "Holger Roth", "authors": "Holger R. Roth, Le Lu, Amal Farag, Hoo-Chang Shin, Jiamin Liu, Evrim\n  Turkbey, and Ronald M. Summers", "title": "DeepOrgan: Multi-level Deep Convolutional Networks for Automated\n  Pancreas Segmentation", "comments": "To be presented at MICCAI 2015 - 18th International Conference on\n  Medical Computing and Computer Assisted Interventions, Munich, Germany", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/publicdomain/", "abstract": "  Automatic organ segmentation is an important yet challenging problem for\nmedical image analysis. The pancreas is an abdominal organ with very high\nanatomical variability. This inhibits previous segmentation methods from\nachieving high accuracies, especially compared to other organs such as the\nliver, heart or kidneys. In this paper, we present a probabilistic bottom-up\napproach for pancreas segmentation in abdominal computed tomography (CT) scans,\nusing multi-level deep convolutional networks (ConvNets). We propose and\nevaluate several variations of deep ConvNets in the context of hierarchical,\ncoarse-to-fine classification on image patches and regions, i.e. superpixels.\nWe first present a dense labeling of local image patches via\n$P{-}\\mathrm{ConvNet}$ and nearest neighbor fusion. Then we describe a regional\nConvNet ($R_1{-}\\mathrm{ConvNet}$) that samples a set of bounding boxes around\neach image superpixel at different scales of contexts in a \"zoom-out\" fashion.\nOur ConvNets learn to assign class probabilities for each superpixel region of\nbeing pancreas. Last, we study a stacked $R_2{-}\\mathrm{ConvNet}$ leveraging\nthe joint space of CT intensities and the $P{-}\\mathrm{ConvNet}$ dense\nprobability maps. Both 3D Gaussian smoothing and 2D conditional random fields\nare exploited as structured predictions for post-processing. We evaluate on CT\nimages of 82 patients in 4-fold cross-validation. We achieve a Dice Similarity\nCoefficient of 83.6$\\pm$6.3% in training and 71.8$\\pm$10.7% in testing.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jun 2015 02:56:42 GMT"}], "update_date": "2015-06-23", "authors_parsed": [["Roth", "Holger R.", ""], ["Lu", "Le", ""], ["Farag", "Amal", ""], ["Shin", "Hoo-Chang", ""], ["Liu", "Jiamin", ""], ["Turkbey", "Evrim", ""], ["Summers", "Ronald M.", ""]]}, {"id": "1506.06579", "submitter": "Jason Yosinski", "authors": "Jason Yosinski and Jeff Clune and Anh Nguyen and Thomas Fuchs and Hod\n  Lipson", "title": "Understanding Neural Networks Through Deep Visualization", "comments": "12 pages. To appear at ICML Deep Learning Workshop 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  Recent years have produced great advances in training large, deep neural\nnetworks (DNNs), including notable successes in training convolutional neural\nnetworks (convnets) to recognize natural images. However, our understanding of\nhow these models work, especially what computations they perform at\nintermediate layers, has lagged behind. Progress in the field will be further\naccelerated by the development of better tools for visualizing and interpreting\nneural nets. We introduce two such tools here. The first is a tool that\nvisualizes the activations produced on each layer of a trained convnet as it\nprocesses an image or video (e.g. a live webcam stream). We have found that\nlooking at live activations that change in response to user input helps build\nvaluable intuitions about how convnets work. The second tool enables\nvisualizing features at each layer of a DNN via regularized optimization in\nimage space. Because previous versions of this idea produced less recognizable\nimages, here we introduce several new regularization methods that combine to\nproduce qualitatively clearer, more interpretable visualizations. Both tools\nare open source and work on a pre-trained convnet with minimal setup.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jun 2015 12:57:15 GMT"}], "update_date": "2015-06-23", "authors_parsed": [["Yosinski", "Jason", ""], ["Clune", "Jeff", ""], ["Nguyen", "Anh", ""], ["Fuchs", "Thomas", ""], ["Lipson", "Hod", ""]]}, {"id": "1506.06628", "submitter": "Yunchao Wei", "authors": "Yunchao Wei, Yao Zhao, Zhenfeng Zhu, Shikui Wei, Yanhui Xiao, Jiashi\n  Feng and Shuicheng Yan", "title": "Modality-dependent Cross-media Retrieval", "comments": "in ACM Transactions on Intelligent Systems and Technology", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we investigate the cross-media retrieval between images and\ntext, i.e., using image to search text (I2T) and using text to search images\n(T2I). Existing cross-media retrieval methods usually learn one couple of\nprojections, by which the original features of images and text can be projected\ninto a common latent space to measure the content similarity. However, using\nthe same projections for the two different retrieval tasks (I2T and T2I) may\nlead to a tradeoff between their respective performances, rather than their\nbest performances. Different from previous works, we propose a\nmodality-dependent cross-media retrieval (MDCR) model, where two couples of\nprojections are learned for different cross-media retrieval tasks instead of\none couple of projections. Specifically, by jointly optimizing the correlation\nbetween images and text and the linear regression from one modal space (image\nor text) to the semantic space, two couples of mappings are learned to project\nimages and text from their original feature spaces into two common latent\nsubspaces (one for I2T and the other for T2I). Extensive experiments show the\nsuperiority of the proposed MDCR compared with other methods. In particular,\nbased the 4,096 dimensional convolutional neural network (CNN) visual feature\nand 100 dimensional LDA textual feature, the mAP of the proposed method\nachieves 41.5\\%, which is a new state-of-the-art performance on the Wikipedia\ndataset.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jun 2015 14:33:39 GMT"}, {"version": "v2", "created": "Tue, 23 Jun 2015 01:34:01 GMT"}], "update_date": "2015-06-24", "authors_parsed": [["Wei", "Yunchao", ""], ["Zhao", "Yao", ""], ["Zhu", "Zhenfeng", ""], ["Wei", "Shikui", ""], ["Xiao", "Yanhui", ""], ["Feng", "Jiashi", ""], ["Yan", "Shuicheng", ""]]}, {"id": "1506.06659", "submitter": "Nayyab Naseem", "authors": "Nayyab Naseem, Mehreen Sirshar", "title": "Target Tracking In Real Time Surveillance Cameras and Videos", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Security concerns has been kept on increasing, so it is important for\neveryone to keep their property safe from thefts and destruction. So the need\nfor surveillance techniques are also increasing. The system has been developed\nto detect the motion in a video. A system has been developed for real time\napplications by using the techniques of background subtraction and frame\ndifferencing. In this system, motion is detected from the webcam or from the\nreal time video. Background subtraction and frames differencing method has been\nused to detect the moving target. In background subtraction method, current\nframe is subtracted from the referenced frame and then the threshold is\napplied. If the difference is greater than the threshold then it is considered\nas the pixel from the moving object, otherwise it is considered as background\npixel. Similarly, two frames difference method takes difference between two\ncontinuous frames. Then that resultant difference frame is thresholded and the\namount of difference pixels is calculated.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jun 2015 15:55:11 GMT"}], "update_date": "2015-06-23", "authors_parsed": [["Naseem", "Nayyab", ""], ["Sirshar", "Mehreen", ""]]}, {"id": "1506.06681", "submitter": "Alex James Dr", "authors": "Sherin Sugathan, Reshma Scaria, Alex Pappachen James", "title": "Adaptive Digital Scan Variable Pixels", "comments": "4th International Conference on Advances in Computing, Communications\n  and Informatics, August, 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  The square and rectangular shape of the pixels in the digital images for\nsensing and display purposes introduces several inaccuracies in the\nrepresentation of digital images. The major disadvantage of square pixel shapes\nis the inability to accurately capture and display the details in the objects\nhaving variable orientations to edges, shapes and regions. This effect can be\nobserved by the inaccurate representation of diagonal edges in low resolution\nsquare pixel images. This paper explores a less investigated idea of using\nvariable shaped pixels for improving visual quality of image scans without\nincreasing the square pixel resolution. The proposed adaptive filtering\ntechnique reports an improvement in image PSNR.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jun 2015 17:09:04 GMT"}], "update_date": "2015-06-23", "authors_parsed": [["Sugathan", "Sherin", ""], ["Scaria", "Reshma", ""], ["James", "Alex Pappachen", ""]]}, {"id": "1506.06724", "submitter": "Yukun Zhu", "authors": "Yukun Zhu, Ryan Kiros, Richard Zemel, Ruslan Salakhutdinov, Raquel\n  Urtasun, Antonio Torralba, Sanja Fidler", "title": "Aligning Books and Movies: Towards Story-like Visual Explanations by\n  Watching Movies and Reading Books", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Books are a rich source of both fine-grained information, how a character, an\nobject or a scene looks like, as well as high-level semantics, what someone is\nthinking, feeling and how these states evolve through a story. This paper aims\nto align books to their movie releases in order to provide rich descriptive\nexplanations for visual content that go semantically far beyond the captions\navailable in current datasets. To align movies and books we exploit a neural\nsentence embedding that is trained in an unsupervised way from a large corpus\nof books, as well as a video-text neural embedding for computing similarities\nbetween movie clips and sentences in the book. We propose a context-aware CNN\nto combine information from multiple sources. We demonstrate good quantitative\nperformance for movie/book alignment and show several qualitative examples that\nshowcase the diversity of tasks our model can be used for.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jun 2015 19:26:56 GMT"}], "update_date": "2015-06-23", "authors_parsed": [["Zhu", "Yukun", ""], ["Kiros", "Ryan", ""], ["Zemel", "Richard", ""], ["Salakhutdinov", "Ruslan", ""], ["Urtasun", "Raquel", ""], ["Torralba", "Antonio", ""], ["Fidler", "Sanja", ""]]}, {"id": "1506.06825", "submitter": "John Flynn", "authors": "John Flynn, Ivan Neulander, James Philbin, Noah Snavely", "title": "DeepStereo: Learning to Predict New Views from the World's Imagery", "comments": "Video showing additional results available at\n  http://youtu.be/cizgVZ8rjKA", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep networks have recently enjoyed enormous success when applied to\nrecognition and classification problems in computer vision, but their use in\ngraphics problems has been limited. In this work, we present a novel deep\narchitecture that performs new view synthesis directly from pixels, trained\nfrom a large number of posed image sets. In contrast to traditional approaches\nwhich consist of multiple complex stages of processing, each of which require\ncareful tuning and can fail in unexpected ways, our system is trained\nend-to-end. The pixels from neighboring views of a scene are presented to the\nnetwork which then directly produces the pixels of the unseen view. The\nbenefits of our approach include generality (we only require posed image sets\nand can easily apply our method to different domains), and high quality results\non traditionally difficult scenes. We believe this is due to the end-to-end\nnature of our system which is able to plausibly generate pixels according to\ncolor, depth, and texture priors learnt automatically from the training data.\nTo verify our method we show that it can convincingly reproduce known test\nviews from nearby imagery. Additionally we show images rendered from novel\nviewpoints. To our knowledge, our work is the first to apply deep learning to\nthe problem of new view synthesis from sets of real-world, natural imagery.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jun 2015 23:48:21 GMT"}], "update_date": "2015-06-24", "authors_parsed": [["Flynn", "John", ""], ["Neulander", "Ivan", ""], ["Philbin", "James", ""], ["Snavely", "Noah", ""]]}, {"id": "1506.06833", "submitter": "Francis Ferraro", "authors": "Francis Ferraro, Nasrin Mostafazadeh, Ting-Hao (Kenneth) Huang, Lucy\n  Vanderwende, Jacob Devlin, Michel Galley, Margaret Mitchell", "title": "A Survey of Current Datasets for Vision and Language Research", "comments": "To appear in EMNLP 2015, short proceedings. Dataset analysis and\n  discussion expanded, including an initial examination into reporting bias for\n  one of them. F.F. and N.M. contributed equally to this work", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.CV cs.GL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Integrating vision and language has long been a dream in work on artificial\nintelligence (AI). In the past two years, we have witnessed an explosion of\nwork that brings together vision and language from images to videos and beyond.\nThe available corpora have played a crucial role in advancing this area of\nresearch. In this paper, we propose a set of quality metrics for evaluating and\nanalyzing the vision & language datasets and categorize them accordingly. Our\nanalyses show that the most recent datasets have been using more complex\nlanguage and more abstract concepts, however, there are different strengths and\nweaknesses in each.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jun 2015 00:59:27 GMT"}, {"version": "v2", "created": "Wed, 19 Aug 2015 04:33:37 GMT"}], "update_date": "2015-08-20", "authors_parsed": [["Ferraro", "Francis", "", "Kenneth"], ["Mostafazadeh", "Nasrin", "", "Kenneth"], ["Ting-Hao", "", "", "Kenneth"], ["Huang", "", ""], ["Vanderwende", "Lucy", ""], ["Devlin", "Jacob", ""], ["Galley", "Michel", ""], ["Mitchell", "Margaret", ""]]}, {"id": "1506.06868", "submitter": "Luping Zhou", "authors": "Luping Zhou, Lei Wang, Lingqiao Liu, Philip Ogunbona, Dinggang Shen", "title": "Learning Discriminative Bayesian Networks from High-dimensional\n  Continuous Neuroimaging Data", "comments": "16 pages and 5 figures for the article (excluding appendix)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to its causal semantics, Bayesian networks (BN) have been widely employed\nto discover the underlying data relationship in exploratory studies, such as\nbrain research. Despite its success in modeling the probability distribution of\nvariables, BN is naturally a generative model, which is not necessarily\ndiscriminative. This may cause the ignorance of subtle but critical network\nchanges that are of investigation values across populations. In this paper, we\npropose to improve the discriminative power of BN models for continuous\nvariables from two different perspectives. This brings two general\ndiscriminative learning frameworks for Gaussian Bayesian networks (GBN). In the\nfirst framework, we employ Fisher kernel to bridge the generative models of GBN\nand the discriminative classifiers of SVMs, and convert the GBN parameter\nlearning to Fisher kernel learning via minimizing a generalization error bound\nof SVMs. In the second framework, we employ the max-margin criterion and build\nit directly upon GBN models to explicitly optimize the classification\nperformance of the GBNs. The advantages and disadvantages of the two frameworks\nare discussed and experimentally compared. Both of them demonstrate strong\npower in learning discriminative parameters of GBNs for neuroimaging based\nbrain network analysis, as well as maintaining reasonable representation\ncapacity. The contributions of this paper also include a new Directed Acyclic\nGraph (DAG) constraint with theoretical guarantee to ensure the graph validity\nof GBN.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jun 2015 05:39:34 GMT"}], "update_date": "2015-06-24", "authors_parsed": [["Zhou", "Luping", ""], ["Wang", "Lei", ""], ["Liu", "Lingqiao", ""], ["Ogunbona", "Philip", ""], ["Shen", "Dinggang", ""]]}, {"id": "1506.06876", "submitter": "Alexander Popov", "authors": "Alexander Popov, Dimitrios Zermas and Nikolaos Papanikolopoulos", "title": "Autonomous 3D Reconstruction Using a MAV", "comments": "6 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An approach is proposed for high resolution 3D reconstruction of an object\nusing a Micro Air Vehicle (MAV). A system is described which autonomously\ncaptures images and performs a dense 3D reconstruction via structure from\nmotion with no prior knowledge of the environment. Only the MAVs own sensors,\nthe front facing camera and the Inertial Measurement Unit (IMU) are utilized.\nPrecision agriculture is considered as an example application for the system.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jun 2015 06:35:08 GMT"}], "update_date": "2015-06-24", "authors_parsed": [["Popov", "Alexander", ""], ["Zermas", "Dimitrios", ""], ["Papanikolopoulos", "Nikolaos", ""]]}, {"id": "1506.06881", "submitter": "Ognjen Arandjelovi\\'c PhD", "authors": "Ognjen Arandjelovic", "title": "Automatic vehicle tracking and recognition from aerial image sequences", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the problem of automated vehicle tracking and\nrecognition from aerial image sequences. Motivated by its successes in the\nexisting literature focus on the use of linear appearance subspaces to describe\nmulti-view object appearance and highlight the challenges involved in their\napplication as a part of a practical system. A working solution which includes\nsteps for data extraction and normalization is described. In experiments on\nreal-world data the proposed methodology achieved promising results with a high\ncorrect recognition rate and few, meaningful errors (type II errors whereby\ngenuinely similar targets are sometimes being confused with one another).\nDirections for future research and possible improvements of the proposed method\nare discussed.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jun 2015 07:12:17 GMT"}], "update_date": "2015-06-24", "authors_parsed": [["Arandjelovic", "Ognjen", ""]]}, {"id": "1506.06882", "submitter": "Xavier Alameda-Pineda", "authors": "Xavier Alameda-Pineda, Jacopo Staiano, Ramanathan Subramanian, Ligia\n  Batrinca, Elisa Ricci, Bruno Lepri, Oswald Lanz, Nicu Sebe", "title": "SALSA: A Novel Dataset for Multimodal Group Behavior Analysis", "comments": "14 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/publicdomain/", "abstract": "  Studying free-standing conversational groups (FCGs) in unstructured social\nsettings (e.g., cocktail party ) is gratifying due to the wealth of information\navailable at the group (mining social networks) and individual (recognizing\nnative behavioral and personality traits) levels. However, analyzing social\nscenes involving FCGs is also highly challenging due to the difficulty in\nextracting behavioral cues such as target locations, their speaking activity\nand head/body pose due to crowdedness and presence of extreme occlusions. To\nthis end, we propose SALSA, a novel dataset facilitating multimodal and\nSynergetic sociAL Scene Analysis, and make two main contributions to research\non automated social interaction analysis: (1) SALSA records social interactions\namong 18 participants in a natural, indoor environment for over 60 minutes,\nunder the poster presentation and cocktail party contexts presenting\ndifficulties in the form of low-resolution images, lighting variations,\nnumerous occlusions, reverberations and interfering sound sources; (2) To\nalleviate these problems we facilitate multimodal analysis by recording the\nsocial interplay using four static surveillance cameras and sociometric badges\nworn by each participant, comprising the microphone, accelerometer, bluetooth\nand infrared sensors. In addition to raw data, we also provide annotations\nconcerning individuals' personality as well as their position, head, body\norientation and F-formation information over the entire event duration. Through\nextensive experiments with state-of-the-art approaches, we show (a) the\nlimitations of current methods and (b) how the recorded multiple cues\nsynergetically aid automatic analysis of social interactions. SALSA is\navailable at http://tev.fbk.eu/salsa.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jun 2015 07:19:24 GMT"}], "update_date": "2015-06-24", "authors_parsed": [["Alameda-Pineda", "Xavier", ""], ["Staiano", "Jacopo", ""], ["Subramanian", "Ramanathan", ""], ["Batrinca", "Ligia", ""], ["Ricci", "Elisa", ""], ["Lepri", "Bruno", ""], ["Lanz", "Oswald", ""], ["Sebe", "Nicu", ""]]}, {"id": "1506.06905", "submitter": "Jiuqing Wan", "authors": "Jiuqing Wan, Menglin Xing", "title": "Person re-identification via efficient inference in fully connected CRF", "comments": "7 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we address the problem of person re-identification problem,\ni.e., retrieving instances from gallery which are generated by the same person\nas the given probe image. This is very challenging because the person's\nappearance usually undergoes significant variations due to changes in\nillumination, camera angle and view, background clutter, and occlusion over the\ncamera network. In this paper, we assume that the matched gallery images should\nnot only be similar to the probe, but also be similar to each other, under\nsuitable metric. We express this assumption with a fully connected CRF model in\nwhich each node corresponds to a gallery and every pair of nodes are connected\nby an edge. A label variable is associated with each node to indicate whether\nthe corresponding image is from target person. We define unary potential for\neach node using existing feature calculation and matching techniques, which\nreflect the similarity between probe and gallery image, and define pairwise\npotential for each edge in terms of a weighed combination of Gaussian kernels,\nwhich encode appearance similarity between pair of gallery images. The specific\nform of pairwise potential allows us to exploit an efficient inference\nalgorithm to calculate the marginal distribution of each label variable for\nthis dense connected CRF. We show the superiority of our method by applying it\nto public datasets and comparing with the state of the art.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jun 2015 08:27:19 GMT"}], "update_date": "2015-06-24", "authors_parsed": [["Wan", "Jiuqing", ""], ["Xing", "Menglin", ""]]}, {"id": "1506.06981", "submitter": "Karel Lenc", "authors": "Karel Lenc and Andrea Vedaldi", "title": "R-CNN minus R", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep convolutional neural networks (CNNs) have had a major impact in most\nareas of image understanding, including object category detection. In object\ndetection, methods such as R-CNN have obtained excellent results by integrating\nCNNs with region proposal generation algorithms such as selective search. In\nthis paper, we investigate the role of proposal generation in CNN-based\ndetectors in order to determine whether it is a necessary modelling component,\ncarrying essential geometric information not contained in the CNN, or whether\nit is merely a way of accelerating detection. We do so by designing and\nevaluating a detector that uses a trivial region generation scheme, constant\nfor each image. Combined with SPP, this results in an excellent and fast\ndetector that does not require to process an image with algorithms other than\nthe CNN itself. We also streamline and simplify the training of CNN-based\ndetectors by integrating several learning steps in a single algorithm, as well\nas by proposing a number of improvements that accelerate detection.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jun 2015 13:26:36 GMT"}], "update_date": "2015-06-24", "authors_parsed": [["Lenc", "Karel", ""], ["Vedaldi", "Andrea", ""]]}, {"id": "1506.07062", "submitter": "Jorg Portegies", "authors": "J.M. Portegies and R.H.J. Fick and G.R. Sanguinetti and S.P.L.\n  Meesters and G. Girard and R. Duits", "title": "Improving Fiber Alignment in HARDI by Combining Contextual PDE Flow with\n  Constrained Spherical Deconvolution", "comments": null, "journal-ref": null, "doi": "10.1371/journal.pone.0138122", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose two strategies to improve the quality of tractography results\ncomputed from diffusion weighted magnetic resonance imaging (DW-MRI) data. Both\nmethods are based on the same PDE framework, defined in the coupled space of\npositions and orientations, associated with a stochastic process describing the\nenhancement of elongated structures while preserving crossing structures. In\nthe first method we use the enhancement PDE for contextual regularization of a\nfiber orientation distribution (FOD) that is obtained on individual voxels from\nhigh angular resolution diffusion imaging (HARDI) data via constrained\nspherical deconvolution (CSD). Thereby we improve the FOD as input for\nsubsequent tractography. Secondly, we introduce the fiber to bundle coherence\n(FBC), a measure for quantification of fiber alignment. The FBC is computed\nfrom a tractography result using the same PDE framework and provides a\ncriterion for removing the spurious fibers. We validate the proposed\ncombination of CSD and enhancement on phantom data and on human data, acquired\nwith different scanning protocols. On the phantom data we find that PDE\nenhancements improve both local metrics and global metrics of tractography\nresults, compared to CSD without enhancements. On the human data we show that\nthe enhancements allow for a better reconstruction of crossing fiber bundles\nand they reduce the variability of the tractography output with respect to the\nacquisition parameters. Finally, we show that both the enhancement of the FODs\nand the use of the FBC measure on the tractography improve the stability with\nrespect to different stochastic realizations of probabilistic tractography.\nThis is shown in a clinical application: the reconstruction of the optic\nradiation for epilepsy surgery planning.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jun 2015 15:58:16 GMT"}], "update_date": "2017-02-08", "authors_parsed": [["Portegies", "J. M.", ""], ["Fick", "R. H. J.", ""], ["Sanguinetti", "G. R.", ""], ["Meesters", "S. P. L.", ""], ["Girard", "G.", ""], ["Duits", "R.", ""]]}, {"id": "1506.07136", "submitter": "Heike Benninghoff", "authors": "Heike Benninghoff and Harald Garcke", "title": "Segmentation of Three-dimensional Images with Parametric Active Surfaces\n  and Topology Changes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce a novel parametric method for segmentation of\nthree-dimensional images. We consider a piecewise constant version of the\nMumford-Shah and the Chan-Vese functionals and perform a region-based\nsegmentation of 3D image data. An evolution law is derived from energy\nminimization problems which push the surfaces to the boundaries of 3D objects\nin the image. We propose a parametric scheme which describes the evolution of\nparametric surfaces. An efficient finite element scheme is proposed for a\nnumerical approximation of the evolution equations. Since standard parametric\nmethods cannot handle topology changes automatically, an efficient method is\npresented to detect, identify and perform changes in the topology of the\nsurfaces. One main focus of this paper are the algorithmic details to handle\ntopology changes like splitting and merging of surfaces and change of the genus\nof a surface. Different artificial images are studied to demonstrate the\nability to detect the different types of topology changes. Finally, the\nparametric method is applied to segmentation of medical 3D images.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jun 2015 18:47:44 GMT"}], "update_date": "2015-06-24", "authors_parsed": [["Benninghoff", "Heike", ""], ["Garcke", "Harald", ""]]}, {"id": "1506.07194", "submitter": "Giuseppe Boccignone", "authors": "Giuseppe Boccignone", "title": "Advanced statistical methods for eye movement analysis and modeling: a\n  gentle introduction", "comments": "Draft of Chapter to appear in \"An introduction to the scientific\n  foundations of eye movement research and its applications\"", "journal-ref": null, "doi": "10.1007/978-3-030-20085-5_9", "report-no": null, "categories": "physics.data-an cs.CV q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this Chapter we show that by considering eye movements, and in particular,\nthe resulting sequence of gaze shifts, a stochastic process, a wide variety of\ntools become available for analyses and modelling beyond conventional\nstatistical methods. Such tools encompass random walk analyses and more complex\ntechniques borrowed from the pattern recognition and machine learning fields.\n  After a brief, though critical, probabilistic tour of current computational\nmodels of eye movements and visual attention, we lay down the basis for gaze\nshift pattern analysis. To this end, the concepts of Markov Processes, the\nWiener process and related random walks within the Gaussian framework of the\nCentral Limit Theorem will be introduced. Then, we will deliberately violate\nfundamental assumptions of the Central Limit Theorem to elicit a larger\nperspective, rooted in statistical physics, for analysing and modelling eye\nmovements in terms of anomalous, non-Gaussian, random walks and modern foraging\ntheory.\n  Eventually, by resorting to machine learning techniques, we discuss how the\nanalyses of movement patterns can develop into the inference of hidden patterns\nof the mind: inferring the observer's task, assessing cognitive impairments,\nclassifying expertise.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jun 2015 20:38:15 GMT"}, {"version": "v2", "created": "Wed, 2 Dec 2015 19:04:30 GMT"}, {"version": "v3", "created": "Sun, 27 Dec 2015 10:42:49 GMT"}, {"version": "v4", "created": "Fri, 25 Aug 2017 18:09:56 GMT"}], "update_date": "2019-11-05", "authors_parsed": [["Boccignone", "Giuseppe", ""]]}, {"id": "1506.07224", "submitter": "Jian Guo", "authors": "Jian Guo, Stephen Gould", "title": "Deep CNN Ensemble with Data Augmentation for Object Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  We report on the methods used in our recent DeepEnsembleCoco submission to\nthe PASCAL VOC 2012 challenge, which achieves state-of-the-art performance on\nthe object detection task. Our method is a variant of the R-CNN model proposed\nGirshick:CVPR14 with two key improvements to training and evaluation. First,\nour method constructs an ensemble of deep CNN models with different\narchitectures that are complementary to each other. Second, we augment the\nPASCAL VOC training set with images from the Microsoft COCO dataset to\nsignificantly enlarge the amount training data. Importantly, we select a subset\nof the Microsoft COCO images to be consistent with the PASCAL VOC task. Results\non the PASCAL VOC evaluation server show that our proposed method outperform\nall previous methods on the PASCAL VOC 2012 detection task at time of\nsubmission.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jun 2015 02:15:17 GMT"}], "update_date": "2015-06-25", "authors_parsed": [["Guo", "Jian", ""], ["Gould", "Stephen", ""]]}, {"id": "1506.07236", "submitter": "Kanji Tanaka", "authors": "Kanji Tanaka, Eiji Kondo", "title": "Incremental RANSAC for Online Relocation in Large Dynamic Environments", "comments": "Offprint of ICRA2006 paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Vehicle relocation is the problem in which a mobile robot has to estimate the\nself-position with respect to an a priori map of landmarks using the perception\nand the motion measurements without using any knowledge of the initial\nself-position. Recently, RANdom SAmple Consensus (RANSAC), a robust\nmulti-hypothesis estimator, has been successfully applied to offline relocation\nin static environments. On the other hand, online relocation in dynamic\nenvironments is still a difficult problem, for available computation time is\nalways limited, and for measurement include many outliers. To realize real time\nalgorithm for such an online process, we have developed an incremental version\nof RANSAC algorithm by extending an efficient preemption RANSAC scheme. This\nnovel scheme named incremental RANSAC is able to find inlier hypotheses of\nself-positions out of large number of outlier hypotheses contaminated by\noutlier measurements.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jun 2015 04:07:28 GMT"}, {"version": "v2", "created": "Thu, 25 Jun 2015 03:24:34 GMT"}], "update_date": "2015-06-26", "authors_parsed": [["Tanaka", "Kanji", ""], ["Kondo", "Eiji", ""]]}, {"id": "1506.07257", "submitter": "Jingyu Gao", "authors": "Jingyu Gao, Jinfu Yang, Guanghui Wang and Mingai Li", "title": "A Novel Feature Extraction Method for Scene Recognition Based on\n  Centered Convolutional Restricted Boltzmann Machines", "comments": "22 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scene recognition is an important research topic in computer vision, while\nfeature extraction is a key step of object recognition. Although classical\nRestricted Boltzmann machines (RBM) can efficiently represent complicated data,\nit is hard to handle large images due to its complexity in computation. In this\npaper, a novel feature extraction method, named Centered Convolutional\nRestricted Boltzmann Machines (CCRBM), is proposed for scene recognition. The\nproposed model is an improved Convolutional Restricted Boltzmann Machines\n(CRBM) by introducing centered factors in its learning strategy to reduce the\nsource of instabilities. First, the visible units of the network are redefined\nusing centered factors. Then, the hidden units are learned with a modified\nenergy function by utilizing a distribution function, and the visible units are\nreconstructed using the learned hidden units. In order to achieve better\ngenerative ability, the Centered Convolutional Deep Belief Networks (CCDBN) is\ntrained in a greedy layer-wise way. Finally, a softmax regression is\nincorporated for scene recognition. Extensive experimental evaluations using\nnatural scenes, MIT-indoor scenes, and Caltech 101 datasets show that the\nproposed approach performs better than other counterparts in terms of\nstability, generalization, and discrimination. The CCDBN model is more suitable\nfor natural scene image recognition by virtue of convolutional property.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jun 2015 06:42:42 GMT"}], "update_date": "2015-06-25", "authors_parsed": [["Gao", "Jingyu", ""], ["Yang", "Jinfu", ""], ["Wang", "Guanghui", ""], ["Li", "Mingai", ""]]}, {"id": "1506.07271", "submitter": "Jingyu Gao", "authors": "Jinfu Yang, Jingyu Gao, Guanghui Wang, Shanshan Zhang", "title": "Natural Scene Recognition Based on Superpixels and Deep Boltzmann\n  Machines", "comments": "29 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Deep Boltzmann Machines (DBM) is a state-of-the-art unsupervised learning\nmodel, which has been successfully applied to handwritten digit recognition\nand, as well as object recognition. However, the DBM is limited in scene\nrecognition due to the fact that natural scene images are usually very large.\nIn this paper, an efficient scene recognition approach is proposed based on\nsuperpixels and the DBMs. First, a simple linear iterative clustering (SLIC)\nalgorithm is employed to generate superpixels of input images, where each\nsuperpixel is regarded as an input of a learning model. Then, a two-layer DBM\nmodel is constructed by stacking two restricted Boltzmann machines (RBMs), and\na greedy layer-wise algorithm is applied to train the DBM model. Finally, a\nsoftmax regression is utilized to categorize scene images. The proposed\ntechnique can effectively reduce the computational complexity and enhance the\nperformance for large natural image recognition. The approach is verified and\nevaluated by extensive experiments, including the fifteen-scene categories\ndataset the UIUC eight-sports dataset, and the SIFT flow dataset, are used to\nevaluate the proposed method. The experimental results show that the proposed\napproach outperforms other state-of-the-art methods in terms of recognition\nrate.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jun 2015 07:53:54 GMT"}], "update_date": "2015-06-25", "authors_parsed": [["Yang", "Jinfu", ""], ["Gao", "Jingyu", ""], ["Wang", "Guanghui", ""], ["Zhang", "Shanshan", ""]]}, {"id": "1506.07310", "submitter": "Jingtuo Liu", "authors": "Jingtuo Liu and Yafeng Deng and Tao Bai and Zhengping Wei and Chang\n  Huang", "title": "Targeting Ultimate Accuracy: Face Recognition via Deep Embedding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Face Recognition has been studied for many decades. As opposed to traditional\nhand-crafted features such as LBP and HOG, much more sophisticated features can\nbe learned automatically by deep learning methods in a data-driven way. In this\npaper, we propose a two-stage approach that combines a multi-patch deep CNN and\ndeep metric learning, which extracts low dimensional but very discriminative\nfeatures for face verification and recognition. Experiments show that this\nmethod outperforms other state-of-the-art methods on LFW dataset, achieving\n99.77% pair-wise verification accuracy and significantly better accuracy under\nother two more practical protocols. This paper also discusses the importance of\ndata size and the number of patches, showing a clear path to practical\nhigh-performance face recognition systems in real world.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jun 2015 10:36:26 GMT"}, {"version": "v2", "created": "Thu, 25 Jun 2015 03:05:20 GMT"}, {"version": "v3", "created": "Fri, 26 Jun 2015 03:05:45 GMT"}, {"version": "v4", "created": "Thu, 23 Jul 2015 02:34:29 GMT"}], "update_date": "2015-07-24", "authors_parsed": [["Liu", "Jingtuo", ""], ["Deng", "Yafeng", ""], ["Bai", "Tao", ""], ["Wei", "Zhengping", ""], ["Huang", "Chang", ""]]}, {"id": "1506.07363", "submitter": "Venkatesh Babu R", "authors": "Sai Srivatsa R and R. Venkatesh Babu", "title": "Salient Object Detection via Objectness Measure", "comments": "In IEEE International Conference on Image Processing (ICIP), 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Salient object detection has become an important task in many image\nprocessing applications. The existing approaches exploit background prior and\ncontrast prior to attain state of the art results. In this paper, instead of\nusing background cues, we estimate the foreground regions in an image using\nobjectness proposals and utilize it to obtain smooth and accurate saliency\nmaps. We propose a novel saliency measure called `foreground connectivity'\nwhich determines how tightly a pixel or a region is connected to the estimated\nforeground. We use the values assigned by this measure as foreground weights\nand integrate these in an optimization framework to obtain the final saliency\nmaps. We extensively evaluate the proposed approach on two benchmark databases\nand demonstrate that the results obtained are better than the existing state of\nthe art approaches.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jun 2015 13:44:21 GMT"}], "update_date": "2015-06-25", "authors_parsed": [["R", "Sai Srivatsa", ""], ["Babu", "R. Venkatesh", ""]]}, {"id": "1506.07365", "submitter": "Manuel Watter", "authors": "Manuel Watter, Jost Tobias Springenberg, Joschka Boedecker, Martin\n  Riedmiller", "title": "Embed to Control: A Locally Linear Latent Dynamics Model for Control\n  from Raw Images", "comments": "Final NIPS version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce Embed to Control (E2C), a method for model learning and control\nof non-linear dynamical systems from raw pixel images. E2C consists of a deep\ngenerative model, belonging to the family of variational autoencoders, that\nlearns to generate image trajectories from a latent space in which the dynamics\nis constrained to be locally linear. Our model is derived directly from an\noptimal control formulation in latent space, supports long-term prediction of\nimage sequences and exhibits strong performance on a variety of complex control\nproblems.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jun 2015 13:48:51 GMT"}, {"version": "v2", "created": "Thu, 25 Jun 2015 21:08:02 GMT"}, {"version": "v3", "created": "Fri, 20 Nov 2015 14:49:18 GMT"}], "update_date": "2015-11-23", "authors_parsed": [["Watter", "Manuel", ""], ["Springenberg", "Jost Tobias", ""], ["Boedecker", "Joschka", ""], ["Riedmiller", "Martin", ""]]}, {"id": "1506.07439", "submitter": "Yuri Boykov", "authors": "Meng Tang, Dmitrii Marin, Ismail Ben Ayed, Yuri Boykov", "title": "Kernel Cuts: MRF meets Kernel & Spectral Clustering", "comments": "The main ideas of this work are published in our conference papers:\n  \"Normalized cut meets MRF\" [70] (ECCV 2016) and \"Secrets of Grabcut and\n  kernel K-means\" [41] (ICCV 2015)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new segmentation model combining common regularization energies,\ne.g. Markov Random Field (MRF) potentials, and standard pairwise clustering\ncriteria like Normalized Cut (NC), average association (AA), etc. These\nclustering and regularization models are widely used in machine learning and\ncomputer vision, but they were not combined before due to significant\ndifferences in the corresponding optimization, e.g. spectral relaxation and\ncombinatorial max-flow techniques. On the one hand, we show that many common\napplications using MRF segmentation energies can benefit from a high-order NC\nterm, e.g. enforcing balanced clustering of arbitrary high-dimensional image\nfeatures combining color, texture, location, depth, motion, etc. On the other\nhand, standard clustering applications can benefit from an inclusion of common\npairwise or higher-order MRF constraints, e.g. edge alignment, bin-consistency,\nlabel cost, etc. To address joint energies like NC+MRF, we propose efficient\nKernel Cut algorithms based on bound optimization. While focusing on graph cut\nand move-making techniques, our new unary (linear) kernel and spectral bound\nformulations for common pairwise clustering criteria allow to integrate them\nwith any regularization functionals with existing discrete or continuous\nsolvers.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jun 2015 16:00:43 GMT"}, {"version": "v2", "created": "Mon, 20 Jun 2016 21:08:46 GMT"}, {"version": "v3", "created": "Sun, 31 Jul 2016 16:46:32 GMT"}, {"version": "v4", "created": "Wed, 3 Aug 2016 06:57:28 GMT"}, {"version": "v5", "created": "Sun, 4 Sep 2016 20:03:02 GMT"}, {"version": "v6", "created": "Wed, 21 Sep 2016 03:41:06 GMT"}], "update_date": "2016-09-22", "authors_parsed": [["Tang", "Meng", ""], ["Marin", "Dmitrii", ""], ["Ayed", "Ismail Ben", ""], ["Boykov", "Yuri", ""]]}, {"id": "1506.07440", "submitter": "Jaderick Pabico", "authors": "Lei Kristoffer R. Lactuan and Jaderick P. Pabico", "title": "Unshredding of Shredded Documents: Computational Framework and\n  Implementation", "comments": "7 pages, 3 figures", "journal-ref": "Asia Pacific Journal of Multidisciplinary Research 3(3):20-25,\n  2015", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  A shredded document $D$ is a document whose pages have been cut into strips\nfor the purpose of destroying private, confidential, or sensitive information\n$I$ contained in $D$. Shredding has become a standard means of government\norganizations, businesses, and private individuals to destroy archival records\nthat have been officially classified for disposal. It can also be used to\ndestroy documentary evidence of wrongdoings by entities who are trying to hide\n$I$.\n  In this paper, we present an optimal $O((n\\times m)^2)$ algorithm $A$ that\nreconstructs an $n$-page $D$, where each page $p$ is shredded into $m$ strips.\nWe also present the efficacy of $A$ in reconstructing three document types:\nhand-written, machine typed-set, and images.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jun 2015 16:01:24 GMT"}], "update_date": "2015-07-23", "authors_parsed": [["Lactuan", "Lei Kristoffer R.", ""], ["Pabico", "Jaderick P.", ""]]}, {"id": "1506.07452", "submitter": "Wonmin Byeon", "authors": "Marijn F. Stollenga, Wonmin Byeon, Marcus Liwicki, Juergen Schmidhuber", "title": "Parallel Multi-Dimensional LSTM, With Application to Fast Biomedical\n  Volumetric Image Segmentation", "comments": "Marijn F. Stollenga and Wonmin Byeon are the shared first authors,\n  both authors contributed equally to this work", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional Neural Networks (CNNs) can be shifted across 2D images or 3D\nvideos to segment them. They have a fixed input size and typically perceive\nonly small local contexts of the pixels to be classified as foreground or\nbackground. In contrast, Multi-Dimensional Recurrent NNs (MD-RNNs) can perceive\nthe entire spatio-temporal context of each pixel in a few sweeps through all\npixels, especially when the RNN is a Long Short-Term Memory (LSTM). Despite\nthese theoretical advantages, however, unlike CNNs, previous MD-LSTM variants\nwere hard to parallelize on GPUs. Here we re-arrange the traditional cuboid\norder of computations in MD-LSTM in pyramidal fashion. The resulting\nPyraMiD-LSTM is easy to parallelize, especially for 3D data such as stacks of\nbrain slice images. PyraMiD-LSTM achieved best known pixel-wise brain image\nsegmentation results on MRBrainS13 (and competitive results on EM-ISBI12).\n", "versions": [{"version": "v1", "created": "Wed, 24 Jun 2015 16:26:51 GMT"}], "update_date": "2015-07-13", "authors_parsed": [["Stollenga", "Marijn F.", ""], ["Byeon", "Wonmin", ""], ["Liwicki", "Marcus", ""], ["Schmidhuber", "Juergen", ""]]}, {"id": "1506.07597", "submitter": "Steven L. Waslander", "authors": "Michael J. Tribou, David W. L. Wang, Steven L. Waslander", "title": "Degenerate Motions in Multicamera Cluster SLAM with Non-overlapping\n  Fields of View", "comments": "18 pages, 18 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An analysis of the relative motion and point feature model configurations\nleading to solution degeneracy is presented, for the case of a Simultaneous\nLocalization and Mapping system using multicamera clusters with non-overlapping\nfields-of-view. The SLAM optimization system seeks to minimize image space\nreprojection error and is formulated for a cluster containing any number of\ncomponent cameras, observing any number of point features over two keyframes.\nThe measurement Jacobian is transformed to expose a reduced-dimension\nrepresentation such that the degeneracy of the system can be determined by the\nrank of a dense submatrix. A set of relative motions sufficient for degeneracy\nare identified for certain cluster configurations, independent of target model\ngeometry. Furthermore, it is shown that increasing the number of cameras within\nthe cluster and observing features across different cameras over the two\nkeyframes reduces the size of the degenerate motion sets significantly.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jun 2015 02:27:29 GMT"}], "update_date": "2015-06-26", "authors_parsed": [["Tribou", "Michael J.", ""], ["Wang", "David W. L.", ""], ["Waslander", "Steven L.", ""]]}, {"id": "1506.07613", "submitter": "Sobhan Naderi Parizi", "authors": "Sobhan Naderi Parizi, Kun He, Reza Aghajani, Stan Sclaroff, Pedro\n  Felzenszwalb", "title": "Generalized Majorization-Minimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IT cs.LG math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Non-convex optimization is ubiquitous in machine learning.\nMajorization-Minimization (MM) is a powerful iterative procedure for optimizing\nnon-convex functions that works by optimizing a sequence of bounds on the\nfunction. In MM, the bound at each iteration is required to \\emph{touch} the\nobjective function at the optimizer of the previous bound. We show that this\ntouching constraint is unnecessary and overly restrictive. We generalize MM by\nrelaxing this constraint, and propose a new optimization framework, named\nGeneralized Majorization-Minimization (G-MM), that is more flexible. For\ninstance, G-MM can incorporate application-specific biases into the\noptimization procedure without changing the objective function. We derive G-MM\nalgorithms for several latent variable models and show empirically that they\nconsistently outperform their MM counterparts in optimizing non-convex\nobjectives. In particular, G-MM algorithms appear to be less sensitive to\ninitialization.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jun 2015 04:56:50 GMT"}, {"version": "v2", "created": "Thu, 28 Jul 2016 04:47:13 GMT"}, {"version": "v3", "created": "Fri, 17 May 2019 17:13:53 GMT"}], "update_date": "2019-05-20", "authors_parsed": [["Parizi", "Sobhan Naderi", ""], ["He", "Kun", ""], ["Aghajani", "Reza", ""], ["Sclaroff", "Stan", ""], ["Felzenszwalb", "Pedro", ""]]}, {"id": "1506.07656", "submitter": "Team Lear", "authors": "Jerome Revaud (LEAR), Philippe Weinzaepfel (LEAR), Zaid Harchaoui\n  (LEAR), Cordelia Schmid (LEAR)", "title": "DeepMatching: Hierarchical Deformable Dense Matching", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a novel matching algorithm, called DeepMatching, to compute\ndense correspondences between images. DeepMatching relies on a hierarchical,\nmulti-layer, correlational architecture designed for matching images and was\ninspired by deep convolutional approaches. The proposed matching algorithm can\nhandle non-rigid deformations and repetitive textures and efficiently\ndetermines dense correspondences in the presence of significant changes between\nimages. We evaluate the performance of DeepMatching, in comparison with\nstate-of-the-art matching algorithms, on the Mikolajczyk (Mikolajczyk et al\n2005), the MPI-Sintel (Butler et al 2012) and the Kitti (Geiger et al 2013)\ndatasets. DeepMatching outperforms the state-of-the-art algorithms and shows\nexcellent results in particular for repetitive textures.We also propose a\nmethod for estimating optical flow, called DeepFlow, by integrating\nDeepMatching in the large displacement optical flow (LDOF) approach of Brox and\nMalik (2011). Compared to existing matching algorithms, additional robustness\nto large displacements and complex motion is obtained thanks to our matching\napproach. DeepFlow obtains competitive performance on public benchmarks for\noptical flow estimation.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jun 2015 08:12:02 GMT"}, {"version": "v2", "created": "Thu, 8 Oct 2015 11:37:28 GMT"}], "update_date": "2015-10-12", "authors_parsed": [["Revaud", "Jerome", "", "LEAR"], ["Weinzaepfel", "Philippe", "", "LEAR"], ["Harchaoui", "Zaid", "", "LEAR"], ["Schmid", "Cordelia", "", "LEAR"]]}, {"id": "1506.07704", "submitter": "Donggeun Yoo", "authors": "Donggeun Yoo, Sunggyun Park, Joon-Young Lee, Anthony S. Paek, In So\n  Kweon", "title": "AttentionNet: Aggregating Weak Directions for Accurate Object Detection", "comments": "To appear in ICCV 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel detection method using a deep convolutional neural network\n(CNN), named AttentionNet. We cast an object detection problem as an iterative\nclassification problem, which is the most suitable form of a CNN. AttentionNet\nprovides quantized weak directions pointing a target object and the ensemble of\niterative predictions from AttentionNet converges to an accurate object\nboundary box. Since AttentionNet is a unified network for object detection, it\ndetects objects without any separated models from the object proposal to the\npost bounding-box regression. We evaluate AttentionNet by a human detection\ntask and achieve the state-of-the-art performance of 65% (AP) on PASCAL VOC\n2007/2012 with an 8-layered architecture only.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jun 2015 11:21:04 GMT"}, {"version": "v2", "created": "Sat, 26 Sep 2015 08:35:51 GMT"}], "update_date": "2015-09-29", "authors_parsed": [["Yoo", "Donggeun", ""], ["Park", "Sunggyun", ""], ["Lee", "Joon-Young", ""], ["Paek", "Anthony S.", ""], ["Kweon", "In So", ""]]}, {"id": "1506.07866", "submitter": "Gil Ben-Artzi", "authors": "Gil Ben-Artzi, Yoni Kasten, Shmuel Peleg, Michael Werman", "title": "Camera Calibration from Dynamic Silhouettes Using Motion Barcodes", "comments": "Update metadata", "journal-ref": "Proc. CVPR'16, Las Vegas, June 2016, pp. 4095-4103", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computing the epipolar geometry between cameras with very different\nviewpoints is often problematic as matching points are hard to find. In these\ncases, it has been proposed to use information from dynamic objects in the\nscene for suggesting point and line correspondences.\n  We propose a speed up of about two orders of magnitude, as well as an\nincrease in robustness and accuracy, to methods computing epipolar geometry\nfrom dynamic silhouettes. This improvement is based on a new temporal\nsignature: motion barcode for lines. Motion barcode is a binary temporal\nsequence for lines, indicating for each frame the existence of at least one\nforeground pixel on that line. The motion barcodes of two corresponding\nepipolar lines are very similar, so the search for corresponding epipolar lines\ncan be limited only to lines having similar barcodes. The use of motion\nbarcodes leads to increased speed, accuracy, and robustness in computing the\nepipolar geometry.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jun 2015 19:37:24 GMT"}, {"version": "v2", "created": "Sat, 7 Nov 2015 16:44:20 GMT"}, {"version": "v3", "created": "Tue, 24 Nov 2015 14:19:45 GMT"}, {"version": "v4", "created": "Sun, 8 Jan 2017 00:45:49 GMT"}], "update_date": "2017-01-10", "authors_parsed": [["Ben-Artzi", "Gil", ""], ["Kasten", "Yoni", ""], ["Peleg", "Shmuel", ""], ["Werman", "Michael", ""]]}, {"id": "1506.07950", "submitter": "Rafal Scherer", "authors": "Marcin Korytkowski, Rafal Scherer, Pawel Staszewski and Piotr Woldan", "title": "Bag-of-Features Image Indexing and Classification in Microsoft SQL\n  Server Relational Database", "comments": "2015 IEEE 2nd International Conference on Cybernetics (CYBCONF),\n  Gdynia, Poland, 24-26 June 2015", "journal-ref": null, "doi": "10.1109/CYBConf.2015.7175981", "report-no": null, "categories": "cs.DB cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a novel relational database architecture aimed to visual\nobjects classification and retrieval. The framework is based on the\nbag-of-features image representation model combined with the Support Vector\nMachine classification and is integrated in a Microsoft SQL Server database.\n", "versions": [{"version": "v1", "created": "Fri, 26 Jun 2015 03:24:39 GMT"}], "update_date": "2016-11-15", "authors_parsed": [["Korytkowski", "Marcin", ""], ["Scherer", "Rafal", ""], ["Staszewski", "Pawel", ""], ["Woldan", "Piotr", ""]]}, {"id": "1506.08006", "submitter": "Ali Boyali", "authors": "Ali Boyali", "title": "Spectral Collaborative Representation based Classification for Hand\n  Gestures recognition on Electromyography Signals", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this study, we introduce a novel variant and application of the\nCollaborative Representation based Classification in spectral domain for\nrecognition of the hand gestures using the raw surface Electromyography\nsignals. The intuitive use of spectral features are explained via circulant\nmatrices. The proposed Spectral Collaborative Representation based\nClassification (SCRC) is able to recognize gestures with higher levels of\naccuracy for a fairly rich gesture set. The worst recognition result which is\nthe best in the literature is obtained as 97.3\\% among the four sets of the\nexperiments for each hand gestures. The recognition results are reported with a\nsubstantial number of experiments and labeling computation.\n", "versions": [{"version": "v1", "created": "Fri, 26 Jun 2015 09:20:05 GMT"}], "update_date": "2015-06-29", "authors_parsed": [["Boyali", "Ali", ""]]}, {"id": "1506.08110", "submitter": "Richard Charles", "authors": "Richard M. Charles, Kye M. Taylor and James H. Curry", "title": "Nonnegative Matrix Factorization applied to reordered pixels of single\n  images based on patches to achieve structured nonnegative dictionaries", "comments": "34 pages, 15 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent improvements in computing allow for the processing and analysis of\nvery large datasets in a variety of fields. Often the analysis requires the\ncreation of low-rank approximations to the datasets leading to efficient\nstorage. This article presents and analyzes a novel approach for creating\nnonnegative, structured dictionaries using NMF applied to reordered pixels of\nsingle, natural images. We reorder the pixels based on patches and present our\napproach in general. We investigate our approach when using the Singular Value\nDecomposition (SVD) and Nonnegative Matrix Factorizations (NMF) as low-rank\napproximations. Peak Signal-to-Noise Ratio (PSNR) and Mean Structural\nSimilarity Index (MSSIM) are used to evaluate the algorithm. We report that\nwhile the SVD provides the best reconstructions, its dictionary of vectors lose\nboth the sign structure of the original image and details of localized image\ncontent. In contrast, the dictionaries produced using NMF preserves the sign\nstructure of the original image matrix and offer a nonnegative, parts-based\ndictionary.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jun 2015 17:27:11 GMT"}], "update_date": "2015-06-29", "authors_parsed": [["Charles", "Richard M.", ""], ["Taylor", "Kye M.", ""], ["Curry", "James H.", ""]]}, {"id": "1506.08301", "submitter": "Yilun Wang", "authors": "Yilun Wang, Zhiqiang Li, Yifeng Wang, Xiaona Wang, Junjie Zheng,\n  Xujuan Duan, Huafu Chen", "title": "A Novel Approach for Stable Selection of Informative Redundant Features\n  from High Dimensional fMRI Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Feature selection is among the most important components because it not only\nhelps enhance the classification accuracy, but also or even more important\nprovides potential biomarker discovery. However, traditional multivariate\nmethods is likely to obtain unstable and unreliable results in case of an\nextremely high dimensional feature space and very limited training samples,\nwhere the features are often correlated or redundant. In order to improve the\nstability, generalization and interpretations of the discovered potential\nbiomarker and enhance the robustness of the resultant classifier, the redundant\nbut informative features need to be also selected. Therefore we introduced a\nnovel feature selection method which combines a recent implementation of the\nstability selection approach and the elastic net approach. The advantage in\nterms of better control of false discoveries and missed discoveries of our\napproach, and the resulted better interpretability of the obtained potential\nbiomarker is verified in both synthetic and real fMRI experiments. In addition,\nwe are among the first to demonstrate the robustness of feature selection\nbenefiting from the incorporation of stability selection and also among the\nfirst to demonstrate the possible unrobustness of the classical univariate\ntwo-sample t-test method. Specifically, we show the robustness of our feature\nselection results in existence of noisy (wrong) training labels, as well as the\nrobustness of the resulted classifier based on our feature selection results in\nthe existence of data variation, demonstrated by a multi-center\nattention-deficit/hyperactivity disorder (ADHD) fMRI data.\n", "versions": [{"version": "v1", "created": "Sat, 27 Jun 2015 15:28:31 GMT"}, {"version": "v2", "created": "Wed, 25 May 2016 02:37:39 GMT"}], "update_date": "2016-05-26", "authors_parsed": [["Wang", "Yilun", ""], ["Li", "Zhiqiang", ""], ["Wang", "Yifeng", ""], ["Wang", "Xiaona", ""], ["Zheng", "Junjie", ""], ["Duan", "Xujuan", ""], ["Chen", "Huafu", ""]]}, {"id": "1506.08316", "submitter": "Jianshu Chao", "authors": "Jianshu Chao and Eckehard Steinbach", "title": "Keypoint Encoding for Improved Feature Extraction from Compressed Video\n  at Low Bitrates", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.CV cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many mobile visual analysis applications, compressed video is transmitted\nover a communication network and analyzed by a server. Typical processing steps\nperformed at the server include keypoint detection, descriptor calculation, and\nfeature matching. Video compression has been shown to have an adverse effect on\nfeature-matching performance. The negative impact of compression can be reduced\nby using the keypoints extracted from the uncompressed video to calculate\ndescriptors from the compressed video. Based on this observation, we propose to\nprovide these keypoints to the server as side information and to extract only\nthe descriptors from the compressed video. First, we introduce four different\nframe types for keypoint encoding to address different types of changes in\nvideo content. These frame types represent a new scene, the same scene, a\nslowly changing scene, or a rapidly moving scene and are determined by\ncomparing features between successive video frames. Then, we propose Intra,\nSkip and Inter modes of encoding the keypoints for different frame types. For\nexample, keypoints for new scenes are encoded using the Intra mode, and\nkeypoints for unchanged scenes are skipped. As a result, the bitrate of the\nside information related to keypoint encoding is significantly reduced.\nFinally, we present pairwise matching and image retrieval experiments conducted\nto evaluate the performance of the proposed approach using the Stanford mobile\naugmented reality dataset and 720p format videos. The results show that the\nproposed approach offers significantly improved feature matching and image\nretrieval performance at a given bitrate.\n", "versions": [{"version": "v1", "created": "Sat, 27 Jun 2015 17:33:34 GMT"}, {"version": "v2", "created": "Fri, 4 Mar 2016 16:43:42 GMT"}], "update_date": "2016-03-07", "authors_parsed": [["Chao", "Jianshu", ""], ["Steinbach", "Eckehard", ""]]}, {"id": "1506.08347", "submitter": "Golnaz Ghiasi", "authors": "Golnaz Ghiasi and Charless C. Fowlkes", "title": "Occlusion Coherence: Detecting and Localizing Occluded Faces", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The presence of occluders significantly impacts object recognition accuracy.\nHowever, occlusion is typically treated as an unstructured source of noise and\nexplicit models for occluders have lagged behind those for object appearance\nand shape. In this paper we describe a hierarchical deformable part model for\nface detection and landmark localization that explicitly models part occlusion.\nThe proposed model structure makes it possible to augment positive training\ndata with large numbers of synthetically occluded instances. This allows us to\neasily incorporate the statistics of occlusion patterns in a discriminatively\ntrained model. We test the model on several benchmarks for landmark\nlocalization and detection including challenging new data sets featuring\nsignificant occlusion. We find that the addition of an explicit occlusion model\nyields a detection system that outperforms existing approaches for occluded\ninstances while maintaining competitive accuracy in detection and landmark\nlocalization for unoccluded instances.\n", "versions": [{"version": "v1", "created": "Sun, 28 Jun 2015 03:12:34 GMT"}, {"version": "v2", "created": "Thu, 25 Aug 2016 00:27:35 GMT"}], "update_date": "2016-08-26", "authors_parsed": [["Ghiasi", "Golnaz", ""], ["Fowlkes", "Charless C.", ""]]}, {"id": "1506.08353", "submitter": "Haijuan Hu", "authors": "Haijuan Hu, Jacques Froment, Quansheng Liu", "title": "A note on patch-based low-rank minimization for fast image denoising", "comments": "4pages (two columns)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Patch-based low-rank minimization for image processing attracts much\nattention in recent years. The minimization of the matrix rank coupled with the\nFrobenius norm data fidelity can be solved by the hard thresholding filter with\nprinciple component analysis (PCA) or singular value decomposition (SVD). Based\non this idea, we propose a patch-based low-rank minimization method for image\ndenoising. The main denoising process is stated in three equivalent way: PCA,\nSVD and low-rank minimization. Compared to recent patch-based sparse\nrepresentation methods, experiments demonstrate that the proposed method is\nrather rapid, and it is effective for a variety of natural grayscale images and\ncolor images, especially for texture parts in images. Further improvements of\nthis method are also given. In addition, due to the simplicity of this method,\nwe could provide an explanation of the choice of the threshold parameter,\nestimation of PSNR values, and give other insights into this method.\n", "versions": [{"version": "v1", "created": "Sun, 28 Jun 2015 03:52:42 GMT"}, {"version": "v2", "created": "Wed, 21 Feb 2018 03:14:36 GMT"}], "update_date": "2018-02-22", "authors_parsed": [["Hu", "Haijuan", ""], ["Froment", "Jacques", ""], ["Liu", "Quansheng", ""]]}, {"id": "1506.08425", "submitter": "Chee Seng Chan", "authors": "Sue Han Lee, Chee Seng Chan, Paul Wilkin and Paolo Remagnino", "title": "Deep-Plant: Plant Identification with convolutional neural networks", "comments": "6 pages, 8 figures, accepted as oral presentation in ICIP2015,\n  Qu\\'ebec City, Canada", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies convolutional neural networks (CNN) to learn unsupervised\nfeature representations for 44 different plant species, collected at the Royal\nBotanic Gardens, Kew, England. To gain intuition on the chosen features from\nthe CNN model (opposed to a 'black box' solution), a visualisation technique\nbased on the deconvolutional networks (DN) is utilized. It is found that\nvenations of different order have been chosen to uniquely represent each of the\nplant species. Experimental results using these CNN features with different\nclassifiers show consistency and superiority compared to the state-of-the art\nsolutions which rely on hand-crafted features.\n", "versions": [{"version": "v1", "created": "Sun, 28 Jun 2015 16:58:47 GMT"}], "update_date": "2015-06-30", "authors_parsed": [["Lee", "Sue Han", ""], ["Chan", "Chee Seng", ""], ["Wilkin", "Paul", ""], ["Remagnino", "Paolo", ""]]}, {"id": "1506.08438", "submitter": "Ozan Sener", "authors": "Ozan Sener, Amir Zamir, Silvio Savarese, Ashutosh Saxena", "title": "Unsupervised Semantic Parsing of Video Collections", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human communication typically has an underlying structure. This is reflected\nin the fact that in many user generated videos, a starting point, ending, and\ncertain objective steps between these two can be identified. In this paper, we\npropose a method for parsing a video into such semantic steps in an\nunsupervised way. The proposed method is capable of providing a semantic\n\"storyline\" of the video composed of its objective steps. We accomplish this\nusing both visual and language cues in a joint generative model. The proposed\nmethod can also provide a textual description for each of the identified\nsemantic steps and video segments. We evaluate this method on a large number of\ncomplex YouTube videos and show results of unprecedented quality for this\nintricate and impactful problem.\n", "versions": [{"version": "v1", "created": "Sun, 28 Jun 2015 19:16:38 GMT"}, {"version": "v2", "created": "Tue, 7 Jul 2015 23:45:17 GMT"}, {"version": "v3", "created": "Mon, 10 Aug 2015 23:57:10 GMT"}, {"version": "v4", "created": "Wed, 27 Jan 2016 12:54:15 GMT"}], "update_date": "2016-01-28", "authors_parsed": [["Sener", "Ozan", ""], ["Zamir", "Amir", ""], ["Savarese", "Silvio", ""], ["Saxena", "Ashutosh", ""]]}, {"id": "1506.08485", "submitter": "Shachaf Melman", "authors": "Shachaf Melman, Yael Moses, G\\'erard Medioni and Yinghao Cai", "title": "The Multi-Strand Graph for a PTZ Tracker", "comments": "9 pages, 7 figures, AVSS2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High-resolution images can be used to resolve matching ambiguities between\ntrajectory fragments (tracklets), which is one of the main challenges in\nmultiple target tracking. A PTZ camera, which can pan, tilt and zoom, is a\npowerful and efficient tool that offers both close-up views and wide area\ncoverage on demand. The wide-area view makes it possible to track many targets\nwhile the close-up view allows individuals to be identified from\nhigh-resolution images of their faces. A central component of a PTZ tracking\nsystem is a scheduling algorithm that determines which target to zoom in on.\n  In this paper we study this scheduling problem from a theoretical\nperspective, where the high resolution images are also used for tracklet\nmatching. We propose a novel data structure, the Multi-Strand Tracking Graph\n(MSG), which represents the set of tracklets computed by a tracker and the\npossible associations between them. The MSG allows efficient scheduling as well\nas resolving -- directly or by elimination -- matching ambiguities between\ntracklets. The main feature of the MSG is the auxiliary data saved in each\nvertex, which allows efficient computation while avoiding time-consuming graph\ntraversal. Synthetic data simulations are used to evaluate our scheduling\nalgorithm and to demonstrate its superiority over a na\\\"ive one.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jun 2015 01:59:32 GMT"}], "update_date": "2015-06-30", "authors_parsed": [["Melman", "Shachaf", ""], ["Moses", "Yael", ""], ["Medioni", "G\u00e9rard", ""], ["Cai", "Yinghao", ""]]}, {"id": "1506.08529", "submitter": "Mohamed Elhoseiny Mohamed Elhoseiny", "authors": "Mohamed Elhoseiny, Ahmed Elgammal, Babak Saleh", "title": "Tell and Predict: Kernel Classifier Prediction for Unseen Visual Classes\n  from Unstructured Text Descriptions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose a framework for predicting kernelized classifiers in\nthe visual domain for categories with no training images where the knowledge\ncomes from textual description about these categories. Through our optimization\nframework, the proposed approach is capable of embedding the class-level\nknowledge from the text domain as kernel classifiers in the visual domain. We\nalso proposed a distributional semantic kernel between text descriptions which\nis shown to be effective in our setting. The proposed framework is not\nrestricted to textual descriptions, and can also be applied to other forms\nknowledge representations. Our approach was applied for the challenging task of\nzero-shot learning of fine-grained categories from text descriptions of these\ncategories.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jun 2015 07:51:28 GMT"}], "update_date": "2015-06-30", "authors_parsed": [["Elhoseiny", "Mohamed", ""], ["Elgammal", "Ahmed", ""], ["Saleh", "Babak", ""]]}, {"id": "1506.08581", "submitter": "Konstantinos Makantasis", "authors": "Konstantinos Makantasis, Anastasios Doulamis, Nikolaos Doulamis", "title": "Variational Inference for Background Subtraction in Infrared Imagery", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a Gaussian mixture model for background subtraction in infrared\nimagery. Following a Bayesian approach, our method automatically estimates the\nnumber of Gaussian components as well as their parameters, while simultaneously\nit avoids over/under fitting. The equations for estimating model parameters are\nanalytically derived and thus our method does not require any sampling\nalgorithm that is computationally and memory inefficient. The pixel density\nestimate is followed by an efficient and highly accurate updating mechanism,\nwhich permits our system to be automatically adapted to dynamically changing\noperation conditions. Experimental results and comparisons with other methods\nshow that our method outperforms, in terms of precision and recall, while at\nthe same time it keeps computational cost suitable for real-time applications.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jun 2015 11:16:41 GMT"}], "update_date": "2015-06-30", "authors_parsed": [["Makantasis", "Konstantinos", ""], ["Doulamis", "Anastasios", ""], ["Doulamis", "Nikolaos", ""]]}, {"id": "1506.08615", "submitter": "Ren\\'e Ciak", "authors": "Ren\\'e Ciak", "title": "Coercive functions from a topological viewpoint and properties of\n  minimizing sets of convex functions appearing in image restoration", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.CV math.CA math.FA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many tasks in image processing can be tackled by modeling an appropriate data\nfidelity term $\\Phi: \\mathbb{R}^n \\rightarrow \\mathbb{R} \\cup \\{+\\infty\\}$ and\nthen solve one of the regularized minimization problems \\begin{align*}\n  &{}(P_{1,\\tau}) \\qquad \\mathop{\\rm argmin}_{x \\in \\mathbb R^n} \\big\\{ \\Phi(x)\n\\;{\\rm s.t.}\\; \\Psi(x) \\leq \\tau \\big\\} \\\\ &{}(P_{2,\\lambda}) \\qquad\n\\mathop{\\rm argmin}_{x \\in \\mathbb R^n} \\{ \\Phi(x) + \\lambda \\Psi(x) \\}, \\;\n\\lambda > 0 \\end{align*} with some function $\\Psi: \\mathbb{R}^n \\rightarrow\n\\mathbb{R} \\cup \\{+\\infty\\}$ and a good choice of the parameter(s). Two tasks\narise naturally here: \\begin{align*} {}& \\text{1. Study the solver sets ${\\rm\nSOL}(P_{1,\\tau})$ and\n  ${\\rm SOL}(P_{2,\\lambda})$ of the minimization problems.} \\\\ {}& \\text{2.\nEnsure that the minimization problems have solutions.} \\end{align*} This thesis\nprovides contributions to both tasks: Regarding the first task for a more\nspecial setting we prove that there are intervals $(0,c)$ and $(0,d)$ such that\nthe setvalued curves \\begin{align*}\n  \\tau \\mapsto {}& {\\rm SOL}(P_{1,\\tau}), \\; \\tau \\in (0,c) \\\\ {} \\lambda\n\\mapsto {}& {\\rm SOL}(P_{2,\\lambda}), \\; \\lambda \\in (0,d) \\end{align*} are the\nsame, besides an order reversing parameter change $g: (0,c) \\rightarrow (0,d)$.\nMoreover we show that the solver sets are changing all the time while $\\tau$\nruns from $0$ to $c$ and $\\lambda$ runs from $d$ to $0$.\n  In the presence of lower semicontinuity the second task is done if we have\nadditionally coercivity. We regard lower semicontinuity and coercivity from a\ntopological point of view and develop a new technique for proving lower\nsemicontinuity plus coercivity.\n  Dropping any lower semicontinuity assumption we also prove a theorem on the\ncoercivity of a sum of functions.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jun 2015 18:16:41 GMT"}], "update_date": "2015-06-30", "authors_parsed": [["Ciak", "Ren\u00e9", ""]]}, {"id": "1506.08670", "submitter": "Furkan Isikdogan", "authors": "F. Isikdogan, A.C. Bovik, P. Passalacqua", "title": "Automatic Channel Network Extraction from Remotely Sensed Images by\n  Singularity Analysis", "comments": "IEEE Geosci. Remote Sens. Lett., in review", "journal-ref": "IEEE Geoscience and Remote Sensing Letters 12/11 (2015): 2218-2221", "doi": "10.1109/LGRS.2015.2458898", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quantitative analysis of channel networks plays an important role in river\nstudies. To provide a quantitative representation of channel networks, we\npropose a new method that extracts channels from remotely sensed images and\nestimates their widths. Our fully automated method is based on a recently\nproposed Multiscale Singularity Index that responds strongly to curvilinear\nstructures but weakly to edges. The algorithm produces a channel map, using a\nsingle image where water and non-water pixels have contrast, such as a Landsat\nnear-infrared band image or a water index defined on multiple bands. The\nproposed method provides a robust alternative to the procedures that are used\nin remote sensing of fluvial geomorphology and makes classification and\nanalysis of channel networks easier. The source code of the algorithm is\navailable at: http://live.ece.utexas.edu/research/cne/.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jun 2015 15:03:04 GMT"}], "update_date": "2019-11-15", "authors_parsed": [["Isikdogan", "F.", ""], ["Bovik", "A. C.", ""], ["Passalacqua", "P.", ""]]}, {"id": "1506.08682", "submitter": "Jayati Ghosh Dastidar", "authors": "Dhriti Sengupta, Merina Kundu, Jayati Ghosh Dastidar", "title": "Human Shape Variation - An Efficient Implementation using Skeleton", "comments": null, "journal-ref": "IJACR, Volume 4, Issue 14, March 2014, pp. 145-150", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is at times important to detect human presence automatically in secure\nenvironments. This needs a shape recognition algorithm that is robust, fast and\nhas low error rates. The algorithm needs to process camera images quickly to\ndetect any human in the range of vision, and generate alerts, especially if the\nobject under scrutiny is moving in certain directions. We present here a\nsimple, efficient and fast algorithm using skeletons of the images, and simple\nfeatures like posture and length of the object.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jun 2015 15:33:30 GMT"}], "update_date": "2015-06-30", "authors_parsed": [["Sengupta", "Dhriti", ""], ["Kundu", "Merina", ""], ["Dastidar", "Jayati Ghosh", ""]]}, {"id": "1506.08704", "submitter": "Jayati Ghosh Dastidar", "authors": "Subhajit Adhikari, Joydeep Kar, Jayati Ghosh Dastidar", "title": "An automatic and efficient foreground object extraction scheme", "comments": null, "journal-ref": "Subhajit Adhikari, Joydeep Kar, Jayati Ghosh Dastidar, \"An\n  automatic and efficient foreground object extraction scheme\", International\n  Journal of Science and Advanced Information Technology, 3 (2), 2014, pp.\n  40-43", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a method to differentiate the foreground objects from the\nbackground of a color image. Firstly a color image of any size is input for\nprocessing. The algorithm converts it to a grayscale image. Next we apply canny\nedge detector to find the boundary of the foreground object. We concentrate to\nfind the maximum distance between each boundary pixel column wise and row wise\nand we fill the region that is bound by the edges. Thus we are able to extract\nthe grayscale values of pixels that are in the bounded region and convert the\ngrayscale image back to original color image containing only the foreground\nobject.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jun 2015 16:01:35 GMT"}], "update_date": "2015-06-30", "authors_parsed": [["Adhikari", "Subhajit", ""], ["Kar", "Joydeep", ""], ["Dastidar", "Jayati Ghosh", ""]]}, {"id": "1506.08765", "submitter": "Federica Arrigoni", "authors": "Federica Arrigoni, Andrea Fusiello, Beatrice Rossi", "title": "Spectral Motion Synchronization in SE(3)", "comments": null, "journal-ref": "In SIAM Journal on Imaging Sciences, 9 (4): 1963-1990, 2016", "doi": "10.1137/16M1060248", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the problem of motion synchronization (or averaging) and\ndescribes a simple, closed-form solution based on a spectral decomposition,\nwhich does not consider rotation and translation separately but works straight\nin SE(3), the manifold of rigid motions. Besides its theoretical interest,\nbeing the first closed form solution in SE(3), experimental results show that\nit compares favourably with the state of the art both in terms of precision and\nspeed.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jun 2015 18:23:17 GMT"}], "update_date": "2017-02-14", "authors_parsed": [["Arrigoni", "Federica", ""], ["Fusiello", "Andrea", ""], ["Rossi", "Beatrice", ""]]}, {"id": "1506.08815", "submitter": "Jayati Ghosh Dastidar", "authors": "Merina Kundu, Dhriti Sengupta, Jayati Ghosh Dastidar", "title": "Tracking Direction of Human Movement - An Efficient Implementation using\n  Skeleton", "comments": "arXiv admin note: text overlap with arXiv:1506.08682", "journal-ref": "International Journal of Computer Applications 96(13):27-33, June\n  2014", "doi": "10.5120/16855-6722", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sometimes a simple and fast algorithm is required to detect human presence\nand movement with a low error rate in a controlled environment for security\npurposes. Here a light weight algorithm has been presented that generates alert\non detection of human presence and its movement towards a certain direction.\nThe algorithm uses fixed angle CCTV camera images taken over time and relies\nupon skeleton transformation of successive images and calculation of difference\nin their coordinates.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jun 2015 15:48:02 GMT"}], "update_date": "2015-07-01", "authors_parsed": [["Kundu", "Merina", ""], ["Sengupta", "Dhriti", ""], ["Dastidar", "Jayati Ghosh", ""]]}, {"id": "1506.08891", "submitter": "Miao Fan", "authors": "Miao Fan, Doo Soon Kim", "title": "Detecting Table Region in PDF Documents Using Distant Supervision", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Superior to state-of-the-art approaches which compete in table recognition\nwith 67 annotated government reports in PDF format released by {\\it ICDAR 2013\nTable Competition}, this paper contributes a novel paradigm leveraging\nlarge-scale unlabeled PDF documents to open-domain table detection. We\nintegrate the paradigm into our latest developed system ({\\it PdfExtra}) to\ndetect the region of tables by means of 9,466 academic articles from the entire\nrepository of {\\it ACL Anthology}, where almost all papers are archived by PDF\nformat without annotation for tables. The paradigm first designs heuristics to\nautomatically construct weakly labeled data. It then feeds diverse evidences,\nsuch as layouts of documents and linguistic features, which are extracted by\n{\\it Apache PDFBox} and processed by {\\it Stanford NLP} toolkit, into different\ncanonical classifiers. We finally use these classifiers, i.e. {\\it Naive\nBayes}, {\\it Logistic Regression} and {\\it Support Vector Machine}, to\ncollaboratively vote on the region of tables. Experimental results show that\n{\\it PdfExtra} achieves a great leap forward, compared with the\nstate-of-the-art approach. Moreover, we discuss the factors of different\nfeatures, learning models and even domains of documents that may impact the\nperformance. Extensive evaluations demonstrate that our paradigm is compatible\nenough to leverage various features and learning models for open-domain table\nregion detection within PDF files.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jun 2015 22:54:17 GMT"}, {"version": "v2", "created": "Thu, 2 Jul 2015 17:13:17 GMT"}, {"version": "v3", "created": "Mon, 6 Jul 2015 01:27:26 GMT"}, {"version": "v4", "created": "Thu, 9 Jul 2015 17:06:10 GMT"}, {"version": "v5", "created": "Mon, 27 Jul 2015 23:00:40 GMT"}, {"version": "v6", "created": "Tue, 22 Sep 2015 17:53:05 GMT"}], "update_date": "2015-09-23", "authors_parsed": [["Fan", "Miao", ""], ["Kim", "Doo Soon", ""]]}, {"id": "1506.08928", "submitter": "Sejong Yoon", "authors": "Changkyu Song and Sejong Yoon and Vladimir Pavlovic", "title": "Fast ADMM Algorithm for Distributed Optimization with Adaptive Penalty", "comments": "8 pages manuscript, 2 pages appendix, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose new methods to speed up convergence of the Alternating Direction\nMethod of Multipliers (ADMM), a common optimization tool in the context of\nlarge scale and distributed learning. The proposed method accelerates the speed\nof convergence by automatically deciding the constraint penalty needed for\nparameter consensus in each iteration. In addition, we also propose an\nextension of the method that adaptively determines the maximum number of\niterations to update the penalty. We show that this approach effectively leads\nto an adaptive, dynamic network topology underlying the distributed\noptimization. The utility of the new penalty update schemes is demonstrated on\nboth synthetic and real data, including a computer vision application of\ndistributed structure from motion.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jun 2015 03:37:07 GMT"}], "update_date": "2016-04-05", "authors_parsed": [["Song", "Changkyu", ""], ["Yoon", "Sejong", ""], ["Pavlovic", "Vladimir", ""]]}, {"id": "1506.08956", "submitter": "Libin Sun", "authors": "Libin Sun, Brian Guenter, Neel Joshi, Patrick Therien, James Hays", "title": "Lens Factory: Automatic Lens Generation Using Off-the-shelf Components", "comments": "12 pages, 18 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Custom optics is a necessity for many imaging applications. Unfortunately,\ncustom lens design is costly (thousands to tens of thousands of dollars), time\nconsuming (10-12 weeks typical lead time), and requires specialized optics\ndesign expertise. By using only inexpensive, off-the-shelf lens components the\nLens Factory automatic design system greatly reduces cost and time. Design,\nordering of parts, delivery, and assembly can be completed in a few days, at a\ncost in the low hundreds of dollars. Lens design constraints, such as focal\nlength and field of view, are specified in terms familiar to the graphics\ncommunity so no optics expertise is necessary. Unlike conventional lens design\nsystems, which only use continuous optimization methods, Lens Factory adds a\ndiscrete optimization stage. This stage searches the combinatorial space of\npossible combinations of lens elements to find novel designs, evolving simple\ncanonical lens designs into more complex, better designs. Intelligent pruning\nrules make the combinatorial search feasible. We have designed and built\nseveral high performance optical systems which demonstrate the practicality of\nthe system.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jun 2015 06:39:19 GMT"}, {"version": "v2", "created": "Thu, 16 Jul 2015 23:30:45 GMT"}], "update_date": "2015-07-20", "authors_parsed": [["Sun", "Libin", ""], ["Guenter", "Brian", ""], ["Joshi", "Neel", ""], ["Therien", "Patrick", ""], ["Hays", "James", ""]]}, {"id": "1506.08959", "submitter": "Linjie Yang", "authors": "Linjie Yang, Ping Luo, Chen Change Loy, Xiaoou Tang", "title": "A Large-Scale Car Dataset for Fine-Grained Categorization and\n  Verification", "comments": "An extension to our conference paper in CVPR 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Updated on 24/09/2015: This update provides preliminary experiment results\nfor fine-grained classification on the surveillance data of CompCars. The\ntrain/test splits are provided in the updated dataset. See details in Section\n6.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jun 2015 06:47:50 GMT"}, {"version": "v2", "created": "Thu, 24 Sep 2015 09:04:24 GMT"}], "update_date": "2015-09-25", "authors_parsed": [["Yang", "Linjie", ""], ["Luo", "Ping", ""], ["Loy", "Chen Change", ""], ["Tang", "Xiaoou", ""]]}, {"id": "1506.09016", "submitter": "Th\\'eo Trouillon", "authors": "Guillaume Bouchard, Th\\'eo Trouillon, Julien Perez, Adrien Gaidon", "title": "Online Learning to Sample", "comments": "Update: removed convergence theorem and proof as there is an error.\n  Submitted to UAI 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NA math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stochastic Gradient Descent (SGD) is one of the most widely used techniques\nfor online optimization in machine learning. In this work, we accelerate SGD by\nadaptively learning how to sample the most useful training examples at each\ntime step. First, we show that SGD can be used to learn the best possible\nsampling distribution of an importance sampling estimator. Second, we show that\nthe sampling distribution of an SGD algorithm can be estimated online by\nincrementally minimizing the variance of the gradient. The resulting algorithm\n- called Adaptive Weighted SGD (AW-SGD) - maintains a set of parameters to\noptimize, as well as a set of parameters to sample learning examples. We show\nthat AWSGD yields faster convergence in three different applications: (i) image\nclassification with deep features, where the sampling of images depends on\ntheir labels, (ii) matrix factorization, where rows and columns are not sampled\nuniformly, and (iii) reinforcement learning, where the optimized and\nexploration policies are estimated at the same time, where our approach\ncorresponds to an off-policy gradient algorithm.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jun 2015 10:08:35 GMT"}, {"version": "v2", "created": "Tue, 15 Mar 2016 16:08:56 GMT"}], "update_date": "2016-03-16", "authors_parsed": [["Bouchard", "Guillaume", ""], ["Trouillon", "Th\u00e9o", ""], ["Perez", "Julien", ""], ["Gaidon", "Adrien", ""]]}, {"id": "1506.09075", "submitter": "Byeongkeun Kang", "authors": "Yuanyuan Wu, Xiaohai He, Byeongkeun Kang, Haiying Song, and Truong Q.\n  Nguyen", "title": "Long-Range Motion Trajectories Extraction of Articulated Human Using\n  Mesh Evolution", "comments": "IEEE Signal Processing Letters", "journal-ref": null, "doi": "10.1109/LSP.2016.2536647", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This letter presents a novel approach to extract reliable dense and\nlong-range motion trajectories of articulated human in a video sequence.\nCompared with existing approaches that emphasize temporal consistency of each\ntracked point, we also consider the spatial structure of tracked points on the\narticulated human. We treat points as a set of vertices, and build a triangle\nmesh to join them in image space. The problem of extracting long-range motion\ntrajectories is changed to the issue of consistency of mesh evolution over\ntime. First, self-occlusion is detected by a novel mesh-based method and an\nadaptive motion estimation method is proposed to initialize mesh between\nsuccessive frames. Furthermore, we propose an iterative algorithm to\nefficiently adjust vertices of mesh for a physically plausible deformation,\nwhich can meet the local rigidity of mesh and silhouette constraints. Finally,\nwe compare the proposed method with the state-of-the-art methods on a set of\nchallenging sequences. Evaluations demonstrate that our method achieves\nfavorable performance in terms of both accuracy and integrity of extracted\ntrajectories.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jun 2015 13:18:18 GMT"}, {"version": "v2", "created": "Mon, 29 Feb 2016 17:10:11 GMT"}, {"version": "v3", "created": "Tue, 29 Mar 2016 00:21:40 GMT"}], "update_date": "2016-03-30", "authors_parsed": [["Wu", "Yuanyuan", ""], ["He", "Xiaohai", ""], ["Kang", "Byeongkeun", ""], ["Song", "Haiying", ""], ["Nguyen", "Truong Q.", ""]]}, {"id": "1506.09110", "submitter": "Alexander Wong", "authors": "Mohammad Javad Shafiee, Alexander Wong, and Paul Fieguth", "title": "Forming A Random Field via Stochastic Cliques: From Random Graphs to\n  Fully Connected Random Fields", "comments": "8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Random fields have remained a topic of great interest over past decades for\nthe purpose of structured inference, especially for problems such as image\nsegmentation. The local nodal interactions commonly used in such models often\nsuffer the short-boundary bias problem, which are tackled primarily through the\nincorporation of long-range nodal interactions. However, the issue of\ncomputational tractability becomes a significant issue when incorporating such\nlong-range nodal interactions, particularly when a large number of long-range\nnodal interactions (e.g., fully-connected random fields) are modeled.\n  In this work, we introduce a generalized random field framework based around\nthe concept of stochastic cliques, which addresses the issue of computational\ntractability when using fully-connected random fields by stochastically forming\na sparse representation of the random field. The proposed framework allows for\nefficient structured inference using fully-connected random fields without any\nrestrictions on the potential functions that can be utilized. Several\nrealizations of the proposed framework using graph cuts are presented and\nevaluated, and experimental results demonstrate that the proposed framework can\nprovide competitive performance for the purpose of image segmentation when\ncompared to existing fully-connected and principled deep random field\nframeworks.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jun 2015 14:52:33 GMT"}], "update_date": "2015-07-01", "authors_parsed": [["Shafiee", "Mohammad Javad", ""], ["Wong", "Alexander", ""], ["Fieguth", "Paul", ""]]}, {"id": "1506.09124", "submitter": "Saehoon Yi", "authors": "Saehoon Yi and Vladimir Pavlovic", "title": "Multi-Cue Structure Preserving MRF for Unconstrained Video Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video segmentation is a stepping stone to understanding video context. Video\nsegmentation enables one to represent a video by decomposing it into coherent\nregions which comprise whole or parts of objects. However, the challenge\noriginates from the fact that most of the video segmentation algorithms are\nbased on unsupervised learning due to expensive cost of pixelwise video\nannotation and intra-class variability within similar unconstrained video\nclasses. We propose a Markov Random Field model for unconstrained video\nsegmentation that relies on tight integration of multiple cues: vertices are\ndefined from contour based superpixels, unary potentials from temporal smooth\nlabel likelihood and pairwise potentials from global structure of a video.\nMulti-cue structure is a breakthrough to extracting coherent object regions for\nunconstrained videos in absence of supervision. Our experiments on VSB100\ndataset show that the proposed model significantly outperforms competing\nstate-of-the-art algorithms. Qualitative analysis illustrates that video\nsegmentation result of the proposed model is consistent with human perception\nof objects.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jun 2015 15:39:37 GMT"}], "update_date": "2015-07-01", "authors_parsed": [["Yi", "Saehoon", ""], ["Pavlovic", "Vladimir", ""]]}, {"id": "1506.09166", "submitter": "Ali Avanaki", "authors": "Ali R. N. Avanaki, Kathryn S. Espig, Sameer Sawhney, Liron\n  Pantanowitz, Anil V. Parwani, Albert Xthona, Tom R. L. Kimpe", "title": "Aging display's effect on interpretation of digital pathology slides", "comments": null, "journal-ref": null, "doi": "10.1117/12.2082315", "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is our conjecture that the variability of colors in a pathology image\neffects the interpretation of pathology cases, whether it is diagnostic\naccuracy, diagnostic confidence, or workflow efficiency. In this paper, digital\npathology images are analyzed to quantify the perceived difference in color\nthat occurs due to display aging, in particular a change in the maximum\nluminance, white point, and color gamut. The digital pathology images studied\ninclude diagnostically important features, such as the conspicuity of nuclei.\nThree different display aging models are applied to images: aging of luminance\n& chrominance, aging of chrominance only, and a stabilized luminance &\nchrominance (i.e., no aging). These display models and images are then used to\ncompare conspicuity of nuclei using CIE deltaE2000, a perceptual color\ndifference metric. The effect of display aging using these display models and\nimages is further analyzed through a human reader study designed to quantify\nthe effects from a clinical perspective. Results from our reader study indicate\nsignificant impact of aged displays on workflow as well as diagnosis as follow.\nAs compared to the originals (no-aging), slides with the effect of aging\nsimulated were significantly more difficult to read (p-value of 0.0005) and\ntook longer to score (p-value of 0.02). Moreover, luminance+chrominance aging\nsignificantly reduced inter-session percent agreement of diagnostic scores\n(p-value of 0.0418).\n", "versions": [{"version": "v1", "created": "Tue, 30 Jun 2015 17:29:43 GMT"}], "update_date": "2015-07-01", "authors_parsed": [["Avanaki", "Ali R. N.", ""], ["Espig", "Kathryn S.", ""], ["Sawhney", "Sameer", ""], ["Pantanowitz", "Liron", ""], ["Parwani", "Anil V.", ""], ["Xthona", "Albert", ""], ["Kimpe", "Tom R. L.", ""]]}, {"id": "1506.09169", "submitter": "Ali Avanaki", "authors": "Ali R. N. Avanaki, Kathryn S. Espig, Tom R. L. Kimpe, Andrew D. A.\n  Maidment", "title": "On anthropomorphic decision making in a model observer", "comments": null, "journal-ref": null, "doi": "10.1117/12.2082129", "report-no": null, "categories": "cs.CV cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  By analyzing human readers' performance in detecting small round lesions in\nsimulated digital breast tomosynthesis background in a location known exactly\nscenario, we have developed a model observer that is a better predictor of\nhuman performance with different levels of background complexity (i.e.,\nanatomical and quantum noise). Our analysis indicates that human observers\nperform a lesion detection task by combining a number of sub-decisions, each an\nindicator of the presence of a lesion in the image stack. This is in contrast\nto a channelized Hotelling observer, where the detection task is conducted\nholistically by thresholding a single decision variable, made from an optimally\nweighted linear combination of channels. However, it seems that the sub-par\nperformance of human readers compared to the CHO cannot be fully explained by\ntheir reliance on sub-decisions, or perhaps we do not consider a sufficient\nnumber of sub-decisions. To bridge the gap between the performances of human\nreaders and the model observer based upon sub-decisions, we use an additive\nnoise model, the power of which is modulated with the level of background\ncomplexity. The proposed model observer better predicts the fast drop in human\ndetection performance with background complexity.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jun 2015 17:36:33 GMT"}], "update_date": "2015-07-01", "authors_parsed": [["Avanaki", "Ali R. N.", ""], ["Espig", "Kathryn S.", ""], ["Kimpe", "Tom R. L.", ""], ["Maidment", "Andrew D. A.", ""]]}, {"id": "1506.09174", "submitter": "Jongpil Kim", "authors": "Jongpil Kim and Vladimir Pavlovic", "title": "Discovering Characteristic Landmarks on Ancient Coins using\n  Convolutional Networks", "comments": null, "journal-ref": null, "doi": "10.1117/1.JEI.26.1.011018", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a novel method to find characteristic landmarks on\nancient Roman imperial coins using deep convolutional neural network models\n(CNNs). We formulate an optimization problem to discover class-specific regions\nwhile guaranteeing specific controlled loss of accuracy. Analysis on\nvisualization of the discovered region confirms that not only can the proposed\nmethod successfully find a set of characteristic regions per class, but also\nthe discovered region is consistent with human expert annotations. We also\npropose a new framework to recognize the Roman coins which exploits\nhierarchical structure of the ancient Roman coins using the state-of-the-art\nclassification power of the CNNs adopted to a new task of coin classification.\nExperimental results show that the proposed framework is able to effectively\nrecognize the ancient Roman coins. For this research, we have collected a new\nRoman coin dataset where all coins are annotated and consist of observe (head)\nand reverse (tail) images.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jun 2015 17:41:12 GMT"}, {"version": "v2", "created": "Wed, 1 Jul 2015 01:10:13 GMT"}], "update_date": "2017-03-08", "authors_parsed": [["Kim", "Jongpil", ""], ["Pavlovic", "Vladimir", ""]]}, {"id": "1506.09179", "submitter": "Ali Madooei", "authors": "Ali Madooei, Mark S. Drew, Hossein Hajimirsadeghi", "title": "Learning to Detect Blue-white Structures in Dermoscopy Images with Weak\n  Supervision", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel approach to identify one of the most significant\ndermoscopic criteria in the diagnosis of Cutaneous Melanoma: the Blue-whitish\nstructure. In this paper, we achieve this goal in a Multiple Instance Learning\nframework using only image-level labels of whether the feature is present or\nnot. As the output, we predict the image classification label and as well\nlocalize the feature in the image. Experiments are conducted on a challenging\ndataset with results outperforming state-of-the-art. This study provides an\nimprovement on the scope of modelling for computerized image analysis of skin\nlesions, in particular in that it puts forward a framework for identification\nof dermoscopic local features from weakly-labelled data.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jun 2015 17:49:40 GMT"}], "update_date": "2015-07-01", "authors_parsed": [["Madooei", "Ali", ""], ["Drew", "Mark S.", ""], ["Hajimirsadeghi", "Hossein", ""]]}, {"id": "1506.09215", "submitter": "Simon Lacoste-Julien", "authors": "Jean-Baptiste Alayrac, Piotr Bojanowski, Nishant Agrawal, Josef Sivic,\n  Ivan Laptev, Simon Lacoste-Julien", "title": "Unsupervised Learning from Narrated Instruction Videos", "comments": "Appears in: 2016 IEEE Conference on Computer Vision and Pattern\n  Recognition (CVPR 2016). 21 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of automatically learning the main steps to complete a\ncertain task, such as changing a car tire, from a set of narrated instruction\nvideos. The contributions of this paper are three-fold. First, we develop a new\nunsupervised learning approach that takes advantage of the complementary nature\nof the input video and the associated narration. The method solves two\nclustering problems, one in text and one in video, applied one after each other\nand linked by joint constraints to obtain a single coherent sequence of steps\nin both modalities. Second, we collect and annotate a new challenging dataset\nof real-world instruction videos from the Internet. The dataset contains about\n800,000 frames for five different tasks that include complex interactions\nbetween people and objects, and are captured in a variety of indoor and outdoor\nsettings. Third, we experimentally demonstrate that the proposed method can\nautomatically discover, in an unsupervised manner, the main steps to achieve\nthe task and locate the steps in the input videos.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jun 2015 19:55:37 GMT"}, {"version": "v2", "created": "Thu, 2 Jul 2015 16:43:36 GMT"}, {"version": "v3", "created": "Mon, 30 Nov 2015 18:10:53 GMT"}, {"version": "v4", "created": "Tue, 28 Jun 2016 18:43:37 GMT"}], "update_date": "2016-06-29", "authors_parsed": [["Alayrac", "Jean-Baptiste", ""], ["Bojanowski", "Piotr", ""], ["Agrawal", "Nishant", ""], ["Sivic", "Josef", ""], ["Laptev", "Ivan", ""], ["Lacoste-Julien", "Simon", ""]]}]